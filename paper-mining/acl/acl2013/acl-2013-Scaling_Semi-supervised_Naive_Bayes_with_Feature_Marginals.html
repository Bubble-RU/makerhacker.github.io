<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>309 acl-2013-Scaling Semi-supervised Naive Bayes with Feature Marginals</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-309" href="#">acl2013-309</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>309 acl-2013-Scaling Semi-supervised Naive Bayes with Feature Marginals</h1>
<br/><p>Source: <a title="acl-2013-309-pdf" href="http://aclweb.org/anthology//P/P13/P13-1034.pdf">pdf</a></p><p>Author: Michael Lucas ; Doug Downey</p><p>Abstract: Semi-supervised learning (SSL) methods augment standard machine learning (ML) techniques to leverage unlabeled data. SSL techniques are often effective in text classification, where labeled data is scarce but large unlabeled corpora are readily available. However, existing SSL techniques typically require multiple passes over the entirety of the unlabeled data, meaning the techniques are not applicable to large corpora being produced today. In this paper, we show that improving marginal word frequency estimates using unlabeled data can enable semi-supervised text classification that scales to massive unlabeled data sets. We present a novel learning algorithm, which optimizes a Naive Bayes model to accord with statistics calculated from the unlabeled corpus. In experiments with text topic classification and sentiment analysis, we show that our method is both more scalable and more accurate than SSL techniques from previous work.</p><p>Reference: <a title="acl-2013-309-reference" href="../acl2013_reference/acl-2013-Scaling_Semi-supervised_Naive_Bayes_with_Feature_Marginals_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract Semi-supervised learning (SSL) methods augment standard machine learning (ML) techniques to leverage unlabeled data. [sent-5, score-0.373]
</p><p>2 SSL techniques are often effective in text classification, where labeled data is scarce but large unlabeled corpora are readily available. [sent-6, score-0.432]
</p><p>3 However, existing SSL techniques typically require multiple passes over the entirety of the unlabeled data, meaning the techniques are not applicable to large corpora being produced today. [sent-7, score-0.498]
</p><p>4 In this paper, we show that improving marginal word frequency estimates using unlabeled data can enable semi-supervised text classification that scales to massive  unlabeled data sets. [sent-8, score-1.114]
</p><p>5 We present a novel learning algorithm, which optimizes a Naive Bayes model to accord with statistics calculated from the unlabeled corpus. [sent-9, score-0.473]
</p><p>6 In experiments with text topic classification and sentiment analysis, we show that our method is both more scalable and more accurate than SSL techniques from previous work. [sent-10, score-0.266]
</p><p>7 1 Introduction Semi-supervised Learning (SSL) is a Machine Learning (ML) approach that utilizes large amounts of unlabeled data, combined with a smaller amount of labeled data, to learn a target function (Zhu, 2006; Chapelle et al. [sent-11, score-0.438]
</p><p>8 Experiments in text classification and other domains have demonstrated that by leveraging unlabeled data, SSL techniques improve machine  learning performance when human input is limited northwe stern . [sent-14, score-0.468]
</p><p>9 Typically, for each target concept to be learned, a semi-supervised classifier is trained using iterative techniques that execute multiple passes over the unlabeled data (e. [sent-20, score-0.477]
</p><p>10 This is problematic for text classification over large unlabeled corpora like the Web: new target concepts (new tasks and new topics of interest) arise frequently, and performing even a single pass over a large corpus for each new target concept is intractable. [sent-24, score-0.428]
</p><p>11 In this paper, we present a new SSL text classification approach that scales to large corpora. [sent-25, score-0.13]
</p><p>12 Instead of utilizing unlabeled examples directly for each given target concept, our approach is to precompute a small set of statistics over the unlabeled data in advance. [sent-26, score-0.769]
</p><p>13 Then, for a given target class and labeled data set, we utilize the statistics to improve a classifier. [sent-27, score-0.182]
</p><p>14 Specifically, we introduce a method that extends Multinomial Naive Bayes (MNB) to leverage marginal probability statistics P(w) of each word w, computed over the unlabeled data. [sent-28, score-0.59]
</p><p>15 The marginal statistics are used as a constraint to improve the class-conditional probability estimates P(w|+) and P(w| −) for the positive and negative classes, )w anhdich P are |o−ft)en fo noisy pwosheitinv eest ainmda nteedg over sparse labeled data sets. [sent-29, score-0.644]
</p><p>16 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioinngauli Lsitnicgsu,i psatgices 343–351, FM improves accuracy, and find that surprisingly MNB-FM is especially useful for improving classconditional probability estimates for words that  never occur in the training set. [sent-35, score-0.256]
</p><p>17 2  Problem Definition  We consider a semi-supervised classification task, in which the goal is to produce a mapping from an instance space X consisting of T-tuples forfo non-negative integer-valued tfienagtu orfes T w = (w1, . [sent-41, score-0.095]
</p><p>18 We assume the following inputs: • A set of zero or more labeled documents DL = o{(fw zedr, oyd) o |d = 1, . [sent-47, score-0.1]
</p><p>19 •  A large set of unlabeled documents DU = A{(w ladrg) |d = n 1, . [sent-57, score-0.374]
</p><p>20 , n u} durmaewnnt sfro Dm the  +  +  marginal distribution P(w)  =  XP(w,y). [sent-60, score-0.124]
</p><p>21 Our semi-supervised technique utilizes statistics computed over the labeled corpus, denoted as follows. [sent-63, score-0.203]
</p><p>22 We use Nw+ to denote the sum of the occurrences of word w over all documents in the positiveP class in the labeled data DL. [sent-64, score-0.171]
</p><p>23 Also, let N+ = Pnw∈DL Nw+ be the sum value of all word counts iPn wth∈eD labeled positive documents. [sent-65, score-0.145]
</p><p>24 The count ofP the remaining words in the positive documents is represented as N¬+w = N+ Nw+. [sent-66, score-0.127]
</p><p>25 −  3  MNB with Feature Marginals  We now introduce our algorithm, which scalably utilizes large unlabeled data stores for classification tasks. [sent-68, score-0.474]
</p><p>26 1 MNB-FM Method In the text classification setting , each feature value wd represents count of observations of word w in document d. [sent-71, score-0.127]
</p><p>27 |L+e)t P(+) denote the prior probability that a document is of the positive class, and P(−) = 1−P(+) the prior hfoer ptohes negative c,l aansds. [sent-75, score-0.214]
</p><p>28 PTh(−en) M =N 1B− represents the class probability of an example as:  Y(θw+)wdP(+) P(+|d) =Y(θw−)wdwYP∈d(−) +Y(θw+)wdP(+) wY∈d  wY∈d  (1)  MNB estimates the parameters θw+ from the corresponding counts in the training set. [sent-76, score-0.298]
</p><p>29 The maximum-likelihood estimate of θw+ is Nw+/N+, and to prevent zero-probability estimates we employ “add-1” smoothing (typical in MNB) to obtain the estimate:  θ+w=NN+w+++ |T 1|. [sent-77, score-0.236]
</p><p>30 MNB-FM attempts to improve MNB’s estimates of θw+ and θw−, using statistics computed over the unlabeled data. [sent-79, score-0.596]
</p><p>31 Formally, MNB-FM leverages the equality: P(w) = θw+Pt(+) + θw−Pt(−)  (2)  The left-hand-side of Equation 2, P(w), represents the probability that a given randomly drawn token from the unlabeled data happens to be the word w. [sent-80, score-0.368]
</p><p>32 Note that Pt(+) can differ from P(+), the prior probability that a document is positive, due to variations in document length. [sent-84, score-0.099]
</p><p>33 MN(−B)-F iMs d eis-  34m4otivated by the insight that the left-hand-side of Equation 2 can be estimated in advance, without knowledge of the target class, simply by counting the number of tokens of each word in the unlabeled data. [sent-86, score-0.333]
</p><p>34 MNB-FM attempts to improve the noisy estimates θw+ and θw− utilizing the robust estimate for P(w)  computed over unlabeled data. [sent-90, score-0.611]
</p><p>35 Specifically, MNB-FM proceeds by assuming the MLEs for P(w) (computed over unlabeled data), Pt(+), and Pt(−) are correct, and reestimates θw+ and θw− un(d−er) )th aer eco cnostrrreaicntt, i ann Equation 2. [sent-91, score-0.356]
</p><p>36 In that case, we default to the add-1 Smoothing estimates used by MNB. [sent-96, score-0.165]
</p><p>37 Finally, after optimizing the values θw+ and θw− for each word w as described above, we normalize the estimates to obtain valid conditional probability distributions, i. [sent-97, score-0.2]
</p><p>38 2 PMNB-FMP PExample The following concrete example illustrates how MNB-FM can improve MNB parameters using the statistic P(w) computed over unlabeled data. [sent-100, score-0.379]
</p><p>39 The example comes from the Reuters Aptemod text classification task addressed in Section 4, using bag-of-words features for the Earnings class. [sent-101, score-0.095]
</p><p>40 In one experiment with 10 labeled training examples, we observed 5 positive and 5 negative examples,  with the word “resources” occurring three times in the set (once in the positive class, twice in the negative class). [sent-102, score-0.38]
</p><p>41 MNB uses add-1 smoothing to estimate the conditional probability of the word “resources” in each class as θw+ = = 5. [sent-103, score-0.177]
</p><p>42 Yet because MNB estimates its parameters from only the sparse training data, it can be inaccurate. [sent-114, score-0.229]
</p><p>43 The optimization in MNB-FM seeks to accord its parameter estimates with the feature frequency, computed from unlabeled data, of P(w) = 4. [sent-115, score-0.608]
</p><p>44 We see that compared with P(w), the θw+ and θw− that MNB estimates from the training data are  argθ+mwaxN w +−ln(θK+w)−+LNθ+w¬) +lnN(1¬− wlθn+w()1+−K+Lθw+)s(bFeuoratvshae dtior ,n tshl)aoeinwsmoscbayxmui erawleumhn aoctselmtickaoenrulienhorteodlifear2belos etfuimt hoa ftneg5tn4fhio7atruodθfboew−. [sent-117, score-0.192]
</p><p>45 The above example illustrates how MNB-FM can leverage frequency marginal statistics computed over unlabeled data to improve MNB’s conditional probability estimates. [sent-125, score-0.619]
</p><p>46 We analyze how frequently MNB-FM succeeds in improving MNB’s estimates in practice, and the resulting impact on classification accuracy, below. [sent-126, score-0.26]
</p><p>47 1 Data Sets  We evaluate on two text classification tasks: topic classification, and sentiment detection. [sent-130, score-0.188]
</p><p>48 , in a binary classification setting) for each topic and measure classification performance for each class individually. [sent-134, score-0.292]
</p><p>49 The sentiment detection task is to determine whether a document is written with a positive or negative sentiment. [sent-135, score-0.241]
</p><p>50 1 RCV1 The Reuters RCV1 corpus is a standard large corpus used for topic classification evaluations (Lewis et al. [sent-139, score-0.126]
</p><p>51 We consider the 5 largest base classes after punctuation and stopwords were removed. [sent-142, score-0.103]
</p><p>52 2 Reuters Aptemod While MNB-FM is designed to improve the scalability of SSL to large corpora, some of the comparison methods from previous work were not tractable on the large topic classification data set RCV1 . [sent-147, score-0.207]
</p><p>53 In the Amazon Sentiment Classification data set, the task is to determine whether a review is positive or negative based solely on the reviewer’s submitted text. [sent-176, score-0.147]
</p><p>54 As such, the positive and negative  Class# Instances# PositiveVocabulary Music124362113997 (91. [sent-177, score-0.147]
</p><p>55 For our metrics, we calculate the scores for both the positive and negative class and report the average of the two (in contrast to the Reuters data sets, in which we only report the scores for the positive class). [sent-188, score-0.304]
</p><p>56 We also experimented with different weighting factors to assign to the unlabeled data. [sent-197, score-0.357]
</p><p>57 While performing per-data-split cross-validation was computationally prohibitive for NB+EM, we performed experiments on one class from each data set that revealed weighting unlabeled examples at 1/5 the weight of a labeled example performed best. [sent-198, score-0.513]
</p><p>58 3 Label Propagation For our large unlabeled data set sizes, we found that a standard Label Propogation (LP) approach, which considers propagating information between all pairs of unlabeled examples, was not tractable. [sent-209, score-0.666]
</p><p>59 Even with these aggressive constraints, Label Propagation was intractable to execute on some of the larger data sets, so we do not report LP results for the RCV1 dataset or for the 5 largest Amazon categories. [sent-215, score-0.119]
</p><p>60 SFE also augments multinomial Naive Bayes with the frequency information P(w), although in a manner distinct from MNB-FM. [sent-223, score-0.101]
</p><p>61 In particular, SFE uses the equality P(+|w) = P(+, w)/P(w) aSnFdE e usstiemsa thtees tqhuea rlhitsy using P(w) computed over  all the unlabeled data, rather than using only labeled data as in standard MNB. [sent-224, score-0.462]
</p><p>62 The primary distinction between MNB-FM and SFE is that SFE adjusts sparse estimates P(+, w) in the same way as non-sparse estimates, whereas MNB-FM is designed to adjust sparse estimates more than nonsparse ones. [sent-225, score-0.404]
</p><p>63 Further, it can be shown that as P(w) of a word w in the unlabeled data becomes larger than that in the labeled data, SFE’s estimate of the ratio P(w|+)/P(w| −) approaches one. [sent-226, score-0.434]
</p><p>64 Each set included at  34le7ast one positive and one negative document. [sent-232, score-0.147]
</p><p>65 These experiments are limited to the 5 largest base classes and show the F1 performance of MNB-FM and the various comparison methods, excluding Label Propagation which was intractable on this data set. [sent-292, score-0.104]
</p><p>66 The results show the runtimes of the SSL methods discussed in this paper as the size of the unlabeled dataset grows. [sent-376, score-0.407]
</p><p>67 As expected, we find that MNB-FM has runtime similar to MNB, and scales much better than methods that take multiple passes over the unlabeled data. [sent-377, score-0.418]
</p><p>68 MNB-FM improves the con-  ditional probability estimates in MNB and, surprisingly, we found that it can often improve these estimates for words that do not even occur in the training set. [sent-379, score-0.421]
</p><p>69 Tables 8 and 9 show the details of the improvements MNB-FM makes on the feature marginal estimates. [sent-380, score-0.156]
</p><p>70 We ran MNB-FM and MNB on the RCV1 class MCAT and stored the computed feature marginals for direct comparison. [sent-381, score-0.189]
</p><p>71 From the data, we can see that MNB-FM improves the estimates for many words not seen in the training set as well as the most common words, even with small training sets. [sent-387, score-0.248]
</p><p>72 fraction of positive  documents classified correctly) of the R highestranked test documents, where R is the total number of positive test documents. [sent-392, score-0.213]
</p><p>73 “Known” iTnadbicleat 8e:s Awonardlys occurring uinr e bo Mtha positive manprdo negative training examples, M“HNaBlf K(|Dnow|n =” in 10di)c. [sent-405, score-0.174]
</p><p>74 at “eKs nwowordns” occurring in only positive or negative training examples, while “Unknown” indicates words that never occur in labelled examples. [sent-406, score-0.174]
</p><p>75 MNB-FM improves estimates by a substantial amount for unknown words and also the most common known and half-known words. [sent-408, score-0.194]
</p><p>76 However, these experiments show that MNB-FM offers more advantages in document classification than in document ranking. [sent-424, score-0.159]
</p><p>77 However, LR underperforms in classification tasks (in terms of F1, Tables 4-6). [sent-426, score-0.095]
</p><p>78 The reason for this is that LR’s learned classification threshold becomes less accurate when datasets are small and classes are highly ClassMNB-FM SFEMNB NBEM LProp Logist. [sent-427, score-0.137]
</p><p>79 6  Related Work  To our knowledge, MNB-FM is the first approach that utilizes a small set of statistics computed over Data SetMNB-FM SFEMNB NBEM Logist. [sent-485, score-0.144]
</p><p>80 689 Table 12: RCV1 : R-Precision, DL= 100 349 a large unlabeled data set as constraints to improve a semi-supervised classifier. [sent-547, score-0.333]
</p><p>81 Our experiments demonstrate that MNB-FM outperforms previous approaches across multiple text classification techniques including topic classification and sentiment analysis. [sent-548, score-0.323]
</p><p>82 identifying a small number of representative unlabeled examples (Liu et al. [sent-555, score-0.359]
</p><p>83 In general, these techniques require passes over the entirety of the unlabeled data for each new learn-  ing task, intractable for massive unlabeled data sets. [sent-557, score-0.827]
</p><p>84 Naive implementations of LP cannot scale to large unlabeled data sets, as they have time complexity that increases quadratically with the number of unlabeled examples. [sent-558, score-0.666]
</p><p>85 Recent LP techniques have achieved greater scalability through the use of parallel processing and heuristics such as Approximate-Nearest Neighbor (Subramanya and Bilmes, 2009), or by decomposing the similarity matrix (Lin and Cohen, 2011). [sent-559, score-0.121]
</p><p>86 Our approach, by contrast, is to pre-compute a small set of marginal statistics over the unlabeled data, which eliminates the need to scan unlabeled data for each new task. [sent-560, score-0.842]
</p><p>87 propose the Semisupervised Frequency Estimate (SFE), which like MNB-FM utilizes the marginal probabilities of features computed from unlabeled data to improve the Multinomial Naive Bayes (MNB) classifier (Su et al. [sent-563, score-0.573]
</p><p>88 However, unlike  our approach, SFE does not compute maximumlikelihood estimates using the marginal statistics as a constraint. [sent-566, score-0.341]
</p><p>89 A distinct method for pre-processing unlabeled data in order to help scale semi-supervised learning techniques involves dimensionality reduction or manifold learning (Belkin and Niyogi, 2004), and for NLP tasks, identifying word representations from unlabeled data (Turian et al. [sent-568, score-0.706]
</p><p>90 In contrast to these approaches, MNB-FM preserves the original feature set and is more scalable (the marginal statistics can be computed in a single pass over the unlabeled data set). [sent-570, score-0.593]
</p><p>91 7  Conclusion  We presented a novel algorithm for efficiently leveraging large unlabeled data sets for semisupervised learning. [sent-571, score-0.388]
</p><p>92 Our MNB-FM technique optimizes a Multinomial Naive Bayes model to accord with statistics of the unlabeled corpus. [sent-572, score-0.473]
</p><p>93 In experiments across topic classification and sentiment analysis, MNB-FM was found to be more accu-  rate and more scalable than several supervised and semi-supervised baselines from previous work. [sent-573, score-0.226]
</p><p>94 In future work, we plan to explore utilizing richer statistics from the unlabeled data, beyond word marginals. [sent-574, score-0.41]
</p><p>95 Further, we plan to experiment with techniques for unlabeled data sets that also include continuous-valued features. [sent-575, score-0.373]
</p><p>96 Lastly, we also wish to explore ensemble approaches that combine the best supervised classifiers with the improved class-conditional estimates provided by MNB-FM. [sent-576, score-0.165]
</p><p>97 Transductive inference for text classification using support vector machines. [sent-595, score-0.095]
</p><p>98 Text classification from labeled and unlabeled documents using em. [sent-621, score-0.528]
</p><p>99 Large scale text classification using semisupervised multinomial naive bayes. [sent-627, score-0.331]
</p><p>100 Learning from labeled and unlabeled data with label propagation. [sent-648, score-0.421]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mnb', 0.507), ('unlabeled', 0.333), ('ssl', 0.247), ('sfe', 0.227), ('estimates', 0.165), ('aptemod', 0.15), ('ecat', 0.15), ('gpol', 0.15), ('mcat', 0.15), ('amzn', 0.129), ('nbem', 0.129), ('sfemnb', 0.129), ('dl', 0.128), ('marginal', 0.124), ('apte', 0.114), ('nw', 0.11), ('naive', 0.109), ('gcat', 0.107), ('classification', 0.095), ('nigam', 0.087), ('positive', 0.086), ('nb', 0.086), ('reuters', 0.084), ('pt', 0.082), ('scalability', 0.081), ('bayes', 0.079), ('multinomial', 0.072), ('marginals', 0.072), ('class', 0.071), ('propagation', 0.067), ('lp', 0.065), ('accord', 0.064), ('mnbfm', 0.064), ('amazon', 0.064), ('sentiment', 0.062), ('negative', 0.061), ('ln', 0.061), ('labeled', 0.059), ('semisupervised', 0.055), ('lr', 0.055), ('statistics', 0.052), ('logistic', 0.051), ('em', 0.051), ('passes', 0.05), ('earnings', 0.047), ('runtimes', 0.047), ('utilizes', 0.046), ('computed', 0.046), ('lprop', 0.043), ('wdp', 0.043), ('estimate', 0.042), ('classes', 0.042), ('equation', 0.041), ('regression', 0.041), ('documents', 0.041), ('techniques', 0.04), ('su', 0.038), ('scalable', 0.038), ('sparse', 0.037), ('intractable', 0.036), ('mann', 0.036), ('stopwords', 0.035), ('scales', 0.035), ('entirety', 0.035), ('tables', 0.035), ('probability', 0.035), ('belkin', 0.033), ('document', 0.032), ('details', 0.032), ('chapelle', 0.031), ('topic', 0.031), ('ml', 0.031), ('execute', 0.03), ('yiming', 0.03), ('frequency', 0.029), ('label', 0.029), ('improves', 0.029), ('gw', 0.029), ('subramanya', 0.029), ('transductive', 0.029), ('smoothing', 0.029), ('zhu', 0.028), ('wy', 0.028), ('lk', 0.028), ('ghahramani', 0.028), ('training', 0.027), ('dataset', 0.027), ('turian', 0.026), ('examples', 0.026), ('largest', 0.026), ('utilizing', 0.025), ('constraint', 0.025), ('icml', 0.025), ('optimizes', 0.024), ('classifier', 0.024), ('weighting', 0.024), ('equality', 0.024), ('lewis', 0.024), ('blitzer', 0.023), ('proceeds', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="309-tfidf-1" href="./acl-2013-Scaling_Semi-supervised_Naive_Bayes_with_Feature_Marginals.html">309 acl-2013-Scaling Semi-supervised Naive Bayes with Feature Marginals</a></p>
<p>Author: Michael Lucas ; Doug Downey</p><p>Abstract: Semi-supervised learning (SSL) methods augment standard machine learning (ML) techniques to leverage unlabeled data. SSL techniques are often effective in text classification, where labeled data is scarce but large unlabeled corpora are readily available. However, existing SSL techniques typically require multiple passes over the entirety of the unlabeled data, meaning the techniques are not applicable to large corpora being produced today. In this paper, we show that improving marginal word frequency estimates using unlabeled data can enable semi-supervised text classification that scales to massive unlabeled data sets. We present a novel learning algorithm, which optimizes a Naive Bayes model to accord with statistics calculated from the unlabeled corpus. In experiments with text topic classification and sentiment analysis, we show that our method is both more scalable and more accurate than SSL techniques from previous work.</p><p>2 0.19387057 <a title="309-tfidf-2" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>Author: Xiaodong Zeng ; Derek F. Wong ; Lidia S. Chao ; Isabel Trancoso</p><p>Abstract: This paper introduces a graph-based semisupervised joint model of Chinese word segmentation and part-of-speech tagging. The proposed approach is based on a graph-based label propagation technique. One constructs a nearest-neighbor similarity graph over all trigrams of labeled and unlabeled data for propagating syntactic information, i.e., label distributions. The derived label distributions are regarded as virtual evidences to regularize the learning of linear conditional random fields (CRFs) on unlabeled data. An inductive character-based joint model is obtained eventually. Empirical results on Chinese tree bank (CTB-7) and Microsoft Research corpora (MSR) reveal that the proposed model can yield better results than the supervised baselines and other competitive semi-supervised CRFs in this task.</p><p>3 0.18548104 <a title="309-tfidf-3" href="./acl-2013-Text_Classification_from_Positive_and_Unlabeled_Data_using_Misclassified_Data_Correction.html">342 acl-2013-Text Classification from Positive and Unlabeled Data using Misclassified Data Correction</a></p>
<p>Author: Fumiyo Fukumoto ; Yoshimi Suzuki ; Suguru Matsuyoshi</p><p>Abstract: This paper addresses the problem of dealing with a collection of labeled training documents, especially annotating negative training documents and presents a method of text classification from positive and unlabeled data. We applied an error detection and correction technique to the results of positive and negative documents classified by the Support Vector Machines (SVM). The results using Reuters documents showed that the method was comparable to the current state-of-the-art biasedSVM method as the F-score obtained by our method was 0.627 and biased-SVM was 0.614.</p><p>4 0.16883054 <a title="309-tfidf-4" href="./acl-2013-Semi-Supervised_Semantic_Tagging_of_Conversational_Understanding_using_Markov_Topic_Regression.html">315 acl-2013-Semi-Supervised Semantic Tagging of Conversational Understanding using Markov Topic Regression</a></p>
<p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur ; Gokhan Tur ; Ruhi Sarikaya</p><p>Abstract: Microsoft Research Microsoft Mountain View, CA, USA Redmond, WA, USA dilek @ ieee .org rus arika@mi cro s o ft . com gokhan .tur @ ieee .org performance (Tur and DeMori, 2011). This requires a tedious and time intensive data collection Finding concepts in natural language utterances is a challenging task, especially given the scarcity of labeled data for learning semantic ambiguity. Furthermore, data mismatch issues, which arise when the expected test (target) data does not exactly match the training data, aggravate this scarcity problem. To deal with these issues, we describe an efficient semisupervised learning (SSL) approach which has two components: (i) Markov Topic Regression is a new probabilistic model to cluster words into semantic tags (concepts). It can efficiently handle semantic ambiguity by extending standard topic models with two new features. First, it encodes word n-gram features from labeled source and unlabeled target data. Second, by going beyond a bag-of-words approach, it takes into account the inherent sequential nature of utterances to learn semantic classes based on context. (ii) Retrospective Learner is a new learning technique that adapts to the unlabeled target data. Our new SSL approach improves semantic tagging performance by 3% absolute over the baseline models, and also compares favorably on semi-supervised syntactic tagging.</p><p>5 0.11726753 <a title="309-tfidf-5" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>Author: Xiaodong Zeng ; Derek F. Wong ; Lidia S. Chao ; Isabel Trancoso</p><p>Abstract: This paper presents a semi-supervised Chinese word segmentation (CWS) approach that co-regularizes character-based and word-based models. Similarly to multi-view learning, the “segmentation agreements” between the two different types of view are used to overcome the scarcity of the label information on unlabeled data. The proposed approach trains a character-based and word-based model on labeled data, respectively, as the initial models. Then, the two models are constantly updated using unlabeled examples, where the learning objective is maximizing their segmentation agreements. The agreements are regarded as a set of valuable constraints for regularizing the learning of both models on unlabeled data. The segmentation for an input sentence is decoded by using a joint scoring function combining the two induced models. The evaluation on the Chinese tree bank reveals that our model results in better gains over the state-of-the-art semi-supervised models reported in the literature.</p><p>6 0.10834722 <a title="309-tfidf-6" href="./acl-2013-LABR%3A_A_Large_Scale_Arabic_Book_Reviews_Dataset.html">211 acl-2013-LABR: A Large Scale Arabic Book Reviews Dataset</a></p>
<p>7 0.10223159 <a title="309-tfidf-7" href="./acl-2013-Co-Regression_for_Cross-Language_Review_Rating_Prediction.html">81 acl-2013-Co-Regression for Cross-Language Review Rating Prediction</a></p>
<p>8 0.089714311 <a title="309-tfidf-8" href="./acl-2013-Improving_Chinese_Word_Segmentation_on_Micro-blog_Using_Rich_Punctuations.html">193 acl-2013-Improving Chinese Word Segmentation on Micro-blog Using Rich Punctuations</a></p>
<p>9 0.086473271 <a title="309-tfidf-9" href="./acl-2013-Dependency_Parser_Adaptation_with_Subtrees_from_Auto-Parsed_Target_Domain_Data.html">112 acl-2013-Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data</a></p>
<p>10 0.077383973 <a title="309-tfidf-10" href="./acl-2013-Categorization_of_Turkish_News_Documents_with_Morphological_Analysis.html">78 acl-2013-Categorization of Turkish News Documents with Morphological Analysis</a></p>
<p>11 0.075698152 <a title="309-tfidf-11" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>12 0.074503526 <a title="309-tfidf-12" href="./acl-2013-Topic_Modeling_Based_Classification_of_Clinical_Reports.html">351 acl-2013-Topic Modeling Based Classification of Clinical Reports</a></p>
<p>13 0.074099667 <a title="309-tfidf-13" href="./acl-2013-Identifying_Sentiment_Words_Using_an_Optimization-based_Model_without_Seed_Words.html">188 acl-2013-Identifying Sentiment Words Using an Optimization-based Model without Seed Words</a></p>
<p>14 0.071702801 <a title="309-tfidf-14" href="./acl-2013-Re-embedding_words.html">294 acl-2013-Re-embedding words</a></p>
<p>15 0.068962842 <a title="309-tfidf-15" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>16 0.064981043 <a title="309-tfidf-16" href="./acl-2013-Smoothed_marginal_distribution_constraints_for_language_modeling.html">325 acl-2013-Smoothed marginal distribution constraints for language modeling</a></p>
<p>17 0.059186604 <a title="309-tfidf-17" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>18 0.056839868 <a title="309-tfidf-18" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>19 0.055633448 <a title="309-tfidf-19" href="./acl-2013-Embedding_Semantic_Similarity_in_Tree_Kernels_for_Domain_Adaptation_of_Relation_Extraction.html">134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</a></p>
<p>20 0.055019535 <a title="309-tfidf-20" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.162), (1, 0.066), (2, -0.034), (3, 0.051), (4, 0.062), (5, -0.094), (6, 0.021), (7, -0.003), (8, -0.083), (9, 0.035), (10, 0.073), (11, -0.004), (12, 0.011), (13, -0.029), (14, -0.058), (15, 0.023), (16, -0.067), (17, 0.078), (18, -0.015), (19, 0.035), (20, 0.052), (21, 0.044), (22, 0.044), (23, 0.066), (24, 0.034), (25, -0.016), (26, 0.018), (27, -0.009), (28, -0.088), (29, -0.044), (30, -0.085), (31, 0.051), (32, -0.025), (33, 0.249), (34, 0.048), (35, 0.0), (36, -0.118), (37, -0.097), (38, -0.004), (39, -0.035), (40, 0.002), (41, -0.053), (42, 0.029), (43, 0.08), (44, 0.031), (45, 0.065), (46, 0.085), (47, -0.022), (48, 0.143), (49, -0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94423395 <a title="309-lsi-1" href="./acl-2013-Scaling_Semi-supervised_Naive_Bayes_with_Feature_Marginals.html">309 acl-2013-Scaling Semi-supervised Naive Bayes with Feature Marginals</a></p>
<p>Author: Michael Lucas ; Doug Downey</p><p>Abstract: Semi-supervised learning (SSL) methods augment standard machine learning (ML) techniques to leverage unlabeled data. SSL techniques are often effective in text classification, where labeled data is scarce but large unlabeled corpora are readily available. However, existing SSL techniques typically require multiple passes over the entirety of the unlabeled data, meaning the techniques are not applicable to large corpora being produced today. In this paper, we show that improving marginal word frequency estimates using unlabeled data can enable semi-supervised text classification that scales to massive unlabeled data sets. We present a novel learning algorithm, which optimizes a Naive Bayes model to accord with statistics calculated from the unlabeled corpus. In experiments with text topic classification and sentiment analysis, we show that our method is both more scalable and more accurate than SSL techniques from previous work.</p><p>2 0.84483647 <a title="309-lsi-2" href="./acl-2013-Text_Classification_from_Positive_and_Unlabeled_Data_using_Misclassified_Data_Correction.html">342 acl-2013-Text Classification from Positive and Unlabeled Data using Misclassified Data Correction</a></p>
<p>Author: Fumiyo Fukumoto ; Yoshimi Suzuki ; Suguru Matsuyoshi</p><p>Abstract: This paper addresses the problem of dealing with a collection of labeled training documents, especially annotating negative training documents and presents a method of text classification from positive and unlabeled data. We applied an error detection and correction technique to the results of positive and negative documents classified by the Support Vector Machines (SVM). The results using Reuters documents showed that the method was comparable to the current state-of-the-art biasedSVM method as the F-score obtained by our method was 0.627 and biased-SVM was 0.614.</p><p>3 0.67884302 <a title="309-lsi-3" href="./acl-2013-Co-Regression_for_Cross-Language_Review_Rating_Prediction.html">81 acl-2013-Co-Regression for Cross-Language Review Rating Prediction</a></p>
<p>Author: Xiaojun Wan</p><p>Abstract: The task of review rating prediction can be well addressed by using regression algorithms if there is a reliable training set of reviews with human ratings. In this paper, we aim to investigate a more challenging task of crosslanguage review rating prediction, which makes use of only rated reviews in a source language (e.g. English) to predict the rating scores of unrated reviews in a target language (e.g. German). We propose a new coregression algorithm to address this task by leveraging unlabeled reviews. Evaluation results on several datasets show that our proposed co-regression algorithm can consistently improve the prediction results. 1</p><p>4 0.66914886 <a title="309-lsi-4" href="./acl-2013-Semi-Supervised_Semantic_Tagging_of_Conversational_Understanding_using_Markov_Topic_Regression.html">315 acl-2013-Semi-Supervised Semantic Tagging of Conversational Understanding using Markov Topic Regression</a></p>
<p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur ; Gokhan Tur ; Ruhi Sarikaya</p><p>Abstract: Microsoft Research Microsoft Mountain View, CA, USA Redmond, WA, USA dilek @ ieee .org rus arika@mi cro s o ft . com gokhan .tur @ ieee .org performance (Tur and DeMori, 2011). This requires a tedious and time intensive data collection Finding concepts in natural language utterances is a challenging task, especially given the scarcity of labeled data for learning semantic ambiguity. Furthermore, data mismatch issues, which arise when the expected test (target) data does not exactly match the training data, aggravate this scarcity problem. To deal with these issues, we describe an efficient semisupervised learning (SSL) approach which has two components: (i) Markov Topic Regression is a new probabilistic model to cluster words into semantic tags (concepts). It can efficiently handle semantic ambiguity by extending standard topic models with two new features. First, it encodes word n-gram features from labeled source and unlabeled target data. Second, by going beyond a bag-of-words approach, it takes into account the inherent sequential nature of utterances to learn semantic classes based on context. (ii) Retrospective Learner is a new learning technique that adapts to the unlabeled target data. Our new SSL approach improves semantic tagging performance by 3% absolute over the baseline models, and also compares favorably on semi-supervised syntactic tagging.</p><p>5 0.66845095 <a title="309-lsi-5" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>Author: Xiaodong Zeng ; Derek F. Wong ; Lidia S. Chao ; Isabel Trancoso</p><p>Abstract: This paper introduces a graph-based semisupervised joint model of Chinese word segmentation and part-of-speech tagging. The proposed approach is based on a graph-based label propagation technique. One constructs a nearest-neighbor similarity graph over all trigrams of labeled and unlabeled data for propagating syntactic information, i.e., label distributions. The derived label distributions are regarded as virtual evidences to regularize the learning of linear conditional random fields (CRFs) on unlabeled data. An inductive character-based joint model is obtained eventually. Empirical results on Chinese tree bank (CTB-7) and Microsoft Research corpora (MSR) reveal that the proposed model can yield better results than the supervised baselines and other competitive semi-supervised CRFs in this task.</p><p>6 0.65718561 <a title="309-lsi-6" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>7 0.57720554 <a title="309-lsi-7" href="./acl-2013-Part-of-speech_tagging_with_antagonistic_adversaries.html">277 acl-2013-Part-of-speech tagging with antagonistic adversaries</a></p>
<p>8 0.50173891 <a title="309-lsi-8" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>9 0.49404269 <a title="309-lsi-9" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>10 0.4848243 <a title="309-lsi-10" href="./acl-2013-Bridging_Languages_through_Etymology%3A_The_case_of_cross_language_text_categorization.html">72 acl-2013-Bridging Languages through Etymology: The case of cross language text categorization</a></p>
<p>11 0.47939169 <a title="309-lsi-11" href="./acl-2013-Re-embedding_words.html">294 acl-2013-Re-embedding words</a></p>
<p>12 0.47442552 <a title="309-lsi-12" href="./acl-2013-A_Novel_Classifier_Based_on_Quantum_Computation.html">14 acl-2013-A Novel Classifier Based on Quantum Computation</a></p>
<p>13 0.46478847 <a title="309-lsi-13" href="./acl-2013-Categorization_of_Turkish_News_Documents_with_Morphological_Analysis.html">78 acl-2013-Categorization of Turkish News Documents with Morphological Analysis</a></p>
<p>14 0.4612433 <a title="309-lsi-14" href="./acl-2013-Dependency_Parser_Adaptation_with_Subtrees_from_Auto-Parsed_Target_Domain_Data.html">112 acl-2013-Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data</a></p>
<p>15 0.44534585 <a title="309-lsi-15" href="./acl-2013-Connotation_Lexicon%3A_A_Dash_of_Sentiment_Beneath_the_Surface_Meaning.html">91 acl-2013-Connotation Lexicon: A Dash of Sentiment Beneath the Surface Meaning</a></p>
<p>16 0.43636894 <a title="309-lsi-16" href="./acl-2013-Transfer_Learning_Based_Cross-lingual_Knowledge_Extraction_for_Wikipedia.html">356 acl-2013-Transfer Learning Based Cross-lingual Knowledge Extraction for Wikipedia</a></p>
<p>17 0.43316045 <a title="309-lsi-17" href="./acl-2013-Real-World_Semi-Supervised_Learning_of_POS-Taggers_for_Low-Resource_Languages.html">295 acl-2013-Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages</a></p>
<p>18 0.43172404 <a title="309-lsi-18" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>19 0.42231882 <a title="309-lsi-19" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>20 0.41893575 <a title="309-lsi-20" href="./acl-2013-Recognizing_Rare_Social_Phenomena_in_Conversation%3A_Empowerment_Detection_in_Support_Group_Chatrooms.html">298 acl-2013-Recognizing Rare Social Phenomena in Conversation: Empowerment Detection in Support Group Chatrooms</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.049), (4, 0.063), (6, 0.02), (11, 0.099), (24, 0.046), (26, 0.056), (35, 0.057), (42, 0.042), (48, 0.07), (67, 0.196), (70, 0.038), (88, 0.034), (90, 0.05), (95, 0.062)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82109058 <a title="309-lda-1" href="./acl-2013-Enriching_Entity_Translation_Discovery_using_Selective_Temporality.html">138 acl-2013-Enriching Entity Translation Discovery using Selective Temporality</a></p>
<p>Author: Gae-won You ; Young-rok Cha ; Jinhan Kim ; Seung-won Hwang</p><p>Abstract: This paper studies named entity translation and proposes “selective temporality” as a new feature, as using temporal features may be harmful for translating “atemporal” entities. Our key contribution is building an automatic classifier to distinguish temporal and atemporal entities then align them in separate procedures to boost translation accuracy by 6. 1%.</p><p>same-paper 2 0.81226754 <a title="309-lda-2" href="./acl-2013-Scaling_Semi-supervised_Naive_Bayes_with_Feature_Marginals.html">309 acl-2013-Scaling Semi-supervised Naive Bayes with Feature Marginals</a></p>
<p>Author: Michael Lucas ; Doug Downey</p><p>Abstract: Semi-supervised learning (SSL) methods augment standard machine learning (ML) techniques to leverage unlabeled data. SSL techniques are often effective in text classification, where labeled data is scarce but large unlabeled corpora are readily available. However, existing SSL techniques typically require multiple passes over the entirety of the unlabeled data, meaning the techniques are not applicable to large corpora being produced today. In this paper, we show that improving marginal word frequency estimates using unlabeled data can enable semi-supervised text classification that scales to massive unlabeled data sets. We present a novel learning algorithm, which optimizes a Naive Bayes model to accord with statistics calculated from the unlabeled corpus. In experiments with text topic classification and sentiment analysis, we show that our method is both more scalable and more accurate than SSL techniques from previous work.</p><p>3 0.78408509 <a title="309-lda-3" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>Author: Nan Yang ; Shujie Liu ; Mu Li ; Ming Zhou ; Nenghai Yu</p><p>Abstract: In this paper, we explore a novel bilingual word alignment approach based on DNN (Deep Neural Network), which has been proven to be very effective in various machine learning tasks (Collobert et al., 2011). We describe in detail how we adapt and extend the CD-DNNHMM (Dahl et al., 2012) method introduced in speech recognition to the HMMbased word alignment model, in which bilingual word embedding is discriminatively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences. While being capable to model the rich bilingual correspondence, our method generates a very compact model with much fewer parameters. Experiments on a large scale EnglishChinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score.</p><p>4 0.7648381 <a title="309-lda-4" href="./acl-2013-Crowdsourcing_Interaction_Logs_to_Understand_Text_Reuse_from_the_Web.html">100 acl-2013-Crowdsourcing Interaction Logs to Understand Text Reuse from the Web</a></p>
<p>Author: Martin Potthast ; Matthias Hagen ; Michael Volske ; Benno Stein</p><p>Abstract: unkown-abstract</p><p>5 0.76015365 <a title="309-lda-5" href="./acl-2013-Deepfix%3A_Statistical_Post-editing_of_Statistical_Machine_Translation_Using_Deep_Syntactic_Analysis.html">110 acl-2013-Deepfix: Statistical Post-editing of Statistical Machine Translation Using Deep Syntactic Analysis</a></p>
<p>Author: Rudolf Rosa ; David Marecek ; Ales Tamchyna</p><p>Abstract: Deepfix is a statistical post-editing system for improving the quality of statistical machine translation outputs. It attempts to correct errors in verb-noun valency using deep syntactic analysis and a simple probabilistic model of valency. On the English-to-Czech translation pair, we show that statistical post-editing of statistical machine translation leads to an improvement of the translation quality when helped by deep linguistic knowledge.</p><p>6 0.74338025 <a title="309-lda-6" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>7 0.70868742 <a title="309-lda-7" href="./acl-2013-Re-embedding_words.html">294 acl-2013-Re-embedding words</a></p>
<p>8 0.68367922 <a title="309-lda-8" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>9 0.67301655 <a title="309-lda-9" href="./acl-2013-Large_tagset_labeling_using_Feed_Forward_Neural_Networks._Case_study_on_Romanian_Language.html">216 acl-2013-Large tagset labeling using Feed Forward Neural Networks. Case study on Romanian Language</a></p>
<p>10 0.66715407 <a title="309-lda-10" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>11 0.66034073 <a title="309-lda-11" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>12 0.64910847 <a title="309-lda-12" href="./acl-2013-Semi-Supervised_Semantic_Tagging_of_Conversational_Understanding_using_Markov_Topic_Regression.html">315 acl-2013-Semi-Supervised Semantic Tagging of Conversational Understanding using Markov Topic Regression</a></p>
<p>13 0.64387161 <a title="309-lda-13" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>14 0.64232844 <a title="309-lda-14" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>15 0.641527 <a title="309-lda-15" href="./acl-2013-A_Lattice-based_Framework_for_Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">7 acl-2013-A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>16 0.63972157 <a title="309-lda-16" href="./acl-2013-Multimodal_DBN_for_Predicting_High-Quality_Answers_in_cQA_portals.html">254 acl-2013-Multimodal DBN for Predicting High-Quality Answers in cQA portals</a></p>
<p>17 0.63892609 <a title="309-lda-17" href="./acl-2013-Paraphrasing_Adaptation_for_Web_Search_Ranking.html">273 acl-2013-Paraphrasing Adaptation for Web Search Ranking</a></p>
<p>18 0.63701594 <a title="309-lda-18" href="./acl-2013-Fast_and_Adaptive_Online_Training_of_Feature-Rich_Translation_Models.html">156 acl-2013-Fast and Adaptive Online Training of Feature-Rich Translation Models</a></p>
<p>19 0.63585615 <a title="309-lda-19" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>20 0.63441503 <a title="309-lda-20" href="./acl-2013-Identifying_Sentiment_Words_Using_an_Optimization-based_Model_without_Seed_Words.html">188 acl-2013-Identifying Sentiment Words Using an Optimization-based Model without Seed Words</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
