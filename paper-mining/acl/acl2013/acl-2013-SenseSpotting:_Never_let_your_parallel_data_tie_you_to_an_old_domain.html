<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-316" href="#">acl2013-316</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</h1>
<br/><p>Source: <a title="acl-2013-316-pdf" href="http://aclweb.org/anthology//P/P13/P13-1141.pdf">pdf</a></p><p>Author: Marine Carpuat ; Hal Daume III ; Katharine Henry ; Ann Irvine ; Jagadeesh Jagarlamudi ; Rachel Rudinger</p><p>Abstract: Words often gain new senses in new domains. Being able to automatically identify, from a corpus of monolingual text, which word tokens are being used in a previously unseen sense has applications to machine translation and other tasks sensitive to lexical semantics. We define a task, SENSESPOTTING, in which we build systems to spot tokens that have new senses in new domain text. Instead of difficult and expensive annotation, we build a goldstandard by leveraging cheaply available parallel corpora, targeting our approach to the problem of domain adaptation for machine translation. Our system is able to achieve F-measures of as much as 80%, when applied to word types it has never seen before. Our approach is based on a large set of novel features that capture varied aspects of how words change when used in new domains.</p><p>Reference: <a title="acl-2013-316-reference" href="../acl2013_reference/acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 SenseSpotting: Never let your parallel data tie you to an old domain Marine Carpuat1, Hal Daum e´ III2, Katharine Henry3, Ann Irvine4, Jagadeesh Jagarlamudi5, Rachel Rudinger6 1National Research Council Canada, marine . [sent-1, score-0.64]
</p><p>2 edu  Abstract Words often gain new senses in new domains. [sent-14, score-0.303]
</p><p>3 Being able to automatically identify, from a corpus of monolingual text, which word tokens are being used in a previously unseen sense has applications to machine translation and other tasks sensitive to lexical semantics. [sent-15, score-0.485]
</p><p>4 We define a task, SENSESPOTTING, in which we build systems to spot tokens that have new senses in new domain text. [sent-16, score-0.61]
</p><p>5 Instead of difficult and expensive annotation, we build a goldstandard by leveraging cheaply available parallel corpora, targeting our approach to the problem of domain adaptation for machine translation. [sent-17, score-0.336]
</p><p>6 (2002) observed, the domain of the text that a word occurs in is a useful signal for performing word sense disambiguation (e. [sent-21, score-0.587]
</p><p>7 However, in the classic WSD task, ambiguous word types and a set of possible senses are known in advance. [sent-24, score-0.226]
</p><p>8 In this work, we focus on the setting where we observe texts in two different domains and want to identify words in the second text that have a sense that did not appear in the first text, without any lexical knowledge in the new domain. [sent-25, score-0.348]
</p><p>9 l(amst inoedf)Fracphtio rwd(spoalintrd´ceagtli)hmreigm eost  frequent senses (translations) in four domains. [sent-29, score-0.211]
</p><p>10 ” However, in moving to a medical or scientific domain, the word gains a new sense: “ratio”, which simply does not exist in the parliament domain. [sent-31, score-0.267]
</p><p>11 In a science domain, the “report” sense exists, but it is dominated about 12: 1 by “ratio. [sent-32, score-0.232]
</p><p>12 ” In a medical domain, the “report” sense remains dominant (about 2: 1), but the new “ratio” sense appears frequently. [sent-33, score-0.567]
</p><p>13 The goal of this task is to identify words in a new domain monolingual text that appeared in old domain text but which have a new, previously unseen sense1 . [sent-35, score-0.832]
</p><p>14 We operate un-  der the framework of phrase sense disambiguation (Carpuat and Wu, 2007), in which we take automatically align parallel data in an old domain to generate an initial old-domain sense inventory. [sent-36, score-1.075]
</p><p>15 This sense inventory provides the set of “known” word senses in the form of phrasal translations. [sent-37, score-0.458]
</p><p>16 One of our key contributions is the development of a rich set of features based on monolingual text that are indicative of new word senses. [sent-39, score-0.234]
</p><p>17 When machine translation (MT) systems are applied in a new domain, many errors are a result of: (1) previously unseen (OOV) source language words, or (2) source language words that appear with a new sense and which require new transla1All features, code, data and raw results are at: github . [sent-41, score-0.509]
</p><p>18 Given monolingual text in a new domain, OOVs are easy to identify,  and their translations can be acquired using dictionary extraction techniques (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Schafer, 2006; Haghighi et al. [sent-46, score-0.227]
</p><p>19 First, we need  an sol adb-oduotm thaeinir sense dictionary, . [sent-56, score-0.232]
</p><p>20 Given these two inputs, our challenge is to find tokens in the new-domain text that are being used in a new sense (w. [sent-59, score-0.356]
</p><p>21 We assume that we have access to a small amount of new domain parallel “tuning data. [sent-63, score-0.367]
</p><p>22 ” From this data, we can extract a small new domain dictionary (§5). [sent-64, score-0.322]
</p><p>23 In this way, we turn the SENSESPOTTING problem into a supervised binary classification problem: an example is a French word in context (in the new domain monolingual text) and its label is positive when it is being used in a sense that did not exist in the old domain dictionary. [sent-67, score-1.119]
</p><p>24 From an applied perspective, the assumption of a small amount of parallel data in the new domain is reasonable: if we want an MT system for a new domain, we will likely have some data for system tuning and evaluation. [sent-74, score-0.479]
</p><p>25 3  Related Work  While word senses have been studied extensively in lexical semantics, research has focused on word sense disambiguation, the task of disambiguating words in context given a predefined sense inventory (e. [sent-75, score-0.779]
</p><p>26 , Agirre and Edmonds (2006)), and word sense induction, the task of learning sense inventories from text (e. [sent-77, score-0.513]
</p><p>27 In contrast, detecting novel senses has not received as much attention, and is typically addressed within word sense induction, rather than as a distinct SENSESPOTTING task. [sent-80, score-0.492]
</p><p>28 Novel sense detection  has been mostly motivated by the study of language change over time. [sent-81, score-0.232]
</p><p>29 Most approaches model changes in co-occurrence patterns for word types when moving between corpora of old and modern language (Sagi et al. [sent-82, score-0.39]
</p><p>30 Since these type-based models do not capture polysemy in the new language, there have been a few attempts at detecting new senses at the tokenlevel as in SENSESPOTTING. [sent-84, score-0.337]
</p><p>31 (2012) leverage a common framework to address sense induction and disambiguation based on topic models (Blei et al. [sent-86, score-0.361]
</p><p>32 Sense induction is framed as learning topic distributions for a word type, while disambiguation consists of assigning topics to word tokens. [sent-88, score-0.262]
</p><p>33 This model can interestingly be used to detect newly coined senses, which might co-exist with old senses in recent language. [sent-89, score-0.437]
</p><p>34 In contrast, the SENSESPOTTING task consists of de-  tecting when senses are unknown in parallel data. [sent-92, score-0.271]
</p><p>35 Such novel sense induction methods require manually annotated datasets for the purpose of evaluation. [sent-93, score-0.278]
</p><p>36 1436  The impact of domain on novel senses has also received some attention. [sent-96, score-0.387]
</p><p>37 Most approaches operate at the type-level, thus capturing changes in the most frequent sense of a word when shifting domains (McCarthy et al. [sent-97, score-0.409]
</p><p>38 Chan and Ng (2007) notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning. [sent-100, score-1.107]
</p><p>39 In contrast, SENSESPOTTING detects when words have new senses and, thus, frequently a new translation. [sent-103, score-0.303]
</p><p>40 Our word type features ignore this context and rely on statistics computed over our entire new domain corpus. [sent-110, score-0.485]
</p><p>41 In contrast, our word token features consider the context of the particular instance of the word. [sent-111, score-0.299]
</p><p>42 If it were the case that only one sense ex-  isted for all word tokens of a particular type within a single domain, we would expect our word type features to be able to spot new senses without the help of the word token features. [sent-112, score-1.038]
</p><p>43 However, in fact, even within a single domain, we find that often a word type is used with several senses, suggesting that word token features may also be useful. [sent-113, score-0.364]
</p><p>44 1 Type-level Features Lexical Item Frequency Features A very basic property of the new domain that we hope to capture is that word frequencies change, and such changes might be indicative of a domain shift. [sent-115, score-0.573]
</p><p>45 As such, we compute unigram log probabilities (via smoothed relative frequencies) of each word under consideration in the old domain and the new domain. [sent-116, score-0.714]
</p><p>46 Topic Model Feature The intuition behind the topic model feature is that if a word’s distribution over topics changes when moving into a new domain, it is likely to also gain a new sense. [sent-125, score-0.333]
</p><p>47 −  For example, suppose that in our old domain, the French word enceinte is only used with the sense “wall,” but in our new domain, enceinte may have senses corresponding to either “wall” or to “pregnant. [sent-126, score-0.955]
</p><p>48 ” We would expect to see this reflected in enceinte’s distribution over topics: the topic that places relatively high probabilities on words such as “b´ eb´ e” (English “baby”) and enfant (English “child”) will also place a high probability on enceinte when trained on new domain data. [sent-127, score-0.512]
</p><p>49 In the old domain, however, we would not expect a similar topic (if it exists) to give a high probability to enceinte. [sent-128, score-0.336]
</p><p>50 For a wPotr∈dT w, Tthe fe(ta|tuwr)eP valu|ew )wcoills tb,et high if, for ePach new domain topic t that places high probability on w, there is an old domain topic t0 that 1437  is similar to t and also places a high probability on w. [sent-130, score-0.963]
</p><p>51 Context Feature It is expected that words acquiring new senses will tend to neighbor different sets of words (e. [sent-137, score-0.24]
</p><p>52 Thus, we define an additional type level feature to be the ratio of the number of new domain n-grams (up to length three) that contain word w and which do not appear in the old domain to the total number of new domain n-grams containing w. [sent-141, score-1.121]
</p><p>53 We do not count n-grams containing OO|NVs|, as they may simply be instances of applying the same sense of a word to a new argument 4. [sent-143, score-0.344]
</p><p>54 2 Token-level Features N-gram Probability Features Akin to the Ngram probability features at the type level (namely, Token:NgramProb), we compute the same values at the token level (new/old domain and unigram/trigram). [sent-144, score-0.553]
</p><p>55 Instead of computing statistics over the entire monolingual corpus, we use the instantaneous values of these features for the token under consideration. [sent-145, score-0.265]
</p><p>56 The six features we construct are: unigram (and trigram) log probabilities in the old domain, the new domain, and their difference. [sent-146, score-0.485]
</p><p>57 Context Features Following the type-level ngram feature, we define features for a particular word token based on its n-gram context. [sent-147, score-0.295]
</p><p>58 For token wi, in position iin a given sentence, we consider its context words in a five word window: wi−2, wi−1, wi+1, and wi+2. [sent-148, score-0.232]
</p><p>59 Towards this end, first, we pose the problem as a phrase sense disambiguation (PSD) problem over the known sense inventory. [sent-152, score-0.511]
</p><p>60 The ground truth labels (target translation for a given source word) for this classifier are generated from the phrase table of the old domain data. [sent-154, score-0.594]
</p><p>61 The idea is that, if a word is used in one of the known senses then its con-  text must have been seen previously and hence we hope that the PSD classifier outputs a spiky distribution. [sent-164, score-0.296]
</p><p>62 On the other hand, if the word takes a new sense then hopefully it is used in an unseen context resulting in the PSD classifier outputting an uniform distribution. [sent-165, score-0.488]
</p><p>63 prediction given the source token: The use of median in the numerator ratherp (tth|sa)n the second best is motivated by the observation that, in most cases, top ranked translations are of the same sense but differ in morphology. [sent-177, score-0.292]
</p><p>64 We train the PSD classifier in two modes: 1) a single global classifier that predicts the target translation given any source word; 2) a local classifier for each source word. [sent-178, score-0.264]
</p><p>65 one classifier per word type, we can define additional features based on the prior (with out the word context) and posterior (given the word’s context) probability distributions output by the classifier, i. [sent-185, score-0.363]
</p><p>66 We compute the following set of features referred to as Token :PSDRatio: SameMax checks if both the prior and posterior distributions have the same translation as the most likely translation. [sent-190, score-0.295]
</p><p>67 5  Data and Gold Standard  The first component of our task is a parallel cor-  pus of old domain data, for which we use the French-English Hansard parliamentary proceedings (http : //www . [sent-200, score-0.602]
</p><p>68 From this, we extract an old domain sense dictionary, using the Moses MT framework (Koehn et al. [sent-204, score-0.702]
</p><p>69 For new domains, we use three sources: (1) the EMEA medical corpus (Tiedemann, 2009), (2) a corpus of scientific abstracts, and (3) a corpus of translated movie subtitles (Tiedemann, 2009). [sent-207, score-0.222]
</p><p>70 To create the gold standard truth, we followed a lexical sample apparoach and collected a set of 300 “representative types” that are interesting to evaluate on, because they have multiple senses within a single domain or whose senses are likely to change in a new domain. [sent-210, score-0.676]
</p><p>71 The columns show: the total amount of parallel development data (# of sentences and tokens in French), # of representative types that appear in this corpus, the corresponding # of tokens, and the percentage of these tokens that correspond to “new senses. [sent-216, score-0.346]
</p><p>72 For each of the three new domains (EMEA, Science, and Subs), we found the intersection of phrases between the old and the new domain. [sent-218, score-0.439]
</p><p>73 3 In practice, we limited our set almost entirely to source words, and included only a single multi-  word phrase, vue des enfants, which usually translates as “for children” in the old domain but almost always translates as “sight of children” in the EMEA domain (as in “. [sent-220, score-0.729]
</p><p>74 Nothing in the way we have defined, approached, or evaluated the SENSESPOTTING task is dependent on the use of representative words instead of longer representative phrases. [sent-224, score-0.26]
</p><p>75 For microaveraging, we compute a separate confusion matrix for each word type on the French side, compute P/R/F for each of these separately, and then average the results. [sent-235, score-0.229]
</p><p>76 ) The AUC and macro-averaged scores give a sense of how well the system is doing on a type-level basis (essentially weighted by type frequency), while the micro-averaged scores give a sense as to how well  the system is doing on individual types, not taking into account their frequencies. [sent-237, score-0.52]
</p><p>77 We evaluate performance using our type-level features only, TYPEONLY, our token-level features only, TOKENONLY, and using both our type and our token level features, ALLFEATURES. [sent-244, score-0.333]
</p><p>78 CONSTANT always predicts new-sense, achieving 100% recall and a macrolevel precision that is equal to the percent of representative words which do have a new sense, modulo cross-validation splits (see Table 3). [sent-247, score-0.225]
</p><p>79 Most of our token-level features capture the intuition that when a word token appears in new or infrequent contexts, it is likely to have gained a new sense. [sent-270, score-0.518]
</p><p>80 are relatively weak for predicting new senses on EMEA data but stronger on Subs (TYPEONLY AUC performance is higher than both baselines) and even stronger on Science data (TYPEONLY AUC and f-measure performance is higher than both baselines as well as the ALLFEATURESmodel). [sent-279, score-0.24]
</p><p>81 Thus, it makes sense that type-level features would be the most informative for the least homogeneous dataset. [sent-281, score-0.299]
</p><p>82 Representative words in scientific text are likely to appear in variety of contexts, while in the EMEA data they may only appear in a few, making it easier to contrast them with the distributions observed in the old domain data. [sent-282, score-0.632]
</p><p>83 Recall that the micro-level evaluation computes precision, recall, and f-measure for all word tokens of a given word type and then averages across word types. [sent-284, score-0.264]
</p><p>84 We observe that words that are less frequent in both the old and the new domains are more likely to have a new sense than more frequent words, which causes the CONSTANT baseline to perform reasonably well. [sent-285, score-0.788]
</p><p>85 A low frequency in the new domain makes type level features (estimated over only a few instances) noisy and unreliable. [sent-287, score-0.396]
</p><p>86 Similarly, a low frequency in the old domain  makes the our token level features, which all contrast with old domain instances of the word type. [sent-288, score-1.168]
</p><p>87 Considering the six most useful features in each domain, the ones that pop out as frequently most useful are the global PSD features, the ngram probability features (either type- or token-based), the relative frequency features and the context features. [sent-307, score-0.317]
</p><p>88 Suppose we have no parallel data in the new domain at all, yet still want to attack the SENSESPOTTING task. [sent-310, score-0.367]
</p><p>89 Now, instead of performing cross-validation  in a single domain (for instance, Science), we take the union of all of the training data in the other domains (e. [sent-313, score-0.263]
</p><p>90 To ease comparison to the results that do not suffer from domain shift, we also present “XV-ALLFEATURES”, which are results copied from Table 4 in which parallel data from NEW is used. [sent-320, score-0.304]
</p><p>91 In this task, instead of predicting if a given word token has a sense which is brand new with respect to the old domain, we predict whether it is being used with a a sense which is not the one that was observed most frequently in the old domain. [sent-325, score-1.239]
</p><p>92 While the SENSESPOTTING task has MT utility in suggesting which new domain words demand a new translation, the MOSTFREQSENSECHANGE task has utility in suggesting which words demand a new translation probability distribution when shifting to a new domain. [sent-337, score-0.662]
</p><p>93 It makes sense that our token level features have a better chance of success on this task. [sent-342, score-0.442]
</p><p>94 The important comparison now is between a new domain token in context and the majority of the old domain tokens of the same word type. [sent-343, score-1.036]
</p><p>95 This comparison is likely to be more informative than when we are equally interested in identifying overlap between  the current token and any old domain senses. [sent-344, score-0.662]
</p><p>96 “representative tokens”) extracted from fairly large new domain parallel corpora (see Table 3), consisting of between 22 and 36 thousand parallel sentences, which yield between 8 and 35 thousand representative tokens. [sent-349, score-0.591]
</p><p>97 Although we expect some new domain parallel tuning data to be available in most MT settings, we would like to know how many representative types are required to achieve good performance on the SENSESPOTTING task. [sent-350, score-0.497]
</p><p>98 Semeval-2007 task 02: Evaluating word sense induction and discrimination systems. [sent-367, score-0.327]
</p><p>99 Domain adaptation with active learning for word sense disambiguation. [sent-395, score-0.357]
</p><p>100 The role of domain information in word sense disambiguation. [sent-437, score-0.491]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sensespotting', 0.457), ('old', 0.26), ('sense', 0.232), ('emea', 0.231), ('domain', 0.21), ('auc', 0.201), ('senses', 0.177), ('subs', 0.174), ('psd', 0.173), ('typeonly', 0.152), ('token', 0.143), ('representative', 0.13), ('parallel', 0.094), ('carpuat', 0.091), ('enceinte', 0.087), ('nngew', 0.087), ('subtitles', 0.077), ('marine', 0.076), ('classifier', 0.07), ('features', 0.067), ('allfeatures', 0.065), ('mostfreqsensechange', 0.065), ('nolgd', 0.065), ('pprior', 0.065), ('tokenonly', 0.065), ('new', 0.063), ('deviations', 0.062), ('tokens', 0.061), ('translations', 0.06), ('french', 0.058), ('schafer', 0.058), ('bloodgood', 0.058), ('mccarthy', 0.057), ('jagadeesh', 0.057), ('type', 0.056), ('monolingual', 0.055), ('translation', 0.054), ('demand', 0.053), ('domains', 0.053), ('posterior', 0.053), ('daum', 0.053), ('log', 0.053), ('hal', 0.052), ('confusion', 0.05), ('predominant', 0.05), ('word', 0.049), ('likely', 0.049), ('johns', 0.049), ('dictionary', 0.049), ('disambiguation', 0.047), ('induction', 0.046), ('agirre', 0.045), ('nw', 0.045), ('active', 0.044), ('cwp', 0.044), ('gulordava', 0.044), ('katharine', 0.044), ('maxt', 0.044), ('ngramprob', 0.044), ('samemax', 0.044), ('samemin', 0.044), ('uolgd', 0.044), ('gained', 0.043), ('probabilities', 0.042), ('scientific', 0.042), ('changes', 0.041), ('intuition', 0.041), ('probability', 0.04), ('context', 0.04), ('medical', 0.04), ('moving', 0.04), ('chan', 0.039), ('hopkins', 0.039), ('rsi', 0.038), ('nrc', 0.038), ('sagi', 0.038), ('parliamentary', 0.038), ('wi', 0.038), ('compute', 0.037), ('ngram', 0.036), ('contrast', 0.036), ('topic', 0.036), ('spot', 0.036), ('clsp', 0.036), ('distributions', 0.035), ('unseen', 0.034), ('cook', 0.034), ('frequent', 0.034), ('detecting', 0.034), ('places', 0.034), ('koeling', 0.033), ('sight', 0.033), ('bamman', 0.033), ('parliament', 0.033), ('kl', 0.033), ('never', 0.033), ('blei', 0.032), ('adaptation', 0.032), ('percent', 0.032), ('hoffman', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="316-tfidf-1" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>Author: Marine Carpuat ; Hal Daume III ; Katharine Henry ; Ann Irvine ; Jagadeesh Jagarlamudi ; Rachel Rudinger</p><p>Abstract: Words often gain new senses in new domains. Being able to automatically identify, from a corpus of monolingual text, which word tokens are being used in a previously unseen sense has applications to machine translation and other tasks sensitive to lexical semantics. We define a task, SENSESPOTTING, in which we build systems to spot tokens that have new senses in new domain text. Instead of difficult and expensive annotation, we build a goldstandard by leveraging cheaply available parallel corpora, targeting our approach to the problem of domain adaptation for machine translation. Our system is able to achieve F-measures of as much as 80%, when applied to word types it has never seen before. Our approach is based on a large set of novel features that capture varied aspects of how words change when used in new domains.</p><p>2 0.19987825 <a title="316-tfidf-2" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>Author: Mohammad Taher Pilehvar ; David Jurgens ; Roberto Navigli</p><p>Abstract: Semantic similarity is an essential component of many Natural Language Processing applications. However, prior methods for computing semantic similarity often operate at different levels, e.g., single words or entire documents, which requires adapting the method for each data type. We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: seman- tic textual similarity, word similarity, and word sense coarsening.</p><p>3 0.16620719 <a title="316-tfidf-3" href="./acl-2013-Density_Maximization_in_Context-Sense_Metric_Space_for_All-words_WSD.html">111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</a></p>
<p>Author: Koichi Tanigaki ; Mitsuteru Shiba ; Tatsuji Munaka ; Yoshinori Sagisaka</p><p>Abstract: This paper proposes a novel smoothing model with a combinatorial optimization scheme for all-words word sense disambiguation from untagged corpora. By generalizing discrete senses to a continuum, we introduce a smoothing in context-sense space to cope with data-sparsity resulting from a large variety of linguistic context and sense, as well as to exploit senseinterdependency among the words in the same text string. Through the smoothing, all the optimal senses are obtained at one time under maximum marginal likelihood criterion, by competitive probabilistic kernels made to reinforce one another among nearby words, and to suppress conflicting sense hypotheses within the same word. Experimental results confirmed the superiority of the proposed method over conventional ones by showing the better performances beyond most-frequent-sense baseline performance where none of SemEval2 unsupervised systems reached.</p><p>4 0.15500522 <a title="316-tfidf-4" href="./acl-2013-Neighbors_Help%3A_Bilingual_Unsupervised_WSD_Using_Context.html">258 acl-2013-Neighbors Help: Bilingual Unsupervised WSD Using Context</a></p>
<p>Author: Sudha Bhingardive ; Samiulla Shaikh ; Pushpak Bhattacharyya</p><p>Abstract: Word Sense Disambiguation (WSD) is one of the toughest problems in NLP, and in WSD, verb disambiguation has proved to be extremely difficult, because of high degree of polysemy, too fine grained senses, absence of deep verb hierarchy and low inter annotator agreement in verb sense annotation. Unsupervised WSD has received widespread attention, but has performed poorly, specially on verbs. Recently an unsupervised bilingual EM based algorithm has been proposed, which makes use only of the raw counts of the translations in comparable corpora (Marathi and Hindi). But the performance of this approach is poor on verbs with accuracy level at 25-38%. We suggest a modifica- tion to this mentioned formulation, using context and semantic relatedness of neighboring words. An improvement of 17% 35% in the accuracy of verb WSD is obtained compared to the existing EM based approach. On a general note, the work can be looked upon as contributing to the framework of unsupervised WSD through context aware expectation maximization.</p><p>5 0.13475716 <a title="316-tfidf-5" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>Author: Jiajun Zhang ; Chengqing Zong</p><p>Abstract: Currently, almost all of the statistical machine translation (SMT) models are trained with the parallel corpora in some specific domains. However, when it comes to a language pair or a different domain without any bilingual resources, the traditional SMT loses its power. Recently, some research works study the unsupervised SMT for inducing a simple word-based translation model from the monolingual corpora. It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result. In this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1</p><p>6 0.12667957 <a title="316-tfidf-6" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>7 0.12191878 <a title="316-tfidf-7" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>8 0.11768735 <a title="316-tfidf-8" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>9 0.1171542 <a title="316-tfidf-9" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>10 0.10920732 <a title="316-tfidf-10" href="./acl-2013-FrameNet_on_the_Way_to_Babel%3A_Creating_a_Bilingual_FrameNet_Using_Wiktionary_as_Interlingual_Connection.html">162 acl-2013-FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection</a></p>
<p>11 0.10794239 <a title="316-tfidf-11" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>12 0.10786251 <a title="316-tfidf-12" href="./acl-2013-DKPro_WSD%3A_A_Generalized_UIMA-based_Framework_for_Word_Sense_Disambiguation.html">105 acl-2013-DKPro WSD: A Generalized UIMA-based Framework for Word Sense Disambiguation</a></p>
<p>13 0.10257305 <a title="316-tfidf-13" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>14 0.098941296 <a title="316-tfidf-14" href="./acl-2013-Detecting_Metaphor_by_Contextual_Analogy.html">116 acl-2013-Detecting Metaphor by Contextual Analogy</a></p>
<p>15 0.096450239 <a title="316-tfidf-15" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>16 0.096405551 <a title="316-tfidf-16" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>17 0.093634062 <a title="316-tfidf-17" href="./acl-2013-Annotation_of_regular_polysemy_and_underspecification.html">53 acl-2013-Annotation of regular polysemy and underspecification</a></p>
<p>18 0.089022942 <a title="316-tfidf-18" href="./acl-2013-Learning_Non-linear_Features_for_Machine_Translation_Using_Gradient_Boosting_Machines.html">221 acl-2013-Learning Non-linear Features for Machine Translation Using Gradient Boosting Machines</a></p>
<p>19 0.085957155 <a title="316-tfidf-19" href="./acl-2013-Context_Vector_Disambiguation_for_Bilingual_Lexicon_Extraction_from_Comparable_Corpora.html">93 acl-2013-Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora</a></p>
<p>20 0.08327204 <a title="316-tfidf-20" href="./acl-2013-Dependency_Parser_Adaptation_with_Subtrees_from_Auto-Parsed_Target_Domain_Data.html">112 acl-2013-Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.243), (1, 0.006), (2, 0.118), (3, -0.053), (4, 0.001), (5, -0.13), (6, -0.094), (7, 0.067), (8, -0.005), (9, -0.053), (10, 0.03), (11, 0.051), (12, -0.084), (13, -0.006), (14, 0.058), (15, -0.041), (16, -0.046), (17, 0.058), (18, -0.078), (19, -0.013), (20, 0.069), (21, -0.087), (22, 0.039), (23, 0.014), (24, 0.01), (25, -0.12), (26, 0.011), (27, -0.052), (28, 0.062), (29, -0.002), (30, 0.101), (31, 0.038), (32, 0.014), (33, 0.142), (34, -0.012), (35, -0.012), (36, 0.006), (37, -0.033), (38, -0.032), (39, -0.002), (40, 0.001), (41, -0.068), (42, -0.003), (43, -0.073), (44, 0.081), (45, 0.051), (46, 0.063), (47, 0.07), (48, -0.004), (49, 0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94554824 <a title="316-lsi-1" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>Author: Marine Carpuat ; Hal Daume III ; Katharine Henry ; Ann Irvine ; Jagadeesh Jagarlamudi ; Rachel Rudinger</p><p>Abstract: Words often gain new senses in new domains. Being able to automatically identify, from a corpus of monolingual text, which word tokens are being used in a previously unseen sense has applications to machine translation and other tasks sensitive to lexical semantics. We define a task, SENSESPOTTING, in which we build systems to spot tokens that have new senses in new domain text. Instead of difficult and expensive annotation, we build a goldstandard by leveraging cheaply available parallel corpora, targeting our approach to the problem of domain adaptation for machine translation. Our system is able to achieve F-measures of as much as 80%, when applied to word types it has never seen before. Our approach is based on a large set of novel features that capture varied aspects of how words change when used in new domains.</p><p>2 0.89051223 <a title="316-lsi-2" href="./acl-2013-Neighbors_Help%3A_Bilingual_Unsupervised_WSD_Using_Context.html">258 acl-2013-Neighbors Help: Bilingual Unsupervised WSD Using Context</a></p>
<p>Author: Sudha Bhingardive ; Samiulla Shaikh ; Pushpak Bhattacharyya</p><p>Abstract: Word Sense Disambiguation (WSD) is one of the toughest problems in NLP, and in WSD, verb disambiguation has proved to be extremely difficult, because of high degree of polysemy, too fine grained senses, absence of deep verb hierarchy and low inter annotator agreement in verb sense annotation. Unsupervised WSD has received widespread attention, but has performed poorly, specially on verbs. Recently an unsupervised bilingual EM based algorithm has been proposed, which makes use only of the raw counts of the translations in comparable corpora (Marathi and Hindi). But the performance of this approach is poor on verbs with accuracy level at 25-38%. We suggest a modifica- tion to this mentioned formulation, using context and semantic relatedness of neighboring words. An improvement of 17% 35% in the accuracy of verb WSD is obtained compared to the existing EM based approach. On a general note, the work can be looked upon as contributing to the framework of unsupervised WSD through context aware expectation maximization.</p><p>3 0.83000308 <a title="316-lsi-3" href="./acl-2013-Density_Maximization_in_Context-Sense_Metric_Space_for_All-words_WSD.html">111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</a></p>
<p>Author: Koichi Tanigaki ; Mitsuteru Shiba ; Tatsuji Munaka ; Yoshinori Sagisaka</p><p>Abstract: This paper proposes a novel smoothing model with a combinatorial optimization scheme for all-words word sense disambiguation from untagged corpora. By generalizing discrete senses to a continuum, we introduce a smoothing in context-sense space to cope with data-sparsity resulting from a large variety of linguistic context and sense, as well as to exploit senseinterdependency among the words in the same text string. Through the smoothing, all the optimal senses are obtained at one time under maximum marginal likelihood criterion, by competitive probabilistic kernels made to reinforce one another among nearby words, and to suppress conflicting sense hypotheses within the same word. Experimental results confirmed the superiority of the proposed method over conventional ones by showing the better performances beyond most-frequent-sense baseline performance where none of SemEval2 unsupervised systems reached.</p><p>4 0.80021572 <a title="316-lsi-4" href="./acl-2013-Annotation_of_regular_polysemy_and_underspecification.html">53 acl-2013-Annotation of regular polysemy and underspecification</a></p>
<p>Author: Hector Martinez Alonso ; Bolette Sandford Pedersen ; Nuria Bel</p><p>Abstract: We present the result of an annotation task on regular polysemy for a series of semantic classes or dot types in English, Danish and Spanish. This article describes the annotation process, the results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations.</p><p>5 0.75824714 <a title="316-lsi-5" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>Author: Tyler Baldwin ; Yunyao Li ; Bogdan Alexe ; Ioana R. Stanoi</p><p>Abstract: While the resolution of term ambiguity is important for information extraction (IE) systems, the cost of resolving each instance of an entity can be prohibitively expensive on large datasets. To combat this, this work looks at ambiguity detection at the term, rather than the instance, level. By making a judgment about the general ambiguity of a term, a system is able to handle ambiguous and unambiguous cases differently, improving throughput and quality. To address the term ambiguity detection problem, we employ a model that combines data from language models, ontologies, and topic modeling. Results over a dataset of entities from four product domains show that the proposed approach achieves significantly above baseline F-measure of 0.96.</p><p>6 0.74494797 <a title="316-lsi-6" href="./acl-2013-DKPro_WSD%3A_A_Generalized_UIMA-based_Framework_for_Word_Sense_Disambiguation.html">105 acl-2013-DKPro WSD: A Generalized UIMA-based Framework for Word Sense Disambiguation</a></p>
<p>7 0.6620844 <a title="316-lsi-7" href="./acl-2013-FrameNet_on_the_Way_to_Babel%3A_Creating_a_Bilingual_FrameNet_Using_Wiktionary_as_Interlingual_Connection.html">162 acl-2013-FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection</a></p>
<p>8 0.63354087 <a title="316-lsi-8" href="./acl-2013-Context-Dependent_Multilingual_Lexical_Lookup_for_Under-Resourced_Languages.html">92 acl-2013-Context-Dependent Multilingual Lexical Lookup for Under-Resourced Languages</a></p>
<p>9 0.62503773 <a title="316-lsi-9" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>10 0.59945339 <a title="316-lsi-10" href="./acl-2013-Understanding_Verbs_based_on_Overlapping_Verbs_Senses.html">366 acl-2013-Understanding Verbs based on Overlapping Verbs Senses</a></p>
<p>11 0.59178281 <a title="316-lsi-11" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>12 0.58091307 <a title="316-lsi-12" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>13 0.57549578 <a title="316-lsi-13" href="./acl-2013-Semi-Supervised_Semantic_Tagging_of_Conversational_Understanding_using_Markov_Topic_Regression.html">315 acl-2013-Semi-Supervised Semantic Tagging of Conversational Understanding using Markov Topic Regression</a></p>
<p>14 0.57253569 <a title="316-lsi-14" href="./acl-2013-Linking_and_Extending_an_Open_Multilingual_Wordnet.html">234 acl-2013-Linking and Extending an Open Multilingual Wordnet</a></p>
<p>15 0.56673998 <a title="316-lsi-15" href="./acl-2013-Context_Vector_Disambiguation_for_Bilingual_Lexicon_Extraction_from_Comparable_Corpora.html">93 acl-2013-Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora</a></p>
<p>16 0.56095815 <a title="316-lsi-16" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>17 0.53151834 <a title="316-lsi-17" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>18 0.52599633 <a title="316-lsi-18" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>19 0.5223673 <a title="316-lsi-19" href="./acl-2013-Detecting_Metaphor_by_Contextual_Analogy.html">116 acl-2013-Detecting Metaphor by Contextual Analogy</a></p>
<p>20 0.51812327 <a title="316-lsi-20" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.064), (6, 0.036), (11, 0.058), (14, 0.012), (24, 0.048), (26, 0.089), (35, 0.086), (42, 0.051), (48, 0.045), (64, 0.018), (67, 0.012), (70, 0.037), (84, 0.172), (88, 0.05), (90, 0.033), (95, 0.113)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88819647 <a title="316-lda-1" href="./acl-2013-A_Novel_Translation_Framework_Based_on_Rhetorical_Structure_Theory.html">16 acl-2013-A Novel Translation Framework Based on Rhetorical Structure Theory</a></p>
<p>Author: Mei Tu ; Yu Zhou ; Chengqing Zong</p><p>Abstract: Rhetorical structure theory (RST) is widely used for discourse understanding, which represents a discourse as a hierarchically semantic structure. In this paper, we propose a novel translation framework with the help of RST. In our framework, the translation process mainly includes three steps: 1) Source RST-tree acquisition: a source sentence is parsed into an RST tree; 2) Rule extraction: translation rules are extracted from the source tree and the target string via bilingual word alignment; 3) RST-based translation: the source RST-tree is translated with translation rules. Experiments on Chinese-to-English show that our RST-based approach achieves improvements of 2.3/0.77/1.43 BLEU points on NIST04/NIST05/CWMT2008 respectively. 1</p><p>2 0.8497017 <a title="316-lda-2" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>Author: Xuchen Yao ; Benjamin Van Durme ; Chris Callison-Burch ; Peter Clark</p><p>Abstract: Fast alignment is essential for many natural language tasks. But in the setting of monolingual alignment, previous work has not been able to align more than one sentence pair per second. We describe a discriminatively trained monolingual word aligner that uses a Conditional Random Field to globally decode the best alignment with features drawn from source and target sentences. Using just part-of-speech tags and WordNet as external resources, our aligner gives state-of-the-art result, while being an order-of-magnitude faster than the previous best performing system.</p><p>same-paper 3 0.84767812 <a title="316-lda-3" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>Author: Marine Carpuat ; Hal Daume III ; Katharine Henry ; Ann Irvine ; Jagadeesh Jagarlamudi ; Rachel Rudinger</p><p>Abstract: Words often gain new senses in new domains. Being able to automatically identify, from a corpus of monolingual text, which word tokens are being used in a previously unseen sense has applications to machine translation and other tasks sensitive to lexical semantics. We define a task, SENSESPOTTING, in which we build systems to spot tokens that have new senses in new domain text. Instead of difficult and expensive annotation, we build a goldstandard by leveraging cheaply available parallel corpora, targeting our approach to the problem of domain adaptation for machine translation. Our system is able to achieve F-measures of as much as 80%, when applied to word types it has never seen before. Our approach is based on a large set of novel features that capture varied aspects of how words change when used in new domains.</p><p>4 0.83784157 <a title="316-lda-4" href="./acl-2013-Recognizing_Partial_Textual_Entailment.html">297 acl-2013-Recognizing Partial Textual Entailment</a></p>
<p>Author: Omer Levy ; Torsten Zesch ; Ido Dagan ; Iryna Gurevych</p><p>Abstract: Textual entailment is an asymmetric relation between two text fragments that describes whether one fragment can be inferred from the other. It thus cannot capture the notion that the target fragment is “almost entailed” by the given text. The recently suggested idea of partial textual entailment may remedy this problem. We investigate partial entailment under the faceted entailment model and the possibility of adapting existing textual entailment methods to this setting. Indeed, our results show that these methods are useful for rec- ognizing partial entailment. We also provide a preliminary assessment of how partial entailment may be used for recognizing (complete) textual entailment.</p><p>5 0.76490217 <a title="316-lda-5" href="./acl-2013-Dual_Training_and_Dual_Prediction_for_Polarity_Classification.html">131 acl-2013-Dual Training and Dual Prediction for Polarity Classification</a></p>
<p>Author: Rui Xia ; Tao Wang ; Xuelei Hu ; Shoushan Li ; Chengqing Zong</p><p>Abstract: Bag-of-words (BOW) is now the most popular way to model text in machine learning based sentiment classification. However, the performance of such approach sometimes remains rather limited due to some fundamental deficiencies of the BOW model. In this paper, we focus on the polarity shift problem, and propose a novel approach, called dual training and dual prediction (DTDP), to address it. The basic idea of DTDP is to first generate artificial samples that are polarity-opposite to the original samples by polarity reversion, and then leverage both the original and opposite samples for (dual) training and (dual) prediction. Experimental results on four datasets demonstrate the effectiveness of the proposed approach for polarity classification. 1</p><p>6 0.76168644 <a title="316-lda-6" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<p>7 0.7604211 <a title="316-lda-7" href="./acl-2013-Co-Regression_for_Cross-Language_Review_Rating_Prediction.html">81 acl-2013-Co-Regression for Cross-Language Review Rating Prediction</a></p>
<p>8 0.75736761 <a title="316-lda-8" href="./acl-2013-Mapping_Source_to_Target_Strings_without_Alignment_by_Analogical_Learning%3A_A_Case_Study_with_Transliteration.html">236 acl-2013-Mapping Source to Target Strings without Alignment by Analogical Learning: A Case Study with Transliteration</a></p>
<p>9 0.75686216 <a title="316-lda-9" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>10 0.75683874 <a title="316-lda-10" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>11 0.75661242 <a title="316-lda-11" href="./acl-2013-Detecting_Turnarounds_in_Sentiment_Analysis%3A_Thwarting.html">117 acl-2013-Detecting Turnarounds in Sentiment Analysis: Thwarting</a></p>
<p>12 0.75536442 <a title="316-lda-12" href="./acl-2013-Explicit_and_Implicit_Syntactic_Features_for_Text_Classification.html">144 acl-2013-Explicit and Implicit Syntactic Features for Text Classification</a></p>
<p>13 0.75533348 <a title="316-lda-13" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>14 0.75331068 <a title="316-lda-14" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>15 0.75287575 <a title="316-lda-15" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>16 0.75142014 <a title="316-lda-16" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<p>17 0.75053632 <a title="316-lda-17" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>18 0.75008184 <a title="316-lda-18" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>19 0.74964917 <a title="316-lda-19" href="./acl-2013-Enlisting_the_Ghost%3A_Modeling_Empty_Categories_for_Machine_Translation.html">137 acl-2013-Enlisting the Ghost: Modeling Empty Categories for Machine Translation</a></p>
<p>20 0.74959493 <a title="316-lda-20" href="./acl-2013-Machine_Translation_Detection_from_Monolingual_Web-Text.html">235 acl-2013-Machine Translation Detection from Monolingual Web-Text</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
