<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>87 acl-2013-Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-87" href="#">acl2013-87</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>87 acl-2013-Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics</h1>
<br/><p>Source: <a title="acl-2013-87-pdf" href="http://aclweb.org/anthology//P/P13/P13-1149.pdf">pdf</a></p><p>Author: Angeliki Lazaridou ; Marco Marelli ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: Speakers of a language can construct an unlimited number of new words through morphological derivation. This is a major cause of data sparseness for corpus-based approaches to lexical semantics, such as distributional semantic models of word meaning. We adapt compositional methods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts. Semantic representations constructed in this way beat a strong baseline and can be of higher quality than representations directly constructed from corpus data. Our results constitute a novel evaluation of the proposed composition methods, in which the full additive model achieves the best performance, and demonstrate the usefulness of a compositional morphology component in distributional semantics.</p><p>Reference: <a title="acl-2013-87-reference" href="../acl2013_reference/acl-2013-Compositional-ly_Derived_Representations_of_Morphologically_Complex_Words_in_Distributional_Semantics_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This is a major cause of data sparseness for corpus-based approaches to lexical semantics, such as distributional semantic models of word meaning. [sent-4, score-0.27]
</p><p>2 We adapt compositional methods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts. [sent-5, score-0.486]
</p><p>3 Our results constitute a novel evaluation of the proposed composition methods, in which the full additive model achieves the best performance, and demonstrate the  usefulness of a compositional morphology component in distributional semantics. [sent-7, score-0.742]
</p><p>4 Reliable distributional vectors can only be extracted for words that occur in many contexts in the corpus. [sent-11, score-0.333]
</p><p>5 While word rarity has many sources, one of the most common and systematic ones is the high pro-  ductivity of morphological derivation processes, whereby an unlimited number of new words can be constructed by adding affixes to existing stems (Baayen, 2005; Bauer, 2001 ; Plag, 1999). [sent-13, score-0.484]
</p><p>6 1 For example, in the multi-billion-word corpus we introduce below, perfectly reasonable derived forms such as lexicalizable or affixless never occur. [sent-14, score-0.3]
</p><p>7 Even without considering the theoretically infinite number of possible derived nonce words, and restricting ourselves instead to words that are already listed in dictionaries, complex forms cover a high portion of the lexicon. [sent-15, score-0.396]
</p><p>8 For example, morphologically complex forms account for 55% of the lemmas in the CELEX English database (see Section 4. [sent-16, score-0.261]
</p><p>9 In most of these cases (80% according to our corpus) the stem is more frequent than the complex form (e. [sent-18, score-0.458]
</p><p>10 , the stem build occurs 15 times more often than the derived form rebuild, and the latter is certainly not an unusual derived form). [sent-20, score-0.75]
</p><p>11 We use stem for the lexical item that constitutes the base of derivation (resource) and affix (prefix or suffix) for the element attached to the stem to derive the new form (-ful). [sent-27, score-1.101]
</p><p>12 In English, stems are typically independent words, affixes bound morphemes, i. [sent-28, score-0.308]
</p><p>13 Note that a stem can in turn be morphologically derived, e. [sent-31, score-0.478]
</p><p>14 Ac s2s0o1ci3a Atiosnso fcoirat Cioonm foprut Caotimonpaulta Lti nognuails Lti cnsg,u piasgteicss 1517–1526, phology would be to identify the stem of rare derived words and use its distributional vector as a proxy to derived-form  meaning. [sent-37, score-0.811]
</p><p>15 Still,  something is clearly lost (if the author of a text felt the need to use the derived form, the stem was not fully appropriate), and sometimes the jump in meaning can be quite dramatic (resourceless and resource mean very different things! [sent-39, score-0.704]
</p><p>16 Compositional distributional semantic models (cDSMs) of word units aim at handling, compositionally, the high productivity of phrases and consequent data sparseness. [sent-45, score-0.249]
</p><p>17 Morphological induction has recently received considerable attention since morphological analysis can mitigate data sparseness in domains such as parsing and machine translation (Goldberg and Tsarfaty, 2008; Lee, 2004). [sent-54, score-0.258]
</p><p>18 Our goal is to automatically construct, given distributional representations of stems and affixes, semantic representations for the derived words containing those stems and affixes. [sent-58, score-0.787]
</p><p>19 A morphological induction system, given rebuild, will segment it into re- and build (possibly using distributional similarity between the words as a cue). [sent-59, score-0.356]
</p><p>20 Another emerging line of research uses distributional semantics to model human intuitions about the semantic transparency of morphologically derived or compound expressions and how these impact various lexical processing tasks (Kuperman, 2009; Wang et al. [sent-61, score-0.524]
</p><p>21 Guevara found a systematic  geometric relation between corpus-based vectors of derived forms sharing an affix and their stems, and used this finding to motivate the composition method we term lexfunc below. [sent-65, score-1.235]
</p><p>22 However, unlike us, he did not test alternative models, and he only presented a qualitative analysis of the trajectories triggered by composition with various affixes. [sent-66, score-0.313]
</p><p>23 Given input vectors u and v, the multiplicative model (mult) returns a composed vector c with: ci = uivi. [sent-76, score-0.289]
</p><p>24 In morphological derivation, at least one ofthe items to be composed (the affix) is a bound morpheme. [sent-83, score-0.364]
</p><p>25 In our adaptation of these composition models, we build bound morpheme vectors by accumulating the contexts in which a set of derived words containing the relevant morphemes occur, e. [sent-84, score-0.891]
</p><p>26 (2010) take inspiration from formal semantics to characterize composition in terms of function application, where the distributional representation of one element in a composition (the functor) is not a vector but a function. [sent-88, score-0.869]
</p><p>27 In the case of morphology, it is natural to treat bound affixes as functions over stems, since affixes encode the systematic semantic patterns we intend to capture. [sent-90, score-0.339]
</p><p>28 Unlike the other composition methods, lexfunc does not require the construction of distributional vectors for affixes. [sent-91, score-0.851]
</p><p>29 A matrix representation for every affix is instead induced directly from examples of stems and the corresponding derived forms, in line with the intuition that every affix corresponds to a different pattern of change of the stem meaning. [sent-92, score-1.173]
</p><p>30 Finally, as already discussed in the Introduction, performing no composition at all but using  the stem vector as a surrogate of the derived form is a reasonable strategy. [sent-93, score-0.945]
</p><p>31 We saw that morphologically derived words tend to appear less frequently than their stems, and in many cases the meanings are close. [sent-94, score-0.281]
</p><p>32 For each derivational affix present in CELEX, we extracted from the database the full list of stem/derived pairs matching its most common part-of-speech signature (e. [sent-99, score-0.298]
</p><p>33 , for -er we only considered pairs 1519  having a verbal stem and nominal derived form). [sent-101, score-0.575]
</p><p>34 Since CELEX was populated by semi-automated morphological analysis, it includes forms that are probably not synchronically related to their stems,  such as crypt+ic or re+form. [sent-102, score-0.288]
</p><p>35 The remaining data were used as training items to estimate the parameters of the composition methods. [sent-108, score-0.421]
</p><p>36 Annotation of quality of test vectors The quality of the corpus-based vectors representing derived test items was determined by collecting hu-  man semantic similarity judgments in a crowdsourcing survey. [sent-110, score-0.78]
</p><p>37 All 900 derived vectors from the test set were matched with their three closest NNs in our semantic space (see Section 4. [sent-119, score-0.376]
</p><p>38 We obtained 30 judgments for each derived form (10 judgments for each of 3 neighbor comparisons), with mean participant agreement of 58%. [sent-123, score-0.268]
</p><p>39 Table 1reports the proportion of HQ test items for each affix, and Table 2 reports some examples of HQ and LQ items with the corresponding NNs. [sent-128, score-0.248]
</p><p>40 Annotation of similarity between stem and derived forms Derived forms differ in terms of how far their meaning is with respect to that of their stem. [sent-130, score-0.927]
</p><p>41 Certain morphological processes have  systematically more impact than others on meaning: For example, the adjectival prefix in- negates the meaning of the stem, whereas -ly has the sole function to convert an adjective into an adverb. [sent-131, score-0.3]
</p><p>42 But the very same affix can affect different stems in different ways. [sent-132, score-0.367]
</p><p>43 Note that the affixes with systematically lower SDR are those carrying a negative meaning (in-, un-, -less), whereas those with highest SDR do little more than changing the POS of the stem (-ion, -ly, ness). [sent-140, score-0.626]
</p><p>44 We collect co-occurrence statistics for the top 20K content words (adjectives, adverbs, nouns, verbs) 5The negative skew is not surprising, as derived forms must have some relation to their stems! [sent-148, score-0.3]
</p><p>45 6Most steps of the semantic space construction and composition pipelines were implemented using the DISSECT toolkit: https : / /github . [sent-149, score-0.356]
</p><p>46 3  Implementation of composition methods  All composition methods except mult and stem have weights to be estimated (e. [sent-167, score-1.077]
</p><p>47 , the λ parameter of dilation or the affix matrices of lexfunc). [sent-169, score-0.363]
</p><p>48 We adopt the estimation strategy proposed by Guevara (2010) and Baroni and Zamparelli (2010), namely we pick parameter values that optimize the mapping between stem and derived vectors directly extracted from the corpus. [sent-170, score-0.733]
</p><p>49 To learn, say, a lexfunc matrix representing the prefix re-, we extract vectors of V/reV pairs that occur with sufficient frequency (visit/revisit, think/rethink. [sent-171, score-0.426]
</p><p>50 We then use least-squares methods to find weights for the re- matrix that minimize the distance between each reV vector generated by the model given the input V and the corresponding corpus-observed derived vector (e. [sent-175, score-0.289]
</p><p>51 This is a general estimation approach that does not require taskspecific hand-labeled data, and for which simple analytical solutions ofthe least-squares error prob1521  lem exist for all our composition methods. [sent-178, score-0.313]
</p><p>52 8 For the lexfunc model, we use the training items separately to obtain weight matrices representing each affix, whereas for the other models all training data are used together to globally derive single sets of affix and stem weights. [sent-182, score-1.078]
</p><p>53 , v of the dilation equation is the stem vector), since it should provide richer contents than the af-  ×  fix to the derived meaning. [sent-193, score-0.668]
</p><p>54 We found that, on average across the training pairs, dilation weighted the stem 20 times more heavily than the affix (0. [sent-194, score-0.724]
</p><p>55 s Wimeil tahre performance to t dhiebaseline stem model, as confirmed below. [sent-198, score-0.4]
</p><p>56 5  Experiment 1: approximating high-quality corpus-extracted vectors  The first experiment investigates to what extent composition models can approximate high-quality (HQ) corpus-extracted vectors representing derived forms. [sent-200, score-0.835]
</p><p>57 Note that since the test items were excluded from training, we are simulating a scenario in which composition models must generate representations for nonce derived forms. [sent-201, score-0.74]
</p><p>58 Cosine similarity between model-generated and corpus-extracted vectors were computed for all models, including the stem baseline (i. [sent-202, score-0.558]
</p><p>59 The  stem method sets the level of performance relatively high, confirming its soundness. [sent-206, score-0.4]
</p><p>60 545u428nc Table 3: Mean similarity of composed vectors to high-quality corpus-extracted derived-form vectors, for all as well as high- (HR) and lowrelatedness (LR) test items larly to the baseline, while wadd outperforms it, although the effect does not reach significance  (p=. [sent-219, score-0.496]
</p><p>61 11 Both fulladd and lexfunc perform significantly better than stem (p < . [sent-221, score-0.866]
</p><p>62 , as vectors aggregating the contexts of words containing them) can work so well, at least when used in conjunction with the granular dimension-by-dimension weights assigned by the fulladd method. [sent-227, score-0.419]
</p><p>63 We hypothesize that these aggregated contexts, by providing information about the set of stems an affix combines with, capture the shared semantic features that the affix operates on. [sent-228, score-0.641]
</p><p>64 When the meaning of the derived form is far from that of its stem, the stem baseline should no longer constitute a suitable surrogate of derivedform meaning. [sent-229, score-0.704]
</p><p>65 1 above) are thus crucial to understand how well composition methods capture not only stem mean-  ing, but also affix-triggered semantics. [sent-231, score-0.713]
</p><p>66 As expected, the stem approach undergoes a strong drop when performance is measured on LR items. [sent-233, score-0.4]
</p><p>67 001), confirming that they capture the meaning of derived forms beyond what their stems contribute to it. [sent-235, score-0.538]
</p><p>68 4 x4 6f5unc  Table 4: Mean similarity of composed vectors to high-quality corpus-extracted derived-form vectors with negative affixes fulladd and lexfunc significantly outperform stem also in the HR subset (p<. [sent-247, score-1.351]
</p><p>69 That is, the models provide better approximations of derived forms even when the stem itself should already be a good surrogate. [sent-249, score-0.731]
</p><p>70 1 that forms containing the “negative” affixes -less, un- and in- received on average low SDR scores, since negation impacts meaning more drastically than other operations. [sent-252, score-0.378]
</p><p>71 Indeed, the stem baseline performs quite poorly, whereas fulladd, lexfunc and, to a lesser extent, wadd are quite effective in this condition as well, all performing greatly above the baseline. [sent-254, score-0.818]
</p><p>72 6  Experiment 2: Comparing the quality of corpus-extracted and compositionally generated words  The first experiment simulated the scenario in which derived forms are not in our corpus, so that directly extracting their representation from it is not an option. [sent-258, score-0.381]
</p><p>73 , the derived forms are attested in the source corpus). [sent-261, score-0.3]
</p><p>74 We evaluated the derived forms generated by  THLaAbR le5:cAo2 v. [sent-264, score-0.3]
</p><p>75 053uto921nrcs  for words with LQ corpus-extracted vectors  the models that performed best in the first experiment (fulladd, lexfunc and wadd), as well as the stem baseline, by means of another crowdsourcing study. [sent-269, score-0.849]
</p><p>76 The first line of Table 5 reports the average quality (on a 7-point scale) of the representations of the derived forms as produced by the models and baseline, as well as of the corpus-harvested ones (corpus column). [sent-271, score-0.477]
</p><p>77 These results indicate that morpheme composition is an effective solution when the quality of corpus-extracted derived forms is low (and the previous experiment showed that, when their  quality is high, composition can at least approximate corpus-based vectors). [sent-276, score-1.084]
</p><p>78 With respect to Experiment 1, we obtain a different ranking of the models, with lexfunc being outperformed by both wadd and fulladd (p<. [sent-277, score-0.651]
</p><p>79 The wadd 1523  composition is dominated by the stem, and by looking at the examples in Table 6 we notice that both this model and fulladd tend to feature the stem as NN (100% of the cases for wadd, 73% for fulladd in the complete test set). [sent-279, score-1.364]
</p><p>80 The question thus arises as to whether the good performance of these composition techniques is simply due to the fact that they produce derived forms that are near their stems, with no added semantic value from the affix (a “stemploitation” strategy). [sent-280, score-0.887]
</p><p>81 However, the stemploitation hypothesis is dispelled by the observation that both models significantly outperform the stem baseline (p<. [sent-281, score-0.469]
</p><p>82 001), despite the fact that the latter, again, has good performance, significantly outperforming the corpus-  derived vectors (p < . [sent-282, score-0.333]
</p><p>83 Thus, we confirm that compositional models provide higher quality vectors that are capturing the meaning of derived forms beyond the information provided by the stem. [sent-284, score-0.731]
</p><p>84 1), fulladd and wadd still significantly outperform the corpus representations (p < . [sent-286, score-0.493]
</p><p>85 001), whereas the quality of the stem representations of LR items is not significantly different form that of the corpus-derived ones. [sent-287, score-0.622]
</p><p>86 7  Conclusion and future work  We investigated to what extent cDSMs can generate effective meaning representations of complex words through morpheme composition. [sent-289, score-0.315]
</p><p>87 Several  state-of-the-art composition models were adapted and evaluated on this novel task. [sent-290, score-0.344]
</p><p>88 Our results suggest that morpheme composition can indeed provide high-quality vectors for complex forms, improving both on vectors directly extracted from the corpus and on a stem-backoff strategy. [sent-291, score-0.767]
</p><p>89 This result is of practical importance for distributional semantics, as it paves the way to address one of the main causes of data sparseness, and it confirms the usefulness of the compositional approach in a new domain. [sent-292, score-0.248]
</p><p>90 Overall, fulladd emerged as the best performing model, with both lexfunc and the simple wadd approach constituting strong rivals. [sent-293, score-0.651]
</p><p>91 The effectiveness of the best models extended also to the challenging cases where the meaning of derived forms is far from that of the stem, including negative affixes. [sent-294, score-0.433]
</p><p>92 The fulladd method requires a vector representation for bound morphemes. [sent-295, score-0.338]
</p><p>93 A first direction for future work will thus be to investigate which aspects of the meaning of bound morphemes are captured by our current simple-minded approach to populating their vectors, and to explore alterna-  tive ways to construct them, seeing if they further improve fulladd performance. [sent-296, score-0.472]
</p><p>94 A natural extension of our research is to address morpheme composition and morphological induction jointly, trying to model the intuition that good candidate morphemes should have coherent semantic representations. [sent-297, score-0.734]
</p><p>95 We would also like to apply composition to inflectional morphology (that currently lies outside the scope of distributional semantics), to capture the nuances of meaning that, for example, distinguish singular and plural nouns (consider, e. [sent-302, score-0.67]
</p><p>96 Finally, in our current setup we focus on a single composition step, e. [sent-305, score-0.313]
</p><p>97 , we derive the meaning of  inoperable by composing the morphemes in- and operable. [sent-307, score-0.265]
</p><p>98 In the future, we will explore recursive morpheme composition, especially since we would like to apply these methods to more complex morphological systems (e. [sent-309, score-0.301]
</p><p>99 Intensionality was only alleged: On adjective-noun composition in distributional semantics. [sent-352, score-0.46]
</p><p>100 Mathematical foundations for a compositional distributional model of meaning. [sent-366, score-0.248]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stem', 0.4), ('composition', 0.313), ('fulladd', 0.233), ('lexfunc', 0.233), ('affix', 0.231), ('wadd', 0.185), ('derived', 0.175), ('morphological', 0.163), ('vectors', 0.158), ('distributional', 0.147), ('stems', 0.136), ('forms', 0.125), ('dsms', 0.125), ('affixes', 0.124), ('cdsms', 0.118), ('lq', 0.109), ('items', 0.108), ('morphology', 0.108), ('meaning', 0.102), ('compositional', 0.101), ('dilation', 0.093), ('celex', 0.093), ('morphemes', 0.089), ('lr', 0.088), ('morpheme', 0.08), ('morphologically', 0.078), ('hq', 0.078), ('sdr', 0.076), ('representations', 0.075), ('mitchell', 0.071), ('guevara', 0.069), ('zamparelli', 0.069), ('rebuild', 0.069), ('baroni', 0.067), ('derivational', 0.067), ('nns', 0.064), ('socher', 0.062), ('complex', 0.058), ('vector', 0.057), ('mult', 0.051), ('sparseness', 0.049), ('bound', 0.048), ('bullinaria', 0.047), ('induction', 0.046), ('additive', 0.046), ('lapata', 0.045), ('composed', 0.045), ('gruyter', 0.044), ('semantic', 0.043), ('coecke', 0.042), ('transparency', 0.042), ('compositionally', 0.042), ('baayen', 0.042), ('hr', 0.042), ('relatedness', 0.04), ('matrices', 0.039), ('quality', 0.039), ('semantics', 0.039), ('mouton', 0.038), ('abdi', 0.038), ('cambrdige', 0.038), ('inoperable', 0.038), ('nonce', 0.038), ('operable', 0.038), ('preller', 0.038), ('stemploitation', 0.038), ('tukey', 0.038), ('wicentowski', 0.038), ('goldwater', 0.038), ('derive', 0.036), ('marco', 0.035), ('prefix', 0.035), ('derivation', 0.034), ('compositionality', 0.034), ('beesley', 0.034), ('dreyer', 0.034), ('nghia', 0.034), ('judgments', 0.033), ('proxy', 0.032), ('reports', 0.032), ('functor', 0.031), ('models', 0.031), ('emiliano', 0.029), ('multiplicative', 0.029), ('naradowsky', 0.029), ('schone', 0.029), ('meanings', 0.028), ('crowdflower', 0.028), ('zanzotto', 0.028), ('productivity', 0.028), ('contexts', 0.028), ('crowdsourcing', 0.027), ('constitute', 0.027), ('sharon', 0.027), ('negation', 0.027), ('mean', 0.027), ('jeff', 0.027), ('mehrnoosh', 0.027), ('boleda', 0.027), ('unlimited', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000014 <a title="87-tfidf-1" href="./acl-2013-Compositional-ly_Derived_Representations_of_Morphologically_Complex_Words_in_Distributional_Semantics.html">87 acl-2013-Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics</a></p>
<p>Author: Angeliki Lazaridou ; Marco Marelli ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: Speakers of a language can construct an unlimited number of new words through morphological derivation. This is a major cause of data sparseness for corpus-based approaches to lexical semantics, such as distributional semantic models of word meaning. We adapt compositional methods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts. Semantic representations constructed in this way beat a strong baseline and can be of higher quality than representations directly constructed from corpus data. Our results constitute a novel evaluation of the proposed composition methods, in which the full additive model achieves the best performance, and demonstrate the usefulness of a compositional morphology component in distributional semantics.</p><p>2 0.35565442 <a title="87-tfidf-2" href="./acl-2013-A_relatedness_benchmark_to_test_the_role_of_determiners_in_compositional_distributional_semantics.html">32 acl-2013-A relatedness benchmark to test the role of determiners in compositional distributional semantics</a></p>
<p>Author: Raffaella Bernardi ; Georgiana Dinu ; Marco Marelli ; Marco Baroni</p><p>Abstract: Distributional models of semantics capture word meaning very effectively, and they have been recently extended to account for compositionally-obtained representations of phrases made of content words. We explore whether compositional distributional semantic models can also handle a construction in which grammatical terms play a crucial role, namely determiner phrases (DPs). We introduce a new publicly available dataset to test distributional representations of DPs, and we evaluate state-of-the-art models on this set.</p><p>3 0.35351688 <a title="87-tfidf-3" href="./acl-2013-Stem_Translation_with_Affix-Based_Rule_Selection_for_Agglutinative_Languages.html">330 acl-2013-Stem Translation with Affix-Based Rule Selection for Agglutinative Languages</a></p>
<p>Author: Zhiyang Wang ; Yajuan Lu ; Meng Sun ; Qun Liu</p><p>Abstract: Current translation models are mainly designed for languages with limited morphology, which are not readily applicable to agglutinative languages as the difference in the way lexical forms are generated. In this paper, we propose a novel approach for translating agglutinative languages by treating stems and affixes differently. We employ stem as the atomic translation unit to alleviate data spareness. In addition, we associate each stemgranularity translation rule with a distribution of related affixes, and select desirable rules according to the similarity of their affix distributions with given spans to be translated. Experimental results show that our approach significantly improves the translation performance on tasks of translating from three Turkic languages to Chinese.</p><p>4 0.30283716 <a title="87-tfidf-4" href="./acl-2013-DISSECT_-_DIStributional_SEmantics_Composition_Toolkit.html">103 acl-2013-DISSECT - DIStributional SEmantics Composition Toolkit</a></p>
<p>Author: Georgiana Dinu ; Nghia The Pham ; Marco Baroni</p><p>Abstract: We introduce DISSECT, a toolkit to build and explore computational models of word, phrase and sentence meaning based on the principles of distributional semantics. The toolkit focuses in particular on compositional meaning, and implements a number of composition methods that have been proposed in the literature. Furthermore, DISSECT can be useful to researchers and practitioners who need models of word meaning (without composition) as well, as it supports various methods to construct distributional semantic spaces, assessing similarity and even evaluating against benchmarks, that are independent of the composition infrastructure.</p><p>5 0.18609056 <a title="87-tfidf-5" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>Author: Kartik Goyal ; Sujay Kumar Jauhar ; Huiying Li ; Mrinmaya Sachan ; Shashank Srivastava ; Eduard Hovy</p><p>Abstract: In this paper we present a novel approach to modelling distributional semantics that represents meaning as distributions over relations in syntactic neighborhoods. We argue that our model approximates meaning in compositional configurations more effectively than standard distributional vectors or bag-of-words models. We test our hypothesis on the problem of judging event coreferentiality, which involves compositional interactions in the predicate-argument structure of sentences, and demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics pre- viously employed in the literature.</p><p>6 0.17868996 <a title="87-tfidf-6" href="./acl-2013-Categorization_of_Turkish_News_Documents_with_Morphological_Analysis.html">78 acl-2013-Categorization of Turkish News Documents with Morphological Analysis</a></p>
<p>7 0.17280164 <a title="87-tfidf-7" href="./acl-2013-DErivBase%3A_Inducing_and_Evaluating_a_Derivational_Morphology_Resource_for_German.html">102 acl-2013-DErivBase: Inducing and Evaluating a Derivational Morphology Resource for German</a></p>
<p>8 0.17083524 <a title="87-tfidf-8" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>9 0.13933459 <a title="87-tfidf-9" href="./acl-2013-Measuring_semantic_content_in_distributional_vectors.html">238 acl-2013-Measuring semantic content in distributional vectors</a></p>
<p>10 0.13894679 <a title="87-tfidf-10" href="./acl-2013-Psycholinguistically_Motivated_Computational_Models_on_the_Organization_and_Processing_of_Morphologically_Complex_Words.html">286 acl-2013-Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words</a></p>
<p>11 0.13704562 <a title="87-tfidf-11" href="./acl-2013-Derivational_Smoothing_for_Syntactic_Distributional_Semantics.html">113 acl-2013-Derivational Smoothing for Syntactic Distributional Semantics</a></p>
<p>12 0.11235064 <a title="87-tfidf-12" href="./acl-2013-Robust_multilingual_statistical_morphological_generation_models.html">303 acl-2013-Robust multilingual statistical morphological generation models</a></p>
<p>13 0.11130268 <a title="87-tfidf-13" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>14 0.10040323 <a title="87-tfidf-14" href="./acl-2013-A_corpus-based_evaluation_method_for_Distributional_Semantic_Models.html">31 acl-2013-A corpus-based evaluation method for Distributional Semantic Models</a></p>
<p>15 0.10007025 <a title="87-tfidf-15" href="./acl-2013-Using_subcategorization_knowledge_to_improve_case_prediction_for_translation_to_German.html">378 acl-2013-Using subcategorization knowledge to improve case prediction for translation to German</a></p>
<p>16 0.090799801 <a title="87-tfidf-16" href="./acl-2013-Real-World_Semi-Supervised_Learning_of_POS-Taggers_for_Low-Resource_Languages.html">295 acl-2013-Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages</a></p>
<p>17 0.090100169 <a title="87-tfidf-17" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>18 0.081251651 <a title="87-tfidf-18" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>19 0.077459812 <a title="87-tfidf-19" href="./acl-2013-Using_Lexical_Expansion_to_Learn_Inference_Rules_from_Sparse_Data.html">376 acl-2013-Using Lexical Expansion to Learn Inference Rules from Sparse Data</a></p>
<p>20 0.076275088 <a title="87-tfidf-20" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.195), (1, 0.019), (2, 0.027), (3, -0.141), (4, -0.127), (5, -0.13), (6, -0.037), (7, 0.053), (8, 0.002), (9, 0.13), (10, -0.103), (11, 0.042), (12, 0.304), (13, -0.132), (14, -0.162), (15, 0.117), (16, -0.148), (17, -0.294), (18, -0.005), (19, -0.021), (20, -0.168), (21, 0.135), (22, 0.067), (23, 0.008), (24, -0.156), (25, -0.039), (26, -0.042), (27, 0.017), (28, -0.051), (29, 0.068), (30, 0.104), (31, 0.102), (32, 0.043), (33, 0.051), (34, -0.031), (35, 0.041), (36, -0.001), (37, 0.053), (38, 0.017), (39, -0.012), (40, 0.05), (41, -0.067), (42, 0.081), (43, 0.106), (44, 0.024), (45, 0.044), (46, -0.036), (47, -0.021), (48, 0.001), (49, 0.113)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9473303 <a title="87-lsi-1" href="./acl-2013-Compositional-ly_Derived_Representations_of_Morphologically_Complex_Words_in_Distributional_Semantics.html">87 acl-2013-Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics</a></p>
<p>Author: Angeliki Lazaridou ; Marco Marelli ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: Speakers of a language can construct an unlimited number of new words through morphological derivation. This is a major cause of data sparseness for corpus-based approaches to lexical semantics, such as distributional semantic models of word meaning. We adapt compositional methods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts. Semantic representations constructed in this way beat a strong baseline and can be of higher quality than representations directly constructed from corpus data. Our results constitute a novel evaluation of the proposed composition methods, in which the full additive model achieves the best performance, and demonstrate the usefulness of a compositional morphology component in distributional semantics.</p><p>2 0.81598932 <a title="87-lsi-2" href="./acl-2013-DISSECT_-_DIStributional_SEmantics_Composition_Toolkit.html">103 acl-2013-DISSECT - DIStributional SEmantics Composition Toolkit</a></p>
<p>Author: Georgiana Dinu ; Nghia The Pham ; Marco Baroni</p><p>Abstract: We introduce DISSECT, a toolkit to build and explore computational models of word, phrase and sentence meaning based on the principles of distributional semantics. The toolkit focuses in particular on compositional meaning, and implements a number of composition methods that have been proposed in the literature. Furthermore, DISSECT can be useful to researchers and practitioners who need models of word meaning (without composition) as well, as it supports various methods to construct distributional semantic spaces, assessing similarity and even evaluating against benchmarks, that are independent of the composition infrastructure.</p><p>3 0.79783142 <a title="87-lsi-3" href="./acl-2013-A_relatedness_benchmark_to_test_the_role_of_determiners_in_compositional_distributional_semantics.html">32 acl-2013-A relatedness benchmark to test the role of determiners in compositional distributional semantics</a></p>
<p>Author: Raffaella Bernardi ; Georgiana Dinu ; Marco Marelli ; Marco Baroni</p><p>Abstract: Distributional models of semantics capture word meaning very effectively, and they have been recently extended to account for compositionally-obtained representations of phrases made of content words. We explore whether compositional distributional semantic models can also handle a construction in which grammatical terms play a crucial role, namely determiner phrases (DPs). We introduce a new publicly available dataset to test distributional representations of DPs, and we evaluate state-of-the-art models on this set.</p><p>4 0.61381245 <a title="87-lsi-4" href="./acl-2013-Psycholinguistically_Motivated_Computational_Models_on_the_Organization_and_Processing_of_Morphologically_Complex_Words.html">286 acl-2013-Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words</a></p>
<p>Author: Tirthankar Dasgupta</p><p>Abstract: In this work we present psycholinguistically motivated computational models for the organization and processing of Bangla morphologically complex words in the mental lexicon. Our goal is to identify whether morphologically complex words are stored as a whole or are they organized along the morphological line. For this, we have conducted a series of psycholinguistic experiments to build up hypothesis on the possible organizational structure of the mental lexicon. Next, we develop computational models based on the collected dataset. We observed that derivationally suffixed Bangla words are in general decomposed during processing and compositionality between the stem . and the suffix plays an important role in the decomposition process. We observed the same phenomena for Bangla verb sequences where experiments showed noncompositional verb sequences are in general stored as a whole in the ML and low traces of compositional verbs are found in the mental lexicon. 1 IInnttrroodduuccttiioonn Mental lexicon is the representation of the words in the human mind and their associations that help fast retrieval and comprehension (Aitchison, 1987). Words are known to be associated with each other in terms of, orthography, phonology, morphology and semantics. However, the precise nature of these relations is unknown. An important issue that has been a subject of study for a long time is to identify the fundamental units in terms of which the mental lexicon is i itkgp .ernet . in organized. That is, whether lexical representations in the mental lexicon are word based or are they organized along morphological lines. For example, whether a word such as “unimaginable” is stored in the mental lexicon as a whole word or do we break it up “un-” , “imagine” and “able”, understand the meaning of each of these constituent and then recombine the units to comprehend the whole word. Such questions are typically answered by designing appropriate priming experiments (Marslen-Wilson et al., 1994) or other lexical decision tasks. The reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain. (See Sec. 2 for models of morphological organization and access and related experiments). A clear understanding of the structure and the processing mechanism of the mental lexicon will further our knowledge of how the human brain processes language. Further, these linguistically important and interesting questions are also highly significant for computational linguistics (CL) and natural language processing (NLP) applications. Their computational significance arises from the issue of their storage in lexical resources like WordNet (Fellbaum, 1998) and raises the questions like, how to store morphologically complex words, in a lexical resource like WordNet keeping in mind the storage and access efficiency. There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English, Hebrew, Italian, French, Dutch, and few other languages (Marslen-Wilson et al., 2008; Frost et al., 1997; Grainger, et al., 1991 ; Drews and Zwitserlood, 1995). However, we do not know of any such investigations for Indian languages, which 123 Sofia, BuPrlgoacreiead, iAngusgu osft 4h-e9 A 2C01L3 S.tu ?c d2en0t1 3Re Ases aorc hiat Wio nrk fsohro Cp,om papguesta 1ti2o3n–a1l2 L9in,guistics are morphologically richer than many of their Indo-European cousins. Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet. On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004). Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages. This work aims to design cognitively motivated computational models that can explain the organization and processing of Bangla morphologically complex words in the mental lexicon. Presently we will concentrate on the following two aspects:   OOrrggaanniizzaattiioonn aanndd pprroocceessssiinngg ooff BBaannggllaa PPo o l yy-mmoorrpphheemmiicc wwoorrddss:: our objective here is to determine whether the mental lexicon decomposes morphologically complex words into its constituent morphemes or does it represent the unanalyzed surface form of a word. OOrrggaanniizzaattiioonn aanndd pprroocceessssiinngg ooff BBaannggllaa ccoomm-ppoouunndd vveerrbbss ((CCVV)) :: compound verbs are the subject of much debate in linguistic theory. No consensus has been reached yet with respect to the issue that whether to consider them as unitary lexical units or are they syntactically assembled combinations of two independent lexical units. As linguistic arguments have so far not led to a consensus, we here use cognitive experiments to probe the brain signatures of verb-verb combinations and propose cognitive as well as computational models regarding the possible organization and processing of Bangla CVs in the mental lexicon (ML). With respect to this, we apply the different priming and other lexical decision experiments, described in literature (Marslen-Wilson et al., 1994; Bentin, S. and Feldman, 1990) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla. Our cross-modal and masked priming experiment on Bangla derivationally suffixed words shows that morphological relatedness between lexical items triggers a significant priming effect, even when the forms are phonologically/orthographically unrelated. These observations are similar to those reported for English and indicate that derivationally suffixed words in Bangla are in general accessed through decomposition of the word into its constituent morphemes. Further, based on the experimental data we have developed a series of computational models that can be used to predict the decomposition of Bangla polymorphemic words. Our evaluation result shows that decom- position of a polymorphemic word depends on several factors like, frequency, productivity of the suffix and the compositionality between the stem and the suffix. The organization of the paper is as follows: Sec. 2 presents related works; Sec. 3 describes experiment design and procedure; Sec. 4 presents the processing of CVs; and finally, Sec. 5 concludes the paper by presenting the future direction of the work. 2 RReellaatteedd WWoorrkkss 2. . 11 RReepprreesseennttaattiioonn ooff ppoollyymmoorrpphheemmiicc wwoorrddss Over the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages. Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model. The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon (Bradley, 1980; Butterworth, 1983). On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units. The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975; Taft, 1981 ; MacKay, 1978). Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately. For instance, the derived morphological forms are believed to be represented as a whole, whereas the representation of the inflected forms follows the morphemic model (Caramazza et al., 1988). Traditionally, priming experiments have been used to study the effects of morphology in language processing. Priming is a process that results in increase in speed or accuracy of response to a stimulus, called the target, based on the occurrence of a prior exposure of another stimulus, called the prime (Tulving et al., 1982). Here, subjects are exposed to a prime word for a short duration, and are subsequently shown a target word. The prime and target words may be morphologically, phonologically or semantically re124 lated. An analysis of the effect of the reaction time of subjects reveals the actual organization and representation of the lexicon at the relevant level. See Pulvermüller (2002) for a detailed account of such phenomena. It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations. (Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency. Similar observation for surface word frequency was also observed by (Bertram et al., 2000;Bradley, 1980;Burani et al., 1987;Burani et al., 1984;Schreuder et al., 1997; Taft 1975;Taft, 2004) where it has been claimed that words having low surface frequency tends to decompose. Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole. If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts. 2. . 22 RReepprreesseennttaattiioonn ooff CCoommppoouunndd A compound verb (CV) consists of two verbs (V1 and V2) acting as and expresses a single expression For example, in the sentence VVeerrbbss a sequence of a single verb of meaning. রুটিগুল ো খেল খেল ো (/ruTigulo kheYe phela/) ―bread-plural-the eat and drop-pres. Imp‖ ―Eat the breads‖ the verb sequence “খেল খেল ো (eat drop)” is an example of CV. Compound verbs are a special phenomena that are abundantly found in IndoEuropean languages like Indian languages. A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus. Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries. Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument struc- ture. Bashir (1993) tried to construct a semantic analysis based on “prepared” and “unprepared mind”. Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compatibility, which is subject to syntactic constraints. Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism. She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible. Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models. 3 TThhee PPrrooppoosseedd AApppprrooaacchheess 3. . 11 TThhee ppssyycchhoolliinngguuiissttiicc eexxppeerriimmeennttss We apply two different priming experiments namely, the cross modal priming and masked priming experiment discussed in (Forster and Davis, 1984; Rastle et al., 2000;Marslen-Wilson et al., 1994; Marslen-Wilson et al., 2008) for Bangla morphologically complex words. Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming). The subjects were asked to make a lexical decision whether the given target is a valid word in that language. The same target word is again probed but with a different audio or visual probe called the control word. The control shows no relationship with the target. For example, baYaska (aged) and baYasa (age) is a prime-target pair, for which the corresponding control-target pair could be naYana (eye) and baYasa (age). Similar to (Marslen-Wilson et al., 2008) the masked priming has been conducted for three different SOA (Stimulus Onset Asynchrony), 48ms, 72ms and 120ms. The SOA is measured as the amount of time between the start the first stimulus till the start of the next stimulus. TCM abl-’+ Sse-+ O1 +:-DatjdgnmAshielbatArDu)f(osiAMrawnteihmsgcdaoe)lEx-npgmAchebamr)iD-gnatmprhdiYlbeaA(n ftrTsli,ae(+gnrmdisc)phroielctn)osrelated, and - implies unrelated. There were 500 prime-target and controltarget pairs classified into five classes. Depending on the class, the prime is related to the target 125 either in terms of morphology, semantics, orthography and/or Phonology (See Table 1). The experiments were conducted on 24 highly educated native Bangla speakers. Nineteen of them have a graduate degree and five hold a post graduate degree. The age of the subjects varies between 22 to 35 years. RReessuullttss:: The RTs with extreme values and incorrect decisions were excluded from the data. The data has been analyzed using two ways ANOVA with three factors: priming (prime and control), conditions (five classes) and prime durations (three different SOA). We observe strong priming effects (p<0.05) when the target word is morphologically derived and has a recognizable suffix, semantically and orthographically related with respect to the prime; no priming effects are observed when the prime and target words are orthographically related but share no morphological or semantic relationship; although not statistically significant (p>0.07), but weak priming is observed for prime target pairs that are only semantically related. We see no significant difference between the prime and control RTs for other classes. We also looked at the RTs for each of the 500 target words. We observe that maximum priming occurs for words in [M+S+O+](69%), some priming is evident in [M+S+O-](51%) and [M'+S-O+](48%), but for most of the words in [M-S+O-](86%) and [M-S-O+](92%) no priming effect was observed. 3. . 22 FFrreeqquueennccyy DDiissttrriibbuuttiioonn MMooddeellss ooff MMoo rrpphhoo-llooggiiccaall PPrroocceessssiinngg From the above results we saw that not all polymorphemic words tend to decompose during processing, thus we need to further investigate the processing phenomena of Bangla derived words. One notable means is to identify whether the stem or suffix frequency is involved in the processing stage of that word. For this, we apply different frequency based models to the Bangla polymorphemic words and try to evaluate their performance by comparing their predicted results with the result obtained through the priming experiment. MMooddeell --11:: BBaassee aanndd SSuurrffaaccee wwoorrdd ffrreeqquueennccyy ee ff-ffeecctt -- It states that the probability of decomposition of a Bangla polymorphemic word depends upon the frequency of its base word. Thus, if the stem frequency of a polymorphemic word crosses a given threshold value, then the word will decomposed into its constituent morpheme. Similar claim has been made for surface word frequency model where decomposition depends upon the frequency of the surface word itself. We have evaluated both the models with the 500 words used in the priming experiments discussed above. We have achieved an accuracy of 62% and 49% respectively for base and surface word frequency models. MMooddeell --22:: CCoommbbiinniinngg tthhee bbaassee aanndd ssuurrffaaccee wwoorrdd ffrreeq quueennccyy -- In a pursuit towards an extended model, we combine model 1 and 2 together. We took the log frequencies of both the base and the derived words and plotted the best-fit regression curve over the given dataset. The evaluation of this model over the same set of 500 target words returns an accuracy of 68% which is better than the base and surface word frequency models. However, the proposed model still fails to predict processing of around 32% of words. This led us to further enhance the model. For this, we analyze the role of suffixes in morphological processing. MMooddeell -- 33:: DDeeggrreeee ooff AAffffiixxaattiioonn aanndd SSuuffffiixx PPrroodd-uuccttiivviittyy:: we examine whether the regression analysis between base and derived frequency of Bangla words varies between suffixes and how these variations affect morphological decomposition. With respect to this, we try to compute the degree of affixation between the suffix and the base word. For this, we perform regression analysis on sixteen different Bangla suffixes with varying degree of type and token frequencies. For each suffix, we choose 100 different derived words. We observe that those suffixes having high value of intercept are forming derived words whose base frequencies are substantially high as compared to their derived forms. Moreover we also observe that high intercept value for a given suffix indicates higher inclination towards decomposition. Next, we try to analyze the role of suffix type/token ratio and compare them with the base/derived frequency ratio model. This has been done by regression analysis between the suffix type-token ratios with the base-surface frequency ratio. We further tried to observe the role of suffix productivity in morphological processing. For this, we computed the three components of productivity P, P* and V as discussed in (Hay and Plag, 2004). P is the “conditioned degree of productivity” and is the probability that we are encountering a word with an affix and it is representing a new type. P* is the “hapaxedconditioned degree of productivity”. It expresses the probability that when an entirely new word is 126 encountered it will contain the suffix. V is the “type frequency”. Finally, we computed the productivity of a suffix through its P, P* and V values. We found that decomposition of Bangla polymorphemic word is directly proportional to the productivity of the suffix. Therefore, words that are composed of productive suffixes (P value ranges between 0.6 and 0.9) like “-oYAlA”, “-giri”, “-tba” and “-panA” are highly decomposable than low productive suffixes like “-Ani”, “-lA”, “-k”, and “-tama”. The evaluation of the proposed model returns an accuracy of 76% which comes to be 8% better than the preceding models. CCoommbbiinniinngg MMooddeell --22 aanndd MMooddeell -- 33:: One important observation that can be made from the above results is that, model-3 performs best in determining the true negative values. It also possesses a high recall value of (85%) but having a low precision of (50%). In other words, the model can predict those words for which decomposition will not take place. On the other hand, results of Model-2 posses a high precision of 70%. Thus, we argue that combining the above two models can better predict the decomposition of Bangla polymorphemic words. Hence, we combine the two models together and finally achieved an overall accuracy of 80% with a precision of 87% and a recall of 78%. This surpasses the performance of the other models discussed earlier. However, around 22% of the test words were wrongly classified which the model fails to justify. Thus, a more rigorous set of experiments and data analysis are required to predict access mechanisms of such Bangla polymorphemic words. 3. . 33 SStteemm- -SSuuffffiixx CCoommppoossiittiioonnaalliittyy Compositionality refers to the fact that meaning of a complex expression is inferred from the meaning of its constituents. Therefore, the cost of retrieving a word from the secondary memory is directly proportional to the cost of retrieving the individual parts (i.e the stem and the suffix). Thus, following the work of (Milin et al., 2009) we define the compositionality of a morphologically complex word (We) as: C(We)=α 1H(We)+α α2H(e)+α α3H(W|e)+ α4H(e|W) Where, H(x) is entropy of an expression x, H(W|e) is the conditional entropy between the stem W and suffix e and is the proportionality factor whose value is computed through regression analysis. Next, we tried to compute the compositionality of the stem and suffixes in terms of relative entropy D(W||e) and Point wise mutual information (PMI). The relative entropy is the measure of the distance between the probability distribution of the stem W and the suffix e. The PMI measures the amount of information that one random variable (the stem) contains about the other (the suffix). We have compared the above three techniques with the actual reaction time data collected through the priming and lexical decision experiment. We observed that all the three information theoretic models perform much better than the frequency based models discussed in the earlier section, for predicting the decomposability of Bangla polymorphemic words. However, we think it is still premature to claim anything concrete at this stage of our work. We believe much more rigorous experiments are needed to be per- formed in order to validate our proposed models. Further, the present paper does not consider factors related to age of acquisition, and word familiarity effects that plays important role in the processing of morphologically complex words. Moreover, it is also very interesting to see how stacking of multiple suffixes in a word are processed by the human brain. 44 OOrrggaanniizzaattiioonn aanndd PPrroocceessssiinngg ooff CCoomm-ppoouunndd VVeerrbbss iinn tthhee MMeennttaall LLeexxiiccoonn Compound verbs, as discussed above, are special type of verb sequences consisting of two or more verbs acting as a single verb and express a single expression of meaning. The verb V1 is known as pole and V2 is called as vector. For example, “ওঠে পড়া ” (getting up) is a compound verb where individual words do not entirely reflects the meaning of the whole expression. However, not all V1+V2 combinations are CVs. For example, expressions like, “নিঠে য়াও ”(take and then go) and “ নিঠে আঠ ়া” (return back) are the examples of verb sequences where meaning of the whole expression can be derived from the mean- ing of the individual component and thus, these verb sequences are not considered as CV. The key question linguists are trying to identify for a long time and debating a lot is whether to consider CVs as a single lexical units or consider them as two separate units. Since linguistic rules fails to explain the process, we for the first time tried to perform cognitive experiments to understand the organization and processing of such verb sequences in the human mind. A clear understanding about these phenomena may help us to classify or extract actual CVs from other verb 127 sequences. In order to do so, presently we have applied three different techniques to collect user data. In the first technique, we annotated 4500 V1+V2 sequences, along with their example sentences, using a group of three linguists (the expert subjects). We asked the experts to classify the verb sequences into three classes namely, CV, not a CV and not sure. Each linguist has received 2000 verb pairs along with their respective example sentences. Out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping. We measure the inter annotator agreement using the Fleiss Kappa (Fleiss et al., 1981) measure (κ) where the agreement lies around 0.79. Next, out of the 500 common verb sequences that were annotated by all the three linguists, we randomly choose 300 V1+V2 pairs and presented them to 36 native Bangla speakers. We ask each subjects to give a compositionality score of each verb sequences under 1-10 point scale, 10 being highly compositional and 1 for noncompositional. We found an agreement of κ=0.69 among the subjects. We also observe a continuum of compositionality score among the verb sequences. This reflects that it is difficult to classify Bangla verb sequences discretely into the classes of CV and not a CV. We then, compare the compositionality score with that of the expert user’s annotation. We found a significant correlation between the expert annotation and the compositionality score. We observe verb sequences that are annotated as CVs (like, খেঠে খিল )কঠে খি ,ওঠে পড ,have got low compositionality score (average score ranges between 1-4) on the other hand high compositional values are in general tagged as not a cv (নিঠে য়া (come and get), নিঠে আে (return back), তুঠল খেঠেনি (kept), গনিঠে পিল (roll on floor)). This reflects that verb sequences which are not CV shows high degree of compositionality. In other words non CV verbs can directly interpret from their constituent verbs. This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the non-compositional verbs which maps to a single expression of meaning. In order to validate such claim we perform a lexical decision experiment using 32 native Bangla speakers with 92 different verb sequences. We followed the same experimental procedure as discussed in (Taft, 2004) for English polymorphemic words. However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination. The reaction time (RT) of each subject is recorded. Our preliminarily observation from the RT analysis shows that as per our claim, RT of verb sequences having high compositionality value is significantly higher than the RTs for low or noncompositional verbs. This proves our hypothesis that Bangla compound verbs that show less compositionality are stored as a hole in the mental lexicon and thus follows the full-listing model whereas compositional verb phrases are individually parsed. However, we do believe that our experiment is composed of a very small set of data and it is premature to conclude anything concrete based only on the current experimental results. 5 FFuuttuurree DDiirreeccttiioonnss In the next phase of our work we will focus on the following aspects of Bangla morphologically complex words: TThhee WWoorrdd FFaammiilliiaarriittyy EEffffeecctt:: Here, our aim is to study the role of familiarity of a word during its processing. We define the familiarity of a word in terms of corpus frequency, Age of acquisition, the level of language exposure of a person, and RT of the word etc. RRoollee ooff ssuuffffiixx ttyyppeess iinn mmoorrpphhoollooggiiccaall ddeeccoo mm ppoo-ssiittiioonn:: For native Bangla speakers which morphological suffixes are internalized and which are just learnt in school, but never internalized. We can compare the representation of Native, Sanskrit derived and foreign suffixes in Bangla words. CCoommppuuttaattiioonnaall mmooddeellss ooff oorrggaanniizzaattiioonn aanndd pprroocceessssiinngg ooff BBaannggllaa ccoommppoouunndd vveerrbbss :: presently we have performed some small set of experiments to study processing of compound verbs in the mental lexicon. In the next phase of our work we will extend the existing experiments and also apply some more techniques like, crowd sourcing and language games to collect more relevant RT and compositionality data. Finally, based on the collected data we will develop computational models that can explain the possible organizational structure and processing mechanism of morphologically complex Bangla words in the mental lexicon. Reference Aitchison, J. (1987). ―Words in the mind: An introduction to the mental lexicon‖. Wiley-Blackwell, 128 Baayen R. H. (2000). ―On frequency, transparency and productivity‖. G. Booij and J. van Marle (eds), Yearbook of Morphology, pages 181-208, Baayen R.H. (2003). ―Probabilistic approaches to morphology‖. Probabilistic linguistics, pages 229287. Baayen R.H., T. Dijkstra, and R. Schreuder. (1997). ―Singulars and plurals in dutch: Evidence for a parallel dual-route model‖. Journal of Memory and Language, 37(1):94-1 17. Bashir, E. (1993), ―Causal Chains and Compound Verbs.‖ In M. K. Verma ed. (1993). Bentin, S. & Feldman, L.B. (1990). The contribution of morphological and semantic relatedness to repetition priming at short and long lags: Evidence from Hebrew. Quarterly Journal of Experimental Psychology, 42, pp. 693–71 1. Bradley, D. (1980). Lexical representation of derivational relation, Juncture, Saratoga, CA: Anma Libri, pp. 37-55. Butt, M. (1993), ―Conscious choice and some light verbs in Urdu.‖ In M. K. Verma ed. (1993). Butterworth, B. (1983). Lexical Representation, Language Production, Vol. 2, pp. 257-294, San Diego, CA: Academic Press. Caramazza, A., Laudanna, A. and Romani, C. (1988). Lexical access and inflectional morphology. Cognition, 28, pp. 297-332. Drews, E., and Zwitserlood, P. (1995).Morphological and orthographic similarity in visual word recognition. Journal of Experimental Psychology:HumanPerception andPerformance, 21, 1098– 1116. Fellbaum, C. (ed.). (1998). WordNet: An Electronic Lexical Database, MIT Press. Forster, K.I., and Davis, C. (1984). Repetition priming and frequency attenuation in lexical access. Journal of Experimental Psychology: Learning, Memory, and Cognition, 10, 680–698. Frost, R., Forster, K.I., & Deutsch, A. (1997). What can we learn from the morphology of Hebrew? A masked-priming investigation of morphological representation. Journal of Experimental Psychology: Learning, Memory, and Cognition, 23, 829–856. Grainger, J., Cole, P., & Segui, J. (1991). Masked morphological priming in visual word recognition. Journal of Memory and Language, 30, 370–384. Hook, P. E. (1981). ―Hindi Structures: Intermediate Level.‖ Michigan Papers on South and Southeast Asia, The University of Michigan Center for South and Southeast Studies, Ann Arbor, Michigan. Joseph L Fleiss, Bruce Levin, and Myunghee Cho Paik. 1981. The measurement of interrater agreement. Statistical methods for rates and proportions,2:212–236. MacKay,D.G.(1978), Derivational rules and the internal lexicon. Journal of Verbal Learning and Verbal Behavior, 17, pp.61-71. Marslen-Wilson, W.D., & Tyler, L.K. (1997). Dissociating types of mental computation. Nature, 387, pp. 592–594. Marslen-Wilson, W.D., & Tyler, L.K. (1998). Rules, representations, and the English past tense. Trends in Cognitive Sciences, 2, pp. 428–435. Marslen-Wilson, W.D., Tyler, L.K., Waksler, R., & Older, L. (1994). Morphology and meaning in the English mental lexicon. Psychological Review, 101, pp. 3–33. Marslen-Wilson,W.D. and Zhou,X.( 1999). Abstractness, allomorphy, and lexical architecture. Language and Cognitive Processes, 14, 321–352. Milin, P., Kuperman, V., Kosti´, A. and Harald R., H. (2009). Paradigms bit by bit: an information- theoretic approach to the processing of paradigmatic structure in inflection and derivation, Analogy in grammar: Form and acquisition, pp: 214— 252. Pandharipande, R. (1993). ―Serial verb construction in Marathi.‖ In M. K. Verma ed. (1993). Paul, S. (2004). An HPSG Account of Bangla Compound Verbs with LKB Implementation, Ph.D. Dissertation. CALT, University of Hyderabad. Pulvermüller, F. (2002). The Neuroscience guage. Cambridge University Press. of Lan- Stolz, J.A., and Feldman, L.B. (1995). The role of orthographic and semantic transparency of the base morpheme in morphological processing. In L.B. Feldman (Ed.) Morphological aspects of language processing. Hillsdale, NJ: Lawrence Erlbaum Associates Inc. Taft, M., and Forster, K.I.(1975). Lexical storage and retrieval of prefix words. Journal of Verbal Learning and Verbal Behavior, Vol.14, pp. 638-647. Taft, M.(1988). A morphological decomposition model of lexical access. Linguistics, 26, pp. 657667. Taft, M. (2004). Morphological decomposition and the reverse base frequency effect. Quarterly Journal of Experimental Psychology, 57A, pp. 745-765 Tulving, E., Schacter D. L., and Heather A.(1982). Priming Effects in Word Fragment Completion are independent of Recognition Memory. Journal of Experimental Psychology: Learning, Memory and Cognition, vol.8 (4). 129</p><p>5 0.59388155 <a title="87-lsi-5" href="./acl-2013-Measuring_semantic_content_in_distributional_vectors.html">238 acl-2013-Measuring semantic content in distributional vectors</a></p>
<p>Author: Aurelie Herbelot ; Mohan Ganesalingam</p><p>Abstract: Some words are more contentful than others: for instance, make is intuitively more general than produce and fifteen is more ‘precise’ than a group. In this paper, we propose to measure the ‘semantic content’ of lexical items, as modelled by distributional representations. We investigate the hypothesis that semantic content can be computed using the KullbackLeibler (KL) divergence, an informationtheoretic measure of the relative entropy of two distributions. In a task focusing on retrieving the correct ordering of hyponym-hypernym pairs, the KL diver- gence achieves close to 80% precision but does not outperform a simpler (linguistically unmotivated) frequency measure. We suggest that this result illustrates the rather ‘intensional’ aspect of distributions.</p><p>6 0.57440698 <a title="87-lsi-6" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>7 0.50987017 <a title="87-lsi-7" href="./acl-2013-Categorization_of_Turkish_News_Documents_with_Morphological_Analysis.html">78 acl-2013-Categorization of Turkish News Documents with Morphological Analysis</a></p>
<p>8 0.50857961 <a title="87-lsi-8" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>9 0.50839365 <a title="87-lsi-9" href="./acl-2013-Derivational_Smoothing_for_Syntactic_Distributional_Semantics.html">113 acl-2013-Derivational Smoothing for Syntactic Distributional Semantics</a></p>
<p>10 0.49456081 <a title="87-lsi-10" href="./acl-2013-Building_and_Evaluating_a_Distributional_Memory_for_Croatian.html">76 acl-2013-Building and Evaluating a Distributional Memory for Croatian</a></p>
<p>11 0.48749319 <a title="87-lsi-11" href="./acl-2013-Stem_Translation_with_Affix-Based_Rule_Selection_for_Agglutinative_Languages.html">330 acl-2013-Stem Translation with Affix-Based Rule Selection for Agglutinative Languages</a></p>
<p>12 0.48434716 <a title="87-lsi-12" href="./acl-2013-DErivBase%3A_Inducing_and_Evaluating_a_Derivational_Morphology_Resource_for_German.html">102 acl-2013-DErivBase: Inducing and Evaluating a Derivational Morphology Resource for German</a></p>
<p>13 0.46141189 <a title="87-lsi-13" href="./acl-2013-Robust_multilingual_statistical_morphological_generation_models.html">303 acl-2013-Robust multilingual statistical morphological generation models</a></p>
<p>14 0.45938429 <a title="87-lsi-14" href="./acl-2013-A_corpus-based_evaluation_method_for_Distributional_Semantic_Models.html">31 acl-2013-A corpus-based evaluation method for Distributional Semantic Models</a></p>
<p>15 0.43134487 <a title="87-lsi-15" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>16 0.41929111 <a title="87-lsi-16" href="./acl-2013-Learning_to_lemmatise_Polish_noun_phrases.html">227 acl-2013-Learning to lemmatise Polish noun phrases</a></p>
<p>17 0.41387615 <a title="87-lsi-17" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>18 0.412292 <a title="87-lsi-18" href="./acl-2013-Using_subcategorization_knowledge_to_improve_case_prediction_for_translation_to_German.html">378 acl-2013-Using subcategorization knowledge to improve case prediction for translation to German</a></p>
<p>19 0.39375484 <a title="87-lsi-19" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>20 0.37317339 <a title="87-lsi-20" href="./acl-2013-Robust_Automated_Natural_Language_Processing_with_Multiword_Expressions_and_Collocations.html">302 acl-2013-Robust Automated Natural Language Processing with Multiword Expressions and Collocations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.04), (6, 0.017), (11, 0.059), (15, 0.011), (24, 0.04), (26, 0.041), (35, 0.154), (42, 0.026), (45, 0.017), (48, 0.384), (70, 0.034), (88, 0.023), (90, 0.018), (95, 0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97336173 <a title="87-lda-1" href="./acl-2013-Are_School-of-thought_Words_Characterizable%3F.html">54 acl-2013-Are School-of-thought Words Characterizable?</a></p>
<p>Author: Xiaorui Jiang ; Xiaoping Sun ; Hai Zhuge</p><p>Abstract: School of thought analysis is an important yet not-well-elaborated scientific knowledge discovery task. This paper makes the first attempt at this problem. We focus on one aspect of the problem: do characteristic school-of-thought words exist and whether they are characterizable? To answer these questions, we propose a probabilistic generative School-Of-Thought (SOT) model to simulate the scientific authoring process based on several assumptions. SOT defines a school of thought as a distribution of topics and assumes that authors determine the school of thought for each sentence before choosing words to deliver scientific ideas. SOT distinguishes between two types of school-ofthought words for either the general background of a school of thought or the original ideas each paper contributes to its school of thought. Narrative and quantitative experiments show positive and promising results to the questions raised above. 1</p><p>2 0.97299081 <a title="87-lda-2" href="./acl-2013-Addressing_Ambiguity_in_Unsupervised_Part-of-Speech_Induction_with_Substitute_Vectors.html">39 acl-2013-Addressing Ambiguity in Unsupervised Part-of-Speech Induction with Substitute Vectors</a></p>
<p>Author: Volkan Cirik</p><p>Abstract: We study substitute vectors to solve the part-of-speech ambiguity problem in an unsupervised setting. Part-of-speech tagging is a crucial preliminary process in many natural language processing applications. Because many words in natural languages have more than one part-of-speech tag, resolving part-of-speech ambiguity is an important task. We claim that partof-speech ambiguity can be solved using substitute vectors. A substitute vector is constructed with possible substitutes of a target word. This study is built on previous work which has proven that word substitutes are very fruitful for part-ofspeech induction. Experiments show that our methodology works for words with high ambiguity.</p><p>3 0.96703303 <a title="87-lda-3" href="./acl-2013-Supervised_Model_Learning_with_Feature_Grouping_based_on_a_Discrete_Constraint.html">334 acl-2013-Supervised Model Learning with Feature Grouping based on a Discrete Constraint</a></p>
<p>Author: Jun Suzuki ; Masaaki Nagata</p><p>Abstract: This paper proposes a framework of supervised model learning that realizes feature grouping to obtain lower complexity models. The main idea of our method is to integrate a discrete constraint into model learning with the help of the dual decomposition technique. Experiments on two well-studied NLP tasks, dependency parsing and NER, demonstrate that our method can provide state-of-the-art performance even if the degrees of freedom in trained models are surprisingly small, i.e., 8 or even 2. This significant benefit enables us to provide compact model representation, which is especially useful in actual use.</p><p>same-paper 4 0.96036595 <a title="87-lda-4" href="./acl-2013-Compositional-ly_Derived_Representations_of_Morphologically_Complex_Words_in_Distributional_Semantics.html">87 acl-2013-Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics</a></p>
<p>Author: Angeliki Lazaridou ; Marco Marelli ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: Speakers of a language can construct an unlimited number of new words through morphological derivation. This is a major cause of data sparseness for corpus-based approaches to lexical semantics, such as distributional semantic models of word meaning. We adapt compositional methods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts. Semantic representations constructed in this way beat a strong baseline and can be of higher quality than representations directly constructed from corpus data. Our results constitute a novel evaluation of the proposed composition methods, in which the full additive model achieves the best performance, and demonstrate the usefulness of a compositional morphology component in distributional semantics.</p><p>5 0.94771391 <a title="87-lda-5" href="./acl-2013-SPred%3A_Large-scale_Harvesting_of_Semantic_Predicates.html">306 acl-2013-SPred: Large-scale Harvesting of Semantic Predicates</a></p>
<p>Author: Tiziano Flati ; Roberto Navigli</p><p>Abstract: We present SPred, a novel method for the creation of large repositories of semantic predicates. We start from existing collocations to form lexical predicates (e.g., break ∗) and learn the semantic classes that best f∗it) tahned ∗ argument. Taon idco this, we extract failtl thhee ∗ occurrences ion Wikipedia ewxthraiccht match the predicate and abstract its arguments to general semantic classes (e.g., break BODY PART, break AGREEMENT, etc.). Our experiments show that we are able to create a large collection of semantic predicates from the Oxford Advanced Learner’s Dictionary with high precision and recall, and perform well against the most similar approach.</p><p>6 0.90901875 <a title="87-lda-6" href="./acl-2013-Training_Nondeficient_Variants_of_IBM-3_and_IBM-4_for_Word_Alignment.html">354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</a></p>
<p>7 0.7792269 <a title="87-lda-7" href="./acl-2013-DISSECT_-_DIStributional_SEmantics_Composition_Toolkit.html">103 acl-2013-DISSECT - DIStributional SEmantics Composition Toolkit</a></p>
<p>8 0.74503678 <a title="87-lda-8" href="./acl-2013-Identifying_Sentiment_Words_Using_an_Optimization-based_Model_without_Seed_Words.html">188 acl-2013-Identifying Sentiment Words Using an Optimization-based Model without Seed Words</a></p>
<p>9 0.72437841 <a title="87-lda-9" href="./acl-2013-Margin-based_Decomposed_Amortized_Inference.html">237 acl-2013-Margin-based Decomposed Amortized Inference</a></p>
<p>10 0.71532166 <a title="87-lda-10" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>11 0.71522444 <a title="87-lda-11" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>12 0.71064878 <a title="87-lda-12" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>13 0.70793104 <a title="87-lda-13" href="./acl-2013-Re-embedding_words.html">294 acl-2013-Re-embedding words</a></p>
<p>14 0.69484615 <a title="87-lda-14" href="./acl-2013-Grounded_Language_Learning_from_Video_Described_with_Sentences.html">175 acl-2013-Grounded Language Learning from Video Described with Sentences</a></p>
<p>15 0.69374359 <a title="87-lda-15" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>16 0.69283205 <a title="87-lda-16" href="./acl-2013-Measuring_semantic_content_in_distributional_vectors.html">238 acl-2013-Measuring semantic content in distributional vectors</a></p>
<p>17 0.69152665 <a title="87-lda-17" href="./acl-2013-A_Lattice-based_Framework_for_Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">7 acl-2013-A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>18 0.69130486 <a title="87-lda-18" href="./acl-2013-Nonconvex_Global_Optimization_for_Latent-Variable_Models.html">260 acl-2013-Nonconvex Global Optimization for Latent-Variable Models</a></p>
<p>19 0.68990874 <a title="87-lda-19" href="./acl-2013-Connotation_Lexicon%3A_A_Dash_of_Sentiment_Beneath_the_Surface_Meaning.html">91 acl-2013-Connotation Lexicon: A Dash of Sentiment Beneath the Surface Meaning</a></p>
<p>20 0.68478262 <a title="87-lda-20" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
