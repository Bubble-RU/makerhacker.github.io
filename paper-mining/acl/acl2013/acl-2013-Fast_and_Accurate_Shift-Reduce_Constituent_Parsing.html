<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-155" href="#">acl2013-155</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</h1>
<br/><p>Source: <a title="acl-2013-155-pdf" href="http://aclweb.org/anthology//P/P13/P13-1043.pdf">pdf</a></p><p>Author: Muhua Zhu ; Yue Zhang ; Wenliang Chen ; Min Zhang ; Jingbo Zhu</p><p>Abstract: Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser.</p><p>Reference: <a title="acl-2013-155-reference" href="../acl2013_reference/acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn  Abstract Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. [sent-12, score-0.875]
</p><p>2 One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. [sent-13, score-0.324]
</p><p>3 Our parser gives comparable accuracies  to the state-of-the-art chart parsers. [sent-16, score-0.454]
</p><p>4 With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser. [sent-17, score-0.289]
</p><p>5 1 Introduction Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. [sent-18, score-0.715]
</p><p>6 Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al. [sent-20, score-0.32]
</p><p>7 With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al. [sent-22, score-0.253]
</p><p>8 Various methods have been proposed to address the disadvantages of greedy local parsing, among which a framework of beam-search and global  discriminative training have been shown effective for dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010). [sent-24, score-0.36]
</p><p>9 While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases (Lafferty et al. [sent-25, score-0.361]
</p><p>10 With the use of rich non-local features, transition-based dependency parsers achieve state-of-the-art accuracies that are comparable to the best-graph-based parsers (Zhang and Nivre, 2011; Bohnet and Nivre, 2012). [sent-28, score-0.679]
</p><p>11 In addition, processing tens of sentences per second (Zhang and Nivre, 2011), these transition-based parsers can be a favorable choice for dependency parsing. [sent-29, score-0.32]
</p><p>12 The best reported accuracies of transition-based constituent parsers still lag behind the state-of-the-art (Sagae and Lavie, 2006; Zhang and Clark, 2009). [sent-32, score-0.474]
</p><p>13 One difference between phrasestructure parsing and dependency parsing is that for the former, parse trees with different numbers of unary rules require different numbers of actions to build. [sent-33, score-0.863]
</p><p>14 For the same sentence, the largest output can take twice as many as actions to build as the 434  Proce dingSsof oifa, th Beu 5l1gsarti Aan,An u aglu Mste 4e-ti9n2g 0 o1f3 t. [sent-35, score-0.185]
</p><p>15 Our method is conceptually simple, requiring only one additional transition action to eliminate size differences between different candidate outputs. [sent-40, score-0.236]
</p><p>16 On standard evaluations using both the Penn Tree-  bank and the Penn Chinese Treebank, our parser gave higher accuracies than the Berkeley parser (Petrov and Klein, 2007), a state-of-the-art chart parser. [sent-41, score-0.684]
</p><p>17 In addition, our parser runs with over 89 sentences per second, which is 14 times faster than the Berkeley parser, and is the fastest that we are aware of for phrase-structure parsing. [sent-42, score-0.23]
</p><p>18 2  Baseline parser  We adopt the parser of Zhang and Clark (2009) for our baseline, which is based on the shift-reduce process of Sagae and Lavie (2005), and employs global perceptron training and beam search. [sent-54, score-0.518]
</p><p>19 At each step, a transition action is applied to consume an input word or construct a new phrase-structure. [sent-57, score-0.236]
</p><p>20 The set of transition actions are •  SHIFT: pop the front word from the buffer, aSHndI push iot pon tthoe t fhreo nstta wcko. [sent-59, score-0.384]
</p><p>21 •  •  •  REDUCE-L/R-X: pop the top two consRtEitDueUntCs Eo-fLf/ tRh-eX stack, c tohmebin toep pth tewmo oin ctoo a new constituent with label X, and push the new constituent onto the stack. [sent-62, score-0.333]
</p><p>22 UNARY-X: pop the top constituent off the stack, Y ra-Xis:e pito pto a new c coonnssttiitutueenntt tw oiftfh tlhaebel X, and push the new constituent onto the stack. [sent-63, score-0.333]
</p><p>23 2  Global Discriminative Training and Beam-Search  For a given input sentence, the initial state has an empty stack and a buffer that contains all the input words. [sent-68, score-0.253]
</p><p>24 An agenda is used to keep the k best state items at each step. [sent-69, score-0.284]
</p><p>25 At each step, every state item in the agenda is popped and expanded by applying a valid transition action, and the top k from the newly constructed state items are put back onto the agenda. [sent-71, score-0.662]
</p><p>26 The score of a state item is the total score of the transition actions that have been applied to build the item:  C(α) =iXN=1Φ(ai) ·θ~  Here Φ(ai) represents the feature vector for the ith action ai in state item α. [sent-75, score-1.032]
</p><p>27 The model parameter with the averaged perceptron algorithm, applied to state items (sequence of actions) globally. [sent-78, score-0.246]
</p><p>28 We apply the early update strategy (Collins and Roark, 2004), stopping parsing for parameter updates when the goldstandard state item falls off the agenda. [sent-79, score-0.436]
</p><p>29 3 Baseline Features Our baseline features are adopted from Zhang and Clark (2009), and are shown in Table 1 Here si represents the ith item on the top of the stack S and qi denotes the ith item in the front end of the queue Q. [sent-81, score-0.792]
</p><p>30 The symbol w denotes the lexical head of an item; the symbol c denotes the constituent label of an item; the symbol t is the POS of a lexical head. [sent-82, score-0.318]
</p><p>31 We remove Chinese specific features and make the baseline parser languageindependent. [sent-84, score-0.331]
</p><p>32 3  Improved hypotheses comparison  Unlike dependency parsing, constituent parse trees for the same sentence can have different numbers of nodes, mainly due to the existence of unary nodes. [sent-85, score-0.429]
</p><p>33 As a result, completed state  NP VBVPNP  NN  NNS  address  issues  address  NNS issues  Figure 2: Example parse trees of the same sentence with different numbers of actions. [sent-86, score-0.275]
</p><p>34 items for the same sentence can have different numbers of unary actions. [sent-87, score-0.226]
</p><p>35 The first parse corresponds to the action sequence [SHIFT, SHIFT, REDUCE-R-NP, FINISH], while the second parse corresponds to the action se-  quence [SHIFT, SHIFT, UNARY-NP, REDUCE-LVP, FINISH], which consists of one more action than the first case. [sent-89, score-0.42]
</p><p>36 In practice, variances between state items can be much larger than the chosen example. [sent-90, score-0.188]
</p><p>37 In the extreme case where a state item does not contain any unary action, the number of actions is 2n, where n is the number of words in the sentence. [sent-91, score-0.55]
</p><p>38 On the other hand, if the maximum number of consequent unary actions is 2 (Sagae and Lavie, 2005; Zhang and Clark, 2009), then the maximum number of actions a state item can have is 4n. [sent-92, score-0.735]
</p><p>39 The significant variance in the number of actions N can have an impact on the linear separability of state items, for which the feature vectors are Φ (ai). [sent-93, score-0.286]
</p><p>40 One way of improving the comparability of state items is to reduce the differences in their sizes, and we use a padding method to achieve this. [sent-95, score-0.533]
</p><p>41 The idea is to extend the set of actions by  PiN=1  adding an IDLE action, so that completed state items can be further expanded using the IDLE action. [sent-96, score-0.442]
</p><p>42 The action does not change the state itself, but simply adds to the number of actions in the sequence. [sent-97, score-0.426]
</p><p>43 A feature vector is extracted for the IDLE action according to the final state context, in the same way as other actions. [sent-98, score-0.241]
</p><p>44 Note that there can be more than one action that are padded to a sequence of actions, and the number of IDLE actions depends on the size difference between the current action sequence and the largest action sequence without IDLE actions. [sent-102, score-0.605]
</p><p>45 The initial item (Axioms) has k = 0, while the goal item has 2n ≤ k ≤ 4n. [sent-105, score-0.362]
</p><p>46 While they used a candidate output to record the best completed state item, and finish decoding when the agenda contains no more items, we can simply finish decoding when all items in the agenda  are completed, and output the best state item in the agenda. [sent-108, score-1.009]
</p><p>47 With this new transition process, we experimented with several extended features,and found that the templates in Table 2 are useful to improve the accuracies further. [sent-109, score-0.389]
</p><p>48 4  Semi-supervised Parsing with Large Data  This section discusses how to extract information from unlabeled data or auto-parsed data to further improve shift-reduce parsing accuracies. [sent-112, score-0.19]
</p><p>49 1 Paradigmatic Relations: Word Clustering Word clusters are regarded as lexical intermediaries for dependency parsing (Koo et al. [sent-118, score-0.28]
</p><p>50 The idea of exploiting lexical dependency information from auto-parsed data has been explored before for dependency parsing (Chen et al. [sent-127, score-0.406]
</p><p>51 To extract lexical dependencies, we first run the baseline parser on unlabeled data. [sent-130, score-0.327]
</p><p>52 To simplify the extraction process, we can convert auto-parsed constituency trees into dependency trees by using Penn2Malt. [sent-131, score-0.26]
</p><p>53 2 From the dependency trees, we extract bigram lexical dependencies hw1 , w2 , L/Ri wtrahcetre b tihgrea symbol a Ll (R) means tiehast w1 (w2) Lis/ Rthei head of w2 (w1). [sent-132, score-0.218]
</p><p>54 Specifically, top-10% most frequent items are assigned to the category of High Frequency (HF); otherwise if an item is among top 20%, we assign it to the category of Middle Frequency (MF); otherwise the category of Low Frequency (LF). [sent-140, score-0.268]
</p><p>55 Hereafter, we refer to the bigram and trigram lexical dependency lists as BLD and TLD, respectively. [sent-141, score-0.166]
</p><p>56 (2008) and is used as additional information for graph-based dependency parsing in Chen et al. [sent-144, score-0.28]
</p><p>57 For each xh ∈ H(y), we have a corresponding dependency stru∈ctHu re(y Dh =  ××  (xLk, . [sent-147, score-0.227]
</p><p>58 Again, we convert constituency trees into dependency trees for the purpose of simplicity. [sent-162, score-0.26]
</p><p>59 The following are the templates of the records of the dependency language models. [sent-164, score-0.174]
</p><p>60 Here the symbol si denotes a stack item, qi denotes a queue item, w  represents a word, and t represents a POS tag. [sent-206, score-0.315]
</p><p>61 140g365lishand  Chinese development sets with the padding technique and new supervised features added incrementally. [sent-211, score-0.385]
</p><p>62 We used EVALB to evaluate parser performances, including labeled precision (LP), labeled recall (LR), and bracketing F1. [sent-217, score-0.23]
</p><p>63 2 Results on Development Sets Table 5 reports the results of the extended parser (baseline + padding + supervised features) on the English and Chinese development sets. [sent-238, score-0.655]
</p><p>64 We integrated the padding method into the baseline parser, based on which we further incorporated the supervised features in Table 2. [sent-239, score-0.446]
</p><p>65 From the results we find that the padding method improves the parser accuracies by 0. [sent-240, score-0.74]
</p><p>66 3 Table 7: Comparison  of our parsers and related  work on the English test set. [sent-298, score-0.194]
</p><p>67 26 Table 8: Comparison of our parsers and related work on the test set of CTB5. [sent-326, score-0.194]
</p><p>68 ∗ Huang (2009) adapted the parsers to Chinese parsing on CTB5. [sent-328, score-0.348]
</p><p>69 We grouped the parsers into three categories: single parsers (SI), discriminative reranking parsers (RE), and semisupervised parsers (SE). [sent-337, score-0.908]
</p><p>70 From the results we can see that our extended parser (baseline + padding + supervised features)  outperforms the Berkeley parser by 0. [sent-339, score-0.885]
</p><p>71 3% on English, and is comparable with the Berkeley parser on Chinese (−0. [sent-340, score-0.23]
</p><p>72 ∗ The results of SVM-based shiftreduce parsing with greedy search. [sent-356, score-0.275]
</p><p>73 The results of MaxEnt-based shift-reduce parser with best-first search. [sent-357, score-0.23]
</p><p>74 The padding technique, supervised features, and semi-supervised features achieve an overall improvement of 1. [sent-364, score-0.385]
</p><p>75 4 Comparison of Running Time We also compared the running times of our parsers with the related single parsers. [sent-369, score-0.194]
</p><p>76 From the table, we can see that incorporating semisupervised features decreases parsing speed, but the semi-supervised parser still has the advantage  of efficiency over other parsers. [sent-373, score-0.463]
</p><p>77 Specifically, the semi-supervised parser is 7 times faster than the Berkeley parser. [sent-374, score-0.23]
</p><p>78 In practice, the running times of the shift-reduce parsers should be much shorter than the reported times in the table. [sent-376, score-0.194]
</p><p>79 5 Error Analysis We conducted error analysis for the three systems: the baseline parser, the extended parser with 440  Span Length  Figure 5: Comparison of parsing accuracies of the baseline, extended parser, and semi-supervised parsers on spans of different lengths. [sent-378, score-0.964]
</p><p>80 the padding technique, and the semi-supervised parser, focusing on the English test set. [sent-379, score-0.345]
</p><p>81 The analysis was performed in four dimensions: parsing accuracies on different phrase types, on constituents of different span lengths, on different sentence lengths, and on sentences with different numbers of unknown words. [sent-380, score-0.458]
</p><p>82 1 Different Phrase Types Table 10 shows the parsing accuracies of the baseline, extended parser, and semi-supervised parser on different phrase types. [sent-383, score-0.629]
</p><p>83 We also show the improvements of the semi-supervised parser over the baseline parser (the last row in the table). [sent-386, score-0.521]
</p><p>84 As the results show, the extended parser achieves improvements on most  of the phrase types with two exceptions: Preposition Prase (PP) and Quantifier Phrase (QP). [sent-387, score-0.31]
</p><p>85 Semisupervised features further improve parsing accuracies over the extended parser (QP is an exception). [sent-388, score-0.669]
</p><p>86 From the last row, we can see that improvements of the semi-supervised parser over the baseline on VP, S, SBAR, ADVP, and ADJP are above the average improvement (1. [sent-389, score-0.291]
</p><p>87 2 Different Span Lengths Figure 5 shows a comparison of the three parsers on spans of different lengths. [sent-393, score-0.194]
</p><p>88 As the results show, both the padding extension and semi-supervised features are more helpful on relatively large spans: the performance gaps between the three parsers are enlarged with increasing span lengths. [sent-395, score-0.618]
</p><p>89 Sentence Length  Figure 6: Comparison of parsing accuracies of the baseline, extended parser, and semi-supervised parser on sentences of different lengths. [sent-396, score-0.629]
</p><p>90 3 Different Sentence Lengths Figure 6 shows a comparison of parsing accuracies of the three parsers on sentences of different lengths. [sent-399, score-0.513]
</p><p>91 From the results we can  see that semi-supervised features improve parsing accuracy on both short and long sentences. [sent-402, score-0.194]
</p><p>92 4 Different Numbers of Unknown Words Figure 4 shows a comparison of parsing accuracies of the baseline, extended parser, and semisupervised parser on sentences with different numbers of unknown words. [sent-407, score-0.768]
</p><p>93 As the results show, the padding method is not very helpful on sentences with large numbers of unknown words, while semi-supervised features help significantly on this aspect. [sent-408, score-0.485]
</p><p>94 The resulting supervised parser outperforms the Berkeley parser, a state-of-the-art chart parser, in both accuracies and speeds. [sent-411, score-0.454]
</p><p>95 2  Table 10: Comparison ofparsing accuracies of the baseline, extended on different phrase types. [sent-449, score-0.245]
</p><p>96 parser,  and semi-supervised  parsers  )rec-%Fos(109870 091. [sent-450, score-0.194]
</p><p>97 4 BaselineExtendedSemi-supervised  Figure 4: Comparison of parsing accuracies of the baseline, extended  parser,  and semi-supervised  parser  on sentences of different unknown words. [sent-474, score-0.673]
</p><p>98 Coarse-  to-fine n-best parsing and maxent discriminative reranking. [sent-499, score-0.194]
</p><p>99 A linear observed time statistical parser based on maximum entropy models. [sent-596, score-0.23]
</p><p>100 Transition-based parsing of the Chinese Treebank using a global discriminative model. [sent-628, score-0.194]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('padding', 0.345), ('parser', 0.23), ('parsers', 0.194), ('actions', 0.185), ('item', 0.181), ('accuracies', 0.165), ('idle', 0.162), ('bldl', 0.161), ('parsing', 0.154), ('sagae', 0.151), ('clu', 0.15), ('chinese', 0.149), ('action', 0.14), ('finish', 0.139), ('shift', 0.133), ('dh', 0.128), ('dependency', 0.126), ('constituent', 0.115), ('bldr', 0.115), ('lavie', 0.105), ('stack', 0.102), ('xh', 0.101), ('state', 0.101), ('agenda', 0.096), ('transition', 0.096), ('lengths', 0.094), ('blml', 0.092), ('tldl', 0.092), ('tldr', 0.092), ('xlk', 0.092), ('items', 0.087), ('unary', 0.083), ('zhang', 0.082), ('shiftreduce', 0.081), ('extended', 0.08), ('zhu', 0.076), ('muhua', 0.07), ('blmr', 0.069), ('completed', 0.069), ('yue', 0.063), ('deduction', 0.061), ('charniak', 0.061), ('baseline', 0.061), ('berkeley', 0.06), ('wenliang', 0.06), ('pop', 0.06), ('cs', 0.059), ('chart', 0.059), ('perceptron', 0.058), ('clark', 0.058), ('numbers', 0.056), ('axioms', 0.056), ('reranking', 0.053), ('collins', 0.052), ('si', 0.05), ('buffer', 0.05), ('chen', 0.049), ('carreras', 0.049), ('dependencies', 0.049), ('trees', 0.049), ('templates', 0.048), ('ith', 0.047), ('jingbo', 0.046), ('queue', 0.046), ('bld', 0.046), ('blm', 0.046), ('eugune', 0.046), ('provement', 0.046), ('tlml', 0.046), ('tlmr', 0.046), ('unk', 0.046), ('nivre', 0.046), ('paradigmatic', 0.045), ('unknown', 0.044), ('push', 0.043), ('petrov', 0.043), ('symbol', 0.043), ('huang', 0.041), ('penn', 0.041), ('kal', 0.041), ('singapore', 0.04), ('trigram', 0.04), ('greedy', 0.04), ('ctb', 0.04), ('iwpt', 0.04), ('features', 0.04), ('discriminative', 0.04), ('span', 0.039), ('semisupervised', 0.039), ('pos', 0.039), ('cr', 0.039), ('mcclosky', 0.038), ('cluster', 0.038), ('semi', 0.037), ('denotes', 0.037), ('constituency', 0.036), ('koo', 0.036), ('unlabeled', 0.036), ('kenji', 0.035), ('wsj', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="155-tfidf-1" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>Author: Muhua Zhu ; Yue Zhang ; Wenliang Chen ; Min Zhang ; Jingbo Zhu</p><p>Abstract: Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser.</p><p>2 0.29145125 <a title="155-tfidf-2" href="./acl-2013-Chinese_Parsing_Exploiting_Characters.html">80 acl-2013-Chinese Parsing Exploiting Characters</a></p>
<p>Author: Meishan Zhang ; Yue Zhang ; Wanxiang Che ; Ting Liu</p><p>Abstract: Characters play an important role in the Chinese language, yet computational processing of Chinese has been dominated by word-based approaches, with leaves in syntax trees being words. We investigate Chinese parsing from the character-level, extending the notion of phrase-structure trees by annotating internal structures of words. We demonstrate the importance of character-level information to Chinese processing by building a joint segmentation, part-of-speech (POS) tagging and phrase-structure parsing system that integrates character-structure features. Our joint system significantly outperforms a state-of-the-art word-based baseline on the standard CTB5 test, and gives the best published results for Chinese parsing.</p><p>3 0.26174378 <a title="155-tfidf-3" href="./acl-2013-Transition-based_Dependency_Parsing_with_Selectional_Branching.html">358 acl-2013-Transition-based Dependency Parsing with Selectional Branching</a></p>
<p>Author: Jinho D. Choi ; Andrew McCallum</p><p>Abstract: We present a novel approach, called selectional branching, which uses confidence estimates to decide when to employ a beam, providing the accuracy of beam search at speeds close to a greedy transition-based dependency parsing approach. Selectional branching is guaranteed to perform a fewer number of transitions than beam search yet performs as accurately. We also present a new transition-based dependency parsing algorithm that gives a complexity of O(n) for projective parsing and an expected linear time speed for non-projective parsing. With the standard setup, our parser shows an unlabeled attachment score of 92.96% and a parsing speed of 9 milliseconds per sentence, which is faster and more accurate than the current state-of-the-art transitionbased parser that uses beam search.</p><p>4 0.24028994 <a title="155-tfidf-4" href="./acl-2013-Efficient_Implementation_of_Beam-Search_Incremental_Parsers.html">133 acl-2013-Efficient Implementation of Beam-Search Incremental Parsers</a></p>
<p>Author: Yoav Goldberg ; Kai Zhao ; Liang Huang</p><p>Abstract: Beam search incremental parsers are accurate, but not as fast as they could be. We demonstrate that, contrary to popular belief, most current implementations of beam parsers in fact run in O(n2), rather than linear time, because each statetransition is actually implemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empiri- cally. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treeb∼a2nxk sentences, a bnads are eosrd oenrs P eofn magnitude faster on much longer sentences.</p><p>5 0.23711263 <a title="155-tfidf-5" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>Author: Yang Liu</p><p>Abstract: We introduce a shift-reduce parsing algorithm for phrase-based string-todependency translation. As the algorithm generates dependency trees for partial translations left-to-right in decoding, it allows for efficient integration of both n-gram and dependency language models. To resolve conflicts in shift-reduce parsing, we propose a maximum entropy model trained on the derivation graph of training data. As our approach combines the merits of phrase-based and string-todependency models, it achieves significant improvements over the two baselines on the NIST Chinese-English datasets.</p><p>6 0.22976971 <a title="155-tfidf-6" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>7 0.22133279 <a title="155-tfidf-7" href="./acl-2013-A_Transition-Based_Dependency_Parser_Using_a_Dynamic_Parsing_Strategy.html">26 acl-2013-A Transition-Based Dependency Parser Using a Dynamic Parsing Strategy</a></p>
<p>8 0.21983767 <a title="155-tfidf-8" href="./acl-2013-An_Empirical_Examination_of_Challenges_in_Chinese_Parsing.html">44 acl-2013-An Empirical Examination of Challenges in Chinese Parsing</a></p>
<p>9 0.21139951 <a title="155-tfidf-9" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>10 0.19429217 <a title="155-tfidf-10" href="./acl-2013-Dependency_Parser_Adaptation_with_Subtrees_from_Auto-Parsed_Target_Domain_Data.html">112 acl-2013-Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data</a></p>
<p>11 0.17873912 <a title="155-tfidf-11" href="./acl-2013-Iterative_Transformation_of_Annotation_Guidelines_for_Constituency_Parsing.html">204 acl-2013-Iterative Transformation of Annotation Guidelines for Constituency Parsing</a></p>
<p>12 0.16136831 <a title="155-tfidf-12" href="./acl-2013-A_Lattice-based_Framework_for_Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">7 acl-2013-A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>13 0.15851963 <a title="155-tfidf-13" href="./acl-2013-Joint_Inference_for_Heterogeneous_Dependency_Parsing.html">208 acl-2013-Joint Inference for Heterogeneous Dependency Parsing</a></p>
<p>14 0.15715899 <a title="155-tfidf-14" href="./acl-2013-Punctuation_Prediction_with_Transition-based_Parsing.html">288 acl-2013-Punctuation Prediction with Transition-based Parsing</a></p>
<p>15 0.15490738 <a title="155-tfidf-15" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<p>16 0.13546753 <a title="155-tfidf-16" href="./acl-2013-Discriminative_Learning_with_Natural_Annotations%3A_Word_Segmentation_as_a_Case_Study.html">123 acl-2013-Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study</a></p>
<p>17 0.1296552 <a title="155-tfidf-17" href="./acl-2013-Turning_on_the_Turbo%3A_Fast_Third-Order_Non-Projective_Turbo_Parsers.html">362 acl-2013-Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers</a></p>
<p>18 0.12642804 <a title="155-tfidf-18" href="./acl-2013-Universal_Dependency_Annotation_for_Multilingual_Parsing.html">368 acl-2013-Universal Dependency Annotation for Multilingual Parsing</a></p>
<p>19 0.11193723 <a title="155-tfidf-19" href="./acl-2013-Adapting_Discriminative_Reranking_to_Grounded_Language_Learning.html">36 acl-2013-Adapting Discriminative Reranking to Grounded Language Learning</a></p>
<p>20 0.11111302 <a title="155-tfidf-20" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.261), (1, -0.201), (2, -0.333), (3, 0.081), (4, -0.046), (5, -0.026), (6, 0.066), (7, -0.013), (8, 0.007), (9, -0.077), (10, -0.005), (11, 0.081), (12, -0.037), (13, 0.018), (14, 0.214), (15, 0.055), (16, -0.121), (17, -0.056), (18, -0.022), (19, -0.0), (20, -0.044), (21, -0.002), (22, 0.037), (23, -0.003), (24, -0.008), (25, 0.013), (26, -0.028), (27, -0.004), (28, 0.049), (29, 0.038), (30, -0.037), (31, 0.002), (32, 0.058), (33, -0.075), (34, -0.05), (35, 0.007), (36, -0.023), (37, -0.011), (38, -0.065), (39, 0.02), (40, -0.007), (41, 0.052), (42, -0.008), (43, -0.014), (44, -0.009), (45, 0.012), (46, 0.006), (47, 0.002), (48, 0.03), (49, 0.002)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96529657 <a title="155-lsi-1" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>Author: Muhua Zhu ; Yue Zhang ; Wenliang Chen ; Min Zhang ; Jingbo Zhu</p><p>Abstract: Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser.</p><p>2 0.9214763 <a title="155-lsi-2" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>Author: Ji Ma ; Jingbo Zhu ; Tong Xiao ; Nan Yang</p><p>Abstract: In this paper, we combine easy-first dependency parsing and POS tagging algorithms with beam search and structured perceptron. We propose a simple variant of “early-update” to ensure valid update in the training process. The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity. On CTB, we achieve 94.01% tagging accuracy and 86.33% unlabeled attachment score with a relatively small beam width. On PTB, we also achieve state-of-the-art performance. 1</p><p>3 0.89955038 <a title="155-lsi-3" href="./acl-2013-Transition-based_Dependency_Parsing_with_Selectional_Branching.html">358 acl-2013-Transition-based Dependency Parsing with Selectional Branching</a></p>
<p>Author: Jinho D. Choi ; Andrew McCallum</p><p>Abstract: We present a novel approach, called selectional branching, which uses confidence estimates to decide when to employ a beam, providing the accuracy of beam search at speeds close to a greedy transition-based dependency parsing approach. Selectional branching is guaranteed to perform a fewer number of transitions than beam search yet performs as accurately. We also present a new transition-based dependency parsing algorithm that gives a complexity of O(n) for projective parsing and an expected linear time speed for non-projective parsing. With the standard setup, our parser shows an unlabeled attachment score of 92.96% and a parsing speed of 9 milliseconds per sentence, which is faster and more accurate than the current state-of-the-art transitionbased parser that uses beam search.</p><p>4 0.89618909 <a title="155-lsi-4" href="./acl-2013-A_Transition-Based_Dependency_Parser_Using_a_Dynamic_Parsing_Strategy.html">26 acl-2013-A Transition-Based Dependency Parser Using a Dynamic Parsing Strategy</a></p>
<p>Author: Francesco Sartorio ; Giorgio Satta ; Joakim Nivre</p><p>Abstract: We present a novel transition-based, greedy dependency parser which implements a flexible mix of bottom-up and top-down strategies. The new strategy allows the parser to postpone difficult decisions until the relevant information becomes available. The novel parser has a ∼12% error reduction in unlabeled attach∼ment score over an arc-eager parser, with a slow-down factor of 2.8.</p><p>5 0.85904384 <a title="155-lsi-5" href="./acl-2013-Efficient_Implementation_of_Beam-Search_Incremental_Parsers.html">133 acl-2013-Efficient Implementation of Beam-Search Incremental Parsers</a></p>
<p>Author: Yoav Goldberg ; Kai Zhao ; Liang Huang</p><p>Abstract: Beam search incremental parsers are accurate, but not as fast as they could be. We demonstrate that, contrary to popular belief, most current implementations of beam parsers in fact run in O(n2), rather than linear time, because each statetransition is actually implemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empiri- cally. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treeb∼a2nxk sentences, a bnads are eosrd oenrs P eofn magnitude faster on much longer sentences.</p><p>6 0.85842538 <a title="155-lsi-6" href="./acl-2013-Punctuation_Prediction_with_Transition-based_Parsing.html">288 acl-2013-Punctuation Prediction with Transition-based Parsing</a></p>
<p>7 0.7977758 <a title="155-lsi-7" href="./acl-2013-Survey_on_parsing_three_dependency_representations_for_English.html">335 acl-2013-Survey on parsing three dependency representations for English</a></p>
<p>8 0.79690862 <a title="155-lsi-8" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>9 0.7571004 <a title="155-lsi-9" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>10 0.74207538 <a title="155-lsi-10" href="./acl-2013-Joint_Inference_for_Heterogeneous_Dependency_Parsing.html">208 acl-2013-Joint Inference for Heterogeneous Dependency Parsing</a></p>
<p>11 0.72187203 <a title="155-lsi-11" href="./acl-2013-Chinese_Parsing_Exploiting_Characters.html">80 acl-2013-Chinese Parsing Exploiting Characters</a></p>
<p>12 0.6750223 <a title="155-lsi-12" href="./acl-2013-Turning_on_the_Turbo%3A_Fast_Third-Order_Non-Projective_Turbo_Parsers.html">362 acl-2013-Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers</a></p>
<p>13 0.66359442 <a title="155-lsi-13" href="./acl-2013-An_Empirical_Examination_of_Challenges_in_Chinese_Parsing.html">44 acl-2013-An Empirical Examination of Challenges in Chinese Parsing</a></p>
<p>14 0.614645 <a title="155-lsi-14" href="./acl-2013-Stop-probability_estimates_computed_on_a_large_corpus_improve_Unsupervised_Dependency_Parsing.html">331 acl-2013-Stop-probability estimates computed on a large corpus improve Unsupervised Dependency Parsing</a></p>
<p>15 0.60795569 <a title="155-lsi-15" href="./acl-2013-A_Lattice-based_Framework_for_Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">7 acl-2013-A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>16 0.6079284 <a title="155-lsi-16" href="./acl-2013-Iterative_Transformation_of_Annotation_Guidelines_for_Constituency_Parsing.html">204 acl-2013-Iterative Transformation of Annotation Guidelines for Constituency Parsing</a></p>
<p>17 0.60441148 <a title="155-lsi-17" href="./acl-2013-Grounded_Unsupervised_Semantic_Parsing.html">176 acl-2013-Grounded Unsupervised Semantic Parsing</a></p>
<p>18 0.59931117 <a title="155-lsi-18" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>19 0.59374064 <a title="155-lsi-19" href="./acl-2013-Dependency_Parser_Adaptation_with_Subtrees_from_Auto-Parsed_Target_Domain_Data.html">112 acl-2013-Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data</a></p>
<p>20 0.59056759 <a title="155-lsi-20" href="./acl-2013-From_Natural_Language_Specifications_to_Program_Input_Parsers.html">163 acl-2013-From Natural Language Specifications to Program Input Parsers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.042), (6, 0.039), (11, 0.1), (14, 0.018), (24, 0.035), (26, 0.047), (28, 0.027), (35, 0.067), (42, 0.076), (43, 0.173), (48, 0.051), (70, 0.096), (71, 0.014), (88, 0.046), (90, 0.038), (95, 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83502311 <a title="155-lda-1" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>Author: Muhua Zhu ; Yue Zhang ; Wenliang Chen ; Min Zhang ; Jingbo Zhu</p><p>Abstract: Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser.</p><p>2 0.72127074 <a title="155-lda-2" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>Author: Ji Ma ; Jingbo Zhu ; Tong Xiao ; Nan Yang</p><p>Abstract: In this paper, we combine easy-first dependency parsing and POS tagging algorithms with beam search and structured perceptron. We propose a simple variant of “early-update” to ensure valid update in the training process. The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity. On CTB, we achieve 94.01% tagging accuracy and 86.33% unlabeled attachment score with a relatively small beam width. On PTB, we also achieve state-of-the-art performance. 1</p><p>3 0.7212103 <a title="155-lda-3" href="./acl-2013-Chinese_Parsing_Exploiting_Characters.html">80 acl-2013-Chinese Parsing Exploiting Characters</a></p>
<p>Author: Meishan Zhang ; Yue Zhang ; Wanxiang Che ; Ting Liu</p><p>Abstract: Characters play an important role in the Chinese language, yet computational processing of Chinese has been dominated by word-based approaches, with leaves in syntax trees being words. We investigate Chinese parsing from the character-level, extending the notion of phrase-structure trees by annotating internal structures of words. We demonstrate the importance of character-level information to Chinese processing by building a joint segmentation, part-of-speech (POS) tagging and phrase-structure parsing system that integrates character-structure features. Our joint system significantly outperforms a state-of-the-art word-based baseline on the standard CTB5 test, and gives the best published results for Chinese parsing.</p><p>4 0.71345949 <a title="155-lda-4" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>Author: Peifeng Li ; Qiaoming Zhu ; Guodong Zhou</p><p>Abstract: As a paratactic language, sentence-level argument extraction in Chinese suffers much from the frequent occurrence of ellipsis with regard to inter-sentence arguments. To resolve such problem, this paper proposes a novel global argument inference model to explore specific relationships, such as Coreference, Sequence and Parallel, among relevant event mentions to recover those intersentence arguments in the sentence, discourse and document layers which represent the cohesion of an event or a topic. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our global argument inference model over a state-of-the-art baseline. 1</p><p>5 0.709966 <a title="155-lda-5" href="./acl-2013-Parsing_Graphs_with_Hyperedge_Replacement_Grammars.html">274 acl-2013-Parsing Graphs with Hyperedge Replacement Grammars</a></p>
<p>Author: David Chiang ; Jacob Andreas ; Daniel Bauer ; Karl Moritz Hermann ; Bevan Jones ; Kevin Knight</p><p>Abstract: Hyperedge replacement grammar (HRG) is a formalism for generating and transforming graphs that has potential applications in natural language understanding and generation. A recognition algorithm due to Lautemann is known to be polynomial-time for graphs that are connected and of bounded degree. We present a more precise characterization of the algorithm’s complexity, an optimization analogous to binarization of contextfree grammars, and some important implementation details, resulting in an algorithm that is practical for natural-language applications. The algorithm is part of Bolinas, a new software toolkit for HRG processing.</p><p>6 0.7077384 <a title="155-lda-6" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>7 0.70683521 <a title="155-lda-7" href="./acl-2013-Transition-based_Dependency_Parsing_with_Selectional_Branching.html">358 acl-2013-Transition-based Dependency Parsing with Selectional Branching</a></p>
<p>8 0.70652676 <a title="155-lda-8" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<p>9 0.70351261 <a title="155-lda-9" href="./acl-2013-Efficient_Implementation_of_Beam-Search_Incremental_Parsers.html">133 acl-2013-Efficient Implementation of Beam-Search Incremental Parsers</a></p>
<p>10 0.70063972 <a title="155-lda-10" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>11 0.69924009 <a title="155-lda-11" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<p>12 0.69644165 <a title="155-lda-12" href="./acl-2013-Arguments_and_Modifiers_from_the_Learner%27s_Perspective.html">57 acl-2013-Arguments and Modifiers from the Learner's Perspective</a></p>
<p>13 0.69526309 <a title="155-lda-13" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>14 0.69418192 <a title="155-lda-14" href="./acl-2013-Statistical_Machine_Translation_Improves_Question_Retrieval_in_Community_Question_Answering_via_Matrix_Factorization.html">329 acl-2013-Statistical Machine Translation Improves Question Retrieval in Community Question Answering via Matrix Factorization</a></p>
<p>15 0.6934787 <a title="155-lda-15" href="./acl-2013-Language-Independent_Discriminative_Parsing_of_Temporal_Expressions.html">212 acl-2013-Language-Independent Discriminative Parsing of Temporal Expressions</a></p>
<p>16 0.69145608 <a title="155-lda-16" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>17 0.69031954 <a title="155-lda-17" href="./acl-2013-Discriminative_Learning_with_Natural_Annotations%3A_Word_Segmentation_as_a_Case_Study.html">123 acl-2013-Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study</a></p>
<p>18 0.68876845 <a title="155-lda-18" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>19 0.68791509 <a title="155-lda-19" href="./acl-2013-Temporal_Signals_Help_Label_Temporal_Relations.html">339 acl-2013-Temporal Signals Help Label Temporal Relations</a></p>
<p>20 0.68790656 <a title="155-lda-20" href="./acl-2013-Generalizing_Image_Captions_for_Image-Text_Parallel_Corpus.html">167 acl-2013-Generalizing Image Captions for Image-Text Parallel Corpus</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
