<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>195 acl-2013-Improving machine translation by training against an automatic semantic frame based evaluation metric</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-195" href="#">acl2013-195</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>195 acl-2013-Improving machine translation by training against an automatic semantic frame based evaluation metric</h1>
<br/><p>Source: <a title="acl-2013-195-pdf" href="http://aclweb.org/anthology//P/P13/P13-2067.pdf">pdf</a></p><p>Author: Chi-kiu Lo ; Karteek Addanki ; Markus Saers ; Dekai Wu</p><p>Abstract: We present the first ever results showing that tuning a machine translation system against a semantic frame based objective function, MEANT, produces more robustly adequate translations than tuning against BLEU or TER as measured across commonly used metrics and human subjective evaluation. Moreover, for informal web forum data, human evaluators preferred MEANT-tuned systems over BLEU- or TER-tuned systems by a significantly wider margin than that for formal newswire—even though automatic semantic parsing might be expected to fare worse on informal language. We argue thatbypreserving the meaning ofthe trans- lations as captured by semantic frames right in the training process, an MT system is constrained to make more accurate choices of both lexical and reordering rules. As a result, MT systems tuned against semantic frame based MT evaluation metrics produce output that is more adequate. Tuning a machine translation system against a semantic frame based objective function is independent ofthe translation model paradigm, so, any translation model can benefit from the semantic knowledge incorporated to improve translation adequacy through our approach.</p><p>Reference: <a title="acl-2013-195-reference" href="../acl2013_reference/acl-2013-Improving_machine_translation_by_training_against_an_automatic_semantic_frame_based_evaluation_metric_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Moreover, for informal web forum data, human evaluators preferred MEANT-tuned systems over BLEU- or TER-tuned systems by a significantly wider margin than that for formal newswire—even though automatic semantic parsing might be expected to fare worse on informal language. [sent-4, score-0.782]
</p><p>2 We argue thatbypreserving the meaning ofthe trans-  lations as captured by semantic frames right in the training process, an MT system is constrained to make more accurate choices of both lexical and reordering rules. [sent-5, score-0.286]
</p><p>3 As a result, MT systems tuned against semantic frame based MT evaluation metrics produce output that is more adequate. [sent-6, score-0.574]
</p><p>4 Tuning a machine translation system against a semantic frame based objective function is independent ofthe translation model paradigm, so, any translation model can benefit from the semantic knowledge incorporated to improve translation adequacy through our approach. [sent-7, score-1.165]
</p><p>5 1 Introduction We present the first ever results of tuning a statistical machine translation (SMT) system against a semantic frame based objective function in order to produce a more adequate output. [sent-8, score-1.108]
</p><p>6 We compare the performance of our system with that of two baseline SMT systems tuned against BLEU and TER, the commonly used n-gram and edit distance  based metrics. [sent-9, score-0.253]
</p><p>7 Our system performs better than the baseline across seven commonly used evaluation metrics and subjective human evaluation on adequacy. [sent-10, score-0.417]
</p><p>8 Surprisingly, tuning against a semantic MT evaluation metric also significantly outperforms the baseline on the domain of informal web forum data wherein automatic semantic parsing might be expected to fare worse. [sent-11, score-1.001]
</p><p>9 These results strongly indicate that using a semantic frame based objective function for tuning would drive development of MT towards direction of higher utility. [sent-12, score-0.691]
</p><p>10 Glaring errors caused by semantic role confusion that plague the state-of-the-art MT systems are a consequence of using fast and cheap lexical n-gram based objective functions like BLEU to drive their development. [sent-13, score-0.266]
</p><p>11 Despite enforcing fluency it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning closely (Callison-Burch et al. [sent-14, score-0.395]
</p><p>12 We argue that instead of BLEU, a metric that focuses on getting the meaning right should be used as an objective function for tuning SMT so as to  drive continuing progress towards higher utility. [sent-16, score-0.613]
</p><p>13 , 2012), is an automatic semantic MT evaluation metric that measures similarity between the MT output and the reference translation via semantic frames. [sent-18, score-0.599]
</p><p>14 It correlates better with human adequacy judgment than other automatic MT evaluation metrics. [sent-19, score-0.26]
</p><p>15 Since a high MEANT score is contingent on correct lexical choices as well as syntactic and semantic structures, we believe that tuning against MEANT would improve both translation adequacy and fluency. [sent-20, score-0.736]
</p><p>16 Incorporating semantic structures into SMT by tuning against a semantic frame based evaluation metric is independent of the MT paradigm. [sent-21, score-0.736]
</p><p>17 Therefore, systems from different MT paradigms (such as hierarchical, phrase based, transduction grammar based) can benefit from the semantic information incorporated through our approach. [sent-22, score-0.102]
</p><p>18 This is because the development ofSMT systems was predominantly driven by tuning against n-gram based evaluation metrics such as BLEU or edit distance based metrics such as TER which do not sufficiently bias SMT system’s decisions to produce adequate translations. [sent-27, score-1.068]
</p><p>19 Although there has been a recent surge of work aimed towards incorporating semantics into the SMT pipeline, none attempt to tune against a semantic objective function. [sent-28, score-0.371]
</p><p>20 Below, we describe some of the attempts to incorporate semantic information into the SMT and present a brief survey on evaluation metrics that focus on rewarding semantically valid translations. [sent-29, score-0.284]
</p><p>21 Utilizing semantics in SMT In the past few years, there has been a surge of work aimed at incorporating semantics into various stages of the SMT. [sent-30, score-0.079]
</p><p>22 Wu and Fung (2009) propose a two-pass model that reorders the MT output to match the SRL of the input, which is too late to affect the  translation decisions made by the MT system during decoding. [sent-31, score-0.297]
</p><p>23 In contrast, training against a semantic objective function attempts to improve the decoding search strategy by incorporating a bias towards meaningful translations into the model instead of postprocessing its results. [sent-32, score-0.393]
</p><p>24 (201 1) preprocess the input sentence to match the verb frame alternations in the output side. [sent-35, score-0.177]
</p><p>25 (2012) trained a discriminative model to predict the position of the semantic roles in the output. [sent-39, score-0.102]
</p><p>26 All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. [sent-40, score-0.178]
</p><p>27 Any of the above models could potentially benefit from tuning with semantic metrics. [sent-41, score-0.355]
</p><p>28 MT evaluation metrics As mentioned previously, tuning against n-gram based metrics such as BLEU (Papineni et al. [sent-42, score-0.574]
</p><p>29 , 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) does not sufficiently drive SMT into mak-  ing decisions to produce adequate translations that correctly preserve ”who did what to whom, when, where and why ”. [sent-43, score-0.632]
</p><p>30 , 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation accuracy. [sent-45, score-0.189]
</p><p>31 Tuning against edit distance based metrics such as CDER (Leusch et al. [sent-46, score-0.215]
</p><p>32 , 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information. [sent-49, score-0.386]
</p><p>33 We argue that an SMT system tuned against an adequacy-oriented metric that correlates well with human adequacy judgement produces more adequate translations. [sent-50, score-0.775]
</p><p>34 For this purpose, we choose MEANT, an automatic semantic MT evaluation metric that focuses on getting the meaning right by comparing the semantic structures of the MT output and the reference. [sent-51, score-0.457]
</p><p>35 We briefly describe some of the alternative semantic metrics below to justify our choice. [sent-52, score-0.241]
</p><p>36 ULC (Giménez and Màrquez, 2007, 2008) is  an aggregated metric that incorporates several semantic similarity features and shows improved correlation with human judgement on translation quality (Callison-Burch et al. [sent-53, score-0.439]
</p><p>37 , 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an MT system against ULC perhaps due to its expensive running time. [sent-55, score-0.369]
</p><p>38 (2006) did tune on QUEEN, a simplified version of ULC that discards the semantic features and is based on pure lexical features. [sent-57, score-0.165]
</p><p>39 Although tuning on QUEEN produced slightly more preferable translations than solely tuning on BLEU, the metric does not make use ofany semantic features and thus fails to exploit any potential gains from tuning to semantic objectives. [sent-58, score-1.169]
</p><p>40 , 2011) is an recalloriented automatic evaluation metric which aims to preserve the basic event structure, no work has been done towards tuning an SMT system against it. [sent-60, score-0.637]
</p><p>41 TINE performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgment. [sent-61, score-0.178]
</p><p>42 , 2012), which is the weighted f-score over the matched semantic role labels of the automatically aligned semantic frames and role fillers, outperforms BLEU, NIST, METEOR, WER, CDER and TER. [sent-63, score-0.24]
</p><p>43 This makes it more suitable for tuning SMT systems to produce much adequate translations. [sent-64, score-0.566]
</p><p>44 1 E6 5A76 7N8T Table 1: Translation quality of MT system tuned against MEANT, BLEU and TER on newswire data  TabflBMoeELr Ru2Am-U:tNuTn re-antdue sldatioBn769. [sent-73, score-0.287]
</p><p>45 Toward improving translation utility of state-ofthe-art MT systems, we chose to use a strong and  competitive system in the DARPA BOLT program as our baseline. [sent-82, score-0.25]
</p><p>46 The baseline system is a Moses hierarchical model trained on a collection of LDC newswire and a small portion of Chinese-English parallel web forum data, together with a 5-gram language model. [sent-83, score-0.397]
</p><p>47 For the newswire experiment, we used a collection of NIST 02-06 test sets as our development set and NIST 08 test set for evaluation. [sent-84, score-0.145]
</p><p>48 For the forum data experiment, the development and test sets were a held-out subset of the BOLT phase 1 training data. [sent-86, score-0.187]
</p><p>49 We use ZMERT (Zaidan, 2009) to tune the baseline because it is a widely used, highly competitive, robust, and reliable implementation of MERT that is also fully configurable and extensible with regard to incorporating new evaluation metrics. [sent-88, score-0.185]
</p><p>50 In each experiment, we tune two contrastive  conventional 100-best MERT tuned baseline systems on both newswire and forum data genres; one tuned against BLEU, an n-gram based evaluation metric and the other using TER, an edit distance based metric. [sent-91, score-0.783]
</p><p>51 As semantic role labeling is expensive we only tuned using 10-best list for MEANTtuned system. [sent-92, score-0.179]
</p><p>52 5 hours and 5 hours per iteration respectively whereas tuning against MEANT took about 1. [sent-94, score-0.323]
</p><p>53 4  Results  Of course, tuning against any metric would maximize the performance of the SMT system on that particular metric, but would be overfitting. [sent-96, score-0.433]
</p><p>54 For example, something would be seriously wrong if tuning against BLEU did not yield the best BLEU scores. [sent-97, score-0.253]
</p><p>55 A far more worthwhile goal would be to bias the SMT system to produce adequate translations while achieving the best scores across all the metrics. [sent-98, score-0.505]
</p><p>56 With this as our objective, we present the results of comparing MEANT-tuned systems against the baselines as evaluated on com-  monly used automatic metrics and human adequacy judgement. [sent-99, score-0.39]
</p><p>57 In the newswire domain, however, our system achieves marginally lower TER score than BLEU-tuned system. [sent-101, score-0.21]
</p><p>58 Figure 1 shows an example where the MEANTtuned system produced a more adequate translation that accurately preserves the semantic structure of the input sentence than the two baseline systems. [sent-102, score-0.586]
</p><p>59 The MEANT scores for the MT output from the BLEU-, TER- and MEANT-tuned systems are 0. [sent-103, score-0.056]
</p><p>60 Note that the MT output of the BLEU-tuned system has no semantic parse output  by the automatic shallow semantic parser. [sent-108, score-0.457]
</p><p>61 Moreover, for the frame “攻 占” in the input sentence, the MEANT-tuned sys-  tem has correctly translated the ARG0 “哈玛斯好战 份子” into “Hamas militants” and the ARG1 “加 萨 走廊” into “Gaza”. [sent-111, score-0.188]
</p><p>62 However, the TER-tuned system has dropped the predicate “施行” so that the corresponding arguments “The Palestinian Authority” and “into a state of emergency” have all been incorrectly associated with the predicate “攻 占 /seized”. [sent-112, score-0.065]
</p><p>63 This example shows that the translation adequacy of SMT has been improved by tuning against MEANT because the MEANT-tuned system is more accurately preserving the semantic structure of the input sentence. [sent-113, score-0.693]
</p><p>64 Our results show that MEANT-tuned system maintains a balance between lexical choices and word order because it performs well on n-gram based metrics that reward lexical matching and edit distance metrics that penalize incorrect word order. [sent-114, score-0.472]
</p><p>65 This is not surprising as a high MEANT score relies on a high degree of semantic structure matching, which is contingent upon correct lexical choices as well as syntactic and semantic structures. [sent-115, score-0.312]
</p><p>66 Human subjective evaluation In line with our original objective ofbiasing SMT systems towards producing adequate translations, we conduct a human evaluation to judge the translation utility of the outputs produced by MEANT-, BLEU- and TER-tuned systems. [sent-116, score-0.767]
</p><p>67 Following the manual evaluation protocol of Lambert et al. [sent-117, score-0.073]
</p><p>68 (2006), we randomly draw 150 sentences from the test set in each domain to form the manual evaluation set. [sent-118, score-0.073]
</p><p>69 Table 3 shows the MEANT scores of the two manual evaluation sets. [sent-119, score-0.073]
</p><p>70 In both evaluation sets, like in the test sets, the output from the MEANT-tuned system score slightly higher in MEANT than that from the BLEU-tuned system and significantly higher than that from the TER-tuned system. [sent-120, score-0.229]
</p><p>71 The output of each tuned MT system along the input sentence and the reference were presented to human evaluators. [sent-121, score-0.245]
</p><p>72 Each evaluation set is ranked by two evaluators for measuring inter-evaluator agreement. [sent-122, score-0.132]
</p><p>73 Table 4 indicates that output of the MEANTtuned system is ranked adequate more frequently compared to BLEU- and TER-tuned baselines for both newswire and web forum genres. [sent-123, score-0.764]
</p><p>74 1r 4u765m36 73  Table 3: MEANT scores ofeach system in the 150sentence manual evaluation set. [sent-126, score-0.138]
</p><p>75 evaluator agreement is 84% and 70% for newswire and forum data genres respectively. [sent-129, score-0.369]
</p><p>76 We performed the right-tailed two proportion significance test on human evaluation of the SMT system outputs for both the genres. [sent-130, score-0.213]
</p><p>77 Table 5 shows that the MEANT-tuned system generates more adequate translations than the TER-tuned system at the 99% significance level for both newswire and web forum genres. [sent-131, score-0.888]
</p><p>78 The MEANT-tuned system is ranked more adequate than the BLEU-tuned system at the 95% significance level on the web forum genre and for the newswire genre the hypothesis is accepted at a significance level of 80%. [sent-132, score-0.935]
</p><p>79 The high inter-evaluator agreement and the significance tests confirm that MEANT-tuned system is better atproducing adequate translations compared to BLEU- or TER-tuned systems. [sent-133, score-0.491]
</p><p>80 formal text The results of table 4 and 5 also show that—surprisingly—the human evaluators preferred MEANT-tuned system output over BLEU-tuned and TER-tuned system output by a far wider margin on the informal forum  text compared to the formal newswire text. [sent-135, score-0.904]
</p><p>81 The MEANT-tuned system is better than both baselines at the 80% significance level for the formal text genre. [sent-136, score-0.193]
</p><p>82 For the informal text genre, it performs the two baselines at the 95% significance level. [sent-137, score-0.214]
</p><p>83 Although one might expect an semantic frame dependent metric such as MEANT to perform poorly on the domain of informal text, surprisingly, it nonetheless significantly outperforms the baselines at the task ofgenerating adequate output. [sent-138, score-0.771]
</p><p>84 This indicates that the design of the MEANT evaluation metric is robust enough to tune an SMT system towards adequate output on informal text domains despite the shortcomings of automatic shallow semantic parsing. [sent-139, score-0.97]
</p><p>85 5  Conclusion  We presented the first ever results to demonstrate that tuning an SMT system against MEANT produces much adequate translation than tuning against BLEU or TER, as measured across all other commonly used metrics and human subjective evaluation. [sent-140, score-1.292]
</p><p>86 We also observed that tuning  against MEANT succeeds in producing adequate output significantly more frequently even on the informal text such as web forum data. [sent-141, score-0.895]
</p><p>87 By preserving the meaning of the translations as captured by semantic frames right in the training process, an MT system is constrained to make more accurate choices of both lexical and reordering rules. [sent-142, score-0.347]
</p><p>88 The performance ofour system as measured across all commonly used metrics indicate that tuning against a semantic MT evaluation metric does produce output which is adequate and fluent. [sent-143, score-1.121]
</p><p>89 287658; and by the Hong Kong Research Grants Council (RGC) research grants GRF62081 1, GRF621008, and GRF612806. [sent-148, score-0.034]
</p><p>90 METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. [sent-154, score-0.244]
</p><p>91 Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. [sent-166, score-0.185]
</p><p>92 Linguistic features for automatic evaluation of heterogenous MT systems. [sent-169, score-0.082]
</p><p>93 Phrase reordering for statistical machine translation based on predicate-argument structure. [sent-178, score-0.142]
</p><p>94 BLEU: a method for automatic evaluation of machine translation. [sent-198, score-0.082]
</p><p>95 A study of translation edit rate with targeted human annotation. [sent-209, score-0.265]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('meant', 0.428), ('adequate', 0.277), ('tuning', 0.253), ('mt', 0.216), ('smt', 0.209), ('forum', 0.187), ('gim', 0.164), ('bleu', 0.157), ('newswire', 0.145), ('translation', 0.142), ('metrics', 0.139), ('adequacy', 0.131), ('informal', 0.122), ('frame', 0.121), ('nez', 0.12), ('ter', 0.119), ('metric', 0.115), ('cder', 0.109), ('semantic', 0.102), ('rquez', 0.097), ('meteor', 0.092), ('translations', 0.091), ('evaluators', 0.089), ('drive', 0.088), ('tine', 0.084), ('meanttuned', 0.082), ('rios', 0.082), ('ulc', 0.082), ('tuned', 0.077), ('objective', 0.076), ('edit', 0.076), ('lo', 0.072), ('preserve', 0.071), ('leusch', 0.067), ('jes', 0.067), ('system', 0.065), ('aziz', 0.063), ('tune', 0.063), ('monz', 0.061), ('lambert', 0.06), ('significance', 0.058), ('nist', 0.056), ('output', 0.056), ('llu', 0.055), ('contingent', 0.055), ('fordyce', 0.055), ('wer', 0.053), ('choices', 0.053), ('towards', 0.051), ('bolt', 0.05), ('human', 0.047), ('subjective', 0.045), ('komachi', 0.045), ('evaluation', 0.043), ('utility', 0.043), ('christof', 0.042), ('surge', 0.042), ('gregor', 0.042), ('configurable', 0.042), ('wilker', 0.042), ('dekai', 0.042), ('june', 0.041), ('wu', 0.04), ('josh', 0.04), ('genre', 0.04), ('automatic', 0.039), ('koehn', 0.038), ('cameron', 0.038), ('miguel', 0.038), ('fare', 0.038), ('incorporating', 0.037), ('shallow', 0.037), ('queen', 0.037), ('genres', 0.037), ('formal', 0.036), ('produce', 0.036), ('bias', 0.036), ('frames', 0.036), ('mert', 0.036), ('ever', 0.036), ('tem', 0.036), ('hours', 0.035), ('sufficiently', 0.035), ('commonly', 0.035), ('pradhan', 0.035), ('nie', 0.035), ('decisions', 0.034), ('baselines', 0.034), ('grants', 0.034), ('predicates', 0.034), ('darpa', 0.033), ('masaaki', 0.033), ('xiong', 0.033), ('judgement', 0.033), ('philipp', 0.032), ('snover', 0.031), ('translated', 0.031), ('surprisingly', 0.031), ('manual', 0.03), ('argue', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="195-tfidf-1" href="./acl-2013-Improving_machine_translation_by_training_against_an_automatic_semantic_frame_based_evaluation_metric.html">195 acl-2013-Improving machine translation by training against an automatic semantic frame based evaluation metric</a></p>
<p>Author: Chi-kiu Lo ; Karteek Addanki ; Markus Saers ; Dekai Wu</p><p>Abstract: We present the first ever results showing that tuning a machine translation system against a semantic frame based objective function, MEANT, produces more robustly adequate translations than tuning against BLEU or TER as measured across commonly used metrics and human subjective evaluation. Moreover, for informal web forum data, human evaluators preferred MEANT-tuned systems over BLEU- or TER-tuned systems by a significantly wider margin than that for formal newswire—even though automatic semantic parsing might be expected to fare worse on informal language. We argue thatbypreserving the meaning ofthe trans- lations as captured by semantic frames right in the training process, an MT system is constrained to make more accurate choices of both lexical and reordering rules. As a result, MT systems tuned against semantic frame based MT evaluation metrics produce output that is more adequate. Tuning a machine translation system against a semantic frame based objective function is independent ofthe translation model paradigm, so, any translation model can benefit from the semantic knowledge incorporated to improve translation adequacy through our approach.</p><p>2 0.17548069 <a title="195-tfidf-2" href="./acl-2013-English-to-Russian_MT_evaluation_campaign.html">135 acl-2013-English-to-Russian MT evaluation campaign</a></p>
<p>Author: Pavel Braslavski ; Alexander Beloborodov ; Maxim Khalilov ; Serge Sharoff</p><p>Abstract: This paper presents the settings and the results of the ROMIP 2013 MT shared task for the English→Russian language directfioorn. t Teh Een quality Rofu generated utraagnsel datiiroencswas assessed using automatic metrics and human evaluation. We also discuss ways to reduce human evaluation efforts using pairwise sentence comparisons by human judges to simulate sort operations.</p><p>3 0.17131391 <a title="195-tfidf-3" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<p>Author: Marzieh Bazrafshan ; Daniel Gildea</p><p>Abstract: We experiment with adding semantic role information to a string-to-tree machine translation system based on the rule extraction procedure of Galley et al. (2004). We compare methods based on augmenting the set of nonterminals by adding semantic role labels, and altering the rule extraction process to produce a separate set of rules for each predicate that encompass its entire predicate-argument structure. Our results demonstrate that the second approach is effective in increasing the quality of translations.</p><p>4 0.15695465 <a title="195-tfidf-4" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>Author: Haibo Li ; Jing Zheng ; Heng Ji ; Qi Li ; Wen Wang</p><p>Abstract: We propose a Name-aware Machine Translation (MT) approach which can tightly integrate name processing into MT model, by jointly annotating parallel corpora, extracting name-aware translation grammar and rules, adding name phrase table and name translation driven decoding. Additionally, we also propose a new MT metric to appropriately evaluate the translation quality of informative words, by assigning different weights to different words according to their importance values in a document. Experiments on Chinese-English translation demonstrated the effectiveness of our approach on enhancing the quality of overall translation, name translation and word alignment over a high-quality MT baseline1 .</p><p>5 0.15542778 <a title="195-tfidf-5" href="./acl-2013-Fast_and_Adaptive_Online_Training_of_Feature-Rich_Translation_Models.html">156 acl-2013-Fast and Adaptive Online Training of Feature-Rich Translation Models</a></p>
<p>Author: Spence Green ; Sida Wang ; Daniel Cer ; Christopher D. Manning</p><p>Abstract: We present a fast and scalable online method for tuning statistical machine translation models with large feature sets. The standard tuning algorithm—MERT—only scales to tens of features. Recent discriminative algorithms that accommodate sparse features have produced smaller than expected translation quality gains in large systems. Our method, which is based on stochastic gradient descent with an adaptive learning rate, scales to millions of features and tuning sets with tens of thousands of sentences, while still converging after only a few epochs. Large-scale experiments on Arabic-English and Chinese-English show that our method produces significant translation quality gains by exploiting sparse features. Equally important is our analysis, which suggests techniques for mitigating overfitting and domain mismatch, and applies to other recent discriminative methods for machine translation. 1</p><p>6 0.14332867 <a title="195-tfidf-6" href="./acl-2013-A_New_Syntactic_Metric_for_Evaluation_of_Machine_Translation.html">13 acl-2013-A New Syntactic Metric for Evaluation of Machine Translation</a></p>
<p>7 0.12964983 <a title="195-tfidf-7" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>8 0.12737828 <a title="195-tfidf-8" href="./acl-2013-A_Tale_about_PRO_and_Monsters.html">24 acl-2013-A Tale about PRO and Monsters</a></p>
<p>9 0.11797266 <a title="195-tfidf-9" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<p>10 0.11726974 <a title="195-tfidf-10" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>11 0.11492515 <a title="195-tfidf-11" href="./acl-2013-Task_Alternation_in_Parallel_Sentence_Retrieval_for_Twitter_Translation.html">338 acl-2013-Task Alternation in Parallel Sentence Retrieval for Twitter Translation</a></p>
<p>12 0.11361998 <a title="195-tfidf-12" href="./acl-2013-Online_Relative_Margin_Maximization_for_Statistical_Machine_Translation.html">264 acl-2013-Online Relative Margin Maximization for Statistical Machine Translation</a></p>
<p>13 0.11217473 <a title="195-tfidf-13" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>14 0.11161759 <a title="195-tfidf-14" href="./acl-2013-QuEst_-_A_translation_quality_estimation_framework.html">289 acl-2013-QuEst - A translation quality estimation framework</a></p>
<p>15 0.11114765 <a title="195-tfidf-15" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>16 0.10961609 <a title="195-tfidf-16" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>17 0.10941148 <a title="195-tfidf-17" href="./acl-2013-Outsourcing_FrameNet_to_the_Crowd.html">265 acl-2013-Outsourcing FrameNet to the Crowd</a></p>
<p>18 0.10787682 <a title="195-tfidf-18" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>19 0.10594049 <a title="195-tfidf-19" href="./acl-2013-Learning_Non-linear_Features_for_Machine_Translation_Using_Gradient_Boosting_Machines.html">221 acl-2013-Learning Non-linear Features for Machine Translation Using Gradient Boosting Machines</a></p>
<p>20 0.10564977 <a title="195-tfidf-20" href="./acl-2013-Stacking_for_Statistical_Machine_Translation.html">328 acl-2013-Stacking for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.215), (1, -0.126), (2, 0.199), (3, 0.056), (4, -0.024), (5, 0.044), (6, 0.011), (7, 0.016), (8, 0.043), (9, 0.106), (10, -0.066), (11, 0.141), (12, -0.083), (13, 0.077), (14, -0.028), (15, 0.045), (16, -0.028), (17, 0.002), (18, 0.112), (19, 0.059), (20, 0.122), (21, 0.01), (22, -0.071), (23, -0.02), (24, -0.001), (25, -0.002), (26, -0.012), (27, 0.036), (28, 0.0), (29, 0.027), (30, 0.071), (31, -0.008), (32, 0.044), (33, -0.05), (34, 0.005), (35, -0.089), (36, -0.093), (37, 0.05), (38, 0.008), (39, -0.015), (40, 0.044), (41, 0.042), (42, -0.01), (43, -0.05), (44, -0.125), (45, 0.035), (46, -0.014), (47, -0.035), (48, 0.042), (49, -0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95773005 <a title="195-lsi-1" href="./acl-2013-Improving_machine_translation_by_training_against_an_automatic_semantic_frame_based_evaluation_metric.html">195 acl-2013-Improving machine translation by training against an automatic semantic frame based evaluation metric</a></p>
<p>Author: Chi-kiu Lo ; Karteek Addanki ; Markus Saers ; Dekai Wu</p><p>Abstract: We present the first ever results showing that tuning a machine translation system against a semantic frame based objective function, MEANT, produces more robustly adequate translations than tuning against BLEU or TER as measured across commonly used metrics and human subjective evaluation. Moreover, for informal web forum data, human evaluators preferred MEANT-tuned systems over BLEU- or TER-tuned systems by a significantly wider margin than that for formal newswire—even though automatic semantic parsing might be expected to fare worse on informal language. We argue thatbypreserving the meaning ofthe trans- lations as captured by semantic frames right in the training process, an MT system is constrained to make more accurate choices of both lexical and reordering rules. As a result, MT systems tuned against semantic frame based MT evaluation metrics produce output that is more adequate. Tuning a machine translation system against a semantic frame based objective function is independent ofthe translation model paradigm, so, any translation model can benefit from the semantic knowledge incorporated to improve translation adequacy through our approach.</p><p>2 0.76686883 <a title="195-lsi-2" href="./acl-2013-English-to-Russian_MT_evaluation_campaign.html">135 acl-2013-English-to-Russian MT evaluation campaign</a></p>
<p>Author: Pavel Braslavski ; Alexander Beloborodov ; Maxim Khalilov ; Serge Sharoff</p><p>Abstract: This paper presents the settings and the results of the ROMIP 2013 MT shared task for the English→Russian language directfioorn. t Teh Een quality Rofu generated utraagnsel datiiroencswas assessed using automatic metrics and human evaluation. We also discuss ways to reduce human evaluation efforts using pairwise sentence comparisons by human judges to simulate sort operations.</p><p>3 0.7371667 <a title="195-lsi-3" href="./acl-2013-A_Tale_about_PRO_and_Monsters.html">24 acl-2013-A Tale about PRO and Monsters</a></p>
<p>Author: Preslav Nakov ; Francisco Guzman ; Stephan Vogel</p><p>Abstract: While experimenting with tuning on long sentences, we made an unexpected discovery: that PRO falls victim to monsters overly long negative examples with very low BLEU+1 scores, which are unsuitable for learning and can cause testing BLEU to drop by several points absolute. We propose several effective ways to address the problem, using length- and BLEU+1based cut-offs, outlier filters, stochastic sampling, and random acceptance. The best of these fixes not only slay and protect against monsters, but also yield higher stability for PRO as well as improved testtime BLEU scores. Thus, we recommend them to anybody using PRO, monsterbeliever or not. – 1 Once Upon a Time... For years, the standard way to do statistical machine translation parameter tuning has been to use minimum error-rate training, or MERT (Och, 2003). However, as researchers started using models with thousands of parameters, new scalable optimization algorithms such as MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011) have emerged. As these algorithms are relatively new, they are still not quite well understood, and studying their properties is an active area of research. For example, Nakov et al. (2012) have pointed out that PRO tends to generate translations that are consistently shorter than desired. They have blamed this on inadequate smoothing in PRO’s optimization objective, namely sentencelevel BLEU+1, and they have addressed the problem using more sensible smoothing. We wondered whether the issue could be partially relieved simply by tuning on longer sentences, for which the effect of smoothing would naturally be smaller. To our surprise, tuning on the longer 50% of the tuning sentences had a disastrous effect on PRO, causing an absolute drop of three BLEU points on testing; at the same time, MERT and MIRA did not have such a problem. While investigating the reasons, we discovered hundreds of monsters creeping under PRO’s surface... Our tale continues as follows. We first explain what monsters are in Section 2, then we present a theory about how they can be slayed in Section 3, we put this theory to test in practice in Section 4, and we discuss some related efforts in Section 5. Finally, we present the moral of our tale, and we hint at some planned future battles in Section 6. 2 Monsters, Inc. PRO uses pairwise ranking optimization, where the learning task is to classify pairs of hypotheses into correctly or incorrectly ordered (Hopkins and May, 2011). It searches for a vector of weights w such that higher evaluation metric scores correspond to higher model scores and vice versa. More formally, PRO looks for weights w such that g(i, j) > g(i, j0) ⇔ hw (i, j) > hw (i, j0), where g is a local scoring fu hnction (typically, sentencelevel BLEU+1) and hw are the model scores for a given input sentence i and two candidate hypotheses j and j0 that were obtained using w. If g(i, j) > g(i, j0), we will refer to j and j0 as the positive and the negative example in the pair. Learning good parameter values requires negative examples that are comparable to the positive ones. Instead, tuning on long sentences quickly introduces monsters, i.e., corrupted negative examples that are unsuitable for learning: they are (i) much longer than the respective positive examples and the references, and (ii) have very low BLEU+1 scores compared to the positive examples and in absolute terms. The low BLEU+1 means that PRO effectively has to learn from positive examples only. 12 Proce dinSgosfi oa,f tB huel 5g1arsita, An Anu gauls Mt 4e-e9ti n2g01 o3f. th ?c e2 A0s1s3oc Aiastsio cnia fotiron C fo mrp Cuotmatpiounta tlio Lninaglu Li sntgicusi,s ptaicgses 12–17, Avg. Lengths Avg. BLEU+1 iter. pos neg ref. pos neg 1 45.2 44.6 46.5 52.5 37.6 2 3 4 5 ... 25 46.4 46.4 46.4 46.3 ... 47.9 70.5 261.0 250.0 248.0 ... 229.0 53.2 53.4 53.0 53.0 ... 52.5 52.8 52.4 52.0 52.1 ... 52.2 14.5 2.19 2.30 2.34 ... 2.81 Table 1: PRO iterations, tuning on long sentences. Table 1shows an optimization run of PRO when tuning on long sentences. We can see monsters after iterations in which positive examples are on average longer than negative ones (e.g., iter. 1). As a result, PRO learns to generate longer sentences, but it overshoots too much (iter. 2), which gives rise to monsters. Ideally, the learning algorithm should be able to recover from overshooting. However, once monsters are encountered, they quickly start dominating, with no chance for PRO to recover since it accumulates n-best lists, and thus also monsters, over iterations. As a result, PRO keeps jumping up and down and converges to random values, as Figure 1 shows. By default, PRO’s parameters are averaged over iterations, and thus the final result is quite mediocre, but selecting the highest tuning score does not solve the problem either: for example, on Figure 1, PRO never achieves a BLEU better than that for the default initialization parameters. iteration Figure 1: PRO tuning results on long sentences across iterations. The dark-gray line shows the tuning BLEU (left axis), the light-gray one is the hypothesis/reference length ratio (right axis). Figure 2 shows the translations after iterations 1, 3 and 4; the last two are monsters. The monster at iteration 3 is potentially useful, but that at iteration 4 is clearly unsuitable as a negative example. Optimizer Objective BLEU PROsent-BLEU+144.57 MERT corpus-BLEU 47.53 MIRA pseudo-doc-BLEU 47.80 PRO (6= objective)pseudo-doc-BLEU21.35 PMRIORA (6= =(6= o bojbejcetcivteiv)e) sent-BLEU+1 47.59 PMRIRO,A PC (6=-sm obojoectthiv,e g)roundfixed sent-BLEU+145.71 Table 2: PRO vs. MERT vs. MIRA. We also checked whether other popular optimizers yield very low BLEU scores at test time when tuned on long sentences. Lines 2-3 in Table 2 show that this is not the case for MERT and MIRA. Since they optimize objectives that are different from PRO’s,1 we further experimented with plugging MIRA’s objective into PRO and PRO’s objective into MIRA. The resulting MIRA scores were not much different from before, while PRO’s score dropped even further; we also found mon- sters. Next, we applied the length fix for PRO proposed in (Nakov et al., 2012); this helped a bit, but still left PRO two BLEU points behind and MIRA, and the monsters did not go away. We can conclude that the monster problem is PRO-specific, cannot be blamed on the objective function, and is different from the length bias. Note also that monsters are not specific to a dataset or language pair. We found them when tuning on the top-50% of WMT10 and testing on WMT1 1 for Spanish-English; this yielded a drop in BLEU from 29.63 (1M2/emEs/RprTes)la to 27.12 n(inPg/RtmOp.)1.1 MERT2 **REF** : but we have to close ranks with each other and realize that in unity there is strength while in division there is weakness . - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - **IT1** : but we are that we add our ranks to some of us and that we know that in the strength and weakness in **IT3** : , we are the but of the that that the , and , of ranks the the on the the our the our the some of we can include , and , of to the of we know the the our in of the of some people , force of the that that the in of the that that the the weakness Union the the , and **IT4** : namely Dr Heba Handossah and Dr Mona been pushed aside because a larger story EU Ambassador to Egypt Ian Burg highlighted 've dragged us backwards and dragged our speaking , never balme your defaulting a December 7th 1941 in Pearl Harbor ) we can include ranks will be joined by all 've dragged us backwards and dragged our $ 3 .8 billion in tourism income proceeds Chamber are divided among themselves : some ' ve dragged us backwards and dragged our were exaggerated . Al @-@ Hakim namely Dr Heba Handossah and Dr Mona December 7th 1941 in Pearl Harbor ) cases might be known to us December 7th 1941 in Pearl Harbor ) platform depends on combating all liberal policies Track and Field Federation shortened strength as well face several challenges , namely Dr Heba Handossah and Dr Mona platform depends on combating all liberal policies the report forecast that the weak structure Ftroai ngtkhsu et rahefef 2he : Ea t h xte ha,e motfo pstohlmeee r leafst eorfe wne c, et etr laonngs olfa t hei opnar a ofn sdo hhee oy fpwhaoitst hh a]r ee usisn i ostu tofra tnhes ilna tbiakoern s, haef ctoeokr it hee roant ainod nthse 1 t,h 3we aknonw d, 4. T@-@h eAl l tahes ft trwce o, tho ypotheses are monsters. 1See (Cherry and Foster, 2012) for details on objectives. 2Also, using PRO to initialize MERT, as implemented in Moses, yields 46.52 BLEU and monsters, but using MERT to initialize PRO yields 47.55 and no monsters. 13 3 Slaying Monsters: Theory Below we explain what monsters are and where they come from. Then, we propose various monster slaying techniques to be applied during PRO’s selection and acceptance steps. 3.1 What is PRO? PRO is a batch optimizer that iterates between (i) translation: using the current parameter values, generate k-best translations, and (ii) optimization: using the translations from all previous iterations, find new parameter values. The optimization step has four substeps: 1. Sampling: For each sentence, sample uniformly at random Γ = 5000 pairs from the set of all candidate translations for that sentence from all previous iterations. 2. Selection: From these sampled pairs, select those for which the absolute difference between their BLEU+1 scores is higher than α = 0.05 (note: this is 5 BLEU+1 points). 3. Acceptance: For each sentence, accept the Ξ = 50 selected pairs with the highest absolute difference in their BLEU+1 scores. 4. Learning: Assemble the accepted pairs for all sentences into a single set and use it to train a ranker to prefer the higher-scoring sentence in each pair. We believe that monsters are nurtured by PRO’s selection and acceptance policies. PRO’s selection step filters pairs involving hypotheses that differ by less than five BLEU+1 points, but it does not cut-off ones that differ too much based on BLEU+1 or length. PRO’s acceptance step selects Ξ = 50 pairs with the highest BLEU+1 differentials, which creates breeding ground for monsters since these pairs are very likely to include one monster and one good hypothesis. Below we discuss monster slaying geared towards the selection and acceptance steps of PRO. 3.2 Slaying at Selection In the selection step, PRO filters pairs for which the difference in BLEU+1 is less than five points, but it has no cut-off on the maximum BLEU+1 differentials nor cut-offs based on absolute length or difference in length. Here, we propose several selection filters, both deterministic and probabilistic. Cut-offs. A cut-off is a deterministic rule that filters out pairs that do not comply with some criteria. We experiment with a maximal cut-off on (a) the difference in BLEU+1 scores and (b) the difference in lengths. These are relative cut-offs because they refer to the pair, but absolute cut-offs that apply to each of the elements in the pair are also possible (not explored here). Cut-offs (a) and (b) slay monsters by not allowing the negative examples to get much worse in BLEU+1 or in length than the positive example in the pair. Filtering outliers. Outliers are rare or extreme observations in a sample. We assume normal distribution of the BLEU+1 scores (or of the lengths) of the translation hypotheses for the same source sentence, and we define as outliers hypotheses whose BLEU+1 (or length) is more than λ standard deviations away from the sample average. We apply the outlier filter to both the positive and the negative example in a pair, but it is more important for the latter. We experiment with values of λ like 2 and 3. This filtering slays monsters because they are likely outliers. However, it will not work if the population gets riddled with monsters, in which case they would become the norm. Stochastic sampling. Instead of filtering extreme examples, we can randomly sample pairs according to their probability of being typical. Let us assume that the values of the local scoring functions, i.e., the BLEU+1 scores, are distributed nor- mally: g(i, j) ∼ N(µ, σ2). Given a sample of hypothesis (tira,nj)sl ∼atio Nn(sµ {j} of the same source sentpeontchee i, we can ensstim {ja}te o σ empirically. Then, the difference ∆ = g(i, j) − g(i, j0) would be tdhisetr diibfufteerde normally w gi(thi, mean zero and variance 2σ2. Now, given a pair of examples, we can calculate their ∆, and we can choose to select the pair with some probability, according to N(0, 2σ2). 3.3 Slaying at Acceptance Another problem is caused by the acceptance mechanism of PRO: among all selected pairs, it accepts the top-Ξ with the highest BLEU+1 differentials. It is easy to see that these differentials are highest for nonmonster–monster pairs if such pairs exist. One way to avoid focusing primarily on such pairs is to accept a random set of pairs, among the ones that survived the selection step. One possible caveat is that we can lose some of the discriminative power of PRO by focusing on examples that are not different enough. Ξ 14 TESTING TUNING (run 1, it. 25, avg.) TEST(tune:full) PRO fix Avg. for 3 reruns BLEU StdDev Pos Lengths Neg Ref BLEU+1 Avg. for 3 reruns Pos Neg BLEU StdDev PRO (baseline)44.700.26647.9229.052.552.22.847.800.052 Max diff. cut-offBLEU+1 max=10†47.940.16547.949.649.449.439.947.770.035 BLEU+1 max=20 † 47.73 0.136 47.7 55.5 51.1 49.8 32.7 47.85 0.049 LEN max=5 † 48.09 0.021 46.8 47.0 47.9 52.9 37.8 47.73 0.051 LEN max=10 † 47.99 0.025 47.3 48.5 48.7 52.5 35.6 47.80 0.056 OutliersBLEU+1 λ=2.0†48.050.11946.847.247.752.239.547.470.090 BLEU+1 λ=3.0 LEN λ=2.0 LEN λ=3.0 47.12 46.68 47.02 1.348 2.005 0.727 47.6 49.3 48.2 168.0 82.7 163.0 53.0 53.1 51.4 51.7 52.3 51.4 3.9 5.3 4.2 47.53 47.49 47.65 0.038 0.085 0.096 Stoch. sampl.∆ BLEU+146.331.00046.8216.053.353.12.447.740.035 ∆ LEN 46.36 1.281 47.4 201.0 52.9 53.4 2.9 47.78 0.081 Table 3: Some fixes to PRO (select pairs with highest BLEU+1 differential, also require at least 5 BLEU+1 points difference). A dagger (†) indicates selection fixes that successfully get rid of monsters. 4 Attacking Monsters: Practice Below, we first present our general experimental setup. Then, we present the results for the various selection alternatives, both with the original acceptance strategy and with random acceptance. 4.1 Experimental Setup We used a phrase-based SMT model (Koehn et al., 2003) as implemented in the Moses toolkit (Koehn et al., 2007). We trained on all Arabic-English data for NIST 2012 except for UN, we tuned on (the longest-50% of) the MT06 sentences, and we tested on MT09. We used the MADA ATB segmentation for Arabic (Roth et al., 2008) and truecasing for English, phrases of maximal length 7, Kneser-Ney smoothing, and lexicalized reorder- ing (Koehn et al., 2005), and a 5-gram language model, trained on GigaWord v.5 using KenLM (Heafield, 2011). We dropped unknown words both at tuning and testing, and we used minimum Bayes risk decoding at testing (Kumar and Byrne, 2004). We evaluated the output with NIST’s scoring tool v.13a, cased. We used the Moses implementations of MERT, PRO and batch MIRA, with the –return-best-dev parameter for the latter. We ran these optimizers for up to 25 iterations and we used 1000-best lists. For stability (Foster and Kuhn, 2009), we performed three reruns of each experiment (tuning + evaluation), and we report averaged scores. 4.2 Selection Alternatives Table 3 presents the results for different selection alternatives. The first two columns show the testing results: average BLEU and standard deviation over three reruns. The following five columns show statistics about the last iteration (it. 25) of PRO’s tuning for the worst rerun: average lengths of the positive and the negative examples and average effective reference length, followed by average BLEU+1 scores for the positive and the negative examples in the pairs. The last two columns present the results when tuning on the full tuning set. These are included to verify the behavior of PRO in a nonmonster prone environment. We can see in Table 3 that all selection mechanisms considerably improve BLEU compared to the baseline PRO, by 2-3 BLEU points. However, not every selection alternative gets rid of monsters, which can be seen by the large lengths and low BLEU+1 for the negative examples (in bold). The max cut-offs for BLEU+1 and for lengths both slay the monsters, but the latter yields much lower standard deviation (thirteen times lower than for the baseline PRO!), thus considerably increasing PRO’s stability. On the full dataset, BLEU scores are about the same as for the original PRO (with small improvement for BLEU+1 max=20), but the standard deviations are slightly better. Rejecting outliers using BLEU+1 and λ = 3 is not strong enough to filter out monsters, but making this criterion more strict by setting λ = 2, yields competitive BLEU and kills the monsters. Rejecting outliers based on length does not work as effectively though. We can think of two possible reasons: (i) lengths are not normally distributed, they are more Poisson-like, and (ii) the acceptance criterion is based on the top-Ξ differentials based on BLEU+1, not based on length. On the full dataset, rejecting outliers, BLEU+1 and length, yields lower BLEU and less stability. 15 TESTING TUNING (run 1, it. 25, avg.) TEST(tune:full) Avg. for 3 reruns Lengths BLEU+1 Avg. for 3 reruns PRO fix BLEU StdDev Pos Neg Ref Pos Neg BLEU StdDev PRO (baseline)44.700.26647.9229.052.552.22.847.800.052 Rand. acceptPRO, rand††47.870.14747.748.548.7047.742.947.590.114 OutliersBLEU+1 λ=2.0, rand∗47.850.07848.248.448.947.543.647.620.091 BLEU+1 λ=3.0, rand 47.97 0.168 47.6 47.6 48.4 47.8 43.6 47.44 0.070 LEN λ=2.0, rand∗ 47.69 0.114 47.8 47.8 48.6 47.9 43.6 47.48 0.046 LEN λ=3.0, rand 47.89 0.235 47.8 48.0 48.7 47.7 43. 1 47.64 0.090 Stoch. sampl.∆ BLEU+1, rand∗47.990.08747.948.048.747.843.547.670.096 ∆ LEN, rand∗ 47.94 0.060 47.8 47.9 48.6 47.8 43.6 47.65 0.097 Table 4: More fixes to PRO (with random acceptance, no minimum BLEU+1). The (††) indicates that random acceptance kills monsters. The asterisk (∗) indicates improved stability over random acceptance. Reasons (i) and (ii) arguably also apply to stochastic sampling of differentials (for BLEU+1 or for length), which fails to kill the monsters, maybe because it gives them some probability of being selected by design. To alleviate this, we test the above settings with random acceptance. 4.3 Random Acceptance Table 4 shows the results for accepting training pairs for PRO uniformly at random. To eliminate possible biases, we also removed the min=0.05 BLEU+1 selection criterion. Surprisingly, this setup effectively eliminated the monster problem. Further coupling this with the distributional criteria can also yield increased stability, and even small further increase in test BLEU. For instance, rejecting BLEU outliers with λ = 2 yields comparable average test BLEU, but with only half the standard deviation. On the other hand, using the stochastic sampling of differentials based on either BLEU+1 or lengths improves the test BLEU score while increasing the stability across runs. The random acceptance has a caveat though: it generally decreases the discriminative power of PRO, yielding worse results when tuning on the full, nonmonster prone tuning dataset. Stochastic selection does help to alleviate this problem. Yet, the results are not as good as when using a max cut-off for the length. Therefore, we recommend using the latter as a default setting. 5 Related Work We are not aware of previous work that discusses the issue of monsters, but there has been work on a different, length problem with PRO (Nakov et al., 2012). We have seen that its solution, fix the smoothing in BLEU+1, did not work for us. The stability of MERT has been improved using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques, there have been studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA– PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity can be an issue when tuning MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011). Large variance between the results obtained with MIRA has also been reported (Simianer et al., 2012). However, none of this work has focused on monsters. 6 Tale’s Moral and Future Battles We have studied a problem with PRO, namely that it can fall victim to monsters, overly long negative examples with very low BLEU+1 scores, which are unsuitable for learning. We have proposed several effective ways to address this problem, based on length- and BLEU+1-based cut-offs, outlier filters and stochastic sampling. The best of these fixes have not only slayed the monsters, but have also brought much higher stability to PRO as well as improved test-time BLEU scores. These benefits are less visible on the full dataset, but we still recommend them to everybody who uses PRO as protection against monsters. Monsters are inherent in PRO; they just do not always take over. In future work, we plan a deeper look at the mechanism of monster creation in PRO and its possible connection to PRO’s length bias. 16 References Daniel Cer, Daniel Jurafsky, and Christopher Manning. 2008. Regularization and search for minimum error rate training. In Proc. of Workshop on Statistical Machine Translation, WMT ’08, pages 26–34. Mauro Cettolo, Nicola Bertoldi, and Marcello Federico. 2011. Methods for smoothing the optimizer instability in SMT. MT Summit XIII: the Machine Translation Summit, pages 32–39. Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics, NAACL-HLT ’ 12, pages 427–436. David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proceedings ofthe Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 224–233. David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine transla- tion. In Proc. of the Conference of the North American Chapter of the Association for Computational Linguistics, NAACL-HLT ’09, pages 218–226. Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of the Meeting of the Association for Computational Linguistics, ACL ’ 11, pages 176–181 . Michael Denkowski and Alon Lavie. 2011. Meteortuned phrase-based SMT: CMU French-English and Haitian-English systems for WMT 2011. Technical report, CMU-LTI-1 1-01 1, Language Technologies Institute, Carnegie Mellon University. George Foster and Roland Kuhn. 2009. Stabilizing minimum error rate training. In Proceedings of the Workshop on Statistical Machine Translation, StatMT ’09, pages 242–249. Kevin Gimpel and Noah Smith. 2012. Structured ramp loss minimization for machine translation. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics, NAACL-HLT ’ 12, pages 221–231. Kenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In Workshop on Statistical Machine Translation, WMT ’ 11, pages 187–197. Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP ’ 11, pages 1352–1362. Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, HLTNAACL ’03, pages 48–54. Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In Proceedings of the International Workshop on Spoken Language Translation, IWSLT ’05. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of the Meeting of the Association for Computational Linguistics, ACL ’07, pages 177–180. Shankar Kumar and William Byrne. 2004. Minimum Bayes-risk decoding for statistical machine translation. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, HLT-NAACL ’04, pages 169–176. Alon Lavie and Michael Denkowski. 2009. The METEOR metric for automatic evaluation of machine translation. Machine Translation, 23: 105–1 15. Robert Moore and Chris Quirk. 2008. Random restarts in minimum error rate training for statistical machine translation. In Proceedings of the International Conference on Computational Linguistics, COLING ’08, pages 585–592. Preslav Nakov, Francisco Guzm a´n, and Stephan Vogel. 2012. Optimizing for sentence-level BLEU+1 yields short translations. In Proceedings ofthe International Conference on Computational Linguistics, COLING ’ 12, pages 1979–1994. Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the Meeting of the Association for Computational Linguistics, ACL ’03, pages 160–167. Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab, and Cynthia Rudin. 2008. Arabic morphological tagging, diacritization, and lemmatization using lexeme models and feature ranking. In Proceedings of the Meeting of the Association for Computational Linguistics, ACL ’08, pages 117–120. Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012. Joint feature selection in distributed stochastic learning for large-scale discriminative training in smt. In Proceedings of the Meeting of the Association for Computational Linguistics, ACL ’ 12, pages 11–21. Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’07, pages 764–773. 17</p><p>4 0.71809566 <a title="195-lsi-4" href="./acl-2013-Fast_and_Adaptive_Online_Training_of_Feature-Rich_Translation_Models.html">156 acl-2013-Fast and Adaptive Online Training of Feature-Rich Translation Models</a></p>
<p>Author: Spence Green ; Sida Wang ; Daniel Cer ; Christopher D. Manning</p><p>Abstract: We present a fast and scalable online method for tuning statistical machine translation models with large feature sets. The standard tuning algorithm—MERT—only scales to tens of features. Recent discriminative algorithms that accommodate sparse features have produced smaller than expected translation quality gains in large systems. Our method, which is based on stochastic gradient descent with an adaptive learning rate, scales to millions of features and tuning sets with tens of thousands of sentences, while still converging after only a few epochs. Large-scale experiments on Arabic-English and Chinese-English show that our method produces significant translation quality gains by exploiting sparse features. Equally important is our analysis, which suggests techniques for mitigating overfitting and domain mismatch, and applies to other recent discriminative methods for machine translation. 1</p><p>5 0.71439141 <a title="195-lsi-5" href="./acl-2013-Deepfix%3A_Statistical_Post-editing_of_Statistical_Machine_Translation_Using_Deep_Syntactic_Analysis.html">110 acl-2013-Deepfix: Statistical Post-editing of Statistical Machine Translation Using Deep Syntactic Analysis</a></p>
<p>Author: Rudolf Rosa ; David Marecek ; Ales Tamchyna</p><p>Abstract: Deepfix is a statistical post-editing system for improving the quality of statistical machine translation outputs. It attempts to correct errors in verb-noun valency using deep syntactic analysis and a simple probabilistic model of valency. On the English-to-Czech translation pair, we show that statistical post-editing of statistical machine translation leads to an improvement of the translation quality when helped by deep linguistic knowledge.</p><p>6 0.68222845 <a title="195-lsi-6" href="./acl-2013-TransDoop%3A_A_Map-Reduce_based_Crowdsourced_Translation_for_Complex_Domain.html">355 acl-2013-TransDoop: A Map-Reduce based Crowdsourced Translation for Complex Domain</a></p>
<p>7 0.68094808 <a title="195-lsi-7" href="./acl-2013-A_New_Syntactic_Metric_for_Evaluation_of_Machine_Translation.html">13 acl-2013-A New Syntactic Metric for Evaluation of Machine Translation</a></p>
<p>8 0.67418945 <a title="195-lsi-8" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>9 0.66996926 <a title="195-lsi-9" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>10 0.66053802 <a title="195-lsi-10" href="./acl-2013-Automatically_Predicting_Sentence_Translation_Difficulty.html">64 acl-2013-Automatically Predicting Sentence Translation Difficulty</a></p>
<p>11 0.65528494 <a title="195-lsi-11" href="./acl-2013-Learning_Non-linear_Features_for_Machine_Translation_Using_Gradient_Boosting_Machines.html">221 acl-2013-Learning Non-linear Features for Machine Translation Using Gradient Boosting Machines</a></p>
<p>12 0.65419346 <a title="195-lsi-12" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>13 0.63884145 <a title="195-lsi-13" href="./acl-2013-Online_Relative_Margin_Maximization_for_Statistical_Machine_Translation.html">264 acl-2013-Online Relative Margin Maximization for Statistical Machine Translation</a></p>
<p>14 0.63771319 <a title="195-lsi-14" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>15 0.62927598 <a title="195-lsi-15" href="./acl-2013-QuEst_-_A_translation_quality_estimation_framework.html">289 acl-2013-QuEst - A translation quality estimation framework</a></p>
<p>16 0.62370223 <a title="195-lsi-16" href="./acl-2013-Mr._MIRA%3A_Open-Source_Large-Margin_Structured_Learning_on_MapReduce.html">251 acl-2013-Mr. MIRA: Open-Source Large-Margin Structured Learning on MapReduce</a></p>
<p>17 0.62305117 <a title="195-lsi-17" href="./acl-2013-SORT%3A_An_Interactive_Source-Rewriting_Tool_for_Improved_Translation.html">305 acl-2013-SORT: An Interactive Source-Rewriting Tool for Improved Translation</a></p>
<p>18 0.61965698 <a title="195-lsi-18" href="./acl-2013-Stacking_for_Statistical_Machine_Translation.html">328 acl-2013-Stacking for Statistical Machine Translation</a></p>
<p>19 0.61558068 <a title="195-lsi-19" href="./acl-2013-Models_of_Translation_Competitions.html">250 acl-2013-Models of Translation Competitions</a></p>
<p>20 0.60863107 <a title="195-lsi-20" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.03), (6, 0.032), (11, 0.041), (14, 0.02), (15, 0.01), (24, 0.017), (26, 0.033), (35, 0.041), (42, 0.053), (48, 0.026), (70, 0.021), (90, 0.033), (95, 0.562)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99033457 <a title="195-lda-1" href="./acl-2013-Improving_machine_translation_by_training_against_an_automatic_semantic_frame_based_evaluation_metric.html">195 acl-2013-Improving machine translation by training against an automatic semantic frame based evaluation metric</a></p>
<p>Author: Chi-kiu Lo ; Karteek Addanki ; Markus Saers ; Dekai Wu</p><p>Abstract: We present the first ever results showing that tuning a machine translation system against a semantic frame based objective function, MEANT, produces more robustly adequate translations than tuning against BLEU or TER as measured across commonly used metrics and human subjective evaluation. Moreover, for informal web forum data, human evaluators preferred MEANT-tuned systems over BLEU- or TER-tuned systems by a significantly wider margin than that for formal newswire—even though automatic semantic parsing might be expected to fare worse on informal language. We argue thatbypreserving the meaning ofthe trans- lations as captured by semantic frames right in the training process, an MT system is constrained to make more accurate choices of both lexical and reordering rules. As a result, MT systems tuned against semantic frame based MT evaluation metrics produce output that is more adequate. Tuning a machine translation system against a semantic frame based objective function is independent ofthe translation model paradigm, so, any translation model can benefit from the semantic knowledge incorporated to improve translation adequacy through our approach.</p><p>2 0.97648364 <a title="195-lda-2" href="./acl-2013-Translating_Dialectal_Arabic_to_English.html">359 acl-2013-Translating Dialectal Arabic to English</a></p>
<p>Author: Hassan Sajjad ; Kareem Darwish ; Yonatan Belinkov</p><p>Abstract: We present a dialectal Egyptian Arabic to English statistical machine translation system that leverages dialectal to Modern Standard Arabic (MSA) adaptation. In contrast to previous work, we first narrow down the gap between Egyptian and MSA by applying an automatic characterlevel transformational model that changes Egyptian to EG0, which looks similar to MSA. The transformations include morphological, phonological and spelling changes. The transformation reduces the out-of-vocabulary (OOV) words from 5.2% to 2.6% and gives a gain of 1.87 BLEU points. Further, adapting large MSA/English parallel data increases the lexical coverage, reduces OOVs to 0.7% and leads to an absolute BLEU improvement of 2.73 points.</p><p>3 0.97560108 <a title="195-lda-3" href="./acl-2013-Named_Entity_Recognition_using_Cross-lingual_Resources%3A_Arabic_as_an_Example.html">256 acl-2013-Named Entity Recognition using Cross-lingual Resources: Arabic as an Example</a></p>
<p>Author: Kareem Darwish</p><p>Abstract: Some languages lack large knowledge bases and good discriminative features for Name Entity Recognition (NER) that can generalize to previously unseen named entities. One such language is Arabic, which: a) lacks a capitalization feature; and b) has relatively small knowledge bases, such as Wikipedia. In this work we address both problems by incorporating cross-lingual features and knowledge bases from English using cross-lingual links. We show that such features have a dramatic positive effect on recall. We show the effectiveness of cross-lingual features and resources on a standard dataset as well as on two new test sets that cover both news and microblogs. On the standard dataset, we achieved a 4.1% relative improvement in Fmeasure over the best reported result in the literature. The features led to improvements of 17.1% and 20.5% on the new news and mi- croblogs test sets respectively.</p><p>4 0.96979964 <a title="195-lda-4" href="./acl-2013-Syntactic_Patterns_versus_Word_Alignment%3A_Extracting_Opinion_Targets_from_Online_Reviews.html">336 acl-2013-Syntactic Patterns versus Word Alignment: Extracting Opinion Targets from Online Reviews</a></p>
<p>Author: Kang Liu ; Liheng Xu ; Jun Zhao</p><p>Abstract: Mining opinion targets is a fundamental and important task for opinion mining from online reviews. To this end, there are usually two kinds of methods: syntax based and alignment based methods. Syntax based methods usually exploited syntactic patterns to extract opinion targets, which were however prone to suffer from parsing errors when dealing with online informal texts. In contrast, alignment based methods used word alignment model to fulfill this task, which could avoid parsing errors without using parsing. However, there is no research focusing on which kind of method is more better when given a certain amount of reviews. To fill this gap, this paper empiri- cally studies how the performance of these two kinds of methods vary when changing the size, domain and language of the corpus. We further combine syntactic patterns with alignment model by using a partially supervised framework and investigate whether this combination is useful or not. In our experiments, we verify that our combination is effective on the corpus with small and medium size.</p><p>5 0.96231753 <a title="195-lda-5" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>Author: Tsutomu Hirao ; Tomoharu Iwata ; Masaaki Nagata</p><p>Abstract: Unsupervised object matching (UOM) is a promising approach to cross-language natural language processing such as bilingual lexicon acquisition, parallel corpus construction, and cross-language text categorization, because it does not require labor-intensive linguistic resources. However, UOM only finds one-to-one correspondences from data sets with the same number of instances in source and target domains, and this prevents us from applying UOM to real-world cross-language natural language processing tasks. To alleviate these limitations, we proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic space. We demonstrate the effectiveness of our method on cross-language text categorization. The results show that our method outperforms conventional unsupervised object matching methods.</p><p>6 0.95112991 <a title="195-lda-6" href="./acl-2013-Adaptive_Parser-Centric_Text_Normalization.html">37 acl-2013-Adaptive Parser-Centric Text Normalization</a></p>
<p>7 0.9374693 <a title="195-lda-7" href="./acl-2013-Beam_Search_for_Solving_Substitution_Ciphers.html">66 acl-2013-Beam Search for Solving Substitution Ciphers</a></p>
<p>8 0.93030655 <a title="195-lda-8" href="./acl-2013-FrameNet_on_the_Way_to_Babel%3A_Creating_a_Bilingual_FrameNet_Using_Wiktionary_as_Interlingual_Connection.html">162 acl-2013-FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection</a></p>
<p>9 0.9039824 <a title="195-lda-9" href="./acl-2013-QuEst_-_A_translation_quality_estimation_framework.html">289 acl-2013-QuEst - A translation quality estimation framework</a></p>
<p>10 0.84437788 <a title="195-lda-10" href="./acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language.html">374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</a></p>
<p>11 0.83715415 <a title="195-lda-11" href="./acl-2013-English-to-Russian_MT_evaluation_campaign.html">135 acl-2013-English-to-Russian MT evaluation campaign</a></p>
<p>12 0.82383275 <a title="195-lda-12" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>13 0.81898272 <a title="195-lda-13" href="./acl-2013-Sentence_Level_Dialect_Identification_in_Arabic.html">317 acl-2013-Sentence Level Dialect Identification in Arabic</a></p>
<p>14 0.81071675 <a title="195-lda-14" href="./acl-2013-A_New_Syntactic_Metric_for_Evaluation_of_Machine_Translation.html">13 acl-2013-A New Syntactic Metric for Evaluation of Machine Translation</a></p>
<p>15 0.80976772 <a title="195-lda-15" href="./acl-2013-Language_Independent_Connectivity_Strength_Features_for_Phrase_Pivot_Statistical_Machine_Translation.html">214 acl-2013-Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation</a></p>
<p>16 0.79703689 <a title="195-lda-16" href="./acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</a></p>
<p>17 0.79402494 <a title="195-lda-17" href="./acl-2013-Social_Text_Normalization_using_Contextual_Graph_Random_Walks.html">326 acl-2013-Social Text Normalization using Contextual Graph Random Walks</a></p>
<p>18 0.78066695 <a title="195-lda-18" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>19 0.78012013 <a title="195-lda-19" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>20 0.76121157 <a title="195-lda-20" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
