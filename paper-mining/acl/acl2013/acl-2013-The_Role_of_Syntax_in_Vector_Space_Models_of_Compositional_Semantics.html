<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-347" href="#">acl2013-347</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</h1>
<br/><p>Source: <a title="acl-2013-347-pdf" href="http://aclweb.org/anthology//P/P13/P13-1088.pdf">pdf</a></p><p>Author: Karl Moritz Hermann ; Phil Blunsom</p><p>Abstract: Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of Natural Language Processing. In this paper we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by Combinatory Categorial Grammar to introduce Combinatory Categorial Autoencoders. This model leverages the CCG combinatory operators to guide a non-linear transformation of meaning within a sentence. We use this model to learn high dimensional embeddings for sentences and evaluate them in a range of tasks, demonstrating that the incorporation of syntax allows a concise model to learn representations that are both effective and general.</p><p>Reference: <a title="acl-2013-347-reference" href="../acl2013_reference/acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of Natural Language Processing. [sent-7, score-0.135]
</p><p>2 In this paper we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by Combinatory Categorial Grammar to introduce Combinatory Categorial Autoencoders. [sent-8, score-0.357]
</p><p>3 This model leverages the CCG combinatory operators to guide a non-linear transformation of meaning within a sentence. [sent-9, score-0.289]
</p><p>4 We use this model to learn high dimensional embeddings for sentences and evaluate them in a range of tasks, demonstrating that  the incorporation of syntax allows a concise model to learn representations that are both effective and general. [sent-10, score-0.309]
</p><p>5 Over a hundred years on the choice of representational unit for this process of compositional semantics, and how these units combine, remain open questions. [sent-13, score-0.135]
</p><p>6 The Montague grammar (Montague, 1970) is a prime example for this, building a model of composition based on lambdacalculus and formal logic. [sent-15, score-0.23]
</p><p>7 Recently those searching for the right representation for compositional semantics have drawn inspiration from the success of distributional models of lexical semantics. [sent-17, score-0.322]
</p><p>8 While distributional semantics is easily applied to single words, sparsity implies that attempts to directly extract distributional representations for larger expressions are doomed to fail. [sent-20, score-0.405]
</p><p>9 Most approaches here use the idea of vector-matrix composition to learn larger representations from single-word encodings (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al. [sent-22, score-0.264]
</p><p>10 While these models have proved very promising for compositional semantics, they make minimal use of  linguistic information beyond the word level. [sent-24, score-0.182]
</p><p>11 We achieve this goal by employing the CCG formalism to consider compositional structures at any point in a parse tree. [sent-26, score-0.253]
</p><p>12 CCG is attractive both for its transparent interface between syntax and semantics, and a small but powerful set of combinatory operators with which we can parametrise our nonlinear transformations of compositional meaning. [sent-27, score-0.534]
</p><p>13 We present a novel class of recursive models, the Combinatory Categorial Autoencoders (CCAE), which marry a semantic process provided by a recursive autoencoder with the syntactic representations of the CCG formalism. [sent-28, score-0.655]
</p><p>14 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 894–904, tions: Can recursive vector space models be reconciled with a more formal notion of compositionality; and is there a role for syntax in guiding semantics in these types of models? [sent-31, score-0.397]
</p><p>15 CCAEs make use of  CCG combinators and types by conditioning each composition function on its equivalent step in a CCG proof. [sent-32, score-0.198]
</p><p>16 In terms of learning complexity and space requirements, our models strike a balance between simpler greedy approaches (Socher et al. [sent-33, score-0.151]
</p><p>17 , 2011b) and the larger recursive vector-matrix models (Socher et al. [sent-34, score-0.204]
</p><p>18 We show that this combination of state of the art machine learning and an advanced linguistic formalism translates into concise models with competitive performance on a variety of tasks. [sent-36, score-0.114]
</p><p>19 In both sentiment and compound similarity experiments we show that our CCAE models match or better comparable recursive autoencoder models. [sent-37, score-0.674]
</p><p>20 Kracht (2008) for a detailed analysis of compositionality in these formalisms. [sent-44, score-0.11]
</p><p>21 CCG relies on combinatory logic (as opposed to lambda calculus) to build its expressions. [sent-48, score-0.256]
</p><p>22 com/ Tina likes  tigers  N(S[dcl]\NP)/NP N NP NP S[dcl]\NP> S[dcl]< Figure 1: CCG derivation for Tina likes tigers with forward (>) and backward application (<). [sent-54, score-0.353]
</p><p>23 CCG’s transparent surface stems from its categorial property: Each point in a derivation corresponds directly to an interpretable category. [sent-61, score-0.143]
</p><p>24 Subsequently the expression likes tigers (as type S[dcl]\NP) requires a second NP on its left. [sent-64, score-0.139]
</p><p>25 Thus at each point in a CCG parse we can deduce the possible next steps in the derivation by considering the available types and combinatory rules. [sent-66, score-0.307]
</p><p>26 While it is theoretically possible to apply the same mechanism to larger expressions, sparsity prevents learning meaningful distributional representations for expressions much larger than single words. [sent-74, score-0.302]
</p><p>27 2 Vector space models of compositional semantics aim to fill this gap by providing a methodology for deriving the representation of an expression from those of its parts. [sent-75, score-0.249]
</p><p>28 While distributional representations frequently serve to encode single words in such approaches this is not a strict requirement. [sent-76, score-0.149]
</p><p>29 There are a number of ideas on how to define composition in such vector spaces. [sent-77, score-0.189]
</p><p>30 A general framework for semantic vector composition was proposed in Mitchell and Lapata (2008), with Mitchell and Lapata (2010) and more recently Blacoe and Lapata (2012) providing good overviews of this topic. [sent-78, score-0.189]
</p><p>31 Notable approaches to this issue include Baroni and Zamparelli (2010), who compose nouns and adjectives by representing them as vectors and matrices, respectively, with the compositional representation achieved by multiplication. [sent-79, score-0.176]
</p><p>32 (2013), producing a tensor-based semantic framework with tensor contraction as composition operation. [sent-82, score-0.152]
</p><p>33 Another set of models that have very successfully been applied in this area are recursive autoencoders (Socher et al. [sent-83, score-0.369]
</p><p>34 One can think of an autoencoder as a funnel through which information has to pass (see Figure 2). [sent-88, score-0.306]
</p><p>35 By forcing the autoencoder to reconstruct an input given only the reduced amount of information available inside the funnel it serves as a compression tool, representing highdimensional objects in a lower-dimensional space. [sent-89, score-0.306]
</p><p>36 Typically a given autoencoder, that is the functions for encoding and reconstructing data, are 2The experimental setup in (Baroni and Zamparelli, 2010) is one of the few examples where distributional representations are used for word pairs. [sent-90, score-0.217]
</p><p>37 By optimizing the two functions to minimize the difference between all inputs and their respective reconstructions, this autoencoder will effectively discover some hidden structures within the data that can be exploited to represent it more efficiently. [sent-94, score-0.265]
</p><p>38 N), weight matrices Wenc ∈ , W,ir e∈c ∈ aigndh tb miaasetrsi beesn cW ∈ Rm, brec ∈ Rn. [sent-97, score-0.14]
</p><p>39 The encoding matrix and bias are u Rsed to cre∈at Re an encoding ei from xi:  R(m∈×n R)  . [sent-98, score-0.205]
</p><p>40 ill intuitively lead to ei encoding a latent structure contained in xi and shared across all xj ,j ∈ (0. [sent-105, score-0.14]
</p><p>41 It is possible to apply multiple autoencoders on top of each other, creating a deep autoencoder (Bengio et al. [sent-110, score-0.462]
</p><p>42 fenc(xi) frec(ei)  (Wencxi + benc) g (Wrecei + brec)  = g =  (4)  Furthermore, autoencoders can easily be used as a composition function by concatenating two input vectors, such that: e  = f(x1, x2) = g (W(x1 kx2) + b)  (5)  (x01kx02) = g ? [sent-117, score-0.317]
</p><p>43 By setting the n = 2m, it is possible to recursively combine a structure into an autoencoder tree. [sent-122, score-0.265]
</p><p>44 The recursive application of autoencoders was first introduced in Pollack (1990), whose recursive auto-associative memories learn vector representations over pre-specified recursive data structures. [sent-124, score-0.785]
</p><p>45 Their purpose is to learn semantically meaningful vector represen-  tations for sentences and phrases of variable size, while the purpose of this paper is to investigate the use of syntax and linguistic formalisms in such vector-based compositional models. [sent-136, score-0.326]
</p><p>46 Let C denote the set of combinatory rules, and T the set of categories used, respectively. [sent-138, score-0.256]
</p><p>47 We use the parse tree to structure an RAE, so that each combinatory step is represented by an autoencoder function. [sent-139, score-0.572]
</p><p>48 In total this paper describes four models making increasing use of the CCG formalism (see table 1). [sent-141, score-0.114]
</p><p>49 CCAE-A uses a single weight matrix each for the encoding and reconstruction step (see Table 2. [sent-143, score-0.174]
</p><p>50 Our second model (CCAE-B) uses the composition function in equation (6), with c ∈ C. [sent-147, score-0.152]
</p><p>51 = g (Wecnc(xky) + becnc) frec(e, c) = g (Wrcece + brcec) fenc(x, y, c)  (6)  This means that for every combinatory rule we define an equivalent autoencoder composition function by parametrizing both the weight matrix and bias on the combinatory rule (e. [sent-148, score-1.003]
</p><p>52 α : Xα/βY : X β : Y>  g (We>nc(αkβ) + be>nc)  Figure 4: Forward application as CCG combinator and autoencoder rule respectively. [sent-160, score-0.265]
</p><p>53 For the remainder of this paper we will focus on the composition step and drop the use of enc and rec in variable names where it isn’t explicitly required. [sent-166, score-0.152]
</p><p>54 While CCAE-B uses only the combinatory rules, we want to make fuller use of the linguistic information available in CCG. [sent-168, score-0.256]
</p><p>55 For this purpose, we build another model CCAE-C, which parametrizes on both the combinatory rule c ∈ C apnarda mtheet rCizCeGs o category et ∈ Tb nata every step (see Figure 2). [sent-169, score-0.256]
</p><p>56 TGhis c mtegoodreyl provides an vaedrdyit siotenpal ( dseeegree of insight, as the categories T are semantically and syntactically more expressive than the CCG combinatory rules by themselves. [sent-170, score-0.325]
</p><p>57 Here we consider the categories not of the element represented, but of the elements it is generated from together with the combinatory rule applied to them. [sent-173, score-0.256]
</p><p>58 Subsequently we combine these two conditioned on their joint combinatory rule. [sent-175, score-0.256]
</p><p>59 Such a representation can be useful for some tasks such as paraphrase detection, but is not sufficient for other tasks such as sentiment classification, which we are considering in this paper. [sent-196, score-0.117]
</p><p>60 k[lx − ek2 Erec(n,θ)  (12) (13)  This method of introducing a supervised aspect to the autoencoder largely follows the model described in Socher et al. [sent-205, score-0.265]
</p><p>61 The first task of sentiment analysis allows us to compare our CCG-conditioned RAE with similar, existing models. [sent-208, score-0.117]
</p><p>62 In a second experiment, we apply our model to a compound similarity evaluation, which allows us to evaluate our models against a larger class of vector-based models (Blacoe and Lapata, 2012). [sent-209, score-0.182]
</p><p>63 We conclude with some qualitative analysis to get a better idea of whether the combination of CCG and RAE can learn semantically expressive embeddings. [sent-210, score-0.145]
</p><p>64 , 2011b) by using an additional sentiment lexicon (Wilson et al. [sent-222, score-0.117]
</p><p>65 6 This dataset consists of 10662 sentences extracted from movie reviews which are manually labelled with positive or negative sentiment and equally distributed across sentiment. [sent-226, score-0.117]
</p><p>66 Experiment 1: Semi-Supervised Training In the first experiment, we use the semi-supervised training strategy described previously and initialize our models with the embeddings provided by Turian et al. [sent-227, score-0.157]
</p><p>67 The MPQA task is further simplified through the use or an additional sentiment lexicon. [sent-233, score-0.117]
</p><p>68 7  Table 3: Accuracy of sentiment classification on the sentiment polarity (SP) and MPQA datasets. [sent-266, score-0.234]
</p><p>69 While the initialization of the word vectors with previously learned embeddings (as was previously shown by Socher et al. [sent-269, score-0.151]
</p><p>70 (201 1b)) helps the models, all other model variables such as composition weights and biases are still initialised randomly and thus highly dependent on the amount of train-  ing data available. [sent-270, score-0.152]
</p><p>71 In this phase only the reconstruction signal is used to learn word embeddings and transformation matrices. [sent-275, score-0.219]
</p><p>72 By learning word embeddings and composition matrices on more data, the model is likely to generalise better. [sent-277, score-0.321]
</p><p>73 Particularly for the more complex  models, where the composition functions are conditioned on various CCG parameters, this should Training Model RegularPretraining CCAE-A77. [sent-278, score-0.152]
</p><p>74 In fact, the trend of the previous results has been reversed, with the more complex models now performing best, whereas in the previous experiments the simpler models performed better. [sent-290, score-0.131]
</p><p>75 We train our models as fully unsupervised autoencoders on the British National Corpus for this task. [sent-297, score-0.212]
</p><p>76 We assume fixed parse trees for all of the compounds (Figure 6), and use these to compute compound level vectors for all word pairs. [sent-298, score-0.217]
</p><p>77 We subsequently use the cosine distance between each compound pair as our similarity measure. [sent-299, score-0.153]
</p><p>78 html 900  VVerBb  ObNjNect  Noun Noun  Adjective Noun  (S\NP)/SN\NPPNNP>  NN/NNNNNN>  NJ/JNNNNN>  Figure 6: Assumed CCG parse structure for the compound similarity evaluation. [sent-311, score-0.139]
</p><p>79 Using one of the models trained on the MPQA corpus, we generate word-level representations of all phrases in this corpus and subsequently identify the most related expressions by using the cosine distance measure. [sent-333, score-0.246]
</p><p>80 We perform this experiment on all expressions of length 5, considering all expressions with a word length between 3 and 7 as potential matches. [sent-334, score-0.116]
</p><p>81 Semantics The qualitative analysis and the experiment on compounds demonstrate that the CCAE models are capable of learning semantics. [sent-340, score-0.124]
</p><p>82 An advantage of our approach—and of autoencoders generally—is their ability to learn in an unsupervised setting. [sent-341, score-0.201]
</p><p>83 The pre-training step for the sentiment task was essentially the same training step as used in the compound similarity task. [sent-342, score-0.205]
</p><p>84 This prevents the possiblity of pretraining, which we showed to have a big impact on results, and further prevents the training of general models: The CCAE models can be used for multiple tasks without the need to re-train the main model. [sent-345, score-0.121]
</p><p>85 By using a grammar formalism we increase the expressive power of the model while the complexity remains low. [sent-347, score-0.143]
</p><p>86 For instance model learning could be adjusted to enforce some mirroring effects between the weight matrices of forward and backward application, or to support similarities between those of forward application and composition. [sent-367, score-0.176]
</p><p>87 CCG-Vector Interface  Exactly how the infor-  mation contained in a CCG derivation is best applied to a vector space model of compositionality is another issue for future research. [sent-368, score-0.147]
</p><p>88 While CCAE-D incorporated the deepest conditioning on the CCG structure, it did not decisively outperform the simpler CCAE-B which just conditioned on the combinatory operators. [sent-370, score-0.339]
</p><p>89 7  Conclusion  In this paper we have brought a more formal notion of semantic compositionality to vector space models based on recursive autoencoders. [sent-372, score-0.389]
</p><p>90 This was achieved through the use of the CCG formalism to provide a conditioning structure for the matrix vector products that define the RAE. [sent-373, score-0.183]
</p><p>91 We have explored a number of models, each of which conditions the compositional operations on different aspects of the CCG derivation. [sent-374, score-0.135]
</p><p>92 Our experimental findings indicate a clear advantage for  a deeper integration of syntax over models that use only the bracketing structure of the parse tree. [sent-375, score-0.149]
</p><p>93 The most effective way to condition the compositional operators on the syntax remains unclear. [sent-376, score-0.219]
</p><p>94 Once the issue of sparsity had been addressed, the complex models outperformed the simpler ones. [sent-377, score-0.142]
</p><p>95 This paper represents one step towards the reconciliation of traditional formal approaches to compositional semantics with modern machine learning. [sent-380, score-0.24]
</p><p>96 Widecoverage efficient statistical parsing with ccg and log-linear models. [sent-403, score-0.406]
</p><p>97 Experimental support for a categorical compositional distributional model of meaning. [sent-431, score-0.208]
</p><p>98 Dependency tree-based sentiment classification using crfs with hidden variables. [sent-488, score-0.117]
</p><p>99 Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales. [sent-492, score-0.117]
</p><p>100 Baselines and bigrams: simple, good sentiment and topic classification. [sent-556, score-0.117]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ccg', 0.406), ('socher', 0.267), ('autoencoder', 0.265), ('rae', 0.265), ('combinatory', 0.256), ('autoencoders', 0.165), ('ccae', 0.163), ('recursive', 0.157), ('composition', 0.152), ('compositional', 0.135), ('sentiment', 0.117), ('mpqa', 0.113), ('embeddings', 0.11), ('compositionality', 0.11), ('blacoe', 0.108), ('dcl', 0.1), ('sp', 0.099), ('compound', 0.088), ('categorial', 0.084), ('brec', 0.081), ('xky', 0.081), ('likes', 0.078), ('representations', 0.076), ('distributional', 0.073), ('reconstruction', 0.073), ('pretraining', 0.072), ('grefenstette', 0.069), ('encoding', 0.068), ('semantics', 0.067), ('formalism', 0.067), ('montague', 0.066), ('subsequently', 0.065), ('benc', 0.061), ('fenc', 0.061), ('frec', 0.061), ('tigers', 0.061), ('transparent', 0.059), ('matrices', 0.059), ('sparsity', 0.058), ('expressions', 0.058), ('np', 0.055), ('lapata', 0.055), ('frege', 0.054), ('lecun', 0.054), ('syntax', 0.051), ('parse', 0.051), ('turian', 0.05), ('hinton', 0.049), ('backpropagation', 0.047), ('models', 0.047), ('conditioning', 0.046), ('tina', 0.045), ('zamparelli', 0.045), ('forward', 0.042), ('baroni', 0.041), ('vectors', 0.041), ('sadrzadeh', 0.041), ('elbl', 0.041), ('erec', 0.041), ('funnel', 0.041), ('iasre', 0.041), ('parametrizing', 0.041), ('wenc', 0.041), ('wencxi', 0.041), ('wrecei', 0.041), ('xnkyn', 0.041), ('grammar', 0.04), ('qualitative', 0.04), ('nakagawa', 0.04), ('steedman', 0.039), ('mitchell', 0.038), ('formal', 0.038), ('vector', 0.037), ('simpler', 0.037), ('prevents', 0.037), ('compounds', 0.037), ('learn', 0.036), ('ei', 0.036), ('neural', 0.036), ('sch', 0.036), ('xi', 0.036), ('tangent', 0.036), ('hyperbolic', 0.036), ('expressive', 0.036), ('greedy', 0.036), ('rn', 0.035), ('formalisms', 0.034), ('karl', 0.034), ('backward', 0.033), ('operators', 0.033), ('goller', 0.033), ('semantically', 0.033), ('matrix', 0.033), ('tze', 0.033), ('curran', 0.033), ('deep', 0.032), ('moritz', 0.031), ('basile', 0.031), ('ccgbank', 0.031), ('strike', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="347-tfidf-1" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>Author: Karl Moritz Hermann ; Phil Blunsom</p><p>Abstract: Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of Natural Language Processing. In this paper we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by Combinatory Categorial Grammar to introduce Combinatory Categorial Autoencoders. This model leverages the CCG combinatory operators to guide a non-linear transformation of meaning within a sentence. We use this model to learn high dimensional embeddings for sentences and evaluate them in a range of tasks, demonstrating that the incorporation of syntax allows a concise model to learn representations that are both effective and general.</p><p>2 0.32539806 <a title="347-tfidf-2" href="./acl-2013-Integrating_Multiple_Dependency_Corpora_for_Inducing_Wide-coverage_Japanese_CCG_Resources.html">199 acl-2013-Integrating Multiple Dependency Corpora for Inducing Wide-coverage Japanese CCG Resources</a></p>
<p>Author: Sumire Uematsu ; Takuya Matsuzaki ; Hiroki Hanaoka ; Yusuke Miyao ; Hideki Mima</p><p>Abstract: This paper describes a method of inducing wide-coverage CCG resources for Japanese. While deep parsers with corpusinduced grammars have been emerging for some languages, those for Japanese have not been widely studied, mainly because most Japanese syntactic resources are dependency-based. Our method first integrates multiple dependency-based corpora into phrase structure trees and then converts the trees into CCG derivations. The method is empirically evaluated in terms of the coverage of the obtained lexi- con and the accuracy of parsing.</p><p>3 0.26923689 <a title="347-tfidf-3" href="./acl-2013-Using_CCG_categories_to_improve_Hindi_dependency_parsing.html">372 acl-2013-Using CCG categories to improve Hindi dependency parsing</a></p>
<p>Author: Bharat Ram Ambati ; Tejaswini Deoskar ; Mark Steedman</p><p>Abstract: We show that informative lexical categories from a strongly lexicalised formalism such as Combinatory Categorial Grammar (CCG) can improve dependency parsing of Hindi, a free word order language. We first describe a novel way to obtain a CCG lexicon and treebank from an existing dependency treebank, using a CCG parser. We use the output of a supertagger trained on the CCGbank as a feature for a state-of-the-art Hindi dependency parser (Malt). Our results show that using CCG categories improves the accuracy of Malt on long distance dependencies, for which it is known to have weak rates of recovery.</p><p>4 0.23384713 <a title="347-tfidf-4" href="./acl-2013-Transfer_Learning_for_Constituency-Based_Grammars.html">357 acl-2013-Transfer Learning for Constituency-Based Grammars</a></p>
<p>Author: Yuan Zhang ; Regina Barzilay ; Amir Globerson</p><p>Abstract: In this paper, we consider the problem of cross-formalism transfer in parsing. We are interested in parsing constituencybased grammars such as HPSG and CCG using a small amount of data specific for the target formalism, and a large quantity of coarse CFG annotations from the Penn Treebank. While all of the target formalisms share a similar basic syntactic structure with Penn Treebank CFG, they also encode additional constraints and semantic features. To handle this apparent discrepancy, we design a probabilistic model that jointly generates CFG and target formalism parses. The model includes features of both parses, allowing trans- fer between the formalisms, while preserving parsing efficiency. We evaluate our approach on three constituency-based grammars CCG, HPSG, and LFG, augmented with the Penn Treebank-1. Our experiments show that across all three formalisms, the target parsers significantly benefit from the coarse annotations.1 —</p><p>5 0.17578694 <a title="347-tfidf-5" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>Author: Richard Socher ; John Bauer ; Christopher D. Manning ; Ng Andrew Y.</p><p>Abstract: Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%. It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.</p><p>6 0.17159399 <a title="347-tfidf-6" href="./acl-2013-Re-embedding_words.html">294 acl-2013-Re-embedding words</a></p>
<p>7 0.17083524 <a title="347-tfidf-7" href="./acl-2013-Compositional-ly_Derived_Representations_of_Morphologically_Complex_Words_in_Distributional_Semantics.html">87 acl-2013-Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics</a></p>
<p>8 0.16930905 <a title="347-tfidf-8" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>9 0.15249784 <a title="347-tfidf-9" href="./acl-2013-A_relatedness_benchmark_to_test_the_role_of_determiners_in_compositional_distributional_semantics.html">32 acl-2013-A relatedness benchmark to test the role of determiners in compositional distributional semantics</a></p>
<p>10 0.14093083 <a title="347-tfidf-10" href="./acl-2013-DISSECT_-_DIStributional_SEmantics_Composition_Toolkit.html">103 acl-2013-DISSECT - DIStributional SEmantics Composition Toolkit</a></p>
<p>11 0.13300665 <a title="347-tfidf-11" href="./acl-2013-Semantic_Parsing_with_Combinatory_Categorial_Grammars.html">313 acl-2013-Semantic Parsing with Combinatory Categorial Grammars</a></p>
<p>12 0.13246627 <a title="347-tfidf-12" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>13 0.10317683 <a title="347-tfidf-13" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>14 0.099175707 <a title="347-tfidf-14" href="./acl-2013-Learning_Entity_Representation_for_Entity_Disambiguation.html">219 acl-2013-Learning Entity Representation for Entity Disambiguation</a></p>
<p>15 0.092082947 <a title="347-tfidf-15" href="./acl-2013-Measuring_semantic_content_in_distributional_vectors.html">238 acl-2013-Measuring semantic content in distributional vectors</a></p>
<p>16 0.09106648 <a title="347-tfidf-16" href="./acl-2013-Identifying_Sentiment_Words_Using_an_Optimization-based_Model_without_Seed_Words.html">188 acl-2013-Identifying Sentiment Words Using an Optimization-based Model without Seed Words</a></p>
<p>17 0.090694986 <a title="347-tfidf-17" href="./acl-2013-Semantic_Parsing_as_Machine_Translation.html">312 acl-2013-Semantic Parsing as Machine Translation</a></p>
<p>18 0.086646698 <a title="347-tfidf-18" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>19 0.08045654 <a title="347-tfidf-19" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>20 0.071661338 <a title="347-tfidf-20" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.207), (1, 0.056), (2, -0.059), (3, -0.008), (4, -0.227), (5, -0.078), (6, 0.06), (7, 0.003), (8, 0.031), (9, 0.098), (10, 0.012), (11, -0.029), (12, 0.319), (13, -0.172), (14, -0.117), (15, 0.131), (16, -0.012), (17, -0.026), (18, -0.199), (19, -0.011), (20, 0.039), (21, -0.2), (22, -0.259), (23, -0.004), (24, 0.095), (25, -0.024), (26, -0.0), (27, 0.085), (28, -0.012), (29, 0.004), (30, 0.105), (31, -0.003), (32, 0.01), (33, 0.028), (34, -0.02), (35, 0.032), (36, -0.024), (37, 0.054), (38, -0.056), (39, 0.129), (40, 0.02), (41, -0.003), (42, -0.001), (43, 0.042), (44, 0.026), (45, 0.033), (46, 0.026), (47, -0.033), (48, 0.014), (49, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91909659 <a title="347-lsi-1" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>Author: Karl Moritz Hermann ; Phil Blunsom</p><p>Abstract: Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of Natural Language Processing. In this paper we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by Combinatory Categorial Grammar to introduce Combinatory Categorial Autoencoders. This model leverages the CCG combinatory operators to guide a non-linear transformation of meaning within a sentence. We use this model to learn high dimensional embeddings for sentences and evaluate them in a range of tasks, demonstrating that the incorporation of syntax allows a concise model to learn representations that are both effective and general.</p><p>2 0.75350177 <a title="347-lsi-2" href="./acl-2013-Integrating_Multiple_Dependency_Corpora_for_Inducing_Wide-coverage_Japanese_CCG_Resources.html">199 acl-2013-Integrating Multiple Dependency Corpora for Inducing Wide-coverage Japanese CCG Resources</a></p>
<p>Author: Sumire Uematsu ; Takuya Matsuzaki ; Hiroki Hanaoka ; Yusuke Miyao ; Hideki Mima</p><p>Abstract: This paper describes a method of inducing wide-coverage CCG resources for Japanese. While deep parsers with corpusinduced grammars have been emerging for some languages, those for Japanese have not been widely studied, mainly because most Japanese syntactic resources are dependency-based. Our method first integrates multiple dependency-based corpora into phrase structure trees and then converts the trees into CCG derivations. The method is empirically evaluated in terms of the coverage of the obtained lexi- con and the accuracy of parsing.</p><p>3 0.68549287 <a title="347-lsi-3" href="./acl-2013-Semantic_Parsing_with_Combinatory_Categorial_Grammars.html">313 acl-2013-Semantic Parsing with Combinatory Categorial Grammars</a></p>
<p>Author: Yoav Artzi ; Nicholas FitzGerald ; Luke Zettlemoyer</p><p>Abstract: unkown-abstract</p><p>4 0.6620912 <a title="347-lsi-4" href="./acl-2013-Using_CCG_categories_to_improve_Hindi_dependency_parsing.html">372 acl-2013-Using CCG categories to improve Hindi dependency parsing</a></p>
<p>Author: Bharat Ram Ambati ; Tejaswini Deoskar ; Mark Steedman</p><p>Abstract: We show that informative lexical categories from a strongly lexicalised formalism such as Combinatory Categorial Grammar (CCG) can improve dependency parsing of Hindi, a free word order language. We first describe a novel way to obtain a CCG lexicon and treebank from an existing dependency treebank, using a CCG parser. We use the output of a supertagger trained on the CCGbank as a feature for a state-of-the-art Hindi dependency parser (Malt). Our results show that using CCG categories improves the accuracy of Malt on long distance dependencies, for which it is known to have weak rates of recovery.</p><p>5 0.64042133 <a title="347-lsi-5" href="./acl-2013-Transfer_Learning_for_Constituency-Based_Grammars.html">357 acl-2013-Transfer Learning for Constituency-Based Grammars</a></p>
<p>Author: Yuan Zhang ; Regina Barzilay ; Amir Globerson</p><p>Abstract: In this paper, we consider the problem of cross-formalism transfer in parsing. We are interested in parsing constituencybased grammars such as HPSG and CCG using a small amount of data specific for the target formalism, and a large quantity of coarse CFG annotations from the Penn Treebank. While all of the target formalisms share a similar basic syntactic structure with Penn Treebank CFG, they also encode additional constraints and semantic features. To handle this apparent discrepancy, we design a probabilistic model that jointly generates CFG and target formalism parses. The model includes features of both parses, allowing trans- fer between the formalisms, while preserving parsing efficiency. We evaluate our approach on three constituency-based grammars CCG, HPSG, and LFG, augmented with the Penn Treebank-1. Our experiments show that across all three formalisms, the target parsers significantly benefit from the coarse annotations.1 —</p><p>6 0.53142709 <a title="347-lsi-6" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>7 0.52909571 <a title="347-lsi-7" href="./acl-2013-DISSECT_-_DIStributional_SEmantics_Composition_Toolkit.html">103 acl-2013-DISSECT - DIStributional SEmantics Composition Toolkit</a></p>
<p>8 0.50394565 <a title="347-lsi-8" href="./acl-2013-A_relatedness_benchmark_to_test_the_role_of_determiners_in_compositional_distributional_semantics.html">32 acl-2013-A relatedness benchmark to test the role of determiners in compositional distributional semantics</a></p>
<p>9 0.46908906 <a title="347-lsi-9" href="./acl-2013-Re-embedding_words.html">294 acl-2013-Re-embedding words</a></p>
<p>10 0.45464444 <a title="347-lsi-10" href="./acl-2013-Compositional-ly_Derived_Representations_of_Morphologically_Complex_Words_in_Distributional_Semantics.html">87 acl-2013-Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics</a></p>
<p>11 0.44542575 <a title="347-lsi-11" href="./acl-2013-The_mathematics_of_language_learning.html">349 acl-2013-The mathematics of language learning</a></p>
<p>12 0.44274697 <a title="347-lsi-12" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>13 0.40150389 <a title="347-lsi-13" href="./acl-2013-Leveraging_Domain-Independent_Information_in_Semantic_Parsing.html">228 acl-2013-Leveraging Domain-Independent Information in Semantic Parsing</a></p>
<p>14 0.36734694 <a title="347-lsi-14" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>15 0.34111282 <a title="347-lsi-15" href="./acl-2013-Measuring_semantic_content_in_distributional_vectors.html">238 acl-2013-Measuring semantic content in distributional vectors</a></p>
<p>16 0.33164385 <a title="347-lsi-16" href="./acl-2013-A_corpus-based_evaluation_method_for_Distributional_Semantic_Models.html">31 acl-2013-A corpus-based evaluation method for Distributional Semantic Models</a></p>
<p>17 0.32635245 <a title="347-lsi-17" href="./acl-2013-Grounded_Unsupervised_Semantic_Parsing.html">176 acl-2013-Grounded Unsupervised Semantic Parsing</a></p>
<p>18 0.3208811 <a title="347-lsi-18" href="./acl-2013-Fluid_Construction_Grammar_for_Historical_and_Evolutionary_Linguistics.html">161 acl-2013-Fluid Construction Grammar for Historical and Evolutionary Linguistics</a></p>
<p>19 0.313595 <a title="347-lsi-19" href="./acl-2013-Semantic_Neighborhoods_as_Hypergraphs.html">311 acl-2013-Semantic Neighborhoods as Hypergraphs</a></p>
<p>20 0.31127125 <a title="347-lsi-20" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.042), (1, 0.183), (6, 0.027), (11, 0.059), (15, 0.024), (24, 0.047), (26, 0.044), (28, 0.011), (35, 0.105), (42, 0.084), (48, 0.089), (63, 0.016), (64, 0.011), (67, 0.018), (70, 0.041), (88, 0.047), (90, 0.024), (95, 0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84139013 <a title="347-lda-1" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>Author: Karl Moritz Hermann ; Phil Blunsom</p><p>Abstract: Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of Natural Language Processing. In this paper we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by Combinatory Categorial Grammar to introduce Combinatory Categorial Autoencoders. This model leverages the CCG combinatory operators to guide a non-linear transformation of meaning within a sentence. We use this model to learn high dimensional embeddings for sentences and evaluate them in a range of tasks, demonstrating that the incorporation of syntax allows a concise model to learn representations that are both effective and general.</p><p>2 0.82866031 <a title="347-lda-2" href="./acl-2013-Parsing_Graphs_with_Hyperedge_Replacement_Grammars.html">274 acl-2013-Parsing Graphs with Hyperedge Replacement Grammars</a></p>
<p>Author: David Chiang ; Jacob Andreas ; Daniel Bauer ; Karl Moritz Hermann ; Bevan Jones ; Kevin Knight</p><p>Abstract: Hyperedge replacement grammar (HRG) is a formalism for generating and transforming graphs that has potential applications in natural language understanding and generation. A recognition algorithm due to Lautemann is known to be polynomial-time for graphs that are connected and of bounded degree. We present a more precise characterization of the algorithm’s complexity, an optimization analogous to binarization of contextfree grammars, and some important implementation details, resulting in an algorithm that is practical for natural-language applications. The algorithm is part of Bolinas, a new software toolkit for HRG processing.</p><p>3 0.71536911 <a title="347-lda-3" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>Author: Richard Socher ; John Bauer ; Christopher D. Manning ; Ng Andrew Y.</p><p>Abstract: Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%. It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.</p><p>4 0.70893556 <a title="347-lda-4" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>Author: Jiwei Tan ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Ordering texts is an important task for many NLP applications. Most previous works on summary sentence ordering rely on the contextual information (e.g. adjacent sentences) of each sentence in the source document. In this paper, we investigate a more challenging task of ordering a set of unordered sentences without any contextual information. We introduce a set of features to characterize the order and coherence of natural language texts, and use the learning to rank technique to determine the order of any two sentences. We also propose to use the genetic algorithm to determine the total order of all sentences. Evaluation results on a news corpus show the effectiveness of our proposed method. 1</p><p>5 0.70705068 <a title="347-lda-5" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>Author: Camille Guinaudeau ; Michael Strube</p><p>Abstract: We propose a computationally efficient graph-based approach for local coherence modeling. We evaluate our system on three tasks: sentence ordering, summary coherence rating and readability assessment. The performance is comparable to entity grid based approaches though these rely on a computationally expensive training phase and face data sparsity problems.</p><p>6 0.70608038 <a title="347-lda-6" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>7 0.70433629 <a title="347-lda-7" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>8 0.70313352 <a title="347-lda-8" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>9 0.70096153 <a title="347-lda-9" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>10 0.69915289 <a title="347-lda-10" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>11 0.69888651 <a title="347-lda-11" href="./acl-2013-Identifying_Sentiment_Words_Using_an_Optimization-based_Model_without_Seed_Words.html">188 acl-2013-Identifying Sentiment Words Using an Optimization-based Model without Seed Words</a></p>
<p>12 0.69288754 <a title="347-lda-12" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>13 0.69279152 <a title="347-lda-13" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>14 0.69268179 <a title="347-lda-14" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>15 0.69266742 <a title="347-lda-15" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>16 0.69232774 <a title="347-lda-16" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>17 0.69191068 <a title="347-lda-17" href="./acl-2013-An_Information_Theoretic_Approach_to_Bilingual_Word_Clustering.html">47 acl-2013-An Information Theoretic Approach to Bilingual Word Clustering</a></p>
<p>18 0.69169778 <a title="347-lda-18" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>19 0.69139707 <a title="347-lda-19" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>20 0.69107002 <a title="347-lda-20" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
