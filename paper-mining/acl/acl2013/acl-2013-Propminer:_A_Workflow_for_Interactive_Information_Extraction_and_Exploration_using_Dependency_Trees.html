<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>285 acl-2013-Propminer: A Workflow for Interactive Information Extraction and Exploration using Dependency Trees</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-285" href="#">acl2013-285</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>285 acl-2013-Propminer: A Workflow for Interactive Information Extraction and Exploration using Dependency Trees</h1>
<br/><p>Source: <a title="acl-2013-285-pdf" href="http://aclweb.org/anthology//P/P13/P13-4027.pdf">pdf</a></p><p>Author: Alan Akbik ; Oresti Konomi ; Michail Melnikov</p><p>Abstract: The use ofdeep syntactic information such as typed dependencies has been shown to be very effective in Information Extraction. Despite this potential, the process of manually creating rule-based information extractors that operate on dependency trees is not intuitive for persons without an extensive NLP background. In this system demonstration, we present a tool and a workflow designed to enable initiate users to interactively explore the effect and expressivity of creating Information Extraction rules over dependency trees. We introduce the proposed five step workflow for creating information extractors, the graph query based rule language, as well as the core features of the PROP- MINER tool.</p><p>Reference: <a title="acl-2013-285-reference" href="../acl2013_reference/acl-2013-Propminer%3A_A_Workflow_for_Interactive_Information_Extraction_and_Exploration_using_Dependency_Trees_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Despite this potential, the process of manually creating rule-based information extractors that operate on dependency trees is not intuitive for persons without an extensive NLP background. [sent-4, score-0.293]
</p><p>2 In this system demonstration, we present a tool and a workflow designed to enable initiate users to interactively explore the effect and expressivity of creating Information Extraction rules over dependency trees. [sent-5, score-0.663]
</p><p>3 We introduce the proposed five step workflow for creating information extractors, the graph query based rule language, as well as the core features of the PROP-  MINER tool. [sent-6, score-0.73]
</p><p>4 Advantages of such systems include a better transparency and explainability of extraction rules, and the resulting maintainability and customizability of rule sets. [sent-12, score-0.466]
</p><p>5 Another trend in IE is to make increasing use of deep syntactic information in extractors (Bunescu and Mooney, 2005), as dependency parsers become faster and more robust on irregular text (Bohnet, 2010). [sent-13, score-0.197]
</p><p>6 The systems KRAKEN (Akbik and L ¨oser, 2012) and CLAUSIE (Del Corro and Gemulla, ) use a set of hand crafted rules on dependency trees to outperform previous classification based approaches. [sent-15, score-0.171]
</p><p>7 The higher level syntactic representation, we argue, may even facilitate rule writing, as - unlike in shallow lexicosyntactic rules - much linguistic variation such as inserted clauses and expressions must not be specifically addressed. [sent-22, score-0.393]
</p><p>8 In this system demonstration, we propose a workflow designed to tap into this potential, and present the PROPMINER tool that allows users to execute this workflow. [sent-25, score-0.425]
</p><p>9 It is specifically designed to help persons familiarize themselves with dependency trees and enable exploration and extraction of relations from parsed document collections. [sent-26, score-0.36]
</p><p>10 Initiate users are guided by a workflow in which they first enter and annotate an archetypical sentence with the desired relation. [sent-28, score-0.72]
</p><p>11 A rule generation process then pre-generates an overspecified rule that users modify along lines suggested by the tool. [sent-29, score-0.855]
</p><p>12 Our preliminary experiments show that this workflow of generating and modifying rules eases the learning curve for non NLP-experts to concepts such as part-of-speech tags and typed dependencies. [sent-30, score-0.549]
</p><p>13 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 157–162,  Figure 1: Sentence view of PROPMINER, where steps one and two of the workflow are executed. [sent-33, score-0.308]
</p><p>14 Users enter (or select) a sentence in the top input field and annotate subject, predicate and object for the desired relation. [sent-34, score-0.43]
</p><p>15 A rule is generated and displayed in the upper right panel. [sent-35, score-0.364]
</p><p>16 Each modification of a rule is immediately queried against a large collection of parsed sentences stored in a distributed graph database. [sent-39, score-0.448]
</p><p>17 The extraction results of the current state of the rule are presented at all times to the user, thereby explaining the rule by showing its effect. [sent-40, score-0.709]
</p><p>18 Extraction rules are formulated as queries against a graph database. [sent-42, score-0.13]
</p><p>19 Our query language allows users to formulate subtree queries as path expressions, a concept borrowed from the SerQL query language (Broekstra and Kampman, 2003) because of its intuitive prop-  erties. [sent-43, score-0.506]
</p><p>20 We show a visualization of the parse tree of the current sentence next to the generated rule to ease users into understanding the query language (see Figure 1). [sent-44, score-0.573]
</p><p>21 All structured information generated by the user, such as extraction rules, sentence annotations and evaluation results, are stored to build up a repository of structured information. [sent-46, score-0.261]
</p><p>22 We noted that users at first stay true to the workflow and limit manual effort to generalizing rules, but tend to more directly modify extraction rules as they grow more experienced. [sent-49, score-0.63]
</p><p>23 Furthermore, PROPMINER’s interactive nature eases the process of understanding typed dependencies and enables the interactive exploration of parsed document collections. [sent-50, score-0.388]
</p><p>24 2  Workflow and Query Language  PROPMINER implements a workflow that consists of five steps (Annotate, Generate, Generalize, Evaluate and Store). [sent-51, score-0.269]
</p><p>25 It is designed to allow users that are unfamiliar with syntactic annotation to create rule-based extractors. [sent-52, score-0.156]
</p><p>26 1 Annotate Users begin the process by constructing an archetypical sentence for the desired information type. [sent-56, score-0.211]
</p><p>27 For instance, a user interested in the PERSONBIRTHPLACE relation can choose a sentence such as “Albert Einstein was born in Germany. [sent-58, score-0.482]
</p><p>28 In this sentence, the user annotates the words 158  belonging to the relation triple, assigning the roles of subject, predicate and object. [sent-60, score-0.271]
</p><p>29 Subject and object are the entities in the example between which  the relation holds. [sent-61, score-0.23]
</p><p>30 The predicate are the words in the sentence that express the relationship. [sent-62, score-0.144]
</p><p>31 Accordingly, the user marks “Albert Einstein” and “Germany” as subject and object, and “born in” as predicate in the example sentence. [sent-63, score-0.345]
</p><p>32 Figure 1 shows the sentence view of PROPMINER, with the example sentence entered and annotated in the top input fields, and the parsed sentence shown in the center panel. [sent-64, score-0.235]
</p><p>33 2 Generate PROPMINER generates a rule from the annotated sentence by determining the minimal subtree in the sentence’s dependency tree that connects all words labeled as subject, predicate and object. [sent-66, score-0.603]
</p><p>34 The rule consists of this minimal subtree, as well as constraints in the part-of-speech (POS) tags and lexical values of all involved words. [sent-67, score-0.3]
</p><p>35 Rules are formulated as queries against a database in which parsed sentences are stored as graphs: Nodes represent words and edges represent typed dependencies. [sent-68, score-0.234]
</p><p>36 A PROPMINER rule (or query) consists mainly of three parts: A SELECT clause, a FROM clause and a WHERE clause. [sent-70, score-0.357]
</p><p>37 The generated rule for the running example is displayed in Figure 1. [sent-71, score-0.407]
</p><p>38 The FROM clause is a path expression that specifies the subgraph in the dependency tree the rule must match, and defines which nodes in the subgraph correspond to the fields in the SELECT clause. [sent-77, score-0.647]
</p><p>39 Each of these triples defines one edge (typed dependency) that must hold between two nodes (words). [sent-79, score-0.154]
</p><p>40 The nodes are denoted in curly brackets, where the text inside curly brackets assigns a variable name to the node. [sent-80, score-0.158]
</p><p>41 Consider the SELECT and FROM clauses for  the rule generated for the running example, illustrated in the following: SELECT subject, FROM {predicate. [sent-81, score-0.343]
</p><p>42 4}, pobj {object}  Here, the SELECT statement defines the desired result of this query, namely a tuple with a “subject”, “object” and a “predicate” field: The path expression in this example is specified in the three lines in the FROM statement. [sent-85, score-0.209]
</p><p>43 It defines a subtree that consists of four nodes connected by three typed dependencies. [sent-86, score-0.265]
</p><p>44 The node “subject” is defined to be a passive subject (typed dependency “nsubjpass”) of the node “predicate. [sent-90, score-0.296]
</p><p>45 If this rule matches, the lexical values of the matching nodes are returned. [sent-95, score-0.358]
</p><p>46 Because the predicate in this example consists of two words (“born” and “in”), two nodes are assigned the “predicate” value, subtyped per convention with a dot and a number (“predicate. [sent-96, score-0.159]
</p><p>47 The rule for the running example is initially restricted as follows:  WHERE AND AND AND AND AND AND AND AND AND  subject predicate. [sent-103, score-0.477]
</p><p>48 4 object subject  POS “NNP” POS “VBN” POS “IN” POS “NNP” TEXT “Einstein” TEXT “born” TEXT “in” TEXT “Germany” FULL_ENTITY  Word attributes are restricted by naming the variable followed either by “POS” or “TEXT” and the restricting value. [sent-107, score-0.254]
</p><p>49 159  a) Generated rule  b) Generalize subject text  SWFA NREHD LOEMRopsCruEbeT jdseictuabjec. [sent-109, score-0.434]
</p><p>50 {c34t,opPlT raOeE pdXS sicTeadt“} ,IVNE BoiNn”ZbPsjet”cin  A AN ND Dop surbe jbde j ic tca t e . [sent-111, score-0.171]
</p><p>51 34 T ATEL EX LXCT HI“L ibGnDo”eRrnEm”Nany” A AN ND Dop surbe jbde j ic tca t e . [sent-112, score-0.171]
</p><p>52 Einstein  born in  Germany  SubjectPredicateObject A. [sent-114, score-0.269]
</p><p>53 c) Generalize subject and object  SubjectPredicateObject A. [sent-127, score-0.254]
</p><p>54 Figure 2: Conceptual example of rule modification through generalization. [sent-139, score-0.339]
</p><p>55 Rule a) is generated from the annotated sentence in the running example, and finds only one triple. [sent-141, score-0.149]
</p><p>56 Rule b) is the same rule without the restriction in the subject text. [sent-142, score-0.434]
</p><p>57 The rule  now finds a number of relation triples in the document collection, representing different entities born in Germany. [sent-143, score-0.8]
</p><p>58 In Rule c) both subject and object text restrictions  are removed. [sent-144, score-0.254]
</p><p>59 This yields a rule that finds  different entities born in any entity. [sent-145, score-0.682]
</p><p>60 For example, the keyword FULL ENTITY causes the variable binding for the subject to expand to all children nodes expected to be part of a named entity. [sent-147, score-0.192]
</p><p>61 3 Generalize The rule generated in step two of the workflow is strongly overspecified to the annotated sentence; all features, including the shallow syntactic and lexical values of all words in the subtree, are constrained. [sent-149, score-0.626]
</p><p>62 The resulting rule only finds exact instances of the relations as seen in the archetypical sentence. [sent-150, score-0.448]
</p><p>63 In step three of the workflow, the user generalizes the auto-generated rule with the help of suggestions. [sent-152, score-0.41]
</p><p>64 Common lines of generalizing rules focus on the WHERE clause; here, users can remove or modify constraints on the attributes of words. [sent-153, score-0.252]
</p><p>65 For example, by removing the restriction on the lexical value of the subject, the rule is generalized to finding all entities that were born in “Germany”, instead of only entities with the lexical value “Einstein”. [sent-154, score-0.669]
</p><p>66 The rule can then be further generalized by removing the lexical constraint on the object, yielding the (desired) rule that finds all entities that were born in any location with an entity name. [sent-156, score-0.982]
</p><p>67 At each modification, extraction results for the current state of the rule are displayed to assist the user. [sent-159, score-0.473]
</p><p>68 When the results match the desired relation, the user can proceed to the next step in the workflow. [sent-160, score-0.241]
</p><p>69 4 Evaluate Each rule created by the user is evaluated in the corpus view of PROPMINER, displayed in Fig-  ure 3. [sent-162, score-0.513]
</p><p>70 This view shows a sample of extraction results of the rule in a table. [sent-163, score-0.448]
</p><p>71 The user can scroll through the table and in each row see the extracted information as well as the sentence the information was extracted from. [sent-164, score-0.153]
</p><p>72 5 Store If the user is satisfied with the extraction rule, he can assign it to a relation and store it in the rule repository. [sent-167, score-0.579]
</p><p>73 As the workflow is repeated, the rule repository will build up, along with a repository ofevalu160  Figure 3: Corpus view of PROPMINER, where extraction rules are modified and evaluated. [sent-169, score-0.944]
</p><p>74 The center  panel is a table that holds the extraction results for the current rule. [sent-170, score-0.147]
</p><p>75 For example, a user might mark a triple with the subject “C. [sent-176, score-0.312]
</p><p>76 Gauss” and object “Germany” as a correct instance of the PERSONBIRTHPLACE relation during evaluation. [sent-178, score-0.18]
</p><p>77 Accordingly, they are suggested to the user upon request. [sent-181, score-0.149]
</p><p>78 In order to prevent conflicts with existing rules, the entire rule set in the repository is applied to each sentence the work-  flow is started with. [sent-183, score-0.41]
</p><p>79 If any existing information extraction rule can be applied, the results of the extraction are presented to the user as annotations in the sentence. [sent-184, score-0.628]
</p><p>80 If this extraction result is already complete from the point of view of the user, he can proceed to a new sentence. [sent-185, score-0.196]
</p><p>81 If not, the user can proceed to generate a new rule, or modify existing ones. [sent-186, score-0.201]
</p><p>82 3  Previous Work  Previous work on improving the rule creation process for IE systems has mainly focused on assisting users with machine learning techniques, such as pre-generation of regular expressions (Brauer et al. [sent-187, score-0.416]
</p><p>83 While previous work focuses on shallow patterns, the focus of PROPMINER is to help create rules over dependency trees and aid in the exploration of parsed document collections. [sent-192, score-0.295]
</p><p>84 4  Evaluation and Outlook  We conducted a preliminary study in which we asked 5 computer scientists unfamiliar with computational linguistics to use the tool to create extractors for the relations PERSONBIRTHPLACE, PERSONMARRIEDTOPERSON and PERSONWONPRIZE. [sent-193, score-0.159]
</p><p>85 The participants were given a two hour introduction explaining information extraction and subject-predicate-object triples. [sent-194, score-0.239]
</p><p>86 We introduced them to the five step workflow using the PERSONBIRTHPLACE example also used as running example in this paper, as well as other, more complex examples. [sent-195, score-0.312]
</p><p>87 The participants were given one hour for each relation and asked to cre161  ate a rule set for each relation. [sent-196, score-0.49]
</p><p>88 After the conclusion we interviewed the participants and asked them to rate the usability both for information extraction, as well as for the exploration of dependency tree information. [sent-197, score-0.267]
</p><p>89 Participants stated that the interactive nature of the tool helped understanding  extraction rules and facilitated exploring information stated in the document collection. [sent-199, score-0.241]
</p><p>90 4 out of 5 participants deviated from the suggested workflow and more directly edited rules as they became more comfortable with the tool. [sent-200, score-0.495]
</p><p>91 All participants consulted information on POS tags and typed dependencies during the process, in order to better understand the rules and query results. [sent-201, score-0.389]
</p><p>92 While all users were generally able to create rule sets for each of the relations, two main problems were cited for the creation of extraction rules. [sent-203, score-0.525]
</p><p>93 The first is a problem in conflict resolution; in some cases, users were not able to discern why a rule gave imperfect extraction results. [sent-204, score-0.525]
</p><p>94 We reviewed some rules and found that many of these cases stem from faulty dependency parses, which non NLP-experts cannot recognize. [sent-205, score-0.213]
</p><p>95 A second problem were limitations of the rule language: Participants expressed the need for named entity types such as PERSON and LOCA-  TION, which in the prototype were not included at the time of evaluation. [sent-207, score-0.3]
</p><p>96 Consequently, current work focuses on extending the range of user studies to gather more suggestions for the query language and the feature set, and integrating additional operators into the system. [sent-209, score-0.224]
</p><p>97 5  Demonstration  In this demonstration we show how PROPMINER can be used for creating extractors or exploring the parsed document collection. [sent-210, score-0.285]
</p><p>98 The hands-on demonstration allows initiate users to execute the workflow presented in this paper, but also enables persons more familiar with syntactic annotation to more directly query the graph database using our query language and feature set. [sent-211, score-0.855]
</p><p>99 Facilitating pattern discovery for relation extraction with semantic-signature-based clustering. [sent-253, score-0.169]
</p><p>100 Distant supervision for relation extraction without  labeled data. [sent-267, score-0.169]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('propminer', 0.426), ('rule', 0.3), ('workflow', 0.269), ('born', 0.269), ('personbirthplace', 0.17), ('einstein', 0.162), ('germany', 0.14), ('subject', 0.134), ('object', 0.12), ('extractors', 0.119), ('users', 0.116), ('query', 0.114), ('akbik', 0.114), ('user', 0.11), ('extraction', 0.109), ('predicate', 0.101), ('participants', 0.094), ('rules', 0.093), ('typed', 0.088), ('archetypical', 0.085), ('clausie', 0.085), ('subjectpredicateobject', 0.085), ('desired', 0.083), ('subtree', 0.081), ('dependency', 0.078), ('chiticariu', 0.07), ('yunyao', 0.07), ('triple', 0.068), ('repository', 0.067), ('parsed', 0.067), ('displayed', 0.064), ('finds', 0.063), ('relation', 0.06), ('initiate', 0.06), ('nodes', 0.058), ('triples', 0.058), ('clause', 0.057), ('exploration', 0.057), ('atel', 0.057), ('brauer', 0.057), ('broekstra', 0.057), ('corro', 0.057), ('eases', 0.057), ('errnem', 0.057), ('explainability', 0.057), ('gauss', 0.057), ('ibgndo', 0.057), ('jbde', 0.057), ('kraken', 0.057), ('lxct', 0.057), ('nany', 0.057), ('oie', 0.057), ('ollie', 0.057), ('overspecified', 0.057), ('serql', 0.057), ('surbe', 0.057), ('tca', 0.057), ('ie', 0.054), ('pos', 0.054), ('nnp', 0.054), ('demonstration', 0.052), ('entities', 0.05), ('dop', 0.05), ('curly', 0.05), ('doan', 0.05), ('persons', 0.049), ('proceed', 0.048), ('creating', 0.047), ('nsubjpass', 0.046), ('reiss', 0.046), ('path', 0.044), ('pobj', 0.044), ('chu', 0.044), ('sentence', 0.043), ('running', 0.043), ('annotate', 0.043), ('modify', 0.043), ('stored', 0.042), ('non', 0.042), ('node', 0.042), ('generalize', 0.042), ('enables', 0.041), ('guided', 0.041), ('select', 0.04), ('unfamiliar', 0.04), ('execute', 0.04), ('frederick', 0.04), ('enter', 0.04), ('suggested', 0.039), ('interactive', 0.039), ('modification', 0.039), ('view', 0.039), ('panel', 0.038), ('usability', 0.038), ('defines', 0.038), ('queries', 0.037), ('albert', 0.037), ('del', 0.037), ('hour', 0.036), ('subgraph', 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="285-tfidf-1" href="./acl-2013-Propminer%3A_A_Workflow_for_Interactive_Information_Extraction_and_Exploration_using_Dependency_Trees.html">285 acl-2013-Propminer: A Workflow for Interactive Information Extraction and Exploration using Dependency Trees</a></p>
<p>Author: Alan Akbik ; Oresti Konomi ; Michail Melnikov</p><p>Abstract: The use ofdeep syntactic information such as typed dependencies has been shown to be very effective in Information Extraction. Despite this potential, the process of manually creating rule-based information extractors that operate on dependency trees is not intuitive for persons without an extensive NLP background. In this system demonstration, we present a tool and a workflow designed to enable initiate users to interactively explore the effect and expressivity of creating Information Extraction rules over dependency trees. We introduce the proposed five step workflow for creating information extractors, the graph query based rule language, as well as the core features of the PROP- MINER tool.</p><p>2 0.15547436 <a title="285-tfidf-2" href="./acl-2013-A_Two_Level_Model_for_Context_Sensitive_Inference_Rules.html">27 acl-2013-A Two Level Model for Context Sensitive Inference Rules</a></p>
<p>Author: Oren Melamud ; Jonathan Berant ; Ido Dagan ; Jacob Goldberger ; Idan Szpektor</p><p>Abstract: Automatic acquisition of inference rules for predicates has been commonly addressed by computing distributional similarity between vectors of argument words, operating at the word space level. A recent line of work, which addresses context sensitivity of rules, represented contexts in a latent topic space and computed similarity over topic vectors. We propose a novel two-level model, which computes similarities between word-level vectors that are biased by topic-level context representations. Evaluations on a naturallydistributed dataset show that our model significantly outperforms prior word-level and topic-level models. We also release a first context-sensitive inference rule set.</p><p>3 0.13523395 <a title="285-tfidf-3" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<p>Author: Marzieh Bazrafshan ; Daniel Gildea</p><p>Abstract: We experiment with adding semantic role information to a string-to-tree machine translation system based on the rule extraction procedure of Galley et al. (2004). We compare methods based on augmenting the set of nonterminals by adding semantic role labels, and altering the rule extraction process to produce a separate set of rules for each predicate that encompass its entire predicate-argument structure. Our results demonstrate that the second approach is effective in increasing the quality of translations.</p><p>4 0.12671794 <a title="285-tfidf-4" href="./acl-2013-Extending_an_interoperable_platform_to_facilitate_the_creation_of_multilingual_and_multimodal_NLP_applications.html">150 acl-2013-Extending an interoperable platform to facilitate the creation of multilingual and multimodal NLP applications</a></p>
<p>Author: Georgios Kontonatsios ; Paul Thompson ; Riza Theresa Batista-Navarro ; Claudiu Mihaila ; Ioannis Korkontzelos ; Sophia Ananiadou</p><p>Abstract: U-Compare is a UIMA-based workflow construction platform for building natural language processing (NLP) applications from heterogeneous language resources (LRs), without the need for programming skills. U-Compare has been adopted within the context of the METANET Network of Excellence, and over 40 LRs that process 15 European languages have been added to the U-Compare component library. In line with METANET’s aims of increasing communication between citizens of different European countries, U-Compare has been extended to facilitate the development of a wider range of applications, including both mul- tilingual and multimodal workflows. The enhancements exploit the UIMA Subject of Analysis (Sofa) mechanism, that allows different facets of the input data to be represented. We demonstrate how our customised extensions to U-Compare allow the construction and testing of NLP applications that transform the input data in different ways, e.g., machine translation, automatic summarisation and text-to-speech.</p><p>5 0.11074948 <a title="285-tfidf-5" href="./acl-2013-ICARUS_-_An_Extensible_Graphical_Search_Tool_for_Dependency_Treebanks.html">183 acl-2013-ICARUS - An Extensible Graphical Search Tool for Dependency Treebanks</a></p>
<p>Author: Markus Gartner ; Gregor Thiele ; Wolfgang Seeker ; Anders Bjorkelund ; Jonas Kuhn</p><p>Abstract: We present ICARUS, a versatile graphical search tool to query dependency treebanks. Search results can be inspected both quantitatively and qualitatively by means of frequency lists, tables, or dependency graphs. ICARUS also ships with plugins that enable it to interface with tool chains running either locally or remotely.</p><p>6 0.10468567 <a title="285-tfidf-6" href="./acl-2013-Development_and_Analysis_of_NLP_Pipelines_in_Argo.html">118 acl-2013-Development and Analysis of NLP Pipelines in Argo</a></p>
<p>7 0.086287685 <a title="285-tfidf-7" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>8 0.07962171 <a title="285-tfidf-8" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>9 0.078791156 <a title="285-tfidf-9" href="./acl-2013-Using_Lexical_Expansion_to_Learn_Inference_Rules_from_Sparse_Data.html">376 acl-2013-Using Lexical Expansion to Learn Inference Rules from Sparse Data</a></p>
<p>10 0.07781174 <a title="285-tfidf-10" href="./acl-2013-SPred%3A_Large-scale_Harvesting_of_Semantic_Predicates.html">306 acl-2013-SPred: Large-scale Harvesting of Semantic Predicates</a></p>
<p>11 0.076272994 <a title="285-tfidf-11" href="./acl-2013-ParaQuery%3A_Making_Sense_of_Paraphrase_Collections.html">271 acl-2013-ParaQuery: Making Sense of Paraphrase Collections</a></p>
<p>12 0.076110125 <a title="285-tfidf-12" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>13 0.074470229 <a title="285-tfidf-13" href="./acl-2013-Parsing_Graphs_with_Hyperedge_Replacement_Grammars.html">274 acl-2013-Parsing Graphs with Hyperedge Replacement Grammars</a></p>
<p>14 0.072020642 <a title="285-tfidf-14" href="./acl-2013-Stem_Translation_with_Affix-Based_Rule_Selection_for_Agglutinative_Languages.html">330 acl-2013-Stem Translation with Affix-Based Rule Selection for Agglutinative Languages</a></p>
<p>15 0.069922335 <a title="285-tfidf-15" href="./acl-2013-Paraphrase-Driven_Learning_for_Open_Question_Answering.html">272 acl-2013-Paraphrase-Driven Learning for Open Question Answering</a></p>
<p>16 0.067824587 <a title="285-tfidf-16" href="./acl-2013-Lightly_Supervised_Learning_of_Procedural_Dialog_Systems.html">230 acl-2013-Lightly Supervised Learning of Procedural Dialog Systems</a></p>
<p>17 0.067433141 <a title="285-tfidf-17" href="./acl-2013-Using_Conceptual_Class_Attributes_to_Characterize_Social_Media_Users.html">373 acl-2013-Using Conceptual Class Attributes to Characterize Social Media Users</a></p>
<p>18 0.066591658 <a title="285-tfidf-18" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>19 0.061941896 <a title="285-tfidf-19" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>20 0.061237179 <a title="285-tfidf-20" href="./acl-2013-Towards_Accurate_Distant_Supervision_for_Relational_Facts_Extraction.html">352 acl-2013-Towards Accurate Distant Supervision for Relational Facts Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.18), (1, 0.006), (2, -0.041), (3, -0.074), (4, -0.046), (5, 0.082), (6, 0.048), (7, -0.056), (8, 0.061), (9, -0.008), (10, -0.063), (11, 0.066), (12, 0.036), (13, 0.067), (14, 0.012), (15, -0.079), (16, 0.084), (17, 0.071), (18, 0.079), (19, 0.014), (20, -0.129), (21, 0.088), (22, -0.089), (23, 0.108), (24, -0.003), (25, -0.077), (26, 0.039), (27, 0.041), (28, -0.034), (29, -0.001), (30, -0.021), (31, -0.035), (32, -0.099), (33, -0.07), (34, 0.07), (35, 0.068), (36, -0.061), (37, -0.008), (38, 0.007), (39, 0.036), (40, -0.067), (41, 0.01), (42, 0.058), (43, 0.014), (44, 0.114), (45, 0.051), (46, -0.053), (47, 0.027), (48, -0.009), (49, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95414591 <a title="285-lsi-1" href="./acl-2013-Propminer%3A_A_Workflow_for_Interactive_Information_Extraction_and_Exploration_using_Dependency_Trees.html">285 acl-2013-Propminer: A Workflow for Interactive Information Extraction and Exploration using Dependency Trees</a></p>
<p>Author: Alan Akbik ; Oresti Konomi ; Michail Melnikov</p><p>Abstract: The use ofdeep syntactic information such as typed dependencies has been shown to be very effective in Information Extraction. Despite this potential, the process of manually creating rule-based information extractors that operate on dependency trees is not intuitive for persons without an extensive NLP background. In this system demonstration, we present a tool and a workflow designed to enable initiate users to interactively explore the effect and expressivity of creating Information Extraction rules over dependency trees. We introduce the proposed five step workflow for creating information extractors, the graph query based rule language, as well as the core features of the PROP- MINER tool.</p><p>2 0.62101936 <a title="285-lsi-2" href="./acl-2013-General_binarization_for_parsing_and_translation.html">165 acl-2013-General binarization for parsing and translation</a></p>
<p>Author: Matthias Buchse ; Alexander Koller ; Heiko Vogler</p><p>Abstract: Binarization ofgrammars is crucial for improving the complexity and performance of parsing and translation. We present a versatile binarization algorithm that can be tailored to a number of grammar formalisms by simply varying a formal parameter. We apply our algorithm to binarizing tree-to-string transducers used in syntax-based machine translation.</p><p>3 0.61752617 <a title="285-lsi-3" href="./acl-2013-Development_and_Analysis_of_NLP_Pipelines_in_Argo.html">118 acl-2013-Development and Analysis of NLP Pipelines in Argo</a></p>
<p>Author: Rafal Rak ; Andrew Rowley ; Jacob Carter ; Sophia Ananiadou</p><p>Abstract: Developing sophisticated NLP pipelines composed of multiple processing tools and components available through different providers may pose a challenge in terms of their interoperability. The Unstructured Information Management Architecture (UIMA) is an industry standard whose aim is to ensure such interoperability by defining common data structures and interfaces. The architecture has been gaining attention from industry and academia alike, resulting in a large volume ofUIMA-compliant processing components. In this paper, we demonstrate Argo, a Web-based workbench for the development and processing of NLP pipelines/workflows. The workbench is based upon UIMA, and thus has the potential of using many of the existing UIMA resources. We present features, and show examples, offacilitating the distributed development of components and the analysis of processing results. The latter includes annotation visualisers and editors, as well as serialisation to RDF format, which enables flexible querying in addition to data manipulation thanks to the semantic query language SPARQL. The distributed development feature allows users to seamlessly connect their tools to workflows running in Argo, and thus take advantage of both the available library of components (without the need of installing them locally) and the analytical tools.</p><p>4 0.60235393 <a title="285-lsi-4" href="./acl-2013-ICARUS_-_An_Extensible_Graphical_Search_Tool_for_Dependency_Treebanks.html">183 acl-2013-ICARUS - An Extensible Graphical Search Tool for Dependency Treebanks</a></p>
<p>Author: Markus Gartner ; Gregor Thiele ; Wolfgang Seeker ; Anders Bjorkelund ; Jonas Kuhn</p><p>Abstract: We present ICARUS, a versatile graphical search tool to query dependency treebanks. Search results can be inspected both quantitatively and qualitatively by means of frequency lists, tables, or dependency graphs. ICARUS also ships with plugins that enable it to interface with tool chains running either locally or remotely.</p><p>5 0.57183075 <a title="285-lsi-5" href="./acl-2013-ParaQuery%3A_Making_Sense_of_Paraphrase_Collections.html">271 acl-2013-ParaQuery: Making Sense of Paraphrase Collections</a></p>
<p>Author: Lili Kotlerman ; Nitin Madnani ; Aoife Cahill</p><p>Abstract: Pivoting on bilingual parallel corpora is a popular approach for paraphrase acquisition. Although such pivoted paraphrase collections have been successfully used to improve the performance of several different NLP applications, it is still difficult to get an intrinsic estimate of the quality and coverage of the paraphrases contained in these collections. We present ParaQuery, a tool that helps a user interactively explore and characterize a given pivoted paraphrase collection, analyze its utility for a particular domain, and compare it to other popular lexical similarity resources all within a single interface.</p><p>6 0.56680584 <a title="285-lsi-6" href="./acl-2013-Extending_an_interoperable_platform_to_facilitate_the_creation_of_multilingual_and_multimodal_NLP_applications.html">150 acl-2013-Extending an interoperable platform to facilitate the creation of multilingual and multimodal NLP applications</a></p>
<p>7 0.55590922 <a title="285-lsi-7" href="./acl-2013-WebAnno%3A_A_Flexible%2C_Web-based_and_Visually_Supported_System_for_Distributed_Annotations.html">385 acl-2013-WebAnno: A Flexible, Web-based and Visually Supported System for Distributed Annotations</a></p>
<p>8 0.54367542 <a title="285-lsi-8" href="./acl-2013-AnnoMarket%3A_An_Open_Cloud_Platform_for_NLP.html">51 acl-2013-AnnoMarket: An Open Cloud Platform for NLP</a></p>
<p>9 0.54226065 <a title="285-lsi-9" href="./acl-2013-Parsing_Graphs_with_Hyperedge_Replacement_Grammars.html">274 acl-2013-Parsing Graphs with Hyperedge Replacement Grammars</a></p>
<p>10 0.53271568 <a title="285-lsi-10" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<p>11 0.5289458 <a title="285-lsi-11" href="./acl-2013-Fluid_Construction_Grammar_for_Historical_and_Evolutionary_Linguistics.html">161 acl-2013-Fluid Construction Grammar for Historical and Evolutionary Linguistics</a></p>
<p>12 0.5163092 <a title="285-lsi-12" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>13 0.4999128 <a title="285-lsi-13" href="./acl-2013-A_Two_Level_Model_for_Context_Sensitive_Inference_Rules.html">27 acl-2013-A Two Level Model for Context Sensitive Inference Rules</a></p>
<p>14 0.49504665 <a title="285-lsi-14" href="./acl-2013-PATHS%3A_A_System_for_Accessing_Cultural_Heritage_Collections.html">268 acl-2013-PATHS: A System for Accessing Cultural Heritage Collections</a></p>
<p>15 0.49114886 <a title="285-lsi-15" href="./acl-2013-Semantic_Parsing_as_Machine_Translation.html">312 acl-2013-Semantic Parsing as Machine Translation</a></p>
<p>16 0.48567328 <a title="285-lsi-16" href="./acl-2013-Semantic_Neighborhoods_as_Hypergraphs.html">311 acl-2013-Semantic Neighborhoods as Hypergraphs</a></p>
<p>17 0.47115728 <a title="285-lsi-17" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>18 0.46231258 <a title="285-lsi-18" href="./acl-2013-Using_Lexical_Expansion_to_Learn_Inference_Rules_from_Sparse_Data.html">376 acl-2013-Using Lexical Expansion to Learn Inference Rules from Sparse Data</a></p>
<p>19 0.46046314 <a title="285-lsi-19" href="./acl-2013-Grounded_Unsupervised_Semantic_Parsing.html">176 acl-2013-Grounded Unsupervised Semantic Parsing</a></p>
<p>20 0.45451623 <a title="285-lsi-20" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.076), (6, 0.024), (11, 0.08), (15, 0.011), (24, 0.062), (26, 0.058), (28, 0.011), (29, 0.26), (35, 0.077), (42, 0.06), (48, 0.037), (64, 0.019), (70, 0.052), (88, 0.025), (90, 0.019), (95, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93188083 <a title="285-lda-1" href="./acl-2013-TopicSpam%3A_a_Topic-Model_based_approach_for_spam_detection.html">350 acl-2013-TopicSpam: a Topic-Model based approach for spam detection</a></p>
<p>Author: Jiwei Li ; Claire Cardie ; Sujian Li</p><p>Abstract: Product reviews are now widely used by individuals and organizations for decision making (Litvin et al., 2008; Jansen, 2010). And because of the profits at stake, people have been known to try to game the system by writing fake reviews to promote target products. As a result, the task of deceptive review detection has been gaining increasing attention. In this paper, we propose a generative LDA-based topic modeling approach for fake review detection. Our model can aptly detect the subtle dif- ferences between deceptive reviews and truthful ones and achieves about 95% accuracy on review spam datasets, outperforming existing baselines by a large margin.</p><p>2 0.86590528 <a title="285-lda-2" href="./acl-2013-Combination_of_Recurrent_Neural_Networks_and_Factored_Language_Models_for_Code-Switching_Language_Modeling.html">84 acl-2013-Combination of Recurrent Neural Networks and Factored Language Models for Code-Switching Language Modeling</a></p>
<p>Author: Heike Adel ; Ngoc Thang Vu ; Tanja Schultz</p><p>Abstract: In this paper, we investigate the application of recurrent neural network language models (RNNLM) and factored language models (FLM) to the task of language modeling for Code-Switching speech. We present a way to integrate partof-speech tags (POS) and language information (LID) into these models which leads to significant improvements in terms of perplexity. Furthermore, a comparison between RNNLMs and FLMs and a detailed analysis of perplexities on the different backoff levels are performed. Finally, we show that recurrent neural networks and factored language models can . be combined using linear interpolation to achieve the best performance. The final combined language model provides 37.8% relative improvement in terms of perplexity on the SEAME development set and a relative improvement of 32.7% on the evaluation set compared to the traditional n-gram language model. Index Terms: multilingual speech processing, code switching, language modeling, recurrent neural networks, factored language models</p><p>same-paper 3 0.80768132 <a title="285-lda-3" href="./acl-2013-Propminer%3A_A_Workflow_for_Interactive_Information_Extraction_and_Exploration_using_Dependency_Trees.html">285 acl-2013-Propminer: A Workflow for Interactive Information Extraction and Exploration using Dependency Trees</a></p>
<p>Author: Alan Akbik ; Oresti Konomi ; Michail Melnikov</p><p>Abstract: The use ofdeep syntactic information such as typed dependencies has been shown to be very effective in Information Extraction. Despite this potential, the process of manually creating rule-based information extractors that operate on dependency trees is not intuitive for persons without an extensive NLP background. In this system demonstration, we present a tool and a workflow designed to enable initiate users to interactively explore the effect and expressivity of creating Information Extraction rules over dependency trees. We introduce the proposed five step workflow for creating information extractors, the graph query based rule language, as well as the core features of the PROP- MINER tool.</p><p>4 0.76993686 <a title="285-lda-4" href="./acl-2013-Modeling_of_term-distance_and_term-occurrence_information_for_improving_n-gram_language_model_performance.html">247 acl-2013-Modeling of term-distance and term-occurrence information for improving n-gram language model performance</a></p>
<p>Author: Tze Yuang Chong ; Rafael E. Banchs ; Eng Siong Chng ; Haizhou Li</p><p>Abstract: In this paper, we explore the use of distance and co-occurrence information of word-pairs for language modeling. We attempt to extract this information from history-contexts of up to ten words in size, and found it complements well the n-gram model, which inherently suffers from data scarcity in learning long history-contexts. Evaluated on the WSJ corpus, bigram and trigram model perplexity were reduced up to 23.5% and 14.0%, respectively. Compared to the distant bigram, we show that word-pairs can be more effectively modeled in terms of both distance and occurrence. 1</p><p>5 0.71005601 <a title="285-lda-5" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>Author: Kai Liu ; Yajuan Lu ; Wenbin Jiang ; Qun Liu</p><p>Abstract: This paper describes a novel strategy for automatic induction of a monolingual dependency grammar under the guidance of bilingually-projected dependency. By moderately leveraging the dependency information projected from the parsed counterpart language, and simultaneously mining the underlying syntactic structure of the language considered, it effectively integrates the advantages of bilingual projection and unsupervised induction, so as to induce a monolingual grammar much better than previous models only using bilingual projection or unsupervised induction. We induced dependency gram- mar for five different languages under the guidance of dependency information projected from the parsed English translation, experiments show that the bilinguallyguided method achieves a significant improvement of 28.5% over the unsupervised baseline and 3.0% over the best projection baseline on average.</p><p>6 0.6409598 <a title="285-lda-6" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>7 0.57792652 <a title="285-lda-7" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>8 0.57754791 <a title="285-lda-8" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>9 0.57710993 <a title="285-lda-9" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>10 0.57699937 <a title="285-lda-10" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>11 0.57542282 <a title="285-lda-11" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<p>12 0.5750218 <a title="285-lda-12" href="./acl-2013-Discriminative_Learning_with_Natural_Annotations%3A_Word_Segmentation_as_a_Case_Study.html">123 acl-2013-Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study</a></p>
<p>13 0.57434499 <a title="285-lda-13" href="./acl-2013-Paraphrase-Driven_Learning_for_Open_Question_Answering.html">272 acl-2013-Paraphrase-Driven Learning for Open Question Answering</a></p>
<p>14 0.57344341 <a title="285-lda-14" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>15 0.57214743 <a title="285-lda-15" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>16 0.57183754 <a title="285-lda-16" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<p>17 0.57160544 <a title="285-lda-17" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>18 0.57157749 <a title="285-lda-18" href="./acl-2013-Embedding_Semantic_Similarity_in_Tree_Kernels_for_Domain_Adaptation_of_Relation_Extraction.html">134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</a></p>
<p>19 0.57132328 <a title="285-lda-19" href="./acl-2013-ICARUS_-_An_Extensible_Graphical_Search_Tool_for_Dependency_Treebanks.html">183 acl-2013-ICARUS - An Extensible Graphical Search Tool for Dependency Treebanks</a></p>
<p>20 0.57064849 <a title="285-lda-20" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
