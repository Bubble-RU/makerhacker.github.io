<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>55 acl-2013-Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-55" href="#">acl2013-55</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>55 acl-2013-Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?</h1>
<br/><p>Source: <a title="acl-2013-55-pdf" href="http://aclweb.org/anthology//P/P13/P13-2027.pdf">pdf</a></p><p>Author: Romain Deveaud ; Eric SanJuan ; Patrice Bellot</p><p>Abstract: The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics. More, the semantic coherence of the topics has never been considered in this field. We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics. We perform a first experimental evaluation using two major TREC test collections. Results show that retrieval perfor- mances tend to be better when using topics with higher semantic coherence.</p><p>Reference: <a title="acl-2013-55-reference" href="../acl2013_reference/acl-2013-Are_Semantically_Coherent_Topic_Models_Useful_for_Ad_Hoc_Information_Retrieval%3F_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Romain Deveaud Eric SanJuan University of Avignon - LIA Avignon, France romain . [sent-2, score-0.046]
</p><p>2 fr Abstract The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics. [sent-6, score-0.39]
</p><p>3 More, the semantic coherence of the topics has never been considered in this field. [sent-7, score-0.729]
</p><p>4 We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics. [sent-8, score-1.081]
</p><p>5 Results show that retrieval perfor-  mances tend to be better when using topics with higher semantic coherence. [sent-10, score-0.514]
</p><p>6 1 Introduction Representing documents as mixtures of “topics” has always been a challenge and an objective for researchers working in text-related fields. [sent-11, score-0.136]
</p><p>7 Based on the words used within a document, topic models learn topic level relations by assuming that the document covers a small set of concepts. [sent-12, score-0.667]
</p><p>8 Learning the topics from a document collection can help to extract high level semantic information, and help humans to understand the meaning of documents. [sent-13, score-0.558]
</p><p>9 , 1990) (LSI), probabilistic Latent Semantic Analysis (Hofmann, 2001) (pLSA) and Latent Dirichlet Allocation (Blei et al. [sent-15, score-0.037]
</p><p>10 , 2003) (LDA) are the most famous approaches that tried to tackle this problem throughout the years. [sent-16, score-0.045]
</p><p>11 Topics produced by these methods are generally fancy and appealing, and often correlate well with human concepts. [sent-17, score-0.033]
</p><p>12 This is one of the reasons of the intensive use of topic models (and especially LDA) in current research in Natural Language Processing  (NLP) related areas. [sent-18, score-0.304]
</p><p>13 One main problem in ad hoc Information Retrieval (IR) is the difficulty for users to translate a Patrice Bellot Aix-Marseille University - LSIS Marseille, France pat rice . [sent-19, score-0.161]
</p><p>14 The most popular and effective approach to overcome this problem is to improve the representation of the query by adding query-related “concepts”. [sent-22, score-0.247]
</p><p>15 This approach mostly relies on pseudorelevance feedback, where these so-called “concepts” are the most frequent words occurring in the top-ranked documents retrieved by the retrieval system (Lavrenko and Croft, 2001). [sent-23, score-0.377]
</p><p>16 From that perspective, topic models seem attractive in the sense that they can provide a descriptive and intuitive representation of concepts. [sent-24, score-0.304]
</p><p>17 But how can we quantify the usefulness of these topics with respect to an IR system? [sent-25, score-0.341]
</p><p>18 Recently, researchers developed measures which evaluate the semantic coherence  of topic models (Newman et al. [sent-26, score-0.692]
</p><p>19 We adopt their view of semantic coherence and apply one of these measures to query-oriented topics. [sent-30, score-0.388]
</p><p>20 Several studies concentrated on improving the quality of document ranking using topic models, especially probabilistic ones. [sent-31, score-0.4]
</p><p>21 The approach by Wei and Croft (2006) was the first to leverage LDA topics to improve the estimate of document language models and achieved good empirical results. [sent-32, score-0.523]
</p><p>22 The reported results suggest that the words and the probability distributions learned by probabilistic topic models are effective for query expansion. [sent-35, score-0.636]
</p><p>23 The main drawback of these approaches is that topics are learned on the whole target document collection prior to retrieval, thus leading to a static topical representation of the collection. [sent-36, score-0.591]
</p><p>24 Depending on the query and on its specificity, topics may ei-  ther be too coarse or too fine to accurately represent the latent concepts of the query. [sent-37, score-0.752]
</p><p>25 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 148–152, LDA and learns topics directly on a limited set of documents. [sent-41, score-0.378]
</p><p>26 While this approach is a first step towards modeling query-oriented topics, it lacks some theoretic principles and only aims to heuristically construct a “best” topic (from all learned topics) before expanding the query with its most probable words. [sent-42, score-0.552]
</p><p>27 More, none of the aforementioned works studied the semantic coherence of those generated topics. [sent-43, score-0.388]
</p><p>28 1 Relevance Models The goal of relevance models is to improve the representation of a query Q by selecting terms from a set of initially retrieved documents (Lavrenko and Croft, 2001). [sent-46, score-0.635]
</p><p>29 As the concentration of relevant documents is usually higher in the top ranks of the ranking list, this is constituted by a number N of top-ranked documents. [sent-47, score-0.169]
</p><p>30 Relevance models usually perform better when combined with the original query model (or maximum likelihood estimate). [sent-48, score-0.325]
</p><p>31 One of the most robust variants of the relevance models is computed as follows:  P(w|θˆQ)  ∝  X  P(θD)P(w|θD)  θDX∈Θ  YP(t|θD) Yt∈Q  (2) where Θ is a set of pseudo-relevant feedback documents and θD is the language model of document D. [sent-50, score-0.801]
</p><p>32 This notion of estimating a query model is often referred to as model-based feedback (Zhai and Lafferty, 2001). [sent-51, score-0.599]
</p><p>33 We assume P(θD) to be uniform, resulting in an estimated relevance model based on a sum of document models weighted by the query likelihood score. [sent-52, score-0.591]
</p><p>34 The final, interpolated, estimate expressed in equation (1) is often referred in the literature as RM3. [sent-53, score-0.029]
</p><p>35 We tackle the null probabilities problem by smoothing the document language model using the well-known Dirichlet smoothing (Zhai and Lafferty, 2004). [sent-54, score-0.225]
</p><p>36 2  LDA-based Feedback Model  θˆQ  The estimation of the feedback model constitutes the first contribution of this work. [sent-56, score-0.352]
</p><p>37 We propose to explicitly model the latent topics (or concepts) that exist behind an information need, and to use them to improve the query representation. [sent-57, score-0.691]
</p><p>38 We consider Θ as the set of pseudo-relevant feedback documents from which the latent concepts would be extracted. [sent-58, score-0.652]
</p><p>39 The retrieval algorithm used to obtain these documents can be of any kind, the important point is that Θ is a reduced collection that contains the top documents ranked by an automatic and state-of-the-art retrieval process. [sent-59, score-0.588]
</p><p>40 Instead of viewing Θ as a set of document language models that are likely to contain topical information about the query, we take a probabilistic topic modeling approach. [sent-60, score-0.479]
</p><p>41 In LDA, each topic multinomial distribution φk is generated by a conjugate Dirichlet prior with parameter while each document multinomial distribution θd is generated by a conjugate Dirichlet prior with parameter α. [sent-62, score-0.645]
</p><p>42 In other words, θd,k is the prob-  β,  ability of topic k occurring in document D (i. [sent-63, score-0.4]
</p><p>43 Respectively, φk,w is the probability of Pwo(krd|D w belonging tivoe topic k (i. [sent-66, score-0.257]
</p><p>44 We use vwaoriradt wion baell oinnfgeinregn ctoe implemented iwn tkh))e. [sent-69, score-0.03]
</p><p>45 L WDeA u-sCe software1 to overcome intractability issues (Blei et al. [sent-70, score-0.033]
</p><p>46 Under this setting, we compute the topic-driven estimation of the query model using the following equation:  P(w|θˆQ) ∝θDX∈Θ? [sent-72, score-0.247]
</p><p>47 (3)  where PTM(w|D) is the probability of word w occurring in( wdo|Dcu)m isen tth eD p using tithye previously 1www . [sent-74, score-0.132]
</p><p>48 3  Number of topics 5 10 15  20  0 1  5  10  20 30 Number of feedback documents  40  50  Robust04 2. [sent-78, score-0.829]
</p><p>49 3  Number of topics 5 10 15  0 1  eCnrhoece9641. [sent-79, score-0.341]
</p><p>50 0  5  10  20 30 Number of feedback documents  40  50  Figure 1: Semantic coherence of the topic models for different values of K, in function of the number  20  N of feedback documents. [sent-83, score-1.485]
</p><p>51 Let TΘ be a topic model learned on the Θ set of fLeeedtba Tck documents, this probability is given by:  PTM(w|D) = X  φk,w · θD,k  (4)  kX∈TΘ  High probabilities are thus given to words that are important in topic k, when k is an important topic in document D. [sent-85, score-0.925]
</p><p>52 3  Measuring the coherence of query-oriented topics  TDRM relies on two important parameters: the number of topics K that we want to learn, and the number of feedback documents N from which LDA learns the topics. [sent-88, score-1.548]
</p><p>53 Term similarities measured in restricted domains was the first step for evaluating semantic coherence (Gliozzo et al. [sent-90, score-0.388]
</p><p>54 , 2007), and was a first basis for the development of several topic coherence evaluation measures (Newman et al. [sent-91, score-0.598]
</p><p>55 Computing the Pointwise Mutual Information (PMI) of all word pairs over Wikipedia was found to be an effective metric using news and books corpora. [sent-93, score-0.037]
</p><p>56 (2012) used (among others) an aggregate version of this metric to evaluate large amounts of topic models. [sent-95, score-0.257]
</p><p>57 We use this method to evaluate the coherence of query-oriented topics. [sent-96, score-0.341]
</p><p>58 Specifically, the coherence of a topic model TΘK composed of K topics is:  c(TΘK) =K1Xi=K1(w,Xw0)∈kilogPP((ww,)wP0()w +0) ? [sent-97, score-0.983]
</p><p>59 Robust04 is composed 528,155 of news articles coming from three newspapers and the FBIS. [sent-105, score-0.164]
</p><p>60 It supported the TREC 2004 Robust track, from which we used the 250 query topics (numbers: 301-450, 601-700). [sent-106, score-0.588]
</p><p>61 The WT10g collection is composed of 1,692,096 web pages, and supported the TREC Web track for four years (2001-2004). [sent-107, score-0.108]
</p><p>62 We focus on the 2000 and 2001 ad-hoc query topics (numbers: 451-550). [sent-108, score-0.588]
</p><p>63 We used the open-source indexing and retrieval system Indri3 to run our experiments. [sent-109, score-0.174]
</p><p>64 We indexed the two collections with the exact same parameters: tokens were stemmed with the well-known light Krovetz stemmer and stopwords were removed using the standard English stoplist embedded with Indri (417 words). [sent-110, score-0.095]
</p><p>65 2 Semantic coherence evaluation Most coherent topics are composed of rare words  that do not often occur in the reference corpus, but  2trec . [sent-112, score-0.83]
</p><p>66 php 150  WT10g  Number of feedback documents  Robust04  Number of feedback documents Figure 2: Retrieval performance in terms of Mean Average Precision (MAP) of the TDRM approach. [sent-116, score-0.976]
</p><p>67 Each line represent a different number of topics K, and the performance are reported in function the number N of feedback documents. [sent-117, score-0.693]
</p><p>68 We see on Figure 1 that very coherent topics are identified in the top 5 and 10 feedback documents for the WT10g collection, suggesting that closely related documents are retrieved in the top ranks. [sent-120, score-1.114]
</p><p>69 Results are quite different on the Robust04 collection, where topic models with 20 topics on 5 documents are the least coherent. [sent-121, score-0.781]
</p><p>70 We hypothesize that the heterogeneous nature of the web allows to model very different topics covering several aspects of the query, while news articles are contributions focused on a single subject. [sent-123, score-0.425]
</p><p>71 Overall, the more coherent topic models contain a reasonable amount of topics (10-15), thus allowing to fit with variable amounts of documents. [sent-124, score-0.749]
</p><p>72 The attentive reader will notice that the topic coherence scores are very high compared to those previously reported in the literature (Stevens et al. [sent-125, score-0.657]
</p><p>73 The TDRM approach captures topics that are centered around a specific information need, often with a limited vocabulary, which favors word co-occurrence. [sent-127, score-0.341]
</p><p>74 On the other hand, topics learned on entire collections are coarser than ours, which leads to lower coherence scores. [sent-128, score-0.792]
</p><p>75 3  Document retrieval results  Since TDRM is based on Relevance Models (Lavrenko and Croft, 2001), we take the RM3 approach presented in Section 2. [sent-130, score-0.126]
</p><p>76 The λ parameter is common between RM3 and TDRM  and is determined for each query using leaveone-query-out cross-validation (that is: learn the best parameter setting for all queries but one, and evaluate the held-out query using the previously learned parameter). [sent-132, score-0.653]
</p><p>77 We report ad hoc document retrieval performances in Figure 2. [sent-133, score-0.393]
</p><p>78 We noticed in the previous section that the most coherent topic models were modeled using 5 feedback documents and 20 topics for the WT10g collection, and this parameter combination also achieves the best retrieval results. [sent-134, score-1.404]
</p><p>79 Overall, using 10, 15 or 20 topics allow it to achieve high and similar performance from 5 to 20 documents. [sent-135, score-0.341]
</p><p>80 We observe than using 20 topics for the Robust04 collection consistently achieves the highest results, with the topic model coherence growing as the number of feedback documents increases. [sent-136, score-1.491]
</p><p>81 Although topics coming from news articles may be limited, they benefit from the rich vocabulary of professional writers who are trained to avoid repetition. [sent-137, score-0.461]
</p><p>82 Their use of synonyms allows TDRM to model deep topics, with a comprehensive description of query aspects. [sent-138, score-0.276]
</p><p>83 Since synonyms  are less likely to co-occur in encyclopedic articles like Wikipedia, we think that, in our case, the semantic coherence measure could be more accurate using other textual resources. [sent-139, score-0.464]
</p><p>84 4  Conclusions & Future Work  Overall, modeling query-oriented topic models and estimating the feedback query model using these topics greatly improves ad hoc Information Retrieval, compared to state-of-the-art relevance models. [sent-141, score-1.565]
</p><p>85 While semantically coherent topic mod151  els do not seem to be effective in the context of a news articles search task, they are a good indicator of effectiveness in the context of web search. [sent-142, score-0.445]
</p><p>86 Measuring the semantic coherence of query topics could help predict query effectiveness or even choose the best query-representative topic model. [sent-143, score-1.48]
</p><p>87 Investigating task performance of probabilistic topic models: an empirical study of PLSA and LDA. [sent-182, score-0.294]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('feedback', 0.352), ('topics', 0.341), ('coherence', 0.341), ('tdrm', 0.261), ('topic', 0.257), ('query', 0.247), ('relevance', 0.16), ('documents', 0.136), ('retrieval', 0.126), ('stevens', 0.121), ('andrzejewski', 0.112), ('ptm', 0.112), ('lda', 0.108), ('document', 0.106), ('coherent', 0.104), ('latent', 0.103), ('lavrenko', 0.1), ('hoc', 0.094), ('zhai', 0.093), ('dirichlet', 0.091), ('newman', 0.089), ('croft', 0.085), ('trec', 0.081), ('plsa', 0.075), ('deveaud', 0.075), ('ad', 0.067), ('collection', 0.064), ('chengxiang', 0.064), ('collections', 0.062), ('concepts', 0.061), ('dx', 0.061), ('gliozzo', 0.061), ('indri', 0.061), ('allocation', 0.059), ('multinomial', 0.054), ('mimno', 0.049), ('learned', 0.048), ('indexing', 0.048), ('sigir', 0.048), ('semantic', 0.047), ('models', 0.047), ('articles', 0.047), ('romain', 0.046), ('conjugate', 0.046), ('lafferty', 0.046), ('retrieved', 0.045), ('ir', 0.045), ('tackle', 0.045), ('xing', 0.045), ('composed', 0.044), ('avignon', 0.043), ('deerwester', 0.043), ('bruce', 0.042), ('blei', 0.041), ('parameter', 0.041), ('ye', 0.038), ('learns', 0.037), ('occurring', 0.037), ('smoothing', 0.037), ('probabilistic', 0.037), ('news', 0.037), ('france', 0.036), ('coming', 0.036), ('thomas', 0.035), ('pmi', 0.035), ('park', 0.034), ('david', 0.034), ('griffiths', 0.033), ('fancy', 0.033), ('buttler', 0.033), ('constituted', 0.033), ('dcu', 0.033), ('edv', 0.033), ('etw', 0.033), ('intractability', 0.033), ('isen', 0.033), ('lemurpro', 0.033), ('lia', 0.033), ('nationale', 0.033), ('pseudorelevance', 0.033), ('pwo', 0.033), ('stoplist', 0.033), ('tier', 0.033), ('tithye', 0.033), ('wdo', 0.033), ('topical', 0.032), ('likelihood', 0.031), ('agence', 0.03), ('patrice', 0.03), ('attentive', 0.03), ('iwn', 0.03), ('fr', 0.03), ('synonyms', 0.029), ('estimate', 0.029), ('previously', 0.029), ('wikipedia', 0.029), ('scientific', 0.029), ('tkh', 0.029), ('pkdd', 0.029), ('lsi', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="55-tfidf-1" href="./acl-2013-Are_Semantically_Coherent_Topic_Models_Useful_for_Ad_Hoc_Information_Retrieval%3F.html">55 acl-2013-Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?</a></p>
<p>Author: Romain Deveaud ; Eric SanJuan ; Patrice Bellot</p><p>Abstract: The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics. More, the semantic coherence of the topics has never been considered in this field. We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics. We perform a first experimental evaluation using two major TREC test collections. Results show that retrieval perfor- mances tend to be better when using topics with higher semantic coherence.</p><p>2 0.24003029 <a title="55-tfidf-2" href="./acl-2013-Topic_Modeling_Based_Classification_of_Clinical_Reports.html">351 acl-2013-Topic Modeling Based Classification of Clinical Reports</a></p>
<p>Author: Efsun Sarioglu ; Kabir Yadav ; Hyeong-Ah Choi</p><p>Abstract: Kabir Yadav Emergency Medicine Department The George Washington University Washington, DC, USA kyadav@ gwu . edu Hyeong-Ah Choi Computer Science Department The George Washington University Washington, DC, USA hcho i gwu . edu @ such as recommending the need for a certain medical test while avoiding intrusive tests or medical Electronic health records (EHRs) contain important clinical information about pa- tients. Some of these data are in the form of free text and require preprocessing to be able to used in automated systems. Efficient and effective use of this data could be vital to the speed and quality of health care. As a case study, we analyzed classification of CT imaging reports into binary categories. In addition to regular text classification, we utilized topic modeling of the entire dataset in various ways. Topic modeling of the corpora provides interpretable themes that exist in these reports. Representing reports according to their topic distributions is more compact than bag-of-words representation and can be processed faster than raw text in subsequent automated processes. A binary topic model was also built as an unsupervised classification approach with the assumption that each topic corresponds to a class. And, finally an aggregate topic classifier was built where reports are classified based on a single discriminative topic that is determined from the training dataset. Our proposed topic based classifier system is shown to be competitive with existing text classification techniques and provides a more efficient and interpretable representation.</p><p>3 0.23990484 <a title="55-tfidf-3" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>Author: Zede Zhu ; Miao Li ; Lei Chen ; Zhenxin Yang</p><p>Abstract: Comparable corpora are important basic resources in cross-language information processing. However, the existing methods of building comparable corpora, which use intertranslate words and relative features, cannot evaluate the topical relation between document pairs. This paper adopts the bilingual LDA model to predict the topical structures of the documents and proposes three algorithms of document similarity in different languages. Experiments show that the novel method can obtain similar documents with consistent top- ics own better adaptability and stability performance.</p><p>4 0.21182038 <a title="55-tfidf-4" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>Author: Yukari Ogura ; Ichiro Kobayashi</p><p>Abstract: In this paper, we propose a method to raise the accuracy of text classification based on latent topics, reconsidering the techniques necessary for good classification for example, to decide important sentences in a document, the sentences with important words are usually regarded as important sentences. In this case, tf.idf is often used to decide important words. On the other hand, we apply the PageRank algorithm to rank important words in each document. Furthermore, before clustering documents, we refine the target documents by representing them as a collection of important sentences in each document. We then classify the documents based on latent information in the documents. As a clustering method, we employ the k-means algorithm and inves– tigate how our proposed method works for good clustering. We conduct experiments with Reuters-21578 corpus under various conditions of important sentence extraction, using latent and surface information for clustering, and have confirmed that our proposed method provides better result among various conditions for clustering.</p><p>5 0.17843911 <a title="55-tfidf-5" href="./acl-2013-Broadcast_News_Story_Segmentation_Using_Manifold_Learning_on_Latent_Topic_Distributions.html">73 acl-2013-Broadcast News Story Segmentation Using Manifold Learning on Latent Topic Distributions</a></p>
<p>Author: Xiaoming Lu ; Lei Xie ; Cheung-Chi Leung ; Bin Ma ; Haizhou Li</p><p>Abstract: We present an efficient approach for broadcast news story segmentation using a manifold learning algorithm on latent topic distributions. The latent topic distribution estimated by Latent Dirichlet Allocation (LDA) is used to represent each text block. We employ Laplacian Eigenmaps (LE) to project the latent topic distributions into low-dimensional semantic representations while preserving the intrinsic local geometric structure. We evaluate two approaches employing LDA and probabilistic latent semantic analysis (PLSA) distributions respectively. The effects of different amounts of training data and different numbers of latent topics on the two approaches are studied. Experimental re- sults show that our proposed LDA-based approach can outperform the corresponding PLSA-based approach. The proposed approach provides the best performance with the highest F1-measure of 0.7860.</p><p>6 0.17794625 <a title="55-tfidf-6" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>7 0.17777972 <a title="55-tfidf-7" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>8 0.1689444 <a title="55-tfidf-8" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>9 0.15624022 <a title="55-tfidf-9" href="./acl-2013-Exploiting_Topic_based_Twitter_Sentiment_for_Stock_Prediction.html">147 acl-2013-Exploiting Topic based Twitter Sentiment for Stock Prediction</a></p>
<p>10 0.15046999 <a title="55-tfidf-10" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<p>11 0.14703283 <a title="55-tfidf-11" href="./acl-2013-Paraphrasing_Adaptation_for_Web_Search_Ranking.html">273 acl-2013-Paraphrasing Adaptation for Web Search Ranking</a></p>
<p>12 0.14568564 <a title="55-tfidf-12" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>13 0.13734412 <a title="55-tfidf-13" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>14 0.13004117 <a title="55-tfidf-14" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>15 0.12770939 <a title="55-tfidf-15" href="./acl-2013-A_Two_Level_Model_for_Context_Sensitive_Inference_Rules.html">27 acl-2013-A Two Level Model for Context Sensitive Inference Rules</a></p>
<p>16 0.12179676 <a title="55-tfidf-16" href="./acl-2013-Diverse_Keyword_Extraction_from_Conversations.html">126 acl-2013-Diverse Keyword Extraction from Conversations</a></p>
<p>17 0.1158385 <a title="55-tfidf-17" href="./acl-2013-Evolutionary_Hierarchical_Dirichlet_Process_for_Timeline_Summarization.html">142 acl-2013-Evolutionary Hierarchical Dirichlet Process for Timeline Summarization</a></p>
<p>18 0.1098863 <a title="55-tfidf-18" href="./acl-2013-Natural_Language_Models_for_Predicting_Programming_Comments.html">257 acl-2013-Natural Language Models for Predicting Programming Comments</a></p>
<p>19 0.10491311 <a title="55-tfidf-19" href="./acl-2013-Automatic_Coupling_of_Answer_Extraction_and_Information_Retrieval.html">60 acl-2013-Automatic Coupling of Answer Extraction and Information Retrieval</a></p>
<p>20 0.095513359 <a title="55-tfidf-20" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.205), (1, 0.15), (2, 0.069), (3, -0.097), (4, 0.161), (5, -0.066), (6, 0.146), (7, -0.123), (8, -0.219), (9, -0.11), (10, 0.155), (11, 0.124), (12, 0.141), (13, 0.185), (14, 0.032), (15, -0.045), (16, -0.075), (17, 0.079), (18, -0.076), (19, -0.042), (20, -0.066), (21, 0.066), (22, -0.097), (23, -0.019), (24, 0.036), (25, -0.022), (26, -0.051), (27, -0.076), (28, -0.035), (29, 0.03), (30, 0.032), (31, -0.015), (32, -0.103), (33, -0.073), (34, -0.029), (35, -0.137), (36, 0.09), (37, 0.01), (38, 0.077), (39, 0.026), (40, -0.027), (41, -0.032), (42, -0.049), (43, 0.027), (44, -0.043), (45, 0.014), (46, 0.013), (47, 0.025), (48, -0.042), (49, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98052657 <a title="55-lsi-1" href="./acl-2013-Are_Semantically_Coherent_Topic_Models_Useful_for_Ad_Hoc_Information_Retrieval%3F.html">55 acl-2013-Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?</a></p>
<p>Author: Romain Deveaud ; Eric SanJuan ; Patrice Bellot</p><p>Abstract: The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics. More, the semantic coherence of the topics has never been considered in this field. We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics. We perform a first experimental evaluation using two major TREC test collections. Results show that retrieval perfor- mances tend to be better when using topics with higher semantic coherence.</p><p>2 0.83452988 <a title="55-lsi-2" href="./acl-2013-Topic_Modeling_Based_Classification_of_Clinical_Reports.html">351 acl-2013-Topic Modeling Based Classification of Clinical Reports</a></p>
<p>Author: Efsun Sarioglu ; Kabir Yadav ; Hyeong-Ah Choi</p><p>Abstract: Kabir Yadav Emergency Medicine Department The George Washington University Washington, DC, USA kyadav@ gwu . edu Hyeong-Ah Choi Computer Science Department The George Washington University Washington, DC, USA hcho i gwu . edu @ such as recommending the need for a certain medical test while avoiding intrusive tests or medical Electronic health records (EHRs) contain important clinical information about pa- tients. Some of these data are in the form of free text and require preprocessing to be able to used in automated systems. Efficient and effective use of this data could be vital to the speed and quality of health care. As a case study, we analyzed classification of CT imaging reports into binary categories. In addition to regular text classification, we utilized topic modeling of the entire dataset in various ways. Topic modeling of the corpora provides interpretable themes that exist in these reports. Representing reports according to their topic distributions is more compact than bag-of-words representation and can be processed faster than raw text in subsequent automated processes. A binary topic model was also built as an unsupervised classification approach with the assumption that each topic corresponds to a class. And, finally an aggregate topic classifier was built where reports are classified based on a single discriminative topic that is determined from the training dataset. Our proposed topic based classifier system is shown to be competitive with existing text classification techniques and provides a more efficient and interpretable representation.</p><p>3 0.80567414 <a title="55-lsi-3" href="./acl-2013-Broadcast_News_Story_Segmentation_Using_Manifold_Learning_on_Latent_Topic_Distributions.html">73 acl-2013-Broadcast News Story Segmentation Using Manifold Learning on Latent Topic Distributions</a></p>
<p>Author: Xiaoming Lu ; Lei Xie ; Cheung-Chi Leung ; Bin Ma ; Haizhou Li</p><p>Abstract: We present an efficient approach for broadcast news story segmentation using a manifold learning algorithm on latent topic distributions. The latent topic distribution estimated by Latent Dirichlet Allocation (LDA) is used to represent each text block. We employ Laplacian Eigenmaps (LE) to project the latent topic distributions into low-dimensional semantic representations while preserving the intrinsic local geometric structure. We evaluate two approaches employing LDA and probabilistic latent semantic analysis (PLSA) distributions respectively. The effects of different amounts of training data and different numbers of latent topics on the two approaches are studied. Experimental re- sults show that our proposed LDA-based approach can outperform the corresponding PLSA-based approach. The proposed approach provides the best performance with the highest F1-measure of 0.7860.</p><p>4 0.80285531 <a title="55-lsi-4" href="./acl-2013-Diverse_Keyword_Extraction_from_Conversations.html">126 acl-2013-Diverse Keyword Extraction from Conversations</a></p>
<p>Author: Maryam Habibi ; Andrei Popescu-Belis</p><p>Abstract: A new method for keyword extraction from conversations is introduced, which preserves the diversity of topics that are mentioned. Inspired from summarization, the method maximizes the coverage of topics that are recognized automatically in transcripts of conversation fragments. The method is evaluated on excerpts of the Fisher and AMI corpora, using a crowdsourcing platform to elicit comparative relevance judgments. The results demonstrate that the method outperforms two competitive baselines.</p><p>5 0.75865471 <a title="55-lsi-5" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>Author: Yukari Ogura ; Ichiro Kobayashi</p><p>Abstract: In this paper, we propose a method to raise the accuracy of text classification based on latent topics, reconsidering the techniques necessary for good classification for example, to decide important sentences in a document, the sentences with important words are usually regarded as important sentences. In this case, tf.idf is often used to decide important words. On the other hand, we apply the PageRank algorithm to rank important words in each document. Furthermore, before clustering documents, we refine the target documents by representing them as a collection of important sentences in each document. We then classify the documents based on latent information in the documents. As a clustering method, we employ the k-means algorithm and inves– tigate how our proposed method works for good clustering. We conduct experiments with Reuters-21578 corpus under various conditions of important sentence extraction, using latent and surface information for clustering, and have confirmed that our proposed method provides better result among various conditions for clustering.</p><p>6 0.74251044 <a title="55-lsi-6" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>7 0.72344887 <a title="55-lsi-7" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>8 0.70644164 <a title="55-lsi-8" href="./acl-2013-Are_School-of-thought_Words_Characterizable%3F.html">54 acl-2013-Are School-of-thought Words Characterizable?</a></p>
<p>9 0.68407565 <a title="55-lsi-9" href="./acl-2013-Evolutionary_Hierarchical_Dirichlet_Process_for_Timeline_Summarization.html">142 acl-2013-Evolutionary Hierarchical Dirichlet Process for Timeline Summarization</a></p>
<p>10 0.64251649 <a title="55-lsi-10" href="./acl-2013-Natural_Language_Models_for_Predicting_Programming_Comments.html">257 acl-2013-Natural Language Models for Predicting Programming Comments</a></p>
<p>11 0.62915093 <a title="55-lsi-11" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>12 0.62787956 <a title="55-lsi-12" href="./acl-2013-Improved_Bayesian_Logistic_Supervised_Topic_Models_with_Data_Augmentation.html">191 acl-2013-Improved Bayesian Logistic Supervised Topic Models with Data Augmentation</a></p>
<p>13 0.62514913 <a title="55-lsi-13" href="./acl-2013-Exploiting_Topic_based_Twitter_Sentiment_for_Stock_Prediction.html">147 acl-2013-Exploiting Topic based Twitter Sentiment for Stock Prediction</a></p>
<p>14 0.61034632 <a title="55-lsi-14" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>15 0.59245896 <a title="55-lsi-15" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<p>16 0.55990136 <a title="55-lsi-16" href="./acl-2013-Paraphrasing_Adaptation_for_Web_Search_Ranking.html">273 acl-2013-Paraphrasing Adaptation for Web Search Ranking</a></p>
<p>17 0.52013457 <a title="55-lsi-17" href="./acl-2013-Learning_Latent_Personas_of_Film_Characters.html">220 acl-2013-Learning Latent Personas of Film Characters</a></p>
<p>18 0.51387864 <a title="55-lsi-18" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>19 0.5052985 <a title="55-lsi-19" href="./acl-2013-PATHS%3A_A_System_for_Accessing_Cultural_Heritage_Collections.html">268 acl-2013-PATHS: A System for Accessing Cultural Heritage Collections</a></p>
<p>20 0.48954529 <a title="55-lsi-20" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.036), (6, 0.021), (11, 0.041), (24, 0.079), (26, 0.026), (35, 0.559), (42, 0.019), (48, 0.033), (70, 0.042), (88, 0.011), (90, 0.012), (95, 0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98686415 <a title="55-lda-1" href="./acl-2013-Are_Semantically_Coherent_Topic_Models_Useful_for_Ad_Hoc_Information_Retrieval%3F.html">55 acl-2013-Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?</a></p>
<p>Author: Romain Deveaud ; Eric SanJuan ; Patrice Bellot</p><p>Abstract: The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics. More, the semantic coherence of the topics has never been considered in this field. We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics. We perform a first experimental evaluation using two major TREC test collections. Results show that retrieval perfor- mances tend to be better when using topics with higher semantic coherence.</p><p>2 0.97980809 <a title="55-lda-2" href="./acl-2013-Patient_Experience_in_Online_Support_Forums%3A_Modeling_Interpersonal_Interactions_and_Medication_Use.html">278 acl-2013-Patient Experience in Online Support Forums: Modeling Interpersonal Interactions and Medication Use</a></p>
<p>Author: Annie Chen</p><p>Abstract: Though there has been substantial research concerning the extraction of information from clinical notes, to date there has been less work concerning the extraction of useful information from patient-generated content. Using a dataset comprised of online support group discussion content, this paper investigates two dimensions that may be important in the extraction of patient-generated experiences from text; significant individuals/groups and medication use. With regard to the former, the paper describes an approach involving the pairing of important figures (e.g. family, husbands, doctors, etc.) and affect, and suggests possible applications of such techniques to research concerning online social support, as well as integration into search interfaces for patients. Additionally, the paper demonstrates the extraction of side effects and sentiment at different phases in patient medication use, e.g. adoption, current use, discontinuation and switching, and demonstrates the utility of such an application for drug safety monitoring in online discussion forums. 1</p><p>3 0.97079301 <a title="55-lda-3" href="./acl-2013-Fine-grained_Semantic_Typing_of_Emerging_Entities.html">160 acl-2013-Fine-grained Semantic Typing of Emerging Entities</a></p>
<p>Author: Ndapandula Nakashole ; Tomasz Tylenda ; Gerhard Weikum</p><p>Abstract: Methods for information extraction (IE) and knowledge base (KB) construction have been intensively studied. However, a largely under-explored case is tapping into highly dynamic sources like news streams and social media, where new entities are continuously emerging. In this paper, we present a method for discovering and semantically typing newly emerging out-ofKB entities, thus improving the freshness and recall of ontology-based IE and improving the precision and semantic rigor of open IE. Our method is based on a probabilistic model that feeds weights into integer linear programs that leverage type signatures of relational phrases and type correlation or disjointness constraints. Our experimental evaluation, based on crowdsourced user studies, show our method performing significantly better than prior work.</p><p>4 0.96805328 <a title="55-lda-4" href="./acl-2013-Building_and_Evaluating_a_Distributional_Memory_for_Croatian.html">76 acl-2013-Building and Evaluating a Distributional Memory for Croatian</a></p>
<p>Author: Jan Snajder ; Sebastian Pado ; Zeljko Agic</p><p>Abstract: We report on the first structured distributional semantic model for Croatian, DM.HR. It is constructed after the model of the English Distributional Memory (Baroni and Lenci, 2010), from a dependencyparsed Croatian web corpus, and covers about 2M lemmas. We give details on the linguistic processing and the design principles. An evaluation shows state-of-theart performance on a semantic similarity task with particularly good performance on nouns. The resource is freely available.</p><p>5 0.96104795 <a title="55-lda-5" href="./acl-2013-Semantic_Neighborhoods_as_Hypergraphs.html">311 acl-2013-Semantic Neighborhoods as Hypergraphs</a></p>
<p>Author: Chris Quirk ; Pallavi Choudhury</p><p>Abstract: Ambiguity preserving representations such as lattices are very useful in a number of NLP tasks, including paraphrase generation, paraphrase recognition, and machine translation evaluation. Lattices compactly represent lexical variation, but word order variation leads to a combinatorial explosion of states. We advocate hypergraphs as compact representations for sets of utterances describing the same event or object. We present a method to construct hypergraphs from sets of utterances, and evaluate this method on a simple recognition task. Given a set of utterances that describe a single object or event, we construct such a hypergraph, and demonstrate that it can recognize novel descriptions of the same event with high accuracy.</p><p>6 0.95623314 <a title="55-lda-6" href="./acl-2013-A_relatedness_benchmark_to_test_the_role_of_determiners_in_compositional_distributional_semantics.html">32 acl-2013-A relatedness benchmark to test the role of determiners in compositional distributional semantics</a></p>
<p>7 0.94955581 <a title="55-lda-7" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>8 0.94782317 <a title="55-lda-8" href="./acl-2013-Discriminative_Approach_to_Fill-in-the-Blank_Quiz_Generation_for_Language_Learners.html">122 acl-2013-Discriminative Approach to Fill-in-the-Blank Quiz Generation for Language Learners</a></p>
<p>9 0.80187017 <a title="55-lda-9" href="./acl-2013-Automatic_Coupling_of_Answer_Extraction_and_Information_Retrieval.html">60 acl-2013-Automatic Coupling of Answer Extraction and Information Retrieval</a></p>
<p>10 0.80153054 <a title="55-lda-10" href="./acl-2013-Measuring_semantic_content_in_distributional_vectors.html">238 acl-2013-Measuring semantic content in distributional vectors</a></p>
<p>11 0.78317314 <a title="55-lda-11" href="./acl-2013-Derivational_Smoothing_for_Syntactic_Distributional_Semantics.html">113 acl-2013-Derivational Smoothing for Syntactic Distributional Semantics</a></p>
<p>12 0.77256191 <a title="55-lda-12" href="./acl-2013-Automated_Collocation_Suggestion_for_Japanese_Second_Language_Learners.html">58 acl-2013-Automated Collocation Suggestion for Japanese Second Language Learners</a></p>
<p>13 0.75530839 <a title="55-lda-13" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>14 0.75393951 <a title="55-lda-14" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>15 0.74307895 <a title="55-lda-15" href="./acl-2013-Feature-Based_Selection_of_Dependency_Paths_in_Ad_Hoc_Information_Retrieval.html">158 acl-2013-Feature-Based Selection of Dependency Paths in Ad Hoc Information Retrieval</a></p>
<p>16 0.72695875 <a title="55-lda-16" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>17 0.72621197 <a title="55-lda-17" href="./acl-2013-Learning_Entity_Representation_for_Entity_Disambiguation.html">219 acl-2013-Learning Entity Representation for Entity Disambiguation</a></p>
<p>18 0.72562498 <a title="55-lda-18" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>19 0.72313809 <a title="55-lda-19" href="./acl-2013-Topic_Modeling_Based_Classification_of_Clinical_Reports.html">351 acl-2013-Topic Modeling Based Classification of Clinical Reports</a></p>
<p>20 0.72126693 <a title="55-lda-20" href="./acl-2013-Unsupervised_joke_generation_from_big_data.html">371 acl-2013-Unsupervised joke generation from big data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
