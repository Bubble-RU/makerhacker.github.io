<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>73 acl-2013-Broadcast News Story Segmentation Using Manifold Learning on Latent Topic Distributions</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-73" href="#">acl2013-73</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>73 acl-2013-Broadcast News Story Segmentation Using Manifold Learning on Latent Topic Distributions</h1>
<br/><p>Source: <a title="acl-2013-73-pdf" href="http://aclweb.org/anthology//P/P13/P13-2034.pdf">pdf</a></p><p>Author: Xiaoming Lu ; Lei Xie ; Cheung-Chi Leung ; Bin Ma ; Haizhou Li</p><p>Abstract: We present an efficient approach for broadcast news story segmentation using a manifold learning algorithm on latent topic distributions. The latent topic distribution estimated by Latent Dirichlet Allocation (LDA) is used to represent each text block. We employ Laplacian Eigenmaps (LE) to project the latent topic distributions into low-dimensional semantic representations while preserving the intrinsic local geometric structure. We evaluate two approaches employing LDA and probabilistic latent semantic analysis (PLSA) distributions respectively. The effects of different amounts of training data and different numbers of latent topics on the two approaches are studied. Experimental re- sults show that our proposed LDA-based approach can outperform the corresponding PLSA-based approach. The proposed approach provides the best performance with the highest F1-measure of 0.7860.</p><p>Reference: <a title="acl-2013-73-reference" href="../acl2013_reference/acl-2013-Broadcast_News_Story_Segmentation_Using_Manifold_Learning_on_Latent_Topic_Distributions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract We present an efficient approach for broadcast news story segmentation using a manifold learning algorithm on latent topic distributions. [sent-4, score-1.009]
</p><p>2 The latent topic distribution estimated by Latent Dirichlet Allocation (LDA) is used to represent each text block. [sent-5, score-0.43]
</p><p>3 We employ Laplacian Eigenmaps (LE) to project the latent topic distributions into low-dimensional semantic representations while preserving the intrinsic local geometric structure. [sent-6, score-0.833]
</p><p>4 We evaluate two approaches employing LDA and probabilistic latent semantic analysis (PLSA) distributions respectively. [sent-7, score-0.389]
</p><p>5 The effects of different amounts of training data and different numbers of latent topics on the two approaches are studied. [sent-8, score-0.321]
</p><p>6 Experimental re-  sults show that our proposed LDA-based approach can outperform the corresponding PLSA-based approach. [sent-9, score-0.038]
</p><p>7 1 Introduction Story segmentation refers to partitioning a multimedia stream into homogenous segments each embodying a main topic or coherent story (Allan, 2002). [sent-12, score-0.8]
</p><p>8 With the explosive growth of multimedia data, it becomes difficult to retrieve the most relevant components. [sent-13, score-0.127]
</p><p>9 For indexing broadcast news programs, it is desirable to divide each of them into a number of independent stories. [sent-14, score-0.214]
</p><p>10 Lexical-cohesion based approaches have been widely studied for automatic broadcast news story segmentation (Beeferman et al. [sent-17, score-0.546]
</p><p>11 In this kind of approaches, the audio portion of the data stream is passed to an automatic speech recognition (ASR) system. [sent-26, score-0.068]
</p><p>12 Lexical cohesion is the phenomenon that different stories tend to employ different sets of terms. [sent-28, score-0.059]
</p><p>13 Term repetition is one of the most common appearances. [sent-29, score-0.081]
</p><p>14 These rigid lexical-cohesion based approaches simply take term repetition into consideration, while term association in lexical cohesion is ignored. [sent-30, score-0.311]
</p><p>15 To deal with these problems, some topic model techniques which provide conceptual level matching have been introduced to text and story segmentation task (Hearst, 1997). [sent-32, score-0.53]
</p><p>16 Probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) is a typical instance and used widely. [sent-33, score-0.23]
</p><p>17 PLSA is the probabilistic variant of latent semantic analysis (LSA) (Choi et al. [sent-34, score-0.269]
</p><p>18 PLSA provides more significant improvement than LSA for story segmentation (Lu et al. [sent-36, score-0.382]
</p><p>19 This makes PLSA not desirable if there is a considerable amount of data available, and causes serious over-fitting problems (Blei, 2012). [sent-39, score-0.05]
</p><p>20 LDA has been proved to be effective in many segmentation tasks (Arora and Ravindran, 2008; Hall et al. [sent-42, score-0.165]
</p><p>21 Recent studies have shown that intrinsic dimensionality of natural text corpus is significantly lower than its ambient Euclidean space (Belkin and Niyogi, 2002; Xie et al. [sent-45, score-0.2]
</p><p>22 It projects data into  a low-dimensional representation while preserving the intrinsic local geometric structure information (Belkin and Niyogi, 2002). [sent-51, score-0.335]
</p><p>23 The locality preserving property attempts to make the lowdimensional data representation more robust to the noise from ASR errors (Xie et al. [sent-52, score-0.239]
</p><p>24 To further improve the segmentation performance, using latent topic distributions and LE instead of term frequencies to represent text blocks is studied in this paper. [sent-54, score-0.814]
</p><p>25 We study the effects of the size of training data and the number of latent topics on the LDA-based and the PLSA-based approaches. [sent-55, score-0.321]
</p><p>26 , 2013) is to use local geometric information to regularize the log-likelihood computation in PLSA. [sent-57, score-0.175]
</p><p>27 2 Our Proposed Approach In this paper, we propose to apply LE on the LDA topic distributions, each of which is estimated from a text block. [sent-58, score-0.148]
</p><p>28 The low-dimensional vectors obtained by LE projection are used to detect story boundaries through dynamic programming. [sent-59, score-0.322]
</p><p>29 , 2012), we incorporate  ×  the temporal distances between block pairs as a penalty factor in the weight matrix. [sent-61, score-0.178]
</p><p>30 1 Latent Dirichlet Allocation Latent Dirichlet allocation (LDA) (Blei et al. [sent-63, score-0.092]
</p><p>31 , 2003) is a generative probabilistic model of a corpus. [sent-64, score-0.039]
</p><p>32 It considers that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over terms. [sent-65, score-0.486]
</p><p>33 , dM} anIdn a DseAt o, fg tveernm as cWor = (w1, w2, . [sent-69, score-0.039]
</p><p>34 , wV), the} generative process can be summarized as follows: 1) For each document d, pick a multinomial distribution θ from a Dirichlet distribution parameter α, denoted as θ ∼ Dir(α). [sent-72, score-0.188]
</p><p>35 2) For each term w in document d, select a topic z from the multinomial distribution θ, denoted as z ∼ Multinomial(θ). [sent-73, score-0.347]
</p><p>36 3) Select a term w from P(w|z, β), which is a mu3lt)i nSoelmeicatl probability ocmon Pdi(twio|nze,dβ on wthhei topic. [sent-74, score-0.063]
</p><p>37 An LDA model is characterized by two sets of prior parameters α and β. [sent-75, score-0.056]
</p><p>38 , αK) represents the Dirichlet prior distributions for each K latent topics. [sent-79, score-0.35]
</p><p>39 β is a K V matrix, which defines  µ  tKhe la altaetnentt to topic d βis itsr aib Kuti×onVs over itxe,rm wsh. [sent-80, score-0.148]
</p><p>40 2 Construction of weight matrix in Laplacian Eigenmaps Laplacian Eigenmaps (LE) is introduced to project high-dimensional data into a low-dimensional representation while preserving its locality property. [sent-82, score-0.394]
</p><p>41 Given the ASR transcripts of N text blocks, we apply LDA algorithm to compute the corresponding latent topic distributions X = [x1, x2, . [sent-83, score-0.533]
</p><p>42 , xN] in RK, where K is the number of latent topics, namely the dimensionality of LDA distributions. [sent-86, score-0.306]
</p><p>43 We use G to denote an N-node (N is number of LDA distributions) graph which represents the relationship between all the text block pairs. [sent-87, score-0.079]
</p><p>44 If distribution vectors xi and xj come from the same story, we put an edge between nodes iand j. [sent-88, score-0.256]
</p><p>45 We define a weight matrix S of the graph G to denote the cohesive strength between the text block pairs. [sent-89, score-0.386]
</p><p>46 Each element of this weight matrix is defined as:  xj)µ|i−j|,  = cos(xi, (1) where µ|i−j| serves the penalty factor for the distance between iand j. [sent-90, score-0.339]
</p><p>47 sij  It makes the cohesive strength of two text blocks dramatically decrease when their distance is much larger than the normal length of a story. [sent-93, score-0.278]
</p><p>48 3 Data projection in Laplacian Eigenmaps Given the weight matrix S, we define C as the diagonal matrix with its element:  cij=∑iK=1sij. [sent-95, score-0.4]
</p><p>49 (2)  Finally, we obtain the Laplacian matrix L, which is defined as: L = C S. [sent-96, score-0.141]
</p><p>50 , yN] (yi is a column vector) to indicate the low-dimensional representation of the latent topic distributions X. [sent-100, score-0.498]
</p><p>51 The projection from the latent topic distribution space to the target space can be defined as: −  f : xi ⇒  yi. [sent-101, score-0.548]
</p><p>52 (4)  A reasonable criterion for computing an optimal mapping is to minimize the objective as follows: ∑K  ∑K  i∑=1j∑=1∥ yi− yj ∥2sij. [sent-102, score-0.065]
</p><p>53 (5)  Under this constraint condition, we can preserve the local geometrical property in LDA distributions. [sent-103, score-0.12]
</p><p>54 (6)  Meanwhile, zero matrix and matrices with its rank less than K are meaningless solutions for our task. [sent-105, score-0.184]
</p><p>55 By the Reyleigh-Ritz theorem (Lutkepohl, 1997), the solution can obtained by the Q smallest eigenvalues of the generalized eigenmaps problem:  XLXTy  = λXCXTy. [sent-107, score-0.489]
</p><p>56 (7)  With this formula, we calculate the mapping matrix Y, and its row vectors y′1,y′2, . [sent-108, score-0.18]
</p><p>57 ,y′Q are in the order of their eigenvalues λ1 λ2 . [sent-111, score-0.103]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('eigenmaps', 0.349), ('laplacian', 0.309), ('plsa', 0.275), ('lda', 0.253), ('latent', 0.23), ('story', 0.217), ('segmentation', 0.165), ('topic', 0.148), ('asr', 0.147), ('matrix', 0.141), ('preserving', 0.125), ('distributions', 0.12), ('broadcast', 0.118), ('ytly', 0.116), ('le', 0.115), ('xie', 0.114), ('dirichlet', 0.107), ('eigenvalues', 0.103), ('geometric', 0.093), ('allocation', 0.092), ('belkin', 0.089), ('niyogi', 0.089), ('blocks', 0.088), ('blei', 0.086), ('manifold', 0.085), ('multinomial', 0.084), ('repetition', 0.081), ('block', 0.079), ('dimensionality', 0.076), ('locality', 0.076), ('multimedia', 0.076), ('sij', 0.076), ('yi', 0.075), ('cohesive', 0.074), ('intrinsic', 0.073), ('hearst', 0.07), ('stream', 0.068), ('projection', 0.066), ('yj', 0.065), ('term', 0.063), ('iand', 0.061), ('cohesion', 0.059), ('characterized', 0.056), ('lsa', 0.055), ('topics', 0.055), ('choi', 0.054), ('weight', 0.052), ('xj', 0.052), ('distribution', 0.052), ('xi', 0.052), ('geometrically', 0.051), ('ambient', 0.051), ('cwor', 0.051), ('explosive', 0.051), ('infocomm', 0.051), ('kuti', 0.051), ('pdi', 0.051), ('rosenberg', 0.051), ('twio', 0.051), ('desirable', 0.05), ('chien', 0.047), ('homogenous', 0.047), ('malioutov', 0.047), ('riedl', 0.047), ('xiaoming', 0.047), ('penalty', 0.047), ('news', 0.046), ('rigid', 0.045), ('beeferman', 0.045), ('cij', 0.045), ('moreno', 0.045), ('northwestern', 0.045), ('tkhe', 0.045), ('wv', 0.045), ('local', 0.044), ('meaningless', 0.043), ('embodying', 0.043), ('lu', 0.042), ('strength', 0.04), ('vectors', 0.039), ('fg', 0.039), ('probabilistic', 0.039), ('geometrical', 0.038), ('cos', 0.038), ('sults', 0.038), ('hirschberg', 0.038), ('regularize', 0.038), ('element', 0.038), ('property', 0.038), ('synonymy', 0.037), ('biemann', 0.037), ('theorem', 0.037), ('tur', 0.037), ('dir', 0.036), ('partitioning', 0.036), ('rk', 0.036), ('effects', 0.036), ('star', 0.035), ('hofmann', 0.035), ('transcripts', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="73-tfidf-1" href="./acl-2013-Broadcast_News_Story_Segmentation_Using_Manifold_Learning_on_Latent_Topic_Distributions.html">73 acl-2013-Broadcast News Story Segmentation Using Manifold Learning on Latent Topic Distributions</a></p>
<p>Author: Xiaoming Lu ; Lei Xie ; Cheung-Chi Leung ; Bin Ma ; Haizhou Li</p><p>Abstract: We present an efficient approach for broadcast news story segmentation using a manifold learning algorithm on latent topic distributions. The latent topic distribution estimated by Latent Dirichlet Allocation (LDA) is used to represent each text block. We employ Laplacian Eigenmaps (LE) to project the latent topic distributions into low-dimensional semantic representations while preserving the intrinsic local geometric structure. We evaluate two approaches employing LDA and probabilistic latent semantic analysis (PLSA) distributions respectively. The effects of different amounts of training data and different numbers of latent topics on the two approaches are studied. Experimental re- sults show that our proposed LDA-based approach can outperform the corresponding PLSA-based approach. The proposed approach provides the best performance with the highest F1-measure of 0.7860.</p><p>2 0.18095234 <a title="73-tfidf-2" href="./acl-2013-Topic_Modeling_Based_Classification_of_Clinical_Reports.html">351 acl-2013-Topic Modeling Based Classification of Clinical Reports</a></p>
<p>Author: Efsun Sarioglu ; Kabir Yadav ; Hyeong-Ah Choi</p><p>Abstract: Kabir Yadav Emergency Medicine Department The George Washington University Washington, DC, USA kyadav@ gwu . edu Hyeong-Ah Choi Computer Science Department The George Washington University Washington, DC, USA hcho i gwu . edu @ such as recommending the need for a certain medical test while avoiding intrusive tests or medical Electronic health records (EHRs) contain important clinical information about pa- tients. Some of these data are in the form of free text and require preprocessing to be able to used in automated systems. Efficient and effective use of this data could be vital to the speed and quality of health care. As a case study, we analyzed classification of CT imaging reports into binary categories. In addition to regular text classification, we utilized topic modeling of the entire dataset in various ways. Topic modeling of the corpora provides interpretable themes that exist in these reports. Representing reports according to their topic distributions is more compact than bag-of-words representation and can be processed faster than raw text in subsequent automated processes. A binary topic model was also built as an unsupervised classification approach with the assumption that each topic corresponds to a class. And, finally an aggregate topic classifier was built where reports are classified based on a single discriminative topic that is determined from the training dataset. Our proposed topic based classifier system is shown to be competitive with existing text classification techniques and provides a more efficient and interpretable representation.</p><p>3 0.17950153 <a title="73-tfidf-3" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>Author: Zede Zhu ; Miao Li ; Lei Chen ; Zhenxin Yang</p><p>Abstract: Comparable corpora are important basic resources in cross-language information processing. However, the existing methods of building comparable corpora, which use intertranslate words and relative features, cannot evaluate the topical relation between document pairs. This paper adopts the bilingual LDA model to predict the topical structures of the documents and proposes three algorithms of document similarity in different languages. Experiments show that the novel method can obtain similar documents with consistent top- ics own better adaptability and stability performance.</p><p>4 0.17843911 <a title="73-tfidf-4" href="./acl-2013-Are_Semantically_Coherent_Topic_Models_Useful_for_Ad_Hoc_Information_Retrieval%3F.html">55 acl-2013-Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?</a></p>
<p>Author: Romain Deveaud ; Eric SanJuan ; Patrice Bellot</p><p>Abstract: The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics. More, the semantic coherence of the topics has never been considered in this field. We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics. We perform a first experimental evaluation using two major TREC test collections. Results show that retrieval perfor- mances tend to be better when using topics with higher semantic coherence.</p><p>5 0.15223889 <a title="73-tfidf-5" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>Author: Tsutomu Hirao ; Tomoharu Iwata ; Masaaki Nagata</p><p>Abstract: Unsupervised object matching (UOM) is a promising approach to cross-language natural language processing such as bilingual lexicon acquisition, parallel corpus construction, and cross-language text categorization, because it does not require labor-intensive linguistic resources. However, UOM only finds one-to-one correspondences from data sets with the same number of instances in source and target domains, and this prevents us from applying UOM to real-world cross-language natural language processing tasks. To alleviate these limitations, we proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic space. We demonstrate the effectiveness of our method on cross-language text categorization. The results show that our method outperforms conventional unsupervised object matching methods.</p><p>6 0.15147743 <a title="73-tfidf-6" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<p>7 0.1152233 <a title="73-tfidf-7" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>8 0.1075877 <a title="73-tfidf-8" href="./acl-2013-A_Two_Level_Model_for_Context_Sensitive_Inference_Rules.html">27 acl-2013-A Two Level Model for Context Sensitive Inference Rules</a></p>
<p>9 0.10037092 <a title="73-tfidf-9" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>10 0.095912762 <a title="73-tfidf-10" href="./acl-2013-Discriminative_Learning_with_Natural_Annotations%3A_Word_Segmentation_as_a_Case_Study.html">123 acl-2013-Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study</a></p>
<p>11 0.09268789 <a title="73-tfidf-11" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>12 0.091082379 <a title="73-tfidf-12" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>13 0.086694062 <a title="73-tfidf-13" href="./acl-2013-Exploiting_Topic_based_Twitter_Sentiment_for_Stock_Prediction.html">147 acl-2013-Exploiting Topic based Twitter Sentiment for Stock Prediction</a></p>
<p>14 0.085274525 <a title="73-tfidf-14" href="./acl-2013-Evaluating_Text_Segmentation_using_Boundary_Edit_Distance.html">140 acl-2013-Evaluating Text Segmentation using Boundary Edit Distance</a></p>
<p>15 0.084749356 <a title="73-tfidf-15" href="./acl-2013-On_the_Predictability_of_Human_Assessment%3A_when_Matrix_Completion_Meets_NLP_Evaluation.html">263 acl-2013-On the Predictability of Human Assessment: when Matrix Completion Meets NLP Evaluation</a></p>
<p>16 0.080250606 <a title="73-tfidf-16" href="./acl-2013-Linking_Tweets_to_News%3A_A_Framework_to_Enrich_Short_Text_Data_in_Social_Media.html">233 acl-2013-Linking Tweets to News: A Framework to Enrich Short Text Data in Social Media</a></p>
<p>17 0.0788913 <a title="73-tfidf-17" href="./acl-2013-Improved_Bayesian_Logistic_Supervised_Topic_Models_with_Data_Augmentation.html">191 acl-2013-Improved Bayesian Logistic Supervised Topic Models with Data Augmentation</a></p>
<p>18 0.078262612 <a title="73-tfidf-18" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>19 0.077826954 <a title="73-tfidf-19" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>20 0.07535658 <a title="73-tfidf-20" href="./acl-2013-Crowd_Prefers_the_Middle_Path%3A_A_New_IAA_Metric_for_Crowdsourcing_Reveals_Turker_Biases_in_Query_Segmentation.html">99 acl-2013-Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.164), (1, 0.054), (2, -0.011), (3, -0.025), (4, 0.171), (5, -0.098), (6, 0.066), (7, -0.019), (8, -0.224), (9, -0.035), (10, 0.097), (11, 0.071), (12, 0.176), (13, 0.103), (14, 0.025), (15, -0.059), (16, -0.048), (17, 0.068), (18, -0.006), (19, -0.027), (20, 0.013), (21, 0.02), (22, -0.044), (23, -0.053), (24, -0.022), (25, -0.005), (26, -0.029), (27, -0.012), (28, -0.065), (29, 0.024), (30, 0.062), (31, -0.006), (32, 0.034), (33, 0.004), (34, 0.031), (35, -0.038), (36, 0.028), (37, 0.038), (38, -0.003), (39, 0.099), (40, 0.043), (41, -0.008), (42, -0.019), (43, 0.024), (44, -0.127), (45, 0.058), (46, -0.017), (47, 0.079), (48, -0.093), (49, -0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98257524 <a title="73-lsi-1" href="./acl-2013-Broadcast_News_Story_Segmentation_Using_Manifold_Learning_on_Latent_Topic_Distributions.html">73 acl-2013-Broadcast News Story Segmentation Using Manifold Learning on Latent Topic Distributions</a></p>
<p>Author: Xiaoming Lu ; Lei Xie ; Cheung-Chi Leung ; Bin Ma ; Haizhou Li</p><p>Abstract: We present an efficient approach for broadcast news story segmentation using a manifold learning algorithm on latent topic distributions. The latent topic distribution estimated by Latent Dirichlet Allocation (LDA) is used to represent each text block. We employ Laplacian Eigenmaps (LE) to project the latent topic distributions into low-dimensional semantic representations while preserving the intrinsic local geometric structure. We evaluate two approaches employing LDA and probabilistic latent semantic analysis (PLSA) distributions respectively. The effects of different amounts of training data and different numbers of latent topics on the two approaches are studied. Experimental re- sults show that our proposed LDA-based approach can outperform the corresponding PLSA-based approach. The proposed approach provides the best performance with the highest F1-measure of 0.7860.</p><p>2 0.79296952 <a title="73-lsi-2" href="./acl-2013-Are_Semantically_Coherent_Topic_Models_Useful_for_Ad_Hoc_Information_Retrieval%3F.html">55 acl-2013-Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?</a></p>
<p>Author: Romain Deveaud ; Eric SanJuan ; Patrice Bellot</p><p>Abstract: The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics. More, the semantic coherence of the topics has never been considered in this field. We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics. We perform a first experimental evaluation using two major TREC test collections. Results show that retrieval perfor- mances tend to be better when using topics with higher semantic coherence.</p><p>3 0.76365358 <a title="73-lsi-3" href="./acl-2013-Topic_Modeling_Based_Classification_of_Clinical_Reports.html">351 acl-2013-Topic Modeling Based Classification of Clinical Reports</a></p>
<p>Author: Efsun Sarioglu ; Kabir Yadav ; Hyeong-Ah Choi</p><p>Abstract: Kabir Yadav Emergency Medicine Department The George Washington University Washington, DC, USA kyadav@ gwu . edu Hyeong-Ah Choi Computer Science Department The George Washington University Washington, DC, USA hcho i gwu . edu @ such as recommending the need for a certain medical test while avoiding intrusive tests or medical Electronic health records (EHRs) contain important clinical information about pa- tients. Some of these data are in the form of free text and require preprocessing to be able to used in automated systems. Efficient and effective use of this data could be vital to the speed and quality of health care. As a case study, we analyzed classification of CT imaging reports into binary categories. In addition to regular text classification, we utilized topic modeling of the entire dataset in various ways. Topic modeling of the corpora provides interpretable themes that exist in these reports. Representing reports according to their topic distributions is more compact than bag-of-words representation and can be processed faster than raw text in subsequent automated processes. A binary topic model was also built as an unsupervised classification approach with the assumption that each topic corresponds to a class. And, finally an aggregate topic classifier was built where reports are classified based on a single discriminative topic that is determined from the training dataset. Our proposed topic based classifier system is shown to be competitive with existing text classification techniques and provides a more efficient and interpretable representation.</p><p>4 0.74573088 <a title="73-lsi-4" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>Author: Tsutomu Hirao ; Tomoharu Iwata ; Masaaki Nagata</p><p>Abstract: Unsupervised object matching (UOM) is a promising approach to cross-language natural language processing such as bilingual lexicon acquisition, parallel corpus construction, and cross-language text categorization, because it does not require labor-intensive linguistic resources. However, UOM only finds one-to-one correspondences from data sets with the same number of instances in source and target domains, and this prevents us from applying UOM to real-world cross-language natural language processing tasks. To alleviate these limitations, we proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic space. We demonstrate the effectiveness of our method on cross-language text categorization. The results show that our method outperforms conventional unsupervised object matching methods.</p><p>5 0.69663501 <a title="73-lsi-5" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>Author: Zede Zhu ; Miao Li ; Lei Chen ; Zhenxin Yang</p><p>Abstract: Comparable corpora are important basic resources in cross-language information processing. However, the existing methods of building comparable corpora, which use intertranslate words and relative features, cannot evaluate the topical relation between document pairs. This paper adopts the bilingual LDA model to predict the topical structures of the documents and proposes three algorithms of document similarity in different languages. Experiments show that the novel method can obtain similar documents with consistent top- ics own better adaptability and stability performance.</p><p>6 0.68510765 <a title="73-lsi-6" href="./acl-2013-Improved_Bayesian_Logistic_Supervised_Topic_Models_with_Data_Augmentation.html">191 acl-2013-Improved Bayesian Logistic Supervised Topic Models with Data Augmentation</a></p>
<p>7 0.68441349 <a title="73-lsi-7" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>8 0.67403799 <a title="73-lsi-8" href="./acl-2013-Diverse_Keyword_Extraction_from_Conversations.html">126 acl-2013-Diverse Keyword Extraction from Conversations</a></p>
<p>9 0.63844651 <a title="73-lsi-9" href="./acl-2013-Are_School-of-thought_Words_Characterizable%3F.html">54 acl-2013-Are School-of-thought Words Characterizable?</a></p>
<p>10 0.58312374 <a title="73-lsi-10" href="./acl-2013-Exploiting_Topic_based_Twitter_Sentiment_for_Stock_Prediction.html">147 acl-2013-Exploiting Topic based Twitter Sentiment for Stock Prediction</a></p>
<p>11 0.57755637 <a title="73-lsi-11" href="./acl-2013-Learning_Latent_Personas_of_Film_Characters.html">220 acl-2013-Learning Latent Personas of Film Characters</a></p>
<p>12 0.56624764 <a title="73-lsi-12" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<p>13 0.55799669 <a title="73-lsi-13" href="./acl-2013-Evolutionary_Hierarchical_Dirichlet_Process_for_Timeline_Summarization.html">142 acl-2013-Evolutionary Hierarchical Dirichlet Process for Timeline Summarization</a></p>
<p>14 0.54306388 <a title="73-lsi-14" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>15 0.53048265 <a title="73-lsi-15" href="./acl-2013-Natural_Language_Models_for_Predicting_Programming_Comments.html">257 acl-2013-Natural Language Models for Predicting Programming Comments</a></p>
<p>16 0.48918563 <a title="73-lsi-16" href="./acl-2013-TopicSpam%3A_a_Topic-Model_based_approach_for_spam_detection.html">350 acl-2013-TopicSpam: a Topic-Model based approach for spam detection</a></p>
<p>17 0.48682541 <a title="73-lsi-17" href="./acl-2013-Variable_Bit_Quantisation_for_LSH.html">381 acl-2013-Variable Bit Quantisation for LSH</a></p>
<p>18 0.47990909 <a title="73-lsi-18" href="./acl-2013-Unsupervised_Transcription_of_Historical_Documents.html">370 acl-2013-Unsupervised Transcription of Historical Documents</a></p>
<p>19 0.46751237 <a title="73-lsi-19" href="./acl-2013-Semi-Supervised_Semantic_Tagging_of_Conversational_Understanding_using_Markov_Topic_Regression.html">315 acl-2013-Semi-Supervised Semantic Tagging of Conversational Understanding using Markov Topic Regression</a></p>
<p>20 0.45951745 <a title="73-lsi-20" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.038), (2, 0.254), (6, 0.024), (11, 0.053), (15, 0.015), (24, 0.085), (26, 0.048), (35, 0.131), (42, 0.045), (48, 0.042), (70, 0.078), (88, 0.02), (90, 0.035), (95, 0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90326464 <a title="73-lda-1" href="./acl-2013-Nonparametric_Bayesian_Inference_and_Efficient_Parsing_for_Tree-adjoining_Grammars.html">261 acl-2013-Nonparametric Bayesian Inference and Efficient Parsing for Tree-adjoining Grammars</a></p>
<p>Author: Elif Yamangil ; Stuart M. Shieber</p><p>Abstract: In the line of research extending statistical parsing to more expressive grammar formalisms, we demonstrate for the first time the use of tree-adjoining grammars (TAG). We present a Bayesian nonparametric model for estimating a probabilistic TAG from a parsed corpus, along with novel block sampling methods and approximation transformations for TAG that allow efficient parsing. Our work shows performance improvements on the Penn Treebank and finds more compact yet linguistically rich representations of the data, but more importantly provides techniques in grammar transformation and statistical inference that make practical the use of these more expressive systems, thereby enabling further experimentation along these lines.</p><p>same-paper 2 0.84242481 <a title="73-lda-2" href="./acl-2013-Broadcast_News_Story_Segmentation_Using_Manifold_Learning_on_Latent_Topic_Distributions.html">73 acl-2013-Broadcast News Story Segmentation Using Manifold Learning on Latent Topic Distributions</a></p>
<p>Author: Xiaoming Lu ; Lei Xie ; Cheung-Chi Leung ; Bin Ma ; Haizhou Li</p><p>Abstract: We present an efficient approach for broadcast news story segmentation using a manifold learning algorithm on latent topic distributions. The latent topic distribution estimated by Latent Dirichlet Allocation (LDA) is used to represent each text block. We employ Laplacian Eigenmaps (LE) to project the latent topic distributions into low-dimensional semantic representations while preserving the intrinsic local geometric structure. We evaluate two approaches employing LDA and probabilistic latent semantic analysis (PLSA) distributions respectively. The effects of different amounts of training data and different numbers of latent topics on the two approaches are studied. Experimental re- sults show that our proposed LDA-based approach can outperform the corresponding PLSA-based approach. The proposed approach provides the best performance with the highest F1-measure of 0.7860.</p><p>3 0.73345602 <a title="73-lda-3" href="./acl-2013-A_Context_Free_TAG_Variant.html">4 acl-2013-A Context Free TAG Variant</a></p>
<p>Author: Ben Swanson ; Elif Yamangil ; Eugene Charniak ; Stuart Shieber</p><p>Abstract: We propose a new variant of TreeAdjoining Grammar that allows adjunction of full wrapping trees but still bears only context-free expressivity. We provide a transformation to context-free form, and a further reduction in probabilistic model size through factorization and pooling of parameters. This collapsed context-free form is used to implement efficient gram- mar estimation and parsing algorithms. We perform parsing experiments the Penn Treebank and draw comparisons to TreeSubstitution Grammars and between different variations in probabilistic model design. Examination of the most probable derivations reveals examples of the linguistically relevant structure that our variant makes possible.</p><p>4 0.72033453 <a title="73-lda-4" href="./acl-2013-Real-World_Semi-Supervised_Learning_of_POS-Taggers_for_Low-Resource_Languages.html">295 acl-2013-Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages</a></p>
<p>Author: Dan Garrette ; Jason Mielens ; Jason Baldridge</p><p>Abstract: Developing natural language processing tools for low-resource languages often requires creating resources from scratch. While a variety of semi-supervised methods exist for training from incomplete data, there are open questions regarding what types of training data should be used and how much is necessary. We discuss a series of experiments designed to shed light on such questions in the context of part-of-speech tagging. We obtain timed annotations from linguists for the low-resource languages Kinyarwanda and Malagasy (as well as English) and eval- uate how the amounts of various kinds of data affect performance of a trained POS-tagger. Our results show that annotation of word types is the most important, provided a sufficiently capable semi-supervised learning infrastructure is in place to project type information onto a raw corpus. We also show that finitestate morphological analyzers are effective sources of type information when few labeled examples are available.</p><p>5 0.69452578 <a title="73-lda-5" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>Author: Richard Socher ; John Bauer ; Christopher D. Manning ; Ng Andrew Y.</p><p>Abstract: Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%. It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.</p><p>6 0.61262125 <a title="73-lda-6" href="./acl-2013-Arguments_and_Modifiers_from_the_Learner%27s_Perspective.html">57 acl-2013-Arguments and Modifiers from the Learner's Perspective</a></p>
<p>7 0.61171389 <a title="73-lda-7" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<p>8 0.60833383 <a title="73-lda-8" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>9 0.60571438 <a title="73-lda-9" href="./acl-2013-Paraphrase-Driven_Learning_for_Open_Question_Answering.html">272 acl-2013-Paraphrase-Driven Learning for Open Question Answering</a></p>
<p>10 0.60320157 <a title="73-lda-10" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>11 0.60311365 <a title="73-lda-11" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>12 0.60135299 <a title="73-lda-12" href="./acl-2013-Improving_Text_Simplification_Language_Modeling_Using_Unsimplified_Text_Data.html">194 acl-2013-Improving Text Simplification Language Modeling Using Unsimplified Text Data</a></p>
<p>13 0.60071903 <a title="73-lda-13" href="./acl-2013-Learning_to_Extract_International_Relations_from_Political_Context.html">224 acl-2013-Learning to Extract International Relations from Political Context</a></p>
<p>14 0.6006788 <a title="73-lda-14" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>15 0.59785777 <a title="73-lda-15" href="./acl-2013-Feature-Based_Selection_of_Dependency_Paths_in_Ad_Hoc_Information_Retrieval.html">158 acl-2013-Feature-Based Selection of Dependency Paths in Ad Hoc Information Retrieval</a></p>
<p>16 0.59783161 <a title="73-lda-16" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>17 0.59664506 <a title="73-lda-17" href="./acl-2013-Automatic_Coupling_of_Answer_Extraction_and_Information_Retrieval.html">60 acl-2013-Automatic Coupling of Answer Extraction and Information Retrieval</a></p>
<p>18 0.59624743 <a title="73-lda-18" href="./acl-2013-Models_of_Semantic_Representation_with_Visual_Attributes.html">249 acl-2013-Models of Semantic Representation with Visual Attributes</a></p>
<p>19 0.59565622 <a title="73-lda-19" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>20 0.59428674 <a title="73-lda-20" href="./acl-2013-Diverse_Keyword_Extraction_from_Conversations.html">126 acl-2013-Diverse Keyword Extraction from Conversations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
