<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-217" href="#">acl2013-217</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</h1>
<br/><p>Source: <a title="acl-2013-217-pdf" href="http://aclweb.org/anthology//P/P13/P13-2038.pdf">pdf</a></p><p>Author: Tsutomu Hirao ; Tomoharu Iwata ; Masaaki Nagata</p><p>Abstract: Unsupervised object matching (UOM) is a promising approach to cross-language natural language processing such as bilingual lexicon acquisition, parallel corpus construction, and cross-language text categorization, because it does not require labor-intensive linguistic resources. However, UOM only finds one-to-one correspondences from data sets with the same number of instances in source and target domains, and this prevents us from applying UOM to real-world cross-language natural language processing tasks. To alleviate these limitations, we proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic space. We demonstrate the effectiveness of our method on cross-language text categorization. The results show that our method outperforms conventional unsupervised object matching methods.</p><p>Reference: <a title="acl-2013-217-reference" href="../acl2013_reference/acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 j p ab  Abstract Unsupervised object matching (UOM) is a promising approach to cross-language natural language processing such as bilingual lexicon acquisition, parallel corpus construction, and cross-language text categorization, because it does not require labor-intensive linguistic resources. [sent-7, score-0.596]
</p><p>2 However, UOM only finds one-to-one correspondences from data sets with the same number of instances in source and target domains, and this prevents us from applying UOM to real-world cross-language natural language processing tasks. [sent-8, score-0.402]
</p><p>3 To alleviate these limitations, we proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic  space. [sent-9, score-1.245]
</p><p>4 We demonstrate the effectiveness of our method on cross-language text categorization. [sent-10, score-0.042]
</p><p>5 The results show that our method outperforms conventional unsupervised object matching methods. [sent-11, score-0.617]
</p><p>6 1 Introduction Unsupervised object matching is a method for finding one-to-one correspondences between objects across different domains without knowledge about the relation between the domains. [sent-12, score-0.838]
</p><p>7 , 2010) and canonical correlation analysis based methods (Haghighi et al. [sent-14, score-0.052]
</p><p>8 , 2010) are two such examples of unsupervised object matching, which have been shown to be quite useful for cross-language natural language processing (NLP) tasks. [sent-16, score-0.326]
</p><p>9 One of the most important properties of the unsupervised object matching is that it does not require any linguistic resources which connects between the languages. [sent-17, score-0.609]
</p><p>10 This distinguishes it from other crosslanguage NLP methods such as machine translation based and projection based approaches (Du-  mais et al. [sent-18, score-0.043]
</p><p>11 , 2010), which we need bilingual dictionaries or parallel sentences. [sent-20, score-0.082]
</p><p>12 When we apply unsupervised object matching methods to cross-language NLP tasks, there are two critical problems. [sent-21, score-0.563]
</p><p>13 The second is they require the same size of source- and target-data. [sent-23, score-0.046]
</p><p>14 These discussions motivate us to introduce a shared space in which both source and target domain objects will reside. [sent-27, score-0.492]
</p><p>15 If we can obtain such a shared space, we can match objects within the space, because we can use standard distance metrics on this space. [sent-28, score-0.17]
</p><p>16 For example, k-nearest objects in the source domain will be retrieved for a query object in the target domain. [sent-30, score-0.603]
</p><p>17 In this paper, we propose a simple but effective  method to find the shared space by assuming that two languages have common latent topics, which we call latent semantic matching. [sent-31, score-0.78]
</p><p>18 With latent semantic matching, we first find latent topics in two domains independently. [sent-32, score-0.936]
</p><p>19 Then, the topics in two domains are aligned by kernelized sorting, and objects are embedded in a shared latent topic space. [sent-33, score-1.19]
</p><p>20 Latent topic representations are successfully used in a wide range of NLP tasks, such as information retrieval and text classification, because they represent intrinsic information of documents (Deerwester et al. [sent-34, score-0.219]
</p><p>21 By matching latent topics, we can find relation between source and target domains, and additionally we can handle different numbers of objects in two domains. [sent-36, score-0.867]
</p><p>22 We compared latent semantic matching with conventional unsupervised object matching meth212  Proce dingSsof oifa, th Beu 5l1gsarti Aan,An u aglu Mste 4e-ti9n2g 0 o1f3 t. [sent-37, score-1.202]
</p><p>23 classifying target side unlabeled documents by label information obtained from source side documents. [sent-41, score-0.439]
</p><p>24 The results show that, with more  source side documents, our method achieved the highest classification accuracy. [sent-42, score-0.152]
</p><p>25 2  Related work  Many cross-language text processing methods have been proposed that require correspondences between source and target languages. [sent-43, score-0.397]
</p><p>26 , 1996) proposed cross-lingual latent semantic indexing, and (Platt et al. [sent-45, score-0.348]
</p><p>27 , 2010) employed oriented principle component analysis and canonical correlation analysis (CCA). [sent-46, score-0.052]
</p><p>28 They concatenate the document pairs (source document and its translation) obtained from a documentlevel parallel corpus. [sent-47, score-0.163]
</p><p>29 They then apply multivariate analysis to acquire the translingual projection. [sent-48, score-0.061]
</p><p>30 There are extensions of latent Dirichlet allocation (LDA) (Blei et al. [sent-49, score-0.312]
</p><p>31 , 2003) for cross-language analysis, such as multilingual topic models (BoydGraber and Blei, 2009), joint LDA (Jagadeesh and Daume III, 2010) and multilingual LDA (Xiaochuan et al. [sent-50, score-0.185]
</p><p>32 They require a bilingual dictionary or document-level parallel corpora. [sent-52, score-0.128]
</p><p>33 Unsupervised object matching methods have been proposed recently (Novi et al. [sent-53, score-0.468]
</p><p>34 These methods are promising in terms of language portability because they do not require external language resources. [sent-56, score-0.046]
</p><p>35 , 2010) proposed kernelized sorting (KS); it finds one-toone correspondences between objects in different domains by permuting a set to maximize the dependence between two sets. [sent-58, score-0.874]
</p><p>36 , 2012) proposed convex kernelized sorting as an extension of KS. [sent-61, score-0.524]
</p><p>37 (Yamada and Sugiyama, 2011) proposed leastsquares object matching which maximizes the squared-loss mutual information between matched pairs. [sent-62, score-0.468]
</p><p>38 , 2008) proposed another framework, matching CCA (MCCA), based on a probabilistic interpretation of CCA (Bach and Jordan, 2005). [sent-64, score-0.237]
</p><p>39 MCCA simultaneously finds latent variables that represent correspondences and latent features so that the latent features of corresponding examples exhibit the maximum correla-  tion. [sent-65, score-1.131]
</p><p>40 They require that the source and target domains have the same data size, and they find one-to-one correspondences. [sent-67, score-0.368]
</p><p>41 3  Latent Semantic Matching  We propose latent semantic matching to find a shared latent space by assuming that two languages have common latent topics. [sent-69, score-1.329]
</p><p>42 1 Topic Extraction as Dimension Reduction Suppose that we have N documents in the source domain. [sent-72, score-0.225]
</p><p>43 sn=(sni)iI=1 is the nth document represented as a multi-dimensional column vector in the domain, i. [sent-73, score-0.157]
</p><p>44 tm=(tmj)jJ=1 is the mth document represented as a multi-dimensional vector. [sent-81, score-0.215]
</p><p>45 Thus, the data set in the source domain is represented by an I N matrix, S=(s1 , · · · , sN), the data set ainn t Ihe × target aitsr represented by a J M matrix,  ,  T=(t1, · · · tM). [sent-83, score-0.325]
</p><p>46 We f,a·c·to·r ,izte these matrices using nonnegative matrix factorization (Lee and Seung, 2000) to find topics as follows: S  ≈  WSHS,  (1)  ×× T  ≈  WTHT. [sent-84, score-0.315]
</p><p>47 (2)  WS is an I K matrix that represents a set of topics, ii. [sent-85, score-0.083]
</p><p>48 ena Ich× cKol mumantr ivxe cthtoatr rdepenroesteesn tws oar sde weights for each topic. [sent-87, score-0.03]
</p><p>49 HS is a K N matrix that denotes a set of latent sseam a Knti ×c representations eofdocuments in the source domain, i. [sent-88, score-0.526]
</p><p>50 vector denotes an embedding of a document in the K-dimensional latent space. [sent-91, score-0.407]
</p><p>51 Similarly, WT is an I K matrix that represents a set of topics in the target domain, hanatd r HT eisn a K a s oMf t ompiactrsix in th thaet denotes a set of latent semisa nati Kc representations oaft target documents. [sent-92, score-0.804]
</p><p>52 and By factorizing the original matrices, we can independently map the documents in the source and target domains to the latent topic spaces whose dimensionality is K. [sent-94, score-0.897]
</p><p>53 2  Finding Optimal Topic Alignments by Unsupervised Object Matching To connect the different latent spaces, topics extracted from the source language must be aligned to one from the target language. [sent-96, score-0.716]
</p><p>54 This is reasonable because we can assume that both languages share the same latent concept. [sent-97, score-0.312]
</p><p>55 However, we cannot quantify the similarity between the topics because we do not have any external language resources such as a dictionary. [sent-98, score-0.161]
</p><p>56 ×  Therefore, we utilize unsupervised object matching method to find one-to-one correspondences between topics. [sent-99, score-0.707]
</p><p>57 In this paper, we employ kernelized sorting (KS) (Novi et al. [sent-100, score-0.453]
</p><p>58 KS finds the best one-to-one matching as follows:  π∗ = aπrg∈ΠmKaxtr(GSπ⊤GTπ), s. [sent-102, score-0.288]
</p><p>59 (3)  Here, π is a K K matrix that represents the onetHo-eroen,e π correspondence tbr eixtw theaetn r topics, it. [sent-105, score-0.083]
</p><p>60 t πij=1 indicates that the ith topic in the source language corresponds to the jth one of the target language. [sent-107, score-0.3]
</p><p>61 l0 l67la72nguagepairs  ×  ΠK indicates the set of all possible matrices storing one-to-one correspondences. [sent-112, score-0.071]
</p><p>62 G denotes the K K kernel matrix obtained froGm d topic proportion, Gij=K(Wi⊤,:, W:,j), naendd G fr iosm mth teo pceicnte prreodmpoarttriioxn o, fG G=. [sent-113, score-0.358]
</p><p>63 According to π∗, we obtain permuted matrices, WT=WTπ∗ and HT=π∗⊤HT, and the product oWf permuted maantdric Hes is the same with that of unpermuted matrices as follows: T ≈ WTHT=WTHT. [sent-118, score-0.313]
</p><p>64 Since documents from both domains are represented in a shared latent space, we can directly calculate the similarity between the nth document in the source domain and the mth document in the target domain based on HT:,m (mth column vector of HT) and HS:,n (nth column vector of HS). [sent-121, score-1.267]
</p><p>65 4  Cross-language Text Categorization via Latent Semantic Matching  Cross-language text categorization is the task of exploiting labeled documents in the source language (e. [sent-122, score-0.317]
</p><p>66 English) to classify documents in the target language (e. [sent-124, score-0.234]
</p><p>67 Suppose we have training data set {sn, yn}nN=1 in the source language dinogm daaitna. [sent-127, score-0.099]
</p><p>68 W∈e can str tahine a acslass lsiafbieerl in the K-dimensional latent space with data set {H⊤S:,n, yn}nN=1. [sent-129, score-0.373]
</p><p>69 Also, th}e mth document in the target language domain tm is projected into the latent space as HT⊤:,m. [sent-131, score-0.824]
</p><p>70 Here, the documents in both domains are projected into the same size latent space and the basis vectors of the spaces are aligned. [sent-132, score-0.703]
</p><p>71 Therefore, we can classify a document in the target domain tm by a classifier trained with {H⊤S:,n, yn}nN=1. [sent-133, score-0.286]
</p><p>72 1 Experimental Settings We compared our method, latent semantic matching (LSM), with three unsupervised object matching methods: Kernelized Sorting (KS), Convex Kernelized Sorting (CKS), Least-Squares Object Matching (LSOM). [sent-137, score-1.148]
</p><p>73 We set the number of the latent topics K to 100 and employed the k-nearest neighbor method (k=10) as the classifier. [sent-138, score-0.473]
</p><p>74 For, KS, CKS and LSOM, we find the oneto-one correspondence between documents in the source language and documents in the target language. [sent-139, score-0.459]
</p><p>75 Then, we assign class labels of the target documents according to the correspondence. [sent-140, score-0.234]
</p><p>76 The corpus is neither sentence level parallel nor comparable. [sent-144, score-0.037]
</p><p>77 For each category, we randomly select 60 documents as the test data (M=300) for all methods and 60 documents as the training data (N=300) for KS, CKS, LSOM and LSM(300). [sent-145, score-0.252]
</p><p>78 We also compared latent semantic matching with 120 training documents for each category (N=600), and called this method LSM(600). [sent-146, score-0.711]
</p><p>79 Note that since KS, CKS and LSOM require that the data sizes are the same for source and target domains, they cannot use training data more than test data. [sent-147, score-0.253]
</p><p>80 The results showed the effectiveness of both unsupervised object matching and latent semantic matching. [sent-153, score-0.953]
</p><p>81 When comparing LSM(300) with KS, CKS and LSOM, LSM(300) obtained better results than these unsupervised object matching methods. [sent-154, score-0.563]
</p><p>82 The result supports the effectiveness of the latent topic matching. [sent-155, score-0.447]
</p><p>83 This result implies not only the effectiveness of the latent topic matching but also increasing the number of source side documents (labeled training data) contributes to improving classification accuracy. [sent-158, score-0.962]
</p><p>84 This is natural in terms of supervised learning but only our method can deal with source side documents that are larger in number. [sent-159, score-0.278]
</p><p>85 Table 2 shows examples of latent topics in English and German extracted and aligned by LSM(600). [sent-160, score-0.509]
</p><p>86 We can see that some author names, words related to camera, and cooking equipment appear in ‘Books’, ‘Electronics’ and ‘Kitchen’ topics, respectively. [sent-161, score-0.06]
</p><p>87 and watch brands in  215  6  Conclusion  As an extension of unsupervised object matching, this paper proposed latent semantic matching that considers the shared latent space between two language domains. [sent-163, score-1.485]
</p><p>88 To generate such a space, topics of the target space are permuted by exploiting unsupervised object matching. [sent-164, score-0.777]
</p><p>89 We can measure distances between objects by standard metrics, which enable us retrieving k-nearest objects in the source domain for a query object in the target domain. [sent-165, score-0.714]
</p><p>90 This is a significant advantage over conventional unsupervised object matching methods. [sent-166, score-0.617]
</p><p>91 We used Amazon review corpus to demonstrate the effectiveness of our method on crosslanguage text categorization. [sent-167, score-0.085]
</p><p>92 The results showed that our method outperformed conventional object matching methods with the same number of training samples. [sent-168, score-0.522]
</p><p>93 Moreover, our method achieved even higher performance by utilizing more documents  in the source domain. [sent-169, score-0.225]
</p><p>94 Cross language text categorization by acquiring multilingual domain models from comparable corpora. [sent-204, score-0.192]
</p><p>95 Cross lingual text classification by mining multilingual topics from wikipedia. [sent-237, score-0.207]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('latent', 0.312), ('kernelized', 0.303), ('matching', 0.237), ('object', 0.231), ('lsm', 0.201), ('ks', 0.173), ('cks', 0.172), ('lsom', 0.172), ('novi', 0.172), ('topics', 0.161), ('sorting', 0.15), ('correspondences', 0.144), ('documents', 0.126), ('permuted', 0.121), ('mth', 0.12), ('ht', 0.117), ('domains', 0.115), ('watch', 0.112), ('objects', 0.111), ('target', 0.108), ('djuric', 0.103), ('source', 0.099), ('unsupervised', 0.095), ('topic', 0.093), ('categorization', 0.092), ('uom', 0.091), ('ntt', 0.084), ('cca', 0.084), ('matrix', 0.083), ('kitchen', 0.079), ('yn', 0.077), ('electronics', 0.075), ('matrices', 0.071), ('convex', 0.071), ('hs', 0.07), ('german', 0.069), ('mcca', 0.069), ('nemanja', 0.069), ('sugiyama', 0.069), ('tripathi', 0.069), ('xiaochuan', 0.069), ('music', 0.063), ('document', 0.063), ('nth', 0.062), ('platt', 0.062), ('space', 0.061), ('timex', 0.061), ('translingual', 0.061), ('tm', 0.061), ('haghighi', 0.061), ('shared', 0.059), ('gliozzo', 0.056), ('domain', 0.054), ('conventional', 0.054), ('side', 0.053), ('canonical', 0.052), ('finds', 0.051), ('books', 0.05), ('dumais', 0.048), ('bach', 0.048), ('yamada', 0.047), ('multilingual', 0.046), ('require', 0.046), ('wt', 0.045), ('projected', 0.045), ('jagadeesh', 0.045), ('daume', 0.045), ('bilingual', 0.045), ('spaces', 0.044), ('crosslanguage', 0.043), ('effectiveness', 0.042), ('sn', 0.042), ('deerwester', 0.04), ('blei', 0.038), ('lda', 0.037), ('parallel', 0.037), ('semantic', 0.036), ('aligned', 0.036), ('unaligned', 0.035), ('nn', 0.033), ('song', 0.033), ('represented', 0.032), ('denotes', 0.032), ('cooking', 0.03), ('tws', 0.03), ('brands', 0.03), ('iosm', 0.03), ('dfu', 0.03), ('supervisor', 0.03), ('slobodan', 0.03), ('prince', 0.03), ('mini', 0.03), ('datum', 0.03), ('prejudice', 0.03), ('hes', 0.03), ('equipment', 0.03), ('aaki', 0.03), ('abhishek', 0.03), ('braun', 0.03), ('espresso', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="217-tfidf-1" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>Author: Tsutomu Hirao ; Tomoharu Iwata ; Masaaki Nagata</p><p>Abstract: Unsupervised object matching (UOM) is a promising approach to cross-language natural language processing such as bilingual lexicon acquisition, parallel corpus construction, and cross-language text categorization, because it does not require labor-intensive linguistic resources. However, UOM only finds one-to-one correspondences from data sets with the same number of instances in source and target domains, and this prevents us from applying UOM to real-world cross-language natural language processing tasks. To alleviate these limitations, we proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic space. We demonstrate the effectiveness of our method on cross-language text categorization. The results show that our method outperforms conventional unsupervised object matching methods.</p><p>2 0.17777972 <a title="217-tfidf-2" href="./acl-2013-Are_Semantically_Coherent_Topic_Models_Useful_for_Ad_Hoc_Information_Retrieval%3F.html">55 acl-2013-Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?</a></p>
<p>Author: Romain Deveaud ; Eric SanJuan ; Patrice Bellot</p><p>Abstract: The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics. More, the semantic coherence of the topics has never been considered in this field. We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics. We perform a first experimental evaluation using two major TREC test collections. Results show that retrieval perfor- mances tend to be better when using topics with higher semantic coherence.</p><p>3 0.16579765 <a title="217-tfidf-3" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>Author: Zede Zhu ; Miao Li ; Lei Chen ; Zhenxin Yang</p><p>Abstract: Comparable corpora are important basic resources in cross-language information processing. However, the existing methods of building comparable corpora, which use intertranslate words and relative features, cannot evaluate the topical relation between document pairs. This paper adopts the bilingual LDA model to predict the topical structures of the documents and proposes three algorithms of document similarity in different languages. Experiments show that the novel method can obtain similar documents with consistent top- ics own better adaptability and stability performance.</p><p>4 0.16579469 <a title="217-tfidf-4" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>Author: Yukari Ogura ; Ichiro Kobayashi</p><p>Abstract: In this paper, we propose a method to raise the accuracy of text classification based on latent topics, reconsidering the techniques necessary for good classification for example, to decide important sentences in a document, the sentences with important words are usually regarded as important sentences. In this case, tf.idf is often used to decide important words. On the other hand, we apply the PageRank algorithm to rank important words in each document. Furthermore, before clustering documents, we refine the target documents by representing them as a collection of important sentences in each document. We then classify the documents based on latent information in the documents. As a clustering method, we employ the k-means algorithm and inves– tigate how our proposed method works for good clustering. We conduct experiments with Reuters-21578 corpus under various conditions of important sentence extraction, using latent and surface information for clustering, and have confirmed that our proposed method provides better result among various conditions for clustering.</p><p>5 0.15223889 <a title="217-tfidf-5" href="./acl-2013-Broadcast_News_Story_Segmentation_Using_Manifold_Learning_on_Latent_Topic_Distributions.html">73 acl-2013-Broadcast News Story Segmentation Using Manifold Learning on Latent Topic Distributions</a></p>
<p>Author: Xiaoming Lu ; Lei Xie ; Cheung-Chi Leung ; Bin Ma ; Haizhou Li</p><p>Abstract: We present an efficient approach for broadcast news story segmentation using a manifold learning algorithm on latent topic distributions. The latent topic distribution estimated by Latent Dirichlet Allocation (LDA) is used to represent each text block. We employ Laplacian Eigenmaps (LE) to project the latent topic distributions into low-dimensional semantic representations while preserving the intrinsic local geometric structure. We evaluate two approaches employing LDA and probabilistic latent semantic analysis (PLSA) distributions respectively. The effects of different amounts of training data and different numbers of latent topics on the two approaches are studied. Experimental re- sults show that our proposed LDA-based approach can outperform the corresponding PLSA-based approach. The proposed approach provides the best performance with the highest F1-measure of 0.7860.</p><p>6 0.13615207 <a title="217-tfidf-6" href="./acl-2013-Topic_Modeling_Based_Classification_of_Clinical_Reports.html">351 acl-2013-Topic Modeling Based Classification of Clinical Reports</a></p>
<p>7 0.11887038 <a title="217-tfidf-7" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<p>8 0.10905857 <a title="217-tfidf-8" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>9 0.097345039 <a title="217-tfidf-9" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>10 0.092438392 <a title="217-tfidf-10" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>11 0.080290325 <a title="217-tfidf-11" href="./acl-2013-Dependency_Parser_Adaptation_with_Subtrees_from_Auto-Parsed_Target_Domain_Data.html">112 acl-2013-Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data</a></p>
<p>12 0.078260496 <a title="217-tfidf-12" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<p>13 0.076572984 <a title="217-tfidf-13" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>14 0.076378226 <a title="217-tfidf-14" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>15 0.074545629 <a title="217-tfidf-15" href="./acl-2013-Bridging_Languages_through_Etymology%3A_The_case_of_cross_language_text_categorization.html">72 acl-2013-Bridging Languages through Etymology: The case of cross language text categorization</a></p>
<p>16 0.073391594 <a title="217-tfidf-16" href="./acl-2013-A_Two_Level_Model_for_Context_Sensitive_Inference_Rules.html">27 acl-2013-A Two Level Model for Context Sensitive Inference Rules</a></p>
<p>17 0.072657183 <a title="217-tfidf-17" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>18 0.072292387 <a title="217-tfidf-18" href="./acl-2013-Exploiting_Topic_based_Twitter_Sentiment_for_Stock_Prediction.html">147 acl-2013-Exploiting Topic based Twitter Sentiment for Stock Prediction</a></p>
<p>19 0.070090465 <a title="217-tfidf-19" href="./acl-2013-Embedding_Semantic_Similarity_in_Tree_Kernels_for_Domain_Adaptation_of_Relation_Extraction.html">134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</a></p>
<p>20 0.06810084 <a title="217-tfidf-20" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.199), (1, 0.046), (2, 0.078), (3, -0.037), (4, 0.067), (5, -0.075), (6, 0.047), (7, -0.041), (8, -0.15), (9, -0.096), (10, 0.101), (11, -0.017), (12, 0.134), (13, 0.135), (14, 0.021), (15, -0.003), (16, -0.054), (17, 0.039), (18, -0.034), (19, 0.007), (20, -0.015), (21, 0.015), (22, -0.041), (23, -0.077), (24, -0.015), (25, 0.014), (26, -0.026), (27, -0.037), (28, -0.071), (29, 0.043), (30, -0.021), (31, 0.037), (32, -0.0), (33, 0.037), (34, 0.036), (35, -0.002), (36, -0.032), (37, -0.034), (38, 0.04), (39, 0.049), (40, -0.04), (41, 0.006), (42, 0.009), (43, 0.024), (44, -0.048), (45, 0.06), (46, 0.026), (47, 0.055), (48, -0.033), (49, -0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97759217 <a title="217-lsi-1" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>Author: Tsutomu Hirao ; Tomoharu Iwata ; Masaaki Nagata</p><p>Abstract: Unsupervised object matching (UOM) is a promising approach to cross-language natural language processing such as bilingual lexicon acquisition, parallel corpus construction, and cross-language text categorization, because it does not require labor-intensive linguistic resources. However, UOM only finds one-to-one correspondences from data sets with the same number of instances in source and target domains, and this prevents us from applying UOM to real-world cross-language natural language processing tasks. To alleviate these limitations, we proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic space. We demonstrate the effectiveness of our method on cross-language text categorization. The results show that our method outperforms conventional unsupervised object matching methods.</p><p>2 0.85213798 <a title="217-lsi-2" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>Author: Yukari Ogura ; Ichiro Kobayashi</p><p>Abstract: In this paper, we propose a method to raise the accuracy of text classification based on latent topics, reconsidering the techniques necessary for good classification for example, to decide important sentences in a document, the sentences with important words are usually regarded as important sentences. In this case, tf.idf is often used to decide important words. On the other hand, we apply the PageRank algorithm to rank important words in each document. Furthermore, before clustering documents, we refine the target documents by representing them as a collection of important sentences in each document. We then classify the documents based on latent information in the documents. As a clustering method, we employ the k-means algorithm and inves– tigate how our proposed method works for good clustering. We conduct experiments with Reuters-21578 corpus under various conditions of important sentence extraction, using latent and surface information for clustering, and have confirmed that our proposed method provides better result among various conditions for clustering.</p><p>3 0.84804583 <a title="217-lsi-3" href="./acl-2013-Broadcast_News_Story_Segmentation_Using_Manifold_Learning_on_Latent_Topic_Distributions.html">73 acl-2013-Broadcast News Story Segmentation Using Manifold Learning on Latent Topic Distributions</a></p>
<p>Author: Xiaoming Lu ; Lei Xie ; Cheung-Chi Leung ; Bin Ma ; Haizhou Li</p><p>Abstract: We present an efficient approach for broadcast news story segmentation using a manifold learning algorithm on latent topic distributions. The latent topic distribution estimated by Latent Dirichlet Allocation (LDA) is used to represent each text block. We employ Laplacian Eigenmaps (LE) to project the latent topic distributions into low-dimensional semantic representations while preserving the intrinsic local geometric structure. We evaluate two approaches employing LDA and probabilistic latent semantic analysis (PLSA) distributions respectively. The effects of different amounts of training data and different numbers of latent topics on the two approaches are studied. Experimental re- sults show that our proposed LDA-based approach can outperform the corresponding PLSA-based approach. The proposed approach provides the best performance with the highest F1-measure of 0.7860.</p><p>4 0.82450736 <a title="217-lsi-4" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>Author: Zede Zhu ; Miao Li ; Lei Chen ; Zhenxin Yang</p><p>Abstract: Comparable corpora are important basic resources in cross-language information processing. However, the existing methods of building comparable corpora, which use intertranslate words and relative features, cannot evaluate the topical relation between document pairs. This paper adopts the bilingual LDA model to predict the topical structures of the documents and proposes three algorithms of document similarity in different languages. Experiments show that the novel method can obtain similar documents with consistent top- ics own better adaptability and stability performance.</p><p>5 0.81888533 <a title="217-lsi-5" href="./acl-2013-Are_Semantically_Coherent_Topic_Models_Useful_for_Ad_Hoc_Information_Retrieval%3F.html">55 acl-2013-Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?</a></p>
<p>Author: Romain Deveaud ; Eric SanJuan ; Patrice Bellot</p><p>Abstract: The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics. More, the semantic coherence of the topics has never been considered in this field. We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics. We perform a first experimental evaluation using two major TREC test collections. Results show that retrieval perfor- mances tend to be better when using topics with higher semantic coherence.</p><p>6 0.79786044 <a title="217-lsi-6" href="./acl-2013-Topic_Modeling_Based_Classification_of_Clinical_Reports.html">351 acl-2013-Topic Modeling Based Classification of Clinical Reports</a></p>
<p>7 0.77022773 <a title="217-lsi-7" href="./acl-2013-Diverse_Keyword_Extraction_from_Conversations.html">126 acl-2013-Diverse Keyword Extraction from Conversations</a></p>
<p>8 0.7272175 <a title="217-lsi-8" href="./acl-2013-Are_School-of-thought_Words_Characterizable%3F.html">54 acl-2013-Are School-of-thought Words Characterizable?</a></p>
<p>9 0.72291583 <a title="217-lsi-9" href="./acl-2013-Bridging_Languages_through_Etymology%3A_The_case_of_cross_language_text_categorization.html">72 acl-2013-Bridging Languages through Etymology: The case of cross language text categorization</a></p>
<p>10 0.69730943 <a title="217-lsi-10" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>11 0.65448546 <a title="217-lsi-11" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<p>12 0.6488921 <a title="217-lsi-12" href="./acl-2013-Improved_Bayesian_Logistic_Supervised_Topic_Models_with_Data_Augmentation.html">191 acl-2013-Improved Bayesian Logistic Supervised Topic Models with Data Augmentation</a></p>
<p>13 0.61971974 <a title="217-lsi-13" href="./acl-2013-Semi-Supervised_Semantic_Tagging_of_Conversational_Understanding_using_Markov_Topic_Regression.html">315 acl-2013-Semi-Supervised Semantic Tagging of Conversational Understanding using Markov Topic Regression</a></p>
<p>14 0.60364896 <a title="217-lsi-14" href="./acl-2013-Evolutionary_Hierarchical_Dirichlet_Process_for_Timeline_Summarization.html">142 acl-2013-Evolutionary Hierarchical Dirichlet Process for Timeline Summarization</a></p>
<p>15 0.59995228 <a title="217-lsi-15" href="./acl-2013-Natural_Language_Models_for_Predicting_Programming_Comments.html">257 acl-2013-Natural Language Models for Predicting Programming Comments</a></p>
<p>16 0.58284122 <a title="217-lsi-16" href="./acl-2013-Exploiting_Topic_based_Twitter_Sentiment_for_Stock_Prediction.html">147 acl-2013-Exploiting Topic based Twitter Sentiment for Stock Prediction</a></p>
<p>17 0.57423103 <a title="217-lsi-17" href="./acl-2013-Variable_Bit_Quantisation_for_LSH.html">381 acl-2013-Variable Bit Quantisation for LSH</a></p>
<p>18 0.56820428 <a title="217-lsi-18" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>19 0.55385488 <a title="217-lsi-19" href="./acl-2013-Unsupervised_Transcription_of_Historical_Documents.html">370 acl-2013-Unsupervised Transcription of Historical Documents</a></p>
<p>20 0.55327392 <a title="217-lsi-20" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.024), (6, 0.025), (11, 0.042), (24, 0.071), (26, 0.026), (28, 0.01), (35, 0.071), (42, 0.023), (48, 0.039), (70, 0.022), (88, 0.02), (95, 0.551)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98628247 <a title="217-lda-1" href="./acl-2013-Improving_machine_translation_by_training_against_an_automatic_semantic_frame_based_evaluation_metric.html">195 acl-2013-Improving machine translation by training against an automatic semantic frame based evaluation metric</a></p>
<p>Author: Chi-kiu Lo ; Karteek Addanki ; Markus Saers ; Dekai Wu</p><p>Abstract: We present the first ever results showing that tuning a machine translation system against a semantic frame based objective function, MEANT, produces more robustly adequate translations than tuning against BLEU or TER as measured across commonly used metrics and human subjective evaluation. Moreover, for informal web forum data, human evaluators preferred MEANT-tuned systems over BLEU- or TER-tuned systems by a significantly wider margin than that for formal newswire—even though automatic semantic parsing might be expected to fare worse on informal language. We argue thatbypreserving the meaning ofthe trans- lations as captured by semantic frames right in the training process, an MT system is constrained to make more accurate choices of both lexical and reordering rules. As a result, MT systems tuned against semantic frame based MT evaluation metrics produce output that is more adequate. Tuning a machine translation system against a semantic frame based objective function is independent ofthe translation model paradigm, so, any translation model can benefit from the semantic knowledge incorporated to improve translation adequacy through our approach.</p><p>same-paper 2 0.98352039 <a title="217-lda-2" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>Author: Tsutomu Hirao ; Tomoharu Iwata ; Masaaki Nagata</p><p>Abstract: Unsupervised object matching (UOM) is a promising approach to cross-language natural language processing such as bilingual lexicon acquisition, parallel corpus construction, and cross-language text categorization, because it does not require labor-intensive linguistic resources. However, UOM only finds one-to-one correspondences from data sets with the same number of instances in source and target domains, and this prevents us from applying UOM to real-world cross-language natural language processing tasks. To alleviate these limitations, we proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic space. We demonstrate the effectiveness of our method on cross-language text categorization. The results show that our method outperforms conventional unsupervised object matching methods.</p><p>3 0.98137826 <a title="217-lda-3" href="./acl-2013-Syntactic_Patterns_versus_Word_Alignment%3A_Extracting_Opinion_Targets_from_Online_Reviews.html">336 acl-2013-Syntactic Patterns versus Word Alignment: Extracting Opinion Targets from Online Reviews</a></p>
<p>Author: Kang Liu ; Liheng Xu ; Jun Zhao</p><p>Abstract: Mining opinion targets is a fundamental and important task for opinion mining from online reviews. To this end, there are usually two kinds of methods: syntax based and alignment based methods. Syntax based methods usually exploited syntactic patterns to extract opinion targets, which were however prone to suffer from parsing errors when dealing with online informal texts. In contrast, alignment based methods used word alignment model to fulfill this task, which could avoid parsing errors without using parsing. However, there is no research focusing on which kind of method is more better when given a certain amount of reviews. To fill this gap, this paper empiri- cally studies how the performance of these two kinds of methods vary when changing the size, domain and language of the corpus. We further combine syntactic patterns with alignment model by using a partially supervised framework and investigate whether this combination is useful or not. In our experiments, we verify that our combination is effective on the corpus with small and medium size.</p><p>4 0.97887069 <a title="217-lda-4" href="./acl-2013-Translating_Dialectal_Arabic_to_English.html">359 acl-2013-Translating Dialectal Arabic to English</a></p>
<p>Author: Hassan Sajjad ; Kareem Darwish ; Yonatan Belinkov</p><p>Abstract: We present a dialectal Egyptian Arabic to English statistical machine translation system that leverages dialectal to Modern Standard Arabic (MSA) adaptation. In contrast to previous work, we first narrow down the gap between Egyptian and MSA by applying an automatic characterlevel transformational model that changes Egyptian to EG0, which looks similar to MSA. The transformations include morphological, phonological and spelling changes. The transformation reduces the out-of-vocabulary (OOV) words from 5.2% to 2.6% and gives a gain of 1.87 BLEU points. Further, adapting large MSA/English parallel data increases the lexical coverage, reduces OOVs to 0.7% and leads to an absolute BLEU improvement of 2.73 points.</p><p>5 0.9781999 <a title="217-lda-5" href="./acl-2013-Named_Entity_Recognition_using_Cross-lingual_Resources%3A_Arabic_as_an_Example.html">256 acl-2013-Named Entity Recognition using Cross-lingual Resources: Arabic as an Example</a></p>
<p>Author: Kareem Darwish</p><p>Abstract: Some languages lack large knowledge bases and good discriminative features for Name Entity Recognition (NER) that can generalize to previously unseen named entities. One such language is Arabic, which: a) lacks a capitalization feature; and b) has relatively small knowledge bases, such as Wikipedia. In this work we address both problems by incorporating cross-lingual features and knowledge bases from English using cross-lingual links. We show that such features have a dramatic positive effect on recall. We show the effectiveness of cross-lingual features and resources on a standard dataset as well as on two new test sets that cover both news and microblogs. On the standard dataset, we achieved a 4.1% relative improvement in Fmeasure over the best reported result in the literature. The features led to improvements of 17.1% and 20.5% on the new news and mi- croblogs test sets respectively.</p><p>6 0.96158409 <a title="217-lda-6" href="./acl-2013-Adaptive_Parser-Centric_Text_Normalization.html">37 acl-2013-Adaptive Parser-Centric Text Normalization</a></p>
<p>7 0.94578737 <a title="217-lda-7" href="./acl-2013-FrameNet_on_the_Way_to_Babel%3A_Creating_a_Bilingual_FrameNet_Using_Wiktionary_as_Interlingual_Connection.html">162 acl-2013-FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection</a></p>
<p>8 0.93880343 <a title="217-lda-8" href="./acl-2013-Beam_Search_for_Solving_Substitution_Ciphers.html">66 acl-2013-Beam Search for Solving Substitution Ciphers</a></p>
<p>9 0.90546501 <a title="217-lda-9" href="./acl-2013-QuEst_-_A_translation_quality_estimation_framework.html">289 acl-2013-QuEst - A translation quality estimation framework</a></p>
<p>10 0.84923369 <a title="217-lda-10" href="./acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language.html">374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</a></p>
<p>11 0.84017324 <a title="217-lda-11" href="./acl-2013-English-to-Russian_MT_evaluation_campaign.html">135 acl-2013-English-to-Russian MT evaluation campaign</a></p>
<p>12 0.83993495 <a title="217-lda-12" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>13 0.83691901 <a title="217-lda-13" href="./acl-2013-Sentence_Level_Dialect_Identification_in_Arabic.html">317 acl-2013-Sentence Level Dialect Identification in Arabic</a></p>
<p>14 0.81246173 <a title="217-lda-14" href="./acl-2013-Social_Text_Normalization_using_Contextual_Graph_Random_Walks.html">326 acl-2013-Social Text Normalization using Contextual Graph Random Walks</a></p>
<p>15 0.80657381 <a title="217-lda-15" href="./acl-2013-Language_Independent_Connectivity_Strength_Features_for_Phrase_Pivot_Statistical_Machine_Translation.html">214 acl-2013-Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation</a></p>
<p>16 0.80405939 <a title="217-lda-16" href="./acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</a></p>
<p>17 0.80280566 <a title="217-lda-17" href="./acl-2013-A_New_Syntactic_Metric_for_Evaluation_of_Machine_Translation.html">13 acl-2013-A New Syntactic Metric for Evaluation of Machine Translation</a></p>
<p>18 0.80025226 <a title="217-lda-18" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>19 0.7955671 <a title="217-lda-19" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>20 0.7793867 <a title="217-lda-20" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
