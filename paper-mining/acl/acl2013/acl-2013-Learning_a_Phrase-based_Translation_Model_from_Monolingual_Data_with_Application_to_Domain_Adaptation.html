<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-223" href="#">acl2013-223</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</h1>
<br/><p>Source: <a title="acl-2013-223-pdf" href="http://aclweb.org/anthology//P/P13/P13-1140.pdf">pdf</a></p><p>Author: Jiajun Zhang ; Chengqing Zong</p><p>Abstract: Currently, almost all of the statistical machine translation (SMT) models are trained with the parallel corpora in some specific domains. However, when it comes to a language pair or a different domain without any bilingual resources, the traditional SMT loses its power. Recently, some research works study the unsupervised SMT for inducing a simple word-based translation model from the monolingual corpora. It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result. In this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1</p><p>Reference: <a title="acl-2013-223-reference" href="../acl2013_reference/acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Recently, some research works study the unsupervised SMT for inducing a simple word-based translation model from the monolingual corpora. [sent-6, score-0.692]
</p><p>2 In this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. [sent-8, score-1.316]
</p><p>3 However, all of these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters. [sent-17, score-0.723]
</p><p>4 Recently, more and more researchers concentrated on taking full advantage of the monolin-  gual corpora in both source and target languages, and proposed methods for bilingual lexicon induction from non-parallel data (Rapp, 1995, 1999; Koehn and Knight, 2002; Haghighi et al. [sent-19, score-0.807]
</p><p>5 , 2008; Daumé III and Jagarlamudi, 2011) and proposed unsupervised statistical machine translation (bilingual lexicon is a byproduct) with only monolingual corpora (Ravi and Knight, 2011; Nuhn et al. [sent-20, score-0.917]
</p><p>6 In the bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al. [sent-22, score-0.598]
</p><p>7 , 2012; Dou and Knight, 2012) viewed the translation task as a decipherment problem and designed a generative model with the objective function to maximize the likelihood of the  source language monolingual data. [sent-25, score-0.78]
</p><p>8 , 2012) or incorporated the learned bilingual lexicon as a new in-domain translation resource into the phrase-based model which is trained with out-of-domain data to improve the domain adaptation performance in machine translation (Dou and Knight, 2012). [sent-30, score-1.088]
</p><p>9 level translation rules and learn a phrase-based model from the monolingual corpora. [sent-36, score-0.742]
</p><p>10 In this paper, we focus on exploring this direction and propose a simple but effective method to induce the phrase-level translation rules from monolingual data. [sent-37, score-0.836]
</p><p>11 The main idea of our method is to divide the phrase-level translation rule induction into two steps: bilingual lexicon induction and phrase pair induction. [sent-38, score-1.552]
</p><p>12 Since many researchers have studied the bilingual lexicon induction, in this paper, we mainly concentrate ourselves on phrase pair induction given a probabilistic bilingual lexicon and two in-domain large monolingual data (source and target language). [sent-39, score-2.13]
</p><p>13 In addition, we will further introduce how to refine the induced phrase pairs and estimate the parameters of the induced phrase pairs, such as four standard translation features and phrase reordering feature used in the conventional phrase-based models (Koehn et al. [sent-40, score-1.75]
</p><p>14 In the rest of this paper, we first explain with  examples to show what new translation knowledge can be learned with our proposed phrase pair induction method (Section 2), and then we introduce the approach for probabilistic bilingual lexicon acquisition in Section 3. [sent-43, score-1.472]
</p><p>15 In Section 4 and 5, we respectively present our method for phrase pair induction and introduce an approach for phrase pair refinement and parameter estimation. [sent-44, score-1.268]
</p><p>16 Readers may doubt that if phrase pair induction is performed only using bilingual lexicon and monolingual data, what new translation knowledge can be learned? [sent-48, score-1.779]
</p><p>17 The bilingual lexicon can only express the translation equivalence between source- and target-side word pair and has little ability to deal with word reordering and idiom translation. [sent-49, score-1.126]
</p><p>18 In contrast, phrase pair induction can make up for  this deficiency to some extent. [sent-50, score-0.665]
</p><p>19 From the induced phrase pairs with our method, we have conducted a deep analysis and find that we can learn three kinds of new translation knowledge: 1) word reordering in a phrase pair; 2) idioms; and 3) unknown word translations. [sent-52, score-1.448]
</p><p>20 For the first example, the source and target phrase are extracted respectively from monolingual data, each word in the source phrase has a translation in the target phrase, but the word order is different. [sent-54, score-1.94]
</p><p>21 The word order encoded in a phrase pair is difficult to learn in a word-based SMT. [sent-55, score-0.579]
</p><p>22 In the second example, the italic source word corresponds to two target words (in italic), and the phrase pair is an idiom which cannot be learned from word-based SMT. [sent-56, score-0.939]
</p><p>23 In the third example, as we learn from the source and target monolingual text that the words around the italic ones are translations with each other, thus we cannot only extract a new phrase pair but also learn a translation pair of unknown words in italic. [sent-57, score-1.877]
</p><p>24 3  Probabilistic quisition  Bilingual  Lexicon  Ac-  In order to induce the phrase pairs from the indomain monolingual data for domain adaptation, the probabilistic bilingual lexicon is essential. [sent-58, score-1.589]
</p><p>25 In this paper, we acquire the probabilistic bilingual lexicon from two approaches: 1) build a bilingual lexicon from large-scale out-of-domain parallel data; 2) adopt a manually collected indomain lexicon. [sent-59, score-1.05]
</p><p>26 The noise of the lexicon can influence the accuracy of the induced phrase pairs to a large extent. [sent-67, score-0.753]
</p><p>27 After using the log-likelihood-ratios algorithm2, we obtain a probabilistic bilingual lexicon with bidirectional translation probabilities from the out-of-domain data. [sent-70, score-0.844]
</p><p>28 , 2008): given an in-domain source language monolingual data, we translate this data with the phrase-based model trained on the out-of-domain News data, the in-domain lexicon and the indomain target language monolingual data (for language model estimation). [sent-76, score-1.424]
</p><p>29 That is if a source word has n translations, then the translation probability of target word given the source word is 1/n. [sent-79, score-0.726]
</p><p>30 We combine LLR-lex and Domain-lex to obtain the final probabilistic bilingual lexicon for phrase pair induction. [sent-83, score-0.974]
</p><p>31 4  Phrase Pair Induction Method  Given a probabilistic bilingual lexicon and two monolingual data, we present a simple but effective method for phrase pair induction in this section. [sent-84, score-1.621]
</p><p>32 Input: Probabilistic bilingual lexicon V (each source word s maps a translation set V[s]) Source language monolingual data S={ sn} n=1. [sent-85, score-1.242]
</p><p>33 For a source phrase (word sequence), we can reorder  the words in the phrase (permutation) first, and then obtain the target phrases with the bilingual lexicon (translation), and finally check if the target phrase is in the target monolingual data. [sent-94, score-2.39]
</p><p>34 permutations), suppose a source word has C translations on average and checking whether the target phrase tij'' in T needs time O T , then, phrase pair induction for a single  O Cj i 1Tji 1 ! [sent-99, score-1.398]
</p><p>35 For example, one can collect distinct n-grams from source and target monolingual data. [sent-103, score-0.702]
</p><p>36 Then, for a source-side phrase with length L, one can find the best translation candidate using the probabilistic bilingual lexicon from the target-side phrases with the same length L. [sent-104, score-1.18]
</p><p>37 The biggest disadvantage of these algorithms is that they can only induce phrase pair (with the 1427  same length) encoding word reordering, but can-  not learn phrase pairs in different length. [sent-105, score-1.145]
</p><p>38 Furthermore, they cannot learn idioms and unknown word translations from monolingual data. [sent-106, score-0.749]
</p><p>39 2  Phrase Index  Pair Induction  with Inverted  In order to make the phrase pair induction both effective and efficient, we propose a method using inverted index data structure which is usually a central component of a typical search engine. [sent-109, score-0.845]
</p><p>40 The inverted index is employed to represent the target language monolingual data. [sent-110, score-0.704]
</p><p>41 For a target language word, the inverted index not only records the sentence position in monolingual data, but also records the word position in a sentence. [sent-111, score-0.812]
</p><p>42 By doing this, we do not need to iterate all the permutations of source language phrase sij to explore possible phrase pairs encoding word reordering. [sent-113, score-1.089]
</p><p>43 We will elaborate how to induce phrase pairs with the help of inverted index. [sent-115, score-0.642]
</p><p>44 TcaormgbeWtlzLuo… an2rimdc:gSautiogmne xam(p92le0,5s)o,(f120i8n6v,2Pe0or1)st,…ie do…i,n (d29e30x2f3o9,r1ta25)r-  get language words, (2,5) means that “communication” occurs at the 5th word of the 2nd sentence in the target monolingual data. [sent-116, score-0.598]
</p><p>45 The new algorithm for phrase pair induction is presented in Figure 2. [sent-117, score-0.665]
</p><p>46 For each distinct source-side phrase, Line 2-5 efficiently collects all the positions in the target monolingual data for the translations of each word in the source phrase. [sent-120, score-0.866]
</p><p>47 That is to say we allow at most one unknown word in an induced phrase pair in order to make the induction  more accurate. [sent-123, score-0.876]
</p><p>48 Input: Probabilistic bilingual lexicon V (each source word s maps a translation set V[s]) Source language monolingual data S={sn} n=1. [sent-126, score-1.242]
</p><p>49 Constraint:  we require  that there exists  at  most one phrase in a target sentence that is the translation of the source-side phrase. [sent-130, score-0.736]
</p><p>50 Similar with Line 9 in Figure 2, we further filter the target continuous phrase if more than one word in source-side phrase has no translation in this target phrase. [sent-139, score-1.292]
</p><p>51 After that, we just choose the continuous target phrase with the largest probability if two or more continuous target phrases exist in the same target sentence. [sent-140, score-0.847]
</p><p>52 The probability of a target-side phrase given the source-side phrase is computed similar to that of (Koehn et al. [sent-141, score-0.774]
</p><p>53 Similarly, we can compute the probability of source-side phrase given the target-side phrase plex s s| |t , a . [sent-144, score-0.868]
</p><p>54 Then, we find the target-side phrase which has the biggest value of plex t t| |s , a  plex s s| |t , a . [sent-145, score-0.582]
</p><p>55 However, we find in the target monolingual data (1 million sentences) that each distinct word happens 110 times on average. [sent-149, score-0.654]
</p><p>56 Then, for a sources-side phrase with 7 words, the average length of positionArray will be 3850, since each source word has averagely 5 target translations  (mentioned in Section 3). [sent-150, score-0.773]
</p><p>57 For example, we extract a phrase pair as follows: 的 商业 信息  of business information of  In the above phrase pair, there are two words “of” in the target side and the first one is redundant. [sent-155, score-1.011]
</p><p>58 The phrase pair induction algorithm presented in Section 4 cannot deal with this situation. [sent-156, score-0.665]
</p><p>59 For the entries in Domain-lex, we constrain that the target translation should be adjacent with the translations of its source-side neighbors and translation order is the same with the source-side words. [sent-160, score-0.755]
</p><p>60 This refinement can be applied before finding the phrase pair with maximum probability (Line 12 in Figure 2) so that the duplicate  words do not affect the calculation of translation probability of phrase pair. [sent-164, score-1.262]
</p><p>61 2 Translation Probability Estimation It is well known that in the phrase-based SMT there are four translation probabilities and the reordering probability for each phrase pair. [sent-166, score-0.836]
</p><p>62 The translation probabilities in the traditional phrase-based SMT include bidirectional phrase translation probabilities and bidirectional lexical weights. [sent-167, score-1.078]
</p><p>63 , 2012) to calculate the phraselevel translation probability with context information in source and target monolingual corpus. [sent-171, score-0.955]
</p><p>64 With source and target vocabularies  ,  s1, s2, sN  and t1, t2 , ,tM  , the source-side phrase s and target-side phrase t can be respectively represented in an N- and M-dimensional vector. [sent-173, score-0.959]
</p><p>65 3 Reordering Probability Estimation For the reordering probabilities of newly induced phrase pairs, we can also follow Klementiev et al. [sent-180, score-0.633]
</p><p>66 (2012) to estimate these probabilities using source and target monolingual data. [sent-181, score-0.734]
</p><p>67 For the phrase pair ( 的 商 业 信 , business information of), we find a source sentence containing 的 商业 信息, and find a target sentence containing business information of. [sent-183, score-0.78]
</p><p>68 If there is another phrase pair  息   s, t, t exactly follows business information of and s occurs in the same source sentence with 的 商业 信 息 , then we compare the position relationship between s and 的 商业 信 息. [sent-184, score-0.652]
</p><p>69 6  Related Work  As far as we know, few researchers study phrase  pair induction from only monolingual data. [sent-187, score-1.102]
</p><p>70 One is using an in-domain probabilistic bilingual lexicon to extract subsentential parallel fragments from comparable corpora (Munteanu and Marcu, 2006; Quirk et al. [sent-189, score-0.625]
</p><p>71 Munteanu and Marcu (2006) first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon. [sent-192, score-0.669]
</p><p>72 Compared with their work, our focus is to induce phrase pairs directly from monolingual data rather than comparable data. [sent-193, score-1.0]
</p><p>73 The third is to estimate the translation parameters and reordering parameters using monolingual data given the phrase pairs (Klementiev et al. [sent-200, score-1.255]
</p><p>74 Their work supposes the phrase pairs are already given and then corresponding parameters can be learned with monolingual data. [sent-202, score-0.892]
</p><p>75 Different from their work, we concentrate ourselves on inducing phrase pairs from monolingual data and then borrow some ideas from theirs for parameter estimation. [sent-203, score-0.893]
</p><p>76 1  Experiments Experimental Setup  Our purpose is to induce phrase pairs to improve translation quality for domain adaptation. [sent-206, score-0.851]
</p><p>77 They are neither parallel nor comparable because we cannot even extract a small number of parallel sentence pairs from this monolingual data using the method of (Munteanu and Marcu, 2006). [sent-210, score-0.708]
</p><p>78 For the in-domain electronic data, we first consider the lexicon as a phrase table in which we assign a constant 1. [sent-217, score-0.643]
</p><p>79 0 for each of the four probabilities, and then we combine this initial phrase table and the induced phrase pairs to form the new phrase table. [sent-218, score-1.248]
</p><p>80 Given the in-domain bilingual lexicon and two monolingual data, previous works also proposed some good methods to explore the potential of the given data to improve the translation quality. [sent-230, score-1.114]
</p><p>81 (2008) regards the in-domain lexicon with corpus translation probability as another phrase table and further use the in-domain language model besides the out-of-domain language model. [sent-233, score-0.894]
</p><p>82 This method has made good use of in-domain lexicon and the target-side in-  domain monolingual data, but it does not take full advantage of the in-domain source-side monolingual data. [sent-239, score-1.197]
</p><p>83 (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+indomain language model) and obtain 1-best translation for each source-side sentence. [sent-242, score-0.843]
</p><p>84 In tranductive learning, in-domain lexicon and both-side monolingual data have been explored. [sent-248, score-0.662]
</p><p>85 However, this method does not take full advantage of both-side monolingual data because it uses source and target monolingual data individually. [sent-249, score-1.117]
</p><p>86 In our method, we explore fully the source  and target monolingual data to induce translation equivalence on the phrase level. [sent-250, score-1.402]
</p><p>87 In order to make the phrase pair induction more efficient, we first sort all the sentences in the both-side monolingual data according to the word hit rate in the bilingual lexicon. [sent-251, score-1.339]
</p><p>88 The method of phrase pair induction using 300k sentences performs quite well. [sent-283, score-0.699]
</p><p>89 These experimental results empirically show the effectiveness of our proposed phrase pair induction method. [sent-288, score-0.665]
</p><p>90 A question remains that how many new phrase pairs are induced with different size of monolingual data. [sent-289, score-0.965]
</p><p>91 We can see from the table that lots of new phrase pairs can be induced since the source and target monolingual data is in the same domain. [sent-292, score-1.174]
</p><p>92 However, since the source and target monolingual data is 1432  far from parallel, most of the phrase pairs are not long. [sent-293, score-1.068]
</p><p>93 For example, in the 108,948 distinct phrase pairs, we find that the phrase pair distribution according to source-side length is (3:50. [sent-294, score-0.945]
</p><p>94 8  Conclusion and Future Work  This paper proposes a simple but effective method to induce phrase pairs from monolingual data. [sent-301, score-1.003]
</p><p>95 Given the probabilistic bilingual lexicon and both-side monolingual data in the same domain, the method employs inverted index structure to represent the target-side monolingual data, and induce the translations for each distinct sourceside phrase with the help of the bilingual lexicon. [sent-302, score-2.435]
</p><p>96 We also introduce how to estimate the translation and reordering parameters for the induced phrase pairs with monolingual data. [sent-304, score-1.361]
</p><p>97 Extensive experiments on domain adaptation have shown that our method can significantly outperform previous methods which also focus on exploring the indomain lexicon and monolingual data. [sent-305, score-0.89]
</p><p>98 However, through the analysis we find that our induced phrase pairs still contain some noise, such as the words in source- and target-side of the phrase pair are all aligned but the target-side phrase expresses the different meaning. [sent-306, score-1.418]
</p><p>99 Domain adaptation for statistical machine translation with monolingual resources. [sent-314, score-0.751]
</p><p>100 Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora. [sent-445, score-0.815]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('monolingual', 0.437), ('phrase', 0.36), ('translation', 0.255), ('lexicon', 0.225), ('bilingual', 0.197), ('induction', 0.176), ('positionarray', 0.157), ('sij', 0.146), ('pair', 0.129), ('translations', 0.124), ('target', 0.121), ('induce', 0.11), ('reordering', 0.11), ('inverted', 0.11), ('induced', 0.106), ('transductive', 0.106), ('munteanu', 0.103), ('idiom', 0.099), ('knight', 0.098), ('plex', 0.094), ('bestconfig', 0.089), ('source', 0.088), ('sk', 0.075), ('parallel', 0.072), ('indomain', 0.071), ('italic', 0.069), ('nuhn', 0.069), ('chengqing', 0.069), ('zong', 0.069), ('bleu', 0.068), ('unknown', 0.065), ('domain', 0.064), ('probabilistic', 0.063), ('koehn', 0.063), ('dou', 0.063), ('jiajun', 0.063), ('pairs', 0.062), ('smt', 0.062), ('marcu', 0.061), ('adaptation', 0.059), ('electronic', 0.058), ('probabilities', 0.057), ('klementiev', 0.057), ('distinct', 0.056), ('feifei', 0.055), ('ueffing', 0.055), ('probability', 0.054), ('gap', 0.052), ('learn', 0.05), ('refinement', 0.05), ('ravi', 0.05), ('sourceside', 0.049), ('bidirectional', 0.047), ('line', 0.046), ('nist', 0.046), ('translate', 0.045), ('jagarlamudi', 0.045), ('rorbert', 0.045), ('thk', 0.045), ('bertoldi', 0.045), ('zhai', 0.045), ('moore', 0.044), ('federico', 0.041), ('schwenk', 0.041), ('rapp', 0.041), ('business', 0.041), ('aligned', 0.041), ('word', 0.04), ('length', 0.04), ('adjacency', 0.04), ('nmax', 0.04), ('seq', 0.04), ('imap', 0.04), ('fragments', 0.037), ('dunning', 0.036), ('index', 0.036), ('nicola', 0.036), ('continuous', 0.035), ('ti', 0.035), ('borrow', 0.034), ('position', 0.034), ('ve', 0.034), ('method', 0.034), ('biggest', 0.034), ('learned', 0.033), ('wu', 0.033), ('daum', 0.033), ('na', 0.033), ('idioms', 0.033), ('deciphering', 0.033), ('melamed', 0.033), ('permutations', 0.033), ('kevin', 0.032), ('equivalence', 0.031), ('cettolo', 0.031), ('news', 0.031), ('zhang', 0.031), ('estimate', 0.031), ('comparable', 0.031), ('respectively', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="223-tfidf-1" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>Author: Jiajun Zhang ; Chengqing Zong</p><p>Abstract: Currently, almost all of the statistical machine translation (SMT) models are trained with the parallel corpora in some specific domains. However, when it comes to a language pair or a different domain without any bilingual resources, the traditional SMT loses its power. Recently, some research works study the unsupervised SMT for inducing a simple word-based translation model from the monolingual corpora. It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result. In this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1</p><p>2 0.28807113 <a title="223-tfidf-2" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>Author: Conghui Zhu ; Taro Watanabe ; Eiichiro Sumita ; Tiejun Zhao</p><p>Abstract: Typical statistical machine translation systems are batch trained with a given training data and their performances are largely influenced by the amount of data. With the growth of the available data across different domains, it is computationally demanding to perform batch training every time when new data comes. In face of the problem, we propose an efficient phrase table combination method. In particular, we train a Bayesian phrasal inversion transduction grammars for each domain separately. The learned phrase tables are hierarchically combined as if they are drawn from a hierarchical Pitman-Yor process. The performance measured by BLEU is at least as comparable to the traditional batch training method. Furthermore, each phrase table is trained separately in each domain, and while computational overhead is significantly reduced by training them in parallel.</p><p>3 0.28766418 <a title="223-tfidf-3" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>Author: Sujith Ravi</p><p>Abstract: In this paper, we propose a new Bayesian inference method to train statistical machine translation systems using only nonparallel corpora. Following a probabilistic decipherment approach, we first introduce a new framework for decipherment training that is flexible enough to incorporate any number/type of features (besides simple bag-of-words) as side-information used for estimating translation models. In order to perform fast, efficient Bayesian inference in this framework, we then derive a hash sampling strategy that is inspired by the work of Ahmed et al. (2012). The new translation hash sampler enables us to scale elegantly to complex models (for the first time) and large vocab- ulary/corpora sizes. We show empirical results on the OPUS data—our method yields the best BLEU scores compared to existing approaches, while achieving significant computational speedups (several orders faster). We also report for the first time—BLEU score results for a largescale MT task using only non-parallel data (EMEA corpus).</p><p>4 0.24942037 <a title="223-tfidf-4" href="./acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language.html">374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</a></p>
<p>Author: Samira Tofighi Zahabi ; Somayeh Bakhshaei ; Shahram Khadivi</p><p>Abstract: Mapping phrases between languages as translation of each other by using an intermediate language (pivot language) may generate translation pairs that are wrong. Since a word or a phrase has different meanings in different contexts, we should map source and target phrases in an intelligent way. We propose a pruning method based on the context vectors to remove those phrase pairs that connect to each other by a polysemous pivot phrase or by weak translations. We use context vectors to implicitly disambiguate the phrase senses and to recognize irrelevant phrase translation pairs. Using the proposed method a relative improvement of 2.8 percent in terms of BLEU score is achieved. 1</p><p>5 0.24811092 <a title="223-tfidf-5" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>Author: Lei Cui ; Dongdong Zhang ; Shujie Liu ; Mu Li ; Ming Zhou</p><p>Abstract: The quality of bilingual data is a key factor in Statistical Machine Translation (SMT). Low-quality bilingual data tends to produce incorrect translation knowledge and also degrades translation modeling performance. Previous work often used supervised learning methods to filter lowquality data, but a fair amount of human labeled examples are needed which are not easy to obtain. To reduce the reliance on labeled examples, we propose an unsupervised method to clean bilingual data. The method leverages the mutual reinforcement between the sentence pairs and the extracted phrase pairs, based on the observation that better sentence pairs often lead to better phrase extraction and vice versa. End-to-end experiments show that the proposed method substantially improves the performance in largescale Chinese-to-English translation tasks.</p><p>6 0.24150114 <a title="223-tfidf-6" href="./acl-2013-An_Information_Theoretic_Approach_to_Bilingual_Word_Clustering.html">47 acl-2013-An Information Theoretic Approach to Bilingual Word Clustering</a></p>
<p>7 0.24046005 <a title="223-tfidf-7" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>8 0.19676912 <a title="223-tfidf-8" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>9 0.19012286 <a title="223-tfidf-9" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<p>10 0.18185885 <a title="223-tfidf-10" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>11 0.17636432 <a title="223-tfidf-11" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>12 0.17411593 <a title="223-tfidf-12" href="./acl-2013-Context_Vector_Disambiguation_for_Bilingual_Lexicon_Extraction_from_Comparable_Corpora.html">93 acl-2013-Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora</a></p>
<p>13 0.17271502 <a title="223-tfidf-13" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>14 0.17158057 <a title="223-tfidf-14" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>15 0.16690712 <a title="223-tfidf-15" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>16 0.16571678 <a title="223-tfidf-16" href="./acl-2013-Language_Independent_Connectivity_Strength_Features_for_Phrase_Pivot_Statistical_Machine_Translation.html">214 acl-2013-Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation</a></p>
<p>17 0.16268323 <a title="223-tfidf-17" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>18 0.16165045 <a title="223-tfidf-18" href="./acl-2013-Integrating_Phrase-based_Reordering_Features_into_a_Chart-based_Decoder_for_Machine_Translation.html">200 acl-2013-Integrating Phrase-based Reordering Features into a Chart-based Decoder for Machine Translation</a></p>
<p>19 0.15929997 <a title="223-tfidf-19" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>20 0.15862973 <a title="223-tfidf-20" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.332), (1, -0.233), (2, 0.336), (3, 0.174), (4, 0.019), (5, -0.006), (6, -0.09), (7, 0.039), (8, -0.003), (9, -0.029), (10, 0.061), (11, -0.067), (12, 0.026), (13, 0.051), (14, 0.076), (15, -0.043), (16, 0.0), (17, -0.078), (18, -0.101), (19, 0.07), (20, -0.014), (21, -0.083), (22, 0.055), (23, 0.023), (24, -0.006), (25, -0.004), (26, -0.046), (27, 0.094), (28, 0.052), (29, 0.056), (30, -0.035), (31, 0.03), (32, -0.009), (33, 0.04), (34, 0.015), (35, 0.058), (36, 0.007), (37, -0.119), (38, -0.036), (39, -0.084), (40, 0.102), (41, -0.023), (42, -0.04), (43, -0.027), (44, -0.039), (45, -0.014), (46, 0.059), (47, -0.023), (48, 0.019), (49, -0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98574847 <a title="223-lsi-1" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>Author: Jiajun Zhang ; Chengqing Zong</p><p>Abstract: Currently, almost all of the statistical machine translation (SMT) models are trained with the parallel corpora in some specific domains. However, when it comes to a language pair or a different domain without any bilingual resources, the traditional SMT loses its power. Recently, some research works study the unsupervised SMT for inducing a simple word-based translation model from the monolingual corpora. It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result. In this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1</p><p>2 0.93667144 <a title="223-lsi-2" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>Author: Lei Cui ; Dongdong Zhang ; Shujie Liu ; Mu Li ; Ming Zhou</p><p>Abstract: The quality of bilingual data is a key factor in Statistical Machine Translation (SMT). Low-quality bilingual data tends to produce incorrect translation knowledge and also degrades translation modeling performance. Previous work often used supervised learning methods to filter lowquality data, but a fair amount of human labeled examples are needed which are not easy to obtain. To reduce the reliance on labeled examples, we propose an unsupervised method to clean bilingual data. The method leverages the mutual reinforcement between the sentence pairs and the extracted phrase pairs, based on the observation that better sentence pairs often lead to better phrase extraction and vice versa. End-to-end experiments show that the proposed method substantially improves the performance in largescale Chinese-to-English translation tasks.</p><p>3 0.87700301 <a title="223-lsi-3" href="./acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language.html">374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</a></p>
<p>Author: Samira Tofighi Zahabi ; Somayeh Bakhshaei ; Shahram Khadivi</p><p>Abstract: Mapping phrases between languages as translation of each other by using an intermediate language (pivot language) may generate translation pairs that are wrong. Since a word or a phrase has different meanings in different contexts, we should map source and target phrases in an intelligent way. We propose a pruning method based on the context vectors to remove those phrase pairs that connect to each other by a polysemous pivot phrase or by weak translations. We use context vectors to implicitly disambiguate the phrase senses and to recognize irrelevant phrase translation pairs. Using the proposed method a relative improvement of 2.8 percent in terms of BLEU score is achieved. 1</p><p>4 0.85462135 <a title="223-lsi-4" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>Author: Conghui Zhu ; Taro Watanabe ; Eiichiro Sumita ; Tiejun Zhao</p><p>Abstract: Typical statistical machine translation systems are batch trained with a given training data and their performances are largely influenced by the amount of data. With the growth of the available data across different domains, it is computationally demanding to perform batch training every time when new data comes. In face of the problem, we propose an efficient phrase table combination method. In particular, we train a Bayesian phrasal inversion transduction grammars for each domain separately. The learned phrase tables are hierarchically combined as if they are drawn from a hierarchical Pitman-Yor process. The performance measured by BLEU is at least as comparable to the traditional batch training method. Furthermore, each phrase table is trained separately in each domain, and while computational overhead is significantly reduced by training them in parallel.</p><p>5 0.83491826 <a title="223-lsi-5" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>Author: Kun Wang ; Chengqing Zong ; Keh-Yih Su</p><p>Abstract: Since statistical machine translation (SMT) and translation memory (TM) complement each other in matched and unmatched regions, integrated models are proposed in this paper to incorporate TM information into phrase-based SMT. Unlike previous multi-stage pipeline approaches, which directly merge TM result into the final output, the proposed models refer to the corresponding TM information associated with each phrase at SMT decoding. On a Chinese–English TM database, our experiments show that the proposed integrated Model-III is significantly better than either the SMT or the TM systems when the fuzzy match score is above 0.4. Furthermore, integrated Model-III achieves overall 3.48 BLEU points improvement and 2.62 TER points reduction in comparison with the pure SMT system. Be- . sides, the proposed models also outperform previous approaches significantly.</p><p>6 0.8185761 <a title="223-lsi-6" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>7 0.81553811 <a title="223-lsi-7" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>8 0.78856999 <a title="223-lsi-8" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>9 0.78806698 <a title="223-lsi-9" href="./acl-2013-Language_Independent_Connectivity_Strength_Features_for_Phrase_Pivot_Statistical_Machine_Translation.html">214 acl-2013-Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation</a></p>
<p>10 0.77033699 <a title="223-lsi-10" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>11 0.76058358 <a title="223-lsi-11" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>12 0.74746281 <a title="223-lsi-12" href="./acl-2013-Task_Alternation_in_Parallel_Sentence_Retrieval_for_Twitter_Translation.html">338 acl-2013-Task Alternation in Parallel Sentence Retrieval for Twitter Translation</a></p>
<p>13 0.7427966 <a title="223-lsi-13" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>14 0.73666102 <a title="223-lsi-14" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>15 0.73646706 <a title="223-lsi-15" href="./acl-2013-TransDoop%3A_A_Map-Reduce_based_Crowdsourced_Translation_for_Complex_Domain.html">355 acl-2013-TransDoop: A Map-Reduce based Crowdsourced Translation for Complex Domain</a></p>
<p>16 0.73641264 <a title="223-lsi-16" href="./acl-2013-Context-Dependent_Multilingual_Lexical_Lookup_for_Under-Resourced_Languages.html">92 acl-2013-Context-Dependent Multilingual Lexical Lookup for Under-Resourced Languages</a></p>
<p>17 0.7139172 <a title="223-lsi-17" href="./acl-2013-Can_Markov_Models_Over_Minimal_Translation_Units_Help_Phrase-Based_SMT%3F.html">77 acl-2013-Can Markov Models Over Minimal Translation Units Help Phrase-Based SMT?</a></p>
<p>18 0.71035904 <a title="223-lsi-18" href="./acl-2013-Vector_Space_Model_for_Adaptation_in_Statistical_Machine_Translation.html">383 acl-2013-Vector Space Model for Adaptation in Statistical Machine Translation</a></p>
<p>19 0.70652944 <a title="223-lsi-19" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>20 0.69493127 <a title="223-lsi-20" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.094), (6, 0.029), (11, 0.067), (15, 0.015), (19, 0.082), (24, 0.057), (26, 0.066), (35, 0.07), (42, 0.1), (48, 0.05), (70, 0.045), (88, 0.034), (90, 0.066), (95, 0.115), (97, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94212079 <a title="223-lda-1" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>Author: Jiajun Zhang ; Chengqing Zong</p><p>Abstract: Currently, almost all of the statistical machine translation (SMT) models are trained with the parallel corpora in some specific domains. However, when it comes to a language pair or a different domain without any bilingual resources, the traditional SMT loses its power. Recently, some research works study the unsupervised SMT for inducing a simple word-based translation model from the monolingual corpora. It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result. In this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1</p><p>2 0.94170994 <a title="223-lda-2" href="./acl-2013-Learning_Semantic_Textual_Similarity_with_Structural_Representations.html">222 acl-2013-Learning Semantic Textual Similarity with Structural Representations</a></p>
<p>Author: Aliaksei Severyn ; Massimo Nicosia ; Alessandro Moschitti</p><p>Abstract: Measuring semantic textual similarity (STS) is at the cornerstone of many NLP applications. Different from the majority of approaches, where a large number of pairwise similarity features are used to represent a text pair, our model features the following: (i) it directly encodes input texts into relational syntactic structures; (ii) relies on tree kernels to handle feature engineering automatically; (iii) combines both structural and feature vector representations in a single scoring model, i.e., in Support Vector Regression (SVR); and (iv) delivers significant improvement over the best STS systems.</p><p>3 0.92793155 <a title="223-lda-3" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>Author: Wenduan Xu ; Yue Zhang ; Philip Williams ; Philipp Koehn</p><p>Abstract: We present a context-sensitive chart pruning method for CKY-style MT decoding. Source phrases that are unlikely to have aligned target constituents are identified using sequence labellers learned from the parallel corpus, and speed-up is obtained by pruning corresponding chart cells. The proposed method is easy to implement, orthogonal to cube pruning and additive to its pruning power. On a full-scale Englishto-German experiment with a string-totree model, we obtain a speed-up of more than 60% over a strong baseline, with no loss in BLEU.</p><p>4 0.9172076 <a title="223-lda-4" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>Author: Majid Razmara ; Maryam Siahbani ; Reza Haffari ; Anoop Sarkar</p><p>Abstract: Out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation especially when a limited amount of parallel text is available for training or when there is a domain shift from training data to test data. In this paper, we propose a novel approach to finding translations for oov words. We induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases. Our method differs from previous approaches by adopting a graph propagation approach that takes into account not only one-step (from oov directly to a source language phrase that has a translation) but multi-step paraphrases from oov source language words to other source language phrases and eventually to target language translations. Experimental results show that our graph propagation method significantly improves performance over two strong baselines under intrinsic and extrinsic evaluation metrics.</p><p>5 0.91705108 <a title="223-lda-5" href="./acl-2013-Text-Driven_Toponym_Resolution_using_Indirect_Supervision.html">340 acl-2013-Text-Driven Toponym Resolution using Indirect Supervision</a></p>
<p>Author: Michael Speriosu ; Jason Baldridge</p><p>Abstract: Toponym resolvers identify the specific locations referred to by ambiguous placenames in text. Most resolvers are based on heuristics using spatial relationships between multiple toponyms in a document, or metadata such as population. This paper shows that text-driven disambiguation for toponyms is far more effective. We exploit document-level geotags to indirectly generate training instances for text classifiers for toponym resolution, and show that textual cues can be straightforwardly integrated with other commonly used ones. Results are given for both 19th century texts pertaining to the American Civil War and 20th century newswire articles.</p><p>6 0.90975982 <a title="223-lda-6" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>7 0.90019006 <a title="223-lda-7" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>8 0.89902669 <a title="223-lda-8" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<p>9 0.89887238 <a title="223-lda-9" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>10 0.8985188 <a title="223-lda-10" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>11 0.89829922 <a title="223-lda-11" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<p>12 0.89781165 <a title="223-lda-12" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>13 0.89774126 <a title="223-lda-13" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>14 0.89746177 <a title="223-lda-14" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>15 0.89606166 <a title="223-lda-15" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>16 0.89529091 <a title="223-lda-16" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>17 0.89521879 <a title="223-lda-17" href="./acl-2013-Task_Alternation_in_Parallel_Sentence_Retrieval_for_Twitter_Translation.html">338 acl-2013-Task Alternation in Parallel Sentence Retrieval for Twitter Translation</a></p>
<p>18 0.8945384 <a title="223-lda-18" href="./acl-2013-Vector_Space_Model_for_Adaptation_in_Statistical_Machine_Translation.html">383 acl-2013-Vector Space Model for Adaptation in Statistical Machine Translation</a></p>
<p>19 0.89325291 <a title="223-lda-19" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>20 0.89228243 <a title="223-lda-20" href="./acl-2013-Sentence_Level_Dialect_Identification_in_Arabic.html">317 acl-2013-Sentence Level Dialect Identification in Arabic</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
