<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>208 acl-2013-Joint Inference for Heterogeneous Dependency Parsing</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-208" href="#">acl2013-208</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>208 acl-2013-Joint Inference for Heterogeneous Dependency Parsing</h1>
<br/><p>Source: <a title="acl-2013-208-pdf" href="http://aclweb.org/anthology//P/P13/P13-2019.pdf">pdf</a></p><p>Author: Guangyou Zhou ; Jun Zhao</p><p>Abstract: This paper is concerned with the problem of heterogeneous dependency parsing. In this paper, we present a novel joint inference scheme, which is able to leverage the consensus information between heterogeneous treebanks in the parsing phase. Different from stacked learning methods (Nivre and McDonald, 2008; Martins et al., 2008), which process the dependency parsing in a pipelined way (e.g., a second level uses the first level outputs), in our method, multiple dependency parsing models are coordinated to exchange consensus information. We conduct experiments on Chinese Dependency Treebank (CDT) and Penn Chinese Treebank (CTB), experimental results show that joint infer- ence can bring significant improvements to all state-of-the-art dependency parsers.</p><p>Reference: <a title="acl-2013-208-reference" href="../acl2013_reference/acl-2013-Joint_Inference_for_Heterogeneous_Dependency_Parsing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Joint Inference for Heterogeneous Dependency Parsing Guangyou Zhou and Jun Zhao National Laboratory of Pattern Recognition Institute of Automation, Chinese Academy of Sciences 95 Zhongguancun East Road, Beijing 100190, China {gy zhou , j zhao} @ nlpr . [sent-1, score-0.132]
</p><p>2 cn a  Abstract This paper is concerned with the problem of heterogeneous dependency parsing. [sent-4, score-0.466]
</p><p>3 In this paper, we present a novel joint inference scheme, which is able to leverage the consensus information between heterogeneous treebanks in the parsing phase. [sent-5, score-1.108]
</p><p>4 Different from stacked learning methods (Nivre and McDonald, 2008; Martins et al. [sent-6, score-0.063]
</p><p>5 , 2008), which process the dependency parsing in a pipelined way (e. [sent-7, score-0.482]
</p><p>6 , a second level uses the first level outputs), in our method, multiple dependency parsing models are coordinated to exchange consensus information. [sent-9, score-0.735]
</p><p>7 We conduct experiments on Chinese Dependency Treebank (CDT) and Penn Chinese Treebank (CTB), experimental results show that joint infer-  ence can bring significant improvements to all state-of-the-art dependency parsers. [sent-10, score-0.46]
</p><p>8 Over the past few years, supervised learning methods have obtained state-of-the-art performance for dependency parsing (Yamada and Matsumoto, 2003; McDonald et al. [sent-13, score-0.412]
</p><p>9 These methods usually rely heavily on the manually annotated treebanks for training the dependency models. [sent-18, score-0.466]
</p><p>10 (Hongkong ) ns  Figure 1: Different grammar formalisms of syntactic structures between CTB (upper) and CDT (below). [sent-33, score-0.132]
</p><p>11 CTB is converted into dependency grammar based on the head rules of (Zhang and Clark, 2008). [sent-34, score-0.322]
</p><p>12 tic structure, either phrase-based or dependencybased, is both time consuming and labor intensive. [sent-35, score-0.068]
</p><p>13 Making full use ofthe existing manually annotated treebanks would yield substantial savings in dataannotation costs. [sent-36, score-0.225]
</p><p>14 In this paper, we present a joint inference scheme for heterogenous dependency parsing. [sent-37, score-0.847]
</p><p>15 This scheme is able to leverage consensus information between heterogenous treebanks during the inference phase instead of using individual output in a pipelined way, such as stacked learning methods (Nivre and McDonald, 2008; Martins et al. [sent-38, score-1.031]
</p><p>16 The basic idea is very simple: although heterogenous treebanks have different grammar formalisms, they share some consensus information in dependency structures for the same sen-  tence. [sent-40, score-0.959]
</p><p>17 For example in Figure 1, the dependency structures actually share some partial agreements for the same sentence, the two words “eyes” and “Hongkong” depend on “cast” in both Chinese Dependency Treebank (CDT) (Liu et al. [sent-41, score-0.383]
</p><p>18 Therefore, we would like to train the dependency parsers on individual heterogenous treebank and jointly parse the same sentences with consensus information exchanged between them. [sent-44, score-1.013]
</p><p>19 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 104–109,  Figure2:GnaJloiTjtrePanbsfiercn1oktcseniutfosermdationexchTaPrnge sbarcnk2hemofterogeneous dependency parsing. [sent-47, score-0.286]
</p><p>20 Section 2 gives a formal description of the joint inference for heterogeneous dependency parsing. [sent-49, score-0.787]
</p><p>21 2  Our Approach  The general joint inference scheme of heterogeneous dependency parsing is shown in Figure 2. [sent-52, score-0.992]
</p><p>22 Here, heterogeneous treebanks refer to two Chinese treebanks: CTB and CDT, therefore we have only two parsers, but the framework is generic enough to integrate more parsers. [sent-53, score-0.36]
</p><p>23 For easy explanation of the joint inference scheme, we regard a parser without consensus information as a baseline parser, a parser incorporates consensus information called a joint parser. [sent-54, score-1.291]
</p><p>24 Joint inference provides a framework that accommodates and coordinates multiple dependency parsing models. [sent-55, score-0.628]
</p><p>25 (2010), the joint inference for heterogeneous dependency parsing consists of four components: (1) Joint In-  ference Model; (2) Parser Coordination; (3) Joint Inference Features; (4) Parameter Estimation. [sent-58, score-0.949]
</p><p>26 1 Joint Inference Model For a given sentence x, a joint dependency parsing model finds the best dependency parsing tree y∗ among the set of possible candidate parses Y(x) baamsoedng on a scoring fousnscibtiloen c Fs:  y∗ = aryg∈Ym(xa)xFs(x,y)  (1)  Following (Li et al. [sent-60, score-1.009]
</p><p>27 , 2009), we will use dk to denote the kth joint parser, and also use the notation Hk (x) for a list of parse candidates of sentence x determined by dk. [sent-61, score-0.625]
</p><p>28 Feature index lranges over all consensusbased features in equation (3). [sent-63, score-0.046]
</p><p>29 2 Parser Coordination Note that in equation (2), though the baseline score function Ps (x, y) can be computed individually, the case of Ψk (y, Hk (x)) is more complicated. [sent-65, score-0.082]
</p><p>30 It is not feasible yto, Henumerate all parse candidates for dependency parsing. [sent-66, score-0.466]
</p><p>31 The basic idea is that we can use baseline models’ nbest output as seeds, and iteratively refine joint models’ n-best output with joint inference. [sent-68, score-0.388]
</p><p>32 The joint inference process is shown in Algorithm 1. [sent-69, score-0.321]
</p><p>33 a tFei risnt, H Hwe extract bigram-subtrees tahnadt sctoonrtea tihne tmw ion wHords. [sent-71, score-0.085]
</p><p>34 If two words have a dependency relation, we add these two words as a subtree into Hk′ (x). [sent-72, score-0.286]
</p><p>35 rSeimlatiiloarnl,y, w we ea cda tnh eesxetra twcto t wriogrradms- assu abt sruebestr. [sent-73, score-0.084]
</p><p>36 Besides, we also store the “ROOT” word of each candidate in Hk′ (x) ; Step3: Use joint parsers to re-parse the sentence x with the baseline features and joint inference features (see subsection 2. [sent-75, score-0.684]
</p><p>37 For joint parser dk, consensus-based features of any dependency parsing candidate are computed based on current setting of Hs′ (x) for all s but k. [sent-77, score-0.73]
</p><p>38 New depenodenn ccyur preanrtsin segt tcianngd oidfa Htes generated by dk in re-parsing are cached in H′k′ (x); Step4: Update all Hk (x) with H′k′ (x) ; Step5: Iterate from Step2 to Step4 until a preset iteration limit is reached. [sent-78, score-0.401]
</p><p>39 In Algorithm 1, dependency parsing candidates of different parsers can be mutually improved. [sent-79, score-0.619]
</p><p>40 For example, given two parsers d1 and d2 with candidates H1 and H2, improvements on H1 enable d2 dtoa improve H2, a,n imd H1 ebmeneenftists o fnro Hm improved H2, manprdo so on. [sent-80, score-0.256]
</p><p>41 We can see that a joint parser does not enlarge the search space of its baseline model, the only change is parse scoring. [sent-81, score-0.429]
</p><p>42 By running a complete inference process, joint model can be applied  to re-parsing all candidates explored by a parser. [sent-82, score-0.42]
</p><p>43 105  Thus Step3 can be viewed as full-scale candidates reranking because the reranking scope is beyond the limited n-best output currently cached in Hk. [sent-83, score-0.282]
</p><p>44 3 Joint Inference Features In this section we introduce the consensus-based feature functions fk,l (y, Hk (x)) introduced in equation (3). [sent-85, score-0.046]
</p><p>45 The formu(lya,tiHon can be written as:  fk,l(y,Hk(x)) =y′∈∑Hk(x)P(y′|dk)Il(y,y′)  (4)  where y is a dependency parse of x by using parser ds (s k), y′ is a dependency parse in Hk (x) and( P(y′ |dk) iys tihse posterior probability ionf dependency parse y′ parsed by parser dk given sentence x. [sent-86, score-1.683]
</p><p>46 Il(y, y′) is a consensus measure defined on y and y′ using different feature functions. [sent-87, score-0.262]
</p><p>47 Dependency parsing model P(y′ |dk) can be predicted by using the global lin|edar models (GLMs) (e. [sent-88, score-0.126]
</p><p>48 Each headmodifier dependency (denoted as “edge”) is a tup∑le t =< h, m, h → m >, so Iedge(y, y′) =  ∑t∈y δ(t,y′). [sent-93, score-0.286]
</p><p>49 (2) sibling dependencies: Each sibling dependency (denoted as “sib”) is a tuple t =< i∑, h, m, h ← i → m >, so Isib(y, y′) =  ∑t∈y δ(t,y′). [sent-94, score-0.458]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hk', 0.474), ('dependency', 0.286), ('dk', 0.271), ('consensus', 0.262), ('cdt', 0.221), ('heterogeneous', 0.18), ('treebanks', 0.18), ('inference', 0.177), ('heterogenous', 0.161), ('ctb', 0.161), ('joint', 0.144), ('hongkong', 0.135), ('parser', 0.133), ('parsing', 0.126), ('mcdonald', 0.111), ('eyes', 0.108), ('parsers', 0.108), ('candidates', 0.099), ('zhou', 0.096), ('parse', 0.081), ('cast', 0.081), ('cached', 0.081), ('scheme', 0.079), ('chinese', 0.077), ('il', 0.076), ('treebank', 0.076), ('grandparent', 0.074), ('ps', 0.072), ('pipelined', 0.07), ('stacked', 0.063), ('sibling', 0.063), ('formalisms', 0.062), ('martins', 0.06), ('coordination', 0.059), ('fs', 0.058), ('sth', 0.054), ('pereira', 0.051), ('reranking', 0.051), ('preset', 0.049), ('dependencybased', 0.049), ('aryg', 0.049), ('cda', 0.049), ('formu', 0.049), ('manprdo', 0.049), ('othre', 0.049), ('sib', 0.049), ('zhongguancun', 0.049), ('tuple', 0.046), ('equation', 0.046), ('tahnadt', 0.045), ('ionf', 0.045), ('savings', 0.045), ('nivre', 0.044), ('yto', 0.042), ('denoted', 0.042), ('candidate', 0.041), ('zhao', 0.04), ('tihne', 0.04), ('memorize', 0.04), ('guangyou', 0.04), ('hwe', 0.04), ('leverage', 0.039), ('coordinates', 0.039), ('exchanged', 0.039), ('gy', 0.037), ('ihn', 0.037), ('baseline', 0.036), ('grammar', 0.036), ('nlpr', 0.036), ('nbest', 0.036), ('ference', 0.036), ('labor', 0.036), ('enlarge', 0.035), ('tnh', 0.035), ('structures', 0.034), ('subsection', 0.034), ('gp', 0.034), ('agreements', 0.033), ('coordinated', 0.033), ('ym', 0.033), ('penn', 0.033), ('vv', 0.032), ('consuming', 0.032), ('xa', 0.031), ('hm', 0.031), ('road', 0.031), ('automation', 0.03), ('east', 0.03), ('kth', 0.03), ('partial', 0.03), ('ence', 0.03), ('matsumoto', 0.03), ('dependencies', 0.03), ('subtrees', 0.029), ('ding', 0.029), ('exchange', 0.028), ('iterate', 0.028), ('refine', 0.028), ('seeds', 0.028), ('ef', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="208-tfidf-1" href="./acl-2013-Joint_Inference_for_Heterogeneous_Dependency_Parsing.html">208 acl-2013-Joint Inference for Heterogeneous Dependency Parsing</a></p>
<p>Author: Guangyou Zhou ; Jun Zhao</p><p>Abstract: This paper is concerned with the problem of heterogeneous dependency parsing. In this paper, we present a novel joint inference scheme, which is able to leverage the consensus information between heterogeneous treebanks in the parsing phase. Different from stacked learning methods (Nivre and McDonald, 2008; Martins et al., 2008), which process the dependency parsing in a pipelined way (e.g., a second level uses the first level outputs), in our method, multiple dependency parsing models are coordinated to exchange consensus information. We conduct experiments on Chinese Dependency Treebank (CDT) and Penn Chinese Treebank (CTB), experimental results show that joint infer- ence can bring significant improvements to all state-of-the-art dependency parsers.</p><p>2 0.21498004 <a title="208-tfidf-2" href="./acl-2013-Iterative_Transformation_of_Annotation_Guidelines_for_Constituency_Parsing.html">204 acl-2013-Iterative Transformation of Annotation Guidelines for Constituency Parsing</a></p>
<p>Author: Xiang Li ; Wenbin Jiang ; Yajuan Lu ; Qun Liu</p><p>Abstract: This paper presents an effective algorithm of annotation adaptation for constituency treebanks, which transforms a treebank from one annotation guideline to another with an iterative optimization procedure, thus to build a much larger treebank to train an enhanced parser without increasing model complexity. Experiments show that the transformed Tsinghua Chinese Treebank as additional training data brings significant improvement over the baseline trained on Penn Chinese Treebank only.</p><p>3 0.18383482 <a title="208-tfidf-3" href="./acl-2013-Universal_Dependency_Annotation_for_Multilingual_Parsing.html">368 acl-2013-Universal Dependency Annotation for Multilingual Parsing</a></p>
<p>Author: Ryan McDonald ; Joakim Nivre ; Yvonne Quirmbach-Brundage ; Yoav Goldberg ; Dipanjan Das ; Kuzman Ganchev ; Keith Hall ; Slav Petrov ; Hao Zhang ; Oscar Tackstrom ; Claudia Bedini ; Nuria Bertomeu Castello ; Jungmee Lee</p><p>Abstract: We present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages: German, English, Swedish, Spanish, French and Korean. To show the usefulness of such a resource, we present a case study of crosslingual transfer parsing with more reliable evaluation than has been possible before. This ‘universal’ treebank is made freely available in order to facilitate research on multilingual dependency parsing.1</p><p>4 0.15992656 <a title="208-tfidf-4" href="./acl-2013-Chinese_Parsing_Exploiting_Characters.html">80 acl-2013-Chinese Parsing Exploiting Characters</a></p>
<p>Author: Meishan Zhang ; Yue Zhang ; Wanxiang Che ; Ting Liu</p><p>Abstract: Characters play an important role in the Chinese language, yet computational processing of Chinese has been dominated by word-based approaches, with leaves in syntax trees being words. We investigate Chinese parsing from the character-level, extending the notion of phrase-structure trees by annotating internal structures of words. We demonstrate the importance of character-level information to Chinese processing by building a joint segmentation, part-of-speech (POS) tagging and phrase-structure parsing system that integrates character-structure features. Our joint system significantly outperforms a state-of-the-art word-based baseline on the standard CTB5 test, and gives the best published results for Chinese parsing.</p><p>5 0.15851963 <a title="208-tfidf-5" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>Author: Muhua Zhu ; Yue Zhang ; Wenliang Chen ; Min Zhang ; Jingbo Zhu</p><p>Abstract: Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser.</p><p>6 0.15535735 <a title="208-tfidf-6" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>7 0.14762983 <a title="208-tfidf-7" href="./acl-2013-Dependency_Parser_Adaptation_with_Subtrees_from_Auto-Parsed_Target_Domain_Data.html">112 acl-2013-Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data</a></p>
<p>8 0.14157389 <a title="208-tfidf-8" href="./acl-2013-Transition-based_Dependency_Parsing_with_Selectional_Branching.html">358 acl-2013-Transition-based Dependency Parsing with Selectional Branching</a></p>
<p>9 0.13633691 <a title="208-tfidf-9" href="./acl-2013-Turning_on_the_Turbo%3A_Fast_Third-Order_Non-Projective_Turbo_Parsers.html">362 acl-2013-Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers</a></p>
<p>10 0.13332123 <a title="208-tfidf-10" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>11 0.12948813 <a title="208-tfidf-11" href="./acl-2013-Coordination_Structures_in_Dependency_Treebanks.html">94 acl-2013-Coordination Structures in Dependency Treebanks</a></p>
<p>12 0.12556745 <a title="208-tfidf-12" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>13 0.12529378 <a title="208-tfidf-13" href="./acl-2013-A_Transition-Based_Dependency_Parser_Using_a_Dynamic_Parsing_Strategy.html">26 acl-2013-A Transition-Based Dependency Parser Using a Dynamic Parsing Strategy</a></p>
<p>14 0.11888374 <a title="208-tfidf-14" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>15 0.11637081 <a title="208-tfidf-15" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<p>16 0.11596055 <a title="208-tfidf-16" href="./acl-2013-An_Empirical_Examination_of_Challenges_in_Chinese_Parsing.html">44 acl-2013-An Empirical Examination of Challenges in Chinese Parsing</a></p>
<p>17 0.1070985 <a title="208-tfidf-17" href="./acl-2013-A_New_Syntactic_Metric_for_Evaluation_of_Machine_Translation.html">13 acl-2013-A New Syntactic Metric for Evaluation of Machine Translation</a></p>
<p>18 0.097136721 <a title="208-tfidf-18" href="./acl-2013-Discriminative_Learning_with_Natural_Annotations%3A_Word_Segmentation_as_a_Case_Study.html">123 acl-2013-Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study</a></p>
<p>19 0.095692508 <a title="208-tfidf-19" href="./acl-2013-Using_CCG_categories_to_improve_Hindi_dependency_parsing.html">372 acl-2013-Using CCG categories to improve Hindi dependency parsing</a></p>
<p>20 0.091500774 <a title="208-tfidf-20" href="./acl-2013-Margin-based_Decomposed_Amortized_Inference.html">237 acl-2013-Margin-based Decomposed Amortized Inference</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.171), (1, -0.135), (2, -0.262), (3, 0.037), (4, -0.082), (5, 0.008), (6, 0.036), (7, -0.03), (8, 0.045), (9, -0.119), (10, 0.019), (11, 0.015), (12, -0.016), (13, 0.04), (14, 0.084), (15, 0.03), (16, -0.064), (17, -0.022), (18, 0.021), (19, 0.01), (20, 0.024), (21, -0.0), (22, -0.094), (23, 0.021), (24, -0.052), (25, 0.087), (26, -0.036), (27, -0.005), (28, 0.04), (29, -0.022), (30, 0.032), (31, -0.009), (32, -0.001), (33, -0.009), (34, 0.074), (35, 0.04), (36, 0.007), (37, -0.106), (38, 0.112), (39, -0.074), (40, -0.001), (41, -0.032), (42, -0.098), (43, -0.119), (44, -0.012), (45, -0.118), (46, -0.03), (47, 0.028), (48, -0.082), (49, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97396016 <a title="208-lsi-1" href="./acl-2013-Joint_Inference_for_Heterogeneous_Dependency_Parsing.html">208 acl-2013-Joint Inference for Heterogeneous Dependency Parsing</a></p>
<p>Author: Guangyou Zhou ; Jun Zhao</p><p>Abstract: This paper is concerned with the problem of heterogeneous dependency parsing. In this paper, we present a novel joint inference scheme, which is able to leverage the consensus information between heterogeneous treebanks in the parsing phase. Different from stacked learning methods (Nivre and McDonald, 2008; Martins et al., 2008), which process the dependency parsing in a pipelined way (e.g., a second level uses the first level outputs), in our method, multiple dependency parsing models are coordinated to exchange consensus information. We conduct experiments on Chinese Dependency Treebank (CDT) and Penn Chinese Treebank (CTB), experimental results show that joint infer- ence can bring significant improvements to all state-of-the-art dependency parsers.</p><p>2 0.84417295 <a title="208-lsi-2" href="./acl-2013-Coordination_Structures_in_Dependency_Treebanks.html">94 acl-2013-Coordination Structures in Dependency Treebanks</a></p>
<p>Author: Martin Popel ; David Marecek ; Jan StÄłpanek ; Daniel Zeman ; ZdÄłnÄłk Zabokrtsky</p><p>Abstract: Paratactic syntactic structures are notoriously difficult to represent in dependency formalisms. This has painful consequences such as high frequency of parsing errors related to coordination. In other words, coordination is a pending problem in dependency analysis of natural languages. This paper tries to shed some light on this area by bringing a systematizing view of various formal means developed for encoding coordination structures. We introduce a novel taxonomy of such approaches and apply it to treebanks across a typologically diverse range of 26 languages. In addition, empirical observations on convertibility between selected styles of representations are shown too.</p><p>3 0.79240978 <a title="208-lsi-3" href="./acl-2013-Survey_on_parsing_three_dependency_representations_for_English.html">335 acl-2013-Survey on parsing three dependency representations for English</a></p>
<p>Author: Angelina Ivanova ; Stephan Oepen ; Lilja vrelid</p><p>Abstract: In this paper we focus on practical issues of data representation for dependency parsing. We carry out an experimental comparison of (a) three syntactic dependency schemes; (b) three data-driven dependency parsers; and (c) the influence of two different approaches to lexical category disambiguation (aka tagging) prior to parsing. Comparing parsing accuracies in various setups, we study the interactions of these three aspects and analyze which configurations are easier to learn for a dependency parser.</p><p>4 0.77264667 <a title="208-lsi-4" href="./acl-2013-Turning_on_the_Turbo%3A_Fast_Third-Order_Non-Projective_Turbo_Parsers.html">362 acl-2013-Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers</a></p>
<p>Author: Andre Martins ; Miguel Almeida ; Noah A. Smith</p><p>Abstract: We present fast, accurate, direct nonprojective dependency parsers with thirdorder features. Our approach uses AD3, an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models. Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-ofthe-art accuracies for the largest datasets (English, Czech, and German).</p><p>5 0.76566905 <a title="208-lsi-5" href="./acl-2013-Iterative_Transformation_of_Annotation_Guidelines_for_Constituency_Parsing.html">204 acl-2013-Iterative Transformation of Annotation Guidelines for Constituency Parsing</a></p>
<p>Author: Xiang Li ; Wenbin Jiang ; Yajuan Lu ; Qun Liu</p><p>Abstract: This paper presents an effective algorithm of annotation adaptation for constituency treebanks, which transforms a treebank from one annotation guideline to another with an iterative optimization procedure, thus to build a much larger treebank to train an enhanced parser without increasing model complexity. Experiments show that the transformed Tsinghua Chinese Treebank as additional training data brings significant improvement over the baseline trained on Penn Chinese Treebank only.</p><p>6 0.73629707 <a title="208-lsi-6" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>7 0.72644019 <a title="208-lsi-7" href="./acl-2013-Universal_Dependency_Annotation_for_Multilingual_Parsing.html">368 acl-2013-Universal Dependency Annotation for Multilingual Parsing</a></p>
<p>8 0.68488985 <a title="208-lsi-8" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>9 0.68041193 <a title="208-lsi-9" href="./acl-2013-Stop-probability_estimates_computed_on_a_large_corpus_improve_Unsupervised_Dependency_Parsing.html">331 acl-2013-Stop-probability estimates computed on a large corpus improve Unsupervised Dependency Parsing</a></p>
<p>10 0.61843377 <a title="208-lsi-10" href="./acl-2013-A_Unified_Morpho-Syntactic_Scheme_of_Stanford_Dependencies.html">28 acl-2013-A Unified Morpho-Syntactic Scheme of Stanford Dependencies</a></p>
<p>11 0.60692745 <a title="208-lsi-11" href="./acl-2013-Transition-based_Dependency_Parsing_with_Selectional_Branching.html">358 acl-2013-Transition-based Dependency Parsing with Selectional Branching</a></p>
<p>12 0.60525829 <a title="208-lsi-12" href="./acl-2013-An_Empirical_Examination_of_Challenges_in_Chinese_Parsing.html">44 acl-2013-An Empirical Examination of Challenges in Chinese Parsing</a></p>
<p>13 0.60108238 <a title="208-lsi-13" href="./acl-2013-A_Transition-Based_Dependency_Parser_Using_a_Dynamic_Parsing_Strategy.html">26 acl-2013-A Transition-Based Dependency Parser Using a Dynamic Parsing Strategy</a></p>
<p>14 0.59781075 <a title="208-lsi-14" href="./acl-2013-Punctuation_Prediction_with_Transition-based_Parsing.html">288 acl-2013-Punctuation Prediction with Transition-based Parsing</a></p>
<p>15 0.57186133 <a title="208-lsi-15" href="./acl-2013-Dependency_Parser_Adaptation_with_Subtrees_from_Auto-Parsed_Target_Domain_Data.html">112 acl-2013-Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data</a></p>
<p>16 0.56189269 <a title="208-lsi-16" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>17 0.51565731 <a title="208-lsi-17" href="./acl-2013-A_New_Syntactic_Metric_for_Evaluation_of_Machine_Translation.html">13 acl-2013-A New Syntactic Metric for Evaluation of Machine Translation</a></p>
<p>18 0.51471436 <a title="208-lsi-18" href="./acl-2013-Using_CCG_categories_to_improve_Hindi_dependency_parsing.html">372 acl-2013-Using CCG categories to improve Hindi dependency parsing</a></p>
<p>19 0.50741082 <a title="208-lsi-19" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>20 0.49377134 <a title="208-lsi-20" href="./acl-2013-Chinese_Parsing_Exploiting_Characters.html">80 acl-2013-Chinese Parsing Exploiting Characters</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.087), (6, 0.049), (11, 0.138), (14, 0.014), (24, 0.04), (26, 0.044), (28, 0.022), (35, 0.046), (42, 0.108), (48, 0.033), (58, 0.22), (70, 0.033), (71, 0.018), (88, 0.011), (90, 0.018), (95, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84572679 <a title="208-lda-1" href="./acl-2013-Joint_Inference_for_Heterogeneous_Dependency_Parsing.html">208 acl-2013-Joint Inference for Heterogeneous Dependency Parsing</a></p>
<p>Author: Guangyou Zhou ; Jun Zhao</p><p>Abstract: This paper is concerned with the problem of heterogeneous dependency parsing. In this paper, we present a novel joint inference scheme, which is able to leverage the consensus information between heterogeneous treebanks in the parsing phase. Different from stacked learning methods (Nivre and McDonald, 2008; Martins et al., 2008), which process the dependency parsing in a pipelined way (e.g., a second level uses the first level outputs), in our method, multiple dependency parsing models are coordinated to exchange consensus information. We conduct experiments on Chinese Dependency Treebank (CDT) and Penn Chinese Treebank (CTB), experimental results show that joint infer- ence can bring significant improvements to all state-of-the-art dependency parsers.</p><p>2 0.79845798 <a title="208-lda-2" href="./acl-2013-%22Let_Everything_Turn_Well_in_Your_Wife%22%3A_Generation_of_Adult_Humor_Using_Lexical_Constraints.html">1 acl-2013-"Let Everything Turn Well in Your Wife": Generation of Adult Humor Using Lexical Constraints</a></p>
<p>Author: Alessandro Valitutti ; Hannu Toivonen ; Antoine Doucet ; Jukka M. Toivanen</p><p>Abstract: We propose a method for automated generation of adult humor by lexical replacement and present empirical evaluation results of the obtained humor. We propose three types of lexical constraints as building blocks of humorous word substitution: constraints concerning the similarity of sounds or spellings of the original word and the substitute, a constraint requiring the substitute to be a taboo word, and constraints concerning the position and context of the replacement. Empirical evidence from extensive user studies indicates that these constraints can increase the effectiveness of humor generation significantly.</p><p>3 0.67913586 <a title="208-lda-3" href="./acl-2013-Transition-based_Dependency_Parsing_with_Selectional_Branching.html">358 acl-2013-Transition-based Dependency Parsing with Selectional Branching</a></p>
<p>Author: Jinho D. Choi ; Andrew McCallum</p><p>Abstract: We present a novel approach, called selectional branching, which uses confidence estimates to decide when to employ a beam, providing the accuracy of beam search at speeds close to a greedy transition-based dependency parsing approach. Selectional branching is guaranteed to perform a fewer number of transitions than beam search yet performs as accurately. We also present a new transition-based dependency parsing algorithm that gives a complexity of O(n) for projective parsing and an expected linear time speed for non-projective parsing. With the standard setup, our parser shows an unlabeled attachment score of 92.96% and a parsing speed of 9 milliseconds per sentence, which is faster and more accurate than the current state-of-the-art transitionbased parser that uses beam search.</p><p>4 0.64593709 <a title="208-lda-4" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>Author: Ji Ma ; Jingbo Zhu ; Tong Xiao ; Nan Yang</p><p>Abstract: In this paper, we combine easy-first dependency parsing and POS tagging algorithms with beam search and structured perceptron. We propose a simple variant of “early-update” to ensure valid update in the training process. The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity. On CTB, we achieve 94.01% tagging accuracy and 86.33% unlabeled attachment score with a relatively small beam width. On PTB, we also achieve state-of-the-art performance. 1</p><p>5 0.64468521 <a title="208-lda-5" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>Author: Jackie Chi Kit Cheung ; Gerald Penn</p><p>Abstract: Generative probabilistic models have been used for content modelling and template induction, and are typically trained on small corpora in the target domain. In contrast, vector space models of distributional semantics are trained on large corpora, but are typically applied to domaingeneral lexical disambiguation tasks. We introduce Distributional Semantic Hidden Markov Models, a novel variant of a hidden Markov model that integrates these two approaches by incorporating contextualized distributional semantic vectors into a generative model as observed emissions. Experiments in slot induction show that our approach yields improvements in learning coherent entity clusters in a domain. In a subsequent extrinsic evaluation, we show that these improvements are also reflected in multi-document summarization.</p><p>6 0.64136004 <a title="208-lda-6" href="./acl-2013-Efficient_Implementation_of_Beam-Search_Incremental_Parsers.html">133 acl-2013-Efficient Implementation of Beam-Search Incremental Parsers</a></p>
<p>7 0.63629597 <a title="208-lda-7" href="./acl-2013-Post-Retrieval_Clustering_Using_Third-Order_Similarity_Measures.html">281 acl-2013-Post-Retrieval Clustering Using Third-Order Similarity Measures</a></p>
<p>8 0.63557976 <a title="208-lda-8" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>9 0.63548815 <a title="208-lda-9" href="./acl-2013-A_Transition-Based_Dependency_Parser_Using_a_Dynamic_Parsing_Strategy.html">26 acl-2013-A Transition-Based Dependency Parser Using a Dynamic Parsing Strategy</a></p>
<p>10 0.6327346 <a title="208-lda-10" href="./acl-2013-Fast_and_Adaptive_Online_Training_of_Feature-Rich_Translation_Models.html">156 acl-2013-Fast and Adaptive Online Training of Feature-Rich Translation Models</a></p>
<p>11 0.63144195 <a title="208-lda-11" href="./acl-2013-Modeling_Human_Inference_Process_for_Textual_Entailment_Recognition.html">245 acl-2013-Modeling Human Inference Process for Textual Entailment Recognition</a></p>
<p>12 0.62935686 <a title="208-lda-12" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>13 0.62931883 <a title="208-lda-13" href="./acl-2013-Automatic_Interpretation_of_the_English_Possessive.html">61 acl-2013-Automatic Interpretation of the English Possessive</a></p>
<p>14 0.62766266 <a title="208-lda-14" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>15 0.62585223 <a title="208-lda-15" href="./acl-2013-Discriminative_Learning_with_Natural_Annotations%3A_Word_Segmentation_as_a_Case_Study.html">123 acl-2013-Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study</a></p>
<p>16 0.62561578 <a title="208-lda-16" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>17 0.62529671 <a title="208-lda-17" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>18 0.62521356 <a title="208-lda-18" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>19 0.62058276 <a title="208-lda-19" href="./acl-2013-Psycholinguistically_Motivated_Computational_Models_on_the_Organization_and_Processing_of_Morphologically_Complex_Words.html">286 acl-2013-Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words</a></p>
<p>20 0.61914027 <a title="208-lda-20" href="./acl-2013-Mining_Equivalent_Relations_from_Linked_Data.html">242 acl-2013-Mining Equivalent Relations from Linked Data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
