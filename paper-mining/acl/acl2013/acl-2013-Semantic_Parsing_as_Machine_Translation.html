<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>312 acl-2013-Semantic Parsing as Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-312" href="#">acl2013-312</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>312 acl-2013-Semantic Parsing as Machine Translation</h1>
<br/><p>Source: <a title="acl-2013-312-pdf" href="http://aclweb.org/anthology//P/P13/P13-2009.pdf">pdf</a></p><p>Author: Jacob Andreas ; Andreas Vlachos ; Stephen Clark</p><p>Abstract: Semantic parsing is the problem of deriving a structured meaning representation from a natural language utterance. Here we approach it as a straightforward machine translation task, and demonstrate that standard machine translation components can be adapted into a semantic parser. In experiments on the multilingual GeoQuery corpus we find that our parser is competitive with the state of the art, and in some cases achieves higher accuracy than recently proposed purpose-built systems. These results support the use of machine translation methods as an informative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could benefit from advances in machine translation.</p><p>Reference: <a title="acl-2013-312-reference" href="../acl2013_reference/acl-2013-Semantic_Parsing_as_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Semantic Parsing as Machine Translation Jacob Andreas Andreas Vlachos Stephen Clark Computer Laboratory Computer Laboratory Computer Laboratory University of Cambridge University of Cambridge University of Cambridge j da3 3 @ cam . [sent-1, score-0.074]
</p><p>2 uk  Abstract Semantic parsing is the problem of deriving a structured meaning representation from a natural language utterance. [sent-7, score-0.126]
</p><p>3 Here we approach it as a straightforward machine translation task, and demonstrate that standard machine translation components can be adapted into a semantic parser. [sent-8, score-0.308]
</p><p>4 In experiments on the multilingual GeoQuery corpus we find that our parser is competitive with the state of the art, and in some cases achieves higher accuracy than recently proposed purpose-built systems. [sent-9, score-0.164]
</p><p>5 These results support the use of machine translation methods as an informative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could benefit from advances in machine translation. [sent-10, score-0.359]
</p><p>6 1 Introduction Semantic parsing (SP) is the problem of transforming a natural language (NL) utterance into a machine-interpretable meaning representation (MR). [sent-11, score-0.19]
</p><p>7 At least superficially, SP is simply a machine translation (MT) task: we transform an NL utterance in one language into a statement of another (un-natural) meaning representation language (MRL). [sent-18, score-0.259]
</p><p>8 Indeed, successful semantic parsers often resemble MT systems in several important respects, including the use of word alignment models as a starting point for rule extraction (Wong and Mooney, 2006; Kwiatkowski et al. [sent-19, score-0.294]
</p><p>9 , 2010) and the use of automata such as tree transducers (Jones et al. [sent-20, score-0.179]
</p><p>10 Contrast this with ordinary MT, where varying degrees of wrongness are tolerated by human readers (and evaluation metrics). [sent-24, score-0.081]
</p><p>11 To avoid producing malformed MRs, almost all of the existing research on SP has focused on developing models with richer structure than those commonly used for MT. [sent-25, score-0.065]
</p><p>12 In this work we attempt to determine how accurate a semantic parser we can build by treating SP as a pure MT task, and describe pre- and postprocessing steps which allow structure to be preserved in the MT process. [sent-26, score-0.143]
</p><p>13 Our contributions are as follows: We develop a semantic parser using off-the-shelf MT components, exploring phrase-based as well as hierarchical models. [sent-27, score-0.228]
</p><p>14 Experiments with four languages on the popular GeoQuery corpus (Zelle, 1995) show that our parser is competitve with the state-ofthe-art, in some cases achieving higher accuracy than recently introduced purpose-built semantic  parsers. [sent-28, score-0.143]
</p><p>15 Our approach also appears to require substantially less time to train than the two bestperforming semantic parsers. [sent-29, score-0.102]
</p><p>16 2  MT-based semantic parsing  The input is a corpus of NL utterances paired with MRs. [sent-31, score-0.12]
</p><p>17 In order to learn a semantic parser using MT we linearize the MRs, learn alignments between the MRL and the NL, extract translation rules, and learn a language model for the MRL. [sent-32, score-0.521]
</p><p>18 We also specify a decoding procedure that will return structured MRs for an utterance during prediction. [sent-33, score-0.176]
</p><p>19 [X]  →  [X]  →  ⇓ EXTRACT (HIER) hstate , state1i hstate [X] texa , state1 [X] state1 stateid1 texas0i . [sent-39, score-0.24]
</p><p>20 Figure 1: Illustration of preprocessing and rule extraction. [sent-42, score-0.097]
</p><p>21 Linearization We assume that the MRL is variable-free (that is, the meaning representation  for each utterance is tree-shaped), noting that formalisms with variables, like the λ-calculus, can be mapped onto variable-free logical forms with combinatory logics (Curry et al. [sent-43, score-0.216]
</p><p>22 In order to learn a semantic parser using MT we begin by converting these MRs to a form more similar to NL. [sent-45, score-0.192]
</p><p>23 To do so, we simply take a preorder traversal of every functional form, and label every function with the number of arguments it takes. [sent-46, score-0.078]
</p><p>24 After translation, recovery of the function is easy: if the arity of every function in the MRL is known, then every traversal uniquely specifies its corresponding tree. [sent-47, score-0.118]
</p><p>25 Most importantly, it eliminates any possible ambiguity from the tree  reconstruction which takes place during decoding: given any sequence of decorated MRL tokens, we can always reconstruct the corresponding tree structure (if one exists). [sent-50, score-0.214]
</p><p>26 Alignment Following the linearization of the MRs, we find alignments between the MR tokens and the NL tokens using the IBM Model 4 (Brown et al. [sent-54, score-0.112]
</p><p>27 Once the alignment algorithm is run in both directions (NL to MRL, MRL to NL), we symmetrize the resulting alignments to obtain a consensus many-to-many alignment (Och and Ney, 2000; Koehn et al. [sent-56, score-0.202]
</p><p>28 Rule extraction From the many-to-many alignment we need to extract a translation rule table, consisting of corresponding phrases in NL and MRL. [sent-58, score-0.272]
</p><p>29 We consider a phrase-based translation model (Koehn et al. [sent-59, score-0.119]
</p><p>30 Rules for  the phrase-based model consist of pairs of aligned source and target sequences, while hierarchical rules are SCFG productions containing at most two instances of a single nonterminal symbol. [sent-61, score-0.145]
</p><p>31 Note that both extraction algorithms can learn rules which a traditional tree-transducer-based approach cannot—for example the right hand side [X] river1 all0 traverse1 [X] corresponding to the pair of disconnected tree fragments: [X] traverse ? [sent-62, score-0.171]
</p><p>32 Language modeling In addition to translation rules learned from a parallel corpus, MT systems also rely on an n-gram language model for the target language, estimated from a (typically larger) monolingual corpus. [sent-68, score-0.179]
</p><p>33 In the case of SP, such a monolingual corpus is rarely available, and we instead use the MRs available in the training data to  learn a language model of the MRL. [sent-69, score-0.049]
</p><p>34 Prediction Given a new NL utterance, we need to find the n best translations (i. [sent-71, score-0.04]
</p><p>35 sequences of decorated MRL tokens) that maximize the weighted sum of the translation score (the probabilities of the translations according to the rule translation table) and the language model score, a process usually referred to as decoding. [sent-73, score-0.465]
</p><p>36 Standard decoding procedures for MT produce an n-best list of all possible translations, but here we need to restrict ourselves to translations corresponding to well-formed MRs. [sent-74, score-0.082]
</p><p>37 In principle this could be done by re-writing the beam search algorithm used in decoding to immediately discard malformed MRs; for the experiments in this paper we simply filter the regular n-best list until we find a well-formed MR. [sent-75, score-0.107]
</p><p>38 Finally, we insert the brackets according to the tree structure specified by the argument number labels. [sent-77, score-0.062]
</p><p>39 All semantic parsers for GeoQuery we compare against also makes use of NP lists (Jones et al. [sent-83, score-0.141]
</p><p>40 In our experiments, the NP list was included by appending all entries as extra training sentences to the end of the training corpus of each language with 50 times the  weight of regular training examples, to ensure that they are learned as translation rules. [sent-85, score-0.119]
</p><p>41 Evaluation for each utterance is performed by executing both the predicted and the gold standard MRs against the database and obtaining their respective answers. [sent-86, score-0.098]
</p><p>42 Implementation In all experiments, we use the IBM Model 4 implementation from the GIZA++ toolkit (Och and Ney, 2000) for alignment, and the phrase-based and hierarchical models implemented in the Moses toolkit (Koehn et al. [sent-94, score-0.085]
</p><p>43 The best symmetrization algorithm, translation and language model weights for each language are selected using cross-validation on the development set. [sent-96, score-0.119]
</p><p>44 4  Results  We first compare the results for the two translation rule extraction models, phrase-based and hierarchical (“MT-phrase” and “MT-hier” respectively in Table 1). [sent-99, score-0.301]
</p><p>45 We find that the hierarchical model performs better in all languages apart from Greek, indicating that the long-range reorderings learned by a hierarchical translation system are useful for this task. [sent-100, score-0.289]
</p><p>46 As expected, the performances are almost uniformly lower, but the parser still produces correct output for the majority of examples. [sent-103, score-0.073]
</p><p>47 It is not evident, a priori, that this search procedure is guaranteed to find any well-formed outputs in reasonable time; to test the effect of this extra requirement on en  de  el  th  MT-phrase  75. [sent-105, score-0.036]
</p><p>48 In practice, increasing search depth in the n-best list from 1to 50 results in a gain of no more than a percentage point or two, and we conclude that our filtering method is appropriate for the task. [sent-126, score-0.034]
</p><p>49 We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL; tsVB (Jones et al. [sent-127, score-0.263]
</p><p>50 , 2012), which uses variational Bayesian inference to learn weights for a tree transducer; UBL (Kwiatkowski et al. [sent-128, score-0.152]
</p><p>51 , 2010), which learns a CCG lexicon with semantic annotations; and hybridtree (Lu et al. [sent-129, score-0.18]
</p><p>52 , 2008), which learns a synchronous generative model over variable-free MRs and NL strings. [sent-130, score-0.037]
</p><p>53 In the results shown in Table 1 we observe that on English GeoQuery data, the hierarchical translation model achieves scores competitive with the state of the art, and in every language one of the MT systems achieves accuracy at least as good as  a purpose-built semantic parser. [sent-131, score-0.365]
</p><p>54 So in addition to competitive performance, the MTbased parser also appears to be considerably more efficient at training time than other parsers in the literature. [sent-135, score-0.179]
</p><p>55 Like the present work, it uses GIZA++ alignments as a starting point for the rule extraction procedure, and algorithms reminiscent of those used in syntactic MT to extract rules. [sent-137, score-0.155]
</p><p>56 tsVB also uses a piece of standard MT ma-  chinery, specifically tree transducers, which have been profitably employed for syntax-based machine translation (Maletti, 2010). [sent-138, score-0.213]
</p><p>57 In that work, however, the usual MT parameter-estimation technique of simply counting the number of rule occurrences does not improve scores, and the authors instead resort to a variational inference procedure to acquire rule weights. [sent-139, score-0.271]
</p><p>58 The present work is also the first we are aware of which uses phrasebased rather than tree-based machine translation techniques to learn a semantic parser. [sent-140, score-0.238]
</p><p>59 It employs resolution procedures specific to the λ-calculus such as splitting and unification in order to generate rule templates. [sent-145, score-0.175]
</p><p>60 Like other systems described, it uses GIZA alignments for initialization. [sent-146, score-0.058]
</p><p>61 Other work which generalizes from variable-free meaning representations to λ-calculus expressions includes the  natural language generation procedure described by Lu and Ng (201 1). [sent-147, score-0.078]
</p><p>62 UBL, like an MT system (and unlike most of the other systems discussed in this section), extracts rules at multiple levels of granularity by means of this splitting and unification procedure. [sent-148, score-0.138]
</p><p>63 hybridtree similarly benefits from the introduction of 50  multi-level rules composed from smaller rules, a process similar to the one used for creating phrase tables in a phrase-based MT system. [sent-149, score-0.133]
</p><p>64 6  Discussion  Our results validate the hypothesis that it is possible to adapt an ordinary MT system into a working semantic parser. [sent-150, score-0.119]
</p><p>65 For this reason, we argue for the use of a machine translation baseline as a point of comparison for new methods. [sent-152, score-0.119]
</p><p>66 The results also demonstrate the usefulness of two techniques  which are crucial for successful MT, but which are not widely used in semantic parsing. [sent-153, score-0.07]
</p><p>67 The second is the use of large, composed rules (rather than rules which trigger on only one lexical item, or on tree portions of limited depth (Lu et al. [sent-155, score-0.216]
</p><p>68 7  Conclusions  We have presented a semantic parser which uses techniques from machine translation to learn mappings from natural language to variable-free meaning representations. [sent-157, score-0.353]
</p><p>69 The parser performs comparably to several recent purpose-built semantic parsers on the GeoQuery dataset, while training considerably faster than state-of-the-art systems. [sent-158, score-0.214]
</p><p>70 Our experiments demonstrate the usefulness of several techniques which might be broadly applied to other semantic parsers, and provides an infor-  mative basis for future work. [sent-159, score-0.102]
</p><p>71 Inducing probabilistic ccg grammars from logical form with higherorder unification. [sent-216, score-0.038]
</p><p>72 A probabilistic  forest-to-string model for language generation from typed lambda calculus expressions. [sent-224, score-0.102]
</p><p>73 A generative model for parsing natural language to meaning representations. [sent-229, score-0.092]
</p><p>74 Towards a theory of natural language interfaces to databases. [sent-241, score-0.036]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mrl', 0.453), ('mrs', 0.359), ('geoquery', 0.253), ('nl', 0.247), ('mt', 0.18), ('sp', 0.162), ('ubl', 0.13), ('translation', 0.119), ('curry', 0.11), ('texa', 0.11), ('tsvb', 0.11), ('jones', 0.1), ('utterance', 0.098), ('rule', 0.097), ('kwiatkowski', 0.095), ('decorated', 0.09), ('mr', 0.085), ('hierarchical', 0.085), ('thai', 0.084), ('lu', 0.079), ('cam', 0.074), ('parser', 0.073), ('cityid', 0.073), ('hybridtree', 0.073), ('wasp', 0.073), ('transducers', 0.072), ('arity', 0.072), ('parsers', 0.071), ('semantic', 0.07), ('andreas', 0.067), ('greek', 0.066), ('hstate', 0.065), ('malformed', 0.065), ('texas', 0.063), ('tree', 0.062), ('koehn', 0.062), ('rules', 0.06), ('alignments', 0.058), ('state', 0.056), ('zelle', 0.056), ('alignment', 0.056), ('calculus', 0.054), ('linearization', 0.054), ('linearize', 0.054), ('vlachos', 0.054), ('wong', 0.053), ('border', 0.051), ('parsing', 0.05), ('ordinary', 0.049), ('learn', 0.049), ('lambda', 0.048), ('goldwasser', 0.048), ('unification', 0.046), ('traversal', 0.046), ('automata', 0.045), ('och', 0.045), ('np', 0.044), ('scfg', 0.044), ('combinatory', 0.044), ('alexandra', 0.042), ('decoding', 0.042), ('minutes', 0.042), ('meaning', 0.042), ('giza', 0.041), ('popescu', 0.041), ('variational', 0.041), ('translations', 0.04), ('luke', 0.039), ('ccg', 0.038), ('learns', 0.037), ('bird', 0.036), ('interfaces', 0.036), ('procedure', 0.036), ('competitive', 0.035), ('laboratory', 0.035), ('sharon', 0.035), ('della', 0.035), ('edward', 0.035), ('depth', 0.034), ('mooney', 0.034), ('uk', 0.034), ('cambridge', 0.033), ('reilly', 0.032), ('logics', 0.032), ('jena', 0.032), ('tolerated', 0.032), ('santa', 0.032), ('symmetrize', 0.032), ('preorder', 0.032), ('spacebook', 0.032), ('prolog', 0.032), ('bestperforming', 0.032), ('churchill', 0.032), ('maletti', 0.032), ('mative', 0.032), ('profitably', 0.032), ('pietra', 0.032), ('hwee', 0.032), ('splitting', 0.032), ('tou', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="312-tfidf-1" href="./acl-2013-Semantic_Parsing_as_Machine_Translation.html">312 acl-2013-Semantic Parsing as Machine Translation</a></p>
<p>Author: Jacob Andreas ; Andreas Vlachos ; Stephen Clark</p><p>Abstract: Semantic parsing is the problem of deriving a structured meaning representation from a natural language utterance. Here we approach it as a straightforward machine translation task, and demonstrate that standard machine translation components can be adapted into a semantic parser. In experiments on the multilingual GeoQuery corpus we find that our parser is competitive with the state of the art, and in some cases achieves higher accuracy than recently proposed purpose-built systems. These results support the use of machine translation methods as an informative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could benefit from advances in machine translation.</p><p>2 0.15376621 <a title="312-tfidf-2" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>Author: Graham Neubig</p><p>Abstract: In this paper we describe Travatar, a forest-to-string machine translation (MT) engine based on tree transducers. It provides an open-source C++ implementation for the entire forest-to-string MT pipeline, including rule extraction, tuning, decoding, and evaluation. There are a number of options for model training, and tuning includes advanced options such as hypergraph MERT, and training of sparse features through online learning. The training pipeline is modeled after that of the popular Moses decoder, so users familiar with Moses should be able to get started quickly. We perform a validation experiment of the decoder on EnglishJapanese machine translation, and find that it is possible to achieve greater accuracy than translation using phrase-based and hierarchical-phrase-based translation. As auxiliary results, we also compare different syntactic parsers and alignment techniques that we tested in the process of developing the decoder. Travatar is available under the LGPL at http : / /phont ron . com/t ravat ar</p><p>3 0.12862542 <a title="312-tfidf-3" href="./acl-2013-Adapting_Discriminative_Reranking_to_Grounded_Language_Learning.html">36 acl-2013-Adapting Discriminative Reranking to Grounded Language Learning</a></p>
<p>Author: Joohyun Kim ; Raymond Mooney</p><p>Abstract: We adapt discriminative reranking to improve the performance of grounded language acquisition, specifically the task of learning to follow navigation instructions from observation. Unlike conventional reranking used in syntactic and semantic parsing, gold-standard reference trees are not naturally available in a grounded setting. Instead, we show how the weak supervision of response feedback (e.g. successful task completion) can be used as an alternative, experimentally demonstrating that its performance is comparable to training on gold-standard parse trees.</p><p>4 0.12815107 <a title="312-tfidf-4" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<p>Author: Marzieh Bazrafshan ; Daniel Gildea</p><p>Abstract: We experiment with adding semantic role information to a string-to-tree machine translation system based on the rule extraction procedure of Galley et al. (2004). We compare methods based on augmenting the set of nonterminals by adding semantic role labels, and altering the rule extraction process to produce a separate set of rules for each predicate that encompass its entire predicate-argument structure. Our results demonstrate that the second approach is effective in increasing the quality of translations.</p><p>5 0.11526992 <a title="312-tfidf-5" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>Author: Fabienne Braune ; Nina Seemann ; Daniel Quernheim ; Andreas Maletti</p><p>Abstract: We present a new translation model integrating the shallow local multi bottomup tree transducer. We perform a largescale empirical evaluation of our obtained system, which demonstrates that we significantly beat a realistic tree-to-tree baseline on the WMT 2009 English → German tlriannes olnati tohne tWasMk.T TA 2s0 an a Edndgitliisonha →l c Gonetrrmibauntion we make the developed software and complete tool-chain publicly available for further experimentation.</p><p>6 0.11410104 <a title="312-tfidf-6" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>7 0.10969835 <a title="312-tfidf-7" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>8 0.10951722 <a title="312-tfidf-8" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>9 0.10398301 <a title="312-tfidf-9" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>10 0.10367972 <a title="312-tfidf-10" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>11 0.10296196 <a title="312-tfidf-11" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>12 0.10277712 <a title="312-tfidf-12" href="./acl-2013-Leveraging_Domain-Independent_Information_in_Semantic_Parsing.html">228 acl-2013-Leveraging Domain-Independent Information in Semantic Parsing</a></p>
<p>13 0.10219361 <a title="312-tfidf-13" href="./acl-2013-Improving_machine_translation_by_training_against_an_automatic_semantic_frame_based_evaluation_metric.html">195 acl-2013-Improving machine translation by training against an automatic semantic frame based evaluation metric</a></p>
<p>14 0.10001923 <a title="312-tfidf-14" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>15 0.099044211 <a title="312-tfidf-15" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>16 0.095499493 <a title="312-tfidf-16" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>17 0.09318357 <a title="312-tfidf-17" href="./acl-2013-A_New_Syntactic_Metric_for_Evaluation_of_Machine_Translation.html">13 acl-2013-A New Syntactic Metric for Evaluation of Machine Translation</a></p>
<p>18 0.090694986 <a title="312-tfidf-18" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>19 0.088945255 <a title="312-tfidf-19" href="./acl-2013-Stem_Translation_with_Affix-Based_Rule_Selection_for_Agglutinative_Languages.html">330 acl-2013-Stem Translation with Affix-Based Rule Selection for Agglutinative Languages</a></p>
<p>20 0.086837232 <a title="312-tfidf-20" href="./acl-2013-Transition-based_Dependency_Parsing_with_Selectional_Branching.html">358 acl-2013-Transition-based Dependency Parsing with Selectional Branching</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.239), (1, -0.132), (2, 0.055), (3, 0.033), (4, -0.106), (5, 0.042), (6, 0.039), (7, -0.03), (8, 0.035), (9, 0.046), (10, -0.024), (11, 0.044), (12, 0.049), (13, 0.014), (14, 0.025), (15, -0.028), (16, 0.016), (17, -0.009), (18, 0.021), (19, -0.009), (20, -0.042), (21, -0.029), (22, -0.051), (23, 0.037), (24, -0.005), (25, -0.01), (26, -0.011), (27, 0.032), (28, 0.026), (29, 0.07), (30, 0.023), (31, -0.015), (32, 0.032), (33, -0.007), (34, -0.027), (35, 0.015), (36, -0.083), (37, 0.04), (38, 0.006), (39, 0.099), (40, -0.1), (41, 0.119), (42, 0.012), (43, -0.017), (44, 0.105), (45, 0.113), (46, -0.043), (47, -0.015), (48, 0.016), (49, -0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93125838 <a title="312-lsi-1" href="./acl-2013-Semantic_Parsing_as_Machine_Translation.html">312 acl-2013-Semantic Parsing as Machine Translation</a></p>
<p>Author: Jacob Andreas ; Andreas Vlachos ; Stephen Clark</p><p>Abstract: Semantic parsing is the problem of deriving a structured meaning representation from a natural language utterance. Here we approach it as a straightforward machine translation task, and demonstrate that standard machine translation components can be adapted into a semantic parser. In experiments on the multilingual GeoQuery corpus we find that our parser is competitive with the state of the art, and in some cases achieves higher accuracy than recently proposed purpose-built systems. These results support the use of machine translation methods as an informative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could benefit from advances in machine translation.</p><p>2 0.80873966 <a title="312-lsi-2" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>Author: Fabienne Braune ; Nina Seemann ; Daniel Quernheim ; Andreas Maletti</p><p>Abstract: We present a new translation model integrating the shallow local multi bottomup tree transducer. We perform a largescale empirical evaluation of our obtained system, which demonstrates that we significantly beat a realistic tree-to-tree baseline on the WMT 2009 English → German tlriannes olnati tohne tWasMk.T TA 2s0 an a Edndgitliisonha →l c Gonetrrmibauntion we make the developed software and complete tool-chain publicly available for further experimentation.</p><p>3 0.78567982 <a title="312-lsi-3" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>Author: Graham Neubig</p><p>Abstract: In this paper we describe Travatar, a forest-to-string machine translation (MT) engine based on tree transducers. It provides an open-source C++ implementation for the entire forest-to-string MT pipeline, including rule extraction, tuning, decoding, and evaluation. There are a number of options for model training, and tuning includes advanced options such as hypergraph MERT, and training of sparse features through online learning. The training pipeline is modeled after that of the popular Moses decoder, so users familiar with Moses should be able to get started quickly. We perform a validation experiment of the decoder on EnglishJapanese machine translation, and find that it is possible to achieve greater accuracy than translation using phrase-based and hierarchical-phrase-based translation. As auxiliary results, we also compare different syntactic parsers and alignment techniques that we tested in the process of developing the decoder. Travatar is available under the LGPL at http : / /phont ron . com/t ravat ar</p><p>4 0.74857396 <a title="312-lsi-4" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>Author: Wenduan Xu ; Yue Zhang ; Philip Williams ; Philipp Koehn</p><p>Abstract: We present a context-sensitive chart pruning method for CKY-style MT decoding. Source phrases that are unlikely to have aligned target constituents are identified using sequence labellers learned from the parallel corpus, and speed-up is obtained by pruning corresponding chart cells. The proposed method is easy to implement, orthogonal to cube pruning and additive to its pruning power. On a full-scale Englishto-German experiment with a string-totree model, we obtain a speed-up of more than 60% over a strong baseline, with no loss in BLEU.</p><p>5 0.7423262 <a title="312-lsi-5" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<p>Author: Marzieh Bazrafshan ; Daniel Gildea</p><p>Abstract: We experiment with adding semantic role information to a string-to-tree machine translation system based on the rule extraction procedure of Galley et al. (2004). We compare methods based on augmenting the set of nonterminals by adding semantic role labels, and altering the rule extraction process to produce a separate set of rules for each predicate that encompass its entire predicate-argument structure. Our results demonstrate that the second approach is effective in increasing the quality of translations.</p><p>6 0.69573551 <a title="312-lsi-6" href="./acl-2013-Adapting_Discriminative_Reranking_to_Grounded_Language_Learning.html">36 acl-2013-Adapting Discriminative Reranking to Grounded Language Learning</a></p>
<p>7 0.66169631 <a title="312-lsi-7" href="./acl-2013-General_binarization_for_parsing_and_translation.html">165 acl-2013-General binarization for parsing and translation</a></p>
<p>8 0.65429038 <a title="312-lsi-8" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>9 0.65371108 <a title="312-lsi-9" href="./acl-2013-Stem_Translation_with_Affix-Based_Rule_Selection_for_Agglutinative_Languages.html">330 acl-2013-Stem Translation with Affix-Based Rule Selection for Agglutinative Languages</a></p>
<p>10 0.65295172 <a title="312-lsi-10" href="./acl-2013-From_Natural_Language_Specifications_to_Program_Input_Parsers.html">163 acl-2013-From Natural Language Specifications to Program Input Parsers</a></p>
<p>11 0.6400106 <a title="312-lsi-11" href="./acl-2013-A_New_Syntactic_Metric_for_Evaluation_of_Machine_Translation.html">13 acl-2013-A New Syntactic Metric for Evaluation of Machine Translation</a></p>
<p>12 0.63811851 <a title="312-lsi-12" href="./acl-2013-A_Novel_Translation_Framework_Based_on_Rhetorical_Structure_Theory.html">16 acl-2013-A Novel Translation Framework Based on Rhetorical Structure Theory</a></p>
<p>13 0.63639402 <a title="312-lsi-13" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>14 0.63520527 <a title="312-lsi-14" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>15 0.62792927 <a title="312-lsi-15" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>16 0.62747073 <a title="312-lsi-16" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<p>17 0.62503678 <a title="312-lsi-17" href="./acl-2013-Automatically_Predicting_Sentence_Translation_Difficulty.html">64 acl-2013-Automatically Predicting Sentence Translation Difficulty</a></p>
<p>18 0.61452687 <a title="312-lsi-18" href="./acl-2013-Leveraging_Domain-Independent_Information_in_Semantic_Parsing.html">228 acl-2013-Leveraging Domain-Independent Information in Semantic Parsing</a></p>
<p>19 0.6090039 <a title="312-lsi-19" href="./acl-2013-Semantic_Parsing_with_Combinatory_Categorial_Grammars.html">313 acl-2013-Semantic Parsing with Combinatory Categorial Grammars</a></p>
<p>20 0.60826099 <a title="312-lsi-20" href="./acl-2013-Propminer%3A_A_Workflow_for_Interactive_Information_Extraction_and_Exploration_using_Dependency_Trees.html">285 acl-2013-Propminer: A Workflow for Interactive Information Extraction and Exploration using Dependency Trees</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.058), (6, 0.057), (11, 0.049), (24, 0.043), (26, 0.069), (28, 0.014), (35, 0.08), (42, 0.077), (48, 0.05), (64, 0.016), (70, 0.034), (76, 0.208), (88, 0.035), (90, 0.069), (95, 0.077)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.81456393 <a title="312-lda-1" href="./acl-2013-Arguments_and_Modifiers_from_the_Learner%27s_Perspective.html">57 acl-2013-Arguments and Modifiers from the Learner's Perspective</a></p>
<p>Author: Leon Bergen ; Edward Gibson ; Timothy J. O'Donnell</p><p>Abstract: We present a model for inducing sentential argument structure, which distinguishes arguments from optional modifiers. We use this model to study whether representing an argument/modifier distinction helps in learning argument structure, and whether a linguistically-natural argument/modifier distinction can be induced from distributional data alone. Our results provide evidence for both hypotheses.</p><p>2 0.812837 <a title="312-lda-2" href="./acl-2013-Exploiting_Topic_based_Twitter_Sentiment_for_Stock_Prediction.html">147 acl-2013-Exploiting Topic based Twitter Sentiment for Stock Prediction</a></p>
<p>Author: Jianfeng Si ; Arjun Mukherjee ; Bing Liu ; Qing Li ; Huayi Li ; Xiaotie Deng</p><p>Abstract: This paper proposes a technique to leverage topic based sentiments from Twitter to help predict the stock market. We first utilize a continuous Dirichlet Process Mixture model to learn the daily topic set. Then, for each topic we derive its sentiment according to its opinion words distribution to build a sentiment time series. We then regress the stock index and the Twitter sentiment time series to predict the market. Experiments on real-life S&P100; Index show that our approach is effective and performs better than existing state-of-the-art non-topic based methods. 1</p><p>same-paper 3 0.80896568 <a title="312-lda-3" href="./acl-2013-Semantic_Parsing_as_Machine_Translation.html">312 acl-2013-Semantic Parsing as Machine Translation</a></p>
<p>Author: Jacob Andreas ; Andreas Vlachos ; Stephen Clark</p><p>Abstract: Semantic parsing is the problem of deriving a structured meaning representation from a natural language utterance. Here we approach it as a straightforward machine translation task, and demonstrate that standard machine translation components can be adapted into a semantic parser. In experiments on the multilingual GeoQuery corpus we find that our parser is competitive with the state of the art, and in some cases achieves higher accuracy than recently proposed purpose-built systems. These results support the use of machine translation methods as an informative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could benefit from advances in machine translation.</p><p>4 0.67372227 <a title="312-lda-4" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>Author: Wenduan Xu ; Yue Zhang ; Philip Williams ; Philipp Koehn</p><p>Abstract: We present a context-sensitive chart pruning method for CKY-style MT decoding. Source phrases that are unlikely to have aligned target constituents are identified using sequence labellers learned from the parallel corpus, and speed-up is obtained by pruning corresponding chart cells. The proposed method is easy to implement, orthogonal to cube pruning and additive to its pruning power. On a full-scale Englishto-German experiment with a string-totree model, we obtain a speed-up of more than 60% over a strong baseline, with no loss in BLEU.</p><p>5 0.66764295 <a title="312-lda-5" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>Author: Lu Wang ; Hema Raghavan ; Vittorio Castelli ; Radu Florian ; Claire Cardie</p><p>Abstract: We consider the problem of using sentence compression techniques to facilitate queryfocused multi-document summarization. We present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. An innovative beam search decoder is proposed to efficiently find highly probable compressions. Under this framework, we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function. Our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task. ,</p><p>6 0.66581124 <a title="312-lda-6" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>7 0.66323012 <a title="312-lda-7" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>8 0.66231984 <a title="312-lda-8" href="./acl-2013-Models_of_Translation_Competitions.html">250 acl-2013-Models of Translation Competitions</a></p>
<p>9 0.66229618 <a title="312-lda-9" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>10 0.66197002 <a title="312-lda-10" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>11 0.66189688 <a title="312-lda-11" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<p>12 0.66162777 <a title="312-lda-12" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>13 0.6613239 <a title="312-lda-13" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>14 0.66058838 <a title="312-lda-14" href="./acl-2013-Online_Relative_Margin_Maximization_for_Statistical_Machine_Translation.html">264 acl-2013-Online Relative Margin Maximization for Statistical Machine Translation</a></p>
<p>15 0.65997529 <a title="312-lda-15" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>16 0.65668046 <a title="312-lda-16" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>17 0.65662557 <a title="312-lda-17" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>18 0.65612531 <a title="312-lda-18" href="./acl-2013-Stacking_for_Statistical_Machine_Translation.html">328 acl-2013-Stacking for Statistical Machine Translation</a></p>
<p>19 0.65569955 <a title="312-lda-19" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>20 0.65482128 <a title="312-lda-20" href="./acl-2013-Modelling_Annotator_Bias_with_Multi-task_Gaussian_Processes%3A_An_Application_to_Machine_Translation_Quality_Estimation.html">248 acl-2013-Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
