<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>381 acl-2013-Variable Bit Quantisation for LSH</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-381" href="#">acl2013-381</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>381 acl-2013-Variable Bit Quantisation for LSH</h1>
<br/><p>Source: <a title="acl-2013-381-pdf" href="http://aclweb.org/anthology//P/P13/P13-2132.pdf">pdf</a></p><p>Author: Sean Moran ; Victor Lavrenko ; Miles Osborne</p><p>Abstract: We introduce a scheme for optimally allocating a variable number of bits per LSH hyperplane. Previous approaches assign a constant number of bits per hyperplane. This neglects the fact that a subset of hyperplanes may be more informative than others. Our method, dubbed Variable Bit Quantisation (VBQ), provides a datadriven non-uniform bit allocation across hyperplanes. Despite only using a fraction of the available hyperplanes, VBQ outperforms uniform quantisation by up to 168% for retrieval across standard text and image datasets.</p><p>Reference: <a title="acl-2013-381-reference" href="../acl2013_reference/acl-2013-Variable_Bit_Quantisation_for_LSH_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract We introduce a scheme for optimally allocating a variable number of bits per LSH hyperplane. [sent-10, score-0.487]
</p><p>2 Previous approaches assign a constant number of bits per hyperplane. [sent-11, score-0.278]
</p><p>3 This neglects the fact that a subset of hyperplanes may be more informative than others. [sent-12, score-0.385]
</p><p>4 Our method, dubbed Variable Bit Quantisation (VBQ), provides a datadriven non-uniform bit allocation across hyperplanes. [sent-13, score-0.389]
</p><p>5 Despite only using a fraction of the available hyperplanes, VBQ outperforms uniform quantisation by up to 168% for retrieval across standard text and image datasets. [sent-14, score-0.61]
</p><p>6 1 Introduction The task of retrieving the nearest neighbours to a  given query document permeates the field of Natural Language Processing (NLP). [sent-15, score-0.444]
</p><p>7 Nearest neighbour search has been used for applications as diverse as automatically detecting document translation pairs for the purposes of training a statistical machine translation system (SMT) (Krstovski and Smith, 2011), the large-scale generation of noun similarity lists (Ravichandran et al. [sent-16, score-0.218]
</p><p>8 Approximate nearest neighbour (ANN) search using hashing techniques has recently gained prominence within NLP. [sent-19, score-0.644]
</p><p>9 The hashing-based approach maps the data into a substantially more compact representation referred to as a fingerprint, that is more efficient  for performing similarity computations. [sent-20, score-0.078]
</p><p>10 The resulting compact binary representation radically reduces memory requirements while also permitting fast sub-linear time retrieval of approximate nearest neighbours. [sent-21, score-0.588]
</p><p>11 Hashing-based ANN techniques generally comprise two main steps: a projection stage followed by a quantisation stage. [sent-22, score-0.693]
</p><p>12 The projection stage performs a neighbourhood preserving embedding, mapping the input data into a lower-dimensional representation. [sent-23, score-0.334]
</p><p>13 The quantisation stage subsequently reduces the cardinality of this representation by converting the real-valued projections to binary. [sent-24, score-0.777]
</p><p>14 Quantisation is a lossy transformation which can have a significant impact on the resulting quality of the binary encoding. [sent-25, score-0.085]
</p><p>15 Previous work has quantised each projected dimension into a uniform number of bits (Indyk and Motwani, 1998) (Kong and Li, 2012) (Kong et al. [sent-26, score-0.37]
</p><p>16 We demonstrate that uniform allocation of bits is sub-optimal and propose a data-driven scheme for variable bit allocation. [sent-29, score-0.728]
</p><p>17 Our approach is distinct from previous work  in that it provides a general objective function for bit allocation. [sent-30, score-0.235]
</p><p>18 VBQ makes no assumptions on the data and, in addition to LSH, it applies to a broad range of other projection functions. [sent-31, score-0.178]
</p><p>19 2  Related Work  Locality sensitive hashing (LSH) (Indyk and Motwani, 1998) is an example of an approximate nearest neighbour search technique that has been widely used within the field ofNLP to preserve the Cosine distances between documents (Charikar, 2002). [sent-32, score-0.737]
</p><p>20 LSH for cosine distance draws a large number of random hyperplanes within the input feature space, effectively dividing the space into non-overlapping regions (or buckets). [sent-33, score-0.475]
</p><p>21 Each hyperplane contributes one bit to the encoding, the value (0 or 1) of which is determined by comput753  Proce dingSsof oifa, th Beu 5l1gsarti Aan,An u aglu Mste 4e-ti9n2g 0 o1f3 t. [sent-34, score-0.399]
</p><p>22 Figure 1: Left: Data points with identical shapes are 1-NN. [sent-192, score-0.105]
</p><p>23 Two hyperplanes h1, h2 are shown alongside their associated normal vectors (n1, n2). [sent-193, score-0.509]
</p><p>24 Right top: Projection of points onto the normal vectors n1 and n2 of the hyperplanes (arrows denote projections). [sent-194, score-0.548]
</p><p>25 Right middle: Positioning of the points along normal vector n2. [sent-195, score-0.162]
</p><p>26 Three quantisation thresholds (t1, t2, t3, and consequently 2 bits) can maintain the neighbourhood structure. [sent-196, score-0.657]
</p><p>27 Right bottom: the high degree of mixing between the 1-NN means that this hyperplane (h1) is likely to have 0 bits assigned (and therefore be discarded entirely). [sent-197, score-0.479]
</p><p>28 ing the dot product of a data-point (x) with the normal vector to the hyperplane (ni): that is, if x. [sent-198, score-0.273]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('quantisation', 0.463), ('hyperplanes', 0.331), ('lsh', 0.293), ('bits', 0.278), ('bit', 0.235), ('nearest', 0.222), ('vbq', 0.199), ('neighbour', 0.185), ('neighbours', 0.152), ('hyperplane', 0.134), ('indyk', 0.132), ('neighbourhood', 0.132), ('edinburgh', 0.121), ('motwani', 0.117), ('hashing', 0.108), ('normal', 0.103), ('projection', 0.1), ('sean', 0.097), ('approximate', 0.09), ('informatics', 0.076), ('inf', 0.074), ('projections', 0.074), ('stage', 0.067), ('uniform', 0.063), ('school', 0.062), ('variable', 0.062), ('uk', 0.061), ('points', 0.059), ('differentiated', 0.059), ('lossy', 0.059), ('charikar', 0.059), ('kong', 0.058), ('gouws', 0.054), ('neglects', 0.054), ('moran', 0.053), ('broad', 0.052), ('allocation', 0.052), ('permitting', 0.051), ('ofnlp', 0.051), ('positioning', 0.051), ('datadriven', 0.051), ('dubbed', 0.051), ('buckets', 0.051), ('compact', 0.051), ('optimally', 0.048), ('radically', 0.048), ('shapes', 0.046), ('alongside', 0.045), ('ravichandran', 0.045), ('lavrenko', 0.045), ('reduces', 0.044), ('locality', 0.043), ('cardinality', 0.041), ('ann', 0.038), ('cosine', 0.038), ('scheme', 0.038), ('arrows', 0.038), ('prominence', 0.038), ('comprise', 0.037), ('victor', 0.036), ('mixing', 0.036), ('dot', 0.036), ('draws', 0.036), ('retrieving', 0.036), ('dividing', 0.035), ('preserving', 0.035), ('osborne', 0.035), ('regions', 0.035), ('embedding', 0.035), ('ac', 0.035), ('preserve', 0.034), ('field', 0.034), ('right', 0.034), ('exact', 0.034), ('search', 0.033), ('miles', 0.033), ('image', 0.033), ('returning', 0.032), ('gained', 0.032), ('maintain', 0.032), ('distances', 0.031), ('converting', 0.031), ('discarded', 0.031), ('contributes', 0.03), ('subsequently', 0.03), ('thresholds', 0.03), ('vectors', 0.03), ('requirements', 0.03), ('ni', 0.029), ('projected', 0.029), ('stephan', 0.028), ('entirely', 0.028), ('representation', 0.027), ('techniques', 0.026), ('transformation', 0.026), ('assumptions', 0.026), ('fraction', 0.026), ('possibility', 0.025), ('onto', 0.025), ('retrieval', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="381-tfidf-1" href="./acl-2013-Variable_Bit_Quantisation_for_LSH.html">381 acl-2013-Variable Bit Quantisation for LSH</a></p>
<p>Author: Sean Moran ; Victor Lavrenko ; Miles Osborne</p><p>Abstract: We introduce a scheme for optimally allocating a variable number of bits per LSH hyperplane. Previous approaches assign a constant number of bits per hyperplane. This neglects the fact that a subset of hyperplanes may be more informative than others. Our method, dubbed Variable Bit Quantisation (VBQ), provides a datadriven non-uniform bit allocation across hyperplanes. Despite only using a fraction of the available hyperplanes, VBQ outperforms uniform quantisation by up to 168% for retrieval across standard text and image datasets.</p><p>2 0.084064752 <a title="381-tfidf-2" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<p>Author: Mo Yu ; Tiejun Zhao ; Yalong Bai ; Hao Tian ; Dianhai Yu</p><p>Abstract: Cross-lingual projection methods can benefit from resource-rich languages to improve performances of NLP tasks in resources-scarce languages. However, these methods confronted the difficulty of syntactic differences between languages especially when the pair of languages varies greatly. To make the projection method well-generalize to diverse languages pairs, we enhance the projection method based on word alignments by introducing target-language word representations as features and proposing a novel noise removing method based on these word representations. Experiments showed that our methods improve the performances greatly on projections between English and Chinese.</p><p>3 0.079243779 <a title="381-tfidf-3" href="./acl-2013-Improving_Text_Simplification_Language_Modeling_Using_Unsimplified_Text_Data.html">194 acl-2013-Improving Text Simplification Language Modeling Using Unsimplified Text Data</a></p>
<p>Author: David Kauchak</p><p>Abstract: In this paper we examine language modeling for text simplification. Unlike some text-to-text translation tasks, text simplification is a monolingual translation task allowing for text in both the input and output domain to be used for training the language model. We explore the relationship between normal English and simplified English and compare language models trained on varying amounts of text from each. We evaluate the models intrinsically with perplexity and extrinsically on the lexical simplification task from SemEval 2012. We find that a combined model using both simplified and normal English data achieves a 23% improvement in perplexity and a 24% improvement on the lexical simplification task over a model trained only on simple data. Post-hoc analysis shows that the additional unsimplified data provides better coverage for unseen and rare n-grams.</p><p>4 0.074587114 <a title="381-tfidf-4" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>Author: Kai Liu ; Yajuan Lu ; Wenbin Jiang ; Qun Liu</p><p>Abstract: This paper describes a novel strategy for automatic induction of a monolingual dependency grammar under the guidance of bilingually-projected dependency. By moderately leveraging the dependency information projected from the parsed counterpart language, and simultaneously mining the underlying syntactic structure of the language considered, it effectively integrates the advantages of bilingual projection and unsupervised induction, so as to induce a monolingual grammar much better than previous models only using bilingual projection or unsupervised induction. We induced dependency gram- mar for five different languages under the guidance of dependency information projected from the parsed English translation, experiments show that the bilinguallyguided method achieves a significant improvement of 28.5% over the unsupervised baseline and 3.0% over the best projection baseline on average.</p><p>5 0.06588982 <a title="381-tfidf-5" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>Author: Sujith Ravi</p><p>Abstract: In this paper, we propose a new Bayesian inference method to train statistical machine translation systems using only nonparallel corpora. Following a probabilistic decipherment approach, we first introduce a new framework for decipherment training that is flexible enough to incorporate any number/type of features (besides simple bag-of-words) as side-information used for estimating translation models. In order to perform fast, efficient Bayesian inference in this framework, we then derive a hash sampling strategy that is inspired by the work of Ahmed et al. (2012). The new translation hash sampler enables us to scale elegantly to complex models (for the first time) and large vocab- ulary/corpora sizes. We show empirical results on the OPUS data—our method yields the best BLEU scores compared to existing approaches, while achieving significant computational speedups (several orders faster). We also report for the first time—BLEU score results for a largescale MT task using only non-parallel data (EMEA corpus).</p><p>6 0.055755161 <a title="381-tfidf-6" href="./acl-2013-Enhanced_and_Portable_Dependency_Projection_Algorithms_Using_Interlinear_Glossed_Text.html">136 acl-2013-Enhanced and Portable Dependency Projection Algorithms Using Interlinear Glossed Text</a></p>
<p>7 0.039654322 <a title="381-tfidf-7" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<p>8 0.037883945 <a title="381-tfidf-8" href="./acl-2013-Categorization_of_Turkish_News_Documents_with_Morphological_Analysis.html">78 acl-2013-Categorization of Turkish News Documents with Morphological Analysis</a></p>
<p>9 0.033063855 <a title="381-tfidf-9" href="./acl-2013-Broadcast_News_Story_Segmentation_Using_Manifold_Learning_on_Latent_Topic_Distributions.html">73 acl-2013-Broadcast News Story Segmentation Using Manifold Learning on Latent Topic Distributions</a></p>
<p>10 0.029760618 <a title="381-tfidf-10" href="./acl-2013-Learning_Entity_Representation_for_Entity_Disambiguation.html">219 acl-2013-Learning Entity Representation for Entity Disambiguation</a></p>
<p>11 0.028035205 <a title="381-tfidf-11" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>12 0.027782856 <a title="381-tfidf-12" href="./acl-2013-DISSECT_-_DIStributional_SEmantics_Composition_Toolkit.html">103 acl-2013-DISSECT - DIStributional SEmantics Composition Toolkit</a></p>
<p>13 0.027475847 <a title="381-tfidf-13" href="./acl-2013-Supervised_Model_Learning_with_Feature_Grouping_based_on_a_Discrete_Constraint.html">334 acl-2013-Supervised Model Learning with Feature Grouping based on a Discrete Constraint</a></p>
<p>14 0.027112383 <a title="381-tfidf-14" href="./acl-2013-Scalable_Modified_Kneser-Ney_Language_Model_Estimation.html">308 acl-2013-Scalable Modified Kneser-Ney Language Model Estimation</a></p>
<p>15 0.0268966 <a title="381-tfidf-15" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>16 0.026862729 <a title="381-tfidf-16" href="./acl-2013-Variational_Inference_for_Structured_NLP_Models.html">382 acl-2013-Variational Inference for Structured NLP Models</a></p>
<p>17 0.026739113 <a title="381-tfidf-17" href="./acl-2013-Re-embedding_words.html">294 acl-2013-Re-embedding words</a></p>
<p>18 0.026143353 <a title="381-tfidf-18" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>19 0.025893649 <a title="381-tfidf-19" href="./acl-2013-Are_Semantically_Coherent_Topic_Models_Useful_for_Ad_Hoc_Information_Retrieval%3F.html">55 acl-2013-Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?</a></p>
<p>20 0.025827983 <a title="381-tfidf-20" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.078), (1, -0.01), (2, 0.011), (3, -0.015), (4, -0.01), (5, -0.028), (6, 0.004), (7, -0.004), (8, -0.021), (9, -0.027), (10, -0.004), (11, -0.059), (12, 0.023), (13, 0.037), (14, -0.03), (15, 0.001), (16, -0.019), (17, -0.01), (18, 0.027), (19, 0.004), (20, 0.042), (21, 0.039), (22, -0.02), (23, -0.013), (24, -0.028), (25, -0.005), (26, 0.061), (27, 0.015), (28, -0.055), (29, 0.009), (30, -0.012), (31, -0.01), (32, 0.017), (33, 0.03), (34, 0.028), (35, -0.029), (36, 0.062), (37, 0.033), (38, 0.016), (39, 0.017), (40, -0.024), (41, -0.014), (42, -0.001), (43, 0.07), (44, -0.044), (45, 0.074), (46, -0.029), (47, -0.001), (48, -0.087), (49, 0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92255384 <a title="381-lsi-1" href="./acl-2013-Variable_Bit_Quantisation_for_LSH.html">381 acl-2013-Variable Bit Quantisation for LSH</a></p>
<p>Author: Sean Moran ; Victor Lavrenko ; Miles Osborne</p><p>Abstract: We introduce a scheme for optimally allocating a variable number of bits per LSH hyperplane. Previous approaches assign a constant number of bits per hyperplane. This neglects the fact that a subset of hyperplanes may be more informative than others. Our method, dubbed Variable Bit Quantisation (VBQ), provides a datadriven non-uniform bit allocation across hyperplanes. Despite only using a fraction of the available hyperplanes, VBQ outperforms uniform quantisation by up to 168% for retrieval across standard text and image datasets.</p><p>2 0.6313979 <a title="381-lsi-2" href="./acl-2013-Enhanced_and_Portable_Dependency_Projection_Algorithms_Using_Interlinear_Glossed_Text.html">136 acl-2013-Enhanced and Portable Dependency Projection Algorithms Using Interlinear Glossed Text</a></p>
<p>Author: Ryan Georgi ; Fei Xia ; William D. Lewis</p><p>Abstract: As most of the world’s languages are under-resourced, projection algorithms offer an enticing way to bootstrap the resources available for one resourcepoor language from a resource-rich language by means of parallel text and word alignment. These algorithms, however, make the strong assumption that the language pairs share common structures and that the parse trees will resemble one another. This assumption is useful but often leads to errors in projection. In this paper, we will address this weakness by using trees created from instances of Interlinear Glossed Text (IGT) to discover patterns of divergence between the lan- guages. We will show that this method improves the performance of projection algorithms significantly in some languages by accounting for divergence between languages using only the partial supervision of a few corrected trees.</p><p>3 0.62129492 <a title="381-lsi-3" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>Author: Kai Liu ; Yajuan Lu ; Wenbin Jiang ; Qun Liu</p><p>Abstract: This paper describes a novel strategy for automatic induction of a monolingual dependency grammar under the guidance of bilingually-projected dependency. By moderately leveraging the dependency information projected from the parsed counterpart language, and simultaneously mining the underlying syntactic structure of the language considered, it effectively integrates the advantages of bilingual projection and unsupervised induction, so as to induce a monolingual grammar much better than previous models only using bilingual projection or unsupervised induction. We induced dependency gram- mar for five different languages under the guidance of dependency information projected from the parsed English translation, experiments show that the bilinguallyguided method achieves a significant improvement of 28.5% over the unsupervised baseline and 3.0% over the best projection baseline on average.</p><p>4 0.58838701 <a title="381-lsi-4" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<p>Author: Mo Yu ; Tiejun Zhao ; Yalong Bai ; Hao Tian ; Dianhai Yu</p><p>Abstract: Cross-lingual projection methods can benefit from resource-rich languages to improve performances of NLP tasks in resources-scarce languages. However, these methods confronted the difficulty of syntactic differences between languages especially when the pair of languages varies greatly. To make the projection method well-generalize to diverse languages pairs, we enhance the projection method based on word alignments by introducing target-language word representations as features and proposing a novel noise removing method based on these word representations. Experiments showed that our methods improve the performances greatly on projections between English and Chinese.</p><p>5 0.54795957 <a title="381-lsi-5" href="./acl-2013-Improving_Text_Simplification_Language_Modeling_Using_Unsimplified_Text_Data.html">194 acl-2013-Improving Text Simplification Language Modeling Using Unsimplified Text Data</a></p>
<p>Author: David Kauchak</p><p>Abstract: In this paper we examine language modeling for text simplification. Unlike some text-to-text translation tasks, text simplification is a monolingual translation task allowing for text in both the input and output domain to be used for training the language model. We explore the relationship between normal English and simplified English and compare language models trained on varying amounts of text from each. We evaluate the models intrinsically with perplexity and extrinsically on the lexical simplification task from SemEval 2012. We find that a combined model using both simplified and normal English data achieves a 23% improvement in perplexity and a 24% improvement on the lexical simplification task over a model trained only on simple data. Post-hoc analysis shows that the additional unsimplified data provides better coverage for unseen and rare n-grams.</p><p>6 0.50687033 <a title="381-lsi-6" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<p>7 0.49265262 <a title="381-lsi-7" href="./acl-2013-Simple%2C_readable_sub-sentences.html">322 acl-2013-Simple, readable sub-sentences</a></p>
<p>8 0.4625735 <a title="381-lsi-8" href="./acl-2013-Does_Korean_defeat_phonotactic_word_segmentation%3F.html">128 acl-2013-Does Korean defeat phonotactic word segmentation?</a></p>
<p>9 0.44657603 <a title="381-lsi-9" href="./acl-2013-A_Comparison_of_Techniques_to_Automatically_Identify_Complex_Words..html">3 acl-2013-A Comparison of Techniques to Automatically Identify Complex Words.</a></p>
<p>10 0.44596916 <a title="381-lsi-10" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>11 0.42620748 <a title="381-lsi-11" href="./acl-2013-The_mathematics_of_language_learning.html">349 acl-2013-The mathematics of language learning</a></p>
<p>12 0.405132 <a title="381-lsi-12" href="./acl-2013-Unsupervised_joke_generation_from_big_data.html">371 acl-2013-Unsupervised joke generation from big data</a></p>
<p>13 0.39813873 <a title="381-lsi-13" href="./acl-2013-Broadcast_News_Story_Segmentation_Using_Manifold_Learning_on_Latent_Topic_Distributions.html">73 acl-2013-Broadcast News Story Segmentation Using Manifold Learning on Latent Topic Distributions</a></p>
<p>14 0.39576453 <a title="381-lsi-14" href="./acl-2013-Bridging_Languages_through_Etymology%3A_The_case_of_cross_language_text_categorization.html">72 acl-2013-Bridging Languages through Etymology: The case of cross language text categorization</a></p>
<p>15 0.39513978 <a title="381-lsi-15" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>16 0.39182079 <a title="381-lsi-16" href="./acl-2013-A_corpus-based_evaluation_method_for_Distributional_Semantic_Models.html">31 acl-2013-A corpus-based evaluation method for Distributional Semantic Models</a></p>
<p>17 0.39033359 <a title="381-lsi-17" href="./acl-2013-Nonconvex_Global_Optimization_for_Latent-Variable_Models.html">260 acl-2013-Nonconvex Global Optimization for Latent-Variable Models</a></p>
<p>18 0.37430909 <a title="381-lsi-18" href="./acl-2013-Scalable_Modified_Kneser-Ney_Language_Model_Estimation.html">308 acl-2013-Scalable Modified Kneser-Ney Language Model Estimation</a></p>
<p>19 0.37229028 <a title="381-lsi-19" href="./acl-2013-Models_of_Semantic_Representation_with_Visual_Attributes.html">249 acl-2013-Models of Semantic Representation with Visual Attributes</a></p>
<p>20 0.36929381 <a title="381-lsi-20" href="./acl-2013-Tag2Blog%3A_Narrative_Generation_from_Satellite_Tag_Data.html">337 acl-2013-Tag2Blog: Narrative Generation from Satellite Tag Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.033), (6, 0.039), (11, 0.059), (24, 0.057), (26, 0.024), (35, 0.104), (36, 0.397), (42, 0.047), (48, 0.028), (70, 0.025), (88, 0.031), (90, 0.028), (95, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80299717 <a title="381-lda-1" href="./acl-2013-Variable_Bit_Quantisation_for_LSH.html">381 acl-2013-Variable Bit Quantisation for LSH</a></p>
<p>Author: Sean Moran ; Victor Lavrenko ; Miles Osborne</p><p>Abstract: We introduce a scheme for optimally allocating a variable number of bits per LSH hyperplane. Previous approaches assign a constant number of bits per hyperplane. This neglects the fact that a subset of hyperplanes may be more informative than others. Our method, dubbed Variable Bit Quantisation (VBQ), provides a datadriven non-uniform bit allocation across hyperplanes. Despite only using a fraction of the available hyperplanes, VBQ outperforms uniform quantisation by up to 168% for retrieval across standard text and image datasets.</p><p>2 0.57037342 <a title="381-lda-2" href="./acl-2013-Learning_to_Extract_International_Relations_from_Political_Context.html">224 acl-2013-Learning to Extract International Relations from Political Context</a></p>
<p>Author: Brendan O'Connor ; Brandon M. Stewart ; Noah A. Smith</p><p>Abstract: We describe a new probabilistic model for extracting events between major political actors from news corpora. Our unsupervised model brings together familiar components in natural language processing (like parsers and topic models) with contextual political information— temporal and dyad dependence—to infer latent event classes. We quantitatively evaluate the model’s performance on political science benchmarks: recovering expert-assigned event class valences, and detecting real-world conflict. We also conduct a small case study based on our model’s inferences. A supplementary appendix, and replication software/data are available online, at: http://brenocon.com/irevents</p><p>3 0.52701759 <a title="381-lda-3" href="./acl-2013-ICARUS_-_An_Extensible_Graphical_Search_Tool_for_Dependency_Treebanks.html">183 acl-2013-ICARUS - An Extensible Graphical Search Tool for Dependency Treebanks</a></p>
<p>Author: Markus Gartner ; Gregor Thiele ; Wolfgang Seeker ; Anders Bjorkelund ; Jonas Kuhn</p><p>Abstract: We present ICARUS, a versatile graphical search tool to query dependency treebanks. Search results can be inspected both quantitatively and qualitatively by means of frequency lists, tables, or dependency graphs. ICARUS also ships with plugins that enable it to interface with tool chains running either locally or remotely.</p><p>4 0.35713464 <a title="381-lda-4" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>Author: Joanne Boisson ; Ting-Hui Kao ; Jian-Cheng Wu ; Tzu-Hsi Yen ; Jason S. Chang</p><p>Abstract: In this paper, we introduce a Web-scale linguistics search engine, Linggle, that retrieves lexical bundles in response to a given query. The query might contain keywords, wildcards, wild parts of speech (PoS), synonyms, and additional regular expression (RE) operators. In our approach, we incorporate inverted file indexing, PoS information from BNC, and semantic indexing based on Latent Dirichlet Allocation with Google Web 1T. The method involves parsing the query to transforming it into several keyword retrieval commands. Word chunks are retrieved with counts, further filtering the chunks with the query as a RE, and finally displaying the results according to the counts, similarities, and topics. Clusters of synonyms or conceptually related words are also provided. In addition, Linggle provides example sentences from The New York Times on demand. The current implementation of Linggle is the most functionally comprehensive, and is in principle language and dataset independent. We plan to extend Linggle to provide fast and convenient access to a wealth of linguistic information embodied in Web scale datasets including Google Web 1T and Google Books Ngram for many major languages in the world. 1</p><p>5 0.35652244 <a title="381-lda-5" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>Author: Olivier Ferret</p><p>Abstract: Distributional thesauri are now widely used in a large number of Natural Language Processing tasks. However, they are far from containing only interesting semantic relations. As a consequence, improving such thesaurus is an important issue that is mainly tackled indirectly through the improvement of semantic similarity measures. In this article, we propose a more direct approach focusing on the identification of the neighbors of a thesaurus entry that are not semantically linked to this entry. This identification relies on a discriminative classifier trained from unsupervised selected examples for building a distributional model of the entry in texts. Its bad neighbors are found by applying this classifier to a representative set of occurrences of each of these neighbors. We evaluate the interest of this method for a large set of English nouns with various frequencies.</p><p>6 0.35487437 <a title="381-lda-6" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>7 0.35480425 <a title="381-lda-7" href="./acl-2013-Measuring_semantic_content_in_distributional_vectors.html">238 acl-2013-Measuring semantic content in distributional vectors</a></p>
<p>8 0.35459256 <a title="381-lda-8" href="./acl-2013-Feature-Based_Selection_of_Dependency_Paths_in_Ad_Hoc_Information_Retrieval.html">158 acl-2013-Feature-Based Selection of Dependency Paths in Ad Hoc Information Retrieval</a></p>
<p>9 0.3532401 <a title="381-lda-9" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>10 0.35223046 <a title="381-lda-10" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>11 0.35088554 <a title="381-lda-11" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>12 0.35088515 <a title="381-lda-12" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>13 0.35087246 <a title="381-lda-13" href="./acl-2013-Language_Acquisition_and_Probabilistic_Models%3A_keeping_it_simple.html">213 acl-2013-Language Acquisition and Probabilistic Models: keeping it simple</a></p>
<p>14 0.35052252 <a title="381-lda-14" href="./acl-2013-Crowd_Prefers_the_Middle_Path%3A_A_New_IAA_Metric_for_Crowdsourcing_Reveals_Turker_Biases_in_Query_Segmentation.html">99 acl-2013-Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation</a></p>
<p>15 0.34991252 <a title="381-lda-15" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>16 0.34976262 <a title="381-lda-16" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>17 0.34896106 <a title="381-lda-17" href="./acl-2013-Automated_Collocation_Suggestion_for_Japanese_Second_Language_Learners.html">58 acl-2013-Automated Collocation Suggestion for Japanese Second Language Learners</a></p>
<p>18 0.34790814 <a title="381-lda-18" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>19 0.34725764 <a title="381-lda-19" href="./acl-2013-A_Context_Free_TAG_Variant.html">4 acl-2013-A Context Free TAG Variant</a></p>
<p>20 0.34712636 <a title="381-lda-20" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
