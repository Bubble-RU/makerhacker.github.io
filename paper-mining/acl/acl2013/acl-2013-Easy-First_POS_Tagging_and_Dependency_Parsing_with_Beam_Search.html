<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-132" href="#">acl2013-132</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</h1>
<br/><p>Source: <a title="acl-2013-132-pdf" href="http://aclweb.org/anthology//P/P13/P13-2020.pdf">pdf</a></p><p>Author: Ji Ma ; Jingbo Zhu ; Tong Xiao ; Nan Yang</p><p>Abstract: In this paper, we combine easy-first dependency parsing and POS tagging algorithms with beam search and structured perceptron. We propose a simple variant of “early-update” to ensure valid update in the training process. The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity. On CTB, we achieve 94.01% tagging accuracy and 86.33% unlabeled attachment score with a relatively small beam width. On PTB, we also achieve state-of-the-art performance. 1</p><p>Reference: <a title="acl-2013-132-reference" href="../acl2013_reference/acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 †  Abstract In this paper, we combine easy-first dependency parsing and POS tagging algorithms with beam search and structured perceptron. [sent-8, score-1.185]
</p><p>2 We propose a simple variant of “early-update” to ensure valid update in the training process. [sent-9, score-0.281]
</p><p>3 The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity. [sent-10, score-1.069]
</p><p>4 33% unlabeled attachment score with a relatively small beam width. [sent-13, score-0.618]
</p><p>5 1  Introduction  The easy-first dependency parsing algorithm (Goldberg and Elhadad, 2010) is attractive due to its good accuracy, fast speed and simplicity. [sent-15, score-0.418]
</p><p>6 The easy-first parser has been applied to many applications (Seeker et al. [sent-16, score-0.026]
</p><p>7 By processing the input tokens in an easyto-hard order, the algorithm could make use of structured information on both sides of the hard token thus making more indicative predictions. [sent-18, score-0.074]
</p><p>8 However, rich structured information also causes exhaustive inference intractable. [sent-19, score-0.032]
</p><p>9 As an alternative, greedy search which only explores a tiny fraction of the search space is adopted (Goldberg and Elhadad, 2010). [sent-20, score-0.359]
</p><p>10 To enlarge the search space, a natural extension to greedy search is beam search. [sent-21, score-0.898]
</p><p>11 Recent work also shows that beam search together with perceptron-based global learning (Collins, 2002) enable the use of non-local features that are helpful to improve parsing performance without overfitting (Zhang and Nivre, 2012). [sent-22, score-0.865]
</p><p>12 Due to these advantages, beam search and global learning has been applied to many NLP tasks (Collins and  ×  Figure 1: Example of cases without/with spurious ambiguity. [sent-23, score-0.92]
</p><p>13 However, to the best of our knowledge, no work in the literature has ever applied the two techniques to easy-first dependency parsing. [sent-28, score-0.187]
</p><p>14 While applying beam-search is relatively straightforward, the main difficulty comes from combining easy-first dependency parsing with  perceptron-based global learning. [sent-29, score-0.399]
</p><p>15 In particular, one needs to guarantee that each parameter update is valid, i. [sent-30, score-0.2]
</p><p>16 , the correct action sequence has lower model score than the predicted The difficulty in ensuring validity of parameter update for the easy-first algorithm is caused by its spurious ambiguity, i. [sent-32, score-0.965]
</p><p>17 , the same result might be derived by more than one action sequences. [sent-34, score-0.309]
</p><p>18 For algorithms which do not exhibit spurious ambiguity, “early update” (Collins and Roark 2004) is always valid: at the k-th step when the single correct action sequence falls off the beam,  one1. [sent-35, score-0.769]
</p><p>19 , 2012), only valid update guarantees the convergence of any perceptron-based training. [sent-37, score-0.282]
</p><p>20 Invalid update may lead to bad learning or even make the learning not converge at all. [sent-38, score-0.17]
</p><p>21 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 1 0–1 4,  Figure 2: An example of parsing “I am valid”. [sent-41, score-0.153]
</p><p>22 its model score must be lower than those still in the beam (as illustrated in figure 1, also see the proof in (Huang et al. [sent-43, score-0.588]
</p><p>23 While for easyfirst dependency parsing, there could be multiple action sequences that yield the gold result (C1 and C2 in figure 1). [sent-45, score-0.564]
</p><p>24 When all correct sequences fall off the beam, some may indeed have higher model score than those still in the beam (C2 in figure 1), causing invalid update. [sent-46, score-0.763]
</p><p>25 For the purpose of valid update, we present a simple solution which is based on early update. [sent-47, score-0.157]
</p><p>26 The basic idea is to use one of the correct action sequences that were pruned right at the k-th step (C1 in figure 1) for parameter update. [sent-48, score-0.535]
</p><p>27 The proposed solution is general and can also be applied to other algorithms that exhibit spurious ambiguity, such as easy-first POS tagging (Ma et al. [sent-49, score-0.476]
</p><p>28 , 2012) and transition-based dependen-  cy parsing with dynamic oracle (Goldberg and Nivre, 2012). [sent-50, score-0.22]
</p><p>29 In this paper, we report experimental results on both easy-first dependency parsing and POS tagging (Ma et al. [sent-51, score-0.5]
</p><p>30 We show that both easy-first POS tagging and dependency parsing can be improved significantly from beam search and global learning. [sent-53, score-1.212]
</p><p>31 01% tagging accuracy which is the best result to date2 for a single tagging model. [sent-55, score-0.361]
</p><p>32 33% unlabeled score (assume gold tags), better than state-of-the-art transition-based parsers (Huang and Sagae, 2010; Zhang and Nivre, 2011). [sent-57, score-0.087]
</p><p>33 On PTB, we also achieve good results that are comparable to the state-of-the-art. [sent-58, score-0.039]
</p><p>34 2  Easy-first dependency parsing  The easy-first dependency parsing algorithm (Goldberg and Elhadad, 2010) builds a dependency tree by performing two types of actions LEFT(i) and RIGHT(i) to a list of sub-tree structures p1,… pr. [sent-59, score-0.976]
</p><p>35 pi is initialized with the i-th word …  …,  2 Joint tagging-parsing models achieve higher accuracy, but those models are not directly comparable to ours. [sent-60, score-0.259]
</p><p>36 Action LEFT(i)/RIGHT(i) attaches pi to its left/right neighbor and then removes pi from the sub-tree list. [sent-62, score-0.469]
</p><p>37 The algorithm proceeds until only one sub-tree left which is the dependency tree of the input sentence (see the example in figure 2). [sent-63, score-0.306]
</p><p>38 Each step, the algorithm chooses the highest score action to perform according to the linear model: ( ) ( ) Here, is the weight vector and is the feature representation. [sent-64, score-0.376]
</p><p>39 In particular, ( ( ) ( )) denotes features extracted from pi. [sent-65, score-0.082]
</p><p>40 The parsing algorithm is greedy which explores a tiny fraction of the search space. [sent-66, score-0.464]
</p><p>41 Once an incorrect action is selected, it can never yield the correct dependency tree. [sent-67, score-0.561]
</p><p>42 To enlarge the search space, we introduce the beam-search extension in the next section. [sent-68, score-0.165]
</p><p>43 3  Easy-first with beam search  In this section, we introduce easy-first with beam search in our own notations that will be used throughout the rest of this paper. [sent-69, score-1.306]
</p><p>44 For a sentence x of n words, let be the action (sub-)sequence that can be applied, in sequence, to x and the result sub-tree list is denoted by ( ) For example, suppose x is “I am valid” and y is [RIGHT(1)], then y(x) yields figure 2(b). [sent-70, score-0.309]
</p><p>45 Let to be LEFT(i)/RIGHT(i) actions where 1 Thus, the set of all possible one-action extension of is: ( ) ( )  . [sent-71, score-0.063]
</p><p>46 , 2012), in order to formalize beam search, we also use the ( ) operation which returns the top s action sequenc-  es in according to ( ). [sent-74, score-0.872]
</p><p>47 Here, denotes a set of action sequences, ( ) denotes the sum of feature vectors of each action in Pseudo-code of easy-first with beam search is shown in algorithm 1. [sent-75, score-1.477]
</p><p>48 falls off the beam 6 ( ̂) ( ) 7 break 8 if () // full update 9 ( ̂) ( ) 10 return  ,  beam (sequences in are sorted in terms of model score, i. [sent-77, score-1.339]
</p><p>49 At each step, the sequences in are expanded in all possible ways and then is filled up with the top s newly expanded sequences (line 2 ~ line 3). [sent-80, score-0.178]
</p><p>50 Finally, it returns the dependency tree built by the top action sequence in 4  Training  . [sent-81, score-0.598]
</p><p>51 ,  To learn the weight vector we use the perceptron-based global learning3 (Collins, 2002) which updates by rewarding the feature weights fired in the correct action sequence and punish those fired in the predicted incorrect action sequence. [sent-82, score-0.9]
</p><p>52 , 2012) rigorously explained that only valid update ensures convergence of any perceptron variants. [sent-84, score-0.376]
</p><p>53 They also justified that the popular “early update” (Collins and Roark, 2004) is valid for the systems that do not exhibit spurious ambiguity4. [sent-85, score-0.373]
</p><p>54 However, for the easy-first algorithm or more generally, systems that exhibit spurious ambiguity, even “early update” could fail to ensure validity of update (see the example in figure 1). [sent-86, score-0.552]
</p><p>55 For  validity of update, we propose a simple solution which is based on “early update” and which can accommodate spurious ambiguity. [sent-87, score-0.288]
</p><p>56 The basic idea is to use the correct action sequence which was  3 Following  (Zhang and Nivre, 2012), we say the training algorithm is global if it optimizes the score of an entire action sequence. [sent-88, score-0.899]
</p><p>57 A local learner trains a classifier which distinguishes between single actions. [sent-89, score-0.035]
</p><p>58 As shown in (Goldberg and Nivre 2012), most transitionbased dependency parsers (Nivre et al. [sent-90, score-0.28]
</p><p>59 , 2003; Huang and Sagae 2010;Zhang and Clark 2008) ignores spurious ambiguity by using a static oracle which maps a dependency tree to a single action sequence. [sent-91, score-0.848]
</p><p>60 wp denotes the head word of p, tp denotes the  POS tag of wp. [sent-93, score-0.164]
</p><p>61 pruned right at the step when all correct sequence falls off the beam (as C1 in figure 1). [sent-97, score-0.798]
</p><p>62 Algorithm 2 shows the pseudo-code of the training procedure over one training sample (), a sentence-tree pair. [sent-98, score-0.052]
</p><p>63 Here we assume to be the set of all correct action sequences/subsequences. [sent-99, score-0.374]
</p><p>64 At step k, the algorithm constructs a correct action sequence ̂ of length k by extending those in (line 3). [sent-100, score-0.48]
</p><p>65 It also checks whether no longer contains any correct sequence. [sent-101, score-0.065]
</p><p>66 If so, ̂ together with are used for parameter update (line 5 ~ line 6). [sent-102, score-0.242]
</p><p>67 It can be easily verified that each update in line 6 is valid. [sent-103, score-0.212]
</p><p>68 Note that both ‘TOPC’ and the operation in line 5 use to check whether an action sequence y is correct or not. [sent-104, score-0.48]
</p><p>69 This can be efficiently implemented (without explicitly enumerating ) by checking if each LEFT(i)/RIGHT(i) in y are compatible with (): pi already collected all its dependents according to t; pi is attached to the correct neighbor suggested by t. [sent-105, score-0.534]
</p><p>70 We use the standard split for dependency parsing and the split used by (Ratnaparkhi, 1996) for POS tagging. [sent-107, score-0.34]
</p><p>71 For dependency parsing, POS tags of the training set are generated using 10-fold jack-knifing. [sent-109, score-0.213]
</p><p>72 For dependency parsing, we assume gold segmentation and POS tags for the input. [sent-114, score-0.215]
</p><p>73 html 112  Features used in English dependency parsing are listed in table 1. [sent-119, score-0.385]
</p><p>74 Besides the features in (Goldberg and Elhadad, 2010), we also include some trigram features and valency features which are useful for transition-based dependency  parsing (Zhang and Nivre, 2011). [sent-120, score-0.34]
</p><p>75 For Chinese POS tagging and dependency parsing, we use the same features as (Ma et al. [sent-123, score-0.347]
</p><p>76 93GHz) machine, both the tagger and parser are implemented using C++. [sent-126, score-0.054]
</p><p>77 1  Effect of beam width  Tagging/parsing performances with different beam widths on the development set are listed in table 2 and table 3. [sent-128, score-1.375]
</p><p>78 We can see that Chinese POS tagging, dependency parsing as well as English dependency parsing greatly benefit from beam search. [sent-129, score-1.243]
</p><p>79 This may because that the accuracy of the greedy baseline tagger is already very high and it is hard to get further improvement. [sent-131, score-0.149]
</p><p>80 Table 2 and table 3 also show that the speed of both tagging and dependency parsing drops linearly with the growth of beam width. [sent-132, score-1.099]
</p><p>81 2  Final results  Tagging results on the test set together with some previous results are listed in table 4. [sent-134, score-0.045]
</p><p>82 Dependency  parsing results on CTB and PTB are listed in table 5 and table 6, respectively. [sent-135, score-0.198]
</p><p>83 On CTB, tagging accuracy of our greedy baseline is already comparable to the state-of-the-art. [sent-136, score-0.281]
</p><p>84 As the beam size grows to 5, tagging accuracy increases to 94. [sent-137, score-0.791]
</p><p>85 This is also the best tagging accuracy comparing with previous single tagging models (For limited space, we do not list the performance of joint tagging-parsing models). [sent-140, score-0.361]
</p><p>86 Parsing performances on both PTB and CTB are significantly improved with a relatively small beam width (s = 8). [sent-141, score-0.741]
</p><p>87 Moreover, the performance is better than the best transition-based parser (Zhang and Nivre, 2011) which adopts a much larger beam width (s = 64). [sent-145, score-0.725]
</p><p>88 6  Conclusion and related work  This work directly extends (Goldberg and Elhadad, 2010) with beam search and global learning. [sent-146, score-0.712]
</p><p>89 We show that both the easy-first POS tagger and dependency parser can be significantly impr-  51s3 9 P7 T. [sent-147, score-0.241]
</p><p>90 91 B157 s15p3 e685e05 d Table 2: Tagging accuracy vs beam width vs. [sent-149, score-0.767]
</p><p>91 08 1274†  Table 3: Parsing accuracy vs beam width. [sent-159, score-0.631]
</p><p>92 ‘uas’ and ‘compl’ denote unlabeled score and complete match rate respectively (all excluding punctuations). [sent-160, score-0.055]
</p><p>93 ‘†’ denotes statistically significant over the greedy baseline by McNemar’s test ( )  (HZhua(nLgitahSentiysd atNlwS. [sent-162, score-0.162]
</p><p>94 One future direction might  be to apply the training method to transitionbased parsers with dynamic oracle (Goldberg and Nivre, 2012) and potentially further advance performances of state-of-the-art transition-based parsers. [sent-176, score-0.202]
</p><p>95 , (2007) and (Shen and Joshi, 2008) also proposed bi-directional sequential classification with beam search for POS tagging and LTAG dependency parsing, respectively. [sent-178, score-1.0]
</p><p>96 The main difference is that their training method aims to learn a classifier which distinguishes between each local action while our training method aims to distinguish between action sequences. [sent-179, score-0.705]
</p><p>97 Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. [sent-186, score-0.094]
</p><p>98 Joint word segmentation and POS tagging using a single perceptron. [sent-254, score-0.188]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('beam', 0.563), ('action', 0.309), ('pi', 0.22), ('spurious', 0.208), ('goldberg', 0.199), ('dependency', 0.187), ('update', 0.17), ('tagging', 0.16), ('parsing', 0.153), ('ctb', 0.151), ('nivre', 0.142), ('width', 0.136), ('elhadad', 0.119), ('ptb', 0.108), ('pos', 0.105), ('search', 0.09), ('zhang', 0.09), ('valid', 0.085), ('denotes', 0.082), ('exhibit', 0.08), ('greedy', 0.08), ('huang', 0.075), ('collins', 0.073), ('perceptron', 0.068), ('sequences', 0.068), ('ambiguity', 0.065), ('correct', 0.065), ('ggard', 0.065), ('ltag', 0.065), ('wlcp', 0.065), ('wulff', 0.065), ('sequence', 0.064), ('transitionbased', 0.061), ('global', 0.059), ('sagae', 0.058), ('roark', 0.055), ('uas', 0.055), ('shen', 0.054), ('seeker', 0.053), ('validity', 0.052), ('ma', 0.051), ('fired', 0.047), ('chinese', 0.045), ('listed', 0.045), ('early', 0.044), ('falls', 0.043), ('line', 0.042), ('invalid', 0.042), ('tiny', 0.042), ('algorithm', 0.042), ('performances', 0.042), ('accuracy', 0.041), ('oracle', 0.041), ('enlarge', 0.041), ('clark', 0.041), ('left', 0.039), ('achieve', 0.039), ('tree', 0.038), ('china', 0.036), ('speed', 0.036), ('duan', 0.035), ('distinguishes', 0.035), ('bidirectional', 0.034), ('extension', 0.034), ('right', 0.033), ('structured', 0.032), ('joshi', 0.032), ('explores', 0.032), ('parsers', 0.032), ('pruned', 0.03), ('parameter', 0.03), ('unlabeled', 0.03), ('yue', 0.03), ('xiao', 0.029), ('neighbor', 0.029), ('actions', 0.029), ('hhiiss', 0.029), ('ingbo', 0.029), ('mcc', 0.029), ('oved', 0.029), ('xiaotong', 0.029), ('tagger', 0.028), ('solution', 0.028), ('segmentation', 0.028), ('convergence', 0.027), ('grows', 0.027), ('extensions', 0.027), ('vs', 0.027), ('cy', 0.026), ('compl', 0.026), ('fayong', 0.026), ('northeastern', 0.026), ('rigorously', 0.026), ('widths', 0.026), ('zhenghua', 0.026), ('training', 0.026), ('parser', 0.026), ('score', 0.025), ('fraction', 0.025), ('incremental', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="132-tfidf-1" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>Author: Ji Ma ; Jingbo Zhu ; Tong Xiao ; Nan Yang</p><p>Abstract: In this paper, we combine easy-first dependency parsing and POS tagging algorithms with beam search and structured perceptron. We propose a simple variant of “early-update” to ensure valid update in the training process. The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity. On CTB, we achieve 94.01% tagging accuracy and 86.33% unlabeled attachment score with a relatively small beam width. On PTB, we also achieve state-of-the-art performance. 1</p><p>2 0.42294058 <a title="132-tfidf-2" href="./acl-2013-Transition-based_Dependency_Parsing_with_Selectional_Branching.html">358 acl-2013-Transition-based Dependency Parsing with Selectional Branching</a></p>
<p>Author: Jinho D. Choi ; Andrew McCallum</p><p>Abstract: We present a novel approach, called selectional branching, which uses confidence estimates to decide when to employ a beam, providing the accuracy of beam search at speeds close to a greedy transition-based dependency parsing approach. Selectional branching is guaranteed to perform a fewer number of transitions than beam search yet performs as accurately. We also present a new transition-based dependency parsing algorithm that gives a complexity of O(n) for projective parsing and an expected linear time speed for non-projective parsing. With the standard setup, our parser shows an unlabeled attachment score of 92.96% and a parsing speed of 9 milliseconds per sentence, which is faster and more accurate than the current state-of-the-art transitionbased parser that uses beam search.</p><p>3 0.25175387 <a title="132-tfidf-3" href="./acl-2013-Chinese_Parsing_Exploiting_Characters.html">80 acl-2013-Chinese Parsing Exploiting Characters</a></p>
<p>Author: Meishan Zhang ; Yue Zhang ; Wanxiang Che ; Ting Liu</p><p>Abstract: Characters play an important role in the Chinese language, yet computational processing of Chinese has been dominated by word-based approaches, with leaves in syntax trees being words. We investigate Chinese parsing from the character-level, extending the notion of phrase-structure trees by annotating internal structures of words. We demonstrate the importance of character-level information to Chinese processing by building a joint segmentation, part-of-speech (POS) tagging and phrase-structure parsing system that integrates character-structure features. Our joint system significantly outperforms a state-of-the-art word-based baseline on the standard CTB5 test, and gives the best published results for Chinese parsing.</p><p>4 0.22996677 <a title="132-tfidf-4" href="./acl-2013-Efficient_Implementation_of_Beam-Search_Incremental_Parsers.html">133 acl-2013-Efficient Implementation of Beam-Search Incremental Parsers</a></p>
<p>Author: Yoav Goldberg ; Kai Zhao ; Liang Huang</p><p>Abstract: Beam search incremental parsers are accurate, but not as fast as they could be. We demonstrate that, contrary to popular belief, most current implementations of beam parsers in fact run in O(n2), rather than linear time, because each statetransition is actually implemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empiri- cally. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treeb∼a2nxk sentences, a bnads are eosrd oenrs P eofn magnitude faster on much longer sentences.</p><p>5 0.22976971 <a title="132-tfidf-5" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>Author: Muhua Zhu ; Yue Zhang ; Wenliang Chen ; Min Zhang ; Jingbo Zhu</p><p>Abstract: Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser.</p><p>6 0.19253148 <a title="132-tfidf-6" href="./acl-2013-Joint_Event_Extraction_via_Structured_Prediction_with_Global_Features.html">206 acl-2013-Joint Event Extraction via Structured Prediction with Global Features</a></p>
<p>7 0.17316742 <a title="132-tfidf-7" href="./acl-2013-A_Transition-Based_Dependency_Parser_Using_a_Dynamic_Parsing_Strategy.html">26 acl-2013-A Transition-Based Dependency Parser Using a Dynamic Parsing Strategy</a></p>
<p>8 0.17265865 <a title="132-tfidf-8" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>9 0.14671983 <a title="132-tfidf-9" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>10 0.14257327 <a title="132-tfidf-10" href="./acl-2013-Punctuation_Prediction_with_Transition-based_Parsing.html">288 acl-2013-Punctuation Prediction with Transition-based Parsing</a></p>
<p>11 0.13823324 <a title="132-tfidf-11" href="./acl-2013-Beam_Search_for_Solving_Substitution_Ciphers.html">66 acl-2013-Beam Search for Solving Substitution Ciphers</a></p>
<p>12 0.13538152 <a title="132-tfidf-12" href="./acl-2013-A_Lattice-based_Framework_for_Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">7 acl-2013-A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>13 0.13332123 <a title="132-tfidf-13" href="./acl-2013-Joint_Inference_for_Heterogeneous_Dependency_Parsing.html">208 acl-2013-Joint Inference for Heterogeneous Dependency Parsing</a></p>
<p>14 0.12858488 <a title="132-tfidf-14" href="./acl-2013-Survey_on_parsing_three_dependency_representations_for_English.html">335 acl-2013-Survey on parsing three dependency representations for English</a></p>
<p>15 0.12610285 <a title="132-tfidf-15" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<p>16 0.11809456 <a title="132-tfidf-16" href="./acl-2013-Discriminative_Learning_with_Natural_Annotations%3A_Word_Segmentation_as_a_Case_Study.html">123 acl-2013-Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study</a></p>
<p>17 0.10941683 <a title="132-tfidf-17" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>18 0.10938255 <a title="132-tfidf-18" href="./acl-2013-Iterative_Transformation_of_Annotation_Guidelines_for_Constituency_Parsing.html">204 acl-2013-Iterative Transformation of Annotation Guidelines for Constituency Parsing</a></p>
<p>19 0.10462227 <a title="132-tfidf-19" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>20 0.10280947 <a title="132-tfidf-20" href="./acl-2013-Universal_Dependency_Annotation_for_Multilingual_Parsing.html">368 acl-2013-Universal Dependency Annotation for Multilingual Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.203), (1, -0.17), (2, -0.314), (3, 0.061), (4, -0.036), (5, 0.001), (6, 0.107), (7, -0.018), (8, -0.021), (9, -0.082), (10, -0.016), (11, 0.056), (12, -0.09), (13, -0.029), (14, 0.26), (15, 0.03), (16, -0.195), (17, -0.106), (18, 0.036), (19, 0.037), (20, -0.059), (21, -0.013), (22, 0.03), (23, 0.013), (24, 0.044), (25, 0.03), (26, -0.024), (27, -0.048), (28, 0.007), (29, 0.032), (30, -0.057), (31, -0.025), (32, 0.004), (33, -0.036), (34, -0.065), (35, -0.029), (36, 0.024), (37, 0.102), (38, -0.088), (39, -0.02), (40, 0.008), (41, -0.047), (42, 0.085), (43, 0.085), (44, 0.028), (45, 0.019), (46, 0.073), (47, -0.018), (48, 0.044), (49, -0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96975607 <a title="132-lsi-1" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>Author: Ji Ma ; Jingbo Zhu ; Tong Xiao ; Nan Yang</p><p>Abstract: In this paper, we combine easy-first dependency parsing and POS tagging algorithms with beam search and structured perceptron. We propose a simple variant of “early-update” to ensure valid update in the training process. The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity. On CTB, we achieve 94.01% tagging accuracy and 86.33% unlabeled attachment score with a relatively small beam width. On PTB, we also achieve state-of-the-art performance. 1</p><p>2 0.90944988 <a title="132-lsi-2" href="./acl-2013-Transition-based_Dependency_Parsing_with_Selectional_Branching.html">358 acl-2013-Transition-based Dependency Parsing with Selectional Branching</a></p>
<p>Author: Jinho D. Choi ; Andrew McCallum</p><p>Abstract: We present a novel approach, called selectional branching, which uses confidence estimates to decide when to employ a beam, providing the accuracy of beam search at speeds close to a greedy transition-based dependency parsing approach. Selectional branching is guaranteed to perform a fewer number of transitions than beam search yet performs as accurately. We also present a new transition-based dependency parsing algorithm that gives a complexity of O(n) for projective parsing and an expected linear time speed for non-projective parsing. With the standard setup, our parser shows an unlabeled attachment score of 92.96% and a parsing speed of 9 milliseconds per sentence, which is faster and more accurate than the current state-of-the-art transitionbased parser that uses beam search.</p><p>3 0.90673256 <a title="132-lsi-3" href="./acl-2013-Efficient_Implementation_of_Beam-Search_Incremental_Parsers.html">133 acl-2013-Efficient Implementation of Beam-Search Incremental Parsers</a></p>
<p>Author: Yoav Goldberg ; Kai Zhao ; Liang Huang</p><p>Abstract: Beam search incremental parsers are accurate, but not as fast as they could be. We demonstrate that, contrary to popular belief, most current implementations of beam parsers in fact run in O(n2), rather than linear time, because each statetransition is actually implemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empiri- cally. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treeb∼a2nxk sentences, a bnads are eosrd oenrs P eofn magnitude faster on much longer sentences.</p><p>4 0.836622 <a title="132-lsi-4" href="./acl-2013-A_Transition-Based_Dependency_Parser_Using_a_Dynamic_Parsing_Strategy.html">26 acl-2013-A Transition-Based Dependency Parser Using a Dynamic Parsing Strategy</a></p>
<p>Author: Francesco Sartorio ; Giorgio Satta ; Joakim Nivre</p><p>Abstract: We present a novel transition-based, greedy dependency parser which implements a flexible mix of bottom-up and top-down strategies. The new strategy allows the parser to postpone difficult decisions until the relevant information becomes available. The novel parser has a ∼12% error reduction in unlabeled attach∼ment score over an arc-eager parser, with a slow-down factor of 2.8.</p><p>5 0.81065696 <a title="132-lsi-5" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>Author: Muhua Zhu ; Yue Zhang ; Wenliang Chen ; Min Zhang ; Jingbo Zhu</p><p>Abstract: Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser.</p><p>6 0.75847584 <a title="132-lsi-6" href="./acl-2013-Punctuation_Prediction_with_Transition-based_Parsing.html">288 acl-2013-Punctuation Prediction with Transition-based Parsing</a></p>
<p>7 0.66131842 <a title="132-lsi-7" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>8 0.65075368 <a title="132-lsi-8" href="./acl-2013-Survey_on_parsing_three_dependency_representations_for_English.html">335 acl-2013-Survey on parsing three dependency representations for English</a></p>
<p>9 0.63429886 <a title="132-lsi-9" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>10 0.56660128 <a title="132-lsi-10" href="./acl-2013-Turning_on_the_Turbo%3A_Fast_Third-Order_Non-Projective_Turbo_Parsers.html">362 acl-2013-Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers</a></p>
<p>11 0.55461532 <a title="132-lsi-11" href="./acl-2013-Joint_Inference_for_Heterogeneous_Dependency_Parsing.html">208 acl-2013-Joint Inference for Heterogeneous Dependency Parsing</a></p>
<p>12 0.54773486 <a title="132-lsi-12" href="./acl-2013-Chinese_Parsing_Exploiting_Characters.html">80 acl-2013-Chinese Parsing Exploiting Characters</a></p>
<p>13 0.48190862 <a title="132-lsi-13" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>14 0.45406368 <a title="132-lsi-14" href="./acl-2013-A_Unified_Morpho-Syntactic_Scheme_of_Stanford_Dependencies.html">28 acl-2013-A Unified Morpho-Syntactic Scheme of Stanford Dependencies</a></p>
<p>15 0.44968668 <a title="132-lsi-15" href="./acl-2013-An_Empirical_Examination_of_Challenges_in_Chinese_Parsing.html">44 acl-2013-An Empirical Examination of Challenges in Chinese Parsing</a></p>
<p>16 0.4460876 <a title="132-lsi-16" href="./acl-2013-Grounded_Unsupervised_Semantic_Parsing.html">176 acl-2013-Grounded Unsupervised Semantic Parsing</a></p>
<p>17 0.4370048 <a title="132-lsi-17" href="./acl-2013-Stop-probability_estimates_computed_on_a_large_corpus_improve_Unsupervised_Dependency_Parsing.html">331 acl-2013-Stop-probability estimates computed on a large corpus improve Unsupervised Dependency Parsing</a></p>
<p>18 0.43297511 <a title="132-lsi-18" href="./acl-2013-Dependency_Parser_Adaptation_with_Subtrees_from_Auto-Parsed_Target_Domain_Data.html">112 acl-2013-Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data</a></p>
<p>19 0.42282766 <a title="132-lsi-19" href="./acl-2013-A_Lattice-based_Framework_for_Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">7 acl-2013-A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>20 0.41643831 <a title="132-lsi-20" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.049), (6, 0.042), (11, 0.117), (15, 0.011), (24, 0.038), (26, 0.084), (28, 0.019), (35, 0.068), (42, 0.128), (48, 0.043), (70, 0.094), (82, 0.111), (88, 0.016), (90, 0.024), (95, 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9140169 <a title="132-lda-1" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>Author: Ji Ma ; Jingbo Zhu ; Tong Xiao ; Nan Yang</p><p>Abstract: In this paper, we combine easy-first dependency parsing and POS tagging algorithms with beam search and structured perceptron. We propose a simple variant of “early-update” to ensure valid update in the training process. The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity. On CTB, we achieve 94.01% tagging accuracy and 86.33% unlabeled attachment score with a relatively small beam width. On PTB, we also achieve state-of-the-art performance. 1</p><p>2 0.87692982 <a title="132-lda-2" href="./acl-2013-Chinese_Parsing_Exploiting_Characters.html">80 acl-2013-Chinese Parsing Exploiting Characters</a></p>
<p>Author: Meishan Zhang ; Yue Zhang ; Wanxiang Che ; Ting Liu</p><p>Abstract: Characters play an important role in the Chinese language, yet computational processing of Chinese has been dominated by word-based approaches, with leaves in syntax trees being words. We investigate Chinese parsing from the character-level, extending the notion of phrase-structure trees by annotating internal structures of words. We demonstrate the importance of character-level information to Chinese processing by building a joint segmentation, part-of-speech (POS) tagging and phrase-structure parsing system that integrates character-structure features. Our joint system significantly outperforms a state-of-the-art word-based baseline on the standard CTB5 test, and gives the best published results for Chinese parsing.</p><p>3 0.86760432 <a title="132-lda-3" href="./acl-2013-Transition-based_Dependency_Parsing_with_Selectional_Branching.html">358 acl-2013-Transition-based Dependency Parsing with Selectional Branching</a></p>
<p>Author: Jinho D. Choi ; Andrew McCallum</p><p>Abstract: We present a novel approach, called selectional branching, which uses confidence estimates to decide when to employ a beam, providing the accuracy of beam search at speeds close to a greedy transition-based dependency parsing approach. Selectional branching is guaranteed to perform a fewer number of transitions than beam search yet performs as accurately. We also present a new transition-based dependency parsing algorithm that gives a complexity of O(n) for projective parsing and an expected linear time speed for non-projective parsing. With the standard setup, our parser shows an unlabeled attachment score of 92.96% and a parsing speed of 9 milliseconds per sentence, which is faster and more accurate than the current state-of-the-art transitionbased parser that uses beam search.</p><p>4 0.86270517 <a title="132-lda-4" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>Author: Greg Coppola ; Mark Steedman</p><p>Abstract: Higher-order dependency features are known to improve dependency parser accuracy. We investigate the incorporation of such features into a cube decoding phrase-structure parser. We find considerable gains in accuracy on the range of standard metrics. What is especially interesting is that we find strong, statistically significant gains on dependency recovery on out-of-domain tests (Brown vs. WSJ). This suggests that higher-order dependency features are not simply overfitting the training material.</p><p>5 0.85800886 <a title="132-lda-5" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>Author: Muhua Zhu ; Yue Zhang ; Wenliang Chen ; Min Zhang ; Jingbo Zhu</p><p>Abstract: Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser.</p><p>6 0.85784656 <a title="132-lda-6" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>7 0.85568398 <a title="132-lda-7" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>8 0.84764707 <a title="132-lda-8" href="./acl-2013-Joint_Inference_for_Heterogeneous_Dependency_Parsing.html">208 acl-2013-Joint Inference for Heterogeneous Dependency Parsing</a></p>
<p>9 0.84467465 <a title="132-lda-9" href="./acl-2013-A_Stacking-based_Approach_to_Twitter_User_Geolocation_Prediction.html">20 acl-2013-A Stacking-based Approach to Twitter User Geolocation Prediction</a></p>
<p>10 0.83936661 <a title="132-lda-10" href="./acl-2013-Efficient_Implementation_of_Beam-Search_Incremental_Parsers.html">133 acl-2013-Efficient Implementation of Beam-Search Incremental Parsers</a></p>
<p>11 0.83583963 <a title="132-lda-11" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>12 0.83476484 <a title="132-lda-12" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<p>13 0.83369118 <a title="132-lda-13" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>14 0.8330521 <a title="132-lda-14" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>15 0.82996768 <a title="132-lda-15" href="./acl-2013-Utterance-Level_Multimodal_Sentiment_Analysis.html">379 acl-2013-Utterance-Level Multimodal Sentiment Analysis</a></p>
<p>16 0.8270691 <a title="132-lda-16" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>17 0.8268941 <a title="132-lda-17" href="./acl-2013-Survey_on_parsing_three_dependency_representations_for_English.html">335 acl-2013-Survey on parsing three dependency representations for English</a></p>
<p>18 0.82639557 <a title="132-lda-18" href="./acl-2013-Arguments_and_Modifiers_from_the_Learner%27s_Perspective.html">57 acl-2013-Arguments and Modifiers from the Learner's Perspective</a></p>
<p>19 0.82390875 <a title="132-lda-19" href="./acl-2013-Discriminative_Learning_with_Natural_Annotations%3A_Word_Segmentation_as_a_Case_Study.html">123 acl-2013-Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study</a></p>
<p>20 0.82343215 <a title="132-lda-20" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
