<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>139 acl-2013-Entity Linking for Tweets</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-139" href="#">acl2013-139</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>139 acl-2013-Entity Linking for Tweets</h1>
<br/><p>Source: <a title="acl-2013-139-pdf" href="http://aclweb.org/anthology//P/P13/P13-1128.pdf">pdf</a></p><p>Author: Xiaohua Liu ; Yitong Li ; Haocheng Wu ; Ming Zhou ; Furu Wei ; Yi Lu</p><p>Abstract: We study the task of entity linking for tweets, which tries to associate each mention in a tweet with a knowledge base entry. Two main challenges of this task are the dearth of information in a single tweet and the rich entity mention variations. To address these challenges, we propose a collective inference method that simultaneously resolves a set of mentions. Particularly, our model integrates three kinds of similarities, i.e., mention-entry similarity, entry-entry similarity, and mention-mention similarity, to enrich the context for entity linking, and to address irregular mentions that are not covered by the entity-variation dictionary. We evaluate our method on a publicly available data set and demonstrate the effectiveness of our method.</p><p>Reference: <a title="acl-2013-139-reference" href="../acl2013_reference/acl-2013-Entity_Linking_for_Tweets_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We study the task of entity linking for tweets, which tries to associate each mention in a tweet with a knowledge base entry. [sent-7, score-1.074]
</p><p>2 Two main challenges of this task are the dearth of information in a single tweet and the rich entity mention variations. [sent-8, score-0.918]
</p><p>3 To address these challenges, we propose a collective inference method that simultaneously resolves a set of mentions. [sent-9, score-0.147]
</p><p>4 , mention-entry similarity, entry-entry similarity, and mention-mention similarity, to enrich the context for entity linking, and to address irregular mentions that are not  covered by the entity-variation dictionary. [sent-12, score-0.51]
</p><p>5 With millions of active users and hundreds of millions of new published tweets every day1 , it has become a popular platform to capture and transmit the human experiences of the moment. [sent-15, score-0.188]
</p><p>6 Many tweet related researches are inspired, from named entity recognition (Liu et al. [sent-16, score-0.537]
</p><p>7 In this work, we study the entity linking task for tweets, which maps each entity mention in a tweet to a unique entity, i. [sent-20, score-1.322]
</p><p>8 Entity linking for tweets is particularly meaningful, considering that tweets are often hard to read owing to its informal written style and length limitation of 140 characters. [sent-28, score-0.547]
</p><p>9 Current entity linking methods are built on top of a large scale knowledge base such as Wikipedia. [sent-29, score-0.448]
</p><p>10 A knowledge base consists of a set of entities, and each entity can have a variation list2. [sent-30, score-0.413]
</p><p>11 To decide which entity should be mapped, they may compute: 1) the similarity between the context of a mention, e. [sent-31, score-0.468]
</p><p>12 , the entity page of Wikipedia (Mihalcea and Csomai, 2007; Han and Zhao, 2009); 2) the coherence among the mapped entities for a set of related mentions, e. [sent-35, score-0.501]
</p><p>13 g, multiple mentions in a document (Milne and Witten, 2008; Kulkarni et al. [sent-36, score-0.218]
</p><p>14 First, a tweet is often too concise and too  noisy to provide enough information for similarity computing, owing to its short and grass root nature. [sent-40, score-0.425]
</p><p>15 Second, tweets have rich variations of named and many of them fall out of the scope of the existing dictionaries mined from Wikipedia (called OOV mentions hereafter). [sent-41, score-0.432]
</p><p>16 On  entities3,  2Entity variation lists can be extracted from the entity resolution pages of Wikipedia. [sent-42, score-0.405]
</p><p>17 org/wiki/Svm” will lead us to a resolution page, where “Svm” are linked to entities like “Space vector modulation” and “Support vector machine”. [sent-45, score-0.158]
</p><p>18 Ac s2s0o1ci3a Atiosnso fcoirat Cioonm foprut Caotimonpaulta Lti nognuails Lti cnsg,u piasgteics 1304–1311, the other hand, the huge redundancy in tweets offers opportunities. [sent-52, score-0.188]
</p><p>19 That means, an entity mention often occurs in many tweets, which allows us to aggregate all related tweets to compute mention-mention similarity and mentionentity similarity. [sent-53, score-1.127]
</p><p>20 We propose a collective inference method  that leverages tweet redundancy to address those two challenges. [sent-54, score-0.366]
</p><p>21 Given a set of mentions, our model tries to ensure that similar mentions are linked to similar entities while pursuing the high total similarity between matched mentionentity pairs. [sent-55, score-0.616]
</p><p>22 More specifically, we define local features, including context similarity and edit distance, to model the similarity between a mention and an entity. [sent-56, score-0.886]
</p><p>23 We adopt in-link based similarity (Milne and Witten, 2008), to measure the similarity between entities. [sent-57, score-0.352]
</p><p>24 Finally, we introduce a set of features to compute the similarity between mentions, including how similar the tweets containing the mentions are, whether they come from the tweets of the same account, and their edit distance. [sent-58, score-0.869]
</p><p>25 Notably, our model can resolve OOV mentions with the help of their similar mentions. [sent-59, score-0.218]
</p><p>26 For example, for the OOV mention “LukeBryanOnline”, our model can find similar mentions like “TheLukeBryan” and “LukeBryan”. [sent-60, score-0.625]
</p><p>27 Considering that most of its similar mentions are mapped to the American country singer “Luke Bryan”, our model tends to link  “LukeBryanOnline” to the same entity. [sent-61, score-0.288]
</p><p>28 We also study the effectiveness of features related to each kind of similarity, and demonstrate the advantage of our method for OOV mention linkage. [sent-69, score-0.499]
</p><p>29 We introduce a novel collective inference method that integrates three kinds of similarities, i. [sent-72, score-0.193]
</p><p>30 , mention-entity similarity, entity-entity similarity, and mention-mention similarity, to simultaneously map a set of tweet mentions to their proper entities. [sent-74, score-0.437]
</p><p>31 We propose modeling the mention-mention similarity and demonstrate its effectiveness 4http://ilps. [sent-76, score-0.202]
</p><p>32 nl/resources/wsdm2012-addingsemantics-to-microblog-posts/ in entity linking for tweets, particularly for  OOV mentions. [sent-79, score-0.404]
</p><p>33 2  Related Work  Existing entity linking work can roughly be divided into two categories. [sent-88, score-0.404]
</p><p>34 Methods of the first category resolve one mention at each time, and mainly consider the similarity between a mention-entity pair. [sent-89, score-0.616]
</p><p>35 In contrast, methods of the second category take a set of related  mentions (e. [sent-90, score-0.251]
</p><p>36 , mentions in the same document) as input, and figure out their corresponding entities simultaneously. [sent-92, score-0.322]
</p><p>37 Examples of the first category include the first Web-scale entity linking system SemTag (Dill et al. [sent-93, score-0.437]
</p><p>38 SemTag uses the TAP knowledge base5, and employs the cosine similarity with TF-IDF weighting scheme to compute the match degree between a mention and an entity, achieving an accuracy of around 82%. [sent-96, score-0.583]
</p><p>39 , the contextual overlap between the paragraph where the mention occurs and the corresponding Wikipedia pages, and a Naive Bayes classifier that predicts whether a mention should be linked to an entity. [sent-101, score-0.868]
</p><p>40 (2009) propose a graphical model that explicitly models the combination of evidence from local mentionentity compatibility and global document-level topical coherence of the entities, and show that considering global coherence between entities  significantly improves the performance. [sent-118, score-0.458]
</p><p>41 (201 1) introduce a graph-based representation, called Referent Graph, to model the global interdependence between different entity linking decisions, and jointly infer the referent entities of all name mentions in a document by exploiting the interdependence captured in Referent Graph. [sent-120, score-0.898]
</p><p>42 (2012) propose LIEGE, a framework to link the entities in web lists with the knowledge base, with the assumption that entities mentioned in a Web list tend to be a collection of entities of the same conceptual type. [sent-122, score-0.415]
</p><p>43 Most work of entity linking focuses on web pages. [sent-123, score-0.431]
</p><p>44 They propose a machine learning based approach using n-gram features, concept features, and tweet features, to identify concepts semantically related to a tweet, and for every entity mention to generate links to its corresponding Wikipedia article. [sent-126, score-0.918]
</p><p>45 Their method belongs to the first category, in the sense that they only consider the similarity between mention  (tweet) and entity (Wikipedia article). [sent-127, score-0.909]
</p><p>46 However, in contrast with existing collective approaches, our method works on tweets which are short and often noisy. [sent-129, score-0.309]
</p><p>47 Furthermore, our method is based on the “similar mention with similar entity” assumption, and explicitly models and integrates the mention similarity into the optimization framework. [sent-130, score-1.07]
</p><p>48 3  Task Definition  Given a sequence of mentions, denoted by = (m1, m2, · · · , mn), our task is to output a sequence o·f , entities, denoted by = (e1, e2, ·· · , en), where ei is the entity corresponding ·to· mi. [sent-133, score-0.528]
</p><p>49 Here, an entity refers to an item of a knowledge base. [sent-134, score-0.292]
</p><p>50 Following most existing work, we use Wikipedia as the knowledge base, and an entity is a definition page in Wikipedia; a mention denotes a sequence of  M⃗ E⃗  tokens in a tweet that can be potentially linked to an entity. [sent-135, score-1.039]
</p><p>51 Third, mentions with the same token sequence may refer to different entities, depending on mention context. [sent-141, score-0.663]
</p><p>52 Finally, we assume each entity e has a variation list6, and a unique ID through which all related information about that entity can be accessed. [sent-142, score-0.661]
</p><p>53 Given mentions “nbcbightlynews”, “Santiago”, “WH” and “Libya” from the following tweet “Chuck Todd: Prepping for @nbcnightlynews here in Santiago, reporting on WH handling of Libya situation. [sent-144, score-0.437]
</p><p>54 4  Our Method  In this section, we first present the framework of our entity linking method. [sent-146, score-0.404]
</p><p>55 1 Framework  M⃗  Given the input mention sequence = (m1, m2, · · · , mn), our method outputs the entity sequence = (e∗1, e2∗, · · · , en∗) according to Formula 1:  E⃗··∗  6For example, the variation list ofthe entity “Obama” may contain “Barack Obama”, “Barack Hussein Obama II”, etc. [sent-149, score-1.178]
</p><p>56 It is  From Formula 1, we can see that: 1) our method considers the mention-entity similarly, entity-entity similarity and mention-mention similarity. [sent-153, score-0.21]
</p><p>57 Mention-entity similarly is used to model local compatibility, while entity-entity similarity and mention-mention similarity combined are to model global consistence; and 2) our method prefers configurations where similar mentions have similar entities and with high local compatibility. [sent-154, score-0.871]
</p><p>58 It represents the search space, which can be generated using the entity variation list. [sent-156, score-0.369]
</p><p>59 To achieve this, we first build an inverted index of all entity variation lists, with each unique variation as an entry pointing to a list of entities. [sent-157, score-0.5]
</p><p>60 Then for any mention m, we look up the index, and get all possible entities, denoted by C(m). [sent-158, score-0.407]
</p><p>61 In this way, given a mention sequence = (m1, m2, · · · , mn), we can enumerate all possible entity sequence = (e1, e2, · · · ∏en), where ei ∈  C(M⃗)  M⃗  E⃗  ∏,  |C(M⃗)|,  C(m). [sent-159, score-0.935]
</p><p>62 f Tcoan addiddraetesss for an OOV mention using its similar mentions. [sent-167, score-0.407]
</p><p>63 Let S(m) denote OOV mention m’s similar mentions, we define C(m) = ∪m′∈S(m)  |iCf m(M⃗ is)|  C(m′). [sent-168, score-0.407]
</p><p>64 Ovals in orange and in blue represent mentions and entities, respectively. [sent-176, score-0.218]
</p><p>65 Each mention pair, entity pair, and mention entity pair have a similarity score represented by s, r and f, respectively. [sent-177, score-1.574]
</p><p>66 We need find out the best entity sequence for mentions = { “Liverpool1”, “Manchester United”, “ManU”, “Liverpool2”}, from the entity sequences = { (Liverpool (film), Manchester United F. [sent-178, score-0.84]
</p><p>67 Notably, “ManU” is an OOV mention, but has a similar mention “Manchester United”, with which “ManU” is successfully mapped. [sent-194, score-0.407]
</p><p>68 2  Features  We group features into three categories: local features related to mention-entity similarity m)), features related to entity-entity similarity (r(ei, ej)) , and features related to mention-mention similarity (s(mi, mj)). [sent-196, score-0.716]
</p><p>69 1 Local Features • Prior Probability:  f1(mi,ei) =∑∀ek∈cCo(umni)tc(eoiu)nt(ek) where count(e) denotes the frequency entity e in Wikipedia’s anchor texts. [sent-199, score-0.292]
</p><p>70 (2) of  • Context Similarity:  f2(mi,ei) =cooctwureeentc leen ngutmhber  (3)  where: coccurence number is the the number of the words that occur in both the tweet containing mi and the Wikipedia page of ei; tweet length denotes the number of tokens of the tweet containing mention mi. [sent-200, score-1.347]
</p><p>71 This feature helps to detect whether a mention is an abbreviation of its  corresponding entity7. [sent-203, score-0.407]
</p><p>72 •  •  Mention Contains Title: If the mention cMoenntatiinosn t Cheo entity title, namely the title of the Wikipedia page introducing the entity ei, f4(mi, ei) = 1, else 0. [sent-204, score-1.117]
</p><p>73 2 Features Related to Entity Similarity There are two representative definitions of entity similarity: in-link based similarity (Milne and Witten, 2008) and category based similarity (Shen et al. [sent-211, score-0.677]
</p><p>74 = (a1, a2, a3, a4, a5) is the feature  a  weight vector for mention s∑imilarity, where (0, 1), k = 1, 2, 3, 4, 5, and ak = 1. [sent-222, score-0.407]
</p><p>75 3 Training and Decoding Given n mentions m1, m2, · · · , mn and their corresponding entities e1, e2, · · · , en, the goal of training is to determine: w⃗ ∗,, t·h·e· weights of local features, and a∗, the weights of the features related to mention similarity, according to Formula 7 9. [sent-224, score-0.879]
</p><p>76 In each iteration, this rounding solution iteratively substitute entry in E⃗ to increase the total score ei  ej  scij  ei  −  cur. [sent-233, score-0.533]
</p><p>77 M⃗ = (m1, Set E⃗ = (e1,  Input: Mention Set  m2, ··· ,  mn)  Output: Entity e2, ··· , en) 1: for i= 1to n do 2: Initialize as the entity with the largest prior probability given mention mi. [sent-238, score-0.699]
</p><p>78 3: end for 4: cur = 5: it = 1 6: while true do 7: for i= 1to n do 8: for ej ∈ C(mi) do 9: if ej then  ei(0)  Score(E⃗(0), M⃗)  10: 11: 12: 13:  = e(iit−1) E⃗(iijt) = E⃗(it−1) − {e(iit−1)}  end if scij = end for  Score(E⃗i(jit), M⃗). [sent-239, score-0.37]
</p><p>79 14: end for 15: (l, m) = argmax(i,j) 16: sc∗ = sclm 17: if sc∗ > cur then 18: cur = sc∗ . [sent-241, score-0.128]
</p><p>80 We index the Wikipedia definition pages, and prepare all required prior knowledge, such as count(e), g(e), and entity variation lists. [sent-249, score-0.369]
</p><p>81 We also build an inverted  index with about 60 million entries for the entity variation lists. [sent-250, score-0.395]
</p><p>82 (2012) ;  •  Using only local features;  •  Using various mention similarity features;  •  Experiments on OOV mentions. [sent-267, score-0.643]
</p><p>83 Since the main difference between our method and the baselines is that our method considers not only local features, but also global features related to entity similarity and mention similarity, these results indicate the effectiveness of collective inference and global features. [sent-271, score-1.286]
</p><p>84 For example, we find two baselines incorrectly link “Nickelodeon” in the tweet “BOH will make a special appearance on Nickelodeon’s ‘Yo Gabba Gabba’ tomorrow” to the theater instead of a TV channel. [sent-272, score-0.285]
</p><p>85 In contrast, our method notices that “Yo Gabba Gabba” in the same tweet can be linked to “Yo Gabba Gabba (TV show)”, and thus it correctly maps “Nickelodeon” to “Nickelodeon (TV channel)”. [sent-273, score-0.307]
</p><p>86 The performance of our method with various mention similarity features is reported in Table 3. [sent-329, score-0.649]
</p><p>87 Second, we notice that TF-IDF (s1) and Topic Model (s2) features perform equally well, and combining all mention similarity features yields the best performance. [sent-333, score-0.647]
</p><p>88 For any OOV mention, we use the strategy of guessing its possible entity candidates using  similar mentions, as discussed in Section 4. [sent-349, score-0.292]
</p><p>89 A further study reveals that among all the 125 OOV mentions, there are 48 for which our method cannot find any entity; and nearly half of these 48 OOV mentions do have corresponding entities 13. [sent-356, score-0.356]
</p><p>90 This suggests that we may need enlarge the size of variation lists or develop some mention normalization techniques. [sent-357, score-0.547]
</p><p>91 It is mapped to NULL but actually has a corresponding entity “Ukraine-NATO relations” 1310  6  Conclusions and Future work  We have presented a collective inference method that jointly links a set of tweet mentions to their corresponding entities. [sent-366, score-0.906]
</p><p>92 One distinguished characteristic of our method is that it integrates mention-entity similarity, entity-entity similarity, and mention-mention similarity, to address the information lack in a tweet and rich OOV mentions. [sent-367, score-0.299]
</p><p>93 Experimental results show our method outperforms two baselines, and suggests the effectiveness of modeling mention-mention similarity, particularly for OOV mention linking. [sent-369, score-0.467]
</p><p>94 First, we are going to enlarge the size of entity variation lists. [sent-371, score-0.396]
</p><p>95 Second, we want to integrate the entity mention normalization techniques as introduced by Liu et al. [sent-372, score-0.699]
</p><p>96 Nlpr-kbp in tac 2009 kbp track: A two-stage method to entity linking. [sent-393, score-0.326]
</p><p>97 Structural semantic relatedness: a knowledge-based method  to named entity disambiguation. [sent-397, score-0.352]
</p><p>98 Collective entity linking in web text: A graph-based method. [sent-401, score-0.431]
</p><p>99 Joint inference of named entity recognition and normalization for tweets. [sent-409, score-0.344]
</p><p>100 Liege: Link entities in web lists with knowledge base. [sent-437, score-0.167]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mention', 0.407), ('entity', 0.292), ('mj', 0.269), ('oov', 0.23), ('mi', 0.225), ('tweet', 0.219), ('mentions', 0.218), ('tweets', 0.188), ('similarity', 0.176), ('wikify', 0.17), ('ei', 0.16), ('gabba', 0.128), ('ej', 0.121), ('meij', 0.114), ('linking', 0.112), ('milne', 0.109), ('entities', 0.104), ('wikipedia', 0.099), ('liverpool', 0.094), ('manu', 0.094), ('collective', 0.087), ('nickelodeon', 0.085), ('variation', 0.077), ('united', 0.072), ('witten', 0.071), ('csomai', 0.069), ('kulkarni', 0.069), ('han', 0.069), ('title', 0.069), ('edit', 0.067), ('cur', 0.064), ('mentionentity', 0.064), ('santiago', 0.064), ('scij', 0.064), ('semtag', 0.064), ('manchester', 0.064), ('formula', 0.061), ('local', 0.06), ('mn', 0.058), ('experimentally', 0.057), ('libya', 0.056), ('xianpei', 0.056), ('linked', 0.054), ('compatibility', 0.052), ('coherence', 0.046), ('integrates', 0.046), ('yo', 0.045), ('base', 0.044), ('mihalcea', 0.043), ('global', 0.043), ('referent', 0.043), ('dill', 0.043), ('englishpremierleague', 0.043), ('grinev', 0.043), ('interdependence', 0.043), ('liege', 0.043), ('lukebryanonline', 0.043), ('modulation', 0.043), ('usera', 0.043), ('link', 0.04), ('singh', 0.039), ('sequence', 0.038), ('obama', 0.038), ('glory', 0.038), ('mathioudakis', 0.038), ('film', 0.037), ('tv', 0.036), ('lists', 0.036), ('ij', 0.035), ('microsoft', 0.035), ('barack', 0.035), ('rosa', 0.035), ('method', 0.034), ('category', 0.033), ('furu', 0.033), ('features', 0.032), ('sand', 0.031), ('decoding', 0.031), ('mapped', 0.03), ('sc', 0.03), ('owing', 0.03), ('china', 0.03), ('length', 0.029), ('page', 0.029), ('en', 0.029), ('xiaohua', 0.029), ('shen', 0.028), ('entry', 0.028), ('else', 0.028), ('similarities', 0.027), ('enlarge', 0.027), ('web', 0.027), ('inference', 0.026), ('ny', 0.026), ('baselines', 0.026), ('named', 0.026), ('effectiveness', 0.026), ('inverted', 0.026), ('iit', 0.026), ('argmax', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000013 <a title="139-tfidf-1" href="./acl-2013-Entity_Linking_for_Tweets.html">139 acl-2013-Entity Linking for Tweets</a></p>
<p>Author: Xiaohua Liu ; Yitong Li ; Haocheng Wu ; Ming Zhou ; Furu Wei ; Yi Lu</p><p>Abstract: We study the task of entity linking for tweets, which tries to associate each mention in a tweet with a knowledge base entry. Two main challenges of this task are the dearth of information in a single tweet and the rich entity mention variations. To address these challenges, we propose a collective inference method that simultaneously resolves a set of mentions. Particularly, our model integrates three kinds of similarities, i.e., mention-entry similarity, entry-entry similarity, and mention-mention similarity, to enrich the context for entity linking, and to address irregular mentions that are not covered by the entity-variation dictionary. We evaluate our method on a publicly available data set and demonstrate the effectiveness of our method.</p><p>2 0.24780695 <a title="139-tfidf-2" href="./acl-2013-HYENA-live%3A_Fine-Grained_Online_Entity_Type_Classification_from_Natural-language_Text.html">179 acl-2013-HYENA-live: Fine-Grained Online Entity Type Classification from Natural-language Text</a></p>
<p>Author: Mohamed Amir Yosef ; Sandro Bauer ; Johannes Hoffart ; Marc Spaniol ; Gerhard Weikum</p><p>Abstract: Recent research has shown progress in achieving high-quality, very fine-grained type classification in hierarchical taxonomies. Within such a multi-level type hierarchy with several hundreds of types at different levels, many entities naturally belong to multiple types. In order to achieve high-precision in type classification, current approaches are either limited to certain domains or require time consuming multistage computations. As a consequence, existing systems are incapable of performing ad-hoc type classification on arbitrary input texts. In this demo, we present a novel Webbased tool that is able to perform domain independent entity type classification under real time conditions. Thanks to its efficient implementation and compacted feature representation, the system is able to process text inputs on-the-fly while still achieving equally high precision as leading state-ofthe-art implementations. Our system offers an online interface where natural-language text can be inserted, which returns semantic type labels for entity mentions. Further more, the user interface allows users to explore the assigned types by visualizing and navigating along the type-hierarchy.</p><p>3 0.21148503 <a title="139-tfidf-3" href="./acl-2013-Linking_Tweets_to_News%3A_A_Framework_to_Enrich_Short_Text_Data_in_Social_Media.html">233 acl-2013-Linking Tweets to News: A Framework to Enrich Short Text Data in Social Media</a></p>
<p>Author: Weiwei Guo ; Hao Li ; Heng Ji ; Mona Diab</p><p>Abstract: Many current Natural Language Processing [NLP] techniques work well assuming a large context of text as input data. However they become ineffective when applied to short texts such as Twitter feeds. To overcome the issue, we want to find a related newswire document to a given tweet to provide contextual support for NLP tasks. This requires robust modeling and understanding of the semantics of short texts. The contribution of the paper is two-fold: 1. we introduce the Linking-Tweets-toNews task as well as a dataset of linked tweet-news pairs, which can benefit many NLP applications; 2. in contrast to previ- ous research which focuses on lexical features within the short texts (text-to-word information), we propose a graph based latent variable model that models the inter short text correlations (text-to-text information). This is motivated by the observation that a tweet usually only covers one aspect of an event. We show that using tweet specific feature (hashtag) and news specific feature (named entities) as well as temporal constraints, we are able to extract text-to-text correlations, and thus completes the semantic picture of a short text. Our experiments show significant improvement of our new model over baselines with three evaluation metrics in the new task.</p><p>4 0.20065783 <a title="139-tfidf-4" href="./acl-2013-Learning_Entity_Representation_for_Entity_Disambiguation.html">219 acl-2013-Learning Entity Representation for Entity Disambiguation</a></p>
<p>Author: Zhengyan He ; Shujie Liu ; Mu Li ; Ming Zhou ; Longkai Zhang ; Houfeng Wang</p><p>Abstract: We propose a novel entity disambiguation model, based on Deep Neural Network (DNN). Instead of utilizing simple similarity measures and their disjoint combinations, our method directly optimizes document and entity representations for a given similarity measure. Stacked Denoising Auto-encoders are first employed to learn an initial document representation in an unsupervised pre-training stage. A supervised fine-tuning stage follows to optimize the representation towards the similarity measure. Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches.</p><p>5 0.16978161 <a title="139-tfidf-5" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>Author: Peifeng Li ; Qiaoming Zhu ; Guodong Zhou</p><p>Abstract: As a paratactic language, sentence-level argument extraction in Chinese suffers much from the frequent occurrence of ellipsis with regard to inter-sentence arguments. To resolve such problem, this paper proposes a novel global argument inference model to explore specific relationships, such as Coreference, Sequence and Parallel, among relevant event mentions to recover those intersentence arguments in the sentence, discourse and document layers which represent the cohesion of an event or a topic. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our global argument inference model over a state-of-the-art baseline. 1</p><p>6 0.16752736 <a title="139-tfidf-6" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>7 0.16260947 <a title="139-tfidf-7" href="./acl-2013-Towards_Accurate_Distant_Supervision_for_Relational_Facts_Extraction.html">352 acl-2013-Towards Accurate Distant Supervision for Relational Facts Extraction</a></p>
<p>8 0.15886092 <a title="139-tfidf-8" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>9 0.15247636 <a title="139-tfidf-9" href="./acl-2013-Detecting_Event-Related_Links_and_Sentiments_from_Social_Media_Texts.html">115 acl-2013-Detecting Event-Related Links and Sentiments from Social Media Texts</a></p>
<p>10 0.1462969 <a title="139-tfidf-10" href="./acl-2013-Fine-grained_Semantic_Typing_of_Emerging_Entities.html">160 acl-2013-Fine-grained Semantic Typing of Emerging Entities</a></p>
<p>11 0.12933661 <a title="139-tfidf-11" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>12 0.12567285 <a title="139-tfidf-12" href="./acl-2013-Sequential_Summarization%3A_A_New_Application_for_Timely_Updated_Twitter_Trending_Topics.html">319 acl-2013-Sequential Summarization: A New Application for Timely Updated Twitter Trending Topics</a></p>
<p>13 0.1249682 <a title="139-tfidf-13" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>14 0.111687 <a title="139-tfidf-14" href="./acl-2013-An_Empirical_Study_on_Uncertainty_Identification_in_Social_Media_Context.html">45 acl-2013-An Empirical Study on Uncertainty Identification in Social Media Context</a></p>
<p>15 0.11149985 <a title="139-tfidf-15" href="./acl-2013-A_Stacking-based_Approach_to_Twitter_User_Geolocation_Prediction.html">20 acl-2013-A Stacking-based Approach to Twitter User Geolocation Prediction</a></p>
<p>16 0.11079607 <a title="139-tfidf-16" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>17 0.10404044 <a title="139-tfidf-17" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>18 0.10368227 <a title="139-tfidf-18" href="./acl-2013-Detecting_Chronic_Critics_Based_on_Sentiment_Polarity_and_User%C3%A2%E2%80%A2%C5%BDs_Behavior_in_Social_Media.html">114 acl-2013-Detecting Chronic Critics Based on Sentiment Polarity and Userâ•Žs Behavior in Social Media</a></p>
<p>19 0.10337421 <a title="139-tfidf-19" href="./acl-2013-Named_Entity_Recognition_using_Cross-lingual_Resources%3A_Arabic_as_an_Example.html">256 acl-2013-Named Entity Recognition using Cross-lingual Resources: Arabic as an Example</a></p>
<p>20 0.094309457 <a title="139-tfidf-20" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.209), (1, 0.127), (2, 0.028), (3, -0.076), (4, 0.185), (5, 0.193), (6, 0.017), (7, 0.151), (8, 0.102), (9, -0.112), (10, -0.052), (11, -0.076), (12, 0.018), (13, -0.094), (14, 0.084), (15, 0.102), (16, -0.016), (17, 0.07), (18, -0.15), (19, 0.048), (20, -0.048), (21, 0.09), (22, -0.023), (23, 0.084), (24, -0.039), (25, 0.069), (26, 0.104), (27, 0.014), (28, -0.013), (29, 0.022), (30, 0.039), (31, -0.12), (32, 0.007), (33, 0.022), (34, -0.099), (35, -0.013), (36, -0.033), (37, -0.016), (38, -0.036), (39, -0.024), (40, 0.08), (41, -0.048), (42, -0.045), (43, 0.087), (44, -0.029), (45, -0.04), (46, -0.033), (47, 0.036), (48, -0.02), (49, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96017009 <a title="139-lsi-1" href="./acl-2013-Entity_Linking_for_Tweets.html">139 acl-2013-Entity Linking for Tweets</a></p>
<p>Author: Xiaohua Liu ; Yitong Li ; Haocheng Wu ; Ming Zhou ; Furu Wei ; Yi Lu</p><p>Abstract: We study the task of entity linking for tweets, which tries to associate each mention in a tweet with a knowledge base entry. Two main challenges of this task are the dearth of information in a single tweet and the rich entity mention variations. To address these challenges, we propose a collective inference method that simultaneously resolves a set of mentions. Particularly, our model integrates three kinds of similarities, i.e., mention-entry similarity, entry-entry similarity, and mention-mention similarity, to enrich the context for entity linking, and to address irregular mentions that are not covered by the entity-variation dictionary. We evaluate our method on a publicly available data set and demonstrate the effectiveness of our method.</p><p>2 0.72432715 <a title="139-lsi-2" href="./acl-2013-HYENA-live%3A_Fine-Grained_Online_Entity_Type_Classification_from_Natural-language_Text.html">179 acl-2013-HYENA-live: Fine-Grained Online Entity Type Classification from Natural-language Text</a></p>
<p>Author: Mohamed Amir Yosef ; Sandro Bauer ; Johannes Hoffart ; Marc Spaniol ; Gerhard Weikum</p><p>Abstract: Recent research has shown progress in achieving high-quality, very fine-grained type classification in hierarchical taxonomies. Within such a multi-level type hierarchy with several hundreds of types at different levels, many entities naturally belong to multiple types. In order to achieve high-precision in type classification, current approaches are either limited to certain domains or require time consuming multistage computations. As a consequence, existing systems are incapable of performing ad-hoc type classification on arbitrary input texts. In this demo, we present a novel Webbased tool that is able to perform domain independent entity type classification under real time conditions. Thanks to its efficient implementation and compacted feature representation, the system is able to process text inputs on-the-fly while still achieving equally high precision as leading state-ofthe-art implementations. Our system offers an online interface where natural-language text can be inserted, which returns semantic type labels for entity mentions. Further more, the user interface allows users to explore the assigned types by visualizing and navigating along the type-hierarchy.</p><p>3 0.69751561 <a title="139-lsi-3" href="./acl-2013-Fine-grained_Semantic_Typing_of_Emerging_Entities.html">160 acl-2013-Fine-grained Semantic Typing of Emerging Entities</a></p>
<p>Author: Ndapandula Nakashole ; Tomasz Tylenda ; Gerhard Weikum</p><p>Abstract: Methods for information extraction (IE) and knowledge base (KB) construction have been intensively studied. However, a largely under-explored case is tapping into highly dynamic sources like news streams and social media, where new entities are continuously emerging. In this paper, we present a method for discovering and semantically typing newly emerging out-ofKB entities, thus improving the freshness and recall of ontology-based IE and improving the precision and semantic rigor of open IE. Our method is based on a probabilistic model that feeds weights into integer linear programs that leverage type signatures of relational phrases and type correlation or disjointness constraints. Our experimental evaluation, based on crowdsourced user studies, show our method performing significantly better than prior work.</p><p>4 0.68151224 <a title="139-lsi-4" href="./acl-2013-Towards_Accurate_Distant_Supervision_for_Relational_Facts_Extraction.html">352 acl-2013-Towards Accurate Distant Supervision for Relational Facts Extraction</a></p>
<p>Author: Xingxing Zhang ; Jianwen Zhang ; Junyu Zeng ; Jun Yan ; Zheng Chen ; Zhifang Sui</p><p>Abstract: Distant supervision (DS) is an appealing learning method which learns from existing relational facts to extract more from a text corpus. However, the accuracy is still not satisfying. In this paper, we point out and analyze some critical factors in DS which have great impact on accuracy, including valid entity type detection, negative training examples construction and ensembles. We propose an approach to handle these factors. By experimenting on Wikipedia articles to extract the facts in Freebase (the top 92 relations), we show the impact of these three factors on the accuracy of DS and the remarkable improvement led by the proposed approach.</p><p>5 0.66670519 <a title="139-lsi-5" href="./acl-2013-Linking_Tweets_to_News%3A_A_Framework_to_Enrich_Short_Text_Data_in_Social_Media.html">233 acl-2013-Linking Tweets to News: A Framework to Enrich Short Text Data in Social Media</a></p>
<p>Author: Weiwei Guo ; Hao Li ; Heng Ji ; Mona Diab</p><p>Abstract: Many current Natural Language Processing [NLP] techniques work well assuming a large context of text as input data. However they become ineffective when applied to short texts such as Twitter feeds. To overcome the issue, we want to find a related newswire document to a given tweet to provide contextual support for NLP tasks. This requires robust modeling and understanding of the semantics of short texts. The contribution of the paper is two-fold: 1. we introduce the Linking-Tweets-toNews task as well as a dataset of linked tweet-news pairs, which can benefit many NLP applications; 2. in contrast to previ- ous research which focuses on lexical features within the short texts (text-to-word information), we propose a graph based latent variable model that models the inter short text correlations (text-to-text information). This is motivated by the observation that a tweet usually only covers one aspect of an event. We show that using tweet specific feature (hashtag) and news specific feature (named entities) as well as temporal constraints, we are able to extract text-to-text correlations, and thus completes the semantic picture of a short text. Our experiments show significant improvement of our new model over baselines with three evaluation metrics in the new task.</p><p>6 0.65505344 <a title="139-lsi-6" href="./acl-2013-Learning_Entity_Representation_for_Entity_Disambiguation.html">219 acl-2013-Learning Entity Representation for Entity Disambiguation</a></p>
<p>7 0.60830891 <a title="139-lsi-7" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>8 0.58742249 <a title="139-lsi-8" href="./acl-2013-Resolving_Entity_Morphs_in_Censored_Data.html">301 acl-2013-Resolving Entity Morphs in Censored Data</a></p>
<p>9 0.57190549 <a title="139-lsi-9" href="./acl-2013-Enriching_Entity_Translation_Discovery_using_Selective_Temporality.html">138 acl-2013-Enriching Entity Translation Discovery using Selective Temporality</a></p>
<p>10 0.56577557 <a title="139-lsi-10" href="./acl-2013-Text-Driven_Toponym_Resolution_using_Indirect_Supervision.html">340 acl-2013-Text-Driven Toponym Resolution using Indirect Supervision</a></p>
<p>11 0.56211287 <a title="139-lsi-11" href="./acl-2013-Detecting_Chronic_Critics_Based_on_Sentiment_Polarity_and_User%C3%A2%E2%80%A2%C5%BDs_Behavior_in_Social_Media.html">114 acl-2013-Detecting Chronic Critics Based on Sentiment Polarity and Userâ•Žs Behavior in Social Media</a></p>
<p>12 0.55759674 <a title="139-lsi-12" href="./acl-2013-An_Empirical_Study_on_Uncertainty_Identification_in_Social_Media_Context.html">45 acl-2013-An Empirical Study on Uncertainty Identification in Social Media Context</a></p>
<p>13 0.53643739 <a title="139-lsi-13" href="./acl-2013-Bootstrapping_Entity_Translation_on_Weakly_Comparable_Corpora.html">71 acl-2013-Bootstrapping Entity Translation on Weakly Comparable Corpora</a></p>
<p>14 0.53042853 <a title="139-lsi-14" href="./acl-2013-Named_Entity_Recognition_using_Cross-lingual_Resources%3A_Arabic_as_an_Example.html">256 acl-2013-Named Entity Recognition using Cross-lingual Resources: Arabic as an Example</a></p>
<p>15 0.5276764 <a title="139-lsi-15" href="./acl-2013-A_Stacking-based_Approach_to_Twitter_User_Geolocation_Prediction.html">20 acl-2013-A Stacking-based Approach to Twitter User Geolocation Prediction</a></p>
<p>16 0.5266428 <a title="139-lsi-16" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>17 0.51648241 <a title="139-lsi-17" href="./acl-2013-HEADY%3A_News_headline_abstraction_through_event_pattern_clustering.html">178 acl-2013-HEADY: News headline abstraction through event pattern clustering</a></p>
<p>18 0.50648868 <a title="139-lsi-18" href="./acl-2013-Aid_is_Out_There%3A_Looking_for_Help_from_Tweets_during_a_Large_Scale_Disaster.html">42 acl-2013-Aid is Out There: Looking for Help from Tweets during a Large Scale Disaster</a></p>
<p>19 0.49679491 <a title="139-lsi-19" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>20 0.47886628 <a title="139-lsi-20" href="./acl-2013-Detecting_Event-Related_Links_and_Sentiments_from_Social_Media_Texts.html">115 acl-2013-Detecting Event-Related Links and Sentiments from Social Media Texts</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.044), (6, 0.05), (11, 0.047), (24, 0.029), (26, 0.043), (35, 0.095), (42, 0.053), (48, 0.037), (70, 0.046), (88, 0.036), (90, 0.374), (95, 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95816934 <a title="139-lda-1" href="./acl-2013-On_the_Predictability_of_Human_Assessment%3A_when_Matrix_Completion_Meets_NLP_Evaluation.html">263 acl-2013-On the Predictability of Human Assessment: when Matrix Completion Meets NLP Evaluation</a></p>
<p>Author: Guillaume Wisniewski</p><p>Abstract: This paper tackles the problem of collecting reliable human assessments. We show that knowing multiple scores for each example instead of a single score results in a more reliable estimation of a system quality. To reduce the cost of collecting these multiple ratings, we propose to use matrix completion techniques to predict some scores knowing only scores of other judges and some common ratings. Even if prediction performance is pretty low, decisions made using the predicted score proved to be more reliable than decision based on a single rating of each example.</p><p>2 0.93369788 <a title="139-lda-2" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>Author: Akiko Eriguchi ; Ichiro Kobayashi</p><p>Abstract: In a multi-class document categorization using graph-based semi-supervised learning (GBSSL), it is essential to construct a proper graph expressing the relation among nodes and to use a reasonable categorization algorithm. Furthermore, it is also important to provide high-quality correct data as training data. In this context, we propose a method to construct a similarity graph by employing both surface information and latent information to express similarity between nodes and a method to select high-quality training data for GBSSL by means of the PageR- ank algorithm. Experimenting on Reuters21578 corpus, we have confirmed that our proposed methods work well for raising the accuracy of a multi-class document categorization.</p><p>3 0.91211963 <a title="139-lda-3" href="./acl-2013-Word_surprisal_predicts_N400_amplitude_during_reading.html">390 acl-2013-Word surprisal predicts N400 amplitude during reading</a></p>
<p>Author: Stefan L. Frank ; Leun J. Otten ; Giulia Galli ; Gabriella Vigliocco</p><p>Abstract: We investigated the effect of word surprisal on the EEG signal during sentence reading. On each word of 205 experimental sentences, surprisal was estimated by three types of language model: Markov models, probabilistic phrasestructure grammars, and recurrent neural networks. Four event-related potential components were extracted from the EEG of 24 readers of the same sentences. Surprisal estimates under each model type formed a significant predictor of the amplitude of the N400 component only, with more surprising words resulting in more negative N400s. This effect was mostly due to content words. These findings provide support for surprisal as a gener- ally applicable measure of processing difficulty during language comprehension.</p><p>4 0.90676558 <a title="139-lda-4" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>Author: Fabienne Braune ; Nina Seemann ; Daniel Quernheim ; Andreas Maletti</p><p>Abstract: We present a new translation model integrating the shallow local multi bottomup tree transducer. We perform a largescale empirical evaluation of our obtained system, which demonstrates that we significantly beat a realistic tree-to-tree baseline on the WMT 2009 English → German tlriannes olnati tohne tWasMk.T TA 2s0 an a Edndgitliisonha →l c Gonetrrmibauntion we make the developed software and complete tool-chain publicly available for further experimentation.</p><p>5 0.89183235 <a title="139-lda-5" href="./acl-2013-Integrating_Phrase-based_Reordering_Features_into_a_Chart-based_Decoder_for_Machine_Translation.html">200 acl-2013-Integrating Phrase-based Reordering Features into a Chart-based Decoder for Machine Translation</a></p>
<p>Author: ThuyLinh Nguyen ; Stephan Vogel</p><p>Abstract: Hiero translation models have two limitations compared to phrase-based models: 1) Limited hypothesis space; 2) No lexicalized reordering model. We propose an extension of Hiero called PhrasalHiero to address Hiero’s second problem. Phrasal-Hiero still has the same hypothesis space as the original Hiero but incorporates a phrase-based distance cost feature and lexicalized reodering features into the chart decoder. The work consists of two parts: 1) for each Hiero translation derivation, find its corresponding dis- continuous phrase-based path. 2) Extend the chart decoder to incorporate features from the phrase-based path. We achieve significant improvement over both Hiero and phrase-based baselines for ArabicEnglish, Chinese-English and GermanEnglish translation.</p><p>same-paper 6 0.88356906 <a title="139-lda-6" href="./acl-2013-Entity_Linking_for_Tweets.html">139 acl-2013-Entity Linking for Tweets</a></p>
<p>7 0.84855986 <a title="139-lda-7" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<p>8 0.63459235 <a title="139-lda-8" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>9 0.62247366 <a title="139-lda-9" href="./acl-2013-Models_of_Translation_Competitions.html">250 acl-2013-Models of Translation Competitions</a></p>
<p>10 0.61475778 <a title="139-lda-10" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>11 0.61142504 <a title="139-lda-11" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>12 0.60888237 <a title="139-lda-12" href="./acl-2013-Generalized_Reordering_Rules_for_Improved_SMT.html">166 acl-2013-Generalized Reordering Rules for Improved SMT</a></p>
<p>13 0.59427905 <a title="139-lda-13" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<p>14 0.59411782 <a title="139-lda-14" href="./acl-2013-Automated_Pyramid_Scoring_of_Summaries_using_Distributional_Semantics.html">59 acl-2013-Automated Pyramid Scoring of Summaries using Distributional Semantics</a></p>
<p>15 0.56553221 <a title="139-lda-15" href="./acl-2013-Diverse_Keyword_Extraction_from_Conversations.html">126 acl-2013-Diverse Keyword Extraction from Conversations</a></p>
<p>16 0.56302834 <a title="139-lda-16" href="./acl-2013-General_binarization_for_parsing_and_translation.html">165 acl-2013-General binarization for parsing and translation</a></p>
<p>17 0.56126219 <a title="139-lda-17" href="./acl-2013-Semantic_Parsing_as_Machine_Translation.html">312 acl-2013-Semantic Parsing as Machine Translation</a></p>
<p>18 0.5589608 <a title="139-lda-18" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>19 0.54963505 <a title="139-lda-19" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>20 0.54796845 <a title="139-lda-20" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
