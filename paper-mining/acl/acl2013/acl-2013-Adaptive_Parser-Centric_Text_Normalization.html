<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>37 acl-2013-Adaptive Parser-Centric Text Normalization</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-37" href="#">acl2013-37</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>37 acl-2013-Adaptive Parser-Centric Text Normalization</h1>
<br/><p>Source: <a title="acl-2013-37-pdf" href="http://aclweb.org/anthology//P/P13/P13-1114.pdf">pdf</a></p><p>Author: Congle Zhang ; Tyler Baldwin ; Howard Ho ; Benny Kimelfeld ; Yunyao Li</p><p>Abstract: Text normalization is an important first step towards enabling many Natural Language Processing (NLP) tasks over informal text. While many of these tasks, such as parsing, perform the best over fully grammatically correct text, most existing text normalization approaches narrowly define the task in the word-to-word sense; that is, the task is seen as that of mapping all out-of-vocabulary non-standard words to their in-vocabulary standard forms. In this paper, we take a parser-centric view of normalization that aims to convert raw informal text into grammatically correct text. To understand the real effect of normalization on the parser, we tie normal- ization performance directly to parser performance. Additionally, we design a customizable framework to address the often overlooked concept of domain adaptability, and illustrate that the system allows for transfer to new domains with a minimal amount of data and effort. Our experimental study over datasets from three domains demonstrates that our approach outperforms not only the state-of-the-art wordto-word normalization techniques, but also manual word-to-word annotations.</p><p>Reference: <a title="acl-2013-37-reference" href="../acl2013_reference/acl-2013-Adaptive_Parser-Centric_Text_Normalization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com fe i  Abstract Text normalization is an important first step towards enabling many Natural Language Processing (NLP) tasks over informal text. [sent-5, score-0.606]
</p><p>2 In this paper, we take a parser-centric view of normalization that aims to convert raw informal text into grammatically correct text. [sent-7, score-0.675]
</p><p>3 To understand the real effect of normalization on the parser, we tie normal-  ization performance directly to parser performance. [sent-8, score-0.569]
</p><p>4 Our experimental study over datasets from three domains demonstrates that our approach outperforms not only the state-of-the-art wordto-word normalization techniques, but also manual word-to-word annotations. [sent-10, score-0.51]
</p><p>5 1 Introduction Text normalization is the task of transforming informal writing into its standard form in the language. [sent-11, score-0.606]
</p><p>6 The use of normalization in these applications poses multiple challenges. [sent-15, score-0.51]
</p><p>7 First, as it is most often conceptualized, normalization is seen as the task of mapping all out-of-vocabulary non-standard word tokens to their in-vocabulary standard forms. [sent-16, score-0.57]
</p><p>8 This broader definition of the normalization task may include modifying punctuation and capitalization, and adding, removing, or reordering words. [sent-18, score-0.679]
</p><p>9 Second, as with other NLP techniques, normalization approaches are often focused on one primary domain of interest (e. [sent-19, score-0.541]
</p><p>10 This work introduces a customizable normalization approach designed with domain transfer in  mind. [sent-24, score-0.571]
</p><p>11 In short, customization is done by providing the normalizer with replacement generators, which we define in Section 3. [sent-25, score-0.336]
</p><p>12 We show that the introduction of a small set of domain-specific generators and training data allows our model to outperform a set of competitive baselines, including state-of-the-art word-to-word normalization. [sent-26, score-0.419]
</p><p>13 Additionally, the flexibility ofthe model also allows it to attempt to produce fully grammatical sentences, something not typically handled by word-to-word normalization approaches. [sent-27, score-0.549]
</p><p>14 Another potential problem with state-of-the-art normalization is the lack of appropriate evaluation metrics. [sent-28, score-0.51]
</p><p>15 The normalization task is most frequently motivated by pointing to the need for clean text for downstream processing applications, such as syntactic parsing. [sent-29, score-0.638]
</p><p>16 However, most studies of normalization give little insight into whether and to what degree the normalization process improves 1159  Proce dingsS o f ita h,e B 5u1lgsta Arinan,u Aaulg Musete 4ti-n9g 2 o0f1 t3h. [sent-30, score-1.02]
</p><p>17 For instance, it is unclear how performance mea-  sured by the typical normalization evaluation metrics of word error rate and BLEU score (Papineni et al. [sent-33, score-0.606]
</p><p>18 To address this problem, this work introduces an evaluation metric that ties normalization performance directly to the performance of a downstream dependency parser. [sent-35, score-0.767]
</p><p>19 In Section 2 we discuss previous approaches to the normalization problem. [sent-37, score-0.51]
</p><p>20 Section 3 presents our normalization framework, including the actual normalization and learning procedures. [sent-38, score-1.02]
</p><p>21 (2001) took the first major look at the normalization problem, citing the need for normalized text for downstream applications. [sent-43, score-0.693]
</p><p>22 Unlike later works that would primarily focus on specific noisy data sets, their work is notable for attempting to develop normalization as a general process that could be applied to different domains. [sent-44, score-0.51]
</p><p>23 The recent rise of heavily informal writing styles such as Twitter and SMS messages set off a new round of interest in the normalization problem. [sent-45, score-0.666]
</p><p>24 Research on SMS and Twitter normalization has been roughly categorized as drawing inspiration from three other areas ofNLP (Kobus et al. [sent-46, score-0.55]
</p><p>25 The statistical machine translation (SMT) metaphor was the first proposed to handle the text normalization problem (Aw et al. [sent-48, score-0.51]
</p><p>26 Recent work has looked at the construction of normalization dictionaries (Han et al. [sent-64, score-0.51]
</p><p>27 Although it is almost universally used as a motivating factor, most normalization work does not directly focus on improving downstream applications. [sent-67, score-0.638]
</p><p>28 While a few notable exceptions highlight the need for normalization as part of textto-speech systems (Beaufort et al. [sent-68, score-0.51]
</p><p>29 , 2010; Pennell and Liu, 2010), these works do not give any direct insight into how much the normalization process actually improves the performance of these systems. [sent-69, score-0.539]
</p><p>30 To our knowledge, the work presented here is the first to clearly link the output of a normalization system to the output of the downstream application. [sent-70, score-0.638]
</p><p>31 3  Model  In this section we introduce our normalization framework, which draws inspiration from our previous work on spelling correction for search (Bao et al. [sent-72, score-0.577]
</p><p>32 Given the input x, we apply a series of replacement generators, where a replacement generator is a function that takes x as input and produces a collection of replacements. [sent-80, score-0.581]
</p><p>33 Here, a replacement is a statement of the form “replace tokens xi, . [sent-81, score-0.288]
</p><p>34 ” More precisely, a replacement is a triple hi, j,si,  wMhoerere p1r ≤ sie ≤ j ≤ n + 1m aenndt s i as a sequence soi,f wtokheenres. [sent-85, score-0.309]
</p><p>35 For instance, in our running example the replacement h2, 3, would noti replaces x2 = weo reupdleacnemt ewnitth h 2w,o3u,wl odu not ; h1, 2, Ayi replaces x1 wdeithn tits weiltfh (hence, dd noeost n;o ht1 change x); h1, 2, ? [sent-90, score-0.336]
</p><p>36 The provided replacement generators can be either generic (cross domain) or domain-specific, allowing for domain customization. [sent-95, score-0.761]
</p><p>37 In Section 4, we discuss the replacement generators used in our empirical study. [sent-96, score-0.65]
</p><p>38 2 Normalization Graph Given the input x and the set of replacements produced by our generators, we associate a unique Boolean variable Xr with each replacement r. [sent-98, score-0.431]
</p><p>39 As expected, Xr being true means that the replacement r takes place in producing the output sequence. [sent-99, score-0.26]
</p><p>40 A truth assignment α to our variables Xr is sound if every two replacements r and r0 with α(Xr) = α(Xr0) = true are locally consistent. [sent-108, score-0.353]
</p><p>41 We say that α is complete if every token of x is captured by at least one replacement r with α(Xr) = true. [sent-109, score-0.26]
</p><p>42 The output (normalized sequence) defined by a legal assignment α is, naturally, the concatenation (from left to right) of the strings s in the replacements r = hi, j,si with α(Xr) = true. [sent-111, score-0.353]
</p><p>43 In this work, dependencies of the second type are restricted to pairs of variables, where each pair corresponds to a replacement and a consistent follower thereof. [sent-117, score-0.307]
</p><p>44 Therefore, we propose a clearer model by a  directed graph, as illustrated in Figure 1 (where nodes are represented by replacements r instead of the variables Xr, for readability). [sent-122, score-0.239]
</p><p>45 Moreover, we introduce two dummy nodes, start and end, with an edge from start to each variable that corresponds to a prefix of the input sequence x, and an edge from each variable that corresponds to a suffix of x to end. [sent-124, score-0.189]
</p><p>46 The principal advantage of modeling the dependencies in such a directed graph is that now, the legal assignments are in one-to-one correspondence with the paths from start to end; this is a straightforward observation that we do not prove here. [sent-125, score-0.201]
</p><p>47 ih42,6h3 ,1sw4e2,o uIfihldmi end  Figure 1: Example of a normalization graph; the nodes are replacements generated by the replacement generators, and every path from start to end implies a legal assignment x, Θ) = 0 if α is not legal, and otherwise,  p(α | x,Θ) =Z(1x)X→YY ∈eαxp(Xjθjφj(X,Y,x)). [sent-131, score-1.208]
</p><p>48 2, a legal assignment α corresponds itno a path nfr 3o. [sent-149, score-0.235]
</p><p>49 4 Learning Our labeled data consists of pairs (xi, where xi is an input sequence (to normalize) and is a (manually) normalized sequence. [sent-155, score-0.199]
</p><p>50 In particular, we describe our replacement generators and features. [sent-172, score-0.65]
</p><p>51 1 Replacement Generators One advantage of our proposed model is that  the reliance on replacement generators allows for strong flexibility. [sent-174, score-0.65]
</p><p>52 Each generator can be seen as a black box, allowing replacements that are created heuristically, statistically, or by external tools to be incorporated within the same framework. [sent-175, score-0.264]
</p><p>53 Table 1: Example replacement generators To build a set of generic replacement generators suitable for normalizing a variety of data types, we collected a set of about 400 Twitter posts as development data. [sent-176, score-1.462]
</p><p>54 Using that data, a series of generators were created; a sample of them are shown in Table 1. [sent-177, score-0.39]
</p><p>55 As shown in the table, these gener-  ators cover a variety of normalization behavior, from changing non-standard word forms to inserting and deleting tokens. [sent-178, score-0.51]
</p><p>56 Positional: Information from positions is used primarily to handle capitalization and punctuation insertion, for example, by incorporating features for capitalized words after stop punctuation or the insertion of stop punctuation at the end of the sentence. [sent-190, score-0.373]
</p><p>57 The goal is to evaluate the framework in two aspects: (1) usefulness for downstream applications (specifically dependency parsing), and (2) domain adaptability. [sent-194, score-0.19]
</p><p>58 In this work, we aim to evaluate the performance of a normalizer based on how it affects the performance of downstream applications. [sent-198, score-0.262]
</p><p>59 They also cannot take into account other aspects that may have an impact on downstream performance, such as the word reordering as seen in the example in Figure 4. [sent-202, score-0.205]
</p><p>60 Therefore, we propose a new evaluation metric that directly equates normalization performance with the performance of a common downstream application—dependency parsing. [sent-203, score-0.736]
</p><p>61 First, we produce gold standard normalized data by manually normalizing sentences to their full grammatically correct form. [sent-205, score-0.276]
</p><p>62 In addition to the word-to-word mapping performed in typical normalization gold standard generation, this annotation procedure includes all actions necessary to make the sentence grammatical, such as word reordering, modifying capitalization, and removing emoticons. [sent-206, score-0.602]
</p><p>63 We then run an off-the-shelf dependency parser on the gold standard normalized data to produce our gold standard parses. [sent-207, score-0.281]
</p><p>64 fied on example test/gold text, and corresponding metric scores To compare the parses produced over automatically normalized data to the gold standard, we look at the subjects, verbs, and objects (SVO) identified in each parse. [sent-209, score-0.186]
</p><p>65 Note that SO denotes the set of identified subjects and objects whereas SOgold denotes the set of subjects and objects identified when parsing the gold-standard normalization. [sent-211, score-0.203]
</p><p>66 2 Results To establish the extensibility of our normalization system, we present results in three different domains: Twitter posts, Short Message Service (SMS) messages, and call-center logs. [sent-215, score-0.51]
</p><p>67 In each case, we ran the proposed system with two different configurations: one using only the generic replacement generators presented in Section 4 (denoted as generic), and one that adds additional domain-specific generators for the cor-  responding domain (denoted as domain-specific). [sent-218, score-1.151]
</p><p>68 We compare our system to the following baseline solutions: w/oN: No normalization is performed. [sent-227, score-0.51]
</p><p>69 w2wN: The output of the word-to-word normalization of Han and Baldwin (201 1). [sent-229, score-0.51]
</p><p>70 To produce Twitter-specific generators, we examined the Twitter development data collected for generic generator production (Section 4). [sent-241, score-0.18]
</p><p>71 These generators focused on the Twitter-specific notions of hashtags (#), ats (@), and retweets (RT). [sent-242, score-0.39]
</p><p>72 For each case, we implemented generators that allowed for either the initial symbol or the entire token to be deleted (e. [sent-243, score-0.39]
</p><p>73 As shown, the domain-specific generators yielded performance significantly above the generic ones and all baselines. [sent-248, score-0.499]
</p><p>74 Even without domain-specific generators, our system outperformed the word-to-word normalization approaches. [sent-249, score-0.51]
</p><p>75 These results validate the hypothesis that simple word-to-word normalization is insufficient if the goal of normalization is to improve dependency parsing; even if a system could produce perfect word-to-word normalization, it would produce lower quality parses than those produced by our approach. [sent-251, score-1.129]
</p><p>76 As a replacement generator for SMS-specific substitutions, we used a mapping dictionary of SMS abbreviations. [sent-266, score-0.321]
</p><p>77 Nonetheless, the trends on SMS data mirror those on Twitter data, with the domain-specific generators achieving the greatest overall performance. [sent-272, score-0.39]
</p><p>78 However, while the generic setting still manages to outperform most baselines, it did not  outperform the gold word-to-word normalization. [sent-273, score-0.201]
</p><p>79 In fact, the gold word-to-word normalization was much more competitive on this data, outperforming even the domain-specific system on verbs alone. [sent-274, score-0.573]
</p><p>80 This should not be seen as surprising, as word-to-word normalization is most likely to be beneficial for cases like this where the proportion of non-standard tokens is high. [sent-275, score-0.57]
</p><p>81 The examination of callcenter logs allows us to examine the ability of our system to perform normalization in more disparate domains. [sent-299, score-0.537]
</p><p>82 However, the use of domain-specific generators once again led to significantly increased perfor-  mance on subjects and objects. [sent-308, score-0.449]
</p><p>83 6  Discussion  The results presented in the previous section suggest that domain transfer using the proposed normalization framework is possible with only a small amount of effort. [sent-309, score-0.541]
</p><p>84 The relatively modest set of additional replacement generators included in each data set allowed the domain-specific approaches to significantly outperform the generic approach. [sent-310, score-0.759]
</p><p>85 2 establish a point that has often been assumed but, to the best of our knowledge, has never been explicitly shown: per-  forming normalization is indeed beneficial to dependency parsing on informal text. [sent-316, score-0.666]
</p><p>86 The parse of the normalized text was substantially better than the parse of the original raw text in all domains, with absolute performance increases ranging from about 18-25% on subjects and objects. [sent-317, score-0.175]
</p><p>87 The proposed approach significantly outperforms the state-of-the-art word-to-word normalization approach. [sent-319, score-0.51]
</p><p>88 This result gives strong evidence for the conclusion that parser-targeted normalization requires a broader understanding of the scope of the normalization task. [sent-321, score-1.054]
</p><p>89 Although word reordering could be incor-  porated into the model as a combination of a deletion and an insertion, the model as currently devised cannot easily link these two replacements to one another. [sent-324, score-0.246]
</p><p>90 As such, no reordering-based replacement generators were implemented in the presented system. [sent-326, score-0.65]
</p><p>91 Similarly, punctuation insertion proved to be challenging, often requiring a deep analysis of the sentence. [sent-332, score-0.181]
</p><p>92 7  Conclusions  This work presents a framework for normalization with an eye towards domain adaptation. [sent-337, score-0.541]
</p><p>93 The proposed framework builds a statistical model over a series of replacement generators. [sent-338, score-0.26]
</p><p>94 Additionally, this work introduces a parsercentric view of normalization, in which the performance of the normalizer is directly tied to the performance of a downstream dependency parser. [sent-341, score-0.293]
</p><p>95 This evaluation metric allows for a deeper understanding of how certain normalization actions impact the output of the parser. [sent-342, score-0.579]
</p><p>96 Using this met-  ric, this work established that, when dependency parsing is the goal, typical word-to-word normalization approaches are insufficient. [sent-343, score-0.618]
</p><p>97 By taking a broader look at the normalization task, the approach presented here is able to outperform not only state-of-the-art word-to-word normalization approaches but also manual word-to-word annotations. [sent-344, score-1.083]
</p><p>98 Although the work presented here established that more than word-to-word normalization was necessary to produce parser-ready normalizations, it remains unclear which specific normalization tasks are most critical to parser performance. [sent-345, score-1.173]
</p><p>99 A hybrid rule/model-based finite-state framework for normalizing sms messages. [sent-365, score-0.379]
</p><p>100 A character-level machine translation approach for normalization of SMS abbreviations. [sent-432, score-0.51]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('normalization', 0.51), ('generators', 0.39), ('sms', 0.297), ('replacement', 0.26), ('replacements', 0.171), ('twitter', 0.151), ('downstream', 0.128), ('xr', 0.12), ('informal', 0.096), ('xi', 0.095), ('legal', 0.093), ('punctuation', 0.09), ('assignment', 0.089), ('ygiold', 0.085), ('normalizing', 0.082), ('generic', 0.08), ('normalizer', 0.076), ('pennell', 0.076), ('choudhury', 0.072), ('yunyao', 0.07), ('han', 0.069), ('giold', 0.064), ('hertz', 0.064), ('gold', 0.063), ('generator', 0.061), ('messages', 0.06), ('subjects', 0.059), ('insertion', 0.058), ('normalized', 0.055), ('path', 0.053), ('kobus', 0.052), ('normalizations', 0.052), ('baldwin', 0.051), ('sequence', 0.049), ('established', 0.048), ('spell', 0.047), ('follower', 0.047), ('reordering', 0.045), ('capitalization', 0.045), ('sproat', 0.045), ('feel', 0.043), ('instantiation', 0.043), ('bao', 0.043), ('beaufort', 0.043), ('benny', 0.043), ('kimelfeld', 0.043), ('reannotated', 0.043), ('treasure', 0.043), ('inspiration', 0.04), ('metric', 0.04), ('produce', 0.039), ('replaces', 0.038), ('directed', 0.038), ('edge', 0.038), ('graph', 0.038), ('deana', 0.038), ('pulls', 0.038), ('grammatically', 0.037), ('ritter', 0.037), ('message', 0.037), ('consistency', 0.037), ('unclear', 0.036), ('chiticariu', 0.035), ('fuliang', 0.035), ('monojit', 0.035), ('systemt', 0.035), ('wat', 0.035), ('broader', 0.034), ('cook', 0.034), ('ep', 0.034), ('ching', 0.033), ('proved', 0.033), ('start', 0.032), ('truth', 0.032), ('seen', 0.032), ('raw', 0.032), ('locally', 0.031), ('normalisation', 0.031), ('metrics', 0.031), ('dependency', 0.031), ('domain', 0.031), ('deletion', 0.03), ('variables', 0.03), ('liu', 0.03), ('parser', 0.03), ('customizable', 0.03), ('weng', 0.03), ('dataset', 0.03), ('parsing', 0.029), ('outperform', 0.029), ('actions', 0.029), ('xj', 0.029), ('performance', 0.029), ('afrl', 0.028), ('tokens', 0.028), ('normalize', 0.028), ('objects', 0.028), ('correction', 0.027), ('valued', 0.027), ('logs', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="37-tfidf-1" href="./acl-2013-Adaptive_Parser-Centric_Text_Normalization.html">37 acl-2013-Adaptive Parser-Centric Text Normalization</a></p>
<p>Author: Congle Zhang ; Tyler Baldwin ; Howard Ho ; Benny Kimelfeld ; Yunyao Li</p><p>Abstract: Text normalization is an important first step towards enabling many Natural Language Processing (NLP) tasks over informal text. While many of these tasks, such as parsing, perform the best over fully grammatically correct text, most existing text normalization approaches narrowly define the task in the word-to-word sense; that is, the task is seen as that of mapping all out-of-vocabulary non-standard words to their in-vocabulary standard forms. In this paper, we take a parser-centric view of normalization that aims to convert raw informal text into grammatically correct text. To understand the real effect of normalization on the parser, we tie normal- ization performance directly to parser performance. Additionally, we design a customizable framework to address the often overlooked concept of domain adaptability, and illustrate that the system allows for transfer to new domains with a minimal amount of data and effort. Our experimental study over datasets from three domains demonstrates that our approach outperforms not only the state-of-the-art wordto-word normalization techniques, but also manual word-to-word annotations.</p><p>2 0.44981983 <a title="37-tfidf-2" href="./acl-2013-Social_Text_Normalization_using_Contextual_Graph_Random_Walks.html">326 acl-2013-Social Text Normalization using Contextual Graph Random Walks</a></p>
<p>Author: Hany Hassan ; Arul Menezes</p><p>Abstract: We introduce a social media text normalization system that can be deployed as a preprocessing step for Machine Translation and various NLP applications to handle social media text. The proposed system is based on unsupervised learning of the normalization equivalences from unlabeled text. The proposed approach uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus. We show that the proposed approach has a very high precision of (92.43) and a reasonable recall of (56.4). When used as a preprocessing step for a state-of-the-art machine translation system, the translation quality on social media text improved by 6%. The proposed approach is domain and language independent and can be deployed as a preprocessing step for any NLP application to handle social media text.</p><p>3 0.090281337 <a title="37-tfidf-3" href="./acl-2013-Punctuation_Prediction_with_Transition-based_Parsing.html">288 acl-2013-Punctuation Prediction with Transition-based Parsing</a></p>
<p>Author: Dongdong Zhang ; Shuangzhi Wu ; Nan Yang ; Mu Li</p><p>Abstract: Punctuations are not available in automatic speech recognition outputs, which could create barriers to many subsequent text processing tasks. This paper proposes a novel method to predict punctuation symbols for the stream of words in transcribed speech texts. Our method jointly performs parsing and punctuation prediction by integrating a rich set of syntactic features when processing words from left to right. It can exploit a global view to capture long-range dependencies for punctuation prediction with linear complexity. The experimental results on the test data sets of IWSLT and TDT4 show that our method can achieve high-level performance in punctuation prediction over the stream of words in transcribed speech text. 1</p><p>4 0.084797584 <a title="37-tfidf-4" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>Author: Wang Ling ; Guang Xiang ; Chris Dyer ; Alan Black ; Isabel Trancoso</p><p>Abstract: In the ever-expanding sea of microblog data, there is a surprising amount of naturally occurring parallel text: some users create post multilingual messages targeting international audiences while others “retweet” translations. We present an efficient method for detecting these messages and extracting parallel segments from them. We have been able to extract over 1M Chinese-English parallel segments from Sina Weibo (the Chinese counterpart of Twitter) using only their public APIs. As a supplement to existing parallel training data, our automatically extracted parallel data yields substantial translation quality improvements in translating microblog text and modest improvements in translating edited news commentary. The resources in described in this paper are available at http://www.cs.cmu.edu/∼lingwang/utopia.</p><p>5 0.084295951 <a title="37-tfidf-5" href="./acl-2013-Detecting_Event-Related_Links_and_Sentiments_from_Social_Media_Texts.html">115 acl-2013-Detecting Event-Related Links and Sentiments from Social Media Texts</a></p>
<p>Author: Alexandra Balahur ; Hristo Tanev</p><p>Abstract: Nowadays, the importance of Social Media is constantly growing, as people often use such platforms to share mainstream media news and comment on the events that they relate to. As such, people no loger remain mere spectators to the events that happen in the world, but become part of them, commenting on their developments and the entities involved, sharing their opinions and distributing related content. This paper describes a system that links the main events detected from clusters of newspaper articles to tweets related to them, detects complementary information sources from the links they contain and subsequently applies sentiment analysis to classify them into positive, negative and neutral. In this manner, readers can follow the main events happening in the world, both from the perspective of mainstream as well as social media and the public’s perception on them. This system will be part of the EMM media monitoring framework working live and it will be demonstrated using Google Earth.</p><p>6 0.078828052 <a title="37-tfidf-6" href="./acl-2013-Learning_Non-linear_Features_for_Machine_Translation_Using_Gradient_Boosting_Machines.html">221 acl-2013-Learning Non-linear Features for Machine Translation Using Gradient Boosting Machines</a></p>
<p>7 0.078620166 <a title="37-tfidf-7" href="./acl-2013-Mining_Informal_Language_from_Chinese_Microtext%3A_Joint_Word_Recognition_and_Segmentation.html">243 acl-2013-Mining Informal Language from Chinese Microtext: Joint Word Recognition and Segmentation</a></p>
<p>8 0.077603742 <a title="37-tfidf-8" href="./acl-2013-Exploiting_Social_Media_for_Natural_Language_Processing%3A_Bridging_the_Gap_between_Language-centric_and_Real-world_Applications.html">146 acl-2013-Exploiting Social Media for Natural Language Processing: Bridging the Gap between Language-centric and Real-world Applications</a></p>
<p>9 0.077470899 <a title="37-tfidf-9" href="./acl-2013-Parsing_Graphs_with_Hyperedge_Replacement_Grammars.html">274 acl-2013-Parsing Graphs with Hyperedge Replacement Grammars</a></p>
<p>10 0.076227017 <a title="37-tfidf-10" href="./acl-2013-A_Stacking-based_Approach_to_Twitter_User_Geolocation_Prediction.html">20 acl-2013-A Stacking-based Approach to Twitter User Geolocation Prediction</a></p>
<p>11 0.072808817 <a title="37-tfidf-11" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>12 0.062322617 <a title="37-tfidf-12" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>13 0.060613599 <a title="37-tfidf-13" href="./acl-2013-Mr._MIRA%3A_Open-Source_Large-Margin_Structured_Learning_on_MapReduce.html">251 acl-2013-Mr. MIRA: Open-Source Large-Margin Structured Learning on MapReduce</a></p>
<p>14 0.059564184 <a title="37-tfidf-14" href="./acl-2013-Exploiting_Topic_based_Twitter_Sentiment_for_Stock_Prediction.html">147 acl-2013-Exploiting Topic based Twitter Sentiment for Stock Prediction</a></p>
<p>15 0.056938656 <a title="37-tfidf-15" href="./acl-2013-Cut_the_noise%3A_Mutually_reinforcing_reordering_and_alignments_for_improved_machine_translation.html">101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</a></p>
<p>16 0.055787168 <a title="37-tfidf-16" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>17 0.05538458 <a title="37-tfidf-17" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<p>18 0.055365007 <a title="37-tfidf-18" href="./acl-2013-Decipherment_Complexity_in_1%3A1_Substitution_Ciphers.html">109 acl-2013-Decipherment Complexity in 1:1 Substitution Ciphers</a></p>
<p>19 0.054793723 <a title="37-tfidf-19" href="./acl-2013-Sequential_Summarization%3A_A_New_Application_for_Timely_Updated_Twitter_Trending_Topics.html">319 acl-2013-Sequential Summarization: A New Application for Timely Updated Twitter Trending Topics</a></p>
<p>20 0.054336168 <a title="37-tfidf-20" href="./acl-2013-Improving_machine_translation_by_training_against_an_automatic_semantic_frame_based_evaluation_metric.html">195 acl-2013-Improving machine translation by training against an automatic semantic frame based evaluation metric</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.187), (1, 0.004), (2, -0.004), (3, 0.023), (4, 0.045), (5, 0.01), (6, 0.055), (7, 0.043), (8, 0.06), (9, -0.061), (10, -0.106), (11, 0.042), (12, -0.007), (13, -0.103), (14, 0.0), (15, -0.05), (16, 0.027), (17, 0.014), (18, 0.002), (19, -0.014), (20, 0.042), (21, 0.052), (22, 0.085), (23, -0.054), (24, 0.006), (25, 0.077), (26, -0.012), (27, 0.07), (28, -0.042), (29, -0.039), (30, -0.053), (31, -0.073), (32, -0.156), (33, 0.167), (34, 0.149), (35, -0.105), (36, -0.112), (37, -0.05), (38, -0.081), (39, 0.064), (40, 0.091), (41, 0.076), (42, 0.134), (43, -0.187), (44, -0.126), (45, 0.062), (46, -0.087), (47, -0.045), (48, -0.029), (49, 0.169)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9405278 <a title="37-lsi-1" href="./acl-2013-Adaptive_Parser-Centric_Text_Normalization.html">37 acl-2013-Adaptive Parser-Centric Text Normalization</a></p>
<p>Author: Congle Zhang ; Tyler Baldwin ; Howard Ho ; Benny Kimelfeld ; Yunyao Li</p><p>Abstract: Text normalization is an important first step towards enabling many Natural Language Processing (NLP) tasks over informal text. While many of these tasks, such as parsing, perform the best over fully grammatically correct text, most existing text normalization approaches narrowly define the task in the word-to-word sense; that is, the task is seen as that of mapping all out-of-vocabulary non-standard words to their in-vocabulary standard forms. In this paper, we take a parser-centric view of normalization that aims to convert raw informal text into grammatically correct text. To understand the real effect of normalization on the parser, we tie normal- ization performance directly to parser performance. Additionally, we design a customizable framework to address the often overlooked concept of domain adaptability, and illustrate that the system allows for transfer to new domains with a minimal amount of data and effort. Our experimental study over datasets from three domains demonstrates that our approach outperforms not only the state-of-the-art wordto-word normalization techniques, but also manual word-to-word annotations.</p><p>2 0.91587335 <a title="37-lsi-2" href="./acl-2013-Social_Text_Normalization_using_Contextual_Graph_Random_Walks.html">326 acl-2013-Social Text Normalization using Contextual Graph Random Walks</a></p>
<p>Author: Hany Hassan ; Arul Menezes</p><p>Abstract: We introduce a social media text normalization system that can be deployed as a preprocessing step for Machine Translation and various NLP applications to handle social media text. The proposed system is based on unsupervised learning of the normalization equivalences from unlabeled text. The proposed approach uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus. We show that the proposed approach has a very high precision of (92.43) and a reasonable recall of (56.4). When used as a preprocessing step for a state-of-the-art machine translation system, the translation quality on social media text improved by 6%. The proposed approach is domain and language independent and can be deployed as a preprocessing step for any NLP application to handle social media text.</p><p>3 0.52884334 <a title="37-lsi-3" href="./acl-2013-%22Let_Everything_Turn_Well_in_Your_Wife%22%3A_Generation_of_Adult_Humor_Using_Lexical_Constraints.html">1 acl-2013-"Let Everything Turn Well in Your Wife": Generation of Adult Humor Using Lexical Constraints</a></p>
<p>Author: Alessandro Valitutti ; Hannu Toivonen ; Antoine Doucet ; Jukka M. Toivanen</p><p>Abstract: We propose a method for automated generation of adult humor by lexical replacement and present empirical evaluation results of the obtained humor. We propose three types of lexical constraints as building blocks of humorous word substitution: constraints concerning the similarity of sounds or spellings of the original word and the substitute, a constraint requiring the substitute to be a taboo word, and constraints concerning the position and context of the replacement. Empirical evidence from extensive user studies indicates that these constraints can increase the effectiveness of humor generation significantly.</p><p>4 0.45844001 <a title="37-lsi-4" href="./acl-2013-Parsing_Graphs_with_Hyperedge_Replacement_Grammars.html">274 acl-2013-Parsing Graphs with Hyperedge Replacement Grammars</a></p>
<p>Author: David Chiang ; Jacob Andreas ; Daniel Bauer ; Karl Moritz Hermann ; Bevan Jones ; Kevin Knight</p><p>Abstract: Hyperedge replacement grammar (HRG) is a formalism for generating and transforming graphs that has potential applications in natural language understanding and generation. A recognition algorithm due to Lautemann is known to be polynomial-time for graphs that are connected and of bounded degree. We present a more precise characterization of the algorithm’s complexity, an optimization analogous to binarization of contextfree grammars, and some important implementation details, resulting in an algorithm that is practical for natural-language applications. The algorithm is part of Bolinas, a new software toolkit for HRG processing.</p><p>5 0.45096502 <a title="37-lsi-5" href="./acl-2013-Exploiting_Social_Media_for_Natural_Language_Processing%3A_Bridging_the_Gap_between_Language-centric_and_Real-world_Applications.html">146 acl-2013-Exploiting Social Media for Natural Language Processing: Bridging the Gap between Language-centric and Real-world Applications</a></p>
<p>Author: Simone Paolo Ponzetto ; Andrea Zielinski</p><p>Abstract: unkown-abstract</p><p>6 0.43093762 <a title="37-lsi-6" href="./acl-2013-Unsupervised_joke_generation_from_big_data.html">371 acl-2013-Unsupervised joke generation from big data</a></p>
<p>7 0.42500779 <a title="37-lsi-7" href="./acl-2013-BRAINSUP%3A_Brainstorming_Support_for_Creative_Sentence_Generation.html">65 acl-2013-BRAINSUP: Brainstorming Support for Creative Sentence Generation</a></p>
<p>8 0.39950326 <a title="37-lsi-8" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>9 0.39820066 <a title="37-lsi-9" href="./acl-2013-Scalable_Modified_Kneser-Ney_Language_Model_Estimation.html">308 acl-2013-Scalable Modified Kneser-Ney Language Model Estimation</a></p>
<p>10 0.39613202 <a title="37-lsi-10" href="./acl-2013-Computerized_Analysis_of_a_Verbal_Fluency_Test.html">89 acl-2013-Computerized Analysis of a Verbal Fluency Test</a></p>
<p>11 0.39304286 <a title="37-lsi-11" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>12 0.37626991 <a title="37-lsi-12" href="./acl-2013-Smatch%3A_an_Evaluation_Metric_for_Semantic_Feature_Structures.html">324 acl-2013-Smatch: an Evaluation Metric for Semantic Feature Structures</a></p>
<p>13 0.37466335 <a title="37-lsi-13" href="./acl-2013-Exploring_Word_Order_Universals%3A_a_Probabilistic_Graphical_Model_Approach.html">149 acl-2013-Exploring Word Order Universals: a Probabilistic Graphical Model Approach</a></p>
<p>14 0.37466219 <a title="37-lsi-14" href="./acl-2013-Connotation_Lexicon%3A_A_Dash_of_Sentiment_Beneath_the_Surface_Meaning.html">91 acl-2013-Connotation Lexicon: A Dash of Sentiment Beneath the Surface Meaning</a></p>
<p>15 0.37449843 <a title="37-lsi-15" href="./acl-2013-Random_Walk_Factoid_Annotation_for_Collective_Discourse.html">293 acl-2013-Random Walk Factoid Annotation for Collective Discourse</a></p>
<p>16 0.37240842 <a title="37-lsi-16" href="./acl-2013-Detecting_Chronic_Critics_Based_on_Sentiment_Polarity_and_User%C3%A2%E2%80%A2%C5%BDs_Behavior_in_Social_Media.html">114 acl-2013-Detecting Chronic Critics Based on Sentiment Polarity and Userâ•Žs Behavior in Social Media</a></p>
<p>17 0.36841512 <a title="37-lsi-17" href="./acl-2013-An_Open_Source_Toolkit_for_Quantitative_Historical_Linguistics.html">48 acl-2013-An Open Source Toolkit for Quantitative Historical Linguistics</a></p>
<p>18 0.36681944 <a title="37-lsi-18" href="./acl-2013-From_Natural_Language_Specifications_to_Program_Input_Parsers.html">163 acl-2013-From Natural Language Specifications to Program Input Parsers</a></p>
<p>19 0.36094832 <a title="37-lsi-19" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<p>20 0.35700577 <a title="37-lsi-20" href="./acl-2013-Resolving_Entity_Morphs_in_Censored_Data.html">301 acl-2013-Resolving Entity Morphs in Censored Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.047), (6, 0.032), (11, 0.044), (15, 0.014), (24, 0.041), (26, 0.065), (35, 0.058), (37, 0.011), (42, 0.048), (48, 0.034), (70, 0.035), (88, 0.015), (90, 0.02), (95, 0.461)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99469793 <a title="37-lda-1" href="./acl-2013-Improving_machine_translation_by_training_against_an_automatic_semantic_frame_based_evaluation_metric.html">195 acl-2013-Improving machine translation by training against an automatic semantic frame based evaluation metric</a></p>
<p>Author: Chi-kiu Lo ; Karteek Addanki ; Markus Saers ; Dekai Wu</p><p>Abstract: We present the first ever results showing that tuning a machine translation system against a semantic frame based objective function, MEANT, produces more robustly adequate translations than tuning against BLEU or TER as measured across commonly used metrics and human subjective evaluation. Moreover, for informal web forum data, human evaluators preferred MEANT-tuned systems over BLEU- or TER-tuned systems by a significantly wider margin than that for formal newswire—even though automatic semantic parsing might be expected to fare worse on informal language. We argue thatbypreserving the meaning ofthe trans- lations as captured by semantic frames right in the training process, an MT system is constrained to make more accurate choices of both lexical and reordering rules. As a result, MT systems tuned against semantic frame based MT evaluation metrics produce output that is more adequate. Tuning a machine translation system against a semantic frame based objective function is independent ofthe translation model paradigm, so, any translation model can benefit from the semantic knowledge incorporated to improve translation adequacy through our approach.</p><p>2 0.99402153 <a title="37-lda-2" href="./acl-2013-Named_Entity_Recognition_using_Cross-lingual_Resources%3A_Arabic_as_an_Example.html">256 acl-2013-Named Entity Recognition using Cross-lingual Resources: Arabic as an Example</a></p>
<p>Author: Kareem Darwish</p><p>Abstract: Some languages lack large knowledge bases and good discriminative features for Name Entity Recognition (NER) that can generalize to previously unseen named entities. One such language is Arabic, which: a) lacks a capitalization feature; and b) has relatively small knowledge bases, such as Wikipedia. In this work we address both problems by incorporating cross-lingual features and knowledge bases from English using cross-lingual links. We show that such features have a dramatic positive effect on recall. We show the effectiveness of cross-lingual features and resources on a standard dataset as well as on two new test sets that cover both news and microblogs. On the standard dataset, we achieved a 4.1% relative improvement in Fmeasure over the best reported result in the literature. The features led to improvements of 17.1% and 20.5% on the new news and mi- croblogs test sets respectively.</p><p>3 0.99194676 <a title="37-lda-3" href="./acl-2013-Translating_Dialectal_Arabic_to_English.html">359 acl-2013-Translating Dialectal Arabic to English</a></p>
<p>Author: Hassan Sajjad ; Kareem Darwish ; Yonatan Belinkov</p><p>Abstract: We present a dialectal Egyptian Arabic to English statistical machine translation system that leverages dialectal to Modern Standard Arabic (MSA) adaptation. In contrast to previous work, we first narrow down the gap between Egyptian and MSA by applying an automatic characterlevel transformational model that changes Egyptian to EG0, which looks similar to MSA. The transformations include morphological, phonological and spelling changes. The transformation reduces the out-of-vocabulary (OOV) words from 5.2% to 2.6% and gives a gain of 1.87 BLEU points. Further, adapting large MSA/English parallel data increases the lexical coverage, reduces OOVs to 0.7% and leads to an absolute BLEU improvement of 2.73 points.</p><p>4 0.98838508 <a title="37-lda-4" href="./acl-2013-Syntactic_Patterns_versus_Word_Alignment%3A_Extracting_Opinion_Targets_from_Online_Reviews.html">336 acl-2013-Syntactic Patterns versus Word Alignment: Extracting Opinion Targets from Online Reviews</a></p>
<p>Author: Kang Liu ; Liheng Xu ; Jun Zhao</p><p>Abstract: Mining opinion targets is a fundamental and important task for opinion mining from online reviews. To this end, there are usually two kinds of methods: syntax based and alignment based methods. Syntax based methods usually exploited syntactic patterns to extract opinion targets, which were however prone to suffer from parsing errors when dealing with online informal texts. In contrast, alignment based methods used word alignment model to fulfill this task, which could avoid parsing errors without using parsing. However, there is no research focusing on which kind of method is more better when given a certain amount of reviews. To fill this gap, this paper empiri- cally studies how the performance of these two kinds of methods vary when changing the size, domain and language of the corpus. We further combine syntactic patterns with alignment model by using a partially supervised framework and investigate whether this combination is useful or not. In our experiments, we verify that our combination is effective on the corpus with small and medium size.</p><p>5 0.98285192 <a title="37-lda-5" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>Author: Tsutomu Hirao ; Tomoharu Iwata ; Masaaki Nagata</p><p>Abstract: Unsupervised object matching (UOM) is a promising approach to cross-language natural language processing such as bilingual lexicon acquisition, parallel corpus construction, and cross-language text categorization, because it does not require labor-intensive linguistic resources. However, UOM only finds one-to-one correspondences from data sets with the same number of instances in source and target domains, and this prevents us from applying UOM to real-world cross-language natural language processing tasks. To alleviate these limitations, we proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic space. We demonstrate the effectiveness of our method on cross-language text categorization. The results show that our method outperforms conventional unsupervised object matching methods.</p><p>same-paper 6 0.98027241 <a title="37-lda-6" href="./acl-2013-Adaptive_Parser-Centric_Text_Normalization.html">37 acl-2013-Adaptive Parser-Centric Text Normalization</a></p>
<p>7 0.96523964 <a title="37-lda-7" href="./acl-2013-Beam_Search_for_Solving_Substitution_Ciphers.html">66 acl-2013-Beam Search for Solving Substitution Ciphers</a></p>
<p>8 0.96360075 <a title="37-lda-8" href="./acl-2013-FrameNet_on_the_Way_to_Babel%3A_Creating_a_Bilingual_FrameNet_Using_Wiktionary_as_Interlingual_Connection.html">162 acl-2013-FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection</a></p>
<p>9 0.94473606 <a title="37-lda-9" href="./acl-2013-QuEst_-_A_translation_quality_estimation_framework.html">289 acl-2013-QuEst - A translation quality estimation framework</a></p>
<p>10 0.8867051 <a title="37-lda-10" href="./acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language.html">374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</a></p>
<p>11 0.88188082 <a title="37-lda-11" href="./acl-2013-English-to-Russian_MT_evaluation_campaign.html">135 acl-2013-English-to-Russian MT evaluation campaign</a></p>
<p>12 0.87762803 <a title="37-lda-12" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>13 0.86974537 <a title="37-lda-13" href="./acl-2013-Sentence_Level_Dialect_Identification_in_Arabic.html">317 acl-2013-Sentence Level Dialect Identification in Arabic</a></p>
<p>14 0.85832644 <a title="37-lda-14" href="./acl-2013-Social_Text_Normalization_using_Contextual_Graph_Random_Walks.html">326 acl-2013-Social Text Normalization using Contextual Graph Random Walks</a></p>
<p>15 0.85183161 <a title="37-lda-15" href="./acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</a></p>
<p>16 0.85127807 <a title="37-lda-16" href="./acl-2013-Language_Independent_Connectivity_Strength_Features_for_Phrase_Pivot_Statistical_Machine_Translation.html">214 acl-2013-Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation</a></p>
<p>17 0.8468141 <a title="37-lda-17" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>18 0.84456235 <a title="37-lda-18" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>19 0.8438186 <a title="37-lda-19" href="./acl-2013-A_New_Syntactic_Metric_for_Evaluation_of_Machine_Translation.html">13 acl-2013-A New Syntactic Metric for Evaluation of Machine Translation</a></p>
<p>20 0.82903051 <a title="37-lda-20" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
