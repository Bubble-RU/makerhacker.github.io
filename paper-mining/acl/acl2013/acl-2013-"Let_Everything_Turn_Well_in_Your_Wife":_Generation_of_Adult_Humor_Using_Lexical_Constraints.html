<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1 acl-2013-"Let Everything Turn Well in Your Wife": Generation of Adult Humor Using Lexical Constraints</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-1" href="#">acl2013-1</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1 acl-2013-"Let Everything Turn Well in Your Wife": Generation of Adult Humor Using Lexical Constraints</h1>
<br/><p>Source: <a title="acl-2013-1-pdf" href="http://aclweb.org/anthology//P/P13/P13-2044.pdf">pdf</a></p><p>Author: Alessandro Valitutti ; Hannu Toivonen ; Antoine Doucet ; Jukka M. Toivanen</p><p>Abstract: We propose a method for automated generation of adult humor by lexical replacement and present empirical evaluation results of the obtained humor. We propose three types of lexical constraints as building blocks of humorous word substitution: constraints concerning the similarity of sounds or spellings of the original word and the substitute, a constraint requiring the substitute to be a taboo word, and constraints concerning the position and context of the replacement. Empirical evidence from extensive user studies indicates that these constraints can increase the effectiveness of humor generation significantly.</p><p>Reference: <a title="acl-2013-1-reference" href="../acl2013_reference/acl-2013-%22Let_Everything_Turn_Well_in_Your_Wife%22%3A_Generation_of_Adult_Humor_Using_Lexical_Constraints_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Empirical evidence from extensive user studies indicates that these constraints can increase the effectiveness of humor generation significantly. [sent-3, score-0.546]
</p><p>2 1 Introduction Incongruity and taboo meanings are typical ingredients of humor. [sent-4, score-0.745]
</p><p>3 When used in the proper context, the expression of contrasting or odd meanings can induce surprise, confusion or embarrassment and, thus, make people laugh. [sent-5, score-0.09]
</p><p>4 While methods from computational linguistics can be used to estimate the capability of words and phrases to induce incongruity or to evoke taboo meanings, computational generation of humorous texts has remained a great challenge. [sent-6, score-1.251]
</p><p>5 In this paper we propose a method for automated generation of adult humor by lexical re-  placement. [sent-7, score-0.497]
</p><p>6 We consider a setting where a short text is provided to the system, such as an instant message, and the task is to make the text funny by replacing one word in it. [sent-8, score-0.097]
</p><p>7 Toivanen Department of Computer Science and HIIT University of Helsinki, Finland on careful introduction of incongruity and taboo words to induce humor. [sent-10, score-0.934]
</p><p>8 We propose three types of lexical constraints as building blocks of humorous word substitution. [sent-11, score-0.372]
</p><p>9 (1) The form constraints turn the text into a pun. [sent-12, score-0.158]
</p><p>10 The constraints thus concern the similarity of sounds or spellings of the original word and the substitute. [sent-13, score-0.146]
</p><p>11 (2) The taboo constraint requires the substitute to be a taboo word. [sent-14, score-1.518]
</p><p>12 We hypothesize that the effectiveness of humorous lexical replacement can be increased with the introduction of taboo constraints. [sent-16, score-1.025]
</p><p>13 (3) Finally, the context constraints concern the position and context of the replacement. [sent-17, score-0.105]
</p><p>14 Our assumption is that a suitably positioned substitution propagates the tabooness (defined here as the capability to evoke taboo meanings) to phrase level and amplifies the semantic contrast with the original text. [sent-18, score-0.873]
</p><p>15 Our second concrete hypothesis is that the context constraints further boost the funniness. [sent-19, score-0.105]
</p><p>16 We evaluated the above hypotheses empirically by generating 300 modified versions of SMS messages and having each of them evaluated by 90 subjects using a crowdsourcing platform. [sent-20, score-0.189]
</p><p>17 The results show a statistically highly significant increase of funniness and agreement with the use of the humorous lexical constraints. [sent-21, score-0.474]
</p><p>18 In Section 2, we give a short overview of theoretical background and related work on humor gener-  ation. [sent-23, score-0.372]
</p><p>19 In Section 3, we present the three types of constraints for lexical replacement to induce humor. [sent-24, score-0.224]
</p><p>20 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 243–248, 2  Background  Humor, Incongruity and Tabooness A set of theories known as incongruity theory is probably the most influential approach to the study of humor and laughter. [sent-29, score-0.569]
</p><p>21 The concept of incongruity, first described by Beattie (1971), is related to the perception of incoherence, semantic contrast, or inappropriateness, even though there is no precise and agreed definition. [sent-30, score-0.028]
</p><p>22 Raskin (1985) formulated the incongruity concept in terms of script opposition. [sent-31, score-0.197]
</p><p>23 A cognitive treatment of incongruity in humor is described by Summerfelt et al. [sent-33, score-0.569]
</p><p>24 One specific form of jokes frequently discussed in the literature consists of the so called forced  reinterpretation jokes. [sent-35, score-0.208]
</p><p>25 In his integrative approach to humor theories, Martin (2007) discusses the connection between tabooness and incongruity resolution. [sent-44, score-0.678]
</p><p>26 , 1972; Attardo and Raskin, 1991), according to which “the purpose of aggressive and  sexual elements in jokes is to make salient the information needed to resolve the incongruity”. [sent-46, score-0.105]
</p><p>27 Humor Generation In previous research on computational humor generation, puns are often used as the core of more complex humorous texts, for example as punchlines of simple jokes (Raskin and Attardo, 1994; Levison and Lessard, 1992; Venour, 1999; McKay, 2002). [sent-47, score-0.675]
</p><p>28 This differs from our setting, where we transform an existing short text into a punning statement. [sent-48, score-0.067]
</p><p>29 Only few humor generation systems have been empirically evaluated. [sent-49, score-0.468]
</p><p>30 HAHAcronym (Stock and Strapparava, 2002) automatically generates humorous versions of existing acronyms, or produces a new funny acronym, starting with concepts provided by the user. [sent-52, score-0.305]
</p><p>31 Below, we will present an approach to evaluation that allows comparison of different systems in the same generation task. [sent-54, score-0.069]
</p><p>32 To m‘liafek’e →it funny, )th, ean nwdo rredt replacement itisn performed according to a number of lexical constraints, to be described below. [sent-61, score-0.09]
</p><p>33 ” The task of humor generation is thus reduced to a task of lexical selection. [sent-63, score-0.464]
</p><p>34 The adopted task for humor generation is an extension of the one described by Valitutti (201 1). [sent-64, score-0.441]
</p><p>35 We define three types of lexical constraints for this task, which will be described next. [sent-65, score-0.128]
</p><p>36 1 Form Constraints Form constraints (FORM) require that the original word and its substitute are similar in form. [sent-67, score-0.173]
</p><p>37 This turns the text given as input into a kind of pun,  “text which relies crucially on phonetic similarity for its humorous effect” (Ritchie, 2005). [sent-68, score-0.227]
</p><p>38 Obviously, simply replacing a word potentially results in a text that induces “conflict” (and confusion) in the audience. [sent-69, score-0.042]
</p><p>39 Using a phonetically similar word as a replacement, however, makes the statement pseudo-ambiguous, since the original intended meaning can also be recovered. [sent-70, score-0.035]
</p><p>40 There then are two “conflicting” and “contrasting” interpretations the literal one and the original one increasing the likelihood of humorous incongruity. [sent-71, score-0.227]
</p><p>41 Requiring the substitute to share part-of-speech with the original word works in this direction too, and additionally increases the likelihood that the resulting text is a valid English statement. [sent-72, score-0.09]
</p><p>42 —  —  244  Implementation We adopt an extended definition of punning and also consider orthographically similar or rhyming words as possible substitutes. [sent-73, score-0.105]
</p><p>43 Two words are considered orthographically similar if one word is obtained with a single character deletion, addition, or replacement from the  other one. [sent-74, score-0.105]
</p><p>44 We call two words phonetically similar if their phonetic transcription is orthographically similar according to the above definition. [sent-75, score-0.073]
</p><p>45 Two words rhyme if they have same positions of tonic accent, and if they are phonetically identical from the most stressed syllable to the end of the word. [sent-76, score-0.035]
</p><p>46 Our implementation of these constraints uses the WordNet lexical database (Fellbaum, 1998) and CMU pronunciation dictionary1 . [sent-77, score-0.128]
</p><p>47 2  Taboo Constraint  Taboo constraint (TABOO) requires that the substitute word is a taboo word or frequently used in taboo expressions, insults, or vulgar expres-  sions. [sent-82, score-1.518]
</p><p>48 Taboo words “represent a class of emotionally arousing references with respect to body products, body parts, sexual acts, ethnic or racial insults, profanity, vulgarity, slang, and scatology” (Jay et al. [sent-83, score-0.029]
</p><p>49 Implementation We collected a list of 700 taboo words. [sent-85, score-0.708]
</p><p>50 Finally, a third subset was collected from a website posting examples of funny autocorrection mistakes3 and includes words that are not directly referring to taboos (e. [sent-88, score-0.078]
</p><p>51 3  Contextual Constraints  Contextual constraints (CONT) require that the substitution takes place at the end of the text, and in a locally coherent manner. [sent-103, score-0.167]
</p><p>52 By local coherence we mean that the substitute word forms a feasible phrase with its immediate predecessor. [sent-104, score-0.091]
</p><p>53 On the other hand, if this is the case, then the taboo meaning is potentially expanded to the phrase level. [sent-106, score-0.731]
</p><p>54 The semantic contrast is potentially even stronger if the taboo word comes as a surprise in the end of a seemingly innocent text. [sent-108, score-0.756]
</p><p>55 The humorous effect then is similar to the one of the forced reinterpretation jokes. [sent-109, score-0.326]
</p><p>56 , 2011) and compute the cohesion of each n-gram, by comparing their expected frequency (assuming word indepence), to their observed number of occurrences. [sent-113, score-0.025]
</p><p>57 A subsequent Student t-test allows to assign a measure of cohesion to each n-gram (Doucet and Ahonen-Myka, 2006). [sent-114, score-0.025]
</p><p>58 We use a substitute word only if its cohesion with the previous word is high. [sent-115, score-0.093]
</p><p>59 4  Evaluation  We evaluated the method empirically using CrowdFlower5, a crowdsourcing service. [sent-118, score-0.074]
</p><p>60 The aim of the evaluation is to measure the potential effect of the three types of constraints on funniness of texts. [sent-119, score-0.346]
</p><p>61 In particular, we test the potential effect of  4available at http : / /books . [sent-120, score-0.037]
</p><p>62 com 245  adding the tabooness constraint to the form constraints, and the potential effect of further adding contextual constraints. [sent-124, score-0.224]
</p><p>63 , we consider three increasingly constrained conditions: (1) substitution according only to the form constraints (FORM), (2) substitution according to both form and taboo constraints (FORM+TABOO), and (3) substitution according to form, taboo and context constraints (FORM+TABOO+CONT). [sent-127, score-2.023]
</p><p>64 One of the reasons for the choice of taboo words as lexical constraint is that they allows the system to generate humorous text potentially appreciated by young adults, which are the majority of crowdsourcing users (Ross et al. [sent-128, score-1.062]
</p><p>65 We applied the humor generation method on the first 5000 messages of NUS SMS Corpus6, a corpus of real SMS messages (Chen and Kan, 2012). [sent-130, score-0.625]
</p><p>66 We carried out every possible lexical replacement under each of the three conditions mentioned  above, one at a time, so that the resulting messages have exactly one word substituted. [sent-131, score-0.198]
</p><p>67 We then randomly picked 100 such modified messages for each ofthe conditions. [sent-132, score-0.092]
</p><p>68 Table 1shows two example outputs of the humor generator under each of the three experimental conditions. [sent-133, score-0.372]
</p><p>69 These two examples are the least funny and the funniest message according to the empirical evaluation (see below). [sent-134, score-0.113]
</p><p>70 For evaluation, this dataset of 300 messages was randomly divided into groups of 20 messages each. [sent-135, score-0.184]
</p><p>71 We recruited 208 evaluators using the crowdsourcing service, asking each subject to evaluate one such group of 20 messages. [sent-136, score-0.047]
</p><p>72 Each message in each group was judged by 90 different participants. [sent-137, score-0.035]
</p><p>73 We asked subjects to assess individual messages for their funniness on a scale from 1 to 5. [sent-138, score-0.319]
</p><p>74 For the analysis of the results, we then measured the effectiveness of the constraints using two derived variables: the Collective Funniness (CF) of a message is its mean funniness, while its Upper Agreement (UA(t)) is the fraction of funniness scores greater than or equal to a given threshold t. [sent-139, score-0.367]
</p><p>75 In order to identify and remove potential scammers in the crowdsourcing system, we simply asked subjects to select the last word in the mes6available at  http :  / /wing . [sent-141, score-0.12]
</p><p>76 The Collective Funniness of messages increases, on average, from 2. [sent-149, score-0.092]
</p><p>77 98 when the taboo constraint is added (FORM+TABOO), and further to 3. [sent-151, score-0.742]
</p><p>78 20 when the contextual constraints are added (FORM+TABOO+CONT) (Table 2). [sent-152, score-0.123]
</p><p>79 5  Conclusions  We have proposed a new approach for the study of computational humor generation by lexical replacement. [sent-161, score-0.464]
</p><p>80 The generation task is based on a simple form of punning, where a given text is modified by replacing one word with a similar one. [sent-162, score-0.141]
</p><p>81 We proved empirically that, in this setting, humor generation is more effective when using a list  of taboo words. [sent-163, score-1.176]
</p><p>82 The other strong empirical result regards the context of substitutions: using bigrams to model people’s expectations, and constraining the position of word replacement to the end of the text, increases funniness significantly. [sent-164, score-0.293]
</p><p>83 This is likely because of the form of surprise they induce. [sent-165, score-0.078]
</p><p>84 At best of our knowledge, this is the first time that these aspects of humor generation have been successfully evaluated with a crowdsourcing system and, thus, in a relatively quick and economical way. [sent-166, score-0.488]
</p><p>85 from different sources and contains words not directly referring to taboo meanings and, thus, not widely recognizable as “taboo words”. [sent-290, score-0.77]
</p><p>86 Furthermore, the possible presence of crowd-working scammers (only partially filtered by the gold standard questions) could have reduced the statistical power of our analysis. [sent-291, score-0.033]
</p><p>87 Finally, the adopted humor generation task (based on a single word substitution) is extremely simple and the constraints might have not been sufficiently capable to produce a detectable increase of humor appreciation. [sent-292, score-0.918]
</p><p>88 In our methodology, we focused attention to the correlation between the parameters of the system (in our case, the constraints used in lexical selection) and the performance of humor generation. [sent-294, score-0.5]
</p><p>89 We used a multi-dimensional mea-  sure of humorous effect (in terms of funniness and agreement) to measure subtly different aspects of the humorous response. [sent-295, score-0.678]
</p><p>90 In the future, it would be interesting to use a similar setting to empirically investigate more subtle ways to generate humor, potentially with weaker effects but still recognizable in this setting. [sent-297, score-0.075]
</p><p>91 For instance, we would like to investigate the use of other word lists besides taboo domains and the extent to which the semantic relatedness itself could contribute to the humorous effect. [sent-298, score-0.935]
</p><p>92 One goal is to apply the proposed methodology to isolate, on one hand, parameters for inducing incongruity and, on the other hand, parameters for making the incongruity funny. [sent-301, score-0.394]
</p><p>93 Finally, we are interested in estimating the prob-  ability to induce a humor response by using different constraints. [sent-302, score-0.401]
</p><p>94 This would offer a novel way to intentionally control the humorous effect. [sent-303, score-0.227]
</p><p>95 Script theory revis(it)ed: joke similarity and joke representation model. [sent-308, score-0.076]
</p><p>96 Creating a live, public short message service corpus: The nus sms corpus. [sent-329, score-0.104]
</p><p>97 Enjoyment of specific types of humor content: Motivation or salience? [sent-351, score-0.372]
</p><p>98 McGhee, editors, The psychology of humor: Theoretical perspectives and empirical issues, pages 159–171 . [sent-356, score-0.033]
</p><p>99 The effect of humor on memory: Constrained by the pun. [sent-467, score-0.392]
</p><p>100 towards a new approach to the evaluation of computational humour generators. [sent-473, score-0.049]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('taboo', 0.708), ('humor', 0.372), ('humorous', 0.227), ('funniness', 0.204), ('incongruity', 0.197), ('ua', 0.189), ('constraints', 0.105), ('messages', 0.092), ('tabooness', 0.082), ('cont', 0.081), ('funny', 0.078), ('jokes', 0.076), ('raskin', 0.073), ('generation', 0.069), ('substitute', 0.068), ('punning', 0.067), ('replacement', 0.067), ('attardo', 0.066), ('substitution', 0.062), ('stock', 0.056), ('form', 0.053), ('collective', 0.052), ('doucet', 0.049), ('hiit', 0.049), ('humour', 0.049), ('inappropriateness', 0.049), ('reinterpretation', 0.049), ('crowdsourcing', 0.047), ('sms', 0.042), ('finland', 0.04), ('wife', 0.039), ('cf', 0.038), ('helsinki', 0.038), ('joke', 0.038), ('orthographically', 0.038), ('meanings', 0.037), ('upper', 0.036), ('message', 0.035), ('phonetically', 0.035), ('constraint', 0.034), ('adult', 0.033), ('buss', 0.033), ('levison', 0.033), ('plat', 0.033), ('scammers', 0.033), ('summerfelt', 0.033), ('psychology', 0.033), ('goldstein', 0.031), ('forced', 0.03), ('agreements', 0.03), ('cavagli', 0.029), ('hahacronym', 0.029), ('insults', 0.029), ('pun', 0.029), ('sexual', 0.029), ('induce', 0.029), ('life', 0.029), ('perception', 0.028), ('empirically', 0.027), ('integrative', 0.027), ('melt', 0.027), ('nus', 0.027), ('binsted', 0.025), ('recognizable', 0.025), ('ritchie', 0.025), ('valitutti', 0.025), ('cohesion', 0.025), ('surprise', 0.025), ('books', 0.024), ('contrasting', 0.024), ('potentially', 0.023), ('mean', 0.023), ('lexical', 0.023), ('ross', 0.023), ('sorry', 0.023), ('subjects', 0.023), ('increases', 0.022), ('evoke', 0.021), ('spellings', 0.021), ('everything', 0.021), ('pragmatics', 0.021), ('wilcoxon', 0.02), ('agreement', 0.02), ('sounds', 0.02), ('effect', 0.02), ('salience', 0.019), ('google', 0.019), ('replacing', 0.019), ('magnini', 0.019), ('strapparava', 0.019), ('jay', 0.018), ('conflict', 0.018), ('mechanisms', 0.018), ('contextual', 0.018), ('condition', 0.017), ('potential', 0.017), ('expectations', 0.017), ('blocks', 0.017), ('cmu', 0.016), ('conditions', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="1-tfidf-1" href="./acl-2013-%22Let_Everything_Turn_Well_in_Your_Wife%22%3A_Generation_of_Adult_Humor_Using_Lexical_Constraints.html">1 acl-2013-"Let Everything Turn Well in Your Wife": Generation of Adult Humor Using Lexical Constraints</a></p>
<p>Author: Alessandro Valitutti ; Hannu Toivonen ; Antoine Doucet ; Jukka M. Toivanen</p><p>Abstract: We propose a method for automated generation of adult humor by lexical replacement and present empirical evaluation results of the obtained humor. We propose three types of lexical constraints as building blocks of humorous word substitution: constraints concerning the similarity of sounds or spellings of the original word and the substitute, a constraint requiring the substitute to be a taboo word, and constraints concerning the position and context of the replacement. Empirical evidence from extensive user studies indicates that these constraints can increase the effectiveness of humor generation significantly.</p><p>2 0.18498932 <a title="1-tfidf-2" href="./acl-2013-Unsupervised_joke_generation_from_big_data.html">371 acl-2013-Unsupervised joke generation from big data</a></p>
<p>Author: Sasa Petrovic ; David Matthews</p><p>Abstract: Humor generation is a very hard problem. It is difficult to say exactly what makes a joke funny, and solving this problem algorithmically is assumed to require deep semantic understanding, as well as cultural and other contextual cues. We depart from previous work that tries to model this knowledge using ad-hoc manually created databases and labeled training examples. Instead we present a model that uses large amounts of unannotated data to generate I like my X like I like my Y, Z jokes, where X, Y, and Z are variables to be filled in. This is, to the best of our knowledge, the first fully unsupervised humor generation system. Our model significantly outperforms a competitive baseline and generates funny jokes 16% of the time, compared to 33% for human-generated jokes.</p><p>3 0.090154961 <a title="1-tfidf-3" href="./acl-2013-BRAINSUP%3A_Brainstorming_Support_for_Creative_Sentence_Generation.html">65 acl-2013-BRAINSUP: Brainstorming Support for Creative Sentence Generation</a></p>
<p>Author: Gozde Ozbal ; Daniele Pighin ; Carlo Strapparava</p><p>Abstract: Daniele Pighin Google Inc. Z ¨urich, Switzerland danie le . pighin@ gmai l com . Carlo Strapparava FBK-irst Trento, Italy st rappa@ fbk . eu you”. As another scenario, creative sentence genWe present BRAINSUP, an extensible framework for the generation of creative sentences in which users are able to force several words to appear in the sentences and to control the generation process across several semantic dimensions, namely emotions, colors, domain relatedness and phonetic properties. We evaluate its performance on a creative sentence generation task, showing its capability of generating well-formed, catchy and effective sentences that have all the good qualities of slogans produced by human copywriters.</p><p>4 0.051850028 <a title="1-tfidf-4" href="./acl-2013-Adaptive_Parser-Centric_Text_Normalization.html">37 acl-2013-Adaptive Parser-Centric Text Normalization</a></p>
<p>Author: Congle Zhang ; Tyler Baldwin ; Howard Ho ; Benny Kimelfeld ; Yunyao Li</p><p>Abstract: Text normalization is an important first step towards enabling many Natural Language Processing (NLP) tasks over informal text. While many of these tasks, such as parsing, perform the best over fully grammatically correct text, most existing text normalization approaches narrowly define the task in the word-to-word sense; that is, the task is seen as that of mapping all out-of-vocabulary non-standard words to their in-vocabulary standard forms. In this paper, we take a parser-centric view of normalization that aims to convert raw informal text into grammatically correct text. To understand the real effect of normalization on the parser, we tie normal- ization performance directly to parser performance. Additionally, we design a customizable framework to address the often overlooked concept of domain adaptability, and illustrate that the system allows for transfer to new domains with a minimal amount of data and effort. Our experimental study over datasets from three domains demonstrates that our approach outperforms not only the state-of-the-art wordto-word normalization techniques, but also manual word-to-word annotations.</p><p>5 0.042486407 <a title="1-tfidf-5" href="./acl-2013-Addressing_Ambiguity_in_Unsupervised_Part-of-Speech_Induction_with_Substitute_Vectors.html">39 acl-2013-Addressing Ambiguity in Unsupervised Part-of-Speech Induction with Substitute Vectors</a></p>
<p>Author: Volkan Cirik</p><p>Abstract: We study substitute vectors to solve the part-of-speech ambiguity problem in an unsupervised setting. Part-of-speech tagging is a crucial preliminary process in many natural language processing applications. Because many words in natural languages have more than one part-of-speech tag, resolving part-of-speech ambiguity is an important task. We claim that partof-speech ambiguity can be solved using substitute vectors. A substitute vector is constructed with possible substitutes of a target word. This study is built on previous work which has proven that word substitutes are very fruitful for part-ofspeech induction. Experiments show that our methodology works for words with high ambiguity.</p><p>6 0.038487241 <a title="1-tfidf-6" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>7 0.037459757 <a title="1-tfidf-7" href="./acl-2013-Nonconvex_Global_Optimization_for_Latent-Variable_Models.html">260 acl-2013-Nonconvex Global Optimization for Latent-Variable Models</a></p>
<p>8 0.036069427 <a title="1-tfidf-8" href="./acl-2013-TransDoop%3A_A_Map-Reduce_based_Crowdsourced_Translation_for_Complex_Domain.html">355 acl-2013-TransDoop: A Map-Reduce based Crowdsourced Translation for Complex Domain</a></p>
<p>9 0.03054825 <a title="1-tfidf-9" href="./acl-2013-Creating_Similarity%3A_Lateral_Thinking_for_Vertical_Similarity_Judgments.html">96 acl-2013-Creating Similarity: Lateral Thinking for Vertical Similarity Judgments</a></p>
<p>10 0.029907569 <a title="1-tfidf-10" href="./acl-2013-Exploiting_Topic_based_Twitter_Sentiment_for_Stock_Prediction.html">147 acl-2013-Exploiting Topic based Twitter Sentiment for Stock Prediction</a></p>
<p>11 0.027594764 <a title="1-tfidf-11" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>12 0.026466034 <a title="1-tfidf-12" href="./acl-2013-Bilingual_Lexical_Cohesion_Trigger_Model_for_Document-Level_Machine_Translation.html">69 acl-2013-Bilingual Lexical Cohesion Trigger Model for Document-Level Machine Translation</a></p>
<p>13 0.0264479 <a title="1-tfidf-13" href="./acl-2013-Computerized_Analysis_of_a_Verbal_Fluency_Test.html">89 acl-2013-Computerized Analysis of a Verbal Fluency Test</a></p>
<p>14 0.026256533 <a title="1-tfidf-14" href="./acl-2013-A_New_Set_of_Norms_for_Semantic_Relatedness_Measures.html">12 acl-2013-A New Set of Norms for Semantic Relatedness Measures</a></p>
<p>15 0.026162446 <a title="1-tfidf-15" href="./acl-2013-Crawling_microblogging_services_to_gather_language-classified_URLs._Workflow_and_case_study.html">95 acl-2013-Crawling microblogging services to gather language-classified URLs. Workflow and case study</a></p>
<p>16 0.026113963 <a title="1-tfidf-16" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>17 0.025807487 <a title="1-tfidf-17" href="./acl-2013-Implicatures_and_Nested_Beliefs_in_Approximate_Decentralized-POMDPs.html">190 acl-2013-Implicatures and Nested Beliefs in Approximate Decentralized-POMDPs</a></p>
<p>18 0.025480326 <a title="1-tfidf-18" href="./acl-2013-Diverse_Keyword_Extraction_from_Conversations.html">126 acl-2013-Diverse Keyword Extraction from Conversations</a></p>
<p>19 0.025190156 <a title="1-tfidf-19" href="./acl-2013-Semantic_Frames_to_Predict_Stock_Price_Movement.html">310 acl-2013-Semantic Frames to Predict Stock Price Movement</a></p>
<p>20 0.025080487 <a title="1-tfidf-20" href="./acl-2013-Beam_Search_for_Solving_Substitution_Ciphers.html">66 acl-2013-Beam Search for Solving Substitution Ciphers</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.072), (1, 0.021), (2, 0.001), (3, -0.02), (4, -0.005), (5, -0.02), (6, 0.029), (7, -0.005), (8, 0.006), (9, -0.003), (10, -0.033), (11, 0.021), (12, -0.019), (13, -0.044), (14, -0.022), (15, -0.034), (16, 0.003), (17, 0.001), (18, 0.012), (19, -0.01), (20, -0.026), (21, -0.017), (22, 0.028), (23, -0.014), (24, 0.009), (25, 0.041), (26, -0.009), (27, 0.019), (28, -0.013), (29, 0.02), (30, -0.0), (31, -0.045), (32, 0.019), (33, 0.002), (34, -0.008), (35, -0.025), (36, 0.032), (37, -0.024), (38, -0.009), (39, -0.008), (40, 0.052), (41, 0.08), (42, 0.002), (43, -0.07), (44, -0.064), (45, 0.057), (46, 0.033), (47, 0.032), (48, -0.097), (49, 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87124777 <a title="1-lsi-1" href="./acl-2013-%22Let_Everything_Turn_Well_in_Your_Wife%22%3A_Generation_of_Adult_Humor_Using_Lexical_Constraints.html">1 acl-2013-"Let Everything Turn Well in Your Wife": Generation of Adult Humor Using Lexical Constraints</a></p>
<p>Author: Alessandro Valitutti ; Hannu Toivonen ; Antoine Doucet ; Jukka M. Toivanen</p><p>Abstract: We propose a method for automated generation of adult humor by lexical replacement and present empirical evaluation results of the obtained humor. We propose three types of lexical constraints as building blocks of humorous word substitution: constraints concerning the similarity of sounds or spellings of the original word and the substitute, a constraint requiring the substitute to be a taboo word, and constraints concerning the position and context of the replacement. Empirical evidence from extensive user studies indicates that these constraints can increase the effectiveness of humor generation significantly.</p><p>2 0.64741904 <a title="1-lsi-2" href="./acl-2013-Tag2Blog%3A_Narrative_Generation_from_Satellite_Tag_Data.html">337 acl-2013-Tag2Blog: Narrative Generation from Satellite Tag Data</a></p>
<p>Author: Kapila Ponnamperuma ; Advaith Siddharthan ; Cheng Zeng ; Chris Mellish ; Rene van der Wal</p><p>Abstract: The aim of the Tag2Blog system is to bring satellite tagged wild animals “to life” through narratives that place their movements in an ecological context. Our motivation is to use such automatically generated texts to enhance public engagement with a specific species reintroduction programme, although the protocols developed here can be applied to any animal or other movement study that involves signal data from tags. We are working with one of the largest nature conservation charities in Europe in this regard, focusing on a single species, the red kite. We describe a system that interprets a sequence of locational fixes obtained from a satellite tagged individual, and constructs a story around its use of the landscape.</p><p>3 0.63602793 <a title="1-lsi-3" href="./acl-2013-Unsupervised_joke_generation_from_big_data.html">371 acl-2013-Unsupervised joke generation from big data</a></p>
<p>Author: Sasa Petrovic ; David Matthews</p><p>Abstract: Humor generation is a very hard problem. It is difficult to say exactly what makes a joke funny, and solving this problem algorithmically is assumed to require deep semantic understanding, as well as cultural and other contextual cues. We depart from previous work that tries to model this knowledge using ad-hoc manually created databases and labeled training examples. Instead we present a model that uses large amounts of unannotated data to generate I like my X like I like my Y, Z jokes, where X, Y, and Z are variables to be filled in. This is, to the best of our knowledge, the first fully unsupervised humor generation system. Our model significantly outperforms a competitive baseline and generates funny jokes 16% of the time, compared to 33% for human-generated jokes.</p><p>4 0.59552789 <a title="1-lsi-4" href="./acl-2013-Conditional_Random_Fields_for_Responsive_Surface_Realisation_using_Global_Features.html">90 acl-2013-Conditional Random Fields for Responsive Surface Realisation using Global Features</a></p>
<p>Author: Nina Dethlefs ; Helen Hastie ; Heriberto Cuayahuitl ; Oliver Lemon</p><p>Abstract: Surface realisers in spoken dialogue systems need to be more responsive than conventional surface realisers. They need to be sensitive to the utterance context as well as robust to partial or changing generator inputs. We formulate surface realisation as a sequence labelling task and combine the use of conditional random fields (CRFs) with semantic trees. Due to their extended notion of context, CRFs are able to take the global utterance context into account and are less constrained by local features than other realisers. This leads to more natural and less repetitive surface realisation. It also allows generation from partial and modified inputs and is therefore applicable to incremental surface realisation. Results from a human rating study confirm that users are sensitive to this extended notion of context and assign ratings that are significantly higher (up to 14%) than those for taking only local context into account.</p><p>5 0.56337303 <a title="1-lsi-5" href="./acl-2013-A_Statistical_NLG_Framework_for_Aggregated_Planning_and_Realization.html">21 acl-2013-A Statistical NLG Framework for Aggregated Planning and Realization</a></p>
<p>Author: Ravi Kondadadi ; Blake Howald ; Frank Schilder</p><p>Abstract: We present a hybrid natural language generation (NLG) system that consolidates macro and micro planning and surface realization tasks into one statistical learning process. Our novel approach is based on deriving a template bank automatically from a corpus of texts from a target domain. First, we identify domain specific entity tags and Discourse Representation Structures on a per sentence basis. Each sentence is then organized into semantically similar groups (representing a domain specific concept) by k-means clustering. After this semi-automatic processing (human review of cluster assignments), a number of corpus–level statistics are compiled and used as features by a ranking SVM to develop model weights from a training corpus. At generation time, a set of input data, the collection of semantically organized templates, and the model weights are used to select optimal templates. Our system is evaluated with automatic, non–expert crowdsourced and expert evaluation metrics. We also introduce a novel automatic metric syntactic variability that represents linguistic variation as a measure of unique template sequences across a collection of automatically generated documents. The metrics for generated weather and biography texts fall within acceptable ranges. In sum, we argue that our statistical approach to NLG reduces the need for complicated knowledge-based architectures and readily adapts to different domains with reduced development time. – – *∗Ravi Kondadadi is now affiliated with Nuance Communications, Inc.</p><p>6 0.55837375 <a title="1-lsi-6" href="./acl-2013-Combining_Referring_Expression_Generation_and_Surface_Realization%3A_A_Corpus-Based_Investigation_of_Architectures.html">86 acl-2013-Combining Referring Expression Generation and Surface Realization: A Corpus-Based Investigation of Architectures</a></p>
<p>7 0.53113627 <a title="1-lsi-7" href="./acl-2013-Using_Integer_Linear_Programming_in_Concept-to-Text_Generation_to_Produce_More_Compact_Texts.html">375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</a></p>
<p>8 0.52712357 <a title="1-lsi-8" href="./acl-2013-BRAINSUP%3A_Brainstorming_Support_for_Creative_Sentence_Generation.html">65 acl-2013-BRAINSUP: Brainstorming Support for Creative Sentence Generation</a></p>
<p>9 0.52076912 <a title="1-lsi-9" href="./acl-2013-Adaptive_Parser-Centric_Text_Normalization.html">37 acl-2013-Adaptive Parser-Centric Text Normalization</a></p>
<p>10 0.45181075 <a title="1-lsi-10" href="./acl-2013-Sign_Language_Lexical_Recognition_With_Propositional_Dynamic_Logic.html">321 acl-2013-Sign Language Lexical Recognition With Propositional Dynamic Logic</a></p>
<p>11 0.43825871 <a title="1-lsi-11" href="./acl-2013-Social_Text_Normalization_using_Contextual_Graph_Random_Walks.html">326 acl-2013-Social Text Normalization using Contextual Graph Random Walks</a></p>
<p>12 0.42787218 <a title="1-lsi-12" href="./acl-2013-The_mathematics_of_language_learning.html">349 acl-2013-The mathematics of language learning</a></p>
<p>13 0.41863814 <a title="1-lsi-13" href="./acl-2013-Exploring_Word_Order_Universals%3A_a_Probabilistic_Graphical_Model_Approach.html">149 acl-2013-Exploring Word Order Universals: a Probabilistic Graphical Model Approach</a></p>
<p>14 0.41302696 <a title="1-lsi-14" href="./acl-2013-Smatch%3A_an_Evaluation_Metric_for_Semantic_Feature_Structures.html">324 acl-2013-Smatch: an Evaluation Metric for Semantic Feature Structures</a></p>
<p>15 0.40997633 <a title="1-lsi-15" href="./acl-2013-Word_Association_Profiles_and_their_Use_for_Automated_Scoring_of_Essays.html">389 acl-2013-Word Association Profiles and their Use for Automated Scoring of Essays</a></p>
<p>16 0.40152913 <a title="1-lsi-16" href="./acl-2013-Leveraging_Domain-Independent_Information_in_Semantic_Parsing.html">228 acl-2013-Leveraging Domain-Independent Information in Semantic Parsing</a></p>
<p>17 0.39930543 <a title="1-lsi-17" href="./acl-2013-Robust_multilingual_statistical_morphological_generation_models.html">303 acl-2013-Robust multilingual statistical morphological generation models</a></p>
<p>18 0.39213988 <a title="1-lsi-18" href="./acl-2013-Does_Korean_defeat_phonotactic_word_segmentation%3F.html">128 acl-2013-Does Korean defeat phonotactic word segmentation?</a></p>
<p>19 0.3891426 <a title="1-lsi-19" href="./acl-2013-Psycholinguistically_Motivated_Computational_Models_on_the_Organization_and_Processing_of_Morphologically_Complex_Words.html">286 acl-2013-Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words</a></p>
<p>20 0.38382211 <a title="1-lsi-20" href="./acl-2013-Implicatures_and_Nested_Beliefs_in_Approximate_Decentralized-POMDPs.html">190 acl-2013-Implicatures and Nested Beliefs in Approximate Decentralized-POMDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.041), (6, 0.034), (11, 0.037), (15, 0.01), (24, 0.039), (26, 0.036), (28, 0.019), (35, 0.058), (42, 0.038), (45, 0.038), (48, 0.049), (56, 0.022), (58, 0.301), (70, 0.039), (80, 0.011), (88, 0.023), (90, 0.026), (95, 0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75865966 <a title="1-lda-1" href="./acl-2013-%22Let_Everything_Turn_Well_in_Your_Wife%22%3A_Generation_of_Adult_Humor_Using_Lexical_Constraints.html">1 acl-2013-"Let Everything Turn Well in Your Wife": Generation of Adult Humor Using Lexical Constraints</a></p>
<p>Author: Alessandro Valitutti ; Hannu Toivonen ; Antoine Doucet ; Jukka M. Toivanen</p><p>Abstract: We propose a method for automated generation of adult humor by lexical replacement and present empirical evaluation results of the obtained humor. We propose three types of lexical constraints as building blocks of humorous word substitution: constraints concerning the similarity of sounds or spellings of the original word and the substitute, a constraint requiring the substitute to be a taboo word, and constraints concerning the position and context of the replacement. Empirical evidence from extensive user studies indicates that these constraints can increase the effectiveness of humor generation significantly.</p><p>2 0.58808196 <a title="1-lda-2" href="./acl-2013-Joint_Inference_for_Heterogeneous_Dependency_Parsing.html">208 acl-2013-Joint Inference for Heterogeneous Dependency Parsing</a></p>
<p>Author: Guangyou Zhou ; Jun Zhao</p><p>Abstract: This paper is concerned with the problem of heterogeneous dependency parsing. In this paper, we present a novel joint inference scheme, which is able to leverage the consensus information between heterogeneous treebanks in the parsing phase. Different from stacked learning methods (Nivre and McDonald, 2008; Martins et al., 2008), which process the dependency parsing in a pipelined way (e.g., a second level uses the first level outputs), in our method, multiple dependency parsing models are coordinated to exchange consensus information. We conduct experiments on Chinese Dependency Treebank (CDT) and Penn Chinese Treebank (CTB), experimental results show that joint infer- ence can bring significant improvements to all state-of-the-art dependency parsers.</p><p>3 0.55560803 <a title="1-lda-3" href="./acl-2013-Topic_Modeling_Based_Classification_of_Clinical_Reports.html">351 acl-2013-Topic Modeling Based Classification of Clinical Reports</a></p>
<p>Author: Efsun Sarioglu ; Kabir Yadav ; Hyeong-Ah Choi</p><p>Abstract: Kabir Yadav Emergency Medicine Department The George Washington University Washington, DC, USA kyadav@ gwu . edu Hyeong-Ah Choi Computer Science Department The George Washington University Washington, DC, USA hcho i gwu . edu @ such as recommending the need for a certain medical test while avoiding intrusive tests or medical Electronic health records (EHRs) contain important clinical information about pa- tients. Some of these data are in the form of free text and require preprocessing to be able to used in automated systems. Efficient and effective use of this data could be vital to the speed and quality of health care. As a case study, we analyzed classification of CT imaging reports into binary categories. In addition to regular text classification, we utilized topic modeling of the entire dataset in various ways. Topic modeling of the corpora provides interpretable themes that exist in these reports. Representing reports according to their topic distributions is more compact than bag-of-words representation and can be processed faster than raw text in subsequent automated processes. A binary topic model was also built as an unsupervised classification approach with the assumption that each topic corresponds to a class. And, finally an aggregate topic classifier was built where reports are classified based on a single discriminative topic that is determined from the training dataset. Our proposed topic based classifier system is shown to be competitive with existing text classification techniques and provides a more efficient and interpretable representation.</p><p>4 0.54750782 <a title="1-lda-4" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>Author: Jackie Chi Kit Cheung ; Gerald Penn</p><p>Abstract: Generative probabilistic models have been used for content modelling and template induction, and are typically trained on small corpora in the target domain. In contrast, vector space models of distributional semantics are trained on large corpora, but are typically applied to domaingeneral lexical disambiguation tasks. We introduce Distributional Semantic Hidden Markov Models, a novel variant of a hidden Markov model that integrates these two approaches by incorporating contextualized distributional semantic vectors into a generative model as observed emissions. Experiments in slot induction show that our approach yields improvements in learning coherent entity clusters in a domain. In a subsequent extrinsic evaluation, we show that these improvements are also reflected in multi-document summarization.</p><p>5 0.44406876 <a title="1-lda-5" href="./acl-2013-DErivBase%3A_Inducing_and_Evaluating_a_Derivational_Morphology_Resource_for_German.html">102 acl-2013-DErivBase: Inducing and Evaluating a Derivational Morphology Resource for German</a></p>
<p>Author: Britta Zeller ; Jan Snajder ; Sebastian Pado</p><p>Abstract: Derivational models are still an underresearched area in computational morphology. Even for German, a rather resourcerich language, there is a lack of largecoverage derivational knowledge. This paper describes a rule-based framework for inducing derivational families (i.e., clusters of lemmas in derivational relationships) and its application to create a highcoverage German resource, DERIVBASE, mapping over 280k lemmas into more than 17k non-singleton clusters. We focus on the rule component and a qualitative and quantitative evaluation. Our approach achieves up to 93% precision and 71% recall. We attribute the high precision to the fact that our rules are based on information from grammar books.</p><p>6 0.43200621 <a title="1-lda-6" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>7 0.43010181 <a title="1-lda-7" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>8 0.42927623 <a title="1-lda-8" href="./acl-2013-Embedding_Semantic_Similarity_in_Tree_Kernels_for_Domain_Adaptation_of_Relation_Extraction.html">134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</a></p>
<p>9 0.42870229 <a title="1-lda-9" href="./acl-2013-HEADY%3A_News_headline_abstraction_through_event_pattern_clustering.html">178 acl-2013-HEADY: News headline abstraction through event pattern clustering</a></p>
<p>10 0.42826328 <a title="1-lda-10" href="./acl-2013-Models_of_Translation_Competitions.html">250 acl-2013-Models of Translation Competitions</a></p>
<p>11 0.42784452 <a title="1-lda-11" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>12 0.42753765 <a title="1-lda-12" href="./acl-2013-Categorization_of_Turkish_News_Documents_with_Morphological_Analysis.html">78 acl-2013-Categorization of Turkish News Documents with Morphological Analysis</a></p>
<p>13 0.42714143 <a title="1-lda-13" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>14 0.42667359 <a title="1-lda-14" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>15 0.4265247 <a title="1-lda-15" href="./acl-2013-Unsupervised_joke_generation_from_big_data.html">371 acl-2013-Unsupervised joke generation from big data</a></p>
<p>16 0.42627308 <a title="1-lda-16" href="./acl-2013-Multimodal_DBN_for_Predicting_High-Quality_Answers_in_cQA_portals.html">254 acl-2013-Multimodal DBN for Predicting High-Quality Answers in cQA portals</a></p>
<p>17 0.42587715 <a title="1-lda-17" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<p>18 0.42551389 <a title="1-lda-18" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>19 0.4252792 <a title="1-lda-19" href="./acl-2013-Joint_Inference_for_Fine-grained_Opinion_Extraction.html">207 acl-2013-Joint Inference for Fine-grained Opinion Extraction</a></p>
<p>20 0.42493564 <a title="1-lda-20" href="./acl-2013-Modelling_Annotator_Bias_with_Multi-task_Gaussian_Processes%3A_An_Application_to_Machine_Translation_Quality_Estimation.html">248 acl-2013-Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
