<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>216 acl-2013-Large tagset labeling using Feed Forward Neural Networks. Case study on Romanian Language</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-216" href="#">acl2013-216</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>216 acl-2013-Large tagset labeling using Feed Forward Neural Networks. Case study on Romanian Language</h1>
<br/><p>Source: <a title="acl-2013-216-pdf" href="http://aclweb.org/anthology//P/P13/P13-1068.pdf">pdf</a></p><p>Author: Tiberiu Boros ; Radu Ion ; Dan Tufis</p><p>Abstract: Radu Ion Research Institute for ?????????? ???????????? ?????? Dr?????????? Romanian Academy radu@ racai . ro Dan Tufi? Research Institute for ?????????? ???????????? ?????? Dr?????????? Romanian Academy tufi s @ racai . ro Networks (Marques and Lopes, 1996) and Conditional Random Fields (CRF) (Lafferty et Standard methods for part-of-speech tagging suffer from data sparseness when used on highly inflectional languages (which require large lexical tagset inventories). For this reason, a number of alternative methods have been proposed over the years. One of the most successful methods used for this task, ?????? ?????? ??????? ??????, 1999), exploits a reduced set of tags derived by removing several recoverable features from the lexicon morpho-syntactic descriptions. A second phase is aimed at recovering the full set of morpho-syntactic features. In this paper we present an alternative method to Tiered Tagging, based on local optimizations with Neural Networks and we show how, by properly encoding the input sequence in a general Neural Network architecture, we achieve results similar to the Tiered Tagging methodology, significantly faster and without requiring extensive linguistic knowledge as implied by the previously mentioned method. 1</p><p>Reference: <a title="acl-2013-216-reference" href="../acl2013_reference/acl-2013-Large_tagset_labeling_using_Feed_Forward_Neural_Networks._Case_study_on_Romanian_Language_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Large tagset labeling using Feed Forward Neural Networks. [sent-1, score-0.122]
</p><p>2 ro  Networks (Marques and Lopes, 1996) and Conditional Random Fields (CRF) (Lafferty et  Standard methods for part-of-speech tagging suffer from data sparseness when used on highly inflectional languages (which require large lexical tagset inventories). [sent-121, score-0.316]
</p><p>3 , 1999), exploits a reduced set of tags derived by removing several recoverable features from the lexicon morpho-syntactic descriptions. [sent-148, score-0.157]
</p><p>4 1  Introduction  Part-of-speech  tagging  is a key  process  for  various tasks such as `information extraction, text-to-speech synthesis, word sense disambiguation and machine translation. [sent-151, score-0.11]
</p><p>5 It is also known as lexical ambiguity resolution and it represents the process of assigning a uniquely interpretable label to every word inside a sentence. [sent-152, score-0.08]
</p><p>6 The labels are called POS tags and the entire inventory of POS tags is called a tagset. [sent-153, score-0.118]
</p><p>7 All these methods are primarily intended for English, which uses a relatively small tagset inventory, compared to highly inflectional languages. [sent-157, score-0.173]
</p><p>8 For the later mentioned languages, the lexicon tagsets (called morphosyntactic descriptions (Calzolari and Monachini, 1995) or MSDs) may be 10-20 times or even larger than the best known tagsets for English. [sent-158, score-0.152]
</p><p>9 For instance Czech MSD tagset requires more than 3000 labels (Collins et al. [sent-159, score-0.122]
</p><p>10 The standard tagging methods, using such large tagsets, face serious data sparseness problems due to lack of statistical evidence, manifested by the non-robustness of the language models. [sent-162, score-0.143]
</p><p>11 When tagging new texts that are not in the same domain as the training data, the accuracy decreases significantly. [sent-163, score-0.211]
</p><p>12 Even tagging in-domain texts may not be satisfactorily accurate. [sent-164, score-0.11]
</p><p>13 , 1999), exploits a reduced set of tags derived by removing several recoverable features from the lexicon morpho-syntactic descriptions. [sent-194, score-0.157]
</p><p>14 According to the MULTEXT EAST lexical specifications (Erjavec and Monachini, 1997), the Romanian tagset consists of a number of 614 MSD tags (by exploiting the case and gender regular syncretism) for wordforms and 10 punctuation tags (Tufi? [sent-195, score-0.414]
</p><p>15 , 1997), which is still significantly larger than the tagset of English. [sent-197, score-0.122]
</p><p>16 As mentioned earlier, each value in the output vector corresponds to a distinct tag from tagset and the tag assigned to the current word is chosen to correspond to the maximum value inside the output vector. [sent-200, score-0.362]
</p><p>17 The previously proposed methods still suffer from the same issue of data sparseness when applied to MSD tagging. [sent-201, score-0.055]
</p><p>18 However, in our approach, we overcome the problem through a different encoding of the input data (see section 2. [sent-202, score-0.154]
</p><p>19 The power of neural networks results mainly from their ability to attain activation functions over different patterns via their learning algorithm. [sent-204, score-0.222]
</p><p>20 By properly encoding the input sequence, the network chooses which input features contribute in determining the output features for MSDs (e. [sent-205, score-0.501]
</p><p>21 1 The MSD binary encoding scheme A MSD language independently encodes a part of speech (POS) with the associated lexical attribute values as a string of positional ordered character codes (Erjavec, 2004). [sent-211, score-0.316]
</p><p>22 The first character is an upper case character denoting the ? [sent-212, score-0.072]
</p><p>23 If a specific attribute is not relevant for a language, or for a given combination of feature-? [sent-422, score-0.113]
</p><p>24 For a language which does not morphologically mark the gender and definiteness features, the earlier ? [sent-447, score-0.084]
</p><p>25 In order to derive a binary vector for each of  the 614 MSDs of Romanian we proceeded to: 1. [sent-483, score-0.048]
</p><p>26 List and sort all possible POSes of Romanian (16 POSes) and form a binary vector with 16 positions in which position k is equal 1 only if the respective MSD has the corresponding POS (i. [sent-484, score-0.135]
</p><p>27 List and sort all possible values of all lexical attributes ? [sent-487, score-0.08]
</p><p>28 for all POSes (94 values) and form another binary vector with 94 positions such that the k-th position of this vector is 1 if the respective MSD has an attribute with the corresponding value; 3. [sent-514, score-0.245]
</p><p>29 Concatenate the vectors from steps 1 and 2 and obtain the binary codification of a MSD as a 110-position binary vector. [sent-515, score-0.05]
</p><p>30 In our framework a training example consists of the features extracted for a single word inside an utterance as input and it? [sent-518, score-0.191]
</p><p>31 The features are extracted from a window of 5 words centered on the current word. [sent-520, score-0.06]
</p><p>32 A single word is characterized by a vector that encodes either its assigned MSD or its possible MSDs. [sent-521, score-0.081]
</p><p>33 To encode the possible MSDs we use equation 2, where each possible attribute a, has a single corresponding position inside the encoded vector. [sent-522, score-0.305]
</p><p>34 To be precise, for every word wk, we obtain its input features by concatenating a number of 5 vectors. [sent-541, score-0.056]
</p><p>35 The first two vectors encode the MSDs assigned to the previous two words (wk-1 and wk694  2). [sent-542, score-0.053]
</p><p>36 The next three vectors are used to encode the possible MSDs for the current word (wk) and the following two words (wk+1 and wk+2). [sent-543, score-0.086]
</p><p>37 During training, we also compute a list of suffixes with associated MSDs, which is used at run-time to build the possible MSDs vector for unknown words. [sent-544, score-0.072]
</p><p>38 When such words are found within the test data, we approximate their possible MSDs vector using a variation of the method proposed by Brants (2000). [sent-545, score-0.049]
</p><p>39 When the tagger is applied to a new utterance, the system iteratively calculates the output MSD for each individual word. [sent-546, score-0.082]
</p><p>40 is edited so it will have the value of 1 for each attribute present in its newly assigned MSD. [sent-571, score-0.145]
</p><p>41 As a consequence of encoding each individual attribute separately for MSDs, the tagger can assign new tags (that were never associated with the current word in the training corpus). [sent-572, score-0.419]
</p><p>42 Although this is a nice behavior for dealing with unknown words it is often the case that it assigns attribute values that are not valid for the wordform. [sent-573, score-0.136]
</p><p>43 When the tagger has to assign a MSD to a given word, it selects one from the possible wordform? [sent-576, score-0.076]
</p><p>44 While other network architectures such as recurrent neural networks may prove to be more suitable for this task, they are extremely hard to train, thus, we traded the advantages of such architectures for the robustness and simplicity of the feed-forward networks. [sent-601, score-0.432]
</p><p>45 (3)  Neuron output The weighted sum of all the neuron outputs from the previous layer  Based on the size of the vectors used for MSD encoding, the output layer has 110 neurons and the input layer is composed of 550 (5 x 110) neurons. [sent-618, score-1.174]
</p><p>46 In order to fully characterize our system, we took into account the following parameters: accuracy, runtime speed, training speed, hidden layer configuration and the number of optimal training iterations. [sent-619, score-0.679]
</p><p>47 For example, the accuracy, the optimal number of training iterations, the training and the runtime speed are all highly dependent on the hidden layer configuration. [sent-621, score-0.725]
</p><p>48 Small hidden layer give high training and runtime speeds, but often under-fit the data. [sent-622, score-0.554]
</p><p>49 If the hidden layer is too large, it can easily over-fit the data and also has a negative impact on the training and runtime speed. [sent-623, score-0.554]
</p><p>50 The number of optimal training iterations changes with the size of the hidden layer (larger layers usually require more training iterations). [sent-624, score-0.701]
</p><p>51 We also used an additional inflectional wordform/MSD lexicon composed of approximately 1million hand-validated entries. [sent-640, score-0.11]
</p><p>52 695  Figure 2 - 130 hidden layer network test and train set tagging accuracy as a function of the number of iterations  The first experiment was designed to determine the trade-off between the run-time speed and the size of the hidden layer. [sent-641, score-1.18]
</p><p>53 We made a series of experiments disregarding the tagging accuracy. [sent-642, score-0.11]
</p><p>54 number of neurons on the hidden layer Because, for a given number of neurons in the hidden layer, the tagging speed is independent on the tagging accuracy, we partially trained (using one iteration and only 1000 training sentences) several network configurations. [sent-644, score-1.61]
</p><p>55 hliadyde rn Tacrcauirna sceyt va clCcidruaortasi co yn  accuracy  50 70 90 110 130 150 170 190 210  99. [sent-647, score-0.062]
</p><p>56 21  Table 2 - Train and test accuracy rates for different hidden layer configurations As shown in Table 1, the runtime speed of our system shows a constant decay when we increase the hidden layer size. [sent-665, score-1.191]
</p><p>57 The same decay can be  seen in the training speed, only this time by an 1ne4. [sent-666, score-0.068]
</p><p>58 t lTwaoyaesdrodwetsh igcsn,hewdoeftoeinrsdesigvtiomiod autaepl eytrhfteor asminzae ndcoeaf operating system load on the tagger (Table 1 shows the figures). [sent-670, score-0.05]
</p><p>59 number of  networks  in 30  iterations,  various hidden layer configurations  using  (50, 70, 90,  696  110, 130, 150, 170, 190, and 210 neurons) and 5 initial random initializations of the weights. [sent-671, score-0.558]
</p><p>60 For each configuration, we stored the accuracy of reproducing the learning data (the tagging of the training corpus) and the accuracy on the unseen data (test sets). [sent-672, score-0.273]
</p><p>61 Although a hidden layer of 210 neurons did not seem to over-fit the data, we stopped the experiment, as the training time got significantly longer. [sent-674, score-0.732]
</p><p>62 The next experiment was designed to see how the number of training iterations influences the tagging performance of networks with different hidden layer configurations. [sent-675, score-0.734]
</p><p>63 Intuitively, the training process must be stopped when the network begins to over-fit the data (i. [sent-676, score-0.257]
</p><p>64 the train set accuracy increases, but the test set accuracy drops). [sent-678, score-0.124]
</p><p>65 So, the problem comes to determining which is the most stable configuration of the neural network (i. [sent-680, score-0.389]
</p><p>66 which hidden unit size will be most likely to return good results on the test set) and establish the number of iterations it takes for the system to be trained. [sent-682, score-0.306]
</p><p>67 To do this, we ran the training procedure for 100 iterations and for each training iteration, we computed the accuracy rate of every individual network on the cross-validation set (see Table 3 for the averaged values). [sent-683, score-0.409]
</p><p>68 As shown, the network configuration using 130 neurons on the hidden layer is most likely to produce better results on the cross-validation set regardless of the number of iterations. [sent-684, score-0.902]
</p><p>69 Although, some other configurations provided better figures for the maximum accuracy, their average accuracy is lower than that of the 130 hidden unit network. [sent-685, score-0.365]
</p><p>70 Other good candidates are the 90 and 110 hidden unit networks, but not the larger valued ones, which display a lower average accuracy and also significantly slower tagging speeds. [sent-686, score-0.42]
</p><p>71 The most suitable network configuration for a given task depends on the language, MSD encoding size, speed and accuracy requirements. [sent-687, score-0.526]
</p><p>72 In our own daily applications we use the 130 hidden unit network. [sent-688, score-0.221]
</p><p>73 After observing the behavior of the various networks on the crossvalidation set we determined that a good choice is to stop the training procedure after 40 iterations. [sent-689, score-0.136]
</p><p>74 The final accuracy was computed as an average between all the accuracy figures measured at the end of the training process (after 40 iterations). [sent-724, score-0.185]
</p><p>75 u (2006) presents a different approach to MSD tagging using the Maximum Entropy framework. [sent-743, score-0.11]
</p><p>76 41%), although it is arguable that our split/train procedure is not identical to the one used in his work (no details were given as how the 1/10 of the training corpus was selected). [sent-749, score-0.06]
</p><p>77 Also, our POS tagger detected cases where the annotation in the Gold Standard was erroneous. [sent-750, score-0.05]
</p><p>78 To determine what input features contribute to the selection of certain MSD attribute values, one  can analyze the weights inside the neural network and extract the input ? [sent-853, score-0.595]
</p><p>79 We used the network with 130 units on the hidden layer, which was previously trained for 100 iterations. [sent-855, score-0.396]
</p><p>80 Based on the input encoding, we divided the features into 5 groups (one group for each MSD inside the local context ? [sent-856, score-0.147]
</p><p>81 two previous MSDs, current and following two possible MSDs). [sent-857, score-0.065]
</p><p>82 For a target attribute value (noun, gender feminine, gender masculine, etc. [sent-858, score-0.281]
</p><p>83 ) and for each input group, we selected the top 3 input values which support the decision of assigning the target value to the attribute (features that increase the output value) and the top 3 features which discourage this decision (features that decrease the output value). [sent-859, score-0.404]
</p><p>84 the possible MSDs for the word at position i+1  G2: group five ? [sent-868, score-0.12]
</p><p>85 the possible MSDs for the word at position i+2 where i corresponds to the position of the word which is currently being tagged. [sent-869, score-0.148]
</p><p>86 Also, we classify the attribute values into two categories (C): (P) want to see (support the decision) and (N) ? [sent-870, score-0.136]
</p><p>87 Table 4 shows partial (G-1 G0 G1) examples of two target attribute values (cat=Noun and gender =Feminine) and their corresponding input features used for discrimination. [sent-888, score-0.276]
</p><p>88 At the next position of the target (G1) we also find a noun in genitive or dative, corresponding to a frequent construction in Romanian, ? [sent-892, score-0.095]
</p><p>89 If the neural network outputs the feminine gender to its current MSD, one may see that it 698  has actually learned the agreement rules (at least locally): the feminine gender is present both before (G-1) the target word as well as after it (G1). [sent-932, score-0.669]
</p><p>90 6  Conclusions and future work  We presented a new approach for large tagset part-of-speech tagging using neural networks. [sent-933, score-0.353]
</p><p>91 Observing which features do not participate in any decision helps design custom topologies for the Neural Network, and provides enhancements in both speed and accuracy. [sent-936, score-0.177]
</p><p>92 If one wants to process a large amount of text and is interested only in assigning grammatical categories to words, he can use a MSD encoding in which he strips off all unnecessary features. [sent-938, score-0.141]
</p><p>93 Thus, the number of necessary neurons would decrease, which assures faster training and tagging. [sent-939, score-0.253]
</p><p>94 This is of course possible in any other tagging approaches, but our framework supports this by masking attributes inside the MSD encoding configuration file, without having to change anything else in the training corpus. [sent-940, score-0.44]
</p><p>95 During testing the system only verifies if the MSD encodings are identical and the displayed accuracy directly reflects the performance of the system on the simplified tagging schema. [sent-941, score-0.203]
</p><p>96 We also proposed a methodology for selecting a network configurations (i. [sent-942, score-0.247]
</p><p>97 number of hidden  units), which best suites the application requirements. [sent-944, score-0.189]
</p><p>98 In our daily applications we use a network with 130 hidden units, as it provides an optimal speed/accuracy trade-off (approx. [sent-945, score-0.403]
</p><p>99 The tagger is implemented as part of a larger application that is primarily intended for text-tospeech (TTS) synthesis. [sent-947, score-0.05]
</p><p>100 From the tagging perspective, our future plans include testing the system on other highly inflectional languages such as Czech and  Slovene and investigating different methods for automatically determining a more suitable custom network topology, such as genetic algorithms. [sent-952, score-0.401]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('msd', 0.564), ('msds', 0.386), ('layer', 0.257), ('neurons', 0.214), ('hidden', 0.189), ('network', 0.185), ('romanian', 0.18), ('tagset', 0.122), ('neural', 0.121), ('encoding', 0.119), ('tufi', 0.115), ('tiered', 0.114), ('attribute', 0.113), ('erjavec', 0.112), ('tagging', 0.11), ('speed', 0.103), ('specifications', 0.09), ('gender', 0.084), ('feminine', 0.078), ('networks', 0.076), ('runtime', 0.069), ('monachini', 0.066), ('iterations', 0.063), ('layers', 0.063), ('accuracy', 0.062), ('position', 0.061), ('tags', 0.059), ('inside', 0.058), ('configuration', 0.057), ('racai', 0.057), ('inflectional', 0.051), ('tagger', 0.05), ('poses', 0.05), ('wk', 0.047), ('discourage', 0.043), ('dragomirescu', 0.043), ('krek', 0.043), ('lopes', 0.043), ('marques', 0.043), ('neuron', 0.043), ('recoverable', 0.043), ('tagsets', 0.042), ('current', 0.039), ('training', 0.039), ('utterance', 0.038), ('multext', 0.038), ('isbn', 0.038), ('samuelsson', 0.038), ('configurations', 0.036), ('character', 0.036), ('input', 0.035), ('dr', 0.034), ('morphosyntactic', 0.034), ('lexicon', 0.034), ('noun', 0.034), ('group', 0.033), ('sparseness', 0.033), ('academy', 0.033), ('stopped', 0.033), ('unit', 0.032), ('output', 0.032), ('brants', 0.032), ('assigned', 0.032), ('attributes', 0.031), ('slovene', 0.031), ('encodings', 0.031), ('entropy', 0.029), ('optimal', 0.029), ('topology', 0.029), ('custom', 0.029), ('decay', 0.029), ('ratnaparkhi', 0.029), ('valued', 0.027), ('calzolari', 0.027), ('contribute', 0.027), ('methodology', 0.026), ('berger', 0.026), ('determining', 0.026), ('possible', 0.026), ('composed', 0.025), ('binary', 0.025), ('architectures', 0.025), ('activation', 0.025), ('pos', 0.025), ('decision', 0.024), ('maximum', 0.024), ('values', 0.023), ('feed', 0.023), ('suffixes', 0.023), ('east', 0.023), ('vector', 0.023), ('radu', 0.023), ('previously', 0.022), ('size', 0.022), ('oov', 0.022), ('assigning', 0.022), ('figures', 0.022), ('procedure', 0.021), ('features', 0.021), ('encode', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="216-tfidf-1" href="./acl-2013-Large_tagset_labeling_using_Feed_Forward_Neural_Networks._Case_study_on_Romanian_Language.html">216 acl-2013-Large tagset labeling using Feed Forward Neural Networks. Case study on Romanian Language</a></p>
<p>Author: Tiberiu Boros ; Radu Ion ; Dan Tufis</p><p>Abstract: Radu Ion Research Institute for ?????????? ???????????? ?????? Dr?????????? Romanian Academy radu@ racai . ro Dan Tufi? Research Institute for ?????????? ???????????? ?????? Dr?????????? Romanian Academy tufi s @ racai . ro Networks (Marques and Lopes, 1996) and Conditional Random Fields (CRF) (Lafferty et Standard methods for part-of-speech tagging suffer from data sparseness when used on highly inflectional languages (which require large lexical tagset inventories). For this reason, a number of alternative methods have been proposed over the years. One of the most successful methods used for this task, ?????? ?????? ??????? ??????, 1999), exploits a reduced set of tags derived by removing several recoverable features from the lexicon morpho-syntactic descriptions. A second phase is aimed at recovering the full set of morpho-syntactic features. In this paper we present an alternative method to Tiered Tagging, based on local optimizations with Neural Networks and we show how, by properly encoding the input sequence in a general Neural Network architecture, we achieve results similar to the Tiered Tagging methodology, significantly faster and without requiring extensive linguistic knowledge as implied by the previously mentioned method. 1</p><p>2 0.1839762 <a title="216-tfidf-2" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>Author: Nan Yang ; Shujie Liu ; Mu Li ; Ming Zhou ; Nenghai Yu</p><p>Abstract: In this paper, we explore a novel bilingual word alignment approach based on DNN (Deep Neural Network), which has been proven to be very effective in various machine learning tasks (Collobert et al., 2011). We describe in detail how we adapt and extend the CD-DNNHMM (Dahl et al., 2012) method introduced in speech recognition to the HMMbased word alignment model, in which bilingual word embedding is discriminatively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences. While being capable to model the rich bilingual correspondence, our method generates a very compact model with much fewer parameters. Experiments on a large scale EnglishChinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score.</p><p>3 0.11109685 <a title="216-tfidf-3" href="./acl-2013-Combination_of_Recurrent_Neural_Networks_and_Factored_Language_Models_for_Code-Switching_Language_Modeling.html">84 acl-2013-Combination of Recurrent Neural Networks and Factored Language Models for Code-Switching Language Modeling</a></p>
<p>Author: Heike Adel ; Ngoc Thang Vu ; Tanja Schultz</p><p>Abstract: In this paper, we investigate the application of recurrent neural network language models (RNNLM) and factored language models (FLM) to the task of language modeling for Code-Switching speech. We present a way to integrate partof-speech tags (POS) and language information (LID) into these models which leads to significant improvements in terms of perplexity. Furthermore, a comparison between RNNLMs and FLMs and a detailed analysis of perplexities on the different backoff levels are performed. Finally, we show that recurrent neural networks and factored language models can . be combined using linear interpolation to achieve the best performance. The final combined language model provides 37.8% relative improvement in terms of perplexity on the SEAME development set and a relative improvement of 32.7% on the evaluation set compared to the traditional n-gram language model. Index Terms: multilingual speech processing, code switching, language modeling, recurrent neural networks, factored language models</p><p>4 0.11084805 <a title="216-tfidf-4" href="./acl-2013-Learning_Entity_Representation_for_Entity_Disambiguation.html">219 acl-2013-Learning Entity Representation for Entity Disambiguation</a></p>
<p>Author: Zhengyan He ; Shujie Liu ; Mu Li ; Ming Zhou ; Longkai Zhang ; Houfeng Wang</p><p>Abstract: We propose a novel entity disambiguation model, based on Deep Neural Network (DNN). Instead of utilizing simple similarity measures and their disjoint combinations, our method directly optimizes document and entity representations for a given similarity measure. Stacked Denoising Auto-encoders are first employed to learn an initial document representation in an unsupervised pre-training stage. A supervised fine-tuning stage follows to optimize the representation towards the similarity measure. Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches.</p><p>5 0.10604455 <a title="216-tfidf-5" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>Author: lemao liu ; Taro Watanabe ; Eiichiro Sumita ; Tiejun Zhao</p><p>Abstract: Most statistical machine translation (SMT) systems are modeled using a loglinear framework. Although the log-linear model achieves success in SMT, it still suffers from some limitations: (1) the features are required to be linear with respect to the model itself; (2) features cannot be further interpreted to reach their potential. A neural network is a reasonable method to address these pitfalls. However, modeling SMT with a neural network is not trivial, especially when taking the decoding efficiency into consideration. In this paper, we propose a variant of a neural network, i.e. additive neural networks, for SMT to go beyond the log-linear translation model. In addition, word embedding is employed as the input to the neural network, which encodes each word as a feature vector. Our model outperforms the log-linear translation models with/without embedding features on Chinese-to-English and Japanese-to-English translation tasks.</p><p>6 0.08551047 <a title="216-tfidf-6" href="./acl-2013-Multimodal_DBN_for_Predicting_High-Quality_Answers_in_cQA_portals.html">254 acl-2013-Multimodal DBN for Predicting High-Quality Answers in cQA portals</a></p>
<p>7 0.076995842 <a title="216-tfidf-7" href="./acl-2013-Using_Conceptual_Class_Attributes_to_Characterize_Social_Media_Users.html">373 acl-2013-Using Conceptual Class Attributes to Characterize Social Media Users</a></p>
<p>8 0.07507664 <a title="216-tfidf-8" href="./acl-2013-Simpler_unsupervised_POS_tagging_with_bilingual_projections.html">323 acl-2013-Simpler unsupervised POS tagging with bilingual projections</a></p>
<p>9 0.074416727 <a title="216-tfidf-9" href="./acl-2013-Adaptation_Data_Selection_using_Neural_Language_Models%3A_Experiments_in_Machine_Translation.html">35 acl-2013-Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation</a></p>
<p>10 0.074227557 <a title="216-tfidf-10" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>11 0.069982857 <a title="216-tfidf-11" href="./acl-2013-Building_and_Evaluating_a_Distributional_Memory_for_Croatian.html">76 acl-2013-Building and Evaluating a Distributional Memory for Croatian</a></p>
<p>12 0.066743672 <a title="216-tfidf-12" href="./acl-2013-Leveraging_Domain-Independent_Information_in_Semantic_Parsing.html">228 acl-2013-Leveraging Domain-Independent Information in Semantic Parsing</a></p>
<p>13 0.063371658 <a title="216-tfidf-13" href="./acl-2013-Scalable_Modified_Kneser-Ney_Language_Model_Estimation.html">308 acl-2013-Scalable Modified Kneser-Ney Language Model Estimation</a></p>
<p>14 0.060648929 <a title="216-tfidf-14" href="./acl-2013-A_Lattice-based_Framework_for_Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">7 acl-2013-A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>15 0.059578989 <a title="216-tfidf-15" href="./acl-2013-Chinese_Parsing_Exploiting_Characters.html">80 acl-2013-Chinese Parsing Exploiting Characters</a></p>
<p>16 0.057025511 <a title="216-tfidf-16" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>17 0.055165675 <a title="216-tfidf-17" href="./acl-2013-Models_of_Semantic_Representation_with_Visual_Attributes.html">249 acl-2013-Models of Semantic Representation with Visual Attributes</a></p>
<p>18 0.053088464 <a title="216-tfidf-18" href="./acl-2013-AnnoMarket%3A_An_Open_Cloud_Platform_for_NLP.html">51 acl-2013-AnnoMarket: An Open Cloud Platform for NLP</a></p>
<p>19 0.051375762 <a title="216-tfidf-19" href="./acl-2013-Mining_Informal_Language_from_Chinese_Microtext%3A_Joint_Word_Recognition_and_Segmentation.html">243 acl-2013-Mining Informal Language from Chinese Microtext: Joint Word Recognition and Segmentation</a></p>
<p>20 0.050503161 <a title="216-tfidf-20" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.141), (1, -0.016), (2, -0.022), (3, -0.001), (4, 0.016), (5, -0.052), (6, 0.007), (7, -0.009), (8, 0.01), (9, 0.054), (10, -0.055), (11, -0.079), (12, 0.036), (13, -0.101), (14, -0.032), (15, 0.027), (16, -0.06), (17, 0.006), (18, 0.017), (19, -0.188), (20, -0.047), (21, -0.033), (22, -0.037), (23, -0.053), (24, 0.069), (25, -0.011), (26, 0.082), (27, -0.114), (28, 0.054), (29, -0.06), (30, -0.137), (31, -0.068), (32, 0.009), (33, -0.022), (34, -0.005), (35, -0.021), (36, 0.009), (37, -0.101), (38, -0.037), (39, -0.052), (40, -0.03), (41, 0.056), (42, 0.004), (43, -0.028), (44, -0.001), (45, 0.002), (46, 0.014), (47, 0.041), (48, 0.036), (49, -0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93055564 <a title="216-lsi-1" href="./acl-2013-Large_tagset_labeling_using_Feed_Forward_Neural_Networks._Case_study_on_Romanian_Language.html">216 acl-2013-Large tagset labeling using Feed Forward Neural Networks. Case study on Romanian Language</a></p>
<p>Author: Tiberiu Boros ; Radu Ion ; Dan Tufis</p><p>Abstract: Radu Ion Research Institute for ?????????? ???????????? ?????? Dr?????????? Romanian Academy radu@ racai . ro Dan Tufi? Research Institute for ?????????? ???????????? ?????? Dr?????????? Romanian Academy tufi s @ racai . ro Networks (Marques and Lopes, 1996) and Conditional Random Fields (CRF) (Lafferty et Standard methods for part-of-speech tagging suffer from data sparseness when used on highly inflectional languages (which require large lexical tagset inventories). For this reason, a number of alternative methods have been proposed over the years. One of the most successful methods used for this task, ?????? ?????? ??????? ??????, 1999), exploits a reduced set of tags derived by removing several recoverable features from the lexicon morpho-syntactic descriptions. A second phase is aimed at recovering the full set of morpho-syntactic features. In this paper we present an alternative method to Tiered Tagging, based on local optimizations with Neural Networks and we show how, by properly encoding the input sequence in a general Neural Network architecture, we achieve results similar to the Tiered Tagging methodology, significantly faster and without requiring extensive linguistic knowledge as implied by the previously mentioned method. 1</p><p>2 0.83259374 <a title="216-lsi-2" href="./acl-2013-Combination_of_Recurrent_Neural_Networks_and_Factored_Language_Models_for_Code-Switching_Language_Modeling.html">84 acl-2013-Combination of Recurrent Neural Networks and Factored Language Models for Code-Switching Language Modeling</a></p>
<p>Author: Heike Adel ; Ngoc Thang Vu ; Tanja Schultz</p><p>Abstract: In this paper, we investigate the application of recurrent neural network language models (RNNLM) and factored language models (FLM) to the task of language modeling for Code-Switching speech. We present a way to integrate partof-speech tags (POS) and language information (LID) into these models which leads to significant improvements in terms of perplexity. Furthermore, a comparison between RNNLMs and FLMs and a detailed analysis of perplexities on the different backoff levels are performed. Finally, we show that recurrent neural networks and factored language models can . be combined using linear interpolation to achieve the best performance. The final combined language model provides 37.8% relative improvement in terms of perplexity on the SEAME development set and a relative improvement of 32.7% on the evaluation set compared to the traditional n-gram language model. Index Terms: multilingual speech processing, code switching, language modeling, recurrent neural networks, factored language models</p><p>3 0.71940041 <a title="216-lsi-3" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>Author: Nan Yang ; Shujie Liu ; Mu Li ; Ming Zhou ; Nenghai Yu</p><p>Abstract: In this paper, we explore a novel bilingual word alignment approach based on DNN (Deep Neural Network), which has been proven to be very effective in various machine learning tasks (Collobert et al., 2011). We describe in detail how we adapt and extend the CD-DNNHMM (Dahl et al., 2012) method introduced in speech recognition to the HMMbased word alignment model, in which bilingual word embedding is discriminatively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences. While being capable to model the rich bilingual correspondence, our method generates a very compact model with much fewer parameters. Experiments on a large scale EnglishChinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score.</p><p>4 0.70388693 <a title="216-lsi-4" href="./acl-2013-Adaptation_Data_Selection_using_Neural_Language_Models%3A_Experiments_in_Machine_Translation.html">35 acl-2013-Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation</a></p>
<p>Author: Kevin Duh ; Graham Neubig ; Katsuhito Sudoh ; Hajime Tsukada</p><p>Abstract: Data selection is an effective approach to domain adaptation in statistical machine translation. The idea is to use language models trained on small in-domain text to select similar sentences from large general-domain corpora, which are then incorporated into the training data. Substantial gains have been demonstrated in previous works, which employ standard ngram language models. Here, we explore the use of neural language models for data selection. We hypothesize that the continuous vector representation of words in neural language models makes them more effective than n-grams for modeling un- known word contexts, which are prevalent in general-domain text. In a comprehensive evaluation of 4 language pairs (English to German, French, Russian, Spanish), we found that neural language models are indeed viable tools for data selection: while the improvements are varied (i.e. 0.1 to 1.7 gains in BLEU), they are fast to train on small in-domain data and can sometimes substantially outperform conventional n-grams.</p><p>5 0.60817671 <a title="216-lsi-5" href="./acl-2013-Multimodal_DBN_for_Predicting_High-Quality_Answers_in_cQA_portals.html">254 acl-2013-Multimodal DBN for Predicting High-Quality Answers in cQA portals</a></p>
<p>Author: Haifeng Hu ; Bingquan Liu ; Baoxun Wang ; Ming Liu ; Xiaolong Wang</p><p>Abstract: In this paper, we address the problem for predicting cQA answer quality as a classification task. We propose a multimodal deep belief nets based approach that operates in two stages: First, the joint representation is learned by taking both textual and non-textual features into a deep learning network. Then, the joint representation learned by the network is used as input features for a linear classifier. Extensive experimental results conducted on two cQA datasets demonstrate the effectiveness of our proposed approach.</p><p>6 0.60246634 <a title="216-lsi-6" href="./acl-2013-Re-embedding_words.html">294 acl-2013-Re-embedding words</a></p>
<p>7 0.56390756 <a title="216-lsi-7" href="./acl-2013-Word_surprisal_predicts_N400_amplitude_during_reading.html">390 acl-2013-Word surprisal predicts N400 amplitude during reading</a></p>
<p>8 0.53766483 <a title="216-lsi-8" href="./acl-2013-Scalable_Modified_Kneser-Ney_Language_Model_Estimation.html">308 acl-2013-Scalable Modified Kneser-Ney Language Model Estimation</a></p>
<p>9 0.53537309 <a title="216-lsi-9" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>10 0.53522974 <a title="216-lsi-10" href="./acl-2013-Learning_Entity_Representation_for_Entity_Disambiguation.html">219 acl-2013-Learning Entity Representation for Entity Disambiguation</a></p>
<p>11 0.52374578 <a title="216-lsi-11" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>12 0.51243675 <a title="216-lsi-12" href="./acl-2013-Addressing_Ambiguity_in_Unsupervised_Part-of-Speech_Induction_with_Substitute_Vectors.html">39 acl-2013-Addressing Ambiguity in Unsupervised Part-of-Speech Induction with Substitute Vectors</a></p>
<p>13 0.48871565 <a title="216-lsi-13" href="./acl-2013-Modeling_of_term-distance_and_term-occurrence_information_for_improving_n-gram_language_model_performance.html">247 acl-2013-Modeling of term-distance and term-occurrence information for improving n-gram language model performance</a></p>
<p>14 0.45110849 <a title="216-lsi-14" href="./acl-2013-The_mathematics_of_language_learning.html">349 acl-2013-The mathematics of language learning</a></p>
<p>15 0.44289032 <a title="216-lsi-15" href="./acl-2013-Smoothed_marginal_distribution_constraints_for_language_modeling.html">325 acl-2013-Smoothed marginal distribution constraints for language modeling</a></p>
<p>16 0.44267994 <a title="216-lsi-16" href="./acl-2013-Exploring_Word_Order_Universals%3A_a_Probabilistic_Graphical_Model_Approach.html">149 acl-2013-Exploring Word Order Universals: a Probabilistic Graphical Model Approach</a></p>
<p>17 0.43651712 <a title="216-lsi-17" href="./acl-2013-Unsupervised_joke_generation_from_big_data.html">371 acl-2013-Unsupervised joke generation from big data</a></p>
<p>18 0.43106043 <a title="216-lsi-18" href="./acl-2013-Is_word-to-phone_mapping_better_than_phone-phone_mapping_for_handling_English_words%3F.html">203 acl-2013-Is word-to-phone mapping better than phone-phone mapping for handling English words?</a></p>
<p>19 0.42850527 <a title="216-lsi-19" href="./acl-2013-Real-World_Semi-Supervised_Learning_of_POS-Taggers_for_Low-Resource_Languages.html">295 acl-2013-Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages</a></p>
<p>20 0.40589041 <a title="216-lsi-20" href="./acl-2013-A_Novel_Classifier_Based_on_Quantum_Computation.html">14 acl-2013-A Novel Classifier Based on Quantum Computation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.079), (3, 0.227), (6, 0.033), (11, 0.051), (24, 0.046), (26, 0.093), (35, 0.07), (42, 0.056), (48, 0.045), (67, 0.047), (70, 0.045), (88, 0.027), (90, 0.025), (95, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94529724 <a title="216-lda-1" href="./acl-2013-Sign_Language_Lexical_Recognition_With_Propositional_Dynamic_Logic.html">321 acl-2013-Sign Language Lexical Recognition With Propositional Dynamic Logic</a></p>
<p>Author: Arturo Curiel ; Christophe Collet</p><p>Abstract: . This paper explores the use of Propositional Dynamic Logic (PDL) as a suitable formal framework for describing Sign Language (SL) , the language of deaf people, in the context of natural language processing. SLs are visual, complete, standalone languages which are just as expressive as oral languages. Signs in SL usually correspond to sequences of highly specific body postures interleaved with movements, which make reference to real world objects, characters or situations. Here we propose a formal representation of SL signs, that will help us with the analysis of automatically-collected hand tracking data from French Sign Language (FSL) video corpora. We further show how such a representation could help us with the design of computer aided SL verification tools, which in turn would bring us closer to the development of an automatic recognition system for these languages.</p><p>same-paper 2 0.7963208 <a title="216-lda-2" href="./acl-2013-Large_tagset_labeling_using_Feed_Forward_Neural_Networks._Case_study_on_Romanian_Language.html">216 acl-2013-Large tagset labeling using Feed Forward Neural Networks. Case study on Romanian Language</a></p>
<p>Author: Tiberiu Boros ; Radu Ion ; Dan Tufis</p><p>Abstract: Radu Ion Research Institute for ?????????? ???????????? ?????? Dr?????????? Romanian Academy radu@ racai . ro Dan Tufi? Research Institute for ?????????? ???????????? ?????? Dr?????????? Romanian Academy tufi s @ racai . ro Networks (Marques and Lopes, 1996) and Conditional Random Fields (CRF) (Lafferty et Standard methods for part-of-speech tagging suffer from data sparseness when used on highly inflectional languages (which require large lexical tagset inventories). For this reason, a number of alternative methods have been proposed over the years. One of the most successful methods used for this task, ?????? ?????? ??????? ??????, 1999), exploits a reduced set of tags derived by removing several recoverable features from the lexicon morpho-syntactic descriptions. A second phase is aimed at recovering the full set of morpho-syntactic features. In this paper we present an alternative method to Tiered Tagging, based on local optimizations with Neural Networks and we show how, by properly encoding the input sequence in a general Neural Network architecture, we achieve results similar to the Tiered Tagging methodology, significantly faster and without requiring extensive linguistic knowledge as implied by the previously mentioned method. 1</p><p>3 0.628299 <a title="216-lda-3" href="./acl-2013-Unsupervised_Consonant-Vowel_Prediction_over_Hundreds_of_Languages.html">369 acl-2013-Unsupervised Consonant-Vowel Prediction over Hundreds of Languages</a></p>
<p>Author: Young-Bum Kim ; Benjamin Snyder</p><p>Abstract: In this paper, we present a solution to one aspect of the decipherment task: the prediction of consonants and vowels for an unknown language and alphabet. Adopting a classical Bayesian perspective, we performs posterior inference over hundreds of languages, leveraging knowledge of known languages and alphabets to uncover general linguistic patterns of typologically coherent language clusters. We achieve average accuracy in the unsupervised consonant/vowel prediction task of 99% across 503 languages. We further show that our methodology can be used to predict more fine-grained phonetic distinctions. On a three-way classification task between vowels, nasals, and nonnasal consonants, our model yields unsu- pervised accuracy of 89% across the same set of languages.</p><p>4 0.61636138 <a title="216-lda-4" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>Author: Akihiro Tamura ; Taro Watanabe ; Eiichiro Sumita ; Hiroya Takamura ; Manabu Okumura</p><p>Abstract: This paper proposes a nonparametric Bayesian method for inducing Part-ofSpeech (POS) tags in dependency trees to improve the performance of statistical machine translation (SMT). In particular, we extend the monolingual infinite tree model (Finkel et al., 2007) to a bilingual scenario: each hidden state (POS tag) of a source-side dependency tree emits a source word together with its aligned target word, either jointly (joint model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forest- to-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model.</p><p>5 0.61555541 <a title="216-lda-5" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>Author: Kai Liu ; Yajuan Lu ; Wenbin Jiang ; Qun Liu</p><p>Abstract: This paper describes a novel strategy for automatic induction of a monolingual dependency grammar under the guidance of bilingually-projected dependency. By moderately leveraging the dependency information projected from the parsed counterpart language, and simultaneously mining the underlying syntactic structure of the language considered, it effectively integrates the advantages of bilingual projection and unsupervised induction, so as to induce a monolingual grammar much better than previous models only using bilingual projection or unsupervised induction. We induced dependency gram- mar for five different languages under the guidance of dependency information projected from the parsed English translation, experiments show that the bilinguallyguided method achieves a significant improvement of 28.5% over the unsupervised baseline and 3.0% over the best projection baseline on average.</p><p>6 0.61297846 <a title="216-lda-6" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>7 0.61170125 <a title="216-lda-7" href="./acl-2013-Learning_Semantic_Textual_Similarity_with_Structural_Representations.html">222 acl-2013-Learning Semantic Textual Similarity with Structural Representations</a></p>
<p>8 0.61149406 <a title="216-lda-8" href="./acl-2013-Scaling_Semi-supervised_Naive_Bayes_with_Feature_Marginals.html">309 acl-2013-Scaling Semi-supervised Naive Bayes with Feature Marginals</a></p>
<p>9 0.61094958 <a title="216-lda-9" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>10 0.610789 <a title="216-lda-10" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>11 0.6090374 <a title="216-lda-11" href="./acl-2013-Dual_Training_and_Dual_Prediction_for_Polarity_Classification.html">131 acl-2013-Dual Training and Dual Prediction for Polarity Classification</a></p>
<p>12 0.60898805 <a title="216-lda-12" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>13 0.60858947 <a title="216-lda-13" href="./acl-2013-Explicit_and_Implicit_Syntactic_Features_for_Text_Classification.html">144 acl-2013-Explicit and Implicit Syntactic Features for Text Classification</a></p>
<p>14 0.60738438 <a title="216-lda-14" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>15 0.60602337 <a title="216-lda-15" href="./acl-2013-Co-Regression_for_Cross-Language_Review_Rating_Prediction.html">81 acl-2013-Co-Regression for Cross-Language Review Rating Prediction</a></p>
<p>16 0.60597211 <a title="216-lda-16" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<p>17 0.60571152 <a title="216-lda-17" href="./acl-2013-Multimodal_DBN_for_Predicting_High-Quality_Answers_in_cQA_portals.html">254 acl-2013-Multimodal DBN for Predicting High-Quality Answers in cQA portals</a></p>
<p>18 0.60568243 <a title="216-lda-18" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>19 0.60538805 <a title="216-lda-19" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>20 0.60429907 <a title="216-lda-20" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
