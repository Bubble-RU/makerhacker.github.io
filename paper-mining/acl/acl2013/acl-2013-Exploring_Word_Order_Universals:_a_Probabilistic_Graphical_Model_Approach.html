<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>149 acl-2013-Exploring Word Order Universals: a Probabilistic Graphical Model Approach</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-149" href="#">acl2013-149</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>149 acl-2013-Exploring Word Order Universals: a Probabilistic Graphical Model Approach</h1>
<br/><p>Source: <a title="acl-2013-149-pdf" href="http://aclweb.org/anthology//P/P13/P13-3022.pdf">pdf</a></p><p>Author: Xia Lu</p><p>Abstract: In this paper we propose a probabilistic graphical model as an innovative framework for studying typological universals. We view language as a system and linguistic features as its components whose relationships are encoded in a Directed Acyclic Graph (DAG). Taking discovery of the word order universals as a knowledge discovery task we learn the graphical representation of a word order sub-system which reveals a finer structure such as direct and indirect dependencies among word order features. Then probabilistic inference enables us to see the strength of such relationships: given the observed value of one feature (or combination of features), the probabilities of values of other features can be calculated. Our model is not restricted to using only two values of a feature. Using imputation technique and EM algorithm it can handle missing val- ues well. Model averaging technique solves the problem of limited data. In addition the incremental and divide-and-conquer method addresses the areal and genetic effects simultaneously instead of separately as in Daumé III and Campbell (2007). 1</p><p>Reference: <a title="acl-2013-149-reference" href="../acl2013_reference/acl-2013-Exploring_Word_Order_Universals%3A_a_Probabilistic_Graphical_Model_Approach_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract In this paper we propose a probabilistic graphical model as an innovative framework for studying typological universals. [sent-2, score-0.25]
</p><p>2 Taking discovery of the word order universals as a knowledge discovery task we learn the graphical representation of a word order sub-system which reveals a finer structure such as direct and indirect dependencies among word order features. [sent-4, score-1.276]
</p><p>3 Then probabilistic inference enables us to see the strength of such relationships: given the observed value of one feature (or combination of features), the probabilities of values of other features can be calculated. [sent-5, score-0.228]
</p><p>4 Using imputation technique and EM algorithm it can handle missing val-  ues well. [sent-7, score-0.112]
</p><p>5 In addition the incremental and divide-and-conquer method addresses the areal and genetic effects simultaneously instead of separately as in Daumé III and Campbell (2007). [sent-9, score-0.185]
</p><p>6 1  Introduction  Ever since Greenberg (1963) proposed 45 universals of language based on a sample of 30 languages, typologists have been pursuing this topic actively for the past half century. [sent-10, score-0.565]
</p><p>7 Indeed the definition of “universals” has never been clear until recently, when most typologists agreed that such universals should be statistical universals which are “statistical tendencies” discovered from data samples by using statistical methods as used in any other science. [sent-12, score-1.077]
</p><p>8 Recent studies using probabilistic models are much more flexible and can handle noise and uncertainty better (Daumé III & Campbell, 2007; Dunn et al. [sent-15, score-0.103]
</p><p>9 However these models still rely on strong theoretic assumptions and heavy data treatment, such as using only two values of word order pairs while discarding other values, purposefully selecting a subset of the languages to study, or selecting partial data with complete values. [sent-17, score-0.268]
</p><p>10 In this paper we introduce a novel approach of using a probabilistic graphical model to study word order universals. [sent-18, score-0.296]
</p><p>11 Using this model we can have a graphic representation of the  structure of language as a complex system composed of linguistic features. [sent-19, score-0.159]
</p><p>12 The paper is organized as follows: in Section 2 we discuss the rationale of using a probabilistic graphic model to study word order universals and introduce our two models; Section 3 is about learning structures and parameters for the two models. [sent-22, score-0.732]
</p><p>13 Section 6 is about inference such as MAP query and in Section 6 we discuss the advantage of using PGM to study word order universals. [sent-24, score-0.149]
</p><p>14 1  A new approach: probabilistic graphical modeling  Rationale for using PGM in word order study The probabilistic graphical model is the marriage  of probabilistic theory and graph theory. [sent-28, score-0.549]
</p><p>15 It combines a graphical representation with a complex distribution over a high-dimensional space. [sent-29, score-0.134]
</p><p>16 There are two advantages of using this model to study word order universals. [sent-34, score-0.149]
</p><p>17 First the graphical structure can reveal much finer structure of language as a complex system. [sent-35, score-0.296]
</p><p>18 Most studies on word order correlations examine the pairwise relationship, for example, how the order of verb and object correlates with the order of noun and adjective. [sent-36, score-0.494]
</p><p>19 However linguists have also noticed other possible interactions among the word order features, like chains of overlapping implications: Prep ? [sent-37, score-0.27]
</p><p>20 NRel)) proposed by Hawkins (1983); multi-conditional implications (Daumé III, 2007); correlations among six word order pairs and three-way inter-  actions (Justeson & Stephens, 1990); spurious word order correlations (Croft et al. [sent-40, score-0.435]
</p><p>21 These claims about the possible interactions among word order features imply complex relationships among the features. [sent-44, score-0.35]
</p><p>22 The study of word order correlations started with pairwise comparison, probably because that was what typologists could do given the limited resources of statistical methods. [sent-45, score-0.294]
</p><p>23 However when we study the properties of a language, by knowing just several word orders such as order of verb and object, noun and adpositions, etc. [sent-46, score-0.314]
</p><p>24 We assume there is a meta-language that has the universal properties of all languages in the world. [sent-49, score-0.177]
</p><p>25 In this paper we focus on the sub-  system of word order only. [sent-53, score-0.149]
</p><p>26 The other advantage of PGM is that it enables us to quantify the relationships among word order features. [sent-54, score-0.299]
</p><p>27 Justeson & Stephens (1990) mentioned the notion of “correlation strength” when they found out that N/A order appears less strongly related to basic V/S/O order and/or adposition type than is N/G order. [sent-55, score-0.257]
</p><p>28 (201 1) used Bayes factor value to quantify the relationships between the word order pairs but they mistook the strength of evidence for an effect as the strength of the effect itself (Levy & Daumé III, 2011). [sent-58, score-0.399]
</p><p>29 A PGM model for a word order subsystem encodes a joint probabilistic distribution of all word order feature pairs. [sent-59, score-0.355]
</p><p>30 Using probability we can describe the degree of confidence about the uncertain nature of word order correlations. [sent-60, score-0.186]
</p><p>31 Such values can be seen as quantified strength of relationship between values of features. [sent-62, score-0.214]
</p><p>32 2  Our model  In our word order universal modeling we will use DAG structure since we think the direction of influence matters when talking about the relationship among features. [sent-64, score-0.366]
</p><p>33 In Greenberg (1966a) most of the universals are unidirectional, such as “If a language has object-verb order, then it also has subject-verb order” while few are bidirectional universals. [sent-65, score-0.466]
</p><p>34 The term “directionality” does not capture the full nature of the different statuses word order features have in the complex language system. [sent-66, score-0.233]
</p><p>35 We notice in all the word order studies the order of SOV or OV was given special attention. [sent-67, score-0.253]
</p><p>36 In Dryer’s study VO order is the dominant one which determines the set of word order pairs correlated with it (or not). [sent-68, score-0.308]
</p><p>37 We assume word order features have different statuses in the language system and such differences should be manifested by directionality of relationships between feature pairs. [sent-69, score-0.256]
</p><p>38 Some typologists (Dryer 1989, Croft 2003) have ar-  gued that the language samples in the WALS database (Haspelmath et al. [sent-72, score-0.145]
</p><p>39 ) because languages can share the same feature values due to either genetic or areal effect. [sent-76, score-0.304]
</p><p>40 1  Data  As we mentioned earlier we will restrict our attention to the domain of word order only in this paper. [sent-89, score-0.149]
</p><p>41 For Neg_V which has 17 values we collapsed its values 7-17 to 6 (“Mixed”). [sent-96, score-0.162]
</p><p>42 For Dem_N and Neg_V, we treat word and suffix as the same and collapsed values 1 and 3 to 1, and values 2 and 4 to 2. [sent-97, score-0.207]
</p><p>43 After deleting those languages with no value for all 15 word order features we have 1646 data entries. [sent-98, score-0.187]
</p><p>44 This database is very sparse: in overall the percentage of missing values is 3 1%. [sent-99, score-0.144]
</p><p>45 For seven features more than 50% of the languages have values missing. [sent-100, score-0.119]
</p><p>46 Because EM method for structures from incomplete data takes very long time to converge due to the large parameter space of our model, we decided to use imputation method to handle the missing data problem (Singh, 1997). [sent-104, score-0.112]
</p><p>47 We use GES (greedy search in the space of equivalent classes) algorithm in BNT_SLP to learn structure from a bootstrap dataset because it uses CPDAGs to represent Markov equivalent classes which makes graph fusion easier. [sent-108, score-0.141]
</p><p>48 The algorithm is as follows: 1) Use nearest-neighbor method to impute missing values in the original dataset D and create a complete dataset ? [sent-109, score-0.193]
</p><p>49 Use MAP estimation to fill in the missing values in D and generate a complete dataset ? [sent-166, score-0.144]
</p><p>50 2, the possible dependencies among language samples pose difficulty for statistical methods using the WALS data. [sent-175, score-0.131]
</p><p>51 Daumé III & Campbell (2007)’s hierarchical models provided a good solution to this problem; however their two models LINGHIER and DISTHIER dealt with genetic and areal influences separately and the two separate results still do not tell us what the “true universals” are. [sent-176, score-0.185]
</p><p>52 Instead of trying to control the areal and genetic and other factors, we propose a different perspective here. [sent-177, score-0.185]
</p><p>53 As we have mentioned, the kind of universals we care about are the stable properties of language, which means they can be found across all subsets of languages. [sent-178, score-0.54]
</p><p>54 We learn a structure for each subset and fuse the n graphs into one single graph. [sent-183, score-0.21]
</p><p>55 The algorithm is as follows: 1) Use nearest-neighbor method to impute missing values and create M complete datasets ? [sent-184, score-0.193]
</p><p>56 Use MAP estimation to fill in the missing values in D and generate another M complete dataset. [sent-251, score-0.144]
</p><p>57 From this graph we can see word order features are on different tiers in the hierarchy. [sent-255, score-0.198]
</p><p>58 The root S_O_V seems to “dominate” all the other features; noun modifiers and noun are in the middle tier while O Obl V, AdSub Cl, _O  _V  _C  Deg_A, Num_N, R, Neg_V and PoQPar are the leaf nodes which might indicate their smallest contribution to the word order properties of a  language. [sent-256, score-0.319]
</p><p>59 Our model reveals a much finer structure of the word order 153  sub-system by distinguishing different types of dependencies that might have been categorized simply as “correlation” in the traditional statistical methods. [sent-269, score-0.296]
</p><p>60 4  Quantitative Analysis of Results  The word order universal results are difficult to evaluate because we do not know the correct answers. [sent-270, score-0.214]
</p><p>61 Instead our UNIV model shows steady accurate prediction for the top ten universals and has more stable performance compared with other models. [sent-277, score-0.504]
</p><p>62 Besides pairwise feature values, we can calculate the probability of any combination of word order feature values. [sent-279, score-0.186]
</p><p>63 We need more evidence for setting a threshold value to define a word order universal but for now we just use 0. [sent-282, score-0.255]
</p><p>64 We calculated the probabilities of all pairwise feature values in the UNIV model which can found at http://www. [sent-284, score-0.119]
</p><p>65 5  Qualitative Analysis of Results  We also did qualitative evaluation through comparison with the well-known findings in word  order correlation studies. [sent-289, score-0.231]
</p><p>66 We noticed there is an asymmetry in terms of V_O’s influence on other word order pairs, which was not discussed in previous work. [sent-302, score-0.241]
</p><p>67 In the correlated pairs, only ADP NP and G N show bidirectional correla_N  _N  tion with O_V while PoQPar becomes a non-  correlated pair. [sent-303, score-0.11]
</p><p>68 In the non-correlated pairs, Dem_N becomes a correlated pair and other pairs also show correlation of weak strength. [sent-304, score-0.101]
</p><p>69 2  Evaluation: compare with Daumé III and Campbell’s work We compared the probabilities of single value pairs of the top ten word order universals with Daumé III and Campbell’s results, which are shown in the following figures. [sent-307, score-0.691]
</p><p>70 5  1  2  3  4 5 6 7 the first ten universals  8  9  10  Figure 4. [sent-312, score-0.504]
</p><p>71 5  1  2  3  4 5 6 7 the first ten universals  8  9  10  154  Figure 5. [sent-323, score-0.504]
</p><p>72 Compare with Daumé III and Campbell’s DIST model P(true) is the probability of having the particular implication; prob is the probability calculated in a different way which is not specified in Daumé III and Campbell’s work. [sent-324, score-0.11]
</p><p>73 In Figure 4 the two universals that have the biggest gaps are: 9) Prepositions ->VO and 10) Adjective-Noun>Demonstrative-Noun. [sent-327, score-0.466]
</p><p>74 In Figure 5 the three universals that have the biggest gaps are: 3) NounGenitive->Initial subordinator word, 6) Noun-  Genitive->Prepositions and 8) OV->SV. [sent-328, score-0.51]
</p><p>75 Their model with two values as nodes does not consider the more complex dependencies among more than two features. [sent-331, score-0.21]
</p><p>76 Our model provides a better solution by trying to maximize the joint probabilities of all word order feature pairs. [sent-332, score-0.187]
</p><p>77 6  Inference  Besides discovering word order universals, our model can reveal more properties of word order sub-system through various inference queries. [sent-333, score-0.372]
</p><p>78 Figure 6 (in Appendix B) gives an example: when we know the language is subject preceding verb and negative morpheme preceding verb, then we know the probability for this language to have postpositions is 0. [sent-335, score-0.124]
</p><p>79 5349, as well as the probabilities for the values of all other features. [sent-336, score-0.119]
</p><p>80 For example, when we only know that language is VO, we can use MAP query to find the combination of values which has the highest probability (0. [sent-338, score-0.118]
</p><p>81 One more useful function is to calculate the likelihood of a language in terms of word order properties. [sent-340, score-0.196]
</p><p>82 If all values of 13 features of a language are known, then the probability (likelihood) of having such a language can be calculated. [sent-341, score-0.118]
</p><p>83 We calculated the likelihood of eight languages and got the results as shown in Figure 7 (in Appendix 3 SamIam is a tool for modeling and reasoning with Bayesian networks ( http://reasoning. [sent-342, score-0.125]
</p><p>84 In other words English is a typical language regarding word order properties while Hakka Chinese is an atypical one. [sent-349, score-0.223]
</p><p>85 7  Discussion  Probabilistic graphic modeling provides solutions to the problems we noticed in the previous studies of word order universals. [sent-350, score-0.249]
</p><p>86 Using PGM we can infer properties about a language given the known values and we can also infer the likelihood of a language given all the values. [sent-352, score-0.202]
</p><p>87 In the future if we include other domains, such as phonology, morphology and syntax, we will be able to discover more properties about language as a whole complex system. [sent-353, score-0.118]
</p><p>88 Regarding the relationships among the features since PGM can give a finer structure we are able to see how the features are related directly or indirectly. [sent-354, score-0.219]
</p><p>89 Instead of saying “A is correlated with B”, we can say “A is correlated with B to a certain extent”. [sent-357, score-0.11]
</p><p>90 PGM enables us to quantify our knowledge about the word order properties of  languages. [sent-358, score-0.261]
</p><p>91 In addition we did not remove most of the values so that we can make inferences based on values such as “no determinant order” and “both orders”. [sent-361, score-0.162]
</p><p>92 We think PGM has the potential to become a new methodology for studying word order universals. [sent-363, score-0.185]
</p><p>93 It also opens up many new possibilities for studying linguistic typology as well: ? [sent-364, score-0.116]
</p><p>94 It can include other domains to build a more complex network and to discover more typological properties of languages. [sent-365, score-0.229]
</p><p>95 It can be used in field work for linguists to make predictions about properties of unknown languages. [sent-367, score-0.11]
</p><p>96 Capturing particulars and universals in clause linkage: a multivariate analysis. [sent-378, score-0.466]
</p><p>97 Learning bayesian network structure from massive datasets: the “sparse candidate” algorithm. [sent-468, score-0.168]
</p><p>98 Some universals of grammar with particular reference to the order of meaningful elements. [sent-475, score-0.57]
</p><p>99 Language universals and stochastic regularity of language change: Evidence from cross-linguistic distributions of case marking patterns. [sent-524, score-0.466]
</p><p>100 , Evolved structure of language shows lineage-specific trends in word order universals]. [sent-546, score-0.204]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('universals', 0.466), ('dryer', 0.247), ('pgm', 0.247), ('greenberg', 0.24), ('campbell', 0.233), ('daum', 0.206), ('univ', 0.153), ('iii', 0.126), ('dunn', 0.121), ('dag', 0.121), ('areal', 0.109), ('bic', 0.109), ('order', 0.104), ('justeson', 0.099), ('stephens', 0.099), ('typologists', 0.099), ('wals', 0.099), ('graphical', 0.09), ('values', 0.081), ('fuse', 0.081), ('friedman', 0.08), ('typology', 0.08), ('genetic', 0.076), ('bnt', 0.074), ('buffalo', 0.074), ('poqpar', 0.074), ('properties', 0.074), ('appendix', 0.072), ('bayesian', 0.069), ('relationships', 0.067), ('typological', 0.067), ('hawkins', 0.066), ('resamples', 0.066), ('universal', 0.065), ('missing', 0.063), ('flat', 0.063), ('graphic', 0.06), ('probabilistic', 0.057), ('croft', 0.056), ('correlated', 0.055), ('structure', 0.055), ('bickel', 0.054), ('vo', 0.054), ('influence', 0.052), ('finer', 0.052), ('strength', 0.052), ('adposition', 0.049), ('hakka', 0.049), ('haspelmath', 0.049), ('imputation', 0.049), ('impute', 0.049), ('leray', 0.049), ('maslova', 0.049), ('samiam', 0.049), ('graph', 0.049), ('noun', 0.048), ('likelihood', 0.047), ('uncertainty', 0.046), ('samples', 0.046), ('correlation', 0.046), ('acyclic', 0.046), ('correlations', 0.046), ('among', 0.045), ('word', 0.045), ('morpheme', 0.044), ('network', 0.044), ('complex', 0.044), ('subordinator', 0.044), ('francois', 0.044), ('ngen', 0.044), ('sov', 0.044), ('indirect', 0.044), ('verb', 0.043), ('koller', 0.042), ('quantitative', 0.041), ('evidence', 0.041), ('undirected', 0.04), ('noticed', 0.04), ('statuses', 0.04), ('networks', 0.04), ('dependencies', 0.04), ('july', 0.039), ('cambridge', 0.039), ('probabilities', 0.038), ('ten', 0.038), ('gn', 0.038), ('evolved', 0.038), ('languages', 0.038), ('quantify', 0.038), ('graphs', 0.037), ('probability', 0.037), ('learn', 0.037), ('em', 0.036), ('qualitative', 0.036), ('studying', 0.036), ('linguists', 0.036), ('tendencies', 0.036), ('implication', 0.036), ('prob', 0.036), ('toolbox', 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999905 <a title="149-tfidf-1" href="./acl-2013-Exploring_Word_Order_Universals%3A_a_Probabilistic_Graphical_Model_Approach.html">149 acl-2013-Exploring Word Order Universals: a Probabilistic Graphical Model Approach</a></p>
<p>Author: Xia Lu</p><p>Abstract: In this paper we propose a probabilistic graphical model as an innovative framework for studying typological universals. We view language as a system and linguistic features as its components whose relationships are encoded in a Directed Acyclic Graph (DAG). Taking discovery of the word order universals as a knowledge discovery task we learn the graphical representation of a word order sub-system which reveals a finer structure such as direct and indirect dependencies among word order features. Then probabilistic inference enables us to see the strength of such relationships: given the observed value of one feature (or combination of features), the probabilities of values of other features can be calculated. Our model is not restricted to using only two values of a feature. Using imputation technique and EM algorithm it can handle missing val- ues well. Model averaging technique solves the problem of limited data. In addition the incremental and divide-and-conquer method addresses the areal and genetic effects simultaneously instead of separately as in Daumé III and Campbell (2007). 1</p><p>2 0.11280052 <a title="149-tfidf-2" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<p>Author: Mehdi Manshadi ; Daniel Gildea ; James Allen</p><p>Abstract: Recent work on statistical quantifier scope disambiguation (QSD) has improved upon earlier work by scoping an arbitrary number and type of noun phrases. No corpusbased method, however, has yet addressed QSD when incorporating the implicit universal of plurals and/or operators such as negation. In this paper we report early, though promising, results for automatic QSD when handling both phenomena. We also present a general model for learning to build partial orders from a set of pairwise preferences. We give an n log n algorithm for finding a guaranteed approximation of the optimal solution, which works very well in practice. Finally, we significantly improve the performance of the pre- vious model using a rich set of automatically generated features.</p><p>3 0.064506344 <a title="149-tfidf-3" href="./acl-2013-Unsupervised_Consonant-Vowel_Prediction_over_Hundreds_of_Languages.html">369 acl-2013-Unsupervised Consonant-Vowel Prediction over Hundreds of Languages</a></p>
<p>Author: Young-Bum Kim ; Benjamin Snyder</p><p>Abstract: In this paper, we present a solution to one aspect of the decipherment task: the prediction of consonants and vowels for an unknown language and alphabet. Adopting a classical Bayesian perspective, we performs posterior inference over hundreds of languages, leveraging knowledge of known languages and alphabets to uncover general linguistic patterns of typologically coherent language clusters. We achieve average accuracy in the unsupervised consonant/vowel prediction task of 99% across 503 languages. We further show that our methodology can be used to predict more fine-grained phonetic distinctions. On a three-way classification task between vowels, nasals, and nonnasal consonants, our model yields unsu- pervised accuracy of 89% across the same set of languages.</p><p>4 0.06253738 <a title="149-tfidf-4" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>Author: Jiwei Tan ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Ordering texts is an important task for many NLP applications. Most previous works on summary sentence ordering rely on the contextual information (e.g. adjacent sentences) of each sentence in the source document. In this paper, we investigate a more challenging task of ordering a set of unordered sentences without any contextual information. We introduce a set of features to characterize the order and coherence of natural language texts, and use the learning to rank technique to determine the order of any two sentences. We also propose to use the genetic algorithm to determine the total order of all sentences. Evaluation results on a news corpus show the effectiveness of our proposed method. 1</p><p>5 0.059386276 <a title="149-tfidf-5" href="./acl-2013-Learning_Non-linear_Features_for_Machine_Translation_Using_Gradient_Boosting_Machines.html">221 acl-2013-Learning Non-linear Features for Machine Translation Using Gradient Boosting Machines</a></p>
<p>Author: Kristina Toutanova ; Byung-Gyu Ahn</p><p>Abstract: In this paper we show how to automatically induce non-linear features for machine translation. The new features are selected to approximately maximize a BLEU-related objective and decompose on the level of local phrases, which guarantees that the asymptotic complexity of machine translation decoding does not increase. We achieve this by applying gradient boosting machines (Friedman, 2000) to learn new weak learners (features) in the form of regression trees, using a differentiable loss function related to BLEU. Our results indicate that small gains in perfor- mance can be achieved using this method but we do not see the dramatic gains observed using feature induction for other important machine learning tasks.</p><p>6 0.057542391 <a title="149-tfidf-6" href="./acl-2013-Learning_to_Extract_International_Relations_from_Political_Context.html">224 acl-2013-Learning to Extract International Relations from Political Context</a></p>
<p>7 0.053489801 <a title="149-tfidf-7" href="./acl-2013-Crowd_Prefers_the_Middle_Path%3A_A_New_IAA_Metric_for_Crowdsourcing_Reveals_Turker_Biases_in_Query_Segmentation.html">99 acl-2013-Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation</a></p>
<p>8 0.051085137 <a title="149-tfidf-8" href="./acl-2013-Universal_Dependency_Annotation_for_Multilingual_Parsing.html">368 acl-2013-Universal Dependency Annotation for Multilingual Parsing</a></p>
<p>9 0.048942432 <a title="149-tfidf-9" href="./acl-2013-Modelling_Annotator_Bias_with_Multi-task_Gaussian_Processes%3A_An_Application_to_Machine_Translation_Quality_Estimation.html">248 acl-2013-Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation</a></p>
<p>10 0.047213305 <a title="149-tfidf-10" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>11 0.045951389 <a title="149-tfidf-11" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>12 0.045724433 <a title="149-tfidf-12" href="./acl-2013-Language_Acquisition_and_Probabilistic_Models%3A_keeping_it_simple.html">213 acl-2013-Language Acquisition and Probabilistic Models: keeping it simple</a></p>
<p>13 0.04523018 <a title="149-tfidf-13" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>14 0.044795137 <a title="149-tfidf-14" href="./acl-2013-Decentralized_Entity-Level_Modeling_for_Coreference_Resolution.html">106 acl-2013-Decentralized Entity-Level Modeling for Coreference Resolution</a></p>
<p>15 0.043606628 <a title="149-tfidf-15" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>16 0.043364089 <a title="149-tfidf-16" href="./acl-2013-The_mathematics_of_language_learning.html">349 acl-2013-The mathematics of language learning</a></p>
<p>17 0.042179003 <a title="149-tfidf-17" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>18 0.041873302 <a title="149-tfidf-18" href="./acl-2013-A_corpus-based_evaluation_method_for_Distributional_Semantic_Models.html">31 acl-2013-A corpus-based evaluation method for Distributional Semantic Models</a></p>
<p>19 0.04182186 <a title="149-tfidf-19" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<p>20 0.041566577 <a title="149-tfidf-20" href="./acl-2013-Chinese_Parsing_Exploiting_Characters.html">80 acl-2013-Chinese Parsing Exploiting Characters</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.15), (1, 0.013), (2, -0.013), (3, -0.038), (4, -0.004), (5, -0.017), (6, 0.023), (7, -0.018), (8, -0.012), (9, 0.004), (10, -0.016), (11, -0.029), (12, -0.02), (13, -0.023), (14, -0.084), (15, -0.042), (16, -0.023), (17, 0.032), (18, -0.001), (19, -0.033), (20, 0.004), (21, 0.006), (22, 0.033), (23, -0.012), (24, 0.015), (25, -0.002), (26, -0.064), (27, 0.016), (28, 0.017), (29, 0.024), (30, 0.005), (31, -0.041), (32, -0.03), (33, -0.011), (34, -0.006), (35, -0.057), (36, -0.038), (37, -0.038), (38, -0.049), (39, -0.015), (40, 0.036), (41, -0.05), (42, -0.019), (43, -0.059), (44, -0.035), (45, -0.032), (46, -0.02), (47, 0.022), (48, -0.045), (49, -0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8911162 <a title="149-lsi-1" href="./acl-2013-Exploring_Word_Order_Universals%3A_a_Probabilistic_Graphical_Model_Approach.html">149 acl-2013-Exploring Word Order Universals: a Probabilistic Graphical Model Approach</a></p>
<p>Author: Xia Lu</p><p>Abstract: In this paper we propose a probabilistic graphical model as an innovative framework for studying typological universals. We view language as a system and linguistic features as its components whose relationships are encoded in a Directed Acyclic Graph (DAG). Taking discovery of the word order universals as a knowledge discovery task we learn the graphical representation of a word order sub-system which reveals a finer structure such as direct and indirect dependencies among word order features. Then probabilistic inference enables us to see the strength of such relationships: given the observed value of one feature (or combination of features), the probabilities of values of other features can be calculated. Our model is not restricted to using only two values of a feature. Using imputation technique and EM algorithm it can handle missing val- ues well. Model averaging technique solves the problem of limited data. In addition the incremental and divide-and-conquer method addresses the areal and genetic effects simultaneously instead of separately as in Daumé III and Campbell (2007). 1</p><p>2 0.71709013 <a title="149-lsi-2" href="./acl-2013-Language_Acquisition_and_Probabilistic_Models%3A_keeping_it_simple.html">213 acl-2013-Language Acquisition and Probabilistic Models: keeping it simple</a></p>
<p>Author: Aline Villavicencio ; Marco Idiart ; Robert Berwick ; Igor Malioutov</p><p>Abstract: Hierarchical Bayesian Models (HBMs) have been used with some success to capture empirically observed patterns of under- and overgeneralization in child language acquisition. However, as is well known, HBMs are “ideal”learningsystems, assumingaccess to unlimited computational resources that may not be available to child language learners. Consequently, it remains crucial to carefully assess the use of HBMs along with alternative, possibly simpler, candidate models. This paper presents such an evaluation for a language acquisi- tion domain where explicit HBMs have been proposed: the acquisition of English dative constructions. In particular, we present a detailed, empiricallygrounded model-selection comparison of HBMs vs. a simpler alternative based on clustering along with maximum likelihood estimation that we call linear competition learning (LCL). Our results demonstrate that LCL can match HBM model performance without incurring on the high computational costs associated with HBMs.</p><p>3 0.66017473 <a title="149-lsi-3" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<p>Author: Mehdi Manshadi ; Daniel Gildea ; James Allen</p><p>Abstract: Recent work on statistical quantifier scope disambiguation (QSD) has improved upon earlier work by scoping an arbitrary number and type of noun phrases. No corpusbased method, however, has yet addressed QSD when incorporating the implicit universal of plurals and/or operators such as negation. In this paper we report early, though promising, results for automatic QSD when handling both phenomena. We also present a general model for learning to build partial orders from a set of pairwise preferences. We give an n log n algorithm for finding a guaranteed approximation of the optimal solution, which works very well in practice. Finally, we significantly improve the performance of the pre- vious model using a rich set of automatically generated features.</p><p>4 0.6482622 <a title="149-lsi-4" href="./acl-2013-The_mathematics_of_language_learning.html">349 acl-2013-The mathematics of language learning</a></p>
<p>Author: Andras Kornai ; Gerald Penn ; James Rogers ; Anssi Yli-Jyra</p><p>Abstract: unkown-abstract</p><p>5 0.634238 <a title="149-lsi-5" href="./acl-2013-Fluid_Construction_Grammar_for_Historical_and_Evolutionary_Linguistics.html">161 acl-2013-Fluid Construction Grammar for Historical and Evolutionary Linguistics</a></p>
<p>Author: Pieter Wellens ; Remi van Trijp ; Katrien Beuls ; Luc Steels</p><p>Abstract: Fluid Construction Grammar (FCG) is an open-source computational grammar formalism that is becoming increasingly popular for studying the history and evolution of language. This demonstration shows how FCG can be used to operationalise the cultural processes and cognitive mechanisms that underly language evolution and change.</p><p>6 0.63338494 <a title="149-lsi-6" href="./acl-2013-Grounded_Language_Learning_from_Video_Described_with_Sentences.html">175 acl-2013-Grounded Language Learning from Video Described with Sentences</a></p>
<p>7 0.62679303 <a title="149-lsi-7" href="./acl-2013-Psycholinguistically_Motivated_Computational_Models_on_the_Organization_and_Processing_of_Morphologically_Complex_Words.html">286 acl-2013-Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words</a></p>
<p>8 0.60949242 <a title="149-lsi-8" href="./acl-2013-Learning_to_Extract_International_Relations_from_Political_Context.html">224 acl-2013-Learning to Extract International Relations from Political Context</a></p>
<p>9 0.60282326 <a title="149-lsi-9" href="./acl-2013-Stop-probability_estimates_computed_on_a_large_corpus_improve_Unsupervised_Dependency_Parsing.html">331 acl-2013-Stop-probability estimates computed on a large corpus improve Unsupervised Dependency Parsing</a></p>
<p>10 0.60201699 <a title="149-lsi-10" href="./acl-2013-Typesetting_for_Improved_Readability_using_Lexical_and_Syntactic_Information.html">364 acl-2013-Typesetting for Improved Readability using Lexical and Syntactic Information</a></p>
<p>11 0.5969407 <a title="149-lsi-11" href="./acl-2013-Models_of_Translation_Competitions.html">250 acl-2013-Models of Translation Competitions</a></p>
<p>12 0.59251076 <a title="149-lsi-12" href="./acl-2013-%22Let_Everything_Turn_Well_in_Your_Wife%22%3A_Generation_of_Adult_Humor_Using_Lexical_Constraints.html">1 acl-2013-"Let Everything Turn Well in Your Wife": Generation of Adult Humor Using Lexical Constraints</a></p>
<p>13 0.58958828 <a title="149-lsi-13" href="./acl-2013-Word_surprisal_predicts_N400_amplitude_during_reading.html">390 acl-2013-Word surprisal predicts N400 amplitude during reading</a></p>
<p>14 0.58762968 <a title="149-lsi-14" href="./acl-2013-Unsupervised_Transcription_of_Historical_Documents.html">370 acl-2013-Unsupervised Transcription of Historical Documents</a></p>
<p>15 0.58528018 <a title="149-lsi-15" href="./acl-2013-Nonconvex_Global_Optimization_for_Latent-Variable_Models.html">260 acl-2013-Nonconvex Global Optimization for Latent-Variable Models</a></p>
<p>16 0.58265787 <a title="149-lsi-16" href="./acl-2013-The_effect_of_non-tightness_on_Bayesian_estimation_of_PCFGs.html">348 acl-2013-The effect of non-tightness on Bayesian estimation of PCFGs</a></p>
<p>17 0.5713129 <a title="149-lsi-17" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>18 0.56965113 <a title="149-lsi-18" href="./acl-2013-HEADY%3A_News_headline_abstraction_through_event_pattern_clustering.html">178 acl-2013-HEADY: News headline abstraction through event pattern clustering</a></p>
<p>19 0.56283092 <a title="149-lsi-19" href="./acl-2013-Improved_Lexical_Acquisition_through_DPP-based_Verb_Clustering.html">192 acl-2013-Improved Lexical Acquisition through DPP-based Verb Clustering</a></p>
<p>20 0.5486151 <a title="149-lsi-20" href="./acl-2013-Semantic_Neighborhoods_as_Hypergraphs.html">311 acl-2013-Semantic Neighborhoods as Hypergraphs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.084), (6, 0.037), (11, 0.056), (15, 0.026), (24, 0.04), (26, 0.074), (35, 0.077), (42, 0.048), (48, 0.05), (70, 0.043), (88, 0.028), (90, 0.032), (93, 0.276), (95, 0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76298767 <a title="149-lda-1" href="./acl-2013-Exploring_Word_Order_Universals%3A_a_Probabilistic_Graphical_Model_Approach.html">149 acl-2013-Exploring Word Order Universals: a Probabilistic Graphical Model Approach</a></p>
<p>Author: Xia Lu</p><p>Abstract: In this paper we propose a probabilistic graphical model as an innovative framework for studying typological universals. We view language as a system and linguistic features as its components whose relationships are encoded in a Directed Acyclic Graph (DAG). Taking discovery of the word order universals as a knowledge discovery task we learn the graphical representation of a word order sub-system which reveals a finer structure such as direct and indirect dependencies among word order features. Then probabilistic inference enables us to see the strength of such relationships: given the observed value of one feature (or combination of features), the probabilities of values of other features can be calculated. Our model is not restricted to using only two values of a feature. Using imputation technique and EM algorithm it can handle missing val- ues well. Model averaging technique solves the problem of limited data. In addition the incremental and divide-and-conquer method addresses the areal and genetic effects simultaneously instead of separately as in Daumé III and Campbell (2007). 1</p><p>2 0.64880234 <a title="149-lda-2" href="./acl-2013-Detecting_Turnarounds_in_Sentiment_Analysis%3A_Thwarting.html">117 acl-2013-Detecting Turnarounds in Sentiment Analysis: Thwarting</a></p>
<p>Author: Ankit Ramteke ; Akshat Malu ; Pushpak Bhattacharyya ; J. Saketha Nath</p><p>Abstract: Thwarting and sarcasm are two uncharted territories in sentiment analysis, the former because of the lack of training corpora and the latter because of the enormous amount of world knowledge it demands. In this paper, we propose a working definition of thwarting amenable to machine learning and create a system that detects if the document is thwarted or not. We focus on identifying thwarting in product reviews, especially in the camera domain. An ontology of the camera domain is created. Thwarting is looked upon as the phenomenon of polarity reversal at a higher level of ontology compared to the polarity expressed at the lower level. This notion of thwarting defined with respect to an ontology is novel, to the best of our knowledge. A rule based implementation building upon this idea forms our baseline. We show that machine learning with annotated corpora (thwarted/nonthwarted) is more effective than the rule based system. Because of the skewed distribution of thwarting, we adopt the Areaunder-the-Curve measure of performance. To the best of our knowledge, this is the first attempt at the difficult problem of thwarting detection, which we hope will at Akshat Malu Dept. of Computer Science & Engg., Indian Institute of Technology Bombay, Mumbai, India. akshatmalu@ cse .i itb .ac .in J. Saketha Nath Dept. of Computer Science & Engg., Indian Institute of Technology Bombay, Mumbai, India. s aketh@ cse .i itb .ac .in least provide a baseline system to compare against. 1 Credits The authors thank the lexicographers at Center for Indian Language Technology (CFILT) at IIT Bombay for their support for this work. 2</p><p>3 0.62679332 <a title="149-lda-3" href="./acl-2013-A_Learner_Corpus-based_Approach_to_Verb_Suggestion_for_ESL.html">8 acl-2013-A Learner Corpus-based Approach to Verb Suggestion for ESL</a></p>
<p>Author: Yu Sawai ; Mamoru Komachi ; Yuji Matsumoto</p><p>Abstract: We propose a verb suggestion method which uses candidate sets and domain adaptation to incorporate error patterns produced by ESL learners. The candidate sets are constructed from a large scale learner corpus to cover various error patterns made by learners. Furthermore, the model is trained using both a native corpus and the learner corpus via a domain adaptation technique. Experiments on two learner corpora show that the candidate sets increase the coverage of error patterns and domain adaptation improves the performance for verb suggestion.</p><p>4 0.53821284 <a title="149-lda-4" href="./acl-2013-Unsupervised_Consonant-Vowel_Prediction_over_Hundreds_of_Languages.html">369 acl-2013-Unsupervised Consonant-Vowel Prediction over Hundreds of Languages</a></p>
<p>Author: Young-Bum Kim ; Benjamin Snyder</p><p>Abstract: In this paper, we present a solution to one aspect of the decipherment task: the prediction of consonants and vowels for an unknown language and alphabet. Adopting a classical Bayesian perspective, we performs posterior inference over hundreds of languages, leveraging knowledge of known languages and alphabets to uncover general linguistic patterns of typologically coherent language clusters. We achieve average accuracy in the unsupervised consonant/vowel prediction task of 99% across 503 languages. We further show that our methodology can be used to predict more fine-grained phonetic distinctions. On a three-way classification task between vowels, nasals, and nonnasal consonants, our model yields unsu- pervised accuracy of 89% across the same set of languages.</p><p>5 0.53760201 <a title="149-lda-5" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>Author: Ulle Endriss ; Raquel Fernandez</p><p>Abstract: Crowdsourcing, which offers new ways of cheaply and quickly gathering large amounts of information contributed by volunteers online, has revolutionised the collection of labelled data. Yet, to create annotated linguistic resources from this data, we face the challenge of having to combine the judgements of a potentially large group of annotators. In this paper we investigate how to aggregate individual annotations into a single collective annotation, taking inspiration from the field of social choice theory. We formulate a general formal model for collective annotation and propose several aggregation methods that go beyond the commonly used majority rule. We test some of our methods on data from a crowdsourcing experiment on textual entailment annotation.</p><p>6 0.53698295 <a title="149-lda-6" href="./acl-2013-Linking_Tweets_to_News%3A_A_Framework_to_Enrich_Short_Text_Data_in_Social_Media.html">233 acl-2013-Linking Tweets to News: A Framework to Enrich Short Text Data in Social Media</a></p>
<p>7 0.53528523 <a title="149-lda-7" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>8 0.53297234 <a title="149-lda-8" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>9 0.53231293 <a title="149-lda-9" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>10 0.52983922 <a title="149-lda-10" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>11 0.52937317 <a title="149-lda-11" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>12 0.52846032 <a title="149-lda-12" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>13 0.52637643 <a title="149-lda-13" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>14 0.5259304 <a title="149-lda-14" href="./acl-2013-Learning_Semantic_Textual_Similarity_with_Structural_Representations.html">222 acl-2013-Learning Semantic Textual Similarity with Structural Representations</a></p>
<p>15 0.5236764 <a title="149-lda-15" href="./acl-2013-Fast_and_Robust_Compressive_Summarization_with_Dual_Decomposition_and_Multi-Task_Learning.html">157 acl-2013-Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning</a></p>
<p>16 0.52364868 <a title="149-lda-16" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>17 0.52350795 <a title="149-lda-17" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>18 0.5226568 <a title="149-lda-18" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>19 0.52211481 <a title="149-lda-19" href="./acl-2013-SEMILAR%3A_The_Semantic_Similarity_Toolkit.html">304 acl-2013-SEMILAR: The Semantic Similarity Toolkit</a></p>
<p>20 0.52063406 <a title="149-lda-20" href="./acl-2013-Word_Association_Profiles_and_their_Use_for_Automated_Scoring_of_Essays.html">389 acl-2013-Word Association Profiles and their Use for Automated Scoring of Essays</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
