<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>202 acl-2010-Reading between the Lines: Learning to Map High-Level Instructions to Commands</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-202" href="../acl2010/acl-2010-Reading_between_the_Lines%3A_Learning_to_Map_High-Level_Instructions_to_Commands.html">acl2010-202</a> <a title="acl-2010-202-reference" href="#">acl2010-202-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>202 acl-2010-Reading between the Lines: Learning to Map High-Level Instructions to Commands</h1>
<br/><p>Source: <a title="acl-2010-202-pdf" href="http://aclweb.org/anthology//P/P10/P10-1129.pdf">pdf</a></p><p>Author: S.R.K. Branavan ; Luke Zettlemoyer ; Regina Barzilay</p><p>Abstract: In this paper, we address the task of mapping high-level instructions to sequences of commands in an external environment. Processing these instructions is challenging—they posit goals to be achieved without specifying the steps required to complete them. We describe a method that fills in missing information using an automatically derived environment model that encodes states, transitions, and commands that cause these transitions to happen. We present an efficient approximate approach for learning this environment model as part of a policygradient reinforcement learning algorithm for text interpretation. This design enables learning for mapping high-level instructions, which previous statistical methods cannot handle.1</p><br/>
<h2>reference text</h2><p>Philip E. Agre and David Chapman. 1988. What are plans for? Technical report, Cambridge, MA, USA.  J. A. Boyan and A. W. Moore. 1995. Generalization in reinforcement learning: Safely approximating the value function. In Advances in NIPS, pages 369– 376. S.R.K Branavan, Harr Chen, Luke Zettlemoyer, and Regina Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Proceedings of ACL, pages 82–90. Christian Darken and John Moody. 1990. Note on learning rate schedules for stochastic optimization. In Advances in NIPS, pages 832–838. Barbara Di Eugenio and Michael White. 1992. On the interpretation of natural language instructions. In Proceedings of COLING, pages 1147–1 151. Barbara Di Eugenio. 1992. Understanding natural language instructions: the case of purpose clauses. In Proceedings of ACL, pages 120–127. Jacob Eisenstein, James Clarke, Dan Goldwasser, and Dan Roth. 2009. Reading to learn: Constructing features from semantic abstracts. In Proceedings of EMNLP, pages 958–967. Michael Fleischman and Deb Roy. 2005. Intentional  context in situated natural language learning. Proceedings of CoNLL, pages 104–1 11.  In  Nicholas K. Jong and Peter Stone. 2007. Model-based function approximation in reinforcement learning. In Proceedings of AAMAS, pages 670–677. Nate Kushman, Micah Brodsky, S.R.K. Branavan, Dina Katabi, Regina Barzilay, and Martin Rinard. 2009. Wikido. In Proceedings of HotNets-VIII. Alex Lascarides and Nicholas Asher. 2004. Imperatives in dialogue. In P. Kuehnlein, H. Rieser, and H. Zeevat, editors, The Semantics and Pragmatics of Dialogue for the New Millenium. Benjamins. Oliver Lemon and Ioannis Konstas. 2009. User simulations for context-sensitive speech recognition in spoken dialogue systems. In Proceedings of EACL, pages 505–513. Percy Liang, Michael I. Jordan, and Dan Klein. 2009. Learning semantic correspondences with less supervision. In Proceedings of ACL, pages 91–99. Matt MacMahon, Brian Stankiewicz, and Benjamin Kuipers. 2006. Walk the talk: connecting language, knowledge, and action in route instructions. In Proceedings of AAAI, pages 1475–1482.  C. Matuszek, D. Fox, and K. Koscher. 2010. Following directions using statistical machine translation. In Proceedings of Human-Robot Interaction, pages 251–258. Raymond J. Mooney. 2008. Learning to connect language and perception. In Proceedings of AAAI, pages 1598–1601 . 1276 James Timothy Oates. 2001. Grounding knowledge in sensors: Unsupervised learning for language and planning. Ph.D. thesis, University of Massachusetts Amherst. Warren B Powell. 2007. Approximate Dynamic Programming. Wiley-Interscience. Jost Schatzmann and Steve Young. 2009. The hidden agenda user simulation model. IEEE Trans. Audio, Speech and Language Processing, 17(4):733–747. Satinder Singh, Diane Litman, Michael Kearns, and Marilyn Walker. 2002. Optimizing dialogue management with reinforcement learning: Experiments with the njfun system. Journal of Artificial Intelligence Research, 16: 105–133. Jeffrey Mark Siskind. 2001. Grounding the lexical semantics of verbs in visual perception using force dynamics and event logic. Journal of Artificial Intelligence Research, 15:3 1–90. Richard S. Sutton and Andrew G. Barto. 1998. Reinforcement Learning: An Introduction. The MIT Press. Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. 2000. Policy gradient methods for reinforcement learning with function approximation. In Advances in NIPS, pages 1057–1063. Bonnie Webber, Norman Badler, Barbara Di Eugenio, Libby Levison Chris Geib, and Michael Moore. 1995. Instructions, intentions and expectations. Artificial Intelligence, 73(1-2). Terry Winograd. 1972. Understanding Natural Language. Academic Press. Chen Yu and Dana H. Ballard. 2004. On the integration of grounding language and learning objects. In Proceedings of AAAI, pages 488–493. 1277</p>
<br/>
<br/><br/><br/></body>
</html>
