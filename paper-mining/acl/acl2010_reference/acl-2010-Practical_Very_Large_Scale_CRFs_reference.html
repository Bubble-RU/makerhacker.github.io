<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>197 acl-2010-Practical Very Large Scale CRFs</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-197" href="../acl2010/acl-2010-Practical_Very_Large_Scale_CRFs.html">acl2010-197</a> <a title="acl-2010-197-reference" href="#">acl2010-197-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>197 acl-2010-Practical Very Large Scale CRFs</h1>
<br/><p>Source: <a title="acl-2010-197-pdf" href="http://aclweb.org/anthology//P/P10/P10-1052.pdf">pdf</a></p><p>Author: Thomas Lavergne ; Olivier Cappe ; Francois Yvon</p><p>Abstract: Conditional Random Fields (CRFs) are a widely-used approach for supervised sequence labelling, notably due to their ability to handle large description spaces and to integrate structural dependency between labels. Even for the simple linearchain model, taking structure into account implies a number of parameters and a computational effort that grows quadratically with the cardinality of the label set. In this paper, we address the issue of training very large CRFs, containing up to hun- dreds output labels and several billion features. Efficiency stems here from the sparsity induced by the use of a â€˜1 penalty term. Based on our own implementation, we compare three recent proposals for implementing this regularization strategy. Our experiments demonstrate that very large CRFs can be trained efficiently and that very large models are able to improve the accuracy, while delivering compact parameter sets.</p><br/>
<h2>reference text</h2><br/>
<br/><br/><br/></body>
</html>
