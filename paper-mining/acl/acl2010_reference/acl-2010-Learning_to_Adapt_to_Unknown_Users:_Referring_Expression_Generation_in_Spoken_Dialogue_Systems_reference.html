<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>167 acl-2010-Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-167" href="../acl2010/acl-2010-Learning_to_Adapt_to_Unknown_Users%3A_Referring_Expression_Generation_in_Spoken_Dialogue_Systems.html">acl2010-167</a> <a title="acl-2010-167-reference" href="#">acl2010-167-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>167 acl-2010-Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems</h1>
<br/><p>Source: <a title="acl-2010-167-pdf" href="http://aclweb.org/anthology//P/P10/P10-1008.pdf">pdf</a></p><p>Author: Srinivasan Janarthanam ; Oliver Lemon</p><p>Abstract: We present a data-driven approach to learn user-adaptive referring expression generation (REG) policies for spoken dialogue systems. Referring expressions can be difficult to understand in technical domains where users may not know the technical ‘jargon’ names of the domain entities. In such cases, dialogue systems must be able to model the user’s (lexical) domain knowledge and use appropriate referring expressions. We present a reinforcement learning (RL) framework in which the sys- tem learns REG policies which can adapt to unknown users online. Furthermore, unlike supervised learning methods which require a large corpus of expert adaptive behaviour to train on, we show that effective adaptive policies can be learned from a small dialogue corpus of non-adaptive human-machine interaction, by using a RL framework and a statistical user simulation. We show that in comparison to adaptive hand-coded baseline policies, the learned policy performs significantly better, with an 18.6% average increase in adaptation accuracy. The best learned policy also takes less dialogue time (average 1.07 min less) than the best hand-coded policy. This is because the learned policies can adapt online to changing evidence about the user’s domain expertise.</p><br/>
<h2>reference text</h2><p>H. Ai and D. Litman. 2007. Knowledge consistent user simulations for dialog systems. In Proceedings of Interspeech 2007, Antwerp, Belgium. T. Akiba and H. Tanaka. 1994. A Bayesian approach for User Modelling in Dialogue Systems. In Proceedings of the 15th conference on Computational Linguistics - Volume 2, Kyoto. A. Bell. 1984. Language style as audience design. Language in Society, 13(2): 145–204. A. Cawsey. 1993. User Modelling in Interactive Explanations. User Modeling and User-Adapted Interaction, 3(3):221–247.  H. H. Clark and G. L. Murphy. 1982. Audience design in meaning and reference. In J. F. LeNy and W. Kintsch, editors, Language and comprehension. Amsterdam: North-Holland. H. Cuayahuitl. 2009. Hierarchical Reinforcement Learning for Spoken Dialogue Systems. Ph.D. thesis, University of Edinburgh, UK. R. Dale. 1989. Cooking up referring expressions. Proc. ACL-1989.  In  K. Georgila, J. Henderson, and O. Lemon. 2005. Learning User Simulations for Information State Update Dialogue Systems. In Proc of Eurospeech/Interspeech. F. Hernandez, E. Gaudioso, and J. G. Boticario. 2003. A Multiagent Approach to Obtain Open and Flexible User Models in Adaptive Learning Communities. In User Modeling 2003, volume 2702/2003 of LNCS. Springer, Berlin / Heidelberg. E. A. Issacs and H. H. Clark. 1987. References in conversations between experts and novices. Journal of Experimental Psychology: General, 116:26–37. S. Janarthanam and O. Lemon. 2009a. A Two-tier User Simulation Model for Reinforcement Learning  of Adaptive Referring Expression Generation Policies. In Proc. SigDial’09. S. Janarthanam and O. Lemon. 2009b. A Wizard-ofOz environment to study Referring Expression Generation in a Situated Spoken Dialogue Task. In Proc. ENLG’09. S. Janarthanam and O. Lemon. 2009c. Learning Lexical Alignment Policies for Generating Referring Expressions for Spoken Dialogue Systems. In Proc. ENLG’09. O. Lemon. 2010. Learning what to say and how to say it: joint optimization of spoken dialogue management and Natural Language Generation. Computer Speech and Language. (to appear). E. Levin, R. Pieraccini, and W. Eckert. 1997. Learning Dialogue Strategies within the Markov Decision Process Framework. In Proc. of ASRU97. K. McKeown, J. Robin, and M. Tanenblatt. 1993. Tailoring Lexical Choice to the User’s Vocabulary in Multimedia Explanation Generation. In Proc. ACL 1993. C. L. Paris. 1987. The Use of Explicit User Models in Text Generations: Tailoring to a User’s Level of Expertise. Ph.D. thesis, Columbia University.  E. Reiter. 1991 . Generating Descriptions that Exploit a User’s Domain Knowledge. In R. Dale, C. Mellish, and M. Zock, editors, Current Research in Natural Language Generation, pages 257–285. Academic Press. V. Rieser and O. Lemon. 2009. Natural Language Generation as Planning Under Uncertainty for Spoken Dialogue Systems. In Proc. EACL’09. V. Rieser and O. Lemon. 2010. Optimising information presentation for spoken dialogue systems. In Proc. ACL. (to appear). J. Schatzmann, K. Weilhammer, M. N. Stuttle, and S. J. Young. 2006. A Survey of Statistical User Simulation Techniques for Reinforcement Learning of Dialogue Management Strategies. Knowledge Engineering Review, pages 97–126. J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and S. J. Young. 2007. Agenda-based User Simulation for Bootstrapping a POMDP Dialogue System. In Proc of HLT/NAACL 2007. D. Shapiro and P. Langley. 2002. Separating skills from preference: Using learning to program by reward. In Proc. ICML-02. R. Sutton and A. Barto. 1998. Reinforcement Learn-  ing. MIT Press. 78</p>
<br/>
<br/><br/><br/></body>
</html>
