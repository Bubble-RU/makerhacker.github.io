<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>194 acl-2010-Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-194" href="../acl2010/acl-2010-Phrase-Based_Statistical_Language_Generation_Using_Graphical_Models_and_Active_Learning.html">acl2010-194</a> <a title="acl-2010-194-reference" href="#">acl2010-194-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>194 acl-2010-Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning</h1>
<br/><p>Source: <a title="acl-2010-194-pdf" href="http://aclweb.org/anthology//P/P10/P10-1157.pdf">pdf</a></p><p>Author: Francois Mairesse ; Milica Gasic ; Filip Jurcicek ; Simon Keizer ; Blaise Thomson ; Kai Yu ; Steve Young</p><p>Abstract: Most previous work on trainable language generation has focused on two paradigms: (a) using a statistical model to rank a set of generated utterances, or (b) using statistics to inform the generation decision process. Both approaches rely on the existence of a handcrafted generator, which limits their scalability to new domains. This paper presents BAGEL, a statistical language generator which uses dynamic Bayesian networks to learn from semantically-aligned data produced by 42 untrained annotators. A human evaluation shows that BAGEL can generate natural and informative utterances from unseen inputs in the information presentation domain. Additionally, generation perfor- mance on sparse datasets is improved significantly by using certainty-based active learning, yielding ratings close to the human gold standard with a fraction of the data.</p><br/>
<h2>reference text</h2><p>S. Bangalore and O. Rambow. Exploiting a probabilistic hierarchical model for generation. In Proceedings of the 18th International Conference on Computational Linguistics (COLING), pages 42–48, 2000. A. Belz. Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models. Natural Language Engineering, 14(4):43 1–455, 2008. J. Bilmes and K. Kirchhoff. Factored language models and generalized parallel backoff. In Proceedings of HLTNAACL, short papers, 2003. J. Bilmes and G. Zweig. The Graphical Models ToolKit: An open source software system for speech and time-series  processing. In Proceedings of ICASSP, 2002. 6The full training corpus and the generated utterances used for evaluation are available at http : / /mi .eng . cam .ac .uk / ∼ farm2 /bage l. 1560  M. Bloodgood and C. Callison-Burch. Bucking the trend: Large-scale cost-focused active learning for statistical machine translation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), 2010. D. Espinosa, M. White, and D. Mehay. Hypertagging: Supertagging for surface realization with CCG. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), 2008. Y. Freund, H. S. Seung, E.Shamir, and N. Tishby. Selective sampling using the query by committee algorithm. Machine Learning, 28: 133–168, 1997. D. Hakkani-T u¨r, G. Riccardi, and A. Gorin. Active learning for automatic speech recognition. In Proceedings of ICASSP, 2002. Y. He and S. Young. Semantic processing using the Hidden Vector State model. Computer Speech & Language, 19 (1):85–106, 2005. A. Isard, C. Brockmann, and J. Oberlander. Individuality and alignment in generated dialogues. In Proceedings of the 4th International Natural Language Generation Conference (INLG), pages 22–29, 2006. I. Langkilde and K. Knight. Generation that exploits corpus-  based statistical knowledge. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (ACL), pages 704–710, 1998. F. Lef e`vre. A DBN-based multi-level stochastic spoken language understanding system. In Proceedings of the IEEE Workshop on Spoken Language Technology (SLT), 2006. D. D. Lewis and J. Catlett. Heterogeneous uncertainty ampling for supervised learning. In Proceedings of ICML, 1994. F. Mairesse and M. A. Walker. Trainable generation of BigFive personality styles through data-driven parameter estimation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), 2008. H. Nakanishi, Y. Miyao, , and J. Tsujii. Probabilistic methods for disambiguation of an HPSG-based chart generator. In Proceedings of the IWPT, 2005. F. J. Och and H. Ney. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1): 19–51, 2003. D. S. Paiva and R. Evans. Empirically-based control of natural language generation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 58–65, 2005. K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: a method for automatic evaluation ofmachine translation. In Proceedings ofthe 40th AnnualMeeting oftheAssociation for Computational Linguistics (ACL), 2002. L. R. Rabiner. Tutorial on Hidden Markov Models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257–285, 1989.  E. Reiter and A. Belz. An investigation into the validity of some metrics for automatically evaluating natural language generation systems. Computational Linguistics, 25: 529–558, 2009. V. Rieser and O. Lemon. Natural language generation as planning under uncertainty for spoken dialogue systems. In Proceedings of the Annual Meeting of the European Chapter of the ACL (EACL), 2009. M. Sassano. An empirical study of active learning with support vector machines for japanese word segmentation. In Proceedings ofthe 40thAnnual Meeting ofthe Association for Computational Linguistics (ACL), 2002. J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and S. Young. Agenda-based user simulation for bootstrapping a POMDP dialogue system. In Proceedings of HLTNAACL, short papers, pages 149–152, 2007. A. Stolcke. SRILM – an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, 2002. M. Tang, X. Luo, and S. Roukos. Active learning for statistical natural language parsing. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), 2002. C. Thompson, M. E. Califf, and R. J. Mooney. Active learning for natural language parsing and information extraction. In Proceedings of ICML, 1999. B. Thomson and S. Young. Bayesian update ofdialogue state: A POMDP framework for spoken dialogue systems. Computer Speech & Language, 24(4):562–588, 2010. Y. Tokuda, T. Yoshimura, T. Masuko, T. Kobayashi, and  T. Kitamura. Speech parameter generation algorithms for HMM-based speech synthesis. In Proceedings of ICASSP, 2000. G. Tur, R. E. Schapire, and D. Hakkani-T u¨r. Active learning for spoken language understanding. In Proceedings of ICASSP, 2003. S. Varges and C. Mellish. Instance-based natural language generation. In Proceedings of the Annual Meeting of the North American Chapter of the ACL (NAACL), 2001. M. A. Walker, O. Rambow, and M. Rogati. Training a sentence planner for spoken dialogue using boosting. Computer Speech and Language, 16(3-4), 2002. M. White, R. Rajkumar, and S. Martin. Towards broad coverage surface realization with CCG. In Proceedings of the Workshop on Using Corpora for NLG: Language Generation and Machine Translation, 2007. Y. W. Wong and R. Mooney. Generation by inverting a semantic parser that uses statistical machine translation. In Proceedings of HLT-NAACL, 2007. S. Young, M. Gaˇ si´ c, S. Keizer, F. Mairesse, J. Schatzmann, B. Thomson, and K. Yu. The Hidden Information State model: a practical framework for POMDP-based spoken dialogue management. Computer Speech and Language, 24(2): 150–174, 2010. 1561</p>
<br/>
<br/><br/><br/></body>
</html>
