<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-263" href="../acl2010/acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">acl2010-263</a> <a title="acl-2010-263-reference" href="#">acl2010-263-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</h1>
<br/><p>Source: <a title="acl-2010-263-pdf" href="http://aclweb.org/anthology//P/P10/P10-1040.pdf">pdf</a></p><p>Author: Joseph Turian ; Lev-Arie Ratinov ; Yoshua Bengio</p><p>Abstract: If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http ://metaoptimize com/proj ects/wordreprs/ .</p><br/>
<h2>reference text</h2><p>Ando, R., & Zhang, T. (2005). A highperformance semi-supervised learning method for text chunking. ACL. Bengio, Y. (2008). Neural net language models. Scholarpedia, 3, 388 1.  Bengio, Y., Ducharme, R., & Vincent, P. (2001). A neural probabilistic language model. NIPS. Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A neural probabilistic language model. Journal of Machine Learning Research, 3, 1137–1 155. Bengio, Y., Louradour, J., Collobert, R., & Weston, J. (2009). Curriculum learning. ICML. Bengio, Y., & S ´en ´ecal, J.-S. (2003). Quick training of probabilistic neural nets by importance sampling. AISTATS. Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of Machine Learning Research, 3, 993–1022. Brown, P. F., deSouza, P. V., Mercer, R. L., Pietra, V. J. D., & Lai, J. C. (1992). Class-based n-gram models of natural language. Computational Linguistics, 18, 467–479. Candito, M., & Crabb ´e, B. (2009). Improving generative statistical parsing with semi-supervised word clustering. IWPT (pp. 138–141). Collobert, R., & Weston, J. (2008). A unified architecture for natural language processing:  Deep neural networks with multitask learning. ICML. Deschacht, K., & Moens, M.-F. (2009). Semisupervised semantic role labeling using the Latent Words Language Model. EMNLP (pp. 21–29). Dumais, S. T., Furnas, G. W., Landauer, T. K., Deerwester, S., & Harshman, R. (1988). Using latent semantic analysis to improve access to textual information. SIGCHI Conference on Human Factors in Computing Systems (pp. 281–285). ACM. Elman, J. L. (1993). Learning and development in neural networks: The importance of starting small. Cognition, 48, 781–799. Goldberg, Y., Tsarfaty, R., Adler, M., & Elhadad, M. (2009). Enhancing unlexicalized parsing performance using a wide coverage lexicon, fuzzy tag-set mapping, and EM-HMM-based lexical probabilities. EACL. Honkela, T. (1997). Self-organizing maps of words for natural language processing applications. Proceedings of the International ICSC  Symposium on Soft Computing. Honkela, T., Pulkki, V., & Kohonen, T. (1995). Contextual relations of words in grimm tales, analyzed by self-organizing map. ICANN. Huang, F., & Yates, A. (2009). Distributional representations for handling sparsity in supervised sequence labeling. ACL. Kaski, S. (1998). Dimensionality reduction by random mapping: Fast similarity computation for clustering. IJCNN (pp. 413–418). Koo, T., Carreras, X., & Collins, M. (2008). Simple semi-supervised dependency parsing. ACL (pp. 595–603). Krishnan, V., & Manning, C. D. (2006). An effective two-stage model for exploiting nonlocal dependencies in named entity recognition. COLING-ACL. Landauer, T. K., Foltz, P. W., & Laham, D. (1998). An introduction to latent semantic analysis. Discourse Processes, 259–284. Li, W., & McCallum, A. (2005). Semi-supervised sequence modeling with syntactic topic models. AAAI.  Liang, P. (2005). Semi-supervised learning for natural language. Master’s thesis, Massachusetts Institute of Technology. Lin, D., & Wu, X. (2009). Phrase clustering for discriminative learning. ACL-IJCNLP (pp. 1030–1038). Lund, K., & Burgess, C. (1996). Producing highdimensional semantic spaces from lexical co-occurrence. Behavior Research Methods, Instrumentation, and Computers, 28, 203–208. Lund, K., Burgess, C., & Atchley, R. A. (1995). Semantic and associative priming in highdimensional semantic space. Cognitive Science Proceedings, LEA (pp. 660–665). Martin, S., Liermann, J., & Ney, H. (1998). Algorithms for bigram and trigram word clustering. Speech Communication, 24, 19–37. Miller, S., Guinness, J., & Zamanian, A. (2004). Name tagging with word clusters and discriminative training. HLT-NAACL (pp. 337–342). 393  Mnih, A., & Hinton,  G. E. (2007).  Three  new graphical models for statistical language modelling. ICML. Mnih, A., & Hinton, G. E. (2009). A scalable hierarchical distributed language model. NIPS (pp. 1081–1088). Morin, F., & Bengio, Y. (2005). Hierarchical probabilistic neural network language model. AISTATS. Pereira, F., Tishby, N., & Lee, L. (1993). Distributional clustering of english words. ACL (pp. 183–190). Ratinov, L., & Roth, D. (2009). Design challenges and misconceptions in named entity recognition. CoNLL. Ritter, H., & Kohonen, T. (1989). Self-organizing semantic maps. Biological Cybernetics, 241–254. Sahlgren, M. (2001). Vector-based semantic analysis: Representing word meanings based on random labels. Proceedings of the Semantic Knowledge Acquisition and Categorisation Workshop, ESSLLI.  Sahlgren, M. (2005). An introduction to random indexing. Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering (TKE). Sahlgren, M. (2006). The word-space model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces. Doctoral dissertation, Stockholm University. Sang, E. T., & Buchholz, S. (2000). Introduction to the CoNLL-2000 shared task: Chunking. CoNLL. Schwenk, H., & Gauvain, J.-L. (2002). Connectionist language modeling for large vocabulary continuous speech recognition. International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 765–768). Orlando, Florida. Sha, F., & Pereira, F. C. N. (2003). Shallow parsing with conditional random fields. HLT-NAACL.  Spitkovsky, V., Alshawi, H., & Jurafsky, D. (2010). From baby steps to leapfrog: How “less is more” in unsupervised dependency parsing. NAACL-HLT. Suzuki, J., & Isozaki, H. (2008). Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data. ACL-08: HLT (pp. 665–673). Suzuki, J., Isozaki, H., Carreras, X., & Collins, M. (2009). An empirical study of semi-supervised structured conditional models for dependency parsing. EMNLP. Turian, J., Ratinov, L., Bengio, Y., & Roth, D. (2009). A preliminary evaluation of word representations for named-entity recognition. NIPS Workshop on Grammar Induction, Representation of Language and Language Learning. Turney, P. D., & Pantel, P. (2010). From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research. Ushioda, A. (1996). Hierarchical clustering of words. COLING (pp. 1159–1 162). V ¨ayrynen, J., & Honkela, T. (2005). Compar-  ison of independent component analysis and singular value decomposition in word context analysis. AKRR ’05, International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning. V ¨ayrynen, J. J., & Honkela, T. (2004). Word category maps based on emergent features created by ICA. Proceedings of the STeP’2004 Cognition + Cybernetics Symposium (pp. 173–185). Finnish Artificial Intelligence Society. V ¨ayrynen, J. J., Honkela, T., & Lindqvist, L. (2007). Towards explicit semantic features using independent component analysis. Proceedings of the Workshop Semantic Content Acquisition and Representation (SCAR). Stockholm, Sweden: Swedish Institute of Computer Science. Rˇeh u˚ ˇrek, R., & Sojka, P. (2010). Software framework for topic modelling with large corpora. LREC. Zhang, T., & Johnson, D. (2003). A robust risk minimization based named entity recognition system. CoNLL.  Zhao, H., Chen, W., Kit, C., & Zhou, G. (2009). Multilingual dependency learning: a huge feature engineering method to semantic dependency parsing. CoNLL (pp. 55–60). 394</p>
<br/>
<br/><br/><br/></body>
</html>
