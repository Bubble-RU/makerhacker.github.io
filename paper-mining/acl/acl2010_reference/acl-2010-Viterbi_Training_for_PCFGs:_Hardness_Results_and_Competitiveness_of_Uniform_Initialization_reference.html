<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-255" href="../acl2010/acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">acl2010-255</a> <a title="acl-2010-255-reference" href="#">acl2010-255-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</h1>
<br/><p>Source: <a title="acl-2010-255-pdf" href="http://aclweb.org/anthology//P/P10/P10-1152.pdf">pdf</a></p><p>Author: Shay Cohen ; Noah A Smith</p><p>Abstract: We consider the search for a maximum likelihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar, the problem approximately solved by “Viterbi training.” We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood.</p><br/>
<h2>reference text</h2><p>N. Abe and M. Warmuth. 1992. On the computational complexity of approximating distributions by prob5Making the assumption that the grammar is in CNF permits us to use L instead of B, since there is a linear relationship between them in that case. abilistic automata. Machine Learning, 9(2–3):205– 260. S. Abney. 2007. Semisupervised Learning for Computational Linguistics. CRC Press. D. Aloise, A. Deshpande, P. Hansen, and P. Popat. 2009. NP-hardness of Euclidean sum-of-squares clustering. Machine Learning, 75(2):245–248. D. Arthur and S. Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proc. of ACMSIAM symposium on Discrete Algorithms. F. Casacuberta and C. de la Higuera. 2000. Com-  putational complexity of problems on probabilistic grammars and transducers. In Proc. of ICGI. E. Charniak. 1997. Statistical parsing with a contextfree grammar and word statistics. In Proc. of AAAI. S. B. Cohen and N. A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proc. of HLTNAACL. M. Collins. 2003. Head-driven statistical models for natural language processing. Computational Linguistics, 29(4):589–637. W. H. E. Day. 1983. Computationally difficult parsimony problems in phylogenetic systematics. Journal of Theoretical Biology, 103. J. DeNero and D. Klein. 2008. The complexity of phrase alignment problems. In Proc. of ACL. Y. Freund, H. Seung, E. Shamir, and N. Tishby. 1997. Selective sampling using the query by committee algorithm. Machine Learning, 28(2–3): 133–168. S. Goldwater and M. Johnson. 2005. Bias in learning syllable structure. In Proc. of CoNLL. J. Goodman. 1998. Parsing Inside-Out. Ph.D. thesis, Harvard University. U. Grenander. 1967. Syntax-controlled probabilities. Technical report, Brown University, Division of Applied Mathematics. M. Johnson, T. L. Griffiths, and S. Goldwater. 2006. Adaptor grammars: A framework for specifying  compositional nonparameteric Bayesian models. In Advances in NIPS. M. Johnson, T. L. Griffiths, and S. Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proc. of NAACL. D. Klein and C. Manning. 2001. Natural language grammar induction using a constituentcontext model. In Advances in NIPS. K. Knight. 1999. Decoding complexity in wordreplacement translation models. Computational Linguistics, 25(4):607–615. S. P. Lloyd. 1982. Least squares quantization in PCM. In IEEE Transactions on Information Theory. R. B. Lyngsø and C. N. S. Pederson. 2002. The consensus string problem and the complexity of comparing hidden Markov models. Journal of Computing and System Science, 65(3):545–569. M. Mahajan, P. Nimbhorkar, and K. Varadarajan. 2009. The planar k-means problem is NP-hard. In Proc. of International Workshop on Algorithms and Computation. 1510 D. McClosky,  E. Charniak,  Effective self-training  2006a.  and M. Johnson.  for parsing. In Proc. of HLT-  NAACL. D. McClosky,  E. Charniak,  and M. Johnson.  2006b.  Reranking and self-training for parser adaptation.  In  Proc. of COLING-ACL. R. McDonald  and G. Satta.  ity of non-projective  2007.  data-driven  On the complexdependency  pars-  ing. In Proc. of IWPT. R. M. Neal and G. E. Hinton.  1998.  A view of the  EM algorithm that justifies incremental, sparse, and other variants.  355–368. K. Sima’an. 1996. pages  In Learning and Graphical Models, Kluwer Academic Publishers. Computational complexity of prob-  abilistic disambiguation by means of tree-grammars. In In Proc. of COLING. M. Sipser. 2006. Introduction to the Theory of Computation, Second Edition. Thomson Course Technology. N. A. Smith and M. Johnson. 2007. Weighted and probabilistic context-free grammars are equally expressive. Computational Linguistics, 33(4):477– 491. V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Manning. 2010. Viterbi training improves unsupervised dependency parsing. In Proc. of CoNLL. R. Udupa and K. Maji. 2006. Computational complexity of statistical machine translation. In Proc. of EACL. M. Wang, N. A. Smith, and T. Mitamura. 2007. What is the Jeopardy model? a quasi-synchronous grammar for question answering. In Proc. of EMNLP. C. Yejin and C. Cardie. 2007. Structured local training and biased potential functions for conditional random fields with application to coreference resolution. In Proc. of HLT-NAACL. 1511</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
