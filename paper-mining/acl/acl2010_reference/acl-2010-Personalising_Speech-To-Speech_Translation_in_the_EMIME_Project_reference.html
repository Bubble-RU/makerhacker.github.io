<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>193 acl-2010-Personalising Speech-To-Speech Translation in the EMIME Project</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-193" href="../acl2010/acl-2010-Personalising_Speech-To-Speech_Translation_in_the_EMIME_Project.html">acl2010-193</a> <a title="acl-2010-193-reference" href="#">acl2010-193-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>193 acl-2010-Personalising Speech-To-Speech Translation in the EMIME Project</h1>
<br/><p>Source: <a title="acl-2010-193-pdf" href="http://aclweb.org/anthology//P/P10/P10-4009.pdf">pdf</a></p><p>Author: Mikko Kurimo ; William Byrne ; John Dines ; Philip N. Garner ; Matthew Gibson ; Yong Guan ; Teemu Hirsimaki ; Reima Karhila ; Simon King ; Hui Liang ; Keiichiro Oura ; Lakshmi Saheer ; Matt Shannon ; Sayaki Shiota ; Jilei Tian</p><p>Abstract: In the EMIME project we have studied unsupervised cross-lingual speaker adaptation. We have employed an HMM statistical framework for both speech recognition and synthesis which provides transformation mechanisms to adapt the synthesized voice in TTS (text-to-speech) using the recognized voice in ASR (automatic speech recognition). An important application for this research is personalised speech-to-speech translation that will use the voice of the speaker in the input language to utter the translated sentences in the output language. In mobile environments this enhances the users’ interaction across language barriers by making the output speech sound more like the original speaker’s way of speaking, even if she or he could not speak the output language.</p><br/>
<h2>reference text</h2><br/>
<br/><br/><br/></body>
</html>
