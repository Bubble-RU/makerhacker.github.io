<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-200" href="../acl2010/acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">acl2010-200</a> <a title="acl-2010-200-reference" href="#">acl2010-200-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</h1>
<br/><p>Source: <a title="acl-2010-200-pdf" href="http://aclweb.org/anthology//P/P10/P10-1130.pdf">pdf</a></p><p>Author: Valentin I. Spitkovsky ; Daniel Jurafsky ; Hiyan Alshawi</p><p>Abstract: We show how web mark-up can be used to improve unsupervised dependency parsing. Starting from raw bracketings of four common HTML tags (anchors, bold, italics and underlines), we refine approximate partial phrase boundaries to yield accurate parsing constraints. Conversion procedures fall out of our linguistic analysis of a newly available million-word hyper-text corpus. We demonstrate that derived constraints aid grammar induction by training Klein and Manningâ€™s Dependency Model with Valence (DMV) on this data set: parsing accuracy on Section 23 (all sentences) of the Wall Street Journal corpus jumps to 50.4%, beating previous state-of-the- art by more than 5%. Web-scale experiments show that the DMV, perhaps because it is unlexicalized, does not benefit from orders of magnitude more annotated but noisier data. Our model, trained on a single blog, generalizes to 53.3% accuracy out-of-domain, against the Brown corpus nearly 10% higher than the previous published best. The fact that web mark-up strongly correlates with syntactic structure may have broad applicability in NLP.</p><br/>
<h2>reference text</h2><br/>
<br/><br/><br/></body>
</html>
