<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>101 emnlp-2010-Storing the Web in Memory: Space Efficient Language Models with Constant Time Retrieval</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2010" href="../home/emnlp2010_home.html">emnlp2010</a> <a title="emnlp-2010-101" href="../emnlp2010/emnlp-2010-Storing_the_Web_in_Memory%3A_Space_Efficient_Language_Models_with_Constant_Time_Retrieval.html">emnlp2010-101</a> <a title="emnlp-2010-101-reference" href="#">emnlp2010-101-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>101 emnlp-2010-Storing the Web in Memory: Space Efficient Language Models with Constant Time Retrieval</h1>
<br/><p>Source: <a title="emnlp-2010-101-pdf" href="http://aclweb.org/anthology//D/D10/D10-1026.pdf">pdf</a></p><p>Author: David Guthrie ; Mark Hepple</p><p>Abstract: We present three novel methods of compactly storing very large n-gram language models. These methods use substantially less space than all known approaches and allow n-gram probabilities or counts to be retrieved in constant time, at speeds comparable to modern language modeling toolkits. Our basic approach generates an explicit minimal perfect hash function, that maps all n-grams in a model to distinct integers to enable storage of associated values. Extensions of this approach exploit distributional characteristics of n-gram data to reduce storage costs, including variable length coding of values and the use of tiered structures that partition the data for more efficient storage. We apply our approach to storing the full Google Web1T n-gram set and all 1-to-5 grams of the Gigaword newswire corpus. For the 1.5 billion n-grams of Gigaword, for example, we can store full count information at a cost of 1.66 bytes per n-gram (around 30% of the cost when using the current stateof-the-art approach), or quantized counts for 1.41 bytes per n-gram. For applications that are tolerant of a certain class of relatively innocuous errors (where unseen n-grams may be accepted as rare n-grams), we can reduce the latter cost to below 1byte per n-gram.</p><br/>
<h2>reference text</h2><p>Djamal Belazzougui, Fabiano Botelho, and Martin Dietzfelbinger. 2009. Hash, displace, and compress. Algorithms - ESA 2009, pages 682–693. Burton H. Bloom. 1970. Space/time trade-offs in hash coding with allowable errors. Commun. ACM, 13(7):422–426. Thorsten Brants and Alex Franz. 2006. Google Web 1T 5-gram Corpus, version 1. Linguistic Data Consortium, Philadelphia, Catalog Number LDC2006T13, September. Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, and Ayellet Tal. 2004. The bloomier filter: an efficient data structure for static support lookup tables. In SODA ’04, pages 30–39, Philadelphia, PA, USA. Philip Clarkson and Ronald Rosenfeld. 1997. Statis-  tical language modeling using the CMU-cambridge toolkit. In Proceedings of ESCA Eurospeech 1997, pages 2707–2710. Marcello Federico and Nicola Bertoldi. 2006. How many bits are needed to store probabilities for phrasebased translation? In StatMT ’06: Proceedings of the Workshop on Statistical Machine Translation, pages 94–101, Morristown, NJ, USA. Association for Computational Linguistics. Marcello Federico and Mauro Cettolo. 2007. Efficient handling of n-gram language models for statistical machine translation. In StatMT ’07: Proceedings of the Second Workshop on Statistical Machine Translation, pages 88–95, Morristown, NJ, USA. Association for Computational Linguistics. Edward Fredkin. 1960. Trie memory. Commun. ACM, 3(9):490–499. Kimmo Fredriksson and Fedor Nikitin. 2007. Simple compression code supporting random access and fast string matching. In Proc. of the 6th International Workshop on Efficient and Experimental Algorithms (WEA’07), pages 203–216. Ulrich Germann, Eric Joanis, and Samuel Larkin. 2009. Tightly packed tries: How to fit large models into memory, and make them load fast, too. Proceedings of the Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language (SETQANLP 2009), pages 31–39.  Joshua Goodman and Jianfeng Gao. 2000. Language model size reduction by pruning and clustering. In Proceedings of ICSLP’00, pages 110–1 13. David Graff. 2003. English Gigaword. Linguistic Data Consortium, catalog number LDC2003T05. Boulos Harb, Ciprian Chelba, Jeffrey Dean, and Sanjay Ghemawat. 2009. Back-off language model compression. In Proceedings of Interspeech, pages 352–355. Bo-June Hsu and James Glass. 2008. Iterative language model estimation:efficient data structure & algorithms. In Proceedings of Interspeech, pages 504–5 11. F. Jelinek, B. Merialdo, S. Roukos, and M. Strauss I. 1990. Self-organized language modeling for speech recognition. In Readings in Speech Recognition, pages 450–506. Morgan Kaufmann. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond ˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In ACL ’07: Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177–180, Morristown, NJ, USA. Association for Computational Linguistics.  Andreas Stolcke.  1998.  Entropy-based  272 pruning of  backoff language models. In Proceedings of DARPA Broadcast News Transcription and Understanding Workshop, pages 270–274. Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, volume 2, pages 901–904, Denver. David Talbot and Thorsten Brants. 2008. Randomized language models via perfect hash functions. Proceedings of ACL-08 HLT, pages 505–513. David Talbot and Miles Osborne. 2007a. Randomised language modelling for statistical machine translation. In Proceedings of ACL 07, pages 5 12–5 19, Prague, Czech Republic, June. David Talbot and Miles Osborne. 2007b. Smoothed bloom filter language models: Tera-scale LMs on the cheap. In Proceedings of EMNLP, pages 468–476. David Talbot and John M. Talbot. 2008. Bloom maps. In 4th Workshop on Analytic Algorithmics and Combinatorics 2008 (ANALCO’08), pages 203—212, San Francisco, California. David Talbot. 2009. Succinct approximate counting of skewed data. In IJCAI’09: Proceedings of the 21st international jont conference on Artifical intelligence, pages 1243–1248, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc. Sebastiano Vigna. 2008. Broadword implementation of  rank/select queries. In WEA’08: Proceedings of the 7th international conference on Experimental algorithms, pages 154–168, Berlin, Heidelberg. SpringerVerlag. Edward Whittaker and Bhinksha Raj. 2001 . Quantization-based language model compression. Technical report, Mitsubishi Electric Research Laboratories, TR-2001-41.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
