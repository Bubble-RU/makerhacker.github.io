<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>78 emnlp-2010-Minimum Error Rate Training by Sampling the Translation Lattice</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2010" href="../home/emnlp2010_home.html">emnlp2010</a> <a title="emnlp-2010-78" href="../emnlp2010/emnlp-2010-Minimum_Error_Rate_Training_by_Sampling_the_Translation_Lattice.html">emnlp2010-78</a> <a title="emnlp-2010-78-reference" href="#">emnlp2010-78-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>78 emnlp-2010-Minimum Error Rate Training by Sampling the Translation Lattice</h1>
<br/><p>Source: <a title="emnlp-2010-78-pdf" href="http://aclweb.org/anthology//D/D10/D10-1059.pdf">pdf</a></p><p>Author: Samidh Chatterjee ; Nicola Cancedda</p><p>Abstract: Minimum Error Rate Training is the algorithm for log-linear model parameter training most used in state-of-the-art Statistical Machine Translation systems. In its original formulation, the algorithm uses N-best lists output by the decoder to grow the Translation Pool that shapes the surface on which the actual optimization is performed. Recent work has been done to extend the algorithm to use the entire translation lattice built by the decoder, instead of N-best lists. We propose here a third, intermediate way, consisting in growing the translation pool using samples randomly drawn from the translation lattice. We empirically measure a systematic im- provement in the BLEU scores compared to training using N-best lists, without suffering the increase in computational complexity associated with operating with the whole lattice.</p><br/>
<h2>reference text</h2><p>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. Further meta-evaluation of machine translation. In Proceedings of the ACL 2008 Workshop on Statistical Machine Translation, Columbus, Ohio, 2008. http://www.statmt.org/wmt08/pdf/WMT09.pdf. Daniel Cer, Daniel Jurafsky, and Christopher D. Manning. Regularization and search for minimum error rate training. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 26–34, Columbus, Ohio, 2008. ISBN 978-1-932432-09-1. David Chiang. Hierarchical phrase-based transla-  tion. Computational Linguistics, 33(2):201–228, 2007. David Chiang, Yuval Marton, and Philip Resnik. Online large-margin training of syntactic and structural translation features. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP ’08), pages 224– 233, Honolulu, Hawaii, 2008. 614 Koby Crammer and Yoram Singer. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research (JMLR), 3:951–991, 2003. ISSN 1532-4435. doi: http://dx.doi.org/10.1 162/jmlr.2003.3.4-5.951. Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. Online passive-aggressive algorithms. Journal of Machine Learning Research (JMLR), 7:551–585, 2006. ISSN 1532-4435. Kevin Knight. Decoding complexity in wordreplacement translation modals. Computational Linguistics, Squibs and Discussion, 25(4),, 1999. Philipp Koehn, Hieu Hoang, Alexandra Birch,  Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. Moses: Open source toolkit for statistical machine translation. In Proceedings of the Annual Meeting of the Association for Computationl Linguistics (ACL ’07), pages 177–180, prague, Czech republic, 2007. Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz Och. Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices. In Proceedings of the Joint 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 163–171, Suntec, Singapore, August 2009. Wolfgang Macherey, Franz Josef Och, Ignacio Thayer, and Jakob Uszkoreit. Lattice-based minimum error rate training for statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP ’08), pages 725–734, Honolulu, Hawaii, 2008.  Josef Och. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL ’03), pages 160–167, Sapporo, Japan, 2003. doi: http://dx.doi.org/10.31 15/1075096.10751 17. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic  Franz  evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL ’02), pages 311–318, Philadelphia, Pennsylvania, 2002. doi: http://dx.doi.org/10.31 15/1073083.1073135. William H. Press. Numerical recipes : the art of scientific computing. Cambridge University Press, third edition, September 2007. ISBN 0521880688. Stefan Riezler and John T. Maxwell. On some pitfalls in automatic evaluation and significance testing for MT. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 57–64, Ann Arbor, Michigan, June 2005. David A. Smith and Jason Eisner. Minimum risk annealing for training log-linear models. In Proceedings of the Joint International Conference on Computational Linguistics and Annual meeting of the Association for Computational Linguistics (COLING/ACL ’06), pages 787–794, Sydney, Australia, 2006. Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. Online large-margin training for statistical machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 764–773, Prague, Czech Republic, June 2007. Richard Zens, Sasa Hasan, and Hermann Ney. A systematic comparison of training criteria for statistical machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 524–532, Prague, Czech Republic, June 2007.  615</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
