<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>45 emnlp-2010-Evaluating Models of Latent Document Semantics in the Presence of OCR Errors</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2010" href="../home/emnlp2010_home.html">emnlp2010</a> <a title="emnlp-2010-45" href="../emnlp2010/emnlp-2010-Evaluating_Models_of_Latent_Document_Semantics_in_the_Presence_of_OCR_Errors.html">emnlp2010-45</a> <a title="emnlp-2010-45-reference" href="#">emnlp2010-45-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>45 emnlp-2010-Evaluating Models of Latent Document Semantics in the Presence of OCR Errors</h1>
<br/><p>Source: <a title="emnlp-2010-45-pdf" href="http://aclweb.org/anthology//D/D10/D10-1024.pdf">pdf</a></p><p>Author: Daniel Walker ; William B. Lund ; Eric K. Ringger</p><p>Abstract: Models of latent document semantics such as the mixture of multinomials model and Latent Dirichlet Allocation have received substantial attention for their ability to discover topical semantics in large collections of text. In an effort to apply such models to noisy optical character recognition (OCR) text output, we endeavor to understand the effect that character-level noise can have on unsupervised topic modeling. We show the effects both with document-level topic analysis (document clustering) and with word-level topic analysis (LDA) on both synthetic and real-world OCR data. As expected, experimental results show that performance declines as word error rates increase. Common techniques for alleviating these problems, such as filtering low-frequency words, are successful in enhancing model quality, but exhibit failure trends similar to models trained on unpro- cessed OCR output in the case of LDA. To our knowledge, this study is the first of its kind.</p><br/>
<h2>reference text</h2><p>ABBYY. 2010. ABBYY finereader. http://finereader.abbyy.com. S. Agarwal, S. Godbole, D. Punjani, and Shourya Roy. 2007. How much noise is too much: A study in automatic text classification. In Proceedings of the Seventh IEEE Intl. Conf. on Data Mining (ICDM 2007), pages 3–12. Steven M. Beitzel, Eric C. Jensen, and David A. Grossman. 2003. A survey of retrieval strategies for ocr text collections. In In Proceedings of the Symposium on Document Image Understanding Technologies. Michael W. Berry, Murray Brown, and Ben Signer. 2007. 2001 topic annotated Enron email data set. David M. Blei and John D. Lafferty. 2006. Dynamic topic models. In Proceedings of the 23rd Intl. Conf. on Machine Learning (ICML 2006). David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022. Horst Bunke. 2003. Recognition of cursive roman handwriting- past, present and future. In 7th International Conference on Document Analysis and Recognition (ICDAR 2003), volume 1, pages 448–459. Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish, Chong Wang, and David Blei. 2009. Reading tea leaves: How humans interpret topic models. In Advances in Neural Information Processing Systems 22, pages 288–296. Faisal Farooq, Anurag Bhardwaj, and Venu Govindaraju. 2009. Using topic models for OCR correction. Intl. Journal on Document Analysis and Recognition (IJDAR), 12(3), September.  Google, Inc. 2010. Tesseract. http://code.google.com/p/tesseract-ocr. Thomas L. Griffiths, Mark Steyvers, David M. Blei, and Joshua B. Tenenbaum. 2005. Integrating topics and syntax. In In Advances in Neural Information Processing Systems 17, pages 537–544. MIT Press. Lawrence Hubert and Phipps Arabie. 1985. Comparing partitions. Journal of Classification, 2(1): 193–218, December. David Reed Jordan. 1945. Daily battle communiques, 19441945. Harold B. Lee Library, L. Tom Perry Special Collections, MSS 2766. Ken Lang. 1995. NewsWeeder: Learning to filter netnews. In Proceedings of the Twelfth International Conference on Machine Learning, pages 33 1–339. D. Lewis. 1997. Reuters-21578 text categorization test collection. http://www.research.att.com/˜lewis. Tao Liu, Shengping Liu, Zheng Chen, and Wei-Ying Ma. 2003. An evaluation on feature selection for text clustering. In Proceedings of the Twentieth Intl. Conf. on Machine Learning (ICML 2003), August. Daniel Lopresti. 2008. Optical character recognition errors and their effects on natural language processing. In Proceedings of the second workshop on Analytics for noisy unstructured text data (AND 2008), pages 9–16. William B. Lund and Eric. K Ringger. 2009. Improving optical character recognition through efficient multiple system alignment. In Proceedings of the Joint Conf. on Digital Libraries (JCDL’09), June. Andrew Kachites McCallum. 2002. MALLET: A machine learning for language toolkit. http://mallet.cs.umass.edu.  250 Marina Meil a˘. 2007. Comparing clusterings—an information based distance. Journal of Multivariate Analysis, 98(5):873–895. David Mimno and Andrew Mccallum. 2007. Organizing the OCA: learning faceted subjects from a library of digital books. In Proceedings ofthe Joint Conf. on Digital Libraries (JCDL’07), pages 376–385. Cosmin Munteanu, Ronald Baecker, Gerald Penn, Elaine Toms, and David James. 2006. The effect of speech recognition accuracy rates on the usefulness and usability of webcast archives. In Proceedings of the SIGCHI conference on Human Factors in computing systems, pages 493–502. David J. Newmann and Sharon Block. 2006. Probabilistic topic decomposition of an eighteenth-century american newspaper. J. Am. Soc. Inf. Sci. Technol., 57(6):753–767, February. Nuance Communications, Inc. 2010. OmniPage Pro. http://www.nuance.com/imaging/products/omnipage.asp. Andrew Rosenberg and Julia Hirschberg. 2007. V-measure: A conditional entropy-based external cluster evaluation measure. In Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning (EMNLP-CoNLL 2007). Kazem Taghva, Julie Borsack, and Allen Condit. 1994. Results of applying probabilistic ir to ocr text. In in Proc. 17th Intl. ACM/SIGIR Conf. on Research and Development in Information Retrieval, pages 202–21 1. Kazem Taghva, Tom Nartker, Julie Borsack, Steve Lumos,  Allen Condit, and Ron Young. 2001. Evaluating text categorization in the presence of ocr errors. In In Proc. IS&T;/SPIE 2001 Intl. Symp. on Electronic Imaging Science and Technology, pages 68–74. SPIE. Daniel Walker and Eric Ringger. 2008. Model-based document clustering with a collapsed gibbs sampler. In Proceedings of the 14th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (KDD 2008). Daniel Walker and Erik K. Ringger. 2010. Top N per document: Fast and effective unsupervised feature selection for document clustering. Technical Report 6, Brigham Young University. http://nlp.cs.byu.edu/techreports/BYUNLPTR6.pdf. Hanna Wallach, David Mimno, and Andrew McCallum. 2009a. Rethinking LDA: Why priors matter. In Advances in Neural Information Processing Systems 22, pages 1973–1981. Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov, and David Mimno. 2009b. Evaluation methods for topic models. In Proceedings of the 26th Annual Intl. Conf. on Machine Learning (ICML 2009), pages 1105–1 112. Xuerui Wang and Andrew McCallum. 2006. Topics over time: A non-markov continuous-time model of topical trends. In Proceedings of the 12th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (KDD 2006). Michael L. Wick, Michael G. Ross, and Erik G. LearnedMiller. 2007. Context-sensitive error correction: Using topic models to improve OCR. In Proceedings of the Ninth Intl. Conf. on Document Analysis and Recognition (ICDAR 2007), pages 1168–1 172.</p>
<br/>
<br/><br/><br/></body>
</html>
