<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>22 emnlp-2010-Automatic Evaluation of Translation Quality for Distant Language Pairs</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2010" href="../home/emnlp2010_home.html">emnlp2010</a> <a title="emnlp-2010-22" href="../emnlp2010/emnlp-2010-Automatic_Evaluation_of_Translation_Quality_for_Distant_Language_Pairs.html">emnlp2010-22</a> <a title="emnlp-2010-22-reference" href="#">emnlp2010-22-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>22 emnlp-2010-Automatic Evaluation of Translation Quality for Distant Language Pairs</h1>
<br/><p>Source: <a title="emnlp-2010-22-pdf" href="http://aclweb.org/anthology//D/D10/D10-1092.pdf">pdf</a></p><p>Author: Hideki Isozaki ; Tsutomu Hirao ; Kevin Duh ; Katsuhito Sudoh ; Hajime Tsukada</p><p>Abstract: Automatic evaluation of Machine Translation (MT) quality is essential to developing highquality MT systems. Various evaluation metrics have been proposed, and BLEU is now used as the de facto standard metric. However, when we consider translation between distant language pairs such as Japanese and English, most popular metrics (e.g., BLEU, NIST, PER, and TER) do not work well. It is well known that Japanese and English have completely different word orders, and special care must be paid to word order in translation. Otherwise, translations with wrong word order often lead to misunderstanding and incomprehensibility. For instance, SMT-based Japanese-to-English translators tend to translate ‘A because B’ as ‘B because A.’ Thus, word order is the most important problem for distant language translation. However, conventional evaluation metrics do not significantly penalize such word order mistakes. Therefore, locally optimizing these metrics leads to inadequate translations. In this paper, we propose an automatic evaluation metric based on rank correlation coefficients modified with precision. Our meta-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics.</p><br/>
<h2>reference text</h2><p>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for MT evaluation with improved correlation with human judgements. In Proc. of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and Summarization, pages 65–72. Alexandra Birch and Miles Osborne. 2010. LRscore for evaluating lexical and reordering quality in MT. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 327– 332. Alexandra Birch, Miles Osborne, and Phil Blunsom. 2010. Metrics for MT evaluation: evaluating reordering. Machine Translation, 24(1): 15–26. Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluatiing the role of Bleu in machine translation research. In Proc. of the Conference of the European Chapter of the Association for Computational Linguistics, pages 249–256. Chris Callison-Burch, Cameron Fordyce, Philipp  Koehn, Chrstof Monz, and Josh Schroeder. 2007. (Meta-)Evaluation of machine translation. In Proc. of the Workshop on Machine Translation (WMT), pages 136–158. Etienne Denoual and Yves Lepage. 2005. BLEU in characters: towards automatic MT evaluation in languages without word delimiters. In Companion Volume to the Proceedings of the Second International Joint Conference on Natural Language Processing, pages 81–86. Hiroshi Echizen-ya and Kenji Araki. 2007. Automatic evaluation of machine translation based on recursive acquisition of an intuitive common parts continuum. In Proceedings of MT Summit XII Workshop on Patent Translation, pages 15 1–158. Hiroshi Echizen-ya, Terumasa Ehara, Sayori Shimohata, Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, Takehito Utsuro, and Noriko Kando. 2009. Metaevaluation of automatic evaluation methods for machine translation using patent translation data in ntcir7. In Proceedings of the 3rd Workshop on Patent Translation, pages 9–16. Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and Takehito Utsuro. 2008. Overview of the patent translation task at the NTCIR-7 workshop. In Working Notes of the NTCIR Workshop Meeting (NTCIR), pages 389–400. 952  Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and Kevin Duh. 2010. Head Finalization: A simple reordering rule for SOV languages. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 250–257. Maurice G. Kendall. 1975. Rank Correlation Methods. Charles Griffin. Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proc. of the North American Chapter of the Association of Computational Linguistics (NAACL), pages 71–78. Dan Melamed, Ryan Green, and Joseph P. Turian. 2007. Precision and recall of machine translation. In Proc. of NAACL-HLT, pages 61–63. Kishore Papineni, Salim Roukos, Todd Ward, John Henderson, and Florence Reeder. 2002a. Corpus-based comprehensive and diagnostic MT evaluation: Initial Arabic, Chinese, French, and Spanish Results. In Proc. of the International Conference on Human Language Technology Research (HLT), pages 132–136. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002b. BLEU: a method for automatic evaluation of machine translation. In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL), pages 3 11–3 18. Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-  nea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings ofAssociationforMachine Translation in the Americas.</p>
<br/>
<br/><br/><br/></body>
</html>
