<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>53 emnlp-2010-Fusing Eye Gaze with Speech Recognition Hypotheses to Resolve Exophoric References in Situated Dialogue</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2010" href="../home/emnlp2010_home.html">emnlp2010</a> <a title="emnlp-2010-53" href="../emnlp2010/emnlp-2010-Fusing_Eye_Gaze_with_Speech_Recognition_Hypotheses_to_Resolve_Exophoric_References_in_Situated_Dialogue.html">emnlp2010-53</a> <a title="emnlp-2010-53-reference" href="#">emnlp2010-53-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>53 emnlp-2010-Fusing Eye Gaze with Speech Recognition Hypotheses to Resolve Exophoric References in Situated Dialogue</h1>
<br/><p>Source: <a title="emnlp-2010-53-pdf" href="http://aclweb.org/anthology//D/D10/D10-1046.pdf">pdf</a></p><p>Author: Zahar Prasov ; Joyce Y. Chai</p><p>Abstract: In situated dialogue humans often utter linguistic expressions that refer to extralinguistic entities in the environment. Correctly resolving these references is critical yet challenging for artificial agents partly due to their limited speech recognition and language understanding capabilities. Motivated by psycholinguistic studies demonstrating a tight link between language production and human eye gaze, we have developed approaches that integrate naturally occurring human eye gaze with speech recognition hypotheses to resolve exophoric references in situated dialogue in a virtual world. In addition to incorporating eye gaze with the best recognized spoken hypothesis, we developed an algorithm to also handle multiple hypotheses modeled as word confusion networks. Our empirical results demonstrate that incorporating eye gaze with recognition hypotheses consistently outperforms the results obtained from processing recognition hypotheses alone. Incorporating eye gaze with word confusion networks further improves performance.</p><br/>
<h2>reference text</h2><p>N. Bee, E. Andr e´, and S. Tober. 2009. Breaking the ice in human-agent communication: Eye-gaze based initiation of contact with an embodied conversational agent. In Proceedings of the 9th International Conference on Intelligent Virtual Agents (IVA’09), pages 229–242. Springer. T. Bickmore and J. Cassell, 2004. Social Dialogue with Embodied Conversational Agents, chapter Natural, Intelligent and Effective Interaction with Multimodal Dialogue Systems. Kluwer Academic. D. K. Byron, T. Mampilly, and T. Sharma, V.and Xu. 2005. Utilizing visual attention for cross-modal coreference interpretation. In Spring Lecture Notes in Computer Science: Proceedings of CONTEXT-05, pages 83–96. N. J. Cooke and M. Russell. 2008. Gaze-contingent automatic speech recognition. IET Signal Processing, 2(4):369–380, December. J. Cooke and J. T. Schwartz. 1970. Programming languages and their compilers: Preliminary notes. Technical report, Courant Institute of Mathematical Science. R. Fang, J. Y. Chai, and F. Ferreira. 2009. Between linguistic attention and gaze fixations in multimodal conversational interfaces. In The 11th International Con-  ference on Multimodal Interfaces (ICMI). P. Gorniak, J. Orkin, and D. Roy. 2006. Speech, space and purpose: Situated language understanding in computer games. In Twenty-eighth Annual Meeting of the Cognitive Science Society Workshop on Computer Games. Z. M. Griffin and K. Bock. 2000. What the eyes say about speaking. In Psychological Science, volume 11, pages 274–279. D. Hakkani-T u¨r, F. B ´echet, G. Riccardi, and G. Tur. 2006. Beyond asr 1-best: Using word confusion networks in spoken language understanding. Computer Speech and Language, 20(4):495–5 14. M. A. Just and P. A. Carpenter. 1976. Eye fixations and cognitive processes. In Cognitive Psychology, volume 8, pages 441–480. T. Kasami. 1965. An efficient recognition and syntaxanalysis algorithm for context-free languages. Scientific report AFCRL-65-758, Air Force Cambridge Research Laboratory, Bedford, Massachusetts. J. Kelleher and J. van Genabith. 2004. Visual salience and reference resolution in simulated 3-d environments. Artificial Intelligence Review, 21(3). Y. Liu, J. Y. Chai, and R. Jin. 2007. Automated vocabulary acquisition and interpretation in multimodal conversational systems. In Proceedings of the 45th Annual Meeting of the Association of Computational  Linguistics (ACL). L. Mangu, E. Brill, and A. Stolcke. 2000. Finding consensus in speech recognition: word error minimization and other applications of confusion networks. Computer Speech and Language, 14(4):373–400. A. S. Meyer and W. J. M. Levelt. 1998. Viewing and naming objects: Eye movements during noun phrase production. In Cognition, volume 66, pages B25–B33. 481 L.-P. Morency, C. M. Christoudias, and T. Darrell. 2006. Recognizing gaze aversion gestures in embodied conversational discourse. In International Conference on Multimodal Interfaces (ICMI). Y. I. Nakano, G. Reinstein, T. Stocky, and J. Cassell. 2003. Towards a model of face-to-face grounding. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL’03), pages 553–561. Z. Prasov and J. Y. Chai. 2008. What’s in a gaze? the role of eye-gaze in reference resolution in multimodal conversational interfaces. In Proceedings of 13th International Conference on Intelligent User interfaces (IUI), pages 20–29. S. Qu and J. Y. Chai. 2007. An exploration of eye gaze in spoken language processing for multimodal conversational interfaces. In Proceedings of the Conference of  the North America Chapter of the Association of Computational Linguistics (NAACL). S. Qu and J. Y. Chai. 2010. Context-based word acquisition for situated dialogue in a virtual world. Journal of Artificial Intelligence Research, 37:347–377, March. P. Qvarfordt and S. Zhai. 2005. Conversing with the user based on eye-gaze patterns. In Proceedings Of the Conference on Human Factors in Computing Systems. ACM. C. L. Sidner, C. D. Kidd, C. Lee, and N. Lesh. 2004. Where to look: A study of human-robot engagement. In Proceedings of the 9th international conference on Intelligent User Interfaces (IUI’04), pages 78–84. ACM Press. A. Stolcke. 2002. SRILM an extensible language modeling toolkit, confusion network. In International Conference on Spoken Language Processing. M. K. Tanenhous, M. Spivey-Knowlton, E. Eberhard, and J. Sedivy. 1995. Integration of visual and linguistic information during spoken language comprehension. In Science, volume 268, pages 1632–1634. D. H. Younger. 1967. Recognition and parsing of context-free languages in time n3. Information and Control, 10(2): 189–208.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
