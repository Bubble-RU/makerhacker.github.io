<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>113 emnlp-2010-Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2010" href="../home/emnlp2010_home.html">emnlp2010</a> <a title="emnlp-2010-113" href="../emnlp2010/emnlp-2010-Unsupervised_Induction_of_Tree_Substitution_Grammars_for_Dependency_Parsing.html">emnlp2010-113</a> <a title="emnlp-2010-113-reference" href="#">emnlp2010-113-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>113 emnlp-2010-Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing</h1>
<br/><p>Source: <a title="emnlp-2010-113-pdf" href="http://aclweb.org/anthology//D/D10/D10-1117.pdf">pdf</a></p><p>Author: Phil Blunsom ; Trevor Cohn</p><p>Abstract: Inducing a grammar directly from text is one of the oldest and most challenging tasks in Computational Linguistics. Significant progress has been made for inducing dependency grammars, however the models employed are overly simplistic, particularly in comparison to supervised parsing models. In this paper we present an approach to dependency grammar induction using tree substitution grammar which is capable of learning large dependency fragments and thereby better modelling the text. We define a hierarchical non-parametric Pitman-Yor Process prior which biases towards a small grammar with simple productions. This approach significantly improves the state-of-the-art, when measured by head attachment accuracy.</p><br/>
<h2>reference text</h2><p>Rens Bod. 2006. An all-subtrees approach to unsupervised parsing. In Proc. of the 44th Annual Meeting of the ACL and 21st International Conference on Computational Linguistics (COLING/ACL-2006), pages 865– 872, Sydney, Australia, July. Stephen Clark and James R. Curran. 2004. Parsing the WSJ using CCG and log-linear models. In Proc. of the 42nd Annual Meeting of the ACL (ACL-2004), pages 103–1 10, Barcelona, Spain. Alexander Clark. 2001 . Unsupervised induction of stochastic context-free grammars using distributional clustering. In ConLL ’01: Proceedings of the 2001 workshop on Computational Natural Language Learning, pages 1–8. Association for Computational Linguistics. Shay B. Cohen and Noah A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 74–82, Morristown, NJ, USA. Association for Computational Linguistics. Shay B. Cohen, Kevin Gimpel, and Noah A. Smith. 2008. Logistic normal priors for unsupervised probabilistic grammar induction. In Daphne Koller, Dale Schuurmans, Yoshua Bengio, and Lon Bottou, editors, NIPS, pages 321–328. MIT Press.  Shay B. Cohen, David M. Blei, and Noah A. Smith. 2010. Variational inference for adaptor grammars. In Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics. Trevor Cohn and Phil Blunsom. 2010. Blocked inference in Bayesian tree substitution grammars. In Proceedings of the 48th Annual Meeting of the Association for 1212 Computational Linguistics, page To Appear, Uppsala, Sweden. Trevor Cohn, Sharon Goldwater, and Phil Blunsom. 2009. Inducing compact but accurate tree-substitution grammars. In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics on ZZZ, pages 548–556, Morristown, NJ, USA. Association for Computational Linguistics. Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2011. Inducing tree-substitution grammars. Journal of Machine Learning Research. To Appear. Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637. Jason Eisner. 2000. Bilexical grammars and their cubic-  time parsing algorithms. In Harry Bunt and Anton Nijholt, editors, Advances in Probabilistic and Other Parsing Technologies, pages 29–62. Kluwer Academic Publishers, October. Stuart Geman and Donald Geman. 1984. Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:721–741 . Sharon Goldwater, Tom Griffiths, and Mark Johnson. 2006. Interpolating between types and tokens by estimating power-law generators. In Y. Weiss, B. Sch o¨lkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 459–466. MIT Press, Cambridge, MA. William P. Headden III, Mark Johnson, and David McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 101–109, Boulder, Colorado, June. Child Tag  NN NNP DT  Predicted Head Correct  Accuracy (%)  181 130 127  0.64 0.71 0.87  NNS VBD JJ IN RB PRP VBZ VBN VBP CD VB  108 108 106 81 65 64 47 36 30 26 25  0.72 0.81 0.80 0.55 0.61 0.97 0.80 0.86 0.77 0.23 0.68  the was The of a to in is n’t were are It for and ’s  42 29 25 18 18 17 16 15 15 12 11 11 9 9 9  0.88 0.97 0.83 0.78 0.90 0.50 0.89 0.79 0.83 0.86 0.92 1.00 0.64 0.75 1.00  Table 5: Per tag type predicted count and accuracy, for the most frequent 15 un/lexicalised tokens on the WSJ Section 22 |w| ≤ 10 heldout set (LexTSG-DMV (WPSlcfJg,P Secfcg,tiPosnh)) 2.2 Mark Johnson. 2007. Transforming projective bilexical dependency grammars into efficiently-parsable CFGs with unfold-fold. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 168–175, Prague, Czech Republic, June. Association for Computational Linguistics. Dan Klein and Christopher D. Manning. 2002. A generative constituent-context model for improved grammar induction. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 128–135, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics. Dan Klein and Christopher D. Manning. 2004. Corpusbased induction of syntactic structure: models of dependency and constituency. In ACL ’04: Proceedings 1213  Distance 1 2  Precision  Recall  F1  0.70 0.70  0.75 0.62  0.72 0.65  3 4 5 6 7 8 9 10  0.66 0.56 0.53 0.59 0.50 0.57 0.67 1.00  0.62 0.56 0.49 0.66 0.44 0.33 0.40 0.17  0.64 0.56 0.51 0.62 0.47 0.42 0.50 0.29  Table 6: Link distance precision, recall and f-score, on the WSJ Section 22 |w| ≤ 10 heldout set. of the 42nd Annual Meeting on Association for Computational Linguistics, page 478. Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn treebank. Computational Linguistics, 19(2):313–330. Ryan McDonald. 2006. Discriminative Training and Spanning Tree Algorithms for Dependency Parsing. Ph.D. thesis, University of Pennsylvania. Igor0 A. Mel0 cˇuk. 1988. Dependency Syntax: theory and practice. State University of New York Press, Albany. Radford Neal. 2003. Slice sampling. Annals of Statistics, 31:705–767. Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-  rafsky. 2010a. From Baby Steps to Leapfrog: How “Less is More” in unsupervised dependency parsing. In Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics. Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky, and Christopher D. Manning. 2010b. Viterbi training improves unsupervised dependency parsing. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010). Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Alshawi. 2010c. Profiting from mark-up: Hyper-text annotations for guided parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010). Y. W. Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 985– 992.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
