<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>73 emnlp-2012-Joint Learning for Coreference Resolution with Markov Logic</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-73" href="#">emnlp2012-73</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>73 emnlp-2012-Joint Learning for Coreference Resolution with Markov Logic</h1>
<br/><p>Source: <a title="emnlp-2012-73-pdf" href="http://aclweb.org/anthology//D/D12/D12-1114.pdf">pdf</a></p><p>Author: Yang Song ; Jing Jiang ; Wayne Xin Zhao ; Sujian Li ; Houfeng Wang</p><p>Abstract: Pairwise coreference resolution models must merge pairwise coreference decisions to generate final outputs. Traditional merging methods adopt different strategies such as the bestfirst method and enforcing the transitivity constraint, but most of these methods are used independently of the pairwise learning methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-201 1shared task on the same dataset. Compared with the best system from CoNLL2011, which employs a rule-based method, our system shows competitive performance.</p><p>Reference: <a title="emnlp-2012-73-reference" href="../emnlp2012_reference/emnlp-2012-Joint_Learning_for_Coreference_Resolution_with_Markov_Logic_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Pairwise coreference resolution models must merge pairwise coreference decisions to generate final outputs. [sent-6, score-0.989]
</p><p>2 Traditional merging methods adopt different strategies such as the bestfirst method and enforcing the transitivity constraint, but most of these methods are used independently of the pairwise learning methods as an isolated inference procedure at the end. [sent-7, score-0.442]
</p><p>3 We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. [sent-8, score-0.755]
</p><p>4 1 Introduction The task of noun phrase coreference resolution is to determine which mentions in a text refer to the same real-world entity. [sent-12, score-0.579]
</p><p>5 The mention-pair model splits the task into three parts: mention detection, pairwise classification and mention clustering. [sent-15, score-0.895]
</p><p>6 Therefore a mention clustering step is needed to resolve conflicts and generate the final mention clusters. [sent-19, score-0.814]
</p><p>7 In most work, pairwise classification and mention clustering are done sequentially. [sent-22, score-0.716]
</p><p>8 A major weakness of this approach is that pairwise classification considers only local information, which may not be sufficient to make correct decisions. [sent-23, score-0.29]
</p><p>9 One way to address this weakness is to jointly learn the pairwise classification model and the mention clustering model. [sent-24, score-0.74]
</p><p>10 In this paper, we study how to use a different learning framework, Markov logic (Richardson and Domingos, 2006), to learn a joint model for both pairwise classification and mention clustering under the mention-pair model. [sent-26, score-0.915]
</p><p>11 Moreover, joint learning is natural under the Markov logic framework, with local pairwise classification and global mention clustering both formulated as  weighted first-order clauses. [sent-29, score-0.983]
</p><p>12 In fact, Markov logic has been previously used by Poon and Domingos (2008) for coreference resolution and achieved good LParnogcue agdein Lgesa ornf tihneg, 2 p0a1g2e Jso 1in24t C5–o1n2f5e4re,n Jce ju on Is Elanmdp,ir Kicoarlea M,e 1t2h–o1d4s J iunly N 2a0tu1r2a. [sent-30, score-0.628]
</p><p>13 Lc a2n0g1u2ag Aes Psorcoicaetsiosin fgo arn Cdo Cmopmutpauti oantiaoln Lailn Ngautiustriacls results, but it was used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. [sent-32, score-0.468]
</p><p>14 More specifically, to combine mention clustering with pairwise classification, we adopt the commonly used strategies (such as best-first clustering and transitivity constraint), and formulate them as first-order logic formulas under the Markov logic framework. [sent-33, score-1.511]
</p><p>15 Transitivity constraint has been applied to coreference resolution by Klenner (2007) and Finkel and Manning (2008), and also achieved good performance. [sent-35, score-0.532]
</p><p>16 Our experiment results demonstrate the advantage of joint learning of pairwise classification and mention clustering over independent learning. [sent-37, score-0.755]
</p><p>17 We examine best-first clustering and transitivity constraint in our  methods, and find that both are very useful for coreference resolution. [sent-38, score-0.768]
</p><p>18 In the rest of this paper, we first describe a standard pairwise coreference resolution system in Section 2. [sent-42, score-0.639]
</p><p>19 We then present our Markov logic model for pairwise coreference resolution in Section 3. [sent-43, score-0.799]
</p><p>20 2  Standard Pairwise Coreference Resolution  In this section, we describe standard learning-based framework for pairwise coreference resolution. [sent-46, score-0.521]
</p><p>21 The major steps include mention detection, pairwise classification and mention clustering. [sent-47, score-0.895]
</p><p>22 1 Mention Detection  For mention detection, traditional methods include learning-based and rule-based methods. [sent-49, score-0.331]
</p><p>23 Without gold standard mention boundaries, we use a well-known preprocessing tool from Stanford’s NLP group1 to extract noun phrases. [sent-52, score-0.368]
</p><p>24 positive and  negative mention pairs) are constructed from known coreference chains, and features are defined to represent these instances. [sent-71, score-0.681]
</p><p>25 For each predicted mention m, we generate a positive mention pair between m and its closest preceding antecedent, and negative mention pairs by pairing m with each of its preceding predicted mentions which are not coreferential with m. [sent-74, score-1.26]
</p><p>26 To avoid having too many negative instances, we impose a maximum sentence distance between the two mentions when constructing mention pairs. [sent-75, score-0.405]
</p><p>27 This is based on the intuition that for each anaphoric mention, its preceding antecedent should appear quite near it, and most coreferential mention pairs which have a long sentence distance can be resolved using string matching. [sent-76, score-0.55]
</p><p>28 During the testing phase, we generate mention pairs for each mention candidate with each of its preceding mention candidates and use the learned model to make coreference decisions for these mention pairs. [sent-77, score-1.745]
</p><p>29 We also impose the sentence distance  constraint and use string matching for mention pairs with a sentence distance exceeding the threshold. [sent-78, score-0.395]
</p><p>30 3 Mention Clustering After obtaining the coreferential results for all mention pairs, some clustering method should be used to generate the final output. [sent-83, score-0.604]
</p><p>31 One strategy is the singlelink method, which links all the mention pairs that have a prediction probability higher than a threshold value. [sent-84, score-0.331]
</p><p>32 Two other alternative methods are the bestfirst clustering method and clustering with the transitivity constraint. [sent-85, score-0.536]
</p><p>33 Best-first clustering means that for each candidate mention m, we select the best one from all its preceding candidate mentions based on the prediction probabilities. [sent-86, score-0.663]
</p><p>34 A threshold value is given to filter out those mention pairs that have a low probability to be coreferential. [sent-87, score-0.331]
</p><p>35 Previous work has found that best-first clustering and transitivity constraint-based clustering are better than the single-link method. [sent-89, score-0.506]
</p><p>36 3  Markov Logic for Pairwise Coreference Resolution  In this section, we present our method for joint learning of pairwise classification and mention clustering using Markov logic. [sent-91, score-0.755]
</p><p>37 For mention detection, training instance generation and postprocessing, our method follows the same procedures as described in Section 2. [sent-92, score-0.331]
</p><p>38 In what follows, we will first describe the basic Markov logic networks (MLN) framework, and then introduce the first-order logic formulas we use in our MLN including local formulas and global formulas which perform pairwise classification and mention clustering respectively. [sent-93, score-1.686]
</p><p>39 1 Markov Logic Networks Markov logic networks combine Markov networks with first-order logic (Richardson and Domingos, 2006; Riedel, 2008). [sent-97, score-0.386]
</p><p>40 A Markov logic network consists of a set of first-order clauses (which we will refer to as formulas in the rest of the paper) just like in  first-order logic. [sent-98, score-0.343]
</p><p>41 These weighted formulas define a probability distribution over sets of ground atoms or so-called possible worlds. [sent-102, score-0.326]
</p><p>42 Take the following formula as one example: (ϕi, wi) : headMatch(a, b)∧(a  = b) ⇒  coref (a, b) . [sent-111, score-0.394]
</p><p>43 Here a and b are variables which can represent any candidate mention, headMatch and coref are observed predicate and hidden predicate respectively. [sent-113, score-0.617]
</p><p>44 coref is a hidden predicate because this is something we would like to predict. [sent-117, score-0.464]
</p><p>45 2  Formulas  We use two kinds of formulas for pairwise classification and mention clustering, respectively. [sent-119, score-0.747]
</p><p>46 For  describing the attributes of mi mentionType(i,t)mihas mention type NAM(named entities), NOM(nominal) or PRO(pronouns). [sent-120, score-0.36]
</p><p>47 describing the attributes of relations between mjand mi mentionDistance(j,i,m)Distance between mjand miin mentions. [sent-133, score-0.357]
</p><p>48 For mention clustering, we use global formulas to implement best-first clustering or transitivity constraint. [sent-153, score-0.903]
</p><p>49 We naturally combine pairwise classification with mention clustering via local and global formulas in the Markov logic framework, which is the essence of  “joint learning” in our work. [sent-154, score-1.127]
</p><p>50 2  Global Formulas  Global formulas are designed to add global constraints for hidden predicates. [sent-170, score-0.275]
</p><p>51 coref, our global formulas incorporate correlations among dif-  ferent ground atoms of the coref predicates. [sent-173, score-0.702]
</p><p>52 Therefore for each candidate mention i,  we should only select at most one candidate mention j to return true for the predicate coref(j,i) from all its preceding candidate mentions. [sent-178, score-0.894]
</p><p>53 1249 We use best-first clustering and transitivity constraint in our joint learning model respectively. [sent-180, score-0.457]
</p><p>54 Here  we use x to represent all the observed ground atoms and y to represent the hidden ground atoms. [sent-185, score-0.259]
</p><p>55 And global formulas should be satisfied as hard constraints when inferring the best yˆ. [sent-188, score-0.243]
</p><p>56 Detailed introduction about transforming ground Markov networks in Markov logic into an ILP problem can be found in (Riedel, 2008). [sent-190, score-0.277]
</p><p>57 The number of false ground atoms of coref predicate is selected as loss function in our experiments. [sent-201, score-0.575]
</p><p>58 best-first clustering or transitivity constraint) must be satisfied when inferring the best y′ in each iteration, which can make learned weights more effective. [sent-204, score-0.354]
</p><p>59 org/201 1/ 1250 and another well-known coreference dataset from ACE. [sent-214, score-0.35]
</p><p>60 Second, only identity coreference is tagged in OntoNotes, but not appositives or predicate nominatives. [sent-216, score-0.48]
</p><p>61 The shared task is to automatically identify both entity coreference and event coreference, although we only focus on entity coreference in this paper. [sent-218, score-0.789]
</p><p>62 We don’t assume that gold standard mention boundaries are given. [sent-219, score-0.331]
</p><p>63 So we develop a heuristic method for mention detection. [sent-220, score-0.331]
</p><p>64 Specifically, for mention detection, we use precision, recall and the F-measure. [sent-229, score-0.331]
</p><p>65 A mention is considered to be correct only if it matches the exact same span of characters in the annotation key. [sent-230, score-0.331]
</p><p>66 The MLN-Local system uses  only the local formulas described in Table 2 without any global constraints under the MLN framework. [sent-236, score-0.276]
</p><p>67 The MLN-Local+BF system replaces the single-link method with best-first clustering to infer mention clustering results after learning the weights for all the local formulas. [sent-238, score-0.668]
</p><p>68 The MLN-Local+Trans system replaces the best-first clustering with transitivity  SystemMRentionP DetectiFonRMUPCFRB-cPubeFRCEPAFFAFvg M LMLN -L -NLo c-Lca ol+clT+arlBaFns6 528. [sent-239, score-0.354]
</p><p>69 The MLN-Joint system is a joint model for both pairwise classification and mention clustering. [sent-267, score-0.603]
</p><p>70 It can combine either best-first clustering or enforcing transitivity constraint with pairwise classification, and we denote these two variants of MLNJoint as MLN-Joint(BF) and MLN-Joint(Trans) respectively. [sent-268, score-0.628]
</p><p>71 When adding best-first or transitivity constraint which is independent of pairwise classification, MLNLocal+BF and MLN-Local+Trans achieve better results of 57. [sent-277, score-0.437]
</p><p>72 Most of all, we can see that the joint learning model (MLN-Joint(BF) or MLN-Joint(Trans)) significantly outperforms independent learning model (MLN-Local+BF or MLNLocal+Trans) no matter whether best-first clustering or transitivity constraint is used (based on a paired 2tailed t-test with p < 0. [sent-280, score-0.457]
</p><p>73 Best-first clustering and transitivity constraint are very useful in Markov logic framework, and both MLN-Local and MLN-Joint benefit from them. [sent-284, score-0.578]
</p><p>74 We implemented a traditional pairwise coreference system using Maximum Entropy as the base classifier and best-first clustering to link the results. [sent-291, score-0.673]
</p><p>75 To replace best-first clustering with transitivity constraint, we have another system named as MaxEnt+Trans. [sent-294, score-0.354]
</p><p>76 Chang’s system uses ILP to perform  best-first clustering after training a pairwise coreference model. [sent-296, score-0.673]
</p><p>77 5  Related Work  Supervised noun phrase coreference resolution has been extensively studied. [sent-356, score-0.505]
</p><p>78 Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. [sent-361, score-0.435]
</p><p>79 (201 1) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. [sent-363, score-0.483]
</p><p>80 In all these studies, however, mention clustering is combined with pairwise classification only at the infer-  ence stage but not at the learning stage. [sent-364, score-0.716]
</p><p>81 Finley and Joachims (2005) proposed a general SVM-based framework for supervised clustering that learns item-pair similarity measures, and applied the framework to noun phrase 1252 coreference resolution. [sent-366, score-0.539]
</p><p>82 As we have shown in Section 3, given the flexibility of Markov logic, it is straightforward to perform joint learning of pairwise classification and mention clustering. [sent-368, score-0.603]
</p><p>83 For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). [sent-371, score-0.818]
</p><p>84 However, OntoNotes corpus (state-of-art NLP data collection) on coreference layer for CoNLL-201 1 has excluded these two conditions of annotations (appositives and predicate nominatives) from their judging guidelines. [sent-374, score-0.441]
</p><p>85 (2009) used Markov logic to predict coreference probabilities for mention pairs followed by correlation clustering to generate the final results. [sent-377, score-0.993]
</p><p>86 Although they also perform joint learning, at the inference stage, they still make pairwise coreference decisions and cluster mentions sequentially. [sent-378, score-0.634]
</p><p>87 Besides combining pairwise classification and mention clustering, there has also been some work that jointly performs mention detection and coreference resolution. [sent-380, score-1.287]
</p><p>88 Rahman and Ng (2009) proposed to learn a cluster-ranker for discourse-new mention detection jointly with coreference resolution. [sent-382, score-0.723]
</p><p>89 Denis and Baldridge (2007) adopted an Integer Linear Programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task. [sent-383, score-0.881]
</p><p>90 6  Conclusion  In this paper we present a joint learning method with Markov logic which naturally combines pairwise classification and mention clustering. [sent-384, score-0.763]
</p><p>91 In the future we will try to design more global constraints and explore deeper relations between training instances generation and mention clustering. [sent-387, score-0.391]
</p><p>92 We will also attempt to introduce more predicates and transform structure learning techniques for MLN into coreference problems. [sent-388, score-0.381]
</p><p>93 Joint determination of anaphoricity and coreference resolution using integer programming. [sent-431, score-0.515]
</p><p>94 Stanford’s multi-pass sieve coreference resolution system at the conll-201 1 shared task. [sent-461, score-0.505]
</p><p>95 A mentionsynchronous coreference resolution algorithm based on the bell tree. [sent-466, score-0.468]
</p><p>96 Supervised noun phrase coreference research: The first fifteen years. [sent-491, score-0.387]
</p><p>97 Conll-201 1 shared task: Modeling unrestricted coreference in ontonotes. [sent-504, score-0.387]
</p><p>98 Relaxcor participation in conll shared task on coreference resolution. [sent-522, score-0.387]
</p><p>99 A machine learning approach to coreference resolution of noun phrases. [sent-527, score-0.505]
</p><p>100 An entity-mention model for coreference resolution with inductive logic programming. [sent-543, score-0.628]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mentiontype', 0.393), ('coreference', 0.35), ('coref', 0.341), ('mention', 0.331), ('mmeennttiioonnttyyppee', 0.242), ('transitivity', 0.202), ('formulas', 0.183), ('pairwise', 0.171), ('logic', 0.16), ('clustering', 0.152), ('mjand', 0.151), ('coreferential', 0.121), ('resolution', 0.118), ('markov', 0.098), ('bf', 0.094), ('predicate', 0.091), ('miis', 0.091), ('ground', 0.084), ('trans', 0.082), ('mln', 0.082), ('mihas', 0.076), ('mentions', 0.074), ('ontonotes', 0.064), ('constraint', 0.064), ('classification', 0.062), ('headmatch', 0.06), ('sapena', 0.06), ('atoms', 0.059), ('poon', 0.059), ('formula', 0.053), ('bengtson', 0.052), ('ilp', 0.047), ('domingos', 0.047), ('fj', 0.046), ('cagree', 0.045), ('entitytype', 0.045), ('firstmention', 0.045), ('gendertype', 0.045), ('ocroerfe', 0.045), ('detection', 0.042), ('pradhan', 0.041), ('fifteenth', 0.041), ('appositives', 0.039), ('joint', 0.039), ('enforcing', 0.039), ('strings', 0.038), ('noun', 0.037), ('shared', 0.037), ('denis', 0.037), ('ij', 0.037), ('preceding', 0.036), ('candidate', 0.035), ('alias', 0.035), ('global', 0.035), ('networks', 0.033), ('finley', 0.033), ('ref', 0.033), ('local', 0.033), ('hidden', 0.032), ('maxent', 0.031), ('predicates', 0.031), ('anaphoric', 0.031), ('antecedent', 0.031), ('bestfirst', 0.03), ('ecf', 0.03), ('ecfo', 0.03), ('exactstrmatch', 0.03), ('hashead', 0.03), ('klenner', 0.03), ('mlnjoint', 0.03), ('mlnlocal', 0.03), ('mmlln', 0.03), ('rbafn', 0.03), ('sentencedistance', 0.03), ('systemmrentionp', 0.03), ('yi', 0.03), ('oregon', 0.03), ('wi', 0.029), ('wt', 0.029), ('luo', 0.029), ('rahman', 0.029), ('richardson', 0.029), ('mi', 0.029), ('ng', 0.028), ('fc', 0.027), ('variables', 0.027), ('entity', 0.026), ('reflexive', 0.026), ('miin', 0.026), ('uryupina', 0.026), ('portland', 0.026), ('co', 0.025), ('constraints', 0.025), ('gender', 0.024), ('hj', 0.024), ('anaphoricity', 0.024), ('weakness', 0.024), ('yoshikawa', 0.024), ('indefinite', 0.024), ('integer', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="73-tfidf-1" href="./emnlp-2012-Joint_Learning_for_Coreference_Resolution_with_Markov_Logic.html">73 emnlp-2012-Joint Learning for Coreference Resolution with Markov Logic</a></p>
<p>Author: Yang Song ; Jing Jiang ; Wayne Xin Zhao ; Sujian Li ; Houfeng Wang</p><p>Abstract: Pairwise coreference resolution models must merge pairwise coreference decisions to generate final outputs. Traditional merging methods adopt different strategies such as the bestfirst method and enforcing the transitivity constraint, but most of these methods are used independently of the pairwise learning methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-201 1shared task on the same dataset. Compared with the best system from CoNLL2011, which employs a rule-based method, our system shows competitive performance.</p><p>2 0.32930198 <a title="73-tfidf-2" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>Author: Heeyoung Lee ; Marta Recasens ; Angel Chang ; Mihai Surdeanu ; Dan Jurafsky</p><p>Abstract: We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable documents, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.</p><p>3 0.21264155 <a title="73-tfidf-3" href="./emnlp-2012-Learning-based_Multi-Sieve_Co-reference_Resolution_with_Knowledge.html">76 emnlp-2012-Learning-based Multi-Sieve Co-reference Resolution with Knowledge</a></p>
<p>Author: Lev Ratinov ; Dan Roth</p><p>Abstract: We explore the interplay of knowledge and structure in co-reference resolution. To inject knowledge, we use a state-of-the-art system which cross-links (or “grounds”) expressions in free text to Wikipedia. We explore ways of using the resulting grounding to boost the performance of a state-of-the-art co-reference resolution system. To maximize the utility of the injected knowledge, we deploy a learningbased multi-sieve approach and develop novel entity-based features. Our end system outperforms the state-of-the-art baseline by 2 B3 F1 points on non-transcript portion of the ACE 2004 dataset.</p><p>4 0.17788798 <a title="73-tfidf-4" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>Author: Quang Do ; Wei Lu ; Dan Roth</p><p>Abstract: This paper addresses the task of constructing a timeline of events mentioned in a given text. To accomplish that, we present a novel representation of the temporal structure of a news article based on time intervals. We then present an algorithmic approach that jointly optimizes the temporal structure by coupling local classifiers that predict associations and temporal relations between pairs of temporal entities with global constraints. Moreover, we present ways to leverage knowledge provided by event coreference to further improve the system performance. Overall, our experiments show that the joint inference model significantly outperformed the local classifiers by 9.2% of relative improvement in F1. The experiments also suggest that good event coreference could make remarkable contribution to a robust event timeline construction system.</p><p>5 0.16881391 <a title="73-tfidf-5" href="./emnlp-2012-Domain_Adaptation_for_Coreference_Resolution%3A_An_Adaptive_Ensemble_Approach.html">36 emnlp-2012-Domain Adaptation for Coreference Resolution: An Adaptive Ensemble Approach</a></p>
<p>Author: Jian Bo Yang ; Qi Mao ; Qiao Liang Xiang ; Ivor Wai-Hung Tsang ; Kian Ming Adam Chai ; Hai Leong Chieu</p><p>Abstract: We propose an adaptive ensemble method to adapt coreference resolution across domains. This method has three features: (1) it can optimize for any user-specified objective measure; (2) it can make document-specific prediction rather than rely on a fixed base model or a fixed set of base models; (3) it can automatically adjust the active ensemble members during prediction. With simplification, this method can be used in the traditional withindomain case, while still retaining the above features. To the best of our knowledge, this work is the first to both (i) develop a domain adaptation algorithm for the coreference resolution problem and (ii) have the above features as an ensemble method. Empirically, we show the benefits of (i) on the six domains of the ACE 2005 data set in domain adaptation set- ting, and of (ii) on both the MUC-6 and the ACE 2005 data sets in within-domain setting.</p><p>6 0.13642964 <a title="73-tfidf-6" href="./emnlp-2012-Monte_Carlo_MCMC%3A_Efficient_Inference_by_Approximate_Sampling.html">91 emnlp-2012-Monte Carlo MCMC: Efficient Inference by Approximate Sampling</a></p>
<p>7 0.1277674 <a title="73-tfidf-7" href="./emnlp-2012-Resolving_Complex_Cases_of_Definite_Pronouns%3A_The_Winograd_Schema_Challenge.html">112 emnlp-2012-Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge</a></p>
<p>8 0.11139368 <a title="73-tfidf-8" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>9 0.078578167 <a title="73-tfidf-9" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>10 0.075573862 <a title="73-tfidf-10" href="./emnlp-2012-An_Entity-Topic_Model_for_Entity_Linking.html">19 emnlp-2012-An Entity-Topic Model for Entity Linking</a></p>
<p>11 0.074490339 <a title="73-tfidf-11" href="./emnlp-2012-Using_Discourse_Information_for_Paraphrase_Extraction.html">135 emnlp-2012-Using Discourse Information for Paraphrase Extraction</a></p>
<p>12 0.064757638 <a title="73-tfidf-12" href="./emnlp-2012-Resolving_This-issue_Anaphora.html">113 emnlp-2012-Resolving This-issue Anaphora</a></p>
<p>13 0.064049527 <a title="73-tfidf-13" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>14 0.056240153 <a title="73-tfidf-14" href="./emnlp-2012-Entity_based_QA_Retrieval.html">41 emnlp-2012-Entity based QA Retrieval</a></p>
<p>15 0.053507566 <a title="73-tfidf-15" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>16 0.051371347 <a title="73-tfidf-16" href="./emnlp-2012-An_%22AI_readability%22_Formula_for_French_as_a_Foreign_Language.html">17 emnlp-2012-An "AI readability" Formula for French as a Foreign Language</a></p>
<p>17 0.047569491 <a title="73-tfidf-17" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>18 0.042534664 <a title="73-tfidf-18" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>19 0.042487167 <a title="73-tfidf-19" href="./emnlp-2012-Locally_Training_the_Log-Linear_Model_for_SMT.html">86 emnlp-2012-Locally Training the Log-Linear Model for SMT</a></p>
<p>20 0.040279202 <a title="73-tfidf-20" href="./emnlp-2012-Name_Phylogeny%3A_A_Generative_Model_of_String_Variation.html">96 emnlp-2012-Name Phylogeny: A Generative Model of String Variation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.197), (1, 0.275), (2, -0.039), (3, -0.315), (4, 0.007), (5, -0.075), (6, -0.15), (7, -0.224), (8, 0.164), (9, -0.008), (10, -0.003), (11, 0.03), (12, 0.07), (13, 0.014), (14, -0.018), (15, -0.016), (16, 0.039), (17, -0.015), (18, 0.029), (19, -0.027), (20, 0.061), (21, -0.003), (22, -0.068), (23, -0.012), (24, -0.075), (25, 0.02), (26, 0.006), (27, -0.024), (28, 0.08), (29, 0.152), (30, -0.151), (31, 0.05), (32, 0.035), (33, -0.03), (34, -0.1), (35, -0.048), (36, 0.079), (37, -0.029), (38, 0.067), (39, 0.032), (40, -0.063), (41, -0.134), (42, 0.074), (43, 0.052), (44, -0.043), (45, -0.032), (46, 0.018), (47, 0.031), (48, -0.071), (49, 0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97323316 <a title="73-lsi-1" href="./emnlp-2012-Joint_Learning_for_Coreference_Resolution_with_Markov_Logic.html">73 emnlp-2012-Joint Learning for Coreference Resolution with Markov Logic</a></p>
<p>Author: Yang Song ; Jing Jiang ; Wayne Xin Zhao ; Sujian Li ; Houfeng Wang</p><p>Abstract: Pairwise coreference resolution models must merge pairwise coreference decisions to generate final outputs. Traditional merging methods adopt different strategies such as the bestfirst method and enforcing the transitivity constraint, but most of these methods are used independently of the pairwise learning methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-201 1shared task on the same dataset. Compared with the best system from CoNLL2011, which employs a rule-based method, our system shows competitive performance.</p><p>2 0.85557342 <a title="73-lsi-2" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>Author: Heeyoung Lee ; Marta Recasens ; Angel Chang ; Mihai Surdeanu ; Dan Jurafsky</p><p>Abstract: We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable documents, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.</p><p>3 0.6823979 <a title="73-lsi-3" href="./emnlp-2012-Learning-based_Multi-Sieve_Co-reference_Resolution_with_Knowledge.html">76 emnlp-2012-Learning-based Multi-Sieve Co-reference Resolution with Knowledge</a></p>
<p>Author: Lev Ratinov ; Dan Roth</p><p>Abstract: We explore the interplay of knowledge and structure in co-reference resolution. To inject knowledge, we use a state-of-the-art system which cross-links (or “grounds”) expressions in free text to Wikipedia. We explore ways of using the resulting grounding to boost the performance of a state-of-the-art co-reference resolution system. To maximize the utility of the injected knowledge, we deploy a learningbased multi-sieve approach and develop novel entity-based features. Our end system outperforms the state-of-the-art baseline by 2 B3 F1 points on non-transcript portion of the ACE 2004 dataset.</p><p>4 0.55983925 <a title="73-lsi-4" href="./emnlp-2012-Domain_Adaptation_for_Coreference_Resolution%3A_An_Adaptive_Ensemble_Approach.html">36 emnlp-2012-Domain Adaptation for Coreference Resolution: An Adaptive Ensemble Approach</a></p>
<p>Author: Jian Bo Yang ; Qi Mao ; Qiao Liang Xiang ; Ivor Wai-Hung Tsang ; Kian Ming Adam Chai ; Hai Leong Chieu</p><p>Abstract: We propose an adaptive ensemble method to adapt coreference resolution across domains. This method has three features: (1) it can optimize for any user-specified objective measure; (2) it can make document-specific prediction rather than rely on a fixed base model or a fixed set of base models; (3) it can automatically adjust the active ensemble members during prediction. With simplification, this method can be used in the traditional withindomain case, while still retaining the above features. To the best of our knowledge, this work is the first to both (i) develop a domain adaptation algorithm for the coreference resolution problem and (ii) have the above features as an ensemble method. Empirically, we show the benefits of (i) on the six domains of the ACE 2005 data set in domain adaptation set- ting, and of (ii) on both the MUC-6 and the ACE 2005 data sets in within-domain setting.</p><p>5 0.5065766 <a title="73-lsi-5" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>Author: Quang Do ; Wei Lu ; Dan Roth</p><p>Abstract: This paper addresses the task of constructing a timeline of events mentioned in a given text. To accomplish that, we present a novel representation of the temporal structure of a news article based on time intervals. We then present an algorithmic approach that jointly optimizes the temporal structure by coupling local classifiers that predict associations and temporal relations between pairs of temporal entities with global constraints. Moreover, we present ways to leverage knowledge provided by event coreference to further improve the system performance. Overall, our experiments show that the joint inference model significantly outperformed the local classifiers by 9.2% of relative improvement in F1. The experiments also suggest that good event coreference could make remarkable contribution to a robust event timeline construction system.</p><p>6 0.48102197 <a title="73-lsi-6" href="./emnlp-2012-Monte_Carlo_MCMC%3A_Efficient_Inference_by_Approximate_Sampling.html">91 emnlp-2012-Monte Carlo MCMC: Efficient Inference by Approximate Sampling</a></p>
<p>7 0.36760679 <a title="73-lsi-7" href="./emnlp-2012-Resolving_Complex_Cases_of_Definite_Pronouns%3A_The_Winograd_Schema_Challenge.html">112 emnlp-2012-Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge</a></p>
<p>8 0.32778466 <a title="73-lsi-8" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>9 0.26391634 <a title="73-lsi-9" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>10 0.2567142 <a title="73-lsi-10" href="./emnlp-2012-Building_a_Lightweight_Semantic_Model_for_Unsupervised_Information_Extraction_on_Short_Listings.html">26 emnlp-2012-Building a Lightweight Semantic Model for Unsupervised Information Extraction on Short Listings</a></p>
<p>11 0.24281906 <a title="73-lsi-11" href="./emnlp-2012-Using_Discourse_Information_for_Paraphrase_Extraction.html">135 emnlp-2012-Using Discourse Information for Paraphrase Extraction</a></p>
<p>12 0.2080659 <a title="73-lsi-12" href="./emnlp-2012-An_%22AI_readability%22_Formula_for_French_as_a_Foreign_Language.html">17 emnlp-2012-An "AI readability" Formula for French as a Foreign Language</a></p>
<p>13 0.19948986 <a title="73-lsi-13" href="./emnlp-2012-Learning_Syntactic_Categories_Using_Paradigmatic_Representations_of_Word_Context.html">79 emnlp-2012-Learning Syntactic Categories Using Paradigmatic Representations of Word Context</a></p>
<p>14 0.19531824 <a title="73-lsi-14" href="./emnlp-2012-An_Entity-Topic_Model_for_Entity_Linking.html">19 emnlp-2012-An Entity-Topic Model for Entity Linking</a></p>
<p>15 0.19157296 <a title="73-lsi-15" href="./emnlp-2012-A_Sequence_Labelling_Approach_to_Quote_Attribution.html">9 emnlp-2012-A Sequence Labelling Approach to Quote Attribution</a></p>
<p>16 0.18707122 <a title="73-lsi-16" href="./emnlp-2012-Entity_based_QA_Retrieval.html">41 emnlp-2012-Entity based QA Retrieval</a></p>
<p>17 0.1859083 <a title="73-lsi-17" href="./emnlp-2012-Locally_Training_the_Log-Linear_Model_for_SMT.html">86 emnlp-2012-Locally Training the Log-Linear Model for SMT</a></p>
<p>18 0.18407778 <a title="73-lsi-18" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>19 0.18268476 <a title="73-lsi-19" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>20 0.18001114 <a title="73-lsi-20" href="./emnlp-2012-Employing_Compositional_Semantics_and_Discourse_Consistency_in_Chinese_Event_Extraction.html">38 emnlp-2012-Employing Compositional Semantics and Discourse Consistency in Chinese Event Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.017), (16, 0.036), (19, 0.293), (25, 0.035), (34, 0.078), (60, 0.097), (63, 0.039), (64, 0.013), (65, 0.069), (70, 0.015), (73, 0.026), (74, 0.022), (76, 0.046), (80, 0.043), (86, 0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73819208 <a title="73-lda-1" href="./emnlp-2012-Joint_Learning_for_Coreference_Resolution_with_Markov_Logic.html">73 emnlp-2012-Joint Learning for Coreference Resolution with Markov Logic</a></p>
<p>Author: Yang Song ; Jing Jiang ; Wayne Xin Zhao ; Sujian Li ; Houfeng Wang</p><p>Abstract: Pairwise coreference resolution models must merge pairwise coreference decisions to generate final outputs. Traditional merging methods adopt different strategies such as the bestfirst method and enforcing the transitivity constraint, but most of these methods are used independently of the pairwise learning methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-201 1shared task on the same dataset. Compared with the best system from CoNLL2011, which employs a rule-based method, our system shows competitive performance.</p><p>2 0.60878748 <a title="73-lda-2" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>Author: Jianxing Yu ; Zheng-Jun Zha ; Tat-Seng Chua</p><p>Abstract: This paper proposes to generate appropriate answers for opinion questions about products by exploiting the hierarchical organization of consumer reviews. The hierarchy organizes product aspects as nodes following their parent-child relations. For each aspect, the reviews and corresponding opinions on this aspect are stored. We develop a new framework for opinion Questions Answering, which enables accurate question analysis and effective answer generation by making use the hierarchy. In particular, we first identify the (explicit/implicit) product aspects asked in the questions and their sub-aspects by referring to the hierarchy. We then retrieve the corresponding review fragments relevant to the aspects from the hierarchy. In order to gener- ate appropriate answers from the review fragments, we develop a multi-criteria optimization approach for answer generation by simultaneously taking into account review salience, coherence, diversity, and parent-child relations among the aspects. We conduct evaluations on 11 popular products in four domains. The evaluated corpus contains 70,359 consumer reviews and 220 questions on these products. Experimental results demonstrate the effectiveness of our approach.</p><p>3 0.50537956 <a title="73-lda-3" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>Author: Heeyoung Lee ; Marta Recasens ; Angel Chang ; Mihai Surdeanu ; Dan Jurafsky</p><p>Abstract: We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable documents, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.</p><p>4 0.47498888 <a title="73-lda-4" href="./emnlp-2012-Learning_Syntactic_Categories_Using_Paradigmatic_Representations_of_Word_Context.html">79 emnlp-2012-Learning Syntactic Categories Using Paradigmatic Representations of Word Context</a></p>
<p>Author: Mehmet Ali Yatbaz ; Enis Sert ; Deniz Yuret</p><p>Abstract: We investigate paradigmatic representations of word context in the domain of unsupervised syntactic category acquisition. Paradigmatic representations of word context are based on potential substitutes of a word in contrast to syntagmatic representations based on properties of neighboring words. We compare a bigram based baseline model with several paradigmatic models and demonstrate significant gains in accuracy. Our best model based on Euclidean co-occurrence embedding combines the paradigmatic context representation with morphological and orthographic features and achieves 80% many-to-one accuracy on a 45-tag 1M word corpus.</p><p>5 0.47002652 <a title="73-lda-5" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms ofweak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.</p><p>6 0.46514648 <a title="73-lda-6" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>7 0.46456569 <a title="73-lda-7" href="./emnlp-2012-Constructing_Task-Specific_Taxonomies_for_Document_Collection_Browsing.html">30 emnlp-2012-Constructing Task-Specific Taxonomies for Document Collection Browsing</a></p>
<p>8 0.45992911 <a title="73-lda-8" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>9 0.45963192 <a title="73-lda-9" href="./emnlp-2012-Domain_Adaptation_for_Coreference_Resolution%3A_An_Adaptive_Ensemble_Approach.html">36 emnlp-2012-Domain Adaptation for Coreference Resolution: An Adaptive Ensemble Approach</a></p>
<p>10 0.45946902 <a title="73-lda-10" href="./emnlp-2012-Learning-based_Multi-Sieve_Co-reference_Resolution_with_Knowledge.html">76 emnlp-2012-Learning-based Multi-Sieve Co-reference Resolution with Knowledge</a></p>
<p>11 0.45910519 <a title="73-lda-11" href="./emnlp-2012-Left-to-Right_Tree-to-String_Decoding_with_Prediction.html">82 emnlp-2012-Left-to-Right Tree-to-String Decoding with Prediction</a></p>
<p>12 0.45815232 <a title="73-lda-12" href="./emnlp-2012-Besting_the_Quiz_Master%3A_Crowdsourcing_Incremental_Classification_Games.html">23 emnlp-2012-Besting the Quiz Master: Crowdsourcing Incremental Classification Games</a></p>
<p>13 0.45775977 <a title="73-lda-13" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>14 0.45684052 <a title="73-lda-14" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<p>15 0.45651269 <a title="73-lda-15" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>16 0.45563498 <a title="73-lda-16" href="./emnlp-2012-Multi-Domain_Learning%3A_When_Do_Domains_Matter%3F.html">92 emnlp-2012-Multi-Domain Learning: When Do Domains Matter?</a></p>
<p>17 0.45388466 <a title="73-lda-17" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>18 0.45290187 <a title="73-lda-18" href="./emnlp-2012-Building_a_Lightweight_Semantic_Model_for_Unsupervised_Information_Extraction_on_Short_Listings.html">26 emnlp-2012-Building a Lightweight Semantic Model for Unsupervised Information Extraction on Short Listings</a></p>
<p>19 0.45032755 <a title="73-lda-19" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>20 0.44874203 <a title="73-lda-20" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
