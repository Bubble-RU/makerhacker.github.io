<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>44 emnlp-2012-Excitatory or Inhibitory: A New Semantic Orientation Extracts Contradiction and Causality from the Web</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-44" href="#">emnlp2012-44</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>44 emnlp-2012-Excitatory or Inhibitory: A New Semantic Orientation Extracts Contradiction and Causality from the Web</h1>
<br/><p>Source: <a title="emnlp-2012-44-pdf" href="http://aclweb.org/anthology//D/D12/D12-1057.pdf">pdf</a></p><p>Author: Chikara Hashimoto ; Kentaro Torisawa ; Stijn De Saeger ; Jong-Hoon Oh ; Jun'ichi Kazama</p><p>Abstract: We propose a new semantic orientation, Excitation, and its automatic acquisition method. Excitation is a semantic property of predicates that classifies them into excitatory, inhibitory and neutral. We show that Excitation is useful for extracting contradiction pairs (e.g., destroy cancer develop cancer) and causality pairs (cea.ng.c,e rin ⊥cre daesvee lionp c craimnece r⇒) ahnedig chatuensa laintyx pieatyir)s. (Oe.ugr. ,ex ipncerreimaseent ins shc roimw eth ⇒at w heitihg ahtuetnom anatxicieatlyly). acquired Excitation knowledge we can extract one million contradiction pairs and 500,000 causality pairs with about 70% precision from a 600 million page Web corpus. Furthermore, by combining these extracted causality and contradiction pairs, we can generate one million plausible causality hypotheses that are not written in any single sentence in our corpus ⊥ with reasonable precision.</p><p>Reference: <a title="emnlp-2012-44-reference" href="../emnlp2012_reference/emnlp-2012-Excitatory_or_Inhibitory%3A_A_New_Semantic_Orientation_Extracts_Contradiction_and_Causality_from_the_Web_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 acquired Excitation knowledge we can extract one million contradiction pairs and 500,000 causality pairs with about 70% precision from a 600 million page Web corpus. [sent-11, score-0.819]
</p><p>2 Furthermore, by combining these extracted causality and contradiction pairs, we can generate one million plausible causality hypotheses that are not written in any single sentence in our corpus  ⊥  with reasonable precision. [sent-12, score-1.138]
</p><p>3 ” a QA system must recognize that the sentence “the Fukushima nuclear power plant caused radioactive pollution and contaminated the crops in Japan ” contains a causal relation and that contaminate crops entails ruin crops but contradicts preserve crops. [sent-15, score-0.426]
</p><p>4 To facilitate the acquisition of causality, contradiction, paraphrase and entailment relations between events we propose a new semantic orientation, Excitation, that classifies unary predicates (templates, hereafter) into excitatory, inhibitory and neutral. [sent-16, score-0.329]
</p><p>5 An excitatory template entails that the main function or 619 § rovellia,  ¶ kazama}@nict . [sent-17, score-0.389]
</p><p>6 Excitation is useful for extracting contradiction; if two templates with similar distributional profiles have opposite Excitation polarities, they tend to be contradictions (e. [sent-24, score-0.403]
</p><p>7 Two excitatory or inhibitory templates that co-occur in some temporal or logical order in the same narrative often describe a causal chain of events, like “the Fukushima nuclear power plant caused radioactive pollution and contaminated  extracted contradictions  can distin-  crops in Japan”. [sent-31, score-1.126]
</p><p>8 Our method acquires Excitation templates based on certain natural, language independent constraints on narrative structures found in text. [sent-33, score-0.317]
</p><p>9 Our methods extract one million contradiction pairs with over 70% precision, and 500,000 causality pairs with about 70% precision from a 600 million page Web corpus. [sent-35, score-0.79]
</p><p>10 Moreover, by combining these extracted causality pairs and contradiction pairs, we generated one million plausible causality hypotheses that were not PLraoncge uadgineg Lse oafr tnhineg 2,0 p1a2g Jeosin 61t C9–o6n3f0e,re Jnecjue Iosnla Enmd,p Kiroicraela, M 1e2t–h1o4ds Ju ilny N 20a1tu2r. [sent-36, score-1.17]
</p><p>11 For example, a causality hypothesis prevent radioactive pollution ⇒ preserve crops can bp ree generated afrcotmive an elxlutrtiaocnte ⇒d causality cause radioactive pollution ⇒ contaminate crops. [sent-39, score-1.122]
</p><p>12 Excitation classifies templates into excitatory, inhibitory, and neutral, as explained below. [sent-42, score-0.317]
</p><p>13 excitatory templates entail that the function, effect, purpose or role of their argument’s referent is activated or enhanced. [sent-43, score-0.676]
</p><p>14 , cause X, buy X, produce X, import X, increase X, enable X) inhibitory templates entail that the function, ef-  fect, purpose or role of their argument’s referent is deactivated or suppressed. [sent-46, score-0.642]
</p><p>15 , prevent X, discard X, remedy X, decrease X, disable X) neutral templates are neither excitatory nor inhibitory. [sent-49, score-0.696]
</p><p>16 In this study, we aim to acquire excitatory and inhibitory templates that are useful for extracting contradiction and causality, though neutral templates are the most frequent in our data (See Section 5. [sent-54, score-1.49]
</p><p>17 Collectively we call excitatory and inhibitory templates Excitation templates, and excitatory and inhibitory two opposite polarities. [sent-56, score-1.459]
</p><p>18 We introduce constraints in the co-occurrence of templates in text that seem both ro-  bust and language independent in Section 3. [sent-74, score-0.317]
</p><p>19 First we construct a template network where nodes are templates and links represent that two connected templates have either SAME or OPPOSITE polarities. [sent-77, score-0.812]
</p><p>20 Given 46 manually prepared seed templates we calculate the Excitation value of each template, a value in range [− 1, 1] that visa positive cifh ht theem template visa excitatory ean [−d negative if it is inhibitory. [sent-78, score-0.795]
</p><p>21 Technically, our method treats all templates as excitatory or inhibitory, and, upon completion, regards templates with small absolute Excitation values as neutral. [sent-79, score-0.92]
</p><p>22 1 Characteristics of Excitation Templates Our method exploits natural discourse constraints on the possible combinations of (a) the polarity of cooccurring templates, (b) the nouns that fill their argument slots and (c) the connectives that link the templates in a given sentence. [sent-83, score-0.519]
</p><p>23 Next we extract sentences from the Web in which two templates co-occur and are joined by one of these connectives, and then classify the noun pairs filling the templates’ argument slots into “positivelyassociated” and “negatively-associated” noun pairs (PNPs and NNPs). [sent-110, score-0.509]
</p><p>24 We found that PNPs only fill the argument slots of (a) same Excitation polarity templates connected by AND/THUS-type connectives (examples 1 and 2 in Figure 1), or (b) opposite Excitation polarity templates connected by a BUTtype connectives (examples 3 and 4). [sent-113, score-1.076]
</p><p>25 NNPs only fill the argument slots of (a) opposite Excitation polarity templates connected by AND/THUS-type connectives (example 5), or (b) same polarity templates connected by a BUT-type connective (example 6). [sent-117, score-1.033]
</p><p>26 Conversely, we can know whether a noun pair is PNP or NNP if we know whether two templates whose slots are filled with the noun pair have the same or opposite polarities. [sent-120, score-0.457]
</p><p>27 We therefore adopt a bootstrapping method (Figure 2) that starts from manually prepared excitatory and inhibitory seed templates (Step 1 in Figure 2). [sent-128, score-0.972]
</p><p>28 Our method begins by extracting noun pairs from the Web that co-occur with two seed templates connected by a AND/THUS- or BUT-type connective, and classifies these noun pairs into PNPs and NNPs based on the constraint matrix (Steps 2 and 3). [sent-129, score-0.521]
</p><p>29 Links (either SAME or OPPOSITE) between all template pairs are determined by the constraint matrix (Step 4), and we construct a template network from both seed and non-seed template pairs (Step 5). [sent-131, score-0.476]
</p><p>30 Our method calculates the Excitation values for all the templates in the network by first assigning Excitation values +1 and −1 to the excitatory ianngd inhibitory s veaeldu templates, a −nd1 applies a spreading activation method proposed by Takamura et al. [sent-132, score-0.958]
</p><p>31 This method calcu-  lates all templates’ excitation values by solving the network constraints imposed by the SAME and OPPOSITE links, and the Excitation values of the seed templates (This method is detailed in Section 3. [sent-134, score-0.963]
</p><p>32 ×  In each iteration i, our method selects the N itoprInan ekaecdh iatnedra tbiootnto i,m o-urarn mkeetdh templates as Nad ×ditiio tonpalseed templates for the next iteration (N is set to 30) (Step 7). [sent-136, score-0.634]
</p><p>33 Our method then constructs a new template network using the augmented seed templates and restarts the calculation process. [sent-137, score-0.523]
</p><p>34 To prepare the initial seed templates we constructed a maximal template network that could in theory be created by our bootstrapping method. [sent-140, score-0.545]
</p><p>35 This maximal network consists of any two templates that co-occur in a sentence with any connective, regardless of their arguments. [sent-141, score-0.362]
</p><p>36 We manually selected 36 excitatory and 10 inhibitory seed templates from among 114 templates with the most links in the network (See supplementary materials). [sent-142, score-1.303]
</p><p>37 Both models capture the spreading of activation (either spin direction or excitation polarity) between neighboring objects in a network. [sent-150, score-0.651]
</p><p>38 W=e { regard templates as esl teoct lirnoknss baentdw Eeexncitation polarities as their spins (up and down correspond to excitatory and inhibitory). [sent-154, score-0.636]
</p><p>39 We define the weight wij of the link between templates iand j as:  wij={ −1/1√/√d(di)(id)(dj)(j) i f SOAPMPOES(IiT,Ej)(i,j)  Here, d(i) den√otes the number of templates linked to i. [sent-155, score-0.66]
</p><p>40 We obtain excitation values by minimizing the above energy function. [sent-157, score-0.543]
</p><p>41 Initially seed templates are given values +1or −1 depending on whether they are excitatory or inhibitory, annddin ogth oenrs w are given e0y. [sent-160, score-0.661]
</p><p>42 1 Contradiction Extraction Our first knowledge acquisition method extracts contradiction pairs like destroy cancer ⊥ develop cancer, ibctaisoend on our assumption t chaant they ⊥oft deenv consist of distributionally similar templates that have a  ×  sharp contrast in Excitation value. [sent-175, score-0.737]
</p><p>43 Concretely, we extract two phrases as a contradiction pair if (a) their templates have opposite Excitation polarities, (b) they share the same argument noun, and (c) the part-of-speech of their predicates is the same. [sent-176, score-0.631]
</p><p>44 2 Causality Extraction Our second knowledge acquisition method extracts causality pairs like increase in crime ⇒ heighten anxiety yth pata co-occur nwcriteha AND/THUS-type connectives in a sentence. [sent-189, score-0.705]
</p><p>45 The assumption is that if two templates (t1 and t2) with a strong Excitation tendency are connected by an AND/THUS-type connective in a sentence, the event described by t1 and its argument n1 tends to be a cause of the event described by t2 and its argument n2. [sent-190, score-0.498]
</p><p>46 We focus on extracting causality pairs that cooccur with only “non-causal connectives” like and, which are AND/THUS-type connectives that do NOT explicitly signal causality, since causal connectives like thus can mask the effectiveness of Excitation. [sent-193, score-0.693]
</p><p>47 We extract two templates such as increase in X and heighten Y co-occurring with only non-causal connectives, as well as the noun pair that fills the two templates’ slots (e. [sent-195, score-0.409]
</p><p>48 Cs ranks the obtained causality pairs: Cs(p1,p2)  = |s1| |s2|  Here p1 and p2 are the phrases of causality pair, and |s1| and |s2 | are absolute Excitation values of p1’s a|snd| pa n2d’s templates. [sent-199, score-0.822]
</p><p>49 sAolsu tise common inn vthaleu elsite oraftu pre, this notion of causality should be interpreted probabilistically rather than logically, i. [sent-200, score-0.411]
</p><p>50 , we interpret causality A ⇒ B as “if A happens, the probability of cBa iunscarlietayse As” ⇒. [sent-202, score-0.411]
</p><p>51 3 Causality Hypothesis Generation Our third knowledge acquisition method generates plausible causality hypotheses that are not written in any single sentence using the previously extracted contradiction and causality pairs. [sent-205, score-1.149]
</p><p>52 However, at least under our probabilistic interpretation, taking the inverse of a given causality pair using the extracted contradiction pairs proves to be a viable strategy for generating non-trivial causality hypotheses, as our experiments in Section 5. [sent-215, score-1.072]
</p><p>53 For an extracted causality pair, we generate its inverse as a causality hypothesis by replacing both phrases in the original pair with their contradiction counterparts. [sent-217, score-1.04]
</p><p>54 Hp(q1, q2) = Ct(p1, q1) Ct(p2, q2) Cs0(p1,p2) Here, q1 and q2 are two phrases of a causality hypothesis. [sent-221, score-0.411]
</p><p>55 That is, p1 ⊥ q1 and p2 ⊥ q2 are contradiction pairs, and Ct(p1⊥ , q q1) and Ct⊥(p q2, q2) are their contradiction scores. [sent-223, score-0.436]
</p><p>56 Cs0(p1 ,p2) is the  ×  original causality’s causality score. [sent-224, score-0.411]
</p><p>57 Moreover, using only the acquired templates we extracted one million contradiction pairs with more than 70% precision, and 500,000 causality pairs with about 70% precision. [sent-230, score-1.075]
</p><p>58 Further, using only these extracted contradiction and causality pairs we generated one million causality hypotheses with 57% precision. [sent-231, score-1.17]
</p><p>59 In our experiments we removed evaluation samples containing the initial seed templates and examples used for annotation instruction from the evalua-  tion data. [sent-232, score-0.423]
</p><p>60 We restricted the argument positions of templates to ha (topic), ga (nominative), wo (accusative), ni (dative), and de (instrumental). [sent-240, score-0.38]
</p><p>61 We discarded templates appearing fewer than 20 times in compound sentences (regardless of connectives) in our corpus. [sent-241, score-0.317]
</p><p>62 Among these, the bootstrapping process classified 8,685 templates as excitatory and 2,140 as inhibitory. [sent-246, score-0.625]
</p><p>63 ALLEXC regards all templates that are randomly extracted from the Web as excitatory, since in our data excitatory templates outnumber inhibitory ones. [sent-249, score-1.178]
</p><p>64 Actually, in our data neutral templates represent the most frequent class, but since our objective is to acquire excitatory and inhibitory templates, a baseline marking all templates as neutral would make little sense. [sent-250, score-1.344]
</p><p>65 SIM is a distributional similarity baseline that takes as input the same 10,825 templates of PROPtmp above, constructs a network by connecting two templates whose distributional similarity is greater than zero, and regards two connected templates as having the same polarity. [sent-251, score-1.026]
</p><p>66 The weight of the links between templates is set to their distri-  butional similarity based on Lin (1998). [sent-252, score-0.317]
</p><p>67 Then SIM is given the same initial seed templates as PROPtmp, by which it calculates the Excitation values of templates using Takamura et al. [sent-253, score-0.692]
</p><p>68 As a result, SIM assigned positive Excitation values to all templates, and except for the 10 inhibitory initial seed templates no templates were regarded inhibitory. [sent-255, score-0.95]
</p><p>69 Evaluation scheme We randomly sampled 100 templates each from PROPtmp’s 8,685 excitatory candidates, PROPtmp’s 2,140 inhibitory candidates, all the ALLEXC’s templates, and all the SIM’s templates, i. [sent-256, score-0.861]
</p><p>70 Results for excitatory In the top graph in Figure 3, ‘Proposed’ shows PROPtmp’s precision curve. [sent-261, score-0.311]
</p><p>71 4861050 10’P0 rop 1o5s0e0d’ 20 0 250 Top-N Figure 3: Precision of template acquisition: excitatory (top) and inhibitory (bottom). [sent-267, score-0.647]
</p><p>72 For calculating precision, only the 37 samples labeled excitatory were regarded as correct. [sent-269, score-0.334]
</p><p>73 SIM’s low performance reflects the fact that templates with opposite polarities are sometimes distributionally similar, and as a result get connected by SAME links. [sent-273, score-0.457]
</p><p>74 Results for inhibitory ‘Proposed’ in the bottom graph in Figure 3 shows the precision curve drawn from the 100 samples of PROPtmp’s inhibitory candidates. [sent-274, score-0.616]
</p><p>75 Only the 41 inhibitory samples were regarded as correct. [sent-276, score-0.306]
</p><p>76 Observations about argument positions Among the 200 evaluation samples of PROPtmp (for both excitatory and inhibitory evaluations), 52 were judged as excitatory, 47 as inhibitory, and 77 as neutral. [sent-284, score-0.669]
</p><p>77 For the excitatory templates, the numbers of nominative, topic, accusative, dative, and instrumental argument positions are 15, 11, 10, 8, and 8, respectively. [sent-285, score-0.328]
</p><p>78 Likewise, we found no noticeable bias regarding their usefulness for contradiction and causality acquisition reported shortly, too. [sent-289, score-0.676]
</p><p>79 2  Contradiction Extraction  This section shows that our proposed method for contradiction extraction (PROPcont) extracted one million contradiction pairs with more than 70% precision, and that Excitation values are useful for contradiction ranking. [sent-294, score-0.722]
</p><p>80 As input for PROPcont we took the top 2,000 excitatory and the top 500 inhibitory templates from the previous experiment (i. [sent-295, score-0.861]
</p><p>81 However, we found that PROPcont’s 200 samples contained 194 different template pairs, suggesting that our method can acquire a large variety of contradiction phrases. [sent-313, score-0.391]
</p><p>82 It is tricky since it is excitatory when taking arguments like function, while it is inhibitory when taking arguments like disorder. [sent-321, score-0.544]
</p><p>83 However, PROPtmp currently cannot distinguish these usages and judged it as inhibitory in our experiments in Section 5. [sent-322, score-0.293]
</p><p>84 Summary PROPcont is a low cost but high performance method, since it acquired one million contradiction pairs with over 70% precision from only the 46 initial seed templates. [sent-326, score-0.398]
</p><p>85 3  Causality Extraction  We show that our method for causality extraction (PROPcaus) extracted 500,000 causality pairs with about 70% precision, and that Excitation values contribute to the ranking of causal pairs. [sent-330, score-0.93]
</p><p>86 PROPcaus took as input all 10,825 templates classified by PROPtmp. [sent-331, score-0.317]
</p><p>87 246801 20 40 6’PR0roapn0’Fodsrem8qd0’ 0 1e+06 Top-N  Figure 5: Precision of causality extraction. [sent-337, score-0.411]
</p><p>88 The labels ‘ ’ and ‘ ’ denote whether a pair is causality or not. [sent-346, score-0.411]
</p><p>89 2; even if two Excitation templates co-occur in a sentence with an AND/THUS-type connective, they sometimes do not constitute causality. [sent-348, score-0.317]
</p><p>90 Summary PROPcaus performs well since it extracted 500,000 causality pairs with about 70%  precision. [sent-351, score-0.443]
</p><p>91 Moreover,  to causality  Excitation  values contribute  ranking since PROPcaus  outperformed  FREQ by a large margin. [sent-352, score-0.411]
</p><p>92 Then we conclude that our assumption on causality extraction is confirmed. [sent-353, score-0.411]
</p><p>93 4  Causality Hypothesis Generation  Here we show that our causality hypothesis generation method in Section 4. [sent-356, score-0.411]
</p><p>94 3, except that we presented them with source causality pairs from which hypotheses were generated, as well as the original sentences of these source pairs. [sent-362, score-0.505]
</p><p>95 The la-  bels ‘ ’ and ‘ ’ denote whether a pair is causality or not. [sent-369, score-0.411]
</p><p>96 our causality extraction method PROPcaus; the case was erroneous since its original causality was erroneous. [sent-371, score-0.822]
</p><p>97 The second  case was due to the fact that  one of the contradiction phrase pairs used to generate the hypothesis was in fact not contradictory  気を  コ  ン ト ロ ールす  る  ⊥ 景気が良 く な る  (景 ‘con-  trol economic conditions 6⊥⊥economic conditions improve’). [sent-372, score-0.296]
</p><p>98 From these results, we conclude that our assumption on causality hypothesis generation is valid. [sent-373, score-0.411]
</p><p>99 Many methods for extracting causality or scriptlike knowledge between events exist (Girju, 2003; Torisawa, 2005; Torisawa, 2006; Abe et al. [sent-383, score-0.435]
</p><p>100 Our experiments showed that Excitation allows to acquire one million contradiction pairs with over 70% precision, as well as causality pairs and causality hypotheses of the same volume with reasonable precision from the Web. [sent-392, score-1.249]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('excitation', 0.543), ('causality', 0.411), ('templates', 0.317), ('excitatory', 0.286), ('inhibitory', 0.258), ('contradiction', 0.218), ('proptmp', 0.122), ('propcont', 0.113), ('propcaus', 0.105), ('template', 0.103), ('connectives', 0.087), ('cont', 0.077), ('cancer', 0.076), ('causal', 0.076), ('neutral', 0.072), ('anxiety', 0.065), ('crops', 0.065), ('hypotheses', 0.062), ('seed', 0.058), ('cigarettes', 0.057), ('lung', 0.057), ('pnps', 0.057), ('spin', 0.056), ('opposite', 0.054), ('allexc', 0.049), ('quasi', 0.049), ('samples', 0.048), ('acquisition', 0.047), ('network', 0.045), ('referent', 0.044), ('connective', 0.044), ('argument', 0.042), ('nnps', 0.041), ('pnp', 0.041), ('orientation', 0.04), ('polarity', 0.039), ('sim', 0.037), ('million', 0.036), ('judged', 0.035), ('slots', 0.034), ('torisawa', 0.034), ('kentaro', 0.033), ('polarities', 0.033), ('takamura', 0.033), ('contaminate', 0.032), ('contradictions', 0.032), ('heighten', 0.032), ('pollution', 0.032), ('quit', 0.032), ('radioactive', 0.032), ('randcont', 0.032), ('undecided', 0.032), ('pairs', 0.032), ('activation', 0.031), ('prepared', 0.031), ('crime', 0.031), ('preserve', 0.031), ('connected', 0.03), ('acquired', 0.029), ('activated', 0.029), ('fleiss', 0.029), ('nuclear', 0.028), ('curve', 0.027), ('annotators', 0.027), ('noun', 0.026), ('wij', 0.026), ('materials', 0.026), ('precision', 0.025), ('freq', 0.025), ('virus', 0.025), ('kazama', 0.025), ('depreciation', 0.024), ('destroy', 0.024), ('immune', 0.024), ('kurohashi', 0.024), ('npfreq', 0.024), ('smells', 0.024), ('smoke', 0.024), ('smoked', 0.024), ('yen', 0.024), ('events', 0.024), ('cr', 0.023), ('distributionally', 0.023), ('contradictory', 0.023), ('saeger', 0.023), ('stijn', 0.023), ('economic', 0.023), ('se', 0.023), ('cause', 0.023), ('japanese', 0.023), ('supplementary', 0.022), ('acquire', 0.022), ('ct', 0.022), ('bootstrapping', 0.022), ('diminish', 0.021), ('inquirer', 0.021), ('spreading', 0.021), ('prevent', 0.021), ('oh', 0.021), ('de', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000015 <a title="44-tfidf-1" href="./emnlp-2012-Excitatory_or_Inhibitory%3A_A_New_Semantic_Orientation_Extracts_Contradiction_and_Causality_from_the_Web.html">44 emnlp-2012-Excitatory or Inhibitory: A New Semantic Orientation Extracts Contradiction and Causality from the Web</a></p>
<p>Author: Chikara Hashimoto ; Kentaro Torisawa ; Stijn De Saeger ; Jong-Hoon Oh ; Jun'ichi Kazama</p><p>Abstract: We propose a new semantic orientation, Excitation, and its automatic acquisition method. Excitation is a semantic property of predicates that classifies them into excitatory, inhibitory and neutral. We show that Excitation is useful for extracting contradiction pairs (e.g., destroy cancer develop cancer) and causality pairs (cea.ng.c,e rin ⊥cre daesvee lionp c craimnece r⇒) ahnedig chatuensa laintyx pieatyir)s. (Oe.ugr. ,ex ipncerreimaseent ins shc roimw eth ⇒at w heitihg ahtuetnom anatxicieatlyly). acquired Excitation knowledge we can extract one million contradiction pairs and 500,000 causality pairs with about 70% precision from a 600 million page Web corpus. Furthermore, by combining these extracted causality and contradiction pairs, we can generate one million plausible causality hypotheses that are not written in any single sentence in our corpus ⊥ with reasonable precision.</p><p>2 0.079772495 <a title="44-tfidf-2" href="./emnlp-2012-Why_Question_Answering_using_Sentiment_Analysis_and_Word_Classes.html">137 emnlp-2012-Why Question Answering using Sentiment Analysis and Word Classes</a></p>
<p>Author: Jong-Hoon Oh ; Kentaro Torisawa ; Chikara Hashimoto ; Takuya Kawada ; Stijn De Saeger ; Jun'ichi Kazama ; Yiou Wang</p><p>Abstract: In this paper we explore the utility of sentiment analysis and semantic word classes for improving why-question answering on a large-scale web corpus. Our work is motivated by the observation that a why-question and its answer often follow the pattern that if something undesirable happens, the reason is also often something undesirable, and if something desirable happens, the reason is also often something desirable. To the best of our knowledge, this is the first work that introduces sentiment analysis to non-factoid question answering. We combine this simple idea with semantic word classes for ranking answers to why-questions and show that on a set of 850 why-questions our method gains 15.2% improvement in precision at the top-1 answer over a baseline state-of-the-art QA system that achieved the best performance in a shared task of Japanese non-factoid QA in NTCIR-6.</p><p>3 0.049327858 <a title="44-tfidf-3" href="./emnlp-2012-Bilingual_Lexicon_Extraction_from_Comparable_Corpora_Using_Label_Propagation.html">25 emnlp-2012-Bilingual Lexicon Extraction from Comparable Corpora Using Label Propagation</a></p>
<p>Author: Akihiro Tamura ; Taro Watanabe ; Eiichiro Sumita</p><p>Abstract: This paper proposes a novel method for lexicon extraction that extracts translation pairs from comparable corpora by using graphbased label propagation. In previous work, it was established that performance drastically decreases when the coverage of a seed lexicon is small. We resolve this problem by utilizing indirect relations with the bilingual seeds together with direct relations, in which each word is represented by a distribution of translated seeds. The seed distributions are propagated over a graph representing relations among words, and translation pairs are extracted by identifying word pairs with a high similarity in the seed distributions. We propose two types of the graphs: a co-occurrence graph, representing co-occurrence relations between words, and a similarity graph, representing context similarities between words. Evaluations using English and Japanese patent comparable corpora show that our proposed graph propagation method outperforms conventional methods. Further, the similarity graph achieved improved performance by clustering synonyms into the same translation.</p><p>4 0.041613467 <a title="44-tfidf-4" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>Author: Lizhen Qu ; Rainer Gemulla ; Gerhard Weikum</p><p>Abstract: We propose the weakly supervised MultiExperts Model (MEM) for analyzing the semantic orientation of opinions expressed in natural language reviews. In contrast to most prior work, MEM predicts both opinion polarity and opinion strength at the level of individual sentences; such fine-grained analysis helps to understand better why users like or dislike the entity under review. A key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength. For this reason, MEM is weakly supervised: It starts with potentially noisy indicators obtained from coarse-grained training data (i.e., document-level ratings), a small set of diverse base predictors, and, if available, small amounts of fine-grained training data. We integrate these noisy indicators into a unified probabilistic framework using ideas from ensemble learning and graph-based semi-supervised learning. Our experiments indicate that MEM outperforms state-of-the-art methods by a significant margin.</p><p>5 0.040091176 <a title="44-tfidf-5" href="./emnlp-2012-Collocation_Polarity_Disambiguation_Using_Web-based_Pseudo_Contexts.html">28 emnlp-2012-Collocation Polarity Disambiguation Using Web-based Pseudo Contexts</a></p>
<p>Author: Yanyan Zhao ; Bing Qin ; Ting Liu</p><p>Abstract: This paper focuses on the task of collocation polarity disambiguation. The collocation refers to a binary tuple of a polarity word and a target (such as ⟨long, battery life⟩ or ⟨long, ast atratrguep⟩t) (, siunc whh aisch ⟨ ltohneg s,en btatitmeernyt l iofrei⟩en otrat ⟨iolonn gof, tshtaer polarity wwohirdch (“long”) changes along owniothf different targets (“battery life” or “startup”). To disambiguate a collocation’s polarity, previous work always turned to investigate the polarities of its surrounding contexts, and then assigned the majority polarity to the collocation. However, these contexts are limited, thus the resulting polarity is insufficient to be reliable. We therefore propose an unsupervised three-component framework to expand some pseudo contexts from web, to help disambiguate a collocation’s polarity.Without using any additional labeled data, experiments , show that our method is effective.</p><p>6 0.03906139 <a title="44-tfidf-6" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>7 0.038233016 <a title="44-tfidf-7" href="./emnlp-2012-Part-of-Speech_Tagging_for_Chinese-English_Mixed_Texts_with_Dynamic_Features.html">106 emnlp-2012-Part-of-Speech Tagging for Chinese-English Mixed Texts with Dynamic Features</a></p>
<p>8 0.037871767 <a title="44-tfidf-8" href="./emnlp-2012-Learning_Verb_Inference_Rules_from_Linguistically-Motivated_Evidence.html">80 emnlp-2012-Learning Verb Inference Rules from Linguistically-Motivated Evidence</a></p>
<p>9 0.037773538 <a title="44-tfidf-9" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>10 0.0377701 <a title="44-tfidf-10" href="./emnlp-2012-Active_Learning_for_Imbalanced_Sentiment_Classification.html">15 emnlp-2012-Active Learning for Imbalanced Sentiment Classification</a></p>
<p>11 0.036122367 <a title="44-tfidf-11" href="./emnlp-2012-Open_Language_Learning_for_Information_Extraction.html">100 emnlp-2012-Open Language Learning for Information Extraction</a></p>
<p>12 0.03572591 <a title="44-tfidf-12" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>13 0.033855814 <a title="44-tfidf-13" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>14 0.032873426 <a title="44-tfidf-14" href="./emnlp-2012-Resolving_Complex_Cases_of_Definite_Pronouns%3A_The_Winograd_Schema_Challenge.html">112 emnlp-2012-Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge</a></p>
<p>15 0.032343753 <a title="44-tfidf-15" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>16 0.031726804 <a title="44-tfidf-16" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>17 0.031613804 <a title="44-tfidf-17" href="./emnlp-2012-Enlarging_Paraphrase_Collections_through_Generalization_and_Instantiation.html">39 emnlp-2012-Enlarging Paraphrase Collections through Generalization and Instantiation</a></p>
<p>18 0.030265799 <a title="44-tfidf-18" href="./emnlp-2012-Identifying_Constant_and_Unique_Relations_by_using_Time-Series_Text.html">62 emnlp-2012-Identifying Constant and Unique Relations by using Time-Series Text</a></p>
<p>19 0.028679786 <a title="44-tfidf-19" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>20 0.02763886 <a title="44-tfidf-20" href="./emnlp-2012-Generalizing_Sub-sentential_Paraphrase_Acquisition_across_Original_Signal_Type_of_Text_Pairs.html">58 emnlp-2012-Generalizing Sub-sentential Paraphrase Acquisition across Original Signal Type of Text Pairs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.104), (1, 0.035), (2, -0.015), (3, 0.03), (4, 0.071), (5, -0.013), (6, 0.015), (7, 0.004), (8, -0.032), (9, -0.041), (10, -0.021), (11, -0.05), (12, -0.002), (13, -0.011), (14, 0.022), (15, -0.03), (16, -0.07), (17, 0.126), (18, 0.022), (19, -0.005), (20, -0.088), (21, 0.035), (22, -0.001), (23, 0.085), (24, 0.027), (25, -0.073), (26, 0.043), (27, 0.026), (28, 0.065), (29, -0.043), (30, -0.104), (31, 0.025), (32, -0.041), (33, 0.124), (34, 0.138), (35, -0.029), (36, 0.124), (37, 0.165), (38, 0.012), (39, 0.033), (40, -0.231), (41, 0.09), (42, -0.343), (43, 0.045), (44, -0.006), (45, -0.18), (46, -0.222), (47, -0.131), (48, -0.125), (49, -0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96327841 <a title="44-lsi-1" href="./emnlp-2012-Excitatory_or_Inhibitory%3A_A_New_Semantic_Orientation_Extracts_Contradiction_and_Causality_from_the_Web.html">44 emnlp-2012-Excitatory or Inhibitory: A New Semantic Orientation Extracts Contradiction and Causality from the Web</a></p>
<p>Author: Chikara Hashimoto ; Kentaro Torisawa ; Stijn De Saeger ; Jong-Hoon Oh ; Jun'ichi Kazama</p><p>Abstract: We propose a new semantic orientation, Excitation, and its automatic acquisition method. Excitation is a semantic property of predicates that classifies them into excitatory, inhibitory and neutral. We show that Excitation is useful for extracting contradiction pairs (e.g., destroy cancer develop cancer) and causality pairs (cea.ng.c,e rin ⊥cre daesvee lionp c craimnece r⇒) ahnedig chatuensa laintyx pieatyir)s. (Oe.ugr. ,ex ipncerreimaseent ins shc roimw eth ⇒at w heitihg ahtuetnom anatxicieatlyly). acquired Excitation knowledge we can extract one million contradiction pairs and 500,000 causality pairs with about 70% precision from a 600 million page Web corpus. Furthermore, by combining these extracted causality and contradiction pairs, we can generate one million plausible causality hypotheses that are not written in any single sentence in our corpus ⊥ with reasonable precision.</p><p>2 0.45067152 <a title="44-lsi-2" href="./emnlp-2012-Bilingual_Lexicon_Extraction_from_Comparable_Corpora_Using_Label_Propagation.html">25 emnlp-2012-Bilingual Lexicon Extraction from Comparable Corpora Using Label Propagation</a></p>
<p>Author: Akihiro Tamura ; Taro Watanabe ; Eiichiro Sumita</p><p>Abstract: This paper proposes a novel method for lexicon extraction that extracts translation pairs from comparable corpora by using graphbased label propagation. In previous work, it was established that performance drastically decreases when the coverage of a seed lexicon is small. We resolve this problem by utilizing indirect relations with the bilingual seeds together with direct relations, in which each word is represented by a distribution of translated seeds. The seed distributions are propagated over a graph representing relations among words, and translation pairs are extracted by identifying word pairs with a high similarity in the seed distributions. We propose two types of the graphs: a co-occurrence graph, representing co-occurrence relations between words, and a similarity graph, representing context similarities between words. Evaluations using English and Japanese patent comparable corpora show that our proposed graph propagation method outperforms conventional methods. Further, the similarity graph achieved improved performance by clustering synonyms into the same translation.</p><p>3 0.34359324 <a title="44-lsi-3" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>Author: Wen-tau Yih ; Geoffrey Zweig ; John Platt</p><p>Abstract: Existing vector space models typically map synonyms and antonyms to similar word vectors, and thus fail to represent antonymy. We introduce a new vector space representation where antonyms lie on opposite sides of a sphere: in the word vector space, synonyms have cosine similarities close to one, while antonyms are close to minus one. We derive this representation with the aid of a thesaurus and latent semantic analysis (LSA). Each entry in the thesaurus a word sense along with its synonyms and antonyms is treated as a “document,” and the resulting document collection is subjected to LSA. The key contribution of this work is to show how to assign signs to the entries in the co-occurrence matrix on which LSA operates, so as to induce a subspace with the desired property. – – We evaluate this procedure with the Graduate Record Examination questions of (Mohammed et al., 2008) and find that the method improves on the results of that study. Further improvements result from refining the subspace representation with discriminative training, and augmenting the training data with general newspaper text. Altogether, we improve on the best previous results by 11points absolute in F measure.</p><p>4 0.29338312 <a title="44-lsi-4" href="./emnlp-2012-Active_Learning_for_Imbalanced_Sentiment_Classification.html">15 emnlp-2012-Active Learning for Imbalanced Sentiment Classification</a></p>
<p>Author: Shoushan Li ; Shengfeng Ju ; Guodong Zhou ; Xiaojun Li</p><p>Abstract: Active learning is a promising way for sentiment classification to reduce the annotation cost. In this paper, we focus on the imbalanced class distribution scenario for sentiment classification, wherein the number of positive samples is quite different from that of negative samples. This scenario posits new challenges to active learning. To address these challenges, we propose a novel active learning approach, named co-selecting, by taking both the imbalanced class distribution issue and uncertainty into account. Specifically, our co-selecting approach employs two feature subspace classifiers to collectively select most informative minority-class samples for manual annotation by leveraging a certainty measurement and an uncertainty measurement, and in the meanwhile, automatically label most informative majority-class samples, to reduce humanannotation efforts. Extensive experiments across four domains demonstrate great potential and effectiveness of our proposed co-selecting approach to active learning for imbalanced sentiment classification. 1</p><p>5 0.279185 <a title="44-lsi-5" href="./emnlp-2012-Open_Language_Learning_for_Information_Extraction.html">100 emnlp-2012-Open Language Learning for Information Extraction</a></p>
<p>Author: Mausam ; Michael Schmitz ; Stephen Soderland ; Robert Bart ; Oren Etzioni</p><p>Abstract: Open Information Extraction (IE) systems extract relational tuples from text, without requiring a pre-specified vocabulary, by identifying relation phrases and associated arguments in arbitrary sentences. However, stateof-the-art Open IE systems such as REVERB and WOE share two important weaknesses (1) they extract only relations that are mediated by verbs, and (2) they ignore context, thus extracting tuples that are not asserted as factual. This paper presents OLLIE, a substantially improved Open IE system that addresses both these limitations. First, OLLIE achieves high yield by extracting relations mediated by nouns, adjectives, and more. Second, a context-analysis step increases precision by including contextual information from the sentence in the extractions. OLLIE obtains 2.7 times the area under precision-yield curve (AUC) compared to REVERB and 1.9 times the AUC of WOEparse. –</p><p>6 0.24330999 <a title="44-lsi-6" href="./emnlp-2012-Why_Question_Answering_using_Sentiment_Analysis_and_Word_Classes.html">137 emnlp-2012-Why Question Answering using Sentiment Analysis and Word Classes</a></p>
<p>7 0.22694387 <a title="44-lsi-7" href="./emnlp-2012-Local_and_Global_Context_for_Supervised_and_Unsupervised_Metonymy_Resolution.html">85 emnlp-2012-Local and Global Context for Supervised and Unsupervised Metonymy Resolution</a></p>
<p>8 0.21350968 <a title="44-lsi-8" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>9 0.19260678 <a title="44-lsi-9" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>10 0.18786007 <a title="44-lsi-10" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>11 0.17998661 <a title="44-lsi-11" href="./emnlp-2012-Building_a_Lightweight_Semantic_Model_for_Unsupervised_Information_Extraction_on_Short_Listings.html">26 emnlp-2012-Building a Lightweight Semantic Model for Unsupervised Information Extraction on Short Listings</a></p>
<p>12 0.17756245 <a title="44-lsi-12" href="./emnlp-2012-Exploiting_Chunk-level_Features_to_Improve_Phrase_Chunking.html">45 emnlp-2012-Exploiting Chunk-level Features to Improve Phrase Chunking</a></p>
<p>13 0.17571902 <a title="44-lsi-13" href="./emnlp-2012-Identifying_Constant_and_Unique_Relations_by_using_Time-Series_Text.html">62 emnlp-2012-Identifying Constant and Unique Relations by using Time-Series Text</a></p>
<p>14 0.16736457 <a title="44-lsi-14" href="./emnlp-2012-Collocation_Polarity_Disambiguation_Using_Web-based_Pseudo_Contexts.html">28 emnlp-2012-Collocation Polarity Disambiguation Using Web-based Pseudo Contexts</a></p>
<p>15 0.16618966 <a title="44-lsi-15" href="./emnlp-2012-Explore_Person_Specific_Evidence_in_Web_Person_Name_Disambiguation.html">47 emnlp-2012-Explore Person Specific Evidence in Web Person Name Disambiguation</a></p>
<p>16 0.16141586 <a title="44-lsi-16" href="./emnlp-2012-Enlarging_Paraphrase_Collections_through_Generalization_and_Instantiation.html">39 emnlp-2012-Enlarging Paraphrase Collections through Generalization and Instantiation</a></p>
<p>17 0.15257925 <a title="44-lsi-17" href="./emnlp-2012-Learning-based_Multi-Sieve_Co-reference_Resolution_with_Knowledge.html">76 emnlp-2012-Learning-based Multi-Sieve Co-reference Resolution with Knowledge</a></p>
<p>18 0.15215704 <a title="44-lsi-18" href="./emnlp-2012-Towards_Efficient_Named-Entity_Rule_Induction_for_Customizability.html">125 emnlp-2012-Towards Efficient Named-Entity Rule Induction for Customizability</a></p>
<p>19 0.14781258 <a title="44-lsi-19" href="./emnlp-2012-Automatically_Constructing_a_Normalisation_Dictionary_for_Microblogs.html">22 emnlp-2012-Automatically Constructing a Normalisation Dictionary for Microblogs</a></p>
<p>20 0.14554437 <a title="44-lsi-20" href="./emnlp-2012-A_Beam-Search_Decoder_for_Grammatical_Error_Correction.html">2 emnlp-2012-A Beam-Search Decoder for Grammatical Error Correction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.028), (16, 0.026), (24, 0.399), (25, 0.011), (34, 0.053), (36, 0.028), (39, 0.014), (60, 0.068), (63, 0.04), (64, 0.021), (65, 0.027), (70, 0.035), (73, 0.015), (74, 0.033), (76, 0.042), (80, 0.014), (86, 0.014), (95, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78830093 <a title="44-lda-1" href="./emnlp-2012-Excitatory_or_Inhibitory%3A_A_New_Semantic_Orientation_Extracts_Contradiction_and_Causality_from_the_Web.html">44 emnlp-2012-Excitatory or Inhibitory: A New Semantic Orientation Extracts Contradiction and Causality from the Web</a></p>
<p>Author: Chikara Hashimoto ; Kentaro Torisawa ; Stijn De Saeger ; Jong-Hoon Oh ; Jun'ichi Kazama</p><p>Abstract: We propose a new semantic orientation, Excitation, and its automatic acquisition method. Excitation is a semantic property of predicates that classifies them into excitatory, inhibitory and neutral. We show that Excitation is useful for extracting contradiction pairs (e.g., destroy cancer develop cancer) and causality pairs (cea.ng.c,e rin ⊥cre daesvee lionp c craimnece r⇒) ahnedig chatuensa laintyx pieatyir)s. (Oe.ugr. ,ex ipncerreimaseent ins shc roimw eth ⇒at w heitihg ahtuetnom anatxicieatlyly). acquired Excitation knowledge we can extract one million contradiction pairs and 500,000 causality pairs with about 70% precision from a 600 million page Web corpus. Furthermore, by combining these extracted causality and contradiction pairs, we can generate one million plausible causality hypotheses that are not written in any single sentence in our corpus ⊥ with reasonable precision.</p><p>2 0.34754288 <a title="44-lda-2" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>Author: Mohamed Yahya ; Klaus Berberich ; Shady Elbassuoni ; Maya Ramanath ; Volker Tresp ; Gerhard Weikum</p><p>Abstract: The Linked Data initiative comprises structured databases in the Semantic-Web data model RDF. Exploring this heterogeneous data by structured query languages is tedious and error-prone even for skilled users. To ease the task, this paper presents a methodology for translating natural language questions into structured SPARQL queries over linked-data sources. Our method is based on an integer linear program to solve several disambiguation tasks jointly: the segmentation of questions into phrases; the mapping of phrases to semantic entities, classes, and relations; and the construction of SPARQL triple patterns. Our solution harnesses the rich type system provided by knowledge bases in the web of linked data, to constrain our semantic-coherence objective function. We present experiments on both the . in question translation and the resulting query answering.</p><p>3 0.28980589 <a title="44-lda-3" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms ofweak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.</p><p>4 0.28958237 <a title="44-lda-4" href="./emnlp-2012-Why_Question_Answering_using_Sentiment_Analysis_and_Word_Classes.html">137 emnlp-2012-Why Question Answering using Sentiment Analysis and Word Classes</a></p>
<p>Author: Jong-Hoon Oh ; Kentaro Torisawa ; Chikara Hashimoto ; Takuya Kawada ; Stijn De Saeger ; Jun'ichi Kazama ; Yiou Wang</p><p>Abstract: In this paper we explore the utility of sentiment analysis and semantic word classes for improving why-question answering on a large-scale web corpus. Our work is motivated by the observation that a why-question and its answer often follow the pattern that if something undesirable happens, the reason is also often something undesirable, and if something desirable happens, the reason is also often something desirable. To the best of our knowledge, this is the first work that introduces sentiment analysis to non-factoid question answering. We combine this simple idea with semantic word classes for ranking answers to why-questions and show that on a set of 850 why-questions our method gains 15.2% improvement in precision at the top-1 answer over a baseline state-of-the-art QA system that achieved the best performance in a shared task of Japanese non-factoid QA in NTCIR-6.</p><p>5 0.28627992 <a title="44-lda-5" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>Author: Lizhen Qu ; Rainer Gemulla ; Gerhard Weikum</p><p>Abstract: We propose the weakly supervised MultiExperts Model (MEM) for analyzing the semantic orientation of opinions expressed in natural language reviews. In contrast to most prior work, MEM predicts both opinion polarity and opinion strength at the level of individual sentences; such fine-grained analysis helps to understand better why users like or dislike the entity under review. A key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength. For this reason, MEM is weakly supervised: It starts with potentially noisy indicators obtained from coarse-grained training data (i.e., document-level ratings), a small set of diverse base predictors, and, if available, small amounts of fine-grained training data. We integrate these noisy indicators into a unified probabilistic framework using ideas from ensemble learning and graph-based semi-supervised learning. Our experiments indicate that MEM outperforms state-of-the-art methods by a significant margin.</p><p>6 0.28303236 <a title="44-lda-6" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>7 0.28156027 <a title="44-lda-7" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<p>8 0.28088012 <a title="44-lda-8" href="./emnlp-2012-Exploiting_Reducibility_in_Unsupervised_Dependency_Parsing.html">46 emnlp-2012-Exploiting Reducibility in Unsupervised Dependency Parsing</a></p>
<p>9 0.28031391 <a title="44-lda-9" href="./emnlp-2012-Multi-Domain_Learning%3A_When_Do_Domains_Matter%3F.html">92 emnlp-2012-Multi-Domain Learning: When Do Domains Matter?</a></p>
<p>10 0.27979311 <a title="44-lda-10" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>11 0.27944374 <a title="44-lda-11" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>12 0.27903247 <a title="44-lda-12" href="./emnlp-2012-Besting_the_Quiz_Master%3A_Crowdsourcing_Incremental_Classification_Games.html">23 emnlp-2012-Besting the Quiz Master: Crowdsourcing Incremental Classification Games</a></p>
<p>13 0.27856487 <a title="44-lda-13" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>14 0.27844429 <a title="44-lda-14" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>15 0.27795836 <a title="44-lda-15" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>16 0.27740949 <a title="44-lda-16" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>17 0.27698562 <a title="44-lda-17" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>18 0.27635014 <a title="44-lda-18" href="./emnlp-2012-Concurrent_Acquisition_of_Word_Meaning_and_Lexical_Categories.html">29 emnlp-2012-Concurrent Acquisition of Word Meaning and Lexical Categories</a></p>
<p>19 0.27588615 <a title="44-lda-19" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<p>20 0.27500051 <a title="44-lda-20" href="./emnlp-2012-Type-Supervised_Hidden_Markov_Models_for_Part-of-Speech_Tagging_with_Incomplete_Tag_Dictionaries.html">129 emnlp-2012-Type-Supervised Hidden Markov Models for Part-of-Speech Tagging with Incomplete Tag Dictionaries</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
