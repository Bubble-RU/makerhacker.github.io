<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 emnlp-2012-An "AI readability" Formula for French as a Foreign Language</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-17" href="#">emnlp2012-17</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>17 emnlp-2012-An "AI readability" Formula for French as a Foreign Language</h1>
<br/><p>Source: <a title="emnlp-2012-17-pdf" href="http://aclweb.org/anthology//D/D12/D12-1043.pdf">pdf</a></p><p>Author: Thomas Francois ; Cedrick Fairon</p><p>Abstract: This paper present a new readability formula for French as a foreign language (FFL), which relies on 46 textual features representative of the lexical, syntactic, and semantic levels as well as some of the specificities of the FFL context. We report comparisons between several techniques for feature selection and various learning algorithms. Our best model, based on support vector machines (SVM), significantly outperforms previous FFL formulas. We also found that semantic features behave poorly in our case, in contrast with some previous readability studies on English as a first language.</p><p>Reference: <a title="emnlp-2012-17-reference" href="../emnlp2012_reference/emnlp-2012-An_%22AI_readability%22_Formula_for_French_as_a_Foreign_Language_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 An “AI readability” formula for French as a foreign language  Thomas Fran ¸cois IRCS, University of Pennsylvania 3401 Walnut Street Suite 400A Room 423 Philadelphia, PA 19104, USA  . [sent-1, score-0.248]
</p><p>2 edu  Abstract This paper present a new readability formula for French as a foreign language (FFL), which relies on 46 textual features representative of the lexical, syntactic, and semantic levels as well as some of the specificities of the FFL context. [sent-3, score-0.607]
</p><p>3 We also found that semantic features behave poorly in our case, in contrast with some previous readability studies on English as a first language. [sent-6, score-0.393]
</p><p>4 When a teacher wants to improve his/her students’ reading skills, he/she uses reading exercises, whether there are guided or independent. [sent-8, score-0.314]
</p><p>5 This condition is sometimes difficult to meet for teachers wishing to get off the beaten tracks by not using texts from levelled textbooks or readers. [sent-11, score-0.272]
</p><p>6 In this context, readability formulas have long been used to help teachers faster select texts for their students. [sent-12, score-0.682]
</p><p>7 These formulas are reproducible methods that aim at matching readers and texts relative to their reading difficulty level. [sent-13, score-0.588]
</p><p>8 The Flesch (1948) and Dale and Chall (1948) formulas are probably 466 C ´edrick Fairon CENTAL, UCLouvain Place Blaise Pascal, 1 1348 Louvain-la-Neuve, Belgium Cedri ck . [sent-14, score-0.166]
</p><p>9 They are typical of classic formulas, the first major methodological paradigm developed in the field during the 40’s and 50’s. [sent-17, score-0.155]
</p><p>10 They were kept as parsimonious as possible, using linear regression to combined two, or sometimes, three surface features, such as word mean length, sentence mean length, or proportion of outof-simple-vocabulary words. [sent-18, score-0.129]
</p><p>11 Later, some scholars (Kintsch and Vipond, 1979; Redish and Selzer, 1985) argued that the classic formulas suffer from several shortcomings. [sent-19, score-0.32]
</p><p>12 These formulas only take into account superficial features, ignoring other important aspects contributing to text difficulty, such as coherence, content density, inference load, etc. [sent-20, score-0.166]
</p><p>13 They also omit the interactive aspect of the reading process. [sent-21, score-0.157]
</p><p>14 In Section 2, it is further argue why a new formula was necessary for FFL. [sent-35, score-0.166]
</p><p>15 2  Readability models for French  Readability of French never enjoyed a large success: while readability studies on English dates back to the 20’s, it is only in 1957 that the Frenchspeaking world discovered it through the work of Conquet (1957). [sent-38, score-0.393]
</p><p>16 The two first French L1 formulas were adaptations of the Flesch formula (Kandel and Moles, 1958; de Landsheere, 1963). [sent-40, score-0.384]
</p><p>17 Henry used cloze tests to assess the level of 60 texts from primary and secondary school textbooks and trained three formulas on this corpus. [sent-42, score-0.422]
</p><p>18 It is worth mentioning that Henry’s formulas have been applied to FFL by Cornaire (1988). [sent-43, score-0.204]
</p><p>19 Although more modern in its conception, Richaudeau’s hard-to-implement formula did not achieve the same recognition in the French speaking world as Henry’s. [sent-46, score-0.166]
</p><p>20 It is worth mentioning two more authors: Mesnager (1989), who designed a classic formula 467 for children that draw inspiration from the Dale and Chall (1948) formula and Daoust et al. [sent-48, score-0.475]
</p><p>21 It is also the last formula published for French L1, if we except the adaptation of the model by Collins-Thompson and Callan (2004) to French. [sent-51, score-0.166]
</p><p>22 Tharp (1939) published a first formula taking into account one particularity of the L2 context: the cognates. [sent-53, score-0.166]
</p><p>23 This idea was recently replicated by Uitdenbogerd (2005), who combined a syntactic feature, the mean number of words per sentence, with the number of cognates per 100 words in her formula. [sent-55, score-0.149]
</p><p>24 Although taking into account this effect of the L1 on L2 reading is very interesting, these two studies are confined to a limited audience: English speakers learning French. [sent-56, score-0.191]
</p><p>25 As regards a more generic approach, Fran ¸cois (2009) recently published an “AI formula” for FFL, based on logistic regression and ten features. [sent-57, score-0.129]
</p><p>26 From all this, it seems clear that FFL readability needs to be addressed more thoroughly, especially if we are willing to get a generic model, able to make predictions for L2 readers with any L1 background. [sent-60, score-0.463]
</p><p>27 3  Design of the formula  The design of an “AI readability” formula involves the same three steps as a classification problem. [sent-62, score-0.332]
</p><p>28 2, consists in defining a set of predictors, that is to say, linguistic characteristics of the texts that will be used to predict the difficulty level of new texts. [sent-66, score-0.196]
</p><p>29 Finally, the best subset of these predictors is combined within a learning algorithm to obtain the best model possible. [sent-67, score-0.236]
</p><p>30 1 The corpus A gold-standard for readability consists in texts labelled according to their difficulty. [sent-71, score-0.467]
</p><p>31 Then, each text have to be assessed with a method able to measure the reading comprehension level of the target population. [sent-73, score-0.275]
</p><p>32 Assessing the reading difficulty of texts with respect to a target population of readers was a more challenging issue. [sent-76, score-0.422]
</p><p>33 Several techniques have been used in the literature, the most important of which are comprehension tests, cloze tests and expert judgements. [sent-77, score-0.153]
</p><p>34 In this case, texts comes from textbooks whose content difficulty have been assessed by the publishers. [sent-79, score-0.372]
</p><p>35 (201 1) found that expert agreement on a same corpus of texts might be insufficient for a classification task. [sent-82, score-0.171]
</p><p>36 For this study, we nevertheless relied on expert judgements, since we needed a large amount of labelled texts to ensure a robust statistical learning. [sent-83, score-0.171]
</p><p>37 From those, we extracted 2,160 texts related to a reading comprehension task and assigned to each of them the same level as the textbook it came from. [sent-85, score-0.371]
</p><p>38 This heterogeneity was detected in three of the six levels (A1, A2, and B 1) using ANOVA based on two classic readability features as independent variables: the mean number of words per sentence and the mean number of letters per word. [sent-88, score-0.658]
</p><p>39 A subsequent qualitative analysis revealed that most of the heterogeneity was coming from textbooks following the new didactic approach recommended by the CEFR: the task-oriented approach, which focuses more on the task than the text when labelling  the overall reading activity. [sent-89, score-0.272]
</p><p>40 2 The predictors In a second step, every text of the corpus was represented as a numeric vector of 406 features, each of them representing a linguistic dimension of the text as a single number. [sent-94, score-0.236]
</p><p>41 Their implementation drew on two different sources of inspiration: the existing predictors in the English and French literature and the psycholinguistic literature on the reading process. [sent-95, score-0.393]
</p><p>42 1 Lexical Features Lexical features have been shown to be the most important level of information in many readability  studies (Chall and Dale, 1995; Lorge, 1944). [sent-101, score-0.393]
</p><p>43 It is then not surprising that a wide range of lexical predictors have been developed in the literature. [sent-102, score-0.274]
</p><p>44 Our own set comprised the following subfamilies: Statistics of lexical frequencies: frequencies of words in a text are a good indicator ofthe text’s overall difficulty (Stenner, 1996). [sent-103, score-0.126]
</p><p>45 202)1,852T(o5t1a0l;543) Table 1: Distribution of the number of texts and tokens per level in our corpus. [sent-113, score-0.142]
</p><p>46 –  –  Word length: mean word length is another classic feature in readability (Flesch, 1948; Smith, 1961). [sent-124, score-0.51]
</p><p>47 469 Lexical diversity: the repetition effect is another factor known to affect the reading process (Bowers, 2000). [sent-132, score-0.157]
</p><p>48 Orthographic neighborhood: we finally suggested a new lexical variable, based on the fact that some characteristics of the orthographic neighbors 3  of a word are known to impact the reading of this word (Andrews, 1997). [sent-136, score-0.286]
</p><p>49 Thirteen predictors were implemented to account for the number or the frequency of the orthographic neighbors of all words in a text. [sent-137, score-0.288]
</p><p>50 Although most of the scholars in the field agree that it does not lead to such efficient predictors as the lexical level, they have noticed it can be combined with the latter to improve performance of readability formulas. [sent-141, score-0.682]
</p><p>51 In order to clarify as much as possible the situation for FFL, we implemented the following features: Personnalization level: Dale and Tyler (1934) suggested that informal texts should be easier to read and that informality might be assessed through the type ofpersonal pronouns found in a text. [sent-159, score-0.208]
</p><p>52 (1975) showed that the number of propositions as well as the number of different arguments in a sentence influence its reading time. [sent-162, score-0.157]
</p><p>53 470 Lexical cohesion : the level of cohesion in a text was measured as the average cosine of all pair of adjacent sentences in the text. [sent-168, score-0.129]
</p><p>54 Since our study intended to design a generic model, we focused on specific predictors affecting L2 reading, whatever the learner’s mother tongue is: Multi-word expressions (MWE): MWEs are acknowledged to cause problems to L2 learners for production (Bahns and Eldaw, 1993). [sent-179, score-0.271]
</p><p>55 This focus on dialogue was  Table 2: Spearman correlation for some predictors in our set with difficulty. [sent-188, score-0.236]
</p><p>56 A positive correlation means that the  difficulty of texts increases with the value of the predictor. [sent-189, score-0.196]
</p><p>57 3  The algorithms  The last step in the development of our formula was to select the most informative subset of features and combine them in a state-of-the-art machine learning algorithm. [sent-197, score-0.166]
</p><p>58 1 The efficiency of predictors Spearman correlation was used to assess the efficiency of each predictor, to better account for nonlinear relationships with the criterion. [sent-205, score-0.236]
</p><p>59 In accordance with the literature, it appeared that the best family of predictors were the lexical one, followed by the syntactic one. [sent-207, score-0.322]
</p><p>60 2 The models Once the best single predictors were identified, it was possible to combine several of them in a read-  ability model for comparison. [sent-212, score-0.236]
</p><p>61 Since preliminary experiments showed that the equal prior probabilities are required to ensure a unbiased training, the whole corpus was resampled to get the same number of texts per level (108), which amounted to a total of 648 texts. [sent-214, score-0.142]
</p><p>62 240 texts were kept for development purposes, mainly feature selection and estimation of the meta-parameters γ and C for the SVM. [sent-216, score-0.149]
</p><p>63 The remaining 408 texts were used for evaluating performance of our readability models. [sent-217, score-0.467]
</p><p>64 The first method was based on the structuro-cognitivist assumption that readability formulas should include other features than just lexicosyntactical ones, in order to maximize variety of information. [sent-221, score-0.558]
</p><p>65 These “expert” approaches were compared to an automatic selection, using either a stepwise procedure for logistic regression (OLR and MLR) or a built-in regularization (Bishop, 2006, 10) for the SVM, based on the 46 best predictors inside each subfamily. [sent-223, score-0.33]
</p><p>66 Their performances, reported in Table 4, were assessed using five measures: the multiple correlation ratio (R), the accuracy (acc), the adjacent accuracy 7 (adjacc), the root mean square error (rmse) and the mean absolute error (mae). [sent-233, score-0.212]
</p><p>67 It appeared that the lexical family was the most accurate set of predictors (40. [sent-287, score-0.322]
</p><p>68 In fact, this was the only set whose absence significantly impacted adjacent accuracy, suggesting that the other type of predictors can only improve the accuracy of predictions, but are not able to reduce the amount of critical mistakes. [sent-289, score-0.295]
</p><p>69 Finally, our two other families was clearly inferior, but they still improved slightly the accuracy of our model, although not the adjacent accuracy. [sent-292, score-0.132]
</p><p>70 3 Comparaison with previous work Comparisons with other FFL models are difficult to provide: not only there are few formulas available for FFL, but some of these focus on a different audience, making comparability low. [sent-304, score-0.166]
</p><p>71 The first of them is a classic readability formula by Kandel and Moles (1958), which is an adaptation of the Flesch (1948) formula for French: Y = 207 − 1. [sent-306, score-0.796]
</p><p>72 736lm  (2)  where Y is a readability score ranging from 100 (easiest) to 0 (harder); lp is the average number of  words per sentence and lm is the average number of syllables per 100 words. [sent-308, score-0.427]
</p><p>73 Although it was not designed for FFL, we considered it, since it is one of the most well-known formula for French and the two features combined are very general. [sent-309, score-0.166]
</p><p>74 5  Discussion and conclusion  In this paper, we introduced a new “AI readability” formula for FFL, able to predict the level of texts according to the largely-spread CEFR scale. [sent-327, score-0.274]
</p><p>75 Therefore, it represent a robust generic solution for FFL readers willing to find various kind of texts that suit their linguistic abilities. [sent-331, score-0.212]
</p><p>76 Besides the creation of a new FFL readability formula, this study produced two valuable insights. [sent-332, score-0.359]
</p><p>77 Not only the expert models, to which we imposed the presence ofone or two semantic predictors, did not perform the best, but none of the features from our semantic set was retained during the automatic selection of the variables for the logistic models. [sent-337, score-0.211]
</p><p>78 With reservations one may have because of the limited number of semantic predictors in our set, these results however raise some concerns about whether the information coming from semantic variables is really different from that carried by lexicosyntactic features. [sent-340, score-0.286]
</p><p>79 This conclusion contradicts the assumptions of the structuro-cognitivist paradigm, but corroborates Chall and Dale (1995)’s view that the information carried by semantic predictors is largely correlated with that of lexicosyntactical ones. [sent-342, score-0.269]
</p><p>80 First, it might be that semantic and lexical predictors are correlated because the methods used for the parameterization of the semantic factors heavily relie on lexical information. [sent-344, score-0.312]
</p><p>81 This might explain why lexico-syntactic predictors were so predominant in our experiments. [sent-349, score-0.236]
</p><p>82 Reading in a foreign language : a reading problem or a language problem ? [sent-364, score-0.239]
</p><p>83 Predict-  ing reading difficulty with statistical language models. [sent-440, score-0.245]
</p><p>84 La lisibilit e´ : essai d’application de la formule courte d’Henry au fran ¸cais langue e´trang e`re. [sent-458, score-0.608]
</p><p>85 A study of the factors influencing the difficulty of reading materials for adults of limited reading ability. [sent-476, score-0.402]
</p><p>86 SATOCALIBRAGE: Pr´ esentation d’un outil d’assistance au choix et `a la r ´edaction de textes pour l’enseignement. [sent-483, score-0.229]
</p><p>87 Pour une application des tests de lisibilit e´ de Flesch `a la langue fran ¸caise. [sent-488, score-0.726]
</p><p>88 On the contribution of MWE-based features to a readability formula for French as a foreign language. [sent-528, score-0.607]
</p><p>89 Combining a statistical language model with logistic regression to predict the lexical and syntactic difficulty of texts for FFL. [sent-533, score-0.328]
</p><p>90 La lisibilit e´ computationnelle : un renouveau pour la lisibilit e´ du fran ¸cais langue premi `ere et seconde ? [sent-538, score-0.783]
</p><p>91 Les apports du traitement automatique du langage `a la lisibilit e´ du fran ¸cais langue e´trang e`re. [sent-543, score-0.716]
</p><p>92 L’´ elaboration du fran c¸ais fondamental (1er degr e´). [sent-561, score-0.261]
</p><p>93 An analysis of statistical models and features for reading difficulty prediction. [sent-584, score-0.245]
</p><p>94 Application de l’indice de Flesch `a la langue fran ¸caise. [sent-595, score-0.479]
</p><p>95 Reading comprehension and readability in educational practice and psychological theory. [sent-606, score-0.477]
</p><p>96 Densid´ ees: calcul automatique de la densit´ e des id´ ees dans un corpus oral. [sent-628, score-0.313]
</p><p>97 In Actes de la douxime Rencontre des tudiants Chercheurs en Informatique pour le Traitement Automatique des langues (RECITAL). [sent-629, score-0.394]
</p><p>98 Lisibilit e´ des textes pour enfants: un nouvel outil? [sent-638, score-0.163]
</p><p>99 Teaching reading to poor readers in the intermediate grades: A comparison of text difficulty. [sent-699, score-0.226]
</p><p>100 A posteriori agreement as a quality measure for readability prediction systems. [sent-780, score-0.359]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ffl', 0.444), ('readability', 0.359), ('predictors', 0.236), ('fran', 0.213), ('formulas', 0.166), ('formula', 0.166), ('reading', 0.157), ('french', 0.148), ('chall', 0.148), ('lisibilit', 0.148), ('flesch', 0.131), ('kintsch', 0.127), ('textbooks', 0.115), ('texts', 0.108), ('dale', 0.105), ('classic', 0.105), ('cois', 0.1), ('des', 0.099), ('difficulty', 0.088), ('langue', 0.082), ('foreign', 0.082), ('la', 0.08), ('families', 0.073), ('henry', 0.071), ('readers', 0.069), ('pour', 0.064), ('expert', 0.063), ('assessed', 0.061), ('educational', 0.061), ('adjacent', 0.059), ('comprehension', 0.057), ('logistic', 0.057), ('svm', 0.053), ('orthographic', 0.052), ('de', 0.052), ('callan', 0.051), ('paradigm', 0.05), ('ai', 0.05), ('variables', 0.05), ('adjacc', 0.049), ('auto', 0.049), ('automatique', 0.049), ('cais', 0.049), ('cefr', 0.049), ('ego', 0.049), ('kandel', 0.049), ('mwe', 0.049), ('oosten', 0.049), ('richaudeau', 0.049), ('scholars', 0.049), ('stressed', 0.049), ('subfamilies', 0.049), ('teachers', 0.049), ('textbook', 0.049), ('uitdenbogerd', 0.049), ('vipond', 0.049), ('family', 0.048), ('du', 0.048), ('teaching', 0.047), ('mean', 0.046), ('quarterly', 0.042), ('ttr', 0.042), ('selection', 0.041), ('suggested', 0.039), ('lexical', 0.038), ('mentioning', 0.038), ('regression', 0.037), ('psychology', 0.036), ('ratios', 0.035), ('cohesion', 0.035), ('replicated', 0.035), ('generic', 0.035), ('verbal', 0.035), ('studies', 0.034), ('letters', 0.034), ('per', 0.034), ('participle', 0.033), ('predictive', 0.033), ('alderson', 0.033), ('bahns', 0.033), ('berthet', 0.033), ('bormuth', 0.033), ('carreiras', 0.033), ('cloze', 0.033), ('conception', 0.033), ('daoust', 0.033), ('densid', 0.033), ('ees', 0.033), ('fairon', 0.033), ('formule', 0.033), ('gougenheim', 0.033), ('graesser', 0.033), ('langages', 0.033), ('lexicosyntactical', 0.033), ('mlr', 0.033), ('moles', 0.033), ('mwes', 0.033), ('outil', 0.033), ('ozasa', 0.033), ('redish', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="17-tfidf-1" href="./emnlp-2012-An_%22AI_readability%22_Formula_for_French_as_a_Foreign_Language.html">17 emnlp-2012-An "AI readability" Formula for French as a Foreign Language</a></p>
<p>Author: Thomas Francois ; Cedrick Fairon</p><p>Abstract: This paper present a new readability formula for French as a foreign language (FFL), which relies on 46 textual features representative of the lexical, syntactic, and semantic levels as well as some of the specificities of the FFL context. We report comparisons between several techniques for feature selection and various learning algorithms. Our best model, based on support vector machines (SVM), significantly outperforms previous FFL formulas. We also found that semantic features behave poorly in our case, in contrast with some previous readability studies on English as a first language.</p><p>2 0.08901789 <a title="17-tfidf-2" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>Author: Lizhen Qu ; Rainer Gemulla ; Gerhard Weikum</p><p>Abstract: We propose the weakly supervised MultiExperts Model (MEM) for analyzing the semantic orientation of opinions expressed in natural language reviews. In contrast to most prior work, MEM predicts both opinion polarity and opinion strength at the level of individual sentences; such fine-grained analysis helps to understand better why users like or dislike the entity under review. A key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength. For this reason, MEM is weakly supervised: It starts with potentially noisy indicators obtained from coarse-grained training data (i.e., document-level ratings), a small set of diverse base predictors, and, if available, small amounts of fine-grained training data. We integrate these noisy indicators into a unified probabilistic framework using ideas from ensemble learning and graph-based semi-supervised learning. Our experiments indicate that MEM outperforms state-of-the-art methods by a significant margin.</p><p>3 0.06765639 <a title="17-tfidf-3" href="./emnlp-2012-Assessment_of_ESL_Learners%27_Syntactic_Competence_Based_on_Similarity_Measures.html">21 emnlp-2012-Assessment of ESL Learners' Syntactic Competence Based on Similarity Measures</a></p>
<p>Author: Su-Youn Yoon ; Suma Bhat</p><p>Abstract: This study presents a novel method that measures English language learners’ syntactic competence towards improving automated speech scoring systems. In contrast to most previous studies which focus on the length of production units such as the mean length of clauses, we focused on capturing the differences in the distribution of morpho-syntactic features or grammatical expressions across proficiency. We estimated the syntactic competence through the use of corpus-based NLP techniques. Assuming that the range and so- phistication of grammatical expressions can be captured by the distribution of Part-ofSpeech (POS) tags, vector space models of POS tags were constructed. We use a large corpus of English learners’ responses that are classified into four proficiency levels by human raters. Our proposed feature measures the similarity of a given response with the most proficient group and is then estimates the learner’s syntactic competence level. Widely outperforming the state-of-the-art measures of syntactic complexity, our method attained a significant correlation with humanrated scores. The correlation between humanrated scores and features based on manual transcription was 0.43 and the same based on ASR-hypothesis was slightly lower, 0.42. An important advantage of our method is its robustness against speech recognition errors not to mention the simplicity of feature generation that captures a reasonable set of learnerspecific syntactic errors. 600 Measures Suma Bhat Beckman Institute, Urbana, IL 61801 . spbhat 2 @ i l l ino i edu s</p><p>4 0.063713878 <a title="17-tfidf-4" href="./emnlp-2012-Generalizing_Sub-sentential_Paraphrase_Acquisition_across_Original_Signal_Type_of_Text_Pairs.html">58 emnlp-2012-Generalizing Sub-sentential Paraphrase Acquisition across Original Signal Type of Text Pairs</a></p>
<p>Author: Aurelien Max ; Houda Bouamor ; Anne Vilnat</p><p>Abstract: This paper describes a study on the impact of the original signal (text, speech, visual scene, event) of a text pair on the task of both manual and automatic sub-sentential paraphrase acquisition. A corpus of 2,500 annotated sentences in English and French is described, and performance on this corpus is reported for an efficient system combination exploiting a large set of features for paraphrase recognition. A detailed quantified typology of subsentential paraphrases found in our corpus types is given.</p><p>5 0.061603304 <a title="17-tfidf-5" href="./emnlp-2012-Extending_Machine_Translation_Evaluation_Metrics_with_Lexical_Cohesion_to_Document_Level.html">50 emnlp-2012-Extending Machine Translation Evaluation Metrics with Lexical Cohesion to Document Level</a></p>
<p>Author: Billy T. M. Wong ; Chunyu Kit</p><p>Abstract: This paper proposes the utilization of lexical cohesion to facilitate evaluation of machine translation at the document level. As a linguistic means to achieve text coherence, lexical cohesion ties sentences together into a meaningfully interwoven structure through words with the same or related meaning. A comparison between machine and human translation is conducted to illustrate one of their critical distinctions that human translators tend to use more cohesion devices than machine. Various ways to apply this feature to evaluate machinetranslated documents are presented, including one without reliance on reference translation. Experimental results show that incorporating this feature into sentence-level evaluation metrics can enhance their correlation with human judgements.</p><p>6 0.059064444 <a title="17-tfidf-6" href="./emnlp-2012-Multiple_Aspect_Summarization_Using_Integer_Linear_Programming.html">94 emnlp-2012-Multiple Aspect Summarization Using Integer Linear Programming</a></p>
<p>7 0.058260076 <a title="17-tfidf-7" href="./emnlp-2012-Part-of-Speech_Tagging_for_Chinese-English_Mixed_Texts_with_Dynamic_Features.html">106 emnlp-2012-Part-of-Speech Tagging for Chinese-English Mixed Texts with Dynamic Features</a></p>
<p>8 0.051371347 <a title="17-tfidf-8" href="./emnlp-2012-Joint_Learning_for_Coreference_Resolution_with_Markov_Logic.html">73 emnlp-2012-Joint Learning for Coreference Resolution with Markov Logic</a></p>
<p>9 0.051253818 <a title="17-tfidf-9" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>10 0.049240023 <a title="17-tfidf-10" href="./emnlp-2012-Syntactic_Surprisal_Affects_Spoken_Word_Duration_in_Conversational_Contexts.html">122 emnlp-2012-Syntactic Surprisal Affects Spoken Word Duration in Conversational Contexts</a></p>
<p>11 0.044488728 <a title="17-tfidf-11" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>12 0.044438928 <a title="17-tfidf-12" href="./emnlp-2012-N-gram-based_Tense_Models_for_Statistical_Machine_Translation.html">95 emnlp-2012-N-gram-based Tense Models for Statistical Machine Translation</a></p>
<p>13 0.04143079 <a title="17-tfidf-13" href="./emnlp-2012-Characterizing_Stylistic_Elements_in_Syntactic_Structure.html">27 emnlp-2012-Characterizing Stylistic Elements in Syntactic Structure</a></p>
<p>14 0.039269794 <a title="17-tfidf-14" href="./emnlp-2012-A_Comparison_of_Vector-based_Representations_for_Semantic_Composition.html">4 emnlp-2012-A Comparison of Vector-based Representations for Semantic Composition</a></p>
<p>15 0.038299337 <a title="17-tfidf-15" href="./emnlp-2012-A_Beam-Search_Decoder_for_Grammatical_Error_Correction.html">2 emnlp-2012-A Beam-Search Decoder for Grammatical Error Correction</a></p>
<p>16 0.036962327 <a title="17-tfidf-16" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>17 0.036096241 <a title="17-tfidf-17" href="./emnlp-2012-Learning_Syntactic_Categories_Using_Paradigmatic_Representations_of_Word_Context.html">79 emnlp-2012-Learning Syntactic Categories Using Paradigmatic Representations of Word Context</a></p>
<p>18 0.035815351 <a title="17-tfidf-18" href="./emnlp-2012-Universal_Grapheme-to-Phoneme_Prediction_Over_Latin_Alphabets.html">132 emnlp-2012-Universal Grapheme-to-Phoneme Prediction Over Latin Alphabets</a></p>
<p>19 0.035524219 <a title="17-tfidf-19" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>20 0.035457011 <a title="17-tfidf-20" href="./emnlp-2012-A_Systematic_Comparison_of_Phrase_Table_Pruning_Techniques.html">11 emnlp-2012-A Systematic Comparison of Phrase Table Pruning Techniques</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.153), (1, 0.016), (2, -0.014), (3, 0.047), (4, 0.027), (5, 0.04), (6, 0.013), (7, -0.048), (8, 0.038), (9, -0.004), (10, 0.003), (11, -0.01), (12, -0.11), (13, 0.029), (14, 0.002), (15, -0.091), (16, -0.018), (17, 0.003), (18, -0.003), (19, 0.058), (20, -0.041), (21, 0.095), (22, 0.129), (23, 0.018), (24, -0.227), (25, 0.132), (26, 0.091), (27, -0.185), (28, 0.08), (29, 0.039), (30, 0.007), (31, -0.145), (32, 0.028), (33, 0.063), (34, -0.192), (35, -0.056), (36, 0.033), (37, 0.037), (38, -0.022), (39, 0.033), (40, -0.019), (41, 0.063), (42, 0.11), (43, 0.084), (44, 0.106), (45, -0.2), (46, 0.129), (47, 0.229), (48, -0.047), (49, -0.095)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9376902 <a title="17-lsi-1" href="./emnlp-2012-An_%22AI_readability%22_Formula_for_French_as_a_Foreign_Language.html">17 emnlp-2012-An "AI readability" Formula for French as a Foreign Language</a></p>
<p>Author: Thomas Francois ; Cedrick Fairon</p><p>Abstract: This paper present a new readability formula for French as a foreign language (FFL), which relies on 46 textual features representative of the lexical, syntactic, and semantic levels as well as some of the specificities of the FFL context. We report comparisons between several techniques for feature selection and various learning algorithms. Our best model, based on support vector machines (SVM), significantly outperforms previous FFL formulas. We also found that semantic features behave poorly in our case, in contrast with some previous readability studies on English as a first language.</p><p>2 0.61682439 <a title="17-lsi-2" href="./emnlp-2012-Syntactic_Surprisal_Affects_Spoken_Word_Duration_in_Conversational_Contexts.html">122 emnlp-2012-Syntactic Surprisal Affects Spoken Word Duration in Conversational Contexts</a></p>
<p>Author: Vera Demberg ; Asad Sayeed ; Philip Gorinski ; Nikolaos Engonopoulos</p><p>Abstract: We present results of a novel experiment to investigate speech production in conversational data that links speech rate to information density. We provide the first evidence for an association between syntactic surprisal and word duration in recorded speech. Using the AMI corpus which contains transcriptions of focus group meetings with precise word durations, we show that word durations correlate with syntactic surprisal estimated from the incremental Roark parser over and above simpler measures, such as word duration estimated from a state-of-the-art text-to-speech system and word frequencies, and that the syntactic surprisal estimates are better predictors of word durations than a simpler version of surprisal based on trigram probabilities. This result supports the uniform information density (UID) hypothesis and points a way to more realistic artificial speech generation.</p><p>3 0.544361 <a title="17-lsi-3" href="./emnlp-2012-Assessment_of_ESL_Learners%27_Syntactic_Competence_Based_on_Similarity_Measures.html">21 emnlp-2012-Assessment of ESL Learners' Syntactic Competence Based on Similarity Measures</a></p>
<p>Author: Su-Youn Yoon ; Suma Bhat</p><p>Abstract: This study presents a novel method that measures English language learners’ syntactic competence towards improving automated speech scoring systems. In contrast to most previous studies which focus on the length of production units such as the mean length of clauses, we focused on capturing the differences in the distribution of morpho-syntactic features or grammatical expressions across proficiency. We estimated the syntactic competence through the use of corpus-based NLP techniques. Assuming that the range and so- phistication of grammatical expressions can be captured by the distribution of Part-ofSpeech (POS) tags, vector space models of POS tags were constructed. We use a large corpus of English learners’ responses that are classified into four proficiency levels by human raters. Our proposed feature measures the similarity of a given response with the most proficient group and is then estimates the learner’s syntactic competence level. Widely outperforming the state-of-the-art measures of syntactic complexity, our method attained a significant correlation with humanrated scores. The correlation between humanrated scores and features based on manual transcription was 0.43 and the same based on ASR-hypothesis was slightly lower, 0.42. An important advantage of our method is its robustness against speech recognition errors not to mention the simplicity of feature generation that captures a reasonable set of learnerspecific syntactic errors. 600 Measures Suma Bhat Beckman Institute, Urbana, IL 61801 . spbhat 2 @ i l l ino i edu s</p><p>4 0.41269374 <a title="17-lsi-4" href="./emnlp-2012-Extending_Machine_Translation_Evaluation_Metrics_with_Lexical_Cohesion_to_Document_Level.html">50 emnlp-2012-Extending Machine Translation Evaluation Metrics with Lexical Cohesion to Document Level</a></p>
<p>Author: Billy T. M. Wong ; Chunyu Kit</p><p>Abstract: This paper proposes the utilization of lexical cohesion to facilitate evaluation of machine translation at the document level. As a linguistic means to achieve text coherence, lexical cohesion ties sentences together into a meaningfully interwoven structure through words with the same or related meaning. A comparison between machine and human translation is conducted to illustrate one of their critical distinctions that human translators tend to use more cohesion devices than machine. Various ways to apply this feature to evaluate machinetranslated documents are presented, including one without reliance on reference translation. Experimental results show that incorporating this feature into sentence-level evaluation metrics can enhance their correlation with human judgements.</p><p>5 0.33358657 <a title="17-lsi-5" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>Author: Lizhen Qu ; Rainer Gemulla ; Gerhard Weikum</p><p>Abstract: We propose the weakly supervised MultiExperts Model (MEM) for analyzing the semantic orientation of opinions expressed in natural language reviews. In contrast to most prior work, MEM predicts both opinion polarity and opinion strength at the level of individual sentences; such fine-grained analysis helps to understand better why users like or dislike the entity under review. A key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength. For this reason, MEM is weakly supervised: It starts with potentially noisy indicators obtained from coarse-grained training data (i.e., document-level ratings), a small set of diverse base predictors, and, if available, small amounts of fine-grained training data. We integrate these noisy indicators into a unified probabilistic framework using ideas from ensemble learning and graph-based semi-supervised learning. Our experiments indicate that MEM outperforms state-of-the-art methods by a significant margin.</p><p>6 0.29570723 <a title="17-lsi-6" href="./emnlp-2012-Multiple_Aspect_Summarization_Using_Integer_Linear_Programming.html">94 emnlp-2012-Multiple Aspect Summarization Using Integer Linear Programming</a></p>
<p>7 0.28313708 <a title="17-lsi-7" href="./emnlp-2012-Lexical_Differences_in_Autobiographical_Narratives_from_Schizophrenic_Patients_and_Healthy_Controls.html">83 emnlp-2012-Lexical Differences in Autobiographical Narratives from Schizophrenic Patients and Healthy Controls</a></p>
<p>8 0.27346009 <a title="17-lsi-8" href="./emnlp-2012-N-gram-based_Tense_Models_for_Statistical_Machine_Translation.html">95 emnlp-2012-N-gram-based Tense Models for Statistical Machine Translation</a></p>
<p>9 0.27069688 <a title="17-lsi-9" href="./emnlp-2012-Revisiting_the_Predictability_of_Language%3A_Response_Completion_in_Social_Media.html">114 emnlp-2012-Revisiting the Predictability of Language: Response Completion in Social Media</a></p>
<p>10 0.2587339 <a title="17-lsi-10" href="./emnlp-2012-A_Statistical_Relational_Learning_Approach_to_Identifying_Evidence_Based_Medicine_Categories.html">10 emnlp-2012-A Statistical Relational Learning Approach to Identifying Evidence Based Medicine Categories</a></p>
<p>11 0.25392604 <a title="17-lsi-11" href="./emnlp-2012-Characterizing_Stylistic_Elements_in_Syntactic_Structure.html">27 emnlp-2012-Characterizing Stylistic Elements in Syntactic Structure</a></p>
<p>12 0.25094599 <a title="17-lsi-12" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>13 0.24802826 <a title="17-lsi-13" href="./emnlp-2012-Part-of-Speech_Tagging_for_Chinese-English_Mixed_Texts_with_Dynamic_Features.html">106 emnlp-2012-Part-of-Speech Tagging for Chinese-English Mixed Texts with Dynamic Features</a></p>
<p>14 0.24482852 <a title="17-lsi-14" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>15 0.23527151 <a title="17-lsi-15" href="./emnlp-2012-Concurrent_Acquisition_of_Word_Meaning_and_Lexical_Categories.html">29 emnlp-2012-Concurrent Acquisition of Word Meaning and Lexical Categories</a></p>
<p>16 0.23289613 <a title="17-lsi-16" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>17 0.23089981 <a title="17-lsi-17" href="./emnlp-2012-Universal_Grapheme-to-Phoneme_Prediction_Over_Latin_Alphabets.html">132 emnlp-2012-Universal Grapheme-to-Phoneme Prediction Over Latin Alphabets</a></p>
<p>18 0.22438258 <a title="17-lsi-18" href="./emnlp-2012-Probabilistic_Finite_State_Machines_for_Regression-based_MT_Evaluation.html">108 emnlp-2012-Probabilistic Finite State Machines for Regression-based MT Evaluation</a></p>
<p>19 0.22399054 <a title="17-lsi-19" href="./emnlp-2012-Learning_Syntactic_Categories_Using_Paradigmatic_Representations_of_Word_Context.html">79 emnlp-2012-Learning Syntactic Categories Using Paradigmatic Representations of Word Context</a></p>
<p>20 0.22193636 <a title="17-lsi-20" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.019), (16, 0.023), (25, 0.015), (34, 0.045), (45, 0.015), (60, 0.068), (63, 0.513), (64, 0.016), (65, 0.022), (70, 0.024), (74, 0.053), (76, 0.038), (80, 0.012), (86, 0.027), (95, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95300192 <a title="17-lda-1" href="./emnlp-2012-Modelling_Sequential_Text_with_an_Adaptive_Topic_Model.html">90 emnlp-2012-Modelling Sequential Text with an Adaptive Topic Model</a></p>
<p>Author: Lan Du ; Wray Buntine ; Huidong Jin</p><p>Abstract: Topic models are increasingly being used for text analysis tasks, often times replacing earlier semantic techniques such as latent semantic analysis. In this paper, we develop a novel adaptive topic model with the ability to adapt topics from both the previous segment and the parent document. For this proposed model, a Gibbs sampler is developed for doing posterior inference. Experimental results show that with topic adaptation, our model significantly improves over existing approaches in terms of perplexity, and is able to uncover clear sequential structure on, for example, Herman Melville’s book “Moby Dick”.</p><p>2 0.94767529 <a title="17-lda-2" href="./emnlp-2012-Multiple_Aspect_Summarization_Using_Integer_Linear_Programming.html">94 emnlp-2012-Multiple Aspect Summarization Using Integer Linear Programming</a></p>
<p>Author: Kristian Woodsend ; Mirella Lapata</p><p>Abstract: Multi-document summarization involves many aspects of content selection and surface realization. The summaries must be informative, succinct, grammatical, and obey stylistic writing conventions. We present a method where such individual aspects are learned separately from data (without any hand-engineering) but optimized jointly using an integer linear programme. The ILP framework allows us to combine the decisions of the expert learners and to select and rewrite source content through a mixture of objective setting, soft and hard constraints. Experimental results on the TAC-08 data set show that our model achieves state-of-the-art performance using ROUGE and significantly improves the informativeness of the summaries.</p><p>3 0.92453444 <a title="17-lda-3" href="./emnlp-2012-Open_Language_Learning_for_Information_Extraction.html">100 emnlp-2012-Open Language Learning for Information Extraction</a></p>
<p>Author: Mausam ; Michael Schmitz ; Stephen Soderland ; Robert Bart ; Oren Etzioni</p><p>Abstract: Open Information Extraction (IE) systems extract relational tuples from text, without requiring a pre-specified vocabulary, by identifying relation phrases and associated arguments in arbitrary sentences. However, stateof-the-art Open IE systems such as REVERB and WOE share two important weaknesses (1) they extract only relations that are mediated by verbs, and (2) they ignore context, thus extracting tuples that are not asserted as factual. This paper presents OLLIE, a substantially improved Open IE system that addresses both these limitations. First, OLLIE achieves high yield by extracting relations mediated by nouns, adjectives, and more. Second, a context-analysis step increases precision by including contextual information from the sentence in the extractions. OLLIE obtains 2.7 times the area under precision-yield curve (AUC) compared to REVERB and 1.9 times the AUC of WOEparse. –</p><p>same-paper 4 0.90912831 <a title="17-lda-4" href="./emnlp-2012-An_%22AI_readability%22_Formula_for_French_as_a_Foreign_Language.html">17 emnlp-2012-An "AI readability" Formula for French as a Foreign Language</a></p>
<p>Author: Thomas Francois ; Cedrick Fairon</p><p>Abstract: This paper present a new readability formula for French as a foreign language (FFL), which relies on 46 textual features representative of the lexical, syntactic, and semantic levels as well as some of the specificities of the FFL context. We report comparisons between several techniques for feature selection and various learning algorithms. Our best model, based on support vector machines (SVM), significantly outperforms previous FFL formulas. We also found that semantic features behave poorly in our case, in contrast with some previous readability studies on English as a first language.</p><p>5 0.82675725 <a title="17-lda-5" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>Author: Mohamed Yahya ; Klaus Berberich ; Shady Elbassuoni ; Maya Ramanath ; Volker Tresp ; Gerhard Weikum</p><p>Abstract: The Linked Data initiative comprises structured databases in the Semantic-Web data model RDF. Exploring this heterogeneous data by structured query languages is tedious and error-prone even for skilled users. To ease the task, this paper presents a methodology for translating natural language questions into structured SPARQL queries over linked-data sources. Our method is based on an integer linear program to solve several disambiguation tasks jointly: the segmentation of questions into phrases; the mapping of phrases to semantic entities, classes, and relations; and the construction of SPARQL triple patterns. Our solution harnesses the rich type system provided by knowledge bases in the web of linked data, to constrain our semantic-coherence objective function. We present experiments on both the . in question translation and the resulting query answering.</p><p>6 0.67649066 <a title="17-lda-6" href="./emnlp-2012-A_Phrase-Discovering_Topic_Model_Using_Hierarchical_Pitman-Yor_Processes.html">8 emnlp-2012-A Phrase-Discovering Topic Model Using Hierarchical Pitman-Yor Processes</a></p>
<p>7 0.67405593 <a title="17-lda-7" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>8 0.65025908 <a title="17-lda-8" href="./emnlp-2012-SSHLDA%3A_A_Semi-Supervised_Hierarchical_Topic_Model.html">115 emnlp-2012-SSHLDA: A Semi-Supervised Hierarchical Topic Model</a></p>
<p>9 0.61476207 <a title="17-lda-9" href="./emnlp-2012-PATTY%3A_A_Taxonomy_of_Relational_Patterns_with_Semantic_Types.html">103 emnlp-2012-PATTY: A Taxonomy of Relational Patterns with Semantic Types</a></p>
<p>10 0.57101625 <a title="17-lda-10" href="./emnlp-2012-Discovering_Diverse_and_Salient_Threads_in_Document_Collections.html">33 emnlp-2012-Discovering Diverse and Salient Threads in Document Collections</a></p>
<p>11 0.56738502 <a title="17-lda-11" href="./emnlp-2012-Characterizing_Stylistic_Elements_in_Syntactic_Structure.html">27 emnlp-2012-Characterizing Stylistic Elements in Syntactic Structure</a></p>
<p>12 0.564138 <a title="17-lda-12" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>13 0.56382382 <a title="17-lda-13" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>14 0.55796796 <a title="17-lda-14" href="./emnlp-2012-A_Systematic_Comparison_of_Phrase_Table_Pruning_Techniques.html">11 emnlp-2012-A Systematic Comparison of Phrase Table Pruning Techniques</a></p>
<p>15 0.55622596 <a title="17-lda-15" href="./emnlp-2012-Translation_Model_Based_Cross-Lingual_Language_Model_Adaptation%3A_from_Word_Models_to_Phrase_Models.html">128 emnlp-2012-Translation Model Based Cross-Lingual Language Model Adaptation: from Word Models to Phrase Models</a></p>
<p>16 0.54927248 <a title="17-lda-16" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>17 0.54143977 <a title="17-lda-17" href="./emnlp-2012-An_Entity-Topic_Model_for_Entity_Linking.html">19 emnlp-2012-An Entity-Topic Model for Entity Linking</a></p>
<p>18 0.54081851 <a title="17-lda-18" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>19 0.53929466 <a title="17-lda-19" href="./emnlp-2012-Revisiting_the_Predictability_of_Language%3A_Response_Completion_in_Social_Media.html">114 emnlp-2012-Revisiting the Predictability of Language: Response Completion in Social Media</a></p>
<p>20 0.53587127 <a title="17-lda-20" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
