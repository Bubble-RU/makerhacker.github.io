<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>13 emnlp-2012-A Unified Approach to Transliteration-based Text Input with Online Spelling Correction</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-13" href="#">emnlp2012-13</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>13 emnlp-2012-A Unified Approach to Transliteration-based Text Input with Online Spelling Correction</h1>
<br/><p>Source: <a title="emnlp-2012-13-pdf" href="http://aclweb.org/anthology//D/D12/D12-1056.pdf">pdf</a></p><p>Author: Hisami Suzuki ; Jianfeng Gao</p><p>Abstract: This paper presents an integrated, end-to-end approach to online spelling correction for text input. Online spelling correction refers to the spelling correction as you type, as opposed to post-editing. The online scenario is particularly important for languages that routinely use transliteration-based text input methods, such as Chinese and Japanese, because the desired target characters cannot be input at all unless they are in the list of candidates provided by an input method, and spelling errors prevent them from appearing in the list. For example, a user might type suesheng by mistake to mean xuesheng 学生 'student' in Chinese; existing input methods fail to convert this misspelled input to the desired target Chinese characters. In this paper, we propose a unified approach to the problem of spelling correction and transliteration-based character conversion using an approach inspired by the phrasebased statistical machine translation framework. At the phrase (substring) level, k most probable pinyin (Romanized Chinese) corrections are generated using a monotone decoder; at the sentence level, input pinyin strings are directly transliterated into target Chinese characters by a decoder using a loglinear model that refer to the features of both levels. A new method of automatically deriving parallel training data from user keystroke logs is also presented. Experiments on Chinese pinyin conversion show that our integrated method reduces the character error rate by 20% (from 8.9% to 7. 12%) over the previous state-of-the art based on a noisy channel model. 609 1</p><p>Reference: <a title="emnlp-2012-13-reference" href="../emnlp2012_reference/emnlp-2012-A_Unified_Approach_to_Transliteration-based_Text_Input_with_Online_Spelling_Correction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract This paper presents an integrated, end-to-end approach to online spelling correction for text input. [sent-2, score-0.852]
</p><p>2 Online spelling correction refers to the spelling correction as you type, as opposed to post-editing. [sent-3, score-1.642]
</p><p>3 In this paper, we propose a unified approach to the  problem of spelling correction and transliteration-based character conversion using an approach inspired by the phrasebased statistical machine translation framework. [sent-6, score-1.249]
</p><p>4 A new method of automatically deriving parallel training data from user keystroke logs is also presented. [sent-8, score-0.294]
</p><p>5 Experiments on Chinese pinyin conversion show that our integrated method reduces the character error rate by 20% (from 8. [sent-9, score-1.104]
</p><p>6 609 1  Introduction  This paper addresses the problem of online spelling correction, which tries to correct users' misspellings as they type, rather than post-editing  them after they have already been input. [sent-12, score-0.321]
</p><p>7 One big challenge posed by spelling mistakes is that they prevent the desired candidates from appearing as conversion candidates, as in Figure 1: suesheng is likely to be a spelling error of xuesheng 学生 'student', but it is not included as one of the candidates. [sent-14, score-1.056]
</p><p>8 Figure 1: Spelling mistake prevents the desired output (学生) from appearing in the list of candidates This severely limits the utility of an IME, as spelling errors are extremely common. [sent-15, score-0.403]
</p><p>9 In this paper, we propose a novel, unified system of text input with spelling correction, using Chinese pinyin-to-hanzi conversion as an example. [sent-18, score-0.652]
</p><p>10 We first formulate the task of pinyin spelling correction as a substring-based monotone translation problem, inspired by phrase-based statistical machine translation (SMT) systems (Koehn et al. [sent-19, score-1.524]
</p><p>11 , 2003; Och and Ney, 2004): we consider the pinyin input (potentially with errors) as the source language and the error-corrected pinyin as the target, and build a log-linear model for spelling correction. [sent-20, score-1.541]
</p><p>12 Second, we build an end-toend pinyin-to-hanzi conversion system by combining all the feature functions used in the error correction and character conversion components in an SMT-style log-linear model, where the feature weights are trained discriminatively for the end-to-end task. [sent-24, score-1.3]
</p><p>13 2011b), in which only the error model and the conversion model probabilities are used and combined with equal  weights. [sent-26, score-0.353]
</p><p>14 Finally, like other statistical systems, the amount and quality of training data control the quality of the outcome; we thus propose a new, language-independent method of deriving parallel data for spelling correction from user keystroke logs. [sent-27, score-1.061]
</p><p>15 We performed experiments on various methods of integrating the error correction and character conversion sub-components. [sent-28, score-0.985]
</p><p>16 Our best system, a 610 fully integrated SMT-based approach, reduces the character error rate by 35% on test data that is completely independent of the creation of error correction and character conversion models. [sent-29, score-1.246]
</p><p>17 We then describe our approach to the spelling correction task (Section 3) and the end-to-end conversion task (Section 4). [sent-31, score-1.072]
</p><p>18 2  Related Work  The current work builds on many previous works on the task of monotone substring-based  transduction, including spelling correction, letterto-phone conversion and transliteration between different scripts. [sent-33, score-0.621]
</p><p>19 In particular, our substring-based approach to spelling correction is motivated by the success on transliteration (e. [sent-34, score-0.839]
</p><p>20 One big challenge of the spelling correction research is the general lack of naturally occurring paired data of contextual spelling errors and their correction. [sent-41, score-1.157]
</p><p>21 Another approach to spelling error data collection uses web search query logs, available in large quantity (albeit to limited  institutions), and limit its focus on the task of correcting misspelled queries (e. [sent-48, score-0.474]
</p><p>22 The problem of data collection is particularly difficult for pinyin error correction, as pinyin is not a final form of text in Chinese, so it is not recorded in final text. [sent-53, score-1.316]
</p><p>23 (201 1a) study a log of pinyin input method and use the backspace key to learn the user mistyping behavior, but they do so only for the purpose of data analysis, and do not build a statistical model from this data. [sent-55, score-0.833]
</p><p>24 There is only a very limited amount of work that deals with spelling correction in the context of text input: Zheng et al. [sent-66, score-0.836]
</p><p>25 Their work is strictly word-based and only handles the correction of out-of-vocabulary pinyin words into in-vocabulary pinyin words, while our substring-based model is not limited by these constraints. [sent-68, score-1.7]
</p><p>26 (2011) is thus relevant to our study, though their approach differs from ours in that we build an integrated system that include the feature functions of both error correction and character conversion sub-systems. [sent-71, score-1.044]
</p><p>27 3  Substring-based Spelling using a Log-linear Model  Correction  In this section, we describe our approach to pinyin error correction within a log-linear framework. [sent-72, score-1.21]
</p><p>28 Though our current target is pinyin error correction, the method described in this section is applicable to any language of interest. [sent-73, score-0.694]
</p><p>29 The spelling correction problem has been standardly formulated within the framework of  noisy channel model (e. [sent-74, score-1.011]
</p><p>30 The task of spelling correction is to search for the best 1 A few examples include Google Transliterate (http://www. [sent-79, score-0.821]
</p><p>31 Our model is inspired by the SMT framework, in which the error correction probability | of Equation (1) is directly modeled using a log-linear model of the following form:  |  ∑  where Z(A) is the normalization factor, hi is a feature function and λi is the feature weight. [sent-92, score-0.618]
</p><p>32 Similarly to phrase-based SMT, many feature functions are derived from the translation and language models, where the translation modelderived features are trained using a parallel corpus of original pinyin and correction pairs. [sent-93, score-1.224]
</p><p>33 The argmax of Equation (1) defines the search operation: we use a left-to-right beam search decoder to seek for each input pinyin the best correction according to Equation (3). [sent-94, score-1.175]
</p><p>34 We discuss the results of  pinyin error correction as an independent task in Section 3. [sent-98, score-1.21]
</p><p>35 1  Generating error correction pairs from keystroke logs Unlike English text, which includes instances of misspelled words explicitly, pinyin spelling errors are not found in a corpus, because pinyin is used as a means of inputting text, and is not part of the final written form of the language. [sent-101, score-2.41]
</p><p>36 Therefore, pinyin error correction pairs must be created intentionally. [sent-102, score-1.21]
</p><p>37 Such keystroke logs include the use of the backspace key, from which we compute the pinyin strings after the usage of the backspace keys as well as the putative pinyin string had the user not corrected it using the backspace key. [sent-104, score-1.891]
</p><p>38 2 Table 1 shows a few examples of the entries in the keystroke log, along with the computed pinyin strings before and after correction. [sent-105, score-0.763]
</p><p>39 Each entry (or phrase) in the log represents the unit that corresponds to the sequence the user input at once,  at the end of which the user committed to a conversion candidate, which typically consists of one or more words. [sent-106, score-0.426]
</p><p>40 In order to recover the desired pre-correcting string, we compared the prefix of the backspace usage (zhonguo) with the substrings after error correction (zhong, zhongg, zhonggu… ). [sent-110, score-0.778]
</p><p>41 (201 1a) also uses the backspace  key in the IME log to generate error-correction pairs, but they focus on the usage of a backspace after the desired hanzi characters have been input, i. [sent-113, score-0.4]
</p><p>42 In contrast, our method focuses on the use of backspace to delete one or more pinyin characters before conversion. [sent-116, score-0.78]
</p><p>43 This simulates the scenario of online error correction more truthfully, and can collect paired data in large quantity faster. [sent-117, score-0.634]
</p><p>44 keystroke pre-postcorrection correction na ns re n z ho ng uo __g uo _  nansen zhonguo  nanren zhongguo  (*zhonguoo) Table 1: Computation of pre- and post-correction strings from keystroke log and is language-independent. [sent-118, score-0.946]
</p><p>45 Since paired error correction data do not exist naturally and is expensive to collect for any language, we believe that the proposed method is useful beyond the case of Chinese text input and applicable to the data collection of the spelling correction task in general. [sent-119, score-1.506]
</p><p>46 3  the error-correction  pairs for research  The extracted pairs are still quite noisy, because one error correction behavior might not completely eliminate the errors in typing a word. [sent-121, score-0.681]
</p><p>47 For example, in trying to type women 我们 'we', a user might first type wmen, hit the backspaces key four times, retype womeen, and commit to a conversion candidate by mistake. [sent-122, score-0.325]
</p><p>48 We used 5,000 pairs for testing, 1,000 pairs for tuning the loglinear model weights (see the next subsection), and the remaining portion for training the error correction component. [sent-127, score-0.651]
</p><p>49 2 Training the log-linear model The translation model captures substring-based spelling error patterns and their transformation  probabilities. [sent-129, score-0.439]
</p><p>50 From this pair, we learn a set of error patterns that are consistent with the character alignment,4 each of which is a pair of substrings indicating how the spelling is transformed from one to another. [sent-136, score-0.557]
</p><p>51 Our error correction model is completely substringbased and does not use a word-based lexicon, which gives us the flexibility of generating unseen correction targets as well as supporting pinyin input consisting of multiple words at a time. [sent-143, score-1.778]
</p><p>52 For the language model, we use a character 9-gram model to capture the knowledge of correctly spelled pinyin words and phrases. [sent-144, score-0.724]
</p><p>53 1, though it is possible to train it with an arbitrary text in pinyin when such data is available. [sent-146, score-0.607]
</p><p>54 It is more directly related with the word/phrase-level accuracy, which we used to evaluate the error correction module in isolation, than the BLEU metric. [sent-158, score-0.618]
</p><p>55 As we will show below, however, using different objective functions turned out to have only a minimal impact  on the spelling correction accuracy. [sent-159, score-0.855]
</p><p>56 3 Experiments and results The performance of pinyin error correction was evaluated on two data sets: (1) log-test: the test set of the data in Section 3. [sent-161, score-1.21]
</p><p>57 This data set consists of 2,000 sentence pairs of pinyin input with errors and the target hanzi characters, constructed by collecting actual user typing logs of the Lancaster corpus (McEnery and Xiao, 2004), which includes text from newspaper, fiction, and essays. [sent-164, score-0.91]
</p><p>58 The CHIME data set does not include the corrected pinyin string; we therefore generated this by running a text-to-pinyin utility, 7 and created the pairs before and after error correction for evaluating our pinyin spelling correction module. [sent-165, score-2.658]
</p><p>59 8 CHIME: CHIME: CHIME: CHIME:  No correction Noisy Channel Proposed (BLEU) Proposed (CER)  92. [sent-187, score-0.516]
</p><p>60 08  Table 2: Pinyin error correction accuracy (in %) defined unit of conversion, consisting of one to a few words), while the CHIME data set is wordsegmented. [sent-197, score-0.618]
</p><p>61 Somewhat surprisingly, the noisy channel model results fall below the baseline in both data sets, while the log-linear model improves over the baseline, especially on the 1-best accuracy: all differences between the noisy channel model and the log-linear model outputs are significant. [sent-205, score-0.38]
</p><p>62 For a monotone decoding task such as spelling correction, using either objective function therefore  seems to suffice, even though BLEU is more indirect and redundant in capturing the phraselevel accuracy. [sent-207, score-0.352]
</p><p>63 4  614 A Unified Model of Character Conversion with Spelling Correction  In this section we describe our unified model of spelling correction and transliteration-based character conversion. [sent-208, score-0.966]
</p><p>64 Analogous to the spelling correction task, the character conversion problem can also be considered as a substring-based translation problem. [sent-209, score-1.22]
</p><p>65 The novelty of our approach lies in the fact that we take advantage of the parallelism between these tasks, and build an integrated model that performs spelling correction and character conversion at the same time, within the log-linear framework. [sent-210, score-1.213]
</p><p>66 1  Noisy channel model approach incorporating error correction character conversion  to in  The task of pinyin-to-hanzi conversion consists of converting the input phonetic strings provided by the user into the appropriate word string using ideographic characters. [sent-213, score-1.524]
</p><p>67 This has been formulated within the noisy channel model (Chen and Lee, 2000), in exactly the same manner as the spelling correction, as describe in Equations (1) and (2) in Section 3. [sent-214, score-0.495]
</p><p>68 Given the pinyin input A, the task is to find the best output hanzi sequence W*: |  | In traditional conversion systems which do not consider spelling errors, P(A|W) is usually set to 1 if the word is found in a dictionary of wordpronunciation pairs, which also defines GEN(A). [sent-215, score-1.295]
</p><p>69 An extension of this formulation to handle spelling errors can be achieved by incorporating an actual error model P(A|W). [sent-217, score-0.438]
</p><p>70 Assuming a conditional independence of A and W given the error-corrected pinyin sequence C, Equation (4) can be re-written  as : k-best error correction candidates c1. [sent-218, score-1.24]
</p><p>71 kn  | |  Here, P(C|W) corresponds to the channel model of traditional input methods, P(W) the language model, and P(C|A) the pinyin error correction model. [sent-230, score-1.39]
</p><p>72 This noisy channel integration of error correction and character conversion is the state-ofthe-art in the task of error-correcting text input, and will serve as our baseline. [sent-233, score-1.215]
</p><p>73 615 substring-based phrase table generated for the pinyin error correction task in Section 3 with the results of character conversion. [sent-238, score-1.353]
</p><p>74 This process is described in detail in Figure 2: k-best candidates for each input pinyin phrase a are generated by the error model in Section 3, which are then submitted  offline to an IME system to obtain n-best conversion candidates with probabilities. [sent-239, score-1.098]
</p><p>75 In the resulting translation table, defined for each (a, w) pair, the feature functions and their values are inherited from the pinyin error correction translation table mediated by the correction candidates c1…k for a, plus the function that defines the IME conversion probability for (cj, w). [sent-241, score-2.146]
</p><p>76 Note that in this final phrase table, the correction candidates for a are latent, only affecting the values of the feature functions. [sent-242, score-0.573]
</p><p>77 This generates maximally 400 conversion candidates for each input pinyin. [sent-250, score-0.333]
</p><p>78 As running MERT on a CERbased target criterion on the similar, monotone translation task of spelling correction did not lead to a significant improvement (Section 3. [sent-252, score-0.9]
</p><p>79 We measured our results using character error rate (CER), which is based on the longest common subsequence match in characters between the reference and the best system output. [sent-259, score-0.293]
</p><p>80 As our goal is to show the effectiveness of the unified approach, we used simpler methods of  integrating pinyin error correction with character conversion to create baselines. [sent-269, score-1.606]
</p><p>81 The simplest 10 From Table 2, we observe that the accuracy of the 20-best output of the spelling correction component is over 99%. [sent-270, score-0.821]
</p><p>82 CER on CER on 1-best 5-best Baseline: No correction 10. [sent-272, score-0.516]
</p><p>83 5 1  Table 3: CER results for the conversion task (%) baseline is a pre-processing approach: we use the pinyin error correction model to convert A into a single best candidate C, and run an IME system on C. [sent-284, score-1.461]
</p><p>84 The oracle CER, which is the result of applying the IME on the gold standard pinyin input derived from the reference text using a hanzi-topinyin converter (as mentioned in Section 3. [sent-293, score-0.659]
</p><p>85 The simple pipeline approach of concatenating the pinyin correction component with the character conversion component improves the CER by 1% to 9. [sent-296, score-1.475]
</p><p>86 With the use of additional feature functions weighted discriminatively for the final conversion task, the 11 Available at http://chime. [sent-306, score-0.299]
</p><p>87 12, a 35% relative error rate reduction compared with the no correction baseline, a 20% reduction against Zheng et al (201 1b) and a 10% reduction from our noisy channel baseline. [sent-315, score-0.826]
</p><p>88 We find that somewhat contrary to our expectation, over-correction of the spelling mistakes was not a conspicuous problem, even though the pinyin correction rate of the training data is much higher than that of the test data. [sent-320, score-1.431]
</p><p>89 We therefore conclude that the error correction model adapts very well to the characteristics of the test data in our integrated SMT-based approach, which trains the unified feature weights to optimize the end goal. [sent-321, score-0.688]
</p><p>90 We have also presented a new method of automatically collecting parallel data for spelling correction from user keystroke logs, and showed that the log-linear model works well on the task of spelling correction in isolation as well. [sent-323, score-1.864]
</p><p>91 617 In this study, we isolated the problem of spelling errors and studied the effectiveness of error correction over a basic IME system that does not include advanced features such as abbreviated input (e. [sent-324, score-1.006]
</p><p>92 , typing only "py" for 朋 友 pengyou 'friend' or 拼 音 pinyin in Chinese) and autocompletion (e. [sent-326, score-0.624]
</p><p>93 Integrating datadriven error correction feature with these advanced features for the benefit of users is the challenge we face in the next step. [sent-329, score-0.632]
</p><p>94 A study of corrected and uncorrected spelling errors using keystroke logs. [sent-337, score-0.521]
</p><p>95 An improved error model for noisy channel spelling correction. [sent-357, score-0.597]
</p><p>96 Spelling correction as an iterative process that exploits the collective knowledge of web users. [sent-376, score-0.516]
</p><p>97 A large scale ranker-based system for search query spelling correction. [sent-402, score-0.305]
</p><p>98 A spelling correction program based on a noisy channel model. [sent-415, score-1.011]
</p><p>99 Modeling letter-to-phoneme conversion as a phrase based statistical machine translation problem with minimum error rate training. [sent-453, score-0.43]
</p><p>100 Understanding user input behaviors in Chinese pinyin input method. [sent-516, score-0.75]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pinyin', 0.592), ('correction', 0.516), ('spelling', 0.305), ('conversion', 0.251), ('ime', 0.164), ('keystroke', 0.15), ('chime', 0.13), ('channel', 0.128), ('cer', 0.122), ('backspace', 0.12), ('character', 0.116), ('error', 0.102), ('hanzi', 0.08), ('noisy', 0.062), ('zheng', 0.061), ('user', 0.054), ('logs', 0.054), ('input', 0.052), ('monotone', 0.047), ('gao', 0.046), ('chinese', 0.044), ('characters', 0.042), ('zhonguo', 0.04), ('misspelled', 0.038), ('japanese', 0.036), ('corrected', 0.035), ('functions', 0.034), ('suzuki', 0.034), ('string', 0.033), ('translation', 0.032), ('typing', 0.032), ('errors', 0.031), ('brill', 0.03), ('candidates', 0.03), ('inputting', 0.03), ('lancaster', 0.03), ('sherif', 0.03), ('correcting', 0.029), ('unified', 0.029), ('smt', 0.028), ('substring', 0.027), ('phrase', 0.027), ('mediated', 0.026), ('delete', 0.026), ('integrated', 0.025), ('integration', 0.025), ('bleu', 0.024), ('mori', 0.023), ('desired', 0.023), ('edit', 0.021), ('strings', 0.021), ('baba', 0.02), ('backspaces', 0.02), ('mcenery', 0.02), ('rama', 0.02), ('routinely', 0.02), ('suesheng', 0.02), ('tokunaga', 0.02), ('uo', 0.02), ('wmen', 0.02), ('womeen', 0.02), ('xuesheng', 0.02), ('mert', 0.02), ('rate', 0.018), ('parallel', 0.018), ('transliteration', 0.018), ('deriving', 0.018), ('microsoft', 0.018), ('moore', 0.018), ('kernighan', 0.017), ('loglinear', 0.017), ('jiampojamarn', 0.017), ('penalties', 0.017), ('substrings', 0.017), ('whitelaw', 0.017), ('pair', 0.017), ('och', 0.017), ('online', 0.016), ('weights', 0.016), ('chen', 0.016), ('kondrak', 0.016), ('micol', 0.016), ('spelled', 0.016), ('straightforwardly', 0.016), ('toutanova', 0.015), ('text', 0.015), ('defines', 0.015), ('longest', 0.015), ('particularly', 0.015), ('log', 0.015), ('offline', 0.014), ('mle', 0.014), ('zhongguo', 0.014), ('aligned', 0.014), ('users', 0.014), ('corrections', 0.014), ('discriminatively', 0.014), ('mistake', 0.014), ('trigram', 0.013), ('cherry', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="13-tfidf-1" href="./emnlp-2012-A_Unified_Approach_to_Transliteration-based_Text_Input_with_Online_Spelling_Correction.html">13 emnlp-2012-A Unified Approach to Transliteration-based Text Input with Online Spelling Correction</a></p>
<p>Author: Hisami Suzuki ; Jianfeng Gao</p><p>Abstract: This paper presents an integrated, end-to-end approach to online spelling correction for text input. Online spelling correction refers to the spelling correction as you type, as opposed to post-editing. The online scenario is particularly important for languages that routinely use transliteration-based text input methods, such as Chinese and Japanese, because the desired target characters cannot be input at all unless they are in the list of candidates provided by an input method, and spelling errors prevent them from appearing in the list. For example, a user might type suesheng by mistake to mean xuesheng 学生 'student' in Chinese; existing input methods fail to convert this misspelled input to the desired target Chinese characters. In this paper, we propose a unified approach to the problem of spelling correction and transliteration-based character conversion using an approach inspired by the phrasebased statistical machine translation framework. At the phrase (substring) level, k most probable pinyin (Romanized Chinese) corrections are generated using a monotone decoder; at the sentence level, input pinyin strings are directly transliterated into target Chinese characters by a decoder using a loglinear model that refer to the features of both levels. A new method of automatically deriving parallel training data from user keystroke logs is also presented. Experiments on Chinese pinyin conversion show that our integrated method reduces the character error rate by 20% (from 8.9% to 7. 12%) over the previous state-of-the art based on a noisy channel model. 609 1</p><p>2 0.49244875 <a title="13-tfidf-2" href="./emnlp-2012-A_Discriminative_Model_for_Query_Spelling_Correction_with_Latent_Structural_SVM.html">5 emnlp-2012-A Discriminative Model for Query Spelling Correction with Latent Structural SVM</a></p>
<p>Author: Huizhong Duan ; Yanen Li ; ChengXiang Zhai ; Dan Roth</p><p>Abstract: Discriminative training in query spelling correction is difficult due to the complex internal structures of the data. Recent work on query spelling correction suggests a two stage approach a noisy channel model that is used to retrieve a number of candidate corrections, followed by discriminatively trained ranker applied to these candidates. The ranker, however, suffers from the fact the low recall of the first, suboptimal, search stage. This paper proposes to directly optimize the search stage with a discriminative model based on latent structural SVM. In this model, we treat query spelling correction as a multiclass classification problem with structured input and output. The latent structural information is used to model the alignment of words in the spelling correction process. Experiment results show that as a standalone speller, our model outperforms all the baseline systems. It also attains a higher recall compared with the noisy channel model, and can therefore serve as a better filtering stage when combined with a ranker.</p><p>3 0.32228562 <a title="13-tfidf-3" href="./emnlp-2012-A_Beam-Search_Decoder_for_Grammatical_Error_Correction.html">2 emnlp-2012-A Beam-Search Decoder for Grammatical Error Correction</a></p>
<p>Author: Daniel Dahlmeier ; Hwee Tou Ng</p><p>Abstract: We present a novel beam-search decoder for grammatical error correction. The decoder iteratively generates new hypothesis corrections from current hypotheses and scores them based on features of grammatical correctness and fluency. These features include scores from discriminative classifiers for specific error categories, such as articles and prepositions. Unlike all previous approaches, our method is able to perform correction of whole sentences with multiple and interacting errors while still taking advantage of powerful existing classifier approaches. Our decoder achieves an F1 correction score significantly higher than all previous published scores on the Helping Our Own (HOO) shared task data set.</p><p>4 0.044969738 <a title="13-tfidf-4" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>Author: Zhongguo Li ; Guodong Zhou</p><p>Abstract: Most previous approaches to syntactic parsing of Chinese rely on a preprocessing step of word segmentation, thereby assuming there was a clearly defined boundary between morphology and syntax in Chinese. We show how this assumption can fail badly, leading to many out-of-vocabulary words and incompatible annotations. Hence in practice the strict separation of morphology and syntax in the Chinese language proves to be untenable. We present a unified dependency parsing approach for Chinese which takes unsegmented sentences as input and outputs both morphological and syntactic structures with a single model and algorithm. By removing the intermediate word segmentation, the unified parser no longer needs separate notions for words and phrases. Evaluation proves the effectiveness of the unified model and algorithm in parsing structures of words, phrases and sen- tences simultaneously. 1</p><p>5 0.042699885 <a title="13-tfidf-5" href="./emnlp-2012-Learning_Lexicon_Models_from_Search_Logs_for_Query_Expansion.html">78 emnlp-2012-Learning Lexicon Models from Search Logs for Query Expansion</a></p>
<p>Author: Jianfeng Gao ; Shasha Xie ; Xiaodong He ; Alnur Ali</p><p>Abstract: This paper explores log-based query expansion (QE) models for Web search. Three lexicon models are proposed to bridge the lexical gap between Web documents and user queries. These models are trained on pairs of user queries and titles of clicked documents. Evaluations on a real world data set show that the lexicon models, integrated into a ranker-based QE system, not only significantly improve the document retrieval performance but also outperform two state-of-the-art log-based QE methods.</p><p>6 0.04175511 <a title="13-tfidf-6" href="./emnlp-2012-Locally_Training_the_Log-Linear_Model_for_SMT.html">86 emnlp-2012-Locally Training the Log-Linear Model for SMT</a></p>
<p>7 0.040465102 <a title="13-tfidf-7" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>8 0.040247601 <a title="13-tfidf-8" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>9 0.038195986 <a title="13-tfidf-9" href="./emnlp-2012-Optimising_Incremental_Dialogue_Decisions_Using_Information_Density_for_Interactive_Systems.html">102 emnlp-2012-Optimising Incremental Dialogue Decisions Using Information Density for Interactive Systems</a></p>
<p>10 0.035949361 <a title="13-tfidf-10" href="./emnlp-2012-Document-Wide_Decoding_for_Phrase-Based_Statistical_Machine_Translation.html">35 emnlp-2012-Document-Wide Decoding for Phrase-Based Statistical Machine Translation</a></p>
<p>11 0.035086378 <a title="13-tfidf-11" href="./emnlp-2012-Revisiting_the_Predictability_of_Language%3A_Response_Completion_in_Social_Media.html">114 emnlp-2012-Revisiting the Predictability of Language: Response Completion in Social Media</a></p>
<p>12 0.033478737 <a title="13-tfidf-12" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>13 0.032461852 <a title="13-tfidf-13" href="./emnlp-2012-Translation_Model_Based_Cross-Lingual_Language_Model_Adaptation%3A_from_Word_Models_to_Phrase_Models.html">128 emnlp-2012-Translation Model Based Cross-Lingual Language Model Adaptation: from Word Models to Phrase Models</a></p>
<p>14 0.031721957 <a title="13-tfidf-14" href="./emnlp-2012-Name_Phylogeny%3A_A_Generative_Model_of_String_Variation.html">96 emnlp-2012-Name Phylogeny: A Generative Model of String Variation</a></p>
<p>15 0.03065809 <a title="13-tfidf-15" href="./emnlp-2012-Forced_Derivation_Tree_based_Model_Training_to_Statistical_Machine_Translation.html">54 emnlp-2012-Forced Derivation Tree based Model Training to Statistical Machine Translation</a></p>
<p>16 0.030387489 <a title="13-tfidf-16" href="./emnlp-2012-Iterative_Annotation_Transformation_with_Predict-Self_Reestimation_for_Chinese_Word_Segmentation.html">68 emnlp-2012-Iterative Annotation Transformation with Predict-Self Reestimation for Chinese Word Segmentation</a></p>
<p>17 0.029823527 <a title="13-tfidf-17" href="./emnlp-2012-Part-of-Speech_Tagging_for_Chinese-English_Mixed_Texts_with_Dynamic_Features.html">106 emnlp-2012-Part-of-Speech Tagging for Chinese-English Mixed Texts with Dynamic Features</a></p>
<p>18 0.029308476 <a title="13-tfidf-18" href="./emnlp-2012-A_Systematic_Comparison_of_Phrase_Table_Pruning_Techniques.html">11 emnlp-2012-A Systematic Comparison of Phrase Table Pruning Techniques</a></p>
<p>19 0.028907003 <a title="13-tfidf-19" href="./emnlp-2012-Automatically_Constructing_a_Normalisation_Dictionary_for_Microblogs.html">22 emnlp-2012-Automatically Constructing a Normalisation Dictionary for Microblogs</a></p>
<p>20 0.028661238 <a title="13-tfidf-20" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.128), (1, -0.098), (2, -0.069), (3, 0.055), (4, -0.105), (5, -0.358), (6, 0.522), (7, -0.332), (8, -0.03), (9, 0.003), (10, -0.045), (11, 0.146), (12, 0.061), (13, -0.032), (14, -0.006), (15, -0.005), (16, 0.036), (17, -0.126), (18, -0.003), (19, -0.086), (20, -0.005), (21, -0.054), (22, 0.031), (23, -0.008), (24, 0.004), (25, -0.007), (26, 0.098), (27, -0.001), (28, 0.105), (29, 0.038), (30, -0.009), (31, 0.004), (32, -0.1), (33, 0.011), (34, 0.019), (35, 0.017), (36, -0.016), (37, 0.004), (38, -0.032), (39, 0.001), (40, 0.003), (41, 0.044), (42, -0.061), (43, 0.009), (44, 0.005), (45, 0.018), (46, 0.021), (47, -0.028), (48, 0.036), (49, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.967484 <a title="13-lsi-1" href="./emnlp-2012-A_Unified_Approach_to_Transliteration-based_Text_Input_with_Online_Spelling_Correction.html">13 emnlp-2012-A Unified Approach to Transliteration-based Text Input with Online Spelling Correction</a></p>
<p>Author: Hisami Suzuki ; Jianfeng Gao</p><p>Abstract: This paper presents an integrated, end-to-end approach to online spelling correction for text input. Online spelling correction refers to the spelling correction as you type, as opposed to post-editing. The online scenario is particularly important for languages that routinely use transliteration-based text input methods, such as Chinese and Japanese, because the desired target characters cannot be input at all unless they are in the list of candidates provided by an input method, and spelling errors prevent them from appearing in the list. For example, a user might type suesheng by mistake to mean xuesheng 学生 'student' in Chinese; existing input methods fail to convert this misspelled input to the desired target Chinese characters. In this paper, we propose a unified approach to the problem of spelling correction and transliteration-based character conversion using an approach inspired by the phrasebased statistical machine translation framework. At the phrase (substring) level, k most probable pinyin (Romanized Chinese) corrections are generated using a monotone decoder; at the sentence level, input pinyin strings are directly transliterated into target Chinese characters by a decoder using a loglinear model that refer to the features of both levels. A new method of automatically deriving parallel training data from user keystroke logs is also presented. Experiments on Chinese pinyin conversion show that our integrated method reduces the character error rate by 20% (from 8.9% to 7. 12%) over the previous state-of-the art based on a noisy channel model. 609 1</p><p>2 0.85225832 <a title="13-lsi-2" href="./emnlp-2012-A_Discriminative_Model_for_Query_Spelling_Correction_with_Latent_Structural_SVM.html">5 emnlp-2012-A Discriminative Model for Query Spelling Correction with Latent Structural SVM</a></p>
<p>Author: Huizhong Duan ; Yanen Li ; ChengXiang Zhai ; Dan Roth</p><p>Abstract: Discriminative training in query spelling correction is difficult due to the complex internal structures of the data. Recent work on query spelling correction suggests a two stage approach a noisy channel model that is used to retrieve a number of candidate corrections, followed by discriminatively trained ranker applied to these candidates. The ranker, however, suffers from the fact the low recall of the first, suboptimal, search stage. This paper proposes to directly optimize the search stage with a discriminative model based on latent structural SVM. In this model, we treat query spelling correction as a multiclass classification problem with structured input and output. The latent structural information is used to model the alignment of words in the spelling correction process. Experiment results show that as a standalone speller, our model outperforms all the baseline systems. It also attains a higher recall compared with the noisy channel model, and can therefore serve as a better filtering stage when combined with a ranker.</p><p>3 0.7612313 <a title="13-lsi-3" href="./emnlp-2012-A_Beam-Search_Decoder_for_Grammatical_Error_Correction.html">2 emnlp-2012-A Beam-Search Decoder for Grammatical Error Correction</a></p>
<p>Author: Daniel Dahlmeier ; Hwee Tou Ng</p><p>Abstract: We present a novel beam-search decoder for grammatical error correction. The decoder iteratively generates new hypothesis corrections from current hypotheses and scores them based on features of grammatical correctness and fluency. These features include scores from discriminative classifiers for specific error categories, such as articles and prepositions. Unlike all previous approaches, our method is able to perform correction of whole sentences with multiple and interacting errors while still taking advantage of powerful existing classifier approaches. Our decoder achieves an F1 correction score significantly higher than all previous published scores on the Helping Our Own (HOO) shared task data set.</p><p>4 0.21751942 <a title="13-lsi-4" href="./emnlp-2012-Learning_Lexicon_Models_from_Search_Logs_for_Query_Expansion.html">78 emnlp-2012-Learning Lexicon Models from Search Logs for Query Expansion</a></p>
<p>Author: Jianfeng Gao ; Shasha Xie ; Xiaodong He ; Alnur Ali</p><p>Abstract: This paper explores log-based query expansion (QE) models for Web search. Three lexicon models are proposed to bridge the lexical gap between Web documents and user queries. These models are trained on pairs of user queries and titles of clicked documents. Evaluations on a real world data set show that the lexicon models, integrated into a ranker-based QE system, not only significantly improve the document retrieval performance but also outperform two state-of-the-art log-based QE methods.</p><p>5 0.1369299 <a title="13-lsi-5" href="./emnlp-2012-Large_Scale_Decipherment_for_Out-of-Domain_Machine_Translation.html">75 emnlp-2012-Large Scale Decipherment for Out-of-Domain Machine Translation</a></p>
<p>Author: Qing Dou ; Kevin Knight</p><p>Abstract: We apply slice sampling to Bayesian decipherment and use our new decipherment framework to improve out-of-domain machine translation. Compared with the state of the art algorithm, our approach is highly scalable and produces better results, which allows us to decipher ciphertext with billions of tokens and hundreds of thousands of word types with high accuracy. We decipher a large amount ofmonolingual data to improve out-of-domain translation and achieve significant gains of up to 3.8 BLEU points.</p><p>6 0.13533245 <a title="13-lsi-6" href="./emnlp-2012-Automatically_Constructing_a_Normalisation_Dictionary_for_Microblogs.html">22 emnlp-2012-Automatically Constructing a Normalisation Dictionary for Microblogs</a></p>
<p>7 0.13404921 <a title="13-lsi-7" href="./emnlp-2012-Document-Wide_Decoding_for_Phrase-Based_Statistical_Machine_Translation.html">35 emnlp-2012-Document-Wide Decoding for Phrase-Based Statistical Machine Translation</a></p>
<p>8 0.12202583 <a title="13-lsi-8" href="./emnlp-2012-Probabilistic_Finite_State_Machines_for_Regression-based_MT_Evaluation.html">108 emnlp-2012-Probabilistic Finite State Machines for Regression-based MT Evaluation</a></p>
<p>9 0.11572009 <a title="13-lsi-9" href="./emnlp-2012-Source_Language_Adaptation_for_Resource-Poor_Machine_Translation.html">118 emnlp-2012-Source Language Adaptation for Resource-Poor Machine Translation</a></p>
<p>10 0.11435061 <a title="13-lsi-10" href="./emnlp-2012-Exploiting_Chunk-level_Features_to_Improve_Phrase_Chunking.html">45 emnlp-2012-Exploiting Chunk-level Features to Improve Phrase Chunking</a></p>
<p>11 0.11019632 <a title="13-lsi-11" href="./emnlp-2012-Language_Model_Rest_Costs_and_Space-Efficient_Storage.html">74 emnlp-2012-Language Model Rest Costs and Space-Efficient Storage</a></p>
<p>12 0.10844072 <a title="13-lsi-12" href="./emnlp-2012-Name_Phylogeny%3A_A_Generative_Model_of_String_Variation.html">96 emnlp-2012-Name Phylogeny: A Generative Model of String Variation</a></p>
<p>13 0.10763855 <a title="13-lsi-13" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>14 0.10748547 <a title="13-lsi-14" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>15 0.10534422 <a title="13-lsi-15" href="./emnlp-2012-Word_Salad%3A_Relating_Food_Prices_and_Descriptions.html">139 emnlp-2012-Word Salad: Relating Food Prices and Descriptions</a></p>
<p>16 0.10463942 <a title="13-lsi-16" href="./emnlp-2012-Optimising_Incremental_Dialogue_Decisions_Using_Information_Density_for_Interactive_Systems.html">102 emnlp-2012-Optimising Incremental Dialogue Decisions Using Information Density for Interactive Systems</a></p>
<p>17 0.10458929 <a title="13-lsi-17" href="./emnlp-2012-Revisiting_the_Predictability_of_Language%3A_Response_Completion_in_Social_Media.html">114 emnlp-2012-Revisiting the Predictability of Language: Response Completion in Social Media</a></p>
<p>18 0.1033888 <a title="13-lsi-18" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>19 0.10139294 <a title="13-lsi-19" href="./emnlp-2012-Regularized_Interlingual_Projections%3A_Evaluation_on_Multilingual_Transliteration.html">111 emnlp-2012-Regularized Interlingual Projections: Evaluation on Multilingual Transliteration</a></p>
<p>20 0.1009979 <a title="13-lsi-20" href="./emnlp-2012-Universal_Grapheme-to-Phoneme_Prediction_Over_Latin_Alphabets.html">132 emnlp-2012-Universal Grapheme-to-Phoneme Prediction Over Latin Alphabets</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.024), (16, 0.023), (25, 0.012), (34, 0.084), (39, 0.016), (60, 0.096), (63, 0.039), (65, 0.021), (70, 0.028), (74, 0.042), (76, 0.026), (80, 0.014), (81, 0.014), (88, 0.37), (95, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.7174933 <a title="13-lda-1" href="./emnlp-2012-Characterizing_Stylistic_Elements_in_Syntactic_Structure.html">27 emnlp-2012-Characterizing Stylistic Elements in Syntactic Structure</a></p>
<p>Author: Song Feng ; Ritwik Banerjee ; Yejin Choi</p><p>Abstract: Much of the writing styles recognized in rhetorical and composition theories involve deep syntactic elements. However, most previous research for computational stylometric analysis has relied on shallow lexico-syntactic patterns. Some very recent work has shown that PCFG models can detect distributional difference in syntactic styles, but without offering much insights into exactly what constitute salient stylistic elements in sentence structure characterizing each authorship. In this paper, we present a comprehensive exploration of syntactic elements in writing styles, with particular emphasis on interpretable characterization of stylistic elements. We present analytic insights with respect to the authorship attribution task in two different domains. ,</p><p>same-paper 2 0.65711391 <a title="13-lda-2" href="./emnlp-2012-A_Unified_Approach_to_Transliteration-based_Text_Input_with_Online_Spelling_Correction.html">13 emnlp-2012-A Unified Approach to Transliteration-based Text Input with Online Spelling Correction</a></p>
<p>Author: Hisami Suzuki ; Jianfeng Gao</p><p>Abstract: This paper presents an integrated, end-to-end approach to online spelling correction for text input. Online spelling correction refers to the spelling correction as you type, as opposed to post-editing. The online scenario is particularly important for languages that routinely use transliteration-based text input methods, such as Chinese and Japanese, because the desired target characters cannot be input at all unless they are in the list of candidates provided by an input method, and spelling errors prevent them from appearing in the list. For example, a user might type suesheng by mistake to mean xuesheng 学生 'student' in Chinese; existing input methods fail to convert this misspelled input to the desired target Chinese characters. In this paper, we propose a unified approach to the problem of spelling correction and transliteration-based character conversion using an approach inspired by the phrasebased statistical machine translation framework. At the phrase (substring) level, k most probable pinyin (Romanized Chinese) corrections are generated using a monotone decoder; at the sentence level, input pinyin strings are directly transliterated into target Chinese characters by a decoder using a loglinear model that refer to the features of both levels. A new method of automatically deriving parallel training data from user keystroke logs is also presented. Experiments on Chinese pinyin conversion show that our integrated method reduces the character error rate by 20% (from 8.9% to 7. 12%) over the previous state-of-the art based on a noisy channel model. 609 1</p><p>3 0.36584795 <a title="13-lda-3" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>Author: Xian Qian ; Yang Liu</p><p>Abstract: In this paper, we propose a novel decoding algorithm for discriminative joint Chinese word segmentation, part-of-speech (POS) tagging, and parsing. Previous work often used a pipeline method Chinese word segmentation followed by POS tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components. In our approach, we train the three individual models separately during training, and incorporate them together in a unified framework during decoding. We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features. As far as we know, this is the first work on joint Chinese word segmentation, POS tagging and parsing. Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system. –</p><p>4 0.36390454 <a title="13-lda-4" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>Author: Greg Durrett ; Adam Pauls ; Dan Klein</p><p>Abstract: We consider the problem of using a bilingual dictionary to transfer lexico-syntactic information from a resource-rich source language to a resource-poor target language. In contrast to past work that used bitexts to transfer analyses of specific sentences at the token level, we instead use features to transfer the behavior of words at a type level. In a discriminative dependency parsing framework, our approach produces gains across a range of target languages, using two different lowresource training methodologies (one weakly supervised and one indirectly supervised) and two different dictionary sources (one manually constructed and one automatically constructed).</p><p>5 0.36370307 <a title="13-lda-5" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>Author: Taylor Berg-Kirkpatrick ; David Burkett ; Dan Klein</p><p>Abstract: We investigate two aspects of the empirical behavior of paired significance tests for NLP systems. First, when one system appears to outperform another, how does significance level relate in practice to the magnitude of the gain, to the size of the test set, to the similarity of the systems, and so on? Is it true that for each task there is a gain which roughly implies significance? We explore these issues across a range of NLP tasks using both large collections of past systems’ outputs and variants of single systems. Next, once significance levels are computed, how well does the standard i.i.d. notion of significance hold up in practical settings where future distributions are neither independent nor identically distributed, such as across domains? We explore this question using a range of test set variations for constituency parsing.</p><p>6 0.36306906 <a title="13-lda-6" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>7 0.36270931 <a title="13-lda-7" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<p>8 0.36236417 <a title="13-lda-8" href="./emnlp-2012-Forced_Derivation_Tree_based_Model_Training_to_Statistical_Machine_Translation.html">54 emnlp-2012-Forced Derivation Tree based Model Training to Statistical Machine Translation</a></p>
<p>9 0.36005563 <a title="13-lda-9" href="./emnlp-2012-A_Discriminative_Model_for_Query_Spelling_Correction_with_Latent_Structural_SVM.html">5 emnlp-2012-A Discriminative Model for Query Spelling Correction with Latent Structural SVM</a></p>
<p>10 0.35992485 <a title="13-lda-10" href="./emnlp-2012-Exploiting_Chunk-level_Features_to_Improve_Phrase_Chunking.html">45 emnlp-2012-Exploiting Chunk-level Features to Improve Phrase Chunking</a></p>
<p>11 0.35992184 <a title="13-lda-11" href="./emnlp-2012-Enlarging_Paraphrase_Collections_through_Generalization_and_Instantiation.html">39 emnlp-2012-Enlarging Paraphrase Collections through Generalization and Instantiation</a></p>
<p>12 0.35953718 <a title="13-lda-12" href="./emnlp-2012-Probabilistic_Finite_State_Machines_for_Regression-based_MT_Evaluation.html">108 emnlp-2012-Probabilistic Finite State Machines for Regression-based MT Evaluation</a></p>
<p>13 0.35851148 <a title="13-lda-13" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>14 0.35814008 <a title="13-lda-14" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>15 0.35774234 <a title="13-lda-15" href="./emnlp-2012-Multi-Domain_Learning%3A_When_Do_Domains_Matter%3F.html">92 emnlp-2012-Multi-Domain Learning: When Do Domains Matter?</a></p>
<p>16 0.35693827 <a title="13-lda-16" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>17 0.35673934 <a title="13-lda-17" href="./emnlp-2012-Regularized_Interlingual_Projections%3A_Evaluation_on_Multilingual_Transliteration.html">111 emnlp-2012-Regularized Interlingual Projections: Evaluation on Multilingual Transliteration</a></p>
<p>18 0.3561348 <a title="13-lda-18" href="./emnlp-2012-Extending_Machine_Translation_Evaluation_Metrics_with_Lexical_Cohesion_to_Document_Level.html">50 emnlp-2012-Extending Machine Translation Evaluation Metrics with Lexical Cohesion to Document Level</a></p>
<p>19 0.35558248 <a title="13-lda-19" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>20 0.35551441 <a title="13-lda-20" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
