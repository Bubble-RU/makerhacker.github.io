<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>139 emnlp-2012-Word Salad: Relating Food Prices and Descriptions</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-139" href="#">emnlp2012-139</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>139 emnlp-2012-Word Salad: Relating Food Prices and Descriptions</h1>
<br/><p>Source: <a title="emnlp-2012-139-pdf" href="http://aclweb.org/anthology//D/D12/D12-1124.pdf">pdf</a></p><p>Author: Victor Chahuneau ; Kevin Gimpel ; Bryan R. Routledge ; Lily Scherlis ; Noah A. Smith</p><p>Abstract: We investigate the use of language in food writing, specifically on restaurant menus and in customer reviews. Our approach is to build predictive models of concrete external variables, such as restaurant menu prices. We make use of a dataset of menus and customer reviews for thousands of restaurants in several U.S. cities. By focusing on prediction tasks and doing our analysis at scale, our methodology allows quantitative, objective measurements of the words and phrases used to de- scribe food in restaurants. We also explore interactions in language use between menu prices and sentiment as expressed in user reviews.</p><p>Reference: <a title="emnlp-2012-139-reference" href="../emnlp2012_reference/emnlp-2012-Word_Salad%3A_Relating_Food_Prices_and_Descriptions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com s Abstract  We investigate the use of language in food writing, specifically on restaurant menus and in customer reviews. [sent-5, score-0.576]
</p><p>2 Our approach is to build predictive models of concrete external variables, such as restaurant menu prices. [sent-6, score-0.768]
</p><p>3 We make use of a dataset of menus and customer reviews for thousands of restaurants in several U. [sent-7, score-0.52]
</p><p>4 We also explore interactions in language use between menu prices and sentiment as expressed in user reviews. [sent-11, score-0.88]
</p><p>5 1 Introduction What words might a menu writer use to justify the high price of a steak? [sent-12, score-1.061]
</p><p>6 In this paper, we explore questions like these that relate restaurant menus, prices, and customer sentiment. [sent-16, score-0.258]
</p><p>7 Our goal is to understand how language is used in the food domain, and we direct our investigation using external variables such as restaurant menu prices. [sent-17, score-0.931]
</p><p>8 We echo this pattern here as we turn our attention to language use on restaurant menus and in user restaurant reviews. [sent-28, score-0.597]
</p><p>9 We use data from a large corpus of restaurant menus and reviews crawled from the web and formulate several prediction tasks. [sent-29, score-0.641]
</p><p>10 In addition to predicting menu prices, we also consider predicting sentiment along with price. [sent-30, score-0.794]
</p><p>11 Much of this research has focused on customer-written reviews of goods and services, and perspectives have been gained on how sentiment is expressed in this type of informal text. [sent-32, score-0.413]
</p><p>12 In addition to sentiment, however, other variables are reflected in a reviewer’s choice of words, such as the price of the item under consideration. [sent-33, score-0.681]
</p><p>13 In this paper, we take a step toward joint modeling of multiple variables in review text, exploring connections between price and sentiment in restaurant reviews. [sent-34, score-0.894]
</p><p>14 Freedman and Jurafsky (201 1) studied the use of language in food advertising, specifically the words on potato chip bags. [sent-41, score-0.233]
</p><p>15 They argued that, due to the ubiquity of food writing across cultures, ethnic groups, and social classes, studying the use of language for describing food can provide perspective on how different socioeconomic groups selfidentify using language and how they are linguistically targeted. [sent-42, score-0.326]
</p><p>16 In particular, they showed that price affects how “authenticity” is realized in marketing language, a point we return to in §5. [sent-43, score-0.496]
</p><p>17 This is an example of how price can affect how an underlying variable is expressed in language. [sent-44, score-0.496]
</p><p>18 Among other explorations in this paper, we consider how price interacts with expression of sentiment in user reviews of restaurants. [sent-45, score-0.896]
</p><p>19 (201 1) had a 1358 similar goal but decomposed user reviews into parts describing particular aspects of the product being reviewed (Hu and Liu, 2004). [sent-58, score-0.306]
</p><p>20 Our paper differs from price modeling based on product reviews in several ways. [sent-59, score-0.766]
</p><p>21 We consider a large set of weaklyrelated products instead of a homogeneous selection of a few products, and the reviews in our dataset are not product-centered but rather describe the overall experience of visiting a restaurant. [sent-60, score-0.268]
</p><p>22 Consequently, menu items are not always mentioned in reviews and rarely appear with their exact names. [sent-61, score-0.86]
</p><p>23 This makes it  difficult to directly use review features in a pricing model for individual menu items. [sent-62, score-0.723]
</p><p>24 , 1994), often including recommendations for writing menu item descriptions (Miller and Pavesic, 1996; McVety et al. [sent-64, score-0.775]
</p><p>25 Their guidelines frequently include example menus from successful restaurants, but typically do not use large corpora of menus or automated analysis, as we do here. [sent-66, score-0.31]
</p><p>26 (2001 ; 2005) showed that the way that menu items are described affects customers’ perceptions and purchasing behavior. [sent-69, score-0.645]
</p><p>27 When menu items are described evocatively, customers choose them more often and report higher satisfaction with quality and value, as compared to when they are given the same items described with conventional names. [sent-70, score-0.696]
</p><p>28 While our goals are related, our experimental approach is different, as we use automated analysis of thousands of restaurant menus and rely on a set of one million reviews as a surrogate for observing customer behavior. [sent-73, score-0.656]
</p><p>29 For example, there are over 50,000 menu items in New York that include  Table 1: Dataset statistics. [sent-76, score-0.617]
</p><p>30 Each menu includes a list of item names with optional text descriptions and prices. [sent-115, score-0.799]
</p><p>31 com) with metadata and user reviews for the restaurant, which we also collected. [sent-118, score-0.387]
</p><p>32 Also, the category of food and a price range ($ to $$$$, indicating the price of a typical meal at the restaurant) are indicated. [sent-122, score-1.225]
</p><p>33 The distribution of prices of individual menu items is highly skewed, with a mean of $9. [sent-124, score-0.803]
</p><p>34 On average, a restaurant has 73 items on its menu with a median price of $8. [sent-127, score-1.394]
</p><p>35 69 and 119 Yelp reviews with a median rating of 3. [sent-128, score-0.374]
</p><p>36 55 1359  Figure 1: F3krequency distributions of restaurant price ranges (left) and review ratings (right). [sent-129, score-0.773]
</p><p>37 The 1stkar rating and price range distributions are shown in Figure 1. [sent-131, score-0.591]
</p><p>38 These include predicting individual menu item prices (§5), predicting the price range for each restaurant (§6), and finally jointly predicting median price and sentiment for each restaurant (§7). [sent-135, score-2.709]
</p><p>39 5  Menu Item Price Prediction  We first consider the problem of predicting the price of each item on a menu. [sent-151, score-0.735]
</p><p>40 In this case, every instance xi corresponds to a single item in the menu parametrized by the features detailed below and yi is the item’s price. [sent-152, score-0.773]
</p><p>41 The first two predict the mean or the median of the prices in the training set for a given item name, and use the overall price mean or median when a name is missing in the training set. [sent-155, score-1.051]
</p><p>42 The third baseline is an ‘1-regularized linear regression model trained with a single binary feature for each item name in the training data. [sent-156, score-0.243]
</p><p>43 We note that there is a wide variation of menu item names in the dataset, with more than 400,000 distinct names. [sent-158, score-0.774]
</p><p>44 Although we address this issue later by introducing local text features, we also performed simple normalization of the item names for all of the baselines described above. [sent-159, score-0.209]
</p><p>45 To do this normalization, we first compiled a stop word list based on the most frequent words in the item names. [sent-160, score-0.207]
</p><p>46 2 We 1The price distribution is more symmetric in the log domain. [sent-161, score-0.496]
</p><p>47 removed stop words and then ordered the words in  each item name lexicographically, in order to collapse together items such as coffee black and black coffee. [sent-163, score-0.259]
</p><p>48 We now introduce several sets of features that we add to the normalized item names:3 I. [sent-167, score-0.208]
</p><p>49 METADATA: Binary features for each restaurant metadata field mentioned above, excluding price range. [sent-168, score-0.83]
</p><p>50 MENUDESC: n-grams in menu item descriptions, as in MENUNAMES. [sent-177, score-0.75]
</p><p>51 Review Features In addition to these features, we consider leveraging the large amount of text present in user reviews to improve predictions. [sent-178, score-0.279]
</p><p>52 We at-  tempted to find mentions of menu items in the reviews and to include features extracted from the surrounding words in the model. [sent-179, score-0.913]
</p><p>53 Perfect item mentions being relatively rare, we consider inexact matches weighted by a coefficient measuring the degree of resemblance: we used the Dice similarity between the set ofwords in the sentence and in the item name. [sent-180, score-0.4]
</p><p>54 We then extracted n-gram features from this sentence, and tried several ways to use them for price prediction. [sent-181, score-0.519]
</p><p>55 Given a review sentence, one option is to add the corresponding features to every item matching this sentence, with a value equal to the similarity coefficient. [sent-182, score-0.282]
</p><p>56 Another option is to select the best matching item and use the same real-valued features but only for this single item. [sent-183, score-0.208]
</p><p>57 All of 3The normalized item names are present as binary features in all of our regression models 1360  Table 2: Results for menu item price prediction. [sent-186, score-1.536]
</p><p>58 We also tried to incorporate the reviews by using them in aggregate via predictions from a separate model; we found this approach to work better than the methods described above which all use features  from the reviews directly in the regression model. [sent-189, score-0.567]
</p><p>59 In particular, we use the review features in a separate model that we will describe below (§6) to predict the price range of each restaurant. [sent-190, score-0.635]
</p><p>60 We use the estimated price range (which we denote PcR) as a single additional real-valued feature for indivcidual item price prediction. [sent-192, score-1.219]
</p><p>61 Using menu name features (MENUNAMES) brings the bulk of the improvement, though menu description features (MENUDESC) and the remaining features also lead to small gains. [sent-196, score-1.199]
</p><p>62 While METADATA features improve over the baselines when used alone, they do not lead to improved performance over the MENU* + PcR features, suggesting that the text features may be cable to sub-  1361 stitute for the information in the metadata, at least for prediction of individual item prices. [sent-198, score-0.271]
</p><p>63 We suspect that these features could be more effective with a better method of linking menu items to mentions in review text. [sent-202, score-0.744]
</p><p>64 By comparing the weights of related features, we can see the relative differences in terms of contribution to menu item prices. [sent-205, score-0.779]
</p><p>65 This is related to observations made by Freedman and Jurafsky (201 1) that cheaper food is marketed by appealing to tradition and historicity, with more expensive food described in terms of naturalness, quality of ingredients, and the preparation process (e. [sent-209, score-0.381]
</p><p>66 Pane (d) shows feature weights for trigrams containing units of chicken; we can see an ordering in terms of size (bits < cubes < strips < cuts) as well as the price increase associated with the use of the word morsels in place of less refined units. [sent-214, score-0.553]
</p><p>67 We also see a difference between pieces and pcs, with the  latter being frequently used to refer to entire cuts of  price prediction. [sent-215, score-0.518]
</p><p>68 Panes (f), (g), and (h) reveal price differences due to slight variations in word form. [sent-221, score-0.496]
</p><p>69 We also find that food items prefixed with roast lead to more expensive prices than the similar roasted, except in the case ofpork, though here the different forms may be evoking two different preparation styles. [sent-223, score-0.402]
</p><p>70 6  Restaurant Price Range Prediction  In addition to predicting the prices of individual menu items, we also considered the task of predicting the price range listed for each restaurant on its Yelp page. [sent-226, score-1.572]
</p><p>71 The values for this field are integers from 1 to 4 and indicate the price of a typical meal from the restaurant. [sent-227, score-0.524]
</p><p>72 For this task, we again train an ‘1-regularized linear regression model with integral price ranges as the true output values yi. [sent-228, score-0.554]
</p><p>73 In addition to the feature sets used for individual menu item price prediction, we used features on reviews (REVIEWS). [sent-236, score-1.512]
</p><p>74 Specifically, we used binary features for unigrams, bigrams, and trigrams in the full set of reviews for each restaurant. [sent-237, score-0.294]
</p><p>75 1 Results Our results for price range prediction are shown in Table 4. [sent-252, score-0.578]
</p><p>76 Predicting the most frequent price range gave us an accuracy of 48. [sent-253, score-0.538]
</p><p>77 Performance improvements were obtained by separately adding menu (MENU*), metadata (METADATA), and review features (REVIEWS). [sent-255, score-0.77]
</p><p>78 Unlike individual item price prediction, the reviews were more helpful than the menu features for predicting overall price range. [sent-256, score-2.062]
</p><p>79 This is not surprising, since reviewers will often generally discuss price in their reviews. [sent-257, score-0.496]
</p><p>80 To do this, we trained a logistic regression model predicting polarity for each review; we used the REVIEWS feature set, but this time considering each review as a single training instance. [sent-260, score-0.223]
</p><p>81 The polarity of a review was determined by whether or not its star rating was greater than the average rating across all reviews in the dataset (3. [sent-261, score-0.531]
</p><p>82 We omit full details of these models because the polarity prediction task for user reviews is wellknown in the sentiment analysis community and our model is not an innovation over prior work (Pang and Lee, 2008). [sent-264, score-0.477]
</p><p>83 2 Interpreting Reviews Given learned models for predicting a restaurant’s price range from its set of reviews as well as polarity for each review, we can turn the process around and use the feature weights to analyze the review text. [sent-267, score-0.975]
</p><p>84 Restricting our attention to reviews of 50–60 words, Table 5 shows sample reviews from our test 1363  set that lead to various predictions of price range and sentiment. [sent-268, score-1.024]
</p><p>85 5 This technique can also be useful when trying to determine the “true” star rating for a review (if provided star ratings are noisy), or to show the most positive and most negative reviews for a product within a particular star rating. [sent-269, score-0.61]
</p><p>86 We can also do a more fine-grained analysis of review text by noting the contribution to the price range prediction of each position in the text stream. [sent-271, score-0.652]
</p><p>87 In Figure 2, we show the influence of each word in a review sentence on the predicted polarity (brown) and price range (yellow). [sent-273, score-0.649]
</p><p>88 The second sentence is a difficult example for sentiment analysis, since there are several positive words and phrases early but the sentiment is chiefly expressed in the final clause. [sent-276, score-0.242]
</p><p>89 In both examples, the yellow bars show that price estimates are reflected mainly through isolated mentions of offerings and amenities (drinks, atmosphere, security, good service). [sent-278, score-0.552]
</p><p>90 sushi tatsatsetde dgood , food was fresh butb tnothing left me yeayrenairnngi tgo roet ruertnu  great dark , sexy atmosphere . [sent-288, score-0.297]
</p><p>91 sushi tasted good , food was fresh but nothing left me yearning to return Figure  2: Local  (position-level)  sentiment (brown) and price (yellow) estimates for two sentences in the test corpus. [sent-291, score-0.844]
</p><p>92 6  ment and price are independent of each We try to capture this interaction by modeling at the same time review polarity and item price: we consider the task of jointly predicting aggregate sentiment and price for a restaurant. [sent-293, score-1.463]
</p><p>93 For every restaurant in our dataset, we compute its median item price ¯ p and its median star rating r¯. [sent-294, score-1.164]
</p><p>94 We notice that the spread of the sentiment values is larger, which suggests that reviews give stronger clues about consumer experience than about the cost of a typical meal. [sent-314, score-0.408]
</p><p>95 expensive) appear in this limited selection, as well as certain phrases indicating both sentiment and price (overpriced vs. [sent-316, score-0.617]
</p><p>96 Other examples of note: gem is used in strongly-positive reviews of cheap restaurants; for expensive restaurants, reviewers use highly recommended or amazing. [sent-318, score-0.272]
</p><p>97 Also, phrases like no flavor and manager appear in negative reviews of more expensive restaurants, while dirty appears more often in negative reviews of cheaper restaurants. [sent-319, score-0.541]
</p><p>98 8  Conclusion  We have explored linguistic relationships between food prices and customer sentiment through quantitative analysis of a large corpus of menus and reviews. [sent-320, score-0.676]
</p><p>99 Movie reviews and revenues: An experiment in text regression. [sent-406, score-0.243]
</p><p>100 How descriptive food names bias sensory perceptions in restaurants. [sent-501, score-0.215]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('menu', 0.565), ('price', 0.496), ('reviews', 0.243), ('restaurant', 0.203), ('item', 0.185), ('food', 0.163), ('prices', 0.158), ('menus', 0.155), ('sentiment', 0.121), ('metadata', 0.108), ('median', 0.078), ('review', 0.074), ('mae', 0.073), ('star', 0.071), ('ghose', 0.071), ('hospitality', 0.071), ('restaurants', 0.067), ('pricing', 0.061), ('mre', 0.061), ('regression', 0.058), ('menudesc', 0.056), ('pane', 0.056), ('pcr', 0.056), ('wansink', 0.056), ('yelp', 0.056), ('customer', 0.055), ('predicting', 0.054), ('rating', 0.053), ('items', 0.052), ('goods', 0.049), ('consumer', 0.044), ('reviewer', 0.044), ('atmosphere', 0.042), ('freedman', 0.042), ('kogan', 0.042), ('nnz', 0.042), ('potato', 0.042), ('sales', 0.042), ('zwicky', 0.042), ('range', 0.042), ('prediction', 0.04), ('polarity', 0.037), ('quarterly', 0.036), ('economics', 0.036), ('fresh', 0.036), ('routledge', 0.036), ('user', 0.036), ('gimpel', 0.033), ('eisenstein', 0.03), ('mentions', 0.03), ('expensive', 0.029), ('weights', 0.029), ('stars', 0.029), ('administrative', 0.028), ('allmenus', 0.028), ('ambience', 0.028), ('archak', 0.028), ('authenticity', 0.028), ('baye', 0.028), ('chicken', 0.028), ('chip', 0.028), ('crispy', 0.028), ('dice', 0.028), ('drinks', 0.028), ('ipeirotis', 0.028), ('kasavana', 0.028), ('mashed', 0.028), ('mcvety', 0.028), ('meal', 0.028), ('menunames', 0.028), ('nonstandard', 0.028), ('perceptions', 0.028), ('relatedly', 0.028), ('revenues', 0.028), ('sexy', 0.028), ('sushi', 0.028), ('mean', 0.028), ('trigrams', 0.028), ('customers', 0.027), ('nf', 0.027), ('product', 0.027), ('cheaper', 0.026), ('yellow', 0.026), ('descriptions', 0.025), ('products', 0.025), ('tasty', 0.024), ('lerman', 0.024), ('crisp', 0.024), ('market', 0.024), ('spellings', 0.024), ('names', 0.024), ('quantitative', 0.024), ('joshi', 0.024), ('opinion', 0.023), ('features', 0.023), ('carnegie', 0.023), ('cmu', 0.023), ('pittsburgh', 0.023), ('stop', 0.022), ('cuts', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="139-tfidf-1" href="./emnlp-2012-Word_Salad%3A_Relating_Food_Prices_and_Descriptions.html">139 emnlp-2012-Word Salad: Relating Food Prices and Descriptions</a></p>
<p>Author: Victor Chahuneau ; Kevin Gimpel ; Bryan R. Routledge ; Lily Scherlis ; Noah A. Smith</p><p>Abstract: We investigate the use of language in food writing, specifically on restaurant menus and in customer reviews. Our approach is to build predictive models of concrete external variables, such as restaurant menu prices. We make use of a dataset of menus and customer reviews for thousands of restaurants in several U.S. cities. By focusing on prediction tasks and doing our analysis at scale, our methodology allows quantitative, objective measurements of the words and phrases used to de- scribe food in restaurants. We also explore interactions in language use between menu prices and sentiment as expressed in user reviews.</p><p>2 0.13665624 <a title="139-tfidf-2" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>Author: Jianxing Yu ; Zheng-Jun Zha ; Tat-Seng Chua</p><p>Abstract: This paper proposes to generate appropriate answers for opinion questions about products by exploiting the hierarchical organization of consumer reviews. The hierarchy organizes product aspects as nodes following their parent-child relations. For each aspect, the reviews and corresponding opinions on this aspect are stored. We develop a new framework for opinion Questions Answering, which enables accurate question analysis and effective answer generation by making use the hierarchy. In particular, we first identify the (explicit/implicit) product aspects asked in the questions and their sub-aspects by referring to the hierarchy. We then retrieve the corresponding review fragments relevant to the aspects from the hierarchy. In order to gener- ate appropriate answers from the review fragments, we develop a multi-criteria optimization approach for answer generation by simultaneously taking into account review salience, coherence, diversity, and parent-child relations among the aspects. We conduct evaluations on 11 popular products in four domains. The evaluated corpus contains 70,359 consumer reviews and 220 questions on these products. Experimental results demonstrate the effectiveness of our approach.</p><p>3 0.1008168 <a title="139-tfidf-3" href="./emnlp-2012-Optimising_Incremental_Dialogue_Decisions_Using_Information_Density_for_Interactive_Systems.html">102 emnlp-2012-Optimising Incremental Dialogue Decisions Using Information Density for Interactive Systems</a></p>
<p>Author: Nina Dethlefs ; Helen Hastie ; Verena Rieser ; Oliver Lemon</p><p>Abstract: Incremental processing allows system designers to address several discourse phenomena that have previously been somewhat neglected in interactive systems, such as backchannels or barge-ins, but that can enhance the responsiveness and naturalness of systems. Unfortunately, prior work has focused largely on deterministic incremental decision making, rendering system behaviour less flexible and adaptive than is desirable. We present a novel approach to incremental decision making that is based on Hierarchical Reinforcement Learning to achieve an interactive optimisation of Information Presentation (IP) strategies, allowing the system to generate and comprehend backchannels and barge-ins, by employing the recent psycholinguistic hypothesis of information density (ID) (Jaeger, 2010). Results in terms of average rewards and a human rating study show that our learnt strategy outperforms several baselines that are | v not sensitive to ID by more than 23%.</p><p>4 0.093409546 <a title="139-tfidf-4" href="./emnlp-2012-Do_Neighbours_Help%3F_An_Exploration_of_Graph-based_Algorithms_for_Cross-domain_Sentiment_Classification.html">34 emnlp-2012-Do Neighbours Help? An Exploration of Graph-based Algorithms for Cross-domain Sentiment Classification</a></p>
<p>Author: Natalia Ponomareva ; Mike Thelwall</p><p>Abstract: This paper presents a comparative study of graph-based approaches for cross-domain sentiment classification. In particular, the paper analyses two existing methods: an optimisation problem and a ranking algorithm. We compare these graph-based methods with each other and with the other state-ofthe-art approaches and conclude that graph domain representations offer a competitive solution to the domain adaptation problem. Analysis of the best parameters for graphbased algorithms reveals that there are no optimal values valid for all domain pairs and that these values are dependent on the characteristics of corresponding domains.</p><p>5 0.076409116 <a title="139-tfidf-5" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>Author: Lizhen Qu ; Rainer Gemulla ; Gerhard Weikum</p><p>Abstract: We propose the weakly supervised MultiExperts Model (MEM) for analyzing the semantic orientation of opinions expressed in natural language reviews. In contrast to most prior work, MEM predicts both opinion polarity and opinion strength at the level of individual sentences; such fine-grained analysis helps to understand better why users like or dislike the entity under review. A key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength. For this reason, MEM is weakly supervised: It starts with potentially noisy indicators obtained from coarse-grained training data (i.e., document-level ratings), a small set of diverse base predictors, and, if available, small amounts of fine-grained training data. We integrate these noisy indicators into a unified probabilistic framework using ideas from ensemble learning and graph-based semi-supervised learning. Our experiments indicate that MEM outperforms state-of-the-art methods by a significant margin.</p><p>6 0.073601231 <a title="139-tfidf-6" href="./emnlp-2012-Parse%2C_Price_and_Cut-Delayed_Column_and_Row_Generation_for_Graph_Based_Parsers.html">104 emnlp-2012-Parse, Price and Cut-Delayed Column and Row Generation for Graph Based Parsers</a></p>
<p>7 0.06650617 <a title="139-tfidf-7" href="./emnlp-2012-Opinion_Target_Extraction_Using_Word-Based_Translation_Model.html">101 emnlp-2012-Opinion Target Extraction Using Word-Based Translation Model</a></p>
<p>8 0.063793972 <a title="139-tfidf-8" href="./emnlp-2012-Semantic_Compositionality_through_Recursive_Matrix-Vector_Spaces.html">116 emnlp-2012-Semantic Compositionality through Recursive Matrix-Vector Spaces</a></p>
<p>9 0.062383484 <a title="139-tfidf-9" href="./emnlp-2012-Collocation_Polarity_Disambiguation_Using_Web-based_Pseudo_Contexts.html">28 emnlp-2012-Collocation Polarity Disambiguation Using Web-based Pseudo Contexts</a></p>
<p>10 0.058122389 <a title="139-tfidf-10" href="./emnlp-2012-Why_Question_Answering_using_Sentiment_Analysis_and_Word_Classes.html">137 emnlp-2012-Why Question Answering using Sentiment Analysis and Word Classes</a></p>
<p>11 0.049474113 <a title="139-tfidf-11" href="./emnlp-2012-Active_Learning_for_Imbalanced_Sentiment_Classification.html">15 emnlp-2012-Active Learning for Imbalanced Sentiment Classification</a></p>
<p>12 0.039230324 <a title="139-tfidf-12" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>13 0.034464471 <a title="139-tfidf-13" href="./emnlp-2012-Multi-Domain_Learning%3A_When_Do_Domains_Matter%3F.html">92 emnlp-2012-Multi-Domain Learning: When Do Domains Matter?</a></p>
<p>14 0.033452027 <a title="139-tfidf-14" href="./emnlp-2012-A_Phrase-Discovering_Topic_Model_Using_Hierarchical_Pitman-Yor_Processes.html">8 emnlp-2012-A Phrase-Discovering Topic Model Using Hierarchical Pitman-Yor Processes</a></p>
<p>15 0.030078365 <a title="139-tfidf-15" href="./emnlp-2012-An_%22AI_readability%22_Formula_for_French_as_a_Foreign_Language.html">17 emnlp-2012-An "AI readability" Formula for French as a Foreign Language</a></p>
<p>16 0.030001739 <a title="139-tfidf-16" href="./emnlp-2012-Streaming_Analysis_of_Discourse_Participants.html">120 emnlp-2012-Streaming Analysis of Discourse Participants</a></p>
<p>17 0.029375682 <a title="139-tfidf-17" href="./emnlp-2012-Generalized_Higher-Order_Dependency_Parsing_with_Cube_Pruning.html">57 emnlp-2012-Generalized Higher-Order Dependency Parsing with Cube Pruning</a></p>
<p>18 0.028961405 <a title="139-tfidf-18" href="./emnlp-2012-Supervised_Text-based_Geolocation_Using_Language_Models_on_an_Adaptive_Grid.html">121 emnlp-2012-Supervised Text-based Geolocation Using Language Models on an Adaptive Grid</a></p>
<p>19 0.027249575 <a title="139-tfidf-19" href="./emnlp-2012-Extracting_Opinion_Expressions_with_semi-Markov_Conditional_Random_Fields.html">51 emnlp-2012-Extracting Opinion Expressions with semi-Markov Conditional Random Fields</a></p>
<p>20 0.027116459 <a title="139-tfidf-20" href="./emnlp-2012-Sketch_Algorithms_for_Estimating_Point_Queries_in_NLP.html">117 emnlp-2012-Sketch Algorithms for Estimating Point Queries in NLP</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.118), (1, 0.069), (2, 0.03), (3, 0.177), (4, 0.074), (5, -0.068), (6, -0.071), (7, -0.025), (8, 0.065), (9, 0.05), (10, 0.07), (11, 0.019), (12, -0.06), (13, -0.086), (14, 0.021), (15, -0.0), (16, 0.076), (17, 0.034), (18, 0.008), (19, -0.008), (20, -0.048), (21, -0.055), (22, -0.029), (23, -0.048), (24, -0.069), (25, -0.018), (26, 0.061), (27, 0.011), (28, 0.02), (29, -0.055), (30, -0.161), (31, -0.229), (32, -0.186), (33, 0.013), (34, 0.018), (35, 0.204), (36, -0.091), (37, -0.133), (38, -0.081), (39, -0.158), (40, 0.22), (41, -0.043), (42, -0.079), (43, -0.025), (44, -0.009), (45, 0.065), (46, 0.011), (47, 0.29), (48, 0.056), (49, 0.192)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96384287 <a title="139-lsi-1" href="./emnlp-2012-Word_Salad%3A_Relating_Food_Prices_and_Descriptions.html">139 emnlp-2012-Word Salad: Relating Food Prices and Descriptions</a></p>
<p>Author: Victor Chahuneau ; Kevin Gimpel ; Bryan R. Routledge ; Lily Scherlis ; Noah A. Smith</p><p>Abstract: We investigate the use of language in food writing, specifically on restaurant menus and in customer reviews. Our approach is to build predictive models of concrete external variables, such as restaurant menu prices. We make use of a dataset of menus and customer reviews for thousands of restaurants in several U.S. cities. By focusing on prediction tasks and doing our analysis at scale, our methodology allows quantitative, objective measurements of the words and phrases used to de- scribe food in restaurants. We also explore interactions in language use between menu prices and sentiment as expressed in user reviews.</p><p>2 0.39149058 <a title="139-lsi-2" href="./emnlp-2012-Do_Neighbours_Help%3F_An_Exploration_of_Graph-based_Algorithms_for_Cross-domain_Sentiment_Classification.html">34 emnlp-2012-Do Neighbours Help? An Exploration of Graph-based Algorithms for Cross-domain Sentiment Classification</a></p>
<p>Author: Natalia Ponomareva ; Mike Thelwall</p><p>Abstract: This paper presents a comparative study of graph-based approaches for cross-domain sentiment classification. In particular, the paper analyses two existing methods: an optimisation problem and a ranking algorithm. We compare these graph-based methods with each other and with the other state-ofthe-art approaches and conclude that graph domain representations offer a competitive solution to the domain adaptation problem. Analysis of the best parameters for graphbased algorithms reveals that there are no optimal values valid for all domain pairs and that these values are dependent on the characteristics of corresponding domains.</p><p>3 0.35142887 <a title="139-lsi-3" href="./emnlp-2012-Supervised_Text-based_Geolocation_Using_Language_Models_on_an_Adaptive_Grid.html">121 emnlp-2012-Supervised Text-based Geolocation Using Language Models on an Adaptive Grid</a></p>
<p>Author: Stephen Roller ; Michael Speriosu ; Sarat Rallapalli ; Benjamin Wing ; Jason Baldridge</p><p>Abstract: The geographical properties of words have recently begun to be exploited for geolocating documents based solely on their text, often in the context of social media and online content. One common approach for geolocating texts is rooted in information retrieval. Given training documents labeled with latitude/longitude coordinates, a grid is overlaid on the Earth and pseudo-documents constructed by concatenating the documents within a given grid cell; then a location for a test document is chosen based on the most similar pseudo-document. Uniform grids are normally used, but they are sensitive to the dispersion of documents over the earth. We define an alternative grid construction using k-d trees that more robustly adapts to data, especially with larger training sets. We also provide a better way of choosing the locations for pseudo-documents. We evaluate these strategies on existing Wikipedia and Twitter corpora, as well as a new, larger Twitter corpus. The adaptive grid achieves competitive results with a uniform grid on small training sets and outperforms it on the large Twitter corpus. The two grid constructions can also be combined to produce consistently strong results across all training sets.</p><p>4 0.3452501 <a title="139-lsi-4" href="./emnlp-2012-Optimising_Incremental_Dialogue_Decisions_Using_Information_Density_for_Interactive_Systems.html">102 emnlp-2012-Optimising Incremental Dialogue Decisions Using Information Density for Interactive Systems</a></p>
<p>Author: Nina Dethlefs ; Helen Hastie ; Verena Rieser ; Oliver Lemon</p><p>Abstract: Incremental processing allows system designers to address several discourse phenomena that have previously been somewhat neglected in interactive systems, such as backchannels or barge-ins, but that can enhance the responsiveness and naturalness of systems. Unfortunately, prior work has focused largely on deterministic incremental decision making, rendering system behaviour less flexible and adaptive than is desirable. We present a novel approach to incremental decision making that is based on Hierarchical Reinforcement Learning to achieve an interactive optimisation of Information Presentation (IP) strategies, allowing the system to generate and comprehend backchannels and barge-ins, by employing the recent psycholinguistic hypothesis of information density (ID) (Jaeger, 2010). Results in terms of average rewards and a human rating study show that our learnt strategy outperforms several baselines that are | v not sensitive to ID by more than 23%.</p><p>5 0.3172684 <a title="139-lsi-5" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>Author: Jianxing Yu ; Zheng-Jun Zha ; Tat-Seng Chua</p><p>Abstract: This paper proposes to generate appropriate answers for opinion questions about products by exploiting the hierarchical organization of consumer reviews. The hierarchy organizes product aspects as nodes following their parent-child relations. For each aspect, the reviews and corresponding opinions on this aspect are stored. We develop a new framework for opinion Questions Answering, which enables accurate question analysis and effective answer generation by making use the hierarchy. In particular, we first identify the (explicit/implicit) product aspects asked in the questions and their sub-aspects by referring to the hierarchy. We then retrieve the corresponding review fragments relevant to the aspects from the hierarchy. In order to gener- ate appropriate answers from the review fragments, we develop a multi-criteria optimization approach for answer generation by simultaneously taking into account review salience, coherence, diversity, and parent-child relations among the aspects. We conduct evaluations on 11 popular products in four domains. The evaluated corpus contains 70,359 consumer reviews and 220 questions on these products. Experimental results demonstrate the effectiveness of our approach.</p><p>6 0.30666003 <a title="139-lsi-6" href="./emnlp-2012-Parse%2C_Price_and_Cut-Delayed_Column_and_Row_Generation_for_Graph_Based_Parsers.html">104 emnlp-2012-Parse, Price and Cut-Delayed Column and Row Generation for Graph Based Parsers</a></p>
<p>7 0.23654424 <a title="139-lsi-7" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>8 0.2295994 <a title="139-lsi-8" href="./emnlp-2012-Generative_Goal-Driven_User_Simulation_for_Dialog_Management.html">60 emnlp-2012-Generative Goal-Driven User Simulation for Dialog Management</a></p>
<p>9 0.20562786 <a title="139-lsi-9" href="./emnlp-2012-Active_Learning_for_Imbalanced_Sentiment_Classification.html">15 emnlp-2012-Active Learning for Imbalanced Sentiment Classification</a></p>
<p>10 0.18968895 <a title="139-lsi-10" href="./emnlp-2012-Opinion_Target_Extraction_Using_Word-Based_Translation_Model.html">101 emnlp-2012-Opinion Target Extraction Using Word-Based Translation Model</a></p>
<p>11 0.18014599 <a title="139-lsi-11" href="./emnlp-2012-Semantic_Compositionality_through_Recursive_Matrix-Vector_Spaces.html">116 emnlp-2012-Semantic Compositionality through Recursive Matrix-Vector Spaces</a></p>
<p>12 0.17622827 <a title="139-lsi-12" href="./emnlp-2012-Why_Question_Answering_using_Sentiment_Analysis_and_Word_Classes.html">137 emnlp-2012-Why Question Answering using Sentiment Analysis and Word Classes</a></p>
<p>13 0.17093284 <a title="139-lsi-13" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>14 0.16236393 <a title="139-lsi-14" href="./emnlp-2012-Discovering_Diverse_and_Salient_Threads_in_Document_Collections.html">33 emnlp-2012-Discovering Diverse and Salient Threads in Document Collections</a></p>
<p>15 0.1551892 <a title="139-lsi-15" href="./emnlp-2012-An_%22AI_readability%22_Formula_for_French_as_a_Foreign_Language.html">17 emnlp-2012-An "AI readability" Formula for French as a Foreign Language</a></p>
<p>16 0.14898257 <a title="139-lsi-16" href="./emnlp-2012-Language_Model_Rest_Costs_and_Space-Efficient_Storage.html">74 emnlp-2012-Language Model Rest Costs and Space-Efficient Storage</a></p>
<p>17 0.14846289 <a title="139-lsi-17" href="./emnlp-2012-Exploring_Adaptor_Grammars_for_Native_Language_Identification.html">48 emnlp-2012-Exploring Adaptor Grammars for Native Language Identification</a></p>
<p>18 0.14224248 <a title="139-lsi-18" href="./emnlp-2012-Collocation_Polarity_Disambiguation_Using_Web-based_Pseudo_Contexts.html">28 emnlp-2012-Collocation Polarity Disambiguation Using Web-based Pseudo Contexts</a></p>
<p>19 0.14092791 <a title="139-lsi-19" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>20 0.13954483 <a title="139-lsi-20" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.017), (14, 0.011), (16, 0.029), (25, 0.014), (34, 0.049), (60, 0.065), (63, 0.058), (64, 0.018), (65, 0.031), (73, 0.013), (74, 0.042), (76, 0.064), (80, 0.02), (86, 0.043), (89, 0.39), (95, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75337541 <a title="139-lda-1" href="./emnlp-2012-Word_Salad%3A_Relating_Food_Prices_and_Descriptions.html">139 emnlp-2012-Word Salad: Relating Food Prices and Descriptions</a></p>
<p>Author: Victor Chahuneau ; Kevin Gimpel ; Bryan R. Routledge ; Lily Scherlis ; Noah A. Smith</p><p>Abstract: We investigate the use of language in food writing, specifically on restaurant menus and in customer reviews. Our approach is to build predictive models of concrete external variables, such as restaurant menu prices. We make use of a dataset of menus and customer reviews for thousands of restaurants in several U.S. cities. By focusing on prediction tasks and doing our analysis at scale, our methodology allows quantitative, objective measurements of the words and phrases used to de- scribe food in restaurants. We also explore interactions in language use between menu prices and sentiment as expressed in user reviews.</p><p>2 0.61997712 <a title="139-lda-2" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: We present a new family of models for unsupervised parsing, Dependency and Boundary models, that use cues at constituent boundaries to inform head-outward dependency tree generation. We build on three intuitions that are explicit in phrase-structure grammars but only implicit in standard dependency formulations: (i) Distributions of words that occur at sentence boundaries such as English determiners resemble constituent edges. (ii) Punctuation at sentence boundaries further helps distinguish full sentences from fragments like headlines and titles, allowing us to model grammatical differences between complete and incomplete sentences. (iii) Sentence-internal punctuation boundaries help with longer-distance dependencies, since punctuation correlates with constituent edges. Our models induce state-of-the-art dependency grammars for many languages without — — special knowledge of optimal input sentence lengths or biased, manually-tuned initializers.</p><p>3 0.32166156 <a title="139-lda-3" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms ofweak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.</p><p>4 0.31654507 <a title="139-lda-4" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>Author: Jianxing Yu ; Zheng-Jun Zha ; Tat-Seng Chua</p><p>Abstract: This paper proposes to generate appropriate answers for opinion questions about products by exploiting the hierarchical organization of consumer reviews. The hierarchy organizes product aspects as nodes following their parent-child relations. For each aspect, the reviews and corresponding opinions on this aspect are stored. We develop a new framework for opinion Questions Answering, which enables accurate question analysis and effective answer generation by making use the hierarchy. In particular, we first identify the (explicit/implicit) product aspects asked in the questions and their sub-aspects by referring to the hierarchy. We then retrieve the corresponding review fragments relevant to the aspects from the hierarchy. In order to gener- ate appropriate answers from the review fragments, we develop a multi-criteria optimization approach for answer generation by simultaneously taking into account review salience, coherence, diversity, and parent-child relations among the aspects. We conduct evaluations on 11 popular products in four domains. The evaluated corpus contains 70,359 consumer reviews and 220 questions on these products. Experimental results demonstrate the effectiveness of our approach.</p><p>5 0.31486103 <a title="139-lda-5" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>Author: Heeyoung Lee ; Marta Recasens ; Angel Chang ; Mihai Surdeanu ; Dan Jurafsky</p><p>Abstract: We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable documents, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.</p><p>6 0.31334144 <a title="139-lda-6" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>7 0.30885163 <a title="139-lda-7" href="./emnlp-2012-Left-to-Right_Tree-to-String_Decoding_with_Prediction.html">82 emnlp-2012-Left-to-Right Tree-to-String Decoding with Prediction</a></p>
<p>8 0.30729538 <a title="139-lda-8" href="./emnlp-2012-Besting_the_Quiz_Master%3A_Crowdsourcing_Incremental_Classification_Games.html">23 emnlp-2012-Besting the Quiz Master: Crowdsourcing Incremental Classification Games</a></p>
<p>9 0.30712709 <a title="139-lda-9" href="./emnlp-2012-Extracting_Opinion_Expressions_with_semi-Markov_Conditional_Random_Fields.html">51 emnlp-2012-Extracting Opinion Expressions with semi-Markov Conditional Random Fields</a></p>
<p>10 0.30515379 <a title="139-lda-10" href="./emnlp-2012-Constructing_Task-Specific_Taxonomies_for_Document_Collection_Browsing.html">30 emnlp-2012-Constructing Task-Specific Taxonomies for Document Collection Browsing</a></p>
<p>11 0.30457476 <a title="139-lda-11" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>12 0.30260354 <a title="139-lda-12" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>13 0.30241996 <a title="139-lda-13" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>14 0.30217594 <a title="139-lda-14" href="./emnlp-2012-Streaming_Analysis_of_Discourse_Participants.html">120 emnlp-2012-Streaming Analysis of Discourse Participants</a></p>
<p>15 0.30076605 <a title="139-lda-15" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>16 0.30030781 <a title="139-lda-16" href="./emnlp-2012-Syntactic_Surprisal_Affects_Spoken_Word_Duration_in_Conversational_Contexts.html">122 emnlp-2012-Syntactic Surprisal Affects Spoken Word Duration in Conversational Contexts</a></p>
<p>17 0.29970041 <a title="139-lda-17" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>18 0.29902425 <a title="139-lda-18" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>19 0.29802036 <a title="139-lda-19" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>20 0.29791445 <a title="139-lda-20" href="./emnlp-2012-Revisiting_the_Predictability_of_Language%3A_Response_Completion_in_Social_Media.html">114 emnlp-2012-Revisiting the Predictability of Language: Response Completion in Social Media</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
