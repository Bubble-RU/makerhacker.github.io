<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>100 emnlp-2012-Open Language Learning for Information Extraction</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-100" href="#">emnlp2012-100</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>100 emnlp-2012-Open Language Learning for Information Extraction</h1>
<br/><p>Source: <a title="emnlp-2012-100-pdf" href="http://aclweb.org/anthology//D/D12/D12-1048.pdf">pdf</a></p><p>Author: Mausam ; Michael Schmitz ; Stephen Soderland ; Robert Bart ; Oren Etzioni</p><p>Abstract: Open Information Extraction (IE) systems extract relational tuples from text, without requiring a pre-specified vocabulary, by identifying relation phrases and associated arguments in arbitrary sentences. However, stateof-the-art Open IE systems such as REVERB and WOE share two important weaknesses (1) they extract only relations that are mediated by verbs, and (2) they ignore context, thus extracting tuples that are not asserted as factual. This paper presents OLLIE, a substantially improved Open IE system that addresses both these limitations. First, OLLIE achieves high yield by extracting relations mediated by nouns, adjectives, and more. Second, a context-analysis step increases precision by including contextual information from the sentence in the extractions. OLLIE obtains 2.7 times the area under precision-yield curve (AUC) compared to REVERB and 1.9 times the AUC of WOEparse. –</p><p>Reference: <a title="emnlp-2012-100-reference" href="../emnlp2012_reference/emnlp-2012-Open_Language_Learning_for_Information_Extraction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Open Information Extraction (IE) systems extract relational tuples from text, without requiring a pre-specified vocabulary, by identifying relation phrases and associated arguments in arbitrary sentences. [sent-3, score-0.256]
</p><p>2 However, stateof-the-art Open IE systems such as REVERB and WOE share two important weaknesses (1) they extract only relations that are mediated by verbs, and (2) they ignore context, thus extracting tuples that are not asserted as factual. [sent-4, score-0.262]
</p><p>3 First, OLLIE achieves high yield by extracting relations mediated by nouns, adjectives, and more. [sent-6, score-0.173]
</p><p>4 –  1 Introduction While traditional Information Extraction (IE) (ARPA, 1991 ; ARPA, 1998) focused on identifying and extracting specific relations of interest, there has been great interest in scaling IE to a broader set of relations and to far larger corpora (Banko et al. [sent-11, score-0.138]
</p><p>5 The substantial endeavor in 523  extractions for the first three sentences where REVERB (R) and WOEparse (W) find none. [sent-19, score-0.169]
</p><p>6 Both extract only relations that are mediated by verbs, and REVERB further restricts this to a subset of verbal patterns. [sent-28, score-0.137]
</p><p>7 This misses important information mediated via other syntactic entities such as nouns and adjectives, as well as a wider range of verbal structures (examples #1-3 in Figure 1). [sent-29, score-0.183]
</p><p>8 lc L2a0n1g2ua Agseso Pcrioactieosnsi fnogr a Cnodm Cpoumtaptiuotna tilo Lnianlg Nuaist uircasl Secondly, REVERB and WOEparse perform only a local analysis of a sentence, so they often extract relations that are not asserted as factual in the sentence (examples #4,5). [sent-32, score-0.192]
</p><p>9 OLLIE extractions obtain a dramatically higher yield at higher or comparable precision relative to existing systems. [sent-35, score-0.229]
</p><p>10 Section 3 describes the syntactic scope expansion component, which is based on a novel approach that learns open pattern templates. [sent-38, score-0.188]
</p><p>11 Moreover, for specific relations commonly mediated by nouns (e. [sent-45, score-0.179]
</p><p>12 2  Background  Open IE systems extract tuples consisting of argument phrases from the input sentence and a phrase 1Available for download  at  http://openie. [sent-50, score-0.134]
</p><p>13 , 2011), which uses shallow syntactic processing to identify relation phrases that begin with a verb and occur between the argument phrases;2 (2) WOEparse (Wu and Weld, 2010), which uses bootstrapping from entries in Wikipedia info-boxes to learn extraction pat-  terns in dependency parses. [sent-56, score-0.382]
</p><p>14 Like REVERB, the relation phrases begin with verbs, but can handle long-range dependencies and relation phrases that do not come between the arguments. [sent-57, score-0.298]
</p><p>15 Unlike REVERB, WOE does not include nouns within the relation phrases (e. [sent-58, score-0.191]
</p><p>16 Both systems ignore context around the extracted relations that may indicate whether it is a supposition or conditionally true rather than asserted as factual (see #4-5 in Figure 1). [sent-61, score-0.212]
</p><p>17 The task of Semantic role labeling is to identify arguments of verbs in a sentence, and then to classify the arguments by mapping the verb to a semantic frame and mapping the argument phrases to roles in that frame, such as agent, patient, instrument, or benefactive. [sent-62, score-0.303]
</p><p>18 SRL systems can also identify and classify arguments of relations that are mediated by nouns when trained on NomBank annotations. [sent-63, score-0.246]
</p><p>19 Where SRL begins with a verb or noun and then looks for arguments that play roles with respect to that verb or noun, Open IE looks for a phrase that expresses a relation between a pair of arguments. [sent-64, score-0.259]
</p><p>20 First, it uses a set of high precision seed tuples from REVERB to bootstrap a large training set. [sent-67, score-0.14]
</p><p>21 Second, it learns open pattern templates over this training set. [sent-68, score-0.211]
</p><p>22 Next, OLLIE applies these pattern templates at extraction time. [sent-69, score-0.163]
</p><p>23 edu/  Figure 2: System architecture: OLLIE begins with seed tuples from REVERB, uses them to build a bootstrap training set, and learns open pattern templates. [sent-75, score-0.281]
</p><p>24 The key observation is that almost every relation can also be expressed via a REVERB-style verb-based expression. [sent-79, score-0.147]
</p><p>25 So, bootstrapping sentences based on REVERB’s tuples will likely capture all relation expressions. [sent-80, score-0.202]
</p><p>26 We start with over 110,000 seed tuples these are high confidence REVERB extractions from a large Web corpus (ClueWeb)3 that are asserted at least twice and contain only proper nouns in the arguments. [sent-81, score-0.436]
</p><p>27 For example, a seed tuple may be (Paul Annacone; is the coach of; Federer) that REVERB extracts from the sentence “Paul Annacone is the coach of Federer. [sent-83, score-0.26]
</p><p>28 As an example, for a seed tuple (Boyle; is born in; Ireland) we may retrieve a sentence “Felix G. [sent-90, score-0.142]
</p><p>29 We only allow sentences where the content words from arguments and relation can be linked to each other via a linear path of size four in the dependency parse. [sent-95, score-0.237]
</p><p>30 In our case, by enforcing that a sentence additionally contains some syntactic form of the relation content words, our bootstrapping set is naturally much cleaner. [sent-113, score-0.185]
</p><p>31 Since the relation words in the sentence and seed match, we can learn general pattern templates that may apply to other relations too. [sent-115, score-0.375]
</p><p>32 LIE learns open pattern templates a mapping from a dependency path to an open extraction, i. [sent-124, score-0.362]
</p><p>33 , one that identifies both the arguments and the exact (REVERB-style) relation phrase. [sent-126, score-0.19]
</p><p>34 Open pattern templates encode the ways in which a relation (in the first column) may be expressed in a sentence (second column). [sent-129, score-0.254]
</p><p>35 For example, a relation (Godse; kill; Gandhi) may be expressed with a dependency path (#2) {Godse}↑nsubj↑{kill:postag=VBD}↓dobj↓{Gandhi}. [sent-130, score-0.194]
</p><p>36 To learn the pattern templates, we first extract the dependency path connecting the arguments and relation words for each seed tuple and the associated sentence. [sent-131, score-0.44]
</p><p>37 We annotate the relation node in the path with the exact relation word (as a lexical constraint) and the POS (postag constraint). [sent-132, score-0.269]
</p><p>38 We create a relation template from the seed tuple by normalizing ‘is’/‘was’/‘will be’ to ‘be’, and replacing the relation content word with {rel}. [sent-133, score-0.388]
</p><p>39 As an example, ‘hired’ is a slot word for the tuple (Annacone; is the coach of; Federer) in  the sentence “Federer hired Annacone as a coach”. [sent-137, score-0.205]
</p><p>40 The checks are: (1) There are no slot 4Our current implementation only allows a single relation content word; extending to multiple words is straightforward the templates will require rel1, rel2,. [sent-142, score-0.241]
</p><p>41 We remove  all lexical restrictions from the relation nodes. [sent-154, score-0.184]
</p><p>42 Both these data points return the same open pattern after generalization: “{arg1 } ↑nsubj↑ {rel:postag=VBD} ↓{prep ∗}↓ {arg2}” }w ↑itnhs tbhje↑ e {xrtrela:cptioosnta template (arg1, {rel} {prep}, arg2). [sent-165, score-0.165]
</p><p>43 To enable such patterns we retain the lexical constraints on the relation words and slot words. [sent-180, score-0.228]
</p><p>44 5 We collect all patterns together based only on the syntactic restrictions and convert the lexical constraint into a list of words with which the pattern was seen. [sent-181, score-0.212]
</p><p>45 This imposes a natural ranking on the patterns more frequent patterns are likely to give higher precision extractions. [sent-197, score-0.158]
</p><p>46 3 Pattern Matching for Extraction We now describe how these open patterns are used to extract binary relations from a new sentence. [sent-199, score-0.24]
</p><p>47 We first match the open patterns with the dependency parse of the sentence and identify the base nodes for arguments and relations. [sent-200, score-0.262]
</p><p>48 To apply pattern #1 from Figure 3 we first match arg1 to ‘festival’, rel to ‘scheduled’ and arg2 to ‘25th’ with prep ‘for’ . [sent-204, score-0.16]
</p><p>49 Since WOE does not have access to a seed relation phrase, it heuristically assigns all intervening words between the arguments in the parse as the relation phrase. [sent-223, score-0.389]
</p><p>50 ” WOE’s heuristics will extract the relation divorced was pursuing between ‘Tom Cruise’ and ‘Nicole Kidman’ . [sent-226, score-0.192]
</p><p>51 OLLIE, in contrast, produces well-formed relation phrases by basing its templates on REVERB relation phrases. [sent-227, score-0.318]
</p><p>52 Finally, WOE is designed to have verb-mediated relation phrases that do not include nouns, thus missing important relations such as ‘is the president of’ . [sent-229, score-0.264]
</p><p>53 4  Context Analysis in OLLIE  We now turn to the context analysis component, which handles the problem ofextractions that are not asserted as factual in the text. [sent-231, score-0.143]
</p><p>54 Our algorithm first checks for the presence of a ccomp edge to the relation node. [sent-246, score-0.196]
</p><p>55 }o and 528 ClausalModifier fields, nearly 98% on a development set, however, these two fields do not cover all the cases where an extraction is not asserted as factual. [sent-260, score-0.163]
</p><p>56 Our training set was 1000 extractions drawn evenly from Wikipedia, News, and Biology sentences. [sent-265, score-0.169]
</p><p>57 (3) How do OLLIE’s extractions compare with semantic role labeling argument identification? [sent-269, score-0.237]
</p><p>58 We ran three systems, OLLIE, REVERB and WOEparse on this dataset resulting in a total of 1,945 extractions from all three systems. [sent-273, score-0.169]
</p><p>59 Two annotators tagged the extractions as correct if the sentence asserted or implied that the relation was true. [sent-274, score-0.377]
</p><p>60 We find that 40% of the OLLIE extractions that REVERB misses are due to OLLIE’s use of parsers REVERB misses those because its shallow syntactic analysis cannot skip over the intervening clauses or prepositional phrases between the relation phrase and the arguments. [sent-290, score-0.441]
</p><p>61 About 30% of the additional yield is those extractions where the relation is not between its arguments (see instance #1 in Figure 1). [sent-291, score-0.395]
</p><p>62 In contrast, OLLIE misses very few extractions returned by REVERB, mostly due to parser errors. [sent-293, score-0.219]
</p><p>63 We find that WOEparse misses extractions found by OLLIE for a variety of reasons. [sent-294, score-0.219]
</p><p>64 The primary cause is that WOEparse does not include nouns in relation phrases. [sent-295, score-0.165]
</p><p>65 In other cases, WOEparse misses extractions due to ill-formed relation phrases (as in the example of Section 3. [sent-297, score-0.368]
</p><p>66 While the bulk of OLLIE’s extractions in our test 6Evaluating recall is difficult at this scale – however, since yield is proportional to recall, the area differences also hold for the equivalent precision-recall curves. [sent-299, score-0.228]
</p><p>67 nsfr  relations that are typically expressed by noun phrases up to 146 times that of REVERB. [sent-301, score-0.142]
</p><p>68 7 OLLIE found up to 146 times as many extractions for these relations than REVERB. [sent-309, score-0.238]
</p><p>69 Because WOEparse does not include nouns in relation phrases, it is unable to extract any instance of these relations. [sent-310, score-0.165]
</p><p>70 We examine a sample ofthe extractions to verify that noun-mediated extractions are the main reason for this large yield boost over REVERB (73% of OLLIE extractions were noun-mediated). [sent-311, score-0.543]
</p><p>71 High-frequency noun patterns like “Obama, the president of the US”, “Obama, the US president”, “US President Obama” far outnumber sentences of the form “Obama is the president of the US”. [sent-312, score-0.182]
</p><p>72 2  Analysis of OLLIE  We perform two control experiments to understand the value of semantic/lexical restrictions in pattern learning and precision boost due to context analysis component. [sent-317, score-0.146]
</p><p>73 7We multiply the total number of extractions with precision on a sample for that relation to estimate the yield. [sent-318, score-0.316]
</p><p>74 Figure 7: Results on the subset of extractions from patterns with semantic/lexical restrictions. [sent-319, score-0.236]
</p><p>75 Are semantic restrictions important for open pattern learning? [sent-323, score-0.226]
</p><p>76 To answer these questions we compare three systems OLLIE without semantic or lexical restrictions (OLLIE[syn]), OLLIE with lexical restrictions but no type generalization (OLLIE[lex]) and the full system (OLLIE). [sent-325, score-0.15]
</p><p>77 We restrict this experiment to the patterns where OLLIE adds semantic/lexical restrictions, rather than dilute the result with patterns that would be unchanged by these variants. [sent-326, score-0.134]
</p><p>78 This matches our intuition, since these are not completely general patterns and generalizing to all unseen relations results in a large number of errors. [sent-329, score-0.136]
</p><p>79 Adding ClausalModifier corrects errors for 21% of extractions that have a ClausalModifier and does not introduce any new errors. [sent-342, score-0.169]
</p><p>80 Adding AttributedTo corrects errors for 55% of the extractions with AttributedTo and introduces an error for 3% of the extractions. [sent-343, score-0.169]
</p><p>81 18% of the errors are due to aggressive generalization of a pattern to all unseen relations and 12% due to incorrect application of lexically annotated patterns. [sent-347, score-0.158]
</p><p>82 SRL, as discussed in Section 2, has a very different goal analyzing verbs and nouns to identify their arguments, then mapping the verb or noun to a semantic frame and determining the role that each argument plays in that frame. [sent-357, score-0.208]
</p><p>83 These verbs and nouns need not make the full relation phrase, although, recent work has shown that they may be converted to Open IE style extractions with additional postprocessing (Christensen et al. [sent-358, score-0.356]
</p><p>84 ” This task is –  –  permissive for both systems, as it does not require finding an exact relation phrase or argument boundary, or determining the argument roles in a relation. [sent-362, score-0.259]
</p><p>85 We only counted relation expressed by a verb or noun in the text, and did not include relations expressed simply with “of” or apostrophe-s. [sent-364, score-0.286]
</p><p>86 Where a verb mediates between an argument and multiple NPs, we represent this as a binary relation for all pairs of NPs. [sent-365, score-0.214]
</p><p>87 Recall is based on the percentage of NP pairs where the head nouns matches head nouns of two different arguments in an extraction or semantic frame. [sent-386, score-0.207]
</p><p>88 5, since it is tuned for high precision extraction, and avoids less reliable extractions from constructions such as reduced relative clauses and  gerunds, or from noun-mediated relations with longrange dependencies. [sent-396, score-0.262]
</p><p>89 The missing recall from SRL is primarily where it does not identify both arguments of a binary relation, or where the correct argument is buried in a long argument phrase, but is not its head noun. [sent-398, score-0.203]
</p><p>90 OLLIE finds the extraction (Clarcor; be a maker of; packaging and filtration products) where the heads of both arguments matched those of the target. [sent-407, score-0.182]
</p><p>91 All these approaches first bootstrap data based on seed instances of a relation (or seed data from existing resources such as Wikipedia) and then learn lexical or lexico-POS patterns to create an extractor. [sent-421, score-0.342]
</p><p>92 First, and most importantly, these previous systems learn an extractor for each relation of interest, whereas OLLIE is an open extractor. [sent-425, score-0.249]
</p><p>93 OLLIE’s strength is its ability to generalize from one relation to many other relations that are expressed in similar forms. [sent-426, score-0.244]
</p><p>94 This happens both via syntactic generalization and type generalization of relation words (sections 3. [sent-427, score-0.202]
</p><p>95 This capability is essential as many relations in the test set are not even seen in the training set in early exper–  532 iments we found that non-generalized pattern learning (equivalent to traditional IE) had significantly less yield at a slightly higher precision. [sent-432, score-0.166]
</p><p>96 The closest to our work is the pattern learning based open extractor WOEparse. [sent-437, score-0.187]
</p><p>97 Second, by an-  alyzing the context around an extraction, OLLIE is able to identify cases where the relation is not asserted as factual, but is hypothetical or conditionally true. [sent-447, score-0.228]
</p><p>98 OLLIE increases precision by reducing con-  fidence in those extractions  or by associating addi-  tional context in the extractions, in the form of attribution and clausal modifiers. [sent-448, score-0.253]
</p><p>99 7 times more area under precisionyield curves compared open extractors. [sent-451, score-0.147]
</p><p>100 An analysis of open information extraction based on semantic role labeling. [sent-516, score-0.16]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ollie', 0.807), ('reverb', 0.205), ('extractions', 0.169), ('woeparse', 0.157), ('relation', 0.123), ('ie', 0.112), ('open', 0.104), ('clausalmodifier', 0.089), ('lund', 0.089), ('asserted', 0.085), ('srl', 0.08), ('annacone', 0.079), ('attributedto', 0.079), ('seed', 0.076), ('relations', 0.069), ('federer', 0.069), ('argument', 0.068), ('mediated', 0.068), ('woe', 0.068), ('patterns', 0.067), ('arguments', 0.067), ('tuple', 0.066), ('restrictions', 0.061), ('pattern', 0.061), ('coach', 0.059), ('prep', 0.059), ('extraction', 0.056), ('misses', 0.05), ('scheduled', 0.049), ('templates', 0.046), ('president', 0.046), ('rna', 0.042), ('hired', 0.042), ('postag', 0.042), ('nouns', 0.042), ('tuples', 0.04), ('rel', 0.04), ('ccomp', 0.039), ('maker', 0.039), ('pursuing', 0.039), ('bootstrapping', 0.039), ('factual', 0.038), ('clausal', 0.038), ('slot', 0.038), ('yield', 0.036), ('fader', 0.036), ('checks', 0.034), ('auc', 0.034), ('festival', 0.034), ('frame', 0.03), ('christensen', 0.03), ('clarcor', 0.03), ('divorced', 0.03), ('macromolecules', 0.03), ('ceo', 0.028), ('generalize', 0.028), ('generalization', 0.028), ('ritter', 0.027), ('phrases', 0.026), ('soderland', 0.026), ('hoffmann', 0.026), ('expressed', 0.024), ('precision', 0.024), ('dependency', 0.024), ('confidence', 0.024), ('suchanek', 0.024), ('curve', 0.024), ('oren', 0.024), ('wikipedia', 0.023), ('verb', 0.023), ('path', 0.023), ('nombank', 0.023), ('ireland', 0.023), ('area', 0.023), ('noun', 0.023), ('syntactic', 0.023), ('verbs', 0.022), ('attribution', 0.022), ('extractor', 0.022), ('fields', 0.022), ('enter', 0.021), ('obama', 0.021), ('curves', 0.02), ('weld', 0.02), ('conditionally', 0.02), ('mausam', 0.02), ('lex', 0.02), ('amod', 0.02), ('boyle', 0.02), ('clueweb', 0.02), ('founder', 0.02), ('godse', 0.02), ('janara', 0.02), ('ofextractions', 0.02), ('orchestra', 0.02), ('packaging', 0.02), ('sasquatch', 0.02), ('symphony', 0.02), ('tubes', 0.02), ('webb', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="100-tfidf-1" href="./emnlp-2012-Open_Language_Learning_for_Information_Extraction.html">100 emnlp-2012-Open Language Learning for Information Extraction</a></p>
<p>Author: Mausam ; Michael Schmitz ; Stephen Soderland ; Robert Bart ; Oren Etzioni</p><p>Abstract: Open Information Extraction (IE) systems extract relational tuples from text, without requiring a pre-specified vocabulary, by identifying relation phrases and associated arguments in arbitrary sentences. However, stateof-the-art Open IE systems such as REVERB and WOE share two important weaknesses (1) they extract only relations that are mediated by verbs, and (2) they ignore context, thus extracting tuples that are not asserted as factual. This paper presents OLLIE, a substantially improved Open IE system that addresses both these limitations. First, OLLIE achieves high yield by extracting relations mediated by nouns, adjectives, and more. Second, a context-analysis step increases precision by including contextual information from the sentence in the extractions. OLLIE obtains 2.7 times the area under precision-yield curve (AUC) compared to REVERB and 1.9 times the AUC of WOEparse. –</p><p>2 0.12079298 <a title="100-tfidf-2" href="./emnlp-2012-Ensemble_Semantics_for_Large-scale_Unsupervised_Relation_Extraction.html">40 emnlp-2012-Ensemble Semantics for Large-scale Unsupervised Relation Extraction</a></p>
<p>Author: Bonan Min ; Shuming Shi ; Ralph Grishman ; Chin-Yew Lin</p><p>Abstract: Discovering significant types of relations from the web is challenging because of its open nature. Unsupervised algorithms are developed to extract relations from a corpus without knowing the relations in advance, but most of them rely on tagging arguments of predefined types. Recently, a new algorithm was proposed to jointly extract relations and their argument semantic classes, taking a set of relation instances extracted by an open IE algorithm as input. However, it cannot handle polysemy of relation phrases and fails to group many similar (“synonymous”) relation instances because of the sparseness of features. In this paper, we present a novel unsupervised algorithm that provides a more general treatment of the polysemy and synonymy problems. The algorithm incorporates various knowledge sources which we will show to be very effective for unsupervised extraction. Moreover, it explicitly disambiguates polysemous relation phrases and groups synonymous ones. While maintaining approximately the same precision, the algorithm achieves significant improvement on recall compared to the previous method. It is also very efficient. Experiments on a realworld dataset show that it can handle 14.7 million relation instances and extract a very large set of relations from the web. Ralph Grishman1 Chin-Yew Lin2 2Microsoft Research Asia Beijing, China { shumings cyl } @mi cro s o ft . com , that has many applications in answering factoid questions, building knowledge bases and improving search engine relevance. The web has become a massive potential source of such relations. However, its open nature brings an open-ended set of relation types. To extract these relations, a system should not assume a fixed set of relation types, nor rely on a fixed set of relation argument types. The past decade has seen some promising solutions, unsupervised relation extraction (URE) algorithms that extract relations from a corpus without knowing the relations in advance. However, most algorithms (Hasegawa et al., 2004, Shinyama and Sekine, 2006, Chen et. al, 2005) rely on tagging predefined types of entities as relation arguments, and thus are not well-suited for the open domain. Recently, Kok and Domingos (2008) proposed Semantic Network Extractor (SNE), which generates argument semantic classes and sets of synonymous relation phrases at the same time, thus avoiding the requirement of tagging relation arguments of predefined types. However, SNE has 2 limitations: 1) Following previous URE algorithms, it only uses features from the set of input relation instances for clustering. Empirically we found that it fails to group many relevant relation instances. These features, such as the surface forms of arguments and lexical sequences in between, are very sparse in practice. In contrast, there exist several well-known corpus-level semantic resources that can be automatically derived from a source corpus and are shown to be useful for generating the key elements of a relation: its 2 argument semantic classes and a set of synonymous phrases. For example, semantic classes can be derived from a source corpus with contextual distributional simi1 Introduction Relation extraction aims at discovering semantic larity and web table co-occurrences. The “synonymy” 1 problem for clustering relation instances relations between entities. It is an important task * Work done during an internship at Microsoft Research Asia 1027 LParnogcue agdein Lgesa ornf tihneg, 2 p0a1g2e Jso 1in02t C7–o1n0f3e7re,n Jce ju on Is Elanmdp,ir Kicoarlea M,e 1t2h–o1d4s J iunly N 2a0tu1r2a.l ? Lc a2n0g1u2ag Aes Psorcoicaetsiosin fgo arn Cdo Cmopmutpauti oantiaoln Lailn Ngautiustriacls could potentially be better solved by adding these resources. 2) SNE assumes that each entity or relation phrase belongs to exactly one cluster, thus is not able to effectively handle polysemy of relation phrases2. An example of a polysemous phrase is be the currency of as in 2 triples</p><p>3 0.1008783 <a title="100-tfidf-3" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms ofweak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.</p><p>4 0.09477362 <a title="100-tfidf-4" href="./emnlp-2012-PATTY%3A_A_Taxonomy_of_Relational_Patterns_with_Semantic_Types.html">103 emnlp-2012-PATTY: A Taxonomy of Relational Patterns with Semantic Types</a></p>
<p>Author: Ndapandula Nakashole ; Gerhard Weikum ; Fabian Suchanek</p><p>Abstract: This paper presents PATTY: a large resource for textual patterns that denote binary relations between entities. The patterns are semantically typed and organized into a subsumption taxonomy. The PATTY system is based on efficient algorithms for frequent itemset mining and can process Web-scale corpora. It harnesses the rich type system and entity population of large knowledge bases. The PATTY taxonomy comprises 350,569 pattern synsets. Random-sampling-based evaluation shows a pattern accuracy of 84.7%. PATTY has 8,162 subsumptions, with a random-sampling-based precision of 75%. The PATTY resource is freely available for interactive access and download.</p><p>5 0.09225107 <a title="100-tfidf-5" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>Author: Mihai Surdeanu ; Julie Tibshirani ; Ramesh Nallapati ; Christopher D. Manning</p><p>Abstract: Distant supervision for relation extraction (RE) gathering training data by aligning a database of facts with text – is an efficient approach to scale RE to thousands of different relations. However, this introduces a challenging learning scenario where the relation expressed by a pair of entities found in a sentence is unknown. For example, a sentence containing Balzac and France may express BornIn or Died, an unknown relation, or no relation at all. Because of this, traditional supervised learning, which assumes that each example is explicitly mapped to a label, is not appropriate. We propose a novel approach to multi-instance multi-label learning for RE, which jointly models all the instances of a pair of entities in text and all their labels using a graphical model with latent variables. Our model performs competitively on two difficult domains. –</p><p>6 0.07503359 <a title="100-tfidf-6" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>7 0.072931424 <a title="100-tfidf-7" href="./emnlp-2012-Identifying_Constant_and_Unique_Relations_by_using_Time-Series_Text.html">62 emnlp-2012-Identifying Constant and Unique Relations by using Time-Series Text</a></p>
<p>8 0.071576409 <a title="100-tfidf-8" href="./emnlp-2012-Learning_Verb_Inference_Rules_from_Linguistically-Motivated_Evidence.html">80 emnlp-2012-Learning Verb Inference Rules from Linguistically-Motivated Evidence</a></p>
<p>9 0.059832692 <a title="100-tfidf-9" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>10 0.058078077 <a title="100-tfidf-10" href="./emnlp-2012-Bilingual_Lexicon_Extraction_from_Comparable_Corpora_Using_Label_Propagation.html">25 emnlp-2012-Bilingual Lexicon Extraction from Comparable Corpora Using Label Propagation</a></p>
<p>11 0.057316601 <a title="100-tfidf-11" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>12 0.042495508 <a title="100-tfidf-12" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>13 0.042409699 <a title="100-tfidf-13" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>14 0.036812473 <a title="100-tfidf-14" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>15 0.036122367 <a title="100-tfidf-15" href="./emnlp-2012-Excitatory_or_Inhibitory%3A_A_New_Semantic_Orientation_Extracts_Contradiction_and_Causality_from_the_Web.html">44 emnlp-2012-Excitatory or Inhibitory: A New Semantic Orientation Extracts Contradiction and Causality from the Web</a></p>
<p>16 0.035475578 <a title="100-tfidf-16" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>17 0.034759909 <a title="100-tfidf-17" href="./emnlp-2012-Building_a_Lightweight_Semantic_Model_for_Unsupervised_Information_Extraction_on_Short_Listings.html">26 emnlp-2012-Building a Lightweight Semantic Model for Unsupervised Information Extraction on Short Listings</a></p>
<p>18 0.034088649 <a title="100-tfidf-18" href="./emnlp-2012-Improving_NLP_through_Marginalization_of_Hidden_Syntactic_Structure.html">65 emnlp-2012-Improving NLP through Marginalization of Hidden Syntactic Structure</a></p>
<p>19 0.031949971 <a title="100-tfidf-19" href="./emnlp-2012-Learning-based_Multi-Sieve_Co-reference_Resolution_with_Knowledge.html">76 emnlp-2012-Learning-based Multi-Sieve Co-reference Resolution with Knowledge</a></p>
<p>20 0.030977238 <a title="100-tfidf-20" href="./emnlp-2012-Resolving_Complex_Cases_of_Definite_Pronouns%3A_The_Winograd_Schema_Challenge.html">112 emnlp-2012-Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.138), (1, 0.095), (2, -0.002), (3, -0.054), (4, 0.06), (5, -0.017), (6, 0.081), (7, 0.177), (8, -0.149), (9, 0.03), (10, -0.066), (11, -0.009), (12, -0.061), (13, -0.02), (14, -0.044), (15, -0.028), (16, -0.199), (17, -0.026), (18, -0.01), (19, -0.009), (20, -0.04), (21, -0.063), (22, -0.008), (23, -0.017), (24, -0.023), (25, -0.154), (26, 0.023), (27, -0.029), (28, 0.065), (29, 0.067), (30, -0.082), (31, 0.038), (32, -0.113), (33, 0.134), (34, -0.032), (35, -0.04), (36, 0.022), (37, 0.06), (38, -0.022), (39, 0.008), (40, -0.093), (41, -0.049), (42, -0.025), (43, -0.06), (44, -0.147), (45, 0.002), (46, -0.188), (47, 0.122), (48, 0.039), (49, 0.132)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92964321 <a title="100-lsi-1" href="./emnlp-2012-Open_Language_Learning_for_Information_Extraction.html">100 emnlp-2012-Open Language Learning for Information Extraction</a></p>
<p>Author: Mausam ; Michael Schmitz ; Stephen Soderland ; Robert Bart ; Oren Etzioni</p><p>Abstract: Open Information Extraction (IE) systems extract relational tuples from text, without requiring a pre-specified vocabulary, by identifying relation phrases and associated arguments in arbitrary sentences. However, stateof-the-art Open IE systems such as REVERB and WOE share two important weaknesses (1) they extract only relations that are mediated by verbs, and (2) they ignore context, thus extracting tuples that are not asserted as factual. This paper presents OLLIE, a substantially improved Open IE system that addresses both these limitations. First, OLLIE achieves high yield by extracting relations mediated by nouns, adjectives, and more. Second, a context-analysis step increases precision by including contextual information from the sentence in the extractions. OLLIE obtains 2.7 times the area under precision-yield curve (AUC) compared to REVERB and 1.9 times the AUC of WOEparse. –</p><p>2 0.76390523 <a title="100-lsi-2" href="./emnlp-2012-Ensemble_Semantics_for_Large-scale_Unsupervised_Relation_Extraction.html">40 emnlp-2012-Ensemble Semantics for Large-scale Unsupervised Relation Extraction</a></p>
<p>Author: Bonan Min ; Shuming Shi ; Ralph Grishman ; Chin-Yew Lin</p><p>Abstract: Discovering significant types of relations from the web is challenging because of its open nature. Unsupervised algorithms are developed to extract relations from a corpus without knowing the relations in advance, but most of them rely on tagging arguments of predefined types. Recently, a new algorithm was proposed to jointly extract relations and their argument semantic classes, taking a set of relation instances extracted by an open IE algorithm as input. However, it cannot handle polysemy of relation phrases and fails to group many similar (“synonymous”) relation instances because of the sparseness of features. In this paper, we present a novel unsupervised algorithm that provides a more general treatment of the polysemy and synonymy problems. The algorithm incorporates various knowledge sources which we will show to be very effective for unsupervised extraction. Moreover, it explicitly disambiguates polysemous relation phrases and groups synonymous ones. While maintaining approximately the same precision, the algorithm achieves significant improvement on recall compared to the previous method. It is also very efficient. Experiments on a realworld dataset show that it can handle 14.7 million relation instances and extract a very large set of relations from the web. Ralph Grishman1 Chin-Yew Lin2 2Microsoft Research Asia Beijing, China { shumings cyl } @mi cro s o ft . com , that has many applications in answering factoid questions, building knowledge bases and improving search engine relevance. The web has become a massive potential source of such relations. However, its open nature brings an open-ended set of relation types. To extract these relations, a system should not assume a fixed set of relation types, nor rely on a fixed set of relation argument types. The past decade has seen some promising solutions, unsupervised relation extraction (URE) algorithms that extract relations from a corpus without knowing the relations in advance. However, most algorithms (Hasegawa et al., 2004, Shinyama and Sekine, 2006, Chen et. al, 2005) rely on tagging predefined types of entities as relation arguments, and thus are not well-suited for the open domain. Recently, Kok and Domingos (2008) proposed Semantic Network Extractor (SNE), which generates argument semantic classes and sets of synonymous relation phrases at the same time, thus avoiding the requirement of tagging relation arguments of predefined types. However, SNE has 2 limitations: 1) Following previous URE algorithms, it only uses features from the set of input relation instances for clustering. Empirically we found that it fails to group many relevant relation instances. These features, such as the surface forms of arguments and lexical sequences in between, are very sparse in practice. In contrast, there exist several well-known corpus-level semantic resources that can be automatically derived from a source corpus and are shown to be useful for generating the key elements of a relation: its 2 argument semantic classes and a set of synonymous phrases. For example, semantic classes can be derived from a source corpus with contextual distributional simi1 Introduction Relation extraction aims at discovering semantic larity and web table co-occurrences. The “synonymy” 1 problem for clustering relation instances relations between entities. It is an important task * Work done during an internship at Microsoft Research Asia 1027 LParnogcue agdein Lgesa ornf tihneg, 2 p0a1g2e Jso 1in02t C7–o1n0f3e7re,n Jce ju on Is Elanmdp,ir Kicoarlea M,e 1t2h–o1d4s J iunly N 2a0tu1r2a.l ? Lc a2n0g1u2ag Aes Psorcoicaetsiosin fgo arn Cdo Cmopmutpauti oantiaoln Lailn Ngautiustriacls could potentially be better solved by adding these resources. 2) SNE assumes that each entity or relation phrase belongs to exactly one cluster, thus is not able to effectively handle polysemy of relation phrases2. An example of a polysemous phrase is be the currency of as in 2 triples</p><p>3 0.64925921 <a title="100-lsi-3" href="./emnlp-2012-Identifying_Constant_and_Unique_Relations_by_using_Time-Series_Text.html">62 emnlp-2012-Identifying Constant and Unique Relations by using Time-Series Text</a></p>
<p>Author: Yohei Takaku ; Nobuhiro Kaji ; Naoki Yoshinaga ; Masashi Toyoda</p><p>Abstract: Because the real world evolves over time, numerous relations between entities written in presently available texts are already obsolete or will potentially evolve in the future. This study aims at resolving the intricacy in consistently compiling relations extracted from text, and presents a method for identifying constancy and uniqueness of the relations in the context of supervised learning. We exploit massive time-series web texts to induce features on the basis of time-series frequency and linguistic cues. Experimental results confirmed that the time-series frequency distributions contributed much to the recall of constancy identification and the precision of the uniqueness identification.</p><p>4 0.60349762 <a title="100-lsi-4" href="./emnlp-2012-PATTY%3A_A_Taxonomy_of_Relational_Patterns_with_Semantic_Types.html">103 emnlp-2012-PATTY: A Taxonomy of Relational Patterns with Semantic Types</a></p>
<p>Author: Ndapandula Nakashole ; Gerhard Weikum ; Fabian Suchanek</p><p>Abstract: This paper presents PATTY: a large resource for textual patterns that denote binary relations between entities. The patterns are semantically typed and organized into a subsumption taxonomy. The PATTY system is based on efficient algorithms for frequent itemset mining and can process Web-scale corpora. It harnesses the rich type system and entity population of large knowledge bases. The PATTY taxonomy comprises 350,569 pattern synsets. Random-sampling-based evaluation shows a pattern accuracy of 84.7%. PATTY has 8,162 subsumptions, with a random-sampling-based precision of 75%. The PATTY resource is freely available for interactive access and download.</p><p>5 0.42896122 <a title="100-lsi-5" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms ofweak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.</p><p>6 0.42403385 <a title="100-lsi-6" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>7 0.42218885 <a title="100-lsi-7" href="./emnlp-2012-Learning_Verb_Inference_Rules_from_Linguistically-Motivated_Evidence.html">80 emnlp-2012-Learning Verb Inference Rules from Linguistically-Motivated Evidence</a></p>
<p>8 0.3963145 <a title="100-lsi-8" href="./emnlp-2012-Excitatory_or_Inhibitory%3A_A_New_Semantic_Orientation_Extracts_Contradiction_and_Causality_from_the_Web.html">44 emnlp-2012-Excitatory or Inhibitory: A New Semantic Orientation Extracts Contradiction and Causality from the Web</a></p>
<p>9 0.3745037 <a title="100-lsi-9" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>10 0.36005843 <a title="100-lsi-10" href="./emnlp-2012-Bilingual_Lexicon_Extraction_from_Comparable_Corpora_Using_Label_Propagation.html">25 emnlp-2012-Bilingual Lexicon Extraction from Comparable Corpora Using Label Propagation</a></p>
<p>11 0.34656072 <a title="100-lsi-11" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>12 0.34636137 <a title="100-lsi-12" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>13 0.27192476 <a title="100-lsi-13" href="./emnlp-2012-Automatically_Constructing_a_Normalisation_Dictionary_for_Microblogs.html">22 emnlp-2012-Automatically Constructing a Normalisation Dictionary for Microblogs</a></p>
<p>14 0.22979718 <a title="100-lsi-14" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>15 0.21851252 <a title="100-lsi-15" href="./emnlp-2012-Supervised_Text-based_Geolocation_Using_Language_Models_on_an_Adaptive_Grid.html">121 emnlp-2012-Supervised Text-based Geolocation Using Language Models on an Adaptive Grid</a></p>
<p>16 0.21017286 <a title="100-lsi-16" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>17 0.20927924 <a title="100-lsi-17" href="./emnlp-2012-Building_a_Lightweight_Semantic_Model_for_Unsupervised_Information_Extraction_on_Short_Listings.html">26 emnlp-2012-Building a Lightweight Semantic Model for Unsupervised Information Extraction on Short Listings</a></p>
<p>18 0.20538633 <a title="100-lsi-18" href="./emnlp-2012-Local_and_Global_Context_for_Supervised_and_Unsupervised_Metonymy_Resolution.html">85 emnlp-2012-Local and Global Context for Supervised and Unsupervised Metonymy Resolution</a></p>
<p>19 0.18467587 <a title="100-lsi-19" href="./emnlp-2012-Characterizing_Stylistic_Elements_in_Syntactic_Structure.html">27 emnlp-2012-Characterizing Stylistic Elements in Syntactic Structure</a></p>
<p>20 0.18339226 <a title="100-lsi-20" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.032), (16, 0.026), (18, 0.015), (25, 0.012), (34, 0.032), (60, 0.06), (63, 0.451), (64, 0.025), (65, 0.051), (70, 0.021), (73, 0.017), (74, 0.036), (76, 0.044), (80, 0.026), (86, 0.017), (94, 0.02), (95, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94535077 <a title="100-lda-1" href="./emnlp-2012-Modelling_Sequential_Text_with_an_Adaptive_Topic_Model.html">90 emnlp-2012-Modelling Sequential Text with an Adaptive Topic Model</a></p>
<p>Author: Lan Du ; Wray Buntine ; Huidong Jin</p><p>Abstract: Topic models are increasingly being used for text analysis tasks, often times replacing earlier semantic techniques such as latent semantic analysis. In this paper, we develop a novel adaptive topic model with the ability to adapt topics from both the previous segment and the parent document. For this proposed model, a Gibbs sampler is developed for doing posterior inference. Experimental results show that with topic adaptation, our model significantly improves over existing approaches in terms of perplexity, and is able to uncover clear sequential structure on, for example, Herman Melville’s book “Moby Dick”.</p><p>2 0.94199574 <a title="100-lda-2" href="./emnlp-2012-Multiple_Aspect_Summarization_Using_Integer_Linear_Programming.html">94 emnlp-2012-Multiple Aspect Summarization Using Integer Linear Programming</a></p>
<p>Author: Kristian Woodsend ; Mirella Lapata</p><p>Abstract: Multi-document summarization involves many aspects of content selection and surface realization. The summaries must be informative, succinct, grammatical, and obey stylistic writing conventions. We present a method where such individual aspects are learned separately from data (without any hand-engineering) but optimized jointly using an integer linear programme. The ILP framework allows us to combine the decisions of the expert learners and to select and rewrite source content through a mixture of objective setting, soft and hard constraints. Experimental results on the TAC-08 data set show that our model achieves state-of-the-art performance using ROUGE and significantly improves the informativeness of the summaries.</p><p>same-paper 3 0.93442327 <a title="100-lda-3" href="./emnlp-2012-Open_Language_Learning_for_Information_Extraction.html">100 emnlp-2012-Open Language Learning for Information Extraction</a></p>
<p>Author: Mausam ; Michael Schmitz ; Stephen Soderland ; Robert Bart ; Oren Etzioni</p><p>Abstract: Open Information Extraction (IE) systems extract relational tuples from text, without requiring a pre-specified vocabulary, by identifying relation phrases and associated arguments in arbitrary sentences. However, stateof-the-art Open IE systems such as REVERB and WOE share two important weaknesses (1) they extract only relations that are mediated by verbs, and (2) they ignore context, thus extracting tuples that are not asserted as factual. This paper presents OLLIE, a substantially improved Open IE system that addresses both these limitations. First, OLLIE achieves high yield by extracting relations mediated by nouns, adjectives, and more. Second, a context-analysis step increases precision by including contextual information from the sentence in the extractions. OLLIE obtains 2.7 times the area under precision-yield curve (AUC) compared to REVERB and 1.9 times the AUC of WOEparse. –</p><p>4 0.90475559 <a title="100-lda-4" href="./emnlp-2012-An_%22AI_readability%22_Formula_for_French_as_a_Foreign_Language.html">17 emnlp-2012-An "AI readability" Formula for French as a Foreign Language</a></p>
<p>Author: Thomas Francois ; Cedrick Fairon</p><p>Abstract: This paper present a new readability formula for French as a foreign language (FFL), which relies on 46 textual features representative of the lexical, syntactic, and semantic levels as well as some of the specificities of the FFL context. We report comparisons between several techniques for feature selection and various learning algorithms. Our best model, based on support vector machines (SVM), significantly outperforms previous FFL formulas. We also found that semantic features behave poorly in our case, in contrast with some previous readability studies on English as a first language.</p><p>5 0.84144604 <a title="100-lda-5" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>Author: Mohamed Yahya ; Klaus Berberich ; Shady Elbassuoni ; Maya Ramanath ; Volker Tresp ; Gerhard Weikum</p><p>Abstract: The Linked Data initiative comprises structured databases in the Semantic-Web data model RDF. Exploring this heterogeneous data by structured query languages is tedious and error-prone even for skilled users. To ease the task, this paper presents a methodology for translating natural language questions into structured SPARQL queries over linked-data sources. Our method is based on an integer linear program to solve several disambiguation tasks jointly: the segmentation of questions into phrases; the mapping of phrases to semantic entities, classes, and relations; and the construction of SPARQL triple patterns. Our solution harnesses the rich type system provided by knowledge bases in the web of linked data, to constrain our semantic-coherence objective function. We present experiments on both the . in question translation and the resulting query answering.</p><p>6 0.67869455 <a title="100-lda-6" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>7 0.67675543 <a title="100-lda-7" href="./emnlp-2012-A_Phrase-Discovering_Topic_Model_Using_Hierarchical_Pitman-Yor_Processes.html">8 emnlp-2012-A Phrase-Discovering Topic Model Using Hierarchical Pitman-Yor Processes</a></p>
<p>8 0.66843343 <a title="100-lda-8" href="./emnlp-2012-SSHLDA%3A_A_Semi-Supervised_Hierarchical_Topic_Model.html">115 emnlp-2012-SSHLDA: A Semi-Supervised Hierarchical Topic Model</a></p>
<p>9 0.6440621 <a title="100-lda-9" href="./emnlp-2012-PATTY%3A_A_Taxonomy_of_Relational_Patterns_with_Semantic_Types.html">103 emnlp-2012-PATTY: A Taxonomy of Relational Patterns with Semantic Types</a></p>
<p>10 0.57728994 <a title="100-lda-10" href="./emnlp-2012-Discovering_Diverse_and_Salient_Threads_in_Document_Collections.html">33 emnlp-2012-Discovering Diverse and Salient Threads in Document Collections</a></p>
<p>11 0.57532579 <a title="100-lda-11" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>12 0.57119668 <a title="100-lda-12" href="./emnlp-2012-Characterizing_Stylistic_Elements_in_Syntactic_Structure.html">27 emnlp-2012-Characterizing Stylistic Elements in Syntactic Structure</a></p>
<p>13 0.56137222 <a title="100-lda-13" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>14 0.55523551 <a title="100-lda-14" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>15 0.55410963 <a title="100-lda-15" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>16 0.55398148 <a title="100-lda-16" href="./emnlp-2012-Translation_Model_Based_Cross-Lingual_Language_Model_Adaptation%3A_from_Word_Models_to_Phrase_Models.html">128 emnlp-2012-Translation Model Based Cross-Lingual Language Model Adaptation: from Word Models to Phrase Models</a></p>
<p>17 0.55091906 <a title="100-lda-17" href="./emnlp-2012-A_Systematic_Comparison_of_Phrase_Table_Pruning_Techniques.html">11 emnlp-2012-A Systematic Comparison of Phrase Table Pruning Techniques</a></p>
<p>18 0.54653138 <a title="100-lda-18" href="./emnlp-2012-An_Entity-Topic_Model_for_Entity_Linking.html">19 emnlp-2012-An Entity-Topic Model for Entity Linking</a></p>
<p>19 0.54375887 <a title="100-lda-19" href="./emnlp-2012-Revisiting_the_Predictability_of_Language%3A_Response_Completion_in_Social_Media.html">114 emnlp-2012-Revisiting the Predictability of Language: Response Completion in Social Media</a></p>
<p>20 0.54205227 <a title="100-lda-20" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
