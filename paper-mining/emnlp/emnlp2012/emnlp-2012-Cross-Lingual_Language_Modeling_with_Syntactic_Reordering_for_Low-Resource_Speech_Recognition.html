<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>31 emnlp-2012-Cross-Lingual Language Modeling with Syntactic Reordering for Low-Resource Speech Recognition</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-31" href="#">emnlp2012-31</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>31 emnlp-2012-Cross-Lingual Language Modeling with Syntactic Reordering for Low-Resource Speech Recognition</h1>
<br/><p>Source: <a title="emnlp-2012-31-pdf" href="http://aclweb.org/anthology//D/D12/D12-1070.pdf">pdf</a></p><p>Author: Ping Xu ; Pascale Fung</p><p>Abstract: This paper proposes cross-lingual language modeling for transcribing source resourcepoor languages and translating them into target resource-rich languages if necessary. Our focus is to improve the speech recognition performance of low-resource languages by leveraging the language model statistics from resource-rich languages. The most challenging work of cross-lingual language modeling is to solve the syntactic discrepancies between the source and target languages. We therefore propose syntactic reordering for cross-lingual language modeling, and present a first result that compares inversion transduction grammar (ITG) reordering constraints to IBM and local constraints in an integrated speech transcription and translation system. Evaluations on resource-poor Cantonese speech transcription and Cantonese to resource-rich Mandarin translation tasks show that our proposed approach improves the system performance significantly, up to 3.4% relative WER reduction in Cantonese transcription and 13.3% relative bilingual evaluation understudy (BLEU) score improvement in Mandarin transcription compared with the system without reordering.</p><p>Reference: <a title="emnlp-2012-31-reference" href="../emnlp2012_reference/emnlp-2012-Cross-Lingual_Language_Modeling_with_Syntactic_Reordering_for_Low-Resource_Speech_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 hk ,  Abstract This paper proposes cross-lingual language modeling for transcribing source resourcepoor languages and translating them into target resource-rich languages if necessary. [sent-2, score-0.162]
</p><p>2 Our focus is to improve the speech recognition performance of low-resource languages by leveraging the language model statistics from resource-rich languages. [sent-3, score-0.273]
</p><p>3 We therefore propose syntactic reordering for cross-lingual language modeling, and present a first result that compares inversion transduction grammar (ITG) reordering constraints to IBM and local constraints in an integrated speech transcription and translation system. [sent-5, score-1.906]
</p><p>4 Evaluations  on resource-poor Cantonese speech transcription and Cantonese to resource-rich Mandarin translation tasks show that our proposed approach improves the system performance significantly, up to 3. [sent-6, score-0.36]
</p><p>5 3% relative bilingual evaluation understudy (BLEU) score improvement in Mandarin transcription compared with the system without reordering. [sent-8, score-0.183]
</p><p>6 some Chinese and Indian languages), and thus there is still a great demand to build speech and language processing systems for these languages. [sent-17, score-0.162]
</p><p>7 They carried out language model transformation since the input speech  766  PLraoncgeeuadgineg Lse oafr tnhineg 2,0 p1a2g Jeosin 76t C6–o7n7f6e,re Jnecjue Iosnla Enmd,p Kiroicraela, M 1e2t–h1o4ds Ju ilny N 20a1tu2r. [sent-31, score-0.162]
</p><p>8 Both interpolation and word-level transduction approaches fail to meet the challenge of syntactic discrepancies between the resource-poor and resource-rich languages. [sent-41, score-0.178]
</p><p>9 We suggest that a better approach than interpolation and word-level transduction is to use crosslingual language modeling with syntactic reorder1For example, Hindi and Malayalam (Geethakumary, 2002). [sent-47, score-0.213]
</p><p>10 A reordering model with reordering constraints, such as ITG constraints (Wu, 1997), IBM con-  straints (Berger et al. [sent-52, score-1.163]
</p><p>11 , 1996), and local constraints (Kumar and Byrne, 2005) can account for the syntactic differences. [sent-53, score-0.218]
</p><p>12 , 2007) that ITG constraints perform better than other constraints when tackling the reordering between many language pairs. [sent-56, score-0.845]
</p><p>13 Previous work on weighted finitestate transducer (WFST) based speech translation such as (Casacuberta et al. [sent-57, score-0.3]
</p><p>14 , 2006; Saon and Picheny, 2007) only train the reordering model using IBM constraints, local constraints or ad hoc rules. [sent-61, score-0.692]
</p><p>15 We will use ITG constraints, which have only been applied to text translation tasks before, to model the syntactic differences in cross-lingual language modeling for speech recognition. [sent-62, score-0.293]
</p><p>16 We will implement a cross-lingual language model using WFSTs, and integrate it into a WFSTbased speech recognition search space to give both resource-poor language and resource-rich language transcriptions. [sent-63, score-0.296]
</p><p>17 This creates an integrated speech transcription and translation framework. [sent-64, score-0.36]
</p><p>18 In Section 3, we discuss speech recognition with cross-lingual language models. [sent-66, score-0.24]
</p><p>19 2  Cross-lingual Language Modeling with Syntactic Reordering  In automatic speech recognition (ASR), given an observed source speech vector X, the decoding process searches the best word sequence vˆ1I (consists of words v1, v2, . [sent-69, score-0.444]
</p><p>20 , vI) by maximizing the posterior probability P(v1I|X), where v1I is the source transcript representing t)h,e w transcription of the source speech (see Eq. [sent-72, score-0.469]
</p><p>21 In order to improve the language model P(v1I) of the resource-poor language Lv, we introduce cross-lingual language modeling by decomposing the language model P(v1I) into a translation model P(v1I|w1J) and a language model P(w1J) of the resou|rwce-rich language Lw (see Eq. [sent-79, score-0.27]
</p><p>22 w1J is the target resource-rich language transcript that con-  sists of words w1, w2, . [sent-81, score-0.212]
</p><p>23 The translation model P(v1I|w1J) can be estimated by addressing the discrepancies between the resource-poor language Lv and the resource-rich language Lw, which can be modeled from a parallel corpus of the Lv transcript v1I and the Lw transcript w1J. [sent-87, score-0.624]
</p><p>24 For the syntactic inversions, we reorder the word or phrase positions of the Lw language model into those of the Lv language model. [sent-88, score-0.184]
</p><p>25 This paper, therefore only considers phrase-level reordering, which effectively preserves the monotonic word sequences within phrases, and significantly reduces the number of reordering paths compared with word-level reordering. [sent-90, score-0.528]
</p><p>26 , ˜v K) segmented from the word-level Lv transcript v1I and w˜ 1K (consists of phrases w˜ 1 , w˜ 2, . [sent-96, score-0.213]
</p><p>27 , w˜ K) segmented from the wordlevel Lw transcript w1J. [sent-99, score-0.213]
</p><p>28 Furthermore, we define a reordering sequence r1K, of which the detail can be found in Section 2. [sent-100, score-0.544]
</p><p>29 (2)): segmentation model P( w˜ K1|w1J), phrasal reordering model P(r1K| w˜ 1K, w1J), phrase-to-phrase  trans-  duction model P|( w˜ v˜ 1K|rK1, w˜ 1K, w1J) and reconstruction model P(v1I|˜ v1K, |rr1K, w˜ 1K, w1J). [sent-103, score-0.65]
</p><p>30 Before presenting each component model, we need to extract two phrase tables for the Lv transcript and the Lw transcript, respectively. [sent-104, score-0.256]
</p><p>31 Figure 1 shows an example of word-to-word alignment results between an Lv transcript (Cantonese) and an Lw transcript (Mandarin), from which phrase-to-phrase alignments are derived by identifying deletion, substitution, insertion and inversion. [sent-107, score-0.394]
</p><p>32 Prior to phrasal reordering, the segmentation model P( w˜ K1|w1J) implemented by a segmentation WFST Sw is applied to segment a word sequence w1J in the Lw language model into a phrase sequence { w˜ 1 , w˜ 2, . [sent-108, score-0.322]
</p><p>33 It segments a word sequence {w1, w2, w3} into a phrase sequence {w1, w2w3} {afwter performing composition (Mohri, 2009) with the} target Lw language model (see Figure 3(b1 & b2))3 . [sent-114, score-0.214]
</p><p>34 , w˜ K} of the Lw transcript, th seeq u roenlec eo f{ wthe reordering mofo tdheel P(r1K| w˜ 1K, w1J) is to reorder phrase positions of the Lw transcript into those of the Lv transcript by permutation of w˜ 1K according to a reordering sequence {rK1 : rk ∈ {1, 2, . [sent-146, score-1.831]
</p><p>35 Since} arbitrary permutations of K phrases are NP-hard (Knight, 1999), reordering constraints have to be set over r1K to reduce the number of permutations. [sent-158, score-0.8]
</p><p>36 There are three reordering constraints widely used in statistical machine translation, namely local constraints, IBM constraints and ITG constraints. [sent-159, score-0.849]
</p><p>37 Here we would like to point out that this is the first time that reordering constraints have been incorporated into a cross-lingual language model for speech recognition. [sent-160, score-0.822]
</p><p>38 Reordering Constraints Local constraints make the restriction that one phrase can jump at most L−1 phrases either forward or backward, mwph earte m Los its L Lt−he1 reordering dheisrta fnocrew (or window size of permutation)4. [sent-161, score-0.732]
</p><p>39 The generation of r1K under local constraints can be viewed as solving of the following problem (Kløve, 2009): secutive words forming a phrase. [sent-162, score-0.189]
</p><p>40 4The concept of reordering distance also applies to other constraints. [sent-163, score-0.503]
</p><p>41 k IBM constraints, a superset of local constraints (Dreyer et al. [sent-174, score-0.189]
</p><p>42 , 2007), generate permutations r1K deviate from the monotonic phrase order {rK1 : rk = k}. [sent-175, score-0.475]
</p><p>43 eM froorme specifically, any phrase position rk can be ske}l. [sent-176, score-0.31]
</p><p>44 A typical value of m is 4 (Zens and Ney, 2003), and we write IBM constraints with m = 4 as IBM(4). [sent-179, score-0.157]
</p><p>45 6=faritkh′6=ul}covera(g3)  of syntactic reordering in the parallel data than local constraints and IBM constraints. [sent-183, score-0.797]
</p><p>46 Our presentation of ITG constraints starts with defining of some permutation sets. [sent-184, score-0.236]
</p><p>47 ∩ITSG constraints allow the permutation set SK(3142, 2413), which forbids subsequence of type (3, 1, 4, 2) and its dual (2, 4, 1, 3). [sent-211, score-0.236]
</p><p>48 Explicitly, ITG constraints avoid any permutation r1K satisfying either ri2 < ri4 < ri1 < ri3 or ri3 < ri1 < ri4 ri2, where 1 ≤ i1 i2 i3 i4 ≤ K. [sent-212, score-0.236]
</p><p>49 In order to get an intuitive sense of the reordering capability ofthose three constraints, we list the number of permutations under local constraints, IBM constraints as well as ITG constraints5 in Table 1. [sent-216, score-0.832]
</p><p>50 <  < < <  5Interestingly, when K = L, the number of permutations under ITG constraints NITG = |SK (3142, 2413) |, and |SK (3142, 2413) | equals the K −1-=th S |Schro¨ (d3e1r4 n2u,m2b4e1r3s) sK−1 |(EShr(e3n1f4eu2c,h2t4 e1t3 3a)l|. [sent-217, score-0.297]
</p><p>51 , e 1998) Table 1: Comparison of permutation number under local constraints (NLocal), IBM constraints  (NIBM(4)) and ITG  constraints (NITG). [sent-218, score-0.582]
</p><p>52 The comparison is constrained by the phrase number K and the reordering distance L. [sent-219, score-0.575]
</p><p>53 We can see that given the same K (K ≤ 10) and LW (L ≤ 6), I tBhMat gcoivnesntr tahinets s ahmaeve K Kles (sK permutations Ltha (nL lo ≤ca 6l constraints, tarandin tIsT Gha cvoen lesstrsai pnetsrm mhautvaet loensss permutations than IBM constraints in general (only one exception when K = L = 6). [sent-220, score-0.437]
</p><p>54 These observations indicate that ITG constraints can filter out more unlikely permutations for a fixed reordering distance, resulting in longer distance reordering capability. [sent-221, score-1.303]
</p><p>55 Table 1 also tells us that the phrase number K and the reordering distance L for any of the con-  straints cannot be too large for practical implementation. [sent-222, score-0.575]
</p><p>56 If long reordering distances are allowed, unlikely permutations should be pruned so that the memory consumption becomes manageable. [sent-225, score-0.643]
</p><p>57 Reordering Sequence Distribution So far we have discussed the issue that how to generate permutations for the reordering model using reordering constraints. [sent-226, score-1.146]
</p><p>58 Another issue is how to parameterize the reordering sequence distribution. [sent-227, score-0.544]
</p><p>59 Both ITG constraints and other constraints assume 770  that all permutations are equally probable. [sent-228, score-0.454]
</p><p>60 P(r1K| w˜ 1K, w1J)  YK  =  P(r1)  Y P(rk|rk−1, w˜ K1) Yk=2 YK  =  P(r1)  YP(rk|rk−1)  (4)  Yk=2  We make a first order Markov assumption over the phrasal reordering model P(r1K| w˜1K, w1J) (see Eq. [sent-231, score-0.543]
</p><p>61 The reordering sequence dis| twr˜ibution is parameterized to assign decreasing likelihood to phrase reorderings { w˜ r1 , w˜ r2 , . [sent-233, score-0.616]
</p><p>62 Suppose w˜ rk = and w˜ r = , the reordering sequence distribution is set as Eq. [sent-239, score-0.782]
</p><p>63 ,K}  P(rk|rk−1) =  (5)  Assume that we have a phrase sequence { w˜ 1 , w˜ 2, w˜ 3}, Figure 2 shows the phrasal reordering {m w˜o del implemented by a reordering h WraFsaSlT re duenridnegr the first order Markov assumption for this phrase sequence. [sent-245, score-1.231]
</p><p>64 e W phitrhaisne th see q WueFnSceT paradigm, reordering models under any of those constraints can be integrated into the cross-lingual language model. [sent-247, score-0.688]
</p><p>65 wr1:w1/P(r1)wr2:w2/P(r2|r1)wr3:w3/P(r3|r2) Figure 2: An example of reordering WFST Ωr implementing the phrasal reordering model under the first order Markov assumption. [sent-248, score-1.046]
</p><p>66 3 Phrase-to-Phrase Transduction Model Once the phrase sequence of the Lw transcript is reordered into the Lv transcript order, we use the phrase-to-phrase transduction model specified in Eq. [sent-250, score-0.581]
</p><p>67 Given sufficient parallel training data, the contextdependent phrase-to-phrase transduction model can be estimated using the GIATI method (Casacuberta and Vidal, 2004). [sent-252, score-0.208]
</p><p>68 However, for the translation task with scarce training data, the contextdependent transduction probabilities may not be reliably estimated. [sent-253, score-0.207]
</p><p>69 Therefore, we assume that a phrase v˜k is generated independently by each phrase w˜ rk . [sent-254, score-0.382]
</p><p>70 C(˜ vk, w˜ rk ) is the number of times that phrase v˜k is  aligned to w˜ rk in the parallel corpus. [sent-255, score-0.624]
</p><p>71 This model can be implemented by a WFST Tvw which transduces v˜k to w˜ rk . [sent-256, score-0.265]
</p><p>72 4  (6)  ReconstruPction Model  Reconstruction model P(v1I|˜ v1K, r1K, w˜ 1K, w1J) operates in the opposite directi|o v˜n as the segmentation 6For simplicity, reordering shown there. [sent-259, score-0.553]
</p><p>73 L is a lexicon transducer which maps context-independent phone sequences to word strings restricted to the input symbols of the crosslingual language model transducer Gcl. [sent-274, score-0.239]
</p><p>74 Before decoding, the recognition transducer ASR can be optimized by a determinization operation right after each composition. [sent-279, score-0.169]
</p><p>75 The parallel transcriptions of the training set constitute a parallel corpus, which includes Cantonese transcription (man-  ual transcription) of 106k words and Mandarin transcription (Hansard7 transcription) of 80k words. [sent-436, score-0.522]
</p><p>76 of substitutions, insertions, deletions and inversions identified in the parallel corpus with different segmentation order s. [sent-440, score-0.189]
</p><p>77 2 Decoding and Evaluation Method Decoding of the speech recognition search space ASR is performed by T3 Decoder (Dixon et al. [sent-452, score-0.212]
</p><p>78 The optimal value depends on the language Table 3: WER and BLEU score for decoding results of H ◦ C ◦ L ◦ G, H ◦ C ◦ L ◦ π(Gcl) without reordering, and THa b◦l Ce 3 ◦: LW ◦E πR( aGncdl) B wLEithU reordering duencdoedri vnagr iroeususl ctso onsft Hrain ◦tCs . [sent-476, score-0.588]
</p><p>79 Thirdly, the number of reordering permutations or paths are formidable when the reordering distance L is long as suggested by Table 1. [sent-484, score-1.146]
</p><p>80 Therefore, we apply histogram pruning to reordering paths, which only maintains top N most likely ones. [sent-485, score-0.503]
</p><p>81 5  Experimental Results  The evaluation results of the proposed cross-lingual language models Gcl with reordering under various  constraints are presented in Table 3, where Gcl = Ts ◦ G = T3 ◦ G. [sent-487, score-0.688]
</p><p>82 8 In general, reordering has a signifTica◦nGt Ge =ffec Tt on enhancing the performance of recognition and translation in the sense of WER reduction and BLEU improvement. [sent-488, score-0.686]
</p><p>83 Compared with the cross-lingual language model without reordering, the cross-lingual language model with reordering under local constraints gives 0. [sent-489, score-0.748]
</p><p>84 The cross-lingual language model with reordering under IBM constraints gives 0. [sent-492, score-0.688]
</p><p>85 The cross-lingual language model with reordering under ITG constraints yields the best performance, with 0. [sent-495, score-0.688]
</p><p>86 s  = 3 because it works  774 6  Conclusions  We have proposed cross-lingual language modeling with phrase-level syntactic reordering for lowresource speech recognition. [sent-500, score-0.721]
</p><p>87 With a cross-lingual language model, our ASR system can decode speech into transcriptions, either in a resource-poor language or a resource-rich language, using a single WFST-based speech decoder. [sent-507, score-0.324]
</p><p>88 We have presented a first end-to-end WFST source to target language transcription and translation system with syntactic reordering and global optimization. [sent-508, score-0.786]
</p><p>89 Our work is the first to use ITG constraints for the syntactic reordering in such an in-  tegrated system. [sent-509, score-0.689]
</p><p>90 We also did comparative study of ITG constraints, IBM constraints and local constraints in the reordering model, for completeness. [sent-510, score-0.849]
</p><p>91 Experiments on Cantonese recognition and Cantonese to Mandarin translation tasks have shown that our proposed cross-lingual language model substantially improves the performance of the recognition and translation. [sent-513, score-0.259]
</p><p>92 Even though the objective of our work is for  speech recognition, our proposed cross-lingual language modeling can be easily applied to speech translation of other language pairs for efficient direct decoding from source speech to target text. [sent-520, score-0.617]
</p><p>93 Efficient estimation of language model statistics of spontaneous speech via statistical transformation model. [sent-532, score-0.162]
</p><p>94 Comparing reordering constraints for smt using efficient bleu oracle computation. [sent-598, score-0.748]
</p><p>95 Development of a wfst based speech recognition system for a resource deficient language using machine translation. [sent-635, score-0.398]
</p><p>96 A weighted finite state transducer translation template model for statistical machine translation. [sent-688, score-0.166]
</p><p>97 Toward a parallel corpus of spoken cantonese and written chinese. [sent-693, score-0.401]
</p><p>98 Integrating speech recognition and machine translation: Where do we stand? [sent-706, score-0.212]
</p><p>99 Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. [sent-782, score-0.213]
</p><p>100 A comparative study on reordering constraints in statistical machine translation. [sent-788, score-0.66]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reordering', 0.503), ('cantonese', 0.285), ('rk', 0.238), ('mandarin', 0.204), ('itg', 0.184), ('wer', 0.184), ('transcript', 0.184), ('lv', 0.176), ('lw', 0.17), ('wfst', 0.158), ('constraints', 0.157), ('transcription', 0.151), ('gcl', 0.142), ('permutations', 0.14), ('speech', 0.134), ('sk', 0.123), ('asr', 0.105), ('transduction', 0.1), ('ibm', 0.093), ('transducer', 0.091), ('bleu', 0.088), ('tvw', 0.079), ('permutation', 0.079), ('recognition', 0.078), ('parallel', 0.076), ('translation', 0.075), ('acoustics', 0.074), ('mohri', 0.074), ('phrase', 0.072), ('transcriptions', 0.068), ('acoustic', 0.064), ('akita', 0.063), ('inversions', 0.063), ('yk', 0.061), ('sw', 0.059), ('decoding', 0.057), ('casacuberta', 0.054), ('signal', 0.051), ('segmentation', 0.05), ('discrepancies', 0.049), ('zens', 0.049), ('hori', 0.047), ('jensson', 0.047), ('kanthak', 0.047), ('nitg', 0.047), ('oonishi', 0.047), ('resourcerich', 0.047), ('vk', 0.047), ('sequence', 0.041), ('matusov', 0.041), ('resourcepoor', 0.041), ('phrasal', 0.04), ('spoken', 0.04), ('khudanpur', 0.038), ('neubig', 0.037), ('rv', 0.037), ('ieee', 0.037), ('inversion', 0.037), ('dreyer', 0.034), ('languages', 0.033), ('composition', 0.032), ('hours', 0.032), ('local', 0.032), ('barcucci', 0.032), ('clarkson', 0.032), ('contextdependent', 0.032), ('contextindependent', 0.032), ('dixon', 0.032), ('gordon', 0.032), ('iwano', 0.032), ('nibm', 0.032), ('nlocal', 0.032), ('saon', 0.032), ('sinitic', 0.032), ('understudy', 0.032), ('wfsts', 0.032), ('reconstruction', 0.03), ('reduction', 0.03), ('absolute', 0.029), ('segmented', 0.029), ('syntactic', 0.029), ('kong', 0.029), ('crosslingual', 0.029), ('language', 0.028), ('substitution', 0.028), ('ney', 0.027), ('coefficients', 0.027), ('deletion', 0.027), ('modeling', 0.027), ('duction', 0.027), ('hindi', 0.027), ('reorder', 0.027), ('transduces', 0.027), ('vw', 0.027), ('kim', 0.027), ('insertion', 0.026), ('och', 0.026), ('kumar', 0.025), ('indian', 0.025), ('monotonic', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="31-tfidf-1" href="./emnlp-2012-Cross-Lingual_Language_Modeling_with_Syntactic_Reordering_for_Low-Resource_Speech_Recognition.html">31 emnlp-2012-Cross-Lingual Language Modeling with Syntactic Reordering for Low-Resource Speech Recognition</a></p>
<p>Author: Ping Xu ; Pascale Fung</p><p>Abstract: This paper proposes cross-lingual language modeling for transcribing source resourcepoor languages and translating them into target resource-rich languages if necessary. Our focus is to improve the speech recognition performance of low-resource languages by leveraging the language model statistics from resource-rich languages. The most challenging work of cross-lingual language modeling is to solve the syntactic discrepancies between the source and target languages. We therefore propose syntactic reordering for cross-lingual language modeling, and present a first result that compares inversion transduction grammar (ITG) reordering constraints to IBM and local constraints in an integrated speech transcription and translation system. Evaluations on resource-poor Cantonese speech transcription and Cantonese to resource-rich Mandarin translation tasks show that our proposed approach improves the system performance significantly, up to 3.4% relative WER reduction in Cantonese transcription and 13.3% relative bilingual evaluation understudy (BLEU) score improvement in Mandarin transcription compared with the system without reordering.</p><p>2 0.32976928 <a title="31-tfidf-2" href="./emnlp-2012-Inducing_a_Discriminative_Parser_to_Optimize_Machine_Translation_Reordering.html">67 emnlp-2012-Inducing a Discriminative Parser to Optimize Machine Translation Reordering</a></p>
<p>Author: Graham Neubig ; Taro Watanabe ; Shinsuke Mori</p><p>Abstract: This paper proposes a method for learning a discriminative parser for machine translation reordering using only aligned parallel text. This is done by treating the parser’s derivation tree as a latent variable in a model that is trained to maximize reordering accuracy. We demonstrate that efficient large-margin training is possible by showing that two measures of reordering accuracy can be factored over the parse tree. Using this model in the pre-ordering framework results in significant gains in translation accuracy over standard phrasebased SMT and previously proposed unsupervised syntax induction methods.</p><p>3 0.17800505 <a title="31-tfidf-3" href="./emnlp-2012-Forced_Derivation_Tree_based_Model_Training_to_Statistical_Machine_Translation.html">54 emnlp-2012-Forced Derivation Tree based Model Training to Statistical Machine Translation</a></p>
<p>Author: Nan Duan ; Mu Li ; Ming Zhou</p><p>Abstract: A forced derivation tree (FDT) of a sentence pair {f, e} denotes a derivation tree that can tpraainrsl {afte, f} idnetono itetss a acc duerraivtea target etrea tnhsaltat cioann e. In this paper, we present an approach that leverages structured knowledge contained in FDTs to train component models for statistical machine translation (SMT) systems. We first describe how to generate different FDTs for each sentence pair in training corpus, and then present how to infer the optimal FDTs based on their derivation and alignment qualities. As the first step in this line of research, we verify the effectiveness of our approach in a BTGbased phrasal system, and propose four FDTbased component models. Experiments are carried out on large scale English-to-Japanese and Chinese-to-English translation tasks, and significant improvements are reported on both translation quality and alignment quality.</p><p>4 0.11904379 <a title="31-tfidf-4" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>Author: Wang Ling ; Joao Graca ; Isabel Trancoso ; Alan Black</p><p>Abstract: Phrase-based machine translation models have shown to yield better translations than Word-based models, since phrase pairs encode the contextual information that is needed for a more accurate translation. However, many phrase pairs do not encode any relevant context, which means that the translation event encoded in that phrase pair is led by smaller translation events that are independent from each other, and can be found on smaller phrase pairs, with little or no loss in translation accuracy. In this work, we propose a relative entropy model for translation models, that measures how likely a phrase pair encodes a translation event that is derivable using smaller translation events with similar probabilities. This model is then applied to phrase table pruning. Tests show that considerable amounts of phrase pairs can be excluded, without much impact on the transla- . tion quality. In fact, we show that better translations can be obtained using our pruned models, due to the compression of the search space during decoding.</p><p>5 0.098666109 <a title="31-tfidf-5" href="./emnlp-2012-A_Systematic_Comparison_of_Phrase_Table_Pruning_Techniques.html">11 emnlp-2012-A Systematic Comparison of Phrase Table Pruning Techniques</a></p>
<p>Author: Richard Zens ; Daisy Stanton ; Peng Xu</p><p>Abstract: When trained on very large parallel corpora, the phrase table component of a machine translation system grows to consume vast computational resources. In this paper, we introduce a novel pruning criterion that places phrase table pruning on a sound theoretical foundation. Systematic experiments on four language pairs under various data conditions show that our principled approach is superior to existing ad hoc pruning methods.</p><p>6 0.082927428 <a title="31-tfidf-6" href="./emnlp-2012-Document-Wide_Decoding_for_Phrase-Based_Statistical_Machine_Translation.html">35 emnlp-2012-Document-Wide Decoding for Phrase-Based Statistical Machine Translation</a></p>
<p>7 0.069868982 <a title="31-tfidf-7" href="./emnlp-2012-A_Bayesian_Model_for_Learning_SCFGs_with_Discontiguous_Rules.html">1 emnlp-2012-A Bayesian Model for Learning SCFGs with Discontiguous Rules</a></p>
<p>8 0.065400451 <a title="31-tfidf-8" href="./emnlp-2012-Locally_Training_the_Log-Linear_Model_for_SMT.html">86 emnlp-2012-Locally Training the Log-Linear Model for SMT</a></p>
<p>9 0.063783742 <a title="31-tfidf-9" href="./emnlp-2012-Name_Phylogeny%3A_A_Generative_Model_of_String_Variation.html">96 emnlp-2012-Name Phylogeny: A Generative Model of String Variation</a></p>
<p>10 0.060378056 <a title="31-tfidf-10" href="./emnlp-2012-Assessment_of_ESL_Learners%27_Syntactic_Competence_Based_on_Similarity_Measures.html">21 emnlp-2012-Assessment of ESL Learners' Syntactic Competence Based on Similarity Measures</a></p>
<p>11 0.059540063 <a title="31-tfidf-11" href="./emnlp-2012-Translation_Model_Based_Cross-Lingual_Language_Model_Adaptation%3A_from_Word_Models_to_Phrase_Models.html">128 emnlp-2012-Translation Model Based Cross-Lingual Language Model Adaptation: from Word Models to Phrase Models</a></p>
<p>12 0.05748263 <a title="31-tfidf-12" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>13 0.05525158 <a title="31-tfidf-13" href="./emnlp-2012-Transforming_Trees_to_Improve_Syntactic_Convergence.html">127 emnlp-2012-Transforming Trees to Improve Syntactic Convergence</a></p>
<p>14 0.05498505 <a title="31-tfidf-14" href="./emnlp-2012-Probabilistic_Finite_State_Machines_for_Regression-based_MT_Evaluation.html">108 emnlp-2012-Probabilistic Finite State Machines for Regression-based MT Evaluation</a></p>
<p>15 0.050296191 <a title="31-tfidf-15" href="./emnlp-2012-Left-to-Right_Tree-to-String_Decoding_with_Prediction.html">82 emnlp-2012-Left-to-Right Tree-to-String Decoding with Prediction</a></p>
<p>16 0.046623886 <a title="31-tfidf-16" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<p>17 0.045799196 <a title="31-tfidf-17" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>18 0.044797041 <a title="31-tfidf-18" href="./emnlp-2012-Language_Model_Rest_Costs_and_Space-Efficient_Storage.html">74 emnlp-2012-Language Model Rest Costs and Space-Efficient Storage</a></p>
<p>19 0.044419125 <a title="31-tfidf-19" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>20 0.043647099 <a title="31-tfidf-20" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.173), (1, -0.183), (2, -0.196), (3, -0.017), (4, -0.099), (5, -0.085), (6, -0.147), (7, 0.073), (8, 0.034), (9, -0.144), (10, -0.114), (11, -0.049), (12, -0.072), (13, 0.025), (14, -0.173), (15, 0.036), (16, 0.045), (17, 0.065), (18, 0.268), (19, -0.005), (20, 0.076), (21, -0.141), (22, -0.12), (23, -0.188), (24, 0.012), (25, 0.095), (26, 0.305), (27, 0.065), (28, 0.137), (29, 0.005), (30, 0.089), (31, -0.021), (32, -0.091), (33, 0.006), (34, 0.079), (35, -0.132), (36, 0.056), (37, -0.003), (38, -0.064), (39, -0.004), (40, 0.055), (41, 0.022), (42, 0.026), (43, -0.017), (44, 0.054), (45, 0.047), (46, -0.101), (47, 0.052), (48, -0.008), (49, -0.099)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94318473 <a title="31-lsi-1" href="./emnlp-2012-Cross-Lingual_Language_Modeling_with_Syntactic_Reordering_for_Low-Resource_Speech_Recognition.html">31 emnlp-2012-Cross-Lingual Language Modeling with Syntactic Reordering for Low-Resource Speech Recognition</a></p>
<p>Author: Ping Xu ; Pascale Fung</p><p>Abstract: This paper proposes cross-lingual language modeling for transcribing source resourcepoor languages and translating them into target resource-rich languages if necessary. Our focus is to improve the speech recognition performance of low-resource languages by leveraging the language model statistics from resource-rich languages. The most challenging work of cross-lingual language modeling is to solve the syntactic discrepancies between the source and target languages. We therefore propose syntactic reordering for cross-lingual language modeling, and present a first result that compares inversion transduction grammar (ITG) reordering constraints to IBM and local constraints in an integrated speech transcription and translation system. Evaluations on resource-poor Cantonese speech transcription and Cantonese to resource-rich Mandarin translation tasks show that our proposed approach improves the system performance significantly, up to 3.4% relative WER reduction in Cantonese transcription and 13.3% relative bilingual evaluation understudy (BLEU) score improvement in Mandarin transcription compared with the system without reordering.</p><p>2 0.80412877 <a title="31-lsi-2" href="./emnlp-2012-Inducing_a_Discriminative_Parser_to_Optimize_Machine_Translation_Reordering.html">67 emnlp-2012-Inducing a Discriminative Parser to Optimize Machine Translation Reordering</a></p>
<p>Author: Graham Neubig ; Taro Watanabe ; Shinsuke Mori</p><p>Abstract: This paper proposes a method for learning a discriminative parser for machine translation reordering using only aligned parallel text. This is done by treating the parser’s derivation tree as a latent variable in a model that is trained to maximize reordering accuracy. We demonstrate that efficient large-margin training is possible by showing that two measures of reordering accuracy can be factored over the parse tree. Using this model in the pre-ordering framework results in significant gains in translation accuracy over standard phrasebased SMT and previously proposed unsupervised syntax induction methods.</p><p>3 0.56578159 <a title="31-lsi-3" href="./emnlp-2012-Forced_Derivation_Tree_based_Model_Training_to_Statistical_Machine_Translation.html">54 emnlp-2012-Forced Derivation Tree based Model Training to Statistical Machine Translation</a></p>
<p>Author: Nan Duan ; Mu Li ; Ming Zhou</p><p>Abstract: A forced derivation tree (FDT) of a sentence pair {f, e} denotes a derivation tree that can tpraainrsl {afte, f} idnetono itetss a acc duerraivtea target etrea tnhsaltat cioann e. In this paper, we present an approach that leverages structured knowledge contained in FDTs to train component models for statistical machine translation (SMT) systems. We first describe how to generate different FDTs for each sentence pair in training corpus, and then present how to infer the optimal FDTs based on their derivation and alignment qualities. As the first step in this line of research, we verify the effectiveness of our approach in a BTGbased phrasal system, and propose four FDTbased component models. Experiments are carried out on large scale English-to-Japanese and Chinese-to-English translation tasks, and significant improvements are reported on both translation quality and alignment quality.</p><p>4 0.33775032 <a title="31-lsi-4" href="./emnlp-2012-Exploiting_Chunk-level_Features_to_Improve_Phrase_Chunking.html">45 emnlp-2012-Exploiting Chunk-level Features to Improve Phrase Chunking</a></p>
<p>Author: Junsheng Zhou ; Weiguang Qu ; Fen Zhang</p><p>Abstract: Most existing systems solved the phrase chunking task with the sequence labeling approaches, in which the chunk candidates cannot be treated as a whole during parsing process so that the chunk-level features cannot be exploited in a natural way. In this paper, we formulate phrase chunking as a joint segmentation and labeling task. We propose an efficient dynamic programming algorithm with pruning for decoding, which allows the direct use of the features describing the internal characteristics of chunk and the features capturing the correlations between adjacent chunks. A relaxed, online maximum margin training algorithm is used for learning. Within this framework, we explored a variety of effective feature representations for Chinese phrase chunking. The experimental results show that the use of chunk-level features can lead to significant performance improvement, and that our approach achieves state-of-the-art performance. In particular, our approach is much better at recognizing long and complicated phrases. 1</p><p>5 0.2743178 <a title="31-lsi-5" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>Author: Wang Ling ; Joao Graca ; Isabel Trancoso ; Alan Black</p><p>Abstract: Phrase-based machine translation models have shown to yield better translations than Word-based models, since phrase pairs encode the contextual information that is needed for a more accurate translation. However, many phrase pairs do not encode any relevant context, which means that the translation event encoded in that phrase pair is led by smaller translation events that are independent from each other, and can be found on smaller phrase pairs, with little or no loss in translation accuracy. In this work, we propose a relative entropy model for translation models, that measures how likely a phrase pair encodes a translation event that is derivable using smaller translation events with similar probabilities. This model is then applied to phrase table pruning. Tests show that considerable amounts of phrase pairs can be excluded, without much impact on the transla- . tion quality. In fact, we show that better translations can be obtained using our pruned models, due to the compression of the search space during decoding.</p><p>6 0.26541135 <a title="31-lsi-6" href="./emnlp-2012-Probabilistic_Finite_State_Machines_for_Regression-based_MT_Evaluation.html">108 emnlp-2012-Probabilistic Finite State Machines for Regression-based MT Evaluation</a></p>
<p>7 0.26488999 <a title="31-lsi-7" href="./emnlp-2012-Locally_Training_the_Log-Linear_Model_for_SMT.html">86 emnlp-2012-Locally Training the Log-Linear Model for SMT</a></p>
<p>8 0.24577151 <a title="31-lsi-8" href="./emnlp-2012-Source_Language_Adaptation_for_Resource-Poor_Machine_Translation.html">118 emnlp-2012-Source Language Adaptation for Resource-Poor Machine Translation</a></p>
<p>9 0.23685873 <a title="31-lsi-9" href="./emnlp-2012-A_Bayesian_Model_for_Learning_SCFGs_with_Discontiguous_Rules.html">1 emnlp-2012-A Bayesian Model for Learning SCFGs with Discontiguous Rules</a></p>
<p>10 0.20824122 <a title="31-lsi-10" href="./emnlp-2012-Name_Phylogeny%3A_A_Generative_Model_of_String_Variation.html">96 emnlp-2012-Name Phylogeny: A Generative Model of String Variation</a></p>
<p>11 0.20774758 <a title="31-lsi-11" href="./emnlp-2012-Assessment_of_ESL_Learners%27_Syntactic_Competence_Based_on_Similarity_Measures.html">21 emnlp-2012-Assessment of ESL Learners' Syntactic Competence Based on Similarity Measures</a></p>
<p>12 0.20149416 <a title="31-lsi-12" href="./emnlp-2012-A_Systematic_Comparison_of_Phrase_Table_Pruning_Techniques.html">11 emnlp-2012-A Systematic Comparison of Phrase Table Pruning Techniques</a></p>
<p>13 0.19675004 <a title="31-lsi-13" href="./emnlp-2012-Translation_Model_Based_Cross-Lingual_Language_Model_Adaptation%3A_from_Word_Models_to_Phrase_Models.html">128 emnlp-2012-Translation Model Based Cross-Lingual Language Model Adaptation: from Word Models to Phrase Models</a></p>
<p>14 0.1827631 <a title="31-lsi-14" href="./emnlp-2012-Syntactic_Surprisal_Affects_Spoken_Word_Duration_in_Conversational_Contexts.html">122 emnlp-2012-Syntactic Surprisal Affects Spoken Word Duration in Conversational Contexts</a></p>
<p>15 0.18090333 <a title="31-lsi-15" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>16 0.17970604 <a title="31-lsi-16" href="./emnlp-2012-Document-Wide_Decoding_for_Phrase-Based_Statistical_Machine_Translation.html">35 emnlp-2012-Document-Wide Decoding for Phrase-Based Statistical Machine Translation</a></p>
<p>17 0.17955005 <a title="31-lsi-17" href="./emnlp-2012-A_Statistical_Relational_Learning_Approach_to_Identifying_Evidence_Based_Medicine_Categories.html">10 emnlp-2012-A Statistical Relational Learning Approach to Identifying Evidence Based Medicine Categories</a></p>
<p>18 0.15916789 <a title="31-lsi-18" href="./emnlp-2012-Transforming_Trees_to_Improve_Syntactic_Convergence.html">127 emnlp-2012-Transforming Trees to Improve Syntactic Convergence</a></p>
<p>19 0.15495498 <a title="31-lsi-19" href="./emnlp-2012-Generalizing_Sub-sentential_Paraphrase_Acquisition_across_Original_Signal_Type_of_Text_Pairs.html">58 emnlp-2012-Generalizing Sub-sentential Paraphrase Acquisition across Original Signal Type of Text Pairs</a></p>
<p>20 0.15376092 <a title="31-lsi-20" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.023), (25, 0.013), (34, 0.093), (41, 0.013), (45, 0.015), (60, 0.074), (63, 0.059), (64, 0.013), (70, 0.022), (74, 0.076), (76, 0.026), (80, 0.015), (81, 0.019), (86, 0.02), (96, 0.395)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74832416 <a title="31-lda-1" href="./emnlp-2012-Cross-Lingual_Language_Modeling_with_Syntactic_Reordering_for_Low-Resource_Speech_Recognition.html">31 emnlp-2012-Cross-Lingual Language Modeling with Syntactic Reordering for Low-Resource Speech Recognition</a></p>
<p>Author: Ping Xu ; Pascale Fung</p><p>Abstract: This paper proposes cross-lingual language modeling for transcribing source resourcepoor languages and translating them into target resource-rich languages if necessary. Our focus is to improve the speech recognition performance of low-resource languages by leveraging the language model statistics from resource-rich languages. The most challenging work of cross-lingual language modeling is to solve the syntactic discrepancies between the source and target languages. We therefore propose syntactic reordering for cross-lingual language modeling, and present a first result that compares inversion transduction grammar (ITG) reordering constraints to IBM and local constraints in an integrated speech transcription and translation system. Evaluations on resource-poor Cantonese speech transcription and Cantonese to resource-rich Mandarin translation tasks show that our proposed approach improves the system performance significantly, up to 3.4% relative WER reduction in Cantonese transcription and 13.3% relative bilingual evaluation understudy (BLEU) score improvement in Mandarin transcription compared with the system without reordering.</p><p>2 0.35195222 <a title="31-lda-2" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>Author: Shujie Liu ; Chi-Ho Li ; Mu Li ; Ming Zhou</p><p>Abstract: The training of most syntactic SMT approaches involves two essential components, word alignment and monolingual parser. In the current state of the art these two components are mutually independent, thus causing problems like lack of rule generalization, and violation of syntactic correspondence in translation rules. In this paper, we propose two ways of re-training monolingual parser with the target of maximizing the consistency between parse trees and alignment matrices. One is targeted self-training with a simple evaluation function; the other is based on training data selection from forced alignment of bilingual data. We also propose an auxiliary method for boosting alignment quality, by symmetrizing alignment matrices with respect to parse trees. The best combination of these novel methods achieves 3 Bleu point gain in an IWSLT task and more than 1 Bleu point gain in NIST tasks. 1</p><p>3 0.35147813 <a title="31-lda-3" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>Author: Wang Ling ; Joao Graca ; Isabel Trancoso ; Alan Black</p><p>Abstract: Phrase-based machine translation models have shown to yield better translations than Word-based models, since phrase pairs encode the contextual information that is needed for a more accurate translation. However, many phrase pairs do not encode any relevant context, which means that the translation event encoded in that phrase pair is led by smaller translation events that are independent from each other, and can be found on smaller phrase pairs, with little or no loss in translation accuracy. In this work, we propose a relative entropy model for translation models, that measures how likely a phrase pair encodes a translation event that is derivable using smaller translation events with similar probabilities. This model is then applied to phrase table pruning. Tests show that considerable amounts of phrase pairs can be excluded, without much impact on the transla- . tion quality. In fact, we show that better translations can be obtained using our pruned models, due to the compression of the search space during decoding.</p><p>4 0.34776777 <a title="31-lda-4" href="./emnlp-2012-Unambiguity_Regularization_for_Unsupervised_Learning_of_Probabilistic_Grammars.html">130 emnlp-2012-Unambiguity Regularization for Unsupervised Learning of Probabilistic Grammars</a></p>
<p>Author: Kewei Tu ; Vasant Honavar</p><p>Abstract: We introduce a novel approach named unambiguity regularization for unsupervised learning of probabilistic natural language grammars. The approach is based on the observation that natural language is remarkably unambiguous in the sense that only a tiny portion of the large number of possible parses of a natural language sentence are syntactically valid. We incorporate an inductive bias into grammar learning in favor of grammars that lead to unambiguous parses on natural language sentences. The resulting family of algorithms includes the expectation-maximization algorithm (EM) and its variant, Viterbi EM, as well as a so-called softmax-EM algorithm. The softmax-EM algorithm can be implemented with a simple and computationally efficient extension to standard EM. In our experiments of unsupervised dependency grammar learn- ing, we show that unambiguity regularization is beneficial to learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art.</p><p>5 0.34688467 <a title="31-lda-5" href="./emnlp-2012-A_Systematic_Comparison_of_Phrase_Table_Pruning_Techniques.html">11 emnlp-2012-A Systematic Comparison of Phrase Table Pruning Techniques</a></p>
<p>Author: Richard Zens ; Daisy Stanton ; Peng Xu</p><p>Abstract: When trained on very large parallel corpora, the phrase table component of a machine translation system grows to consume vast computational resources. In this paper, we introduce a novel pruning criterion that places phrase table pruning on a sound theoretical foundation. Systematic experiments on four language pairs under various data conditions show that our principled approach is superior to existing ad hoc pruning methods.</p><p>6 0.34233522 <a title="31-lda-6" href="./emnlp-2012-A_Novel_Discriminative_Framework_for_Sentence-Level_Discourse_Analysis.html">7 emnlp-2012-A Novel Discriminative Framework for Sentence-Level Discourse Analysis</a></p>
<p>7 0.34029734 <a title="31-lda-7" href="./emnlp-2012-Forced_Derivation_Tree_based_Model_Training_to_Statistical_Machine_Translation.html">54 emnlp-2012-Forced Derivation Tree based Model Training to Statistical Machine Translation</a></p>
<p>8 0.33892038 <a title="31-lda-8" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>9 0.33719409 <a title="31-lda-9" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>10 0.33710936 <a title="31-lda-10" href="./emnlp-2012-Left-to-Right_Tree-to-String_Decoding_with_Prediction.html">82 emnlp-2012-Left-to-Right Tree-to-String Decoding with Prediction</a></p>
<p>11 0.33566582 <a title="31-lda-11" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>12 0.33508903 <a title="31-lda-12" href="./emnlp-2012-Exploiting_Chunk-level_Features_to_Improve_Phrase_Chunking.html">45 emnlp-2012-Exploiting Chunk-level Features to Improve Phrase Chunking</a></p>
<p>13 0.33492953 <a title="31-lda-13" href="./emnlp-2012-Inducing_a_Discriminative_Parser_to_Optimize_Machine_Translation_Reordering.html">67 emnlp-2012-Inducing a Discriminative Parser to Optimize Machine Translation Reordering</a></p>
<p>14 0.33466282 <a title="31-lda-14" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>15 0.33383429 <a title="31-lda-15" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<p>16 0.33366325 <a title="31-lda-16" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>17 0.33182082 <a title="31-lda-17" href="./emnlp-2012-N-gram-based_Tense_Models_for_Statistical_Machine_Translation.html">95 emnlp-2012-N-gram-based Tense Models for Statistical Machine Translation</a></p>
<p>18 0.33177546 <a title="31-lda-18" href="./emnlp-2012-Syntactic_Surprisal_Affects_Spoken_Word_Duration_in_Conversational_Contexts.html">122 emnlp-2012-Syntactic Surprisal Affects Spoken Word Duration in Conversational Contexts</a></p>
<p>19 0.33158475 <a title="31-lda-19" href="./emnlp-2012-A_Bayesian_Model_for_Learning_SCFGs_with_Discontiguous_Rules.html">1 emnlp-2012-A Bayesian Model for Learning SCFGs with Discontiguous Rules</a></p>
<p>20 0.3304078 <a title="31-lda-20" href="./emnlp-2012-Type-Supervised_Hidden_Markov_Models_for_Part-of-Speech_Tagging_with_Incomplete_Tag_Dictionaries.html">129 emnlp-2012-Type-Supervised Hidden Markov Models for Part-of-Speech Tagging with Incomplete Tag Dictionaries</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
