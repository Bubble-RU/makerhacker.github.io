<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-105" href="#">emnlp2012-105</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</h1>
<br/><p>Source: <a title="emnlp-2012-105-pdf" href="http://aclweb.org/anthology//D/D12/D12-1096.pdf">pdf</a></p><p>Author: Jonathan K. Kummerfeld ; David Hall ; James R. Curran ; Dan Klein</p><p>Abstract: Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors. We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for stateof-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.</p><p>Reference: <a title="emnlp-2012-105-reference" href="../emnlp2012_reference/emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. [sent-6, score-0.797]
</p><p>2 We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for stateof-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text. [sent-7, score-0.52]
</p><p>3 Broad investigations of parser errors beyond the PARSEVAL metric (Abney et al. [sent-11, score-0.478]
</p><p>4 In all of these cases, the analysis has not taken into consideration how a set of errors can have a common cause, e. [sent-17, score-0.287]
</p><p>5 au tree are repaired using subtree movement, node creation, and node deletion. [sent-25, score-0.418]
</p><p>6 Each step in the process is  then associated with a linguistically meaningful error type, based on factors such as the node that is moved, its siblings, and parents. [sent-26, score-0.402]
</p><p>7 Using our method we analyse the output of thirteen constituency parsers on newswire. [sent-27, score-0.299]
</p><p>8 However, other significant types have not received as much attention, such as clause attachment and modifier attachment. [sent-29, score-0.312]
</p><p>9 We are able to decompose the drop in performance and show that a disproportionate number of the extra errors are due to coordination and clause attachment. [sent-33, score-0.698]
</p><p>10 This work presents a comprehensive investigation of parser behaviour in terms of linguistically meaningful errors. [sent-34, score-0.38]
</p><p>11 By applying our method to multiple parsers and domains we are able to answer questions about parser behaviour that were previously only approachable through approximate measures, such as  counts of node errors. [sent-35, score-0.583]
</p><p>12 We show which errors have been reduced over the past fifteen years of parsing research; where rerankers are making their gains and where they are not exploiting the full potential of kbest lists; and what types of errors arise when moving out-of-domain. [sent-36, score-0.783]
</p><p>13 Lc a2n0g1u2ag Aes Psorcoicaetsiosin fgo arn Cdo Cmopmutpauti oantiaoln Lailn Ngautiustriacls 2  Background  Most attempts to understand the behaviour of constituency parsers have focused on overall evaluation metrics. [sent-42, score-0.331]
</p><p>14 Intrinsic evaluation with PARSEVAL, which calculates precision and recall over labeled tree nodes, is a useful indicator of overall performance, but does not pinpoint which structures the parser has most difficulty with. [sent-44, score-0.297]
</p><p>15 Even when the breakdown for par-  ticular node types is presented (e. [sent-45, score-0.284]
</p><p>16 Collins, 2003), the interaction between node errors is not taken into account. [sent-47, score-0.443]
</p><p>17 For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake. [sent-48, score-0.676]
</p><p>18 There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al. [sent-49, score-0.45]
</p><p>19 By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. [sent-52, score-0.49]
</p><p>20 , 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. [sent-57, score-0.284]
</p><p>21 , 2010), as well as work on finding relations between errors (Hara et al. [sent-60, score-0.287]
</p><p>22 , 2009), and breaking down errors by a range offactors (McDonald and  Nivre, 2007). [sent-61, score-0.325]
</p><p>23 However, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parser’s output (Clark and Hockenmaier, 2002). [sent-62, score-0.305]
</p><p>24 The work we present here differs from existing approaches by directly and automatically classifying errors into meaningful types. [sent-68, score-0.342]
</p><p>25 This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. [sent-69, score-0.393]
</p><p>26 3  Parsers  Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. [sent-70, score-0.307]
</p><p>27 We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. [sent-89, score-0.676]
</p><p>28 4  Error Classification  While the statistics in Table 1 give a sense of overall parser performance they do not provide linguistically meaningful intuition for the source of remaining errors. [sent-99, score-0.308]
</p><p>29 Breaking down the remaining errors by node type is not particularly informative, as a single attachment error can cause multiple node errors, many of which are for unrelated node types. [sent-100, score-1.166]
</p><p>30 For example, in Figure 1 there is a PP attachment error  that causes seven bracket errors (extra S, NP, PP, and NP, missing S, NP, and PP). [sent-101, score-0.968]
</p><p>31 Determining that these correspond to a PP attachment error from just the labels of the missing and extra nodes is difficult. [sent-102, score-0.775]
</p><p>32 In this figure and those that follow the top tree is the incorrect parse and the bottom tree is the correct parse. [sent-106, score-0.298]
</p><p>33 Bold, boxed nodes are either extra (marked in the incorrect tree) or missing (marked in the correct tree). [sent-107, score-0.455]
</p><p>34 Second, the transformation are classified into error types such as PP attachment and coordination. [sent-110, score-0.449]
</p><p>35 For example, in Figure 1 there are four extra nodes that all cover spans ending at Applied in 1986: S, NP, PP, NP. [sent-118, score-0.391]
</p><p>36 There are also three missing nodes with spans ending between Applied and in: PP, NP, and S. [sent-119, score-0.364]
</p><p>37 Figure 2 depicts these errors as spans, showing that this case fits three criteria: (1) there are a set of extra spans all ending at the same point, (2) there are a set of missing spans all ending at the same point, and (3) the extra spans cross the missing spans, extending beyond their end-point. [sent-120, score-1.422]
</p><p>38 This indicates that the node starting after Applied is attaching too low and should be  moved up, outside all of the extra nodes. [sent-121, score-0.418]
</p><p>39 In the example this is done by moving the node spanning in 1986 up in the tree until it is outside of all the extra spans. [sent-124, score-0.508]
</p><p>40 1051  named chief executive officer of Applied in 1986 Figure 2: Templates are defined in terms of extra and missing spans, shown here with unbroken lines above and dashed lines below, respectively. [sent-127, score-0.53]
</p><p>41 This is an example of a set of extra spans that cross a set of missing spans (which in both cases all end at the same position). [sent-128, score-0.577]
</p><p>42 If the last two words are moved, two of the extra spans will match the two missing spans. [sent-129, score-0.47]
</p><p>43 The other extra span is deleted during the move as it creates an NP→NP unary production. [sent-130, score-0.317]
</p><p>44 errors, as there are three cases in which an extra node is present that matches a missing node once the PP is moved. [sent-131, score-0.675]
</p><p>45 All of these errors are placed in a single group and information about the nearby tree structure before and after the transformation is recorded. [sent-132, score-0.521]
</p><p>46 We continue to make passes through the list until no errors are corrected on a pass. [sent-133, score-0.287]
</p><p>47 For each remaining node error an individual error group is created. [sent-134, score-0.46]
</p><p>48 They cover a range of combinations of extra and missing spans, with further variation for whether crossing is occurring and if so whether the crossing bracket starts or ends in the middle of the correct bracket. [sent-136, score-0.519]
</p><p>49 2 Transformation Classification We began with a large set of node errors, in the first stage they were placed into groups, one group per tree transformation used to get from the test tree to the gold tree. [sent-139, score-0.496]
</p><p>50 This causes three node errors (extra NP, missing NP and VP). [sent-150, score-0.649]
</p><p>51 This causes six  node errors (extra S, VP, and VP, missing S, VP, and VP). [sent-152, score-0.649]
</p><p>52 Modifier Attachment Cases involving incorrectly placed adjectives and adverbs, including errors corrected by subtree movement and errors requiring only creation of a node, e. [sent-153, score-0.612]
</p><p>53 (NP (ADVP even more) severe setbacks) where there should be an extra ADVP node over even more severe. [sent-155, score-0.351]
</p><p>54 This causes six node errors (extra S, VP, and VP, missing S, VP and VP). [sent-168, score-0.649]
</p><p>55 This causes four node errors (extra PP and NP, missing NP and PP). [sent-173, score-0.649]
</p><p>56 Unary Mistakes involving unary productions that are not linked to a nearby error such as a matching extra or missing node. [sent-174, score-0.614]
</p><p>57 We do not include a breakdown by unary type, though we did find that clause labeling (S, SINV, etc) accounted for a large proportion of the errors. [sent-175, score-0.324]
</p><p>58 We form a single group for each NP that has one or more errors involving these types of nodes. [sent-178, score-0.333]
</p><p>59 Different label In many cases a node is present in the tree that spans the correct set of words, but has the wrong label, in which case we group the two node errors, (one extra, one missing), as a single error. [sent-179, score-0.571]
</p><p>60 Single word phrase A range of node errors that span a single word, with checks to ensure this is not linked to another error (e. [sent-180, score-0.572]
</p><p>61 Even for error types that can be measured by counting node errors or rule production errors, our approach has the advantage that we identify groups of errors with a single cause. [sent-187, score-0.9]
</p><p>62 For example, a missing unary production may correspond to an extra bracket that contains a subtree that attached incorrectly. [sent-188, score-0.634]
</p><p>63 3 Methodology We used sections 00 and 24 as development data while constructing the tree transformation and error group classification methods. [sent-190, score-0.363]
</p><p>64 Our evaluation is entirely focused on the errors of the parsers, so unless there is a particular construction that is unusually prevalent in section 23, we are not revealing any information about the test set that could bias future work. [sent-193, score-0.287]
</p><p>65 In Table 2 we consider the breakdown of parser PPClauseDiffModNP1-WordNP Parser  F-score  Best Charniak-RS Charniak-R Charniak-S Berkeley  Attach  Attach  Label  Attach  0. [sent-196, score-0.319]
</p><p>66 13  Table 2: Average number of bracket errors per sentence due to the top ten error types. [sent-230, score-0.524]
</p><p>67 12 bracket errors per sentence that are due to PP attachment. [sent-232, score-0.395]
</p><p>68 Ratio is the average number of node errors caused by each error we identify (i. [sent-236, score-0.572]
</p><p>69 The shaded area of each bar indicates the frequency of parse errors (i. [sent-240, score-0.326]
</p><p>70 The area filled in is  determined by the expected number of node errors per sentence that are attributed to that type of error. [sent-243, score-0.443]
</p><p>71 The average number of node errors per sentence for a completely full bar is indicated by the Worst row, and the value for a completely empty bar is indicated by the Best row. [sent-244, score-0.443]
</p><p>72 We use counts of node errors to make the contributions of each type of error more interpretable. [sent-248, score-0.572]
</p><p>73 As Table 3 shows, some errors typically cause only a single node error, where as others, such as coordination, generally cause several. [sent-249, score-0.443]
</p><p>74 single word phrase errors are second most important by number of groups (in Table 3), but seventh by total number of node errors (in Table 2). [sent-252, score-0.73]
</p><p>75 Interestingly, coordination is sixth on the list, though that is partly due to the fact that there are fewer coordination decisions to be made in the treebank. [sent-254, score-0.284]
</p><p>76 clause attachment errors and different label errors, the change has been more limited (24% and 29% reductions respectively). [sent-258, score-0.599]
</p><p>77 We investigated the breakdown of the different label errors by label, but no particular cases of la3This is indicated by the frequency of CCs and PPs in sections 02–21 of the treebank, 16,844 and 95,581 respectively. [sent-259, score-0.415]
</p><p>78 60 Table 4: Average number of bracket errors per sentence for a range of K-best list lengths using the Charniak parser with reranking and the self-trained model. [sent-297, score-0.67]
</p><p>79 One such pair is the Stanford parser, where the factored parser combines the unlexicalised parser with a lexicalised dependency parser. [sent-301, score-0.485]
</p><p>80 The pruning methods used in BUBS are particularly damaging for PP attachment errors and unary errors. [sent-305, score-0.691]
</p><p>81 Here we see gains on all error types, though particularly for clause attachment, modifier attachment and coordination, which fits with their observations. [sent-310, score-0.485]
</p><p>82 While there is improvement on all errors when using the reranker, there is very little additional gain beyond the first 5-10 parses. [sent-321, score-0.287]
</p><p>83 073  Table 5: Average number of node errors per word for a range of domains using the Charniak parser with reranking and the self-trained model. [sent-355, score-0.718]
</p><p>84 for the reranker may be due to the importance of the base parser output probability feature (which, by definition, decreases within the K-best list). [sent-357, score-0.299]
</p><p>85 The PP attachment improvement of the oracle is considerably higher than that of the reranker, particularly compared to the differences for other errors, suggesting that the reranker lacks the features necessary to make the decision better than the parser. [sent-361, score-0.446]
</p><p>86 Clegg and Shepherd (2005) evaluated parsers qualitatively on node types and rule productions. [sent-365, score-0.32]
</p><p>87 To provide a deeper understanding of the errors arising when parsing outside of the newswire domain, we analyse performance of the Charniak parser with reranker and self-trained model on the eight parts of the Brown corpus (Marcus et al. [sent-368, score-0.668]
</p><p>88 The variation in average sentence lengths skews the results for errors per sentences, and so in Table 5 we consider errors per word. [sent-371, score-0.622]
</p><p>89 The variation in average sentence lengths skew the results for errors per sentence. [sent-374, score-0.335]
</p><p>90 First, on the Brown datasets, while the general trend  is towards worse performance on all errors, NP internal structure is a notable exception and in some cases PP attachment and unaries are as well. [sent-377, score-0.292]
</p><p>91 In the other errors we see similar patterns across the corpora, except humour (Brown R), on which the parser is particularly bad at coordination and clause attachment. [sent-378, score-0.738]
</p><p>92 This typographical error is extremely difficult to handle for a parser trained only on well-formed text. [sent-390, score-0.32]
</p><p>93 Breaking the errors down by label we found that the majority of the new errors are missing or extra NPs over single words. [sent-392, score-0.937]
</p><p>94 Here the main problem appears to be temporal expressions, though there also appear to be a substantial number of errors that are also at the POS level, such as when NNP is assigned to ta in this case: . [sent-393, score-0.287]
</p><p>95 For instance, a single attachment error can lead to one or many mismatched brackets. [sent-400, score-0.367]
</p><p>96 We have created a novel tree-transformation methodology for evaluating parsers that categorises errors into linguistically meaningful types. [sent-401, score-0.606]
</p><p>97 Using this approach, we presented the first detailed exam-  ination of the errors produced by a wide range of constituency parsers for English. [sent-402, score-0.546]
</p><p>98 We found that PP attachment and clause attachment are the most challenging constructions, while coordination turns out to be less problematic than previously thought. [sent-403, score-0.692]
</p><p>99 We 1057 also noted interesting variations in error types for parsers variants. [sent-404, score-0.293]
</p><p>100 We investigated the errors resolved in reranking, and introduced by changing domains. [sent-405, score-0.287]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('errors', 0.287), ('np', 0.271), ('pp', 0.244), ('attachment', 0.238), ('extra', 0.195), ('parser', 0.191), ('vp', 0.188), ('missing', 0.168), ('parsers', 0.164), ('node', 0.156), ('attach', 0.151), ('coordination', 0.142), ('charniak', 0.13), ('error', 0.129), ('breakdown', 0.128), ('unary', 0.122), ('bracket', 0.108), ('reranker', 0.108), ('spans', 0.107), ('tree', 0.106), ('constituency', 0.095), ('briscoe', 0.085), ('reranking', 0.084), ('transformation', 0.082), ('bubs', 0.079), ('clause', 0.074), ('behaviour', 0.072), ('carroll', 0.071), ('rerankers', 0.068), ('hara', 0.068), ('moved', 0.067), ('berkeley', 0.063), ('linguistically', 0.062), ('chief', 0.062), ('parseval', 0.062), ('miyao', 0.06), ('vbg', 0.06), ('officer', 0.059), ('spain', 0.057), ('lexicalised', 0.057), ('ag', 0.057), ('oracle', 0.056), ('usa', 0.056), ('meaningful', 0.055), ('australia', 0.055), ('internal', 0.054), ('iwpt', 0.053), ('sydney', 0.053), ('dublin', 0.051), ('bodenstab', 0.051), ('dunlop', 0.051), ('market', 0.051), ('moving', 0.051), ('petrov', 0.051), ('collins', 0.049), ('variation', 0.048), ('fifteen', 0.048), ('mcclosky', 0.048), ('wsj', 0.048), ('incorrect', 0.047), ('sinv', 0.046), ('ireland', 0.046), ('advp', 0.046), ('unlexicalised', 0.046), ('executive', 0.046), ('dependencies', 0.046), ('group', 0.046), ('nodes', 0.045), ('particularly', 0.044), ('ending', 0.044), ('decline', 0.043), ('yusuke', 0.042), ('parsing', 0.042), ('production', 0.041), ('ccg', 0.041), ('nnp', 0.04), ('analyse', 0.04), ('king', 0.04), ('anp', 0.04), ('auli', 0.04), ('canary', 0.04), ('clegg', 0.04), ('dominick', 0.04), ('dresdner', 0.04), ('dridan', 0.04), ('islands', 0.04), ('mannesmann', 0.04), ('nlpied', 0.04), ('palmas', 0.04), ('parc', 0.04), ('rtc', 0.04), ('ssn', 0.04), ('tadayoshi', 0.04), ('tovp', 0.04), ('parse', 0.039), ('causes', 0.038), ('breaking', 0.038), ('movement', 0.038), ('ptb', 0.038), ('evaluating', 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="105-tfidf-1" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>Author: Jonathan K. Kummerfeld ; David Hall ; James R. Curran ; Dan Klein</p><p>Abstract: Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors. We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for stateof-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.</p><p>2 0.20356736 <a title="105-tfidf-2" href="./emnlp-2012-Transforming_Trees_to_Improve_Syntactic_Convergence.html">127 emnlp-2012-Transforming Trees to Improve Syntactic Convergence</a></p>
<p>Author: David Burkett ; Dan Klein</p><p>Abstract: We describe a transformation-based learning method for learning a sequence of monolingual tree transformations that improve the agreement between constituent trees and word alignments in bilingual corpora. Using the manually annotated English Chinese Translation Treebank, we show how our method automatically discovers transformations that accommodate differences in English and Chinese syntax. Furthermore, when transformations are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic MT system, the transformed trees achieve a 0.9 BLEU improvement over baseline trees.</p><p>3 0.13590595 <a title="105-tfidf-3" href="./emnlp-2012-Forest_Reranking_through_Subtree_Ranking.html">55 emnlp-2012-Forest Reranking through Subtree Ranking</a></p>
<p>Author: Richard Farkas ; Helmut Schmid</p><p>Abstract: We propose the subtree ranking approach to parse forest reranking which is a generalization of current perceptron-based reranking methods. For the training of the reranker, we extract competing local subtrees, hence the training instances (candidate subtree sets) are very similar to those used during beamsearch parsing. This leads to better parameter optimization. Another chief advantage of the framework is that arbitrary learning to rank methods can be applied. We evaluated our reranking approach on German and English phrase structure parsing tasks and compared it to various state-of-the-art reranking approaches such as the perceptron-based forest reranker. The subtree ranking approach with a Maximum Entropy model significantly outperformed the other approaches.</p><p>4 0.12253343 <a title="105-tfidf-4" href="./emnlp-2012-Training_Factored_PCFGs_with_Expectation_Propagation.html">126 emnlp-2012-Training Factored PCFGs with Expectation Propagation</a></p>
<p>Author: David Hall ; Dan Klein</p><p>Abstract: PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar. We present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion. Our method works with linguisticallymotivated annotations, induced latent structure, lexicalization, or any mix of the three. We use a structured expectation propagation algorithm that makes use of the factored structure in two ways. First, by partitioning the factors, it speeds up parsing exponentially over the unfactored approach. Second, it minimizes the redundancy of the factors during training, improving accuracy over an independent approach. Using purely latent variable annotations, we can efficiently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the na¨ ıve approach. Combining latent, lexicalized, and unlexicalized anno- tations, our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank.</p><p>5 0.12218773 <a title="105-tfidf-5" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>Author: Greg Durrett ; Adam Pauls ; Dan Klein</p><p>Abstract: We consider the problem of using a bilingual dictionary to transfer lexico-syntactic information from a resource-rich source language to a resource-poor target language. In contrast to past work that used bitexts to transfer analyses of specific sentences at the token level, we instead use features to transfer the behavior of words at a type level. In a discriminative dependency parsing framework, our approach produces gains across a range of target languages, using two different lowresource training methodologies (one weakly supervised and one indirectly supervised) and two different dictionary sources (one manually constructed and one automatically constructed).</p><p>6 0.118916 <a title="105-tfidf-6" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>7 0.11814558 <a title="105-tfidf-7" href="./emnlp-2012-Generalized_Higher-Order_Dependency_Parsing_with_Cube_Pruning.html">57 emnlp-2012-Generalized Higher-Order Dependency Parsing with Cube Pruning</a></p>
<p>8 0.11738244 <a title="105-tfidf-8" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>9 0.11668412 <a title="105-tfidf-9" href="./emnlp-2012-Left-to-Right_Tree-to-String_Decoding_with_Prediction.html">82 emnlp-2012-Left-to-Right Tree-to-String Decoding with Prediction</a></p>
<p>10 0.11530524 <a title="105-tfidf-10" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>11 0.11029223 <a title="105-tfidf-11" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>12 0.10220888 <a title="105-tfidf-12" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>13 0.09835048 <a title="105-tfidf-13" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>14 0.097739086 <a title="105-tfidf-14" href="./emnlp-2012-Minimal_Dependency_Length_in_Realization_Ranking.html">88 emnlp-2012-Minimal Dependency Length in Realization Ranking</a></p>
<p>15 0.092510298 <a title="105-tfidf-15" href="./emnlp-2012-Characterizing_Stylistic_Elements_in_Syntactic_Structure.html">27 emnlp-2012-Characterizing Stylistic Elements in Syntactic Structure</a></p>
<p>16 0.089501858 <a title="105-tfidf-16" href="./emnlp-2012-A_Novel_Discriminative_Framework_for_Sentence-Level_Discourse_Analysis.html">7 emnlp-2012-A Novel Discriminative Framework for Sentence-Level Discourse Analysis</a></p>
<p>17 0.087843738 <a title="105-tfidf-17" href="./emnlp-2012-Exploiting_Reducibility_in_Unsupervised_Dependency_Parsing.html">46 emnlp-2012-Exploiting Reducibility in Unsupervised Dependency Parsing</a></p>
<p>18 0.082348503 <a title="105-tfidf-18" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>19 0.077471726 <a title="105-tfidf-19" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>20 0.076720215 <a title="105-tfidf-20" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.27), (1, -0.163), (2, 0.124), (3, -0.086), (4, 0.082), (5, -0.017), (6, -0.003), (7, 0.025), (8, -0.155), (9, 0.151), (10, -0.059), (11, 0.047), (12, -0.087), (13, 0.236), (14, 0.073), (15, 0.088), (16, 0.15), (17, 0.0), (18, -0.158), (19, -0.082), (20, 0.015), (21, -0.055), (22, 0.065), (23, 0.018), (24, 0.142), (25, 0.005), (26, -0.053), (27, -0.074), (28, 0.001), (29, -0.133), (30, -0.082), (31, 0.004), (32, -0.009), (33, 0.127), (34, 0.026), (35, -0.094), (36, -0.046), (37, -0.032), (38, 0.203), (39, -0.023), (40, -0.059), (41, 0.024), (42, 0.035), (43, 0.028), (44, -0.042), (45, 0.131), (46, -0.014), (47, 0.044), (48, 0.065), (49, -0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98006266 <a title="105-lsi-1" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>Author: Jonathan K. Kummerfeld ; David Hall ; James R. Curran ; Dan Klein</p><p>Abstract: Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors. We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for stateof-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.</p><p>2 0.66195446 <a title="105-lsi-2" href="./emnlp-2012-Forest_Reranking_through_Subtree_Ranking.html">55 emnlp-2012-Forest Reranking through Subtree Ranking</a></p>
<p>Author: Richard Farkas ; Helmut Schmid</p><p>Abstract: We propose the subtree ranking approach to parse forest reranking which is a generalization of current perceptron-based reranking methods. For the training of the reranker, we extract competing local subtrees, hence the training instances (candidate subtree sets) are very similar to those used during beamsearch parsing. This leads to better parameter optimization. Another chief advantage of the framework is that arbitrary learning to rank methods can be applied. We evaluated our reranking approach on German and English phrase structure parsing tasks and compared it to various state-of-the-art reranking approaches such as the perceptron-based forest reranker. The subtree ranking approach with a Maximum Entropy model significantly outperformed the other approaches.</p><p>3 0.6149829 <a title="105-lsi-3" href="./emnlp-2012-Transforming_Trees_to_Improve_Syntactic_Convergence.html">127 emnlp-2012-Transforming Trees to Improve Syntactic Convergence</a></p>
<p>Author: David Burkett ; Dan Klein</p><p>Abstract: We describe a transformation-based learning method for learning a sequence of monolingual tree transformations that improve the agreement between constituent trees and word alignments in bilingual corpora. Using the manually annotated English Chinese Translation Treebank, we show how our method automatically discovers transformations that accommodate differences in English and Chinese syntax. Furthermore, when transformations are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic MT system, the transformed trees achieve a 0.9 BLEU improvement over baseline trees.</p><p>4 0.44995922 <a title="105-lsi-4" href="./emnlp-2012-Training_Factored_PCFGs_with_Expectation_Propagation.html">126 emnlp-2012-Training Factored PCFGs with Expectation Propagation</a></p>
<p>Author: David Hall ; Dan Klein</p><p>Abstract: PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar. We present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion. Our method works with linguisticallymotivated annotations, induced latent structure, lexicalization, or any mix of the three. We use a structured expectation propagation algorithm that makes use of the factored structure in two ways. First, by partitioning the factors, it speeds up parsing exponentially over the unfactored approach. Second, it minimizes the redundancy of the factors during training, improving accuracy over an independent approach. Using purely latent variable annotations, we can efficiently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the na¨ ıve approach. Combining latent, lexicalized, and unlexicalized anno- tations, our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank.</p><p>5 0.43788552 <a title="105-lsi-5" href="./emnlp-2012-Characterizing_Stylistic_Elements_in_Syntactic_Structure.html">27 emnlp-2012-Characterizing Stylistic Elements in Syntactic Structure</a></p>
<p>Author: Song Feng ; Ritwik Banerjee ; Yejin Choi</p><p>Abstract: Much of the writing styles recognized in rhetorical and composition theories involve deep syntactic elements. However, most previous research for computational stylometric analysis has relied on shallow lexico-syntactic patterns. Some very recent work has shown that PCFG models can detect distributional difference in syntactic styles, but without offering much insights into exactly what constitute salient stylistic elements in sentence structure characterizing each authorship. In this paper, we present a comprehensive exploration of syntactic elements in writing styles, with particular emphasis on interpretable characterization of stylistic elements. We present analytic insights with respect to the authorship attribution task in two different domains. ,</p><p>6 0.43251738 <a title="105-lsi-6" href="./emnlp-2012-Generalized_Higher-Order_Dependency_Parsing_with_Cube_Pruning.html">57 emnlp-2012-Generalized Higher-Order Dependency Parsing with Cube Pruning</a></p>
<p>7 0.42511246 <a title="105-lsi-7" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>8 0.41779894 <a title="105-lsi-8" href="./emnlp-2012-Left-to-Right_Tree-to-String_Decoding_with_Prediction.html">82 emnlp-2012-Left-to-Right Tree-to-String Decoding with Prediction</a></p>
<p>9 0.40573362 <a title="105-lsi-9" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>10 0.39813346 <a title="105-lsi-10" href="./emnlp-2012-Minimal_Dependency_Length_in_Realization_Ranking.html">88 emnlp-2012-Minimal Dependency Length in Realization Ranking</a></p>
<p>11 0.38788468 <a title="105-lsi-11" href="./emnlp-2012-A_Novel_Discriminative_Framework_for_Sentence-Level_Discourse_Analysis.html">7 emnlp-2012-A Novel Discriminative Framework for Sentence-Level Discourse Analysis</a></p>
<p>12 0.38681516 <a title="105-lsi-12" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>13 0.37613392 <a title="105-lsi-13" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>14 0.37268397 <a title="105-lsi-14" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>15 0.35321796 <a title="105-lsi-15" href="./emnlp-2012-Exploiting_Reducibility_in_Unsupervised_Dependency_Parsing.html">46 emnlp-2012-Exploiting Reducibility in Unsupervised Dependency Parsing</a></p>
<p>16 0.34855336 <a title="105-lsi-16" href="./emnlp-2012-Spectral_Dependency_Parsing_with_Latent_Variables.html">119 emnlp-2012-Spectral Dependency Parsing with Latent Variables</a></p>
<p>17 0.34701988 <a title="105-lsi-17" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>18 0.34474242 <a title="105-lsi-18" href="./emnlp-2012-Dynamic_Programming_for_Higher_Order_Parsing_of_Gap-Minding_Trees.html">37 emnlp-2012-Dynamic Programming for Higher Order Parsing of Gap-Minding Trees</a></p>
<p>19 0.33873555 <a title="105-lsi-19" href="./emnlp-2012-Exploiting_Chunk-level_Features_to_Improve_Phrase_Chunking.html">45 emnlp-2012-Exploiting Chunk-level Features to Improve Phrase Chunking</a></p>
<p>20 0.33271858 <a title="105-lsi-20" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.022), (11, 0.013), (16, 0.043), (25, 0.012), (29, 0.364), (34, 0.052), (41, 0.012), (60, 0.08), (63, 0.061), (64, 0.017), (65, 0.019), (70, 0.014), (73, 0.011), (74, 0.069), (76, 0.066), (80, 0.02), (86, 0.023), (95, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83848935 <a title="105-lda-1" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>Author: Jonathan K. Kummerfeld ; David Hall ; James R. Curran ; Dan Klein</p><p>Abstract: Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors. We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for stateof-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.</p><p>2 0.4789975 <a title="105-lda-2" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms ofweak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.</p><p>3 0.40270025 <a title="105-lda-3" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>Author: Greg Durrett ; Adam Pauls ; Dan Klein</p><p>Abstract: We consider the problem of using a bilingual dictionary to transfer lexico-syntactic information from a resource-rich source language to a resource-poor target language. In contrast to past work that used bitexts to transfer analyses of specific sentences at the token level, we instead use features to transfer the behavior of words at a type level. In a discriminative dependency parsing framework, our approach produces gains across a range of target languages, using two different lowresource training methodologies (one weakly supervised and one indirectly supervised) and two different dictionary sources (one manually constructed and one automatically constructed).</p><p>4 0.38238543 <a title="105-lda-4" href="./emnlp-2012-Forest_Reranking_through_Subtree_Ranking.html">55 emnlp-2012-Forest Reranking through Subtree Ranking</a></p>
<p>Author: Richard Farkas ; Helmut Schmid</p><p>Abstract: We propose the subtree ranking approach to parse forest reranking which is a generalization of current perceptron-based reranking methods. For the training of the reranker, we extract competing local subtrees, hence the training instances (candidate subtree sets) are very similar to those used during beamsearch parsing. This leads to better parameter optimization. Another chief advantage of the framework is that arbitrary learning to rank methods can be applied. We evaluated our reranking approach on German and English phrase structure parsing tasks and compared it to various state-of-the-art reranking approaches such as the perceptron-based forest reranker. The subtree ranking approach with a Maximum Entropy model significantly outperformed the other approaches.</p><p>5 0.3796778 <a title="105-lda-5" href="./emnlp-2012-Improving_Transition-Based_Dependency_Parsing_with_Buffer_Transitions.html">66 emnlp-2012-Improving Transition-Based Dependency Parsing with Buffer Transitions</a></p>
<p>Author: Daniel Fernandez-Gonzalez ; Carlos Gomez-Rodriguez</p><p>Abstract: In this paper, we show that significant improvements in the accuracy of well-known transition-based parsers can be obtained, without sacrificing efficiency, by enriching the parsers with simple transitions that act on buffer nodes. First, we show how adding a specific transition to create either a left or right arc of length one between the first two buffer nodes produces improvements in the accuracy of Nivre’s arc-eager projective parser on a number of datasets from the CoNLL-X shared task. Then, we show that accuracy can also be improved by adding transitions involving the topmost stack node and the second buffer node (allowing a limited form of non-projectivity). None of these transitions has a negative impact on the computational complexity of the algorithm. Although the experiments in this paper use the arc-eager parser, the approach is generic enough to be applicable to any stackbased dependency parser.</p><p>6 0.37825903 <a title="105-lda-6" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>7 0.37307394 <a title="105-lda-7" href="./emnlp-2012-Minimal_Dependency_Length_in_Realization_Ranking.html">88 emnlp-2012-Minimal Dependency Length in Realization Ranking</a></p>
<p>8 0.37097794 <a title="105-lda-8" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>9 0.37070471 <a title="105-lda-9" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>10 0.37066311 <a title="105-lda-10" href="./emnlp-2012-Extracting_Opinion_Expressions_with_semi-Markov_Conditional_Random_Fields.html">51 emnlp-2012-Extracting Opinion Expressions with semi-Markov Conditional Random Fields</a></p>
<p>11 0.36890626 <a title="105-lda-11" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>12 0.36373845 <a title="105-lda-12" href="./emnlp-2012-Syntactic_Surprisal_Affects_Spoken_Word_Duration_in_Conversational_Contexts.html">122 emnlp-2012-Syntactic Surprisal Affects Spoken Word Duration in Conversational Contexts</a></p>
<p>13 0.36166701 <a title="105-lda-13" href="./emnlp-2012-Besting_the_Quiz_Master%3A_Crowdsourcing_Incremental_Classification_Games.html">23 emnlp-2012-Besting the Quiz Master: Crowdsourcing Incremental Classification Games</a></p>
<p>14 0.36131665 <a title="105-lda-14" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>15 0.36056405 <a title="105-lda-15" href="./emnlp-2012-Left-to-Right_Tree-to-String_Decoding_with_Prediction.html">82 emnlp-2012-Left-to-Right Tree-to-String Decoding with Prediction</a></p>
<p>16 0.35990617 <a title="105-lda-16" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>17 0.35951334 <a title="105-lda-17" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>18 0.359341 <a title="105-lda-18" href="./emnlp-2012-Transforming_Trees_to_Improve_Syntactic_Convergence.html">127 emnlp-2012-Transforming Trees to Improve Syntactic Convergence</a></p>
<p>19 0.35809463 <a title="105-lda-19" href="./emnlp-2012-A_Novel_Discriminative_Framework_for_Sentence-Level_Discourse_Analysis.html">7 emnlp-2012-A Novel Discriminative Framework for Sentence-Level Discourse Analysis</a></p>
<p>20 0.35766745 <a title="105-lda-20" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
