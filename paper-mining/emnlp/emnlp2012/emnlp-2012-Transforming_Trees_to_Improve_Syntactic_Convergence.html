<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>127 emnlp-2012-Transforming Trees to Improve Syntactic Convergence</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-127" href="#">emnlp2012-127</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>127 emnlp-2012-Transforming Trees to Improve Syntactic Convergence</h1>
<br/><p>Source: <a title="emnlp-2012-127-pdf" href="http://aclweb.org/anthology//D/D12/D12-1079.pdf">pdf</a></p><p>Author: David Burkett ; Dan Klein</p><p>Abstract: We describe a transformation-based learning method for learning a sequence of monolingual tree transformations that improve the agreement between constituent trees and word alignments in bilingual corpora. Using the manually annotated English Chinese Translation Treebank, we show how our method automatically discovers transformations that accommodate differences in English and Chinese syntax. Furthermore, when transformations are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic MT system, the transformed trees achieve a 0.9 BLEU improvement over baseline trees.</p><p>Reference: <a title="emnlp-2012-127-reference" href="../emnlp2012_reference/emnlp-2012-Transforming_Trees_to_Improve_Syntactic_Convergence_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We describe a transformation-based learning method for learning a sequence of monolingual tree transformations that improve the agreement between constituent trees and word alignments in bilingual corpora. [sent-3, score-1.147]
</p><p>2 Using the manually annotated English Chinese Translation Treebank, we show how our method automatically discovers transformations that accommodate differences in English and Chinese syntax. [sent-4, score-0.557]
</p><p>3 Furthermore, when transformations are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic MT system, the transformed trees achieve a 0. [sent-5, score-0.99]
</p><p>4 However, once bilingual data is involved, such treebank conventions entail constraints on rule extraction that may not be borne out by semantic alignments. [sent-11, score-0.211]
</p><p>5 To the extent that there are simply divergences in the syntactic structure of the two languages, it will often be impossible to construct syntax trees that are simultaneously in full agreement with monolingual linguistic theories and with the alignments between sentences in both languages. [sent-12, score-0.532]
</p><p>6 The 863 lowest VP in this tree is headed by ‘select,’ which aligns to the Chinese verb ‘ 挑选. [sent-15, score-0.154]
</p><p>7 Because of this  violating alignment, many syntactic machine translation systems (Galley et al. [sent-17, score-0.206]
</p><p>8 , 2006) won’t extract any translation rules for this constituent. [sent-19, score-0.192]
</p><p>9 In this work, we develop a method based on transformation-based learning (Brill, 1995) for automatically acquiring a sequence of tree transformations of the sort in Figure 1. [sent-21, score-0.668]
</p><p>10 Once the transformation sequence has been learned, it can be deterministically applied to any parsed sentences, yielding new parse trees with constituency structures that agree better with the bilingual alignments yet remain consistent across the corpus. [sent-22, score-0.73]
</p><p>11 In particular, we use this method to learn a transformation sequence for the English trees in a set ofEnglish to Chinese MT training data. [sent-23, score-0.474]
</p><p>12 In experiments with a string-to-tree translation system, we show resulting improvements of up to 0. [sent-24, score-0.152]
</p><p>13 A great deal of research in syntactic machine translation has been devoted to handling the inherent syntactic divergence between source and target languages. [sent-26, score-0.26]
</p><p>14 (b) After Figure 1: An example tree transformation merging a VB node with the TO sibling of its parent VP. [sent-35, score-0.454]
</p><p>15 Before the transformation (a), the bolded VP cannot be extracted as a translation rule, but afterwards (b), both this VP and the newly  created TO+VB node are extractable. [sent-36, score-0.525]
</p><p>16 for learning translation rules (Mi and Huang, 2008; Zhang et al. [sent-37, score-0.192]
</p><p>17 , 2009), or by learning rules that encode syntactic information but do not strictly adhere to constituency boundaries (Zollmann et al. [sent-38, score-0.132]
</p><p>18 (201 1), who train a rule extraction system to transform the subtrees that make up individual translation rules using a manually constructed set of transformations similar to those learned by our system. [sent-41, score-0.848]
</p><p>19 Most systems of this sort learn how to modify word alignments to agree better with the syntactic parse trees (DeNero and Klein, 2007; Fossum et al. [sent-43, score-0.346]
</p><p>20 , 2008), but there has also been other work directly related to improving agreement by modifying the trees. [sent-44, score-0.196]
</p><p>21 (2010) train a bilingual parsing model that uses bilingual agreement features to improve parsing accuracy. [sent-46, score-0.272]
</p><p>22 (201 1) retrain a parser to directly optimize a word reordering metric in order to improve a downstream machine translation system that uses dependency parses in a preprocessing reordering step. [sent-48, score-0.229]
</p><p>23 Our system is in the same basic spirit, using a proxy evaluation metric (agreement with alignments; see Section 2 for details) to improve performance on a downstream translation  864 task. [sent-49, score-0.229]
</p><p>24 However, we are concerned more generally with the goal of creating trees that are more compatible with a wide range of syntactically-informed translation systems, particularly those that extract translation rules based on syntactic constituents. [sent-50, score-0.51]
</p><p>25 2  Agreement  Our primary goal in adapting parse trees is to improve their agreement with a set of external word alignments. [sent-51, score-0.349]
</p><p>26 Thus, our first step is to define an agreement score metric to operationalize this concept. [sent-52, score-0.269]
</p><p>27 Central to the definition of our agreement score is the notion of an extractable node. [sent-53, score-0.445]
</p><p>28 Intuitively, an extractable English1 tree node (also often called a “frontier node” in the literature), is one whose span aligns to a contiguous span in the foreign sentence. [sent-54, score-0.622]
</p><p>29 Formally, we assume a fixed word alignment a = {(i, j)}, where (i, j) ∈ a means that English word i{ (iis, aligned etor foreign ∈w aord m j. [sent-55, score-0.164]
</p><p>30 −11 o[kth,e‘r]w isi esextractable Importantly, the sum in Equation 1ranges over all unique spans in t. [sent-58, score-0.088]
</p><p>31 This is simply to make the metric less gameable, preventing degenerate solutions such as an arbitrarily long chain of unary productions over an extractable span. [sent-59, score-0.231]
</p><p>32 As a concrete example of agreement score, we can return to Figure 1. [sent-61, score-0.196]
</p><p>33 The tree in Figure 1a has 6 unique spans, but only 5 are extractable, so the total agreement score is 5 - 1 = 4. [sent-62, score-0.308]
</p><p>34 After the transformation, though, the tree in Figure 1b has 6 extractable spans, so the agreement score is 6. [sent-63, score-0.497]
</p><p>35 2Unextractable spans are penalized in order to ensure that space is saved for the formation of extractable ones. [sent-66, score-0.277]
</p><p>36 First, you apply the initial state annotator (here, the source of original trees) to your training sentences to ensure that they all begin with a legal annotation. [sent-68, score-0.108]
</p><p>37 Then, you test each transformation in your inventory to see which one will yield the greatest improvement in the evaluation metric if applied to the training data. [sent-69, score-0.451]
</p><p>38 You greedily apply this transformation to the full training set and then repeat the procedure, applying transformations until some stopping criterion is met (usually either a maximum number of transformations, or a threshold on the marginal improvement in the evaluation metric). [sent-70, score-0.923]
</p><p>39 To annotate new data, you  simply label it with the same initial state annotator and then apply each of the learned transformations in order. [sent-72, score-0.716]
</p><p>40 For our task, we have already defined the evaluation metric (Section 2) and the initial state annotator will either be the gold Treebank trees or a Treebanktrained PCFG parser. [sent-74, score-0.262]
</p><p>41 4  Tree Transformations  The definition of an atomic transformation consists of two parts: a rewrite rule and the triggering environment (Brill, 1995). [sent-76, score-0.692]
</p><p>42 Tree transformations are best illustrated visually, and so for each of our transformation types, both parts of the definition are repre-  sented schematically in Figures 2-7. [sent-77, score-1.018]
</p><p>43 tween two and four syntactic category arguments, and most also take a DIRECTION argument that can have the value left or right. [sent-147, score-0.087]
</p><p>44 3 We refer to the nodes in the schematics whose categories are arguments of the transformation definition as participating nodes. [sent-148, score-0.531]
</p><p>45 Basically, a particular transformation is triggered anywhere in a parse tree where all participating nodes appear in the configuration shown. [sent-149, score-0.59]
</p><p>46 The exact rules for the triggering environment are: 1. [sent-150, score-0.288]
</p><p>47 Each participating node must appear in the schematically illustrated relationship to the others. [sent-151, score-0.196]
</p><p>48 The non-participating nodes in the schematic do not have to appear. [sent-152, score-0.172]
</p><p>49 Similarly, any number of additional nodes can appear as sib-  lings, parents, or children of the explicitly illustrated nodes. [sent-153, score-0.107]
</p><p>50 Any node that will gain a new child as a result of the transformation must already have at least one nonterminal child. [sent-155, score-0.373]
</p><p>51 We have drawn the schematics to reflect this, so this condition is 3To save space, the schematic for each of these transformations is only shown for the left direction, but the right version is simply the mirror image. [sent-156, score-0.756]
</p><p>52 867 equivalent to saying that any participating node that is drawn with children must have a phrasal syntactic category (i. [sent-157, score-0.15]
</p><p>53 That is, the newly created nodes that result from an ARTICULATE or ADOPT transformation cannot then participate as the LEFT or RIGHT argument of a subsequent ARTICULATE transformation or as the AUNT or TARGET argument of a subsequent ADOPT transformation. [sent-162, score-0.774]
</p><p>54 The rewrite rule for a transformation is essentially captured in the corresponding schematic. [sent-164, score-0.415]
</p><p>55 Additional nodes that do not appear in the schematic are generally handled in the obvious way: unillustrated children or parents of illustrated nodes remain in place, while unillustrated siblings of illustrated nodes are handled identically to their illustrated siblings. [sent-165, score-0.543]
</p><p>56 The only additional part of the rewrite that is not shown explicitly in the schematics is that if the node in the PARENT position of a TRANSFER or ADOPT transformation is left childless by the transformation (because the TARGET node was its only child), then it is deleted from the parse tree. [sent-166, score-0.93]
</p><p>57 In the case of a transformation whose triggering environment appears multiple times in a single tree, transformations are always applied leftmost/bottom-up and exhaustively. [sent-167, score-1.137]
</p><p>58 4 In principle, our transformation inventory consists of all possible assignments of syntactic categories to the arguments of each of the transformation types (subject to the triggering environment constraints). [sent-168, score-1.009]
</p><p>59 In practice, though, we only ever consider transformations whose triggering environments appear in the training corpus (including new triggering environments that appear as the result of earlier trans-  formations). [sent-169, score-1.017]
</p><p>60 While the theoretical space of possible transformations is exponentially large, the set of transformations we actually have to consider is quite manageable, and empirically grows substantially sublinearly in the size of the training set. [sent-170, score-1.161]
</p><p>61 Since we intend for our system to be used as a pre-trained annotator, it is important to ensure that the learned transformation sequence achieves agreement score gains that generalize well to unseen data. [sent-176, score-0.64]
</p><p>62 The value plotted is the average (per-sentence) improvement in agreement score over the baseline trees. [sent-180, score-0.261]
</p><p>63 Table 1: Average span counts and agreement scores on the English Chinese Translation Treebank development set. [sent-181, score-0.278]
</p><p>64 The highest agreement score was attained at 1584 transformations, but most of the improvement happened much earlier. [sent-182, score-0.261]
</p><p>65 The improvements in agreement score are shown in Figure 8, with a slightly more detailed breakdown at a few fixed points in Table 1. [sent-185, score-0.227]
</p><p>66 We also see from Table 1that, though the first few transformations deleted many non-extractable spans, the overall trend was to produce more finely articulated trees, with the full transformation sequence increasing the number of spans by more than 10%. [sent-190, score-1.122]
</p><p>67 As discussed in Section 3, one advantage of TBL is that the learned transformations can themselves often be interesting. [sent-191, score-0.608]
</p><p>68 For this task, some of the highest scoring transformations did uninteresting things like conjoining conjunctions or punctuation, which are often either unaligned or aligned monotonically with adjacent phrases. [sent-192, score-0.605]
</p><p>69 However, by filtering out all ARTICULATE transformations where either the LEFT or RIGHT argument is “CC”, “-RRB-”, “,”, or “. [sent-193, score-0.59]
</p><p>70 #1, #7, #10) are additional ways of creating new spans when English and Chinese phrase structures roughly agree, but many others do recover known differences  869 in English and Chinese syntax. [sent-197, score-0.117]
</p><p>71 For example, many of these transformations directly address compound verb forms in English, which tend to align to single words in Chinese: #3 (past participle constructions), #4 (infinitive), #6 (all), and #17 (present perfect). [sent-198, score-0.557]
</p><p>72 6  Machine Translation  The ultimate goal of our system is to improve the agreement between the automatically generated parse trees and word alignments that are used as training data for syntactic machine translation systems. [sent-202, score-0.659]
</p><p>73 Thus, all rules used for MT experiments were learned from automatically annotated text. [sent-205, score-0.091]
</p><p>74 For our Chinese to English translation experiments, we generated word alignments using the Berkeley Aligner (Liang et al. [sent-206, score-0.256]
</p><p>75 We used an MT pipeline that conditions on target-side syntax, so our initial state annotator was the Berkeley Parser (Petrov and Klein, 2007), trained on a modified English treebank that has been adapted to match standard MT tokenization and capitalization schemes. [sent-208, score-0.182]
</p><p>76 Since the 6By using a simple hashing scheme to keep track of triggering environments, this cost can be reduced greatly but is still  linear in the number of training sentences. [sent-211, score-0.182]
</p><p>77 VBG+IN PP VBG IN  (e) ADOPThPP,VBG,PP,IN,lefti  Figure 9: Illustrations of the top 5 transformations from Table 2. [sent-258, score-0.557]
</p><p>78 870 most useful transformations almost by definition are ones that are triggered the most frequently, any reasonably sized training set is likely to contain them, and so it is not actually likely that dramatically increasing the size of the training set will yield particularly large gains. [sent-259, score-0.586]
</p><p>79 7 We also extracted an additional 1000 sentence test set to use for rapidly evaluating agreement score generalization. [sent-261, score-0.227]
</p><p>80 Figure 10 illustrates the improvements in agreement score for the automatically annotated data, analogous to Figure 8. [sent-262, score-0.227]
</p><p>81 The same general patterns hold, although we do see that the automatically annotated data is more idiosyncratic and so more than twice as many transformations are learned before training set agreement stops improving, even though the training set sizes are roughly the same. [sent-263, score-0.833]
</p><p>82 8 Furthermore, test set generalization in the automatic annotation setting is a little bit worse,  with later transformations tending to actually hurt test set agreement. [sent-264, score-0.557]
</p><p>83 For our machine translation experiments, we used the string-to-tree syntactic pipeline included in the current version of Moses (Koehn et al. [sent-265, score-0.206]
</p><p>84 Since the transformed trees tended to be more finely articulated, and increasing the number of unique spans often helps with rule extraction (Wang et al. [sent-269, score-0.289]
</p><p>85 , 2007), we equalized the span count by also testing binarized versions of each set of trees, using the leftbranching and right-branching binarization scripts included with Moses. [sent-270, score-0.148]
</p><p>86 8Note that the training set improvement curves don’t actu-  ally flatten out because training halts once no improving transformation exists. [sent-272, score-0.403]
</p><p>87 9Binarized trees are guaranteed to have k − 1 unique spans for sBenitneanriczeesd do tfr length k g. [sent-273, score-0.2]
</p><p>88 The training and test sets are disjoint in order to measure how well the learned transformation sequence generalizes. [sent-275, score-0.413]
</p><p>89 Though 5151 transformations were learned from the training set, the maximum test set agreement was achieved at 630 transformations, with an average improvement of 2. [sent-277, score-0.838]
</p><p>90 To mitigate this effect, we first used the Moses training scripts to extract a table of translation rules for each set of English trees. [sent-283, score-0.224]
</p><p>91 Table 3 shows the results of our translation experiments. [sent-285, score-0.152]
</p><p>92 The best translation results are achieved by using the first 139 transformations, giving a BLEU improvement of more than 0. [sent-286, score-0.186]
</p><p>93 7  Conclusion  We have demonstrated a simple but effective procedure for learning a tree transformation sequence that improves agreement between parse trees and word alignments. [sent-288, score-0.792]
</p><p>94 This method yields clear improvements in the quality of Chinese to English translation, showing that by manipulating English syn-  tax to converge with Chinese phrasal structure, we improve our ability to explicitly model the types of 871  Table 3: Machine translation results. [sent-289, score-0.152]
</p><p>95 Note that using 0 transformations just yields the original baseline trees. [sent-291, score-0.557]
</p><p>96 The transformation sequence cutoffs at 32, 58, and 139 were chosen to correspond to marginal training (total) agreement gain thresholds of 50, 25, and 10, respectively. [sent-292, score-0.558]
</p><p>97 The cutoff at 630 was chosen to maximize test agreement score and the cutoff at 5151 maximized training agreement score. [sent-293, score-0.423]
</p><p>98 Binarizing syntax trees to improve syntax-based machine translation accuracy. [sent-417, score-0.301]
</p><p>99 Learning to transform and select elementary trees for improved syntax-based machine translations. [sent-429, score-0.112]
</p><p>100 The CMU-AKA syntax augmented machine translation system for IWSLT-06. [sent-433, score-0.189]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('transformations', 0.557), ('transformation', 0.332), ('vp', 0.202), ('agreement', 0.196), ('extractable', 0.189), ('triggering', 0.182), ('translation', 0.152), ('schematic', 0.128), ('trees', 0.112), ('alignments', 0.104), ('brill', 0.1), ('chinese', 0.098), ('articulate', 0.094), ('fset', 0.094), ('mt', 0.093), ('spans', 0.088), ('span', 0.082), ('tree', 0.081), ('vb', 0.077), ('foreign', 0.074), ('annotator', 0.074), ('treebank', 0.074), ('aligns', 0.073), ('schematics', 0.071), ('np', 0.066), ('environment', 0.066), ('illustrated', 0.063), ('pp', 0.061), ('eset', 0.061), ('english', 0.058), ('infinitive', 0.055), ('tbl', 0.055), ('participating', 0.055), ('syntactic', 0.054), ('learned', 0.051), ('conventions', 0.051), ('bleu', 0.049), ('aligned', 0.048), ('environments', 0.048), ('burkett', 0.048), ('rule', 0.048), ('bies', 0.047), ('cnh', 0.047), ('enijpng', 0.047), ('espan', 0.047), ('fspan', 0.047), ('lefti', 0.047), ('righti', 0.047), ('sublinearly', 0.047), ('thefirst', 0.047), ('tnr', 0.047), ('unillustrated', 0.047), ('knight', 0.046), ('nodes', 0.044), ('moses', 0.044), ('inventory', 0.043), ('vbg', 0.043), ('alignment', 0.042), ('metric', 0.042), ('parse', 0.041), ('node', 0.041), ('finely', 0.041), ('efron', 0.041), ('rules', 0.04), ('bilingual', 0.038), ('kevin', 0.038), ('constituency', 0.038), ('flatten', 0.037), ('chapman', 0.037), ('anywhere', 0.037), ('zollmann', 0.037), ('fossum', 0.037), ('deleted', 0.037), ('articulated', 0.037), ('schematically', 0.037), ('attaches', 0.037), ('syntax', 0.037), ('agree', 0.035), ('rewrite', 0.035), ('downstream', 0.035), ('initial', 0.034), ('chunking', 0.034), ('binarization', 0.034), ('improvement', 0.034), ('argument', 0.033), ('berkeley', 0.032), ('vbn', 0.032), ('ari', 0.032), ('strongest', 0.032), ('ramshaw', 0.032), ('aligners', 0.032), ('scripts', 0.032), ('liang', 0.031), ('score', 0.031), ('optimizer', 0.03), ('eric', 0.03), ('sequence', 0.03), ('definition', 0.029), ('roughly', 0.029), ('monolingual', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="127-tfidf-1" href="./emnlp-2012-Transforming_Trees_to_Improve_Syntactic_Convergence.html">127 emnlp-2012-Transforming Trees to Improve Syntactic Convergence</a></p>
<p>Author: David Burkett ; Dan Klein</p><p>Abstract: We describe a transformation-based learning method for learning a sequence of monolingual tree transformations that improve the agreement between constituent trees and word alignments in bilingual corpora. Using the manually annotated English Chinese Translation Treebank, we show how our method automatically discovers transformations that accommodate differences in English and Chinese syntax. Furthermore, when transformations are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic MT system, the transformed trees achieve a 0.9 BLEU improvement over baseline trees.</p><p>2 0.24559486 <a title="127-tfidf-2" href="./emnlp-2012-Iterative_Annotation_Transformation_with_Predict-Self_Reestimation_for_Chinese_Word_Segmentation.html">68 emnlp-2012-Iterative Annotation Transformation with Predict-Self Reestimation for Chinese Word Segmentation</a></p>
<p>Author: Wenbin Jiang ; Fandong Meng ; Qun Liu ; Yajuan Lu</p><p>Abstract: In this paper we first describe the technology of automatic annotation transformation, which is based on the annotation adaptation algorithm (Jiang et al., 2009). It can automatically transform a human-annotated corpus from one annotation guideline to another. We then propose two optimization strategies, iterative training and predict-selfreestimation, to further improve the accuracy of annotation guideline transformation. Experiments on Chinese word segmentation show that, the iterative training strategy together with predictself reestimation brings significant improvement over the simple annotation transformation baseline, and leads to classifiers with significantly higher accuracy and several times faster processing than annotation adaptation does. On the Penn Chinese Treebank 5.0, , it achieves an F-measure of 98.43%, significantly outperforms previous works although using a single classifier with only local features.</p><p>3 0.20356736 <a title="127-tfidf-3" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>Author: Jonathan K. Kummerfeld ; David Hall ; James R. Curran ; Dan Klein</p><p>Abstract: Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors. We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for stateof-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.</p><p>4 0.14401008 <a title="127-tfidf-4" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>Author: Shujie Liu ; Chi-Ho Li ; Mu Li ; Ming Zhou</p><p>Abstract: The training of most syntactic SMT approaches involves two essential components, word alignment and monolingual parser. In the current state of the art these two components are mutually independent, thus causing problems like lack of rule generalization, and violation of syntactic correspondence in translation rules. In this paper, we propose two ways of re-training monolingual parser with the target of maximizing the consistency between parse trees and alignment matrices. One is targeted self-training with a simple evaluation function; the other is based on training data selection from forced alignment of bilingual data. We also propose an auxiliary method for boosting alignment quality, by symmetrizing alignment matrices with respect to parse trees. The best combination of these novel methods achieves 3 Bleu point gain in an IWSLT task and more than 1 Bleu point gain in NIST tasks. 1</p><p>5 0.1208532 <a title="127-tfidf-5" href="./emnlp-2012-Inducing_a_Discriminative_Parser_to_Optimize_Machine_Translation_Reordering.html">67 emnlp-2012-Inducing a Discriminative Parser to Optimize Machine Translation Reordering</a></p>
<p>Author: Graham Neubig ; Taro Watanabe ; Shinsuke Mori</p><p>Abstract: This paper proposes a method for learning a discriminative parser for machine translation reordering using only aligned parallel text. This is done by treating the parser’s derivation tree as a latent variable in a model that is trained to maximize reordering accuracy. We demonstrate that efficient large-margin training is possible by showing that two measures of reordering accuracy can be factored over the parse tree. Using this model in the pre-ordering framework results in significant gains in translation accuracy over standard phrasebased SMT and previously proposed unsupervised syntax induction methods.</p><p>6 0.11672105 <a title="127-tfidf-6" href="./emnlp-2012-Left-to-Right_Tree-to-String_Decoding_with_Prediction.html">82 emnlp-2012-Left-to-Right Tree-to-String Decoding with Prediction</a></p>
<p>7 0.10283571 <a title="127-tfidf-7" href="./emnlp-2012-Forced_Derivation_Tree_based_Model_Training_to_Statistical_Machine_Translation.html">54 emnlp-2012-Forced Derivation Tree based Model Training to Statistical Machine Translation</a></p>
<p>8 0.096570015 <a title="127-tfidf-8" href="./emnlp-2012-Locally_Training_the_Log-Linear_Model_for_SMT.html">86 emnlp-2012-Locally Training the Log-Linear Model for SMT</a></p>
<p>9 0.094858281 <a title="127-tfidf-9" href="./emnlp-2012-A_Bayesian_Model_for_Learning_SCFGs_with_Discontiguous_Rules.html">1 emnlp-2012-A Bayesian Model for Learning SCFGs with Discontiguous Rules</a></p>
<p>10 0.087597623 <a title="127-tfidf-10" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>11 0.084142968 <a title="127-tfidf-11" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>12 0.082214549 <a title="127-tfidf-12" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>13 0.081301108 <a title="127-tfidf-13" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>14 0.078423895 <a title="127-tfidf-14" href="./emnlp-2012-Training_Factored_PCFGs_with_Expectation_Propagation.html">126 emnlp-2012-Training Factored PCFGs with Expectation Propagation</a></p>
<p>15 0.077876359 <a title="127-tfidf-15" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>16 0.076391451 <a title="127-tfidf-16" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>17 0.07174132 <a title="127-tfidf-17" href="./emnlp-2012-Multiple_Aspect_Summarization_Using_Integer_Linear_Programming.html">94 emnlp-2012-Multiple Aspect Summarization Using Integer Linear Programming</a></p>
<p>18 0.071636558 <a title="127-tfidf-18" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>19 0.070626624 <a title="127-tfidf-19" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>20 0.068564661 <a title="127-tfidf-20" href="./emnlp-2012-Dynamic_Programming_for_Higher_Order_Parsing_of_Gap-Minding_Trees.html">37 emnlp-2012-Dynamic Programming for Higher Order Parsing of Gap-Minding Trees</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.258), (1, -0.209), (2, -0.066), (3, -0.072), (4, 0.024), (5, -0.029), (6, -0.069), (7, -0.002), (8, -0.132), (9, -0.036), (10, -0.105), (11, 0.062), (12, -0.148), (13, 0.208), (14, 0.006), (15, 0.214), (16, 0.204), (17, 0.072), (18, -0.098), (19, -0.095), (20, -0.088), (21, -0.001), (22, 0.074), (23, 0.085), (24, 0.137), (25, -0.061), (26, -0.189), (27, -0.123), (28, -0.042), (29, 0.052), (30, -0.045), (31, -0.015), (32, -0.179), (33, 0.09), (34, -0.053), (35, 0.075), (36, 0.117), (37, -0.043), (38, 0.092), (39, 0.096), (40, 0.15), (41, -0.016), (42, 0.148), (43, 0.004), (44, 0.109), (45, 0.023), (46, -0.058), (47, -0.038), (48, -0.016), (49, -0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95856851 <a title="127-lsi-1" href="./emnlp-2012-Transforming_Trees_to_Improve_Syntactic_Convergence.html">127 emnlp-2012-Transforming Trees to Improve Syntactic Convergence</a></p>
<p>Author: David Burkett ; Dan Klein</p><p>Abstract: We describe a transformation-based learning method for learning a sequence of monolingual tree transformations that improve the agreement between constituent trees and word alignments in bilingual corpora. Using the manually annotated English Chinese Translation Treebank, we show how our method automatically discovers transformations that accommodate differences in English and Chinese syntax. Furthermore, when transformations are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic MT system, the transformed trees achieve a 0.9 BLEU improvement over baseline trees.</p><p>2 0.75660098 <a title="127-lsi-2" href="./emnlp-2012-Iterative_Annotation_Transformation_with_Predict-Self_Reestimation_for_Chinese_Word_Segmentation.html">68 emnlp-2012-Iterative Annotation Transformation with Predict-Self Reestimation for Chinese Word Segmentation</a></p>
<p>Author: Wenbin Jiang ; Fandong Meng ; Qun Liu ; Yajuan Lu</p><p>Abstract: In this paper we first describe the technology of automatic annotation transformation, which is based on the annotation adaptation algorithm (Jiang et al., 2009). It can automatically transform a human-annotated corpus from one annotation guideline to another. We then propose two optimization strategies, iterative training and predict-selfreestimation, to further improve the accuracy of annotation guideline transformation. Experiments on Chinese word segmentation show that, the iterative training strategy together with predictself reestimation brings significant improvement over the simple annotation transformation baseline, and leads to classifiers with significantly higher accuracy and several times faster processing than annotation adaptation does. On the Penn Chinese Treebank 5.0, , it achieves an F-measure of 98.43%, significantly outperforms previous works although using a single classifier with only local features.</p><p>3 0.58640361 <a title="127-lsi-3" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>Author: Jonathan K. Kummerfeld ; David Hall ; James R. Curran ; Dan Klein</p><p>Abstract: Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors. We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for stateof-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.</p><p>4 0.50792623 <a title="127-lsi-4" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>Author: Shujie Liu ; Chi-Ho Li ; Mu Li ; Ming Zhou</p><p>Abstract: The training of most syntactic SMT approaches involves two essential components, word alignment and monolingual parser. In the current state of the art these two components are mutually independent, thus causing problems like lack of rule generalization, and violation of syntactic correspondence in translation rules. In this paper, we propose two ways of re-training monolingual parser with the target of maximizing the consistency between parse trees and alignment matrices. One is targeted self-training with a simple evaluation function; the other is based on training data selection from forced alignment of bilingual data. We also propose an auxiliary method for boosting alignment quality, by symmetrizing alignment matrices with respect to parse trees. The best combination of these novel methods achieves 3 Bleu point gain in an IWSLT task and more than 1 Bleu point gain in NIST tasks. 1</p><p>5 0.41119522 <a title="127-lsi-5" href="./emnlp-2012-Left-to-Right_Tree-to-String_Decoding_with_Prediction.html">82 emnlp-2012-Left-to-Right Tree-to-String Decoding with Prediction</a></p>
<p>Author: Yang Feng ; Yang Liu ; Qun Liu ; Trevor Cohn</p><p>Abstract: Decoding algorithms for syntax based machine translation suffer from high computational complexity, a consequence of intersecting a language model with a context free grammar. Left-to-right decoding, which generates the target string in order, can improve decoding efficiency by simplifying the language model evaluation. This paper presents a novel left to right decoding algorithm for tree-to-string translation, using a bottom-up parsing strategy and dynamic future cost estimation for each partial translation. Our method outperforms previously published tree-to-string decoders, including a competing left-to-right method.</p><p>6 0.38419995 <a title="127-lsi-6" href="./emnlp-2012-Forced_Derivation_Tree_based_Model_Training_to_Statistical_Machine_Translation.html">54 emnlp-2012-Forced Derivation Tree based Model Training to Statistical Machine Translation</a></p>
<p>7 0.35692137 <a title="127-lsi-7" href="./emnlp-2012-Training_Factored_PCFGs_with_Expectation_Propagation.html">126 emnlp-2012-Training Factored PCFGs with Expectation Propagation</a></p>
<p>8 0.33879834 <a title="127-lsi-8" href="./emnlp-2012-A_Bayesian_Model_for_Learning_SCFGs_with_Discontiguous_Rules.html">1 emnlp-2012-A Bayesian Model for Learning SCFGs with Discontiguous Rules</a></p>
<p>9 0.33654028 <a title="127-lsi-9" href="./emnlp-2012-Inducing_a_Discriminative_Parser_to_Optimize_Machine_Translation_Reordering.html">67 emnlp-2012-Inducing a Discriminative Parser to Optimize Machine Translation Reordering</a></p>
<p>10 0.31062743 <a title="127-lsi-10" href="./emnlp-2012-Probabilistic_Finite_State_Machines_for_Regression-based_MT_Evaluation.html">108 emnlp-2012-Probabilistic Finite State Machines for Regression-based MT Evaluation</a></p>
<p>11 0.30767542 <a title="127-lsi-11" href="./emnlp-2012-Characterizing_Stylistic_Elements_in_Syntactic_Structure.html">27 emnlp-2012-Characterizing Stylistic Elements in Syntactic Structure</a></p>
<p>12 0.30648586 <a title="127-lsi-12" href="./emnlp-2012-Locally_Training_the_Log-Linear_Model_for_SMT.html">86 emnlp-2012-Locally Training the Log-Linear Model for SMT</a></p>
<p>13 0.29965717 <a title="127-lsi-13" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>14 0.29171678 <a title="127-lsi-14" href="./emnlp-2012-Extending_Machine_Translation_Evaluation_Metrics_with_Lexical_Cohesion_to_Document_Level.html">50 emnlp-2012-Extending Machine Translation Evaluation Metrics with Lexical Cohesion to Document Level</a></p>
<p>15 0.28860438 <a title="127-lsi-15" href="./emnlp-2012-Dynamic_Programming_for_Higher_Order_Parsing_of_Gap-Minding_Trees.html">37 emnlp-2012-Dynamic Programming for Higher Order Parsing of Gap-Minding Trees</a></p>
<p>16 0.2860392 <a title="127-lsi-16" href="./emnlp-2012-Name_Phylogeny%3A_A_Generative_Model_of_String_Variation.html">96 emnlp-2012-Name Phylogeny: A Generative Model of String Variation</a></p>
<p>17 0.27829552 <a title="127-lsi-17" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>18 0.27146903 <a title="127-lsi-18" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>19 0.26311827 <a title="127-lsi-19" href="./emnlp-2012-Multiple_Aspect_Summarization_Using_Integer_Linear_Programming.html">94 emnlp-2012-Multiple Aspect Summarization Using Integer Linear Programming</a></p>
<p>20 0.26250648 <a title="127-lsi-20" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.019), (11, 0.301), (14, 0.022), (16, 0.031), (25, 0.019), (29, 0.016), (34, 0.093), (60, 0.113), (63, 0.058), (65, 0.02), (70, 0.016), (74, 0.074), (76, 0.065), (80, 0.016), (86, 0.021), (95, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93129313 <a title="127-lda-1" href="./emnlp-2012-Dynamic_Programming_for_Higher_Order_Parsing_of_Gap-Minding_Trees.html">37 emnlp-2012-Dynamic Programming for Higher Order Parsing of Gap-Minding Trees</a></p>
<p>Author: Emily Pitler ; Sampath Kannan ; Mitchell Marcus</p><p>Abstract: We introduce gap inheritance, a new structural property on trees, which provides a way to quantify the degree to which intervals of descendants can be nested. Based on this property, two new classes of trees are derived that provide a closer approximation to the set of plausible natural language dependency trees than some alternative classes of trees: unlike projective trees, a word can have descendants in more than one interval; unlike spanning trees, these intervals cannot be nested in arbitrary ways. The 1-Inherit class of trees has exactly the same empirical coverage of natural language sentences as the class of mildly nonprojective trees, yet the optimal scoring tree can be found in an order of magnitude less time. Gap-minding trees (the second class) have the property that all edges into an interval of descendants come from the same node, and thus an algorithm which uses only single in- tervals can produce trees in which a node has descendants in multiple intervals.</p><p>same-paper 2 0.81386644 <a title="127-lda-2" href="./emnlp-2012-Transforming_Trees_to_Improve_Syntactic_Convergence.html">127 emnlp-2012-Transforming Trees to Improve Syntactic Convergence</a></p>
<p>Author: David Burkett ; Dan Klein</p><p>Abstract: We describe a transformation-based learning method for learning a sequence of monolingual tree transformations that improve the agreement between constituent trees and word alignments in bilingual corpora. Using the manually annotated English Chinese Translation Treebank, we show how our method automatically discovers transformations that accommodate differences in English and Chinese syntax. Furthermore, when transformations are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic MT system, the transformed trees achieve a 0.9 BLEU improvement over baseline trees.</p><p>3 0.53371024 <a title="127-lda-3" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>Author: Greg Durrett ; Adam Pauls ; Dan Klein</p><p>Abstract: We consider the problem of using a bilingual dictionary to transfer lexico-syntactic information from a resource-rich source language to a resource-poor target language. In contrast to past work that used bitexts to transfer analyses of specific sentences at the token level, we instead use features to transfer the behavior of words at a type level. In a discriminative dependency parsing framework, our approach produces gains across a range of target languages, using two different lowresource training methodologies (one weakly supervised and one indirectly supervised) and two different dictionary sources (one manually constructed and one automatically constructed).</p><p>4 0.53300643 <a title="127-lda-4" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: We present a new family of models for unsupervised parsing, Dependency and Boundary models, that use cues at constituent boundaries to inform head-outward dependency tree generation. We build on three intuitions that are explicit in phrase-structure grammars but only implicit in standard dependency formulations: (i) Distributions of words that occur at sentence boundaries such as English determiners resemble constituent edges. (ii) Punctuation at sentence boundaries further helps distinguish full sentences from fragments like headlines and titles, allowing us to model grammatical differences between complete and incomplete sentences. (iii) Sentence-internal punctuation boundaries help with longer-distance dependencies, since punctuation correlates with constituent edges. Our models induce state-of-the-art dependency grammars for many languages without — — special knowledge of optimal input sentence lengths or biased, manually-tuned initializers.</p><p>5 0.52909827 <a title="127-lda-5" href="./emnlp-2012-Left-to-Right_Tree-to-String_Decoding_with_Prediction.html">82 emnlp-2012-Left-to-Right Tree-to-String Decoding with Prediction</a></p>
<p>Author: Yang Feng ; Yang Liu ; Qun Liu ; Trevor Cohn</p><p>Abstract: Decoding algorithms for syntax based machine translation suffer from high computational complexity, a consequence of intersecting a language model with a context free grammar. Left-to-right decoding, which generates the target string in order, can improve decoding efficiency by simplifying the language model evaluation. This paper presents a novel left to right decoding algorithm for tree-to-string translation, using a bottom-up parsing strategy and dynamic future cost estimation for each partial translation. Our method outperforms previously published tree-to-string decoders, including a competing left-to-right method.</p><p>6 0.52417409 <a title="127-lda-6" href="./emnlp-2012-Improving_Transition-Based_Dependency_Parsing_with_Buffer_Transitions.html">66 emnlp-2012-Improving Transition-Based Dependency Parsing with Buffer Transitions</a></p>
<p>7 0.52028626 <a title="127-lda-7" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>8 0.51783091 <a title="127-lda-8" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>9 0.51587754 <a title="127-lda-9" href="./emnlp-2012-Inducing_a_Discriminative_Parser_to_Optimize_Machine_Translation_Reordering.html">67 emnlp-2012-Inducing a Discriminative Parser to Optimize Machine Translation Reordering</a></p>
<p>10 0.51516312 <a title="127-lda-10" href="./emnlp-2012-Source_Language_Adaptation_for_Resource-Poor_Machine_Translation.html">118 emnlp-2012-Source Language Adaptation for Resource-Poor Machine Translation</a></p>
<p>11 0.51493543 <a title="127-lda-11" href="./emnlp-2012-Exploiting_Reducibility_in_Unsupervised_Dependency_Parsing.html">46 emnlp-2012-Exploiting Reducibility in Unsupervised Dependency Parsing</a></p>
<p>12 0.50789297 <a title="127-lda-12" href="./emnlp-2012-Unsupervised_PCFG_Induction_for_Grounded_Language_Learning_with_Highly_Ambiguous_Supervision.html">133 emnlp-2012-Unsupervised PCFG Induction for Grounded Language Learning with Highly Ambiguous Supervision</a></p>
<p>13 0.50780797 <a title="127-lda-13" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>14 0.507568 <a title="127-lda-14" href="./emnlp-2012-Generating_Non-Projective_Word_Order_in_Statistical_Linearization.html">59 emnlp-2012-Generating Non-Projective Word Order in Statistical Linearization</a></p>
<p>15 0.5064317 <a title="127-lda-15" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>16 0.50539356 <a title="127-lda-16" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>17 0.50446892 <a title="127-lda-17" href="./emnlp-2012-Characterizing_Stylistic_Elements_in_Syntactic_Structure.html">27 emnlp-2012-Characterizing Stylistic Elements in Syntactic Structure</a></p>
<p>18 0.49935234 <a title="127-lda-18" href="./emnlp-2012-Forced_Derivation_Tree_based_Model_Training_to_Statistical_Machine_Translation.html">54 emnlp-2012-Forced Derivation Tree based Model Training to Statistical Machine Translation</a></p>
<p>19 0.4981029 <a title="127-lda-19" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>20 0.49742287 <a title="127-lda-20" href="./emnlp-2012-Exploiting_Chunk-level_Features_to_Improve_Phrase_Chunking.html">45 emnlp-2012-Exploiting Chunk-level Features to Improve Phrase Chunking</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
