<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>126 emnlp-2012-Training Factored PCFGs with Expectation Propagation</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-126" href="#">emnlp2012-126</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>126 emnlp-2012-Training Factored PCFGs with Expectation Propagation</h1>
<br/><p>Source: <a title="emnlp-2012-126-pdf" href="http://aclweb.org/anthology//D/D12/D12-1105.pdf">pdf</a></p><p>Author: David Hall ; Dan Klein</p><p>Abstract: PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar. We present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion. Our method works with linguisticallymotivated annotations, induced latent structure, lexicalization, or any mix of the three. We use a structured expectation propagation algorithm that makes use of the factored structure in two ways. First, by partitioning the factors, it speeds up parsing exponentially over the unfactored approach. Second, it minimizes the redundancy of the factors during training, improving accuracy over an independent approach. Using purely latent variable annotations, we can efficiently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the na¨ ıve approach. Combining latent, lexicalized, and unlexicalized anno- tations, our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank.</p><p>Reference: <a title="emnlp-2012-126-reference" href="../emnlp2012_reference/emnlp-2012-Training_Factored_PCFGs_with_Expectation_Propagation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar. [sent-3, score-0.377]
</p><p>2 We present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion. [sent-4, score-0.546]
</p><p>3 We use a structured expectation propagation algorithm that makes use of the factored structure in two ways. [sent-6, score-0.497]
</p><p>4 Using purely latent variable annotations, we can efficiently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88. [sent-9, score-0.392]
</p><p>5 Combining latent, lexicalized, and unlexicalized anno-  tations, our best parser gets 89. [sent-11, score-0.175]
</p><p>6 1 Introduction Many high-performance PCFG parsers take an initially simple base grammar over treebank labels like NP and enrich it with deeper syntactic features to improve accuracy. [sent-13, score-0.379]
</p><p>7 This broad characterization includes lexicalized parsers (Collins, 1997), unlexicalized parsers (Klein and Manning, 2003), and latent variable parsers (Matsuzaki et al. [sent-14, score-0.517]
</p><p>8 When multi-part annotations are used in the same grammar, systems have generally multiplied these annotations together, in the sense that an NP that 1146 was definite, possessive, and VP-dominated would have a single unstructured PCFG symbol that encoded all three facts. [sent-17, score-0.724]
</p><p>9 In addition, modulo backoff or smoothing, that unstructured symbol would often have rewrite parameters entirely distinct from, say, the indefinite but otherwise similar variant of  the symbol (Klein and Manning, 2003). [sent-18, score-0.277]
</p><p>10 Should a definiteness annotation be included, doubling the number of NPs in the grammar and perhaps overly fragmenting statistics? [sent-20, score-0.369]
</p><p>11 Klein and Manning (2003) discuss exactly such trade-offs and omit annotations that were helpful on their own because they were not worth the combinatorial or statistical cost when combined with other annotations. [sent-22, score-0.342]
</p><p>12 In this paper, we argue for grammars with factored annotations, that is, grammars with annotations that have structured component parts that are partially decoupled. [sent-23, score-0.936]
</p><p>13 Our annotated grammars can include both latent and explicit annotations, as illustrated in Figure 1(d), and we demonstrate that these factored grammars outperform parsers with unstructured annotations. [sent-24, score-0.946]
</p><p>14 After discussing the factored representation, we describe a method for parsing with factored annotations, using an approximate inference technique called expectation propagation (Minka, 2001). [sent-25, score-0.895]
</p><p>15 Our algorithm has runtime linear in the number of an-  notation factors in the grammar, improving on the na¨ ıve algorithm, which has runtime exponential in the number of annotations. [sent-26, score-0.268]
</p><p>16 Our method, the Expectation Propagation for Inferring Constituency (EPIC) parser, jointly trains a model over factored annotations, where each factor naturally leverages information from other annotation factors and improves on their mistakes. [sent-27, score-0.602]
</p><p>17 (2005); and (d) the factored, mixed annotations we argue for in our paper. [sent-31, score-0.27]
</p><p>18 First, we efficiently train a latent-variable grammar with 8 disjoint one-bit latent annotation factors, with scores as high as 89. [sent-33, score-0.532]
</p><p>19 Second, we combine our latent variable factors with lexicalized and unlexicalized annotations, resulting in our best F1 score of 89. [sent-38, score-0.442]
</p><p>20 2  Intuitions  Modern theories of grammar such as HPSG (Pollard and Sag, 1994) and Minimalism (Chomsky, 1992) do not ascribe unstructured conjunctions of annotations to phrasal categories. [sent-40, score-0.575]
</p><p>21 For instance, an NP might have annotations to the effect that it is singular, masculine, and nominative, with perhaps further information about its animacy or other aspects of the head noun. [sent-42, score-0.27]
</p><p>22 Thus, it is appealing for a grammar to be able to model these (somewhat) orthogonal notions, but most models have no mechanism to encourage this. [sent-43, score-0.214]
</p><p>23 As a notable exception, Dreyer and Eisner (2006) tried to capture this kind ofinsight by allowing factored annotations to pass unchanged from parent label to child label, though they were not 1147 able to demonstrate substantial gains in accuracy. [sent-44, score-0.514]
</p><p>24 Moreover, there has been to our knowledge no attempt to employ both latent and non-latent annotations at the same time. [sent-45, score-0.392]
</p><p>25 There is good reason for this: lexicalized or highly annotated grammars like those of Collins (1997) or Klein and Manning (2003) have  a very large number of states and an even larger number of rules. [sent-46, score-0.37]
</p><p>26 Further annotating these rules with latent annotations would produce an infeasibly large grammar. [sent-47, score-0.392]
</p><p>27 Nevertheless, it is a shame to sacrifice expert annotation just to get latent annotations. [sent-48, score-0.277]
</p><p>28 We are interested in the specific case where each x is actually factored into M disjoint parts: A[x1, x2, . [sent-56, score-0.285]
</p><p>29 ) We call each component of x an annotation factor  or an annotation component. [sent-61, score-0.452]
</p><p>30 1 Annotation Classes In this paper, we consider three kinds of annotation models, representing three of the major traditions in constituency parsing. [sent-63, score-0.201]
</p><p>31 This parser starts from a grammar with labels annotated with sibling and parent information, and then adds specific annotations, such as whether an NP is possessive or whether a symbol rewrites as a unary. [sent-71, score-0.486]
</p><p>32 Here, each symbol is given a latent annotation, referred to as a substate. [sent-75, score-0.215]
</p><p>33 Often, these latent integers are considered as bit strings, with each bit indicating one latent annotation. [sent-78, score-0.332]
</p><p>34 , 2006; Petrov and Klein, 2007), as well as “multiscale” grammars (Petrov and Klein, 2008b). [sent-80, score-0.18]
</p><p>35 With two states (or one bit of annotation), our version of this parser gets 81. [sent-81, score-0.186]
</p><p>36 Formally, we begin with a parse tree T over base symbols for some sentence w, and we decorate the tree with annotations X, giving a parse tree T[X]. [sent-91, score-0.526]
</p><p>37 These components are decoupled in the sense that, conditioned on the coarse tree T, each column of the annotation is independent of every other column. [sent-96, score-0.292]
</p><p>38 The conditional probability P(T[X] |w, θ) of an annTohteate codn ntrdeieti given wroboardbsi i ts:y P(T[X]|w, θ)  =PT0Q,Xm0Qfmm(fTm[(XTm0[]X;wm0,]θ;wm),θm)  (1)  =PZ(w1,θ)QYmfm(T[Xm];w,θm) where the factors fm for each model take the form: fm(T[Xm]; w, θm) = exp ? [sent-99, score-0.322]
</p><p>39 The features ϕ need to decompose into features for each factor fm; we do not allow features that take into account the annotation from two different components. [sent-103, score-0.235]
</p><p>40 We further add a pruning filter that assigns zero weight to any tree with a constituent that a baseline unannotated grammar finds sufficiently unlikely, and a weight ofone to any other tree. [sent-104, score-0.433]
</p><p>41 (a) The full joint distribution consists of a product of three grammars with different annotations, here lexicalized, latent, and unlexicalized. [sent-109, score-0.315]
</p><p>42 (b) The core approximation is an anchored PCFG with one factor corresponding to each annotation component, described in Section 5. [sent-112, score-0.723]
</p><p>43 (c) Fitting the approximation with expectation propagation, as described in Section 5. [sent-114, score-0.243]
</p><p>44 During each step, an “augmented” distribution qm is created by taking one annotation factor from the full grammar and the rest from the approximate grammar. [sent-117, score-0.797]
</p><p>45 For instance, in upper left hand corner the full fLEX is substituted for This new augmented distribution is projected back to the core approximation. [sent-118, score-0.212]
</p><p>46 4  The Complexity of Annotated Grammars  Note that the first term of Equation 3—which is conditioned on the coarse tree T—factors into M pieces, one for each of the annotation components. [sent-129, score-0.252]
</p><p>47 Let G0 be the number of binary rules in the unannotated “base” grammar. [sent-133, score-0.174]
</p><p>48 Each annotation component can have up to A primi-  tive annotations per rule. [sent-135, score-0.487]
</p><p>49 For instance, a latent variable grammar will have A = 8b where b is the number of bits of annotation. [sent-136, score-0.439]
</p><p>50 If we compile all annotation components into unstructured annotations, we can end up with a total grammar size of O(AMG0), and so in general parsing time scales exponentially with the number of annotation components. [sent-137, score-0.753]
</p><p>51 Thus, if we use latent annotations and the hierarchical splitting approach of Petrov et al. [sent-138, score-0.423]
</p><p>52 (2006), then the grammar has size O(8SG0), where S is the number of times the grammar was split in two. [sent-139, score-0.428]
</p><p>53 Therefore, the size of annotated grammars can reach intractable levels very quickly, particularly in the case of latent annotations, where all combinations of annotations are possible. [sent-140, score-0.635]
</p><p>54 Petrov (2010) considered an approach to slowing this growth down by using a set of M independently trained parsers Pm, and parsed using the product of the scores from each parser as the score for the tree. [sent-141, score-0.215]
</p><p>55 In what follows, we propose another solution that exploits the factored structure of our grammar with expectation propagation. [sent-144, score-0.601]
</p><p>56 Crucially, we are able to jointly train and parse with all annotation factors, minimizing redundancy across the models. [sent-145, score-0.229]
</p><p>57 While not exact, we will see that expectation propagation is indeed effective. [sent-146, score-0.253]
</p><p>58 5  Factored Inference  The key insight behind the approximate inference methods we consider here is that the full model is a product of complex factors that interact in complicated ways, and we will approximate it with a product of corresponding simple factors that interact in simple ways. [sent-147, score-0.684]
</p><p>59 Since each annotation factor is a reasonable model in both power and complexity on its own, we can consider them one at a time, replacing all others with their approximations, as shown in Figure 2(c). [sent-148, score-0.267]
</p><p>60 The way we will build these approximations is 1150 with expectation propagation (Minka, 2001). [sent-149, score-0.363]
</p><p>61 Expectation propagation (EP) is a general method for  approximate inference that generalizes belief propagation. [sent-150, score-0.207]
</p><p>62 To begin, we note that each of these components captures different phenomena: an unlexicalized grammar is good at capturing structural relationships in a parse tree (e. [sent-157, score-0.405]
</p><p>63 subject noun phrases have different distributions than object noun phrases), while a lexicalized grammar captures preferred attachments for different verbs. [sent-159, score-0.313]
</p><p>64 At the same time, each ofthese component grammars can be thought of as a refinement of the raw unannotated treebank grammar. [sent-160, score-0.478]
</p><p>65 By itself, each of these grammars induces a different posterior distribution over unannotated trees for each sen-  tence. [sent-161, score-0.544]
</p><p>66 If we can approximate each model’s contribution by using only unannotated symbols, we can define an algorithm that avoids the exponential overhead of parsing with the full grammar, and instead works with each factor in turn. [sent-162, score-0.453]
</p><p>67 To do so, we define a sentence specific core aQpproximation over unannotated trees q(T|w) = Qm of˜mxi(mTa,t w). [sent-163, score-0.275]
</p><p>68 We will approximate each model fm by its corresponding f˜m. [sent-166, score-0.296]
</p><p>69 Thus, there is one colorcoordinated approximate factor for each component of the model in Figure 2(a). [sent-167, score-0.239]
</p><p>70 Here, iAj is a symbol representing building the base symbol A over the span [i, j] . [sent-170, score-0.217]
</p><p>71 Billott and Lang (1989) introduced anchored CFGs as “shared forests,” and Matsuzaki et al. [sent-171, score-0.335]
</p><p>72 (2005) have previously used these grammars for finding an approximate one-best tree in a latent vari-  able parser. [sent-172, score-0.444]
</p><p>73 Note that, even though an anchored grammar is unannotated, because it is sentence specific it can represent many complex properties of the full grammar’s posterior distribution for a given sentence. [sent-173, score-0.691]
</p><p>74 Before continuing, note that a pointwise product of anchored grammars is still an anchored grammar. [sent-175, score-0.935]
</p><p>75 The complexity of parsing with a product of these grammars is therefore no more expensive than parsing with just one. [sent-176, score-0.411]
</p><p>76 Indeed, anchoring adds no inferential cost at all over parsing with an unannotated grammar: the anchored indices i,j,k have to be computed just to parse the sentence at all. [sent-177, score-0.611]
</p><p>77 2 Assumed Density Filtering We now describe a simplified version of EP: parsing with assumed density filtering (Boyen and Koller, 1998). [sent-180, score-0.224]
</p><p>78 Then, we factor in one of the annotated grammars and parse with this new augmented grammar. [sent-185, score-0.445]
</p><p>79 This gives us a new posterior distribution for this sentence over trees annotated with just that annotation component. [sent-186, score-0.408]
</p><p>80 Then, we can marginalize out the annotations, giving us a new q that approximates the annotated grammar as closely as possible without using any annotations. [sent-187, score-0.277]
</p><p>81 In this way, information from all grammars is incorporated into a final posterior distribution over trees 1151 using only unannotated symbols. [sent-189, score-0.544]
</p><p>82 by  Step 1 of the inner looQp forms an approximate posterior distribution using fm, which is the parsing model associated with component m, and q, which is the anchored core approximation to the posterior induced by the first m − 1 models. [sent-200, score-0.938]
</p><p>83 Then, the marginals are computed, mand − th 1e m new posterior ,d tihsetribution is projected to an anchored grammar, creating More intuitively, we create an anchored PCFG that makes the approximation close as possible” to the augmented grammar. [sent-201, score-0.971]
</p><p>84 ) Thus, each term fm is approximated in the context of the terms that come before it. [sent-204, score-0.199]
</p><p>85 would have complexity O Critically, for a latent variable parser? [sent-222, score-0.191]
</p><p>86 otation bits, the exact algorithm takes time exponential in M, while this approximate algorithm takes time linear in M. [sent-224, score-0.208]
</p><p>87 At each step, we have in q an approximation to what the posterior distribution looks like with the first m − 1 models. [sent-226, score-0.242]
</p><p>88 Intuitively, EP cycles among the models, updating the approximation for that model in turn so that it closely resembles the predictions made by fm in the context of all other approximations, as in Figure 2(c). [sent-234, score-0.362]
</p><p>89 Thus, each approximate term is created using information from all other meaning that the different annotation factors can still “talk” to each other. [sent-235, score-0.375]
</p><p>90 The product of these approximations q will therefore come to act as an approximation to the true posterior: it takes into account joint infor-  f˜m f˜m0,  mation about all annotation components, all within one tractable anchored grammar. [sent-236, score-0.818]
</p><p>91 With that intuition in mind, EP is defined as follows: •  •  Initialize contributions f˜m to the approximate posterior q. [sent-237, score-0.189]
</p><p>92 Create the augmentQed distribution by including the actual factor for component m  fm(T[Xm])q\m(T)  qm(T[Xm]) ∝ and compute ]i)ns ∝ide f and outside scores. [sent-242, score-0.192]
</p><p>93 1152 Step 2 creates the augmented distribution qm, which includes fm along with the approximate factors for all models except the current model. [sent-250, score-0.546]
</p><p>94 Step 3 creates  f˜m  a new anchored that has the same marginal distribution as the true model fm in the context of the other approximations, just as we did in ADF. [sent-251, score-0.584]
</p><p>95 At each step, one piece of the core approximation is replaced with the corresponding component from the full model. [sent-255, score-0.215]
</p><p>96 This augmented model is then reapproximated by a new core approximation q after updating the corresponding This process repeats until convergence. [sent-256, score-0.261]
</p><p>97 q and each of the are anchored grammars that assign weights to unannotated rules. [sent-260, score-0.689]
</p><p>98 The product of anchored grammars with the annotated factor fm need not be carried out explicitly. [sent-261, score-0.942]
</p><p>99 Instead, note that an anchored grammar is just a function q(A →  f˜m  tBh C, i, k, j) ∈ dR g+r tahmatm returns a score cfotiro every anBcho Cre,di, binary r uRle. [sent-262, score-0.549]
</p><p>100 This function can be easily integrated into the CKY algorithm for a single annotated grammar by simply multiplying in the value of q whenever computing the score of the respective production over some span. [sent-263, score-0.316]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('anchored', 0.335), ('annotations', 0.27), ('factored', 0.244), ('grammar', 0.214), ('qm', 0.201), ('fm', 0.199), ('grammars', 0.18), ('unannotated', 0.174), ('xm', 0.174), ('agenda', 0.155), ('annotation', 0.155), ('expectation', 0.143), ('ep', 0.133), ('petrov', 0.126), ('klein', 0.123), ('factors', 0.123), ('latent', 0.122), ('propagation', 0.11), ('approximations', 0.11), ('np', 0.104), ('approximation', 0.1), ('lexicalized', 0.099), ('approximate', 0.097), ('symbol', 0.093), ('posterior', 0.092), ('unstructured', 0.091), ('adf', 0.086), ('product', 0.085), ('factor', 0.08), ('augmented', 0.077), ('minka', 0.067), ('matsuzaki', 0.066), ('bits', 0.066), ('parsers', 0.066), ('parser', 0.064), ('annotated', 0.063), ('binarization', 0.062), ('component', 0.062), ('unlexicalized', 0.061), ('inside', 0.059), ('boyen', 0.057), ('iaj', 0.057), ('parsing', 0.057), ('pcfg', 0.055), ('density', 0.055), ('manning', 0.054), ('president', 0.053), ('core', 0.053), ('possessive', 0.052), ('conditioned', 0.052), ('gets', 0.05), ('distribution', 0.05), ('runtime', 0.05), ('epic', 0.049), ('dkl', 0.049), ('derivative', 0.049), ('pcfgs', 0.048), ('trees', 0.048), ('constituency', 0.046), ('tree', 0.045), ('exponential', 0.045), ('parse', 0.045), ('schematically', 0.045), ('bit', 0.044), ('filtering', 0.043), ('exponentially', 0.041), ('disjoint', 0.041), ('lexicalization', 0.041), ('components', 0.04), ('omit', 0.04), ('assumed', 0.039), ('xd', 0.039), ('multiplying', 0.039), ('variable', 0.037), ('beliefs', 0.037), ('koller', 0.037), ('crucially', 0.037), ('interact', 0.037), ('collins', 0.036), ('na', 0.036), ('penn', 0.035), ('fitting', 0.035), ('initially', 0.035), ('nn', 0.034), ('productions', 0.033), ('takes', 0.033), ('treebank', 0.033), ('complexity', 0.032), ('resembles', 0.032), ('combinatorial', 0.032), ('projected', 0.032), ('base', 0.031), ('splitting', 0.031), ('updating', 0.031), ('simplified', 0.03), ('division', 0.03), ('raw', 0.029), ('nps', 0.029), ('redundancy', 0.029), ('states', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="126-tfidf-1" href="./emnlp-2012-Training_Factored_PCFGs_with_Expectation_Propagation.html">126 emnlp-2012-Training Factored PCFGs with Expectation Propagation</a></p>
<p>Author: David Hall ; Dan Klein</p><p>Abstract: PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar. We present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion. Our method works with linguisticallymotivated annotations, induced latent structure, lexicalization, or any mix of the three. We use a structured expectation propagation algorithm that makes use of the factored structure in two ways. First, by partitioning the factors, it speeds up parsing exponentially over the unfactored approach. Second, it minimizes the redundancy of the factors during training, improving accuracy over an independent approach. Using purely latent variable annotations, we can efficiently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the na¨ ıve approach. Combining latent, lexicalized, and unlexicalized anno- tations, our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank.</p><p>2 0.15680528 <a title="126-tfidf-2" href="./emnlp-2012-Improving_NLP_through_Marginalization_of_Hidden_Syntactic_Structure.html">65 emnlp-2012-Improving NLP through Marginalization of Hidden Syntactic Structure</a></p>
<p>Author: Jason Naradowsky ; Sebastian Riedel ; David Smith</p><p>Abstract: Many NLP tasks make predictions that are inherently coupled to syntactic relations, but for many languages the resources required to provide such syntactic annotations are unavailable. For others it is unclear exactly how much of the syntactic annotations can be effectively leveraged with current models, and what structures in the syntactic trees are most relevant to the current task. We propose a novel method which avoids the need for any syntactically annotated data when predicting a related NLP task. Our method couples latent syntactic representations, constrained to form valid dependency graphs or constituency parses, with the prediction task via specialized factors in a Markov random field. At both training and test time we marginalize over this hidden structure, learning the optimal latent representations for the problem. Results show that this approach provides significant gains over a syntactically uninformed baseline, outperforming models that observe syntax on an English relation extraction task, and performing comparably to them in semantic role labeling.</p><p>3 0.12253343 <a title="126-tfidf-3" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>Author: Jonathan K. Kummerfeld ; David Hall ; James R. Curran ; Dan Klein</p><p>Abstract: Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors. We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for stateof-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.</p><p>4 0.11596425 <a title="126-tfidf-4" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: We present a new family of models for unsupervised parsing, Dependency and Boundary models, that use cues at constituent boundaries to inform head-outward dependency tree generation. We build on three intuitions that are explicit in phrase-structure grammars but only implicit in standard dependency formulations: (i) Distributions of words that occur at sentence boundaries such as English determiners resemble constituent edges. (ii) Punctuation at sentence boundaries further helps distinguish full sentences from fragments like headlines and titles, allowing us to model grammatical differences between complete and incomplete sentences. (iii) Sentence-internal punctuation boundaries help with longer-distance dependencies, since punctuation correlates with constituent edges. Our models induce state-of-the-art dependency grammars for many languages without — — special knowledge of optimal input sentence lengths or biased, manually-tuned initializers.</p><p>5 0.10434116 <a title="126-tfidf-5" href="./emnlp-2012-Unambiguity_Regularization_for_Unsupervised_Learning_of_Probabilistic_Grammars.html">130 emnlp-2012-Unambiguity Regularization for Unsupervised Learning of Probabilistic Grammars</a></p>
<p>Author: Kewei Tu ; Vasant Honavar</p><p>Abstract: We introduce a novel approach named unambiguity regularization for unsupervised learning of probabilistic natural language grammars. The approach is based on the observation that natural language is remarkably unambiguous in the sense that only a tiny portion of the large number of possible parses of a natural language sentence are syntactically valid. We incorporate an inductive bias into grammar learning in favor of grammars that lead to unambiguous parses on natural language sentences. The resulting family of algorithms includes the expectation-maximization algorithm (EM) and its variant, Viterbi EM, as well as a so-called softmax-EM algorithm. The softmax-EM algorithm can be implemented with a simple and computationally efficient extension to standard EM. In our experiments of unsupervised dependency grammar learn- ing, we show that unambiguity regularization is beneficial to learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art.</p><p>6 0.10245813 <a title="126-tfidf-6" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>7 0.10047762 <a title="126-tfidf-7" href="./emnlp-2012-Generalized_Higher-Order_Dependency_Parsing_with_Cube_Pruning.html">57 emnlp-2012-Generalized Higher-Order Dependency Parsing with Cube Pruning</a></p>
<p>8 0.095851392 <a title="126-tfidf-8" href="./emnlp-2012-Document-Wide_Decoding_for_Phrase-Based_Statistical_Machine_Translation.html">35 emnlp-2012-Document-Wide Decoding for Phrase-Based Statistical Machine Translation</a></p>
<p>9 0.095250569 <a title="126-tfidf-9" href="./emnlp-2012-Spectral_Dependency_Parsing_with_Latent_Variables.html">119 emnlp-2012-Spectral Dependency Parsing with Latent Variables</a></p>
<p>10 0.094397753 <a title="126-tfidf-10" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<p>11 0.093108088 <a title="126-tfidf-11" href="./emnlp-2012-A_Bayesian_Model_for_Learning_SCFGs_with_Discontiguous_Rules.html">1 emnlp-2012-A Bayesian Model for Learning SCFGs with Discontiguous Rules</a></p>
<p>12 0.090285257 <a title="126-tfidf-12" href="./emnlp-2012-Exploring_Adaptor_Grammars_for_Native_Language_Identification.html">48 emnlp-2012-Exploring Adaptor Grammars for Native Language Identification</a></p>
<p>13 0.08627478 <a title="126-tfidf-13" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>14 0.082073763 <a title="126-tfidf-14" href="./emnlp-2012-Iterative_Annotation_Transformation_with_Predict-Self_Reestimation_for_Chinese_Word_Segmentation.html">68 emnlp-2012-Iterative Annotation Transformation with Predict-Self Reestimation for Chinese Word Segmentation</a></p>
<p>15 0.081090763 <a title="126-tfidf-15" href="./emnlp-2012-Monte_Carlo_MCMC%3A_Efficient_Inference_by_Approximate_Sampling.html">91 emnlp-2012-Monte Carlo MCMC: Efficient Inference by Approximate Sampling</a></p>
<p>16 0.078423895 <a title="126-tfidf-16" href="./emnlp-2012-Transforming_Trees_to_Improve_Syntactic_Convergence.html">127 emnlp-2012-Transforming Trees to Improve Syntactic Convergence</a></p>
<p>17 0.074741706 <a title="126-tfidf-17" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>18 0.064525865 <a title="126-tfidf-18" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>19 0.062583245 <a title="126-tfidf-19" href="./emnlp-2012-Inducing_a_Discriminative_Parser_to_Optimize_Machine_Translation_Reordering.html">67 emnlp-2012-Inducing a Discriminative Parser to Optimize Machine Translation Reordering</a></p>
<p>20 0.060303375 <a title="126-tfidf-20" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.237), (1, -0.119), (2, 0.118), (3, -0.042), (4, -0.022), (5, 0.019), (6, -0.051), (7, 0.017), (8, -0.002), (9, 0.15), (10, -0.014), (11, 0.186), (12, -0.023), (13, 0.056), (14, -0.129), (15, 0.136), (16, 0.015), (17, 0.039), (18, -0.231), (19, -0.06), (20, -0.049), (21, 0.145), (22, -0.042), (23, 0.091), (24, 0.05), (25, 0.075), (26, 0.062), (27, -0.022), (28, -0.001), (29, 0.07), (30, 0.118), (31, -0.124), (32, 0.144), (33, 0.031), (34, 0.14), (35, -0.072), (36, 0.147), (37, -0.205), (38, -0.009), (39, -0.095), (40, 0.014), (41, -0.08), (42, -0.055), (43, -0.026), (44, -0.052), (45, 0.009), (46, -0.104), (47, -0.047), (48, -0.031), (49, -0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97781229 <a title="126-lsi-1" href="./emnlp-2012-Training_Factored_PCFGs_with_Expectation_Propagation.html">126 emnlp-2012-Training Factored PCFGs with Expectation Propagation</a></p>
<p>Author: David Hall ; Dan Klein</p><p>Abstract: PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar. We present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion. Our method works with linguisticallymotivated annotations, induced latent structure, lexicalization, or any mix of the three. We use a structured expectation propagation algorithm that makes use of the factored structure in two ways. First, by partitioning the factors, it speeds up parsing exponentially over the unfactored approach. Second, it minimizes the redundancy of the factors during training, improving accuracy over an independent approach. Using purely latent variable annotations, we can efficiently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the na¨ ıve approach. Combining latent, lexicalized, and unlexicalized anno- tations, our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank.</p><p>2 0.68418735 <a title="126-lsi-2" href="./emnlp-2012-Improving_NLP_through_Marginalization_of_Hidden_Syntactic_Structure.html">65 emnlp-2012-Improving NLP through Marginalization of Hidden Syntactic Structure</a></p>
<p>Author: Jason Naradowsky ; Sebastian Riedel ; David Smith</p><p>Abstract: Many NLP tasks make predictions that are inherently coupled to syntactic relations, but for many languages the resources required to provide such syntactic annotations are unavailable. For others it is unclear exactly how much of the syntactic annotations can be effectively leveraged with current models, and what structures in the syntactic trees are most relevant to the current task. We propose a novel method which avoids the need for any syntactically annotated data when predicting a related NLP task. Our method couples latent syntactic representations, constrained to form valid dependency graphs or constituency parses, with the prediction task via specialized factors in a Markov random field. At both training and test time we marginalize over this hidden structure, learning the optimal latent representations for the problem. Results show that this approach provides significant gains over a syntactically uninformed baseline, outperforming models that observe syntax on an English relation extraction task, and performing comparably to them in semantic role labeling.</p><p>3 0.52945399 <a title="126-lsi-3" href="./emnlp-2012-Spectral_Dependency_Parsing_with_Latent_Variables.html">119 emnlp-2012-Spectral Dependency Parsing with Latent Variables</a></p>
<p>Author: Paramveer Dhillon ; Jordan Rodu ; Michael Collins ; Dean Foster ; Lyle Ungar</p><p>Abstract: Recently there has been substantial interest in using spectral methods to learn generative sequence models like HMMs. Spectral methods are attractive as they provide globally consistent estimates of the model parameters and are very fast and scalable, unlike EM methods, which can get stuck in local minima. In this paper, we present a novel extension of this class of spectral methods to learn dependency tree structures. We propose a simple yet powerful latent variable generative model for dependency parsing, and a spectral learning method to efficiently estimate it. As a pilot experimental evaluation, we use the spectral tree probabilities estimated by our model to re-rank the outputs of a near state-of-theart parser. Our approach gives us a moderate reduction in error of up to 4.6% over the baseline re-ranker. .</p><p>4 0.52024984 <a title="126-lsi-4" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: We present a new family of models for unsupervised parsing, Dependency and Boundary models, that use cues at constituent boundaries to inform head-outward dependency tree generation. We build on three intuitions that are explicit in phrase-structure grammars but only implicit in standard dependency formulations: (i) Distributions of words that occur at sentence boundaries such as English determiners resemble constituent edges. (ii) Punctuation at sentence boundaries further helps distinguish full sentences from fragments like headlines and titles, allowing us to model grammatical differences between complete and incomplete sentences. (iii) Sentence-internal punctuation boundaries help with longer-distance dependencies, since punctuation correlates with constituent edges. Our models induce state-of-the-art dependency grammars for many languages without — — special knowledge of optimal input sentence lengths or biased, manually-tuned initializers.</p><p>5 0.50552291 <a title="126-lsi-5" href="./emnlp-2012-Unambiguity_Regularization_for_Unsupervised_Learning_of_Probabilistic_Grammars.html">130 emnlp-2012-Unambiguity Regularization for Unsupervised Learning of Probabilistic Grammars</a></p>
<p>Author: Kewei Tu ; Vasant Honavar</p><p>Abstract: We introduce a novel approach named unambiguity regularization for unsupervised learning of probabilistic natural language grammars. The approach is based on the observation that natural language is remarkably unambiguous in the sense that only a tiny portion of the large number of possible parses of a natural language sentence are syntactically valid. We incorporate an inductive bias into grammar learning in favor of grammars that lead to unambiguous parses on natural language sentences. The resulting family of algorithms includes the expectation-maximization algorithm (EM) and its variant, Viterbi EM, as well as a so-called softmax-EM algorithm. The softmax-EM algorithm can be implemented with a simple and computationally efficient extension to standard EM. In our experiments of unsupervised dependency grammar learn- ing, we show that unambiguity regularization is beneficial to learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art.</p><p>6 0.44830438 <a title="126-lsi-6" href="./emnlp-2012-Exploring_Adaptor_Grammars_for_Native_Language_Identification.html">48 emnlp-2012-Exploring Adaptor Grammars for Native Language Identification</a></p>
<p>7 0.39717442 <a title="126-lsi-7" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>8 0.39154387 <a title="126-lsi-8" href="./emnlp-2012-Monte_Carlo_MCMC%3A_Efficient_Inference_by_Approximate_Sampling.html">91 emnlp-2012-Monte Carlo MCMC: Efficient Inference by Approximate Sampling</a></p>
<p>9 0.33887061 <a title="126-lsi-9" href="./emnlp-2012-A_Bayesian_Model_for_Learning_SCFGs_with_Discontiguous_Rules.html">1 emnlp-2012-A Bayesian Model for Learning SCFGs with Discontiguous Rules</a></p>
<p>10 0.33271188 <a title="126-lsi-10" href="./emnlp-2012-Generalized_Higher-Order_Dependency_Parsing_with_Cube_Pruning.html">57 emnlp-2012-Generalized Higher-Order Dependency Parsing with Cube Pruning</a></p>
<p>11 0.33108985 <a title="126-lsi-11" href="./emnlp-2012-Transforming_Trees_to_Improve_Syntactic_Convergence.html">127 emnlp-2012-Transforming Trees to Improve Syntactic Convergence</a></p>
<p>12 0.32930484 <a title="126-lsi-12" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>13 0.29676658 <a title="126-lsi-13" href="./emnlp-2012-Iterative_Annotation_Transformation_with_Predict-Self_Reestimation_for_Chinese_Word_Segmentation.html">68 emnlp-2012-Iterative Annotation Transformation with Predict-Self Reestimation for Chinese Word Segmentation</a></p>
<p>14 0.29673254 <a title="126-lsi-14" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<p>15 0.29412025 <a title="126-lsi-15" href="./emnlp-2012-Language_Model_Rest_Costs_and_Space-Efficient_Storage.html">74 emnlp-2012-Language Model Rest Costs and Space-Efficient Storage</a></p>
<p>16 0.28423783 <a title="126-lsi-16" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>17 0.2821506 <a title="126-lsi-17" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>18 0.28034201 <a title="126-lsi-18" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>19 0.26606482 <a title="126-lsi-19" href="./emnlp-2012-Characterizing_Stylistic_Elements_in_Syntactic_Structure.html">27 emnlp-2012-Characterizing Stylistic Elements in Syntactic Structure</a></p>
<p>20 0.25918594 <a title="126-lsi-20" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.014), (16, 0.032), (25, 0.034), (34, 0.061), (60, 0.063), (63, 0.034), (64, 0.011), (65, 0.02), (74, 0.058), (76, 0.549), (80, 0.017), (86, 0.018), (95, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95223862 <a title="126-lda-1" href="./emnlp-2012-Training_Factored_PCFGs_with_Expectation_Propagation.html">126 emnlp-2012-Training Factored PCFGs with Expectation Propagation</a></p>
<p>Author: David Hall ; Dan Klein</p><p>Abstract: PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar. We present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion. Our method works with linguisticallymotivated annotations, induced latent structure, lexicalization, or any mix of the three. We use a structured expectation propagation algorithm that makes use of the factored structure in two ways. First, by partitioning the factors, it speeds up parsing exponentially over the unfactored approach. Second, it minimizes the redundancy of the factors during training, improving accuracy over an independent approach. Using purely latent variable annotations, we can efficiently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the na¨ ıve approach. Combining latent, lexicalized, and unlexicalized anno- tations, our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank.</p><p>2 0.92016572 <a title="126-lda-2" href="./emnlp-2012-Semantic_Compositionality_through_Recursive_Matrix-Vector_Spaces.html">116 emnlp-2012-Semantic Compositionality through Recursive Matrix-Vector Spaces</a></p>
<p>Author: Richard Socher ; Brody Huval ; Christopher D. Manning ; Andrew Y. Ng</p><p>Abstract: Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.</p><p>3 0.9077273 <a title="126-lda-3" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>Author: Ahmed Hassan ; Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: A mixture of positive (friendly) and negative (antagonistic) relations exist among users in most social media applications. However, many such applications do not allow users to explicitly express the polarity of their interactions. As a result most research has either ignored negative links or was limited to the few domains where such relations are explicitly expressed (e.g. Epinions trust/distrust). We study text exchanged between users in online communities. We find that the polarity of the links between users can be predicted with high accuracy given the text they exchange. This allows us to build a signed network representation of discussions; where every edge has a sign: positive to denote a friendly relation, or negative to denote an antagonistic relation. We also connect our analysis to social psychology theories of balance. We show that the automatically predicted networks are consistent with those theories. Inspired by that, we present a technique for identifying subgroups in discussions by partitioning singed networks representing them.</p><p>4 0.54886609 <a title="126-lda-4" href="./emnlp-2012-A_Comparison_of_Vector-based_Representations_for_Semantic_Composition.html">4 emnlp-2012-A Comparison of Vector-based Representations for Semantic Composition</a></p>
<p>Author: William Blacoe ; Mirella Lapata</p><p>Abstract: In this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods. We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication. Some are shallow while others operate over syntactic structure, rely on parameter learning, or require access to very large corpora. We find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection. The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method.</p><p>5 0.49054518 <a title="126-lda-5" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>Author: Shujie Liu ; Chi-Ho Li ; Mu Li ; Ming Zhou</p><p>Abstract: The training of most syntactic SMT approaches involves two essential components, word alignment and monolingual parser. In the current state of the art these two components are mutually independent, thus causing problems like lack of rule generalization, and violation of syntactic correspondence in translation rules. In this paper, we propose two ways of re-training monolingual parser with the target of maximizing the consistency between parse trees and alignment matrices. One is targeted self-training with a simple evaluation function; the other is based on training data selection from forced alignment of bilingual data. We also propose an auxiliary method for boosting alignment quality, by symmetrizing alignment matrices with respect to parse trees. The best combination of these novel methods achieves 3 Bleu point gain in an IWSLT task and more than 1 Bleu point gain in NIST tasks. 1</p><p>6 0.48772421 <a title="126-lda-6" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>7 0.47143331 <a title="126-lda-7" href="./emnlp-2012-Constructing_Task-Specific_Taxonomies_for_Document_Collection_Browsing.html">30 emnlp-2012-Constructing Task-Specific Taxonomies for Document Collection Browsing</a></p>
<p>8 0.46908474 <a title="126-lda-8" href="./emnlp-2012-User_Demographics_and_Language_in_an_Implicit_Social_Network.html">134 emnlp-2012-User Demographics and Language in an Implicit Social Network</a></p>
<p>9 0.46064299 <a title="126-lda-9" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>10 0.45461836 <a title="126-lda-10" href="./emnlp-2012-Streaming_Analysis_of_Discourse_Participants.html">120 emnlp-2012-Streaming Analysis of Discourse Participants</a></p>
<p>11 0.45087811 <a title="126-lda-11" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>12 0.44971392 <a title="126-lda-12" href="./emnlp-2012-Fast_Large-Scale_Approximate_Graph_Construction_for_NLP.html">52 emnlp-2012-Fast Large-Scale Approximate Graph Construction for NLP</a></p>
<p>13 0.44093335 <a title="126-lda-13" href="./emnlp-2012-Extracting_Opinion_Expressions_with_semi-Markov_Conditional_Random_Fields.html">51 emnlp-2012-Extracting Opinion Expressions with semi-Markov Conditional Random Fields</a></p>
<p>14 0.4396255 <a title="126-lda-14" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>15 0.437837 <a title="126-lda-15" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>16 0.43653455 <a title="126-lda-16" href="./emnlp-2012-Word_Salad%3A_Relating_Food_Prices_and_Descriptions.html">139 emnlp-2012-Word Salad: Relating Food Prices and Descriptions</a></p>
<p>17 0.43206254 <a title="126-lda-17" href="./emnlp-2012-First_Order_vs._Higher_Order_Modification_in_Distributional_Semantics.html">53 emnlp-2012-First Order vs. Higher Order Modification in Distributional Semantics</a></p>
<p>18 0.41978517 <a title="126-lda-18" href="./emnlp-2012-Unsupervised_PCFG_Induction_for_Grounded_Language_Learning_with_Highly_Ambiguous_Supervision.html">133 emnlp-2012-Unsupervised PCFG Induction for Grounded Language Learning with Highly Ambiguous Supervision</a></p>
<p>19 0.4197135 <a title="126-lda-19" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>20 0.40670332 <a title="126-lda-20" href="./emnlp-2012-Syntactic_Surprisal_Affects_Spoken_Word_Duration_in_Conversational_Contexts.html">122 emnlp-2012-Syntactic Surprisal Affects Spoken Word Duration in Conversational Contexts</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
