<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-14" href="#">emnlp2012-14</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</h1>
<br/><p>Source: <a title="emnlp-2012-14-pdf" href="http://aclweb.org/anthology//D/D12/D12-1014.pdf">pdf</a></p><p>Author: Lizhen Qu ; Rainer Gemulla ; Gerhard Weikum</p><p>Abstract: We propose the weakly supervised MultiExperts Model (MEM) for analyzing the semantic orientation of opinions expressed in natural language reviews. In contrast to most prior work, MEM predicts both opinion polarity and opinion strength at the level of individual sentences; such fine-grained analysis helps to understand better why users like or dislike the entity under review. A key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength. For this reason, MEM is weakly supervised: It starts with potentially noisy indicators obtained from coarse-grained training data (i.e., document-level ratings), a small set of diverse base predictors, and, if available, small amounts of fine-grained training data. We integrate these noisy indicators into a unified probabilistic framework using ideas from ensemble learning and graph-based semi-supervised learning. Our experiments indicate that MEM outperforms state-of-the-art methods by a significant margin.</p><p>Reference: <a title="emnlp-2012-14-reference" href="../emnlp2012_reference/emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In contrast to most prior work, MEM predicts both opinion polarity and opinion strength at the level of individual sentences; such fine-grained analysis helps to understand better why users like or dislike the entity under review. [sent-4, score-0.883]
</p><p>2 A key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength. [sent-5, score-0.525]
</p><p>3 For example, many internet websites allow their users to provide both natural language reviews and numerical ratings to items of interest (such as products or movies). [sent-12, score-0.273]
</p><p>4 Preferences of users to items can be well understood by coarse-grained methods of opinion mining, which 149 focus on analyzing the semantic orientation of documents as a whole. [sent-14, score-0.243]
</p><p>5 The SO consists of polarity (positive, negative, or other1) and strength (degree to which a sentence is  positive or negative). [sent-17, score-0.667]
</p><p>6 A key challenge in finegrained rating prediction is that fine-grained training data for both polarity and strength is hard to obtain. [sent-19, score-0.787]
</p><p>7 We thus focus on a weakly supervised setting in which only coarse-level training data (such as document ratings and subjectivity lexicons) and, optionally, a small amount of fine-grained training data (such as sentence polarities) is available. [sent-20, score-0.414]
</p><p>8 Such an approach performs  1We assign polarity other to text fragments that are off-topic or not directly related to the entity under review. [sent-28, score-0.525]
</p><p>9 To address the challenges outlined above, we propose the weakly supervised Multi-Experts Model (MEM) for sentence-level rating prediction. [sent-35, score-0.239]
</p><p>10 MEM is designed such that new base predictors can be easily integrated. [sent-38, score-0.401]
</p><p>11 Since the information provided by the base predictors can be contradicting, we use ideas from ensemble learning (Dietterichl, 2002) to learn the most confident indicators and to exploit domain-dependent information revealed by document ratings. [sent-39, score-0.557]
</p><p>12 It forms a special realization of stacking (Dzeroski and Zenko, 2004) but uses the features from the base predictors instead of the actual predictions. [sent-44, score-0.401]
</p><p>13 Our experiments indicate that MEM significantly  outperforms prior work in both sentence-level rating prediction and sentence-level polarity classification. [sent-53, score-0.769]
</p><p>14 Our approach is unique in that it is weakly supervised, predicts both polarity and strength, and operates on the sentence level. [sent-55, score-0.613]
</p><p>15 Supervised approaches for sentiment analysis focus mainly on opinion mining at the document level (Pang and Lee, 2004; Pang et al. [sent-56, score-0.263]
</p><p>16 , 2002; Pang and Lee, 2005; Goldberg and Zhu, 2006), but have also been applied to sentence-level polarity classification in specific domains (Mao and Lebanon, 2006; Pang and Lee, 2004; McDonald et al. [sent-57, score-0.559]
</p><p>17 The weakly supervised HCRF model (T¨ackstro¨m and McDonald, 2011b; Ta¨ckstro¨m and McDonald, 2011a) for sentence-level polarity classification is per-  haps closest to our work in spirit. [sent-61, score-0.64]
</p><p>18 There exists a large number of lexicon-based methods for polarity classification (Ding et al. [sent-64, score-0.559]
</p><p>19 This allows us to integrate our base predictors as well as the available training data into a unified model that exploits that strengths of algorithms from both families. [sent-76, score-0.401]
</p><p>20 3  Base Predictors  Each of our base predictors predicts the polarity or the rating of a single phrase. [sent-77, score-1.084]
</p><p>21 As indicated above, we do not use these predictions directly in MEM but instead integrate the features of the base predictors (see Sec. [sent-78, score-0.429]
</p><p>22 MEM is designed such that new base predictors can be integrated easily. [sent-81, score-0.401]
</p><p>23 Our base predictors use a diverse set of available web and linguistic resources. [sent-82, score-0.401]
</p><p>24 The hope is that this diversity increases overall prediction performance (Dietterichl, 2002): The statistical polarity predictor focuses on local syntactic patterns; it is based on corpus statistics for SO-carrying words and opinion topic words. [sent-83, score-0.933]
</p><p>25 The heuristic polarity predictor uses manually constructed rules to achieve high precision but low recall. [sent-84, score-0.785]
</p><p>26 Both the bag-of-opinions rating predictor and the SO-CAL rating predictor are based on lexicons. [sent-85, score-0.836]
</p><p>27 The BoO predictor uses a lexicon trained from a large generic-domain corpus and is recall-oriented; the SO-CAL predictor uses a different lexicon with manually assigned weights and is precision-oriented. [sent-86, score-0.588]
</p><p>28 1 Statistical Polarity Predictor The polarity of an SO-carrying word strongly depends on its target word. [sent-88, score-0.525]
</p><p>29 Our statistical polarity predictor learns the polarity of opinions and targets jointly, which increases the robustness of its predictions. [sent-95, score-1.31]
</p><p>30 For each extracted pair z, we count how often it co-occurs with each document polarity y ∈ Y, where Y = {positive, negative, other} denotesy t h∈e set of polYar =itie {sp. [sent-103, score-0.595]
</p><p>31 The marginal distribution of polarity label y given that z occurs in a sentence is estimated as P(y | z) = #(y, z)/#z. [sent-110, score-0.561]
</p><p>32 The predictor is trained using tyhe | zte)x =t a #nd( ratings ozf the reviews in the training data, i. [sent-111, score-0.499]
</p><p>33 The statistical polarity predictor can be used to predict sentence-level polarities by averaging the phraselevel predictions. [sent-114, score-0.898]
</p><p>34 To predict the sentence polarity y ∈ Y, we take the Bayesian averaPge of the phrase-level predPictors: P(y | Z(x)) =  Pz∈Z(x) P(y | z)P(z) = Pz∈Z(x) P(y, z). [sent-120, score-0.561]
</p><p>35 T))hu =s  tPhez ∈mZo(sx)t likely polarity i=s thPe one xw)ith the highest co-occurrence count. [sent-121, score-0.525]
</p><p>36 Each opinion is scored based on these words (represented as a boolean vector b) and the polarity of the SO-carrying word (represented as sgn(r) ∈ {−1, 1}) as indicated by the MPQA lexiscgonn( ro)f ∈ W {i−lso1n,1 e}t al. [sent-137, score-0.641]
</p><p>37 The SO-CAL predictor uses a manually created lexicon, in which each word is classified as either an SOcarrying word (associated with a numerical score), an intensifier (associated with a modifier on the numerical score), or a negator. [sent-145, score-0.379]
</p><p>38 , document ratings or polarities), similarities between sentences, and optionally a small amount of  sentence polarity labels into an unified probabilistic model. [sent-150, score-0.916]
</p><p>39 Note that the number of initial labels may vary from sentence to sentence and that initial labels are heterogeneous in that they refer to either polarities or ratings. [sent-158, score-0.27]
</p><p>40 PE integrates b |o Xth, βfe)atures from the base predictors and sentence similarities. [sent-171, score-0.474]
</p><p>41 We correlate ratings to initial labels via a set of conditional distributions Pb( yˆb | r), where b denotes the  Yˆ  Yˆ,  type of initial label (Sec. [sent-172, score-0.25]
</p><p>42 Vector xi contains the features of all the base predictors but also includes bigram features for increased coverage of syntactic patterns; see Sec. [sent-186, score-0.433]
</p><p>43 Letm(xi) = βTxi be a linear predictor forri, where β is a real weight vector. [sent-189, score-0.26]
</p><p>44 N|o mte that predictor m can be regarded as a linear combination of base predictors because both m and each of the base predictors are linear functions. [sent-191, score-1.062]
</p><p>45 By integrating all features into a single function, the base predictors are trained jointly so that weight vector β automatically adapts to domain-dependent properties of the data. [sent-192, score-0.401]
</p><p>46 3 Incorporating Initial Labels Recall that the initial labels Yˆ are strong indicators of semantic orientation associated with each sentence; they correspond to either discrete polarity labels or to continuous rating labels. [sent-216, score-0.945]
</p><p>47 Our experiment suggests that document ratings constitute the most important indicator of the SO of a sentence. [sent-223, score-0.293]
</p><p>48 Thus sentence ratings should be  close to document ratings unless strong evidence to 153 the contrary exists. [sent-224, score-0.488]
</p><p>49 When no manually created sentence-level polarity labels are available, we set the value ofηiDoc depending on the polarity class. [sent-226, score-1.109]
</p><p>50 The reasoning behind this choice is that sentence ratings in neutral documents express higher variance because these documents often contain a mixture of positive and negative sentences. [sent-228, score-0.339]
</p><p>51 When a small set of manually created sentence polarity labels is available, we train a classifier that predicts whether the sentence polarity coincides with the document polarity. [sent-229, score-1.251]
</p><p>52 Polarity Labels We now describe how to model the correlation between the polarity of a sentence and its rating. [sent-233, score-0.561]
</p><p>53 An simple and effective approach is to partition the range of ratings into three consecutive partitions, one for each polarity class. [sent-234, score-0.716]
</p><p>54 We thus considering the polarity classes {positive, other, negative} as ordered and formulate p{oploarsiittyiv cel,aostshiefirca,tnieogna atisv ean} ordinal regression problem (Chu and Ghahramani, 2006). [sent-235, score-0.553]
</p><p>55 We can use the same distribution to use MEM for sentence-level polarity classification; in this case, we pick the polarity with the highest probability. [sent-249, score-1.05]
</p><p>56 4 Incorporating Base Predictors Base predictors are integrated into MEM via component N1(ri | mi, σ2) of the multi-expert prior (see Sec. [sent-251, score-0.305]
</p><p>57 New base predictors can be integrated easily by exposing their features to MEM. [sent-256, score-0.401]
</p><p>58 Most base predictors operate on the phrase level; our goal is to construct features for the entire sentence. [sent-257, score-0.401]
</p><p>59 Denote by nib the number of phrases in the i-th sentence covered by base predictor b, and let oibj denote a set of associated features. [sent-258, score-0.49]
</p><p>60 Features oibj may or may not correspond directly to the features of base predictor b; see the discussion below. [sent-259, score-0.454]
</p><p>61 We proceed slightly differently and average Pthe features associated with phrases of positive prior polarity separately from those of phrases with negative prior polarity (Taboada et al. [sent-261, score-1.232]
</p><p>62 , we set xib = where denotes the average of the feature vectors oibj associated with phrases of prior polarity p. [sent-265, score-0.667]
</p><p>63 We construct xi by concatenating the sentence-level features xib of each base predictor and a feature vector of bigrams. [sent-269, score-0.486]
</p><p>64 The prior polarity of a SOCAL phrase is given by the polarity of its SOcarrying word in the SO-CAL lexicon. [sent-272, score-1.104]
</p><p>65 Similar to SO-CAL, we determine the prior polarity of a phrase based on the BoO  dictionary. [sent-277, score-0.579]
</p><p>66 In contrast to SO-CAL, we directly use the BoO score as a feature because the BoO predictor weights have been trained on a very large corpus and are thus reliable. [sent-278, score-0.26]
</p><p>67 Recall that the statistical polarity predictor is based on co-occurrence counts of opinion-topic pairs and document polarities. [sent-281, score-0.855]
</p><p>68 We treat each opinion-topic pair as a phrase and use the most frequently co-occurring polarity as the phrase’s prior polarity. [sent-282, score-0.579]
</p><p>69 We use the logarithm of the co-occurrence counts with positive, negative, and other polarity as features; this set of features performed better than using the co-occurrence counts or estimated class probabilities directly. [sent-283, score-0.525]
</p><p>70 The main purposee of N2 is to  ×  propagate information from sentences on wNhich the base predictors perform well to sentences for which base prediction is unreliable or unavailable (e. [sent-289, score-0.637]
</p><p>71 The left term in the exponent forces the ratings of similar sentences to be similar: the larger the sentence similarity wij, the more penalty is paid for dissimilar ratings. [sent-298, score-0.287]
</p><p>72 (2003) by (1) BoO weights to strengthen the correlation of sentence similarity and rating similarity and (2) synonym resolution based on WordNet (Miller, 1995). [sent-303, score-0.26]
</p><p>73 This IDF decay factor is not wellsuited to our setting: Important opinion words such as “great” have a low IDF value due to their high  document frequency. [sent-321, score-0.239]
</p><p>74 5  Experiments  We evaluated both MEM and a number of alternative approaches for both sentence-level polarity classification and sentence-level strength prediction across a number of domains. [sent-339, score-0.663]
</p><p>75 1 Experimental Setup We implemented MEM as well as the HCRF classifier of (T¨ackstro¨m and McDonald, 2011a; Ta¨ckstro¨m and McDonald, 2011b), which is the best-performing estimator of sentence-level polarity in the weaklysupervised setting reported in the literature. [sent-342, score-0.525]
</p><p>76 We also implemented a number of baselines for both polarity classification and strength prediction: a document oracle (DocOracle) that simply uses  the document label for each sentence, the BoO rating predictor (BaseBoO), and the SO-CAL rating predictor (BaseSO-CAL). [sent-344, score-1.607]
</p><p>77 For polarity classification, we compare our methods also to the statistical polarity predictor (Basepolarity). [sent-345, score-1.31]
</p><p>78 To judge on the effectiveness of our multi-export prior for combining base predictors, we take the majority vote of all base predictors and document polarity as an additional baseline (Majority-Vote). [sent-346, score-1.2]
</p><p>79 Similarly, for strength prediction, we take the arithmetic mean of the document rating and the phrase-level predictions of BaseBoO and BaseSO-CAL as a baseline (Mean-Rating). [sent-347, score-0.328]
</p><p>80 For sentence polarity classification, we use the test set of Ta¨ckstro¨m and McDonald (201 1a), which  5We used the best-performing model that fuses HCRF-Coarse and the supervised model (McDonald et al. [sent-354, score-0.59]
</p><p>81 For strength evaluation, we created a test set of 300 pairs of sentences per domain from the polarity test set. [sent-357, score-0.624]
</p><p>82 For MEMFine and HCRF-Fine, we use the data from the other two folds as fine-grained polarity annotations. [sent-362, score-0.525]
</p><p>83 For our experiments on polarity classification, we converted the predicted ratings of MEM, BaseBoO, and  BaseSO-CAL into polarities by the method described in Sec. [sent-363, score-0.796]
</p><p>84 We compare the performance of each method in terms of accuracy, which is defined as the fraction of correct predictions on the test set (correct label for polarity / correct ranking for strength). [sent-366, score-0.553]
</p><p>85 2 Results for Polarity Classification Table 1summarizes the results of our experiments for sentence polarity classification. [sent-371, score-0.561]
</p><p>86 The base predictors perform poorly across all domains, mainly due to the aforementioned problems associated with averaging phrase-level predictions. [sent-372, score-0.434]
</p><p>87 However, accurracy increases when we combine base predictors and DocOracle using majority voting, which indicates that ensemble methods work well. [sent-374, score-0.442]
</p><p>88 0 Table 1: Accuracy of polarity classification per domain and averaged across domains. [sent-413, score-0.559]
</p><p>89 Also, MEM learns weights of features of base predictors, which leads to a more adaptive integration, and our ordinal regression formulation for polarity prediction allows direct competition among positive and negative evidence for improved accuracy. [sent-415, score-0.809]
</p><p>90 When we incorporate a small amount of sentence  polarity labels (HCRF-Fine, MEM-Fine), the accuracy of all models greatly improves. [sent-416, score-0.62]
</p><p>91 8 Table 2: Accuracy of polarity classification for sentences with opinion words (op) and without opinion words (fact). [sent-438, score-0.818]
</p><p>92 Moreover, sentences with polarity opposite to the document polarity are hard cases if they do not feature frequent strong patterns. [sent-441, score-1.147]
</p><p>93 Although document ratings are strong indicators in the polarity classification task, they lead to worse performance than lexicon-based methods. [sent-449, score-0.865]
</p><p>94 The main reason for this drop in accuracy is that the document oracle assigns the same rating to all sentences within a review. [sent-450, score-0.255]
</p><p>95 This shortage can be partly compensated by averaging the base predictions and document rating (Mean-Rating). [sent-452, score-0.439]
</p><p>96 Note that it is nontrivial to apply existing ensemble methods for the weights of individual base predictors because of the absence of the sentence ratings as training labels. [sent-453, score-0.669]
</p><p>97 Similar to polarity classification, a small amount of sentence polarity labels often improved  the performance of MEM. [sent-455, score-1.145]
</p><p>98 6  Conclusion  We proposed the Multi-Experts Model for analyzing both opinion polarity and opinion strength at the sentence level. [sent-481, score-0.893]
</p><p>99 MEM is driven by a novel multi-expert prior, which integrates a number of diverse base predictors and propagates information across sentences using a sentiment-augmented word sequence kernel. [sent-483, score-0.503]
</p><p>100 Adapting a polarity lexicon using integer linear programming for domainspecific sentiment classification. [sent-504, score-0.636]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('polarity', 0.525), ('mem', 0.414), ('predictor', 0.26), ('predictors', 0.251), ('boo', 0.192), ('ratings', 0.191), ('rating', 0.158), ('base', 0.15), ('ckstro', 0.118), ('opinion', 0.116), ('orientation', 0.099), ('pb', 0.09), ('docoracle', 0.089), ('idoc', 0.089), ('polarities', 0.08), ('sentiment', 0.077), ('strength', 0.072), ('ri', 0.072), ('document', 0.07), ('gaussian', 0.066), ('taboada', 0.064), ('baseboo', 0.059), ('gssl', 0.059), ('hcrf', 0.059), ('irrealis', 0.059), ('labels', 0.059), ('mcdonald', 0.059), ('prior', 0.054), ('decay', 0.053), ('weakly', 0.052), ('intensifier', 0.051), ('subsequences', 0.049), ('reviews', 0.048), ('kernel', 0.047), ('pang', 0.047), ('cancedda', 0.046), ('indicators', 0.045), ('ackstro', 0.044), ('chapelle', 0.044), ('dietterichl', 0.044), ('intensifiers', 0.044), ('oibj', 0.044), ('sgn', 0.044), ('socarrying', 0.044), ('xib', 0.044), ('pe', 0.043), ('bi', 0.043), ('ta', 0.042), ('ensemble', 0.041), ('negative', 0.04), ('zhu', 0.039), ('belkin', 0.038), ('propagates', 0.038), ('zhuang', 0.038), ('variance', 0.038), ('ding', 0.038), ('wij', 0.038), ('integrates', 0.037), ('sentence', 0.036), ('subjectivity', 0.036), ('op', 0.036), ('qu', 0.036), ('hopes', 0.035), ('optionally', 0.035), ('numerical', 0.034), ('positive', 0.034), ('classification', 0.034), ('hyperparameter', 0.034), ('greatest', 0.034), ('lexicon', 0.034), ('averaging', 0.033), ('subsequence', 0.033), ('similarity', 0.033), ('xi', 0.032), ('prediction', 0.032), ('electronics', 0.032), ('indicator', 0.032), ('easy', 0.031), ('experts', 0.03), ('stars', 0.03), ('bookelectronicsmusicavg', 0.03), ('cons', 0.03), ('dzeroski', 0.03), ('grams', 0.03), ('memfine', 0.03), ('negator', 0.03), ('negators', 0.03), ('nnw', 0.03), ('rescale', 0.03), ('wg', 0.03), ('yib', 0.03), ('supervised', 0.029), ('lexicons', 0.029), ('analyzing', 0.028), ('ordinal', 0.028), ('zoubin', 0.028), ('predictions', 0.028), ('sentences', 0.027), ('encourages', 0.027), ('ghahramani', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="14-tfidf-1" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>Author: Lizhen Qu ; Rainer Gemulla ; Gerhard Weikum</p><p>Abstract: We propose the weakly supervised MultiExperts Model (MEM) for analyzing the semantic orientation of opinions expressed in natural language reviews. In contrast to most prior work, MEM predicts both opinion polarity and opinion strength at the level of individual sentences; such fine-grained analysis helps to understand better why users like or dislike the entity under review. A key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength. For this reason, MEM is weakly supervised: It starts with potentially noisy indicators obtained from coarse-grained training data (i.e., document-level ratings), a small set of diverse base predictors, and, if available, small amounts of fine-grained training data. We integrate these noisy indicators into a unified probabilistic framework using ideas from ensemble learning and graph-based semi-supervised learning. Our experiments indicate that MEM outperforms state-of-the-art methods by a significant margin.</p><p>2 0.34648538 <a title="14-tfidf-2" href="./emnlp-2012-Collocation_Polarity_Disambiguation_Using_Web-based_Pseudo_Contexts.html">28 emnlp-2012-Collocation Polarity Disambiguation Using Web-based Pseudo Contexts</a></p>
<p>Author: Yanyan Zhao ; Bing Qin ; Ting Liu</p><p>Abstract: This paper focuses on the task of collocation polarity disambiguation. The collocation refers to a binary tuple of a polarity word and a target (such as ⟨long, battery life⟩ or ⟨long, ast atratrguep⟩t) (, siunc whh aisch ⟨ ltohneg s,en btatitmeernyt l iofrei⟩en otrat ⟨iolonn gof, tshtaer polarity wwohirdch (“long”) changes along owniothf different targets (“battery life” or “startup”). To disambiguate a collocation’s polarity, previous work always turned to investigate the polarities of its surrounding contexts, and then assigned the majority polarity to the collocation. However, these contexts are limited, thus the resulting polarity is insufficient to be reliable. We therefore propose an unsupervised three-component framework to expand some pseudo contexts from web, to help disambiguate a collocation’s polarity.Without using any additional labeled data, experiments , show that our method is effective.</p><p>3 0.16384847 <a title="14-tfidf-3" href="./emnlp-2012-Why_Question_Answering_using_Sentiment_Analysis_and_Word_Classes.html">137 emnlp-2012-Why Question Answering using Sentiment Analysis and Word Classes</a></p>
<p>Author: Jong-Hoon Oh ; Kentaro Torisawa ; Chikara Hashimoto ; Takuya Kawada ; Stijn De Saeger ; Jun'ichi Kazama ; Yiou Wang</p><p>Abstract: In this paper we explore the utility of sentiment analysis and semantic word classes for improving why-question answering on a large-scale web corpus. Our work is motivated by the observation that a why-question and its answer often follow the pattern that if something undesirable happens, the reason is also often something undesirable, and if something desirable happens, the reason is also often something desirable. To the best of our knowledge, this is the first work that introduces sentiment analysis to non-factoid question answering. We combine this simple idea with semantic word classes for ranking answers to why-questions and show that on a set of 850 why-questions our method gains 15.2% improvement in precision at the top-1 answer over a baseline state-of-the-art QA system that achieved the best performance in a shared task of Japanese non-factoid QA in NTCIR-6.</p><p>4 0.13205442 <a title="14-tfidf-4" href="./emnlp-2012-Opinion_Target_Extraction_Using_Word-Based_Translation_Model.html">101 emnlp-2012-Opinion Target Extraction Using Word-Based Translation Model</a></p>
<p>Author: Kang Liu ; Liheng Xu ; Jun Zhao</p><p>Abstract: This paper proposes a novel approach to extract opinion targets based on wordbased translation model (WTM). At first, we apply WTM in a monolingual scenario to mine the associations between opinion targets and opinion words. Then, a graphbased algorithm is exploited to extract opinion targets, where candidate opinion relevance estimated from the mined associations, is incorporated with candidate importance to generate a global measure. By using WTM, our method can capture opinion relations more precisely, especially for long-span relations. In particular, compared with previous syntax-based methods, our method can effectively avoid noises from parsing errors when dealing with informal texts in large Web corpora. By using graph-based algorithm, opinion targets are extracted in a global process, which can effectively alleviate the problem of error propagation in traditional bootstrap-based methods, such as Double Propagation. The experimental results on three real world datasets in different sizes and languages show that our approach is more effective and robust than state-of-art methods. 1</p><p>5 0.12101223 <a title="14-tfidf-5" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>Author: Ahmed Hassan ; Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: A mixture of positive (friendly) and negative (antagonistic) relations exist among users in most social media applications. However, many such applications do not allow users to explicitly express the polarity of their interactions. As a result most research has either ignored negative links or was limited to the few domains where such relations are explicitly expressed (e.g. Epinions trust/distrust). We study text exchanged between users in online communities. We find that the polarity of the links between users can be predicted with high accuracy given the text they exchange. This allows us to build a signed network representation of discussions; where every edge has a sign: positive to denote a friendly relation, or negative to denote an antagonistic relation. We also connect our analysis to social psychology theories of balance. We show that the automatically predicted networks are consistent with those theories. Inspired by that, we present a technique for identifying subgroups in discussions by partitioning singed networks representing them.</p><p>6 0.12005671 <a title="14-tfidf-6" href="./emnlp-2012-Do_Neighbours_Help%3F_An_Exploration_of_Graph-based_Algorithms_for_Cross-domain_Sentiment_Classification.html">34 emnlp-2012-Do Neighbours Help? An Exploration of Graph-based Algorithms for Cross-domain Sentiment Classification</a></p>
<p>7 0.11579979 <a title="14-tfidf-7" href="./emnlp-2012-Resolving_Complex_Cases_of_Definite_Pronouns%3A_The_Winograd_Schema_Challenge.html">112 emnlp-2012-Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge</a></p>
<p>8 0.10952633 <a title="14-tfidf-8" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>9 0.094431877 <a title="14-tfidf-9" href="./emnlp-2012-Extracting_Opinion_Expressions_with_semi-Markov_Conditional_Random_Fields.html">51 emnlp-2012-Extracting Opinion Expressions with semi-Markov Conditional Random Fields</a></p>
<p>10 0.08901789 <a title="14-tfidf-10" href="./emnlp-2012-An_%22AI_readability%22_Formula_for_French_as_a_Foreign_Language.html">17 emnlp-2012-An "AI readability" Formula for French as a Foreign Language</a></p>
<p>11 0.076409116 <a title="14-tfidf-11" href="./emnlp-2012-Word_Salad%3A_Relating_Food_Prices_and_Descriptions.html">139 emnlp-2012-Word Salad: Relating Food Prices and Descriptions</a></p>
<p>12 0.070550457 <a title="14-tfidf-12" href="./emnlp-2012-Semantic_Compositionality_through_Recursive_Matrix-Vector_Spaces.html">116 emnlp-2012-Semantic Compositionality through Recursive Matrix-Vector Spaces</a></p>
<p>13 0.067535624 <a title="14-tfidf-13" href="./emnlp-2012-Domain_Adaptation_for_Coreference_Resolution%3A_An_Adaptive_Ensemble_Approach.html">36 emnlp-2012-Domain Adaptation for Coreference Resolution: An Adaptive Ensemble Approach</a></p>
<p>14 0.064836286 <a title="14-tfidf-14" href="./emnlp-2012-Multiple_Aspect_Summarization_Using_Integer_Linear_Programming.html">94 emnlp-2012-Multiple Aspect Summarization Using Integer Linear Programming</a></p>
<p>15 0.063308619 <a title="14-tfidf-15" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>16 0.05915539 <a title="14-tfidf-16" href="./emnlp-2012-Learning_Verb_Inference_Rules_from_Linguistically-Motivated_Evidence.html">80 emnlp-2012-Learning Verb Inference Rules from Linguistically-Motivated Evidence</a></p>
<p>17 0.053747129 <a title="14-tfidf-17" href="./emnlp-2012-Active_Learning_for_Imbalanced_Sentiment_Classification.html">15 emnlp-2012-Active Learning for Imbalanced Sentiment Classification</a></p>
<p>18 0.052773107 <a title="14-tfidf-18" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>19 0.048413962 <a title="14-tfidf-19" href="./emnlp-2012-Syntactic_Surprisal_Affects_Spoken_Word_Duration_in_Conversational_Contexts.html">122 emnlp-2012-Syntactic Surprisal Affects Spoken Word Duration in Conversational Contexts</a></p>
<p>20 0.044677209 <a title="14-tfidf-20" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.195), (1, 0.132), (2, 0.02), (3, 0.316), (4, 0.217), (5, -0.156), (6, -0.134), (7, -0.043), (8, 0.061), (9, -0.009), (10, 0.006), (11, 0.009), (12, 0.106), (13, 0.11), (14, -0.064), (15, -0.086), (16, -0.084), (17, 0.239), (18, -0.025), (19, 0.039), (20, -0.198), (21, -0.001), (22, 0.076), (23, -0.036), (24, -0.252), (25, 0.078), (26, -0.05), (27, -0.089), (28, 0.132), (29, -0.143), (30, 0.099), (31, -0.008), (32, -0.039), (33, -0.056), (34, 0.052), (35, -0.048), (36, 0.131), (37, 0.053), (38, 0.051), (39, -0.026), (40, 0.067), (41, -0.037), (42, 0.068), (43, -0.077), (44, 0.019), (45, 0.031), (46, 0.031), (47, -0.03), (48, -0.005), (49, -0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95800954 <a title="14-lsi-1" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>Author: Lizhen Qu ; Rainer Gemulla ; Gerhard Weikum</p><p>Abstract: We propose the weakly supervised MultiExperts Model (MEM) for analyzing the semantic orientation of opinions expressed in natural language reviews. In contrast to most prior work, MEM predicts both opinion polarity and opinion strength at the level of individual sentences; such fine-grained analysis helps to understand better why users like or dislike the entity under review. A key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength. For this reason, MEM is weakly supervised: It starts with potentially noisy indicators obtained from coarse-grained training data (i.e., document-level ratings), a small set of diverse base predictors, and, if available, small amounts of fine-grained training data. We integrate these noisy indicators into a unified probabilistic framework using ideas from ensemble learning and graph-based semi-supervised learning. Our experiments indicate that MEM outperforms state-of-the-art methods by a significant margin.</p><p>2 0.89486605 <a title="14-lsi-2" href="./emnlp-2012-Collocation_Polarity_Disambiguation_Using_Web-based_Pseudo_Contexts.html">28 emnlp-2012-Collocation Polarity Disambiguation Using Web-based Pseudo Contexts</a></p>
<p>Author: Yanyan Zhao ; Bing Qin ; Ting Liu</p><p>Abstract: This paper focuses on the task of collocation polarity disambiguation. The collocation refers to a binary tuple of a polarity word and a target (such as ⟨long, battery life⟩ or ⟨long, ast atratrguep⟩t) (, siunc whh aisch ⟨ ltohneg s,en btatitmeernyt l iofrei⟩en otrat ⟨iolonn gof, tshtaer polarity wwohirdch (“long”) changes along owniothf different targets (“battery life” or “startup”). To disambiguate a collocation’s polarity, previous work always turned to investigate the polarities of its surrounding contexts, and then assigned the majority polarity to the collocation. However, these contexts are limited, thus the resulting polarity is insufficient to be reliable. We therefore propose an unsupervised three-component framework to expand some pseudo contexts from web, to help disambiguate a collocation’s polarity.Without using any additional labeled data, experiments , show that our method is effective.</p><p>3 0.50848395 <a title="14-lsi-3" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>Author: Ahmed Hassan ; Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: A mixture of positive (friendly) and negative (antagonistic) relations exist among users in most social media applications. However, many such applications do not allow users to explicitly express the polarity of their interactions. As a result most research has either ignored negative links or was limited to the few domains where such relations are explicitly expressed (e.g. Epinions trust/distrust). We study text exchanged between users in online communities. We find that the polarity of the links between users can be predicted with high accuracy given the text they exchange. This allows us to build a signed network representation of discussions; where every edge has a sign: positive to denote a friendly relation, or negative to denote an antagonistic relation. We also connect our analysis to social psychology theories of balance. We show that the automatically predicted networks are consistent with those theories. Inspired by that, we present a technique for identifying subgroups in discussions by partitioning singed networks representing them.</p><p>4 0.38450539 <a title="14-lsi-4" href="./emnlp-2012-Do_Neighbours_Help%3F_An_Exploration_of_Graph-based_Algorithms_for_Cross-domain_Sentiment_Classification.html">34 emnlp-2012-Do Neighbours Help? An Exploration of Graph-based Algorithms for Cross-domain Sentiment Classification</a></p>
<p>Author: Natalia Ponomareva ; Mike Thelwall</p><p>Abstract: This paper presents a comparative study of graph-based approaches for cross-domain sentiment classification. In particular, the paper analyses two existing methods: an optimisation problem and a ranking algorithm. We compare these graph-based methods with each other and with the other state-ofthe-art approaches and conclude that graph domain representations offer a competitive solution to the domain adaptation problem. Analysis of the best parameters for graphbased algorithms reveals that there are no optimal values valid for all domain pairs and that these values are dependent on the characteristics of corresponding domains.</p><p>5 0.33349293 <a title="14-lsi-5" href="./emnlp-2012-Why_Question_Answering_using_Sentiment_Analysis_and_Word_Classes.html">137 emnlp-2012-Why Question Answering using Sentiment Analysis and Word Classes</a></p>
<p>Author: Jong-Hoon Oh ; Kentaro Torisawa ; Chikara Hashimoto ; Takuya Kawada ; Stijn De Saeger ; Jun'ichi Kazama ; Yiou Wang</p><p>Abstract: In this paper we explore the utility of sentiment analysis and semantic word classes for improving why-question answering on a large-scale web corpus. Our work is motivated by the observation that a why-question and its answer often follow the pattern that if something undesirable happens, the reason is also often something undesirable, and if something desirable happens, the reason is also often something desirable. To the best of our knowledge, this is the first work that introduces sentiment analysis to non-factoid question answering. We combine this simple idea with semantic word classes for ranking answers to why-questions and show that on a set of 850 why-questions our method gains 15.2% improvement in precision at the top-1 answer over a baseline state-of-the-art QA system that achieved the best performance in a shared task of Japanese non-factoid QA in NTCIR-6.</p><p>6 0.32280847 <a title="14-lsi-6" href="./emnlp-2012-An_%22AI_readability%22_Formula_for_French_as_a_Foreign_Language.html">17 emnlp-2012-An "AI readability" Formula for French as a Foreign Language</a></p>
<p>7 0.31792068 <a title="14-lsi-7" href="./emnlp-2012-Resolving_Complex_Cases_of_Definite_Pronouns%3A_The_Winograd_Schema_Challenge.html">112 emnlp-2012-Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge</a></p>
<p>8 0.30485046 <a title="14-lsi-8" href="./emnlp-2012-Syntactic_Surprisal_Affects_Spoken_Word_Duration_in_Conversational_Contexts.html">122 emnlp-2012-Syntactic Surprisal Affects Spoken Word Duration in Conversational Contexts</a></p>
<p>9 0.30302352 <a title="14-lsi-9" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>10 0.30003807 <a title="14-lsi-10" href="./emnlp-2012-Word_Salad%3A_Relating_Food_Prices_and_Descriptions.html">139 emnlp-2012-Word Salad: Relating Food Prices and Descriptions</a></p>
<p>11 0.26223066 <a title="14-lsi-11" href="./emnlp-2012-Opinion_Target_Extraction_Using_Word-Based_Translation_Model.html">101 emnlp-2012-Opinion Target Extraction Using Word-Based Translation Model</a></p>
<p>12 0.22361542 <a title="14-lsi-12" href="./emnlp-2012-Extracting_Opinion_Expressions_with_semi-Markov_Conditional_Random_Fields.html">51 emnlp-2012-Extracting Opinion Expressions with semi-Markov Conditional Random Fields</a></p>
<p>13 0.21637337 <a title="14-lsi-13" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>14 0.21630234 <a title="14-lsi-14" href="./emnlp-2012-Learning_Lexicon_Models_from_Search_Logs_for_Query_Expansion.html">78 emnlp-2012-Learning Lexicon Models from Search Logs for Query Expansion</a></p>
<p>15 0.20613164 <a title="14-lsi-15" href="./emnlp-2012-Learning_Syntactic_Categories_Using_Paradigmatic_Representations_of_Word_Context.html">79 emnlp-2012-Learning Syntactic Categories Using Paradigmatic Representations of Word Context</a></p>
<p>16 0.19579218 <a title="14-lsi-16" href="./emnlp-2012-Learning_Verb_Inference_Rules_from_Linguistically-Motivated_Evidence.html">80 emnlp-2012-Learning Verb Inference Rules from Linguistically-Motivated Evidence</a></p>
<p>17 0.19113354 <a title="14-lsi-17" href="./emnlp-2012-Multiple_Aspect_Summarization_Using_Integer_Linear_Programming.html">94 emnlp-2012-Multiple Aspect Summarization Using Integer Linear Programming</a></p>
<p>18 0.18859187 <a title="14-lsi-18" href="./emnlp-2012-Domain_Adaptation_for_Coreference_Resolution%3A_An_Adaptive_Ensemble_Approach.html">36 emnlp-2012-Domain Adaptation for Coreference Resolution: An Adaptive Ensemble Approach</a></p>
<p>19 0.18699044 <a title="14-lsi-19" href="./emnlp-2012-Excitatory_or_Inhibitory%3A_A_New_Semantic_Orientation_Extracts_Contradiction_and_Causality_from_the_Web.html">44 emnlp-2012-Excitatory or Inhibitory: A New Semantic Orientation Extracts Contradiction and Causality from the Web</a></p>
<p>20 0.18317989 <a title="14-lsi-20" href="./emnlp-2012-Active_Learning_for_Imbalanced_Sentiment_Classification.html">15 emnlp-2012-Active Learning for Imbalanced Sentiment Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.025), (16, 0.025), (25, 0.013), (34, 0.064), (45, 0.017), (60, 0.069), (63, 0.079), (64, 0.063), (65, 0.021), (70, 0.027), (73, 0.012), (74, 0.056), (76, 0.07), (80, 0.015), (86, 0.029), (90, 0.254), (95, 0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.76864076 <a title="14-lda-1" href="./emnlp-2012-Opinion_Target_Extraction_Using_Word-Based_Translation_Model.html">101 emnlp-2012-Opinion Target Extraction Using Word-Based Translation Model</a></p>
<p>Author: Kang Liu ; Liheng Xu ; Jun Zhao</p><p>Abstract: This paper proposes a novel approach to extract opinion targets based on wordbased translation model (WTM). At first, we apply WTM in a monolingual scenario to mine the associations between opinion targets and opinion words. Then, a graphbased algorithm is exploited to extract opinion targets, where candidate opinion relevance estimated from the mined associations, is incorporated with candidate importance to generate a global measure. By using WTM, our method can capture opinion relations more precisely, especially for long-span relations. In particular, compared with previous syntax-based methods, our method can effectively avoid noises from parsing errors when dealing with informal texts in large Web corpora. By using graph-based algorithm, opinion targets are extracted in a global process, which can effectively alleviate the problem of error propagation in traditional bootstrap-based methods, such as Double Propagation. The experimental results on three real world datasets in different sizes and languages show that our approach is more effective and robust than state-of-art methods. 1</p><p>same-paper 2 0.75720376 <a title="14-lda-2" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>Author: Lizhen Qu ; Rainer Gemulla ; Gerhard Weikum</p><p>Abstract: We propose the weakly supervised MultiExperts Model (MEM) for analyzing the semantic orientation of opinions expressed in natural language reviews. In contrast to most prior work, MEM predicts both opinion polarity and opinion strength at the level of individual sentences; such fine-grained analysis helps to understand better why users like or dislike the entity under review. A key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength. For this reason, MEM is weakly supervised: It starts with potentially noisy indicators obtained from coarse-grained training data (i.e., document-level ratings), a small set of diverse base predictors, and, if available, small amounts of fine-grained training data. We integrate these noisy indicators into a unified probabilistic framework using ideas from ensemble learning and graph-based semi-supervised learning. Our experiments indicate that MEM outperforms state-of-the-art methods by a significant margin.</p><p>3 0.51885772 <a title="14-lda-3" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>Author: Jianxing Yu ; Zheng-Jun Zha ; Tat-Seng Chua</p><p>Abstract: This paper proposes to generate appropriate answers for opinion questions about products by exploiting the hierarchical organization of consumer reviews. The hierarchy organizes product aspects as nodes following their parent-child relations. For each aspect, the reviews and corresponding opinions on this aspect are stored. We develop a new framework for opinion Questions Answering, which enables accurate question analysis and effective answer generation by making use the hierarchy. In particular, we first identify the (explicit/implicit) product aspects asked in the questions and their sub-aspects by referring to the hierarchy. We then retrieve the corresponding review fragments relevant to the aspects from the hierarchy. In order to gener- ate appropriate answers from the review fragments, we develop a multi-criteria optimization approach for answer generation by simultaneously taking into account review salience, coherence, diversity, and parent-child relations among the aspects. We conduct evaluations on 11 popular products in four domains. The evaluated corpus contains 70,359 consumer reviews and 220 questions on these products. Experimental results demonstrate the effectiveness of our approach.</p><p>4 0.51358485 <a title="14-lda-4" href="./emnlp-2012-Exploring_Topic_Coherence_over_Many_Models_and_Many_Topics.html">49 emnlp-2012-Exploring Topic Coherence over Many Models and Many Topics</a></p>
<p>Author: Keith Stevens ; Philip Kegelmeyer ; David Andrzejewski ; David Buttler</p><p>Abstract: We apply two new automated semantic evaluations to three distinct latent topic models. Both metrics have been shown to align with human evaluations and provide a balance between internal measures of information gain and comparisons to human ratings of coherent topics. We improve upon the measures by introducing new aggregate measures that allows for comparing complete topic models. We further compare the automated measures to other metrics for topic models, comparison to manually crafted semantic tests and document classification. Our experiments reveal that LDA and LSA each have different strengths; LDA best learns descriptive topics while LSA is best at creating a compact semantic representation ofdocuments and words in a corpus.</p><p>5 0.51124293 <a title="14-lda-5" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms ofweak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.</p><p>6 0.50964284 <a title="14-lda-6" href="./emnlp-2012-Type-Supervised_Hidden_Markov_Models_for_Part-of-Speech_Tagging_with_Incomplete_Tag_Dictionaries.html">129 emnlp-2012-Type-Supervised Hidden Markov Models for Part-of-Speech Tagging with Incomplete Tag Dictionaries</a></p>
<p>7 0.50539476 <a title="14-lda-7" href="./emnlp-2012-Lexical_Differences_in_Autobiographical_Narratives_from_Schizophrenic_Patients_and_Healthy_Controls.html">83 emnlp-2012-Lexical Differences in Autobiographical Narratives from Schizophrenic Patients and Healthy Controls</a></p>
<p>8 0.5021652 <a title="14-lda-8" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>9 0.49835655 <a title="14-lda-9" href="./emnlp-2012-A_Phrase-Discovering_Topic_Model_Using_Hierarchical_Pitman-Yor_Processes.html">8 emnlp-2012-A Phrase-Discovering Topic Model Using Hierarchical Pitman-Yor Processes</a></p>
<p>10 0.4934181 <a title="14-lda-10" href="./emnlp-2012-Collocation_Polarity_Disambiguation_Using_Web-based_Pseudo_Contexts.html">28 emnlp-2012-Collocation Polarity Disambiguation Using Web-based Pseudo Contexts</a></p>
<p>11 0.49275514 <a title="14-lda-11" href="./emnlp-2012-Employing_Compositional_Semantics_and_Discourse_Consistency_in_Chinese_Event_Extraction.html">38 emnlp-2012-Employing Compositional Semantics and Discourse Consistency in Chinese Event Extraction</a></p>
<p>12 0.49166912 <a title="14-lda-12" href="./emnlp-2012-Streaming_Analysis_of_Discourse_Participants.html">120 emnlp-2012-Streaming Analysis of Discourse Participants</a></p>
<p>13 0.49079263 <a title="14-lda-13" href="./emnlp-2012-Learning_Lexicon_Models_from_Search_Logs_for_Query_Expansion.html">78 emnlp-2012-Learning Lexicon Models from Search Logs for Query Expansion</a></p>
<p>14 0.49056619 <a title="14-lda-14" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>15 0.49039465 <a title="14-lda-15" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>16 0.49026603 <a title="14-lda-16" href="./emnlp-2012-A_Discriminative_Model_for_Query_Spelling_Correction_with_Latent_Structural_SVM.html">5 emnlp-2012-A Discriminative Model for Query Spelling Correction with Latent Structural SVM</a></p>
<p>17 0.48967728 <a title="14-lda-17" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>18 0.48937106 <a title="14-lda-18" href="./emnlp-2012-Extracting_Opinion_Expressions_with_semi-Markov_Conditional_Random_Fields.html">51 emnlp-2012-Extracting Opinion Expressions with semi-Markov Conditional Random Fields</a></p>
<p>19 0.4871406 <a title="14-lda-19" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>20 0.48575816 <a title="14-lda-20" href="./emnlp-2012-Revisiting_the_Predictability_of_Language%3A_Response_Completion_in_Social_Media.html">114 emnlp-2012-Revisiting the Predictability of Language: Response Completion in Social Media</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
