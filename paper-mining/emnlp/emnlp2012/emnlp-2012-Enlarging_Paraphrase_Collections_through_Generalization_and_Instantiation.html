<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>39 emnlp-2012-Enlarging Paraphrase Collections through Generalization and Instantiation</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-39" href="#">emnlp2012-39</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>39 emnlp-2012-Enlarging Paraphrase Collections through Generalization and Instantiation</h1>
<br/><p>Source: <a title="emnlp-2012-39-pdf" href="http://aclweb.org/anthology//D/D12/D12-1058.pdf">pdf</a></p><p>Author: Atsushi Fujita ; Pierre Isabelle ; Roland Kuhn</p><p>Abstract: This paper presents a paraphrase acquisition method that uncovers and exploits generalities underlying paraphrases: paraphrase patterns are first induced and then used to collect novel instances. Unlike existing methods, ours uses both bilingual parallel and monolingual corpora. While the former are regarded as a source of high-quality seed paraphrases, the latter are searched for paraphrases that match patterns learned from the seed paraphrases. We show how one can use monolingual corpora, which are far more numerous and larger than bilingual corpora, to obtain paraphrases that rival in quality those derived directly from bilingual corpora. In our experiments, the number of paraphrase pairs obtained in this way from monolingual corpora was a large multiple of the number of seed paraphrases. Human evaluation through a paraphrase substitution test demonstrated that the newly acquired paraphrase pairs are ofreasonable quality. Remaining noise can be further reduced by filtering seed paraphrases.</p><p>Reference: <a title="emnlp-2012-39-reference" href="../emnlp2012_reference/emnlp-2012-Enlarging_Paraphrase_Collections_through_Generalization_and_Instantiation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp Abstract This paper presents a paraphrase acquisition method that uncovers and exploits generalities underlying paraphrases: paraphrase patterns are first induced and then used to collect novel instances. [sent-3, score-1.097]
</p><p>2 Unlike existing methods, ours uses both bilingual parallel and monolingual corpora. [sent-4, score-0.451]
</p><p>3 While the former are regarded as a source of high-quality seed paraphrases, the latter are searched for paraphrases that match patterns learned from the seed paraphrases. [sent-5, score-0.709]
</p><p>4 We show how one can use monolingual corpora, which are far more numerous and larger than bilingual corpora, to obtain paraphrases that rival in quality those derived directly from bilingual corpora. [sent-6, score-0.927]
</p><p>5 In our experiments, the number of paraphrase pairs obtained in this  way from monolingual corpora was a large multiple of the number of seed paraphrases. [sent-7, score-0.834]
</p><p>6 Human evaluation through a paraphrase substitution test demonstrated that the newly acquired paraphrase pairs are ofreasonable quality. [sent-8, score-1.109]
</p><p>7 Because “equivalence” is the most fundamental semantic relationship, techniques for generating and recognizing paraphrases play an important role in a wide range of natural language processing tasks (Madnani and Dorr, 2010). [sent-11, score-0.414]
</p><p>8 In the last decade, automatic acquisition ofknowledge about paraphrases from corpora has been drawing the attention of many researchers. [sent-12, score-0.517]
</p><p>9 clooonktro lilk system ⇔ cbolentroller The challenge in acquiring paraphrases is to ensure good coverage of the targeted classes of paraphrases along with a low proportion of incorrect pairs. [sent-20, score-0.887]
</p><p>10 However, no matter what type of resource has been used, it has proven difficult to acquire paraphrase pairs with both high recall and high precision. [sent-21, score-0.548]
</p><p>11 Among various types of corpora, monolingual corpora can be considered the best source for highcoverage paraphrase acquisition, because there is far more monolingual than bilingual text available. [sent-22, score-1.013]
</p><p>12 However, if one uses purely distributional criteria, it is difficult to distinguish real paraphrases from pairs of expressions that are related in other ways, such as antonyms and cousin words. [sent-24, score-0.627]
</p><p>13 In contrast, since the work in (Bannard and Callison-Burch, 2005), bilingual parallel corpora have been acknowledged as a good source of high-  quality paraphrases: paraphrases are obtained by putting together expressions that receive the same translation in the other language (pivot language). [sent-25, score-0.91]
</p><p>14 Because translation expresses a specific meaning more directly than context in the aforementioned approach, pairs of expressions acquired in this manner tend to be correct paraphrases. [sent-26, score-0.3]
</p><p>15 However, the coverage problem remains: there is much less bilingual parallel than monolingual text available. [sent-27, score-0.477]
</p><p>16 To achieve this, we propose a method that exploits general patterns underlying paraphrases and uses both bilingual parallel and monolingual sources of information. [sent-31, score-0.959]
</p><p>17 Given a relatively high-quality set of paraphrases obtained from a bilingual parallel corpus, a set of paraphrase patterns is first induced. [sent-32, score-1.263]
</p><p>18 Section 4 describes our experiments in acquiring paraphrases and presents statistics summarizing the coverage of our method. [sent-37, score-0.473]
</p><p>19 2 Literature on Paraphrase Acquisition This section summarizes existing corpus-based methods for paraphrase acquisition, following the classification in (Hashimoto et al. [sent-40, score-0.466]
</p><p>20 Because a large quantity of monolingual data is available for many languages, a large number of paraphrase candidates can be acquired (Lin and Pantel, 2001 ; Pas ¸ca and Dienes, 2005; Bhagat and Ravichandran, 2008, etc. [sent-44, score-0.752]
</p><p>21 (2003) 632 created monolingual parallel corpora from multiple human translations of the same source. [sent-55, score-0.346]
</p><p>22 Leveraging recent advances in statistical machine translation (SMT), Bannard and CallisonBurch (2005) proposed a method for acquiring subsentential paraphrases from bilingual parallel corpora. [sent-57, score-0.79]
</p><p>23 The likelihood of e2 being a paraphrase of e1 is given by  p(e2|e1) =  ∑  p(e2|f)p(f|e1),  (1)  f∈ T∑r(e1 ,e2) where Tr(e1 , e2) stands for the set of shared translations of e1 and e2. [sent-60, score-0.466]
</p><p>24 Kok and Brockett (2010) showed how one can discover paraphrases that do not share any translation in one language by traversing a graph created from multiple translation tables, each corresponding to a bilingual parallel corpus. [sent-63, score-0.811]
</p><p>25 This approach, however, suffers from a coverage problem, because both monolingual parallel and bilingual parallel corpora tend to be significantly smaller than monolingual non-parallel corpora. [sent-64, score-0.823]
</p><p>26 One limitation of this approach is that it requires a considerable amount of labeled data for both the corpus construc-  tion and the paraphrase extraction steps. [sent-76, score-0.466]
</p><p>27 3 Summary Existing methods have investigated one of the following four types of corpora as their principal resource1 : monolingual non-parallel corpora, monolingual parallel corpora, monolingual comparable corpora, and bilingual parallel corpora. [sent-78, score-0.98]
</p><p>28 No matter what type of resource has been used, however, it has proven difficult to acquire paraphrases with both high recall and precision, with the possible exception of the method in (Hashimoto et al. [sent-79, score-0.438]
</p><p>29 3  Proposed Method  While most existing methods deal with expressions only at the surface level, ours exploits generalities underlying paraphrases to achieve better coverage while retaining high precision. [sent-81, score-0.538]
</p><p>30 Furthermore, unlike existing methods, ours uses both bilingual parallel and monolingual non-parallel corpora as sources for acquiring paraphrases. [sent-82, score-0.543]
</p><p>31 First, a set of high-quality seed paraphrases, PSeed, is ac-  quired from bilingual parallel corpora by using an alignment-based method. [sent-84, score-0.437]
</p><p>32 Then, our method collects further paraphrases through the following two steps. [sent-85, score-0.414]
</p><p>33 Instantiation (Step 3): A novel set of paraphrase pairs, PHvst, is finally harvested from monolingual non-parallel corpora using the learned patterns; each newly acquired paraphrase pair is assessed by contextual similarity. [sent-87, score-1.343]
</p><p>34 (2011) used monolingual corpora only for reranking paraphrases obtained from bilingual parallel corpora. [sent-89, score-0.924]
</p><p>35 Seed Paraphrase Acquisition The goal of the first step is to obtain a set of highquality paraphrase pairs, PSeed. [sent-94, score-0.489]
</p><p>36 For this purpose, alignment-based methods with bilingual or monolingual parallel corpora are preferable to similarity-based methods applied to non-  parallel corpora. [sent-95, score-0.635]
</p><p>37 Among various options, in this paper, we start from the standard technique proposed by Bannard and Callison-Burch (2005) with bilingual parallel corpora (see also Section 2. [sent-96, score-0.348]
</p><p>38 As a result, a naive application of the paraphrase acquisition method produces pairs of expressions that are not exact paraphrases. [sent-102, score-0.639]
</p><p>39 Let PRaw be the initial set of paraphrase pairs extracted from the sanitized translation table. [sent-109, score-0.578]
</p><p>40 Given a set of paraphrase pairs, RHS phrases corresponding to the same LHS phrase lp are compared. [sent-121, score-0.676]
</p><p>41 A RHS phrase rp is not licensed iff lp has another RHS phrase rp′ (̸= rp) which satisfies the following two conditions( (see pa)ls wo Figure 2). [sent-122, score-0.358]
</p><p>42 • rp′ is a word sub-sequence of rp • rp′ is a more likely paraphrase than rp,  p(rp′| lp) > p(rp| lp)  i. [sent-123, score-0.589]
</p><p>43 , a LHS phrase lp is not qualified as a legitimate source of rp iff rp has another LHS phrase lp′ (̸= lp) which satisfies the following conditions (see =also lp Figure 3). [sent-127, score-0.634]
</p><p>44 Furthermore, we also require that LHS and RHS phrases exceed a threshold (ths) on their contextual similarity in a monolingual corpus. [sent-139, score-0.346]
</p><p>45 Paraphrase Pattern Induction From a set of seed paraphrases, PSeed, paraphrase patterns are induced. [sent-144, score-0.649]
</p><p>46 For instance, from paraphrases in (3), we induce paraphrase patterns in (4). [sent-145, score-0.974]
</p><p>47 Note that our aim is to automatically capture general paraphrase patterns of the kind that have sometimes been manually described (Jacquemin, 1999; Fujita et al. [sent-157, score-0.56]
</p><p>48 This is different from approaches that attach variable slots to paraphrases for calculating their similarity (Lin and Pantel, 2001 ; Szpektor and Dagan, 2008) or for constraining the context in which they are regarded legitimate (Callison-Burch, 2008; Zhao et al. [sent-159, score-0.53]
</p><p>49 Paraphrase Instance Acquisition Given a set of paraphrase patterns, such as those shown in (4), a set of novel instances, i. [sent-163, score-0.466]
</p><p>50 We estimate how likely RHS(w) is to be a paraphrase of LHS(w) based on the contextual similarity between them using a monolingual corpus; a pair of phrases is discarded if they are used in substantially dissimilar contexts. [sent-173, score-0.766]
</p><p>51 However, this is not a problem in our framework, because semantic equivalence between LHS(w) and RHS(w) is almost entirely guaranteed as a result of the way the corresponding patterns were learned from a bilingual parallel corpus. [sent-176, score-0.383]
</p><p>52 8M sentence pairs (51M words in English and 56M words in French) was used as a bilingual parallel corpus, while its English side and the English side of the French-English corpus4 consisting of 23. [sent-192, score-0.391]
</p><p>53 2M sentence pairs (122M morphemes in Japanese and 106M words in English) was used as a bilingual parallel corpus, while its English side and the 30. [sent-196, score-0.369]
</p><p>54 98  Figure 4: # of paraphrase pairs in PSeed (left: Europarl, right: Patent). [sent-206, score-0.524]
</p><p>55 Stop word lists for sanitizing translation pairs and paraphrase pairs were manually compiled: we enumerated 442 English words, 193 French words, and 149 Japanese morphemes, respectively. [sent-207, score-0.636]
</p><p>56 From a bilingual parallel corpus, a translation table was created by our in-house phrase-based SMT system, PORTAGE (Sadat et al. [sent-208, score-0.343]
</p><p>57 As contextual features for computing similarity of each paraphrase pair, all of the 1- to 4-grams of words adjacent to each occurrence of a phrase were counted. [sent-213, score-0.597]
</p><p>58 2 Statistics on Acquired Paraphrases Seed Paraphrases (PSeed) of paraphrase pairs PSeed obtained from the bilingual parallel corpora. [sent-219, score-0.813]
</p><p>59 The general trend is simply that the larger the corpus is, the more paraphrases are acquired. [sent-220, score-0.414]
</p><p>60 The percentage of paraphrase pairs thereby discarded varied greatly depending on the corpus size (17-78% in Europarl and 31-82% in Patent), suggesting that  the threshold value should be determined depending on the given corpus. [sent-236, score-0.57]
</p><p>61 01 (“△”) to ensure the quality of PSeed that we will be u“s△ing )fo tor inducing paraphrase patterns, even though this results in discarding some less frequent but correct paraphrase pairs, such as “control apparatus” ⇒ “controlling device” in Figure 2. [sent-238, score-0.955]
</p><p>62 Paraphrase Patterns Paraphrase  Patterns  Figures 5 and 6 show the number of paraphrase patterns that our method induced and their coverage against PSeed, respectively. [sent-239, score-0.586]
</p><p>63 Figure 7: # of paraphrase pairs and unique LHS phrases in PSeed and PHvst (left: Europarl, right: Patent). [sent-248, score-0.596]
</p><p>64 Novel Paraphrases (PHvst) Using the paraphrase patterns, novel paraphrase pairs, PHvst, were harvested from the monolingual non-parallel corpora. [sent-249, score-1.139]
</p><p>65 Nevertheless, we managed to acquire a large number of paraphrase pairs as depicted in Figure 7, where pairs having zero similarity were excluded. [sent-251, score-0.651]
</p><p>66 Figure 8 highlights the remarkably large ratio of PHvst to PSeed in terms of the number of paraphrase pairs and the number of unique LHS phrases. [sent-257, score-0.552]
</p><p>67 The alignment-based method with bilingual corpora cannot produce very many RHS phrases per unique LHS phrase due to its reliance on conditional probability and the surface level processing. [sent-264, score-0.332]
</p><p>68 One limitation of our method is that it cannot achieve high yield for PHvst whenever only a small number of paraphrase patterns can be extracted from the bilingual corpus (see also Figure 5). [sent-266, score-0.724]
</p><p>69 Similarity threshold ths Figure 10: # of acquired paraphrase pairs against threshold values. [sent-276, score-0.849]
</p><p>70 , thp on the conditional probability and ths on the contextual similarity, respectively. [sent-283, score-0.359]
</p><p>71 When the pairs were filtered only with thp, the number of paraphrase pairs in PHvst decreased more slowly than that of PSeed according to the increase ofthe threshold value. [sent-285, score-0.665]
</p><p>72 The same paraphrase pattern is often induced from more than one paraphrase pair in PSeed. [sent-287, score-0.952]
</p><p>73 Thus, as long as at least one of them has a probability higher than the given threshold value, corresponding novel paraphrases can be harvested. [sent-288, score-0.46]
</p><p>74 On the other hand, as a results of assessing each individual paraphrase pair by the contextual similarity, many pairs in PHvst, which are supposed to be incorrect instances of their corresponding pattern, are filtered out by a larger threshold value for ths. [sent-289, score-0.656]
</p><p>75 First, by substituting sub-sentential paraphrases to existing sentences in a given test corpus, pairs of slightly different sentences were automatically generated. [sent-296, score-0.472]
</p><p>76 , 2011), we evaluated paraphrases acquired from the Europarl corpus on news sentences. [sent-314, score-0.51]
</p><p>77 On the other hand, paraphrases acquired from patent documents are much more difficult to evaluate due to the following reasons. [sent-318, score-0.623]
</p><p>78 We expect that paraphrases from a domain can be used safely in that domain. [sent-321, score-0.414]
</p><p>79 To propose multiple paraphrase candidates at the same time, we also restricted phrases to be paraphrased (LHS phrases) to those having at least five paraphrases including ones from PHvst. [sent-324, score-0.96]
</p><p>80 This resulted in 60,421 paraphrases for 988 phrase tokens (353 unique phrases). [sent-325, score-0.479]
</p><p>81 Finally, we randomly sampled 80 unique phrase tokens and five unique paraphrases for each phrase token (400 examples in total), and asked six people having a high level of English proficiency to evaluate them. [sent-326, score-0.544]
</p><p>82 The performance of paraphrases drawn from PHvst was reasonably high and similar to the scores 0. [sent-331, score-0.414]
</p><p>83 The most promising way for improving the quality of PHvst is to ensure that paraphrase patterns cover only legitimate paraphrases. [sent-342, score-0.607]
</p><p>84 We investigated  this by filtering the manually scored paraphrase examples with two thresholds for cleaning seed paraphrases PSeed: thp on the conditional probability estimated using the bilingual parallel corpus and ths on the contextual similarity in the monolingual nonparallel corpus. [sent-343, score-1.931]
</p><p>85 Figure 11 shows the average score of the examples whose corresponding paraphrase is obtainable with the given threshold values. [sent-344, score-0.512]
</p><p>86 9vestd)1 Similarity threshold ths  Figure 11: Average score of paraphrase examples against threshold values. [sent-360, score-0.695]
</p><p>87 Nevertheless, it indicates that better filtering of PSeed with higher threshold values is likely to produce a better-quality set of paraphrases PHvst. [sent-368, score-0.525]
</p><p>88 For instance, an inappropriate paraphrase pattern (9a) was excluded with thp = 0. [sent-369, score-0.659]
</p><p>89 6  Conclusion  In this paper, we exploited general patterns underlying paraphrases to acquire automatically a large number of high-quality paraphrase pairs using both bilingual parallel and monolingual non-parallel corpora. [sent-385, score-1.507]
</p><p>90 Experiments using two sets of corpora demonstrated that our method is able to leverage information in a relatively small bilingual parallel corpus to exploit large amounts of information in a relatively large monolingual non-parallel corpus. [sent-386, score-0.51]
</p><p>91 Human evaluation through a paraphrase substitution test revealed that the acquired paraphrases are generally of reasonable quality. [sent-387, score-0.999]
</p><p>92 Our original objective was to extract from monolingual corpora a large quantity of paraphrases whose quality is as high as  640 that of paraphrases from bilingual parallel corpora. [sent-388, score-1.389]
</p><p>93 2, exploitation of patterns with more than one variable, learning curve experiments with different amounts of monolingual data, and comparison of in-domain and general-purpose monolingual corpora. [sent-395, score-0.418]
</p><p>94 Second, we have an interest in exploiting sophisticated paraphrase patterns; for instance, by inducing patterns hierarchically (recursively) and incorporating lexical resources such as those exemplified in (4). [sent-396, score-0.56]
</p><p>95 Finally, the developed paraphrase collection will be attested through applications, such  as sentence compression (Cohn and Lapata, 2008; Ganitkevitch et al. [sent-397, score-0.466]
</p><p>96 Large scale acquisition of paraphrases for learning surface patterns. [sent-418, score-0.458]
</p><p>97 Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. [sent-453, score-0.591]
</p><p>98 Learning sentential paraphrases from bilingual parallel corpora for text-to-text generation. [sent-469, score-0.762]
</p><p>99 Filtering antonymous, trend-contrasting, and polarity-dissimilar distributional paraphrases for improving statistical machine translation. [sent-518, score-0.434]
</p><p>100 Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences. [sent-530, score-0.414]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('paraphrase', 0.466), ('paraphrases', 0.414), ('pseed', 0.346), ('phvst', 0.293), ('lhs', 0.191), ('thp', 0.173), ('bilingual', 0.164), ('rhs', 0.163), ('monolingual', 0.162), ('ths', 0.137), ('lp', 0.129), ('parallel', 0.125), ('rp', 0.123), ('patent', 0.113), ('acquired', 0.096), ('patterns', 0.094), ('seed', 0.089), ('expressions', 0.071), ('europarl', 0.071), ('filtering', 0.065), ('apparatus', 0.062), ('corpora', 0.059), ('pairs', 0.058), ('hashimoto', 0.057), ('translation', 0.054), ('contextual', 0.049), ('threshold', 0.046), ('denkowski', 0.046), ('fujita', 0.046), ('similarity', 0.045), ('harvested', 0.045), ('acquisition', 0.044), ('phrases', 0.044), ('bannard', 0.041), ('cousin', 0.04), ('grammaticality', 0.038), ('evaluators', 0.038), ('phrase', 0.037), ('chris', 0.037), ('filtered', 0.037), ('paraphrased', 0.036), ('atsushi', 0.034), ('eastern', 0.034), ('recipe', 0.034), ('acquiring', 0.033), ('iff', 0.032), ('marton', 0.032), ('roland', 0.029), ('szpektor', 0.029), ('quantity', 0.028), ('unique', 0.028), ('device', 0.027), ('east', 0.027), ('shinyama', 0.027), ('smt', 0.027), ('dass', 0.027), ('europ', 0.027), ('generalities', 0.027), ('hakodate', 0.027), ('osungen', 0.027), ('portage', 0.027), ('praw', 0.027), ('prehistoric', 0.027), ('restraint', 0.027), ('sadat', 0.027), ('coverage', 0.026), ('stop', 0.026), ('instantiation', 0.025), ('dolan', 0.025), ('pivot', 0.025), ('slots', 0.024), ('legitimate', 0.024), ('antonyms', 0.024), ('acquire', 0.024), ('french', 0.024), ('barzilay', 0.023), ('regarded', 0.023), ('substitution', 0.023), ('chan', 0.023), ('du', 0.023), ('quality', 0.023), ('highquality', 0.023), ('lizard', 0.023), ('recipes', 0.023), ('control', 0.023), ('side', 0.022), ('lavie', 0.021), ('meaning', 0.021), ('investigated', 0.021), ('fujii', 0.021), ('wubben', 0.021), ('schools', 0.021), ('nonparallel', 0.021), ('ntcir', 0.021), ('kato', 0.021), ('xx', 0.021), ('pattern', 0.02), ('canada', 0.02), ('distributional', 0.02), ('paraphrasing', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="39-tfidf-1" href="./emnlp-2012-Enlarging_Paraphrase_Collections_through_Generalization_and_Instantiation.html">39 emnlp-2012-Enlarging Paraphrase Collections through Generalization and Instantiation</a></p>
<p>Author: Atsushi Fujita ; Pierre Isabelle ; Roland Kuhn</p><p>Abstract: This paper presents a paraphrase acquisition method that uncovers and exploits generalities underlying paraphrases: paraphrase patterns are first induced and then used to collect novel instances. Unlike existing methods, ours uses both bilingual parallel and monolingual corpora. While the former are regarded as a source of high-quality seed paraphrases, the latter are searched for paraphrases that match patterns learned from the seed paraphrases. We show how one can use monolingual corpora, which are far more numerous and larger than bilingual corpora, to obtain paraphrases that rival in quality those derived directly from bilingual corpora. In our experiments, the number of paraphrase pairs obtained in this way from monolingual corpora was a large multiple of the number of seed paraphrases. Human evaluation through a paraphrase substitution test demonstrated that the newly acquired paraphrase pairs are ofreasonable quality. Remaining noise can be further reduced by filtering seed paraphrases.</p><p>2 0.53034103 <a title="39-tfidf-2" href="./emnlp-2012-Generalizing_Sub-sentential_Paraphrase_Acquisition_across_Original_Signal_Type_of_Text_Pairs.html">58 emnlp-2012-Generalizing Sub-sentential Paraphrase Acquisition across Original Signal Type of Text Pairs</a></p>
<p>Author: Aurelien Max ; Houda Bouamor ; Anne Vilnat</p><p>Abstract: This paper describes a study on the impact of the original signal (text, speech, visual scene, event) of a text pair on the task of both manual and automatic sub-sentential paraphrase acquisition. A corpus of 2,500 annotated sentences in English and French is described, and performance on this corpus is reported for an efficient system combination exploiting a large set of features for paraphrase recognition. A detailed quantified typology of subsentential paraphrases found in our corpus types is given.</p><p>3 0.35527349 <a title="39-tfidf-3" href="./emnlp-2012-Using_Discourse_Information_for_Paraphrase_Extraction.html">135 emnlp-2012-Using Discourse Information for Paraphrase Extraction</a></p>
<p>Author: Michaela Regneri ; Rui Wang</p><p>Abstract: Previous work on paraphrase extraction using parallel or comparable corpora has generally not considered the documents’ discourse structure as a useful information source. We propose a novel method for collecting paraphrases relying on the sequential event order in the discourse, using multiple sequence alignment with a semantic similarity measure. We show that adding discourse information boosts the performance of sentence-level paraphrase acquisition, which consequently gives a tremendous advantage for extracting phraselevel paraphrase fragments from matched sentences. Our system beats an informed baseline by a margin of 50%.</p><p>4 0.152771 <a title="39-tfidf-4" href="./emnlp-2012-Bilingual_Lexicon_Extraction_from_Comparable_Corpora_Using_Label_Propagation.html">25 emnlp-2012-Bilingual Lexicon Extraction from Comparable Corpora Using Label Propagation</a></p>
<p>Author: Akihiro Tamura ; Taro Watanabe ; Eiichiro Sumita</p><p>Abstract: This paper proposes a novel method for lexicon extraction that extracts translation pairs from comparable corpora by using graphbased label propagation. In previous work, it was established that performance drastically decreases when the coverage of a seed lexicon is small. We resolve this problem by utilizing indirect relations with the bilingual seeds together with direct relations, in which each word is represented by a distribution of translated seeds. The seed distributions are propagated over a graph representing relations among words, and translation pairs are extracted by identifying word pairs with a high similarity in the seed distributions. We propose two types of the graphs: a co-occurrence graph, representing co-occurrence relations between words, and a similarity graph, representing context similarities between words. Evaluations using English and Japanese patent comparable corpora show that our proposed graph propagation method outperforms conventional methods. Further, the similarity graph achieved improved performance by clustering synonyms into the same translation.</p><p>5 0.15227786 <a title="39-tfidf-5" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>Author: Michael Roth ; Anette Frank</p><p>Abstract: Generating coherent discourse is an important aspect in natural language generation. Our aim is to learn factors that constitute coherent discourse from data, with a focus on how to realize predicate-argument structures in a model that exceeds the sentence level. We present an important subtask for this overall goal, in which we align predicates across comparable texts, admitting partial argument structure correspondence. The contribution of this work is two-fold: We first construct a large corpus resource of comparable texts, including an evaluation set with manual predicate alignments. Secondly, we present a novel approach for aligning predicates across comparable texts using graph-based clustering with Mincuts. Our method significantly outperforms other alignment techniques when applied to this novel alignment task, by a margin of at least 6.5 percentage points in F1-score.</p><p>6 0.11824635 <a title="39-tfidf-6" href="./emnlp-2012-A_Comparison_of_Vector-based_Representations_for_Semantic_Composition.html">4 emnlp-2012-A Comparison of Vector-based Representations for Semantic Composition</a></p>
<p>7 0.07589823 <a title="39-tfidf-7" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>8 0.07220874 <a title="39-tfidf-8" href="./emnlp-2012-A_Systematic_Comparison_of_Phrase_Table_Pruning_Techniques.html">11 emnlp-2012-A Systematic Comparison of Phrase Table Pruning Techniques</a></p>
<p>9 0.07050515 <a title="39-tfidf-9" href="./emnlp-2012-Multiple_Aspect_Summarization_Using_Integer_Linear_Programming.html">94 emnlp-2012-Multiple Aspect Summarization Using Integer Linear Programming</a></p>
<p>10 0.069209434 <a title="39-tfidf-10" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>11 0.06867943 <a title="39-tfidf-11" href="./emnlp-2012-Probabilistic_Finite_State_Machines_for_Regression-based_MT_Evaluation.html">108 emnlp-2012-Probabilistic Finite State Machines for Regression-based MT Evaluation</a></p>
<p>12 0.063037269 <a title="39-tfidf-12" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>13 0.059945416 <a title="39-tfidf-13" href="./emnlp-2012-A_Bayesian_Model_for_Learning_SCFGs_with_Discontiguous_Rules.html">1 emnlp-2012-A Bayesian Model for Learning SCFGs with Discontiguous Rules</a></p>
<p>14 0.055237744 <a title="39-tfidf-14" href="./emnlp-2012-PATTY%3A_A_Taxonomy_of_Relational_Patterns_with_Semantic_Types.html">103 emnlp-2012-PATTY: A Taxonomy of Relational Patterns with Semantic Types</a></p>
<p>15 0.055028453 <a title="39-tfidf-15" href="./emnlp-2012-Forced_Derivation_Tree_based_Model_Training_to_Statistical_Machine_Translation.html">54 emnlp-2012-Forced Derivation Tree based Model Training to Statistical Machine Translation</a></p>
<p>16 0.049749464 <a title="39-tfidf-16" href="./emnlp-2012-Learning_Verb_Inference_Rules_from_Linguistically-Motivated_Evidence.html">80 emnlp-2012-Learning Verb Inference Rules from Linguistically-Motivated Evidence</a></p>
<p>17 0.048237789 <a title="39-tfidf-17" href="./emnlp-2012-Parse%2C_Price_and_Cut-Delayed_Column_and_Row_Generation_for_Graph_Based_Parsers.html">104 emnlp-2012-Parse, Price and Cut-Delayed Column and Row Generation for Graph Based Parsers</a></p>
<p>18 0.046629243 <a title="39-tfidf-18" href="./emnlp-2012-Translation_Model_Based_Cross-Lingual_Language_Model_Adaptation%3A_from_Word_Models_to_Phrase_Models.html">128 emnlp-2012-Translation Model Based Cross-Lingual Language Model Adaptation: from Word Models to Phrase Models</a></p>
<p>19 0.046341568 <a title="39-tfidf-19" href="./emnlp-2012-Transforming_Trees_to_Improve_Syntactic_Convergence.html">127 emnlp-2012-Transforming Trees to Improve Syntactic Convergence</a></p>
<p>20 0.044724122 <a title="39-tfidf-20" href="./emnlp-2012-Joining_Forces_Pays_Off%3A_Multilingual_Joint_Word_Sense_Disambiguation.html">69 emnlp-2012-Joining Forces Pays Off: Multilingual Joint Word Sense Disambiguation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.205), (1, -0.036), (2, -0.483), (3, 0.02), (4, 0.202), (5, 0.38), (6, 0.15), (7, -0.12), (8, -0.15), (9, 0.006), (10, 0.206), (11, -0.006), (12, 0.107), (13, -0.09), (14, 0.031), (15, 0.09), (16, -0.015), (17, 0.075), (18, -0.074), (19, 0.023), (20, -0.028), (21, -0.02), (22, -0.038), (23, -0.124), (24, -0.011), (25, -0.002), (26, 0.021), (27, 0.057), (28, -0.079), (29, -0.051), (30, -0.004), (31, 0.013), (32, 0.022), (33, 0.09), (34, 0.028), (35, -0.072), (36, 0.032), (37, -0.034), (38, 0.009), (39, 0.005), (40, 0.018), (41, 0.05), (42, -0.015), (43, 0.042), (44, 0.009), (45, -0.0), (46, -0.044), (47, 0.054), (48, -0.039), (49, -0.001)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96536267 <a title="39-lsi-1" href="./emnlp-2012-Enlarging_Paraphrase_Collections_through_Generalization_and_Instantiation.html">39 emnlp-2012-Enlarging Paraphrase Collections through Generalization and Instantiation</a></p>
<p>Author: Atsushi Fujita ; Pierre Isabelle ; Roland Kuhn</p><p>Abstract: This paper presents a paraphrase acquisition method that uncovers and exploits generalities underlying paraphrases: paraphrase patterns are first induced and then used to collect novel instances. Unlike existing methods, ours uses both bilingual parallel and monolingual corpora. While the former are regarded as a source of high-quality seed paraphrases, the latter are searched for paraphrases that match patterns learned from the seed paraphrases. We show how one can use monolingual corpora, which are far more numerous and larger than bilingual corpora, to obtain paraphrases that rival in quality those derived directly from bilingual corpora. In our experiments, the number of paraphrase pairs obtained in this way from monolingual corpora was a large multiple of the number of seed paraphrases. Human evaluation through a paraphrase substitution test demonstrated that the newly acquired paraphrase pairs are ofreasonable quality. Remaining noise can be further reduced by filtering seed paraphrases.</p><p>2 0.93330526 <a title="39-lsi-2" href="./emnlp-2012-Generalizing_Sub-sentential_Paraphrase_Acquisition_across_Original_Signal_Type_of_Text_Pairs.html">58 emnlp-2012-Generalizing Sub-sentential Paraphrase Acquisition across Original Signal Type of Text Pairs</a></p>
<p>Author: Aurelien Max ; Houda Bouamor ; Anne Vilnat</p><p>Abstract: This paper describes a study on the impact of the original signal (text, speech, visual scene, event) of a text pair on the task of both manual and automatic sub-sentential paraphrase acquisition. A corpus of 2,500 annotated sentences in English and French is described, and performance on this corpus is reported for an efficient system combination exploiting a large set of features for paraphrase recognition. A detailed quantified typology of subsentential paraphrases found in our corpus types is given.</p><p>3 0.81608701 <a title="39-lsi-3" href="./emnlp-2012-Using_Discourse_Information_for_Paraphrase_Extraction.html">135 emnlp-2012-Using Discourse Information for Paraphrase Extraction</a></p>
<p>Author: Michaela Regneri ; Rui Wang</p><p>Abstract: Previous work on paraphrase extraction using parallel or comparable corpora has generally not considered the documents’ discourse structure as a useful information source. We propose a novel method for collecting paraphrases relying on the sequential event order in the discourse, using multiple sequence alignment with a semantic similarity measure. We show that adding discourse information boosts the performance of sentence-level paraphrase acquisition, which consequently gives a tremendous advantage for extracting phraselevel paraphrase fragments from matched sentences. Our system beats an informed baseline by a margin of 50%.</p><p>4 0.40426204 <a title="39-lsi-4" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>Author: Michael Roth ; Anette Frank</p><p>Abstract: Generating coherent discourse is an important aspect in natural language generation. Our aim is to learn factors that constitute coherent discourse from data, with a focus on how to realize predicate-argument structures in a model that exceeds the sentence level. We present an important subtask for this overall goal, in which we align predicates across comparable texts, admitting partial argument structure correspondence. The contribution of this work is two-fold: We first construct a large corpus resource of comparable texts, including an evaluation set with manual predicate alignments. Secondly, we present a novel approach for aligning predicates across comparable texts using graph-based clustering with Mincuts. Our method significantly outperforms other alignment techniques when applied to this novel alignment task, by a margin of at least 6.5 percentage points in F1-score.</p><p>5 0.39115867 <a title="39-lsi-5" href="./emnlp-2012-Bilingual_Lexicon_Extraction_from_Comparable_Corpora_Using_Label_Propagation.html">25 emnlp-2012-Bilingual Lexicon Extraction from Comparable Corpora Using Label Propagation</a></p>
<p>Author: Akihiro Tamura ; Taro Watanabe ; Eiichiro Sumita</p><p>Abstract: This paper proposes a novel method for lexicon extraction that extracts translation pairs from comparable corpora by using graphbased label propagation. In previous work, it was established that performance drastically decreases when the coverage of a seed lexicon is small. We resolve this problem by utilizing indirect relations with the bilingual seeds together with direct relations, in which each word is represented by a distribution of translated seeds. The seed distributions are propagated over a graph representing relations among words, and translation pairs are extracted by identifying word pairs with a high similarity in the seed distributions. We propose two types of the graphs: a co-occurrence graph, representing co-occurrence relations between words, and a similarity graph, representing context similarities between words. Evaluations using English and Japanese patent comparable corpora show that our proposed graph propagation method outperforms conventional methods. Further, the similarity graph achieved improved performance by clustering synonyms into the same translation.</p><p>6 0.26394194 <a title="39-lsi-6" href="./emnlp-2012-Source_Language_Adaptation_for_Resource-Poor_Machine_Translation.html">118 emnlp-2012-Source Language Adaptation for Resource-Poor Machine Translation</a></p>
<p>7 0.23430447 <a title="39-lsi-7" href="./emnlp-2012-A_Comparison_of_Vector-based_Representations_for_Semantic_Composition.html">4 emnlp-2012-A Comparison of Vector-based Representations for Semantic Composition</a></p>
<p>8 0.22844499 <a title="39-lsi-8" href="./emnlp-2012-Probabilistic_Finite_State_Machines_for_Regression-based_MT_Evaluation.html">108 emnlp-2012-Probabilistic Finite State Machines for Regression-based MT Evaluation</a></p>
<p>9 0.21766765 <a title="39-lsi-9" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>10 0.20587119 <a title="39-lsi-10" href="./emnlp-2012-Excitatory_or_Inhibitory%3A_A_New_Semantic_Orientation_Extracts_Contradiction_and_Causality_from_the_Web.html">44 emnlp-2012-Excitatory or Inhibitory: A New Semantic Orientation Extracts Contradiction and Causality from the Web</a></p>
<p>11 0.20078659 <a title="39-lsi-11" href="./emnlp-2012-Forced_Derivation_Tree_based_Model_Training_to_Statistical_Machine_Translation.html">54 emnlp-2012-Forced Derivation Tree based Model Training to Statistical Machine Translation</a></p>
<p>12 0.17546481 <a title="39-lsi-12" href="./emnlp-2012-Multiple_Aspect_Summarization_Using_Integer_Linear_Programming.html">94 emnlp-2012-Multiple Aspect Summarization Using Integer Linear Programming</a></p>
<p>13 0.16358763 <a title="39-lsi-13" href="./emnlp-2012-Language_Model_Rest_Costs_and_Space-Efficient_Storage.html">74 emnlp-2012-Language Model Rest Costs and Space-Efficient Storage</a></p>
<p>14 0.16232638 <a title="39-lsi-14" href="./emnlp-2012-PATTY%3A_A_Taxonomy_of_Relational_Patterns_with_Semantic_Types.html">103 emnlp-2012-PATTY: A Taxonomy of Relational Patterns with Semantic Types</a></p>
<p>15 0.15976457 <a title="39-lsi-15" href="./emnlp-2012-A_Bayesian_Model_for_Learning_SCFGs_with_Discontiguous_Rules.html">1 emnlp-2012-A Bayesian Model for Learning SCFGs with Discontiguous Rules</a></p>
<p>16 0.15768623 <a title="39-lsi-16" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>17 0.15534887 <a title="39-lsi-17" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>18 0.15112524 <a title="39-lsi-18" href="./emnlp-2012-Joining_Forces_Pays_Off%3A_Multilingual_Joint_Word_Sense_Disambiguation.html">69 emnlp-2012-Joining Forces Pays Off: Multilingual Joint Word Sense Disambiguation</a></p>
<p>19 0.14970775 <a title="39-lsi-19" href="./emnlp-2012-Cross-Lingual_Language_Modeling_with_Syntactic_Reordering_for_Low-Resource_Speech_Recognition.html">31 emnlp-2012-Cross-Lingual Language Modeling with Syntactic Reordering for Low-Resource Speech Recognition</a></p>
<p>20 0.14619969 <a title="39-lsi-20" href="./emnlp-2012-Translation_Model_Based_Cross-Lingual_Language_Model_Adaptation%3A_from_Word_Models_to_Phrase_Models.html">128 emnlp-2012-Translation Model Based Cross-Lingual Language Model Adaptation: from Word Models to Phrase Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.024), (16, 0.022), (25, 0.01), (30, 0.216), (34, 0.069), (45, 0.025), (60, 0.204), (63, 0.079), (64, 0.027), (65, 0.027), (70, 0.047), (74, 0.05), (76, 0.034), (79, 0.012), (80, 0.015), (86, 0.021), (95, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90850735 <a title="39-lda-1" href="./emnlp-2012-Unsupervised_PCFG_Induction_for_Grounded_Language_Learning_with_Highly_Ambiguous_Supervision.html">133 emnlp-2012-Unsupervised PCFG Induction for Grounded Language Learning with Highly Ambiguous Supervision</a></p>
<p>Author: Joohyun Kim ; Raymond Mooney</p><p>Abstract: “Grounded” language learning employs training data in the form of sentences paired with relevant but ambiguous perceptual contexts. B ¨orschinger et al. (201 1) introduced an approach to grounded language learning based on unsupervised PCFG induction. Their approach works well when each sentence potentially refers to one of a small set of possible meanings, such as in the sportscasting task. However, it does not scale to problems with a large set of potential meanings for each sentence, such as the navigation instruction following task studied by Chen and Mooney (201 1). This paper presents an enhancement of the PCFG approach that scales to such problems with highly-ambiguous supervision. Experimental results on the navigation task demonstrates the effectiveness of our approach.</p><p>same-paper 2 0.85476983 <a title="39-lda-2" href="./emnlp-2012-Enlarging_Paraphrase_Collections_through_Generalization_and_Instantiation.html">39 emnlp-2012-Enlarging Paraphrase Collections through Generalization and Instantiation</a></p>
<p>Author: Atsushi Fujita ; Pierre Isabelle ; Roland Kuhn</p><p>Abstract: This paper presents a paraphrase acquisition method that uncovers and exploits generalities underlying paraphrases: paraphrase patterns are first induced and then used to collect novel instances. Unlike existing methods, ours uses both bilingual parallel and monolingual corpora. While the former are regarded as a source of high-quality seed paraphrases, the latter are searched for paraphrases that match patterns learned from the seed paraphrases. We show how one can use monolingual corpora, which are far more numerous and larger than bilingual corpora, to obtain paraphrases that rival in quality those derived directly from bilingual corpora. In our experiments, the number of paraphrase pairs obtained in this way from monolingual corpora was a large multiple of the number of seed paraphrases. Human evaluation through a paraphrase substitution test demonstrated that the newly acquired paraphrase pairs are ofreasonable quality. Remaining noise can be further reduced by filtering seed paraphrases.</p><p>3 0.73392552 <a title="39-lda-3" href="./emnlp-2012-Exploring_Adaptor_Grammars_for_Native_Language_Identification.html">48 emnlp-2012-Exploring Adaptor Grammars for Native Language Identification</a></p>
<p>Author: Sze-Meng Jojo Wong ; Mark Dras ; Mark Johnson</p><p>Abstract: The task of inferring the native language of an author based on texts written in a second language has generally been tackled as a classification problem, typically using as features a mix of n-grams over characters and part of speech tags (for small and fixed n) and unigram function words. To capture arbitrarily long n-grams that syntax-based approaches have suggested are useful, adaptor grammars have some promise. In this work we investigate their extension to identifying n-gram collocations of arbitrary length over a mix of PoS tags and words, using both maxent and induced syntactic language model approaches to classification. After presenting a new, simple baseline, we show that learned collocations used as features in a maxent model perform better still, but that the story is more mixed for the syntactic language model.</p><p>4 0.73287225 <a title="39-lda-4" href="./emnlp-2012-Grounded_Models_of_Semantic_Representation.html">61 emnlp-2012-Grounded Models of Semantic Representation</a></p>
<p>Author: Carina Silberer ; Mirella Lapata</p><p>Abstract: A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two.</p><p>5 0.72733176 <a title="39-lda-5" href="./emnlp-2012-Wiki-ly_Supervised_Part-of-Speech_Tagging.html">138 emnlp-2012-Wiki-ly Supervised Part-of-Speech Tagging</a></p>
<p>Author: Shen Li ; Joao Graca ; Ben Taskar</p><p>Abstract: Despite significant recent work, purely unsupervised techniques for part-of-speech (POS) tagging have not achieved useful accuracies required by many language processing tasks. Use of parallel text between resource-rich and resource-poor languages is one source ofweak supervision that significantly improves accuracy. However, parallel text is not always available and techniques for using it require multiple complex algorithmic steps. In this paper we show that we can build POS-taggers exceeding state-of-the-art bilingual methods by using simple hidden Markov models and a freely available and naturally growing resource, the Wiktionary. Across eight languages for which we have labeled data to evaluate results, we achieve accuracy that significantly exceeds best unsupervised and parallel text methods. We achieve highest accuracy reported for several languages and show that our . approach yields better out-of-domain taggers than those trained using fully supervised Penn Treebank.</p><p>6 0.7178368 <a title="39-lda-6" href="./emnlp-2012-Multi-Domain_Learning%3A_When_Do_Domains_Matter%3F.html">92 emnlp-2012-Multi-Domain Learning: When Do Domains Matter?</a></p>
<p>7 0.7169407 <a title="39-lda-7" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>8 0.71576309 <a title="39-lda-8" href="./emnlp-2012-Using_Discourse_Information_for_Paraphrase_Extraction.html">135 emnlp-2012-Using Discourse Information for Paraphrase Extraction</a></p>
<p>9 0.71160275 <a title="39-lda-9" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>10 0.70962423 <a title="39-lda-10" href="./emnlp-2012-An_Entity-Topic_Model_for_Entity_Linking.html">19 emnlp-2012-An Entity-Topic Model for Entity Linking</a></p>
<p>11 0.70688999 <a title="39-lda-11" href="./emnlp-2012-Generalizing_Sub-sentential_Paraphrase_Acquisition_across_Original_Signal_Type_of_Text_Pairs.html">58 emnlp-2012-Generalizing Sub-sentential Paraphrase Acquisition across Original Signal Type of Text Pairs</a></p>
<p>12 0.70480478 <a title="39-lda-12" href="./emnlp-2012-Entity_based_QA_Retrieval.html">41 emnlp-2012-Entity based QA Retrieval</a></p>
<p>13 0.70460272 <a title="39-lda-13" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>14 0.7045716 <a title="39-lda-14" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>15 0.70095521 <a title="39-lda-15" href="./emnlp-2012-Why_Question_Answering_using_Sentiment_Analysis_and_Word_Classes.html">137 emnlp-2012-Why Question Answering using Sentiment Analysis and Word Classes</a></p>
<p>16 0.69934368 <a title="39-lda-16" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>17 0.69822633 <a title="39-lda-17" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>18 0.69811213 <a title="39-lda-18" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>19 0.69801104 <a title="39-lda-19" href="./emnlp-2012-Probabilistic_Finite_State_Machines_for_Regression-based_MT_Evaluation.html">108 emnlp-2012-Probabilistic Finite State Machines for Regression-based MT Evaluation</a></p>
<p>20 0.69571584 <a title="39-lda-20" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
