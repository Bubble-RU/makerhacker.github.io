<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-77" href="#">emnlp2012-77</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</h1>
<br/><p>Source: <a title="emnlp-2012-77-pdf" href="http://aclweb.org/anthology//D/D12/D12-1080.pdf">pdf</a></p><p>Author: David McClosky ; Christopher D. Manning</p><p>Abstract: We present a distantly supervised system for extracting the temporal bounds of fluents (relations which only hold during certain times, such as attends school). Unlike previous pipelined approaches, our model does not assume independence between each fluent or even between named entities with known connections (parent, spouse, employer, etc.). Instead, we model what makes timelines of fluents consistent by learning cross-fluent constraints, potentially spanning entities as well. For example, our model learns that someone is unlikely to start a job at age two or to marry someone who hasn’t been born yet. Our system achieves a 36% error reduction over a pipelined baseline.</p><p>Reference: <a title="emnlp-2012-77-reference" href="../emnlp2012_reference/emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  ,  Abstract We present a distantly supervised system for extracting the temporal bounds of fluents (relations which only hold during certain times, such as attends school). [sent-3, score-1.183]
</p><p>2 Unlike previous pipelined approaches, our model does not assume independence between each fluent or even between named entities with known connections (parent, spouse, employer, etc. [sent-4, score-0.479]
</p><p>3 Instead, we model what makes timelines of fluents consistent by learning cross-fluent constraints, potentially spanning entities as well. [sent-6, score-0.771]
</p><p>4 1 Introduction Many information extraction (IE) systems tradition-  ally extracted just relations, but a great many real world relations such as attends school or has spouse vary over time. [sent-9, score-0.264]
</p><p>5 To capture this, some recent IE systems have extended their focus from relations to fluents (relations combined with temporal bounds). [sent-10, score-1.007]
</p><p>6 This can be seen in the temporal slot filling track in the TAC-KBP 2011 shared task (Ji et al. [sent-11, score-0.558]
</p><p>7 Fluents can be grouped together to form timelines (see Figure 1 for an example) and provide easily capturable consistency constraints. [sent-15, score-0.275]
</p><p>8 Each time span represents afluent (a relation with temporal bounds). [sent-17, score-0.538]
</p><p>9 These heuristics tend to optimize only local consistency (within a single fluent) but ignore more global constraints across fluents (e. [sent-27, score-0.741]
</p><p>10 , attending a school before being born) or across fluents of two linked entities (e. [sent-29, score-0.718]
</p><p>11 Additionally, our general approach is not specific to extracting temporal boundaries of fluents. [sent-34, score-0.474]
</p><p>12 Queries are named entities (people or organizations) with their gold relations but no temporal bounds. [sent-41, score-0.565]
</p><p>13 The database consists of training entities with their fluents, including known temporal bounds for each fluent. [sent-42, score-0.662]
</p><p>14 In addition to missing fluents for an entity, some temporal bounds may be missing from the database; missing bounds are unfortunately indistinguishable from unbounded ranges. [sent-45, score-1.276]
</p><p>15 As a result, we can only trust concrete temporal boundaries in the database. [sent-46, score-0.474]
</p><p>16 For each fluent, systems must output their predicted temporal  bounds, along with references to source documents to provide provenance. [sent-48, score-0.433]
</p><p>17 In Temporal KBP, the temporal representation allows for upper and lower bounds on both the event start and end: hsl, su, el, eui where sl ≤ start ≤ su, el ≤ end ≤ eu. [sent-52, score-0.686]
</p><p>18 Our temporal representation is limited to bounds of the form hstart, endi wtathioerne seit lhimeri can ob eb uunnbdosu onfde thde or rumnk hnsotwarnt (both represented as ±∞). [sent-55, score-0.552]
</p><p>19 Our goal was to use as much temporal information as possible, with the hope of each fluent providing additional potential constraints. [sent-58, score-0.815]
</p><p>20 org 2This is because these fluents  are  rarely present in Freebase  874 people and organizations, we add a special fluent, lifespan, which doesn’t take a slot value. [sent-61, score-0.611]
</p><p>21 3  Model  To operate on a set of queries, we first collect candidate temporal expression mentions for each fluent from our source documents. [sent-63, score-0.982]
</p><p>22 This limits us to using temporal expression mentions which appear near fluent mentions in text. [sent-64, score-0.993]
</p><p>23 It also ensures that we can provide provenance for each temporal boundary assertion. [sent-65, score-0.461]
</p><p>24 The classifier component determines how each candidate temporal expression mention connects to its fluent (§3. [sent-69, score-1.09]
</p><p>25 4 For features, the classifier uses the surrounding textual and syntactic context of temporal expression and fluent mentions. [sent-73, score-0.983]
</p><p>26 The consistency component learns what makes timelines consistent (§3. [sent-76, score-0.351]
</p><p>27 Unlike the classifier component, the consistency component is blind to the underlying text in the source documents. [sent-84, score-0.247]
</p><p>28 The two components work together to find a global timeline that is both based on textual evidence and coherent across entities using with temporal bounds. [sent-85, score-0.684]
</p><p>29 4Other metarelations are possible under more complex temporal representations. [sent-87, score-0.46]
</p><p>30 Note that temporal bounds differ in their resolution (some are days of the year, others are only years). [sent-92, score-0.552]
</p><p>31 , the start of the attends school fluent) and indistinguishable from unbounded. [sent-95, score-0.23]
</p><p>32 1 Temporal expression retrieval Given a fluent, we search for all textual mentions of the fluent and collect nearby temporal expression mentions. [sent-101, score-1.072]
</p><p>33 These temporal expressions are used as candidate boundaries for the fluent in later steps. [sent-102, score-0.914]
</p><p>34 On top of this, we apply a rule-based temporal expression extractor (Chang and Manning, 2012). [sent-110, score-0.567]
</p><p>35 The temporal expression extractor handles most standard date and time formats. [sent-112, score-0.567]
</p><p>36 For this work, we use the gold lifespan bounds as slot values for the purpose of document retrieval. [sent-119, score-0.389]
</p><p>37 We create training datums by computing the metarelation between each temporal expression and its gold fluent. [sent-130, score-0.606]
</p><p>38 For example, for the temporal expression mention “September 15th, 1981” and gold lifespan relation that spans [19 8 1 9-15, -0 +∞), we would assign the START metarelation. [sent-131, score-0.861]
</p><p>39 2 Classifier component We use a classifier to determine the nature of the link between fluents and candidate temporal expression mentions. [sent-135, score-1.212]
</p><p>40 These include standard relation extraction features such as the dependency paths between the temporal expression and the entity or slot value. [sent-139, score-0.668]
</p><p>41 , the words and tags between the entity and the temporal expression) if the path is five tokens or shorter. [sent-144, score-0.456]
</p><p>42 For temporal expressions, we include their century and decade as features. [sent-145, score-0.433]
</p><p>43 These features act as a crude prior over when valid temporal expressions occur. [sent-146, score-0.469]
</p><p>44 There are also features for the precision of the temporal expression (year only, has month, and has day). [sent-147, score-0.545]
</p><p>45 To calculate the likelihood of a specific temporal span for a fluent f, we represent the span as a series of metarelations and take the product of their probabilities. [sent-155, score-1.0]
</p><p>46 For example, if the candidate span is [19 8 1 9-15, +∞) and we have two temporal -0 expressions, “September 15th, e19 h8av1e” a twndo o“2 te0m1p2”o:r P? [sent-156, score-0.534]
</p><p>47 Our consistency component is designed to be as general as possible it does not even include basic constraints about timelines such as “starts are before ends. [sent-178, score-0.371]
</p><p>48 ” Instead, we provide several simple templates for temporal constraints to allow it to learn these basic tendencies as well as more complex ones. [sent-179, score-0.477]
</p><p>49 Other questions can be asked at the fluent level rather than the boundary level (Allen, 1983). [sent-188, score-0.473]
</p><p>50 One set of fluent level questions asks whether two fluents’ spans OVERLAP. [sent-189, score-0.506]
</p><p>51 For example, in Table 1, Jon Stewart’s lifespan OVERLAPs with the span of his has spouse fluent. [sent-190, score-0.335]
</p><p>52 Other sets of fluent level questions ask whether the span of a fluent completely CONTAINS the span of another one, whether a fluent is COMPLETELY BEFORE another fluent, and whether two fluents TOUCH (the start of one fluent is the same as the end of another). [sent-191, score-2.39]
</p><p>53 There is nothing which requires that the fluents in question come from a single entity. [sent-194, score-0.537]
</p><p>54 For example, since Jon Stewart is linked to Tracey McShane by the has spouse fluent (Table 1), we could ask the question of whether Jon Stewart’s lifespan OVERLAPS Tracey McShane’s lifespan. [sent-196, score-0.691]
</p><p>55 We can ask any type of question about two linked entities and distinguish the questions by prefixing them with the nature of the link (has spouse in this case). [sent-197, score-0.271]
</p><p>56 For example, the start of the Jon Stewart’s attends school fluent is un-  defined in the database, but clearly not actually −∞. [sent-200, score-0.583]
</p><p>57 Fluent level questions have known answers as long as both fluents have at least one finite value. [sent-203, score-0.635]
</p><p>58 To train our model, we gather the answers to questions over all the fluents from training entities. [sent-204, score-0.635]
</p><p>59 Additionally, it is possible to see entirely new questions wtiohnenal we see 877  a new combination of fluent types. [sent-210, score-0.445]
</p><p>60 To adjust the weight of the consistency component relative to the classifier component, we take the geometric mean of the likelihood using the total number of questions, |Q(t) |, as the exponent and rtaalis neu mtheb resulting mean |tQo an exponent, β. [sent-213, score-0.247]
</p><p>61 Since it assumes all fluents are independent, the bounds for each fluent can be inferred separately. [sent-219, score-1.038]
</p><p>62 To perform inference on a specific fluent, we consider all of its possible temporal spans, limited by the temporal expression mentions found by the retrieval system (§3. [sent-220, score-1.038]
</p><p>63 io Enasc htop eoascshib claensdpiadnaates temporal expression for the fluent. [sent-224, score-0.545]
</p><p>64 Of course, we typically have multiple candidate temporal expressions and thus potentially many more than four possible spans. [sent-226, score-0.491]
</p><p>65 All temporal expression mentions that resolve to the same time are grouped together, since it  wouldn’t make sense to assign “August 28th, 2010” one metarelation and a different one to “8/28/2010. [sent-227, score-0.618]
</p><p>66 Thus, we need to apply techniques like Gibbs sampling or randomrestart hillclimbing (RRHC) to determine the optimal temporal spans for each fluent. [sent-229, score-0.534]
</p><p>67 RRHC involves looping over all fluents in our testing entities, shuffling the order of the fluents at the beginning of each pass. [sent-231, score-1.074]
</p><p>68 For each fluent and span hf, si ∈ t, we pick the optimal span for f:  s∗ = asr′g∈Sm(fa)xscoreJCC(ts′) where S(f) determines all possible temporal spans for the fluent f and ts′ = (t ∪ hf, s′i) −hf, si is a copy of t where s′ is th=e span f,osr f −inhsfte,asdi of s. [sent-233, score-1.495]
</p><p>69 Rather than calculating th=e score hoff,tshe if)u l−l timeline, we can save tailmcuel by using only the relevant fluents in ts′ . [sent-235, score-0.537]
</p><p>70 For example, if our fluent is the has spouse fluent for Jon Stewart, we include all the fluents involving Jon Stewart and any relevant linked entities. [sent-236, score-1.41]
</p><p>71 In this case, we also include all the fluents for Tracey McShane. [sent-237, score-0.537]
</p><p>72 Each round of RRHC consists of two passes through the fluents we are inferring: An arg max pass followed by a randomization pass where we randomly choose spans for a random fraction of the fluents. [sent-238, score-0.598]
</p><p>73 4  Experiments  We evaluate our models (CC and JCC) according to their ability to predict the temporal bounds of fluents from Freebase. [sent-240, score-1.089]
</p><p>74 Since ited effect if entities entities, we restrict to at least one other  our consistency model has limdo not have any links to other our attention to entities linked entity this eliminates a large –  878 portion of possible entities. [sent-245, score-0.338]
</p><p>75 , 2011) to work with 2-tuples for temporal representations rather than the 4-tuples in Temporal KBP. [sent-254, score-0.433]
</p><p>76 The metric favors tighter bounds on fluents while giving partial credit. [sent-255, score-0.656]
</p><p>77 Thus, for gold fluents  with only year- or month-level resolution, we treat them as their earliest (for starts) or latest (for ends) possible day. [sent-257, score-0.609]
</p><p>78 In these cases, we give systems the benefit of the doubt and greedily align fluents in such a way as to maximize the metric. [sent-265, score-0.537]
</p><p>79 The total metric computes the score of each fluent divided by the number of fluents. [sent-266, score-0.382]
</p><p>80 This baseline assumes that  all fluents are unbounded in their spans. [sent-272, score-0.576]
</p><p>81 of this baseline is primarily to show the approximate minimal value for the temporal metric. [sent-281, score-0.433]
</p><p>82 If the classifier assigns START AND END, we add the candidate temporal expression to both. [sent-288, score-0.623]
</p><p>83 Our baseline assigns the earliest start and the latest end as the bounds for each fluent, assigning ±∞ for empty lists. [sent-291, score-0.249]
</p><p>84 To determine the best possible score given our temporal expression retrieval system, we calculate the oracle score by assigning each fluent the span which maximizes the temporal metric. [sent-294, score-1.477]
</p><p>85 This is presumably because the classifier suffers from insufficient data and the consistency component is able to learn consistency rules to recover from this. [sent-305, score-0.386]
</p><p>86 5  Discussion  Table 3 shows the performance of four systems and baselines on individual fluent types. [sent-313, score-0.382]
</p><p>87 The JCC model derives most of its improvement from the two lifespan fluents and other high frequency fluents. [sent-314, score-0.712]
</p><p>88 The null baseline is especially strong for several fluents since these tend to be unbounded or (more likely) missing their values in Freebase. [sent-320, score-0.609]
</p><p>89 The two basic aggregation models differ primarily on their predictions  for the lifespan fluents. [sent-321, score-0.232]
</p><p>90 Additionally, the lifespan fluent is always present for entities while other fluents are sparser. [sent-323, score-1.168]
</p><p>91 Inspecting the multinomials in the consistency component, we can see that the model learns reasonable answers to questions such as whether an entity  “was born before getting married? [sent-327, score-0.299]
</p><p>92 6  Related work  There is a large body of related work that focuses on ordering events or classifying temporal relations between them (Ling and Weld, 2010; Yoshikawa et al. [sent-338, score-0.47]
</p><p>93 Two recent examples in information extraction include using Markov Logic for temporal ordering (Ling and Weld, 2010) and using dualdecomposition for event extraction (Riedel and McCallum, 2011). [sent-350, score-0.433]
</p><p>94 , 2011) for this task used classifiers to determine the relation between temporal expressions and fluents. [sent-354, score-0.495]
</p><p>95 These systems use the hard decisions from the classifier and combine the decisions by finding a span that includes all temporal expressions. [sent-355, score-0.568]
</p><p>96 In contrast, our system uses the classifier’s marginal probabilities along with the consistency component to incorporate global consistency constraints. [sent-356, score-0.33]
</p><p>97 Outside of Temporal KBP, there are several works on the task of extracting fluents from text. [sent-360, score-0.537]
</p><p>98 (2012) apply a similar approach by ag-  gregating local classification decisions using temporal constraints (e. [sent-364, score-0.477]
</p><p>99 This could be accomplished by using the marginal probabilities on the extracted relations and multiplying them with the probabilities from the classifier and consistency components. [sent-376, score-0.232]
</p><p>100 Furthermore, the consistency component can be extended with new question types to incorporate non-temporal constraints as well. [sent-378, score-0.235]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fluents', 0.537), ('temporal', 0.433), ('fluent', 0.382), ('timeline', 0.177), ('jcc', 0.175), ('lifespan', 0.175), ('consistency', 0.139), ('timelines', 0.136), ('kbp', 0.125), ('bounds', 0.119), ('expression', 0.112), ('attends', 0.094), ('stewart', 0.081), ('spouse', 0.081), ('span', 0.079), ('slot', 0.074), ('entities', 0.074), ('artiles', 0.069), ('questions', 0.063), ('spans', 0.061), ('tac', 0.057), ('aggregation', 0.057), ('classifier', 0.056), ('start', 0.055), ('rrhc', 0.054), ('tracey', 0.054), ('jon', 0.052), ('component', 0.052), ('school', 0.052), ('filling', 0.051), ('september', 0.051), ('constraints', 0.044), ('hf', 0.043), ('boundaries', 0.041), ('job', 0.041), ('hillclimbing', 0.04), ('mcshane', 0.04), ('metarelation', 0.04), ('unbounded', 0.039), ('born', 0.039), ('cc', 0.039), ('oracle', 0.038), ('relations', 0.037), ('expressions', 0.036), ('database', 0.036), ('answers', 0.035), ('freebase', 0.035), ('null', 0.033), ('mention', 0.033), ('mentions', 0.033), ('dates', 0.033), ('someone', 0.033), ('surdeanu', 0.031), ('yes', 0.031), ('stanford', 0.031), ('allen', 0.029), ('indistinguishable', 0.029), ('linked', 0.028), ('boundary', 0.028), ('toutanova', 0.028), ('distant', 0.028), ('unknown', 0.027), ('latest', 0.027), ('attending', 0.027), ('burman', 0.027), ('garrido', 0.027), ('hasn', 0.027), ('marry', 0.027), ('metarelations', 0.027), ('pconsistency', 0.027), ('stewarthas', 0.027), ('underspecified', 0.027), ('inference', 0.027), ('relation', 0.026), ('joint', 0.026), ('ask', 0.025), ('earliest', 0.024), ('angel', 0.024), ('end', 0.024), ('consistent', 0.024), ('entity', 0.023), ('pcc', 0.023), ('pipelined', 0.023), ('christopher', 0.023), ('chang', 0.023), ('candidate', 0.022), ('extractor', 0.022), ('ji', 0.022), ('manning', 0.022), ('conference', 0.022), ('unrelated', 0.021), ('association', 0.021), ('heuristics', 0.021), ('coreference', 0.021), ('ford', 0.021), ('yoshikawa', 0.021), ('craven', 0.021), ('cuny', 0.021), ('ling', 0.021), ('gold', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="77-tfidf-1" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>Author: David McClosky ; Christopher D. Manning</p><p>Abstract: We present a distantly supervised system for extracting the temporal bounds of fluents (relations which only hold during certain times, such as attends school). Unlike previous pipelined approaches, our model does not assume independence between each fluent or even between named entities with known connections (parent, spouse, employer, etc.). Instead, we model what makes timelines of fluents consistent by learning cross-fluent constraints, potentially spanning entities as well. For example, our model learns that someone is unlikely to start a job at age two or to marry someone who hasn’t been born yet. Our system achieves a 36% error reduction over a pipelined baseline.</p><p>2 0.25741804 <a title="77-tfidf-2" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>Author: Quang Do ; Wei Lu ; Dan Roth</p><p>Abstract: This paper addresses the task of constructing a timeline of events mentioned in a given text. To accomplish that, we present a novel representation of the temporal structure of a news article based on time intervals. We then present an algorithmic approach that jointly optimizes the temporal structure by coupling local classifiers that predict associations and temporal relations between pairs of temporal entities with global constraints. Moreover, we present ways to leverage knowledge provided by event coreference to further improve the system performance. Overall, our experiments show that the joint inference model significantly outperformed the local classifiers by 9.2% of relative improvement in F1. The experiments also suggest that good event coreference could make remarkable contribution to a robust event timeline construction system.</p><p>3 0.12070706 <a title="77-tfidf-3" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>Author: Mihai Surdeanu ; Julie Tibshirani ; Ramesh Nallapati ; Christopher D. Manning</p><p>Abstract: Distant supervision for relation extraction (RE) gathering training data by aligning a database of facts with text – is an efficient approach to scale RE to thousands of different relations. However, this introduces a challenging learning scenario where the relation expressed by a pair of entities found in a sentence is unknown. For example, a sentence containing Balzac and France may express BornIn or Died, an unknown relation, or no relation at all. Because of this, traditional supervised learning, which assumes that each example is explicitly mapped to a label, is not appropriate. We propose a novel approach to multi-instance multi-label learning for RE, which jointly models all the instances of a pair of entities in text and all their labels using a graphical model with latent variables. Our model performs competitively on two difficult domains. –</p><p>4 0.084317446 <a title="77-tfidf-4" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>Author: Thomas Lin ; Mausam ; Oren Etzioni</p><p>Abstract: Entity linking systems link noun-phrase mentions in text to their corresponding Wikipedia articles. However, NLP applications would gain from the ability to detect and type all entities mentioned in text, including the long tail of entities not prominent enough to have their own Wikipedia articles. In this paper we show that once the Wikipedia entities mentioned in a corpus of textual assertions are linked, this can further enable the detection and fine-grained typing of the unlinkable entities. Our proposed method for detecting unlinkable entities achieves 24% greater accuracy than a Named Entity Recognition baseline, and our method for fine-grained typing is able to propagate over 1,000 types from linked Wikipedia entities to unlinkable entities. Detection and typing of unlinkable entities can increase yield for NLP applications such as typed question answering.</p><p>5 0.068816319 <a title="77-tfidf-5" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>Author: Heeyoung Lee ; Marta Recasens ; Angel Chang ; Mihai Surdeanu ; Dan Jurafsky</p><p>Abstract: We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable documents, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.</p><p>6 0.064078838 <a title="77-tfidf-6" href="./emnlp-2012-Discovering_Diverse_and_Salient_Threads_in_Document_Collections.html">33 emnlp-2012-Discovering Diverse and Salient Threads in Document Collections</a></p>
<p>7 0.060450166 <a title="77-tfidf-7" href="./emnlp-2012-Identifying_Constant_and_Unique_Relations_by_using_Time-Series_Text.html">62 emnlp-2012-Identifying Constant and Unique Relations by using Time-Series Text</a></p>
<p>8 0.059804056 <a title="77-tfidf-8" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>9 0.058622841 <a title="77-tfidf-9" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<p>10 0.058589689 <a title="77-tfidf-10" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>11 0.057654802 <a title="77-tfidf-11" href="./emnlp-2012-Learning-based_Multi-Sieve_Co-reference_Resolution_with_Knowledge.html">76 emnlp-2012-Learning-based Multi-Sieve Co-reference Resolution with Knowledge</a></p>
<p>12 0.055726849 <a title="77-tfidf-12" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>13 0.052687578 <a title="77-tfidf-13" href="./emnlp-2012-A_Novel_Discriminative_Framework_for_Sentence-Level_Discourse_Analysis.html">7 emnlp-2012-A Novel Discriminative Framework for Sentence-Level Discourse Analysis</a></p>
<p>14 0.050994869 <a title="77-tfidf-14" href="./emnlp-2012-Entity_based_QA_Retrieval.html">41 emnlp-2012-Entity based QA Retrieval</a></p>
<p>15 0.050505798 <a title="77-tfidf-15" href="./emnlp-2012-An_Entity-Topic_Model_for_Entity_Linking.html">19 emnlp-2012-An Entity-Topic Model for Entity Linking</a></p>
<p>16 0.047569491 <a title="77-tfidf-16" href="./emnlp-2012-Joint_Learning_for_Coreference_Resolution_with_Markov_Logic.html">73 emnlp-2012-Joint Learning for Coreference Resolution with Markov Logic</a></p>
<p>17 0.046178516 <a title="77-tfidf-17" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>18 0.045722622 <a title="77-tfidf-18" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>19 0.044481263 <a title="77-tfidf-19" href="./emnlp-2012-Besting_the_Quiz_Master%3A_Crowdsourcing_Incremental_Classification_Games.html">23 emnlp-2012-Besting the Quiz Master: Crowdsourcing Incremental Classification Games</a></p>
<p>20 0.043889042 <a title="77-tfidf-20" href="./emnlp-2012-Multiple_Aspect_Summarization_Using_Integer_Linear_Programming.html">94 emnlp-2012-Multiple Aspect Summarization Using Integer Linear Programming</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.172), (1, 0.15), (2, 0.01), (3, -0.141), (4, 0.016), (5, -0.065), (6, -0.051), (7, -0.003), (8, -0.07), (9, 0.044), (10, 0.005), (11, -0.016), (12, -0.043), (13, -0.13), (14, -0.08), (15, 0.009), (16, -0.039), (17, -0.034), (18, 0.223), (19, -0.214), (20, -0.019), (21, -0.045), (22, 0.133), (23, 0.121), (24, 0.145), (25, -0.015), (26, -0.027), (27, 0.005), (28, -0.139), (29, -0.268), (30, 0.204), (31, -0.119), (32, 0.046), (33, -0.041), (34, -0.019), (35, 0.047), (36, 0.021), (37, 0.024), (38, 0.178), (39, 0.001), (40, 0.051), (41, 0.19), (42, -0.145), (43, 0.193), (44, -0.056), (45, 0.007), (46, -0.061), (47, 0.008), (48, 0.048), (49, -0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95956045 <a title="77-lsi-1" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>Author: David McClosky ; Christopher D. Manning</p><p>Abstract: We present a distantly supervised system for extracting the temporal bounds of fluents (relations which only hold during certain times, such as attends school). Unlike previous pipelined approaches, our model does not assume independence between each fluent or even between named entities with known connections (parent, spouse, employer, etc.). Instead, we model what makes timelines of fluents consistent by learning cross-fluent constraints, potentially spanning entities as well. For example, our model learns that someone is unlikely to start a job at age two or to marry someone who hasn’t been born yet. Our system achieves a 36% error reduction over a pipelined baseline.</p><p>2 0.70095074 <a title="77-lsi-2" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>Author: Quang Do ; Wei Lu ; Dan Roth</p><p>Abstract: This paper addresses the task of constructing a timeline of events mentioned in a given text. To accomplish that, we present a novel representation of the temporal structure of a news article based on time intervals. We then present an algorithmic approach that jointly optimizes the temporal structure by coupling local classifiers that predict associations and temporal relations between pairs of temporal entities with global constraints. Moreover, we present ways to leverage knowledge provided by event coreference to further improve the system performance. Overall, our experiments show that the joint inference model significantly outperformed the local classifiers by 9.2% of relative improvement in F1. The experiments also suggest that good event coreference could make remarkable contribution to a robust event timeline construction system.</p><p>3 0.43241096 <a title="77-lsi-3" href="./emnlp-2012-Discovering_Diverse_and_Salient_Threads_in_Document_Collections.html">33 emnlp-2012-Discovering Diverse and Salient Threads in Document Collections</a></p>
<p>Author: Jennifer Gillenwater ; Alex Kulesza ; Ben Taskar</p><p>Abstract: We propose a novel probabilistic technique for modeling and extracting salient structure from large document collections. As in clustering and topic modeling, our goal is to provide an organizing perspective into otherwise overwhelming amounts of information. We are particularly interested in revealing and exploiting relationships between documents. To this end, we focus on extracting diverse sets of threads—singlylinked, coherent chains of important documents. To illustrate, we extract research threads from citation graphs and construct timelines from news articles. Our method is highly scalable, running on a corpus of over 30 million words in about four minutes, more than 75 times faster than a dynamic topic model. Finally, the results from our model more closely resemble human news summaries according to several metrics and are also preferred by human judges.</p><p>4 0.3967441 <a title="77-lsi-4" href="./emnlp-2012-Employing_Compositional_Semantics_and_Discourse_Consistency_in_Chinese_Event_Extraction.html">38 emnlp-2012-Employing Compositional Semantics and Discourse Consistency in Chinese Event Extraction</a></p>
<p>Author: Peifeng Li ; Guodong Zhou ; Qiaoming Zhu ; Libin Hou</p><p>Abstract: Current Chinese event extraction systems suffer much from two problems in trigger identification: unknown triggers and word segmentation errors to known triggers. To resolve these problems, this paper proposes two novel inference mechanisms to explore special characteristics in Chinese via compositional semantics inside Chinese triggers and discourse consistency between Chinese trigger mentions. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our approach over a strong baseline. 1</p><p>5 0.34831077 <a title="77-lsi-5" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>Author: Mihai Surdeanu ; Julie Tibshirani ; Ramesh Nallapati ; Christopher D. Manning</p><p>Abstract: Distant supervision for relation extraction (RE) gathering training data by aligning a database of facts with text – is an efficient approach to scale RE to thousands of different relations. However, this introduces a challenging learning scenario where the relation expressed by a pair of entities found in a sentence is unknown. For example, a sentence containing Balzac and France may express BornIn or Died, an unknown relation, or no relation at all. Because of this, traditional supervised learning, which assumes that each example is explicitly mapped to a label, is not appropriate. We propose a novel approach to multi-instance multi-label learning for RE, which jointly models all the instances of a pair of entities in text and all their labels using a graphical model with latent variables. Our model performs competitively on two difficult domains. –</p><p>6 0.30276096 <a title="77-lsi-6" href="./emnlp-2012-Identifying_Constant_and_Unique_Relations_by_using_Time-Series_Text.html">62 emnlp-2012-Identifying Constant and Unique Relations by using Time-Series Text</a></p>
<p>7 0.24959493 <a title="77-lsi-7" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>8 0.23862264 <a title="77-lsi-8" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<p>9 0.23585626 <a title="77-lsi-9" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>10 0.21189803 <a title="77-lsi-10" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>11 0.20665561 <a title="77-lsi-11" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>12 0.19991785 <a title="77-lsi-12" href="./emnlp-2012-Excitatory_or_Inhibitory%3A_A_New_Semantic_Orientation_Extracts_Contradiction_and_Causality_from_the_Web.html">44 emnlp-2012-Excitatory or Inhibitory: A New Semantic Orientation Extracts Contradiction and Causality from the Web</a></p>
<p>13 0.19609244 <a title="77-lsi-13" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>14 0.1942648 <a title="77-lsi-14" href="./emnlp-2012-Besting_the_Quiz_Master%3A_Crowdsourcing_Incremental_Classification_Games.html">23 emnlp-2012-Besting the Quiz Master: Crowdsourcing Incremental Classification Games</a></p>
<p>15 0.19367039 <a title="77-lsi-15" href="./emnlp-2012-Identifying_Event-related_Bursts_via_Social_Media_Activities.html">63 emnlp-2012-Identifying Event-related Bursts via Social Media Activities</a></p>
<p>16 0.19054408 <a title="77-lsi-16" href="./emnlp-2012-Entity_based_QA_Retrieval.html">41 emnlp-2012-Entity based QA Retrieval</a></p>
<p>17 0.1793607 <a title="77-lsi-17" href="./emnlp-2012-A_Sequence_Labelling_Approach_to_Quote_Attribution.html">9 emnlp-2012-A Sequence Labelling Approach to Quote Attribution</a></p>
<p>18 0.17934856 <a title="77-lsi-18" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>19 0.16643646 <a title="77-lsi-19" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>20 0.16473234 <a title="77-lsi-20" href="./emnlp-2012-Local_and_Global_Context_for_Supervised_and_Unsupervised_Metonymy_Resolution.html">85 emnlp-2012-Local and Global Context for Supervised and Unsupervised Metonymy Resolution</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.04), (16, 0.036), (18, 0.025), (25, 0.021), (34, 0.076), (45, 0.014), (60, 0.082), (63, 0.067), (64, 0.036), (65, 0.026), (68, 0.262), (70, 0.014), (73, 0.025), (74, 0.055), (76, 0.057), (80, 0.023), (86, 0.019), (95, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.85400099 <a title="77-lda-1" href="./emnlp-2012-Identifying_Event-related_Bursts_via_Social_Media_Activities.html">63 emnlp-2012-Identifying Event-related Bursts via Social Media Activities</a></p>
<p>Author: Xin Zhao ; Baihan Shu ; Jing Jiang ; Yang Song ; Hongfei Yan ; Xiaoming Li</p><p>Abstract: Activities on social media increase at a dramatic rate. When an external event happens, there is a surge in the degree of activities related to the event. These activities may be temporally correlated with one another, but they may also capture different aspects of an event and therefore exhibit different bursty patterns. In this paper, we propose to identify event-related bursts via social media activities. We study how to correlate multiple types of activities to derive a global bursty pattern. To model smoothness of one state sequence, we propose a novel function which can capture the state context. The experiments on a large Twitter dataset shows our methods are very effective.</p><p>same-paper 2 0.79389757 <a title="77-lda-2" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>Author: David McClosky ; Christopher D. Manning</p><p>Abstract: We present a distantly supervised system for extracting the temporal bounds of fluents (relations which only hold during certain times, such as attends school). Unlike previous pipelined approaches, our model does not assume independence between each fluent or even between named entities with known connections (parent, spouse, employer, etc.). Instead, we model what makes timelines of fluents consistent by learning cross-fluent constraints, potentially spanning entities as well. For example, our model learns that someone is unlikely to start a job at age two or to marry someone who hasn’t been born yet. Our system achieves a 36% error reduction over a pipelined baseline.</p><p>3 0.51505083 <a title="77-lda-3" href="./emnlp-2012-Streaming_Analysis_of_Discourse_Participants.html">120 emnlp-2012-Streaming Analysis of Discourse Participants</a></p>
<p>Author: Benjamin Van Durme</p><p>Abstract: Inferring attributes of discourse participants has been treated as a batch-processing task: data such as all tweets from a given author are gathered in bulk, processed, analyzed for a particular feature, then reported as a result of academic interest. Given the sources and scale of material used in these efforts, along with potential use cases of such analytic tools, discourse analysis should be reconsidered as a streaming challenge. We show that under certain common formulations, the batchprocessing analytic framework can be decomposed into a sequential series of updates, using as an example the task of gender classification. Once in a streaming framework, and motivated by large data sets generated by social media services, we present novel results in approximate counting, showing its applicability to space efficient streaming classification.</p><p>4 0.513089 <a title="77-lda-4" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms ofweak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.</p><p>5 0.50786626 <a title="77-lda-5" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: We present a new family of models for unsupervised parsing, Dependency and Boundary models, that use cues at constituent boundaries to inform head-outward dependency tree generation. We build on three intuitions that are explicit in phrase-structure grammars but only implicit in standard dependency formulations: (i) Distributions of words that occur at sentence boundaries such as English determiners resemble constituent edges. (ii) Punctuation at sentence boundaries further helps distinguish full sentences from fragments like headlines and titles, allowing us to model grammatical differences between complete and incomplete sentences. (iii) Sentence-internal punctuation boundaries help with longer-distance dependencies, since punctuation correlates with constituent edges. Our models induce state-of-the-art dependency grammars for many languages without — — special knowledge of optimal input sentence lengths or biased, manually-tuned initializers.</p><p>6 0.50479251 <a title="77-lda-6" href="./emnlp-2012-Automatically_Constructing_a_Normalisation_Dictionary_for_Microblogs.html">22 emnlp-2012-Automatically Constructing a Normalisation Dictionary for Microblogs</a></p>
<p>7 0.50429493 <a title="77-lda-7" href="./emnlp-2012-Exploiting_Reducibility_in_Unsupervised_Dependency_Parsing.html">46 emnlp-2012-Exploiting Reducibility in Unsupervised Dependency Parsing</a></p>
<p>8 0.50404245 <a title="77-lda-8" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>9 0.50075841 <a title="77-lda-9" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>10 0.49866083 <a title="77-lda-10" href="./emnlp-2012-Type-Supervised_Hidden_Markov_Models_for_Part-of-Speech_Tagging_with_Incomplete_Tag_Dictionaries.html">129 emnlp-2012-Type-Supervised Hidden Markov Models for Part-of-Speech Tagging with Incomplete Tag Dictionaries</a></p>
<p>11 0.49738789 <a title="77-lda-11" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>12 0.49495873 <a title="77-lda-12" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>13 0.49440482 <a title="77-lda-13" href="./emnlp-2012-Besting_the_Quiz_Master%3A_Crowdsourcing_Incremental_Classification_Games.html">23 emnlp-2012-Besting the Quiz Master: Crowdsourcing Incremental Classification Games</a></p>
<p>14 0.49417368 <a title="77-lda-14" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>15 0.49285981 <a title="77-lda-15" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>16 0.49269554 <a title="77-lda-16" href="./emnlp-2012-Extracting_Opinion_Expressions_with_semi-Markov_Conditional_Random_Fields.html">51 emnlp-2012-Extracting Opinion Expressions with semi-Markov Conditional Random Fields</a></p>
<p>17 0.49254099 <a title="77-lda-17" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>18 0.49234623 <a title="77-lda-18" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>19 0.4886992 <a title="77-lda-19" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>20 0.48858356 <a title="77-lda-20" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
