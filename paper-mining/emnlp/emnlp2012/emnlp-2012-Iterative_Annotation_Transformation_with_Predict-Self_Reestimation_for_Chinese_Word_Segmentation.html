<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>68 emnlp-2012-Iterative Annotation Transformation with Predict-Self Reestimation for Chinese Word Segmentation</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-68" href="#">emnlp2012-68</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>68 emnlp-2012-Iterative Annotation Transformation with Predict-Self Reestimation for Chinese Word Segmentation</h1>
<br/><p>Source: <a title="emnlp-2012-68-pdf" href="http://aclweb.org/anthology//D/D12/D12-1038.pdf">pdf</a></p><p>Author: Wenbin Jiang ; Fandong Meng ; Qun Liu ; Yajuan Lu</p><p>Abstract: In this paper we first describe the technology of automatic annotation transformation, which is based on the annotation adaptation algorithm (Jiang et al., 2009). It can automatically transform a human-annotated corpus from one annotation guideline to another. We then propose two optimization strategies, iterative training and predict-selfreestimation, to further improve the accuracy of annotation guideline transformation. Experiments on Chinese word segmentation show that, the iterative training strategy together with predictself reestimation brings significant improvement over the simple annotation transformation baseline, and leads to classifiers with significantly higher accuracy and several times faster processing than annotation adaptation does. On the Penn Chinese Treebank 5.0, , it achieves an F-measure of 98.43%, significantly outperforms previous works although using a single classifier with only local features.</p><p>Reference: <a title="emnlp-2012-68-reference" href="../emnlp2012_reference/emnlp-2012-Iterative_Annotation_Transformation_with_Predict-Self_Reestimation_for_Chinese_Word_Segmentation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 It can automatically transform a human-annotated corpus from one annotation guideline to another. [sent-3, score-0.585]
</p><p>2 We then propose two optimization strategies, iterative training and predict-selfreestimation, to further improve the accuracy of annotation guideline transformation. [sent-4, score-0.788]
</p><p>3 1 Introduction Annotation guideline adaptation depicts a general pipeline to integrate the knowledge of corpora with different underling annotation guidelines (Jiang et al. [sent-9, score-0.808]
</p><p>4 In annotation adaptation two classifiers are cascaded together, where the classification results of the lower classifier are used as guiding features of the upper classifier, in order to achieve more accurate classification. [sent-11, score-0.801]
</p><p>5 This method can automatically adapt the divergence between different annotation guidelines and bring improvement to Chil iuqun lvya j uan} @ i . [sent-12, score-0.374]
</p><p>6 In this paper, we first describe the algorithm of  automatic annotation transformation. [sent-16, score-0.336]
</p><p>7 It is based on the annotation adaptation algorithm, and it focuses on the automatic transformation (rather than adaptation) of a human-annotated corpus from one annotation guideline to another. [sent-17, score-1.599]
</p><p>8 First, a classifier is trained on the corpus with an annotation guideline not desired, it is used to classify the corpus with the annotation guideline we want, so as to obtain a corpus with parallel annotation guidelines. [sent-18, score-1.598]
</p><p>9 Then a second classifier is trained on the parallelly annotated corpus to learn the statistical regularity of annotation transformation, and it is used to process the previous corpus to transform its annotation guideline to that of the target corpus. [sent-19, score-1.358]
</p><p>10 Instead of the online knowledge integration methodology of annotation adaptation, annotation transformation can lead to improved classification accuracy in an offline manner by using the transformed corpora as additional training data for the classifier. [sent-20, score-1.463]
</p><p>11 This method leads to an enhanced classifier with much faster processing than the cascaded classifiers in annotation adaptation. [sent-21, score-0.686]
</p><p>12 We then propose two optimization strategies, iterative training and predict-self reestimation, to further improve the accuracy of annotation transformation. [sent-22, score-0.584]
</p><p>13 The predict-self reestimation is based on the following hypothesis, a better transformation result should be easier to be transformed back to the original form. [sent-27, score-0.976]
</p><p>14 Experiments in Chinese word segmentation show that, the iterative training strategy together with predict-self reestimation brings significant improvement over the simple annotation transformation  baseline. [sent-29, score-1.66]
</p><p>15 We perform optimized annotation transformation from the People’s Daily (Yu et al. [sent-30, score-0.93]
</p><p>16 , 2005), in order to improve the word segmenter with CTB annotation guideline. [sent-33, score-0.36]
</p><p>17 Compared to annotation adaptation, the optimized annotation transformation strategy leads to classifiers with significantly higher accuracy and several times faster processing on the same data sets. [sent-34, score-1.496]
</p><p>18 Section 3 details the simple annotation transformation algorithm and the two optimization methods. [sent-40, score-0.922]
</p><p>19 o 3  Iterative and Predict-Self Annotation Transformation  This section first describes the technology of automatic annotation transformation, then introduces the two optimization strategies, iterative training and predict-self reestimation. [sent-66, score-0.558]
</p><p>20 Iterative training takes a global view, it conducts several rounds of bidirectional annotation transformations, and improve the transformation performance round by round. [sent-67, score-0.898]
</p><p>21 Predict-self reestimation takes a local view instead, it considers each training sentence, and improves the transformation performance by taking into account the predication result of the reverse transformation. [sent-68, score-0.839]
</p><p>22 The two strategies can be adopted jointly to obtain better transformation performance. [sent-69, score-0.599]
</p><p>23 1 Automatic Annotation Transformation Annotation adaptation can integrate the knowledge from two corpora with different underling annotation guidelines. [sent-71, score-0.566]
</p><p>24 First, a classifier (source classi-  fier) is trained on the corpus (source corpus) with an annotation standard (source annotation) not desired, it is then used to classify the corpus (target corpus) with the annotation standard (target annotation) we want. [sent-72, score-0.831]
</p><p>25 Then a second classifier (transformation classifier 1) is trained on the target corpus with 1It is called target classifier in (Jiang et al. [sent-73, score-0.434]
</p><p>26 We think that transformation classifier better reflects its role, the 414  where α is short for α(C0), representing the source annotation of C0. [sent-75, score-1.093]
</p><p>27 In decoding, a raw sentence is first decoded by the source classifier, and then inputted into the transformation classifier together with the annotations given by the source classifier, so as to obtain an improved classification result. [sent-77, score-0.898]
</p><p>28 However, annotation adaptation has a drawback, it has to cascade two classifiers in decoding to integrate the knowledge in two corpora, thus seriously degrades the processing speed. [sent-78, score-0.569]
</p><p>29 This paper describes a variant of annotation adaptation, name annotation transformation, aiming at automatic transformation (rather than adaptation) between annotation standards of human-annotated corpora. [sent-79, score-1.57]
</p><p>30 In annotation transformation, a source classifier and a transformation classifier are trained in the same way as in annotation adaptation. [sent-80, score-1.542]
</p><p>31 The transformation classifier is used to process the source corpus, with the classification label derived from the segmented sentences as the guiding features, so as to relabel the source  corpus with the target annotation guideline. [sent-81, score-1.323]
</p><p>32 By integrating the target corpus and the transformed source corpus for the training of the character classifier, improved classification accuracy can be achieved. [sent-82, score-0.366]
</p><p>33 Both the source classifier and the transformation classifier are trained with the perceptron algorithm. [sent-83, score-0.933]
</p><p>34 The feature templates for the transformation classifier are the same with those in  annotation adaptation, as listed in Table 2. [sent-88, score-1.011]
</p><p>35 Algorithm 2 shows the overall training algorithm for annotation transformation. [sent-89, score-0.336]
</p><p>36 Compared to the online knowledge integration methodology of annotation adaptation, annotation transformation leads to improved performance in an offline manner by integrating corpora  before the training procedure. [sent-93, score-1.352]
</p><p>37 This manner could achieve processing several times as fast as the cascaded classifiers in annotation adaptation. [sent-94, score-0.493]
</p><p>38 2  Iterative Training for Annotation Transformation The training of annotation transformation is based on an auto-generated (rather than gold) parallelly annotated corpus, where the source annotation is pro415 Algorithm  3  Iterative  annotation  transformation. [sent-97, score-1.917]
</p><p>39 Therefore, the performance of transformation training is correspondingly determined by the accuracy of the source classifier. [sent-100, score-0.67]
</p><p>40 We propose an iterative training procedure to gradually improve the transformation accuracy by iteratively optimizing the parallelly annotated corpora. [sent-101, score-1.08]
</p><p>41 In each training iteration, both source-to-target and target-to-source annotation transformations are performed, and the transformed corpora are used to provide better annotations for the parallelly annotated corpora of the next iteration. [sent-102, score-0.897]
</p><p>42 Then in the new iteration, the better parallelly annotated corpora will result in more accurate transformation classifiers, so as to generate better transformed corpora. [sent-103, score-0.998]
</p><p>43 The loop of lines 6-13 iteratively performs source-to-target and target-to-  source annotation transformations. [sent-105, score-0.441]
</p><p>44 The source annotations of the parallelly annotated corpora, Cts and nCsto,t are isn oitfia thlizee pda by applying ttheed source a,n Cd target classifiers respectively on the target and source corpora (lines 2-5). [sent-106, score-0.772]
</p><p>45 In each training iteration, the transformation classifiers are trained on the current parallelly annotated corpora (lines 7-8), they are used to produce the transformed corpora (lines 9-10) which provide better annotations for the parallelly annotated corpora of the next iteration. [sent-107, score-1.51]
</p><p>46 The iterative training terminates when the performance of the classifier trained on the merged corpus Cst ∪ Ct converges. [sent-108, score-0.376]
</p><p>47 In the first iteration, the transformed corpora generated by the transformation classifiers are better than the initialized ones generated by the source and target classifiers, due to the assistance of the guiding features. [sent-110, score-1.003]
</p><p>48 In the following iterations, the transformed corpora provide better annotations for the parallelly annotated corpora of the subsequent iteration, the transformation accuracy will improve gradually along with optimiza-  tion of the parallelly annotated corpora until convergence. [sent-111, score-1.47]
</p><p>49 If applied to the task of annotation transformation, predict-self indicates that a better transformation candidate following the target annotation guideline can be easier transformed back to the original form following the source annotation guideline. [sent-116, score-2.029]
</p><p>50 The most intuitionistic strategy to introduce the predict-self methodology into annotation transformation is using a reversed annotation transformation procedure to filter out unreliable predictions of the previous transformation. [sent-117, score-1.92]
</p><p>51 Instead of using the reversed transformation procedure for filtration, the reestimation strategy integrates the scores given by the source-to-target and target-to-source annotation 416 transformation models when evaluating the transformation candidates. [sent-121, score-2.391]
</p><p>52 By properly tuning the relative weights of the two transformation directions, better transformation performance would be achieved. [sent-122, score-1.124]
</p><p>53 The scores of the two transformation models are weighted integrated in a log-linear manner: S+(y|Ms→t, Mt→s, Φ, x) = (1 − λ) S (y|Ms→t, Φ, x) + λ S (x |Mt→s , Φ, y)  (2)  The weight parameter λ is tuned on the developing set. [sent-123, score-0.6]
</p><p>54 To integrating the predict-self reestimation into the iterative transformation training, a reversed transformation model is introduced and the enhanced scoring function above is used when the function TRANSANNOTATE invokes the function DECODE. [sent-124, score-1.662]
</p><p>55 4  Related Works  Researches focused on the automatic adaptation between different corpora can be roughly classified into two kinds, adaptation between different domains (with different statistical distribution) (Blitzer et al. [sent-125, score-0.334]
</p><p>56 , 2006; Daum e´ III, 2007), and adaptation between different annotation guidelines (Jiang et al. [sent-126, score-0.512]
</p><p>57 There are also some efforts that totally or partially resort to manual transformation rules, to conduct treebank conversion (Cahill and Mccarthy, 2002; Hockenmaier and Steedman, 2007; Clark and Curran, 2009), and word segmentation guideline transformation (Gao et al. [sent-129, score-1.566]
</p><p>58 This work focuses on the automatic transformation between annotation guidelines, and proposes better annotation transfor-  mation technologies to improve the transformation accuracy and the utilization rate of human-annotated knowledge. [sent-132, score-1.846]
</p><p>59 We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. [sent-140, score-1.46]
</p><p>60 We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further im-  prove segmentation performance. [sent-146, score-1.124]
</p><p>61 5  Experiments and Analysis  We perform annotation transformation from People’s Daily (PD) (Yu et al. [sent-147, score-0.898]
</p><p>62 , 2005), following the same experimental setting as the annotation adaptation work (Jiang et al. [sent-150, score-0.474]
</p><p>63 The two corpora are segmented following different segmentation guidelines and differ largely in quantity of data. [sent-152, score-0.327]
</p><p>64 76ya3t59i7(oFn1%tra)nsfo-  mation, annotation adaptation and a simple corpus merging strategy. [sent-164, score-0.497]
</p><p>65 To approximate more general scenarios of annotation adaptation problems, we extract from PD a subset which is comparable to CTB in size. [sent-165, score-0.474]
</p><p>66 The balanced source corpus and target corpus also facilitate the investigation of annotation transformation. [sent-169, score-0.5]
</p><p>67 A classifier performs better in its corresponding test set, and performs significantly worse on a test set following a different annotation guideline. [sent-177, score-0.449]
</p><p>68 are many  Training iterations  Figure 1: Learning curve of iterative training for annotation transformation. [sent-179, score-0.619]
</p><p>69 Annotation Adaptation Experiments of annotation transformation are conducted on the direction of SPD-to-CTB. [sent-182, score-0.898]
</p><p>70 As comparison, the cascaded model of annotation adaptation (Jiang et al. [sent-184, score-0.536]
</p><p>71 Table 5 shows the performances of the classifiers resulted by the baseline annotation transformation and annotation adaptation, as well as the classifier trained on the directly merged corpus. [sent-186, score-1.484]
</p><p>72 We find that the simple corpus merging strategy leads to dramatic decrease in accuracy, due to the different and incompatible annotation guidelines. [sent-188, score-0.439]
</p><p>73 The baseline annotation transformation method leads to a classifier with accuracy increment comparable to that of the annotation adaptation strategy, while consuming only one third of the decoding time. [sent-189, score-1.563]
</p><p>74 3  Iterative Training with Predict-Self Reestimation  We adopt the iterative training strategy to the baseline annotation transformation model. [sent-191, score-1.148]
</p><p>75 The CTB developing set is used to determine the best training iteration for annotation transformation from SPD to CTB. [sent-192, score-1.008]
</p><p>76 of predict-self  filtration and  Training iterations  Figure 3: Learning curve of iterative training with predict-self reestimation for annotation transformation. [sent-195, score-1.013]
</p><p>77 The performance of the baseline annotation transformation model is naturally included in the curve (located at iteration 1). [sent-197, score-1.027]
</p><p>78 The curve shows that the performance of the classifier trained on the merged corpus consistently improves from iteration 2 to iteration 5. [sent-198, score-0.379]
</p><p>79 Experimental results of predict-self filtration and predict-self reestimation are shown in Figure 2. [sent-199, score-0.394]
</p><p>80 The curve shows the performance of the predict-self reestimation according to a series of weight param-  eters, ranging from 0 to 1with step 0. [sent-200, score-0.334]
</p><p>81 The point at λ = 0 shows the performance of the baseline annotation transformation strategy. [sent-202, score-0.898]
</p><p>82 We find that predict-self filtration brings slight improvement over the baseline, and predictself reestimation outperforms the filtration strategy when λ falls in a proper range. [sent-204, score-0.667]
</p><p>83 17 Table 6: The performance of the iterative annotation  transformation with predict-self reestimation compared with annotation adaptation. [sent-227, score-1.709]
</p><p>84 The best Fmeasure improvement achieved over the annotation transformation baseline is 0. [sent-231, score-0.898]
</p><p>85 Figure 3 shows the performance curve of iterative annotation transformation with predict-self reestimation. [sent-233, score-1.153]
</p><p>86 We find that the predict-self reestimation brings improvement to the iterative training at each iteration. [sent-234, score-0.511]
</p><p>87 Compared to annotation adaptation, the optimized annotation transformation strategy leads to a classifier with significantly higher accuracy and several times faster processing. [sent-237, score-1.514]
</p><p>88 This work aim to find another way to improve Chinese word segmentation, which focuses on the collection of more training data instead of mak3The predict-self reestimation ratio λ is fixed after the first training iteration for efficiency. [sent-241, score-0.349]
</p><p>89 Considering the fact that today some corpora for word segmentation are really large (usually tens of thousands of sentences), it is necessary to obtain the latest CTB and investigate whether and how much does annotation transformation bring improvement to a much higher baseline. [sent-244, score-1.155]
</p><p>90 6  Conclusion and Future Works  In this paper, we first describe an annotation transformation algorithm to automatically transform a human-annotated corpus from one annotation guideline to another. [sent-248, score-1.483]
</p><p>91 Then we propose two optimization strategies, iterative training and predict-self reestimation, to further improve the accuracy of annotation guideline transformation. [sent-249, score-0.788]
</p><p>92 On Chinese word segmentation, the optimized annotation transformation strategy leads to classifiers with obviously better performance and several times faster processing on the same datasets, compared to annotation adaptation. [sent-250, score-1.47]
</p><p>93 As future works, we will investigate the acceleration of the iterative training and the weight parameter tuning, and extend the optimized annotation transformation strategy to joint Chinese word segmentation and POS tagging, parsing and other NLP tasks. [sent-253, score-1.379]
</p><p>94 Automatic annotation of the penn treebank with lfg f-structure information. [sent-263, score-0.411]
</p><p>95 A cascaded linear model for joint chinese word segmentation and part-of-speech tagging. [sent-300, score-0.363]
</p><p>96 Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging–a case study. [sent-304, score-0.673]
</p><p>97 An error-driven word-character hybrid model for joint chinese word segmentation and pos tagging. [sent-312, score-0.301]
</p><p>98 A stacked sub-word model for joint chinese word segmentation and part-of-speech tagging. [sent-341, score-0.324]
</p><p>99 The penn chinese treebank: Phrase structure annotation of a large corpus. [sent-353, score-0.474]
</p><p>100 Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition. [sent-369, score-0.484]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('transformation', 0.562), ('annotation', 0.336), ('reestimation', 0.277), ('parallelly', 0.22), ('guideline', 0.204), ('segmentation', 0.199), ('iterative', 0.198), ('ctb', 0.16), ('adaptation', 0.138), ('filtration', 0.117), ('pd', 0.114), ('classifier', 0.113), ('transformed', 0.113), ('chinese', 0.102), ('ms', 0.097), ('classifiers', 0.095), ('cts', 0.087), ('jiang', 0.087), ('source', 0.082), ('ct', 0.073), ('iteration', 0.072), ('gen', 0.072), ('predictself', 0.068), ('transtrain', 0.068), ('perceptron', 0.063), ('character', 0.063), ('cascaded', 0.062), ('corpora', 0.058), ('guiding', 0.057), ('curve', 0.057), ('strategy', 0.052), ('cct', 0.051), ('spd', 0.051), ('tce', 0.051), ('tnnota', 0.051), ('xue', 0.047), ('daum', 0.046), ('annotated', 0.045), ('clark', 0.043), ('merged', 0.042), ('reversed', 0.04), ('kruengkrai', 0.04), ('qun', 0.039), ('treebank', 0.039), ('guidelines', 0.038), ('developing', 0.038), ('strategies', 0.037), ('wenbin', 0.037), ('annotations', 0.036), ('brings', 0.036), ('penn', 0.036), ('target', 0.036), ('hewlett', 0.034), ('mcts', 0.034), ('mmt', 0.034), ('mochihashi', 0.034), ('nese', 0.034), ('transannotate', 0.034), ('underling', 0.034), ('cs', 0.033), ('methodology', 0.032), ('sighan', 0.032), ('optimized', 0.032), ('segmented', 0.032), ('transformations', 0.031), ('works', 0.03), ('zhu', 0.03), ('gradually', 0.029), ('argmaxy', 0.029), ('cst', 0.029), ('faster', 0.029), ('iterations', 0.028), ('leads', 0.028), ('zi', 0.027), ('complicated', 0.027), ('adopting', 0.026), ('fmeasure', 0.026), ('cahill', 0.026), ('collins', 0.026), ('decode', 0.026), ('accuracy', 0.026), ('unsupervised', 0.026), ('zhang', 0.025), ('increment', 0.024), ('segmenter', 0.024), ('mation', 0.024), ('hockenmaier', 0.024), ('ra', 0.024), ('optimization', 0.024), ('back', 0.024), ('tagging', 0.023), ('raw', 0.023), ('corpus', 0.023), ('enhanced', 0.023), ('kiyotaka', 0.023), ('uchimoto', 0.023), ('stacked', 0.023), ('yajuan', 0.023), ('lines', 0.023), ('transform', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="68-tfidf-1" href="./emnlp-2012-Iterative_Annotation_Transformation_with_Predict-Self_Reestimation_for_Chinese_Word_Segmentation.html">68 emnlp-2012-Iterative Annotation Transformation with Predict-Self Reestimation for Chinese Word Segmentation</a></p>
<p>Author: Wenbin Jiang ; Fandong Meng ; Qun Liu ; Yajuan Lu</p><p>Abstract: In this paper we first describe the technology of automatic annotation transformation, which is based on the annotation adaptation algorithm (Jiang et al., 2009). It can automatically transform a human-annotated corpus from one annotation guideline to another. We then propose two optimization strategies, iterative training and predict-selfreestimation, to further improve the accuracy of annotation guideline transformation. Experiments on Chinese word segmentation show that, the iterative training strategy together with predictself reestimation brings significant improvement over the simple annotation transformation baseline, and leads to classifiers with significantly higher accuracy and several times faster processing than annotation adaptation does. On the Penn Chinese Treebank 5.0, , it achieves an F-measure of 98.43%, significantly outperforms previous works although using a single classifier with only local features.</p><p>2 0.24559486 <a title="68-tfidf-2" href="./emnlp-2012-Transforming_Trees_to_Improve_Syntactic_Convergence.html">127 emnlp-2012-Transforming Trees to Improve Syntactic Convergence</a></p>
<p>Author: David Burkett ; Dan Klein</p><p>Abstract: We describe a transformation-based learning method for learning a sequence of monolingual tree transformations that improve the agreement between constituent trees and word alignments in bilingual corpora. Using the manually annotated English Chinese Translation Treebank, we show how our method automatically discovers transformations that accommodate differences in English and Chinese syntax. Furthermore, when transformations are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic MT system, the transformed trees achieve a 0.9 BLEU improvement over baseline trees.</p><p>3 0.19557156 <a title="68-tfidf-3" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>Author: Xian Qian ; Yang Liu</p><p>Abstract: In this paper, we propose a novel decoding algorithm for discriminative joint Chinese word segmentation, part-of-speech (POS) tagging, and parsing. Previous work often used a pipeline method Chinese word segmentation followed by POS tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components. In our approach, we train the three individual models separately during training, and incorporate them together in a unified framework during decoding. We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features. As far as we know, this is the first work on joint Chinese word segmentation, POS tagging and parsing. Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system. –</p><p>4 0.16263509 <a title="68-tfidf-4" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>Author: Zhongguo Li ; Guodong Zhou</p><p>Abstract: Most previous approaches to syntactic parsing of Chinese rely on a preprocessing step of word segmentation, thereby assuming there was a clearly defined boundary between morphology and syntax in Chinese. We show how this assumption can fail badly, leading to many out-of-vocabulary words and incompatible annotations. Hence in practice the strict separation of morphology and syntax in the Chinese language proves to be untenable. We present a unified dependency parsing approach for Chinese which takes unsegmented sentences as input and outputs both morphological and syntactic structures with a single model and algorithm. By removing the intermediate word segmentation, the unified parser no longer needs separate notions for words and phrases. Evaluation proves the effectiveness of the unified model and algorithm in parsing structures of words, phrases and sen- tences simultaneously. 1</p><p>5 0.10880718 <a title="68-tfidf-5" href="./emnlp-2012-Part-of-Speech_Tagging_for_Chinese-English_Mixed_Texts_with_Dynamic_Features.html">106 emnlp-2012-Part-of-Speech Tagging for Chinese-English Mixed Texts with Dynamic Features</a></p>
<p>Author: Jiayi Zhao ; Xipeng Qiu ; Shu Zhang ; Feng Ji ; Xuanjing Huang</p><p>Abstract: In modern Chinese articles or conversations, it is very popular to involve a few English words, especially in emails and Internet literature. Therefore, it becomes an important and challenging topic to analyze Chinese-English mixed texts. The underlying problem is how to tag part-of-speech (POS) for the English words involved. Due to the lack of specially annotated corpus, most of the English words are tagged as the oversimplified type, “foreign words”. In this paper, we present a method using dynamic features to tag POS of mixed texts. Experiments show that our method achieves higher performance than traditional sequence labeling methods. Meanwhile, our method also boosts the performance of POS tagging for pure Chinese texts.</p><p>6 0.082073763 <a title="68-tfidf-6" href="./emnlp-2012-Training_Factored_PCFGs_with_Expectation_Propagation.html">126 emnlp-2012-Training Factored PCFGs with Expectation Propagation</a></p>
<p>7 0.069994457 <a title="68-tfidf-7" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>8 0.069344461 <a title="68-tfidf-8" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>9 0.067803107 <a title="68-tfidf-9" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>10 0.060621399 <a title="68-tfidf-10" href="./emnlp-2012-Name_Phylogeny%3A_A_Generative_Model_of_String_Variation.html">96 emnlp-2012-Name Phylogeny: A Generative Model of String Variation</a></p>
<p>11 0.060352221 <a title="68-tfidf-11" href="./emnlp-2012-Domain_Adaptation_for_Coreference_Resolution%3A_An_Adaptive_Ensemble_Approach.html">36 emnlp-2012-Domain Adaptation for Coreference Resolution: An Adaptive Ensemble Approach</a></p>
<p>12 0.054186158 <a title="68-tfidf-12" href="./emnlp-2012-A_Discriminative_Model_for_Query_Spelling_Correction_with_Latent_Structural_SVM.html">5 emnlp-2012-A Discriminative Model for Query Spelling Correction with Latent Structural SVM</a></p>
<p>13 0.050028816 <a title="68-tfidf-13" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<p>14 0.049981434 <a title="68-tfidf-14" href="./emnlp-2012-Exploiting_Chunk-level_Features_to_Improve_Phrase_Chunking.html">45 emnlp-2012-Exploiting Chunk-level Features to Improve Phrase Chunking</a></p>
<p>15 0.048763402 <a title="68-tfidf-15" href="./emnlp-2012-Active_Learning_for_Imbalanced_Sentiment_Classification.html">15 emnlp-2012-Active Learning for Imbalanced Sentiment Classification</a></p>
<p>16 0.048725206 <a title="68-tfidf-16" href="./emnlp-2012-A_Novel_Discriminative_Framework_for_Sentence-Level_Discourse_Analysis.html">7 emnlp-2012-A Novel Discriminative Framework for Sentence-Level Discourse Analysis</a></p>
<p>17 0.044255968 <a title="68-tfidf-17" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>18 0.042632662 <a title="68-tfidf-18" href="./emnlp-2012-Locally_Training_the_Log-Linear_Model_for_SMT.html">86 emnlp-2012-Locally Training the Log-Linear Model for SMT</a></p>
<p>19 0.039565895 <a title="68-tfidf-19" href="./emnlp-2012-Multi-Domain_Learning%3A_When_Do_Domains_Matter%3F.html">92 emnlp-2012-Multi-Domain Learning: When Do Domains Matter?</a></p>
<p>20 0.039111502 <a title="68-tfidf-20" href="./emnlp-2012-Do_Neighbours_Help%3F_An_Exploration_of_Graph-based_Algorithms_for_Cross-domain_Sentiment_Classification.html">34 emnlp-2012-Do Neighbours Help? An Exploration of Graph-based Algorithms for Cross-domain Sentiment Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.186), (1, -0.116), (2, 0.109), (3, -0.072), (4, 0.079), (5, -0.036), (6, -0.005), (7, -0.105), (8, -0.088), (9, -0.217), (10, -0.132), (11, 0.006), (12, -0.078), (13, 0.0), (14, 0.081), (15, 0.261), (16, 0.156), (17, 0.143), (18, -0.135), (19, -0.124), (20, -0.113), (21, 0.057), (22, 0.081), (23, 0.057), (24, 0.067), (25, -0.152), (26, -0.101), (27, -0.005), (28, -0.127), (29, 0.169), (30, 0.052), (31, -0.046), (32, -0.182), (33, 0.064), (34, -0.004), (35, -0.006), (36, 0.146), (37, 0.025), (38, 0.024), (39, 0.127), (40, 0.175), (41, -0.1), (42, 0.118), (43, 0.006), (44, 0.199), (45, 0.026), (46, -0.089), (47, -0.003), (48, -0.042), (49, -0.087)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98297238 <a title="68-lsi-1" href="./emnlp-2012-Iterative_Annotation_Transformation_with_Predict-Self_Reestimation_for_Chinese_Word_Segmentation.html">68 emnlp-2012-Iterative Annotation Transformation with Predict-Self Reestimation for Chinese Word Segmentation</a></p>
<p>Author: Wenbin Jiang ; Fandong Meng ; Qun Liu ; Yajuan Lu</p><p>Abstract: In this paper we first describe the technology of automatic annotation transformation, which is based on the annotation adaptation algorithm (Jiang et al., 2009). It can automatically transform a human-annotated corpus from one annotation guideline to another. We then propose two optimization strategies, iterative training and predict-selfreestimation, to further improve the accuracy of annotation guideline transformation. Experiments on Chinese word segmentation show that, the iterative training strategy together with predictself reestimation brings significant improvement over the simple annotation transformation baseline, and leads to classifiers with significantly higher accuracy and several times faster processing than annotation adaptation does. On the Penn Chinese Treebank 5.0, , it achieves an F-measure of 98.43%, significantly outperforms previous works although using a single classifier with only local features.</p><p>2 0.68787223 <a title="68-lsi-2" href="./emnlp-2012-Transforming_Trees_to_Improve_Syntactic_Convergence.html">127 emnlp-2012-Transforming Trees to Improve Syntactic Convergence</a></p>
<p>Author: David Burkett ; Dan Klein</p><p>Abstract: We describe a transformation-based learning method for learning a sequence of monolingual tree transformations that improve the agreement between constituent trees and word alignments in bilingual corpora. Using the manually annotated English Chinese Translation Treebank, we show how our method automatically discovers transformations that accommodate differences in English and Chinese syntax. Furthermore, when transformations are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic MT system, the transformed trees achieve a 0.9 BLEU improvement over baseline trees.</p><p>3 0.43904215 <a title="68-lsi-3" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>Author: Zhongguo Li ; Guodong Zhou</p><p>Abstract: Most previous approaches to syntactic parsing of Chinese rely on a preprocessing step of word segmentation, thereby assuming there was a clearly defined boundary between morphology and syntax in Chinese. We show how this assumption can fail badly, leading to many out-of-vocabulary words and incompatible annotations. Hence in practice the strict separation of morphology and syntax in the Chinese language proves to be untenable. We present a unified dependency parsing approach for Chinese which takes unsegmented sentences as input and outputs both morphological and syntactic structures with a single model and algorithm. By removing the intermediate word segmentation, the unified parser no longer needs separate notions for words and phrases. Evaluation proves the effectiveness of the unified model and algorithm in parsing structures of words, phrases and sen- tences simultaneously. 1</p><p>4 0.41335571 <a title="68-lsi-4" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>Author: Xian Qian ; Yang Liu</p><p>Abstract: In this paper, we propose a novel decoding algorithm for discriminative joint Chinese word segmentation, part-of-speech (POS) tagging, and parsing. Previous work often used a pipeline method Chinese word segmentation followed by POS tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components. In our approach, we train the three individual models separately during training, and incorporate them together in a unified framework during decoding. We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features. As far as we know, this is the first work on joint Chinese word segmentation, POS tagging and parsing. Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system. –</p><p>5 0.32105196 <a title="68-lsi-5" href="./emnlp-2012-Part-of-Speech_Tagging_for_Chinese-English_Mixed_Texts_with_Dynamic_Features.html">106 emnlp-2012-Part-of-Speech Tagging for Chinese-English Mixed Texts with Dynamic Features</a></p>
<p>Author: Jiayi Zhao ; Xipeng Qiu ; Shu Zhang ; Feng Ji ; Xuanjing Huang</p><p>Abstract: In modern Chinese articles or conversations, it is very popular to involve a few English words, especially in emails and Internet literature. Therefore, it becomes an important and challenging topic to analyze Chinese-English mixed texts. The underlying problem is how to tag part-of-speech (POS) for the English words involved. Due to the lack of specially annotated corpus, most of the English words are tagged as the oversimplified type, “foreign words”. In this paper, we present a method using dynamic features to tag POS of mixed texts. Experiments show that our method achieves higher performance than traditional sequence labeling methods. Meanwhile, our method also boosts the performance of POS tagging for pure Chinese texts.</p><p>6 0.30257311 <a title="68-lsi-6" href="./emnlp-2012-Exploiting_Chunk-level_Features_to_Improve_Phrase_Chunking.html">45 emnlp-2012-Exploiting Chunk-level Features to Improve Phrase Chunking</a></p>
<p>7 0.28943953 <a title="68-lsi-7" href="./emnlp-2012-Training_Factored_PCFGs_with_Expectation_Propagation.html">126 emnlp-2012-Training Factored PCFGs with Expectation Propagation</a></p>
<p>8 0.2688843 <a title="68-lsi-8" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>9 0.25111419 <a title="68-lsi-9" href="./emnlp-2012-Employing_Compositional_Semantics_and_Discourse_Consistency_in_Chinese_Event_Extraction.html">38 emnlp-2012-Employing Compositional Semantics and Discourse Consistency in Chinese Event Extraction</a></p>
<p>10 0.23433098 <a title="68-lsi-10" href="./emnlp-2012-Name_Phylogeny%3A_A_Generative_Model_of_String_Variation.html">96 emnlp-2012-Name Phylogeny: A Generative Model of String Variation</a></p>
<p>11 0.2127396 <a title="68-lsi-11" href="./emnlp-2012-Translation_Model_Based_Cross-Lingual_Language_Model_Adaptation%3A_from_Word_Models_to_Phrase_Models.html">128 emnlp-2012-Translation Model Based Cross-Lingual Language Model Adaptation: from Word Models to Phrase Models</a></p>
<p>12 0.21177271 <a title="68-lsi-12" href="./emnlp-2012-Domain_Adaptation_for_Coreference_Resolution%3A_An_Adaptive_Ensemble_Approach.html">36 emnlp-2012-Domain Adaptation for Coreference Resolution: An Adaptive Ensemble Approach</a></p>
<p>13 0.20515271 <a title="68-lsi-13" href="./emnlp-2012-Multi-Domain_Learning%3A_When_Do_Domains_Matter%3F.html">92 emnlp-2012-Multi-Domain Learning: When Do Domains Matter?</a></p>
<p>14 0.18787456 <a title="68-lsi-14" href="./emnlp-2012-Source_Language_Adaptation_for_Resource-Poor_Machine_Translation.html">118 emnlp-2012-Source Language Adaptation for Resource-Poor Machine Translation</a></p>
<p>15 0.18685873 <a title="68-lsi-15" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>16 0.18375342 <a title="68-lsi-16" href="./emnlp-2012-A_Statistical_Relational_Learning_Approach_to_Identifying_Evidence_Based_Medicine_Categories.html">10 emnlp-2012-A Statistical Relational Learning Approach to Identifying Evidence Based Medicine Categories</a></p>
<p>17 0.18358597 <a title="68-lsi-17" href="./emnlp-2012-Active_Learning_for_Imbalanced_Sentiment_Classification.html">15 emnlp-2012-Active Learning for Imbalanced Sentiment Classification</a></p>
<p>18 0.17450115 <a title="68-lsi-18" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<p>19 0.16696553 <a title="68-lsi-19" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>20 0.15825543 <a title="68-lsi-20" href="./emnlp-2012-Probabilistic_Finite_State_Machines_for_Regression-based_MT_Evaluation.html">108 emnlp-2012-Probabilistic Finite State Machines for Regression-based MT Evaluation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.033), (34, 0.066), (41, 0.015), (60, 0.614), (63, 0.019), (64, 0.019), (65, 0.02), (70, 0.02), (74, 0.034), (76, 0.023), (82, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99133247 <a title="68-lda-1" href="./emnlp-2012-Iterative_Annotation_Transformation_with_Predict-Self_Reestimation_for_Chinese_Word_Segmentation.html">68 emnlp-2012-Iterative Annotation Transformation with Predict-Self Reestimation for Chinese Word Segmentation</a></p>
<p>Author: Wenbin Jiang ; Fandong Meng ; Qun Liu ; Yajuan Lu</p><p>Abstract: In this paper we first describe the technology of automatic annotation transformation, which is based on the annotation adaptation algorithm (Jiang et al., 2009). It can automatically transform a human-annotated corpus from one annotation guideline to another. We then propose two optimization strategies, iterative training and predict-selfreestimation, to further improve the accuracy of annotation guideline transformation. Experiments on Chinese word segmentation show that, the iterative training strategy together with predictself reestimation brings significant improvement over the simple annotation transformation baseline, and leads to classifiers with significantly higher accuracy and several times faster processing than annotation adaptation does. On the Penn Chinese Treebank 5.0, , it achieves an F-measure of 98.43%, significantly outperforms previous works although using a single classifier with only local features.</p><p>2 0.98707336 <a title="68-lda-2" href="./emnlp-2012-Generalizing_Sub-sentential_Paraphrase_Acquisition_across_Original_Signal_Type_of_Text_Pairs.html">58 emnlp-2012-Generalizing Sub-sentential Paraphrase Acquisition across Original Signal Type of Text Pairs</a></p>
<p>Author: Aurelien Max ; Houda Bouamor ; Anne Vilnat</p><p>Abstract: This paper describes a study on the impact of the original signal (text, speech, visual scene, event) of a text pair on the task of both manual and automatic sub-sentential paraphrase acquisition. A corpus of 2,500 annotated sentences in English and French is described, and performance on this corpus is reported for an efficient system combination exploiting a large set of features for paraphrase recognition. A detailed quantified typology of subsentential paraphrases found in our corpus types is given.</p><p>3 0.98579234 <a title="68-lda-3" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>Author: Avirup Sil ; Ernest Cronin ; Penghai Nie ; Yinfei Yang ; Ana-Maria Popescu ; Alexander Yates</p><p>Abstract: Existing techniques for disambiguating named entities in text mostly focus on Wikipedia as a target catalog of entities. Yet for many types of entities, such as restaurants and cult movies, relational databases exist that contain far more extensive information than Wikipedia. This paper introduces a new task, called Open-Database Named-Entity Disambiguation (Open-DB NED), in which a system must be able to resolve named entities to symbols in an arbitrary database, without requiring labeled data for each new database. We introduce two techniques for Open-DB NED, one based on distant supervision and the other based on domain adaptation. In experiments on two domains, one with poor coverage by Wikipedia and the other with near-perfect coverage, our Open-DB NED strategies outperform a state-of-the-art Wikipedia NED system by over 25% in accuracy.</p><p>4 0.98245293 <a title="68-lda-4" href="./emnlp-2012-Entity_based_QA_Retrieval.html">41 emnlp-2012-Entity based QA Retrieval</a></p>
<p>Author: Amit Singh</p><p>Abstract: Bridging the lexical gap between the user’s question and the question-answer pairs in the Q&A; archives has been a major challenge for Q&A; retrieval. State-of-the-art approaches address this issue by implicitly expanding the queries with additional words using statistical translation models. While useful, the effectiveness of these models is highly dependant on the availability of quality corpus in the absence of which they are troubled by noise issues. Moreover these models perform word based expansion in a context agnostic manner resulting in translation that might be mixed and fairly general. This results in degraded retrieval performance. In this work we address the above issues by extending the lexical word based translation model to incorporate semantic concepts (entities). We explore strategies to learn the translation probabilities between words and the concepts using the Q&A; archives and a popular entity catalog. Experiments conducted on a large scale real data show that the proposed techniques are promising.</p><p>5 0.95683634 <a title="68-lda-5" href="./emnlp-2012-Grounded_Models_of_Semantic_Representation.html">61 emnlp-2012-Grounded Models of Semantic Representation</a></p>
<p>Author: Carina Silberer ; Mirella Lapata</p><p>Abstract: A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two.</p><p>6 0.93337953 <a title="68-lda-6" href="./emnlp-2012-Exploring_Adaptor_Grammars_for_Native_Language_Identification.html">48 emnlp-2012-Exploring Adaptor Grammars for Native Language Identification</a></p>
<p>7 0.87061626 <a title="68-lda-7" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>8 0.84578979 <a title="68-lda-8" href="./emnlp-2012-Enlarging_Paraphrase_Collections_through_Generalization_and_Instantiation.html">39 emnlp-2012-Enlarging Paraphrase Collections through Generalization and Instantiation</a></p>
<p>9 0.83030206 <a title="68-lda-9" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>10 0.82282519 <a title="68-lda-10" href="./emnlp-2012-Wiki-ly_Supervised_Part-of-Speech_Tagging.html">138 emnlp-2012-Wiki-ly Supervised Part-of-Speech Tagging</a></p>
<p>11 0.81804091 <a title="68-lda-11" href="./emnlp-2012-Why_Question_Answering_using_Sentiment_Analysis_and_Word_Classes.html">137 emnlp-2012-Why Question Answering using Sentiment Analysis and Word Classes</a></p>
<p>12 0.8177188 <a title="68-lda-12" href="./emnlp-2012-Multi-Domain_Learning%3A_When_Do_Domains_Matter%3F.html">92 emnlp-2012-Multi-Domain Learning: When Do Domains Matter?</a></p>
<p>13 0.81153327 <a title="68-lda-13" href="./emnlp-2012-An_Entity-Topic_Model_for_Entity_Linking.html">19 emnlp-2012-An Entity-Topic Model for Entity Linking</a></p>
<p>14 0.80731565 <a title="68-lda-14" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>15 0.79103589 <a title="68-lda-15" href="./emnlp-2012-Using_Discourse_Information_for_Paraphrase_Extraction.html">135 emnlp-2012-Using Discourse Information for Paraphrase Extraction</a></p>
<p>16 0.78652775 <a title="68-lda-16" href="./emnlp-2012-Probabilistic_Finite_State_Machines_for_Regression-based_MT_Evaluation.html">108 emnlp-2012-Probabilistic Finite State Machines for Regression-based MT Evaluation</a></p>
<p>17 0.78465044 <a title="68-lda-17" href="./emnlp-2012-A_Unified_Approach_to_Transliteration-based_Text_Input_with_Online_Spelling_Correction.html">13 emnlp-2012-A Unified Approach to Transliteration-based Text Input with Online Spelling Correction</a></p>
<p>18 0.7836808 <a title="68-lda-18" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>19 0.78297698 <a title="68-lda-19" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>20 0.7760213 <a title="68-lda-20" href="./emnlp-2012-Translation_Model_Based_Cross-Lingual_Language_Model_Adaptation%3A_from_Word_Models_to_Phrase_Models.html">128 emnlp-2012-Translation Model Based Cross-Lingual Language Model Adaptation: from Word Models to Phrase Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
