<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>113 emnlp-2012-Resolving This-issue Anaphora</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-113" href="#">emnlp2012-113</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>113 emnlp-2012-Resolving This-issue Anaphora</h1>
<br/><p>Source: <a title="emnlp-2012-113-pdf" href="http://aclweb.org/anthology//D/D12/D12-1115.pdf">pdf</a></p><p>Author: Varada Kolhatkar ; Graeme Hirst</p><p>Abstract: We annotate and resolve a particular case of abstract anaphora, namely, thisissue anaphora. We propose a candidate ranking model for this-issue anaphora resolution that explores different issuespecific and general abstract-anaphora features. The model is not restricted to nominal or verbal antecedents; rather, it is able to identify antecedents that are arbitrary spans of text. Our results show that (a) the model outperforms the strong adjacent-sentence baseline; (b) general abstract-anaphora features, as distinguished from issue-specific features, play a crucial role in this-issue anaphora resolution, suggesting that our approach can be generalized for other NPs such as this problem and this debate; and (c) it is possible to reduce the search space in order to improve performance.</p><p>Reference: <a title="emnlp-2012-113-reference" href="../emnlp2012_reference/emnlp-2012-Resolving_This-issue_Anaphora_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We propose a candidate ranking model for this-issue anaphora resolution that explores different issuespecific and general abstract-anaphora features. [sent-4, score-0.873]
</p><p>2 The model is not restricted to nominal or verbal antecedents; rather, it is able to identify antecedents that are arbitrary spans of text. [sent-5, score-0.522]
</p><p>3 The anaphor That in (1) refers to the proposition in the previous utterance, whereas the anaphor this issue in (2) refers to a clause from the previous text. [sent-24, score-0.833]
</p><p>4 Second, the antecedents do not always have precisely defined boundaries. [sent-26, score-0.444]
</p><p>5 The actual referent in (2), the issue to be clarified,  is whether oral carvedilol is more effective than oral metoprolol in the prevention of AF after on-pump CABG or not, a variant of the antecedent text. [sent-29, score-0.526]
</p><p>6 observed that sortal anaphors are prevalent in the biomedical literature. [sent-45, score-0.234]
</p><p>7 This shows that abstract anaphora resolution is an important component of general anaphora resolution in the biomedical domain. [sent-52, score-1.382]
</p><p>8 However, automatic resolution ofthis type ofanaphora has not attracted much attention and the previous work for this task is limited. [sent-53, score-0.218]
</p><p>9 The present work is a step towards resolving abstract anaphora in written text. [sent-54, score-0.569]
</p><p>10 In particular, we choose the interesting abstract concept issue and demonstrate the complexities of resolving this-issue anaphora manually as well as automatically in the  Medline domain. [sent-55, score-0.662]
</p><p>11 We present our algorithm, results, and error analysis for this-issue anaphora resolution. [sent-56, score-0.49]
</p><p>12 There are 13,489 issue anaphora instances in the New York Times corpus and 1,116 instances in 65,000 Medline abstracts. [sent-59, score-0.681]
</p><p>13 But CL research has mostly focused on nominal anaphora resolution (e. [sent-70, score-0.748]
</p><p>14 First, nominal anaphora is the most frequently occurring anaphora in most domains, and second, there is a substantial amount of annotated data available for this kind of anaphora. [sent-73, score-1.086]
</p><p>15 Besides pronominal anaphora, some work has been done on complement anaphora (Modjeska, 2003) (e. [sent-74, score-0.526]
</p><p>16 There is also some research on resolving sortal anaphora in the medical domain using domain knowledge (Casta˜ no et al. [sent-77, score-0.653]
</p><p>17 By contrast, the area of abstract object anaphora remains relatively unexplored mainly because the  standard anaphora resolution features such as agreement and apposition cannot be applied to abstract anaphora resolution. [sent-80, score-1.65]
</p><p>18 He divided discourse abstract anaphora into three broad categories: event anaphora, proposition anaphora, and fact anaphora, and discussed how abstract entities can be resolved using discourse representation theory. [sent-82, score-0.62]
</p><p>19 (201 1) focused on a subset of event anaphora and resolved event coreference chains in terms of the representative verbs of the events from the OntoNotes corpus. [sent-84, score-0.49]
</p><p>20 But this-issue antecedents cannot usually be represented by a verb. [sent-88, score-0.444]
</p><p>21 There are also some prominent approaches to abstract anaphora resolution in the spoken dialogue domain (Eckert and Strube, 2000; Byron, 2004; M ¨uller, 2008). [sent-90, score-0.67]
</p><p>22 In addition to research on resolution, there is also some work on effective annotation of abstract anaphora (Strube and M ¨uller, 2003; Botley, 2006; Poesio and Artstein, 2008; Dipper and Zinsmeister, 2011). [sent-92, score-0.49]
</p><p>23 However, to the best of our knowledge, there is currently no English corpus annotated for issue anaphora antecedents. [sent-93, score-0.611]
</p><p>24 The annotator’s task was to mark arbitrary text segments as antecedents (without concern for their linguistic  types). [sent-98, score-0.444]
</p><p>25 To make the task tractable, we assumed that an antecedent does not span multiple sentences but lies in a single sentence (since we are dealing with singular this-issue anaphors) and that it is a continuous span of text. [sent-99, score-0.291]
</p><p>26 Bold segments denote the marked antecedents for the corresponding anaphor ids. [sent-102, score-0.831]
</p><p>27 In our context, unitizing means marking the spans of the text that serve as the antecedent for the given anaphors within the given text. [sent-107, score-0.441]
</p><p>28 The annotators  mark the antecedents corresponding to each anaphor  in their respective copies ofthe text, as shown in Figure 1. [sent-116, score-0.856]
</p><p>29 In examples, the antecedent type is in bold and the marked antecedent is in italics. [sent-135, score-0.672]
</p><p>30 Annotator  1  by identi-  in their copies  1 has not marked any an-  tecedent for the anaphor with id  = 1, while  annotator  2 has marked r21 for the same anaphor. [sent-138, score-0.532]
</p><p>31 Both anno-  tators have marked exactly the same antecedent for the anaphor with id = 4. [sent-139, score-0.678]
</p><p>32 5% of the antecedents were in the current or previous sentence and 99. [sent-153, score-0.444]
</p><p>33 Only one antecedent was found more than two sentences back and it was six sentences back. [sent-155, score-0.291]
</p><p>34 One instance was a cataphor, but the antecedent occurred in the same sentence as the anaphor. [sent-156, score-0.291]
</p><p>35 The distribution of the different linguistic forms that an antecedent of this-issue can take in our data set is shown in Table 1. [sent-158, score-0.291]
</p><p>36 The majority of antecedents are clauses or whole sentences. [sent-159, score-0.444]
</p><p>37 A number of antecedents are noun phrases, but these are gener-  ally nominalizations that refer to abstract concepts (e. [sent-160, score-0.444]
</p><p>38 Some antecedents are not even welldefined syntactic but are combinations of several well-defined constituents. [sent-163, score-0.474]
</p><p>39 2% of the antecedents are of this type, suggesting that it is not sufficient to restrict the antecedent search space to well-defined syntactic constituents. [sent-166, score-0.765]
</p><p>40 6 In our data, we did not find anaphoric chains for any of the this-issue anaphor instances, which indicates that the antecedents of this-issue anaphors are  constituents5  5We refer to every syntactic constituent identified by the parser as a well-defined syntactic constituent. [sent-167, score-0.994]
</p><p>41 6Indeed, many of mixed type antecedents (nearly threequarters of them) are the result of parser attachment errors, but many are not. [sent-168, score-0.541]
</p><p>42 1  Resolution Algorithm Candidate Extraction  For correct resolution, the set ofextracted candidates must contain the correct antecedent in the first place. [sent-175, score-0.321]
</p><p>43 The problem of candidate extraction is non-trivial in abstract anaphora resolution because the antecedents are of many different types of syntactic constituents such as clauses, sentences, and nominalizations. [sent-176, score-1.397]
</p><p>44 Drawing on our observation that the mixed type antecedents are generally a combination of different well-defined syntactic constituents, we extract the set of candidate antecedents as follows. [sent-177, score-1.19]
</p><p>45 First, we create a set of candidate sentences which contains the sentence containing the this-issue anaphor and the two preceding sentences. [sent-178, score-0.51]
</p><p>46 Initially, the set of candidate constituents contains a list of well-defined syntactic constituents. [sent-180, score-0.283]
</p><p>47 For  example, in (4), the set of well-defined eligible candidate constituents is {NP, NP1} and so NP1 PP1 is a imdaixteed c type ucaenndtsid iast{e N. [sent-186, score-0.291]
</p><p>48 (4)  NP NP1  PP1  PP2  The set of candidate constituents is updated with the extracted mixed type constituents. [sent-187, score-0.35]
</p><p>49 Extracting mixed type candidate constituents not only deals with mixed type instances as shown in Table 1, but as a side effect it also corrects some attachment errors made by the parser. [sent-188, score-0.496]
</p><p>50 The feature IVERB checks whether the governing verb of the candidate is an issue verb (e. [sent-201, score-0.318]
</p><p>51 , speculate, hypothesize, argue, debate), whereas IHEAD checks whether the candidate head in the dependency tree is an issue word (e. [sent-203, score-0.297]
</p><p>52 The EL feature is borrowed from M ¨uller (2008) and encodes the embedding level of the candidate within the candidate sen-  tence. [sent-210, score-0.39]
</p><p>53 , 1993; Poesio and Modjeska, 2002) that the antecedents of thisNP anaphors are not the center of the previous utterance. [sent-212, score-0.552]
</p><p>54 The general abstract-anaphora features in the SR feature class capture the semantic role ofthe candidate in the candidate sentence. [sent-213, score-0.35]
</p><p>55 , however, but, yet) ISCAUSE 1iff the candidate starts with a causal subordinating conjunction (e. [sent-239, score-0.215]
</p><p>56 , because, as, since) ISCOND 1iff the candidate starts with a conditional subordinating conjunction (e. [sent-241, score-0.215]
</p><p>57 3 Candidate Ranking Model Given an anaphor ai and a set of candidate antecedents C = {C1,C2, . [sent-247, score-0.954]
</p><p>58 ,Ck}, the problem of anaphora rtseso Clu =tio n{C is to choose} t,h teh bees ptr coablnedmida otef antecedent for ai. [sent-250, score-0.781]
</p><p>59 If the anaphor  is a this-issue anaphor, the set C is extracted using the candidate extraction algorithm from Section 4. [sent-254, score-0.51]
</p><p>60 Note that the instance creation is simpler than for general coreference resolution because of the absence of anaphoric chains in our data. [sent-262, score-0.227]
</p><p>61 For every anaphor ai and eligible candidates Cf = {Cf1,Cf2, . [sent-263, score-0.365]
</p><p>62 cTreheat ela tbraeil niisn 1g if Ci is the true a,lnatebceel)de,∀nCt of∈ ∈th Ce anaphor ai, otherwise the label is −1. [sent-267, score-0.335]
</p><p>63 1 Evaluation of Candidate Extraction  The set of candidate antecedents extracted by the method from Section 4. [sent-274, score-0.619]
</p><p>64 1 contained the correct antecedent 92% of the time. [sent-275, score-0.291]
</p><p>65 The error analysis of the 8% of the instances where we failed to extract the correct antecedent revealed that most of these errors were parsing errors 1261 which could not be corrected by our candidate extraction method. [sent-280, score-0.515]
</p><p>66 10 In these cases, the parts of the antecedent had been placed in completely different branches of the parse tree. [sent-281, score-0.291]
</p><p>67 For example, in (5), the correct antecedent is a combination of the NP from the S → VP → NP → PP → NP branch and the PP ftrhoem S S → → VPP → → PPP → → bra PnPch →. [sent-282, score-0.291]
</p><p>68 2 Evaluation of this-issue Resolution We propose two metrics for abstract anaphora evaluation. [sent-288, score-0.49]
</p><p>69 The simplest metric is the percentage of antecedents on which the system and the annotated gold data agree. [sent-289, score-0.472]
</p><p>70 We denote this metric as EXACTM (Exact Match) and compute it as the ratio of number of correctly identified antecedents to the total number of marked antecedents. [sent-290, score-0.496]
</p><p>71 Let the marked antecedents of the gold corpus for k anaphor instances be G = hg1 , g2, . [sent-293, score-0.88]
</p><p>72 , gki and the system-annotated antecedents hbge A = ha1, a2, . [sent-296, score-0.444]
</p><p>73 PRLL is the total number of word overlaps between the gold and system-annotated antecedents normalized by the number of words in system-annotated antecedents and RRLL is the total number of such word overlaps normalized by the number of words in the gold antecedents. [sent-303, score-0.888]
</p><p>74 The F-score, 10Extracting  candidate constituents from the dependency trees did not add any new candidates to the set of candidates. [sent-305, score-0.283]
</p><p>75 86  12 13  Oracle candidate extractor + row 3 Oracle candidate sentence extractor + row 3  79. [sent-394, score-0.442]
</p><p>76 PRLL=n1i∑=k1LCS(gi,ai)  RRLL=m1i∑=k1LCS(gi,ai) FRLL=2×PRPLLRL+L×RRRLRLLL The lower bound of FRLL is 0, where no true antecedent has any common substring with the predicted antecedents and the upper bound is 1, where all the predicted and true antecedents are exactly the same. [sent-414, score-1.179]
</p><p>77 There are no implemented systems that resolve issue anaphora or abstract anaphora signalled by label nouns in arbitrary text to use as a comparison. [sent-416, score-1.199]
</p><p>78 1% of the antecedents lie within the adjacent sentence. [sent-420, score-0.444]
</p><p>79 The FRLL results from using only issue-specific features were below baseline, suggesting that the more general features associated with abstract anaphora play a crucial role in resolving this-issue anaphora. [sent-441, score-0.569]
</p><p>80 In the second experiment, we determined the error caused by the candidate extractor component of  our system. [sent-442, score-0.221]
</p><p>81 Row 12 of the table gives the result when an oracle candidate extractor was used to add the correct antecedent in the set of candidates whenever our candidate extractor failed. [sent-443, score-0.763]
</p><p>82 This shows that our resolution algorithm was able to identify antecedents that were arbitrary spans of text. [sent-446, score-0.624]
</p><p>83 We assumed an oracle candidate sentence extractor (Row 13) which knows the exact candidate sentence in which the antecedent lies. [sent-448, score-0.687]
</p><p>84 In response to these results, we trained a decision-tree classifier to identify the correct antecedent sentence with simple location and length features and achieved 95% accuracy in identifying the correct candidate sentence. [sent-450, score-0.466]
</p><p>85 6  Discussion and Conclusions  We have demonstrated the possibility of resolving complex abstract anaphora, namely, this-issue  anaphora having arbitrary antecedents. [sent-451, score-0.569]
</p><p>86 The work takes the annotation work of Botley (2006) and Dipper and Zinsmeister (201 1) to the next level by resolving this-issue anaphora automatically. [sent-452, score-0.569]
</p><p>87 The results also show that reduction of search space markedly improves the resolution performance, suggesting that a two-stage process that first identifies the broad region ofthe antecedent and then pinpoints the exact antecedent might work better —  than the current single-stage approach. [sent-460, score-0.762]
</p><p>88 First, the search space of abstract anaphora is large and noisy compared to nominal anaphora. [sent-462, score-0.568]
</p><p>89 13 And second, it is possible to reduce the search space and accurately identify the broad region of the antecedents using simple features such as the location of the anaphor in the anaphor sentence (e. [sent-463, score-1.114]
</p><p>90 , if the anaphor occurs at the beginning of the sentence, the antecedent is most likely present in the previous sentence). [sent-465, score-0.626]
</p><p>91 In the news domain, for instance, which we have also examined and are presently annotating, a large percentage of this-issue antecedents lie outside the text. [sent-469, score-0.444]
</p><p>92 Hence if we consider the antecedent candidates from the previous 2 or 3 sentences, the search space can become quite large and noisy. [sent-477, score-0.321]
</p><p>93 ” In such a case, the antecedent of this issue is not always in the text of the newspaper article itself, but must be inferred from the context of the quotation and the world of the speaker quoted. [sent-481, score-0.384]
</p><p>94 Our features are solely based on distance, syntactic structure, and semantic  and lexical properties of the candidate antecedents which could be extracted for text in any domain. [sent-483, score-0.649]
</p><p>95 Issue anaphora can also be signalled by demonstratives other than this. [sent-484, score-0.574]
</p><p>96 Our broad goal is to resolve abstract anaphora signalled by label nouns in all kinds of text. [sent-489, score-0.616]
</p><p>97 At present, the major obstacle is that there is very little annotated data available that could be used to train an abstract anaphora resolution system. [sent-490, score-0.698]
</p><p>98 And the understanding of abstract anaphora itself is still at an early stage; it would be premature to think about unsupervised approaches. [sent-491, score-0.49]
</p><p>99 In this work, we studied the narrow problem of resolution of this-issue anaphora in the medical domain to get a good grasp of the general abstract-anaphora resolution problem. [sent-492, score-0.85]
</p><p>100 Pronominal and  sortal anaphora resolution for biomedical literature. [sent-576, score-0.796]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('anaphora', 0.49), ('antecedents', 0.444), ('anaphor', 0.335), ('antecedent', 0.291), ('resolution', 0.18), ('candidate', 0.175), ('frll', 0.126), ('anaphors', 0.108), ('issue', 0.093), ('poesio', 0.087), ('sortal', 0.084), ('resolving', 0.079), ('constituents', 0.078), ('nominal', 0.078), ('sr', 0.073), ('ip', 0.067), ('annotator', 0.063), ('medline', 0.06), ('demonstrative', 0.06), ('mixed', 0.059), ('casta', 0.056), ('iverb', 0.056), ('prll', 0.056), ('rrll', 0.056), ('signalled', 0.056), ('uller', 0.054), ('marked', 0.052), ('governing', 0.05), ('discourse', 0.05), ('instances', 0.049), ('krippendorff', 0.048), ('toronto', 0.048), ('sbar', 0.047), ('anaphoric', 0.047), ('dt', 0.047), ('np', 0.047), ('annotators', 0.047), ('extractor', 0.046), ('oral', 0.043), ('mc', 0.043), ('biomedical', 0.042), ('botley', 0.042), ('ccaannddiiddaattee', 0.042), ('gundel', 0.042), ('ihead', 0.042), ('iiffff', 0.042), ('torii', 0.042), ('unitizing', 0.042), ('clause', 0.04), ('tthhee', 0.04), ('embedding', 0.04), ('subordinating', 0.04), ('type', 0.038), ('debate', 0.038), ('resolve', 0.036), ('dipper', 0.036), ('patients', 0.036), ('udo', 0.036), ('controversial', 0.036), ('modjeska', 0.036), ('sbarq', 0.036), ('sq', 0.036), ('pronominal', 0.036), ('sc', 0.035), ('nouns', 0.034), ('abstracts', 0.033), ('intersections', 0.033), ('eckert', 0.033), ('clausal', 0.033), ('reliability', 0.032), ('disagreement', 0.031), ('strube', 0.031), ('syntactic', 0.03), ('proposition', 0.03), ('copies', 0.03), ('candidates', 0.03), ('node', 0.029), ('head', 0.029), ('annotated', 0.028), ('massimo', 0.028), ('cancer', 0.028), ('el', 0.028), ('asher', 0.028), ('cabg', 0.028), ('carvedilol', 0.028), ('cfi', 0.028), ('controversy', 0.028), ('demonstratives', 0.028), ('estrogen', 0.028), ('ffoolllloowwss', 0.028), ('issuespecific', 0.028), ('metoprolol', 0.028), ('mismatching', 0.028), ('oront', 0.028), ('prospective', 0.028), ('ssbbaarr', 0.028), ('testdata', 0.028), ('thisissue', 0.028), ('varada', 0.028), ('weren', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="113-tfidf-1" href="./emnlp-2012-Resolving_This-issue_Anaphora.html">113 emnlp-2012-Resolving This-issue Anaphora</a></p>
<p>Author: Varada Kolhatkar ; Graeme Hirst</p><p>Abstract: We annotate and resolve a particular case of abstract anaphora, namely, thisissue anaphora. We propose a candidate ranking model for this-issue anaphora resolution that explores different issuespecific and general abstract-anaphora features. The model is not restricted to nominal or verbal antecedents; rather, it is able to identify antecedents that are arbitrary spans of text. Our results show that (a) the model outperforms the strong adjacent-sentence baseline; (b) general abstract-anaphora features, as distinguished from issue-specific features, play a crucial role in this-issue anaphora resolution, suggesting that our approach can be generalized for other NPs such as this problem and this debate; and (c) it is possible to reduce the search space in order to improve performance.</p><p>2 0.3681986 <a title="113-tfidf-2" href="./emnlp-2012-Resolving_Complex_Cases_of_Definite_Pronouns%3A_The_Winograd_Schema_Challenge.html">112 emnlp-2012-Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge</a></p>
<p>Author: Altaf Rahman ; Vincent Ng</p><p>Abstract: We examine the task of resolving complex cases of definite pronouns, specifically those for which traditional linguistic constraints on coreference (e.g., Binding Constraints, gender and number agreement) as well as commonly-used resolution heuristics (e.g., string-matching facilities, syntactic salience) are not useful. Being able to solve this task has broader implications in artificial intelligence: a restricted version of it, sometimes referred to as the Winograd Schema Challenge, has been suggested as a conceptually and practically appealing alternative to the Turing Test. We employ a knowledge-rich approach to this task, which yields a pronoun resolver that outperforms state-of-the-art resolvers by nearly 18 points in accuracy on our dataset.</p><p>3 0.11839724 <a title="113-tfidf-3" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>Author: Heeyoung Lee ; Marta Recasens ; Angel Chang ; Mihai Surdeanu ; Dan Jurafsky</p><p>Abstract: We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable documents, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.</p><p>4 0.067380808 <a title="113-tfidf-4" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>Author: Jonathan K. Kummerfeld ; David Hall ; James R. Curran ; Dan Klein</p><p>Abstract: Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors. We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for stateof-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.</p><p>5 0.064757638 <a title="113-tfidf-5" href="./emnlp-2012-Joint_Learning_for_Coreference_Resolution_with_Markov_Logic.html">73 emnlp-2012-Joint Learning for Coreference Resolution with Markov Logic</a></p>
<p>Author: Yang Song ; Jing Jiang ; Wayne Xin Zhao ; Sujian Li ; Houfeng Wang</p><p>Abstract: Pairwise coreference resolution models must merge pairwise coreference decisions to generate final outputs. Traditional merging methods adopt different strategies such as the bestfirst method and enforcing the transitivity constraint, but most of these methods are used independently of the pairwise learning methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-201 1shared task on the same dataset. Compared with the best system from CoNLL2011, which employs a rule-based method, our system shows competitive performance.</p><p>6 0.059287068 <a title="113-tfidf-6" href="./emnlp-2012-A_Novel_Discriminative_Framework_for_Sentence-Level_Discourse_Analysis.html">7 emnlp-2012-A Novel Discriminative Framework for Sentence-Level Discourse Analysis</a></p>
<p>7 0.057471778 <a title="113-tfidf-7" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>8 0.051206723 <a title="113-tfidf-8" href="./emnlp-2012-Left-to-Right_Tree-to-String_Decoding_with_Prediction.html">82 emnlp-2012-Left-to-Right Tree-to-String Decoding with Prediction</a></p>
<p>9 0.050498515 <a title="113-tfidf-9" href="./emnlp-2012-Domain_Adaptation_for_Coreference_Resolution%3A_An_Adaptive_Ensemble_Approach.html">36 emnlp-2012-Domain Adaptation for Coreference Resolution: An Adaptive Ensemble Approach</a></p>
<p>10 0.050255064 <a title="113-tfidf-10" href="./emnlp-2012-Using_Discourse_Information_for_Paraphrase_Extraction.html">135 emnlp-2012-Using Discourse Information for Paraphrase Extraction</a></p>
<p>11 0.047879424 <a title="113-tfidf-11" href="./emnlp-2012-Local_and_Global_Context_for_Supervised_and_Unsupervised_Metonymy_Resolution.html">85 emnlp-2012-Local and Global Context for Supervised and Unsupervised Metonymy Resolution</a></p>
<p>12 0.046112109 <a title="113-tfidf-12" href="./emnlp-2012-Learning-based_Multi-Sieve_Co-reference_Resolution_with_Knowledge.html">76 emnlp-2012-Learning-based Multi-Sieve Co-reference Resolution with Knowledge</a></p>
<p>13 0.045742348 <a title="113-tfidf-13" href="./emnlp-2012-Part-of-Speech_Tagging_for_Chinese-English_Mixed_Texts_with_Dynamic_Features.html">106 emnlp-2012-Part-of-Speech Tagging for Chinese-English Mixed Texts with Dynamic Features</a></p>
<p>14 0.045126896 <a title="113-tfidf-14" href="./emnlp-2012-Document-Wide_Decoding_for_Phrase-Based_Statistical_Machine_Translation.html">35 emnlp-2012-Document-Wide Decoding for Phrase-Based Statistical Machine Translation</a></p>
<p>15 0.04472186 <a title="113-tfidf-15" href="./emnlp-2012-Learning_Verb_Inference_Rules_from_Linguistically-Motivated_Evidence.html">80 emnlp-2012-Learning Verb Inference Rules from Linguistically-Motivated Evidence</a></p>
<p>16 0.043835782 <a title="113-tfidf-16" href="./emnlp-2012-A_Statistical_Relational_Learning_Approach_to_Identifying_Evidence_Based_Medicine_Categories.html">10 emnlp-2012-A Statistical Relational Learning Approach to Identifying Evidence Based Medicine Categories</a></p>
<p>17 0.038838118 <a title="113-tfidf-17" href="./emnlp-2012-Minimal_Dependency_Length_in_Realization_Ranking.html">88 emnlp-2012-Minimal Dependency Length in Realization Ranking</a></p>
<p>18 0.038089141 <a title="113-tfidf-18" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>19 0.037928745 <a title="113-tfidf-19" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>20 0.037698146 <a title="113-tfidf-20" href="./emnlp-2012-Lexical_Differences_in_Autobiographical_Narratives_from_Schizophrenic_Patients_and_Healthy_Controls.html">83 emnlp-2012-Lexical Differences in Autobiographical Narratives from Schizophrenic Patients and Healthy Controls</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.172), (1, 0.103), (2, -0.014), (3, -0.114), (4, 0.093), (5, -0.061), (6, -0.069), (7, -0.153), (8, 0.041), (9, 0.039), (10, -0.077), (11, -0.062), (12, -0.099), (13, 0.351), (14, 0.065), (15, -0.27), (16, -0.105), (17, -0.012), (18, -0.171), (19, 0.122), (20, 0.108), (21, 0.166), (22, -0.154), (23, -0.187), (24, 0.203), (25, -0.156), (26, 0.056), (27, 0.085), (28, -0.088), (29, 0.004), (30, 0.203), (31, 0.031), (32, -0.075), (33, 0.135), (34, 0.024), (35, 0.139), (36, -0.07), (37, -0.027), (38, -0.039), (39, 0.009), (40, 0.079), (41, 0.164), (42, -0.097), (43, 0.02), (44, 0.034), (45, -0.034), (46, 0.053), (47, 0.089), (48, 0.021), (49, 0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96088868 <a title="113-lsi-1" href="./emnlp-2012-Resolving_This-issue_Anaphora.html">113 emnlp-2012-Resolving This-issue Anaphora</a></p>
<p>Author: Varada Kolhatkar ; Graeme Hirst</p><p>Abstract: We annotate and resolve a particular case of abstract anaphora, namely, thisissue anaphora. We propose a candidate ranking model for this-issue anaphora resolution that explores different issuespecific and general abstract-anaphora features. The model is not restricted to nominal or verbal antecedents; rather, it is able to identify antecedents that are arbitrary spans of text. Our results show that (a) the model outperforms the strong adjacent-sentence baseline; (b) general abstract-anaphora features, as distinguished from issue-specific features, play a crucial role in this-issue anaphora resolution, suggesting that our approach can be generalized for other NPs such as this problem and this debate; and (c) it is possible to reduce the search space in order to improve performance.</p><p>2 0.84610623 <a title="113-lsi-2" href="./emnlp-2012-Resolving_Complex_Cases_of_Definite_Pronouns%3A_The_Winograd_Schema_Challenge.html">112 emnlp-2012-Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge</a></p>
<p>Author: Altaf Rahman ; Vincent Ng</p><p>Abstract: We examine the task of resolving complex cases of definite pronouns, specifically those for which traditional linguistic constraints on coreference (e.g., Binding Constraints, gender and number agreement) as well as commonly-used resolution heuristics (e.g., string-matching facilities, syntactic salience) are not useful. Being able to solve this task has broader implications in artificial intelligence: a restricted version of it, sometimes referred to as the Winograd Schema Challenge, has been suggested as a conceptually and practically appealing alternative to the Turing Test. We employ a knowledge-rich approach to this task, which yields a pronoun resolver that outperforms state-of-the-art resolvers by nearly 18 points in accuracy on our dataset.</p><p>3 0.2319275 <a title="113-lsi-3" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>Author: Heeyoung Lee ; Marta Recasens ; Angel Chang ; Mihai Surdeanu ; Dan Jurafsky</p><p>Abstract: We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable documents, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.</p><p>4 0.23102359 <a title="113-lsi-4" href="./emnlp-2012-Local_and_Global_Context_for_Supervised_and_Unsupervised_Metonymy_Resolution.html">85 emnlp-2012-Local and Global Context for Supervised and Unsupervised Metonymy Resolution</a></p>
<p>Author: Vivi Nastase ; Alex Judea ; Katja Markert ; Michael Strube</p><p>Abstract: Computational approaches to metonymy resolution have focused almost exclusively on the local context, especially the constraints placed on a potentially metonymic word by its grammatical collocates. We expand such approaches by taking into account the larger context. Our algorithm is tested on the data from the metonymy resolution task (Task 8) at SemEval 2007. The results show that incorporation of the global context can improve over the use of the local context alone, depending on the types of metonymies addressed. As a second contribution, we move towards unsupervised resolution of metonymies, made feasible by considering ontological relations as possible readings. We show that such an unsupervised approach delivers promising results: it beats the supervised most frequent sense baseline and performs close to a supervised approach using only standard lexico-syntactic features.</p><p>5 0.19501919 <a title="113-lsi-5" href="./emnlp-2012-A_Novel_Discriminative_Framework_for_Sentence-Level_Discourse_Analysis.html">7 emnlp-2012-A Novel Discriminative Framework for Sentence-Level Discourse Analysis</a></p>
<p>Author: Shafiq Joty ; Giuseppe Carenini ; Raymond Ng</p><p>Abstract: We propose a complete probabilistic discriminative framework for performing sentencelevel discourse analysis. Our framework comprises a discourse segmenter, based on a binary classifier, and a discourse parser, which applies an optimal CKY-like parsing algorithm to probabilities inferred from a Dynamic Conditional Random Field. We show on two corpora that our approach outperforms the state-of-the-art, often by a wide margin.</p><p>6 0.18542445 <a title="113-lsi-6" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>7 0.16491799 <a title="113-lsi-7" href="./emnlp-2012-Using_Discourse_Information_for_Paraphrase_Extraction.html">135 emnlp-2012-Using Discourse Information for Paraphrase Extraction</a></p>
<p>8 0.16379595 <a title="113-lsi-8" href="./emnlp-2012-Lexical_Differences_in_Autobiographical_Narratives_from_Schizophrenic_Patients_and_Healthy_Controls.html">83 emnlp-2012-Lexical Differences in Autobiographical Narratives from Schizophrenic Patients and Healthy Controls</a></p>
<p>9 0.15878963 <a title="113-lsi-9" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>10 0.15552716 <a title="113-lsi-10" href="./emnlp-2012-A_Statistical_Relational_Learning_Approach_to_Identifying_Evidence_Based_Medicine_Categories.html">10 emnlp-2012-A Statistical Relational Learning Approach to Identifying Evidence Based Medicine Categories</a></p>
<p>11 0.15050822 <a title="113-lsi-11" href="./emnlp-2012-A_Sequence_Labelling_Approach_to_Quote_Attribution.html">9 emnlp-2012-A Sequence Labelling Approach to Quote Attribution</a></p>
<p>12 0.14979246 <a title="113-lsi-12" href="./emnlp-2012-Learning_Verb_Inference_Rules_from_Linguistically-Motivated_Evidence.html">80 emnlp-2012-Learning Verb Inference Rules from Linguistically-Motivated Evidence</a></p>
<p>13 0.14183605 <a title="113-lsi-13" href="./emnlp-2012-Forest_Reranking_through_Subtree_Ranking.html">55 emnlp-2012-Forest Reranking through Subtree Ranking</a></p>
<p>14 0.13955662 <a title="113-lsi-14" href="./emnlp-2012-Building_a_Lightweight_Semantic_Model_for_Unsupervised_Information_Extraction_on_Short_Listings.html">26 emnlp-2012-Building a Lightweight Semantic Model for Unsupervised Information Extraction on Short Listings</a></p>
<p>15 0.13892385 <a title="113-lsi-15" href="./emnlp-2012-Joint_Learning_for_Coreference_Resolution_with_Markov_Logic.html">73 emnlp-2012-Joint Learning for Coreference Resolution with Markov Logic</a></p>
<p>16 0.13829181 <a title="113-lsi-16" href="./emnlp-2012-Extending_Machine_Translation_Evaluation_Metrics_with_Lexical_Cohesion_to_Document_Level.html">50 emnlp-2012-Extending Machine Translation Evaluation Metrics with Lexical Cohesion to Document Level</a></p>
<p>17 0.13736817 <a title="113-lsi-17" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>18 0.13085651 <a title="113-lsi-18" href="./emnlp-2012-First_Order_vs._Higher_Order_Modification_in_Distributional_Semantics.html">53 emnlp-2012-First Order vs. Higher Order Modification in Distributional Semantics</a></p>
<p>19 0.13051228 <a title="113-lsi-19" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>20 0.13027468 <a title="113-lsi-20" href="./emnlp-2012-Minimal_Dependency_Length_in_Realization_Ranking.html">88 emnlp-2012-Minimal Dependency Length in Realization Ranking</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.014), (16, 0.461), (34, 0.038), (60, 0.07), (63, 0.046), (64, 0.024), (65, 0.033), (70, 0.015), (73, 0.033), (74, 0.054), (76, 0.029), (79, 0.01), (80, 0.014), (86, 0.027), (95, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97914267 <a title="113-lda-1" href="./emnlp-2012-Framework_of_Automatic_Text_Summarization_Using_Reinforcement_Learning.html">56 emnlp-2012-Framework of Automatic Text Summarization Using Reinforcement Learning</a></p>
<p>Author: Seonggi Ryang ; Takeshi Abekawa</p><p>Abstract: We present a new approach to the problem of automatic text summarization called Automatic Summarization using Reinforcement Learning (ASRL) in this paper, which models the process of constructing a summary within the framework of reinforcement learning and attempts to optimize the given score function with the given feature representation of a summary. We demonstrate that the method of reinforcement learning can be adapted to automatic summarization problems naturally and simply, and other summarizing techniques, such as sentence compression, can be easily adapted as actions of the framework. The experimental results indicated ASRL was superior to the best performing method in DUC2004 and comparable to the state of the art ILP-style method, in terms of ROUGE scores. The results also revealed ASRL can search for sub-optimal solutions efficiently under conditions for effectively selecting features and the score function.</p><p>2 0.94582045 <a title="113-lda-2" href="./emnlp-2012-Generalized_Higher-Order_Dependency_Parsing_with_Cube_Pruning.html">57 emnlp-2012-Generalized Higher-Order Dependency Parsing with Cube Pruning</a></p>
<p>Author: Hao Zhang ; Ryan McDonald</p><p>Abstract: State-of-the-art graph-based parsers use features over higher-order dependencies that rely on decoding algorithms that are slow and difficult to generalize. On the other hand, transition-based dependency parsers can easily utilize such features without increasing the linear complexity of the shift-reduce system beyond a constant. In this paper, we attempt to address this imbalance for graph-based parsing by generalizing the Eisner (1996) algorithm to handle arbitrary features over higherorder dependencies. The generalization is at the cost of asymptotic efficiency. To account for this, cube pruning for decoding is utilized (Chiang, 2007). For the first time, label tuple and structural features such as valencies can be scored efficiently with third-order features in a graph-based parser. Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation ofthe third-ordermodel of Koo et al. (2010).</p><p>same-paper 3 0.90589988 <a title="113-lda-3" href="./emnlp-2012-Resolving_This-issue_Anaphora.html">113 emnlp-2012-Resolving This-issue Anaphora</a></p>
<p>Author: Varada Kolhatkar ; Graeme Hirst</p><p>Abstract: We annotate and resolve a particular case of abstract anaphora, namely, thisissue anaphora. We propose a candidate ranking model for this-issue anaphora resolution that explores different issuespecific and general abstract-anaphora features. The model is not restricted to nominal or verbal antecedents; rather, it is able to identify antecedents that are arbitrary spans of text. Our results show that (a) the model outperforms the strong adjacent-sentence baseline; (b) general abstract-anaphora features, as distinguished from issue-specific features, play a crucial role in this-issue anaphora resolution, suggesting that our approach can be generalized for other NPs such as this problem and this debate; and (c) it is possible to reduce the search space in order to improve performance.</p><p>4 0.61092156 <a title="113-lda-4" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>Author: Bernd Bohnet ; Joakim Nivre</p><p>Abstract: Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins. We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees. Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages.</p><p>5 0.51303178 <a title="113-lda-5" href="./emnlp-2012-Improving_Transition-Based_Dependency_Parsing_with_Buffer_Transitions.html">66 emnlp-2012-Improving Transition-Based Dependency Parsing with Buffer Transitions</a></p>
<p>Author: Daniel Fernandez-Gonzalez ; Carlos Gomez-Rodriguez</p><p>Abstract: In this paper, we show that significant improvements in the accuracy of well-known transition-based parsers can be obtained, without sacrificing efficiency, by enriching the parsers with simple transitions that act on buffer nodes. First, we show how adding a specific transition to create either a left or right arc of length one between the first two buffer nodes produces improvements in the accuracy of Nivre’s arc-eager projective parser on a number of datasets from the CoNLL-X shared task. Then, we show that accuracy can also be improved by adding transitions involving the topmost stack node and the second buffer node (allowing a limited form of non-projectivity). None of these transitions has a negative impact on the computational complexity of the algorithm. Although the experiments in this paper use the arc-eager parser, the approach is generic enough to be applicable to any stackbased dependency parser.</p><p>6 0.50855571 <a title="113-lda-6" href="./emnlp-2012-Document-Wide_Decoding_for_Phrase-Based_Statistical_Machine_Translation.html">35 emnlp-2012-Document-Wide Decoding for Phrase-Based Statistical Machine Translation</a></p>
<p>7 0.49904427 <a title="113-lda-7" href="./emnlp-2012-Left-to-Right_Tree-to-String_Decoding_with_Prediction.html">82 emnlp-2012-Left-to-Right Tree-to-String Decoding with Prediction</a></p>
<p>8 0.49544656 <a title="113-lda-8" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<p>9 0.48492676 <a title="113-lda-9" href="./emnlp-2012-A_Discriminative_Model_for_Query_Spelling_Correction_with_Latent_Structural_SVM.html">5 emnlp-2012-A Discriminative Model for Query Spelling Correction with Latent Structural SVM</a></p>
<p>10 0.48420683 <a title="113-lda-10" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>11 0.47917038 <a title="113-lda-11" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>12 0.46759126 <a title="113-lda-12" href="./emnlp-2012-Exploiting_Chunk-level_Features_to_Improve_Phrase_Chunking.html">45 emnlp-2012-Exploiting Chunk-level Features to Improve Phrase Chunking</a></p>
<p>13 0.46097335 <a title="113-lda-13" href="./emnlp-2012-Resolving_Complex_Cases_of_Definite_Pronouns%3A_The_Winograd_Schema_Challenge.html">112 emnlp-2012-Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge</a></p>
<p>14 0.46016526 <a title="113-lda-14" href="./emnlp-2012-Forest_Reranking_through_Subtree_Ranking.html">55 emnlp-2012-Forest Reranking through Subtree Ranking</a></p>
<p>15 0.45824948 <a title="113-lda-15" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>16 0.4565579 <a title="113-lda-16" href="./emnlp-2012-Minimal_Dependency_Length_in_Realization_Ranking.html">88 emnlp-2012-Minimal Dependency Length in Realization Ranking</a></p>
<p>17 0.44338271 <a title="113-lda-17" href="./emnlp-2012-Besting_the_Quiz_Master%3A_Crowdsourcing_Incremental_Classification_Games.html">23 emnlp-2012-Besting the Quiz Master: Crowdsourcing Incremental Classification Games</a></p>
<p>18 0.442709 <a title="113-lda-18" href="./emnlp-2012-Extracting_Opinion_Expressions_with_semi-Markov_Conditional_Random_Fields.html">51 emnlp-2012-Extracting Opinion Expressions with semi-Markov Conditional Random Fields</a></p>
<p>19 0.44257256 <a title="113-lda-19" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>20 0.44190499 <a title="113-lda-20" href="./emnlp-2012-Parse%2C_Price_and_Cut-Delayed_Column_and_Row_Generation_for_Graph_Based_Parsers.html">104 emnlp-2012-Parse, Price and Cut-Delayed Column and Row Generation for Graph Based Parsers</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
