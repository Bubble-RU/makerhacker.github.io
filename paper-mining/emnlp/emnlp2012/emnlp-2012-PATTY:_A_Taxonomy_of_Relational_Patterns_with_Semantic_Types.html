<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>103 emnlp-2012-PATTY: A Taxonomy of Relational Patterns with Semantic Types</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-103" href="#">emnlp2012-103</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>103 emnlp-2012-PATTY: A Taxonomy of Relational Patterns with Semantic Types</h1>
<br/><p>Source: <a title="emnlp-2012-103-pdf" href="http://aclweb.org/anthology//D/D12/D12-1104.pdf">pdf</a></p><p>Author: Ndapandula Nakashole ; Gerhard Weikum ; Fabian Suchanek</p><p>Abstract: This paper presents PATTY: a large resource for textual patterns that denote binary relations between entities. The patterns are semantically typed and organized into a subsumption taxonomy. The PATTY system is based on efficient algorithms for frequent itemset mining and can process Web-scale corpora. It harnesses the rich type system and entity population of large knowledge bases. The PATTY taxonomy comprises 350,569 pattern synsets. Random-sampling-based evaluation shows a pattern accuracy of 84.7%. PATTY has 8,162 subsumptions, with a random-sampling-based precision of 75%. The PATTY resource is freely available for interactive access and download.</p><p>Reference: <a title="emnlp-2012-103-reference" href="../emnlp2012_reference/emnlp-2012-PATTY%3A_A_Taxonomy_of_Relational_Patterns_with_Semantic_Types_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract This paper presents PATTY: a large resource for textual patterns that denote binary relations between entities. [sent-3, score-0.456]
</p><p>2 The patterns are semantically typed and organized into a subsumption taxonomy. [sent-4, score-0.457]
</p><p>3 For example, WordNet does not contain the pattern X is romantically involved with Y. [sent-17, score-0.264]
</p><p>4 Just like words, patterns can be synonymous, and they can subsume each other. [sent-18, score-0.261]
</p><p>5 The pattern X is romantically involved with Y is synonymous with the pattern X is dating Y. [sent-19, score-0.541]
</p><p>6 Ifa large-scale resource ofrelational patterns were available, this could boost progress in NLP and AI tasks. [sent-22, score-0.32]
</p><p>7 However, no attempt is made to organize these patterns into synonymous  patterns, let alone into a taxonomy. [sent-26, score-0.304]
</p><p>8 Our goal in this paper is to systematically compile relational patterns from a corpus, and to impose a semantically typed structure on them. [sent-29, score-0.438]
</p><p>9 In particular, we aim at patterns that contain semantic types, such as hsingeri sings hsongi . [sent-31, score-0.628]
</p><p>10 Waine saelmsoa want to automatically generalize syntactic Wveari aaltsioon ws snutc hto as sings chaelrly hsongi lainzde sings htiisc hsongi, ninsto s a more general pattern sings [prp] hsongi nwgiit,h i nPtOoS a tag [prp]. [sent-32, score-0.864]
</p><p>11 Analogously n bgust more demandingly, we want to automatically yin bfeurt that the above patterns are semantically subsumed by the pattern hmusiciani performs on hmusical compositioni wni hthm more general types f oonr t hhme entity arguments nini t whiet pattern. [sent-33, score-0.782]
</p><p>12 Compiling and organizing such patterns is challenging for the following reasons. [sent-34, score-0.261]
</p><p>13 1) The number of possible patterns increases exponentially with the length of the patterns. [sent-35, score-0.261]
</p><p>14 For example, the string “Amy sings ‘Rehab”’ can give rise to the patterns hsingeri sings hsongi , hpersoni sings hartifacti , hpersoni  [vbz] hentityi, etc. [sent-36, score-1.459]
</p><p>15 sIofn nwiil sdicnagrsds h faortri multiple ewrsoorndis are za]llo hewnetdit (such as ifn w hpersoni sings *u hsongi), trdhes anurem ablleorw oefd possible patterns explodes. [sent-37, score-0.68]
</p><p>16 , the patterns hsingeri later disliked her song hsongi ,a tnhde hsingeri sang hsongi, may apply to tshoen same set aonfd entity pairs ning thhseo corpus. [sent-45, score-0.652]
</p><p>17 4) Computing mutual subsumptions on a large set of patterns may be prohibitively slow. [sent-47, score-0.56]
</p><p>18 Moreover, due to noise and vague semantics, patterns may even not form a crisp taxonomy, but require a hierarchy in which subsumption relations have to be weighted by statistical confidence measures. [sent-48, score-0.463]
</p><p>19 In this paper, we present PATTY, a large resource of relational patterns that are arranged  in a semantically meaningful taxonomy, along with entity-pair instances. [sent-50, score-0.497]
</p><p>20 More precisely, our contributions are as follows: 1) SOL patterns: We define an expressive family of relational patterns, which combines syntactic features (S), ontological type signatures (O), and lexical features (L). [sent-51, score-0.277]
</p><p>21 When compared to a state-of-the-art pattern language, we found that SOL patterns yield higher recall while achieving similar precision. [sent-53, score-0.495]
</p><p>22 2) Mining algorithms: We present efficient and scalable algorithms that can infer SOL patterns and subsumptions at scale, based on instance-level overlaps and an ontological type hierarchy. [sent-54, score-0.686]
</p><p>23 On the Wikipedia corpus, we obtained 350,569 pattern synsets with 84. [sent-56, score-0.362]
</p><p>24 We make our pattern taxonomy available for further research at www . [sent-58, score-0.392]
</p><p>25 ReVerb (Fader 2011) constrains patterns to verbs or verb phrases that end with prepositions, while PATTY can learn arbitrary patterns. [sent-82, score-0.261]
</p><p>26 , personHasCitizenship: hpersoni hcountryi), and pleearrsnosn to extract ssuhiitapb:l hep noun-phrase pairs fir),om an a large Web corpus. [sent-87, score-0.299]
</p><p>27 For example, the relation musicianPlaysInstrument is found by clustering pattern co-occurrences for the noun-phrase pairs that fall into the specific type signature hmusiciani hmusicinstrumenti . [sent-90, score-0.515]
</p><p>28 In contrast, PATTY efficiently acquires patterns from large-scale corpora and organizes them into a subsumption hierarchy. [sent-93, score-0.377]
</p><p>29 Class-based attribute discovery is a special case of mining relational patterns (e. [sent-94, score-0.4]
</p><p>30 Another task in this research avenue is to characterize and predict the argument types for a given relation or pattern (Kozareva 2010; Nakov 2008). [sent-102, score-0.284]
</p><p>31 This is closer to KB population and less related to our task of discovering relational patterns and systematically organizing them. [sent-103, score-0.358]
</p><p>32 From a linguistic perspective, there is ample work on patterns for unary predicates of the form class(entity). [sent-104, score-0.261]
</p><p>33 3  Pattern Extraction  This section explains how we obtain basic textual patterns from the input corpus. [sent-118, score-0.311]
</p><p>34 In the example, the textual pattern is Amy Winehouse effortlessly performed Rehab (song). [sent-137, score-0.314]
</p><p>35 4  SOL Pattern Model  Textual patterns are tied to the particular surface form of the text. [sent-138, score-0.261]
</p><p>36 Therefore, we transform the textual  patterns into a new type of patterns, called syntacticontologic-lexical patterns (SOL patterns). [sent-139, score-0.617]
</p><p>37 SOL patterns extend lexico-syntactic patterns by ontological type signatures for entities. [sent-140, score-0.702]
</p><p>38 The SOL pattern language is expressive enough to capture fine-grained relational patterns, yet simple enough to be dealt with by efficient mining algorithms at Web scale. [sent-141, score-0.373]
</p><p>39 A SOL pattern is an abstraction of a textual pattern that connects two entities of interest. [sent-142, score-0.569]
</p><p>40 n Wdisld fcoarrd asn are essential to avoid overfitting of patterns to the corpus. [sent-147, score-0.261]
</p><p>41 For example, the pattern hpersoni ’s [adj] voice * hsongi ammatpclhee,s t tehe p strings “Amy W’si [naedhjo]uvs oei’cs eso *ft h svoonicgei in ‘Rehab”’ and “Elvis Presley’s solid voice in his song ‘All shook up”’ . [sent-152, score-0.711]
</p><p>42 The type signature of a pattern is the pair of the entity placeholders. [sent-153, score-0.423]
</p><p>43 The support set otyfp a pattern rise t ihse set soofn pairs oofn gen. [sent-155, score-0.351]
</p><p>44 In the example, the support set of the pattern could be {(Amy, Rehab), (Elvis, AllShookUp)}. [sent-157, score-0.306]
</p><p>45 If A is semantically more general than B and B is semantically more general than A, the patterns are called synonymous. [sent-161, score-0.421]
</p><p>46 A set of synonymous patterns is called a pattern synset. [sent-162, score-0.538]
</p><p>47 To generate SOL patterns from the textual pat-  terns, we decompose the textual patterns into ngrams (n consecutive words). [sent-164, score-0.622]
</p><p>48 For example, in the sentence “was the first female to run for the governor of” might give rise to the pattern * the first female * governor of, if “the first female” and “governor of” are frequent in the corpus. [sent-166, score-0.328]
</p><p>49 We generate multiple patterns with different types, one for each combination of types that the detected entities have in the underlying ontology. [sent-169, score-0.312]
</p><p>50 We quantify the statistical strength of a pattern by  ×  means of its support set. [sent-170, score-0.306]
</p><p>51 For a given pattern p with type signature t1 t2, the support of p is the size otyfp iets s support set. [sent-171, score-0.549]
</p><p>52 5 Syntactic Pattern Generalization Almost every pattern can be generalized into a syntactically more general pattern in several ways: by replacing words by POS-tags, by introducing wildcards (combining more n-grams), or by generalizing the types in the pattern. [sent-174, score-0.498]
</p><p>53 We observe, however, that generalizing a pattern may create a pattern that subsumes two semantically different patterns. [sent-176, score-0.593]
</p><p>54 For example, the generalization hpersoni [vb] hpersoni subsumes the two semantically odniiffe [rvebn]t patterns hpersoni slo tvhees hpersoni anntidhpersoni ehraetnest hpersoni . [sent-177, score-1.881]
</p><p>55 pTerhisos means tsh hapt ethrseo pattern hisp semantically meaningless. [sent-178, score-0.314]
</p><p>56 If a generalization subsumes multiple patterns with dis-  joint support sets, we abandon the generalized pattern. [sent-181, score-0.378]
</p><p>57 6  Semantic Pattern Generalization  The main difficulty in generating semantic subsumptions is that the support sets may contain spurious pairs or be incomplete, thus destroying crisp set inclusions. [sent-183, score-0.408]
</p><p>58 d Hueo to sparsity, uitp may become a subset of another support set B, even if the 1139 two patterns are semantically different. [sent-189, score-0.413]
</p><p>59 We interpret S as a random sample from the “true” support set S0 that the pattern  would have on an infinitely large corpus. [sent-195, score-0.306]
</p><p>60 −  −  7  Taxonomy Construction  We now have to arrange the patterns in a semantic  taxonomy. [sent-212, score-0.298]
</p><p>61 A baseline solution would compare every pattern support set to every other pattern support set in order to determine inclusion, mutual inclusion, or independence. [sent-213, score-0.612]
</p><p>62 For this reason, we make use of a prefix-tree for frequent patterns (Han 2005). [sent-215, score-0.261]
</p><p>63 1 Prefix-Tree Construction Suppose we have pattern synsets and their support sets as shown in Table 1. [sent-219, score-0.434]
</p><p>64 For example, in the support set for the pattern hPoliticani was governor of hStatei, t thhee entry hA,80i may ndein wotaes th goev entity  Figure 1: Prefix-Tree for the Synsets in Table 1. [sent-221, score-0.416]
</p><p>65 This enables subsumptions to be directly “read off” from the tree, while representing the tree in a compact manner. [sent-225, score-0.299]
</p><p>66 Each node (entity pair) stores the identifiers of the pattern sysnets whose support sets contain that entity pair. [sent-228, score-0.369]
</p><p>67 Figure 1 shows the tree for the pattern synsets in Table 1. [sent-230, score-0.362]
</p><p>68 The two patterns have a prefix in common, 1140  thus they share the same path. [sent-232, score-0.292]
</p><p>69 2 Mining Subsumptions from the Prefix-Tree To efficiently mine subsumptions from the prefixtree, we have to avoid comparing every path to every other path as this introduces the same inefficiencies that the baseline approach suffers from. [sent-240, score-0.369]
</p><p>70 By traversing the entire path of a synset Pi, we can reach all the pattern  synsets sharing common nodes with Pi. [sent-242, score-0.458]
</p><p>71 Traversing the tree this way for all patterns gives us the sizes of the support set intersection. [sent-244, score-0.333]
</p><p>72 3  DAG Construction  Once we have generated subsumptions between relational patterns, there might be cycles in the graph we generate. [sent-247, score-0.396]
</p><p>73 We ideally want to remove the minimal total number of subsumptions whose removal results in an a directed acyclic graph (DAG). [sent-248, score-0.299]
</p><p>74 Redundancy occurs when there already exists a path, by transitivity of subsumptions, between pattern synsets linked by the subsumption. [sent-254, score-0.362]
</p><p>75 This process finally yields a DAG of pattern synsets the PATTY taxonony. [sent-255, score-0.362]
</p><p>76 All relational patterns and their respective entity pairs are stored in a MongoDB database. [sent-261, score-0.421]
</p><p>77 In terms of runtimes, he most expensive part is the pattern extraction, where we identify pattern candidates through dependency parsing and perform entity recognition on the entire corpus. [sent-265, score-0.531]
</p><p>78 2  Precision of Relational Patterns  To assess the precision of the automatically mined patterns (patterns in this section always mean pattern  synsets), we sampled the PATTY taxonomy for each combination of input corpus and type system. [sent-273, score-0.69]
</p><p>79 We ranked the patterns by their statistical strength (Section 4), and evaluated the precision of the top 100 pattern synsets. [sent-274, score-0.532]
</p><p>80 Several human judges were shown a sampled pattern synset, its type signature, and a few example instances, and then stated whether the pattern synset indicates a valid relation or not. [sent-275, score-0.624]
</p><p>81 Evaluators checked the correctness of the type signature, whether the majority of patterns in the synset is reasonable, and whether the instances seem plausible. [sent-276, score-0.367]
</p><p>82 First, Wikipedia patterns have higher precision than those from the New York Times corpus. [sent-300, score-0.298]
</p><p>83 This is because some the language in the news corpus does not express relational information; especially the news on stock markets produced noisy patterns picked up by PATTY. [sent-301, score-0.358]
</p><p>84 However, we still manage to have a precision of close to 90% for the top 100 patterns and around 72% for random sample on the NYT corpus. [sent-302, score-0.298]
</p><p>85 3 Precision of Subsumptions We evaluated the quality of the subsumptions by assessing 100 top-ranked as well as 100 randomly selected subsumptions. [sent-306, score-0.299]
</p><p>86 As shown in Table 3, a  large number of the subsumptions are correct. [sent-307, score-0.299]
</p><p>87 0097 Table 3: Quality of Subsumptions Example subsumptions from Wikipedia are: • hpersoni nominated for hawardi hpersoni winner of hawardi • hpersoni ’ s wife hpersoni hpersoni ’’s s sw widifoew hp hpersoni  =  =  8. [sent-325, score-2.153]
</p><p>88 We then compared our relational patterns to the relations included in four major knowledge bases, namely, YAGO2, DBpedia (DBP), Freebase (FB), and NELL, limited to the specific domain of music. [sent-340, score-0.444]
</p><p>89 For PATTY, the patterns were derived from the Wikipedia corpus with the YAGO2 type system. [sent-342, score-0.306]
</p><p>90 We looked at three main alternatives: the first is verb-phrase-centric patterns advocated by ReVerb (Fader 2011), the second is the PATTY language without type signatures (just using sets of n-grams with syntactic generalizations), and the third one is the full PATTY language. [sent-348, score-0.36]
</p><p>91 for  YAGO2 type system are shown in Table 5; precision figures are based on the respective top 100 pat-  terns or subsumption edges. [sent-376, score-0.249]
</p><p>92 Moreover, the number of patterns, subsumptions and facts found by verb-phrase-centric patterns (ReVerb (Fader 2011)), are limited in recall. [sent-378, score-0.56]
</p><p>93 General pattern synsets with type signatures, as newly pursued in this paper, substantially outperform the verb-phrase-centric alternative in terms of pattern and subsumption recall while yielding high precision. [sent-379, score-0.757]
</p><p>94 6 Extrinsic Study: Relation Paraphrasing To further evaluate the usefulness of PATTY, we performed a study on relation paraphrasing: given a relation from a knowledge base, identify patterns that can be used to express that relation. [sent-381, score-0.361]
</p><p>95 Paraphrasing relations with high-quality patterns is important for populating knowledge bases and counters the problem of semantic drifting caused by ambiguous and noisy patterns. [sent-382, score-0.431]
</p><p>96 , PATTY picked up patterns like “worked with” as paraphrases. [sent-390, score-0.261]
</p><p>97 Different from existing resources, PATTY organizes patterns into synsets and a taxonomy, similar in spirit to WordNet. [sent-397, score-0.389]
</p><p>98 Our evaluation shows that PATTY’s patterns are semantically meaningful, and that they cover large parts of the relations  of other knowledge bases. [sent-398, score-0.427]
</p><p>99 The Wikipedia-based version of PATTY contains 350,569 pattern synsets at a precision of 84. [sent-399, score-0.399]
</p><p>100 MIT Press, 1998 Jiawei Han, Jian Pei , Yiwen Yin : Mining frequent patterns without candidate generation. [sent-444, score-0.261]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('patty', 0.502), ('hpersoni', 0.299), ('subsumptions', 0.299), ('patterns', 0.261), ('pattern', 0.234), ('sol', 0.18), ('hsongi', 0.135), ('synsets', 0.128), ('sings', 0.12), ('subsumption', 0.116), ('taxonomy', 0.113), ('hmusiciani', 0.105), ('relational', 0.097), ('dbpedia', 0.093), ('rehab', 0.09), ('winehouse', 0.09), ('relations', 0.086), ('ontological', 0.081), ('signature', 0.081), ('semantically', 0.08), ('hsingeri', 0.075), ('suchanek', 0.073), ('support', 0.072), ('entity', 0.063), ('wikipedia', 0.062), ('freebase', 0.062), ('pasca', 0.061), ('synset', 0.061), ('amy', 0.061), ('resource', 0.059), ('nell', 0.058), ('entailment', 0.056), ('fader', 0.054), ('signatures', 0.054), ('dag', 0.054), ('reverb', 0.054), ('terns', 0.051), ('entities', 0.051), ('weikum', 0.05), ('textual', 0.05), ('relation', 0.05), ('bases', 0.047), ('auer', 0.047), ('governor', 0.047), ('prespecified', 0.047), ('type', 0.045), ('conceptnet', 0.045), ('otyfp', 0.045), ('subsumes', 0.045), ('www', 0.045), ('song', 0.043), ('paraphrasing', 0.043), ('synonymous', 0.043), ('paths', 0.042), ('mining', 0.042), ('bollacker', 0.04), ('gerhard', 0.04), ('marius', 0.04), ('agrawal', 0.039), ('deg', 0.039), ('subsumed', 0.039), ('fabian', 0.038), ('semantic', 0.037), ('shortest', 0.037), ('precision', 0.037), ('path', 0.035), ('musicians', 0.035), ('generalizations', 0.035), ('interval', 0.035), ('wordnet', 0.034), ('attributes', 0.033), ('hoffart', 0.032), ('nakov', 0.032), ('nsubj', 0.032), ('nastase', 0.032), ('stands', 0.031), ('wilson', 0.031), ('prefix', 0.031), ('http', 0.031), ('androutsopoulos', 0.03), ('effortlessly', 0.03), ('elvis', 0.03), ('engl', 0.03), ('hartifacti', 0.03), ('havasi', 0.03), ('hawardi', 0.03), ('hentityi', 0.03), ('itemset', 0.03), ('kann', 0.03), ('limaye', 0.03), ('omega', 0.03), ('philpot', 0.03), ('placeholders', 0.03), ('pvldb', 0.03), ('rakesh', 0.03), ('romantically', 0.03), ('srikant', 0.03), ('venetis', 0.03), ('wildcards', 0.03), ('wu', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="103-tfidf-1" href="./emnlp-2012-PATTY%3A_A_Taxonomy_of_Relational_Patterns_with_Semantic_Types.html">103 emnlp-2012-PATTY: A Taxonomy of Relational Patterns with Semantic Types</a></p>
<p>Author: Ndapandula Nakashole ; Gerhard Weikum ; Fabian Suchanek</p><p>Abstract: This paper presents PATTY: a large resource for textual patterns that denote binary relations between entities. The patterns are semantically typed and organized into a subsumption taxonomy. The PATTY system is based on efficient algorithms for frequent itemset mining and can process Web-scale corpora. It harnesses the rich type system and entity population of large knowledge bases. The PATTY taxonomy comprises 350,569 pattern synsets. Random-sampling-based evaluation shows a pattern accuracy of 84.7%. PATTY has 8,162 subsumptions, with a random-sampling-based precision of 75%. The PATTY resource is freely available for interactive access and download.</p><p>2 0.14366268 <a title="103-tfidf-2" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>Author: Mohamed Yahya ; Klaus Berberich ; Shady Elbassuoni ; Maya Ramanath ; Volker Tresp ; Gerhard Weikum</p><p>Abstract: The Linked Data initiative comprises structured databases in the Semantic-Web data model RDF. Exploring this heterogeneous data by structured query languages is tedious and error-prone even for skilled users. To ease the task, this paper presents a methodology for translating natural language questions into structured SPARQL queries over linked-data sources. Our method is based on an integer linear program to solve several disambiguation tasks jointly: the segmentation of questions into phrases; the mapping of phrases to semantic entities, classes, and relations; and the construction of SPARQL triple patterns. Our solution harnesses the rich type system provided by knowledge bases in the web of linked data, to constrain our semantic-coherence objective function. We present experiments on both the . in question translation and the resulting query answering.</p><p>3 0.12969878 <a title="103-tfidf-3" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>Author: Thomas Lin ; Mausam ; Oren Etzioni</p><p>Abstract: Entity linking systems link noun-phrase mentions in text to their corresponding Wikipedia articles. However, NLP applications would gain from the ability to detect and type all entities mentioned in text, including the long tail of entities not prominent enough to have their own Wikipedia articles. In this paper we show that once the Wikipedia entities mentioned in a corpus of textual assertions are linked, this can further enable the detection and fine-grained typing of the unlinkable entities. Our proposed method for detecting unlinkable entities achieves 24% greater accuracy than a Named Entity Recognition baseline, and our method for fine-grained typing is able to propagate over 1,000 types from linked Wikipedia entities to unlinkable entities. Detection and typing of unlinkable entities can increase yield for NLP applications such as typed question answering.</p><p>4 0.10716435 <a title="103-tfidf-4" href="./emnlp-2012-Learning_Verb_Inference_Rules_from_Linguistically-Motivated_Evidence.html">80 emnlp-2012-Learning Verb Inference Rules from Linguistically-Motivated Evidence</a></p>
<p>Author: Hila Weisman ; Jonathan Berant ; Idan Szpektor ; Ido Dagan</p><p>Abstract: Learning inference relations between verbs is at the heart of many semantic applications. However, most prior work on learning such rules focused on a rather narrow set of information sources: mainly distributional similarity, and to a lesser extent manually constructed verb co-occurrence patterns. In this paper, we claim that it is imperative to utilize information from various textual scopes: verb co-occurrence within a sentence, verb cooccurrence within a document, as well as overall corpus statistics. To this end, we propose a much richer novel set of linguistically motivated cues for detecting entailment between verbs and combine them as features in a supervised classification framework. We empirically demonstrate that our model significantly outperforms previous methods and that information from each textual scope contributes to the verb entailment learning task.</p><p>5 0.10172338 <a title="103-tfidf-5" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>Author: Ni Lao ; Amarnag Subramanya ; Fernando Pereira ; William W. Cohen</p><p>Abstract: We study how to extend a large knowledge base (Freebase) by reading relational information from a large Web text corpus. Previous studies on extracting relational knowledge from text show the potential of syntactic patterns for extraction, but they do not exploit background knowledge of other relations in the knowledge base. We describe a distributed, Web-scale implementation of a path-constrained random walk model that learns syntactic-semantic inference rules for binary relations from a graph representation of the parsed text and the knowledge base. Experiments show significant accuracy improvements in binary relation prediction over methods that consider only text, or only the existing knowledge base.</p><p>6 0.099351391 <a title="103-tfidf-6" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>7 0.09477362 <a title="103-tfidf-7" href="./emnlp-2012-Open_Language_Learning_for_Information_Extraction.html">100 emnlp-2012-Open Language Learning for Information Extraction</a></p>
<p>8 0.081578344 <a title="103-tfidf-8" href="./emnlp-2012-Ensemble_Semantics_for_Large-scale_Unsupervised_Relation_Extraction.html">40 emnlp-2012-Ensemble Semantics for Large-scale Unsupervised Relation Extraction</a></p>
<p>9 0.081567407 <a title="103-tfidf-9" href="./emnlp-2012-Constructing_Task-Specific_Taxonomies_for_Document_Collection_Browsing.html">30 emnlp-2012-Constructing Task-Specific Taxonomies for Document Collection Browsing</a></p>
<p>10 0.068343222 <a title="103-tfidf-10" href="./emnlp-2012-Identifying_Constant_and_Unique_Relations_by_using_Time-Series_Text.html">62 emnlp-2012-Identifying Constant and Unique Relations by using Time-Series Text</a></p>
<p>11 0.063407913 <a title="103-tfidf-11" href="./emnlp-2012-Learning-based_Multi-Sieve_Co-reference_Resolution_with_Knowledge.html">76 emnlp-2012-Learning-based Multi-Sieve Co-reference Resolution with Knowledge</a></p>
<p>12 0.062277954 <a title="103-tfidf-12" href="./emnlp-2012-Joining_Forces_Pays_Off%3A_Multilingual_Joint_Word_Sense_Disambiguation.html">69 emnlp-2012-Joining Forces Pays Off: Multilingual Joint Word Sense Disambiguation</a></p>
<p>13 0.062014282 <a title="103-tfidf-13" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>14 0.061200723 <a title="103-tfidf-14" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>15 0.059488181 <a title="103-tfidf-15" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>16 0.055237744 <a title="103-tfidf-16" href="./emnlp-2012-Enlarging_Paraphrase_Collections_through_Generalization_and_Instantiation.html">39 emnlp-2012-Enlarging Paraphrase Collections through Generalization and Instantiation</a></p>
<p>17 0.053983033 <a title="103-tfidf-17" href="./emnlp-2012-Explore_Person_Specific_Evidence_in_Web_Person_Name_Disambiguation.html">47 emnlp-2012-Explore Person Specific Evidence in Web Person Name Disambiguation</a></p>
<p>18 0.052253962 <a title="103-tfidf-18" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>19 0.052097537 <a title="103-tfidf-19" href="./emnlp-2012-Local_and_Global_Context_for_Supervised_and_Unsupervised_Metonymy_Resolution.html">85 emnlp-2012-Local and Global Context for Supervised and Unsupervised Metonymy Resolution</a></p>
<p>20 0.048287012 <a title="103-tfidf-20" href="./emnlp-2012-An_Entity-Topic_Model_for_Entity_Linking.html">19 emnlp-2012-An Entity-Topic Model for Entity Linking</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.181), (1, 0.146), (2, -0.018), (3, -0.039), (4, 0.041), (5, -0.003), (6, 0.141), (7, 0.23), (8, -0.151), (9, 0.006), (10, 0.01), (11, -0.049), (12, -0.051), (13, 0.011), (14, 0.031), (15, -0.023), (16, -0.104), (17, -0.066), (18, 0.018), (19, 0.108), (20, -0.016), (21, 0.01), (22, -0.016), (23, -0.019), (24, 0.051), (25, 0.037), (26, 0.027), (27, 0.012), (28, 0.024), (29, 0.059), (30, -0.075), (31, -0.041), (32, -0.119), (33, -0.103), (34, 0.108), (35, -0.1), (36, 0.008), (37, -0.061), (38, -0.077), (39, 0.054), (40, 0.048), (41, -0.02), (42, 0.196), (43, -0.111), (44, -0.021), (45, 0.027), (46, -0.227), (47, 0.024), (48, 0.193), (49, 0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95622802 <a title="103-lsi-1" href="./emnlp-2012-PATTY%3A_A_Taxonomy_of_Relational_Patterns_with_Semantic_Types.html">103 emnlp-2012-PATTY: A Taxonomy of Relational Patterns with Semantic Types</a></p>
<p>Author: Ndapandula Nakashole ; Gerhard Weikum ; Fabian Suchanek</p><p>Abstract: This paper presents PATTY: a large resource for textual patterns that denote binary relations between entities. The patterns are semantically typed and organized into a subsumption taxonomy. The PATTY system is based on efficient algorithms for frequent itemset mining and can process Web-scale corpora. It harnesses the rich type system and entity population of large knowledge bases. The PATTY taxonomy comprises 350,569 pattern synsets. Random-sampling-based evaluation shows a pattern accuracy of 84.7%. PATTY has 8,162 subsumptions, with a random-sampling-based precision of 75%. The PATTY resource is freely available for interactive access and download.</p><p>2 0.60553366 <a title="103-lsi-2" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>Author: Mohamed Yahya ; Klaus Berberich ; Shady Elbassuoni ; Maya Ramanath ; Volker Tresp ; Gerhard Weikum</p><p>Abstract: The Linked Data initiative comprises structured databases in the Semantic-Web data model RDF. Exploring this heterogeneous data by structured query languages is tedious and error-prone even for skilled users. To ease the task, this paper presents a methodology for translating natural language questions into structured SPARQL queries over linked-data sources. Our method is based on an integer linear program to solve several disambiguation tasks jointly: the segmentation of questions into phrases; the mapping of phrases to semantic entities, classes, and relations; and the construction of SPARQL triple patterns. Our solution harnesses the rich type system provided by knowledge bases in the web of linked data, to constrain our semantic-coherence objective function. We present experiments on both the . in question translation and the resulting query answering.</p><p>3 0.585697 <a title="103-lsi-3" href="./emnlp-2012-Open_Language_Learning_for_Information_Extraction.html">100 emnlp-2012-Open Language Learning for Information Extraction</a></p>
<p>Author: Mausam ; Michael Schmitz ; Stephen Soderland ; Robert Bart ; Oren Etzioni</p><p>Abstract: Open Information Extraction (IE) systems extract relational tuples from text, without requiring a pre-specified vocabulary, by identifying relation phrases and associated arguments in arbitrary sentences. However, stateof-the-art Open IE systems such as REVERB and WOE share two important weaknesses (1) they extract only relations that are mediated by verbs, and (2) they ignore context, thus extracting tuples that are not asserted as factual. This paper presents OLLIE, a substantially improved Open IE system that addresses both these limitations. First, OLLIE achieves high yield by extracting relations mediated by nouns, adjectives, and more. Second, a context-analysis step increases precision by including contextual information from the sentence in the extractions. OLLIE obtains 2.7 times the area under precision-yield curve (AUC) compared to REVERB and 1.9 times the AUC of WOEparse. –</p><p>4 0.53582567 <a title="103-lsi-4" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>Author: Ni Lao ; Amarnag Subramanya ; Fernando Pereira ; William W. Cohen</p><p>Abstract: We study how to extend a large knowledge base (Freebase) by reading relational information from a large Web text corpus. Previous studies on extracting relational knowledge from text show the potential of syntactic patterns for extraction, but they do not exploit background knowledge of other relations in the knowledge base. We describe a distributed, Web-scale implementation of a path-constrained random walk model that learns syntactic-semantic inference rules for binary relations from a graph representation of the parsed text and the knowledge base. Experiments show significant accuracy improvements in binary relation prediction over methods that consider only text, or only the existing knowledge base.</p><p>5 0.53409046 <a title="103-lsi-5" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>Author: Thomas Lin ; Mausam ; Oren Etzioni</p><p>Abstract: Entity linking systems link noun-phrase mentions in text to their corresponding Wikipedia articles. However, NLP applications would gain from the ability to detect and type all entities mentioned in text, including the long tail of entities not prominent enough to have their own Wikipedia articles. In this paper we show that once the Wikipedia entities mentioned in a corpus of textual assertions are linked, this can further enable the detection and fine-grained typing of the unlinkable entities. Our proposed method for detecting unlinkable entities achieves 24% greater accuracy than a Named Entity Recognition baseline, and our method for fine-grained typing is able to propagate over 1,000 types from linked Wikipedia entities to unlinkable entities. Detection and typing of unlinkable entities can increase yield for NLP applications such as typed question answering.</p><p>6 0.49486774 <a title="103-lsi-6" href="./emnlp-2012-Learning_Verb_Inference_Rules_from_Linguistically-Motivated_Evidence.html">80 emnlp-2012-Learning Verb Inference Rules from Linguistically-Motivated Evidence</a></p>
<p>7 0.48420492 <a title="103-lsi-7" href="./emnlp-2012-Ensemble_Semantics_for_Large-scale_Unsupervised_Relation_Extraction.html">40 emnlp-2012-Ensemble Semantics for Large-scale Unsupervised Relation Extraction</a></p>
<p>8 0.47596893 <a title="103-lsi-8" href="./emnlp-2012-Constructing_Task-Specific_Taxonomies_for_Document_Collection_Browsing.html">30 emnlp-2012-Constructing Task-Specific Taxonomies for Document Collection Browsing</a></p>
<p>9 0.3897438 <a title="103-lsi-9" href="./emnlp-2012-Identifying_Constant_and_Unique_Relations_by_using_Time-Series_Text.html">62 emnlp-2012-Identifying Constant and Unique Relations by using Time-Series Text</a></p>
<p>10 0.33904883 <a title="103-lsi-10" href="./emnlp-2012-Local_and_Global_Context_for_Supervised_and_Unsupervised_Metonymy_Resolution.html">85 emnlp-2012-Local and Global Context for Supervised and Unsupervised Metonymy Resolution</a></p>
<p>11 0.31960624 <a title="103-lsi-11" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>12 0.27978808 <a title="103-lsi-12" href="./emnlp-2012-Explore_Person_Specific_Evidence_in_Web_Person_Name_Disambiguation.html">47 emnlp-2012-Explore Person Specific Evidence in Web Person Name Disambiguation</a></p>
<p>13 0.2365551 <a title="103-lsi-13" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>14 0.23118497 <a title="103-lsi-14" href="./emnlp-2012-Towards_Efficient_Named-Entity_Rule_Induction_for_Customizability.html">125 emnlp-2012-Towards Efficient Named-Entity Rule Induction for Customizability</a></p>
<p>15 0.22481126 <a title="103-lsi-15" href="./emnlp-2012-A_Sequence_Labelling_Approach_to_Quote_Attribution.html">9 emnlp-2012-A Sequence Labelling Approach to Quote Attribution</a></p>
<p>16 0.21834444 <a title="103-lsi-16" href="./emnlp-2012-Joining_Forces_Pays_Off%3A_Multilingual_Joint_Word_Sense_Disambiguation.html">69 emnlp-2012-Joining Forces Pays Off: Multilingual Joint Word Sense Disambiguation</a></p>
<p>17 0.21637446 <a title="103-lsi-17" href="./emnlp-2012-Lyrics%2C_Music%2C_and_Emotions.html">87 emnlp-2012-Lyrics, Music, and Emotions</a></p>
<p>18 0.20278481 <a title="103-lsi-18" href="./emnlp-2012-Employing_Compositional_Semantics_and_Discourse_Consistency_in_Chinese_Event_Extraction.html">38 emnlp-2012-Employing Compositional Semantics and Discourse Consistency in Chinese Event Extraction</a></p>
<p>19 0.20153689 <a title="103-lsi-19" href="./emnlp-2012-Entity_based_QA_Retrieval.html">41 emnlp-2012-Entity based QA Retrieval</a></p>
<p>20 0.20134713 <a title="103-lsi-20" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.027), (16, 0.03), (25, 0.018), (34, 0.075), (45, 0.015), (58, 0.011), (60, 0.091), (63, 0.097), (64, 0.036), (65, 0.043), (70, 0.012), (73, 0.013), (74, 0.036), (76, 0.033), (79, 0.015), (80, 0.025), (86, 0.021), (94, 0.309), (95, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78557569 <a title="103-lda-1" href="./emnlp-2012-PATTY%3A_A_Taxonomy_of_Relational_Patterns_with_Semantic_Types.html">103 emnlp-2012-PATTY: A Taxonomy of Relational Patterns with Semantic Types</a></p>
<p>Author: Ndapandula Nakashole ; Gerhard Weikum ; Fabian Suchanek</p><p>Abstract: This paper presents PATTY: a large resource for textual patterns that denote binary relations between entities. The patterns are semantically typed and organized into a subsumption taxonomy. The PATTY system is based on efficient algorithms for frequent itemset mining and can process Web-scale corpora. It harnesses the rich type system and entity population of large knowledge bases. The PATTY taxonomy comprises 350,569 pattern synsets. Random-sampling-based evaluation shows a pattern accuracy of 84.7%. PATTY has 8,162 subsumptions, with a random-sampling-based precision of 75%. The PATTY resource is freely available for interactive access and download.</p><p>2 0.7270245 <a title="103-lda-2" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>Author: Michael J. Paul</p><p>Abstract: Recent work has explored the use of hidden Markov models for unsupervised discourse and conversation modeling, where each segment or block of text such as a message in a conversation is associated with a hidden state in a sequence. We extend this approach to allow each block of text to be a mixture of multiple classes. Under our model, the probability of a class in a text block is a log-linear function of the classes in the previous block. We show that this model performs well at predictive tasks on two conversation data sets, improving thread reconstruction accuracy by up to 15 percentage points over a standard HMM. Additionally, we show quantitatively that the induced word clusters correspond to speech acts more closely than baseline models.</p><p>3 0.50650036 <a title="103-lda-3" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>Author: Mohamed Yahya ; Klaus Berberich ; Shady Elbassuoni ; Maya Ramanath ; Volker Tresp ; Gerhard Weikum</p><p>Abstract: The Linked Data initiative comprises structured databases in the Semantic-Web data model RDF. Exploring this heterogeneous data by structured query languages is tedious and error-prone even for skilled users. To ease the task, this paper presents a methodology for translating natural language questions into structured SPARQL queries over linked-data sources. Our method is based on an integer linear program to solve several disambiguation tasks jointly: the segmentation of questions into phrases; the mapping of phrases to semantic entities, classes, and relations; and the construction of SPARQL triple patterns. Our solution harnesses the rich type system provided by knowledge bases in the web of linked data, to constrain our semantic-coherence objective function. We present experiments on both the . in question translation and the resulting query answering.</p><p>4 0.46858367 <a title="103-lda-4" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>Author: Jianxing Yu ; Zheng-Jun Zha ; Tat-Seng Chua</p><p>Abstract: This paper proposes to generate appropriate answers for opinion questions about products by exploiting the hierarchical organization of consumer reviews. The hierarchy organizes product aspects as nodes following their parent-child relations. For each aspect, the reviews and corresponding opinions on this aspect are stored. We develop a new framework for opinion Questions Answering, which enables accurate question analysis and effective answer generation by making use the hierarchy. In particular, we first identify the (explicit/implicit) product aspects asked in the questions and their sub-aspects by referring to the hierarchy. We then retrieve the corresponding review fragments relevant to the aspects from the hierarchy. In order to gener- ate appropriate answers from the review fragments, we develop a multi-criteria optimization approach for answer generation by simultaneously taking into account review salience, coherence, diversity, and parent-child relations among the aspects. We conduct evaluations on 11 popular products in four domains. The evaluated corpus contains 70,359 consumer reviews and 220 questions on these products. Experimental results demonstrate the effectiveness of our approach.</p><p>5 0.46591401 <a title="103-lda-5" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms ofweak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.</p><p>6 0.46264902 <a title="103-lda-6" href="./emnlp-2012-Type-Supervised_Hidden_Markov_Models_for_Part-of-Speech_Tagging_with_Incomplete_Tag_Dictionaries.html">129 emnlp-2012-Type-Supervised Hidden Markov Models for Part-of-Speech Tagging with Incomplete Tag Dictionaries</a></p>
<p>7 0.45968658 <a title="103-lda-7" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>8 0.45920542 <a title="103-lda-8" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>9 0.45692408 <a title="103-lda-9" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>10 0.4566718 <a title="103-lda-10" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>11 0.45605597 <a title="103-lda-11" href="./emnlp-2012-An_%22AI_readability%22_Formula_for_French_as_a_Foreign_Language.html">17 emnlp-2012-An "AI readability" Formula for French as a Foreign Language</a></p>
<p>12 0.45525995 <a title="103-lda-12" href="./emnlp-2012-Open_Language_Learning_for_Information_Extraction.html">100 emnlp-2012-Open Language Learning for Information Extraction</a></p>
<p>13 0.45423937 <a title="103-lda-13" href="./emnlp-2012-Besting_the_Quiz_Master%3A_Crowdsourcing_Incremental_Classification_Games.html">23 emnlp-2012-Besting the Quiz Master: Crowdsourcing Incremental Classification Games</a></p>
<p>14 0.45137954 <a title="103-lda-14" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>15 0.45123258 <a title="103-lda-15" href="./emnlp-2012-N-gram-based_Tense_Models_for_Statistical_Machine_Translation.html">95 emnlp-2012-N-gram-based Tense Models for Statistical Machine Translation</a></p>
<p>16 0.45116854 <a title="103-lda-16" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>17 0.45047328 <a title="103-lda-17" href="./emnlp-2012-Revisiting_the_Predictability_of_Language%3A_Response_Completion_in_Social_Media.html">114 emnlp-2012-Revisiting the Predictability of Language: Response Completion in Social Media</a></p>
<p>18 0.44974744 <a title="103-lda-18" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>19 0.4493278 <a title="103-lda-19" href="./emnlp-2012-A_Phrase-Discovering_Topic_Model_Using_Hierarchical_Pitman-Yor_Processes.html">8 emnlp-2012-A Phrase-Discovering Topic Model Using Hierarchical Pitman-Yor Processes</a></p>
<p>20 0.44820428 <a title="103-lda-20" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
