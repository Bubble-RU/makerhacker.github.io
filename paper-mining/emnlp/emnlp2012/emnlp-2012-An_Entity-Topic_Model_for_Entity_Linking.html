<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>19 emnlp-2012-An Entity-Topic Model for Entity Linking</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-19" href="#">emnlp2012-19</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>19 emnlp-2012-An Entity-Topic Model for Entity Linking</h1>
<br/><p>Source: <a title="emnlp-2012-19-pdf" href="http://aclweb.org/anthology//D/D12/D12-1010.pdf">pdf</a></p><p>Author: Xianpei Han ; Le Sun</p><p>Abstract: Entity Linking (EL) has received considerable attention in recent years. Given many name mentions in a document, the goal of EL is to predict their referent entities in a knowledge base. Traditionally, there have been two distinct directions of EL research: one focusing on the effects of mention’s context compatibility, assuming that “the referent entity of a mention is reflected by its context”; the other dealing with the effects of document’s topic coherence, assuming that “a mention ’s referent entity should be coherent with the document’ ’s main topics”. In this paper, we propose a generative model called entitytopic model, to effectively join the above two complementary directions together. By jointly modeling and exploiting the context compatibility, the topic coherence and the correlation between them, our model can – accurately link all mentions in a document using both the local information (including the words and the mentions in a document) and the global knowledge (including the topic knowledge, the entity context knowledge and the entity name knowledge). Experimental results demonstrate the effectiveness of the proposed model. 1</p><p>Reference: <a title="emnlp-2012-19-reference" href="../emnlp2012_reference/emnlp-2012-An_Entity-Topic_Model_for_Entity_Linking_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Given many name mentions in a document, the goal of EL is to predict their referent entities in a knowledge base. [sent-6, score-0.684]
</p><p>2 Given many name mentions in a document, the goal of EL is to predict their referent entities in a given knowledge base (KB), such as the Wikipedia1. [sent-13, score-0.714]
</p><p>3 org 105 shown in Figure 1, an EL system should identify the referent entities of the three mentions WWDC, Apple and Lion correspondingly are the entities Apple Worldwide Developers Conference, Apple Inc. [sent-16, score-0.716]
</p><p>4 For instance, in many applications we need to collect all appearances of a specific entity in different documents, EL is an effective way to resolve such an information integration problem. [sent-19, score-0.314]
</p><p>5 Traditionally, there have been two distinct directions in EL to resolve the name ambiguity problem: one focusing on the effects of mention’ s context compatibility and the other dealing with the effects of document’s topic coherence. [sent-29, score-0.904]
</p><p>6 lc L2a0n1g2ua Agseso Pcrioactieosnsi fnogr a Cnodm Cpoumtaptiuotna tilo Lnianlg Nuaist uircasl compatibility assume that “the referent entity of a mention is reflected by its context”(Mihalcea & Cosomai, 2007; Zhang et al. [sent-34, score-1.004]
</p><p>7 For example, the context compatibility based methods will identify the referent entity of the mention Lion in Figure 1 is the entity Mac OS X Lion, since this entity is more compatible with its context words operating system and release than other candidates such as Lion(big cats) or Lion(band). [sent-38, score-1.766]
</p><p>8 EL methods based on topic coherence assume that “a mention ’s referent entity should be coherent with document’ ’s main topics” (Medelyan  et al. [sent-39, score-1.111]
</p><p>9 For example, the topic coherence based methods will link the mention Apple in Figure 1 to the entity Apple Inc. [sent-43, score-0.932]
</p><p>10 , since it is more coherent with the document’s topic MAC OS X Lion Release than other referent candidates such as Apple (band) or Apple Bank. [sent-44, score-0.495]
</p><p>11 , the context compatibility and the topic coherence are first separately modeled, then their EL evidence are combined through an additional model. [sent-51, score-0.88]
</p><p>12 The main drawback of these hybrid methods, however, is that they model the context compatibility and the topic coherence separately, which makes it difficult to capture the mutual reinforcement effect between the above two directions. [sent-53, score-0.942]
</p><p>13 That is, the topic coherence and the context compatibility are highly correlated and their evidence can be used to reinforce each other in EL decisions. [sent-54, score-0.905]
</p><p>14 For example, in Figure 1, if the context compatibility gives a high likelihood the mention Apple refers to the entity Apple Inc. [sent-55, score-0.857]
</p><p>15 , then this likelihood will give more evidence for this 106 document’s topic is about MAC OS X Lion, and it in turn will reinforce the topic coherence between the entity MAC OS X Lion and the document. [sent-56, score-1.08]
</p><p>16 In reverse, once we known the topic of this document is about MAC OS X Lion, the context compatibility between the mention Apple and the entity Apple Inc. [sent-57, score-1.263]
</p><p>17 can be improved as the importance of the context words operating system and release will be increased using the topic knowledge. [sent-58, score-0.348]
</p><p>18 In this way, we believe that modeling the above two directions jointly, rather than separately, will further improve the EL performance by capturing the mutual  reinforcement effect between the context compatibility and the topic coherence. [sent-59, score-0.789]
</p><p>19 In this paper, we propose a method to jointly model and exploit the context compatibility, the topic coherence and the correlation between them for better EL performance. [sent-60, score-0.588]
</p><p>20 tends to occur in documents about IT, but the entity Apple Bank will more likely to occur in documents about bank or investment. [sent-63, score-0.37]
</p><p>21 2) Context compatibility assumption: The context words of a mention should be centered on its referent entity. [sent-64, score-0.757]
</p><p>22 For example, the words computer, phone and music tends to occur in the context of the entity Apple Inc. [sent-65, score-0.381]
</p><p>23 , meanwhile the words loan, invest and deposit will more likely to –  occur in the context of the entity Apple Bank. [sent-66, score-0.381]
</p><p>24 And the EL problem can now be decomposed into the following two inference tasks: 1) Predicting the underlying topics and the underlying entities of a document based on the observed information and the global knowledge. [sent-68, score-0.361]
</p><p>25 Notice that the topic knowledge, the entity name knowledge and the entity context knowledge are all not previously given, thus we need to estimate them from data. [sent-70, score-1.211]
</p><p>26 We propose a generative probabilistic model, the entity-topic model, which can jointly model and exploit the context compatibility, the topic coherence and the correlation between them for better EL performance; ? [sent-75, score-0.618]
</p><p>27 In following we first demonstrate how to capture the context compatibility, the topic coherence and the correlation between them in the document generative process, then we incorporate  the global knowledge generation into our model for knowledge estimation from data. [sent-84, score-0.887]
</p><p>28 1  Document Generative Process  As shown in Section 1, we jointly model the context compatibility and the topic coherence as the statistical dependencies in the entity-topic model by assuming that all documents are generated in a topical coherent and context 107 we describe the  compatible way. [sent-86, score-1.002]
</p><p>29 Formally, we represent a document as: A document is a collection of M mentions and N words, denoted as d = {m1, mM; w1, wN}, with mi the ith mention and wj the jth word. [sent-91, score-0.534]
</p><p>30 Topic Knowledge Á (The entity distribution of topics): In our model, all entities in a document are generated based on its underlying topics, with each topic is a group of semantically related entities. [sent-94, score-0.905]
</p><p>31 Statistically, we model each topic as a multinomial distribution of entities, with the probability indicating the likelihood an entity to be extracted from this topic. [sent-95, score-0.639]
</p><p>32 08, …}, indicating the likelihood of the entity Steve Jobs be extracted from this topic is 0. [sent-99, score-0.595]
</p><p>33 Entity Name Knowledge Ã(The name distribution of entities): In our model, all name mentions are generated using the name knowledge of its referent entity. [sent-102, score-0.789]
</p><p>34 Specifically, we model the name knowledge of an entity as a multinomial distribution of its names, with the probability indicating the likelihood this entity is mentioned by the name. [sent-103, score-0.84]
</p><p>35 For example, the name knowledge of the entity Apple Inc. [sent-104, score-0.482]
</p><p>36 Entity Context Knowledge (The context word distribution of entities): In our model, all …  »  context words of an entity’s mention are generated using its context knowledge. [sent-114, score-0.368]
</p><p>37 Concretely, we model the context knowledge of an entity as a multinomial distribution of words, with the probability indicating the likelihood a word appearing in this entity’s context. [sent-115, score-0.52]
</p><p>38 002, }, indicating that the word computer appearing in the context of the entity Apple Inc. [sent-120, score-0.409]
</p><p>39 To demonstrate the generation process, we also demonstrate how the document in Figure 1 can be generated using our model in following steps: Step 1: The model generates the topic distribution of the document as μμdd = {Apple Inc. [sent-124, score-0.605]
</p><p>40 According to the topic distribution μμdd, the model generates their topic assignments as z1=Apple Inc. [sent-128, score-0.775]
</p><p>41 According to the topic knowledge ÁApple , ÁOS and the topic assignments z1, z2, z3, the model generates their entity assignments as e1 = Apple Worldwide Developers Conference, e2 = Apple Inc. [sent-131, score-1.251]
</p><p>42 According to the referent entity set in document ed = {Apple Worldwide Developers Conference, Apple Inc. [sent-136, score-0.653]
</p><p>43 , Mac OS X Lion }, the model generates the target entity they describes as 108 Conference and  a3=Apple Worldwide Developers a4=Apple Inc. [sent-137, score-0.344]
</p><p>44 According to their target entity and the context knowledge of these entities, the model generates the context words in the document. [sent-139, score-0.545]
</p><p>45 For example, according to the context knowledge of the entities Apple Worldwide Developers Conference, the model generates its context word w3 =conference, and according to the context knowledge of the entity Apple Inc. [sent-140, score-0.82]
</p><p>46 Furthermore, the generation of topics, entities, mentions and words are highly correlated, thus our model can capture the correlation between the topic coherence and the context compatibility. [sent-143, score-0.722]
</p><p>47 2  Global Knowledge Generative Process  The entity-topic model relies on three types of global knowledge (including the topic knowledge, the entity name knowledge and the entity context knowledge) to generate a document. [sent-145, score-1.248]
</p><p>48 3  Inference using Gibbs Sampling  In this section, we describe how to resolve the entity linking problem using the entity-topic model. [sent-155, score-0.383]
</p><p>49 Given a document d, predicting its entity assignments (eedd for mentions and aadd for words) and topic assignments ( zzdd ). [sent-157, score-1.184]
</p><p>50 Notice that here the EL decisions are just the prediction of per-mention entity assignments (eedd). [sent-158, score-0.453]
</p><p>51 Given a corpus D={d1, d2, dD}, estimating the global knowledge (including the entity distribution of topics Á, the name distribution Ãand the context word distribution of entities) from data. [sent-160, score-0.776]
</p><p>52 In Gibbs sampling, we first construct the posterior distribution PP((zz;; ee;; aajjDD)) , then this posterior distribution is used to: 1) estimate μ, Á,Ã and and 2) predict the entities and the topics of all documents in D. [sent-165, score-0.315]
</p><p>53 1)  is the probability of the joint topic assignment z to all mentions m in corpus D, and  PP((e j z ) = = ( ¡ ¡ ( ( E¯E¯) ¯ EE)) )TTttYY=TT=11QQ¡¡e( eEE¡¡(¯ (¯ ++ ++ CC CCtTTt¤ tTtTEEeeEE)) )  (3. [sent-169, score-0.495]
</p><p>54 2)  is the conditional probability of the joint entity assignments e to all mentions m in corpus D given all topic assignments z, and  PP((mmjje ) = = ( (¡ ¡ ( ( K°K°) ° KK)) )EEeeYY=EE=11QQ¡¡mm((KK¡¡(° (° ++ ++ CC CCeEEe¤ eEeEMMmmMM)) )  (3. [sent-170, score-1.034]
</p><p>55 3)  is the conditional probability of all mentions mm given all per-mention entity assignments e, and  PP((a j e ) = dYdY= DD11e YY½½eed ¡ CCCCdDd DDdDe¤e¤EEEE¢¢CCddDDeeAA  (3. [sent-171, score-0.654]
</p><p>56 4)  is the conditional probability of the joint entity assignments a to all words w in corpus D given all per-mention entity assignments e, and  PP((wwjja ) = ( ¡ ¡ ( ( V± V) ± VV)) )EEYYeeE=E=1 QQ¡¡ww((VV¡¡ ±( ±(± + + CC CCeEEe¤ eEeEWWwwWW)) )  (3. [sent-172, score-0.906]
</p><p>57 5)  is the conditional probability of all words ww given all per-word entity assignments a . [sent-173, score-0.485]
</p><p>58 In all above formulas, ¡¡((::)) is the Gamma function, CCDdDdttTT is the times topic t has been assigned for all mentions in document d, CCDdDd¤¤TT == tt CCDdDdttTT is the topic number in document d, and CCTttTeeEE, CCEEeemmMM,CCdDdDeeEE, CCDddDeeAA, CCEeEewwWW have similar explanation. [sent-174, score-1.037]
</p><p>59 For entity-topic model, each state in the Markov chain is an assignment (including topic assignment to a mention, entity assignment to a mention and entity assignment to a word). [sent-176, score-1.244]
</p><p>60 Finally, using the above three conditional distributions, we iteratively update all assignments of corpus D until coverage, then the global knowledge is estimated using the final assignments, and the final entity assignments are used as the referents of their corresponding mentions. [sent-182, score-0.696]
</p><p>61 , we iteratively update the entity assignments and the topic assignments of an unseen document as  ° °  the same as the above inference process, but with the previously learned global knowledge fixed. [sent-187, score-1.102]
</p><p>62 For we notice that KK°° is the number of pseudo names added to each entity, when == 00 our model only mentions an entity using its previously used names. [sent-190, score-0.475]
</p><p>63 Observed that an entity typically has a fixed set of names, we set to a small value by setting KK°° == 11::00. [sent-191, score-0.314]
</p><p>64 As there is typically a relatively loose correlation between an entity and its context words, we set to a relatively large value by fixing the total smoothing words added to each entity, a typical value is VV ±± = 2000. [sent-193, score-0.415]
</p><p>65 ) as entities, so the entity in this paper may not strictly follow its definition. [sent-204, score-0.314]
</p><p>66 For each document, the name mentions’ referent entities in Wikipedia are manually annotated to be as exhaustive as possible. [sent-215, score-0.456]
</p><p>67 In total, 17,200 name mentions are annotated, with 161 name mentions per document on average. [sent-216, score-0.649]
</p><p>68 In our experiments, we use only the name mentions whose referent entities are contained in Wikipedia. [sent-217, score-0.617]
</p><p>69 This is a context compatibility based EL method using vector space model (Mihalcea & Csomai, 2007). [sent-231, score-0.42]
</p><p>70 computes the context compatibility using the word overlap between the mention’s context and the entity’s Wikipedia entry. [sent-233, score-0.487]
</p><p>71 This is a statistical context compatibility based EL method described in Han & Sun(201 1), which computes the compatibility by integrating the evidence from the entity popularity, the entity name knowledge and the context word distribution of entities. [sent-235, score-1.68]
</p><p>72 ; This is a relational topic coherence based EL method described in Milne & Witten(2008). [sent-237, score-0.46]
</p><p>73 M&W; measures an entity’s topic coherence to a document as its average semantic relatedness to the unambiguous entities in the document. [sent-238, score-0.726]
</p><p>74 This is an EL method which combines context compatibility and topic coherence using a hybrid method (Kulkarni et al. [sent-240, score-0.915]
</p><p>75 Except for CSA W and EL-Graph, all other baselines are designed only to link the salient name mentions (i. [sent-245, score-0.326]
</p><p>76 e 65387t19203 From the overall results in Table 1, we can see that:  1) By jointly modeling and exploiting the context compatibility and the topic coherence, our method can achieve competitive performance: ○1 compared with the context compatibility baselines Wikify! [sent-270, score-1.148]
</p><p>77 and EM-Model, our method correspondingly gets 43% and 19% F1 improvement; ○2 compared with the topic coherence baselines M&W;, our method achieves 28% F1 improvement; ③ compared with the hybrid baselines CSA W and EL-Graph, our method correspondingly achieves 11% and 7% F1 improvement. [sent-271, score-0.613]
</p><p>78 Generally, we believe the main advantages of our method are: 1) The effects of topic knowledge. [sent-286, score-0.342]
</p><p>79 One main advantage of our model is that the topic knowledge can provide a document-specific entity prior for EL. [sent-287, score-0.662]
</p><p>80 Concretely, using the topic knowledge and the topic distribution of documents, the prior for an entity appearing in a document d is highly related to the document’s topics: PP((eejjdd)) == PP((zzjjdd))PP((eejjzz)) This prior is obviously more reasonable than the “information less prior” (i. [sent-288, score-1.14]
</p><p>81 , all entities have equal  zz  prior) or “a global entity popularity prior” (Han & Sun, 2011). [sent-290, score-0.551]
</p><p>82 We can see that the topic knowledge can provide a reasonable prior for entities appearing in a document: the Apple Inc. [sent-293, score-0.517]
</p><p>83 838 correspondingly), we believe this is because: ○1 The mentions to be linked in TAC data set are  mostly salient mentions; ○2 The influence of the NIL referent entity problem, i. [sent-315, score-0.745]
</p><p>84 , the referent entity is not contained in the given knowledge base: Most referent entities (67. [sent-317, score-0.95]
</p><p>85 5%) on TAC 2009 are NIL entity and our method has no special handling on this problem, rather than other methods such as the EM-Model, which affects the overall performance of our method. [sent-318, score-0.314]
</p><p>86 Traditionally, the context compatibility based methods link a mention to the entity which has the largest compatibility with it. [sent-321, score-1.245]
</p><p>87 Cucerzan (2007) modeled the compatibility as the cosine similarity between the vector space representation  of mention’s context and of entity’s Wikipedia entry. [sent-322, score-0.454]
</p><p>88 (201 1) extended the vector space model with more information such as the entity category and the acronym expansion, etc. [sent-326, score-0.314]
</p><p>89 Han & Sun (201 1) proposed a generative model which computes the compatibility using the evidences from entity’s popularity, name distribution and context word distribution. [sent-327, score-0.595]
</p><p>90 (201 1) and Sen (2012) used a latent topic model to learn the context model of entities. [sent-329, score-0.348]
</p><p>91 On the other side, the topic coherence based methods link a mention to the entity which are most coherent to the document containing it. [sent-335, score-1.057]
</p><p>92 (2008) measured the topic coherence of an entity to a document as the weighted average of its relatedness to the unambiguous entities in the document. [sent-337, score-1.04]
</p><p>93 Bhattacharya and Getoor (2006)  modeled the topic coherence as the likelihood an entity is generated from the latent topics of a document. [sent-340, score-0.866]
</p><p>94 Sen (2012) modeled the topic coherence as the groups of co-occurring entities. [sent-341, score-0.494]
</p><p>95 (2009) modeled the topic coherence as the sum of all pair-wise relatedness between the referent entities of a document. [sent-343, score-0.849]
</p><p>96 (201 1) modeled the topic coherence of an entity as its node importance in a graph which captures all mention-entity and entity-entity relations in a document. [sent-346, score-0.808]
</p><p>97 6  Conclusions and Future Work  This paper proposes a generative model, topic model, for entity linking. [sent-347, score-0.625]
</p><p>98 By modeling context compatibility, topic and the correlation between them as  the entityuniformly coherence statistical  114 dependencies, our model provides an effective way to jointly exploit them for better EL performance. [sent-348, score-0.588]
</p><p>99 In this paper, the entity-topic model can only link mentions to the previously given entities in a knowledge base. [sent-349, score-0.404]
</p><p>100 For future work, we want to overcome this limit by incorporating an entity discovery ability into our model, so that it can also  discover and learn the knowledge of previously unseen entities from a corpus for linking name mentions to these entities. [sent-350, score-0.853]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('apple', 0.377), ('compatibility', 0.353), ('entity', 0.314), ('topic', 0.281), ('el', 0.264), ('referent', 0.214), ('lion', 0.202), ('coherence', 0.179), ('mentions', 0.161), ('dd', 0.16), ('entities', 0.141), ('pp', 0.139), ('assignments', 0.139), ('document', 0.125), ('mention', 0.123), ('ee', 0.103), ('name', 0.101), ('mac', 0.088), ('han', 0.081), ('tac', 0.081), ('os', 0.081), ('eejjzz', 0.076), ('worldwide', 0.076), ('linking', 0.069), ('context', 0.067), ('knowledge', 0.067), ('developers', 0.065), ('tt', 0.064), ('kulkarni', 0.064), ('correspondingly', 0.059), ('zz', 0.059), ('topics', 0.058), ('jj', 0.057), ('assignment', 0.053), ('wikipedia', 0.052), ('cccc', 0.05), ('ddiirr', 0.05), ('entitytopic', 0.05), ('kataria', 0.05), ('wwdc', 0.05), ('medelyan', 0.049), ('milne', 0.048), ('aa', 0.045), ('wikify', 0.045), ('gibbs', 0.045), ('distribution', 0.044), ('eedd', 0.043), ('sen', 0.043), ('mm', 0.04), ('vv', 0.04), ('mcnamee', 0.039), ('nil', 0.039), ('aajjdd', 0.038), ('band', 0.038), ('csa', 0.038), ('eeii', 0.038), ('iitb', 0.038), ('global', 0.037), ('doc', 0.036), ('sun', 0.036), ('link', 0.035), ('witten', 0.035), ('hybrid', 0.035), ('correlation', 0.034), ('directions', 0.034), ('modeled', 0.034), ('kk', 0.034), ('effects', 0.034), ('sampling', 0.033), ('ww', 0.032), ('cc', 0.03), ('generative', 0.03), ('base', 0.03), ('generates', 0.03), ('hyperparameter', 0.029), ('salient', 0.029), ('ii', 0.028), ('appearing', 0.028), ('documents', 0.028), ('dirichlet', 0.028), ('jointly', 0.027), ('csomai', 0.027), ('believe', 0.027), ('reinforcement', 0.027), ('collective', 0.026), ('aadd', 0.025), ('aapppplleeiinncc', 0.025), ('bhattacharya', 0.025), ('ccdddd', 0.025), ('ccddddeeaa', 0.025), ('ccddddtttt', 0.025), ('cceeee', 0.025), ('eeddjj', 0.025), ('gottipati', 0.025), ('mmdd', 0.025), ('mmddjjeedd', 0.025), ('reinforce', 0.025), ('wwdd', 0.025), ('traditionally', 0.025), ('dang', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="19-tfidf-1" href="./emnlp-2012-An_Entity-Topic_Model_for_Entity_Linking.html">19 emnlp-2012-An Entity-Topic Model for Entity Linking</a></p>
<p>Author: Xianpei Han ; Le Sun</p><p>Abstract: Entity Linking (EL) has received considerable attention in recent years. Given many name mentions in a document, the goal of EL is to predict their referent entities in a knowledge base. Traditionally, there have been two distinct directions of EL research: one focusing on the effects of mention’s context compatibility, assuming that “the referent entity of a mention is reflected by its context”; the other dealing with the effects of document’s topic coherence, assuming that “a mention ’s referent entity should be coherent with the document’ ’s main topics”. In this paper, we propose a generative model called entitytopic model, to effectively join the above two complementary directions together. By jointly modeling and exploiting the context compatibility, the topic coherence and the correlation between them, our model can – accurately link all mentions in a document using both the local information (including the words and the mentions in a document) and the global knowledge (including the topic knowledge, the entity context knowledge and the entity name knowledge). Experimental results demonstrate the effectiveness of the proposed model. 1</p><p>2 0.20661393 <a title="19-tfidf-2" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>Author: Thomas Lin ; Mausam ; Oren Etzioni</p><p>Abstract: Entity linking systems link noun-phrase mentions in text to their corresponding Wikipedia articles. However, NLP applications would gain from the ability to detect and type all entities mentioned in text, including the long tail of entities not prominent enough to have their own Wikipedia articles. In this paper we show that once the Wikipedia entities mentioned in a corpus of textual assertions are linked, this can further enable the detection and fine-grained typing of the unlinkable entities. Our proposed method for detecting unlinkable entities achieves 24% greater accuracy than a Named Entity Recognition baseline, and our method for fine-grained typing is able to propagate over 1,000 types from linked Wikipedia entities to unlinkable entities. Detection and typing of unlinkable entities can increase yield for NLP applications such as typed question answering.</p><p>3 0.19692859 <a title="19-tfidf-3" href="./emnlp-2012-Learning-based_Multi-Sieve_Co-reference_Resolution_with_Knowledge.html">76 emnlp-2012-Learning-based Multi-Sieve Co-reference Resolution with Knowledge</a></p>
<p>Author: Lev Ratinov ; Dan Roth</p><p>Abstract: We explore the interplay of knowledge and structure in co-reference resolution. To inject knowledge, we use a state-of-the-art system which cross-links (or “grounds”) expressions in free text to Wikipedia. We explore ways of using the resulting grounding to boost the performance of a state-of-the-art co-reference resolution system. To maximize the utility of the injected knowledge, we deploy a learningbased multi-sieve approach and develop novel entity-based features. Our end system outperforms the state-of-the-art baseline by 2 B3 F1 points on non-transcript portion of the ACE 2004 dataset.</p><p>4 0.1846831 <a title="19-tfidf-4" href="./emnlp-2012-Explore_Person_Specific_Evidence_in_Web_Person_Name_Disambiguation.html">47 emnlp-2012-Explore Person Specific Evidence in Web Person Name Disambiguation</a></p>
<p>Author: Liwei Chen ; Yansong Feng ; Lei Zou ; Dongyan Zhao</p><p>Abstract: In this paper, we investigate different usages of feature representations in the web person name disambiguation task which has been suffering from the mismatch of vocabulary and lack of clues in web environments. In literature, the latter receives less attention and remains more challenging. We explore the feature space in this task and argue that collecting person specific evidences from a corpus level can provide a more reasonable and robust estimation for evaluating a feature’s importance in a given web page. This can alleviate the lack of clues where discriminative features can be reasonably weighted by taking their corpus level importance into account, not just relying on the current local context. We therefore propose a topic-based model to exploit the person specific global importance and embed it into the person name similarity. The experimental results show that the corpus level topic in- formation provides more stable evidences for discriminative features and our method outperforms the state-of-the-art systems on three WePS datasets.</p><p>5 0.17434064 <a title="19-tfidf-5" href="./emnlp-2012-Exploring_Topic_Coherence_over_Many_Models_and_Many_Topics.html">49 emnlp-2012-Exploring Topic Coherence over Many Models and Many Topics</a></p>
<p>Author: Keith Stevens ; Philip Kegelmeyer ; David Andrzejewski ; David Buttler</p><p>Abstract: We apply two new automated semantic evaluations to three distinct latent topic models. Both metrics have been shown to align with human evaluations and provide a balance between internal measures of information gain and comparisons to human ratings of coherent topics. We improve upon the measures by introducing new aggregate measures that allows for comparing complete topic models. We further compare the automated measures to other metrics for topic models, comparison to manually crafted semantic tests and document classification. Our experiments reveal that LDA and LSA each have different strengths; LDA best learns descriptive topics while LSA is best at creating a compact semantic representation ofdocuments and words in a corpus.</p><p>6 0.14767028 <a title="19-tfidf-6" href="./emnlp-2012-Modelling_Sequential_Text_with_an_Adaptive_Topic_Model.html">90 emnlp-2012-Modelling Sequential Text with an Adaptive Topic Model</a></p>
<p>7 0.13935721 <a title="19-tfidf-7" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>8 0.13243994 <a title="19-tfidf-8" href="./emnlp-2012-A_Phrase-Discovering_Topic_Model_Using_Hierarchical_Pitman-Yor_Processes.html">8 emnlp-2012-A Phrase-Discovering Topic Model Using Hierarchical Pitman-Yor Processes</a></p>
<p>9 0.13157649 <a title="19-tfidf-9" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>10 0.12865347 <a title="19-tfidf-10" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>11 0.12303295 <a title="19-tfidf-11" href="./emnlp-2012-Entity_based_QA_Retrieval.html">41 emnlp-2012-Entity based QA Retrieval</a></p>
<p>12 0.11094415 <a title="19-tfidf-12" href="./emnlp-2012-SSHLDA%3A_A_Semi-Supervised_Hierarchical_Topic_Model.html">115 emnlp-2012-SSHLDA: A Semi-Supervised Hierarchical Topic Model</a></p>
<p>13 0.10480966 <a title="19-tfidf-13" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>14 0.088731959 <a title="19-tfidf-14" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>15 0.086456582 <a title="19-tfidf-15" href="./emnlp-2012-Monte_Carlo_MCMC%3A_Efficient_Inference_by_Approximate_Sampling.html">91 emnlp-2012-Monte Carlo MCMC: Efficient Inference by Approximate Sampling</a></p>
<p>16 0.086022355 <a title="19-tfidf-16" href="./emnlp-2012-Name_Phylogeny%3A_A_Generative_Model_of_String_Variation.html">96 emnlp-2012-Name Phylogeny: A Generative Model of String Variation</a></p>
<p>17 0.08160165 <a title="19-tfidf-17" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>18 0.075573862 <a title="19-tfidf-18" href="./emnlp-2012-Joint_Learning_for_Coreference_Resolution_with_Markov_Logic.html">73 emnlp-2012-Joint Learning for Coreference Resolution with Markov Logic</a></p>
<p>19 0.071361557 <a title="19-tfidf-19" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>20 0.063266829 <a title="19-tfidf-20" href="./emnlp-2012-Discovering_Diverse_and_Salient_Threads_in_Document_Collections.html">33 emnlp-2012-Discovering Diverse and Salient Threads in Document Collections</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.226), (1, 0.3), (2, 0.054), (3, -0.058), (4, -0.308), (5, 0.027), (6, -0.039), (7, -0.009), (8, -0.165), (9, -0.069), (10, -0.015), (11, -0.063), (12, 0.138), (13, 0.008), (14, 0.081), (15, 0.054), (16, 0.221), (17, 0.046), (18, -0.001), (19, 0.1), (20, 0.031), (21, -0.003), (22, 0.001), (23, 0.034), (24, -0.125), (25, 0.079), (26, -0.001), (27, -0.083), (28, -0.024), (29, -0.146), (30, 0.019), (31, 0.038), (32, 0.051), (33, 0.125), (34, 0.165), (35, 0.056), (36, -0.103), (37, 0.028), (38, -0.021), (39, -0.026), (40, 0.024), (41, 0.053), (42, -0.009), (43, -0.078), (44, 0.04), (45, 0.01), (46, -0.05), (47, -0.002), (48, 0.041), (49, 0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98436302 <a title="19-lsi-1" href="./emnlp-2012-An_Entity-Topic_Model_for_Entity_Linking.html">19 emnlp-2012-An Entity-Topic Model for Entity Linking</a></p>
<p>Author: Xianpei Han ; Le Sun</p><p>Abstract: Entity Linking (EL) has received considerable attention in recent years. Given many name mentions in a document, the goal of EL is to predict their referent entities in a knowledge base. Traditionally, there have been two distinct directions of EL research: one focusing on the effects of mention’s context compatibility, assuming that “the referent entity of a mention is reflected by its context”; the other dealing with the effects of document’s topic coherence, assuming that “a mention ’s referent entity should be coherent with the document’ ’s main topics”. In this paper, we propose a generative model called entitytopic model, to effectively join the above two complementary directions together. By jointly modeling and exploiting the context compatibility, the topic coherence and the correlation between them, our model can – accurately link all mentions in a document using both the local information (including the words and the mentions in a document) and the global knowledge (including the topic knowledge, the entity context knowledge and the entity name knowledge). Experimental results demonstrate the effectiveness of the proposed model. 1</p><p>2 0.67842495 <a title="19-lsi-2" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>Author: Thomas Lin ; Mausam ; Oren Etzioni</p><p>Abstract: Entity linking systems link noun-phrase mentions in text to their corresponding Wikipedia articles. However, NLP applications would gain from the ability to detect and type all entities mentioned in text, including the long tail of entities not prominent enough to have their own Wikipedia articles. In this paper we show that once the Wikipedia entities mentioned in a corpus of textual assertions are linked, this can further enable the detection and fine-grained typing of the unlinkable entities. Our proposed method for detecting unlinkable entities achieves 24% greater accuracy than a Named Entity Recognition baseline, and our method for fine-grained typing is able to propagate over 1,000 types from linked Wikipedia entities to unlinkable entities. Detection and typing of unlinkable entities can increase yield for NLP applications such as typed question answering.</p><p>3 0.61118031 <a title="19-lsi-3" href="./emnlp-2012-Exploring_Topic_Coherence_over_Many_Models_and_Many_Topics.html">49 emnlp-2012-Exploring Topic Coherence over Many Models and Many Topics</a></p>
<p>Author: Keith Stevens ; Philip Kegelmeyer ; David Andrzejewski ; David Buttler</p><p>Abstract: We apply two new automated semantic evaluations to three distinct latent topic models. Both metrics have been shown to align with human evaluations and provide a balance between internal measures of information gain and comparisons to human ratings of coherent topics. We improve upon the measures by introducing new aggregate measures that allows for comparing complete topic models. We further compare the automated measures to other metrics for topic models, comparison to manually crafted semantic tests and document classification. Our experiments reveal that LDA and LSA each have different strengths; LDA best learns descriptive topics while LSA is best at creating a compact semantic representation ofdocuments and words in a corpus.</p><p>4 0.60341191 <a title="19-lsi-4" href="./emnlp-2012-Explore_Person_Specific_Evidence_in_Web_Person_Name_Disambiguation.html">47 emnlp-2012-Explore Person Specific Evidence in Web Person Name Disambiguation</a></p>
<p>Author: Liwei Chen ; Yansong Feng ; Lei Zou ; Dongyan Zhao</p><p>Abstract: In this paper, we investigate different usages of feature representations in the web person name disambiguation task which has been suffering from the mismatch of vocabulary and lack of clues in web environments. In literature, the latter receives less attention and remains more challenging. We explore the feature space in this task and argue that collecting person specific evidences from a corpus level can provide a more reasonable and robust estimation for evaluating a feature’s importance in a given web page. This can alleviate the lack of clues where discriminative features can be reasonably weighted by taking their corpus level importance into account, not just relying on the current local context. We therefore propose a topic-based model to exploit the person specific global importance and embed it into the person name similarity. The experimental results show that the corpus level topic in- formation provides more stable evidences for discriminative features and our method outperforms the state-of-the-art systems on three WePS datasets.</p><p>5 0.59422207 <a title="19-lsi-5" href="./emnlp-2012-Learning-based_Multi-Sieve_Co-reference_Resolution_with_Knowledge.html">76 emnlp-2012-Learning-based Multi-Sieve Co-reference Resolution with Knowledge</a></p>
<p>Author: Lev Ratinov ; Dan Roth</p><p>Abstract: We explore the interplay of knowledge and structure in co-reference resolution. To inject knowledge, we use a state-of-the-art system which cross-links (or “grounds”) expressions in free text to Wikipedia. We explore ways of using the resulting grounding to boost the performance of a state-of-the-art co-reference resolution system. To maximize the utility of the injected knowledge, we deploy a learningbased multi-sieve approach and develop novel entity-based features. Our end system outperforms the state-of-the-art baseline by 2 B3 F1 points on non-transcript portion of the ACE 2004 dataset.</p><p>6 0.58342904 <a title="19-lsi-6" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>7 0.56890643 <a title="19-lsi-7" href="./emnlp-2012-Entity_based_QA_Retrieval.html">41 emnlp-2012-Entity based QA Retrieval</a></p>
<p>8 0.521402 <a title="19-lsi-8" href="./emnlp-2012-Modelling_Sequential_Text_with_an_Adaptive_Topic_Model.html">90 emnlp-2012-Modelling Sequential Text with an Adaptive Topic Model</a></p>
<p>9 0.48339504 <a title="19-lsi-9" href="./emnlp-2012-A_Phrase-Discovering_Topic_Model_Using_Hierarchical_Pitman-Yor_Processes.html">8 emnlp-2012-A Phrase-Discovering Topic Model Using Hierarchical Pitman-Yor Processes</a></p>
<p>10 0.46575838 <a title="19-lsi-10" href="./emnlp-2012-SSHLDA%3A_A_Semi-Supervised_Hierarchical_Topic_Model.html">115 emnlp-2012-SSHLDA: A Semi-Supervised Hierarchical Topic Model</a></p>
<p>11 0.39212003 <a title="19-lsi-11" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>12 0.36132011 <a title="19-lsi-12" href="./emnlp-2012-Name_Phylogeny%3A_A_Generative_Model_of_String_Variation.html">96 emnlp-2012-Name Phylogeny: A Generative Model of String Variation</a></p>
<p>13 0.35714105 <a title="19-lsi-13" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>14 0.34980989 <a title="19-lsi-14" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>15 0.32920629 <a title="19-lsi-15" href="./emnlp-2012-Discovering_Diverse_and_Salient_Threads_in_Document_Collections.html">33 emnlp-2012-Discovering Diverse and Salient Threads in Document Collections</a></p>
<p>16 0.31173846 <a title="19-lsi-16" href="./emnlp-2012-Grounded_Models_of_Semantic_Representation.html">61 emnlp-2012-Grounded Models of Semantic Representation</a></p>
<p>17 0.29141086 <a title="19-lsi-17" href="./emnlp-2012-Monte_Carlo_MCMC%3A_Efficient_Inference_by_Approximate_Sampling.html">91 emnlp-2012-Monte Carlo MCMC: Efficient Inference by Approximate Sampling</a></p>
<p>18 0.27834719 <a title="19-lsi-18" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>19 0.26118886 <a title="19-lsi-19" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>20 0.24955299 <a title="19-lsi-20" href="./emnlp-2012-PATTY%3A_A_Taxonomy_of_Relational_Patterns_with_Semantic_Types.html">103 emnlp-2012-PATTY: A Taxonomy of Relational Patterns with Semantic Types</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.017), (16, 0.025), (25, 0.013), (28, 0.31), (34, 0.053), (60, 0.191), (63, 0.092), (64, 0.017), (65, 0.036), (70, 0.027), (73, 0.011), (74, 0.034), (76, 0.021), (80, 0.013), (86, 0.013), (95, 0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80521262 <a title="19-lda-1" href="./emnlp-2012-An_Entity-Topic_Model_for_Entity_Linking.html">19 emnlp-2012-An Entity-Topic Model for Entity Linking</a></p>
<p>Author: Xianpei Han ; Le Sun</p><p>Abstract: Entity Linking (EL) has received considerable attention in recent years. Given many name mentions in a document, the goal of EL is to predict their referent entities in a knowledge base. Traditionally, there have been two distinct directions of EL research: one focusing on the effects of mention’s context compatibility, assuming that “the referent entity of a mention is reflected by its context”; the other dealing with the effects of document’s topic coherence, assuming that “a mention ’s referent entity should be coherent with the document’ ’s main topics”. In this paper, we propose a generative model called entitytopic model, to effectively join the above two complementary directions together. By jointly modeling and exploiting the context compatibility, the topic coherence and the correlation between them, our model can – accurately link all mentions in a document using both the local information (including the words and the mentions in a document) and the global knowledge (including the topic knowledge, the entity context knowledge and the entity name knowledge). Experimental results demonstrate the effectiveness of the proposed model. 1</p><p>2 0.57953006 <a title="19-lda-2" href="./emnlp-2012-Grounded_Models_of_Semantic_Representation.html">61 emnlp-2012-Grounded Models of Semantic Representation</a></p>
<p>Author: Carina Silberer ; Mirella Lapata</p><p>Abstract: A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two.</p><p>3 0.57540983 <a title="19-lda-3" href="./emnlp-2012-Enlarging_Paraphrase_Collections_through_Generalization_and_Instantiation.html">39 emnlp-2012-Enlarging Paraphrase Collections through Generalization and Instantiation</a></p>
<p>Author: Atsushi Fujita ; Pierre Isabelle ; Roland Kuhn</p><p>Abstract: This paper presents a paraphrase acquisition method that uncovers and exploits generalities underlying paraphrases: paraphrase patterns are first induced and then used to collect novel instances. Unlike existing methods, ours uses both bilingual parallel and monolingual corpora. While the former are regarded as a source of high-quality seed paraphrases, the latter are searched for paraphrases that match patterns learned from the seed paraphrases. We show how one can use monolingual corpora, which are far more numerous and larger than bilingual corpora, to obtain paraphrases that rival in quality those derived directly from bilingual corpora. In our experiments, the number of paraphrase pairs obtained in this way from monolingual corpora was a large multiple of the number of seed paraphrases. Human evaluation through a paraphrase substitution test demonstrated that the newly acquired paraphrase pairs are ofreasonable quality. Remaining noise can be further reduced by filtering seed paraphrases.</p><p>4 0.57528061 <a title="19-lda-4" href="./emnlp-2012-Exploring_Adaptor_Grammars_for_Native_Language_Identification.html">48 emnlp-2012-Exploring Adaptor Grammars for Native Language Identification</a></p>
<p>Author: Sze-Meng Jojo Wong ; Mark Dras ; Mark Johnson</p><p>Abstract: The task of inferring the native language of an author based on texts written in a second language has generally been tackled as a classification problem, typically using as features a mix of n-grams over characters and part of speech tags (for small and fixed n) and unigram function words. To capture arbitrarily long n-grams that syntax-based approaches have suggested are useful, adaptor grammars have some promise. In this work we investigate their extension to identifying n-gram collocations of arbitrary length over a mix of PoS tags and words, using both maxent and induced syntactic language model approaches to classification. After presenting a new, simple baseline, we show that learned collocations used as features in a maxent model perform better still, but that the story is more mixed for the syntactic language model.</p><p>5 0.57449955 <a title="19-lda-5" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>Author: Thomas Lin ; Mausam ; Oren Etzioni</p><p>Abstract: Entity linking systems link noun-phrase mentions in text to their corresponding Wikipedia articles. However, NLP applications would gain from the ability to detect and type all entities mentioned in text, including the long tail of entities not prominent enough to have their own Wikipedia articles. In this paper we show that once the Wikipedia entities mentioned in a corpus of textual assertions are linked, this can further enable the detection and fine-grained typing of the unlinkable entities. Our proposed method for detecting unlinkable entities achieves 24% greater accuracy than a Named Entity Recognition baseline, and our method for fine-grained typing is able to propagate over 1,000 types from linked Wikipedia entities to unlinkable entities. Detection and typing of unlinkable entities can increase yield for NLP applications such as typed question answering.</p><p>6 0.56515229 <a title="19-lda-6" href="./emnlp-2012-Why_Question_Answering_using_Sentiment_Analysis_and_Word_Classes.html">137 emnlp-2012-Why Question Answering using Sentiment Analysis and Word Classes</a></p>
<p>7 0.56462252 <a title="19-lda-7" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>8 0.56090474 <a title="19-lda-8" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>9 0.56059784 <a title="19-lda-9" href="./emnlp-2012-Generalizing_Sub-sentential_Paraphrase_Acquisition_across_Original_Signal_Type_of_Text_Pairs.html">58 emnlp-2012-Generalizing Sub-sentential Paraphrase Acquisition across Original Signal Type of Text Pairs</a></p>
<p>10 0.55991441 <a title="19-lda-10" href="./emnlp-2012-Entity_based_QA_Retrieval.html">41 emnlp-2012-Entity based QA Retrieval</a></p>
<p>11 0.55833417 <a title="19-lda-11" href="./emnlp-2012-Multi-Domain_Learning%3A_When_Do_Domains_Matter%3F.html">92 emnlp-2012-Multi-Domain Learning: When Do Domains Matter?</a></p>
<p>12 0.55506176 <a title="19-lda-12" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>13 0.5539313 <a title="19-lda-13" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>14 0.5530495 <a title="19-lda-14" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>15 0.55219406 <a title="19-lda-15" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>16 0.55053103 <a title="19-lda-16" href="./emnlp-2012-Translation_Model_Based_Cross-Lingual_Language_Model_Adaptation%3A_from_Word_Models_to_Phrase_Models.html">128 emnlp-2012-Translation Model Based Cross-Lingual Language Model Adaptation: from Word Models to Phrase Models</a></p>
<p>17 0.55029941 <a title="19-lda-17" href="./emnlp-2012-Probabilistic_Finite_State_Machines_for_Regression-based_MT_Evaluation.html">108 emnlp-2012-Probabilistic Finite State Machines for Regression-based MT Evaluation</a></p>
<p>18 0.54966629 <a title="19-lda-18" href="./emnlp-2012-Wiki-ly_Supervised_Part-of-Speech_Tagging.html">138 emnlp-2012-Wiki-ly Supervised Part-of-Speech Tagging</a></p>
<p>19 0.54924518 <a title="19-lda-19" href="./emnlp-2012-Explore_Person_Specific_Evidence_in_Web_Person_Name_Disambiguation.html">47 emnlp-2012-Explore Person Specific Evidence in Web Person Name Disambiguation</a></p>
<p>20 0.548406 <a title="19-lda-20" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
