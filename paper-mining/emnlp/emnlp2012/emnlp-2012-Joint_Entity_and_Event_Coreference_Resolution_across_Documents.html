<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-71" href="#">emnlp2012-71</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</h1>
<br/><p>Source: <a title="emnlp-2012-71-pdf" href="http://aclweb.org/anthology//D/D12/D12-1045.pdf">pdf</a></p><p>Author: Heeyoung Lee ; Marta Recasens ; Angel Chang ; Mihai Surdeanu ; Dan Jurafsky</p><p>Abstract: We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable documents, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.</p><p>Reference: <a title="emnlp-2012-71-reference" href="../emnlp2012_reference/emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We introduce a novel coreference resolution system that models entities and events jointly. [sent-2, score-1.001]
</p><p>2 Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. [sent-3, score-1.241]
</p><p>3 As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. [sent-4, score-0.968]
</p><p>4 Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. [sent-5, score-1.384]
</p><p>5 In a cross-document domain with comparable documents, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately. [sent-6, score-1.001]
</p><p>6 1 Introduction Most coreference resolution systems focus on enti-  ties and tacitly assume a correspondence between entities and noun phrases (NPs). [sent-7, score-0.845]
</p><p>7 Focusing on NPs is a way to restrict the challenging problem of coreference resolution, but misses coreference relations like the one between hanged and his suicide in (1), and between placed and put in (2). [sent-8, score-1.158]
</p><p>8 Since arguments play a key role in describing an event, knowing that two arguments corefer is useful for finding coreference relations between events, and knowing that two events corefer is useful for finding coreference relations between entities. [sent-21, score-1.54]
</p><p>9 In (1), the coreference relation between One of the key suspected Mafia bosses arrested yesterday and Lo Presti can be found by knowing that their predicates (i. [sent-22, score-0.767]
</p><p>10 On the other hand, the coreference relations between the arguments Saints and Bush in (2) helps to determine the coreference relation between their predicates placed and put. [sent-25, score-1.175]
</p><p>11 We annotate a corpus with cross-document coreference relations for nominal and verbal mentions. [sent-27, score-0.82]
</p><p>12 We focus on both intra and inter-document coreference because this scenario is at the same time more challenging and more relevant to real-world applications such as news aggregation. [sent-28, score-0.549]
</p><p>13 lc L2a0n1g2ua Agseso Pcrioactieosnsi fnogr a Cnodm Cpoumtaptiuotna tilo Lnianlg Nuaist uircasl our approach is an iterative algorithm that cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. [sent-33, score-1.241]
</p><p>14 We evaluate our cross-document coreference rWeesol euvtaioluna system on tshs-isd corpus atn cdo srhefoewre tnhcaet our joint approach significantly outperforms two strong baselines that resolve entities and events separately. [sent-37, score-0.761]
</p><p>15 Related Work  Entity coreference resolution is a well studied problem with many successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al. [sent-38, score-1.148]
</p><p>16 Prior work showed that models thatjointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al. [sent-42, score-0.478]
</p><p>17 A natural extension is to perform coreference jointly across both entities and events. [sent-45, score-0.634]
</p><p>18 We confirm 490 that such features are useful but also show that the complementary features for verbal mentions lead to even better performance, especially when event and entity clusters are jointly modeled. [sent-50, score-1.059]
</p><p>19 Compared to the extensive work on entity coreference, the related problem of event coreference remains relatively under-explored, with minimal work on how entity and event coreference can be considered jointly on an open domain. [sent-51, score-1.883]
</p><p>20 Early work on event coreference for MUC (Humphreys et al. [sent-52, score-0.766]
</p><p>21 More recently, there have been approaches that looked at event coreference for wider domains. [sent-54, score-0.766]
</p><p>22 To our knowledge, the only previous work that considered entity and event coreference resolution jointly is He (2007), but limited to the medical domain and focused on just five semantic categories. [sent-60, score-1.196]
</p><p>23 3  Architecture  Following the intuition introduced in Section 1, our approach iteratively builds clusters of event and entity mentions jointly. [sent-61, score-0.862]
</p><p>24 , finding out that two verbal mentions have arguments that belong to the same entity cluster), the features of both entity and event mentions are re-generated, which prompts future clustering operations. [sent-64, score-1.251]
</p><p>25 Our model follows a cautious (or “baby steps”) approach, which we previously showed to be successful for entity coreference resolution (Raghunathan et al. [sent-65, score-0.92]
</p><p>26 However, unlike our previous work, which used deterministic  rules, in this paper we learn a coreference resolution model using linear regression. [sent-68, score-0.805]
</p><p>27 1 Document Clustering Our approach starts with several steps that reduce the search space for the actual coreference resolution task. [sent-73, score-0.759]
</p><p>28 For example, with-  out document clustering, our algorithm may decide to cluster two mentions of the verb hit, but knowing that one belongs to a cluster containing earthquake reports and the other to a cluster with reports on criminal activities, this decision can be avoided. [sent-78, score-0.635]
</p><p>29 We extract nom-  inal and pronominal mentions using the mention identification component in the publicly downloadable Stanford coreference resolution system (Raghunathan et al. [sent-88, score-1.143]
</p><p>30 Crucially, we do not make a formal distinction between entity and event mentions. [sent-93, score-0.408]
</p><p>31 , is the noun earthquake an entity or an event mention? [sent-96, score-0.438]
</p><p>32 ) and an imperfect classification would negatively affect the following coreference resolution. [sent-97, score-0.519]
</p><p>33 3 High-precision Entity Resolution Sieves To further reduce the problem’s search space, in step 6 of Algorithm 1 we apply a set of high-  precision filters from the Stanford coreference resolution system. [sent-103, score-0.759]
</p><p>34 This system is a collection of deterministic models (or “sieves”) for entity coreference resolution that incorporate lexical, syntactic, semantic, and discourse information. [sent-104, score-0.966]
</p><p>35 As clusters are built, information such as mention gender and number is propagated across mentions in the same cluster, which helps subsequent decisions. [sent-106, score-0.617]
</p><p>36 The Stanford system obtained the highest score at the CoNLL2011 shared task on English coreference resolution. [sent-107, score-0.519]
</p><p>37 For this step, we selected all the sieves from the Stanford system with the exception of the pronoun resolution sieve. [sent-108, score-0.468]
</p><p>38 , High-precision sieves Discourse processing sieve Exact string match sieve Relaxed string match sieve Precise constructs sieve (e. [sent-111, score-0.879]
</p><p>39 , appositives) Strict head match sieves Proper head noun match sieve Relaxed head matching sieve  Table 1: Deterministic sieves in step 6 of Algorithm 1. [sent-113, score-0.846]
</p><p>40 one sieve clusters together two entity mentions only when they have the same head word. [sent-114, score-0.83]
</p><p>41 That is, all verbal mentions are still in singleton clusters after this step. [sent-118, score-0.672]
</p><p>42 Furthermore, none of these sieves use features that facilitate the joint resolution of nominal and verbal mentions (e. [sent-119, score-0.924]
</p><p>43 4 Iterative Entity/Event Resolution In this stage (steps 7 9 in Algorithm 1), we construct entity and event clusters using a cautious or “baby steps” approach. [sent-124, score-0.666]
</p><p>44 We use a single linear regressor (Θ) to model cluster merge operations between both verbal and nominal clusters. [sent-125, score-0.606]
</p><p>45 Once two clusters are merged (step 9) we regenerate all the mention features to reflect the current clusters. [sent-132, score-0.434]
</p><p>46 This iterative procedure is the core of our joint coreference resolution approach. [sent-134, score-0.807]
</p><p>47 This algorithm transparently merges both entity and event mentions and, importantly, allows information to flow between clusters of both types as merge operations take place. [sent-135, score-1.086]
</p><p>48 Because of this merge, in iteration i+ 1the nominal mentions Lo Presti and One of the key suspected Mafia bosses have the same semantic role for verbs assigned to the same cluster. [sent-137, score-0.491]
</p><p>49 5 Pronoun Sieve Our approach concludes with the pronominal coreference resolution sieve from the Stanford system. [sent-141, score-0.989]
</p><p>50 This sieve is necessary because our current resolution algorithm ignores mention ordering and distance (i. [sent-142, score-0.544]
</p><p>51 , in step 7 we compare all clusters regardless of where their mentions appear in the text). [sent-144, score-0.454]
</p><p>52 As previous work has proved, the structure of the text is crucial for pronominal coreference (Hobbs, 1978). [sent-145, score-0.576]
</p><p>53 The algorithm uses gold coreference labels to train a linear regressor that models the quality of the clusters produced by merge operations. [sent-153, score-0.984]
</p><p>54 , not present in either one of the clusters to be merged) that are correct:  q =linkscorrleinctk+sc loirrnekctsincorrect  (1)  where links(in)correct is the number of newly introduced (in)correct pairwise mention links when two clusters are merged. [sent-156, score-0.675]
</p><p>55 , –  –  2We skip the pronoun sieve here because it does not affect the decisions taken during the iterative resolution steps. [sent-166, score-0.502]
</p><p>56 Since these deterministic models address only nominal clusters, at the end we generate training data for events by inspecting all the pairs of singleton verbal clusters. [sent-177, score-0.553]
</p><p>57 For example, when comparing the event clusters {bought} taond c {acquired}, eitxyt. [sent-202, score-0.505]
</p><p>58 iti Foonr t eon PropBank-style isro lfeesat, furore event mentions we also include the closest left and right entity mentions in order to capture any arguments missed by the SRL system. [sent-206, score-0.875]
</p><p>59 Indicator feature set to 1if the two clusters have at least one coreferent argument in a given role. [sent-207, score-0.441]
</p><p>60 Indicator feature set to 1if the two clusters have at least one coreferent predicate for a given role. [sent-212, score-0.452]
</p><p>61 If any of the two clusters contains a verbal mention we consider the merge an operation between event (V) clusters; otherwise it is a merge between entity (E) clusters. [sent-222, score-1.203]
</p><p>62 1 Corpus The training and test data sets were derived from  the EventCorefBank (ECB) corpus5 created by Bejan and Harabagiu (2010) to study event coreference since standard corpora such as OntoNotes (Pradhan et al. [sent-233, score-0.766]
</p><p>63 The reason for including comparable documents was to increase the number of cross-document coreference relations. [sent-236, score-0.519]
</p><p>64 For the purpose of our study, we extended the original corpus in two directions: (i) fully annotated sentences, and (ii) entity coreference relations. [sent-238, score-0.711]
</p><p>65 In addition, we removed relations other than coreference (e. [sent-239, score-0.519]
</p><p>66 2 Evaluation We use five coreference evaluation metrics widely used in the literature: MUC (Vilain et al. [sent-328, score-0.519]
</p><p>67 BLANC (Recasens and Hovy, 2011) Metric based on the Rand index (Rand, 1971) that considers both coreference and non-coreference links to address the imbalance between singleton and coreferent mentions. [sent-333, score-0.749]
</p><p>68 Note that the gold corpus separates clusters into entity and event clusters (see Table 3), but our We report scores for entity clusters, event clusters and the system does not make this distinction at runtime. [sent-341, score-1.618]
</p><p>69 , all spurious mentions that our system in-  cludes in a cluster with a gold entity mention are considered for the entity score, regardless of their gold type (event or entity). [sent-348, score-0.831]
</p><p>70 Baseline 1 uses a modified Stanford coreference resolution system after our document clustering and mention identification steps. [sent-351, score-0.937]
</p><p>71 Because the original Stanford system implements only entity coreference, we extended it with an extra sieve that implements lemma matching for events. [sent-352, score-0.442]
</p><p>72 , clusters that contain at least one verbal mention) or a verbal and a nominal cluster when at least two lemmas of mention head words are the same between clusters, e. [sent-355, score-1.056]
</p><p>73 Both these sieves model entity and event 496  contextual information using semantic roles. [sent-359, score-0.595]
</p><p>74 The first sieve merges two nominal clusters when two mentions in the respective clusters have the same  head words and two mentions (possibly with different heads) modify with the same role label two predicates that have the same lemma. [sent-360, score-1.438]
</p><p>75 The second sieve implements the complementary action for event clusters. [sent-362, score-0.449]
</p><p>76 That is, it merges two verbal clusters when at least two mentions have the same lemma and at least two mentions have semantic arguments with the same role label and the same lemma. [sent-363, score-1.063]
</p><p>77 This demonstrates that local syntactico-semantic context is important for coreference resolution even in a cross-document setting and that the current state-of-the-art in SRL can model this context accurately. [sent-369, score-0.759]
</p><p>78 This demonstrates that a holistic approach to coreference resolution improves the resolution of both entities and events more than models that address aspects of the task separately. [sent-376, score-1.273]
</p><p>79 For example, the “Coreferent Argument for Arg1” feature is triggered when two event clusters have Arg1 arguments that already belong to the same entity cluster. [sent-390, score-0.741]
</p><p>80 This allows information from previous entity coreference operations to impact future merges of event clusters. [sent-391, score-1.003]
</p><p>81 This is the crux of our iterative approach to joint coreference resolution. [sent-392, score-0.567]
</p><p>82 This work demonstrates that an approach that jointly models entities and events is better for crossdocument coreference resolution. [sent-396, score-0.82]
</p><p>83 For example, document clustering and coreference resolution can be solved jointly, which we expect would improve both tasks. [sent-398, score-0.806]
</p><p>84 Furthermore, our iterative coreference resolution procedure (Algorithm 1) could be modified to account for mention ordering and distance, which would allow us to include pronominal resolution in our joint model, rather than addressing it with a separate deterministic sieve. [sent-399, score-1.281]
</p><p>85 8  Conclusion  We have presented a holistic model for crossdocument coreference resolution that jointly solves references to events and entities by handling both nominal and verbal mentions. [sent-400, score-1.393]
</p><p>86 Our joint resolution algorithm allows event coreference to help improve entity coreference, and vice versa. [sent-401, score-1.167]
</p><p>87 In addition, our iterative procedure, based on a linear regressor that models the quality of cluster merges, allows each Error Type (Ratio)  DExeascmrippletion  Pronoun resolution (36%)  The pronoun is incorrectly resolved by the pronominal sieve of the Stanford deterministic entity system. [sent-402, score-0.952]
</p><p>88 Semantics beyond role frames  The semantics of the coreference relation cannot be captured by role frames or WordNet. [sent-405, score-0.607]
</p><p>89 Initial high-precision sieves (6%) Phrasal verbs (6%) Linear regression (4%)  An error made by the initial high-precision entity resolution sieves is propagated to our model. [sent-426, score-0.869]
</p><p>90 merging state to benefit from the previous merged entity and event mentions. [sent-453, score-0.488]
</p><p>91 This approach allows us to start with a set of high-precision coreference relations and gradually add new ones to increase recall. [sent-454, score-0.519]
</p><p>92 This is noteworthy since each measure has been shown to place primary emphasis in evaluating a different aspect of the coreference resolution task. [sent-456, score-0.759]
</p><p>93 Our system is tailored for cross-document coreference resolution on a corpus that contains news articles that repeatedly report on a smaller number of topics. [sent-457, score-0.759]
</p><p>94 Joint determination of anaphoricity and coreference resolution using integer programming. [sent-488, score-0.759]
</p><p>95 Simple coreference resolution with rich syntactic and semantic features. [sent-496, score-0.759]
</p><p>96 Stanford’s multi-pass sieve coreference resolution system at the CoNLL-201 1 shared task. [sent-521, score-0.932]
</p><p>97 Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. [sent-548, score-0.563]
</p><p>98 CoNLL-201 1 shared task: Modeling unrestricted coreference in OntoNotes. [sent-561, score-0.519]
</p><p>99 Conundrums in noun phrase coreference resolution: Making sense ofthe state-of-the-art. [sent-582, score-0.519]
</p><p>100 A unified approach for schema matching, coreference and canonicalization. [sent-604, score-0.519]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('coreference', 0.519), ('clusters', 0.258), ('event', 0.247), ('resolution', 0.24), ('mentions', 0.196), ('sieves', 0.187), ('sieve', 0.173), ('verbal', 0.168), ('entity', 0.161), ('events', 0.156), ('coreferent', 0.152), ('nominal', 0.133), ('mention', 0.131), ('cluster', 0.126), ('hanged', 0.12), ('merge', 0.119), ('entities', 0.086), ('merges', 0.076), ('bejan', 0.075), ('arguments', 0.075), ('strike', 0.065), ('obama', 0.064), ('predicates', 0.062), ('amd', 0.06), ('mafia', 0.06), ('regressor', 0.06), ('stanford', 0.06), ('pronominal', 0.057), ('regression', 0.056), ('surdeanu', 0.056), ('srl', 0.052), ('pradhan', 0.051), ('lemma', 0.05), ('singleton', 0.05), ('iterative', 0.048), ('clustering', 0.047), ('raghunathan', 0.047), ('deterministic', 0.046), ('arrested', 0.045), ('bosses', 0.045), ('corefer', 0.045), ('ecb', 0.045), ('humphreys', 0.045), ('presti', 0.045), ('saints', 0.045), ('timmons', 0.045), ('merged', 0.045), ('role', 0.044), ('hit', 0.043), ('head', 0.042), ('predicate', 0.042), ('pronoun', 0.041), ('muc', 0.04), ('ati', 0.04), ('heeyoung', 0.04), ('blanc', 0.039), ('recasens', 0.039), ('verbs', 0.038), ('nps', 0.037), ('mihai', 0.037), ('helped', 0.036), ('suspected', 0.035), ('rand', 0.035), ('tbl', 0.035), ('police', 0.035), ('bush', 0.035), ('bought', 0.035), ('merging', 0.035), ('president', 0.035), ('heads', 0.033), ('bagga', 0.033), ('conll', 0.033), ('holistic', 0.032), ('gender', 0.032), ('argument', 0.031), ('haghighi', 0.031), ('knowing', 0.031), ('annotated', 0.031), ('earthquake', 0.03), ('lemmas', 0.03), ('israeli', 0.03), ('cautiously', 0.03), ('clusterdocuments', 0.03), ('cosmin', 0.03), ('crossdocument', 0.03), ('dead', 0.03), ('hevent', 0.03), ('hospital', 0.03), ('intra', 0.03), ('logan', 0.03), ('roper', 0.03), ('swirl', 0.03), ('yesterday', 0.03), ('jointly', 0.029), ('topics', 0.029), ('flow', 0.029), ('implements', 0.029), ('rahman', 0.029), ('gold', 0.028), ('links', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000011 <a title="71-tfidf-1" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>Author: Heeyoung Lee ; Marta Recasens ; Angel Chang ; Mihai Surdeanu ; Dan Jurafsky</p><p>Abstract: We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable documents, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.</p><p>2 0.35611707 <a title="71-tfidf-2" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>Author: Quang Do ; Wei Lu ; Dan Roth</p><p>Abstract: This paper addresses the task of constructing a timeline of events mentioned in a given text. To accomplish that, we present a novel representation of the temporal structure of a news article based on time intervals. We then present an algorithmic approach that jointly optimizes the temporal structure by coupling local classifiers that predict associations and temporal relations between pairs of temporal entities with global constraints. Moreover, we present ways to leverage knowledge provided by event coreference to further improve the system performance. Overall, our experiments show that the joint inference model significantly outperformed the local classifiers by 9.2% of relative improvement in F1. The experiments also suggest that good event coreference could make remarkable contribution to a robust event timeline construction system.</p><p>3 0.33188197 <a title="71-tfidf-3" href="./emnlp-2012-Learning-based_Multi-Sieve_Co-reference_Resolution_with_Knowledge.html">76 emnlp-2012-Learning-based Multi-Sieve Co-reference Resolution with Knowledge</a></p>
<p>Author: Lev Ratinov ; Dan Roth</p><p>Abstract: We explore the interplay of knowledge and structure in co-reference resolution. To inject knowledge, we use a state-of-the-art system which cross-links (or “grounds”) expressions in free text to Wikipedia. We explore ways of using the resulting grounding to boost the performance of a state-of-the-art co-reference resolution system. To maximize the utility of the injected knowledge, we deploy a learningbased multi-sieve approach and develop novel entity-based features. Our end system outperforms the state-of-the-art baseline by 2 B3 F1 points on non-transcript portion of the ACE 2004 dataset.</p><p>4 0.32930198 <a title="71-tfidf-4" href="./emnlp-2012-Joint_Learning_for_Coreference_Resolution_with_Markov_Logic.html">73 emnlp-2012-Joint Learning for Coreference Resolution with Markov Logic</a></p>
<p>Author: Yang Song ; Jing Jiang ; Wayne Xin Zhao ; Sujian Li ; Houfeng Wang</p><p>Abstract: Pairwise coreference resolution models must merge pairwise coreference decisions to generate final outputs. Traditional merging methods adopt different strategies such as the bestfirst method and enforcing the transitivity constraint, but most of these methods are used independently of the pairwise learning methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-201 1shared task on the same dataset. Compared with the best system from CoNLL2011, which employs a rule-based method, our system shows competitive performance.</p><p>5 0.21893592 <a title="71-tfidf-5" href="./emnlp-2012-Resolving_Complex_Cases_of_Definite_Pronouns%3A_The_Winograd_Schema_Challenge.html">112 emnlp-2012-Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge</a></p>
<p>Author: Altaf Rahman ; Vincent Ng</p><p>Abstract: We examine the task of resolving complex cases of definite pronouns, specifically those for which traditional linguistic constraints on coreference (e.g., Binding Constraints, gender and number agreement) as well as commonly-used resolution heuristics (e.g., string-matching facilities, syntactic salience) are not useful. Being able to solve this task has broader implications in artificial intelligence: a restricted version of it, sometimes referred to as the Winograd Schema Challenge, has been suggested as a conceptually and practically appealing alternative to the Turing Test. We employ a knowledge-rich approach to this task, which yields a pronoun resolver that outperforms state-of-the-art resolvers by nearly 18 points in accuracy on our dataset.</p><p>6 0.20800823 <a title="71-tfidf-6" href="./emnlp-2012-Domain_Adaptation_for_Coreference_Resolution%3A_An_Adaptive_Ensemble_Approach.html">36 emnlp-2012-Domain Adaptation for Coreference Resolution: An Adaptive Ensemble Approach</a></p>
<p>7 0.16463064 <a title="71-tfidf-7" href="./emnlp-2012-Monte_Carlo_MCMC%3A_Efficient_Inference_by_Approximate_Sampling.html">91 emnlp-2012-Monte Carlo MCMC: Efficient Inference by Approximate Sampling</a></p>
<p>8 0.15275745 <a title="71-tfidf-8" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>9 0.13553201 <a title="71-tfidf-9" href="./emnlp-2012-Using_Discourse_Information_for_Paraphrase_Extraction.html">135 emnlp-2012-Using Discourse Information for Paraphrase Extraction</a></p>
<p>10 0.13157649 <a title="71-tfidf-10" href="./emnlp-2012-An_Entity-Topic_Model_for_Entity_Linking.html">19 emnlp-2012-An Entity-Topic Model for Entity Linking</a></p>
<p>11 0.13085893 <a title="71-tfidf-11" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>12 0.11839724 <a title="71-tfidf-12" href="./emnlp-2012-Resolving_This-issue_Anaphora.html">113 emnlp-2012-Resolving This-issue Anaphora</a></p>
<p>13 0.10859459 <a title="71-tfidf-13" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>14 0.098211333 <a title="71-tfidf-14" href="./emnlp-2012-Employing_Compositional_Semantics_and_Discourse_Consistency_in_Chinese_Event_Extraction.html">38 emnlp-2012-Employing Compositional Semantics and Discourse Consistency in Chinese Event Extraction</a></p>
<p>15 0.076125823 <a title="71-tfidf-15" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>16 0.075969741 <a title="71-tfidf-16" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>17 0.075008824 <a title="71-tfidf-17" href="./emnlp-2012-Entity_based_QA_Retrieval.html">41 emnlp-2012-Entity based QA Retrieval</a></p>
<p>18 0.068816319 <a title="71-tfidf-18" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>19 0.068208851 <a title="71-tfidf-19" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>20 0.065529615 <a title="71-tfidf-20" href="./emnlp-2012-Building_a_Lightweight_Semantic_Model_for_Unsupervised_Information_Extraction_on_Short_Listings.html">26 emnlp-2012-Building a Lightweight Semantic Model for Unsupervised Information Extraction on Short Listings</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.274), (1, 0.393), (2, -0.086), (3, -0.428), (4, 0.031), (5, -0.062), (6, -0.159), (7, -0.273), (8, 0.118), (9, 0.001), (10, -0.036), (11, -0.022), (12, 0.072), (13, 0.053), (14, -0.009), (15, -0.062), (16, 0.045), (17, -0.052), (18, 0.015), (19, -0.052), (20, 0.051), (21, -0.024), (22, -0.039), (23, 0.085), (24, -0.028), (25, 0.006), (26, -0.028), (27, -0.017), (28, 0.057), (29, 0.075), (30, -0.088), (31, 0.024), (32, -0.01), (33, -0.041), (34, -0.009), (35, -0.033), (36, 0.069), (37, -0.037), (38, -0.012), (39, 0.002), (40, -0.03), (41, -0.091), (42, 0.027), (43, -0.02), (44, 0.001), (45, -0.001), (46, -0.048), (47, 0.005), (48, -0.014), (49, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97961611 <a title="71-lsi-1" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>Author: Heeyoung Lee ; Marta Recasens ; Angel Chang ; Mihai Surdeanu ; Dan Jurafsky</p><p>Abstract: We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable documents, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.</p><p>2 0.91210067 <a title="71-lsi-2" href="./emnlp-2012-Joint_Learning_for_Coreference_Resolution_with_Markov_Logic.html">73 emnlp-2012-Joint Learning for Coreference Resolution with Markov Logic</a></p>
<p>Author: Yang Song ; Jing Jiang ; Wayne Xin Zhao ; Sujian Li ; Houfeng Wang</p><p>Abstract: Pairwise coreference resolution models must merge pairwise coreference decisions to generate final outputs. Traditional merging methods adopt different strategies such as the bestfirst method and enforcing the transitivity constraint, but most of these methods are used independently of the pairwise learning methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-201 1shared task on the same dataset. Compared with the best system from CoNLL2011, which employs a rule-based method, our system shows competitive performance.</p><p>3 0.74400431 <a title="71-lsi-3" href="./emnlp-2012-Learning-based_Multi-Sieve_Co-reference_Resolution_with_Knowledge.html">76 emnlp-2012-Learning-based Multi-Sieve Co-reference Resolution with Knowledge</a></p>
<p>Author: Lev Ratinov ; Dan Roth</p><p>Abstract: We explore the interplay of knowledge and structure in co-reference resolution. To inject knowledge, we use a state-of-the-art system which cross-links (or “grounds”) expressions in free text to Wikipedia. We explore ways of using the resulting grounding to boost the performance of a state-of-the-art co-reference resolution system. To maximize the utility of the injected knowledge, we deploy a learningbased multi-sieve approach and develop novel entity-based features. Our end system outperforms the state-of-the-art baseline by 2 B3 F1 points on non-transcript portion of the ACE 2004 dataset.</p><p>4 0.70955193 <a title="71-lsi-4" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>Author: Quang Do ; Wei Lu ; Dan Roth</p><p>Abstract: This paper addresses the task of constructing a timeline of events mentioned in a given text. To accomplish that, we present a novel representation of the temporal structure of a news article based on time intervals. We then present an algorithmic approach that jointly optimizes the temporal structure by coupling local classifiers that predict associations and temporal relations between pairs of temporal entities with global constraints. Moreover, we present ways to leverage knowledge provided by event coreference to further improve the system performance. Overall, our experiments show that the joint inference model significantly outperformed the local classifiers by 9.2% of relative improvement in F1. The experiments also suggest that good event coreference could make remarkable contribution to a robust event timeline construction system.</p><p>5 0.51831734 <a title="71-lsi-5" href="./emnlp-2012-Domain_Adaptation_for_Coreference_Resolution%3A_An_Adaptive_Ensemble_Approach.html">36 emnlp-2012-Domain Adaptation for Coreference Resolution: An Adaptive Ensemble Approach</a></p>
<p>Author: Jian Bo Yang ; Qi Mao ; Qiao Liang Xiang ; Ivor Wai-Hung Tsang ; Kian Ming Adam Chai ; Hai Leong Chieu</p><p>Abstract: We propose an adaptive ensemble method to adapt coreference resolution across domains. This method has three features: (1) it can optimize for any user-specified objective measure; (2) it can make document-specific prediction rather than rely on a fixed base model or a fixed set of base models; (3) it can automatically adjust the active ensemble members during prediction. With simplification, this method can be used in the traditional withindomain case, while still retaining the above features. To the best of our knowledge, this work is the first to both (i) develop a domain adaptation algorithm for the coreference resolution problem and (ii) have the above features as an ensemble method. Empirically, we show the benefits of (i) on the six domains of the ACE 2005 data set in domain adaptation set- ting, and of (ii) on both the MUC-6 and the ACE 2005 data sets in within-domain setting.</p><p>6 0.49694526 <a title="71-lsi-6" href="./emnlp-2012-Resolving_Complex_Cases_of_Definite_Pronouns%3A_The_Winograd_Schema_Challenge.html">112 emnlp-2012-Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge</a></p>
<p>7 0.44947901 <a title="71-lsi-7" href="./emnlp-2012-Monte_Carlo_MCMC%3A_Efficient_Inference_by_Approximate_Sampling.html">91 emnlp-2012-Monte Carlo MCMC: Efficient Inference by Approximate Sampling</a></p>
<p>8 0.43095312 <a title="71-lsi-8" href="./emnlp-2012-Employing_Compositional_Semantics_and_Discourse_Consistency_in_Chinese_Event_Extraction.html">38 emnlp-2012-Employing Compositional Semantics and Discourse Consistency in Chinese Event Extraction</a></p>
<p>9 0.37245655 <a title="71-lsi-9" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>10 0.33996445 <a title="71-lsi-10" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>11 0.33746031 <a title="71-lsi-11" href="./emnlp-2012-An_Entity-Topic_Model_for_Entity_Linking.html">19 emnlp-2012-An Entity-Topic Model for Entity Linking</a></p>
<p>12 0.33735713 <a title="71-lsi-12" href="./emnlp-2012-Using_Discourse_Information_for_Paraphrase_Extraction.html">135 emnlp-2012-Using Discourse Information for Paraphrase Extraction</a></p>
<p>13 0.30245185 <a title="71-lsi-13" href="./emnlp-2012-Resolving_This-issue_Anaphora.html">113 emnlp-2012-Resolving This-issue Anaphora</a></p>
<p>14 0.28431278 <a title="71-lsi-14" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>15 0.2792564 <a title="71-lsi-15" href="./emnlp-2012-Building_a_Lightweight_Semantic_Model_for_Unsupervised_Information_Extraction_on_Short_Listings.html">26 emnlp-2012-Building a Lightweight Semantic Model for Unsupervised Information Extraction on Short Listings</a></p>
<p>16 0.26831833 <a title="71-lsi-16" href="./emnlp-2012-Entity_based_QA_Retrieval.html">41 emnlp-2012-Entity based QA Retrieval</a></p>
<p>17 0.254118 <a title="71-lsi-17" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>18 0.23603402 <a title="71-lsi-18" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>19 0.23581739 <a title="71-lsi-19" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>20 0.22912009 <a title="71-lsi-20" href="./emnlp-2012-A_Sequence_Labelling_Approach_to_Quote_Attribution.html">9 emnlp-2012-A Sequence Labelling Approach to Quote Attribution</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.028), (11, 0.01), (12, 0.081), (16, 0.068), (19, 0.01), (23, 0.045), (25, 0.018), (34, 0.051), (60, 0.11), (63, 0.061), (64, 0.041), (65, 0.08), (70, 0.013), (73, 0.037), (74, 0.031), (76, 0.053), (80, 0.054), (86, 0.091), (95, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89343953 <a title="71-lda-1" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>Author: Heeyoung Lee ; Marta Recasens ; Angel Chang ; Mihai Surdeanu ; Dan Jurafsky</p><p>Abstract: We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable documents, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.</p><p>2 0.82398176 <a title="71-lda-2" href="./emnlp-2012-Explore_Person_Specific_Evidence_in_Web_Person_Name_Disambiguation.html">47 emnlp-2012-Explore Person Specific Evidence in Web Person Name Disambiguation</a></p>
<p>Author: Liwei Chen ; Yansong Feng ; Lei Zou ; Dongyan Zhao</p><p>Abstract: In this paper, we investigate different usages of feature representations in the web person name disambiguation task which has been suffering from the mismatch of vocabulary and lack of clues in web environments. In literature, the latter receives less attention and remains more challenging. We explore the feature space in this task and argue that collecting person specific evidences from a corpus level can provide a more reasonable and robust estimation for evaluating a feature’s importance in a given web page. This can alleviate the lack of clues where discriminative features can be reasonably weighted by taking their corpus level importance into account, not just relying on the current local context. We therefore propose a topic-based model to exploit the person specific global importance and embed it into the person name similarity. The experimental results show that the corpus level topic in- formation provides more stable evidences for discriminative features and our method outperforms the state-of-the-art systems on three WePS datasets.</p><p>3 0.79437423 <a title="71-lda-3" href="./emnlp-2012-Constructing_Task-Specific_Taxonomies_for_Document_Collection_Browsing.html">30 emnlp-2012-Constructing Task-Specific Taxonomies for Document Collection Browsing</a></p>
<p>Author: Hui Yang</p><p>Abstract: Taxonomies can serve as browsing tools for document collections. However, given an arbitrary collection, pre-constructed taxonomies could not easily adapt to the specific topic/task present in the collection. This paper explores techniques to quickly derive task-specific taxonomies supporting browsing in arbitrary document collections. The supervised approach directly learns semantic distances from users to propose meaningful task-specific taxonomies. The approach aims to produce globally optimized taxonomy structures by incorporating path consistency control and usergenerated task specification into the general learning framework. A comparison to stateof-the-art systems and a user study jointly demonstrate that our techniques are highly effective. .</p><p>4 0.78175801 <a title="71-lda-4" href="./emnlp-2012-Learning_Syntactic_Categories_Using_Paradigmatic_Representations_of_Word_Context.html">79 emnlp-2012-Learning Syntactic Categories Using Paradigmatic Representations of Word Context</a></p>
<p>Author: Mehmet Ali Yatbaz ; Enis Sert ; Deniz Yuret</p><p>Abstract: We investigate paradigmatic representations of word context in the domain of unsupervised syntactic category acquisition. Paradigmatic representations of word context are based on potential substitutes of a word in contrast to syntagmatic representations based on properties of neighboring words. We compare a bigram based baseline model with several paradigmatic models and demonstrate significant gains in accuracy. Our best model based on Euclidean co-occurrence embedding combines the paradigmatic context representation with morphological and orthographic features and achieves 80% many-to-one accuracy on a 45-tag 1M word corpus.</p><p>5 0.77704948 <a title="71-lda-5" href="./emnlp-2012-Domain_Adaptation_for_Coreference_Resolution%3A_An_Adaptive_Ensemble_Approach.html">36 emnlp-2012-Domain Adaptation for Coreference Resolution: An Adaptive Ensemble Approach</a></p>
<p>Author: Jian Bo Yang ; Qi Mao ; Qiao Liang Xiang ; Ivor Wai-Hung Tsang ; Kian Ming Adam Chai ; Hai Leong Chieu</p><p>Abstract: We propose an adaptive ensemble method to adapt coreference resolution across domains. This method has three features: (1) it can optimize for any user-specified objective measure; (2) it can make document-specific prediction rather than rely on a fixed base model or a fixed set of base models; (3) it can automatically adjust the active ensemble members during prediction. With simplification, this method can be used in the traditional withindomain case, while still retaining the above features. To the best of our knowledge, this work is the first to both (i) develop a domain adaptation algorithm for the coreference resolution problem and (ii) have the above features as an ensemble method. Empirically, we show the benefits of (i) on the six domains of the ACE 2005 data set in domain adaptation set- ting, and of (ii) on both the MUC-6 and the ACE 2005 data sets in within-domain setting.</p><p>6 0.77318656 <a title="71-lda-6" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>7 0.77264458 <a title="71-lda-7" href="./emnlp-2012-Joint_Learning_for_Coreference_Resolution_with_Markov_Logic.html">73 emnlp-2012-Joint Learning for Coreference Resolution with Markov Logic</a></p>
<p>8 0.77080262 <a title="71-lda-8" href="./emnlp-2012-Left-to-Right_Tree-to-String_Decoding_with_Prediction.html">82 emnlp-2012-Left-to-Right Tree-to-String Decoding with Prediction</a></p>
<p>9 0.7666319 <a title="71-lda-9" href="./emnlp-2012-Besting_the_Quiz_Master%3A_Crowdsourcing_Incremental_Classification_Games.html">23 emnlp-2012-Besting the Quiz Master: Crowdsourcing Incremental Classification Games</a></p>
<p>10 0.76158726 <a title="71-lda-10" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>11 0.7528218 <a title="71-lda-11" href="./emnlp-2012-Learning-based_Multi-Sieve_Co-reference_Resolution_with_Knowledge.html">76 emnlp-2012-Learning-based Multi-Sieve Co-reference Resolution with Knowledge</a></p>
<p>12 0.75122702 <a title="71-lda-12" href="./emnlp-2012-Building_a_Lightweight_Semantic_Model_for_Unsupervised_Information_Extraction_on_Short_Listings.html">26 emnlp-2012-Building a Lightweight Semantic Model for Unsupervised Information Extraction on Short Listings</a></p>
<p>13 0.74745458 <a title="71-lda-13" href="./emnlp-2012-Optimising_Incremental_Dialogue_Decisions_Using_Information_Density_for_Interactive_Systems.html">102 emnlp-2012-Optimising Incremental Dialogue Decisions Using Information Density for Interactive Systems</a></p>
<p>14 0.74318886 <a title="71-lda-14" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>15 0.74278295 <a title="71-lda-15" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>16 0.74054188 <a title="71-lda-16" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>17 0.74035144 <a title="71-lda-17" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<p>18 0.73787761 <a title="71-lda-18" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>19 0.73650295 <a title="71-lda-19" href="./emnlp-2012-Extracting_Opinion_Expressions_with_semi-Markov_Conditional_Random_Fields.html">51 emnlp-2012-Extracting Opinion Expressions with semi-Markov Conditional Random Fields</a></p>
<p>20 0.73567003 <a title="71-lda-20" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
