<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 emnlp-2012-Identifying Constant and Unique Relations by using Time-Series Text</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-62" href="#">emnlp2012-62</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>62 emnlp-2012-Identifying Constant and Unique Relations by using Time-Series Text</h1>
<br/><p>Source: <a title="emnlp-2012-62-pdf" href="http://aclweb.org/anthology//D/D12/D12-1081.pdf">pdf</a></p><p>Author: Yohei Takaku ; Nobuhiro Kaji ; Naoki Yoshinaga ; Masashi Toyoda</p><p>Abstract: Because the real world evolves over time, numerous relations between entities written in presently available texts are already obsolete or will potentially evolve in the future. This study aims at resolving the intricacy in consistently compiling relations extracted from text, and presents a method for identifying constancy and uniqueness of the relations in the context of supervised learning. We exploit massive time-series web texts to induce features on the basis of time-series frequency and linguistic cues. Experimental results confirmed that the time-series frequency distributions contributed much to the recall of constancy identification and the precision of the uniqueness identification.</p><p>Reference: <a title="emnlp-2012-62-reference" href="../emnlp2012_reference/emnlp-2012-Identifying_Constant_and_Unique_Relations_by_using_Time-Series_Text_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 t oyoda @ t kl i s i Abstract Because the real world evolves over time, numerous relations between entities written in presently available texts are already obsolete or will potentially evolve in the future. [sent-9, score-0.392]
</p><p>2 This study aims at resolving the intricacy in consistently compiling relations extracted from text, and presents a method for identifying constancy and uniqueness of the relations in the context of supervised learning. [sent-10, score-1.513]
</p><p>3 We exploit massive time-series web texts to induce features on the basis of time-series frequency and linguistic cues. [sent-11, score-0.233]
</p><p>4 Experimental results confirmed that the time-series frequency distributions contributed much to the recall of constancy identification and the precision of the uniqueness identification. [sent-12, score-1.221]
</p><p>5 1 Introduction We have witnessed a number of success stories in acquiring semantic relations between entities from ever-increasing text on the web (Pantel and Pennacchiotti, 2006; Banko et al. [sent-13, score-0.267]
</p><p>6 jp There exists, however, a great challenge to compile consistently relations extracted from text by these methods, because they assume a simplifying assumption that relations are time-invariant. [sent-25, score-0.437]
</p><p>7 In other words, they implicitly disregard the fact that state-  ments in texts actually reflect the state of the world at the time when they were written, which follows that relations extracted from such texts eventually become outdated as the real world evolves over time. [sent-26, score-0.33]
</p><p>8 Let us consider that relations are extracted from the following sentences:1 (1) a. [sent-27, score-0.204]
</p><p>9 The relations in statements 1a and 1b are true across time, so we can simply accumulate all the relation instances. [sent-38, score-0.426]
</p><p>10 The relations in 1c and 1d in contrast evolve over time. [sent-39, score-0.25]
</p><p>11 The relation written in 1c becomes outdated when the other person takes the position, so we need to supersede it when a new relation is extracted from text (e. [sent-40, score-0.493]
</p><p>12 For the relation in 1d, we do not always need to supersede it with a new relation. [sent-44, score-0.228]
</p><p>13 This study is motivated from the above consider-  1Since our task settings are language-independent, we hereafter employ English examples as much as possible to widen the potential readership of the paper, although we conducted experiments with relations between entities in Japanese. [sent-45, score-0.267]
</p><p>14 lc L2a0n1g2ua Agseso Pcrioactieosnsi fnogr a Cnodm Cpoumtaptiuotna tilo Lnianlg Nuaist uircasl ations and proposes a method for identifying constancy and uniqueness of relations in order to select an appropriate strategy to maintain relation instances extracted from text. [sent-48, score-1.462]
</p><p>15 For example, the relations written in statements 1a and 1b are constant, while those in 1c and 1d are non-constant; the relation in 1c is unique,2 whereas the relation in 1d is non-unique. [sent-49, score-0.613]
</p><p>16 With these properties of relations in mind, we can accumulate constant relations while appropriately superseding non-constant, unique relations with newly acquired relations. [sent-50, score-0.85]
</p><p>17 The key challenge in solving these classification tasks is how to induce an effective feature that identifies unique, non-constant relations (statement 1c) that seemingly appear as non-unique relations on text (statement 1b). [sent-52, score-0.529]
</p><p>18 We exploit massive time-series web text to observe actual evolutions of relation instances and induce features  from the relation instances taken from a time sliding window and linguistic cues modifying the predicate and arguments of the target relation. [sent-53, score-0.651]
</p><p>19 We evaluated our method on 1000 relations extracted from 6-year’s worth of Japanese blog posts with 2. [sent-54, score-0.409]
</p><p>20 We have thereby confirmed that the features induced from this time-series text contributed much to improve the classification accuracy. [sent-56, score-0.188]
</p><p>21 The main contributions of this paper are twofold: • We have introduced a novel task for identifying constancy druelcaetido ans. [sent-57, score-0.606]
</p><p>22 n oSvienlce ta smko sfot ro ifd tehnet existing studies assume that relations are timeinvariant as discussed by Weikum et al. [sent-58, score-0.204]
</p><p>23 (201 1), non-constant relations prevalent in their outcome incur a serious problem in maintaining the acquired relations. [sent-59, score-0.204]
</p><p>24 The notion of constancy is meant to resolve this stalemate. [sent-60, score-0.546]
</p><p>25 •  We have for the first time demonstrated the uWseefu hlanveess f oofr a etim fiers-ster tiimese ete dxetm mino n restlraatiteodn acquisition and confirmed its impact in the two relation classification tasks. [sent-61, score-0.338]
</p><p>26 The features induced from the time-series text have greatly contributed to the accuracy of the classification  based on uniqueness as well as the recall of the classification based on constancy. [sent-62, score-0.682]
</p><p>27 2This kind of relation is referred to as functional relation in the literature (Ritter et al. [sent-63, score-0.402]
</p><p>28 884 Constant Non-constant arg1 was born in arg2arg1’s president is arg2 arg1 is a father of arg2 arg1 belongs to arg2 arg1 is written by arg2 arg1 lives in arg2 Table 1: Examples of constant, non-constant relations. [sent-66, score-0.348]
</p><p>29 Section 2 introduces the two properties of relations (constancy and uniqueness) and then defines the task setting of this study. [sent-68, score-0.204]
</p><p>30 Sections 3 and 4 describe the features induced from time-series text for constancy and uniqueness classification, respectively. [sent-69, score-1.045]
</p><p>31 1 Constancy and uniqueness We introduce two properties of relations: constancy and uniqueness. [sent-74, score-1.012]
</p><p>32 A relation is constant if, for most values of arg1, the value of arg2 is independent of time (Table 1). [sent-75, score-0.321]
</p><p>33 A relation is unique if, for most values of arg1, there exists, at any given point in time, only one value of arg2 that satisfies the relation (Table 2). [sent-79, score-0.481]
</p><p>34 In contrast, the relation harg1 is funded by arg2i eis. [sent-85, score-0.186]
</p><p>35 2 Discussion Both constancy and uniqueness are properties that usually, not always, hold for most, not all, of the arg1’s values. [sent-88, score-1.012]
</p><p>36 To see this, let us examine the relation harg1 ’s president is arg2i . [sent-89, score-0.353]
</p><p>37 Although this relation is Unique Non-unique arg1 was born in arg2arg1 is funded by arg2 arg1 is headquartered in arg2 arg1 consists of arg2 arg1’s president is arg2 arg1 borders on arg2 Table 2: Examples of unique and non-unique relations. [sent-90, score-0.672]
</p><p>38 For example, a country might exist in which the president has never changed; a country might have more than one president at the same time during civil war. [sent-92, score-0.376]
</p><p>39 However, since such situations are rare, the relation harg1 ’s president his s arg2i niss c aorens riadreer,ed th as neleaitthioenr hcoarngst1an ’st nor non-unique. [sent-93, score-0.353]
</p><p>40 The above discussion implies that the constancy and uniqueness of relations can not be determined completely objectively. [sent-94, score-1.216]
</p><p>41 We, nevertheless, claim that these properties of relations are intuitively accept-  able and thus they can be identified with moderate agreement by different people (see section 5). [sent-95, score-0.204]
</p><p>42 3 Task and our approach This paper explores classifying given relations on the basis of constancy and uniqueness. [sent-97, score-0.857]
</p><p>43 Section 3 presents features based on time-series frequency and linguistic cues for classifying constant and nonconstant relations. [sent-100, score-0.366]
</p><p>44 Similarly, section 4 presents analogous features for classifying unique and nonunique relations. [sent-101, score-0.218]
</p><p>45 1 Time-series frequency It is intuitive to identify constant relations by comparing frequency distributions over arg2 in different time periods. [sent-103, score-0.557]
</p><p>46 Time-series text For a time-series text, we used Japanese blog posts that had been gathered from  Feb. [sent-105, score-0.205]
</p><p>47 These posts were aggregated on a monthly basis by using time stamps attached with them, i. [sent-110, score-0.311]
</p><p>48 , the unit of time is one month 885  Figure 1: Time-series frequency distribution of harg1 belFoigngusr eto 1 :a Trgim2ie w-sehreien arg1 tuaeknecsy K deiissturikbue tHioonn odaf. [sent-112, score-0.194]
</p><p>49 Averaging such similarities over representative values of arg1, we have  N1e∈∑EN(r)cos(Fw1(r,e),Fw2(r,e) , where r is a relation (e. [sent-127, score-0.186]
</p><p>50 , harg1 ’s president is arg2i), e i ss a n raemlateido entity (e. [sent-129, score-0.224]
</p><p>51 Nominal modifiers We observe that non-constant relations could be indicated by some nominal modifiers:  (2) a. [sent-154, score-0.304]
</p><p>52 The use of the prefix ex- and the adjective first implies that the president changes, and hence the relation harg1 ’s president is arg2i is not constant. [sent-158, score-0.52]
</p><p>53 •  •  Next, we extract nouns from a relation to bNee xctl,as wsiefie dex (e. [sent-164, score-0.186]
</p><p>54 A given relation is transformed into different forms by attaching the suffixes to a verb in the relation, and their frequencies are counted. [sent-181, score-0.256]
</p><p>55 1 Time-series frequency Number of entity types A straightforward approach to identifying unique relations is, for a given arg1, to count the number of entity types appearing in arg2 (Lin et al. [sent-189, score-0.609]
</p><p>56 Even if the estimate is contaminated by noise, a small number of entity types can still be considered to indicate the uniqueness of the relation. [sent-192, score-0.523]
</p><p>57 Presume counting the number of entity types in arg2 of the relation harg1 is headquartered in arg2i, owfhi tchhe i rse nlaotnio-nco nhsatragn1t a isnd h unique. [sent-194, score-0.307]
</p><p>58 eIfr we use large size of time window to obtain counts, we will observe multiple types of entities in arg2, not because the relation is non-unique, but because it is nonconstant. [sent-195, score-0.347]
</p><p>59 Ratio of entity frequency Since it is not reliable enough to use only the number of entity types, we also exploit the frequency of the entity. [sent-202, score-0.304]
</p><p>60 If the frequency of e1st is much larger than that of e2nd, the relation is likely to be constant. [sent-204, score-0.281]
</p><p>61 The coordination structure in the first example implies an entity can border on more than one entity, and hence the relation harg1 borders on arg2i is not unique. [sent-212, score-0.423]
</p><p>62 To capture this intuition, we introduce two types of linguistic features for classifying unique and nonunique relations. [sent-216, score-0.218]
</p><p>63 The feature is fired if the number of times that coordination structures are found in arg2 exceeds threshold θ3. [sent-218, score-0.19]
</p><p>64 The feature is fired if the number of times that an entity in arg2 is followed by one of the four keywords exceeds threshold θ3. [sent-225, score-0.207]
</p><p>65 1 Data We built a dataset for evaluation by extracting relations from the time-series text (section 3. [sent-229, score-0.204]
</p><p>66 Then, annotators were asked to label 1000 relations as not only constant or non-constant but also  unique or non-unique. [sent-244, score-0.438]
</p><p>67 We have briefly investigated the relations whose labels assigned by the annotators conflicted. [sent-250, score-0.236]
</p><p>68 Constancy classification Figure 2 illustrates the recall-precision curve in constancy classification. [sent-265, score-0.666]
</p><p>69 Because we are unaware of any previous methods for classifying constant and non-constant relations, a simple method based on the cosine similarity was  Recall  Figure 3: Recall-precision curve (uniqueness classification). [sent-266, score-0.211]
</p><p>70 used as a baseline:  N1e∈∑EN(r)cos(Fw1(r,e),Fw2(r,e) , where the time windows w1 and w2 are determined as the first and last month in which the relation r is observed. [sent-267, score-0.395]
</p><p>71 A given relation is classified as nonconstant if the above similarity exceeds a threshold. [sent-268, score-0.306]
</p><p>72 Uniqueness classification Figure 3 illustrates the recall-precision curve in uniqueness classification. [sent-274, score-0.586]
</p><p>73 889  Recall  Figure 4: Comparison with the methods varying a value of N for constancy classification. [sent-280, score-0.546]
</p><p>74 Recall  Figure 5: Comparison with the methods varying a value of N for uniqueness classification. [sent-281, score-0.466]
</p><p>75 We hence conclude time-series information is useful for classifying not only constant but also unique relations. [sent-282, score-0.269]
</p><p>76 ll S feotr-  constancy classification and the better precision for uniqueness classification (Figures 4 and 5). [sent-286, score-1.15]
</p><p>77 Recall  Figure 6: Comparison with the methods using only a single value of T for constancy classification. [sent-288, score-0.546]
</p><p>78 0  TT == 11,3,6,12  Recall  Figure 7: Comparison with the methods using only a single value of T for uniqueness classification. [sent-290, score-0.466]
</p><p>79 4  Investigation into the window size, T  Our method uses multiple time windows of different sizes (i. [sent-292, score-0.208]
</p><p>80 The results in the uniqueness classification task  demonstrated that our method achieves better overall results than the methods using a single value of T. [sent-298, score-0.535]
</p><p>81 On the other hand, we could not confirm the effect of using multiple time windows of different sizes in the constancy classification task. [sent-300, score-0.767]
</p><p>82 5 Error analysis We randomly selected and analyzed 200 misclassified relations for both tasks. [sent-302, score-0.204]
</p><p>83 Paraphrases We observed that constant relations are prone to be miss-classified as non-constant when more than one paraphrase appear in arg2 and thus the value of arg2 is pretended to change. [sent-304, score-0.297]
</p><p>84 A similar problem was observed for unique relations as well. [sent-306, score-0.313]
</p><p>85 Topical bias Topics mentioned in the blog posts are sometimes biased, and such bias can have a neg-  ative effect on classification, especially when a relation takes a small number of entity types in arg2 for given arg1. [sent-307, score-0.448]
</p><p>86 i Short-/Long-term evolution Since we have aggregated on a monthly basis the 6-year’s worth of blog posts, the induced features cannot capture evolutions that occur in shorter or longer intervals. [sent-310, score-0.271]
</p><p>87 Reference to past, future, or speculative facts The blog authors sometimes refer to relations that do  not occur around when they write their posts; such relations actually occurred in the past, will occur in the future, or even speculative. [sent-314, score-0.498]
</p><p>88 Since our method exploits the time stamps attached to the posts to associate the relations with time, those relations introduce noises in the frequency distributions. [sent-315, score-0.731]
</p><p>89 6  Related Work  In recent years, much attention has been given to extracting relations from a massive amount of textual data, especially the web (cf. [sent-317, score-0.25]
</p><p>90 Most of those studies, however, explored just extracting relations from text. [sent-319, score-0.204]
</p><p>91 There has been no previous work on identifying the constancy of relations. [sent-321, score-0.606]
</p><p>92 Such temporal information alone is not sufficient for identifying the constancy of relations, while we think it would be helpful. [sent-330, score-0.657]
</p><p>93 On the other hand, the uniqueness of relations has so far been discussed in some studies. [sent-331, score-0.67]
</p><p>94 (2008) have pointed out the importance of identifying unique relations for various NLP tasks such as contradiction detection, quantifier scope disambiguation, and synonym resolution. [sent-333, score-0.402]
</p><p>95 They proposed an EM-style algorithm for scoring the uniqueness of relations. [sent-334, score-0.466]
</p><p>96 While those studies discussed the same problem as this paper, they did not point out the importance of the constancy in identifying unique relations (cf. [sent-337, score-0.919]
</p><p>97 7  Conclusion  This paper discussed that the notion of constancy is essential in compiling relations between entities extracted from real-world text and proposed a method for classifying relations on the basis of constancy and uniqueness. [sent-340, score-1.703]
</p><p>98 The time-series web text  was fully exploited to induce frequency-based features from time-series frequency distribution on relation instances as well as language-based features tailored for individual classification tasks. [sent-341, score-0.402]
</p><p>99 We will utilize the identified properties of the relations to adopt an appropriate strategy to compile 891 their instances. [sent-343, score-0.233]
</p><p>100 We consider that the notion of constancy will even be beneficial in acquiring world knowledge, other than relations between entities, from text; we aim at extending the notion of constancy to other types of knowledge involving real-world entities, such as concept-instance relations. [sent-345, score-1.296]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('constancy', 0.546), ('uniqueness', 0.466), ('relations', 0.204), ('relation', 0.186), ('president', 0.167), ('posts', 0.115), ('windows', 0.11), ('unique', 0.109), ('coordination', 0.107), ('yoshinaga', 0.106), ('frequency', 0.095), ('constant', 0.093), ('blog', 0.09), ('en', 0.08), ('japanese', 0.077), ('born', 0.073), ('borders', 0.073), ('classification', 0.069), ('keywords', 0.067), ('classifying', 0.067), ('verhagen', 0.066), ('wm', 0.066), ('tokyo', 0.064), ('fw', 0.064), ('headquartered', 0.064), ('nonconstant', 0.064), ('entities', 0.063), ('months', 0.061), ('identifying', 0.06), ('month', 0.057), ('entity', 0.057), ('exceeds', 0.056), ('window', 0.056), ('okyo', 0.055), ('induce', 0.052), ('temporal', 0.051), ('curve', 0.051), ('modifiers', 0.051), ('naoki', 0.049), ('particles', 0.049), ('nominal', 0.049), ('cues', 0.047), ('massive', 0.046), ('evolve', 0.046), ('contributed', 0.045), ('tense', 0.044), ('ritter', 0.043), ('time', 0.042), ('evolves', 0.042), ('father', 0.042), ('headquarters', 0.042), ('monthly', 0.042), ('nonunique', 0.042), ('outdated', 0.042), ('stamps', 0.042), ('supersede', 0.042), ('ynaga', 0.042), ('confirmed', 0.041), ('suffixes', 0.04), ('basis', 0.04), ('written', 0.037), ('honda', 0.036), ('sons', 0.036), ('accumulate', 0.036), ('kitsuregawa', 0.036), ('landis', 0.036), ('masaru', 0.036), ('evolutions', 0.036), ('italy', 0.036), ('timeseries', 0.036), ('gerhard', 0.034), ('george', 0.033), ('compiling', 0.033), ('timely', 0.033), ('bush', 0.033), ('induced', 0.033), ('annotators', 0.032), ('lin', 0.031), ('oren', 0.03), ('aggregated', 0.03), ('attaching', 0.03), ('ferrucci', 0.03), ('fleiss', 0.03), ('industrial', 0.03), ('functional', 0.03), ('marc', 0.029), ('eight', 0.029), ('belongs', 0.029), ('weikum', 0.029), ('noises', 0.029), ('contradiction', 0.029), ('compile', 0.029), ('conjunctive', 0.029), ('wu', 0.028), ('distributions', 0.028), ('zhu', 0.028), ('harvesting', 0.027), ('banko', 0.027), ('fired', 0.027), ('appearing', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999923 <a title="62-tfidf-1" href="./emnlp-2012-Identifying_Constant_and_Unique_Relations_by_using_Time-Series_Text.html">62 emnlp-2012-Identifying Constant and Unique Relations by using Time-Series Text</a></p>
<p>Author: Yohei Takaku ; Nobuhiro Kaji ; Naoki Yoshinaga ; Masashi Toyoda</p><p>Abstract: Because the real world evolves over time, numerous relations between entities written in presently available texts are already obsolete or will potentially evolve in the future. This study aims at resolving the intricacy in consistently compiling relations extracted from text, and presents a method for identifying constancy and uniqueness of the relations in the context of supervised learning. We exploit massive time-series web texts to induce features on the basis of time-series frequency and linguistic cues. Experimental results confirmed that the time-series frequency distributions contributed much to the recall of constancy identification and the precision of the uniqueness identification.</p><p>2 0.14218543 <a title="62-tfidf-2" href="./emnlp-2012-Ensemble_Semantics_for_Large-scale_Unsupervised_Relation_Extraction.html">40 emnlp-2012-Ensemble Semantics for Large-scale Unsupervised Relation Extraction</a></p>
<p>Author: Bonan Min ; Shuming Shi ; Ralph Grishman ; Chin-Yew Lin</p><p>Abstract: Discovering significant types of relations from the web is challenging because of its open nature. Unsupervised algorithms are developed to extract relations from a corpus without knowing the relations in advance, but most of them rely on tagging arguments of predefined types. Recently, a new algorithm was proposed to jointly extract relations and their argument semantic classes, taking a set of relation instances extracted by an open IE algorithm as input. However, it cannot handle polysemy of relation phrases and fails to group many similar (“synonymous”) relation instances because of the sparseness of features. In this paper, we present a novel unsupervised algorithm that provides a more general treatment of the polysemy and synonymy problems. The algorithm incorporates various knowledge sources which we will show to be very effective for unsupervised extraction. Moreover, it explicitly disambiguates polysemous relation phrases and groups synonymous ones. While maintaining approximately the same precision, the algorithm achieves significant improvement on recall compared to the previous method. It is also very efficient. Experiments on a realworld dataset show that it can handle 14.7 million relation instances and extract a very large set of relations from the web. Ralph Grishman1 Chin-Yew Lin2 2Microsoft Research Asia Beijing, China { shumings cyl } @mi cro s o ft . com , that has many applications in answering factoid questions, building knowledge bases and improving search engine relevance. The web has become a massive potential source of such relations. However, its open nature brings an open-ended set of relation types. To extract these relations, a system should not assume a fixed set of relation types, nor rely on a fixed set of relation argument types. The past decade has seen some promising solutions, unsupervised relation extraction (URE) algorithms that extract relations from a corpus without knowing the relations in advance. However, most algorithms (Hasegawa et al., 2004, Shinyama and Sekine, 2006, Chen et. al, 2005) rely on tagging predefined types of entities as relation arguments, and thus are not well-suited for the open domain. Recently, Kok and Domingos (2008) proposed Semantic Network Extractor (SNE), which generates argument semantic classes and sets of synonymous relation phrases at the same time, thus avoiding the requirement of tagging relation arguments of predefined types. However, SNE has 2 limitations: 1) Following previous URE algorithms, it only uses features from the set of input relation instances for clustering. Empirically we found that it fails to group many relevant relation instances. These features, such as the surface forms of arguments and lexical sequences in between, are very sparse in practice. In contrast, there exist several well-known corpus-level semantic resources that can be automatically derived from a source corpus and are shown to be useful for generating the key elements of a relation: its 2 argument semantic classes and a set of synonymous phrases. For example, semantic classes can be derived from a source corpus with contextual distributional simi1 Introduction Relation extraction aims at discovering semantic larity and web table co-occurrences. The “synonymy” 1 problem for clustering relation instances relations between entities. It is an important task * Work done during an internship at Microsoft Research Asia 1027 LParnogcue agdein Lgesa ornf tihneg, 2 p0a1g2e Jso 1in02t C7–o1n0f3e7re,n Jce ju on Is Elanmdp,ir Kicoarlea M,e 1t2h–o1d4s J iunly N 2a0tu1r2a.l ? Lc a2n0g1u2ag Aes Psorcoicaetsiosin fgo arn Cdo Cmopmutpauti oantiaoln Lailn Ngautiustriacls could potentially be better solved by adding these resources. 2) SNE assumes that each entity or relation phrase belongs to exactly one cluster, thus is not able to effectively handle polysemy of relation phrases2. An example of a polysemous phrase is be the currency of as in 2 triples</p><p>3 0.10710189 <a title="62-tfidf-3" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>Author: Thomas Lin ; Mausam ; Oren Etzioni</p><p>Abstract: Entity linking systems link noun-phrase mentions in text to their corresponding Wikipedia articles. However, NLP applications would gain from the ability to detect and type all entities mentioned in text, including the long tail of entities not prominent enough to have their own Wikipedia articles. In this paper we show that once the Wikipedia entities mentioned in a corpus of textual assertions are linked, this can further enable the detection and fine-grained typing of the unlinkable entities. Our proposed method for detecting unlinkable entities achieves 24% greater accuracy than a Named Entity Recognition baseline, and our method for fine-grained typing is able to propagate over 1,000 types from linked Wikipedia entities to unlinkable entities. Detection and typing of unlinkable entities can increase yield for NLP applications such as typed question answering.</p><p>4 0.10375594 <a title="62-tfidf-4" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>Author: Mihai Surdeanu ; Julie Tibshirani ; Ramesh Nallapati ; Christopher D. Manning</p><p>Abstract: Distant supervision for relation extraction (RE) gathering training data by aligning a database of facts with text – is an efficient approach to scale RE to thousands of different relations. However, this introduces a challenging learning scenario where the relation expressed by a pair of entities found in a sentence is unknown. For example, a sentence containing Balzac and France may express BornIn or Died, an unknown relation, or no relation at all. Because of this, traditional supervised learning, which assumes that each example is explicitly mapped to a label, is not appropriate. We propose a novel approach to multi-instance multi-label learning for RE, which jointly models all the instances of a pair of entities in text and all their labels using a graphical model with latent variables. Our model performs competitively on two difficult domains. –</p><p>5 0.093007721 <a title="62-tfidf-5" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms ofweak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.</p><p>6 0.081674397 <a title="62-tfidf-6" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>7 0.079624034 <a title="62-tfidf-7" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>8 0.079309657 <a title="62-tfidf-8" href="./emnlp-2012-Learning_Verb_Inference_Rules_from_Linguistically-Motivated_Evidence.html">80 emnlp-2012-Learning Verb Inference Rules from Linguistically-Motivated Evidence</a></p>
<p>9 0.075539373 <a title="62-tfidf-9" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>10 0.072931424 <a title="62-tfidf-10" href="./emnlp-2012-Open_Language_Learning_for_Information_Extraction.html">100 emnlp-2012-Open Language Learning for Information Extraction</a></p>
<p>11 0.068343222 <a title="62-tfidf-11" href="./emnlp-2012-PATTY%3A_A_Taxonomy_of_Relational_Patterns_with_Semantic_Types.html">103 emnlp-2012-PATTY: A Taxonomy of Relational Patterns with Semantic Types</a></p>
<p>12 0.066479094 <a title="62-tfidf-12" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>13 0.060450166 <a title="62-tfidf-13" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>14 0.058174662 <a title="62-tfidf-14" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>15 0.055157769 <a title="62-tfidf-15" href="./emnlp-2012-N-gram-based_Tense_Models_for_Statistical_Machine_Translation.html">95 emnlp-2012-N-gram-based Tense Models for Statistical Machine Translation</a></p>
<p>16 0.052800138 <a title="62-tfidf-16" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>17 0.049943332 <a title="62-tfidf-17" href="./emnlp-2012-Learning-based_Multi-Sieve_Co-reference_Resolution_with_Knowledge.html">76 emnlp-2012-Learning-based Multi-Sieve Co-reference Resolution with Knowledge</a></p>
<p>18 0.046800788 <a title="62-tfidf-18" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>19 0.045910321 <a title="62-tfidf-19" href="./emnlp-2012-Bilingual_Lexicon_Extraction_from_Comparable_Corpora_Using_Label_Propagation.html">25 emnlp-2012-Bilingual Lexicon Extraction from Comparable Corpora Using Label Propagation</a></p>
<p>20 0.043689817 <a title="62-tfidf-20" href="./emnlp-2012-An_Entity-Topic_Model_for_Entity_Linking.html">19 emnlp-2012-An Entity-Topic Model for Entity Linking</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.176), (1, 0.152), (2, -0.011), (3, -0.046), (4, 0.043), (5, -0.018), (6, 0.076), (7, 0.16), (8, -0.137), (9, 0.017), (10, -0.063), (11, -0.063), (12, -0.096), (13, -0.064), (14, -0.061), (15, -0.016), (16, -0.178), (17, -0.021), (18, 0.018), (19, -0.027), (20, -0.075), (21, -0.097), (22, 0.101), (23, 0.065), (24, -0.018), (25, -0.098), (26, 0.051), (27, 0.01), (28, 0.011), (29, 0.024), (30, 0.034), (31, 0.013), (32, -0.011), (33, 0.248), (34, 0.001), (35, -0.023), (36, 0.016), (37, -0.078), (38, 0.014), (39, -0.006), (40, 0.013), (41, 0.018), (42, 0.04), (43, 0.074), (44, -0.057), (45, 0.111), (46, 0.036), (47, -0.07), (48, -0.106), (49, 0.06)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95605415 <a title="62-lsi-1" href="./emnlp-2012-Identifying_Constant_and_Unique_Relations_by_using_Time-Series_Text.html">62 emnlp-2012-Identifying Constant and Unique Relations by using Time-Series Text</a></p>
<p>Author: Yohei Takaku ; Nobuhiro Kaji ; Naoki Yoshinaga ; Masashi Toyoda</p><p>Abstract: Because the real world evolves over time, numerous relations between entities written in presently available texts are already obsolete or will potentially evolve in the future. This study aims at resolving the intricacy in consistently compiling relations extracted from text, and presents a method for identifying constancy and uniqueness of the relations in the context of supervised learning. We exploit massive time-series web texts to induce features on the basis of time-series frequency and linguistic cues. Experimental results confirmed that the time-series frequency distributions contributed much to the recall of constancy identification and the precision of the uniqueness identification.</p><p>2 0.76640576 <a title="62-lsi-2" href="./emnlp-2012-Ensemble_Semantics_for_Large-scale_Unsupervised_Relation_Extraction.html">40 emnlp-2012-Ensemble Semantics for Large-scale Unsupervised Relation Extraction</a></p>
<p>Author: Bonan Min ; Shuming Shi ; Ralph Grishman ; Chin-Yew Lin</p><p>Abstract: Discovering significant types of relations from the web is challenging because of its open nature. Unsupervised algorithms are developed to extract relations from a corpus without knowing the relations in advance, but most of them rely on tagging arguments of predefined types. Recently, a new algorithm was proposed to jointly extract relations and their argument semantic classes, taking a set of relation instances extracted by an open IE algorithm as input. However, it cannot handle polysemy of relation phrases and fails to group many similar (“synonymous”) relation instances because of the sparseness of features. In this paper, we present a novel unsupervised algorithm that provides a more general treatment of the polysemy and synonymy problems. The algorithm incorporates various knowledge sources which we will show to be very effective for unsupervised extraction. Moreover, it explicitly disambiguates polysemous relation phrases and groups synonymous ones. While maintaining approximately the same precision, the algorithm achieves significant improvement on recall compared to the previous method. It is also very efficient. Experiments on a realworld dataset show that it can handle 14.7 million relation instances and extract a very large set of relations from the web. Ralph Grishman1 Chin-Yew Lin2 2Microsoft Research Asia Beijing, China { shumings cyl } @mi cro s o ft . com , that has many applications in answering factoid questions, building knowledge bases and improving search engine relevance. The web has become a massive potential source of such relations. However, its open nature brings an open-ended set of relation types. To extract these relations, a system should not assume a fixed set of relation types, nor rely on a fixed set of relation argument types. The past decade has seen some promising solutions, unsupervised relation extraction (URE) algorithms that extract relations from a corpus without knowing the relations in advance. However, most algorithms (Hasegawa et al., 2004, Shinyama and Sekine, 2006, Chen et. al, 2005) rely on tagging predefined types of entities as relation arguments, and thus are not well-suited for the open domain. Recently, Kok and Domingos (2008) proposed Semantic Network Extractor (SNE), which generates argument semantic classes and sets of synonymous relation phrases at the same time, thus avoiding the requirement of tagging relation arguments of predefined types. However, SNE has 2 limitations: 1) Following previous URE algorithms, it only uses features from the set of input relation instances for clustering. Empirically we found that it fails to group many relevant relation instances. These features, such as the surface forms of arguments and lexical sequences in between, are very sparse in practice. In contrast, there exist several well-known corpus-level semantic resources that can be automatically derived from a source corpus and are shown to be useful for generating the key elements of a relation: its 2 argument semantic classes and a set of synonymous phrases. For example, semantic classes can be derived from a source corpus with contextual distributional simi1 Introduction Relation extraction aims at discovering semantic larity and web table co-occurrences. The “synonymy” 1 problem for clustering relation instances relations between entities. It is an important task * Work done during an internship at Microsoft Research Asia 1027 LParnogcue agdein Lgesa ornf tihneg, 2 p0a1g2e Jso 1in02t C7–o1n0f3e7re,n Jce ju on Is Elanmdp,ir Kicoarlea M,e 1t2h–o1d4s J iunly N 2a0tu1r2a.l ? Lc a2n0g1u2ag Aes Psorcoicaetsiosin fgo arn Cdo Cmopmutpauti oantiaoln Lailn Ngautiustriacls could potentially be better solved by adding these resources. 2) SNE assumes that each entity or relation phrase belongs to exactly one cluster, thus is not able to effectively handle polysemy of relation phrases2. An example of a polysemous phrase is be the currency of as in 2 triples</p><p>3 0.68905097 <a title="62-lsi-3" href="./emnlp-2012-Open_Language_Learning_for_Information_Extraction.html">100 emnlp-2012-Open Language Learning for Information Extraction</a></p>
<p>Author: Mausam ; Michael Schmitz ; Stephen Soderland ; Robert Bart ; Oren Etzioni</p><p>Abstract: Open Information Extraction (IE) systems extract relational tuples from text, without requiring a pre-specified vocabulary, by identifying relation phrases and associated arguments in arbitrary sentences. However, stateof-the-art Open IE systems such as REVERB and WOE share two important weaknesses (1) they extract only relations that are mediated by verbs, and (2) they ignore context, thus extracting tuples that are not asserted as factual. This paper presents OLLIE, a substantially improved Open IE system that addresses both these limitations. First, OLLIE achieves high yield by extracting relations mediated by nouns, adjectives, and more. Second, a context-analysis step increases precision by including contextual information from the sentence in the extractions. OLLIE obtains 2.7 times the area under precision-yield curve (AUC) compared to REVERB and 1.9 times the AUC of WOEparse. –</p><p>4 0.63095713 <a title="62-lsi-4" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>Author: Mihai Surdeanu ; Julie Tibshirani ; Ramesh Nallapati ; Christopher D. Manning</p><p>Abstract: Distant supervision for relation extraction (RE) gathering training data by aligning a database of facts with text – is an efficient approach to scale RE to thousands of different relations. However, this introduces a challenging learning scenario where the relation expressed by a pair of entities found in a sentence is unknown. For example, a sentence containing Balzac and France may express BornIn or Died, an unknown relation, or no relation at all. Because of this, traditional supervised learning, which assumes that each example is explicitly mapped to a label, is not appropriate. We propose a novel approach to multi-instance multi-label learning for RE, which jointly models all the instances of a pair of entities in text and all their labels using a graphical model with latent variables. Our model performs competitively on two difficult domains. –</p><p>5 0.56141269 <a title="62-lsi-5" href="./emnlp-2012-Learning_Verb_Inference_Rules_from_Linguistically-Motivated_Evidence.html">80 emnlp-2012-Learning Verb Inference Rules from Linguistically-Motivated Evidence</a></p>
<p>Author: Hila Weisman ; Jonathan Berant ; Idan Szpektor ; Ido Dagan</p><p>Abstract: Learning inference relations between verbs is at the heart of many semantic applications. However, most prior work on learning such rules focused on a rather narrow set of information sources: mainly distributional similarity, and to a lesser extent manually constructed verb co-occurrence patterns. In this paper, we claim that it is imperative to utilize information from various textual scopes: verb co-occurrence within a sentence, verb cooccurrence within a document, as well as overall corpus statistics. To this end, we propose a much richer novel set of linguistically motivated cues for detecting entailment between verbs and combine them as features in a supervised classification framework. We empirically demonstrate that our model significantly outperforms previous methods and that information from each textual scope contributes to the verb entailment learning task.</p><p>6 0.48465031 <a title="62-lsi-6" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>7 0.45417649 <a title="62-lsi-7" href="./emnlp-2012-N-gram-based_Tense_Models_for_Statistical_Machine_Translation.html">95 emnlp-2012-N-gram-based Tense Models for Statistical Machine Translation</a></p>
<p>8 0.4299323 <a title="62-lsi-8" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>9 0.41993976 <a title="62-lsi-9" href="./emnlp-2012-PATTY%3A_A_Taxonomy_of_Relational_Patterns_with_Semantic_Types.html">103 emnlp-2012-PATTY: A Taxonomy of Relational Patterns with Semantic Types</a></p>
<p>10 0.41446745 <a title="62-lsi-10" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>11 0.38428327 <a title="62-lsi-11" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>12 0.37756765 <a title="62-lsi-12" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>13 0.37241477 <a title="62-lsi-13" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>14 0.33044708 <a title="62-lsi-14" href="./emnlp-2012-Local_and_Global_Context_for_Supervised_and_Unsupervised_Metonymy_Resolution.html">85 emnlp-2012-Local and Global Context for Supervised and Unsupervised Metonymy Resolution</a></p>
<p>15 0.283833 <a title="62-lsi-15" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>16 0.28294411 <a title="62-lsi-16" href="./emnlp-2012-A_Statistical_Relational_Learning_Approach_to_Identifying_Evidence_Based_Medicine_Categories.html">10 emnlp-2012-A Statistical Relational Learning Approach to Identifying Evidence Based Medicine Categories</a></p>
<p>17 0.26219708 <a title="62-lsi-17" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>18 0.25975493 <a title="62-lsi-18" href="./emnlp-2012-Excitatory_or_Inhibitory%3A_A_New_Semantic_Orientation_Extracts_Contradiction_and_Causality_from_the_Web.html">44 emnlp-2012-Excitatory or Inhibitory: A New Semantic Orientation Extracts Contradiction and Causality from the Web</a></p>
<p>19 0.24316154 <a title="62-lsi-19" href="./emnlp-2012-User_Demographics_and_Language_in_an_Implicit_Social_Network.html">134 emnlp-2012-User Demographics and Language in an Implicit Social Network</a></p>
<p>20 0.23651299 <a title="62-lsi-20" href="./emnlp-2012-Bilingual_Lexicon_Extraction_from_Comparable_Corpora_Using_Label_Propagation.html">25 emnlp-2012-Bilingual Lexicon Extraction from Comparable Corpora Using Label Propagation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.45), (16, 0.028), (25, 0.016), (34, 0.047), (60, 0.09), (63, 0.042), (64, 0.033), (65, 0.058), (70, 0.022), (73, 0.013), (74, 0.026), (76, 0.05), (80, 0.015), (86, 0.016), (95, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86793786 <a title="62-lda-1" href="./emnlp-2012-Generative_Goal-Driven_User_Simulation_for_Dialog_Management.html">60 emnlp-2012-Generative Goal-Driven User Simulation for Dialog Management</a></p>
<p>Author: Aciel Eshky ; Ben Allison ; Mark Steedman</p><p>Abstract: User simulation is frequently used to train statistical dialog managers for task-oriented domains. At present, goal-driven simulators (those that have a persistent notion of what they wish to achieve in the dialog) require some task-specific engineering, making them impossible to evaluate intrinsically. Instead, they have been evaluated extrinsically by means of the dialog managers they are intended to train, leading to circularity of argument. In this paper, we propose the first fully generative goal-driven simulator that is fully induced from data, without hand-crafting or goal annotation. Our goals are latent, and take the form of topics in a topic model, clustering together semantically equivalent and phonetically confusable strings, implicitly modelling synonymy and speech recognition noise. We evaluate on two standard dialog resources, the Communicator and Let’s Go datasets, and demonstrate that our model has substantially better fit to held out data than competing approaches. We also show that features derived from our model allow significantly greater improvement over a baseline at distinguishing real from randomly permuted dialogs.</p><p>same-paper 2 0.867679 <a title="62-lda-2" href="./emnlp-2012-Identifying_Constant_and_Unique_Relations_by_using_Time-Series_Text.html">62 emnlp-2012-Identifying Constant and Unique Relations by using Time-Series Text</a></p>
<p>Author: Yohei Takaku ; Nobuhiro Kaji ; Naoki Yoshinaga ; Masashi Toyoda</p><p>Abstract: Because the real world evolves over time, numerous relations between entities written in presently available texts are already obsolete or will potentially evolve in the future. This study aims at resolving the intricacy in consistently compiling relations extracted from text, and presents a method for identifying constancy and uniqueness of the relations in the context of supervised learning. We exploit massive time-series web texts to induce features on the basis of time-series frequency and linguistic cues. Experimental results confirmed that the time-series frequency distributions contributed much to the recall of constancy identification and the precision of the uniqueness identification.</p><p>3 0.73810107 <a title="62-lda-3" href="./emnlp-2012-Exploiting_Reducibility_in_Unsupervised_Dependency_Parsing.html">46 emnlp-2012-Exploiting Reducibility in Unsupervised Dependency Parsing</a></p>
<p>Author: David Marecek ; Zdene20 ek Zabokrtsky</p><p>Abstract: The possibility of deleting a word from a sentence without violating its syntactic correctness belongs to traditionally known manifestations of syntactic dependency. We introduce a novel unsupervised parsing approach that is based on a new n-gram reducibility measure. We perform experiments across 18 languages available in CoNLL data and we show that our approach achieves better accuracy for the majority of the languages then previously reported results.</p><p>4 0.39719149 <a title="62-lda-4" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>Author: Quang Do ; Wei Lu ; Dan Roth</p><p>Abstract: This paper addresses the task of constructing a timeline of events mentioned in a given text. To accomplish that, we present a novel representation of the temporal structure of a news article based on time intervals. We then present an algorithmic approach that jointly optimizes the temporal structure by coupling local classifiers that predict associations and temporal relations between pairs of temporal entities with global constraints. Moreover, we present ways to leverage knowledge provided by event coreference to further improve the system performance. Overall, our experiments show that the joint inference model significantly outperformed the local classifiers by 9.2% of relative improvement in F1. The experiments also suggest that good event coreference could make remarkable contribution to a robust event timeline construction system.</p><p>5 0.38404763 <a title="62-lda-5" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>Author: David McClosky ; Christopher D. Manning</p><p>Abstract: We present a distantly supervised system for extracting the temporal bounds of fluents (relations which only hold during certain times, such as attends school). Unlike previous pipelined approaches, our model does not assume independence between each fluent or even between named entities with known connections (parent, spouse, employer, etc.). Instead, we model what makes timelines of fluents consistent by learning cross-fluent constraints, potentially spanning entities as well. For example, our model learns that someone is unlikely to start a job at age two or to marry someone who hasn’t been born yet. Our system achieves a 36% error reduction over a pipelined baseline.</p><p>6 0.37432334 <a title="62-lda-6" href="./emnlp-2012-Besting_the_Quiz_Master%3A_Crowdsourcing_Incremental_Classification_Games.html">23 emnlp-2012-Besting the Quiz Master: Crowdsourcing Incremental Classification Games</a></p>
<p>7 0.37361357 <a title="62-lda-7" href="./emnlp-2012-Revisiting_the_Predictability_of_Language%3A_Response_Completion_in_Social_Media.html">114 emnlp-2012-Revisiting the Predictability of Language: Response Completion in Social Media</a></p>
<p>8 0.37219256 <a title="62-lda-8" href="./emnlp-2012-Streaming_Analysis_of_Discourse_Participants.html">120 emnlp-2012-Streaming Analysis of Discourse Participants</a></p>
<p>9 0.36889347 <a title="62-lda-9" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>10 0.36581251 <a title="62-lda-10" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>11 0.35716444 <a title="62-lda-11" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<p>12 0.35478371 <a title="62-lda-12" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>13 0.35476771 <a title="62-lda-13" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>14 0.35381514 <a title="62-lda-14" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>15 0.35233366 <a title="62-lda-15" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>16 0.352166 <a title="62-lda-16" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>17 0.35212123 <a title="62-lda-17" href="./emnlp-2012-Optimising_Incremental_Dialogue_Decisions_Using_Information_Density_for_Interactive_Systems.html">102 emnlp-2012-Optimising Incremental Dialogue Decisions Using Information Density for Interactive Systems</a></p>
<p>18 0.34681579 <a title="62-lda-18" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>19 0.34592542 <a title="62-lda-19" href="./emnlp-2012-Improving_Transition-Based_Dependency_Parsing_with_Buffer_Transitions.html">66 emnlp-2012-Improving Transition-Based Dependency Parsing with Buffer Transitions</a></p>
<p>20 0.34485111 <a title="62-lda-20" href="./emnlp-2012-Concurrent_Acquisition_of_Word_Meaning_and_Lexical_Categories.html">29 emnlp-2012-Concurrent Acquisition of Word Meaning and Lexical Categories</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
