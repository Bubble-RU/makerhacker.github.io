<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-70" href="#">emnlp2012-70</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</h1>
<br/><p>Source: <a title="emnlp-2012-70-pdf" href="http://aclweb.org/anthology//D/D12/D12-1046.pdf">pdf</a></p><p>Author: Xian Qian ; Yang Liu</p><p>Abstract: In this paper, we propose a novel decoding algorithm for discriminative joint Chinese word segmentation, part-of-speech (POS) tagging, and parsing. Previous work often used a pipeline method Chinese word segmentation followed by POS tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components. In our approach, we train the three individual models separately during training, and incorporate them together in a unified framework during decoding. We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features. As far as we know, this is the first work on joint Chinese word segmentation, POS tagging and parsing. Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system. –</p><p>Reference: <a title="emnlp-2012-70-reference" href="../emnlp2012_reference/emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract In this paper, we propose a novel decoding algorithm for discriminative joint Chinese word segmentation, part-of-speech (POS) tagging, and parsing. [sent-2, score-0.286]
</p><p>2 Previous work often used a pipeline method Chinese word segmentation followed by POS tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components. [sent-3, score-0.951]
</p><p>3 We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features. [sent-5, score-0.845]
</p><p>4 As far as we know, this is the first work on joint Chinese word segmentation, POS tagging and parsing. [sent-6, score-0.411]
</p><p>5 –  1 Introduction For Asian languages such as Japanese and Chinese that do not contain explicitly marked word boundaries, word segmentation is an important first step for many subsequent language processing tasks, such as POS tagging, parsing, semantic role labeling, and various applications. [sent-8, score-0.533]
</p><p>6 Previous studies for POS tagging and syntax parsing on these languages sometimes assume that gold standard word segmentation information is provided, which is not the real scenario. [sent-9, score-0.877]
</p><p>7 edu a  501  first segmented into word sequences, then POS tagging and parsing are performed. [sent-12, score-0.462]
</p><p>8 For example, word segmentation errors will result in tagging and parsing errors. [sent-14, score-0.845]
</p><p>9 , 2001) to train the word segmentation model and POS tagging model, and averaged perceptron (Collins, 2002) to learn the parsing model. [sent-21, score-0.891]
</p><p>10 When searching the parse tree, the word and POS tagging features are dynamically generated and the transition information of POS tagging is considered in the span merge operation. [sent-25, score-0.756]
</p><p>11 We compare our proposed joint model with the pipeline system, both built using the state-of-the-art sub-  models. [sent-27, score-0.383]
</p><p>12 lc L2a0n1g2ua Agseso Pcrioactieosnsi fnogr a Cnodm Cpoumtaptiuotna tilo Lnianlg Nuaist uircasl calculate the bracket scores for parsing in the face of word segmentation errors. [sent-30, score-0.719]
</p><p>13 Our experimental results show that the joint model significantly outperforms the pipeline method based on the state-of-the-art sub-models. [sent-31, score-0.383]
</p><p>14 Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al. [sent-33, score-0.882]
</p><p>15 Recently, joint tagging anntd o dvee-r pendency parsing has been studied as well (Li et al. [sent-40, score-0.5]
</p><p>16 Previous research has showed that word segmentation has a great impact on parsing accuracy in −  the pipeline method (Harper and Huang, 2009). [sent-43, score-0.863]
</p><p>17 , 2009), additional data was used to improve Chinese word segmentation, which resulted in significant improvement on the parsing task using the pipeline framework. [sent-45, score-0.422]
</p><p>18 Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010). [sent-46, score-0.576]
</p><p>19 A study that is closely related to ours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew. [sent-47, score-0.718]
</p><p>20 Another main difference is that, besides segmentation and parsing, we also incorporate the POS tagging model into the CYK parsing framework. [sent-49, score-0.799]
</p><p>21 A joint model is expected to make more optimal decisions than a pipeline approach; however, such a model will be too complex and it is difficult to estimate model pa-  rameters. [sent-51, score-0.383]
</p><p>22 1 Word Segmentation Model Methods for Chinese word segmentation can be broadly categorized into character based and word based models. [sent-56, score-0.696]
</p><p>23 , cl (where ci is the ith Chinese character, l is the sentence length), the character-based model assigns each character with a word boundary tag. [sent-63, score-0.361]
</p><p>24 Here we use the BCDIES tag set, which achieved  the best official performance (Zhao and Kit, 2008): B, C, D, E denote the first, second, third, and last character of a multi-character word respectively, I denotes the other characters, and S denotes the single character word. [sent-64, score-0.498]
</p><p>25 The top K candidate segmentation results for each training sample are generated using the character-based model, and the gold segmentation is added if it is not in the candidate set. [sent-77, score-0.953]
</p><p>26 A problem arises when combining the two mod-  els and using it in joint segmentation and parsing, since the linear chain used in the character-based model is incompatible with CYK parsing model and the word-based model due to the transition informaCharacter Level Feature Templates (1. [sent-79, score-0.877]
</p><p>27 6) (1  subword(w) character bigrams within w  Table 1: Feature templates for word segmentation. [sent-85, score-0.288]
</p><p>28 These word scores will be used in the joint segmentation and parsing task Section 3. [sent-112, score-0.764]
</p><p>29 Second, the state-of-the-art POS tagging systems are often trained by sequence labeling models, not parsing models. [sent-119, score-0.358]
</p><p>30 The POS tagging problem is to assign a POS tag t ∈ T to each word in a sentence. [sent-124, score-0.365]
</p><p>31 Three feature sets are considered: (i) word level features, including surrounding word unigrams, bigrams, and word length; (ii) character level features, such as the first and last characters in the words; (iii) transition features. [sent-129, score-0.491]
</p><p>32 3 Parsing Model We choose discriminative models for parsing since it is easy to handle unknown words by simply adding character level features. [sent-131, score-0.298]
</p><p>33 We used the same preprocessing step as (Harper and Huang, 2009), collapsing all the allowed nonterminal-yield unary chains to single unary rules. [sent-145, score-0.468]
</p><p>34 To facilitate decoding, we unify the form of spans so that each span contains exactly one unary rule. [sent-147, score-0.385]
</p><p>35 This is done by adding identity unary rules (N → N ) to spans that have no unary rule. [sent-148, score-0.547]
</p><p>36 Hence, there are two states of a span: the top state N and the bottom state N that correspond to the left  runary  and right hand of the unary rule =N→ N respectively, as shown in Figure 2. [sent-151, score-0.717]
</p><p>37 There are 4 feature sets: (i) bottom state features fbottom(i, j,x, Ni,j), which depend on the bot504  CIP VP  top stateCP bottom  state  VP  NNTP  VV  NNTP  VVVV  ! [sent-153, score-0.459]
</p><p>38 Nonterminal-yield unary chains are collapsed to single unary rules. [sent-157, score-0.468]
</p><p>39 Identity unary rules are added to spans that have no unary rule. [sent-158, score-0.511]
</p><p>40 In test-  Table  3: Feature  templates for parsing, where X can be word, first and last character of word, first and last character  bigram of word, POS tag. [sent-165, score-0.465]
</p><p>41 Parameters of word segmentation (θseg), POS tagging (θpos), and parsing models (θparse θbottom⊕top⊕ are scaled by three positive hyper-parameters α, and γ respectively, which control their contribution in the joint model. [sent-172, score-1.04]
</p><p>42 If α >> >> γ, then the joint model is equivalent to a pipeline model, in which there is no feedback from downstream models to upstream ones. [sent-173, score-0.383]
</p><p>43 For well tuned hyper-parameters, we expect that segmentation and POS tagging results can be improved by parsing information. [sent-174, score-0.799]
</p><p>44 The basic idea of our decoding algorithm is to extend the CYK parsing algorithm so that it can deal with transition features in POS tagging and segmentation scores in word segmentation. [sent-177, score-1.059]
</p><p>45 Surrounding words are important features for 505 POS tagging and parsing; however, they are unavailable because segmentation is incomplete before parsing. [sent-186, score-0.664]
</p><p>46 In the forward direction, the algorithm starts from the first character boundary to the last, and finds the best previous word for the ith character boundary bi. [sent-195, score-0.51]
</p><p>47 In Line 2, for each word candidate, we can calculate the score of each POS tag using state features in the POS tagging model, since the context words are available now. [sent-197, score-0.421]
</p><p>48 The score function of word wi,j with POS tag t is: scoreseg⊕pos(x, i,j, t) = scoreseg(x, i,j)  + θpos  ·  fpos (x, wi,j , t)  (2)  In Line 3, POS tags of surrounding words can be obtained similarly using bidirectional decoding. [sent-198, score-0.334]
</p><p>49 ,cl, beam size B, scaled word segmentation model, POS tagging model and parsing model. [sent-202, score-0.898]
</p><p>50 4: For each word candidate wi,j , 0 ≤ i ≤ j ≤ l− 1 5: For each bottom state N, P,O0S ≤ tag ≤t ∈ j T≤ ? [sent-204, score-0.352]
</p><p>51 step 1 (Line 14-15): get bottom states 16: For each top state N ? [sent-220, score-0.331]
</p><p>52 A span structure in the joint model is a 6-tuple: S(i,j, w, t,N, N), where i,j are the boundary indices, w, t are the word sequence and POS sequence within the span respectively, and N, N are the bottom and top states. [sent-227, score-0.666]
</p><p>53 The score of a basic span depends on its corresponding word and POS pair score, and the weights of the active state and unary features. [sent-229, score-0.477]
</p><p>54 To avoid enumerating the combination of the bottom and top states, initialization for each span is divided into 2 steps. [sent-230, score-0.301]
</p><p>55 In the first step, the score of every bottom state is calculated using bottom state features, and only the B best states are maintained (see Line 6-7). [sent-231, score-0.502]
</p><p>56 In the second step, top state features and unary rule features are used to get the score of each top state (Line 9), and only the top B states are preserved. [sent-232, score-0.577]
</p><p>57 The score bottom state N is calculated using binary features fbinary(x, i,j,k, w, t,N → Nr +Nr), bottom state  of the  feature(sx fbottom(x, i,j,w, t,N), and POS tag transition features that depend on the boundary POS tags of Sl and Sr. [sent-234, score-0.755]
</p><p>58 See Line 14 of Algorithm 1, where tllast and are the POS tags of the last word in the left child span and the first word in the right child span respectively. [sent-235, score-0.497]
</p><p>59 1Note that the joint task refers to automatic segmentation and tagging/parsing. [sent-251, score-0.583]
</p><p>60 It can be achieved using a pipeline system or our joint decoding method. [sent-252, score-0.481]
</p><p>61 507 For joint word segmentation and POS tagging, a word is correctly predicted if both the boundaries and the POS tag are correctly identified. [sent-253, score-0.806]
</p><p>62 For joint segmentation, POS tagging, and parsing task, when calculating the bracket scores using existing parseval tools, we need to consider possible word segmenta-  tion errors. [sent-254, score-0.452]
</p><p>63 To do this, we add the word boundary information in states a bracket is correct only if its boundaries, label and word segmentation are all correct. [sent-255, score-0.781]
</p><p>64 If the segmentation is perfect, then the bracket scores of the modified tree are exactly the same as the original tree. [sent-260, score-0.574]
</p><p>65 This is similar to evaluating parsing performance on speech transcripts with automatic sentence segmentation (Roark et al. [sent-261, score-0.576]
</p><p>66 " Shanghai  -  -  -  # $ % office  Figure 3: Boundary information is added to states to calculate the bracket scores in the face of word segmentation errors. [sent-265, score-0.709]
</p><p>67 3  Parameter Estimation  We train three submodels using the gold features, that is, POS tagger is trained using the perfect segmentation, and parser is trained using perfect segmentation and POS tags. [sent-269, score-0.562]
</p><p>68 4 Experimental Results In this section we first show that our sub-models are better than or comparable to state-of-the-art systems,  and then the joint model is superior to the pipeline approach. [sent-288, score-0.383]
</p><p>69 1 Evaluating Sub-models Table 7 shows word segmentation results using our word segmentation submodel, in comparison to a few state-of-the-art systems. [sent-291, score-0.974]
</p><p>70 Since the one without transition features can be naturally integrated into the joint system, we use it in the following joint tasks. [sent-296, score-0.4]
</p><p>71 For the POS tagging only task that takes gold standard word segmentation as input, we have two systems. [sent-315, score-0.742]
</p><p>72 However, since there are no reported results for this setup, we demonstrate the competence ofour POS tagger using the joint word segmentation and POS tagging task. [sent-324, score-0.9]
</p><p>73 Table 8 shows the performance of a few systems along with ours, all using the pipeline approach where automatic segmentation is followed –  by POS tagging. [sent-325, score-0.682]
</p><p>74 53 Table 8: Results for the joint word segmentation and POS tagging task. [sent-341, score-0.852]
</p><p>75 Finally Table 10 shows the results for the three tasks using our joint decoding method in comparison to the pipeline method. [sent-358, score-0.481]
</p><p>76 We can see that the joint model outperforms the pipeline one. [sent-359, score-0.383]
</p><p>77 509  Table 10: Results for the joint segmentation, tagging, and parsing task using pipeline and joint models. [sent-368, score-0.66]
</p><p>78 5 Error Analysis We compared the results from the pipeline and our joint decoding systems in order to understand the impact of the joint model on word segmentation and POS tagging. [sent-370, score-1.11]
</p><p>79 We notice that the joint model tend to generate more words than the pipeline model. [sent-371, score-0.383]
</p><p>80 For example, “巴 尔 一 行” is one word in the pipeline model, but correctly segmented as two words “ 巴 尔/一行” in the joint model. [sent-372, score-0.487]
</p><p>81 This tendency of segmentation also makes it fail to recognize some long words, especially OOV words. [sent-373, score-0.441]
</p><p>82 In the data set, we find that, the joint model corrected 10 missing boundaries over the pipeline method, and introduced 3 false positive segmentation errors. [sent-375, score-0.859]
</p><p>83 For the analysis of POS tags, we only examined the words that are correctly segmented by both the pipeline and the joint models. [sent-376, score-0.441]
</p><p>84 Table 11 shows the increase and decrease of error patterns of the joint model over the pipeline POS tagger. [sent-377, score-0.383]
</p><p>85 # means the error number ofthe corresponding pattern made by the pipeline tagging model. [sent-387, score-0.464]
</p><p>86 In the future work, we will compare this joint model to the pipeline approach that uses multiple candidates or soft decisions in the early modules. [sent-392, score-0.383]
</p><p>87 We will also investigate methods for joint learning as well as ways to speed up the joint decoding algorithm. [sent-393, score-0.382]
</p><p>88 A single generative model for joint morphological segmentation and syntactic parsing. [sent-413, score-0.583]
</p><p>89 Incremental joint pos tagging and dependency parsing in chinese. [sent-426, score-0.845]
</p><p>90 A cascaded linear model for joint chinese word segmentation and part-of-speech tagging. [sent-430, score-0.796]
</p><p>91 Word lattice reranking for chinese word segmentation and partof-speech tagging. [sent-434, score-0.616]
</p><p>92 Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging a case study. [sent-438, score-1.055]
</p><p>93 The fourth international chinese language processing bakeoff: Chinese word segmentation, named entity recognition and chinese pos tagging. [sent-442, score-0.649]
</p><p>94 An error-driven word-character hybrid model for joint chinese word segmentation and pos tagging. [sent-446, score-1.103]
</p><p>95 Joint models for chinese pos tagging and dependency parsing. [sent-459, score-0.697]
</p><p>96 Joint training and decoding using virtual nodes for cascaded segmentation and tagging tasks. [sent-467, score-0.83]
</p><p>97 A stacked sub-word model for joint chinese word segmentation and part-of-speech tagging. [sent-481, score-0.758]
</p><p>98 Joint word segmentation and POS tagging using a single perceptron. [sent-485, score-0.71]
</p><p>99 A fast decoder for joint word segmentation and POS-tagging using a single discriminative model. [sent-489, score-0.629]
</p><p>100 Unsupervised segmentation helps supervised learning of character tagging forword segmentation and named entity recognition. [sent-503, score-1.268]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('segmentation', 0.441), ('pos', 0.345), ('pipeline', 0.241), ('unary', 0.234), ('tagging', 0.223), ('character', 0.163), ('bottom', 0.154), ('nr', 0.144), ('joint', 0.142), ('parsing', 0.135), ('chinese', 0.129), ('cyk', 0.124), ('scoreseg', 0.124), ('transition', 0.116), ('span', 0.108), ('jiang', 0.106), ('harper', 0.104), ('decoding', 0.098), ('bracket', 0.097), ('tag', 0.096), ('ci', 0.083), ('fbinary', 0.083), ('fbottom', 0.083), ('fcrf', 0.083), ('funary', 0.083), ('qian', 0.083), ('scoretop', 0.083), ('states', 0.082), ('templates', 0.079), ('boundary', 0.069), ('crf', 0.066), ('shanghai', 0.064), ('ftop', 0.062), ('seg', 0.062), ('segmented', 0.058), ('state', 0.056), ('ctb', 0.056), ('zhang', 0.056), ('nn', 0.055), ('crfs', 0.055), ('line', 0.054), ('tags', 0.054), ('bidirectional', 0.053), ('scaled', 0.053), ('kruengkrai', 0.048), ('tagger', 0.048), ('perceptron', 0.046), ('word', 0.046), ('clark', 0.045), ('wenbin', 0.045), ('binarization', 0.045), ('surrounding', 0.044), ('spans', 0.043), ('chain', 0.043), ('office', 0.043), ('chongming', 0.041), ('customs', 0.041), ('fpos', 0.041), ('maxn', 0.041), ('nntp', 0.041), ('scorebottom', 0.041), ('submodels', 0.041), ('systemprf', 0.041), ('tllast', 0.041), ('zhongqiang', 0.041), ('huang', 0.041), ('parse', 0.04), ('wl', 0.04), ('sighan', 0.04), ('top', 0.039), ('che', 0.038), ('cascaded', 0.038), ('identity', 0.036), ('hlt', 0.036), ('qun', 0.036), ('tree', 0.036), ('ath', 0.036), ('bakeoff', 0.036), ('fme', 0.036), ('subword', 0.036), ('boundaries', 0.035), ('cj', 0.034), ('viterbi', 0.033), ('sun', 0.033), ('weights', 0.033), ('right', 0.033), ('rule', 0.032), ('zhenghua', 0.032), ('sx', 0.032), ('xian', 0.032), ('xl', 0.032), ('parseval', 0.032), ('lisa', 0.032), ('yue', 0.032), ('gold', 0.032), ('left', 0.031), ('last', 0.03), ('bi', 0.03), ('semimarkov', 0.03), ('virtual', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="70-tfidf-1" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>Author: Xian Qian ; Yang Liu</p><p>Abstract: In this paper, we propose a novel decoding algorithm for discriminative joint Chinese word segmentation, part-of-speech (POS) tagging, and parsing. Previous work often used a pipeline method Chinese word segmentation followed by POS tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components. In our approach, we train the three individual models separately during training, and incorporate them together in a unified framework during decoding. We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features. As far as we know, this is the first work on joint Chinese word segmentation, POS tagging and parsing. Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system. –</p><p>2 0.36233157 <a title="70-tfidf-2" href="./emnlp-2012-Part-of-Speech_Tagging_for_Chinese-English_Mixed_Texts_with_Dynamic_Features.html">106 emnlp-2012-Part-of-Speech Tagging for Chinese-English Mixed Texts with Dynamic Features</a></p>
<p>Author: Jiayi Zhao ; Xipeng Qiu ; Shu Zhang ; Feng Ji ; Xuanjing Huang</p><p>Abstract: In modern Chinese articles or conversations, it is very popular to involve a few English words, especially in emails and Internet literature. Therefore, it becomes an important and challenging topic to analyze Chinese-English mixed texts. The underlying problem is how to tag part-of-speech (POS) for the English words involved. Due to the lack of specially annotated corpus, most of the English words are tagged as the oversimplified type, “foreign words”. In this paper, we present a method using dynamic features to tag POS of mixed texts. Experiments show that our method achieves higher performance than traditional sequence labeling methods. Meanwhile, our method also boosts the performance of POS tagging for pure Chinese texts.</p><p>3 0.28873643 <a title="70-tfidf-3" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>Author: Zhongguo Li ; Guodong Zhou</p><p>Abstract: Most previous approaches to syntactic parsing of Chinese rely on a preprocessing step of word segmentation, thereby assuming there was a clearly defined boundary between morphology and syntax in Chinese. We show how this assumption can fail badly, leading to many out-of-vocabulary words and incompatible annotations. Hence in practice the strict separation of morphology and syntax in the Chinese language proves to be untenable. We present a unified dependency parsing approach for Chinese which takes unsegmented sentences as input and outputs both morphological and syntactic structures with a single model and algorithm. By removing the intermediate word segmentation, the unified parser no longer needs separate notions for words and phrases. Evaluation proves the effectiveness of the unified model and algorithm in parsing structures of words, phrases and sen- tences simultaneously. 1</p><p>4 0.23929219 <a title="70-tfidf-4" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>Author: Bernd Bohnet ; Joakim Nivre</p><p>Abstract: Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins. We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees. Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages.</p><p>5 0.19557156 <a title="70-tfidf-5" href="./emnlp-2012-Iterative_Annotation_Transformation_with_Predict-Self_Reestimation_for_Chinese_Word_Segmentation.html">68 emnlp-2012-Iterative Annotation Transformation with Predict-Self Reestimation for Chinese Word Segmentation</a></p>
<p>Author: Wenbin Jiang ; Fandong Meng ; Qun Liu ; Yajuan Lu</p><p>Abstract: In this paper we first describe the technology of automatic annotation transformation, which is based on the annotation adaptation algorithm (Jiang et al., 2009). It can automatically transform a human-annotated corpus from one annotation guideline to another. We then propose two optimization strategies, iterative training and predict-selfreestimation, to further improve the accuracy of annotation guideline transformation. Experiments on Chinese word segmentation show that, the iterative training strategy together with predictself reestimation brings significant improvement over the simple annotation transformation baseline, and leads to classifiers with significantly higher accuracy and several times faster processing than annotation adaptation does. On the Penn Chinese Treebank 5.0, , it achieves an F-measure of 98.43%, significantly outperforms previous works although using a single classifier with only local features.</p><p>6 0.1537033 <a title="70-tfidf-6" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<p>7 0.13522071 <a title="70-tfidf-7" href="./emnlp-2012-A_Novel_Discriminative_Framework_for_Sentence-Level_Discourse_Analysis.html">7 emnlp-2012-A Novel Discriminative Framework for Sentence-Level Discourse Analysis</a></p>
<p>8 0.12383451 <a title="70-tfidf-8" href="./emnlp-2012-Exploiting_Chunk-level_Features_to_Improve_Phrase_Chunking.html">45 emnlp-2012-Exploiting Chunk-level Features to Improve Phrase Chunking</a></p>
<p>9 0.11934385 <a title="70-tfidf-9" href="./emnlp-2012-Type-Supervised_Hidden_Markov_Models_for_Part-of-Speech_Tagging_with_Incomplete_Tag_Dictionaries.html">129 emnlp-2012-Type-Supervised Hidden Markov Models for Part-of-Speech Tagging with Incomplete Tag Dictionaries</a></p>
<p>10 0.11738244 <a title="70-tfidf-10" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>11 0.11106859 <a title="70-tfidf-11" href="./emnlp-2012-Assessment_of_ESL_Learners%27_Syntactic_Competence_Based_on_Similarity_Measures.html">21 emnlp-2012-Assessment of ESL Learners' Syntactic Competence Based on Similarity Measures</a></p>
<p>12 0.10967262 <a title="70-tfidf-12" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<p>13 0.1006885 <a title="70-tfidf-13" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>14 0.097186096 <a title="70-tfidf-14" href="./emnlp-2012-Generalized_Higher-Order_Dependency_Parsing_with_Cube_Pruning.html">57 emnlp-2012-Generalized Higher-Order Dependency Parsing with Cube Pruning</a></p>
<p>15 0.087321952 <a title="70-tfidf-15" href="./emnlp-2012-Wiki-ly_Supervised_Part-of-Speech_Tagging.html">138 emnlp-2012-Wiki-ly Supervised Part-of-Speech Tagging</a></p>
<p>16 0.082941413 <a title="70-tfidf-16" href="./emnlp-2012-Extracting_Opinion_Expressions_with_semi-Markov_Conditional_Random_Fields.html">51 emnlp-2012-Extracting Opinion Expressions with semi-Markov Conditional Random Fields</a></p>
<p>17 0.082214549 <a title="70-tfidf-17" href="./emnlp-2012-Transforming_Trees_to_Improve_Syntactic_Convergence.html">127 emnlp-2012-Transforming Trees to Improve Syntactic Convergence</a></p>
<p>18 0.069141395 <a title="70-tfidf-18" href="./emnlp-2012-Exploring_Adaptor_Grammars_for_Native_Language_Identification.html">48 emnlp-2012-Exploring Adaptor Grammars for Native Language Identification</a></p>
<p>19 0.068788446 <a title="70-tfidf-19" href="./emnlp-2012-Document-Wide_Decoding_for_Phrase-Based_Statistical_Machine_Translation.html">35 emnlp-2012-Document-Wide Decoding for Phrase-Based Statistical Machine Translation</a></p>
<p>20 0.064284407 <a title="70-tfidf-20" href="./emnlp-2012-Exploiting_Reducibility_in_Unsupervised_Dependency_Parsing.html">46 emnlp-2012-Exploiting Reducibility in Unsupervised Dependency Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.293), (1, -0.253), (2, 0.292), (3, -0.105), (4, 0.151), (5, 0.028), (6, 0.001), (7, -0.175), (8, -0.167), (9, -0.325), (10, -0.13), (11, -0.142), (12, -0.033), (13, -0.145), (14, 0.09), (15, 0.085), (16, 0.005), (17, 0.071), (18, 0.01), (19, -0.018), (20, -0.039), (21, 0.141), (22, -0.042), (23, -0.031), (24, -0.022), (25, -0.01), (26, 0.016), (27, 0.086), (28, -0.028), (29, 0.005), (30, -0.02), (31, 0.013), (32, 0.031), (33, -0.024), (34, -0.003), (35, -0.012), (36, 0.005), (37, 0.032), (38, -0.019), (39, 0.008), (40, -0.036), (41, 0.057), (42, -0.004), (43, -0.005), (44, -0.038), (45, -0.034), (46, 0.027), (47, 0.064), (48, 0.021), (49, 0.109)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98289764 <a title="70-lsi-1" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>Author: Xian Qian ; Yang Liu</p><p>Abstract: In this paper, we propose a novel decoding algorithm for discriminative joint Chinese word segmentation, part-of-speech (POS) tagging, and parsing. Previous work often used a pipeline method Chinese word segmentation followed by POS tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components. In our approach, we train the three individual models separately during training, and incorporate them together in a unified framework during decoding. We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features. As far as we know, this is the first work on joint Chinese word segmentation, POS tagging and parsing. Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system. –</p><p>2 0.92507315 <a title="70-lsi-2" href="./emnlp-2012-Part-of-Speech_Tagging_for_Chinese-English_Mixed_Texts_with_Dynamic_Features.html">106 emnlp-2012-Part-of-Speech Tagging for Chinese-English Mixed Texts with Dynamic Features</a></p>
<p>Author: Jiayi Zhao ; Xipeng Qiu ; Shu Zhang ; Feng Ji ; Xuanjing Huang</p><p>Abstract: In modern Chinese articles or conversations, it is very popular to involve a few English words, especially in emails and Internet literature. Therefore, it becomes an important and challenging topic to analyze Chinese-English mixed texts. The underlying problem is how to tag part-of-speech (POS) for the English words involved. Due to the lack of specially annotated corpus, most of the English words are tagged as the oversimplified type, “foreign words”. In this paper, we present a method using dynamic features to tag POS of mixed texts. Experiments show that our method achieves higher performance than traditional sequence labeling methods. Meanwhile, our method also boosts the performance of POS tagging for pure Chinese texts.</p><p>3 0.80521327 <a title="70-lsi-3" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>Author: Zhongguo Li ; Guodong Zhou</p><p>Abstract: Most previous approaches to syntactic parsing of Chinese rely on a preprocessing step of word segmentation, thereby assuming there was a clearly defined boundary between morphology and syntax in Chinese. We show how this assumption can fail badly, leading to many out-of-vocabulary words and incompatible annotations. Hence in practice the strict separation of morphology and syntax in the Chinese language proves to be untenable. We present a unified dependency parsing approach for Chinese which takes unsegmented sentences as input and outputs both morphological and syntactic structures with a single model and algorithm. By removing the intermediate word segmentation, the unified parser no longer needs separate notions for words and phrases. Evaluation proves the effectiveness of the unified model and algorithm in parsing structures of words, phrases and sen- tences simultaneously. 1</p><p>4 0.72929764 <a title="70-lsi-4" href="./emnlp-2012-Exploiting_Chunk-level_Features_to_Improve_Phrase_Chunking.html">45 emnlp-2012-Exploiting Chunk-level Features to Improve Phrase Chunking</a></p>
<p>Author: Junsheng Zhou ; Weiguang Qu ; Fen Zhang</p><p>Abstract: Most existing systems solved the phrase chunking task with the sequence labeling approaches, in which the chunk candidates cannot be treated as a whole during parsing process so that the chunk-level features cannot be exploited in a natural way. In this paper, we formulate phrase chunking as a joint segmentation and labeling task. We propose an efficient dynamic programming algorithm with pruning for decoding, which allows the direct use of the features describing the internal characteristics of chunk and the features capturing the correlations between adjacent chunks. A relaxed, online maximum margin training algorithm is used for learning. Within this framework, we explored a variety of effective feature representations for Chinese phrase chunking. The experimental results show that the use of chunk-level features can lead to significant performance improvement, and that our approach achieves state-of-the-art performance. In particular, our approach is much better at recognizing long and complicated phrases. 1</p><p>5 0.58746344 <a title="70-lsi-5" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>Author: Bernd Bohnet ; Joakim Nivre</p><p>Abstract: Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins. We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees. Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages.</p><p>6 0.5850023 <a title="70-lsi-6" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<p>7 0.52747494 <a title="70-lsi-7" href="./emnlp-2012-Iterative_Annotation_Transformation_with_Predict-Self_Reestimation_for_Chinese_Word_Segmentation.html">68 emnlp-2012-Iterative Annotation Transformation with Predict-Self Reestimation for Chinese Word Segmentation</a></p>
<p>8 0.45593116 <a title="70-lsi-8" href="./emnlp-2012-A_Novel_Discriminative_Framework_for_Sentence-Level_Discourse_Analysis.html">7 emnlp-2012-A Novel Discriminative Framework for Sentence-Level Discourse Analysis</a></p>
<p>9 0.44048253 <a title="70-lsi-9" href="./emnlp-2012-Assessment_of_ESL_Learners%27_Syntactic_Competence_Based_on_Similarity_Measures.html">21 emnlp-2012-Assessment of ESL Learners' Syntactic Competence Based on Similarity Measures</a></p>
<p>10 0.43616146 <a title="70-lsi-10" href="./emnlp-2012-Employing_Compositional_Semantics_and_Discourse_Consistency_in_Chinese_Event_Extraction.html">38 emnlp-2012-Employing Compositional Semantics and Discourse Consistency in Chinese Event Extraction</a></p>
<p>11 0.39937451 <a title="70-lsi-11" href="./emnlp-2012-Exploring_Adaptor_Grammars_for_Native_Language_Identification.html">48 emnlp-2012-Exploring Adaptor Grammars for Native Language Identification</a></p>
<p>12 0.37580389 <a title="70-lsi-12" href="./emnlp-2012-Type-Supervised_Hidden_Markov_Models_for_Part-of-Speech_Tagging_with_Incomplete_Tag_Dictionaries.html">129 emnlp-2012-Type-Supervised Hidden Markov Models for Part-of-Speech Tagging with Incomplete Tag Dictionaries</a></p>
<p>13 0.36716294 <a title="70-lsi-13" href="./emnlp-2012-Generalized_Higher-Order_Dependency_Parsing_with_Cube_Pruning.html">57 emnlp-2012-Generalized Higher-Order Dependency Parsing with Cube Pruning</a></p>
<p>14 0.35412279 <a title="70-lsi-14" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<p>15 0.35232791 <a title="70-lsi-15" href="./emnlp-2012-Exploiting_Reducibility_in_Unsupervised_Dependency_Parsing.html">46 emnlp-2012-Exploiting Reducibility in Unsupervised Dependency Parsing</a></p>
<p>16 0.34946045 <a title="70-lsi-16" href="./emnlp-2012-Wiki-ly_Supervised_Part-of-Speech_Tagging.html">138 emnlp-2012-Wiki-ly Supervised Part-of-Speech Tagging</a></p>
<p>17 0.30663544 <a title="70-lsi-17" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>18 0.29429865 <a title="70-lsi-18" href="./emnlp-2012-Transforming_Trees_to_Improve_Syntactic_Convergence.html">127 emnlp-2012-Transforming Trees to Improve Syntactic Convergence</a></p>
<p>19 0.27686033 <a title="70-lsi-19" href="./emnlp-2012-Learning_Syntactic_Categories_Using_Paradigmatic_Representations_of_Word_Context.html">79 emnlp-2012-Learning Syntactic Categories Using Paradigmatic Representations of Word Context</a></p>
<p>20 0.27240795 <a title="70-lsi-20" href="./emnlp-2012-Spectral_Dependency_Parsing_with_Latent_Variables.html">119 emnlp-2012-Spectral Dependency Parsing with Latent Variables</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.013), (14, 0.011), (16, 0.053), (25, 0.017), (29, 0.018), (34, 0.103), (41, 0.016), (53, 0.171), (60, 0.158), (63, 0.034), (64, 0.02), (65, 0.016), (70, 0.071), (73, 0.01), (74, 0.098), (76, 0.05), (80, 0.018), (86, 0.03), (95, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85548741 <a title="70-lda-1" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>Author: Xian Qian ; Yang Liu</p><p>Abstract: In this paper, we propose a novel decoding algorithm for discriminative joint Chinese word segmentation, part-of-speech (POS) tagging, and parsing. Previous work often used a pipeline method Chinese word segmentation followed by POS tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components. In our approach, we train the three individual models separately during training, and incorporate them together in a unified framework during decoding. We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features. As far as we know, this is the first work on joint Chinese word segmentation, POS tagging and parsing. Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system. –</p><p>2 0.76847285 <a title="70-lda-2" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>Author: Greg Durrett ; Adam Pauls ; Dan Klein</p><p>Abstract: We consider the problem of using a bilingual dictionary to transfer lexico-syntactic information from a resource-rich source language to a resource-poor target language. In contrast to past work that used bitexts to transfer analyses of specific sentences at the token level, we instead use features to transfer the behavior of words at a type level. In a discriminative dependency parsing framework, our approach produces gains across a range of target languages, using two different lowresource training methodologies (one weakly supervised and one indirectly supervised) and two different dictionary sources (one manually constructed and one automatically constructed).</p><p>3 0.7535904 <a title="70-lda-3" href="./emnlp-2012-Multi-Domain_Learning%3A_When_Do_Domains_Matter%3F.html">92 emnlp-2012-Multi-Domain Learning: When Do Domains Matter?</a></p>
<p>Author: Mahesh Joshi ; Mark Dredze ; William W. Cohen ; Carolyn Rose</p><p>Abstract: We present a systematic analysis of existing multi-domain learning approaches with respect to two questions. First, many multidomain learning algorithms resemble ensemble learning algorithms. (1) Are multi-domain learning improvements the result of ensemble learning effects? Second, these algorithms are traditionally evaluated in a balanced class label setting, although in practice many multidomain settings have domain-specific class label biases. When multi-domain learning is applied to these settings, (2) are multidomain methods improving because they capture domain-specific class biases? An understanding of these two issues presents a clearer idea about where the field has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art.</p><p>4 0.75355041 <a title="70-lda-4" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>Author: Fei Huang ; Alexander Yates</p><p>Abstract: Representation learning is a promising technique for discovering features that allow supervised classifiers to generalize from a source domain dataset to arbitrary new domains. We present a novel, formal statement of the representation learning task. We argue that because the task is computationally intractable in general, it is important for a representation learner to be able to incorporate expert knowledge during its search for helpful features. Leveraging the Posterior Regularization framework, we develop an architecture for incorporating biases into representation learning. We investigate three types of biases, and experiments on two domain adaptation tasks show that our biased learners identify significantly better sets of features than unbiased learners, resulting in a relative reduction in error of more than 16% for both tasks, with respect to existing state-of-the-art representation learning techniques.</p><p>5 0.75339299 <a title="70-lda-5" href="./emnlp-2012-Exploiting_Chunk-level_Features_to_Improve_Phrase_Chunking.html">45 emnlp-2012-Exploiting Chunk-level Features to Improve Phrase Chunking</a></p>
<p>Author: Junsheng Zhou ; Weiguang Qu ; Fen Zhang</p><p>Abstract: Most existing systems solved the phrase chunking task with the sequence labeling approaches, in which the chunk candidates cannot be treated as a whole during parsing process so that the chunk-level features cannot be exploited in a natural way. In this paper, we formulate phrase chunking as a joint segmentation and labeling task. We propose an efficient dynamic programming algorithm with pruning for decoding, which allows the direct use of the features describing the internal characteristics of chunk and the features capturing the correlations between adjacent chunks. A relaxed, online maximum margin training algorithm is used for learning. Within this framework, we explored a variety of effective feature representations for Chinese phrase chunking. The experimental results show that the use of chunk-level features can lead to significant performance improvement, and that our approach achieves state-of-the-art performance. In particular, our approach is much better at recognizing long and complicated phrases. 1</p><p>6 0.74939597 <a title="70-lda-6" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>7 0.74839377 <a title="70-lda-7" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>8 0.74716365 <a title="70-lda-8" href="./emnlp-2012-Concurrent_Acquisition_of_Word_Meaning_and_Lexical_Categories.html">29 emnlp-2012-Concurrent Acquisition of Word Meaning and Lexical Categories</a></p>
<p>9 0.74460554 <a title="70-lda-9" href="./emnlp-2012-Exploring_Adaptor_Grammars_for_Native_Language_Identification.html">48 emnlp-2012-Exploring Adaptor Grammars for Native Language Identification</a></p>
<p>10 0.7441361 <a title="70-lda-10" href="./emnlp-2012-Enlarging_Paraphrase_Collections_through_Generalization_and_Instantiation.html">39 emnlp-2012-Enlarging Paraphrase Collections through Generalization and Instantiation</a></p>
<p>11 0.74371952 <a title="70-lda-11" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<p>12 0.74266475 <a title="70-lda-12" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>13 0.73879862 <a title="70-lda-13" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>14 0.73377061 <a title="70-lda-14" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>15 0.73274136 <a title="70-lda-15" href="./emnlp-2012-Forced_Derivation_Tree_based_Model_Training_to_Statistical_Machine_Translation.html">54 emnlp-2012-Forced Derivation Tree based Model Training to Statistical Machine Translation</a></p>
<p>16 0.73213589 <a title="70-lda-16" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>17 0.73000324 <a title="70-lda-17" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>18 0.72963023 <a title="70-lda-18" href="./emnlp-2012-Regularized_Interlingual_Projections%3A_Evaluation_on_Multilingual_Transliteration.html">111 emnlp-2012-Regularized Interlingual Projections: Evaluation on Multilingual Transliteration</a></p>
<p>19 0.72889316 <a title="70-lda-19" href="./emnlp-2012-Syntactic_Surprisal_Affects_Spoken_Word_Duration_in_Conversational_Contexts.html">122 emnlp-2012-Syntactic Surprisal Affects Spoken Word Duration in Conversational Contexts</a></p>
<p>20 0.72732192 <a title="70-lda-20" href="./emnlp-2012-Probabilistic_Finite_State_Machines_for_Regression-based_MT_Evaluation.html">108 emnlp-2012-Probabilistic Finite State Machines for Regression-based MT Evaluation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
