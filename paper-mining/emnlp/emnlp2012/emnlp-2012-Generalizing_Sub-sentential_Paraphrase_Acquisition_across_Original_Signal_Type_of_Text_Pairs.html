<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>58 emnlp-2012-Generalizing Sub-sentential Paraphrase Acquisition across Original Signal Type of Text Pairs</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-58" href="#">emnlp2012-58</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>58 emnlp-2012-Generalizing Sub-sentential Paraphrase Acquisition across Original Signal Type of Text Pairs</h1>
<br/><p>Source: <a title="emnlp-2012-58-pdf" href="http://aclweb.org/anthology//D/D12/D12-1066.pdf">pdf</a></p><p>Author: Aurelien Max ; Houda Bouamor ; Anne Vilnat</p><p>Abstract: This paper describes a study on the impact of the original signal (text, speech, visual scene, event) of a text pair on the task of both manual and automatic sub-sentential paraphrase acquisition. A corpus of 2,500 annotated sentences in English and French is described, and performance on this corpus is reported for an efficient system combination exploiting a large set of features for paraphrase recognition. A detailed quantified typology of subsentential paraphrases found in our corpus types is given.</p><p>Reference: <a title="emnlp-2012-58-reference" href="../emnlp2012_reference/emnlp-2012-Generalizing_Sub-sentential_Paraphrase_Acquisition_across_Original_Signal_Type_of_Text_Pairs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 fr ims  Abstract This paper describes a study on the impact of the original signal (text, speech, visual scene, event) of a text pair on the task of both manual and automatic sub-sentential paraphrase acquisition. [sent-4, score-0.694]
</p><p>2 A corpus of 2,500 annotated sentences in English and French is described, and performance on this corpus is reported for an efficient system combination exploiting a large set of features for paraphrase recognition. [sent-5, score-0.636]
</p><p>3 A detailed quantified typology of subsentential paraphrases found in our corpus types is given. [sent-6, score-0.607]
</p><p>4 1 Introduction Sub-sentential paraphrases can be acquired from text pairs expressing the same meaning (Madnani and Dorr, 2010). [sent-7, score-0.54]
</p><p>5 If the semantic similarity of a text pair has a direct impact on the quality of the ac-  quired paraphrases, it has, to our knowledge, never been shown what impact the type of original signal has on paraphrase acquisition. [sent-8, score-0.702]
</p><p>6 How well can representative paraphrase acquisition systems perform on each corpus type, and how performance can be improved through combination? [sent-11, score-0.678]
</p><p>7 On what corpus types can performance be improved by using training material from other corpus types? [sent-12, score-0.125]
</p><p>8 Our ex-  perimental results will provide several indications of the differences and complementarities of the corpus types under study, and will notably show that performance on the most readily available corpus type can be improved by using training data from the set of all other corpus types. [sent-13, score-0.21]
</p><p>9 We will first describe the building procedures and characteristics of our corpora (section 2), and then describe our experimental settings for evaluating paraphrase acquisition (section 3. [sent-14, score-0.715]
</p><p>10 3) of a system combination on each corpus type and then of our system provided with additional training data from the other corpus types (section 3. [sent-18, score-0.205]
</p><p>11 2  Collection of sentence pair corpora  In this study, we will focus on paraphrase acquisition from related sentence pairs characteristic of 4 corpus types, which correspond to different original signal types of text pairs illustrated by the word alignment matrices on Figure 1. [sent-21, score-1.143]
</p><p>12 A corpus for each type has  been collected for 2 languages, English and French, and comprises 625 sentence pairs per language. [sent-22, score-0.167]
</p><p>13 , 2008)) consisting of sets of news article translations from Chinese, and for French the CESTA corpus2 consisting of sets of news article translations from English. [sent-32, score-0.168]
</p><p>14 For each sentence cluster, we selected sentence pairs with minimal edit distance above an empirically-selected threshold, covering all clusters first and then selecting from already used clusters to reach the target number of sentence pairs. [sent-33, score-0.236]
</p><p>15 SPEECH For English, we used two freely available subtitle files3 of the French movies Le Fabuleux Destin d’Am e´lie Poulain and Les Choristes, and for French we used two subtitle files from the Desperate Housewives TV series. [sent-38, score-0.118]
</p><p>16 We first aligned each parallel corpus using the algorithm described in (Tiedemann, 2007), based on time frames and developed for bilingual subtitles, we then filtered out sentence pairs below a minimal edit distance threshold, and manually removed obvious errors made by the algorithm. [sent-39, score-0.327]
</p><p>17 Similarly to what we did for TEXT, we selected sentence pairs from clusters by minimal edit distance above a threshold. [sent-45, score-0.158]
</p><p>18 We decided to use this corpus nonetheless, but with the knowledge that this source for French is of a substantially lower quality (this corpus type will therefore appear as “(SCENE)” in all tables to reflect this). [sent-48, score-0.157]
</p><p>19 46271 689567824617 ENGLISH  FRENCH  Table  1:  Description  of all corpora and paraphrase reference sets for English (top) and French (bottom). [sent-78, score-0.656]
</p><p>20 The main guidelines that they had to follow were that sure and possible paraphrases must be distinguished, smaller alignments were to be prefered but any-to-any alignments may be used, and sentences should be aligned as much as possible. [sent-89, score-0.659]
</p><p>21 Henceforth, we will only consider for all reported statistics and experiments those paraphrases that are not identity pairs (e. [sent-90, score-0.499]
</p><p>22 pdf 723 considered trivial as far as acquisition is concerned. [sent-100, score-0.115]
</p><p>23 We find that acceptable values are obtained for sure paraphrases, but that low values are obtained for possible paraphrases. [sent-102, score-0.157]
</p><p>24 Table 1 finally shows proportions and absolute numbers of paraphrases of each type for all corpora. [sent-105, score-0.502]
</p><p>25 We find that there are approximately the same total number of paraphrases for English (16,799) and French (17,001), but that English corpora collectively have an equivalent number of sure and possible paraphrases (8,303 vs. [sent-106, score-1.083]
</p><p>26 8,496) and French have more sure paraphrases (11,953 vs. [sent-107, score-0.551]
</p><p>27 Other salient results include the fact that TEXT contains more sure paraphrases in number than the other corpora, that SPEECH contains relatively more possible paraphrases than the other corpora, and that SCENE has significantly fewer paraphrases, both in proportion and number. [sent-110, score-1.007]
</p><p>28 In Figure 2 various mea6For each paraphrase type, we used the average of recall values obtained for each annotator set as the reference . [sent-111, score-0.611]
</p><p>29 (synonymy),  (California  50 randomly  selected sentence pairs for reference paraphrases for English  Classes are illustrated by the following examples:  ↔. [sent-137, score-0.594]
</p><p>30 While the metrics used can only provide a crude account of semantic equivalence at the sentence level, these results clearly indicate that translating from text yields more similar sentences than translating from speech. [sent-142, score-0.185]
</p><p>31 Table 2 provides a typology of paraphrases found in all our corpora and two languages, where each class has been quantified with respect to the reference alignments. [sent-143, score-0.653]
</p><p>32 gIt ↔is also interesting to note that the EVENT corpus type, which is easy to collect on a daily basis, contains reference paraphrases spread over all classes. [sent-148, score-0.551]
</p><p>33 Lastly, it is expected that paraphrases in the pragmatics class (e. [sent-149, score-0.524]
</p><p>34 8 7Note that typologies of paraphrases have already been proposed in the literature (e. [sent-154, score-0.456]
</p><p>35 , 2011)), but that the choice of our classes has been primarily motivated by potential subsequent uses of the acquired paraphrases (paraphrases could be annotated as belonging to more than one class). [sent-157, score-0.456]
</p><p>36 8Reusing such types of paraphrases into applications would however often be too strongly context-dependent. [sent-160, score-0.503]
</p><p>37 724  1463520 0 CTOESIXTNE*1S0PECHBSLCEUNE VNT1-ERMTEOR76532140 0 COSNIE*10BLEU1-TERMTEOR Figure 2: Sentence pair average similarities for all corpora for English (left) and French (right) using the cosine of token vectors, BLEU (Papineni et al. [sent-161, score-0.119]
</p><p>38 1 Evaluation of paraphrase acquisition We followed the PARAMETRIC methodology described in (Callison-Burch et al. [sent-165, score-0.639]
</p><p>39 , 2008) for assessing the performance of systems on the task of sub-  sentential paraphrase acquisition. [sent-166, score-0.557]
</p><p>40 In this methodology, a set of paraphrase candidates extracted from a sentence pair is compared with a set of reference paraphrases, obtained through human annotation, by computing usual measures of precision (P) and recall (R). [sent-167, score-0.662]
</p><p>41 The first value corresponds to the proportion of paraphrase candidates, denoted H, produced by a system ahnrads eth caatn are actoersr,e dcte nroetlaetdiv He ,t op tohdeu rceefderence set containing sure and possible paraphrases, denoted Rall. [sent-168, score-0.619]
</p><p>42 Recall is obtained by measuring the proportion of the reference set of sure paraphrases,  Figure 3: Architecture of our combination system for paraphrase identification. [sent-169, score-0.709]
</p><p>43 All data sets of cross-validation contain 500 sentence pairs per corpus type, and 125 pairs are kept for development. [sent-174, score-0.164]
</p><p>44 2  A framework for sub-sentential paraphrase identification We now describe the systems that will be tested on the various corpora described in section 2 using the methodology described in section 3. [sent-176, score-0.6]
</p><p>45 , 2012), a combination system is used to automatically weight paraphrase pair candidates produced by individual systems using a set of features aiming at recognizing paraphrases, as illustrated on Figure 3. [sent-179, score-0.64]
</p><p>46 Statistical learning of word alignments (GIZA) The GIZA++ tool (Och and Ney, 2004) computes statistical word alignment models of increasing complexity from parallel corpora. [sent-181, score-0.224]
</p><p>47 It was run on each monolingual corpus of sentence pairs in both directions, symmetrized alignments were kept and classical phrase extraction heuristics were applied (Koehn et al. [sent-182, score-0.211]
</p><p>48 9 Edit types are parameterized by one or more weights which were optimized towards F-measure by hill climbing with 100 random restarts using the held-out data set consisting of 125 sentence pairs for each corpus type. [sent-189, score-0.168]
</p><p>49 Translational equivalence (PIVOT) We exploited the paraphrase probability defined by Bannard and Callison-Burch (2005) on bilingual parallel corpora. [sent-190, score-0.703]
</p><p>50 7 million parallel sentences, using each language as source and pivot in turn. [sent-192, score-0.217]
</p><p>51 30 million parallel sentences) at System features combination of the individual systems that proposed the paraphrase pair –  –  –  Table 3: Features used by our classifiers. [sent-200, score-0.731]
</p><p>52 For each  phrase of a sentence pair, we built its set of paraphrases, and extracted its paraphrase from the other sentence with highest probability. [sent-204, score-0.602]
</p><p>53 We repeated this  process in both directions, and finally kept for each phrase its paraphrase  pair from any direction with  highest probability. [sent-205, score-0.567]
</p><p>54 3 Experimental results Results for individual systems, their union and our validation system trained on each corpus type are given on Table 4. [sent-211, score-0.189]
</p><p>55 12 In all cases, our combination system manages to increase F-measure substantially over the best individual system for a corpus type and the simple union. [sent-224, score-0.191]
</p><p>56 4  12Note that the fact that English and French were used as the pivot for one another may have had some positive effect here, but, incidentally, the two corpora obtained by translating from the other language (TEXT and SPEECH) are not those where PIVOT fares better. [sent-233, score-0.23]
</p><p>57 801 ENGLISH  Table 4: Evaluation results for individual systems (left) and combination systems (right) on all corpus types for English (top) and French (bottom). [sent-272, score-0.159]
</p><p>58 Values in bold are for highest values for a given metric for each corpus type and language. [sent-273, score-0.116]
</p><p>59 Recall from Table 1 that TEXT and  SPEECH were the two corpus types with the highest number of sure paraphrase examples for both languages: results show that our classifier was able to efficiently use them. [sent-276, score-0.705]
</p><p>60 Further study of false negatives should help with engineering new features to improve paraphrase recognition. [sent-281, score-0.524]
</p><p>61 As seen in Table 2, synonymy is the most present phenomenon in all our corpora; it is also probably one of the most useful type of knowledge for many applications. [sent-286, score-0.126]
</p><p>62 We now therefore focus on this class, for which all the sure paraphrases in our corpora falling in this class have been annotated. [sent-287, score-0.627]
</p><p>63 Table 5 shows F-measure values for the individual techniques and our combination systems on all corpus types. [sent-288, score-0.143]
</p><p>64 9237 FRENCH  Table 5: F-measure values for test instances in the synonymy class (see Table 2) for all individual systems and our validation system for English (top) and French (bottom). [sent-301, score-0.184]
</p><p>65 4 Experiments across corpus types To test how different the corpora under study are as  regards paraphrase identification, we now consider using as additional training data for our classifiers corpora of the other types, both individually and collectively. [sent-304, score-0.762]
</p><p>66 261937 Table 6: Evaluation results (F1 scores) for all corpus types for English (top) and French (bottom) when adding training material from other corpus types (values with  gray background on the diagonal are when no additional training data are used). [sent-317, score-0.172]
</p><p>67 “#ex+” rows indicate numbers of positive paraphrase examples for each additional corpus type. [sent-318, score-0.563]
</p><p>68 It should be noted that no individual corpus type, save TEXT, individually improves results on EVENT, and that results are yet substantially improved over the use of training data from TEXT when using all available data, revealing a collective contribution of all corpus types. [sent-322, score-0.15]
</p><p>69 The second major observation is that all other corpus types seem to be quite specific in nature, as no addition of training data from other types yields any improvement (with the exception of SPEECH on English), but they often in fact decrease performance. [sent-323, score-0.133]
</p><p>70 This underlines the specific nature of this corpus type: independent descriptions of the same scene in a video may be  worded with much variation that mostly differ from that present in other corpus types. [sent-325, score-0.408]
</p><p>71 Our main conclusion here is therefore that all our corpora under study are quite specific in nature, but that EVENT can benefit from all training data from the other corpus types. [sent-326, score-0.115]
</p><p>72 We can further note that the 728 fact that TEXT is almost not impacted by additional data may also be explained by the fact that this corpus type contains more than half of the total number of examples for the two languages. [sent-327, score-0.116]
</p><p>73 Finally, there are substantially more positive paraphrase examples for French (19,427) than for English (12,593). [sent-328, score-0.557]
</p><p>74 4  Related work  Over the years, paraphrase acquisition and generation have attracted a wealth of research works that are too many to adequatly summarize here: (Madnani and Dorr, 2010) presents a complete and upto-date review of the main approaches. [sent-329, score-0.639]
</p><p>75 Sentential paraphrase collection has been tackled from specific resources increasing the probability of sentences be-  ing paraphrases (Dolan et al. [sent-330, score-0.98]
</p><p>76 , 2009), from comparable monolingual corpora (Barzilay and Elhadad, 2003; Fung and Cheung, 2004; Nelken and Shieber, 2006), and even at web scale (Pas ¸ca and Dienes, 2005; Bhagat and Ravichandran, 2008). [sent-332, score-0.112]
</p><p>77 Various techniques have been proposed for paraphrase acquisition from related sentence pairs (Barzilay and McKeown, 2001 ; Pang et al. [sent-333, score-0.721]
</p><p>78 , 2003) and from bilingual parallel corpora (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010). [sent-334, score-0.206]
</p><p>79 The issue of corpus construction for developing and evaluating paraphrase acquisition techniques are addressed in (Cohn et al. [sent-335, score-0.678]
</p><p>80 To the best of our knowledge, this is the first time that a study in paraphrase acquisition is conducted on several corpus types and for 2 languages. [sent-338, score-0.725]
</p><p>81 Faruqui and Pad o´ (201 1) study the acquisition of entailment pairs (premise and hypothesis), with experiments in 3 languages and various domains of newspaper corpora for one language. [sent-339, score-0.234]
</p><p>82 Laslty, the evaluation of automatically generated  paraphrases has recently received some attention (Liu et al. [sent-341, score-0.456]
</p><p>83 Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al. [sent-344, score-1.048]
</p><p>84 5  Discussion and future work  This work has addressed the issue of sub-sentential paraphrase acquisition from text pairs. [sent-352, score-0.68]
</p><p>85 Analoguously to bilingual parallel corpora, which are still to date the most reliable resources for automatic acquisition of sub-sentential translations, monolingual parallel corpora are generally regarded as very appropriate for paraphrase acquisition. [sent-353, score-0.972]
</p><p>86 However, their low availability makes searching for less parallel corpora a necessity. [sent-354, score-0.167]
</p><p>87 In this study, we have attempted to identify corpora of various degrees of semantic  textual similarity by considering text pairs originating from various signal types. [sent-355, score-0.239]
</p><p>88 The results presented in this paper have shown how these corpora differed in various aspects. [sent-357, score-0.11]
</p><p>89 First, they contain varying quantities of paraphrases that are differently distributed into paraphrase classes. [sent-358, score-0.98]
</p><p>90 729 tors aside, emphasizes the fact that the correct idenfication of paraphrases is facilitated when equivalence of semantic content is more probable. [sent-366, score-0.505]
</p><p>91 Many works have accordingly attempted to identify text units that are as parallel as possible from large corpora, and the task of measuring semantic textual similarity, which can find many uses, has received some attention lately (Agirre et al. [sent-367, score-0.132]
</p><p>92 The first one is to continue our current line of work and study the impact of additional individual acquisition techniques and better characterizations of paraphrases in context, in tandem with working  on identifying parallel text pairs in large corpora. [sent-371, score-0.785]
</p><p>93 Another avenue is to start from the output of high recall techniques and to attempt to characterize the contexts of possible substitution for candidate paraphrases from large corpora as a means to acquire precise paraphrases. [sent-372, score-0.532]
</p><p>94 As the examples from Table 7 show, some classes of paraphrases, and in particular in the continuum from our synonymy to pragmatics classes, require thejoint acquisition ofcontextual information that license substitution. [sent-373, score-0.263]
</p><p>95 Table 7: Examples in English for the synonymy and pragmatics classes. [sent-375, score-0.148]
</p><p>96 Large scale acquisition of paraphrases for learning surface patterns. [sent-401, score-0.571]
</p><p>97 Validation of sub-sentential paraphrases acquired from  parallel monolingual corpora. [sent-405, score-0.583]
</p><p>98 Constructing corpora for the development and evaluation of paraphrase systems. [sent-417, score-0.6]
</p><p>99 Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. [sent-427, score-0.615]
</p><p>100 Shiqi  tracting paraphrases and generating new sentences. [sent-507, score-0.456]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('paraphrase', 0.524), ('paraphrases', 0.456), ('scene', 0.257), ('french', 0.25), ('pivot', 0.126), ('terp', 0.119), ('fastr', 0.119), ('acquisition', 0.115), ('sure', 0.095), ('event', 0.092), ('parallel', 0.091), ('madnani', 0.086), ('synonymy', 0.08), ('xen', 0.079), ('corpora', 0.076), ('edit', 0.076), ('giza', 0.074), ('cohn', 0.069), ('pragmatics', 0.068), ('amplitude', 0.059), ('bouamor', 0.059), ('rsure', 0.059), ('subtitle', 0.059), ('speech', 0.057), ('reference', 0.056), ('chris', 0.055), ('alignments', 0.054), ('translations', 0.054), ('dorr', 0.053), ('bottom', 0.052), ('aur', 0.051), ('lien', 0.051), ('dolan', 0.05), ('snover', 0.05), ('equivalence', 0.049), ('english', 0.049), ('signal', 0.048), ('types', 0.047), ('bannard', 0.046), ('alignment', 0.046), ('type', 0.046), ('descriptions', 0.045), ('pairs', 0.043), ('nitin', 0.043), ('pair', 0.043), ('bonnie', 0.042), ('text', 0.041), ('translational', 0.04), ('atmt', 0.04), ('exn', 0.04), ('faruqui', 0.04), ('houda', 0.04), ('pigeons', 0.04), ('primates', 0.04), ('remind', 0.04), ('schroeder', 0.04), ('vila', 0.04), ('yawat', 0.04), ('corpus', 0.039), ('individual', 0.039), ('bilingual', 0.039), ('sentence', 0.039), ('visual', 0.038), ('meteor', 0.038), ('monolingual', 0.036), ('barzilay', 0.035), ('combination', 0.034), ('validation', 0.034), ('bernhard', 0.034), ('eh', 0.034), ('morphosyntactic', 0.034), ('boy', 0.034), ('differed', 0.034), ('nelken', 0.034), ('typology', 0.034), ('substantially', 0.033), ('sentential', 0.033), ('cat', 0.033), ('ct', 0.033), ('tool', 0.033), ('lie', 0.032), ('lastly', 0.032), ('resnik', 0.032), ('union', 0.031), ('wubben', 0.031), ('impacted', 0.031), ('originating', 0.031), ('metzler', 0.031), ('shiqi', 0.031), ('quantified', 0.031), ('values', 0.031), ('translation', 0.03), ('article', 0.03), ('columbus', 0.029), ('bhagat', 0.029), ('paradigmatic', 0.029), ('translating', 0.028), ('trevor', 0.028), ('variation', 0.028), ('proceedings', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999851 <a title="58-tfidf-1" href="./emnlp-2012-Generalizing_Sub-sentential_Paraphrase_Acquisition_across_Original_Signal_Type_of_Text_Pairs.html">58 emnlp-2012-Generalizing Sub-sentential Paraphrase Acquisition across Original Signal Type of Text Pairs</a></p>
<p>Author: Aurelien Max ; Houda Bouamor ; Anne Vilnat</p><p>Abstract: This paper describes a study on the impact of the original signal (text, speech, visual scene, event) of a text pair on the task of both manual and automatic sub-sentential paraphrase acquisition. A corpus of 2,500 annotated sentences in English and French is described, and performance on this corpus is reported for an efficient system combination exploiting a large set of features for paraphrase recognition. A detailed quantified typology of subsentential paraphrases found in our corpus types is given.</p><p>2 0.53034103 <a title="58-tfidf-2" href="./emnlp-2012-Enlarging_Paraphrase_Collections_through_Generalization_and_Instantiation.html">39 emnlp-2012-Enlarging Paraphrase Collections through Generalization and Instantiation</a></p>
<p>Author: Atsushi Fujita ; Pierre Isabelle ; Roland Kuhn</p><p>Abstract: This paper presents a paraphrase acquisition method that uncovers and exploits generalities underlying paraphrases: paraphrase patterns are first induced and then used to collect novel instances. Unlike existing methods, ours uses both bilingual parallel and monolingual corpora. While the former are regarded as a source of high-quality seed paraphrases, the latter are searched for paraphrases that match patterns learned from the seed paraphrases. We show how one can use monolingual corpora, which are far more numerous and larger than bilingual corpora, to obtain paraphrases that rival in quality those derived directly from bilingual corpora. In our experiments, the number of paraphrase pairs obtained in this way from monolingual corpora was a large multiple of the number of seed paraphrases. Human evaluation through a paraphrase substitution test demonstrated that the newly acquired paraphrase pairs are ofreasonable quality. Remaining noise can be further reduced by filtering seed paraphrases.</p><p>3 0.40558675 <a title="58-tfidf-3" href="./emnlp-2012-Using_Discourse_Information_for_Paraphrase_Extraction.html">135 emnlp-2012-Using Discourse Information for Paraphrase Extraction</a></p>
<p>Author: Michaela Regneri ; Rui Wang</p><p>Abstract: Previous work on paraphrase extraction using parallel or comparable corpora has generally not considered the documents’ discourse structure as a useful information source. We propose a novel method for collecting paraphrases relying on the sequential event order in the discourse, using multiple sequence alignment with a semantic similarity measure. We show that adding discourse information boosts the performance of sentence-level paraphrase acquisition, which consequently gives a tremendous advantage for extracting phraselevel paraphrase fragments from matched sentences. Our system beats an informed baseline by a margin of 50%.</p><p>4 0.20626779 <a title="58-tfidf-4" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>Author: Michael Roth ; Anette Frank</p><p>Abstract: Generating coherent discourse is an important aspect in natural language generation. Our aim is to learn factors that constitute coherent discourse from data, with a focus on how to realize predicate-argument structures in a model that exceeds the sentence level. We present an important subtask for this overall goal, in which we align predicates across comparable texts, admitting partial argument structure correspondence. The contribution of this work is two-fold: We first construct a large corpus resource of comparable texts, including an evaluation set with manual predicate alignments. Secondly, we present a novel approach for aligning predicates across comparable texts using graph-based clustering with Mincuts. Our method significantly outperforms other alignment techniques when applied to this novel alignment task, by a margin of at least 6.5 percentage points in F1-score.</p><p>5 0.13229772 <a title="58-tfidf-5" href="./emnlp-2012-A_Comparison_of_Vector-based_Representations_for_Semantic_Composition.html">4 emnlp-2012-A Comparison of Vector-based Representations for Semantic Composition</a></p>
<p>Author: William Blacoe ; Mirella Lapata</p><p>Abstract: In this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods. We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication. Some are shallow while others operate over syntactic structure, rely on parameter learning, or require access to very large corpora. We find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection. The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method.</p><p>6 0.11350653 <a title="58-tfidf-6" href="./emnlp-2012-Probabilistic_Finite_State_Machines_for_Regression-based_MT_Evaluation.html">108 emnlp-2012-Probabilistic Finite State Machines for Regression-based MT Evaluation</a></p>
<p>7 0.090829156 <a title="58-tfidf-7" href="./emnlp-2012-Concurrent_Acquisition_of_Word_Meaning_and_Lexical_Categories.html">29 emnlp-2012-Concurrent Acquisition of Word Meaning and Lexical Categories</a></p>
<p>8 0.089161187 <a title="58-tfidf-8" href="./emnlp-2012-Multiple_Aspect_Summarization_Using_Integer_Linear_Programming.html">94 emnlp-2012-Multiple Aspect Summarization Using Integer Linear Programming</a></p>
<p>9 0.073287494 <a title="58-tfidf-9" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>10 0.072367087 <a title="58-tfidf-10" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>11 0.070032224 <a title="58-tfidf-11" href="./emnlp-2012-Bilingual_Lexicon_Extraction_from_Comparable_Corpora_Using_Label_Propagation.html">25 emnlp-2012-Bilingual Lexicon Extraction from Comparable Corpora Using Label Propagation</a></p>
<p>12 0.066358387 <a title="58-tfidf-12" href="./emnlp-2012-A_Systematic_Comparison_of_Phrase_Table_Pruning_Techniques.html">11 emnlp-2012-A Systematic Comparison of Phrase Table Pruning Techniques</a></p>
<p>13 0.063713878 <a title="58-tfidf-13" href="./emnlp-2012-An_%22AI_readability%22_Formula_for_French_as_a_Foreign_Language.html">17 emnlp-2012-An "AI readability" Formula for French as a Foreign Language</a></p>
<p>14 0.058368202 <a title="58-tfidf-14" href="./emnlp-2012-A_Bayesian_Model_for_Learning_SCFGs_with_Discontiguous_Rules.html">1 emnlp-2012-A Bayesian Model for Learning SCFGs with Discontiguous Rules</a></p>
<p>15 0.057285201 <a title="58-tfidf-15" href="./emnlp-2012-Forced_Derivation_Tree_based_Model_Training_to_Statistical_Machine_Translation.html">54 emnlp-2012-Forced Derivation Tree based Model Training to Statistical Machine Translation</a></p>
<p>16 0.055893313 <a title="58-tfidf-16" href="./emnlp-2012-Locally_Training_the_Log-Linear_Model_for_SMT.html">86 emnlp-2012-Locally Training the Log-Linear Model for SMT</a></p>
<p>17 0.052641328 <a title="58-tfidf-17" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>18 0.052116536 <a title="58-tfidf-18" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>19 0.050983433 <a title="58-tfidf-19" href="./emnlp-2012-Source_Language_Adaptation_for_Resource-Poor_Machine_Translation.html">118 emnlp-2012-Source Language Adaptation for Resource-Poor Machine Translation</a></p>
<p>20 0.050673302 <a title="58-tfidf-20" href="./emnlp-2012-Document-Wide_Decoding_for_Phrase-Based_Statistical_Machine_Translation.html">35 emnlp-2012-Document-Wide Decoding for Phrase-Based Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.239), (1, -0.011), (2, -0.496), (3, -0.001), (4, 0.205), (5, 0.401), (6, 0.145), (7, -0.182), (8, -0.134), (9, 0.016), (10, 0.175), (11, -0.01), (12, 0.07), (13, -0.115), (14, 0.038), (15, 0.083), (16, -0.009), (17, 0.067), (18, -0.062), (19, 0.026), (20, -0.012), (21, -0.026), (22, -0.006), (23, -0.092), (24, -0.026), (25, 0.023), (26, 0.042), (27, 0.068), (28, -0.042), (29, -0.037), (30, 0.053), (31, -0.014), (32, 0.018), (33, 0.02), (34, -0.008), (35, -0.047), (36, 0.023), (37, -0.041), (38, 0.064), (39, -0.008), (40, 0.035), (41, 0.041), (42, 0.028), (43, 0.017), (44, 0.04), (45, 0.018), (46, -0.002), (47, 0.029), (48, -0.019), (49, -0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96636868 <a title="58-lsi-1" href="./emnlp-2012-Generalizing_Sub-sentential_Paraphrase_Acquisition_across_Original_Signal_Type_of_Text_Pairs.html">58 emnlp-2012-Generalizing Sub-sentential Paraphrase Acquisition across Original Signal Type of Text Pairs</a></p>
<p>Author: Aurelien Max ; Houda Bouamor ; Anne Vilnat</p><p>Abstract: This paper describes a study on the impact of the original signal (text, speech, visual scene, event) of a text pair on the task of both manual and automatic sub-sentential paraphrase acquisition. A corpus of 2,500 annotated sentences in English and French is described, and performance on this corpus is reported for an efficient system combination exploiting a large set of features for paraphrase recognition. A detailed quantified typology of subsentential paraphrases found in our corpus types is given.</p><p>2 0.94243133 <a title="58-lsi-2" href="./emnlp-2012-Enlarging_Paraphrase_Collections_through_Generalization_and_Instantiation.html">39 emnlp-2012-Enlarging Paraphrase Collections through Generalization and Instantiation</a></p>
<p>Author: Atsushi Fujita ; Pierre Isabelle ; Roland Kuhn</p><p>Abstract: This paper presents a paraphrase acquisition method that uncovers and exploits generalities underlying paraphrases: paraphrase patterns are first induced and then used to collect novel instances. Unlike existing methods, ours uses both bilingual parallel and monolingual corpora. While the former are regarded as a source of high-quality seed paraphrases, the latter are searched for paraphrases that match patterns learned from the seed paraphrases. We show how one can use monolingual corpora, which are far more numerous and larger than bilingual corpora, to obtain paraphrases that rival in quality those derived directly from bilingual corpora. In our experiments, the number of paraphrase pairs obtained in this way from monolingual corpora was a large multiple of the number of seed paraphrases. Human evaluation through a paraphrase substitution test demonstrated that the newly acquired paraphrase pairs are ofreasonable quality. Remaining noise can be further reduced by filtering seed paraphrases.</p><p>3 0.87540078 <a title="58-lsi-3" href="./emnlp-2012-Using_Discourse_Information_for_Paraphrase_Extraction.html">135 emnlp-2012-Using Discourse Information for Paraphrase Extraction</a></p>
<p>Author: Michaela Regneri ; Rui Wang</p><p>Abstract: Previous work on paraphrase extraction using parallel or comparable corpora has generally not considered the documents’ discourse structure as a useful information source. We propose a novel method for collecting paraphrases relying on the sequential event order in the discourse, using multiple sequence alignment with a semantic similarity measure. We show that adding discourse information boosts the performance of sentence-level paraphrase acquisition, which consequently gives a tremendous advantage for extracting phraselevel paraphrase fragments from matched sentences. Our system beats an informed baseline by a margin of 50%.</p><p>4 0.46865824 <a title="58-lsi-4" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>Author: Michael Roth ; Anette Frank</p><p>Abstract: Generating coherent discourse is an important aspect in natural language generation. Our aim is to learn factors that constitute coherent discourse from data, with a focus on how to realize predicate-argument structures in a model that exceeds the sentence level. We present an important subtask for this overall goal, in which we align predicates across comparable texts, admitting partial argument structure correspondence. The contribution of this work is two-fold: We first construct a large corpus resource of comparable texts, including an evaluation set with manual predicate alignments. Secondly, we present a novel approach for aligning predicates across comparable texts using graph-based clustering with Mincuts. Our method significantly outperforms other alignment techniques when applied to this novel alignment task, by a margin of at least 6.5 percentage points in F1-score.</p><p>5 0.3037985 <a title="58-lsi-5" href="./emnlp-2012-Probabilistic_Finite_State_Machines_for_Regression-based_MT_Evaluation.html">108 emnlp-2012-Probabilistic Finite State Machines for Regression-based MT Evaluation</a></p>
<p>Author: Mengqiu Wang ; Christopher D. Manning</p><p>Abstract: Accurate and robust metrics for automatic evaluation are key to the development of statistical machine translation (MT) systems. We first introduce a new regression model that uses a probabilistic finite state machine (pFSM) to compute weighted edit distance as predictions of translation quality. We also propose a novel pushdown automaton extension of the pFSM model for modeling word swapping and cross alignments that cannot be captured by standard edit distance models. Our models can easily incorporate a rich set of linguistic features, and automatically learn their weights, eliminating the need for ad-hoc parameter tuning. Our methods achieve state-of-the-art correlation with human judgments on two different prediction tasks across a diverse set of standard evaluations (NIST OpenMT06,08; WMT0608).</p><p>6 0.2912353 <a title="58-lsi-6" href="./emnlp-2012-Bilingual_Lexicon_Extraction_from_Comparable_Corpora_Using_Label_Propagation.html">25 emnlp-2012-Bilingual Lexicon Extraction from Comparable Corpora Using Label Propagation</a></p>
<p>7 0.27433446 <a title="58-lsi-7" href="./emnlp-2012-Source_Language_Adaptation_for_Resource-Poor_Machine_Translation.html">118 emnlp-2012-Source Language Adaptation for Resource-Poor Machine Translation</a></p>
<p>8 0.27111295 <a title="58-lsi-8" href="./emnlp-2012-A_Comparison_of_Vector-based_Representations_for_Semantic_Composition.html">4 emnlp-2012-A Comparison of Vector-based Representations for Semantic Composition</a></p>
<p>9 0.21429482 <a title="58-lsi-9" href="./emnlp-2012-Language_Model_Rest_Costs_and_Space-Efficient_Storage.html">74 emnlp-2012-Language Model Rest Costs and Space-Efficient Storage</a></p>
<p>10 0.20663276 <a title="58-lsi-10" href="./emnlp-2012-Multiple_Aspect_Summarization_Using_Integer_Linear_Programming.html">94 emnlp-2012-Multiple Aspect Summarization Using Integer Linear Programming</a></p>
<p>11 0.2051114 <a title="58-lsi-11" href="./emnlp-2012-Forced_Derivation_Tree_based_Model_Training_to_Statistical_Machine_Translation.html">54 emnlp-2012-Forced Derivation Tree based Model Training to Statistical Machine Translation</a></p>
<p>12 0.19746785 <a title="58-lsi-12" href="./emnlp-2012-An_%22AI_readability%22_Formula_for_French_as_a_Foreign_Language.html">17 emnlp-2012-An "AI readability" Formula for French as a Foreign Language</a></p>
<p>13 0.19326963 <a title="58-lsi-13" href="./emnlp-2012-Concurrent_Acquisition_of_Word_Meaning_and_Lexical_Categories.html">29 emnlp-2012-Concurrent Acquisition of Word Meaning and Lexical Categories</a></p>
<p>14 0.17396633 <a title="58-lsi-14" href="./emnlp-2012-A_Bayesian_Model_for_Learning_SCFGs_with_Discontiguous_Rules.html">1 emnlp-2012-A Bayesian Model for Learning SCFGs with Discontiguous Rules</a></p>
<p>15 0.16731882 <a title="58-lsi-15" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>16 0.16666001 <a title="58-lsi-16" href="./emnlp-2012-Cross-Lingual_Language_Modeling_with_Syntactic_Reordering_for_Low-Resource_Speech_Recognition.html">31 emnlp-2012-Cross-Lingual Language Modeling with Syntactic Reordering for Low-Resource Speech Recognition</a></p>
<p>17 0.15858655 <a title="58-lsi-17" href="./emnlp-2012-PATTY%3A_A_Taxonomy_of_Relational_Patterns_with_Semantic_Types.html">103 emnlp-2012-PATTY: A Taxonomy of Relational Patterns with Semantic Types</a></p>
<p>18 0.1580936 <a title="58-lsi-18" href="./emnlp-2012-Universal_Grapheme-to-Phoneme_Prediction_Over_Latin_Alphabets.html">132 emnlp-2012-Universal Grapheme-to-Phoneme Prediction Over Latin Alphabets</a></p>
<p>19 0.15791023 <a title="58-lsi-19" href="./emnlp-2012-Assessment_of_ESL_Learners%27_Syntactic_Competence_Based_on_Similarity_Measures.html">21 emnlp-2012-Assessment of ESL Learners' Syntactic Competence Based on Similarity Measures</a></p>
<p>20 0.15776652 <a title="58-lsi-20" href="./emnlp-2012-Learning_Syntactic_Categories_Using_Paradigmatic_Representations_of_Word_Context.html">79 emnlp-2012-Learning Syntactic Categories Using Paradigmatic Representations of Word Context</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.019), (16, 0.015), (25, 0.011), (34, 0.07), (45, 0.018), (60, 0.556), (63, 0.052), (64, 0.022), (65, 0.024), (70, 0.02), (74, 0.042), (76, 0.024), (79, 0.013), (80, 0.02), (86, 0.014), (95, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99539727 <a title="58-lda-1" href="./emnlp-2012-Generalizing_Sub-sentential_Paraphrase_Acquisition_across_Original_Signal_Type_of_Text_Pairs.html">58 emnlp-2012-Generalizing Sub-sentential Paraphrase Acquisition across Original Signal Type of Text Pairs</a></p>
<p>Author: Aurelien Max ; Houda Bouamor ; Anne Vilnat</p><p>Abstract: This paper describes a study on the impact of the original signal (text, speech, visual scene, event) of a text pair on the task of both manual and automatic sub-sentential paraphrase acquisition. A corpus of 2,500 annotated sentences in English and French is described, and performance on this corpus is reported for an efficient system combination exploiting a large set of features for paraphrase recognition. A detailed quantified typology of subsentential paraphrases found in our corpus types is given.</p><p>2 0.99284333 <a title="58-lda-2" href="./emnlp-2012-Iterative_Annotation_Transformation_with_Predict-Self_Reestimation_for_Chinese_Word_Segmentation.html">68 emnlp-2012-Iterative Annotation Transformation with Predict-Self Reestimation for Chinese Word Segmentation</a></p>
<p>Author: Wenbin Jiang ; Fandong Meng ; Qun Liu ; Yajuan Lu</p><p>Abstract: In this paper we first describe the technology of automatic annotation transformation, which is based on the annotation adaptation algorithm (Jiang et al., 2009). It can automatically transform a human-annotated corpus from one annotation guideline to another. We then propose two optimization strategies, iterative training and predict-selfreestimation, to further improve the accuracy of annotation guideline transformation. Experiments on Chinese word segmentation show that, the iterative training strategy together with predictself reestimation brings significant improvement over the simple annotation transformation baseline, and leads to classifiers with significantly higher accuracy and several times faster processing than annotation adaptation does. On the Penn Chinese Treebank 5.0, , it achieves an F-measure of 98.43%, significantly outperforms previous works although using a single classifier with only local features.</p><p>3 0.99160749 <a title="58-lda-3" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>Author: Avirup Sil ; Ernest Cronin ; Penghai Nie ; Yinfei Yang ; Ana-Maria Popescu ; Alexander Yates</p><p>Abstract: Existing techniques for disambiguating named entities in text mostly focus on Wikipedia as a target catalog of entities. Yet for many types of entities, such as restaurants and cult movies, relational databases exist that contain far more extensive information than Wikipedia. This paper introduces a new task, called Open-Database Named-Entity Disambiguation (Open-DB NED), in which a system must be able to resolve named entities to symbols in an arbitrary database, without requiring labeled data for each new database. We introduce two techniques for Open-DB NED, one based on distant supervision and the other based on domain adaptation. In experiments on two domains, one with poor coverage by Wikipedia and the other with near-perfect coverage, our Open-DB NED strategies outperform a state-of-the-art Wikipedia NED system by over 25% in accuracy.</p><p>4 0.98977983 <a title="58-lda-4" href="./emnlp-2012-Entity_based_QA_Retrieval.html">41 emnlp-2012-Entity based QA Retrieval</a></p>
<p>Author: Amit Singh</p><p>Abstract: Bridging the lexical gap between the user’s question and the question-answer pairs in the Q&A; archives has been a major challenge for Q&A; retrieval. State-of-the-art approaches address this issue by implicitly expanding the queries with additional words using statistical translation models. While useful, the effectiveness of these models is highly dependant on the availability of quality corpus in the absence of which they are troubled by noise issues. Moreover these models perform word based expansion in a context agnostic manner resulting in translation that might be mixed and fairly general. This results in degraded retrieval performance. In this work we address the above issues by extending the lexical word based translation model to incorporate semantic concepts (entities). We explore strategies to learn the translation probabilities between words and the concepts using the Q&A; archives and a popular entity catalog. Experiments conducted on a large scale real data show that the proposed techniques are promising.</p><p>5 0.97530019 <a title="58-lda-5" href="./emnlp-2012-Grounded_Models_of_Semantic_Representation.html">61 emnlp-2012-Grounded Models of Semantic Representation</a></p>
<p>Author: Carina Silberer ; Mirella Lapata</p><p>Abstract: A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two.</p><p>6 0.95032686 <a title="58-lda-6" href="./emnlp-2012-Exploring_Adaptor_Grammars_for_Native_Language_Identification.html">48 emnlp-2012-Exploring Adaptor Grammars for Native Language Identification</a></p>
<p>7 0.89466637 <a title="58-lda-7" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>8 0.87637228 <a title="58-lda-8" href="./emnlp-2012-Enlarging_Paraphrase_Collections_through_Generalization_and_Instantiation.html">39 emnlp-2012-Enlarging Paraphrase Collections through Generalization and Instantiation</a></p>
<p>9 0.85732478 <a title="58-lda-9" href="./emnlp-2012-Wiki-ly_Supervised_Part-of-Speech_Tagging.html">138 emnlp-2012-Wiki-ly Supervised Part-of-Speech Tagging</a></p>
<p>10 0.85145694 <a title="58-lda-10" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>11 0.84587789 <a title="58-lda-11" href="./emnlp-2012-Why_Question_Answering_using_Sentiment_Analysis_and_Word_Classes.html">137 emnlp-2012-Why Question Answering using Sentiment Analysis and Word Classes</a></p>
<p>12 0.84481108 <a title="58-lda-12" href="./emnlp-2012-Multi-Domain_Learning%3A_When_Do_Domains_Matter%3F.html">92 emnlp-2012-Multi-Domain Learning: When Do Domains Matter?</a></p>
<p>13 0.84450978 <a title="58-lda-13" href="./emnlp-2012-An_Entity-Topic_Model_for_Entity_Linking.html">19 emnlp-2012-An Entity-Topic Model for Entity Linking</a></p>
<p>14 0.83672065 <a title="58-lda-14" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>15 0.83145332 <a title="58-lda-15" href="./emnlp-2012-Using_Discourse_Information_for_Paraphrase_Extraction.html">135 emnlp-2012-Using Discourse Information for Paraphrase Extraction</a></p>
<p>16 0.81799185 <a title="58-lda-16" href="./emnlp-2012-Probabilistic_Finite_State_Machines_for_Regression-based_MT_Evaluation.html">108 emnlp-2012-Probabilistic Finite State Machines for Regression-based MT Evaluation</a></p>
<p>17 0.81557882 <a title="58-lda-17" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>18 0.81183994 <a title="58-lda-18" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>19 0.80972463 <a title="58-lda-19" href="./emnlp-2012-A_Unified_Approach_to_Transliteration-based_Text_Input_with_Online_Spelling_Correction.html">13 emnlp-2012-A Unified Approach to Transliteration-based Text Input with Online Spelling Correction</a></p>
<p>20 0.80715322 <a title="58-lda-20" href="./emnlp-2012-Translation_Model_Based_Cross-Lingual_Language_Model_Adaptation%3A_from_Word_Models_to_Phrase_Models.html">128 emnlp-2012-Translation Model Based Cross-Lingual Language Model Adaptation: from Word Models to Phrase Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
