<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-12" href="#">emnlp2012-12</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</h1>
<br/><p>Source: <a title="emnlp-2012-12-pdf" href="http://aclweb.org/anthology//D/D12/D12-1133.pdf">pdf</a></p><p>Author: Bernd Bohnet ; Joakim Nivre</p><p>Abstract: Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins. We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees. Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages.</p><p>Reference: <a title="emnlp-2012-12-reference" href="../emnlp2012_reference/emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins. [sent-3, score-0.69]
</p><p>2 We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees. [sent-4, score-0.66]
</p><p>3 Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages. [sent-5, score-0.488]
</p><p>4 1 Introduction Dependency-based  syntactic parsing has been the  focus of intense research efforts during the last decade, and the state of the art today is represented by globally normalized discriminative models that are induced using structured learning. [sent-6, score-0.195]
</p><p>5 Graphbased models parameterize the parsing problem by the structure of the dependency graph and normally use dynamic programming for inference (McDonald et al. [sent-7, score-0.355]
</p><p>6 Transitionbased models parameterize the problem by elementary parsing actions and typically use incremental beam search (Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Clark, 2011). [sent-12, score-0.494]
</p><p>7 It is noteworthy, however, that almost all dependency parsers presuppose that the words of an input sentence have been morphologically disambiguated using (at least) a part-of-speech tagger. [sent-17, score-0.458]
</p><p>8 , 2006; Petrov and Klein, 2007), which not only can perform their own part-of-speech tagging but normally give better parsing accuracy when they are allowed to do so. [sent-19, score-0.396]
</p><p>9 This suggests that joint models for tagging and parsing might improve accuracy also in the case of dependency parsing. [sent-20, score-0.67]
</p><p>10 It has been argued that joint morphological and syntactic disambiguation is especially important for richly inflected languages, where there is considerable interaction between morphology and syntax such that neither can be fully disambiguated without considering the other. [sent-21, score-0.309]
</p><p>11 (201 1) show that a discriminative model for joint morphological disambiguation and dependency parsing out-  performs a pipeline model in experiments on Latin, Ancient Greek, Czech and Hungarian. [sent-23, score-0.545]
</p><p>12 (201 1) report improvements with a joint model also for Chinese, which is not a richly inflected language but is nevertheless rich in part-of-speech ambiguities. [sent-26, score-0.23]
</p><p>13 In this paper, we present a transition-based model for joint part-of-speech tagging and labeled dependency parsing with non-projective trees. [sent-27, score-0.606]
</p><p>14 To our knowledge, this is the first joint system that performs labeled dependency parsing. [sent-31, score-0.274]
</p><p>15 It is also the first joint system that achieves state-of-the-art accuracy for non-projective dependency parsing. [sent-32, score-0.338]
</p><p>16 2  Transition-Based Tagging and Parsing  Transition-based dependency parsing was pioneered  by Yamada and Matsumoto (2003) and Nivre et al. [sent-33, score-0.355]
</p><p>17 In addition, a number of different transition systems have been proposed, in particular for dealing with non-projective dependencies, which were beyond the scope of early systems (Attardi, 2006; Nivre, 2007; Nivre, 2009; Titov et al. [sent-36, score-0.189]
</p><p>18 In this section, we start by defining a transition system for joint tagging and parsing based on the non-projective transition system proposed in Nivre (2009). [sent-38, score-0.782]
</p><p>19 We then show how to perform beam search and structured online learning with this model, and conclude by discussing feature representations. [sent-39, score-0.282]
</p><p>20 1 Transition System Given a set P of part-of-speech tags and a set D of dependency labels, a tagged dependency tree for a sentence x = w1, . [sent-41, score-0.489]
</p><p>21 The functions π and δ assign a unique part-of-speech label to each node/word and a unique dependency label to each arc, respectively. [sent-55, score-0.238]
</p><p>22 This notion of dependency tree differs from the standard definition only by including part-of-speech labels as well as dependency labels (K¨ ubler et al. [sent-56, score-0.444]
</p><p>23 Following Nivre (2008), we define a transition system for dependency parsing as a quadruple S = (C, T, cs, Ct), where 1. [sent-58, score-0.544]
</p><p>24 A transition sequence for a sentence x in S is a sequence of configuration-transition pairs C0,m = [(c0, t0) , (c1, t1) , . [sent-63, score-0.189]
</p><p>25 1 In th)is ∈ paper, we take the set (C0 o≤f configurations to be the set of all 5-tuples c = (Σ, B, A, π, δ) such that (the stack) and B (the buffer) are disjoint sublists of the nodes Vx of some sentence x, A is a set of dependency arcs over Vx, and π and δ are labeling functions as defined above. [sent-67, score-0.407]
</p><p>26 The tagged dependency tree defined for x by c = (Σ, B, A, π, δ) is the tree (Vx, A) with labeling functions π and δ, which we write TREE(x, c). [sent-78, score-0.369]
</p><p>27 The LEFT-ARCd and RIGHT-ARCd transitions both add an arc (with dependency label d) between the two nodes on top of the stack and replaces these nodes by the head node of the new arc (which is the rightmost node for LEFT-ARCd and the leftmost node for RIGHT-ARCd). [sent-80, score-0.603]
</p><p>28 The SHIFTp transition extracts the 1This definition of transition sequence differs from that of Nivre (2008) but is equivalent and suits our presentation better. [sent-81, score-0.378]
</p><p>29 represented as a list with its head to the right (and tail σ) and the buffer B as a list with its head to the left (and tail β). [sent-82, score-0.257]
</p><p>30 first node in the buffer, pushes it onto the stack and labels it with the part-of-speech tag p. [sent-84, score-0.268]
</p><p>31 The SWAP transition extracts the second topmost node from the stack and moves it back to the buffer, subject to the condition that the two top nodes on the stack are still in the order given by the sentence. [sent-85, score-0.523]
</p><p>32 Except for the addition of a tag parameter p to the SHIFT transition, this is equivalent to the system described in Nivre (2009), which thanks to the SWAP transition can handle arbitrary non-projective  trees. [sent-86, score-0.29]
</p><p>33 The only thing to note is that, before a terminal configuration can be reached, every word has to be pushed onto the stack in a SHIFTp transition, which ensures that every node/word in the output tree will be tagged. [sent-88, score-0.352]
</p><p>34 2 Inference and Learning While early transition-based parsers generally used greedy best-first inference and locally trained classifiers, recent work has shown that higher accuracy can be obtained using beam search and global structure learning to mitigate error propagation. [sent-90, score-0.418]
</p><p>35 Since joint tagging and parsing increases the size of the search space and is likely to require novel features, we use beam search in combination with structured perceptron learning. [sent-92, score-0.763]
</p><p>36 The beam search algorithm used to derive the best parse y for a sentence x is outlined in Figure 2. [sent-93, score-0.318]
</p><p>37 00}dim(w)  + +  Figure 2: Beam search algorithm forjoint tagging and dependency parsing of input sentence x with weight vector w and beam parameters b1 and b2. [sent-116, score-0.816]
</p><p>38 A parse hypothesis h is represented by a configuration h. [sent-125, score-0.186]
</p><p>39 Hypotheses are stored in the list BEAM, which is sorted by descending scores and initialized to hold the hypothesis h0 corresponding to the initial configuration cs (x) with score 0. [sent-130, score-0.238]
</p><p>40 The main loop terminates when all hypotheses in BEAM contain terminal configurations, and the dependency tree extracted from the top scoring hypothesis is returned (lines 14–16). [sent-134, score-0.419]
</p><p>41 The set of new hypotheses is created in two nested loops (lines 7–12), where every hypothesis h in BEAM is updated using every permissible transition t for the configuration h. [sent-135, score-0.486]
</p><p>42 The pruning parameters b1 and b2 determine the number of hypotheses allowed in the beam and at the same time control the tradeoff between syntactic and morphological ambiguity. [sent-145, score-0.435]
</p><p>43 First, we extract the b1 highest scoring hypotheses with distinct dependency trees. [sent-146, score-0.28]
</p><p>44 Then we extract the b2 highest scoring remaining hypotheses, which will typically be tag-  ging variants of dependency trees that are already in the beam. [sent-147, score-0.202]
</p><p>45 In this way, we prevent the beam from getting filled up with too many tagging variants of the same dependency tree, which was found to be harmful in preliminary experiments. [sent-148, score-0.663]
</p><p>46 Thus, in the experiments later on, we will typically constrain the parser so that SHIFTp is permissible only if p is one of the k best part-of-speech tags with a score no more than α below the score of the 1-best tag, as determined by a preprocessing tagger. [sent-150, score-0.212]
</p><p>47 Features may refer to any aspect of a configuration, as encoded in the stack Σ, the buffer B, the arc set A and the labelings π and δ. [sent-159, score-0.389]
</p><p>48 (201 1), we have omitted all features that presuppose an arc-eager parsing order, since our transition system defines an arc-standard order. [sent-166, score-0.401]
</p><p>49 Secondly, any  feature that refers to the part-of-speech tag of a word w in the buffer B will in our system refer to the topscoring tag π1 (w), rather than the finally predicted tag. [sent-167, score-0.363]
</p><p>50 By contrast, for a word in the stack Σ, part-ofspeech features refer to the tag π(w) chosen when shifting w onto the stack (which may or may not be the same as π1 (w)). [sent-168, score-0.435]
</p><p>51 In addition to the standard features for transitionbased dependency parsing, we have added features specifically to improve the tagging step in the joint model. [sent-169, score-0.507]
</p><p>52 The templates for these features, which are specified in Figure 3, all involve the ith best tag assigned to the first word of the buffer B (the next word to be shifted in a SHIFTp transition) in combination with neighboring words, word prefixes, word suffixes, score differences and tag rank. [sent-170, score-0.412]
</p><p>53 Graph features are defined over the factors of a graph-based dependency parser, which was shown to improve the accuracy of a transition-based parser by Zhang and Clark (2008). [sent-172, score-0.266]
</p><p>54 (201 1) call delayed features, although they use it for part-of-speech tags in the lookahead while we use it for subgraphs ofthe dependency tree. [sent-175, score-0.247]
</p><p>55 This is usually done via a hash table, but significant speedups can be achieved by using a hash kernel, which simply replaces table lookup by a hash function (Bloom, 1970; Shi et al. [sent-180, score-0.412]
</p><p>56 As a result, the hash kernel often improves accuracy as well as efficiency compared to traditional techniques that only make use of features that occur in gold standard parses (Bohnet, 2010). [sent-183, score-0.23]
</p><p>57 3  Experiments  We have evaluated the model for joint tagging and dependency parsing on four typologically diverse languages: Chinese, Czech, English, and German. [sent-184, score-0.606]
</p><p>58 4 In order to assign k-best part-of-speech tags and scores to words in the training set, we used a perceptron tagger with 10-fold jack-knifing. [sent-199, score-0.201]
</p><p>59 The 1-best tagging accuracy for section 23 of the Penn Treebank is 97. [sent-202, score-0.243]
</p><p>60 We report the following evaluation metrics: partof-speech accuracy (POS), unlabeled attachment score (UAS), labeled attachment score (LAS), and tagged labeled attachment score (TLAS). [sent-225, score-0.211]
</p><p>61 TLAS is a new metric defined as the percentage of words that are assigned the correct part-of-speech tag, the correct head and the correct dependency label. [sent-226, score-0.25]
</p><p>62 2 Results Table 1 presents results on the development sets of the CoNLL 2009 shared task with varying values of the two tag parameters k (number of candidates) and α (maximum score difference to 1-best tag) and beam parameters fixed at b1 = 40 and b2 = 4. [sent-229, score-0.432]
</p><p>63 For German, finally, we see the greatest improvement with k = 3 6While tagging accuracy (POS) increases with larger values of α, TLAS decreases because of a drop in LAS. [sent-244, score-0.243]
</p><p>64 Rows 5–6: Wider beam (b1  = 80) and added  = 1) and best settings  graph features (G) and cluster features  (C). [sent-248, score-0.332]
</p><p>65 Second beam parameter b2 fixed at 4 in all cases. [sent-249, score-0.282]
</p><p>66 For all languages except English, we obtain state-of-the-art results already with b1 = 40 (row 4), and for all languages both tagging and parsing accuracy improve compared to the baseline (row 3). [sent-255, score-0.484]
</p><p>67 Row 5 shows the scores with a beam of 80 and the addi-  tional graph features. [sent-258, score-0.282]
</p><p>68 Our joint tagger and dependency parser with graph features gives very competitive unlabeled dependency scores for English with 93. [sent-285, score-0.555]
</p><p>69 To the best of our knowledge, this is the highest score reported for a (transition-based) dependency parser that does not use additional information sources. [sent-287, score-0.251]
</p><p>70 By adding cluster features and widening the beam to b1 = 80, we achieve 93. [sent-288, score-0.332]
</p><p>71 42, with a beam of 80 and added graph features, in which case POS accuracy increases from 92. [sent-318, score-0.346]
</p><p>72 The speed of the joint tagger and dependency parser is quite reasonable with about 0. [sent-324, score-0.353]
</p><p>73 4 seconds per sentence on the WSJ-PTB test set, given that we  perform tagging and labeled parsing with a beam of 80 while incorporating the features of a third-order graph-based model. [sent-325, score-0.614]
</p><p>74 and English, where we compared the baseline and  the joint model with respect to F-scores for individual part-of-speech categories and dependency labels. [sent-333, score-0.312]
</p><p>75 Table 5 shows selected entries from the confusion matrix for German, where we see substantial improvements for finite and non-finite verbs, which are often morphologically ambiguous but which can be disambiguated using syntactic context. [sent-335, score-0.174]
</p><p>76 Table 6 gives a similar snapshot for English, and we again see improvements for verb categories that are often morphologically ambiguous, such as past participles, which can be confused for past tense verbs, and present tense verbs in third person singular, which can be confused for nouns. [sent-337, score-0.44]
</p><p>77 For dependency labels, it is hard to extract any striking patterns and it seems that we mainly see an improvement in overall parsing accuracy thanks to less severe tagging errors. [sent-342, score-0.598]
</p><p>78 (201 1), who allpresent discriminative models for joint tagging and dependency parsing. [sent-357, score-0.453]
</p><p>79 However, all three models only perform unlabeled parsing, while our model incorporates dependency labels into the parsing process. [sent-358, score-0.355]
</p><p>80 (201 1) take a graph-based approach to dependency parsing, Hatori et al. [sent-361, score-0.202]
</p><p>81 (201 1) use a transition-based model similar to ours but limited to projective dependency trees. [sent-362, score-0.202]
</p><p>82 (201 1) report consistent improvements in both tagging and parsing accuracy. [sent-366, score-0.381]
</p><p>83 We are thus the first to show consistent improvements in both tagging and (labeled) parsing accuracy across typologically diverse languages at the state-of-the-art level. [sent-369, score-0.489]
</p><p>84 The use of beam search in transition-based dependency parsing in order to mitigate the problem of error propagation was first proposed by Johansson and Nugues (2006), although they still used a locally trained model. [sent-371, score-0.637]
</p><p>85 (201 1) and our own work, although Titov and Henderson (2007) used it to define a generative model by parameterizing the SHIFT transition by an input word. [sent-373, score-0.189]
</p><p>86 Zhang and Clark (2008) was the first to combine beam search with a globally normalized discriminative model, using structured perceptron learning and the early update strategy of Collins and Roark (2004), and also explored the addition of graphbased features to a transition-based parser. [sent-374, score-0.401]
</p><p>87 This approach was further pursued in Zhang and Clark (201 1) and was used by Zhang and Nivre (201 1) to achieve state-of-the-art results in dependency parsing for both Chinese and English through the addition of rich non-local features. [sent-375, score-0.355]
</p><p>88 Huang and Sagae (2010) combined structured perceptron learning and beam search with the use of a graph-structured stack to allow ambiguity packing in the beam, a technique that was reused by Hatori et al. [sent-376, score-0.526]
</p><p>89 5  Conclusion  We have presented the first system for joint partof-speech tagging and labeled dependency parsing with non-projective dependency trees. [sent-381, score-0.808]
</p><p>90 Evaluation on four languages shows consistent improvements in both tagging and parsing accuracy over a pipeline system with state-of-the-art results across the board. [sent-382, score-0.532]
</p><p>91 The error analysis reveals improvements in tagging accuracy for syntactically central categories, mainly verbs, with improvement in syntactic accuracy for core grammatical functions as a result. [sent-383, score-0.392]
</p><p>92 In future work we intend to explore joint models that incorporate not only basic part-of-speech tags but also more fine-grained morphological features. [sent-384, score-0.192]
</p><p>93 Top accuracy and fast dependency parsing is not a contradiction. [sent-400, score-0.419]
</p><p>94 Incremental joint pos tagging and dependency parsing in chinese. [sent-458, score-0.679]
</p><p>95 A discriminative model for joint morphological disambiguation and dependency parsing. [sent-488, score-0.349]
</p><p>96 Joint models for chinese POS tagging and dependency parsing. [sent-492, score-0.467]
</p><p>97 An empirical study of semi-supervised structured conditional models for dependency parsing. [sent-560, score-0.202]
</p><p>98 Feature-rich part-of-speech  tagging with a cyclic dependency network. [sent-573, score-0.381]
</p><p>99 A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing. [sent-585, score-0.202]
</p><p>100 Syntactic processing using the generalized perceptron and beam search. [sent-589, score-0.359]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('beam', 0.282), ('hatori', 0.237), ('dependency', 0.202), ('transition', 0.189), ('tagging', 0.179), ('vx', 0.178), ('bohnet', 0.171), ('stack', 0.167), ('tlas', 0.161), ('buffer', 0.161), ('las', 0.155), ('german', 0.155), ('parsing', 0.153), ('nivre', 0.147), ('hash', 0.124), ('shiftp', 0.115), ('conll', 0.113), ('titov', 0.107), ('tag', 0.101), ('configuration', 0.098), ('zhang', 0.095), ('clark', 0.092), ('tmp', 0.092), ('joakim', 0.088), ('chinese', 0.086), ('koo', 0.083), ('uas', 0.083), ('tagger', 0.079), ('hypotheses', 0.078), ('perceptron', 0.077), ('morphological', 0.075), ('pos', 0.073), ('parsers', 0.072), ('joint', 0.072), ('morphologically', 0.072), ('collins', 0.071), ('permissible', 0.069), ('bernd', 0.066), ('accuracy', 0.064), ('transitions', 0.064), ('henderson', 0.064), ('configurations', 0.062), ('arc', 0.061), ('czech', 0.06), ('presuppose', 0.059), ('richly', 0.059), ('tense', 0.059), ('incremental', 0.059), ('confused', 0.059), ('arcs', 0.056), ('transitionbased', 0.054), ('xavier', 0.053), ('disambiguated', 0.053), ('hypothesis', 0.052), ('labeling', 0.051), ('matsumoto', 0.051), ('cluster', 0.05), ('inflected', 0.05), ('improvements', 0.049), ('carreras', 0.049), ('martins', 0.049), ('score', 0.049), ('head', 0.048), ('pages', 0.048), ('terminal', 0.047), ('terry', 0.047), ('participle', 0.047), ('configurationtransition', 0.046), ('ofyamada', 0.046), ('xxjj', 0.046), ('xj', 0.046), ('tags', 0.045), ('verb', 0.045), ('johansson', 0.044), ('ivan', 0.044), ('schmid', 0.044), ('languages', 0.044), ('pipeline', 0.043), ('petrov', 0.042), ('yamada', 0.042), ('globally', 0.042), ('kernel', 0.042), ('proceedings', 0.042), ('charniak', 0.04), ('sagae', 0.04), ('tree', 0.04), ('gesmundo', 0.04), ('speedups', 0.04), ('treebank', 0.039), ('cs', 0.039), ('categories', 0.038), ('ct', 0.038), ('singular', 0.038), ('li', 0.037), ('converted', 0.037), ('adverb', 0.037), ('shift', 0.037), ('yj', 0.037), ('functions', 0.036), ('parse', 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="12-tfidf-1" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>Author: Bernd Bohnet ; Joakim Nivre</p><p>Abstract: Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins. We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees. Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages.</p><p>2 0.3173351 <a title="12-tfidf-2" href="./emnlp-2012-Improving_Transition-Based_Dependency_Parsing_with_Buffer_Transitions.html">66 emnlp-2012-Improving Transition-Based Dependency Parsing with Buffer Transitions</a></p>
<p>Author: Daniel Fernandez-Gonzalez ; Carlos Gomez-Rodriguez</p><p>Abstract: In this paper, we show that significant improvements in the accuracy of well-known transition-based parsers can be obtained, without sacrificing efficiency, by enriching the parsers with simple transitions that act on buffer nodes. First, we show how adding a specific transition to create either a left or right arc of length one between the first two buffer nodes produces improvements in the accuracy of Nivre’s arc-eager projective parser on a number of datasets from the CoNLL-X shared task. Then, we show that accuracy can also be improved by adding transitions involving the topmost stack node and the second buffer node (allowing a limited form of non-projectivity). None of these transitions has a negative impact on the computational complexity of the algorithm. Although the experiments in this paper use the arc-eager parser, the approach is generic enough to be applicable to any stackbased dependency parser.</p><p>3 0.28409517 <a title="12-tfidf-3" href="./emnlp-2012-Generalized_Higher-Order_Dependency_Parsing_with_Cube_Pruning.html">57 emnlp-2012-Generalized Higher-Order Dependency Parsing with Cube Pruning</a></p>
<p>Author: Hao Zhang ; Ryan McDonald</p><p>Abstract: State-of-the-art graph-based parsers use features over higher-order dependencies that rely on decoding algorithms that are slow and difficult to generalize. On the other hand, transition-based dependency parsers can easily utilize such features without increasing the linear complexity of the shift-reduce system beyond a constant. In this paper, we attempt to address this imbalance for graph-based parsing by generalizing the Eisner (1996) algorithm to handle arbitrary features over higherorder dependencies. The generalization is at the cost of asymptotic efficiency. To account for this, cube pruning for decoding is utilized (Chiang, 2007). For the first time, label tuple and structural features such as valencies can be scored efficiently with third-order features in a graph-based parser. Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation ofthe third-ordermodel of Koo et al. (2010).</p><p>4 0.23929219 <a title="12-tfidf-4" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>Author: Xian Qian ; Yang Liu</p><p>Abstract: In this paper, we propose a novel decoding algorithm for discriminative joint Chinese word segmentation, part-of-speech (POS) tagging, and parsing. Previous work often used a pipeline method Chinese word segmentation followed by POS tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components. In our approach, we train the three individual models separately during training, and incorporate them together in a unified framework during decoding. We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features. As far as we know, this is the first work on joint Chinese word segmentation, POS tagging and parsing. Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system. –</p><p>5 0.23165092 <a title="12-tfidf-5" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>Author: Zhongguo Li ; Guodong Zhou</p><p>Abstract: Most previous approaches to syntactic parsing of Chinese rely on a preprocessing step of word segmentation, thereby assuming there was a clearly defined boundary between morphology and syntax in Chinese. We show how this assumption can fail badly, leading to many out-of-vocabulary words and incompatible annotations. Hence in practice the strict separation of morphology and syntax in the Chinese language proves to be untenable. We present a unified dependency parsing approach for Chinese which takes unsegmented sentences as input and outputs both morphological and syntactic structures with a single model and algorithm. By removing the intermediate word segmentation, the unified parser no longer needs separate notions for words and phrases. Evaluation proves the effectiveness of the unified model and algorithm in parsing structures of words, phrases and sen- tences simultaneously. 1</p><p>6 0.15405671 <a title="12-tfidf-6" href="./emnlp-2012-Generating_Non-Projective_Word_Order_in_Statistical_Linearization.html">59 emnlp-2012-Generating Non-Projective Word Order in Statistical Linearization</a></p>
<p>7 0.15142432 <a title="12-tfidf-7" href="./emnlp-2012-Part-of-Speech_Tagging_for_Chinese-English_Mixed_Texts_with_Dynamic_Features.html">106 emnlp-2012-Part-of-Speech Tagging for Chinese-English Mixed Texts with Dynamic Features</a></p>
<p>8 0.14442816 <a title="12-tfidf-8" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<p>9 0.1437999 <a title="12-tfidf-9" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>10 0.13338521 <a title="12-tfidf-10" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<p>11 0.12059612 <a title="12-tfidf-11" href="./emnlp-2012-Type-Supervised_Hidden_Markov_Models_for_Part-of-Speech_Tagging_with_Incomplete_Tag_Dictionaries.html">129 emnlp-2012-Type-Supervised Hidden Markov Models for Part-of-Speech Tagging with Incomplete Tag Dictionaries</a></p>
<p>12 0.12037957 <a title="12-tfidf-12" href="./emnlp-2012-Spectral_Dependency_Parsing_with_Latent_Variables.html">119 emnlp-2012-Spectral Dependency Parsing with Latent Variables</a></p>
<p>13 0.11973955 <a title="12-tfidf-13" href="./emnlp-2012-Exploiting_Reducibility_in_Unsupervised_Dependency_Parsing.html">46 emnlp-2012-Exploiting Reducibility in Unsupervised Dependency Parsing</a></p>
<p>14 0.118916 <a title="12-tfidf-14" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>15 0.10674686 <a title="12-tfidf-15" href="./emnlp-2012-Left-to-Right_Tree-to-String_Decoding_with_Prediction.html">82 emnlp-2012-Left-to-Right Tree-to-String Decoding with Prediction</a></p>
<p>16 0.096618295 <a title="12-tfidf-16" href="./emnlp-2012-A_Beam-Search_Decoder_for_Grammatical_Error_Correction.html">2 emnlp-2012-A Beam-Search Decoder for Grammatical Error Correction</a></p>
<p>17 0.094448969 <a title="12-tfidf-17" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>18 0.093656383 <a title="12-tfidf-18" href="./emnlp-2012-Forest_Reranking_through_Subtree_Ranking.html">55 emnlp-2012-Forest Reranking through Subtree Ranking</a></p>
<p>19 0.092280366 <a title="12-tfidf-19" href="./emnlp-2012-Wiki-ly_Supervised_Part-of-Speech_Tagging.html">138 emnlp-2012-Wiki-ly Supervised Part-of-Speech Tagging</a></p>
<p>20 0.091131933 <a title="12-tfidf-20" href="./emnlp-2012-Parse%2C_Price_and_Cut-Delayed_Column_and_Row_Generation_for_Graph_Based_Parsers.html">104 emnlp-2012-Parse, Price and Cut-Delayed Column and Row Generation for Graph Based Parsers</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.342), (1, -0.283), (2, 0.305), (3, -0.137), (4, 0.159), (5, 0.032), (6, 0.02), (7, -0.04), (8, -0.031), (9, 0.101), (10, 0.072), (11, -0.139), (12, 0.143), (13, -0.06), (14, 0.209), (15, -0.037), (16, -0.031), (17, 0.0), (18, 0.091), (19, 0.135), (20, 0.017), (21, -0.199), (22, -0.088), (23, 0.111), (24, -0.116), (25, -0.064), (26, 0.028), (27, -0.028), (28, 0.043), (29, 0.085), (30, 0.081), (31, 0.118), (32, 0.05), (33, -0.062), (34, -0.021), (35, 0.021), (36, -0.023), (37, -0.113), (38, -0.049), (39, -0.013), (40, 0.022), (41, 0.02), (42, -0.034), (43, 0.087), (44, 0.035), (45, 0.014), (46, 0.001), (47, 0.013), (48, -0.006), (49, -0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95270067 <a title="12-lsi-1" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>Author: Bernd Bohnet ; Joakim Nivre</p><p>Abstract: Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins. We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees. Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages.</p><p>2 0.90401477 <a title="12-lsi-2" href="./emnlp-2012-Improving_Transition-Based_Dependency_Parsing_with_Buffer_Transitions.html">66 emnlp-2012-Improving Transition-Based Dependency Parsing with Buffer Transitions</a></p>
<p>Author: Daniel Fernandez-Gonzalez ; Carlos Gomez-Rodriguez</p><p>Abstract: In this paper, we show that significant improvements in the accuracy of well-known transition-based parsers can be obtained, without sacrificing efficiency, by enriching the parsers with simple transitions that act on buffer nodes. First, we show how adding a specific transition to create either a left or right arc of length one between the first two buffer nodes produces improvements in the accuracy of Nivre’s arc-eager projective parser on a number of datasets from the CoNLL-X shared task. Then, we show that accuracy can also be improved by adding transitions involving the topmost stack node and the second buffer node (allowing a limited form of non-projectivity). None of these transitions has a negative impact on the computational complexity of the algorithm. Although the experiments in this paper use the arc-eager parser, the approach is generic enough to be applicable to any stackbased dependency parser.</p><p>3 0.75712615 <a title="12-lsi-3" href="./emnlp-2012-Generalized_Higher-Order_Dependency_Parsing_with_Cube_Pruning.html">57 emnlp-2012-Generalized Higher-Order Dependency Parsing with Cube Pruning</a></p>
<p>Author: Hao Zhang ; Ryan McDonald</p><p>Abstract: State-of-the-art graph-based parsers use features over higher-order dependencies that rely on decoding algorithms that are slow and difficult to generalize. On the other hand, transition-based dependency parsers can easily utilize such features without increasing the linear complexity of the shift-reduce system beyond a constant. In this paper, we attempt to address this imbalance for graph-based parsing by generalizing the Eisner (1996) algorithm to handle arbitrary features over higherorder dependencies. The generalization is at the cost of asymptotic efficiency. To account for this, cube pruning for decoding is utilized (Chiang, 2007). For the first time, label tuple and structural features such as valencies can be scored efficiently with third-order features in a graph-based parser. Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation ofthe third-ordermodel of Koo et al. (2010).</p><p>4 0.68997854 <a title="12-lsi-4" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>Author: Zhongguo Li ; Guodong Zhou</p><p>Abstract: Most previous approaches to syntactic parsing of Chinese rely on a preprocessing step of word segmentation, thereby assuming there was a clearly defined boundary between morphology and syntax in Chinese. We show how this assumption can fail badly, leading to many out-of-vocabulary words and incompatible annotations. Hence in practice the strict separation of morphology and syntax in the Chinese language proves to be untenable. We present a unified dependency parsing approach for Chinese which takes unsegmented sentences as input and outputs both morphological and syntactic structures with a single model and algorithm. By removing the intermediate word segmentation, the unified parser no longer needs separate notions for words and phrases. Evaluation proves the effectiveness of the unified model and algorithm in parsing structures of words, phrases and sen- tences simultaneously. 1</p><p>5 0.54028058 <a title="12-lsi-5" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>Author: Xian Qian ; Yang Liu</p><p>Abstract: In this paper, we propose a novel decoding algorithm for discriminative joint Chinese word segmentation, part-of-speech (POS) tagging, and parsing. Previous work often used a pipeline method Chinese word segmentation followed by POS tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components. In our approach, we train the three individual models separately during training, and incorporate them together in a unified framework during decoding. We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features. As far as we know, this is the first work on joint Chinese word segmentation, POS tagging and parsing. Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system. –</p><p>6 0.52276468 <a title="12-lsi-6" href="./emnlp-2012-Generating_Non-Projective_Word_Order_in_Statistical_Linearization.html">59 emnlp-2012-Generating Non-Projective Word Order in Statistical Linearization</a></p>
<p>7 0.50427228 <a title="12-lsi-7" href="./emnlp-2012-Exploiting_Reducibility_in_Unsupervised_Dependency_Parsing.html">46 emnlp-2012-Exploiting Reducibility in Unsupervised Dependency Parsing</a></p>
<p>8 0.47453776 <a title="12-lsi-8" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<p>9 0.45177937 <a title="12-lsi-9" href="./emnlp-2012-Spectral_Dependency_Parsing_with_Latent_Variables.html">119 emnlp-2012-Spectral Dependency Parsing with Latent Variables</a></p>
<p>10 0.43357667 <a title="12-lsi-10" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>11 0.43147609 <a title="12-lsi-11" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<p>12 0.41381958 <a title="12-lsi-12" href="./emnlp-2012-Part-of-Speech_Tagging_for_Chinese-English_Mixed_Texts_with_Dynamic_Features.html">106 emnlp-2012-Part-of-Speech Tagging for Chinese-English Mixed Texts with Dynamic Features</a></p>
<p>13 0.37891015 <a title="12-lsi-13" href="./emnlp-2012-Minimal_Dependency_Length_in_Realization_Ranking.html">88 emnlp-2012-Minimal Dependency Length in Realization Ranking</a></p>
<p>14 0.36300039 <a title="12-lsi-14" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>15 0.34260836 <a title="12-lsi-15" href="./emnlp-2012-Dynamic_Programming_for_Higher_Order_Parsing_of_Gap-Minding_Trees.html">37 emnlp-2012-Dynamic Programming for Higher Order Parsing of Gap-Minding Trees</a></p>
<p>16 0.33128944 <a title="12-lsi-16" href="./emnlp-2012-Exploiting_Chunk-level_Features_to_Improve_Phrase_Chunking.html">45 emnlp-2012-Exploiting Chunk-level Features to Improve Phrase Chunking</a></p>
<p>17 0.32916713 <a title="12-lsi-17" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>18 0.32690659 <a title="12-lsi-18" href="./emnlp-2012-Parse%2C_Price_and_Cut-Delayed_Column_and_Row_Generation_for_Graph_Based_Parsers.html">104 emnlp-2012-Parse, Price and Cut-Delayed Column and Row Generation for Graph Based Parsers</a></p>
<p>19 0.32157624 <a title="12-lsi-19" href="./emnlp-2012-Forest_Reranking_through_Subtree_Ranking.html">55 emnlp-2012-Forest Reranking through Subtree Ranking</a></p>
<p>20 0.32048133 <a title="12-lsi-20" href="./emnlp-2012-Wiki-ly_Supervised_Part-of-Speech_Tagging.html">138 emnlp-2012-Wiki-ly Supervised Part-of-Speech Tagging</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.026), (10, 0.016), (16, 0.114), (25, 0.02), (29, 0.015), (34, 0.112), (39, 0.015), (41, 0.012), (45, 0.011), (47, 0.224), (60, 0.088), (63, 0.032), (64, 0.027), (65, 0.027), (70, 0.027), (74, 0.047), (76, 0.038), (80, 0.02), (86, 0.036), (95, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79655313 <a title="12-lda-1" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>Author: Bernd Bohnet ; Joakim Nivre</p><p>Abstract: Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins. We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees. Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages.</p><p>2 0.7700361 <a title="12-lda-2" href="./emnlp-2012-Syntactic_Surprisal_Affects_Spoken_Word_Duration_in_Conversational_Contexts.html">122 emnlp-2012-Syntactic Surprisal Affects Spoken Word Duration in Conversational Contexts</a></p>
<p>Author: Vera Demberg ; Asad Sayeed ; Philip Gorinski ; Nikolaos Engonopoulos</p><p>Abstract: We present results of a novel experiment to investigate speech production in conversational data that links speech rate to information density. We provide the first evidence for an association between syntactic surprisal and word duration in recorded speech. Using the AMI corpus which contains transcriptions of focus group meetings with precise word durations, we show that word durations correlate with syntactic surprisal estimated from the incremental Roark parser over and above simpler measures, such as word duration estimated from a state-of-the-art text-to-speech system and word frequencies, and that the syntactic surprisal estimates are better predictors of word durations than a simpler version of surprisal based on trigram probabilities. This result supports the uniform information density (UID) hypothesis and points a way to more realistic artificial speech generation.</p><p>3 0.66320807 <a title="12-lda-3" href="./emnlp-2012-Improving_Transition-Based_Dependency_Parsing_with_Buffer_Transitions.html">66 emnlp-2012-Improving Transition-Based Dependency Parsing with Buffer Transitions</a></p>
<p>Author: Daniel Fernandez-Gonzalez ; Carlos Gomez-Rodriguez</p><p>Abstract: In this paper, we show that significant improvements in the accuracy of well-known transition-based parsers can be obtained, without sacrificing efficiency, by enriching the parsers with simple transitions that act on buffer nodes. First, we show how adding a specific transition to create either a left or right arc of length one between the first two buffer nodes produces improvements in the accuracy of Nivre’s arc-eager projective parser on a number of datasets from the CoNLL-X shared task. Then, we show that accuracy can also be improved by adding transitions involving the topmost stack node and the second buffer node (allowing a limited form of non-projectivity). None of these transitions has a negative impact on the computational complexity of the algorithm. Although the experiments in this paper use the arc-eager parser, the approach is generic enough to be applicable to any stackbased dependency parser.</p><p>4 0.60919017 <a title="12-lda-4" href="./emnlp-2012-Resolving_This-issue_Anaphora.html">113 emnlp-2012-Resolving This-issue Anaphora</a></p>
<p>Author: Varada Kolhatkar ; Graeme Hirst</p><p>Abstract: We annotate and resolve a particular case of abstract anaphora, namely, thisissue anaphora. We propose a candidate ranking model for this-issue anaphora resolution that explores different issuespecific and general abstract-anaphora features. The model is not restricted to nominal or verbal antecedents; rather, it is able to identify antecedents that are arbitrary spans of text. Our results show that (a) the model outperforms the strong adjacent-sentence baseline; (b) general abstract-anaphora features, as distinguished from issue-specific features, play a crucial role in this-issue anaphora resolution, suggesting that our approach can be generalized for other NPs such as this problem and this debate; and (c) it is possible to reduce the search space in order to improve performance.</p><p>5 0.60541666 <a title="12-lda-5" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>Author: Greg Durrett ; Adam Pauls ; Dan Klein</p><p>Abstract: We consider the problem of using a bilingual dictionary to transfer lexico-syntactic information from a resource-rich source language to a resource-poor target language. In contrast to past work that used bitexts to transfer analyses of specific sentences at the token level, we instead use features to transfer the behavior of words at a type level. In a discriminative dependency parsing framework, our approach produces gains across a range of target languages, using two different lowresource training methodologies (one weakly supervised and one indirectly supervised) and two different dictionary sources (one manually constructed and one automatically constructed).</p><p>6 0.6023035 <a title="12-lda-6" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<p>7 0.59899801 <a title="12-lda-7" href="./emnlp-2012-Generalized_Higher-Order_Dependency_Parsing_with_Cube_Pruning.html">57 emnlp-2012-Generalized Higher-Order Dependency Parsing with Cube Pruning</a></p>
<p>8 0.5964067 <a title="12-lda-8" href="./emnlp-2012-A_Discriminative_Model_for_Query_Spelling_Correction_with_Latent_Structural_SVM.html">5 emnlp-2012-A Discriminative Model for Query Spelling Correction with Latent Structural SVM</a></p>
<p>9 0.59610963 <a title="12-lda-9" href="./emnlp-2012-Left-to-Right_Tree-to-String_Decoding_with_Prediction.html">82 emnlp-2012-Left-to-Right Tree-to-String Decoding with Prediction</a></p>
<p>10 0.59429753 <a title="12-lda-10" href="./emnlp-2012-Exploiting_Chunk-level_Features_to_Improve_Phrase_Chunking.html">45 emnlp-2012-Exploiting Chunk-level Features to Improve Phrase Chunking</a></p>
<p>11 0.58642745 <a title="12-lda-11" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>12 0.58502859 <a title="12-lda-12" href="./emnlp-2012-Document-Wide_Decoding_for_Phrase-Based_Statistical_Machine_Translation.html">35 emnlp-2012-Document-Wide Decoding for Phrase-Based Statistical Machine Translation</a></p>
<p>13 0.58327115 <a title="12-lda-13" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>14 0.58063036 <a title="12-lda-14" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>15 0.5759142 <a title="12-lda-15" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>16 0.57064575 <a title="12-lda-16" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>17 0.56983244 <a title="12-lda-17" href="./emnlp-2012-A_Beam-Search_Decoder_for_Grammatical_Error_Correction.html">2 emnlp-2012-A Beam-Search Decoder for Grammatical Error Correction</a></p>
<p>18 0.56909871 <a title="12-lda-18" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>19 0.56881565 <a title="12-lda-19" href="./emnlp-2012-Forest_Reranking_through_Subtree_Ranking.html">55 emnlp-2012-Forest Reranking through Subtree Ranking</a></p>
<p>20 0.56848681 <a title="12-lda-20" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
