<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>33 emnlp-2012-Discovering Diverse and Salient Threads in Document Collections</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-33" href="#">emnlp2012-33</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>33 emnlp-2012-Discovering Diverse and Salient Threads in Document Collections</h1>
<br/><p>Source: <a title="emnlp-2012-33-pdf" href="http://aclweb.org/anthology//D/D12/D12-1065.pdf">pdf</a></p><p>Author: Jennifer Gillenwater ; Alex Kulesza ; Ben Taskar</p><p>Abstract: We propose a novel probabilistic technique for modeling and extracting salient structure from large document collections. As in clustering and topic modeling, our goal is to provide an organizing perspective into otherwise overwhelming amounts of information. We are particularly interested in revealing and exploiting relationships between documents. To this end, we focus on extracting diverse sets of threads—singlylinked, coherent chains of important documents. To illustrate, we extract research threads from citation graphs and construct timelines from news articles. Our method is highly scalable, running on a corpus of over 30 million words in about four minutes, more than 75 times faster than a dynamic topic model. Finally, the results from our model more closely resemble human news summaries according to several metrics and are also preferred by human judges.</p><p>Reference: <a title="emnlp-2012-33-reference" href="../emnlp2012_reference/emnlp-2012-Discovering_Diverse_and_Salient_Threads_in_Document_Collections_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We propose a novel probabilistic technique for modeling and extracting salient structure from large document collections. [sent-3, score-0.152]
</p><p>2 As in clustering and topic modeling, our goal is to provide an organizing perspective into otherwise overwhelming amounts of information. [sent-4, score-0.094]
</p><p>3 To this end, we focus on extracting diverse sets of threads—singlylinked, coherent chains of important documents. [sent-6, score-0.09]
</p><p>4 To illustrate, we extract research threads from citation graphs and construct timelines from news articles. [sent-7, score-0.922]
</p><p>5 Our method is highly scalable, running on a corpus of over 30 million words in about four minutes, more than 75 times faster than a dynamic topic model. [sent-8, score-0.129]
</p><p>6 Finally, the results from our model more closely resemble human news summaries according to several metrics and  are also preferred by human judges. [sent-9, score-0.16]
</p><p>7 In this work we propose a novel approach: threading structured document collections. [sent-13, score-0.198]
</p><p>8 Con710 sider a large graph, with documents as nodes and edges indicating relationships, as in Figure 1. [sent-14, score-0.136]
</p><p>9 Our goal is to find a diverse set of paths (or threads) through the collection that are individually coherent and together cover the most salient parts. [sent-15, score-0.178]
</p><p>10 For example, given a collection of academic papers, we might want to identify the most significant lines of research, threading  the citation graph to produce chains of important papers. [sent-16, score-0.22]
</p><p>11 Or, given news articles connected chronologically, we might want to extract threads of articles to form timelines describing the major events from the most significant news stories. [sent-17, score-1.177]
</p><p>12 Top-tier news organizations like The New York Times and The Guardian regularly publish such timelines, but have so far been limited to creating them by hand. [sent-18, score-0.089]
</p><p>13 We show how these kinds of threading tasks can be done efficiently, providing a simple, practical tool for representing graph-based data that offers new possibilities compared with existing models. [sent-20, score-0.134]
</p><p>14 Several of TDT’s core tasks, like link detection, topic detection, and topic tracking, can be seen as subroutines for the threading problem. [sent-22, score-0.322]
</p><p>15 and relatedness  to weight nodes  We first build a graph from the collection, using  (documents)  and build edges  from this graph, we extract a diverse, salient set of threads to represent  (relationships) . [sent-27, score-0.902]
</p><p>16 Then,  The supplement  contains a version of this figure for our real-world news dataset. [sent-29, score-0.118]
</p><p>17 and Taskar, 2010) , which offer a natural probabilistic model over sets of structures (such as threads) where diversity is desired, and we incorporate k-DPP extensions to control the number of threads (Kulesza and Taskar, 2011) . [sent-30, score-0.709]
</p><p>18 We apply our model to two real-world datasets, extracting threads of research papers and timelines of news articles. [sent-31, score-0.919]
</p><p>19 An example of news threads extracted using our model is shown in Figure 2. [sent-32, score-0.768]
</p><p>20 Quantitative evaluation shows that our model significantly outperforms multiple baselines, including dynamic topic models, in comparisons with human-produced news summaries. [sent-33, score-0.218]
</p><p>21 It also outperforms baseline methods in a user evaluation of thread coherence, and runs 75 times  faster than a dynamic topic model. [sent-34, score-0.271]
</p><p>22 2  Related Work  A variety of papers from the topic tracking literature are broadly related to our work (Mei and Zhai, 2005; Blei and Lafferty, 2006; Leskovec et al. [sent-36, score-0.17]
</p><p>23 Blei and Lafferty (2006) recently introduced dynamic topic models (DTMs) . [sent-38, score-0.129]
</p><p>24 Assuming a division of documents into time slices, a DTM draws in each slice a set of topics from a Gaussian distribution whose mean is determined by the topics from the previous slice. [sent-39, score-0.111]
</p><p>25 We engineer a baseline for constructing document threads from DTM topic threads (see Section 6. [sent-42, score-1.516]
</p><p>26 However, iDTMs still require placing documents into discrete epochs, and the issue of generating topic rather than document threads remains. [sent-47, score-0.888]
</p><p>27 Swan and Jensen (2000) proposed a system for finding temporally clustered named entities in news text and presenting them on a timeline. [sent-50, score-0.089]
</p><p>28 Allan, Gupta, and Khandelwal (2001) introduced the task of temporal summarization, which takes a stream  of news articles on a particular topic and tries to extract sentences describing important events as they occur. [sent-51, score-0.284]
</p><p>29 Here, we are interested not in extracting topically grouped entities or sentences, but instead in organizing a subset of the articles themselves into timelines, with topic identification as a side effect. [sent-53, score-0.195]
</p><p>30 Above, the threads are shown on a timeline with the most salient words superimposed; below, the dates and headlines from the threads appearing at the bottom are listed. [sent-56, score-1.582]
</p><p>31 Topic models are not designed for threading and often link together topically similar documents that do not constitute a coherent  news story, as on the right. [sent-57, score-0.305]
</p><p>32 Shahaf, Guestrin, and Horvitz (2012) recently proposed metro maps as alternative structured representations of related news stories. [sent-59, score-0.123]
</p><p>33 Metro maps are effectively sets of non-chronological threads that are encouraged to intersect and thus create a “map” of events and topics. [sent-60, score-0.679]
</p><p>34 Shahaf and Guestrin (2010) , for example, assume the thread endpoints are specified, and Chieu and Lee (2004) require a set of query words. [sent-62, score-0.142]
</p><p>35 We prove that  even a logarithmic number of projections is sufficient to yield a close approximation to the original SDPP distribution. [sent-70, score-0.104]
</p><p>36 We assume that the collection has been transformed into a directed graph G = (V, E) on n vertices, where each node corresponds to a document and each edge represents a relationship between documents whose semantics depend on the task. [sent-72, score-0.204]
</p><p>37 The feature map on a thread is then just a sum over the nodes in the thread:  φ(y) =tXT=1φ? [sent-84, score-0.173]
</p><p>38 ) Given this framework, our goal is to develop a probabilistic model over sets of k threads of length T, favoring sets whose threads have large weight but are also distinct from one another with respect to φ. [sent-88, score-1.358]
</p><p>39 In other words, a high-probability set under the model should include threads that are both salient and diverse. [sent-89, score-0.767]
</p><p>40 This is a daunting problem, given that the number of possible sets of threads is O(nkT). [sent-90, score-0.679]
</p><p>41 as a normalized D-dimensional feature vector such that φ(yi)>φ(yj) ∈ [−1, 1] is a measure of similarity between item∈s yi and yj . [sent-105, score-0.174]
</p><p>42 To understand why this is the case, note that determinants are closely related to volumes; in particular, det(LY) is proportional  to the volume spanned by the vectors q(yi)φ(yi) for yi ∈ Y. [sent-107, score-0.224]
</p><p>43 In our setting, Y contains all threads of length T, so each yi ∈ Y Yis a sequence where is the document included in the thread at position t. [sent-111, score-1.014]
</p><p>44 However, it is possible (and efficient, due to the linear scaling) to allow longer threads, as well as threads of variable length. [sent-127, score-0.679]
</p><p>45 The latter effect can be achieved by adding a sin-  gle “dummy” node to the document graph, with incoming edges from all other documents and a single outgoing self-loop edge. [sent-128, score-0.242]
</p><p>46 Shorter threads will simply transition to this dummy node when they are complete. [sent-129, score-0.718]
</p><p>47 714 5 Random projections As described above, the time complexity for sampling sets from SDPPs is O(TrnD2) . [sent-136, score-0.112]
</p><p>48 More recently, Magen and Zouzias (2008) extended this idea to the preservation of volumes spanned by sets of points. [sent-142, score-0.112]
</p><p>49 Here, we use a relationship between determinants and volumes to  ×  adapt the latter result. [sent-143, score-0.103]
</p><p>50 Practically, Theorem 1 says that if we project φ down to dimension d logarithmic in the number of documents and linear in thread length, the L1  ××  variational distance between the true model and the projected model is bounded. [sent-163, score-0.311]
</p><p>51 Displayed beside each  thread are a few of its maximum-tfidf words. [sent-234, score-0.142]
</p><p>52 Paper titles from two of the threads are shown to the right. [sent-235, score-0.679]
</p><p>53 6  Experiments  We begin by showing the performance of random projections on a small, synthetic threading task where the exact model is tractable, with n = 600 and D = 150. [sent-236, score-0.202]
</p><p>54 1 Cora citation graph To qualitatively illustrate our model, we apply it to Cora (McCallum et al. [sent-240, score-0.086]
</p><p>55 We construct a directed graph with papers as nodes and citations as edges; after removing papers with missing metadata or zero outgoing citations, our graph contains n = 28,155 papers. [sent-243, score-0.267]
</p><p>56 We represent each document by the 1000 documents to which it is most similar according to NCS; this results in binary φ of dimension m = n with exactly 1000 non-zeros. [sent-250, score-0.115]
</p><p>57 The dot product between the similarity features  of two documents is thus proportional to the fraction of top-1000 similar documents they have in common. [sent-251, score-0.102]
</p><p>58 The discovered threads occupy distinct regions of word-space, standing apart visually, and contain diverse salient terms. [sent-254, score-0.826]
</p><p>59 2  News articles  For quantitative evaluation, we use newswire data. [sent-256, score-0.101]
</p><p>60 Our dataset comprises over 200,000 articles from the New York Times, collected from 2005-2007 as part of the English Gigaword corpus (Graff and Cieri, 2009) . [sent-257, score-0.101]
</p><p>61 We split the articles into six-month time periods, with an average of n = 34,504 articles per period. [sent-258, score-0.202]
</p><p>62 For each time period, we generate a graph with articles as nodes. [sent-260, score-0.151]
</p><p>63 We use LexRank for node weights and the top-1000 similar documents as similarity features φ, projecting to d = 50, as before (Section 6. [sent-265, score-0.09]
</p><p>64 For all of the following results, we use T = 8 and k = 10 so that the resulting timelines are of a manageable size for analysis. [sent-269, score-0.118]
</p><p>65 1  Graph visualizations  The (very large) news graph for the first half of 2005 can be viewed interactively at http://zoom. [sent-274, score-0.139]
</p><p>66 In this graph each node (dark circle) represents a news article, and is an-  notated with its headline. [sent-276, score-0.178]
</p><p>67 The five colored paths indicate a set of threads sampled from the k-SDPP. [sent-279, score-0.708]
</p><p>68 Headlines of the articles in each thread are colored to match the thread. [sent-280, score-0.272]
</p><p>69 We provide a view of a small subgraph for illustration purposes in Figure 6, which shows the incoming and outgoing edges for a single node. [sent-283, score-0.088]
</p><p>70 SNIXFEACPUSTNBRFEIMOHAUTNYDRBOZIAUFMSTGEDVR'OIAEDBROPUANISEGLOBRVAE'SILGMTNOUH ARTSNM  Figure 6: Snapshot of a single article node and all of its neghboring article nodes. [sent-330, score-0.101]
</p><p>71 2  Baselines  k-means baseline: A simple baseline is to split each six-month period of articles into T equal time slices, then apply k-means clustering to each slice, using NCS to measure distance. [sent-335, score-0.101]
</p><p>72 We then select the most central article from each cluster, and finally match the k articles from time slice ione-to-one with those from slice i+ 1 by computing the pairing that maximizes the average NCS of the pairs, i. [sent-336, score-0.252]
</p><p>73 The result is a set of k threads of length T, where no two threads contain the same article. [sent-339, score-1.358]
</p><p>74 DTM baseline: A more sophisticated baseline is the dynamic topic model (Blei and Lafferty, 2006) , which explicitly attempts to find topics that are smooth through time. [sent-341, score-0.129]
</p><p>75 We then choose, for each topic at each time step, the document with the highest per-word probability of being generated by that topic. [sent-343, score-0.158]
</p><p>76 Documents from the same topic form a single thread. [sent-344, score-0.094]
</p><p>77 3 Comparison to human summaries We compare the threads generated by our baselines and sampled from the k-SDPP to a set of human-generated news summaries. [sent-360, score-0.839]
</p><p>78 The human summaries are not threaded; they are flat, roughly daily news summaries published by Agence France-Presse and found in the Gigaword  corpus, distinguished by their “multi” type tag. [sent-361, score-0.231]
</p><p>79 We compute four statistics: •  •  Cosine similarity: NCS (in percent) betCwoeseinn eth sei mcoilnacraitteynated threads and concatenated human summaries. [sent-365, score-0.679]
</p><p>80 The hyperparameters for all methods—such as the constant feature magnitude ρ for k-SDPPs and the parameter governing topic proportions for DTMs—were tuned to optimize cosine similarity on a development set from January-June 2005. [sent-366, score-0.094]
</p><p>81 Under each measure, the k-SDPP threads more closely resemble human summaries. [sent-369, score-0.679]
</p><p>82 Interlopers: average number of interloper articles identified (out of 2) . [sent-375, score-0.151]
</p><p>83 To obtain a large-scale evaluation of thread coherence, we turn to Mechanical Turk. [sent-379, score-0.142]
</p><p>84 We asked  Turkers to read the headlines and first few sentences of each article in a timeline and then rate the overall narrative coherence of the timeline on a scale of 1 ( “the articles are totally unrelated” ) to 5 ( “the articles tell a single clear story” ) . [sent-380, score-0.51]
</p><p>85 We also had Turkers evaluate threads implicitly by performing a simple task. [sent-383, score-0.679]
</p><p>86 We showed them timelines into which two additional “interloper” articles selected at random had been inserted, and asked them to remove the two articles that they thought should be removed to “improve the flow of the timeline” . [sent-384, score-0.32]
</p><p>87 Intuitively, the interlopers should be selected more often when the original timeline is coherent. [sent-386, score-0.139]
</p><p>88 The average number of interloper articles correctly identified is shown in Table 2. [sent-387, score-0.151]
</p><p>89 5 Runtimes Finally, we report in Table 3 the time required to produce a complete set of threads for each method. [sent-393, score-0.679]
</p><p>90 •  •  •  Neither baseline directly models the document threads themselves. [sent-399, score-0.743]
</p><p>91 This makes the k-SDPP a better choice for applications where, for instance, the coherence of individual threads is important. [sent-401, score-0.715]
</p><p>92 While the baselines seek threads that cover or explain as much of the dataset as possible, k-SDPPs are better suited for tasks where a balance between quality and diversity is key, since its hyperparameters correspond to weights on these quantities. [sent-402, score-0.709]
</p><p>93 With news timelines, for example, we want not just topical diversity but also a focus on the most important stories. [sent-403, score-0.119]
</p><p>94 Both baselines require input to be split into time slices, whereas the k-SDPP does not; this flexibility allows the k-SDPP to put multiple articles from a single time slice in  a thread, or to build threads that span only part of the input period. [sent-404, score-0.84]
</p><p>95 •  While clustering and topic models rely on EM to approximately optimize their objectives, the k-SDPP comes with an exact, polynomial-time sampling algorithm. [sent-405, score-0.138]
</p><p>96 The k-SDPP produces more consistent threads due to its use of graph information, while the DTM threads, though topic-focused, are less coherent as a story. [sent-407, score-0.76]
</p><p>97 Furthermore, DTM threads span the entire time period, while our method selects threads covering only relevant spans. [sent-408, score-1.358]
</p><p>98 7  Conclusion  We introduced the novel problem of finding diverse and salient threads in graphs of large document collections. [sent-410, score-0.89]
</p><p>99 We developed a probabilistic approach, combining SDPPs and k-SDPPs, and showed how random projections make inference  efficient and yield an approximate model with bounded variational distance to the original. [sent-411, score-0.113]
</p><p>100 We then demonstrated that the method produces qualitatively reasonable results, and, relative to several baslines, reproduces human news summaries more faithfully, builds more coherent story threads, and is significantly faster. [sent-412, score-0.191]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('threads', 0.679), ('sdpps', 0.168), ('yx', 0.159), ('thread', 0.142), ('shahaf', 0.134), ('threading', 0.134), ('yi', 0.129), ('kulesza', 0.118), ('dtm', 0.118), ('dtms', 0.118), ('ncs', 0.118), ('timelines', 0.118), ('timeline', 0.105), ('articles', 0.101), ('topic', 0.094), ('news', 0.089), ('salient', 0.088), ('determinantal', 0.084), ('dpps', 0.084), ('kpk', 0.084), ('mobile', 0.072), ('guestrin', 0.072), ('slices', 0.072), ('summaries', 0.071), ('projections', 0.068), ('dpp', 0.067), ('idtms', 0.067), ('magen', 0.067), ('sdpp', 0.067), ('lemma', 0.065), ('document', 0.064), ('slice', 0.06), ('volumes', 0.06), ('diverse', 0.059), ('taskar', 0.058), ('lexrank', 0.058), ('chieu', 0.058), ('cora', 0.058), ('theorem', 0.056), ('edges', 0.054), ('pk', 0.054), ('det', 0.054), ('vol', 0.052), ('spanned', 0.052), ('documents', 0.051), ('baghdad', 0.05), ('clients', 0.05), ('hesterberg', 0.05), ('interloper', 0.05), ('turkers', 0.05), ('graph', 0.05), ('ahmed', 0.048), ('mar', 0.045), ('variational', 0.045), ('yj', 0.045), ('sampling', 0.044), ('determinants', 0.043), ('server', 0.043), ('erkan', 0.043), ('swan', 0.043), ('tracking', 0.043), ('xt', 0.041), ('blei', 0.04), ('gy', 0.039), ('mei', 0.039), ('leskovec', 0.039), ('node', 0.039), ('projected', 0.037), ('citations', 0.036), ('citation', 0.036), ('logarithmic', 0.036), ('coherence', 0.036), ('dynamic', 0.035), ('jan', 0.035), ('outgoing', 0.034), ('apr', 0.034), ('chronologically', 0.034), ('egt', 0.034), ('ekt', 0.034), ('feb', 0.034), ('gaza', 0.034), ('interlopers', 0.034), ('iraqi', 0.034), ('lij', 0.034), ('metro', 0.034), ('tdt', 0.034), ('zoomable', 0.034), ('zouzias', 0.034), ('papers', 0.033), ('allan', 0.032), ('yan', 0.032), ('article', 0.031), ('coherent', 0.031), ('nodes', 0.031), ('headlines', 0.031), ('discovering', 0.03), ('diversity', 0.03), ('graff', 0.029), ('supplement', 0.029), ('colored', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="33-tfidf-1" href="./emnlp-2012-Discovering_Diverse_and_Salient_Threads_in_Document_Collections.html">33 emnlp-2012-Discovering Diverse and Salient Threads in Document Collections</a></p>
<p>Author: Jennifer Gillenwater ; Alex Kulesza ; Ben Taskar</p><p>Abstract: We propose a novel probabilistic technique for modeling and extracting salient structure from large document collections. As in clustering and topic modeling, our goal is to provide an organizing perspective into otherwise overwhelming amounts of information. We are particularly interested in revealing and exploiting relationships between documents. To this end, we focus on extracting diverse sets of threads—singlylinked, coherent chains of important documents. To illustrate, we extract research threads from citation graphs and construct timelines from news articles. Our method is highly scalable, running on a corpus of over 30 million words in about four minutes, more than 75 times faster than a dynamic topic model. Finally, the results from our model more closely resemble human news summaries according to several metrics and are also preferred by human judges.</p><p>2 0.12749316 <a title="33-tfidf-2" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>Author: Michael J. Paul</p><p>Abstract: Recent work has explored the use of hidden Markov models for unsupervised discourse and conversation modeling, where each segment or block of text such as a message in a conversation is associated with a hidden state in a sequence. We extend this approach to allow each block of text to be a mixture of multiple classes. Under our model, the probability of a class in a text block is a log-linear function of the classes in the previous block. We show that this model performs well at predictive tasks on two conversation data sets, improving thread reconstruction accuracy by up to 15 percentage points over a standard HMM. Additionally, we show quantitatively that the induced word clusters correspond to speech acts more closely than baseline models.</p><p>3 0.096983835 <a title="33-tfidf-3" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>Author: Ahmed Hassan ; Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: A mixture of positive (friendly) and negative (antagonistic) relations exist among users in most social media applications. However, many such applications do not allow users to explicitly express the polarity of their interactions. As a result most research has either ignored negative links or was limited to the few domains where such relations are explicitly expressed (e.g. Epinions trust/distrust). We study text exchanged between users in online communities. We find that the polarity of the links between users can be predicted with high accuracy given the text they exchange. This allows us to build a signed network representation of discussions; where every edge has a sign: positive to denote a friendly relation, or negative to denote an antagonistic relation. We also connect our analysis to social psychology theories of balance. We show that the automatically predicted networks are consistent with those theories. Inspired by that, we present a technique for identifying subgroups in discussions by partitioning singed networks representing them.</p><p>4 0.074007519 <a title="33-tfidf-4" href="./emnlp-2012-Exploring_Topic_Coherence_over_Many_Models_and_Many_Topics.html">49 emnlp-2012-Exploring Topic Coherence over Many Models and Many Topics</a></p>
<p>Author: Keith Stevens ; Philip Kegelmeyer ; David Andrzejewski ; David Buttler</p><p>Abstract: We apply two new automated semantic evaluations to three distinct latent topic models. Both metrics have been shown to align with human evaluations and provide a balance between internal measures of information gain and comparisons to human ratings of coherent topics. We improve upon the measures by introducing new aggregate measures that allows for comparing complete topic models. We further compare the automated measures to other metrics for topic models, comparison to manually crafted semantic tests and document classification. Our experiments reveal that LDA and LSA each have different strengths; LDA best learns descriptive topics while LSA is best at creating a compact semantic representation ofdocuments and words in a corpus.</p><p>5 0.071763739 <a title="33-tfidf-5" href="./emnlp-2012-Multiple_Aspect_Summarization_Using_Integer_Linear_Programming.html">94 emnlp-2012-Multiple Aspect Summarization Using Integer Linear Programming</a></p>
<p>Author: Kristian Woodsend ; Mirella Lapata</p><p>Abstract: Multi-document summarization involves many aspects of content selection and surface realization. The summaries must be informative, succinct, grammatical, and obey stylistic writing conventions. We present a method where such individual aspects are learned separately from data (without any hand-engineering) but optimized jointly using an integer linear programme. The ILP framework allows us to combine the decisions of the expert learners and to select and rewrite source content through a mixture of objective setting, soft and hard constraints. Experimental results on the TAC-08 data set show that our model achieves state-of-the-art performance using ROUGE and significantly improves the informativeness of the summaries.</p><p>6 0.070783533 <a title="33-tfidf-6" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>7 0.067603722 <a title="33-tfidf-7" href="./emnlp-2012-Modelling_Sequential_Text_with_an_Adaptive_Topic_Model.html">90 emnlp-2012-Modelling Sequential Text with an Adaptive Topic Model</a></p>
<p>8 0.064078838 <a title="33-tfidf-8" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>9 0.063266829 <a title="33-tfidf-9" href="./emnlp-2012-An_Entity-Topic_Model_for_Entity_Linking.html">19 emnlp-2012-An Entity-Topic Model for Entity Linking</a></p>
<p>10 0.063186973 <a title="33-tfidf-10" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>11 0.062451191 <a title="33-tfidf-11" href="./emnlp-2012-A_Phrase-Discovering_Topic_Model_Using_Hierarchical_Pitman-Yor_Processes.html">8 emnlp-2012-A Phrase-Discovering Topic Model Using Hierarchical Pitman-Yor Processes</a></p>
<p>12 0.058935832 <a title="33-tfidf-12" href="./emnlp-2012-SSHLDA%3A_A_Semi-Supervised_Hierarchical_Topic_Model.html">115 emnlp-2012-SSHLDA: A Semi-Supervised Hierarchical Topic Model</a></p>
<p>13 0.05556035 <a title="33-tfidf-13" href="./emnlp-2012-Explore_Person_Specific_Evidence_in_Web_Person_Name_Disambiguation.html">47 emnlp-2012-Explore Person Specific Evidence in Web Person Name Disambiguation</a></p>
<p>14 0.050761107 <a title="33-tfidf-14" href="./emnlp-2012-Name_Phylogeny%3A_A_Generative_Model_of_String_Variation.html">96 emnlp-2012-Name Phylogeny: A Generative Model of String Variation</a></p>
<p>15 0.047897089 <a title="33-tfidf-15" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>16 0.044602409 <a title="33-tfidf-16" href="./emnlp-2012-Exact_Sampling_and_Decoding_in_High-Order_Hidden_Markov_Models.html">43 emnlp-2012-Exact Sampling and Decoding in High-Order Hidden Markov Models</a></p>
<p>17 0.042844497 <a title="33-tfidf-17" href="./emnlp-2012-Monte_Carlo_MCMC%3A_Efficient_Inference_by_Approximate_Sampling.html">91 emnlp-2012-Monte Carlo MCMC: Efficient Inference by Approximate Sampling</a></p>
<p>18 0.040199496 <a title="33-tfidf-18" href="./emnlp-2012-Revisiting_the_Predictability_of_Language%3A_Response_Completion_in_Social_Media.html">114 emnlp-2012-Revisiting the Predictability of Language: Response Completion in Social Media</a></p>
<p>19 0.038348779 <a title="33-tfidf-19" href="./emnlp-2012-Supervised_Text-based_Geolocation_Using_Language_Models_on_an_Adaptive_Grid.html">121 emnlp-2012-Supervised Text-based Geolocation Using Language Models on an Adaptive Grid</a></p>
<p>20 0.037335251 <a title="33-tfidf-20" href="./emnlp-2012-Do_Neighbours_Help%3F_An_Exploration_of_Graph-based_Algorithms_for_Cross-domain_Sentiment_Classification.html">34 emnlp-2012-Do Neighbours Help? An Exploration of Graph-based Algorithms for Cross-domain Sentiment Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.162), (1, 0.074), (2, 0.043), (3, 0.063), (4, -0.135), (5, 0.077), (6, -0.023), (7, -0.058), (8, 0.013), (9, 0.063), (10, 0.02), (11, -0.031), (12, 0.02), (13, -0.013), (14, -0.021), (15, 0.018), (16, -0.022), (17, -0.026), (18, 0.175), (19, -0.004), (20, -0.097), (21, 0.027), (22, 0.188), (23, -0.042), (24, 0.185), (25, 0.018), (26, -0.085), (27, -0.088), (28, 0.045), (29, -0.114), (30, 0.123), (31, 0.107), (32, -0.057), (33, -0.061), (34, 0.055), (35, -0.004), (36, 0.055), (37, 0.051), (38, -0.001), (39, 0.085), (40, 0.102), (41, 0.052), (42, -0.067), (43, 0.278), (44, -0.004), (45, 0.147), (46, 0.005), (47, 0.115), (48, 0.048), (49, 0.188)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93977189 <a title="33-lsi-1" href="./emnlp-2012-Discovering_Diverse_and_Salient_Threads_in_Document_Collections.html">33 emnlp-2012-Discovering Diverse and Salient Threads in Document Collections</a></p>
<p>Author: Jennifer Gillenwater ; Alex Kulesza ; Ben Taskar</p><p>Abstract: We propose a novel probabilistic technique for modeling and extracting salient structure from large document collections. As in clustering and topic modeling, our goal is to provide an organizing perspective into otherwise overwhelming amounts of information. We are particularly interested in revealing and exploiting relationships between documents. To this end, we focus on extracting diverse sets of threads—singlylinked, coherent chains of important documents. To illustrate, we extract research threads from citation graphs and construct timelines from news articles. Our method is highly scalable, running on a corpus of over 30 million words in about four minutes, more than 75 times faster than a dynamic topic model. Finally, the results from our model more closely resemble human news summaries according to several metrics and are also preferred by human judges.</p><p>2 0.52615976 <a title="33-lsi-2" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>Author: Ahmed Hassan ; Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: A mixture of positive (friendly) and negative (antagonistic) relations exist among users in most social media applications. However, many such applications do not allow users to explicitly express the polarity of their interactions. As a result most research has either ignored negative links or was limited to the few domains where such relations are explicitly expressed (e.g. Epinions trust/distrust). We study text exchanged between users in online communities. We find that the polarity of the links between users can be predicted with high accuracy given the text they exchange. This allows us to build a signed network representation of discussions; where every edge has a sign: positive to denote a friendly relation, or negative to denote an antagonistic relation. We also connect our analysis to social psychology theories of balance. We show that the automatically predicted networks are consistent with those theories. Inspired by that, we present a technique for identifying subgroups in discussions by partitioning singed networks representing them.</p><p>3 0.47966129 <a title="33-lsi-3" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>Author: David McClosky ; Christopher D. Manning</p><p>Abstract: We present a distantly supervised system for extracting the temporal bounds of fluents (relations which only hold during certain times, such as attends school). Unlike previous pipelined approaches, our model does not assume independence between each fluent or even between named entities with known connections (parent, spouse, employer, etc.). Instead, we model what makes timelines of fluents consistent by learning cross-fluent constraints, potentially spanning entities as well. For example, our model learns that someone is unlikely to start a job at age two or to marry someone who hasn’t been born yet. Our system achieves a 36% error reduction over a pipelined baseline.</p><p>4 0.45769393 <a title="33-lsi-4" href="./emnlp-2012-Supervised_Text-based_Geolocation_Using_Language_Models_on_an_Adaptive_Grid.html">121 emnlp-2012-Supervised Text-based Geolocation Using Language Models on an Adaptive Grid</a></p>
<p>Author: Stephen Roller ; Michael Speriosu ; Sarat Rallapalli ; Benjamin Wing ; Jason Baldridge</p><p>Abstract: The geographical properties of words have recently begun to be exploited for geolocating documents based solely on their text, often in the context of social media and online content. One common approach for geolocating texts is rooted in information retrieval. Given training documents labeled with latitude/longitude coordinates, a grid is overlaid on the Earth and pseudo-documents constructed by concatenating the documents within a given grid cell; then a location for a test document is chosen based on the most similar pseudo-document. Uniform grids are normally used, but they are sensitive to the dispersion of documents over the earth. We define an alternative grid construction using k-d trees that more robustly adapts to data, especially with larger training sets. We also provide a better way of choosing the locations for pseudo-documents. We evaluate these strategies on existing Wikipedia and Twitter corpora, as well as a new, larger Twitter corpus. The adaptive grid achieves competitive results with a uniform grid on small training sets and outperforms it on the large Twitter corpus. The two grid constructions can also be combined to produce consistently strong results across all training sets.</p><p>5 0.42354792 <a title="33-lsi-5" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>Author: Michael J. Paul</p><p>Abstract: Recent work has explored the use of hidden Markov models for unsupervised discourse and conversation modeling, where each segment or block of text such as a message in a conversation is associated with a hidden state in a sequence. We extend this approach to allow each block of text to be a mixture of multiple classes. Under our model, the probability of a class in a text block is a log-linear function of the classes in the previous block. We show that this model performs well at predictive tasks on two conversation data sets, improving thread reconstruction accuracy by up to 15 percentage points over a standard HMM. Additionally, we show quantitatively that the induced word clusters correspond to speech acts more closely than baseline models.</p><p>6 0.32881895 <a title="33-lsi-6" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>7 0.32109776 <a title="33-lsi-7" href="./emnlp-2012-Exploring_Topic_Coherence_over_Many_Models_and_Many_Topics.html">49 emnlp-2012-Exploring Topic Coherence over Many Models and Many Topics</a></p>
<p>8 0.31768441 <a title="33-lsi-8" href="./emnlp-2012-Multiple_Aspect_Summarization_Using_Integer_Linear_Programming.html">94 emnlp-2012-Multiple Aspect Summarization Using Integer Linear Programming</a></p>
<p>9 0.29995561 <a title="33-lsi-9" href="./emnlp-2012-Modelling_Sequential_Text_with_an_Adaptive_Topic_Model.html">90 emnlp-2012-Modelling Sequential Text with an Adaptive Topic Model</a></p>
<p>10 0.29909429 <a title="33-lsi-10" href="./emnlp-2012-Large_Scale_Decipherment_for_Out-of-Domain_Machine_Translation.html">75 emnlp-2012-Large Scale Decipherment for Out-of-Domain Machine Translation</a></p>
<p>11 0.29232183 <a title="33-lsi-11" href="./emnlp-2012-SSHLDA%3A_A_Semi-Supervised_Hierarchical_Topic_Model.html">115 emnlp-2012-SSHLDA: A Semi-Supervised Hierarchical Topic Model</a></p>
<p>12 0.26950809 <a title="33-lsi-12" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>13 0.24271798 <a title="33-lsi-13" href="./emnlp-2012-Revisiting_the_Predictability_of_Language%3A_Response_Completion_in_Social_Media.html">114 emnlp-2012-Revisiting the Predictability of Language: Response Completion in Social Media</a></p>
<p>14 0.24024661 <a title="33-lsi-14" href="./emnlp-2012-Exact_Sampling_and_Decoding_in_High-Order_Hidden_Markov_Models.html">43 emnlp-2012-Exact Sampling and Decoding in High-Order Hidden Markov Models</a></p>
<p>15 0.23789866 <a title="33-lsi-15" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>16 0.23697983 <a title="33-lsi-16" href="./emnlp-2012-A_Phrase-Discovering_Topic_Model_Using_Hierarchical_Pitman-Yor_Processes.html">8 emnlp-2012-A Phrase-Discovering Topic Model Using Hierarchical Pitman-Yor Processes</a></p>
<p>17 0.22325456 <a title="33-lsi-17" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>18 0.22254543 <a title="33-lsi-18" href="./emnlp-2012-An_Entity-Topic_Model_for_Entity_Linking.html">19 emnlp-2012-An Entity-Topic Model for Entity Linking</a></p>
<p>19 0.21137363 <a title="33-lsi-19" href="./emnlp-2012-Explore_Person_Specific_Evidence_in_Web_Person_Name_Disambiguation.html">47 emnlp-2012-Explore Person Specific Evidence in Web Person Name Disambiguation</a></p>
<p>20 0.20593624 <a title="33-lsi-20" href="./emnlp-2012-Exploiting_Reducibility_in_Unsupervised_Dependency_Parsing.html">46 emnlp-2012-Exploiting Reducibility in Unsupervised Dependency Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.017), (11, 0.012), (16, 0.035), (25, 0.026), (34, 0.077), (45, 0.013), (60, 0.069), (63, 0.073), (64, 0.021), (65, 0.019), (70, 0.014), (73, 0.015), (74, 0.03), (76, 0.048), (78, 0.353), (80, 0.039), (86, 0.022), (95, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76006657 <a title="33-lda-1" href="./emnlp-2012-Discovering_Diverse_and_Salient_Threads_in_Document_Collections.html">33 emnlp-2012-Discovering Diverse and Salient Threads in Document Collections</a></p>
<p>Author: Jennifer Gillenwater ; Alex Kulesza ; Ben Taskar</p><p>Abstract: We propose a novel probabilistic technique for modeling and extracting salient structure from large document collections. As in clustering and topic modeling, our goal is to provide an organizing perspective into otherwise overwhelming amounts of information. We are particularly interested in revealing and exploiting relationships between documents. To this end, we focus on extracting diverse sets of threads—singlylinked, coherent chains of important documents. To illustrate, we extract research threads from citation graphs and construct timelines from news articles. Our method is highly scalable, running on a corpus of over 30 million words in about four minutes, more than 75 times faster than a dynamic topic model. Finally, the results from our model more closely resemble human news summaries according to several metrics and are also preferred by human judges.</p><p>2 0.38265964 <a title="33-lda-2" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>Author: Jianxing Yu ; Zheng-Jun Zha ; Tat-Seng Chua</p><p>Abstract: This paper proposes to generate appropriate answers for opinion questions about products by exploiting the hierarchical organization of consumer reviews. The hierarchy organizes product aspects as nodes following their parent-child relations. For each aspect, the reviews and corresponding opinions on this aspect are stored. We develop a new framework for opinion Questions Answering, which enables accurate question analysis and effective answer generation by making use the hierarchy. In particular, we first identify the (explicit/implicit) product aspects asked in the questions and their sub-aspects by referring to the hierarchy. We then retrieve the corresponding review fragments relevant to the aspects from the hierarchy. In order to gener- ate appropriate answers from the review fragments, we develop a multi-criteria optimization approach for answer generation by simultaneously taking into account review salience, coherence, diversity, and parent-child relations among the aspects. We conduct evaluations on 11 popular products in four domains. The evaluated corpus contains 70,359 consumer reviews and 220 questions on these products. Experimental results demonstrate the effectiveness of our approach.</p><p>3 0.37700668 <a title="33-lda-3" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>Author: Lizhen Qu ; Rainer Gemulla ; Gerhard Weikum</p><p>Abstract: We propose the weakly supervised MultiExperts Model (MEM) for analyzing the semantic orientation of opinions expressed in natural language reviews. In contrast to most prior work, MEM predicts both opinion polarity and opinion strength at the level of individual sentences; such fine-grained analysis helps to understand better why users like or dislike the entity under review. A key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength. For this reason, MEM is weakly supervised: It starts with potentially noisy indicators obtained from coarse-grained training data (i.e., document-level ratings), a small set of diverse base predictors, and, if available, small amounts of fine-grained training data. We integrate these noisy indicators into a unified probabilistic framework using ideas from ensemble learning and graph-based semi-supervised learning. Our experiments indicate that MEM outperforms state-of-the-art methods by a significant margin.</p><p>4 0.37425175 <a title="33-lda-4" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms ofweak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.</p><p>5 0.37373936 <a title="33-lda-5" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>Author: Michael J. Paul</p><p>Abstract: Recent work has explored the use of hidden Markov models for unsupervised discourse and conversation modeling, where each segment or block of text such as a message in a conversation is associated with a hidden state in a sequence. We extend this approach to allow each block of text to be a mixture of multiple classes. Under our model, the probability of a class in a text block is a log-linear function of the classes in the previous block. We show that this model performs well at predictive tasks on two conversation data sets, improving thread reconstruction accuracy by up to 15 percentage points over a standard HMM. Additionally, we show quantitatively that the induced word clusters correspond to speech acts more closely than baseline models.</p><p>6 0.37272936 <a title="33-lda-6" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>7 0.37099022 <a title="33-lda-7" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>8 0.37030718 <a title="33-lda-8" href="./emnlp-2012-A_Discriminative_Model_for_Query_Spelling_Correction_with_Latent_Structural_SVM.html">5 emnlp-2012-A Discriminative Model for Query Spelling Correction with Latent Structural SVM</a></p>
<p>9 0.36934397 <a title="33-lda-9" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>10 0.36871013 <a title="33-lda-10" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>11 0.36839843 <a title="33-lda-11" href="./emnlp-2012-Besting_the_Quiz_Master%3A_Crowdsourcing_Incremental_Classification_Games.html">23 emnlp-2012-Besting the Quiz Master: Crowdsourcing Incremental Classification Games</a></p>
<p>12 0.36815363 <a title="33-lda-12" href="./emnlp-2012-A_Phrase-Discovering_Topic_Model_Using_Hierarchical_Pitman-Yor_Processes.html">8 emnlp-2012-A Phrase-Discovering Topic Model Using Hierarchical Pitman-Yor Processes</a></p>
<p>13 0.36591056 <a title="33-lda-13" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>14 0.36541623 <a title="33-lda-14" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<p>15 0.36522424 <a title="33-lda-15" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>16 0.36493945 <a title="33-lda-16" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>17 0.36442307 <a title="33-lda-17" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>18 0.36349031 <a title="33-lda-18" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>19 0.36344397 <a title="33-lda-19" href="./emnlp-2012-Type-Supervised_Hidden_Markov_Models_for_Part-of-Speech_Tagging_with_Incomplete_Tag_Dictionaries.html">129 emnlp-2012-Type-Supervised Hidden Markov Models for Part-of-Speech Tagging with Incomplete Tag Dictionaries</a></p>
<p>20 0.36310732 <a title="33-lda-20" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
