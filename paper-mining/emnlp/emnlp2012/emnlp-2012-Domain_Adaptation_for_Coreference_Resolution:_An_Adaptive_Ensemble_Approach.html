<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>36 emnlp-2012-Domain Adaptation for Coreference Resolution: An Adaptive Ensemble Approach</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-36" href="#">emnlp2012-36</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>36 emnlp-2012-Domain Adaptation for Coreference Resolution: An Adaptive Ensemble Approach</h1>
<br/><p>Source: <a title="emnlp-2012-36-pdf" href="http://aclweb.org/anthology//D/D12/D12-1068.pdf">pdf</a></p><p>Author: Jian Bo Yang ; Qi Mao ; Qiao Liang Xiang ; Ivor Wai-Hung Tsang ; Kian Ming Adam Chai ; Hai Leong Chieu</p><p>Abstract: We propose an adaptive ensemble method to adapt coreference resolution across domains. This method has three features: (1) it can optimize for any user-specified objective measure; (2) it can make document-specific prediction rather than rely on a fixed base model or a fixed set of base models; (3) it can automatically adjust the active ensemble members during prediction. With simplification, this method can be used in the traditional withindomain case, while still retaining the above features. To the best of our knowledge, this work is the first to both (i) develop a domain adaptation algorithm for the coreference resolution problem and (ii) have the above features as an ensemble method. Empirically, we show the benefits of (i) on the six domains of the ACE 2005 data set in domain adaptation set- ting, and of (ii) on both the MUC-6 and the ACE 2005 data sets in within-domain setting.</p><p>Reference: <a title="emnlp-2012-36-reference" href="../emnlp2012_reference/emnlp-2012-Domain_Adaptation_for_Coreference_Resolution%3A_An_Adaptive_Ensemble_Approach_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 sg  duke  Abstract We propose an adaptive ensemble method to adapt coreference resolution across domains. [sent-9, score-1.137]
</p><p>2 This method has three features: (1) it can optimize for any user-specified objective measure; (2) it can make document-specific prediction rather than rely on a fixed base model or a fixed set of base models; (3) it can automatically adjust the active ensemble members during prediction. [sent-10, score-0.967]
</p><p>3 To the best of our knowledge, this work is the first to both (i) develop a domain adaptation algorithm for the coreference resolution problem and (ii) have the above features as an ensemble method. [sent-12, score-1.291]
</p><p>4 Empirically, we show the benefits of (i) on the six domains of the ACE 2005 data set in domain adaptation set-  ting, and of (ii) on both the MUC-6 and the ACE 2005 data sets in within-domain setting. [sent-13, score-0.389]
</p><p>5 In the past decade, several coreference resolution systems have been proposed, e. [sent-17, score-0.357]
</p><p>6 All of these focus on the within-domain case to use the labeled documents from a domain to predict on the unlabeled —  ∗The work is done during postdoc in NTU, Singapore. [sent-21, score-0.34]
</p><p>7 This is called domain adaptation, and, in this context, the former domains is called the source domains, while the latter domain is called the target domain (Blitzer et al. [sent-25, score-0.668]
</p><p>8 Based on the type of the knowledge to be transferred to the target domain, domain adaptation learning can be categorized as instance-based method, feature-based method, parameter-based method or relational-knowledge-based method (Pan and Yang, 2010). [sent-27, score-0.42]
</p><p>9 Previously, domain adaptation learning has been successfully used in other NLP tasks such as relation extraction (Jiang, 2009) and POS tagging (Jiang and Zhai, 2007), semantic detection (Tan et al. [sent-28, score-0.355]
</p><p>10 In this paper, we propose an adaptive ensemble method to adapt coreference resolution across domains. [sent-32, score-1.083]
</p><p>11 This proposed method can be categorized as both feature-based and parameter-based domain adaptation learning methods. [sent-33, score-0.374]
</p><p>12 It has three main steps: ensemble creation, cross-domain knowledge learn-  ing and decision inference. [sent-34, score-0.679]
</p><p>13 The first step creates the ensemble by collecting a set of base models, which can be any individual methods with various features/instances/parameters settings. [sent-35, score-0.684]
</p><p>14 lc L2a0n1g2ua Agseso Pcrioactieosnsi fnogr a Cnodm Cpoumtaptiuotna tilo Lnianlg Nuaist uircasl ous domains and learns the cross-domain knowledge between each target domain and the source domain. [sent-38, score-0.363]
</p><p>15 The third step infers the final decision in the target domain based on all ensemble results. [sent-39, score-0.877]
</p><p>16 In addition to domain adaptation, the proposed adaptive ensemble method has the following features that are absent in the other ensemble methods. [sent-40, score-1.539]
</p><p>17 Third, it can automatically adjust the active ensemble members in decision inference so that underperforming base models are filtered out. [sent-43, score-0.862]
</p><p>18 We conduct experiments for coreference resolution under both the within-domain setting and the domain-adaptation setting. [sent-45, score-0.391]
</p><p>19 In the within-domain setting, we compare the proposed adaptive ensemble method with the mention-pair methods and other ensemble methods on the MUC-6 and ACE 2005 corpora. [sent-46, score-1.372]
</p><p>20 The results show that the proposed adaptive ensemble method consistently outperforms these baselines. [sent-47, score-0.769]
</p><p>21 In the domain adaptation setting, we use the ACE 2005 corpora to create six domain adaptation tasks to evaluate the effectiveness of our domain adaptation learning. [sent-48, score-1.017]
</p><p>22 Section 2 reviews some existing ensemble methods for coreference resolution. [sent-51, score-0.838]
</p><p>23 Section 3 presents the proposed adaptive ensemble method for domain adaptation problems. [sent-52, score-1.1]
</p><p>24 Section 5 presents the experiments under both the within-domain and the domain adaptation settings. [sent-54, score-0.331]
</p><p>25 2  Existing Ensemble Methods  Many ensemble methods have been proposed in the machine learning literature, e. [sent-56, score-0.646]
</p><p>26 Some of them have been success745 fully used in coreference resolution (Pang and Fan, 2009; Munson et al. [sent-59, score-0.357]
</p><p>27 All these methods comprise of two steps: ensemble creation and decision inference. [sent-62, score-0.707]
</p><p>28 Recently, Rahman and Ng (201 1a) further enriched the ensemble by considering various feature sets and learning models. [sent-65, score-0.603]
</p><p>29 Although their approaches achieved promising results in their end-to-end systems, these do not consider the user-specific performance measure during the ensemble learning. [sent-69, score-0.626]
</p><p>30 Another branch of ensemble methods uses model selection (Munson et al. [sent-70, score-0.603]
</p><p>31 3  Adaptive Ensemble Method  In this section, we give our adaptive ensemble method for domain adaptation for coreference resolution. [sent-79, score-1.292]
</p><p>32 We consider the typical domain adaptation prob-  lem, which has one target domain t and p (p ≥ 1) source dicohma hianss s1, . [sent-91, score-0.607]
</p><p>33 Taihne target (dpo m≥a 1in) contains labeled documents and M unlabeled documents, while source domains contain N(s1) , . [sent-95, score-0.34]
</p><p>34 1 Ensemble Creation Mention-pair methods have been widely-used for coreference resolution due to their efficiency and effectiveness, and they have often been taken as base models in ensemble learning (Rahman and Ng, 2011a; Munson et al. [sent-102, score-1.041]
</p><p>35 , ni) paired with its closest antecedent mention mia (a < b), while the negative mention pairs are the mention mib paired with each of the intervening mentions . [sent-115, score-0.4]
</p><p>36 Ensemble For domain v, we create a domainspecified ensemble = {f1, . [sent-145, score-0.77]
</p><p>37 If multiple domains are provided, we gather all the domain-specific ensembles into a grand ensemble F = F(s1) ∪ ···F(sp) ∪ F(t). [sent-149, score-0.78]
</p><p>38 Therefore, effective domain adaptation requires using some knowledge of crossdomain similarity. [sent-152, score-0.387]
</p><p>39 where W is a diagonal matrix with diagonal entries Matrix W is diagonal to reduce computation cost and to increase statistical confidence in estimation  µµ µ  when there is limited target labeled data (as is typically the case in domain adaptation). [sent-158, score-0.423]
</p><p>40 Then, for each source domain su and document in the target domain, we find the su)  Dj(t)  set I(Dj(t);  obefs t h pe r dfo crumminengt sba isne d moomdaeiln a s u thtahta ftor ha Dvj(et) t:he same  µ µ  I(Dj(t); su) = {D(isu) | fi(su)∗  fj(t)∗, i= 1, . [sent-164, score-0.466]
</p><p>41 =  (7)  The key idea in I(Dj(t);  su) is to select documents in a source ddeoam inain I su that are similar to document in the sense that they have the same best performing base model under a specific Λ. [sent-168, score-0.393]
</p><p>42 Dj(t)  Optimization We determine the vector by minimizing the parametric distance (4) between all target labeled documents and their corresponding source labeled document identified in the previous step. [sent-170, score-0.387]
</p><p>43 (8)  The solution to this linear programming problem can be regarded as the cross-domain knowledge be-  tween source domain su and the target domain t. [sent-172, score-0.579]
</p><p>44 3 Decision Inference After ensemble creation and cross-domain knowledge learning, we need to provide the coreference result on an unseen document in the target domain based on the results of all the members in F. [sent-180, score-1.303]
</p><p>45 Given the grand ensemble F and all labeled documents, th thee et gasrkan nisd to predict on tahned target uenleladb deolecd= 1, . [sent-183, score-0.843]
</p><p>46 The idea of the proposed menetth Dod is to first find the k most similar docu-  document Dj(t),j  µ  ments N(Dj(t)) from all labeled documents for document Dj(t). [sent-187, score-0.343]
</p><p>47 Secondly, based on the computed distance values, we select k nearest neighbor documents for the target unlabeled document  Dj(t)  from all labeled doc-  D(iv),∀v,i. [sent-192, score-0.403]
</p><p>48 We have not known of other (ensemble) coreference resolution methods that optimize for these measures. [sent-196, score-0.384]
</p><p>49 4 Discussion The above proposed adaptive ensemble approach incorporates the domain adaptation knowledge during 748 (a) the identification of similar documents between different domains and (b) the determination of active ensemble members. [sent-199, score-1.925]
</p><p>50 Beside these, it has the following features over other (ensemble) coreference methods: (i) It can optimize any user-specified objective measure via (6) and (9). [sent-200, score-0.329]
</p><p>51 (iii) The prediction on the testing docu-  ment D(jt) is not based on all members in F but only on the active ensemble members N(Dj(t)). [sent-203, score-0.803]
</p><p>52 This can foinlte trh eou act some potentially ubnesrsuit Nab(lDe base models Moreover, the active ensemble  for document D(jt). [sent-204, score-0.829]
</p><p>53 For computational cost, the majority is by ensemble creation, since a large number of base models are usually used. [sent-206, score-0.684]
</p><p>54 4  Special Case: Within-domain Setting  The adaptive ensemble method presented in Section 3 is for the domain adaptation setting. [sent-210, score-1.057]
</p><p>55 In the within-domain setting, the adaptive ensemble method only has ensem-  ble creation and decision inference steps. [sent-212, score-0.83]
</p><p>56 In the ensemble creation step, we still use the closest-first and best-first mention-pair methods with various parameters to create the ensemble. [sent-213, score-0.66]
</p><p>57 Unlike the domain adaptation setting, here we can only use the labeled documents in the target domain to create the ensemble . [sent-214, score-1.295]
</p><p>58 Therefore, the size of ensceremabtele t hhee reen sise m rebdluec Fed by p times compared to the domain adaptation setting. [sent-215, score-0.331]
</p><p>59 Based on these disEuclidean distance  tance values, we similarly select k nearest neighbor for document and then de-  documents N(D(jt))  fj(t)∗  Dj(t),  termine the final method for document (9) but with F replaced by F(t) . [sent-223, score-0.326]
</p><p>60 5  Dj(t) by  Experiments  We test the proposed adaptive method and several baselines under both the within-domain and the domain adaptation settings on the MUC-6 and ACE 2005 corpora. [sent-224, score-0.573]
</p><p>61 In all our experiments, we use two popular performance measures, B-CUBED Fmeasure (Bagga and Baldwin, 1998) and CEAF Fmeasure (Luo, 2005) 1, to evaluate the coreference resolution result. [sent-227, score-0.357]
</p><p>62 Since the focus of the paper is to investigate the effectiveness of coreference resolution methods, we use the gold standard mentions in all experiments. [sent-228, score-0.388]
</p><p>63 For the proposed method, the ensemble in every d thoem parino v ehdas m 2e0t8h dm,em thbee erns totally. [sent-229, score-0.646]
</p><p>64 First, we investigate whether the proposed ensemble method is better than the tuned mentionpair methods and other ensemble methods. [sent-258, score-1.249]
</p><p>65 Second, we investigate the optimal number of active ensemble members. [sent-259, score-0.665]
</p><p>66 For the proposed ensemble method, we experi-  mented with nearest neighbor set of sizes k = 1, 3, 5 paired with objective function Λ in (9) set to Rand Index, CEAF or B-CUBED. [sent-261, score-0.777]
</p><p>67 6  These two baselines use the same ensemble as the proposed method for fair comparison. [sent-375, score-0.696]
</p><p>68 In decision inference, these two baselines use the mention-based voting and cluster-based voting respectively, as proposed in (Rahman and Ng, 2011a). [sent-376, score-0.388]
</p><p>69 In these two baselines, all members in the ensemble participate the voting process. [sent-377, score-0.796]
</p><p>70 These two ensemble baselines are named as Em and Ec for short. [sent-378, score-0.653]
</p><p>71 From the results, we observe that the proposed ensemble method with objective function matching the evaluation measure and with k = 1 generally performs best among all methods and all tasks. [sent-382, score-0.713]
</p><p>72 Surprisingly, the common ensemble method with mentionbased voting Em and cluster-based voting Ec strategies do not perform well. [sent-383, score-0.851]
</p><p>73 The plausible reason is the current ensemble may incorporate some bad base models due to inappropriate C and t values, which would undermine the voting result. [sent-384, score-0.808]
</p><p>74 Nevertheless, it is difficult to judge the quality of the ensemble members in advance. [sent-385, score-0.672]
</p><p>75 This is reasonable, as different base mod750 els in the ensemble would be good at predicting  the different documents. [sent-388, score-0.684]
</p><p>76 For the proposed ensemble method with various configurations, we observe using an objective function that matches the evaluation measures is generally better. [sent-389, score-0.69]
</p><p>77 We also observe that the ensemble method with k = 1 is generally better than that with the larger k, except the BN and UN tasks in B-CUBED F-measure. [sent-391, score-0.627]
</p><p>78 This suggests that the fewer the active ensemble members the better the generalization performance. [sent-392, score-0.734]
</p><p>79 In contrast, the two baseline ensemble methods that use voting are significantly worse than the best baseline. [sent-394, score-0.727]
</p><p>80 2  Domain-adaptation Setting  We employ ACE 2005 corpora to simulate the domain adaptation settings in experiments. [sent-398, score-0.357]
</p><p>81 Specifically, we create six domain adaptation tasks, BC, BN, CTS, NW, UN, WL in total. [sent-399, score-0.331]
</p><p>82 For example, in the task UN, the target domain is UN while the other five source domains are BC, BN, CTS, NW and WL. [sent-401, score-0.334]
</p><p>83 The number of labeled documents in each domain is as the same as in Table 1, except when that domain is the target domain, in which case we use only five labeled documents. [sent-402, score-0.589]
</p><p>84 For the proposed ensemble method, we heuristically determine the parameter B in to be the number of non-zero elements in Γ, where  Γ =∑N(t)  ∑  ∑j=1 Di(su)∈∑I(Dj(t);su)  ∆(Di(su),Dj(t)). [sent-510, score-0.646]
</p><p>85 •  Three proposed adaptive ensemble methods Twhitrheoeut p ocrpoossse-ddo amdaaipnt knowledge learning. [sent-515, score-0.798]
</p><p>86 These three baselines uses neighborhood sizes k = 1, 3, 5 with the grand ensemble F rather tkha =n t 1h,e3 target dho tmheai gnr aenndse emnbselme . [sent-516, score-0.832]
</p><p>87 bFle(t) F  Tables 4 and 5 show the experimental results in the domain adaptation settings using B-CUBED and 751 CEAF as the final performance measures respectively. [sent-518, score-0.357]
</p><p>88 Among them, the best proposed domain adaptation method on average outperforms the best of Sc, Sb by 7. [sent-520, score-0.374]
</p><p>89 6  Conclusions and Future Work  In this paper, we proposed an adaptive ensemble method for coreference resolution under both within-domain and domain adaptation settings. [sent-530, score-1.457]
</p><p>90 The key advantage of the proposed method is incor-  Table 4: B-CUBED F-measure results by all methods under domain adaptation setting on ACE 2005 corpora, with Λ set to B-CUBED. [sent-531, score-0.408]
</p><p>91 The within-domain and grand ensemble methods are the baselines. [sent-532, score-0.722]
</p><p>92 Within-domain  Grand ensemble  Domain-adaptation  Sc  Sb  k=1  3  5  k=1  3  5  BC BN CTS NW UN WL  58. [sent-533, score-0.603]
</p><p>93 7  Table 5: CEAF F-measure results by all methods under domain adaptation setting on ACE 2005 corpora, with Λ set to CEAF. [sent-589, score-0.365]
</p><p>94 The within-domain and grand ensemble methods are the baselines. [sent-590, score-0.722]
</p><p>95 Within-domain  Grand ensemble  Domain-adaptation  Sc  Sb  k=1  3  5  k=1  3  5  BC BN CTS NW UN WL  55. [sent-591, score-0.603]
</p><p>96 that the proposed  adaptive  We also ensemble  method can be readily applied to conventional coreference tasks without cross-domain knowledge learning. [sent-649, score-1.079]
</p><p>97 Compared with existing ensemble methods, the proposed method is simultaneously  endowed with  the following three distinctive features:  optimizing  any user-specified performance measure, making the document-specific  prediction and automatically  justing the active ensemble members. [sent-650, score-1.311]
</p><p>98 ad-  In the exper-  iments under both within-domain settings and domain adaptation settings, the results evidence the effectiveness of the proposed cross-domain knowledge learning method, and also demonstrate the superiority of the proposed adaptive ensemble method over other baselines. [sent-651, score-1.225]
</p><p>99 Joint determination of anaphoricity and coreference resolution using integer programming. [sent-686, score-0.357]
</p><p>100 A machine learning approach to coreference resolution of noun phrases. [sent-771, score-0.357]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ensemble', 0.603), ('dj', 0.304), ('coreference', 0.235), ('domain', 0.167), ('adaptation', 0.164), ('ceaf', 0.159), ('isu', 0.136), ('cts', 0.132), ('voting', 0.124), ('adaptive', 0.123), ('resolution', 0.122), ('bn', 0.12), ('rahman', 0.12), ('grand', 0.119), ('ace', 0.118), ('su', 0.107), ('bc', 0.098), ('un', 0.093), ('sb', 0.092), ('mention', 0.089), ('document', 0.083), ('jt', 0.083), ('base', 0.081), ('rand', 0.08), ('munson', 0.08), ('wl', 0.076), ('nw', 0.076), ('ng', 0.074), ('documents', 0.073), ('di', 0.071), ('members', 0.069), ('withindomain', 0.068), ('fj', 0.062), ('active', 0.062), ('labeled', 0.061), ('target', 0.06), ('domains', 0.058), ('creation', 0.057), ('cardie', 0.057), ('sc', 0.057), ('mia', 0.051), ('mib', 0.051), ('soon', 0.051), ('baselines', 0.05), ('source', 0.049), ('decision', 0.047), ('diagonal', 0.045), ('neighbor', 0.045), ('objective', 0.044), ('dso', 0.044), ('stoyanov', 0.043), ('proposed', 0.043), ('nearest', 0.042), ('centroids', 0.04), ('jiang', 0.04), ('unlabeled', 0.039), ('vincent', 0.037), ('claire', 0.035), ('bagging', 0.034), ('ntu', 0.034), ('vemulapalli', 0.034), ('setting', 0.034), ('ec', 0.034), ('luo', 0.033), ('mentions', 0.031), ('fan', 0.03), ('validation', 0.03), ('breiman', 0.029), ('duke', 0.029), ('proc', 0.029), ('knowledge', 0.029), ('guo', 0.028), ('zhai', 0.028), ('bagga', 0.028), ('boosting', 0.027), ('optimize', 0.027), ('crossdomain', 0.027), ('superiority', 0.027), ('chai', 0.027), ('fmeasure', 0.027), ('leo', 0.027), ('lth', 0.027), ('settings', 0.026), ('sg', 0.025), ('freund', 0.025), ('broadcast', 0.025), ('finley', 0.025), ('vilain', 0.025), ('tasks', 0.024), ('seven', 0.024), ('yang', 0.024), ('pang', 0.023), ('nto', 0.023), ('pan', 0.023), ('altaf', 0.023), ('muc', 0.023), ('clustering', 0.023), ('measure', 0.023), ('conventional', 0.022), ('singapore', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="36-tfidf-1" href="./emnlp-2012-Domain_Adaptation_for_Coreference_Resolution%3A_An_Adaptive_Ensemble_Approach.html">36 emnlp-2012-Domain Adaptation for Coreference Resolution: An Adaptive Ensemble Approach</a></p>
<p>Author: Jian Bo Yang ; Qi Mao ; Qiao Liang Xiang ; Ivor Wai-Hung Tsang ; Kian Ming Adam Chai ; Hai Leong Chieu</p><p>Abstract: We propose an adaptive ensemble method to adapt coreference resolution across domains. This method has three features: (1) it can optimize for any user-specified objective measure; (2) it can make document-specific prediction rather than rely on a fixed base model or a fixed set of base models; (3) it can automatically adjust the active ensemble members during prediction. With simplification, this method can be used in the traditional withindomain case, while still retaining the above features. To the best of our knowledge, this work is the first to both (i) develop a domain adaptation algorithm for the coreference resolution problem and (ii) have the above features as an ensemble method. Empirically, we show the benefits of (i) on the six domains of the ACE 2005 data set in domain adaptation set- ting, and of (ii) on both the MUC-6 and the ACE 2005 data sets in within-domain setting.</p><p>2 0.20800823 <a title="36-tfidf-2" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>Author: Heeyoung Lee ; Marta Recasens ; Angel Chang ; Mihai Surdeanu ; Dan Jurafsky</p><p>Abstract: We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable documents, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.</p><p>3 0.17622299 <a title="36-tfidf-3" href="./emnlp-2012-Multi-Domain_Learning%3A_When_Do_Domains_Matter%3F.html">92 emnlp-2012-Multi-Domain Learning: When Do Domains Matter?</a></p>
<p>Author: Mahesh Joshi ; Mark Dredze ; William W. Cohen ; Carolyn Rose</p><p>Abstract: We present a systematic analysis of existing multi-domain learning approaches with respect to two questions. First, many multidomain learning algorithms resemble ensemble learning algorithms. (1) Are multi-domain learning improvements the result of ensemble learning effects? Second, these algorithms are traditionally evaluated in a balanced class label setting, although in practice many multidomain settings have domain-specific class label biases. When multi-domain learning is applied to these settings, (2) are multidomain methods improving because they capture domain-specific class biases? An understanding of these two issues presents a clearer idea about where the field has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art.</p><p>4 0.16881391 <a title="36-tfidf-4" href="./emnlp-2012-Joint_Learning_for_Coreference_Resolution_with_Markov_Logic.html">73 emnlp-2012-Joint Learning for Coreference Resolution with Markov Logic</a></p>
<p>Author: Yang Song ; Jing Jiang ; Wayne Xin Zhao ; Sujian Li ; Houfeng Wang</p><p>Abstract: Pairwise coreference resolution models must merge pairwise coreference decisions to generate final outputs. Traditional merging methods adopt different strategies such as the bestfirst method and enforcing the transitivity constraint, but most of these methods are used independently of the pairwise learning methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-201 1shared task on the same dataset. Compared with the best system from CoNLL2011, which employs a rule-based method, our system shows competitive performance.</p><p>5 0.12774698 <a title="36-tfidf-5" href="./emnlp-2012-Learning-based_Multi-Sieve_Co-reference_Resolution_with_Knowledge.html">76 emnlp-2012-Learning-based Multi-Sieve Co-reference Resolution with Knowledge</a></p>
<p>Author: Lev Ratinov ; Dan Roth</p><p>Abstract: We explore the interplay of knowledge and structure in co-reference resolution. To inject knowledge, we use a state-of-the-art system which cross-links (or “grounds”) expressions in free text to Wikipedia. We explore ways of using the resulting grounding to boost the performance of a state-of-the-art co-reference resolution system. To maximize the utility of the injected knowledge, we deploy a learningbased multi-sieve approach and develop novel entity-based features. Our end system outperforms the state-of-the-art baseline by 2 B3 F1 points on non-transcript portion of the ACE 2004 dataset.</p><p>6 0.10490745 <a title="36-tfidf-6" href="./emnlp-2012-Do_Neighbours_Help%3F_An_Exploration_of_Graph-based_Algorithms_for_Cross-domain_Sentiment_Classification.html">34 emnlp-2012-Do Neighbours Help? An Exploration of Graph-based Algorithms for Cross-domain Sentiment Classification</a></p>
<p>7 0.099575117 <a title="36-tfidf-7" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>8 0.098824978 <a title="36-tfidf-8" href="./emnlp-2012-Resolving_Complex_Cases_of_Definite_Pronouns%3A_The_Winograd_Schema_Challenge.html">112 emnlp-2012-Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge</a></p>
<p>9 0.089226298 <a title="36-tfidf-9" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>10 0.082303219 <a title="36-tfidf-10" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>11 0.069926001 <a title="36-tfidf-11" href="./emnlp-2012-Joining_Forces_Pays_Off%3A_Multilingual_Joint_Word_Sense_Disambiguation.html">69 emnlp-2012-Joining Forces Pays Off: Multilingual Joint Word Sense Disambiguation</a></p>
<p>12 0.067535624 <a title="36-tfidf-12" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>13 0.066080876 <a title="36-tfidf-13" href="./emnlp-2012-A_New_Minimally-Supervised_Framework_for_Domain_Word_Sense_Disambiguation.html">6 emnlp-2012-A New Minimally-Supervised Framework for Domain Word Sense Disambiguation</a></p>
<p>14 0.065277658 <a title="36-tfidf-14" href="./emnlp-2012-Monte_Carlo_MCMC%3A_Efficient_Inference_by_Approximate_Sampling.html">91 emnlp-2012-Monte Carlo MCMC: Efficient Inference by Approximate Sampling</a></p>
<p>15 0.060352221 <a title="36-tfidf-15" href="./emnlp-2012-Iterative_Annotation_Transformation_with_Predict-Self_Reestimation_for_Chinese_Word_Segmentation.html">68 emnlp-2012-Iterative Annotation Transformation with Predict-Self Reestimation for Chinese Word Segmentation</a></p>
<p>16 0.053422507 <a title="36-tfidf-16" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>17 0.050646883 <a title="36-tfidf-17" href="./emnlp-2012-Using_Discourse_Information_for_Paraphrase_Extraction.html">135 emnlp-2012-Using Discourse Information for Paraphrase Extraction</a></p>
<p>18 0.050498515 <a title="36-tfidf-18" href="./emnlp-2012-Resolving_This-issue_Anaphora.html">113 emnlp-2012-Resolving This-issue Anaphora</a></p>
<p>19 0.050264783 <a title="36-tfidf-19" href="./emnlp-2012-Supervised_Text-based_Geolocation_Using_Language_Models_on_an_Adaptive_Grid.html">121 emnlp-2012-Supervised Text-based Geolocation Using Language Models on an Adaptive Grid</a></p>
<p>20 0.049735777 <a title="36-tfidf-20" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.191), (1, 0.188), (2, -0.011), (3, -0.139), (4, 0.022), (5, -0.063), (6, -0.12), (7, -0.123), (8, 0.261), (9, -0.137), (10, 0.044), (11, 0.128), (12, 0.05), (13, 0.103), (14, 0.155), (15, 0.106), (16, -0.076), (17, 0.015), (18, -0.023), (19, -0.03), (20, 0.085), (21, -0.111), (22, 0.128), (23, -0.137), (24, -0.073), (25, -0.14), (26, -0.043), (27, -0.025), (28, -0.03), (29, 0.141), (30, -0.12), (31, -0.036), (32, 0.054), (33, -0.057), (34, -0.06), (35, -0.14), (36, -0.004), (37, 0.046), (38, -0.014), (39, 0.05), (40, 0.025), (41, -0.018), (42, 0.092), (43, -0.077), (44, -0.039), (45, 0.054), (46, -0.074), (47, -0.021), (48, 0.006), (49, 0.088)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97416639 <a title="36-lsi-1" href="./emnlp-2012-Domain_Adaptation_for_Coreference_Resolution%3A_An_Adaptive_Ensemble_Approach.html">36 emnlp-2012-Domain Adaptation for Coreference Resolution: An Adaptive Ensemble Approach</a></p>
<p>Author: Jian Bo Yang ; Qi Mao ; Qiao Liang Xiang ; Ivor Wai-Hung Tsang ; Kian Ming Adam Chai ; Hai Leong Chieu</p><p>Abstract: We propose an adaptive ensemble method to adapt coreference resolution across domains. This method has three features: (1) it can optimize for any user-specified objective measure; (2) it can make document-specific prediction rather than rely on a fixed base model or a fixed set of base models; (3) it can automatically adjust the active ensemble members during prediction. With simplification, this method can be used in the traditional withindomain case, while still retaining the above features. To the best of our knowledge, this work is the first to both (i) develop a domain adaptation algorithm for the coreference resolution problem and (ii) have the above features as an ensemble method. Empirically, we show the benefits of (i) on the six domains of the ACE 2005 data set in domain adaptation set- ting, and of (ii) on both the MUC-6 and the ACE 2005 data sets in within-domain setting.</p><p>2 0.71698928 <a title="36-lsi-2" href="./emnlp-2012-Multi-Domain_Learning%3A_When_Do_Domains_Matter%3F.html">92 emnlp-2012-Multi-Domain Learning: When Do Domains Matter?</a></p>
<p>Author: Mahesh Joshi ; Mark Dredze ; William W. Cohen ; Carolyn Rose</p><p>Abstract: We present a systematic analysis of existing multi-domain learning approaches with respect to two questions. First, many multidomain learning algorithms resemble ensemble learning algorithms. (1) Are multi-domain learning improvements the result of ensemble learning effects? Second, these algorithms are traditionally evaluated in a balanced class label setting, although in practice many multidomain settings have domain-specific class label biases. When multi-domain learning is applied to these settings, (2) are multidomain methods improving because they capture domain-specific class biases? An understanding of these two issues presents a clearer idea about where the field has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art.</p><p>3 0.62775874 <a title="36-lsi-3" href="./emnlp-2012-Joint_Learning_for_Coreference_Resolution_with_Markov_Logic.html">73 emnlp-2012-Joint Learning for Coreference Resolution with Markov Logic</a></p>
<p>Author: Yang Song ; Jing Jiang ; Wayne Xin Zhao ; Sujian Li ; Houfeng Wang</p><p>Abstract: Pairwise coreference resolution models must merge pairwise coreference decisions to generate final outputs. Traditional merging methods adopt different strategies such as the bestfirst method and enforcing the transitivity constraint, but most of these methods are used independently of the pairwise learning methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-201 1shared task on the same dataset. Compared with the best system from CoNLL2011, which employs a rule-based method, our system shows competitive performance.</p><p>4 0.53755724 <a title="36-lsi-4" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>Author: Heeyoung Lee ; Marta Recasens ; Angel Chang ; Mihai Surdeanu ; Dan Jurafsky</p><p>Abstract: We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable documents, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.</p><p>5 0.50119019 <a title="36-lsi-5" href="./emnlp-2012-Do_Neighbours_Help%3F_An_Exploration_of_Graph-based_Algorithms_for_Cross-domain_Sentiment_Classification.html">34 emnlp-2012-Do Neighbours Help? An Exploration of Graph-based Algorithms for Cross-domain Sentiment Classification</a></p>
<p>Author: Natalia Ponomareva ; Mike Thelwall</p><p>Abstract: This paper presents a comparative study of graph-based approaches for cross-domain sentiment classification. In particular, the paper analyses two existing methods: an optimisation problem and a ranking algorithm. We compare these graph-based methods with each other and with the other state-ofthe-art approaches and conclude that graph domain representations offer a competitive solution to the domain adaptation problem. Analysis of the best parameters for graphbased algorithms reveals that there are no optimal values valid for all domain pairs and that these values are dependent on the characteristics of corresponding domains.</p><p>6 0.49222967 <a title="36-lsi-6" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>7 0.42249453 <a title="36-lsi-7" href="./emnlp-2012-Learning-based_Multi-Sieve_Co-reference_Resolution_with_Knowledge.html">76 emnlp-2012-Learning-based Multi-Sieve Co-reference Resolution with Knowledge</a></p>
<p>8 0.36801159 <a title="36-lsi-8" href="./emnlp-2012-A_New_Minimally-Supervised_Framework_for_Domain_Word_Sense_Disambiguation.html">6 emnlp-2012-A New Minimally-Supervised Framework for Domain Word Sense Disambiguation</a></p>
<p>9 0.34669122 <a title="36-lsi-9" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>10 0.30518666 <a title="36-lsi-10" href="./emnlp-2012-Supervised_Text-based_Geolocation_Using_Language_Models_on_an_Adaptive_Grid.html">121 emnlp-2012-Supervised Text-based Geolocation Using Language Models on an Adaptive Grid</a></p>
<p>11 0.2887291 <a title="36-lsi-11" href="./emnlp-2012-Resolving_Complex_Cases_of_Definite_Pronouns%3A_The_Winograd_Schema_Challenge.html">112 emnlp-2012-Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge</a></p>
<p>12 0.27126667 <a title="36-lsi-12" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>13 0.26500216 <a title="36-lsi-13" href="./emnlp-2012-Locally_Training_the_Log-Linear_Model_for_SMT.html">86 emnlp-2012-Locally Training the Log-Linear Model for SMT</a></p>
<p>14 0.26250541 <a title="36-lsi-14" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>15 0.24284585 <a title="36-lsi-15" href="./emnlp-2012-Iterative_Annotation_Transformation_with_Predict-Self_Reestimation_for_Chinese_Word_Segmentation.html">68 emnlp-2012-Iterative Annotation Transformation with Predict-Self Reestimation for Chinese Word Segmentation</a></p>
<p>16 0.22392438 <a title="36-lsi-16" href="./emnlp-2012-Monte_Carlo_MCMC%3A_Efficient_Inference_by_Approximate_Sampling.html">91 emnlp-2012-Monte Carlo MCMC: Efficient Inference by Approximate Sampling</a></p>
<p>17 0.22359811 <a title="36-lsi-17" href="./emnlp-2012-Local_and_Global_Context_for_Supervised_and_Unsupervised_Metonymy_Resolution.html">85 emnlp-2012-Local and Global Context for Supervised and Unsupervised Metonymy Resolution</a></p>
<p>18 0.21826886 <a title="36-lsi-18" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>19 0.20266618 <a title="36-lsi-19" href="./emnlp-2012-A_Sequence_Labelling_Approach_to_Quote_Attribution.html">9 emnlp-2012-A Sequence Labelling Approach to Quote Attribution</a></p>
<p>20 0.19380514 <a title="36-lsi-20" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.029), (25, 0.014), (34, 0.063), (60, 0.109), (63, 0.039), (64, 0.036), (65, 0.039), (70, 0.014), (73, 0.017), (74, 0.029), (76, 0.031), (80, 0.024), (86, 0.462), (95, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9143129 <a title="36-lda-1" href="./emnlp-2012-Optimising_Incremental_Dialogue_Decisions_Using_Information_Density_for_Interactive_Systems.html">102 emnlp-2012-Optimising Incremental Dialogue Decisions Using Information Density for Interactive Systems</a></p>
<p>Author: Nina Dethlefs ; Helen Hastie ; Verena Rieser ; Oliver Lemon</p><p>Abstract: Incremental processing allows system designers to address several discourse phenomena that have previously been somewhat neglected in interactive systems, such as backchannels or barge-ins, but that can enhance the responsiveness and naturalness of systems. Unfortunately, prior work has focused largely on deterministic incremental decision making, rendering system behaviour less flexible and adaptive than is desirable. We present a novel approach to incremental decision making that is based on Hierarchical Reinforcement Learning to achieve an interactive optimisation of Information Presentation (IP) strategies, allowing the system to generate and comprehend backchannels and barge-ins, by employing the recent psycholinguistic hypothesis of information density (ID) (Jaeger, 2010). Results in terms of average rewards and a human rating study show that our learnt strategy outperforms several baselines that are | v not sensitive to ID by more than 23%.</p><p>same-paper 2 0.87024307 <a title="36-lda-2" href="./emnlp-2012-Domain_Adaptation_for_Coreference_Resolution%3A_An_Adaptive_Ensemble_Approach.html">36 emnlp-2012-Domain Adaptation for Coreference Resolution: An Adaptive Ensemble Approach</a></p>
<p>Author: Jian Bo Yang ; Qi Mao ; Qiao Liang Xiang ; Ivor Wai-Hung Tsang ; Kian Ming Adam Chai ; Hai Leong Chieu</p><p>Abstract: We propose an adaptive ensemble method to adapt coreference resolution across domains. This method has three features: (1) it can optimize for any user-specified objective measure; (2) it can make document-specific prediction rather than rely on a fixed base model or a fixed set of base models; (3) it can automatically adjust the active ensemble members during prediction. With simplification, this method can be used in the traditional withindomain case, while still retaining the above features. To the best of our knowledge, this work is the first to both (i) develop a domain adaptation algorithm for the coreference resolution problem and (ii) have the above features as an ensemble method. Empirically, we show the benefits of (i) on the six domains of the ACE 2005 data set in domain adaptation set- ting, and of (ii) on both the MUC-6 and the ACE 2005 data sets in within-domain setting.</p><p>3 0.80525345 <a title="36-lda-3" href="./emnlp-2012-Learning_Syntactic_Categories_Using_Paradigmatic_Representations_of_Word_Context.html">79 emnlp-2012-Learning Syntactic Categories Using Paradigmatic Representations of Word Context</a></p>
<p>Author: Mehmet Ali Yatbaz ; Enis Sert ; Deniz Yuret</p><p>Abstract: We investigate paradigmatic representations of word context in the domain of unsupervised syntactic category acquisition. Paradigmatic representations of word context are based on potential substitutes of a word in contrast to syntagmatic representations based on properties of neighboring words. We compare a bigram based baseline model with several paradigmatic models and demonstrate significant gains in accuracy. Our best model based on Euclidean co-occurrence embedding combines the paradigmatic context representation with morphological and orthographic features and achieves 80% many-to-one accuracy on a 45-tag 1M word corpus.</p><p>4 0.63780183 <a title="36-lda-4" href="./emnlp-2012-Left-to-Right_Tree-to-String_Decoding_with_Prediction.html">82 emnlp-2012-Left-to-Right Tree-to-String Decoding with Prediction</a></p>
<p>Author: Yang Feng ; Yang Liu ; Qun Liu ; Trevor Cohn</p><p>Abstract: Decoding algorithms for syntax based machine translation suffer from high computational complexity, a consequence of intersecting a language model with a context free grammar. Left-to-right decoding, which generates the target string in order, can improve decoding efficiency by simplifying the language model evaluation. This paper presents a novel left to right decoding algorithm for tree-to-string translation, using a bottom-up parsing strategy and dynamic future cost estimation for each partial translation. Our method outperforms previously published tree-to-string decoders, including a competing left-to-right method.</p><p>5 0.54461986 <a title="36-lda-5" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>Author: Heeyoung Lee ; Marta Recasens ; Angel Chang ; Mihai Surdeanu ; Dan Jurafsky</p><p>Abstract: We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable documents, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.</p><p>6 0.45817056 <a title="36-lda-6" href="./emnlp-2012-Joint_Learning_for_Coreference_Resolution_with_Markov_Logic.html">73 emnlp-2012-Joint Learning for Coreference Resolution with Markov Logic</a></p>
<p>7 0.44970441 <a title="36-lda-7" href="./emnlp-2012-Besting_the_Quiz_Master%3A_Crowdsourcing_Incremental_Classification_Games.html">23 emnlp-2012-Besting the Quiz Master: Crowdsourcing Incremental Classification Games</a></p>
<p>8 0.43285209 <a title="36-lda-8" href="./emnlp-2012-Fast_Large-Scale_Approximate_Graph_Construction_for_NLP.html">52 emnlp-2012-Fast Large-Scale Approximate Graph Construction for NLP</a></p>
<p>9 0.4190774 <a title="36-lda-9" href="./emnlp-2012-Building_a_Lightweight_Semantic_Model_for_Unsupervised_Information_Extraction_on_Short_Listings.html">26 emnlp-2012-Building a Lightweight Semantic Model for Unsupervised Information Extraction on Short Listings</a></p>
<p>10 0.41708967 <a title="36-lda-10" href="./emnlp-2012-Constructing_Task-Specific_Taxonomies_for_Document_Collection_Browsing.html">30 emnlp-2012-Constructing Task-Specific Taxonomies for Document Collection Browsing</a></p>
<p>11 0.41385916 <a title="36-lda-11" href="./emnlp-2012-Syntactic_Surprisal_Affects_Spoken_Word_Duration_in_Conversational_Contexts.html">122 emnlp-2012-Syntactic Surprisal Affects Spoken Word Duration in Conversational Contexts</a></p>
<p>12 0.4124786 <a title="36-lda-12" href="./emnlp-2012-Multi-Domain_Learning%3A_When_Do_Domains_Matter%3F.html">92 emnlp-2012-Multi-Domain Learning: When Do Domains Matter?</a></p>
<p>13 0.40799427 <a title="36-lda-13" href="./emnlp-2012-Word_Salad%3A_Relating_Food_Prices_and_Descriptions.html">139 emnlp-2012-Word Salad: Relating Food Prices and Descriptions</a></p>
<p>14 0.40524846 <a title="36-lda-14" href="./emnlp-2012-Revisiting_the_Predictability_of_Language%3A_Response_Completion_in_Social_Media.html">114 emnlp-2012-Revisiting the Predictability of Language: Response Completion in Social Media</a></p>
<p>15 0.40240061 <a title="36-lda-15" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>16 0.39403558 <a title="36-lda-16" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>17 0.38923565 <a title="36-lda-17" href="./emnlp-2012-Opinion_Target_Extraction_Using_Word-Based_Translation_Model.html">101 emnlp-2012-Opinion Target Extraction Using Word-Based Translation Model</a></p>
<p>18 0.38301674 <a title="36-lda-18" href="./emnlp-2012-Extracting_Opinion_Expressions_with_semi-Markov_Conditional_Random_Fields.html">51 emnlp-2012-Extracting Opinion Expressions with semi-Markov Conditional Random Fields</a></p>
<p>19 0.38298646 <a title="36-lda-19" href="./emnlp-2012-Automatically_Constructing_a_Normalisation_Dictionary_for_Microblogs.html">22 emnlp-2012-Automatically Constructing a Normalisation Dictionary for Microblogs</a></p>
<p>20 0.38287371 <a title="36-lda-20" href="./emnlp-2012-User_Demographics_and_Language_in_an_Implicit_Social_Network.html">134 emnlp-2012-User Demographics and Language in an Implicit Social Network</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
