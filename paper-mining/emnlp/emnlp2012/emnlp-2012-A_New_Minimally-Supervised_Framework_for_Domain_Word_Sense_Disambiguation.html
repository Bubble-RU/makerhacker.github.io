<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>6 emnlp-2012-A New Minimally-Supervised Framework for Domain Word Sense Disambiguation</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-6" href="#">emnlp2012-6</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>6 emnlp-2012-A New Minimally-Supervised Framework for Domain Word Sense Disambiguation</h1>
<br/><p>Source: <a title="emnlp-2012-6-pdf" href="http://aclweb.org/anthology//D/D12/D12-1129.pdf">pdf</a></p><p>Author: Stefano Faralli ; Roberto Navigli</p><p>Abstract: We present a new minimally-supervised framework for performing domain-driven Word Sense Disambiguation (WSD). Glossaries for several domains are iteratively acquired from the Web by means of a bootstrapping technique. The acquired glosses are then used as the sense inventory for fullyunsupervised domain WSD. Our experiments, on new and gold-standard datasets, show that our wide-coverage framework enables highperformance results on dozens of domains at a coarse and fine-grained level.</p><p>Reference: <a title="emnlp-2012-6-reference" href="../emnlp2012_reference/emnlp-2012-A_New_Minimally-Supervised_Framework_for_Domain_Word_Sense_Disambiguation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A New Minimally-Supervised Framework for Domain Word Sense Disambiguation Stefano Faralli and Roberto Navigli Dipartimento di Informatica Sapienza Universit a` di Roma {faral l ,navigl i}@ di . [sent-1, score-0.126]
</p><p>2 it i  Abstract We present a new minimally-supervised framework for performing domain-driven Word Sense Disambiguation (WSD). [sent-3, score-0.033]
</p><p>3 Glossaries for several domains are iteratively acquired from the Web by means of a bootstrapping technique. [sent-4, score-0.295]
</p><p>4 The acquired glosses are then used as the sense inventory for fullyunsupervised domain WSD. [sent-5, score-0.743]
</p><p>5 Our experiments, on new and gold-standard datasets, show that our wide-coverage framework enables highperformance results on dozens of domains at a coarse and fine-grained level. [sent-6, score-0.305]
</p><p>6 If we just think of the Web, the vast majority of its textual content is domain oriented. [sent-8, score-0.275]
</p><p>7 A case in point is Wikipedia, which provides ency-  clopedic coverage for a huge number of knowledge domains (Medelyan et al. [sent-9, score-0.097]
</p><p>8 , 2009), but most blogs, Web sites and newspapers also provide a great deal of information focused on specific areas of knowledge. [sent-10, score-0.065]
</p><p>9 1411 Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now. [sent-15, score-0.275]
</p><p>10 Many approaches have been devised, including the identification of domain-specific predominant senses (McCarthy et al. [sent-16, score-0.156]
</p><p>11 , 2007; Lapata and Keller, 2007), the development of domain resources (Magnini and Cavagli a`, 2000; Magnini et al. [sent-17, score-0.275]
</p><p>12 More recently, semi-supervised approaches to domain WSD have been proposed which aim at decreasing the amount of supervision needed to carry out the task (Khapra et al. [sent-22, score-0.307]
</p><p>13 High-performance domain WSD, however, has been hampered by the widespread use of a generalpurpose sense inventory, i. [sent-24, score-0.478]
</p><p>14 Unfortunately WordNet does not contain many specialized terms, making it difficult to use it in work on arbitrary specialized domains. [sent-28, score-0.102]
</p><p>15 In this paper we provide three main contributions: • We tackle the above issues by introducing a new kfrlaem tehweor abk vbaese idss on bthye minimallysupervised acquisition of specialized glossaries  for dozens of domains. [sent-30, score-0.414]
</p><p>16 Lc a2n0g1u2ag Aes Psorcoicaetsiosin fgo arn Cdo Cmopmutpauti oantiaoln Lailn Ngautiustriacls •  •  In turn, we use the acquired domain glossaries as a sense inventory fcoqru diroemda dinom m WaiSnD g. [sent-33, score-0.839]
</p><p>17 Aosss a result, we redefine the domain WSD task as one of picking out the most appropriate gloss (finegrained setting) or domain (coarse-grained setting) from a multi-domain glossary. [sent-34, score-0.783]
</p><p>18 We show that our framework represents a consWidee srahbolwe departure fmroemw trhke r common usage of a general-purpose sense inventory such as WordNet, in that, thanks to the wide coverage of domain meanings, it enables highperformance unsupervised WSD on many domains in the range of 69-80% F1. [sent-35, score-0.825]
</p><p>19 Furthermore, our approach can be customized to any set of domains of interest, and new senses, i. [sent-36, score-0.128]
</p><p>20 , glosses, can be added at any time (either manually or automatically) to the multi-domain sense inventory. [sent-38, score-0.106]
</p><p>21 An important research direction identifies distributionally similar neighbors in raw text as cues for determining the predominant sense of a target word by means of a semantic similarity measure (McCarthy et al. [sent-40, score-0.219]
</p><p>22 Other distributional methods include the use ofa word-category cooccurrence matrix, where categories are coarse senses obtained from an existing thesaurus (Mohammad and Hirst, 2006), and synonym-based word occurrence counts (Lapata and Keller, 2007). [sent-44, score-0.14]
</p><p>23 Domain-informed methods have also been proposed which make use of domain labels as cues for disambiguation purposes (Gliozzo et al. [sent-45, score-0.381]
</p><p>24 , 2004), especially when domain kernels are coupled with a syntagmatic one (Gliozzo et al. [sent-48, score-0.275]
</p><p>25 However, their performance is typically lower than supervised systems. [sent-50, score-0.029]
</p><p>26 On the other hand, supervised systems fall short of carrying out high-performance WSD within domains, the main reason being the need for retraining on each new specific knowledge domain. [sent-51, score-0.058]
</p><p>27 Nonetheless, the knowledge acquisition bottleneck can be  relieved by means of domain adaptation (Chan and 1412 Ng, 2006; Chan and Ng, 2007; Agirre and de Lacalle, 2009) or by effectively injecting a generalpurpose corpus into a smaller domain-specific training set (Khapra et al. [sent-52, score-0.447]
</p><p>28 However, as mentioned above, most work on domain WSD uses WordNet as a sense inventory. [sent-54, score-0.381]
</p><p>29 But even if WordNet senses have been enriched with topically-distinctive words and concepts (Agirre and de Lacalle, 2004; Cuadros and Rigau, 2008), manually-developed domain labels (Magnini et al. [sent-55, score-0.406]
</p><p>30 , 2002), and disambiguated semantic relations (Navigli, 2005), the main obstacle of being stuck with an open-ended fine-grained sense inventory remains. [sent-56, score-0.275]
</p><p>31 , 2005) show that domain WSD can achieve accuracy in the 5060% ballpark when a state-of-the-art algorithm such as Personalized PageRank is paired with a distributional approach (Agirre et al. [sent-58, score-0.275]
</p><p>32 , 2009) or with semantic model vectors acquired for many domains (Navigli et al. [sent-59, score-0.187]
</p><p>33 In this paper, we take domain WSD to the next level by proposing a new framework based on the minimally-supervised acquisition of large domain sense inventories thanks to which high performance can be attained on virtually any domain using unsupervised algorithms. [sent-61, score-1.161]
</p><p>34 Glossary acquisition approaches in the literature are mostly focused on pattern-based definition extraction (Fujii and Ishikawa, 2000; Hovy et al. [sent-62, score-0.104]
</p><p>35 To do so, we adapt bootstrapping techniques (Brin, 1998; Agichtein and Gravano, 2000; Pasca et al. [sent-64, score-0.079]
</p><p>36 , 2006) to the novel task of domain glossary acquisition from the Web. [sent-65, score-0.786]
</p><p>37 Approaches to domain sense modeling have already been proposed which go beyond the WordNet sense inventory (Duan and Yates, 2010). [sent-66, score-0.656]
</p><p>38 Distinctive collocations are extracted from corpora and used as features to bootstrap a supervised WSD system. [sent-67, score-0.029]
</p><p>39 Experiments in the biomedical domain show good performance, however only in-domain ambiguity is addressed. [sent-68, score-0.275]
</p><p>40 In contrast, our approach tackles cross-  Figure 1: The bootstrapping process for glossary acquisition. [sent-69, score-0.551]
</p><p>41 domain ambiguity, by working with virtually any set of domains and minimizing the requirements by harvesting domain terms and definitions from the Web, bootstrapped using a small number of seeds. [sent-70, score-0.771]
</p><p>42 The existing approach closest to ours is that of Huang and Riloff (2010), who devised a bootstrapping approach to induce semantic class taggers from domain text. [sent-71, score-0.411]
</p><p>43 Our objective, instead, is to perform domain disambiguation at the word level. [sent-73, score-0.351]
</p><p>44 To do this, we redefine the domain WSD problem as one of selecting the most suitable gloss from those available in our full-fledged multi-domain glossary. [sent-74, score-0.508]
</p><p>45 3  A Minimally-Supervised Framework for Domain WSD  In this section we present our new framework for performing domain WSD. [sent-75, score-0.308]
</p><p>46 The framework consists of two phases: glossary bootstrapping (Section 3. [sent-76, score-0.55]
</p><p>47 1 Phase 1: Bootstrapping Domain Glossaries The objective of the first phase is to acquire a multidomain glossary from the Web with minimal supervision. [sent-80, score-0.536]
</p><p>48 We initially select a set D of domains of interest. [sent-81, score-0.097]
</p><p>49 For each individual domain d ∈ D we start iwntitehr an empty cseht ionfd HiviTdMuaLl patterns Pd (i. [sent-82, score-0.275]
</p><p>50 During this phase we iteratively populate athrvee pattern suerti by means sofe wsixe steps, described in the next six subsections and depicted in Figure 1. [sent-85, score-0.093]
</p><p>51 The final output of this phase will be a glossary Gd consisting of domain terms and their automatically-harvested glosses. [sent-86, score-0.777]
</p><p>52 1 Step 1: Initial seed selection First, given the domain d, we manually pick out K hypernymy relation seeds Sd = 1413 {(t1, h1) , . [sent-89, score-0.458]
</p><p>53 , (tK, hK)}, where the pair (ti, hi)  c{o(nttains a domain term)} ti awnhde riets generalization hi (e. [sent-92, score-0.6]
</p><p>54 The only constraint we impose is that the selected relations must be distinctive for the domain d of interest. [sent-95, score-0.332]
</p><p>55 The chosen hypernymy relations have to be as topical and representative as possible for the given domain (e. [sent-96, score-0.355]
</p><p>56 , (compiler, computer program) is an appropriate pair for computer science, while (byte, unit of measurement) is not, as it might cause the extraction of several glossaries of various units and measures). [sent-98, score-0.199]
</p><p>57 Note that this is the only human intervention in the entire glossary acquisition process. [sent-99, score-0.545]
</p><p>58 We now set the iteration counter k to 1 and start the first iteration of the process (steps 2-5). [sent-100, score-0.122]
</p><p>59 After each iteration k, we keep track of the set of glosses Gdk, acquired during iteration k. [sent-101, score-0.315]
</p><p>60 2 Step 2: Seed queries For each seed pair (ti, hi), we submit the following three queries to a Web search engine: “ti” “hi” glo s s ary1, “ti” “hi” de finit i on, “ti” “hi” dict ionary and collect the 64 top-ranking results for each query2. [sent-104, score-0.156]
</p><p>61 Each resulting page is a candidate glossary for the domain d identified by our relation  seeds Sd. [sent-105, score-0.759]
</p><p>62 3 Step 3: Pattern and glossary extraction We initialize the glossary for iteration k as follows: Gdk := ∅. [sent-108, score-0.937]
</p><p>63 the N etextx,t snippets s starting wagieth, ti and ending with hi (e. [sent-110, score-0.291]
</p><p>64 For each such text snippet s, we perform five substeps: a) extraction of the term/gloss  separator:  we  1In what follows, we use the typewriter font for keywords and term/gloss separators. [sent-118, score-0.083]
</p><p>65 start from ti and move right until we extract the longest sequence pM of HTML tags and non-alphanumeric characters, which we call the term/gloss separator, between ti and the glossary definition (e. [sent-120, score-0.704]
</p><p>66 , “< /b> --” between “firewall” and “a” in the above example);  b) gloss extraction: we expand the snippet s to the right of hi in search of the entire gloss of ti, i. [sent-122, score-0.554]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wsd', 0.458), ('glossary', 0.438), ('domain', 0.275), ('glossaries', 0.199), ('navigli', 0.172), ('gloss', 0.171), ('inventory', 0.169), ('hi', 0.158), ('ti', 0.133), ('firewall', 0.119), ('sense', 0.106), ('gliozzo', 0.103), ('glosses', 0.103), ('senses', 0.102), ('domains', 0.097), ('agirre', 0.096), ('magnini', 0.093), ('acquired', 0.09), ('mccarthy', 0.08), ('gdk', 0.08), ('highperformance', 0.08), ('hypernymy', 0.08), ('koeling', 0.08), ('separator', 0.08), ('bootstrapping', 0.079), ('wordnet', 0.078), ('disambiguation', 0.076), ('acquisition', 0.073), ('generalpurpose', 0.068), ('lacalle', 0.068), ('phase', 0.064), ('khapra', 0.062), ('pagerank', 0.062), ('redefine', 0.062), ('virtually', 0.062), ('iteration', 0.061), ('seed', 0.057), ('distinctive', 0.057), ('devised', 0.057), ('dozens', 0.057), ('personalized', 0.057), ('roma', 0.057), ('keller', 0.054), ('pd', 0.054), ('predominant', 0.054), ('snippet', 0.054), ('specialized', 0.051), ('chan', 0.046), ('seeds', 0.046), ('security', 0.044), ('di', 0.042), ('web', 0.038), ('coarse', 0.038), ('finance', 0.034), ('awnhde', 0.034), ('brin', 0.034), ('departure', 0.034), ('dipartimento', 0.034), ('faralli', 0.034), ('idss', 0.034), ('intervention', 0.034), ('ionary', 0.034), ('ishikawa', 0.034), ('lexicographic', 0.034), ('multidomain', 0.034), ('newspapers', 0.034), ('sapienza', 0.034), ('stefano', 0.034), ('submit', 0.034), ('tackles', 0.034), ('framework', 0.033), ('supervision', 0.032), ('definitions', 0.031), ('thanks', 0.031), ('focused', 0.031), ('strapparava', 0.031), ('bootstrapped', 0.031), ('bouma', 0.031), ('customized', 0.031), ('dict', 0.031), ('fujii', 0.031), ('harvest', 0.031), ('informatica', 0.031), ('injecting', 0.031), ('inventories', 0.031), ('medelyan', 0.031), ('widecoverage', 0.031), ('yates', 0.031), ('lapata', 0.031), ('cues', 0.03), ('iteratively', 0.029), ('supervised', 0.029), ('api', 0.029), ('carrying', 0.029), ('distributionally', 0.029), ('enriched', 0.029), ('font', 0.029), ('gd', 0.029), ('gravano', 0.029), ('hampered', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="6-tfidf-1" href="./emnlp-2012-A_New_Minimally-Supervised_Framework_for_Domain_Word_Sense_Disambiguation.html">6 emnlp-2012-A New Minimally-Supervised Framework for Domain Word Sense Disambiguation</a></p>
<p>Author: Stefano Faralli ; Roberto Navigli</p><p>Abstract: We present a new minimally-supervised framework for performing domain-driven Word Sense Disambiguation (WSD). Glossaries for several domains are iteratively acquired from the Web by means of a bootstrapping technique. The acquired glosses are then used as the sense inventory for fullyunsupervised domain WSD. Our experiments, on new and gold-standard datasets, show that our wide-coverage framework enables highperformance results on dozens of domains at a coarse and fine-grained level.</p><p>2 0.40071061 <a title="6-tfidf-2" href="./emnlp-2012-Joining_Forces_Pays_Off%3A_Multilingual_Joint_Word_Sense_Disambiguation.html">69 emnlp-2012-Joining Forces Pays Off: Multilingual Joint Word Sense Disambiguation</a></p>
<p>Author: Roberto Navigli ; Simone Paolo Ponzetto</p><p>Abstract: We present a multilingual joint approach to Word Sense Disambiguation (WSD). Our method exploits BabelNet, a very large multilingual knowledge base, to perform graphbased WSD across different languages, and brings together empirical evidence from these languages using ensemble methods. The results show that, thanks to complementing wide-coverage multilingual lexical knowledge with robust graph-based algorithms and combination methods, we are able to achieve the state of the art in both monolingual and multilingual WSD settings.</p><p>3 0.085705221 <a title="6-tfidf-3" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>Author: Fei Huang ; Alexander Yates</p><p>Abstract: Representation learning is a promising technique for discovering features that allow supervised classifiers to generalize from a source domain dataset to arbitrary new domains. We present a novel, formal statement of the representation learning task. We argue that because the task is computationally intractable in general, it is important for a representation learner to be able to incorporate expert knowledge during its search for helpful features. Leveraging the Posterior Regularization framework, we develop an architecture for incorporating biases into representation learning. We investigate three types of biases, and experiments on two domain adaptation tasks show that our biased learners identify significantly better sets of features than unbiased learners, resulting in a relative reduction in error of more than 16% for both tasks, with respect to existing state-of-the-art representation learning techniques.</p><p>4 0.084354907 <a title="6-tfidf-4" href="./emnlp-2012-Do_Neighbours_Help%3F_An_Exploration_of_Graph-based_Algorithms_for_Cross-domain_Sentiment_Classification.html">34 emnlp-2012-Do Neighbours Help? An Exploration of Graph-based Algorithms for Cross-domain Sentiment Classification</a></p>
<p>Author: Natalia Ponomareva ; Mike Thelwall</p><p>Abstract: This paper presents a comparative study of graph-based approaches for cross-domain sentiment classification. In particular, the paper analyses two existing methods: an optimisation problem and a ranking algorithm. We compare these graph-based methods with each other and with the other state-ofthe-art approaches and conclude that graph domain representations offer a competitive solution to the domain adaptation problem. Analysis of the best parameters for graphbased algorithms reveals that there are no optimal values valid for all domain pairs and that these values are dependent on the characteristics of corresponding domains.</p><p>5 0.083759844 <a title="6-tfidf-5" href="./emnlp-2012-Multi-Domain_Learning%3A_When_Do_Domains_Matter%3F.html">92 emnlp-2012-Multi-Domain Learning: When Do Domains Matter?</a></p>
<p>Author: Mahesh Joshi ; Mark Dredze ; William W. Cohen ; Carolyn Rose</p><p>Abstract: We present a systematic analysis of existing multi-domain learning approaches with respect to two questions. First, many multidomain learning algorithms resemble ensemble learning algorithms. (1) Are multi-domain learning improvements the result of ensemble learning effects? Second, these algorithms are traditionally evaluated in a balanced class label setting, although in practice many multidomain settings have domain-specific class label biases. When multi-domain learning is applied to these settings, (2) are multidomain methods improving because they capture domain-specific class biases? An understanding of these two issues presents a clearer idea about where the field has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art.</p><p>6 0.079883739 <a title="6-tfidf-6" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>7 0.066460095 <a title="6-tfidf-7" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>8 0.066080876 <a title="6-tfidf-8" href="./emnlp-2012-Domain_Adaptation_for_Coreference_Resolution%3A_An_Adaptive_Ensemble_Approach.html">36 emnlp-2012-Domain Adaptation for Coreference Resolution: An Adaptive Ensemble Approach</a></p>
<p>9 0.063097097 <a title="6-tfidf-9" href="./emnlp-2012-Bilingual_Lexicon_Extraction_from_Comparable_Corpora_Using_Label_Propagation.html">25 emnlp-2012-Bilingual Lexicon Extraction from Comparable Corpora Using Label Propagation</a></p>
<p>10 0.046113849 <a title="6-tfidf-10" href="./emnlp-2012-Type-Supervised_Hidden_Markov_Models_for_Part-of-Speech_Tagging_with_Incomplete_Tag_Dictionaries.html">129 emnlp-2012-Type-Supervised Hidden Markov Models for Part-of-Speech Tagging with Incomplete Tag Dictionaries</a></p>
<p>11 0.042202786 <a title="6-tfidf-11" href="./emnlp-2012-Explore_Person_Specific_Evidence_in_Web_Person_Name_Disambiguation.html">47 emnlp-2012-Explore Person Specific Evidence in Web Person Name Disambiguation</a></p>
<p>12 0.041537613 <a title="6-tfidf-12" href="./emnlp-2012-Improving_NLP_through_Marginalization_of_Hidden_Syntactic_Structure.html">65 emnlp-2012-Improving NLP through Marginalization of Hidden Syntactic Structure</a></p>
<p>13 0.039355941 <a title="6-tfidf-13" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>14 0.038135458 <a title="6-tfidf-14" href="./emnlp-2012-PATTY%3A_A_Taxonomy_of_Relational_Patterns_with_Semantic_Types.html">103 emnlp-2012-PATTY: A Taxonomy of Relational Patterns with Semantic Types</a></p>
<p>15 0.036077809 <a title="6-tfidf-15" href="./emnlp-2012-Locally_Training_the_Log-Linear_Model_for_SMT.html">86 emnlp-2012-Locally Training the Log-Linear Model for SMT</a></p>
<p>16 0.035714861 <a title="6-tfidf-16" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>17 0.034060258 <a title="6-tfidf-17" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>18 0.03327436 <a title="6-tfidf-18" href="./emnlp-2012-Concurrent_Acquisition_of_Word_Meaning_and_Lexical_Categories.html">29 emnlp-2012-Concurrent Acquisition of Word Meaning and Lexical Categories</a></p>
<p>19 0.031187234 <a title="6-tfidf-19" href="./emnlp-2012-Enlarging_Paraphrase_Collections_through_Generalization_and_Instantiation.html">39 emnlp-2012-Enlarging Paraphrase Collections through Generalization and Instantiation</a></p>
<p>20 0.030829923 <a title="6-tfidf-20" href="./emnlp-2012-Local_and_Global_Context_for_Supervised_and_Unsupervised_Metonymy_Resolution.html">85 emnlp-2012-Local and Global Context for Supervised and Unsupervised Metonymy Resolution</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.139), (1, 0.074), (2, -0.018), (3, 0.028), (4, 0.034), (5, 0.018), (6, 0.098), (7, 0.205), (8, 0.175), (9, -0.277), (10, 0.206), (11, 0.14), (12, 0.015), (13, 0.229), (14, 0.354), (15, 0.183), (16, -0.205), (17, -0.171), (18, 0.096), (19, -0.224), (20, -0.07), (21, 0.097), (22, -0.147), (23, 0.067), (24, -0.002), (25, 0.148), (26, 0.052), (27, 0.049), (28, 0.048), (29, -0.087), (30, 0.078), (31, 0.053), (32, -0.039), (33, 0.045), (34, -0.008), (35, 0.061), (36, 0.018), (37, -0.035), (38, 0.032), (39, 0.003), (40, -0.028), (41, -0.043), (42, 0.007), (43, 0.022), (44, -0.013), (45, 0.03), (46, 0.049), (47, -0.001), (48, -0.03), (49, -0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95624739 <a title="6-lsi-1" href="./emnlp-2012-A_New_Minimally-Supervised_Framework_for_Domain_Word_Sense_Disambiguation.html">6 emnlp-2012-A New Minimally-Supervised Framework for Domain Word Sense Disambiguation</a></p>
<p>Author: Stefano Faralli ; Roberto Navigli</p><p>Abstract: We present a new minimally-supervised framework for performing domain-driven Word Sense Disambiguation (WSD). Glossaries for several domains are iteratively acquired from the Web by means of a bootstrapping technique. The acquired glosses are then used as the sense inventory for fullyunsupervised domain WSD. Our experiments, on new and gold-standard datasets, show that our wide-coverage framework enables highperformance results on dozens of domains at a coarse and fine-grained level.</p><p>2 0.90113604 <a title="6-lsi-2" href="./emnlp-2012-Joining_Forces_Pays_Off%3A_Multilingual_Joint_Word_Sense_Disambiguation.html">69 emnlp-2012-Joining Forces Pays Off: Multilingual Joint Word Sense Disambiguation</a></p>
<p>Author: Roberto Navigli ; Simone Paolo Ponzetto</p><p>Abstract: We present a multilingual joint approach to Word Sense Disambiguation (WSD). Our method exploits BabelNet, a very large multilingual knowledge base, to perform graphbased WSD across different languages, and brings together empirical evidence from these languages using ensemble methods. The results show that, thanks to complementing wide-coverage multilingual lexical knowledge with robust graph-based algorithms and combination methods, we are able to achieve the state of the art in both monolingual and multilingual WSD settings.</p><p>3 0.28418893 <a title="6-lsi-3" href="./emnlp-2012-Multi-Domain_Learning%3A_When_Do_Domains_Matter%3F.html">92 emnlp-2012-Multi-Domain Learning: When Do Domains Matter?</a></p>
<p>Author: Mahesh Joshi ; Mark Dredze ; William W. Cohen ; Carolyn Rose</p><p>Abstract: We present a systematic analysis of existing multi-domain learning approaches with respect to two questions. First, many multidomain learning algorithms resemble ensemble learning algorithms. (1) Are multi-domain learning improvements the result of ensemble learning effects? Second, these algorithms are traditionally evaluated in a balanced class label setting, although in practice many multidomain settings have domain-specific class label biases. When multi-domain learning is applied to these settings, (2) are multidomain methods improving because they capture domain-specific class biases? An understanding of these two issues presents a clearer idea about where the field has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art.</p><p>4 0.28165737 <a title="6-lsi-4" href="./emnlp-2012-Do_Neighbours_Help%3F_An_Exploration_of_Graph-based_Algorithms_for_Cross-domain_Sentiment_Classification.html">34 emnlp-2012-Do Neighbours Help? An Exploration of Graph-based Algorithms for Cross-domain Sentiment Classification</a></p>
<p>Author: Natalia Ponomareva ; Mike Thelwall</p><p>Abstract: This paper presents a comparative study of graph-based approaches for cross-domain sentiment classification. In particular, the paper analyses two existing methods: an optimisation problem and a ranking algorithm. We compare these graph-based methods with each other and with the other state-ofthe-art approaches and conclude that graph domain representations offer a competitive solution to the domain adaptation problem. Analysis of the best parameters for graphbased algorithms reveals that there are no optimal values valid for all domain pairs and that these values are dependent on the characteristics of corresponding domains.</p><p>5 0.27041656 <a title="6-lsi-5" href="./emnlp-2012-Domain_Adaptation_for_Coreference_Resolution%3A_An_Adaptive_Ensemble_Approach.html">36 emnlp-2012-Domain Adaptation for Coreference Resolution: An Adaptive Ensemble Approach</a></p>
<p>Author: Jian Bo Yang ; Qi Mao ; Qiao Liang Xiang ; Ivor Wai-Hung Tsang ; Kian Ming Adam Chai ; Hai Leong Chieu</p><p>Abstract: We propose an adaptive ensemble method to adapt coreference resolution across domains. This method has three features: (1) it can optimize for any user-specified objective measure; (2) it can make document-specific prediction rather than rely on a fixed base model or a fixed set of base models; (3) it can automatically adjust the active ensemble members during prediction. With simplification, this method can be used in the traditional withindomain case, while still retaining the above features. To the best of our knowledge, this work is the first to both (i) develop a domain adaptation algorithm for the coreference resolution problem and (ii) have the above features as an ensemble method. Empirically, we show the benefits of (i) on the six domains of the ACE 2005 data set in domain adaptation set- ting, and of (ii) on both the MUC-6 and the ACE 2005 data sets in within-domain setting.</p><p>6 0.26945674 <a title="6-lsi-6" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>7 0.22544792 <a title="6-lsi-7" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>8 0.20012744 <a title="6-lsi-8" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>9 0.18780242 <a title="6-lsi-9" href="./emnlp-2012-Bilingual_Lexicon_Extraction_from_Comparable_Corpora_Using_Label_Propagation.html">25 emnlp-2012-Bilingual Lexicon Extraction from Comparable Corpora Using Label Propagation</a></p>
<p>10 0.16120797 <a title="6-lsi-10" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>11 0.14632811 <a title="6-lsi-11" href="./emnlp-2012-Building_a_Lightweight_Semantic_Model_for_Unsupervised_Information_Extraction_on_Short_Listings.html">26 emnlp-2012-Building a Lightweight Semantic Model for Unsupervised Information Extraction on Short Listings</a></p>
<p>12 0.14003228 <a title="6-lsi-12" href="./emnlp-2012-PATTY%3A_A_Taxonomy_of_Relational_Patterns_with_Semantic_Types.html">103 emnlp-2012-PATTY: A Taxonomy of Relational Patterns with Semantic Types</a></p>
<p>13 0.1313438 <a title="6-lsi-13" href="./emnlp-2012-Improving_NLP_through_Marginalization_of_Hidden_Syntactic_Structure.html">65 emnlp-2012-Improving NLP through Marginalization of Hidden Syntactic Structure</a></p>
<p>14 0.12562115 <a title="6-lsi-14" href="./emnlp-2012-Local_and_Global_Context_for_Supervised_and_Unsupervised_Metonymy_Resolution.html">85 emnlp-2012-Local and Global Context for Supervised and Unsupervised Metonymy Resolution</a></p>
<p>15 0.12532747 <a title="6-lsi-15" href="./emnlp-2012-Explore_Person_Specific_Evidence_in_Web_Person_Name_Disambiguation.html">47 emnlp-2012-Explore Person Specific Evidence in Web Person Name Disambiguation</a></p>
<p>16 0.12383416 <a title="6-lsi-16" href="./emnlp-2012-Large_Scale_Decipherment_for_Out-of-Domain_Machine_Translation.html">75 emnlp-2012-Large Scale Decipherment for Out-of-Domain Machine Translation</a></p>
<p>17 0.11933143 <a title="6-lsi-17" href="./emnlp-2012-Concurrent_Acquisition_of_Word_Meaning_and_Lexical_Categories.html">29 emnlp-2012-Concurrent Acquisition of Word Meaning and Lexical Categories</a></p>
<p>18 0.11599377 <a title="6-lsi-18" href="./emnlp-2012-Universal_Grapheme-to-Phoneme_Prediction_Over_Latin_Alphabets.html">132 emnlp-2012-Universal Grapheme-to-Phoneme Prediction Over Latin Alphabets</a></p>
<p>19 0.1159265 <a title="6-lsi-19" href="./emnlp-2012-A_Statistical_Relational_Learning_Approach_to_Identifying_Evidence_Based_Medicine_Categories.html">10 emnlp-2012-A Statistical Relational Learning Approach to Identifying Evidence Based Medicine Categories</a></p>
<p>20 0.10236125 <a title="6-lsi-20" href="./emnlp-2012-Locally_Training_the_Log-Linear_Model_for_SMT.html">86 emnlp-2012-Locally Training the Log-Linear Model for SMT</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.013), (16, 0.033), (25, 0.018), (26, 0.342), (34, 0.143), (60, 0.099), (63, 0.046), (64, 0.026), (65, 0.037), (70, 0.021), (73, 0.01), (74, 0.022), (76, 0.047), (86, 0.017), (95, 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77525926 <a title="6-lda-1" href="./emnlp-2012-A_New_Minimally-Supervised_Framework_for_Domain_Word_Sense_Disambiguation.html">6 emnlp-2012-A New Minimally-Supervised Framework for Domain Word Sense Disambiguation</a></p>
<p>Author: Stefano Faralli ; Roberto Navigli</p><p>Abstract: We present a new minimally-supervised framework for performing domain-driven Word Sense Disambiguation (WSD). Glossaries for several domains are iteratively acquired from the Web by means of a bootstrapping technique. The acquired glosses are then used as the sense inventory for fullyunsupervised domain WSD. Our experiments, on new and gold-standard datasets, show that our wide-coverage framework enables highperformance results on dozens of domains at a coarse and fine-grained level.</p><p>2 0.475734 <a title="6-lda-2" href="./emnlp-2012-Joining_Forces_Pays_Off%3A_Multilingual_Joint_Word_Sense_Disambiguation.html">69 emnlp-2012-Joining Forces Pays Off: Multilingual Joint Word Sense Disambiguation</a></p>
<p>Author: Roberto Navigli ; Simone Paolo Ponzetto</p><p>Abstract: We present a multilingual joint approach to Word Sense Disambiguation (WSD). Our method exploits BabelNet, a very large multilingual knowledge base, to perform graphbased WSD across different languages, and brings together empirical evidence from these languages using ensemble methods. The results show that, thanks to complementing wide-coverage multilingual lexical knowledge with robust graph-based algorithms and combination methods, we are able to achieve the state of the art in both monolingual and multilingual WSD settings.</p><p>3 0.46742651 <a title="6-lda-3" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>Author: Wang Ling ; Joao Graca ; Isabel Trancoso ; Alan Black</p><p>Abstract: Phrase-based machine translation models have shown to yield better translations than Word-based models, since phrase pairs encode the contextual information that is needed for a more accurate translation. However, many phrase pairs do not encode any relevant context, which means that the translation event encoded in that phrase pair is led by smaller translation events that are independent from each other, and can be found on smaller phrase pairs, with little or no loss in translation accuracy. In this work, we propose a relative entropy model for translation models, that measures how likely a phrase pair encodes a translation event that is derivable using smaller translation events with similar probabilities. This model is then applied to phrase table pruning. Tests show that considerable amounts of phrase pairs can be excluded, without much impact on the transla- . tion quality. In fact, we show that better translations can be obtained using our pruned models, due to the compression of the search space during decoding.</p><p>4 0.46327433 <a title="6-lda-4" href="./emnlp-2012-Forced_Derivation_Tree_based_Model_Training_to_Statistical_Machine_Translation.html">54 emnlp-2012-Forced Derivation Tree based Model Training to Statistical Machine Translation</a></p>
<p>Author: Nan Duan ; Mu Li ; Ming Zhou</p><p>Abstract: A forced derivation tree (FDT) of a sentence pair {f, e} denotes a derivation tree that can tpraainrsl {afte, f} idnetono itetss a acc duerraivtea target etrea tnhsaltat cioann e. In this paper, we present an approach that leverages structured knowledge contained in FDTs to train component models for statistical machine translation (SMT) systems. We first describe how to generate different FDTs for each sentence pair in training corpus, and then present how to infer the optimal FDTs based on their derivation and alignment qualities. As the first step in this line of research, we verify the effectiveness of our approach in a BTGbased phrasal system, and propose four FDTbased component models. Experiments are carried out on large scale English-to-Japanese and Chinese-to-English translation tasks, and significant improvements are reported on both translation quality and alignment quality.</p><p>5 0.4623614 <a title="6-lda-5" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>Author: Fei Huang ; Alexander Yates</p><p>Abstract: Representation learning is a promising technique for discovering features that allow supervised classifiers to generalize from a source domain dataset to arbitrary new domains. We present a novel, formal statement of the representation learning task. We argue that because the task is computationally intractable in general, it is important for a representation learner to be able to incorporate expert knowledge during its search for helpful features. Leveraging the Posterior Regularization framework, we develop an architecture for incorporating biases into representation learning. We investigate three types of biases, and experiments on two domain adaptation tasks show that our biased learners identify significantly better sets of features than unbiased learners, resulting in a relative reduction in error of more than 16% for both tasks, with respect to existing state-of-the-art representation learning techniques.</p><p>6 0.46059883 <a title="6-lda-6" href="./emnlp-2012-A_Discriminative_Model_for_Query_Spelling_Correction_with_Latent_Structural_SVM.html">5 emnlp-2012-A Discriminative Model for Query Spelling Correction with Latent Structural SVM</a></p>
<p>7 0.46053946 <a title="6-lda-7" href="./emnlp-2012-Spectral_Dependency_Parsing_with_Latent_Variables.html">119 emnlp-2012-Spectral Dependency Parsing with Latent Variables</a></p>
<p>8 0.45698881 <a title="6-lda-8" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>9 0.4568927 <a title="6-lda-9" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>10 0.4564701 <a title="6-lda-10" href="./emnlp-2012-Extending_Machine_Translation_Evaluation_Metrics_with_Lexical_Cohesion_to_Document_Level.html">50 emnlp-2012-Extending Machine Translation Evaluation Metrics with Lexical Cohesion to Document Level</a></p>
<p>11 0.45474902 <a title="6-lda-11" href="./emnlp-2012-Multi-Domain_Learning%3A_When_Do_Domains_Matter%3F.html">92 emnlp-2012-Multi-Domain Learning: When Do Domains Matter?</a></p>
<p>12 0.45301366 <a title="6-lda-12" href="./emnlp-2012-Exploiting_Chunk-level_Features_to_Improve_Phrase_Chunking.html">45 emnlp-2012-Exploiting Chunk-level Features to Improve Phrase Chunking</a></p>
<p>13 0.45283875 <a title="6-lda-13" href="./emnlp-2012-N-gram-based_Tense_Models_for_Statistical_Machine_Translation.html">95 emnlp-2012-N-gram-based Tense Models for Statistical Machine Translation</a></p>
<p>14 0.45241684 <a title="6-lda-14" href="./emnlp-2012-Regularized_Interlingual_Projections%3A_Evaluation_on_Multilingual_Transliteration.html">111 emnlp-2012-Regularized Interlingual Projections: Evaluation on Multilingual Transliteration</a></p>
<p>15 0.45037261 <a title="6-lda-15" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>16 0.44977474 <a title="6-lda-16" href="./emnlp-2012-Joint_Learning_for_Coreference_Resolution_with_Markov_Logic.html">73 emnlp-2012-Joint Learning for Coreference Resolution with Markov Logic</a></p>
<p>17 0.44973513 <a title="6-lda-17" href="./emnlp-2012-Probabilistic_Finite_State_Machines_for_Regression-based_MT_Evaluation.html">108 emnlp-2012-Probabilistic Finite State Machines for Regression-based MT Evaluation</a></p>
<p>18 0.44966006 <a title="6-lda-18" href="./emnlp-2012-Type-Supervised_Hidden_Markov_Models_for_Part-of-Speech_Tagging_with_Incomplete_Tag_Dictionaries.html">129 emnlp-2012-Type-Supervised Hidden Markov Models for Part-of-Speech Tagging with Incomplete Tag Dictionaries</a></p>
<p>19 0.44954628 <a title="6-lda-19" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>20 0.44905755 <a title="6-lda-20" href="./emnlp-2012-Explore_Person_Specific_Evidence_in_Web_Person_Name_Disambiguation.html">47 emnlp-2012-Explore Person Specific Evidence in Web Person Name Disambiguation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
