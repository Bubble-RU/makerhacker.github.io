<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-136" href="#">emnlp2012-136</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</h1>
<br/><p>Source: <a title="emnlp-2012-136-pdf" href="http://aclweb.org/anthology//D/D12/D12-1069.pdf">pdf</a></p><p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms ofweak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.</p><p>Reference: <a title="emnlp-2012-136-reference" href="../emnlp2012_reference/emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. [sent-3, score-0.627]
</p><p>2 Our key observation is that multiple forms ofweak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. [sent-4, score-1.386]
</p><p>3 We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. [sent-5, score-0.586]
</p><p>4 This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially  shared arguments. [sent-6, score-1.101]
</p><p>5 We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. [sent-7, score-0.612]
</p><p>6 On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form. [sent-8, score-0.817]
</p><p>7 1 Introduction Semantic parsing converts natural language statements into logical forms in a meaning representation language. [sent-9, score-0.541]
</p><p>8 The expressivity and utility of semantic parsing is derived from this meaning representation, which is essentially a program that is directly executable by a computer. [sent-12, score-0.371]
</p><p>9 The best performing semantic parsers are trained using extensive manual annotation: typically, a number of sentences must be annotated with their desired logical form. [sent-18, score-0.675]
</p><p>10 This paper presents an algorithm for training a semantic parser without per-sentence annotations. [sent-23, score-0.429]
</p><p>11 The semantic parser is trained to identify relation instances from the knowledge base while simultaneously producing parses that syntactically agree  with the dependency parses. [sent-25, score-1.019]
</p><p>12 Combining these two sources of supervision allows us to train an accurate semantic parser for any knowledge base without annotated training data. [sent-26, score-0.861]
</p><p>13 We demonstrate our approach by training a Combinatory Categorial Grammar (CCG) (Steedman, 1996) that parses sentences into logical forms containing any of 77 relations from Freebase. [sent-27, score-0.66]
</p><p>14 The trained semantic parser extracts binary relations with state-of-the-art performance, while recovering considerably richer semantic structure. [sent-29, score-0.895]
</p><p>15 We demonstrate recovery of this semantic structure using natural language queries PLraoncge uadgineg Lse oafr tnhineg 2,0 p1a2g Jeosin 75t C4–o7n6f5e,re Jnecjue Iosnla Enmd,p Kiroicraela, M 1e2t–h1o4ds Ju ilny N 20a1tu2r. [sent-30, score-0.391]
</p><p>16 The second stage applies CCG combination rules, in this case both forms of function application, to combine these categories into a semantic parse. [sent-44, score-0.405]
</p><p>17 Our weakly-supervised semantic parser predicts the correct logical form for 56% of queries, despite never seeing a labeled logical form. [sent-46, score-1.055]
</p><p>18 Section 3 formulates the weakly supervised training problem for semantic parsers and presents our algorithm. [sent-49, score-0.425]
</p><p>19 Section 4 describes how we applied our algorithm to construct a semantic parser for Freebase, and Section 5 presents our results. [sent-50, score-0.429]
</p><p>20 This lexicon contains syntactic and semantic categories for each word. [sent-55, score-0.492]
</p><p>21 f(y) ∧ g(x) ∧ LOCATEDIN(x, y) Each entry of the lexicon w := s : lmaps a word or short phrase w to a syntactic category s and a logical form l. [sent-62, score-0.55]
</p><p>22 These logical forms combine during parsing to form a complete logical form for the parsed text. [sent-66, score-0.778]
</p><p>23 The result of parsing is an ordered pair, containing both a syntactic parse tree and an associated logical form. [sent-69, score-0.595]
</p><p>24 2 Knowledge Base The main input to our system is a propositional knowledge base K = (E, R, C, ∆), containing entities E, categories C, relations R and relation instances ∆. [sent-74, score-0.665]
</p><p>25 Categories and relations are predicates which operate on entities and return truth values; categories c ∈ C are one-place predicates (CITY(e)) and r∈elations r ∈ R are twoplace predicates (LOCATEDIN(e1 , e2)). [sent-75, score-0.72]
</p><p>26 The knowledge base influences the semantic parser in two ways. [sent-84, score-0.586]
</p><p>27 First, CCG logical forms are constructed by combining categories, relations and  entities from the knowledge base with logical connectives; hence, the predicates in the knowledge base determine the expressivity of the parser’s semantic representation. [sent-85, score-1.633]
</p><p>28 Second, the known relation instances r(e1, e2) ∈ ∆ are used as weak supervision to train the se)m ∈ant ∆ic parser. [sent-86, score-0.636]
</p><p>29 3 Weakly Supervised Semantic Parsing We define weakly supervised semantic parsing as the following learning problem. [sent-87, score-0.427]
</p><p>30 A CCG lexicon Λ that produces logical forms containing predicates from K. [sent-93, score-0.663]
</p><p>31 Parameters for the CCG that produce correct semantic parses ‘ for sentences s ∈ S. [sent-102, score-0.375]
</p><p>32 This problem is ill-posed without additional assumptions: since the correct logical form for a sen-  θ  tence is never observed, there is no a priori reason to prefer one semantic parse to another. [sent-103, score-0.692]
</p><p>33 Our training algorithm makes two assumptions about correct semantic parses, which are encoded as weak supervision constraints. [sent-104, score-0.635]
</p><p>34 The correct semantic parse of a sentence s contains a subset of the syntactic dependencies contained in a dependency parse of s. [sent-110, score-0.643]
</p><p>35 Our weakly supervised training uses these constraints as a proxy for labeled semantic parses. [sent-111, score-0.364]
</p><p>36 First, the algorithm constructs a graphical model that contains both the semantic parser and constant factors encoding the above two constraints. [sent-113, score-0.514]
</p><p>37 This graphical model is then used to estimate parameters θ for the semantic parser, essentially optimizing θ to produce parses that satisfy the weak supervision constraints. [sent-114, score-0.871]
</p><p>38 1 Encoding the Weak Supervision Constraints The first step of training constructs a graphical model containing the semantic parser and two weak supervision constraints. [sent-117, score-0.93]
</p><p>39 However, the first weak supervision constraint couples the semantic parses for every sentence s ∈ S. [sent-118, score-0.863]
</p><p>40 The semantic constraint couples the extractions for all sentences S(e1,e2), so the graphical model is instantiated once per (e1, e2) tuple. [sent-123, score-0.589]
</p><p>41 The model has 4 types of random variables and values: Si = si represents a sentence, Li = ‘i represents a semantic parse, Zi = zi represents the satisfaction of the syntactic constraint and Yr = yr repre-  sents the truth value of relation r. [sent-124, score-0.86]
</p><p>42 Γ represents the semantic parser, which is parametrized by θ and produces a semantic parse ‘i for each sentence si. [sent-128, score-0.632]
</p><p>43 Ψ and are deterministic factors representing the two weak supervision constraints. [sent-129, score-0.416]
</p><p>44 Φ  Semantic Parser The factor Γ represents the semantic parser, which is a log-linear probabilistic CCG using the input lexicon Λ. [sent-131, score-0.37]
</p><p>45 Given a sentence s and parameters θ, the parser defines an unnormalized probability distribu-  tion over semantic parses ‘, each of which includes both a syntactic CCG parse tree and logical form. [sent-132, score-1.076]
</p><p>46 Γ and weak supervision constraints Ψ and Φ, instantiated for an (e1, e2) tuple occurring in 2 sentences S1 and S2, with corresponding semantic parses L1 and L2. [sent-133, score-0.894]
</p><p>47 Semantic Constraint The semantic constraint states that, given an entity tuple (e1, e2), every relation instance r(e1, e2) ∈ ∆ must be expressed somewhere in S(e1 ,e2) . [sent-140, score-0.553]
</p><p>48 Fu)r ∈the ∆rmore, no semantic parse can express a relation instance which is not in the knowledge base. [sent-141, score-0.602]
</p><p>49 The graphical model contains a semantic constraint factor Ψ and one binary variable Yr for each relation r in the knowledge base. [sent-144, score-0.607]
</p><p>50 The Ψ factor determines whether each semantic parse in ‘ extracts a relation between e1 and e2. [sent-146, score-0.654]
</p><p>51 3 describes the features used by our semantic  parser  for Freebase. [sent-150, score-0.429]
</p><p>52 EXTRACTS(‘i, r, e1, e2) 0 otherwise The EXTRACTS function determines the relation instances that are asserted by a semantic parse ‘. [sent-153, score-0.636]
</p><p>53 The syntactic constraint penalizes ungrammatical parses  by encouraging the semantic parser to produce parse trees that agree with a dependency parse of the same sentence. [sent-158, score-1.101]
</p><p>54 Specifically, the syntactic constraint requires the predicate-argument structure of the CCG parse to agree with the predicate-argument structure of the dependency parse. [sent-159, score-0.397]
</p><p>55 The weak supervision variables, y, z, are the output of the model. [sent-167, score-0.416]
</p><p>56 This setting trains the sema)n ∈tic parser t o0 extract every true relation instance between (e1, e2) from some sentence in S(e1,e2), while simultaneously avoiding incorrect instances. [sent-169, score-0.378]
</p><p>57 Training optimizes the semantic parser parameters θ to predict Y = yj , Z = zj given S = sj. [sent-172, score-0.429]
</p><p>58 Therefore, it is solved by finding the maximum probability assignment ‘, then choosing values for y and z that satisfy the weak supervision constraints. [sent-176, score-0.452]
</p><p>59 When y and z are given, t,hse; θinference procedure must restrict its search to the parses ‘ which satisfy these weak supervision constraints. [sent-178, score-0.567]
</p><p>60 We then check the value of for each generated parse and eliminate parses which do not satisfy this syntactic constraint. [sent-183, score-0.37]
</p><p>61 4  Building a Grammar for Freebase  We apply the training algorithm from the previous section to produce a semantic parser for a subset of Freebase. [sent-186, score-0.429]
</p><p>62 In this section, we assume access to a knowledge base K = (E, C, R, ∆), a corpus of dependencyparsed sentences S and a procedure for identifying mentions of entities in sentences. [sent-188, score-0.369]
</p><p>63 1 Constructing the Lexicon Λ The first step in constructing the semantic parser is defining a lexicon Λ. [sent-190, score-0.546]
</p><p>64 e After instantiating lexical categories for each sentence in S, we prune infrequent lexical categories to improve parser efficiency. [sent-199, score-0.474]
</p><p>65 The instantiated lexicon represents the semantics of words and phrases as conjunctions of predicates from the knowledge base, possibly including existentially quantified variables and λ expressions. [sent-213, score-0.464]
</p><p>66 2  Extensions to CCG  The semantic parser is trained using sentences from a web corpus, which contains many out-of-domain words. [sent-222, score-0.47]
</p><p>67 5  Evaluation  In this section, we evaluate the performance of a semantic parser for Freebase, trained using our weakly-supervised algorithm. [sent-235, score-0.429]
</p><p>68 Empirical comparison is somewhat difficult because the most comparable previous work weakly-supervised relation extraction uses a shallower semantic representation. [sent-236, score-0.43]
</p><p>69 The validation set was used to estimate performance during algorithm develop2These relations are defined by a set of MQL queries and potentially traverse multiple relation links. [sent-247, score-0.436]
</p><p>70 We compare our semantic parser to MULTIR (Hoffmann et al. [sent-260, score-0.429]
</p><p>71 This method uses the same weak supervision constraint and parameter estimation procedure, but replaces the semantic parser by a linear classifier. [sent-262, score-0.925]
</p><p>72 Both the semantic parser and MULTIR were trained by running 5 iterations of the structured per4Note that the positive/negative ratio was much lower without the length filter or entity disambiguation, which is partly why filtering was performed. [sent-265, score-0.474]
</p><p>73 R oTrh NeO parser parses ethe sentence without considering the entities marked in the sentence, then applies the EXTRACTS function defined in Section 3. [sent-272, score-0.414]
</p><p>74 We compare three versions of the semantic parser: PARSE, which is the basic semantic parser, PARSE+DEP which additionally observes the correct dependency parse at test time, and PARSE-DEP which is trained without the syntactic constraint. [sent-274, score-0.702]
</p><p>75 The difference between PARSE+DEP’s aggregate and sentential precision stems from the fact that PARSE+DEP extracts each relation instance from more sentences than either MULTIR or PARSE. [sent-289, score-0.525]
</p><p>76 For example, the semantic parsers learn that “in” often combines with a city to form a prepositional phrase; the parsers can apply this knowledge to identify city arguments of any relation. [sent-295, score-0.572]
</p><p>77 However, MULTIR is capable of higher recall, since its dependency parse features can represent syntactic dependencies that cannot be represented by our semantic parsers. [sent-296, score-0.483]
</p><p>78 3 Natural Language Database Queries The second experiment measures our trained parser’s ability to correctly translate natural language queries into logical queries against Freebase. [sent-299, score-0.571]
</p><p>79 ” Each candidate query was then annotated with a logical form using categories and relations from the knowledge base; candidate queries without satisfactory logical forms were discarded. [sent-304, score-1.177]
</p><p>80 Example queries with their annotated logical forms are shown in Table 3. [sent-307, score-0.572]
</p><p>81 Precision is the percentage of successfully parsed queries for which the correct logical form was predicted. [sent-310, score-0.442]
</p><p>82 Recall is the percentage of all queries for which the correct logical form was predicted. [sent-311, score-0.442]
</p><p>83 This evaluation demonstrates that the semantic parser successfully interprets common nouns and identifies mul-  tiple relations with shared arguments. [sent-312, score-0.531]
</p><p>84 5c3a62l  Table 4: Precision and recall for predicting logical forms of natural language queries against Freebase. [sent-317, score-0.531]
</p><p>85 Several recent papers have attempted to reduce the amount of human supervision required to train a semantic parser. [sent-331, score-0.453]
</p><p>86 One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liang et al. [sent-332, score-0.428]
</p><p>87 It is also possible to self-train a semantic parser without any labeled data (Goldwasser et al. [sent-337, score-0.429]
</p><p>88 This work extends weakly supervised relation extraction to produce richer semantic structure, using only slightly more supervision in the form of dependency parses. [sent-346, score-0.849]
</p><p>89 7  Discussion  This paper presents a method for training a semantic parser using only a knowledge base and a corpus of unlabeled sentences. [sent-347, score-0.586]
</p><p>90 Our key observation is that multiple forms of weak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base offacts, and syntactic supervision in the form of a standard dependency parser. [sent-348, score-1.672]
</p><p>91 We presented an algorithm for training a semantic parser in the form of a probabilistic Combinatory Categorial Grammar, using these two types of weak supervision. [sent-349, score-0.611]
</p><p>92 We used this algorithm to train a semantic parser for an ontology of 77 Freebase predicates, using Freebase itself as the weak semantic supervision. [sent-350, score-0.83]
</p><p>93 Experimental results show that our trained semantic parser extracts binary relations as well as  a state-of-the-art weakly supervised relation extractor (Hoffmann et al. [sent-351, score-0.951]
</p><p>94 Further experiments 763 tested our trained parser’s ability to extract more complex meanings from sentences, including logical forms involving conjunctions ofmultiple relation and category predicates with shared arguments (e. [sent-353, score-0.87]
</p><p>95 The semantic parser correctly interpreted 56% of these queries, despite the broad domain and never having seen an annotated logical form. [sent-358, score-0.783]
</p><p>96 Together, these two experimental analyses suggest that the combination of syntactic and semantic weak supervision is indeed a sufficient basis for training semantic parsers for a diverse range of corpora and predicate ontologies. [sent-359, score-0.974]
</p><p>97 A statistical semantic parser that integrates syntax and semantics. [sent-403, score-0.429]
</p><p>98 Knowledgebased weak supervision for information extraction of overlapping relations. [sent-413, score-0.416]
</p><p>99 Learning to map sentences to logical form: structured classification with probabilistic categorial grammars. [sent-486, score-0.426]
</p><p>100 Online learning of relaxed ccg grammars for parsing to logical form. [sent-491, score-0.69]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('logical', 0.313), ('ccg', 0.277), ('supervision', 0.234), ('semantic', 0.219), ('parser', 0.21), ('yr', 0.186), ('multir', 0.185), ('weak', 0.182), ('relation', 0.168), ('parse', 0.16), ('predicates', 0.144), ('freebase', 0.144), ('locatedin', 0.129), ('queries', 0.129), ('lexicon', 0.117), ('parses', 0.115), ('extracts', 0.107), ('relations', 0.102), ('base', 0.102), ('categories', 0.097), ('sentential', 0.095), ('weakly', 0.094), ('california', 0.092), ('hoffmann', 0.092), ('entities', 0.089), ('forms', 0.089), ('graphical', 0.085), ('aggregate', 0.08), ('constraint', 0.08), ('zettlemoyer', 0.078), ('dep', 0.077), ('categorial', 0.072), ('extractions', 0.069), ('city', 0.068), ('combinatory', 0.067), ('luke', 0.065), ('parsing', 0.063), ('instantiated', 0.062), ('category', 0.061), ('parsers', 0.061), ('syntactic', 0.059), ('wong', 0.057), ('knowledge', 0.055), ('agree', 0.053), ('conjunctions', 0.052), ('instances', 0.052), ('supervised', 0.051), ('expressivity', 0.05), ('goldwasser', 0.05), ('kwiatkowski', 0.05), ('wah', 0.05), ('yuk', 0.05), ('clarke', 0.05), ('maximization', 0.05), ('raymond', 0.049), ('replicated', 0.046), ('displays', 0.046), ('zi', 0.046), ('entity', 0.045), ('dependency', 0.045), ('mooney', 0.044), ('town', 0.043), ('dependencyparsed', 0.043), ('ofmultiple', 0.043), ('preclude', 0.043), ('recovery', 0.043), ('shallower', 0.043), ('triples', 0.043), ('tuple', 0.041), ('annotated', 0.041), ('sentences', 0.041), ('grammar', 0.04), ('prepositional', 0.04), ('meaning', 0.039), ('mentions', 0.039), ('sj', 0.039), ('rule', 0.039), ('query', 0.038), ('richer', 0.038), ('grammars', 0.037), ('validation', 0.037), ('converts', 0.037), ('compounds', 0.037), ('asserted', 0.037), ('argm', 0.037), ('kate', 0.037), ('zelle', 0.037), ('rules', 0.037), ('inductive', 0.036), ('mintz', 0.036), ('database', 0.036), ('satisfy', 0.036), ('lexical', 0.035), ('represents', 0.034), ('precision', 0.034), ('crawl', 0.033), ('couples', 0.033), ('forbes', 0.033), ('craven', 0.033), ('behaves', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999893 <a title="136-tfidf-1" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms ofweak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.</p><p>2 0.2279689 <a title="136-tfidf-2" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>Author: Mihai Surdeanu ; Julie Tibshirani ; Ramesh Nallapati ; Christopher D. Manning</p><p>Abstract: Distant supervision for relation extraction (RE) gathering training data by aligning a database of facts with text – is an efficient approach to scale RE to thousands of different relations. However, this introduces a challenging learning scenario where the relation expressed by a pair of entities found in a sentence is unknown. For example, a sentence containing Balzac and France may express BornIn or Died, an unknown relation, or no relation at all. Because of this, traditional supervised learning, which assumes that each example is explicitly mapped to a label, is not appropriate. We propose a novel approach to multi-instance multi-label learning for RE, which jointly models all the instances of a pair of entities in text and all their labels using a graphical model with latent variables. Our model performs competitively on two difficult domains. –</p><p>3 0.14779817 <a title="136-tfidf-3" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>Author: Ni Lao ; Amarnag Subramanya ; Fernando Pereira ; William W. Cohen</p><p>Abstract: We study how to extend a large knowledge base (Freebase) by reading relational information from a large Web text corpus. Previous studies on extracting relational knowledge from text show the potential of syntactic patterns for extraction, but they do not exploit background knowledge of other relations in the knowledge base. We describe a distributed, Web-scale implementation of a path-constrained random walk model that learns syntactic-semantic inference rules for binary relations from a graph representation of the parsed text and the knowledge base. Experiments show significant accuracy improvements in binary relation prediction over methods that consider only text, or only the existing knowledge base.</p><p>4 0.14379166 <a title="136-tfidf-4" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>Author: Thomas Lin ; Mausam ; Oren Etzioni</p><p>Abstract: Entity linking systems link noun-phrase mentions in text to their corresponding Wikipedia articles. However, NLP applications would gain from the ability to detect and type all entities mentioned in text, including the long tail of entities not prominent enough to have their own Wikipedia articles. In this paper we show that once the Wikipedia entities mentioned in a corpus of textual assertions are linked, this can further enable the detection and fine-grained typing of the unlinkable entities. Our proposed method for detecting unlinkable entities achieves 24% greater accuracy than a Named Entity Recognition baseline, and our method for fine-grained typing is able to propagate over 1,000 types from linked Wikipedia entities to unlinkable entities. Detection and typing of unlinkable entities can increase yield for NLP applications such as typed question answering.</p><p>5 0.1436983 <a title="136-tfidf-5" href="./emnlp-2012-Ensemble_Semantics_for_Large-scale_Unsupervised_Relation_Extraction.html">40 emnlp-2012-Ensemble Semantics for Large-scale Unsupervised Relation Extraction</a></p>
<p>Author: Bonan Min ; Shuming Shi ; Ralph Grishman ; Chin-Yew Lin</p><p>Abstract: Discovering significant types of relations from the web is challenging because of its open nature. Unsupervised algorithms are developed to extract relations from a corpus without knowing the relations in advance, but most of them rely on tagging arguments of predefined types. Recently, a new algorithm was proposed to jointly extract relations and their argument semantic classes, taking a set of relation instances extracted by an open IE algorithm as input. However, it cannot handle polysemy of relation phrases and fails to group many similar (“synonymous”) relation instances because of the sparseness of features. In this paper, we present a novel unsupervised algorithm that provides a more general treatment of the polysemy and synonymy problems. The algorithm incorporates various knowledge sources which we will show to be very effective for unsupervised extraction. Moreover, it explicitly disambiguates polysemous relation phrases and groups synonymous ones. While maintaining approximately the same precision, the algorithm achieves significant improvement on recall compared to the previous method. It is also very efficient. Experiments on a realworld dataset show that it can handle 14.7 million relation instances and extract a very large set of relations from the web. Ralph Grishman1 Chin-Yew Lin2 2Microsoft Research Asia Beijing, China { shumings cyl } @mi cro s o ft . com , that has many applications in answering factoid questions, building knowledge bases and improving search engine relevance. The web has become a massive potential source of such relations. However, its open nature brings an open-ended set of relation types. To extract these relations, a system should not assume a fixed set of relation types, nor rely on a fixed set of relation argument types. The past decade has seen some promising solutions, unsupervised relation extraction (URE) algorithms that extract relations from a corpus without knowing the relations in advance. However, most algorithms (Hasegawa et al., 2004, Shinyama and Sekine, 2006, Chen et. al, 2005) rely on tagging predefined types of entities as relation arguments, and thus are not well-suited for the open domain. Recently, Kok and Domingos (2008) proposed Semantic Network Extractor (SNE), which generates argument semantic classes and sets of synonymous relation phrases at the same time, thus avoiding the requirement of tagging relation arguments of predefined types. However, SNE has 2 limitations: 1) Following previous URE algorithms, it only uses features from the set of input relation instances for clustering. Empirically we found that it fails to group many relevant relation instances. These features, such as the surface forms of arguments and lexical sequences in between, are very sparse in practice. In contrast, there exist several well-known corpus-level semantic resources that can be automatically derived from a source corpus and are shown to be useful for generating the key elements of a relation: its 2 argument semantic classes and a set of synonymous phrases. For example, semantic classes can be derived from a source corpus with contextual distributional simi1 Introduction Relation extraction aims at discovering semantic larity and web table co-occurrences. The “synonymy” 1 problem for clustering relation instances relations between entities. It is an important task * Work done during an internship at Microsoft Research Asia 1027 LParnogcue agdein Lgesa ornf tihneg, 2 p0a1g2e Jso 1in02t C7–o1n0f3e7re,n Jce ju on Is Elanmdp,ir Kicoarlea M,e 1t2h–o1d4s J iunly N 2a0tu1r2a.l ? Lc a2n0g1u2ag Aes Psorcoicaetsiosin fgo arn Cdo Cmopmutpauti oantiaoln Lailn Ngautiustriacls could potentially be better solved by adding these resources. 2) SNE assumes that each entity or relation phrase belongs to exactly one cluster, thus is not able to effectively handle polysemy of relation phrases2. An example of a polysemous phrase is be the currency of as in 2 triples</p><p>6 0.14326216 <a title="136-tfidf-6" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>7 0.11723527 <a title="136-tfidf-7" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>8 0.11530524 <a title="136-tfidf-8" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>9 0.11111895 <a title="136-tfidf-9" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>10 0.10226524 <a title="136-tfidf-10" href="./emnlp-2012-Improving_NLP_through_Marginalization_of_Hidden_Syntactic_Structure.html">65 emnlp-2012-Improving NLP through Marginalization of Hidden Syntactic Structure</a></p>
<p>11 0.1008783 <a title="136-tfidf-11" href="./emnlp-2012-Open_Language_Learning_for_Information_Extraction.html">100 emnlp-2012-Open Language Learning for Information Extraction</a></p>
<p>12 0.10059507 <a title="136-tfidf-12" href="./emnlp-2012-Concurrent_Acquisition_of_Word_Meaning_and_Lexical_Categories.html">29 emnlp-2012-Concurrent Acquisition of Word Meaning and Lexical Categories</a></p>
<p>13 0.099351391 <a title="136-tfidf-13" href="./emnlp-2012-PATTY%3A_A_Taxonomy_of_Relational_Patterns_with_Semantic_Types.html">103 emnlp-2012-PATTY: A Taxonomy of Relational Patterns with Semantic Types</a></p>
<p>14 0.094448969 <a title="136-tfidf-14" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>15 0.093275324 <a title="136-tfidf-15" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>16 0.093007721 <a title="136-tfidf-16" href="./emnlp-2012-Identifying_Constant_and_Unique_Relations_by_using_Time-Series_Text.html">62 emnlp-2012-Identifying Constant and Unique Relations by using Time-Series Text</a></p>
<p>17 0.090291396 <a title="136-tfidf-17" href="./emnlp-2012-Unsupervised_PCFG_Induction_for_Grounded_Language_Learning_with_Highly_Ambiguous_Supervision.html">133 emnlp-2012-Unsupervised PCFG Induction for Grounded Language Learning with Highly Ambiguous Supervision</a></p>
<p>18 0.089273207 <a title="136-tfidf-18" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>19 0.086896285 <a title="136-tfidf-19" href="./emnlp-2012-Generalized_Higher-Order_Dependency_Parsing_with_Cube_Pruning.html">57 emnlp-2012-Generalized Higher-Order Dependency Parsing with Cube Pruning</a></p>
<p>20 0.08627478 <a title="136-tfidf-20" href="./emnlp-2012-Training_Factored_PCFGs_with_Expectation_Propagation.html">126 emnlp-2012-Training Factored PCFGs with Expectation Propagation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.318), (1, 0.114), (2, 0.061), (3, -0.12), (4, 0.047), (5, -0.032), (6, 0.108), (7, 0.264), (8, -0.195), (9, 0.129), (10, -0.095), (11, 0.117), (12, -0.03), (13, 0.011), (14, -0.114), (15, 0.046), (16, -0.128), (17, 0.084), (18, -0.074), (19, -0.05), (20, 0.006), (21, -0.03), (22, -0.103), (23, -0.04), (24, -0.129), (25, -0.045), (26, -0.011), (27, -0.009), (28, 0.046), (29, 0.024), (30, -0.035), (31, 0.082), (32, 0.033), (33, -0.077), (34, -0.126), (35, 0.066), (36, -0.052), (37, -0.02), (38, 0.178), (39, -0.073), (40, -0.017), (41, 0.002), (42, -0.063), (43, -0.02), (44, 0.112), (45, -0.024), (46, 0.104), (47, 0.053), (48, 0.047), (49, -0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97431505 <a title="136-lsi-1" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms ofweak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.</p><p>2 0.72203505 <a title="136-lsi-2" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>Author: Mihai Surdeanu ; Julie Tibshirani ; Ramesh Nallapati ; Christopher D. Manning</p><p>Abstract: Distant supervision for relation extraction (RE) gathering training data by aligning a database of facts with text – is an efficient approach to scale RE to thousands of different relations. However, this introduces a challenging learning scenario where the relation expressed by a pair of entities found in a sentence is unknown. For example, a sentence containing Balzac and France may express BornIn or Died, an unknown relation, or no relation at all. Because of this, traditional supervised learning, which assumes that each example is explicitly mapped to a label, is not appropriate. We propose a novel approach to multi-instance multi-label learning for RE, which jointly models all the instances of a pair of entities in text and all their labels using a graphical model with latent variables. Our model performs competitively on two difficult domains. –</p><p>3 0.64522904 <a title="136-lsi-3" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>Author: Ni Lao ; Amarnag Subramanya ; Fernando Pereira ; William W. Cohen</p><p>Abstract: We study how to extend a large knowledge base (Freebase) by reading relational information from a large Web text corpus. Previous studies on extracting relational knowledge from text show the potential of syntactic patterns for extraction, but they do not exploit background knowledge of other relations in the knowledge base. We describe a distributed, Web-scale implementation of a path-constrained random walk model that learns syntactic-semantic inference rules for binary relations from a graph representation of the parsed text and the knowledge base. Experiments show significant accuracy improvements in binary relation prediction over methods that consider only text, or only the existing knowledge base.</p><p>4 0.63099146 <a title="136-lsi-4" href="./emnlp-2012-Ensemble_Semantics_for_Large-scale_Unsupervised_Relation_Extraction.html">40 emnlp-2012-Ensemble Semantics for Large-scale Unsupervised Relation Extraction</a></p>
<p>Author: Bonan Min ; Shuming Shi ; Ralph Grishman ; Chin-Yew Lin</p><p>Abstract: Discovering significant types of relations from the web is challenging because of its open nature. Unsupervised algorithms are developed to extract relations from a corpus without knowing the relations in advance, but most of them rely on tagging arguments of predefined types. Recently, a new algorithm was proposed to jointly extract relations and their argument semantic classes, taking a set of relation instances extracted by an open IE algorithm as input. However, it cannot handle polysemy of relation phrases and fails to group many similar (“synonymous”) relation instances because of the sparseness of features. In this paper, we present a novel unsupervised algorithm that provides a more general treatment of the polysemy and synonymy problems. The algorithm incorporates various knowledge sources which we will show to be very effective for unsupervised extraction. Moreover, it explicitly disambiguates polysemous relation phrases and groups synonymous ones. While maintaining approximately the same precision, the algorithm achieves significant improvement on recall compared to the previous method. It is also very efficient. Experiments on a realworld dataset show that it can handle 14.7 million relation instances and extract a very large set of relations from the web. Ralph Grishman1 Chin-Yew Lin2 2Microsoft Research Asia Beijing, China { shumings cyl } @mi cro s o ft . com , that has many applications in answering factoid questions, building knowledge bases and improving search engine relevance. The web has become a massive potential source of such relations. However, its open nature brings an open-ended set of relation types. To extract these relations, a system should not assume a fixed set of relation types, nor rely on a fixed set of relation argument types. The past decade has seen some promising solutions, unsupervised relation extraction (URE) algorithms that extract relations from a corpus without knowing the relations in advance. However, most algorithms (Hasegawa et al., 2004, Shinyama and Sekine, 2006, Chen et. al, 2005) rely on tagging predefined types of entities as relation arguments, and thus are not well-suited for the open domain. Recently, Kok and Domingos (2008) proposed Semantic Network Extractor (SNE), which generates argument semantic classes and sets of synonymous relation phrases at the same time, thus avoiding the requirement of tagging relation arguments of predefined types. However, SNE has 2 limitations: 1) Following previous URE algorithms, it only uses features from the set of input relation instances for clustering. Empirically we found that it fails to group many relevant relation instances. These features, such as the surface forms of arguments and lexical sequences in between, are very sparse in practice. In contrast, there exist several well-known corpus-level semantic resources that can be automatically derived from a source corpus and are shown to be useful for generating the key elements of a relation: its 2 argument semantic classes and a set of synonymous phrases. For example, semantic classes can be derived from a source corpus with contextual distributional simi1 Introduction Relation extraction aims at discovering semantic larity and web table co-occurrences. The “synonymy” 1 problem for clustering relation instances relations between entities. It is an important task * Work done during an internship at Microsoft Research Asia 1027 LParnogcue agdein Lgesa ornf tihneg, 2 p0a1g2e Jso 1in02t C7–o1n0f3e7re,n Jce ju on Is Elanmdp,ir Kicoarlea M,e 1t2h–o1d4s J iunly N 2a0tu1r2a.l ? Lc a2n0g1u2ag Aes Psorcoicaetsiosin fgo arn Cdo Cmopmutpauti oantiaoln Lailn Ngautiustriacls could potentially be better solved by adding these resources. 2) SNE assumes that each entity or relation phrase belongs to exactly one cluster, thus is not able to effectively handle polysemy of relation phrases2. An example of a polysemous phrase is be the currency of as in 2 triples</p><p>5 0.60071349 <a title="136-lsi-5" href="./emnlp-2012-Open_Language_Learning_for_Information_Extraction.html">100 emnlp-2012-Open Language Learning for Information Extraction</a></p>
<p>Author: Mausam ; Michael Schmitz ; Stephen Soderland ; Robert Bart ; Oren Etzioni</p><p>Abstract: Open Information Extraction (IE) systems extract relational tuples from text, without requiring a pre-specified vocabulary, by identifying relation phrases and associated arguments in arbitrary sentences. However, stateof-the-art Open IE systems such as REVERB and WOE share two important weaknesses (1) they extract only relations that are mediated by verbs, and (2) they ignore context, thus extracting tuples that are not asserted as factual. This paper presents OLLIE, a substantially improved Open IE system that addresses both these limitations. First, OLLIE achieves high yield by extracting relations mediated by nouns, adjectives, and more. Second, a context-analysis step increases precision by including contextual information from the sentence in the extractions. OLLIE obtains 2.7 times the area under precision-yield curve (AUC) compared to REVERB and 1.9 times the AUC of WOEparse. –</p><p>6 0.53043902 <a title="136-lsi-6" href="./emnlp-2012-Identifying_Constant_and_Unique_Relations_by_using_Time-Series_Text.html">62 emnlp-2012-Identifying Constant and Unique Relations by using Time-Series Text</a></p>
<p>7 0.51106524 <a title="136-lsi-7" href="./emnlp-2012-Unsupervised_PCFG_Induction_for_Grounded_Language_Learning_with_Highly_Ambiguous_Supervision.html">133 emnlp-2012-Unsupervised PCFG Induction for Grounded Language Learning with Highly Ambiguous Supervision</a></p>
<p>8 0.50752372 <a title="136-lsi-8" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>9 0.43241379 <a title="136-lsi-9" href="./emnlp-2012-Concurrent_Acquisition_of_Word_Meaning_and_Lexical_Categories.html">29 emnlp-2012-Concurrent Acquisition of Word Meaning and Lexical Categories</a></p>
<p>10 0.43206447 <a title="136-lsi-10" href="./emnlp-2012-A_Statistical_Relational_Learning_Approach_to_Identifying_Evidence_Based_Medicine_Categories.html">10 emnlp-2012-A Statistical Relational Learning Approach to Identifying Evidence Based Medicine Categories</a></p>
<p>11 0.42974079 <a title="136-lsi-11" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>12 0.4285886 <a title="136-lsi-12" href="./emnlp-2012-Linking_Named_Entities_to_Any_Database.html">84 emnlp-2012-Linking Named Entities to Any Database</a></p>
<p>13 0.42749003 <a title="136-lsi-13" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>14 0.42639765 <a title="136-lsi-14" href="./emnlp-2012-PATTY%3A_A_Taxonomy_of_Relational_Patterns_with_Semantic_Types.html">103 emnlp-2012-PATTY: A Taxonomy of Relational Patterns with Semantic Types</a></p>
<p>15 0.40613255 <a title="136-lsi-15" href="./emnlp-2012-A_Novel_Discriminative_Framework_for_Sentence-Level_Discourse_Analysis.html">7 emnlp-2012-A Novel Discriminative Framework for Sentence-Level Discourse Analysis</a></p>
<p>16 0.39960703 <a title="136-lsi-16" href="./emnlp-2012-Improving_NLP_through_Marginalization_of_Hidden_Syntactic_Structure.html">65 emnlp-2012-Improving NLP through Marginalization of Hidden Syntactic Structure</a></p>
<p>17 0.3937245 <a title="136-lsi-17" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>18 0.36688951 <a title="136-lsi-18" href="./emnlp-2012-Building_a_Lightweight_Semantic_Model_for_Unsupervised_Information_Extraction_on_Short_Listings.html">26 emnlp-2012-Building a Lightweight Semantic Model for Unsupervised Information Extraction on Short Listings</a></p>
<p>19 0.35147953 <a title="136-lsi-19" href="./emnlp-2012-Local_and_Global_Context_for_Supervised_and_Unsupervised_Metonymy_Resolution.html">85 emnlp-2012-Local and Global Context for Supervised and Unsupervised Metonymy Resolution</a></p>
<p>20 0.34416312 <a title="136-lsi-20" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.026), (16, 0.046), (18, 0.024), (25, 0.037), (29, 0.07), (34, 0.063), (45, 0.02), (50, 0.094), (60, 0.1), (63, 0.072), (64, 0.028), (65, 0.052), (70, 0.027), (73, 0.013), (74, 0.084), (76, 0.074), (80, 0.024), (86, 0.015), (94, 0.016), (95, 0.032), (98, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90407264 <a title="136-lda-1" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms ofweak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.</p><p>2 0.84552258 <a title="136-lda-2" href="./emnlp-2012-Source_Language_Adaptation_for_Resource-Poor_Machine_Translation.html">118 emnlp-2012-Source Language Adaptation for Resource-Poor Machine Translation</a></p>
<p>Author: Pidong Wang ; Preslav Nakov ; Hwee Tou Ng</p><p>Abstract: We propose a novel, language-independent approach for improving machine translation from a resource-poor language to X by adapting a large bi-text for a related resource-rich language and X (the same target language). We assume a small bi-text for the resourcepoor language to X pair, which we use to learn word-level and phrase-level paraphrases and cross-lingual morphological variants between the resource-rich and the resource-poor language; we then adapt the former to get closer to the latter. Our experiments for Indonesian/Malay–English translation show that using the large adapted resource-rich bitext yields 6.7 BLEU points of improvement over the unadapted one and 2.6 BLEU points over the original small bi-text. Moreover, combining the small bi-text with the adapted bi-text outperforms the corresponding combinations with the unadapted bi-text by 1.5– 3 BLEU points. We also demonstrate applicability to other languages and domains.</p><p>3 0.83608389 <a title="136-lda-3" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>Author: Jonathan K. Kummerfeld ; David Hall ; James R. Curran ; Dan Klein</p><p>Abstract: Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors. We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for stateof-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.</p><p>4 0.79081351 <a title="136-lda-4" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: We present a new family of models for unsupervised parsing, Dependency and Boundary models, that use cues at constituent boundaries to inform head-outward dependency tree generation. We build on three intuitions that are explicit in phrase-structure grammars but only implicit in standard dependency formulations: (i) Distributions of words that occur at sentence boundaries such as English determiners resemble constituent edges. (ii) Punctuation at sentence boundaries further helps distinguish full sentences from fragments like headlines and titles, allowing us to model grammatical differences between complete and incomplete sentences. (iii) Sentence-internal punctuation boundaries help with longer-distance dependencies, since punctuation correlates with constituent edges. Our models induce state-of-the-art dependency grammars for many languages without — — special knowledge of optimal input sentence lengths or biased, manually-tuned initializers.</p><p>5 0.77681142 <a title="136-lda-5" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>Author: Lizhen Qu ; Rainer Gemulla ; Gerhard Weikum</p><p>Abstract: We propose the weakly supervised MultiExperts Model (MEM) for analyzing the semantic orientation of opinions expressed in natural language reviews. In contrast to most prior work, MEM predicts both opinion polarity and opinion strength at the level of individual sentences; such fine-grained analysis helps to understand better why users like or dislike the entity under review. A key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength. For this reason, MEM is weakly supervised: It starts with potentially noisy indicators obtained from coarse-grained training data (i.e., document-level ratings), a small set of diverse base predictors, and, if available, small amounts of fine-grained training data. We integrate these noisy indicators into a unified probabilistic framework using ideas from ensemble learning and graph-based semi-supervised learning. Our experiments indicate that MEM outperforms state-of-the-art methods by a significant margin.</p><p>6 0.77368104 <a title="136-lda-6" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>7 0.77192521 <a title="136-lda-7" href="./emnlp-2012-Extracting_Opinion_Expressions_with_semi-Markov_Conditional_Random_Fields.html">51 emnlp-2012-Extracting Opinion Expressions with semi-Markov Conditional Random Fields</a></p>
<p>8 0.77157742 <a title="136-lda-8" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>9 0.76792485 <a title="136-lda-9" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>10 0.76472664 <a title="136-lda-10" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>11 0.76256835 <a title="136-lda-11" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>12 0.75684005 <a title="136-lda-12" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>13 0.75438982 <a title="136-lda-13" href="./emnlp-2012-Unambiguity_Regularization_for_Unsupervised_Learning_of_Probabilistic_Grammars.html">130 emnlp-2012-Unambiguity Regularization for Unsupervised Learning of Probabilistic Grammars</a></p>
<p>14 0.75352561 <a title="136-lda-14" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>15 0.75322574 <a title="136-lda-15" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>16 0.75102097 <a title="136-lda-16" href="./emnlp-2012-Syntactic_Surprisal_Affects_Spoken_Word_Duration_in_Conversational_Contexts.html">122 emnlp-2012-Syntactic Surprisal Affects Spoken Word Duration in Conversational Contexts</a></p>
<p>17 0.75063694 <a title="136-lda-17" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>18 0.74948227 <a title="136-lda-18" href="./emnlp-2012-PATTY%3A_A_Taxonomy_of_Relational_Patterns_with_Semantic_Types.html">103 emnlp-2012-PATTY: A Taxonomy of Relational Patterns with Semantic Types</a></p>
<p>19 0.74774557 <a title="136-lda-19" href="./emnlp-2012-Besting_the_Quiz_Master%3A_Crowdsourcing_Incremental_Classification_Games.html">23 emnlp-2012-Besting the Quiz Master: Crowdsourcing Incremental Classification Games</a></p>
<p>20 0.74576759 <a title="136-lda-20" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
