<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>81 emnlp-2012-Learning to Map into a Universal POS Tagset</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-81" href="#">emnlp2012-81</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>81 emnlp-2012-Learning to Map into a Universal POS Tagset</h1>
<br/><p>Source: <a title="emnlp-2012-81-pdf" href="http://aclweb.org/anthology//D/D12/D12-1125.pdf">pdf</a></p><p>Author: Yuan Zhang ; Roi Reichart ; Regina Barzilay ; Amir Globerson</p><p>Abstract: We present an automatic method for mapping language-specific part-of-speech tags to a set of universal tags. This unified representation plays a crucial role in cross-lingual syntactic transfer of multilingual dependency parsers. Until now, however, such conversion schemes have been created manually. Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consistent across source and target languages. We encode this intuition in an objective function that captures a range of distributional and typological characteristics of the derived mapping. Given the exponential size of the mapping space, we propose a novel method for optimizing over soft mappings, and use entropy regularization to drive those towards hard mappings. Our results demonstrate that automatically induced mappings rival the quality of their manually designed counterparts when evaluated in the . context of multilingual parsing.1</p><p>Reference: <a title="emnlp-2012-81-reference" href="../emnlp2012_reference/emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consistent across source and target languages. [sent-4, score-0.672]
</p><p>2 We encode this intuition in an objective function that captures a range of distributional and typological characteristics of the derived mapping. [sent-5, score-0.397]
</p><p>3 Given the exponential size of the mapping space, we propose a novel method for optimizing over soft mappings, and use entropy regularization to drive those towards hard mappings. [sent-6, score-0.393]
</p><p>4 Our results demonstrate that automatically induced mappings rival the quality of their manually designed counterparts when evaluated in the  . [sent-7, score-0.473]
</p><p>5 1 1 Introduction In this paper, we explore an automatic method for mapping language-specific part-of-speech tags to a universal tagset. [sent-9, score-1.009]
</p><p>6 Specifically, the universal tagset annotations enable an unlexicalized parser to capitalize on annotations from one language when learning a model for another. [sent-11, score-0.681]
</p><p>7 i l While the notion of a universal POS tagset is widely accepted, in practice it is hardly ever used for annotation of monolingual resources. [sent-19, score-0.542]
</p><p>8 In fact, available POS annotations are designed to capture language-specific idiosyncrasies and therefore are substantially more detailed than a coarse universal tagset. [sent-20, score-0.532]
</p><p>9 To reconcile these cross-lingual annotation  differences, a number of mapping schemes have been proposed in the parsing community (Zeman and Resnik, 2008; Petrov et al. [sent-21, score-0.465]
</p><p>10 In fact, the universal tagsets manually induced by Petrov et al. [sent-26, score-0.599]
</p><p>11 An example of such discrepancy is the mapping of the Japanese tag “PVfin” to the universal tag “particle” according to one scheme, and to “verb” according to another. [sent-29, score-1.036]
</p><p>12 In the Japanese example above, this difference in mapping yields a 6. [sent-31, score-0.329]
</p><p>13 The goal of our work is to induce the mapping for a new language, utilizing existing manuallyconstructed mappings as training data. [sent-33, score-0.616]
</p><p>14 The existing mappings developed in the parsing community rely on gold POS tags for the target language. [sent-34, score-0.689]
</p><p>15 A more realistic scenario is to employ the mapping  technique to resource-poor languages where gold POS annotations are lacking. [sent-35, score-0.478]
</p><p>16 , using the Brown algorithm) and convert them to universal tags. [sent-40, score-0.484]
</p><p>17 We are interested in a mapping approach that can effectively handle both gold tags and induced clusters. [sent-41, score-0.609]
</p><p>18 Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consistent across languages. [sent-42, score-0.448]
</p><p>19 Since universal tags play the same linguistic role in source and target languages, we expect similarity in their global distributional statistics. [sent-43, score-1.043]
</p><p>20 Finally, the mappings can be further constrained by typological properties of the target language that specify likely tag sequences. [sent-49, score-0.729]
</p><p>21 We construct the objective in a way that facilitates simple monotonically improving updates corresponding to solving convex optimization problems. [sent-58, score-0.381]
</p><p>22 We evaluate our mapping approach on 19 languages that include representatives of IndoEuropean, Semitic, Basque, Japonic and Turkic families. [sent-59, score-0.43]
</p><p>23 We measure mapping quality based on the target language parsing accuracy. [sent-60, score-0.565]
</p><p>24 In addition to  considering gold POS tags for the target language, 2Instances of related hard problems subset-sum. [sent-61, score-0.333]
</p><p>25 are  3-partition and  1369 we also evaluate the mapping algorithm on automatically induced POS tags. [sent-62, score-0.382]
</p><p>26 We further show that while all characteristics of the mapping contribute to the objective, our largest gain comes from distributional features that capture global statistics. [sent-65, score-0.358]
</p><p>27 Finally, we establish that the mapping quality has a significant impact on the accuracy of syntactic transfer, which motivates further study of this topic. [sent-66, score-0.384]
</p><p>28 In this setup, finding the mapping between various POS annotation schemes was not essential; instead, the transfer algorithm could induce it directly from the parallel data (Hwa et al. [sent-68, score-0.509]
</p><p>29 These approaches assume access to a common input representation in the form of universal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language. [sent-74, score-0.74]
</p><p>30 Despite ongoing efforts to standardize POS tags across languages (e. [sent-75, score-0.359]
</p><p>31 In previous work, their mapping to universal tags was performed manually. [sent-78, score-1.009]
</p><p>32 First, these methods have been developed in the monolingual setting, while our mapping algorithm is designed for multilingual parsing. [sent-96, score-0.356]
</p><p>33 3  Task Formulation  The input to our task consists of a target corpus written in a language T, and a set of non-parallel source corpora written in languages {S1, . [sent-100, score-0.356]
</p><p>34 th I n b tohthe a language-specific POS tag and a universal POS tag (Petrov et al. [sent-105, score-0.738]
</p><p>35 Our goal is to find a map from the set of LT target language tags to the set of K universal tags. [sent-108, score-0.916]
</p><p>36 We as1370 sume that each language-specific tag is only mapped  ×  to one universal tag, which means we never split a language-specific tag and LT ≥ K holds for every language. [sent-109, score-0.738]
</p><p>37 We represent the map by a mdsa ftroirx Ave royf size K LT where A(c|f) = 1 if the target language tag f isw mapped ct|of t)he = u n1iv iefr tshael tag c, laanndA(c| f) = 0 otherwise. [sent-110, score-0.423]
</p><p>38 A candirdealtaex mapping iAre can btteh applied fto) t ∈he { target language to produce sentences labeled with universal tags. [sent-113, score-0.888]
</p><p>39 Our key insight is that for a good mapping, the statistics over the universal tags should be similar for source and target languages because these tags play the same role cross-linguistically. [sent-115, score-1.336]
</p><p>40 For example, we should expect the frequency of a particular universal tag to be similar in the source and target languages. [sent-116, score-0.883]
</p><p>41 One choice to make when constructing an objective is the source languages to which we want to be similar. [sent-117, score-0.375]
</p><p>42 It is clear that choosing all languages is not a good idea, since they are not all expected to have dis-  tributional properties similar to the target language. [sent-118, score-0.316]
</p><p>43 There is strong evidence that projecting from single languages can lead to good parsing performance 3We use c and f to reflect the fact that universal tags are a coarse version (hence c) of the language specific fine tags (hence f). [sent-119, score-1.253]
</p><p>44 The choice of the source language is based on similarity between typological properties; we describe this in detail in Section 5. [sent-123, score-0.33]
</p><p>45 Our model utilizes three linguistic phenomena which are consistent across languages: POS tag global distributional statistics, POS tag per sentence statistics, and typology-based ordering statistics. [sent-125, score-0.344]
</p><p>46 Global distributional statistics: The unigram and  bigram statistics of the universal tags are expected to be similar across languages with close typological profiles. [sent-130, score-1.221]
</p><p>47 We use pS(c1 , c2) to denote the bigram distribution over universal tags in the source language, andpT(f1 , f2) to denote the bigram distribution over language specific tags in the target language. [sent-131, score-1.358]
</p><p>48 4We use the KL divergence because it assigns low weights to infrequent universal tags. [sent-135, score-0.484]
</p><p>49 , the universal tags corresponding to verbs according to A) in sentence s. [sent-139, score-0.745]
</p><p>50 If we know that the target language has a particular typological feature, we expect its universal tags to obey the given relative ordering. [sent-147, score-1.077]
</p><p>51 Specifically, we expect it to agree with ordering statistics for source languages with a similar typology. [sent-148, score-0.34]
</p><p>52 Formally, we define C1 = {noun, adj, num, pron, det} and consider the set of bigram dadijs,tri nbuumti,on psro Spre tt}ha atn satisfy tdheer following cboignrsatmrai ndti:s  X pT(adp,c) ≥ apre  (3)  cX∈C1  where apre the source convex in gram term. [sent-154, score-0.409]
</p><p>53 (The weights in this equation are Plearned; we discussed the procedure in Section 5) Optimizing over the set of mappings is difficult since each mapping is a discrete set whose size is exponential size in LT. [sent-175, score-0.541]
</p><p>54 This objective is non convex since the function H[A] is concave, and the objective F(A) involves bilinear terms in A and logarithms of their sums (see Equations (1) and (2)). [sent-187, score-0.387]
</p><p>55 In step 3, we calculate the bigram posterior over language specific tags given a pair ostfe eurinoirversal tags. [sent-202, score-0.33]
</p><p>56 In step 4, we use the posterior in step 3 and the bigram 4d,i wsteri ubusteio thnes pS (c1, c2) a sntdep r 3k (c1, c2) to obtain joint counts over language specific and universal bigrams. [sent-204, score-0.587]
</p><p>57 In step 5, we use the joint counts from step 4 Iton o stbetapin 5 ,co wuent uss over pairs to cfo language specific and universal tags. [sent-205, score-0.484]
</p><p>58 To turn it into a hard mapping we round A by mapping each f to the c that maximizes A(c|f) and then perform greedy improvement steps (one f aant a time) teorf fourmrthe grr improve the objective. [sent-213, score-0.676]
</p><p>59 Assume that languages are described by binary typological vectors vL. [sent-246, score-0.344]
</p><p>60 (201 1), our model maps these language-specific tags to a set of 12 universal tags: noun, verb, adjective, adverb, pronoun, determiner, adposition, numeral, conjunction, particle, punctuation mark and X (a general tag). [sent-259, score-0.744]
</p><p>61 1374 Evaluation Procedure We perform a separate experiment for each of the 19 languages as the target and a source language chosen from the rest (using the method from Section 5. [sent-261, score-0.356]
</p><p>62 For the selected source language, we assume access to the mapping of Petrov et al. [sent-263, score-0.416]
</p><p>63 Evaluation Measures We evaluate the quality of the derived mapping in the context of the target language parsing accuracy. [sent-265, score-0.565]
</p><p>64 In both the training and test data, the language-specific tags are replaced with universal tags: Petrov’s tags for the source languages and learned tags for the target language. [sent-266, score-1.521]
</p><p>65 We train two non-lexicalized parsers using source annotations and apply them to the target language. [sent-267, score-0.31]
</p><p>66 In the second parser, parameters of the target language are estimated as a weighted mixture of parameters learned from supervised source languages (Cohen et  al. [sent-271, score-0.356]
</p><p>67 —  Baselines We compare mappings induced by our model against three baselines: the manually constructed mapping of Petrov et al. [sent-277, score-0.656]
</p><p>68 (201 1), a randomly constructed mapping and a greedy mapping. [sent-278, score-0.378]
</p><p>69 The greedy mapping uses the same objective as our full model, but optimizes it using a greedy method. [sent-279, score-0.583]
</p><p>70 Initialization To reduce the dimension of our algorithm’s search space and speed up our method, we start by clustering the language-specific POS tags of the target into |K| = 12 clusters using an unsuper-  10We also experimented with a version of the Cohen et al. [sent-281, score-0.39]
</p><p>71 , Our mapping algorithm then learns the connection between these clusters and universal tags. [sent-290, score-0.839]
</p><p>72 7  Results  We first present the results of our model using the gold POS tags for the target language. [sent-292, score-0.333]
</p><p>73 Comparison against Baselines On average, the mapping produced by our model yields parsers with higher accuracy than all of the baselines. [sent-294, score-0.405]
</p><p>74 The low accuracy of parsers that rely on the Greedy mapping 29. [sent-301, score-0.374]
</p><p>75 4% show that a greedy approach is a poor strategy for mapping optimization. [sent-303, score-0.378]
</p><p>76 1375  Overall, the manually constructed mapping and our model’s output disagree on 21% of the assignments (measured on the token level). [sent-315, score-0.387]
</p><p>77 For instance, the manual and automatic mappings for Catalan disagree on 8% of the tags and their parsing accuracy differs by 5%. [sent-317, score-0.679]
</p><p>78 For Greek on the other hand, the disagreement between mappings is much higher 17%, yet the parsing accuracy is very close. [sent-318, score-0.394]
</p><p>79 Figure 3 shows the objective values for the mappings computed by our method and the baselines for four languages. [sent-330, score-0.368]
</p><p>80 Table 2: Directed dependency accuracy of our model and the baselines using gold POS tags for the target language. [sent-333, score-0.371]
</p><p>81 The third column (Petrov) shows the results when the mapping of Petrov et al. [sent-339, score-0.344]
</p><p>82 The fourth column (Model) shows the results when our mapping is used and the fifth column in the first section (Best Pair) shows the performance of our model when the best source language is selected for every target language. [sent-341, score-0.614]
</p><p>83 ) presents the difference between our mapping and the mapping of Petrov et al. [sent-343, score-0.596]
</p><p>84 (2011) by showing the percentage of target language tokens for which the  two mappings select a different universal tag. [sent-344, score-0.833]
</p><p>85 For instance, on Greek our mapping has a better objective value, but lower parsing performance. [sent-347, score-0.536]
</p><p>86 This is not surprising the source language is selected to be typologically similar to the target language, and thus its distributional properties are consistent with typological features. [sent-359, score-0.567]
</p><p>87 —  Application to Automatically Induced POS Tags A potential benefit of the proposed method is to relate automatically induced clusters in the target language to universal tags. [sent-362, score-0.731]
</p><p>88 We then map these clusters to the universal tags using our algorithm. [sent-372, score-0.831]
</p><p>89 Not surprisingly, automatically induced tags negatively impact parsing performance, yielding a decrease of 11% when compared to mappings obtained using manual POS annotations (see Ta-  ble 2). [sent-375, score-0.715]
</p><p>90 To further investigate the impact of inaccurate tags on the mapping performance, we compare our model against the oracle mapping model that maps each cluster to the most common universal tag of its members. [sent-376, score-1.467]
</p><p>91 An alternative approach to mapping words into universal tags is to directly partition words into K clusters (without passing through language specific tags). [sent-379, score-1.066]
</p><p>92 In order for these clusters to be meaningful as universal tags, we can provide several prototypes for each cluster (e. [sent-380, score-0.611]
</p><p>93 To test this approach we used the prototype driven tagger of Haghighi and Klein (2006) with 15 prototypes per universal tag. [sent-384, score-0.554]
</p><p>94 14 The resulting universal tags yield an average parsing accuracy of 40. [sent-385, score-0.862]
</p><p>95 Our method (using Brown clustering as above) outperforms this  14Oracle prototypes  were obtained by taking the 15 most frequent words for each universal tag. [sent-387, score-0.554]
</p><p>96 8  Conclusions  We present an automatic method for mapping language-specific part-of-speech tags to a set of universal tags. [sent-390, score-1.009]
</p><p>97 Our work capitalizes on manually designed conversion schemes to automatically create mappings for new languages. [sent-391, score-0.376]
</p><p>98 Our experimental results demonstrate that automatically induced mappings rival the quality of their hand-crafted counterparts. [sent-392, score-0.41]
</p><p>99 We also establish that the mapping quality has a significant impact on the accuracy of syntactic transfer, which motivates further study of this topic. [sent-393, score-0.384]
</p><p>100 Finally, our experiments show that the choice of mapping optimization scheme plays a crucial role in the quality of the derived mapping, highlighting the importance of optimization for the mapping task. [sent-394, score-0.852]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('universal', 0.484), ('mapping', 0.298), ('mappings', 0.243), ('tags', 0.227), ('typological', 0.212), ('petrov', 0.163), ('convex', 0.137), ('languages', 0.132), ('tag', 0.127), ('objective', 0.125), ('source', 0.118), ('transfer', 0.118), ('pt', 0.115), ('parsing', 0.113), ('pos', 0.111), ('target', 0.106), ('zeman', 0.09), ('induced', 0.084), ('mcdonald', 0.082), ('nv', 0.081), ('greedy', 0.08), ('si', 0.075), ('optimization', 0.071), ('projecting', 0.07), ('prototypes', 0.07), ('naseem', 0.068), ('bigram', 0.064), ('map', 0.063), ('regularization', 0.063), ('distributional', 0.06), ('tagset', 0.058), ('disagree', 0.058), ('adposition', 0.058), ('multilingual', 0.058), ('clusters', 0.057), ('cohen', 0.057), ('schemes', 0.054), ('jective', 0.052), ('haspelmath', 0.052), ('ps', 0.052), ('monotonically', 0.048), ('greek', 0.048), ('annotations', 0.048), ('expect', 0.048), ('conversion', 0.048), ('resnik', 0.048), ('quality', 0.048), ('lt', 0.046), ('column', 0.046), ('slav', 0.045), ('fi', 0.045), ('adet', 0.045), ('apre', 0.045), ('boyd', 0.045), ('cvx', 0.045), ('esv', 0.045), ('garey', 0.045), ('vsv', 0.045), ('yuille', 0.045), ('parser', 0.043), ('hwa', 0.043), ('statistics', 0.042), ('properties', 0.041), ('pronoun', 0.04), ('svm', 0.04), ('posterior', 0.039), ('induce', 0.039), ('fverb', 0.038), ('basque', 0.038), ('concave', 0.038), ('accuracy', 0.038), ('parsers', 0.038), ('choosing', 0.037), ('regina', 0.037), ('haghighi', 0.037), ('conll', 0.037), ('goal', 0.036), ('klein', 0.036), ('buchholz', 0.036), ('rival', 0.035), ('highlighting', 0.035), ('atlas', 0.035), ('denote', 0.034), ('verbs', 0.034), ('maps', 0.033), ('objectives', 0.032), ('counterparts', 0.032), ('yuan', 0.032), ('entropy', 0.032), ('noun', 0.032), ('requirement', 0.031), ('plays', 0.031), ('manually', 0.031), ('yields', 0.031), ('burkett', 0.03), ('refinement', 0.03), ('roi', 0.03), ('consistent', 0.03), ('ryan', 0.03), ('em', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="81-tfidf-1" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<p>Author: Yuan Zhang ; Roi Reichart ; Regina Barzilay ; Amir Globerson</p><p>Abstract: We present an automatic method for mapping language-specific part-of-speech tags to a set of universal tags. This unified representation plays a crucial role in cross-lingual syntactic transfer of multilingual dependency parsers. Until now, however, such conversion schemes have been created manually. Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consistent across source and target languages. We encode this intuition in an objective function that captures a range of distributional and typological characteristics of the derived mapping. Given the exponential size of the mapping space, we propose a novel method for optimizing over soft mappings, and use entropy regularization to drive those towards hard mappings. Our results demonstrate that automatically induced mappings rival the quality of their manually designed counterparts when evaluated in the . context of multilingual parsing.1</p><p>2 0.20258649 <a title="81-tfidf-2" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>Author: Greg Durrett ; Adam Pauls ; Dan Klein</p><p>Abstract: We consider the problem of using a bilingual dictionary to transfer lexico-syntactic information from a resource-rich source language to a resource-poor target language. In contrast to past work that used bitexts to transfer analyses of specific sentences at the token level, we instead use features to transfer the behavior of words at a type level. In a discriminative dependency parsing framework, our approach produces gains across a range of target languages, using two different lowresource training methodologies (one weakly supervised and one indirectly supervised) and two different dictionary sources (one manually constructed and one automatically constructed).</p><p>3 0.1665711 <a title="81-tfidf-3" href="./emnlp-2012-Wiki-ly_Supervised_Part-of-Speech_Tagging.html">138 emnlp-2012-Wiki-ly Supervised Part-of-Speech Tagging</a></p>
<p>Author: Shen Li ; Joao Graca ; Ben Taskar</p><p>Abstract: Despite significant recent work, purely unsupervised techniques for part-of-speech (POS) tagging have not achieved useful accuracies required by many language processing tasks. Use of parallel text between resource-rich and resource-poor languages is one source ofweak supervision that significantly improves accuracy. However, parallel text is not always available and techniques for using it require multiple complex algorithmic steps. In this paper we show that we can build POS-taggers exceeding state-of-the-art bilingual methods by using simple hidden Markov models and a freely available and naturally growing resource, the Wiktionary. Across eight languages for which we have labeled data to evaluate results, we achieve accuracy that significantly exceeds best unsupervised and parallel text methods. We achieve highest accuracy reported for several languages and show that our . approach yields better out-of-domain taggers than those trained using fully supervised Penn Treebank.</p><p>4 0.14442816 <a title="81-tfidf-4" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>Author: Bernd Bohnet ; Joakim Nivre</p><p>Abstract: Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins. We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees. Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages.</p><p>5 0.13988687 <a title="81-tfidf-5" href="./emnlp-2012-Type-Supervised_Hidden_Markov_Models_for_Part-of-Speech_Tagging_with_Incomplete_Tag_Dictionaries.html">129 emnlp-2012-Type-Supervised Hidden Markov Models for Part-of-Speech Tagging with Incomplete Tag Dictionaries</a></p>
<p>Author: Dan Garrette ; Jason Baldridge</p><p>Abstract: Past work on learning part-of-speech taggers from tag dictionaries and raw data has reported good results, but the assumptions made about those dictionaries are often unrealistic: due to historical precedents, they assume access to information about labels in the raw and test sets. Here, we demonstrate ways to learn hidden Markov model taggers from incomplete tag dictionaries. Taking the MINGREEDY algorithm (Ravi et al., 2010) as a starting point, we improve it with several intuitive heuristics. We also define a simple HMM emission initialization that takes advantage of the tag dictionary and raw data to capture both the openness of a given tag and its estimated prevalence in the raw data. Altogether, our augmentations produce improvements to per- formance over the original MIN-GREEDY algorithm for both English and Italian data.</p><p>6 0.13096257 <a title="81-tfidf-6" href="./emnlp-2012-Universal_Grapheme-to-Phoneme_Prediction_Over_Latin_Alphabets.html">132 emnlp-2012-Universal Grapheme-to-Phoneme Prediction Over Latin Alphabets</a></p>
<p>7 0.11667065 <a title="81-tfidf-7" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<p>8 0.10967262 <a title="81-tfidf-8" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>9 0.094756156 <a title="81-tfidf-9" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>10 0.094397753 <a title="81-tfidf-10" href="./emnlp-2012-Training_Factored_PCFGs_with_Expectation_Propagation.html">126 emnlp-2012-Training Factored PCFGs with Expectation Propagation</a></p>
<p>11 0.094275318 <a title="81-tfidf-11" href="./emnlp-2012-Part-of-Speech_Tagging_for_Chinese-English_Mixed_Texts_with_Dynamic_Features.html">106 emnlp-2012-Part-of-Speech Tagging for Chinese-English Mixed Texts with Dynamic Features</a></p>
<p>12 0.089655019 <a title="81-tfidf-12" href="./emnlp-2012-Generalized_Higher-Order_Dependency_Parsing_with_Cube_Pruning.html">57 emnlp-2012-Generalized Higher-Order Dependency Parsing with Cube Pruning</a></p>
<p>13 0.089643531 <a title="81-tfidf-13" href="./emnlp-2012-Assessment_of_ESL_Learners%27_Syntactic_Competence_Based_on_Similarity_Measures.html">21 emnlp-2012-Assessment of ESL Learners' Syntactic Competence Based on Similarity Measures</a></p>
<p>14 0.089504138 <a title="81-tfidf-14" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>15 0.089176208 <a title="81-tfidf-15" href="./emnlp-2012-Unambiguity_Regularization_for_Unsupervised_Learning_of_Probabilistic_Grammars.html">130 emnlp-2012-Unambiguity Regularization for Unsupervised Learning of Probabilistic Grammars</a></p>
<p>16 0.086632378 <a title="81-tfidf-16" href="./emnlp-2012-Exploiting_Reducibility_in_Unsupervised_Dependency_Parsing.html">46 emnlp-2012-Exploiting Reducibility in Unsupervised Dependency Parsing</a></p>
<p>17 0.078466214 <a title="81-tfidf-17" href="./emnlp-2012-Learning_Syntactic_Categories_Using_Paradigmatic_Representations_of_Word_Context.html">79 emnlp-2012-Learning Syntactic Categories Using Paradigmatic Representations of Word Context</a></p>
<p>18 0.076966785 <a title="81-tfidf-18" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>19 0.076313555 <a title="81-tfidf-19" href="./emnlp-2012-Concurrent_Acquisition_of_Word_Meaning_and_Lexical_Categories.html">29 emnlp-2012-Concurrent Acquisition of Word Meaning and Lexical Categories</a></p>
<p>20 0.072394595 <a title="81-tfidf-20" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.282), (1, -0.137), (2, 0.164), (3, -0.038), (4, 0.081), (5, 0.097), (6, 0.062), (7, 0.037), (8, 0.096), (9, -0.126), (10, 0.162), (11, 0.039), (12, 0.043), (13, 0.05), (14, -0.208), (15, -0.168), (16, 0.051), (17, -0.02), (18, 0.03), (19, -0.056), (20, 0.118), (21, -0.019), (22, 0.022), (23, 0.05), (24, -0.002), (25, -0.062), (26, -0.018), (27, -0.026), (28, -0.011), (29, 0.043), (30, -0.131), (31, 0.006), (32, 0.078), (33, 0.067), (34, 0.168), (35, -0.008), (36, 0.016), (37, -0.023), (38, 0.034), (39, -0.083), (40, 0.09), (41, -0.052), (42, 0.058), (43, 0.064), (44, 0.096), (45, -0.022), (46, -0.052), (47, -0.062), (48, 0.151), (49, -0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97323233 <a title="81-lsi-1" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<p>Author: Yuan Zhang ; Roi Reichart ; Regina Barzilay ; Amir Globerson</p><p>Abstract: We present an automatic method for mapping language-specific part-of-speech tags to a set of universal tags. This unified representation plays a crucial role in cross-lingual syntactic transfer of multilingual dependency parsers. Until now, however, such conversion schemes have been created manually. Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consistent across source and target languages. We encode this intuition in an objective function that captures a range of distributional and typological characteristics of the derived mapping. Given the exponential size of the mapping space, we propose a novel method for optimizing over soft mappings, and use entropy regularization to drive those towards hard mappings. Our results demonstrate that automatically induced mappings rival the quality of their manually designed counterparts when evaluated in the . context of multilingual parsing.1</p><p>2 0.76464629 <a title="81-lsi-2" href="./emnlp-2012-Wiki-ly_Supervised_Part-of-Speech_Tagging.html">138 emnlp-2012-Wiki-ly Supervised Part-of-Speech Tagging</a></p>
<p>Author: Shen Li ; Joao Graca ; Ben Taskar</p><p>Abstract: Despite significant recent work, purely unsupervised techniques for part-of-speech (POS) tagging have not achieved useful accuracies required by many language processing tasks. Use of parallel text between resource-rich and resource-poor languages is one source ofweak supervision that significantly improves accuracy. However, parallel text is not always available and techniques for using it require multiple complex algorithmic steps. In this paper we show that we can build POS-taggers exceeding state-of-the-art bilingual methods by using simple hidden Markov models and a freely available and naturally growing resource, the Wiktionary. Across eight languages for which we have labeled data to evaluate results, we achieve accuracy that significantly exceeds best unsupervised and parallel text methods. We achieve highest accuracy reported for several languages and show that our . approach yields better out-of-domain taggers than those trained using fully supervised Penn Treebank.</p><p>3 0.72181177 <a title="81-lsi-3" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>Author: Greg Durrett ; Adam Pauls ; Dan Klein</p><p>Abstract: We consider the problem of using a bilingual dictionary to transfer lexico-syntactic information from a resource-rich source language to a resource-poor target language. In contrast to past work that used bitexts to transfer analyses of specific sentences at the token level, we instead use features to transfer the behavior of words at a type level. In a discriminative dependency parsing framework, our approach produces gains across a range of target languages, using two different lowresource training methodologies (one weakly supervised and one indirectly supervised) and two different dictionary sources (one manually constructed and one automatically constructed).</p><p>4 0.58693445 <a title="81-lsi-4" href="./emnlp-2012-Type-Supervised_Hidden_Markov_Models_for_Part-of-Speech_Tagging_with_Incomplete_Tag_Dictionaries.html">129 emnlp-2012-Type-Supervised Hidden Markov Models for Part-of-Speech Tagging with Incomplete Tag Dictionaries</a></p>
<p>Author: Dan Garrette ; Jason Baldridge</p><p>Abstract: Past work on learning part-of-speech taggers from tag dictionaries and raw data has reported good results, but the assumptions made about those dictionaries are often unrealistic: due to historical precedents, they assume access to information about labels in the raw and test sets. Here, we demonstrate ways to learn hidden Markov model taggers from incomplete tag dictionaries. Taking the MINGREEDY algorithm (Ravi et al., 2010) as a starting point, we improve it with several intuitive heuristics. We also define a simple HMM emission initialization that takes advantage of the tag dictionary and raw data to capture both the openness of a given tag and its estimated prevalence in the raw data. Altogether, our augmentations produce improvements to per- formance over the original MIN-GREEDY algorithm for both English and Italian data.</p><p>5 0.52661532 <a title="81-lsi-5" href="./emnlp-2012-Universal_Grapheme-to-Phoneme_Prediction_Over_Latin_Alphabets.html">132 emnlp-2012-Universal Grapheme-to-Phoneme Prediction Over Latin Alphabets</a></p>
<p>Author: Young-Bum Kim ; Benjamin Snyder</p><p>Abstract: We consider the problem of inducing grapheme-to-phoneme mappings for unknown languages written in a Latin alphabet. First, we collect a data-set of 107 languages with known grapheme-phoneme relationships, along with a short text in each language. We then cast our task in the framework of supervised learning, where each known language serves as a training example, and predictions are made on unknown languages. We induce an undirected graphical model that learns phonotactic regularities, thus relating textual patterns to plausible phonemic interpretations across the entire range of languages. Our model correctly predicts grapheme-phoneme pairs with over 88% F1-measure.</p><p>6 0.48565927 <a title="81-lsi-6" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<p>7 0.47504258 <a title="81-lsi-7" href="./emnlp-2012-Exploiting_Reducibility_in_Unsupervised_Dependency_Parsing.html">46 emnlp-2012-Exploiting Reducibility in Unsupervised Dependency Parsing</a></p>
<p>8 0.43295369 <a title="81-lsi-8" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>9 0.42787921 <a title="81-lsi-9" href="./emnlp-2012-Assessment_of_ESL_Learners%27_Syntactic_Competence_Based_on_Similarity_Measures.html">21 emnlp-2012-Assessment of ESL Learners' Syntactic Competence Based on Similarity Measures</a></p>
<p>10 0.41832119 <a title="81-lsi-10" href="./emnlp-2012-Regularized_Interlingual_Projections%3A_Evaluation_on_Multilingual_Transliteration.html">111 emnlp-2012-Regularized Interlingual Projections: Evaluation on Multilingual Transliteration</a></p>
<p>11 0.41366428 <a title="81-lsi-11" href="./emnlp-2012-Learning_Syntactic_Categories_Using_Paradigmatic_Representations_of_Word_Context.html">79 emnlp-2012-Learning Syntactic Categories Using Paradigmatic Representations of Word Context</a></p>
<p>12 0.40966788 <a title="81-lsi-12" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>13 0.39600551 <a title="81-lsi-13" href="./emnlp-2012-Unambiguity_Regularization_for_Unsupervised_Learning_of_Probabilistic_Grammars.html">130 emnlp-2012-Unambiguity Regularization for Unsupervised Learning of Probabilistic Grammars</a></p>
<p>14 0.35704941 <a title="81-lsi-14" href="./emnlp-2012-On_Amortizing_Inference_Cost_for_Structured_Prediction.html">99 emnlp-2012-On Amortizing Inference Cost for Structured Prediction</a></p>
<p>15 0.35613379 <a title="81-lsi-15" href="./emnlp-2012-Generalized_Higher-Order_Dependency_Parsing_with_Cube_Pruning.html">57 emnlp-2012-Generalized Higher-Order Dependency Parsing with Cube Pruning</a></p>
<p>16 0.32910571 <a title="81-lsi-16" href="./emnlp-2012-Concurrent_Acquisition_of_Word_Meaning_and_Lexical_Categories.html">29 emnlp-2012-Concurrent Acquisition of Word Meaning and Lexical Categories</a></p>
<p>17 0.32816145 <a title="81-lsi-17" href="./emnlp-2012-Training_Factored_PCFGs_with_Expectation_Propagation.html">126 emnlp-2012-Training Factored PCFGs with Expectation Propagation</a></p>
<p>18 0.32750797 <a title="81-lsi-18" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>19 0.31987524 <a title="81-lsi-19" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>20 0.31415513 <a title="81-lsi-20" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.031), (16, 0.034), (25, 0.018), (34, 0.083), (39, 0.28), (45, 0.02), (60, 0.093), (63, 0.057), (64, 0.03), (65, 0.028), (70, 0.013), (73, 0.013), (74, 0.056), (76, 0.059), (80, 0.033), (86, 0.032), (95, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84762841 <a title="81-lda-1" href="./emnlp-2012-A_Beam-Search_Decoder_for_Grammatical_Error_Correction.html">2 emnlp-2012-A Beam-Search Decoder for Grammatical Error Correction</a></p>
<p>Author: Daniel Dahlmeier ; Hwee Tou Ng</p><p>Abstract: We present a novel beam-search decoder for grammatical error correction. The decoder iteratively generates new hypothesis corrections from current hypotheses and scores them based on features of grammatical correctness and fluency. These features include scores from discriminative classifiers for specific error categories, such as articles and prepositions. Unlike all previous approaches, our method is able to perform correction of whole sentences with multiple and interacting errors while still taking advantage of powerful existing classifier approaches. Our decoder achieves an F1 correction score significantly higher than all previous published scores on the Helping Our Own (HOO) shared task data set.</p><p>same-paper 2 0.78404623 <a title="81-lda-2" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<p>Author: Yuan Zhang ; Roi Reichart ; Regina Barzilay ; Amir Globerson</p><p>Abstract: We present an automatic method for mapping language-specific part-of-speech tags to a set of universal tags. This unified representation plays a crucial role in cross-lingual syntactic transfer of multilingual dependency parsers. Until now, however, such conversion schemes have been created manually. Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consistent across source and target languages. We encode this intuition in an objective function that captures a range of distributional and typological characteristics of the derived mapping. Given the exponential size of the mapping space, we propose a novel method for optimizing over soft mappings, and use entropy regularization to drive those towards hard mappings. Our results demonstrate that automatically induced mappings rival the quality of their manually designed counterparts when evaluated in the . context of multilingual parsing.1</p><p>3 0.53729111 <a title="81-lda-3" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>Author: Greg Durrett ; Adam Pauls ; Dan Klein</p><p>Abstract: We consider the problem of using a bilingual dictionary to transfer lexico-syntactic information from a resource-rich source language to a resource-poor target language. In contrast to past work that used bitexts to transfer analyses of specific sentences at the token level, we instead use features to transfer the behavior of words at a type level. In a discriminative dependency parsing framework, our approach produces gains across a range of target languages, using two different lowresource training methodologies (one weakly supervised and one indirectly supervised) and two different dictionary sources (one manually constructed and one automatically constructed).</p><p>4 0.52666646 <a title="81-lda-4" href="./emnlp-2012-Left-to-Right_Tree-to-String_Decoding_with_Prediction.html">82 emnlp-2012-Left-to-Right Tree-to-String Decoding with Prediction</a></p>
<p>Author: Yang Feng ; Yang Liu ; Qun Liu ; Trevor Cohn</p><p>Abstract: Decoding algorithms for syntax based machine translation suffer from high computational complexity, a consequence of intersecting a language model with a context free grammar. Left-to-right decoding, which generates the target string in order, can improve decoding efficiency by simplifying the language model evaluation. This paper presents a novel left to right decoding algorithm for tree-to-string translation, using a bottom-up parsing strategy and dynamic future cost estimation for each partial translation. Our method outperforms previously published tree-to-string decoders, including a competing left-to-right method.</p><p>5 0.52470005 <a title="81-lda-5" href="./emnlp-2012-Wiki-ly_Supervised_Part-of-Speech_Tagging.html">138 emnlp-2012-Wiki-ly Supervised Part-of-Speech Tagging</a></p>
<p>Author: Shen Li ; Joao Graca ; Ben Taskar</p><p>Abstract: Despite significant recent work, purely unsupervised techniques for part-of-speech (POS) tagging have not achieved useful accuracies required by many language processing tasks. Use of parallel text between resource-rich and resource-poor languages is one source ofweak supervision that significantly improves accuracy. However, parallel text is not always available and techniques for using it require multiple complex algorithmic steps. In this paper we show that we can build POS-taggers exceeding state-of-the-art bilingual methods by using simple hidden Markov models and a freely available and naturally growing resource, the Wiktionary. Across eight languages for which we have labeled data to evaluate results, we achieve accuracy that significantly exceeds best unsupervised and parallel text methods. We achieve highest accuracy reported for several languages and show that our . approach yields better out-of-domain taggers than those trained using fully supervised Penn Treebank.</p><p>6 0.51468641 <a title="81-lda-6" href="./emnlp-2012-Universal_Grapheme-to-Phoneme_Prediction_Over_Latin_Alphabets.html">132 emnlp-2012-Universal Grapheme-to-Phoneme Prediction Over Latin Alphabets</a></p>
<p>7 0.51260787 <a title="81-lda-7" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>8 0.50279337 <a title="81-lda-8" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>9 0.50144523 <a title="81-lda-9" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>10 0.50022495 <a title="81-lda-10" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>11 0.49740511 <a title="81-lda-11" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>12 0.49612537 <a title="81-lda-12" href="./emnlp-2012-Type-Supervised_Hidden_Markov_Models_for_Part-of-Speech_Tagging_with_Incomplete_Tag_Dictionaries.html">129 emnlp-2012-Type-Supervised Hidden Markov Models for Part-of-Speech Tagging with Incomplete Tag Dictionaries</a></p>
<p>13 0.49587819 <a title="81-lda-13" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>14 0.49453014 <a title="81-lda-14" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>15 0.4945136 <a title="81-lda-15" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>16 0.49408126 <a title="81-lda-16" href="./emnlp-2012-Besting_the_Quiz_Master%3A_Crowdsourcing_Incremental_Classification_Games.html">23 emnlp-2012-Besting the Quiz Master: Crowdsourcing Incremental Classification Games</a></p>
<p>17 0.49334544 <a title="81-lda-17" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>18 0.49168977 <a title="81-lda-18" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>19 0.49134946 <a title="81-lda-19" href="./emnlp-2012-Extracting_Opinion_Expressions_with_semi-Markov_Conditional_Random_Fields.html">51 emnlp-2012-Extracting Opinion Expressions with semi-Markov Conditional Random Fields</a></p>
<p>20 0.49101642 <a title="81-lda-20" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
