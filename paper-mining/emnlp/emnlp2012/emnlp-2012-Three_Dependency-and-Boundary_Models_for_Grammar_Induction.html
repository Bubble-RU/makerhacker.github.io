<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-124" href="#">emnlp2012-124</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</h1>
<br/><p>Source: <a title="emnlp-2012-124-pdf" href="http://aclweb.org/anthology//D/D12/D12-1063.pdf">pdf</a></p><p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: We present a new family of models for unsupervised parsing, Dependency and Boundary models, that use cues at constituent boundaries to inform head-outward dependency tree generation. We build on three intuitions that are explicit in phrase-structure grammars but only implicit in standard dependency formulations: (i) Distributions of words that occur at sentence boundaries such as English determiners resemble constituent edges. (ii) Punctuation at sentence boundaries further helps distinguish full sentences from fragments like headlines and titles, allowing us to model grammatical differences between complete and incomplete sentences. (iii) Sentence-internal punctuation boundaries help with longer-distance dependencies, since punctuation correlates with constituent edges. Our models induce state-of-the-art dependency grammars for many languages without — — special knowledge of optimal input sentence lengths or biased, manually-tuned initializers.</p><p>Reference: <a title="emnlp-2012-124-reference" href="../emnlp2012_reference/emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We present a new family of models for unsupervised parsing, Dependency and Boundary models, that use cues at constituent boundaries to inform head-outward dependency tree generation. [sent-8, score-0.286]
</p><p>2 We build on three intuitions that are explicit in phrase-structure grammars but only implicit in standard dependency formulations: (i) Distributions of words that occur at sentence boundaries such as English determiners resemble constituent edges. [sent-9, score-0.387]
</p><p>3 (ii) Punctuation at sentence boundaries further helps distinguish full sentences from fragments like headlines and titles, allowing us to model grammatical differences between complete and incomplete sentences. [sent-10, score-0.4]
</p><p>4 (iii) Sentence-internal punctuation boundaries help with longer-distance dependencies, since punctuation correlates with constituent edges. [sent-11, score-0.53]
</p><p>5 Our models induce state-of-the-art dependency grammars for many languages without —  —  special knowledge of optimal input sentence lengths or biased, manually-tuned initializers. [sent-12, score-0.262]
</p><p>6 , 2006) and punctuation marks (Seginer, 2007; Ponvert et al. [sent-15, score-0.208]
</p><p>7 We will show that boundary information can also be useful in dependency grammar induction models, which traditionally focus on head rather than fringe words (Carroll and Charniak, 1992). [sent-17, score-0.735]
</p><p>8 Similarly, the fact that the noun head (NN) of the object the mail appears at the right edge of the sentence could help identify the noun check as the right edge of the subject NP. [sent-23, score-0.201]
</p><p>9 Because typical headdriven grammars model valence separately for each class of head, however, they cannot see that the left fringe boundary, The check, of the verb-phrase is shared with its daughter’s, check. [sent-26, score-0.326]
</p><p>10 Neither of these insights is available to traditional dependency formulations, which could learn from the boundaries  of this sentence only that determiners might have no left- and that nouns might have no right-dependents. [sent-27, score-0.238]
</p><p>11 We propose a family of dependency parsing models that are capable of inducing longer-range implications from sentence edges than just fertilities of their fringe words. [sent-28, score-0.283]
</p><p>12 Our ideas conveniently lend themselves to implementations that can reuse much of the standard grammar induction machinery, including efficient dynamic programming routines for the relevant expectation-maximization algorithms. [sent-29, score-0.261]
</p><p>13 Each (head) word ch generates a left-dependent with probability 1− PSTOP( · | L; · · · ), where dots represent additional  parameterization on which it may be conditioned. [sent-34, score-0.172]
</p><p>14 If the child is indeed generated, its identity cd is chosen with probability PATTACH(cd | ch; · · · ), influenced by the identity of the parent ch and possibly other parameters (again represented by dots). [sent-35, score-0.29]
</p><p>15 The child then generates its own subtree recursively and the whole process continues, moving away from the head, until ch fails to generate a left-dependent. [sent-36, score-0.172]
</p><p>16 Instances of these split-head automata have been heavily used in grammar induction (Paskin, 2001b; Klein and Manning, 2004; Headden et al. [sent-39, score-0.339]
</p><p>17 The basic tenet of split-head grammars is that every head word generates its left-dependents independently of its right-dependents. [sent-41, score-0.217]
</p><p>18 But it does —  —  not imply that descendants that are closer to the head cannot influence the generation of farther dependents on the same side. [sent-43, score-0.142]
</p><p>19 Nevertheless, many popular grammars for unsupervised parsing behave as if a word had to generate all of its children (to one side) or at least their count before allowing any ofthese children themselves to recurse. [sent-44, score-0.176]
</p><p>20 For example, Klein and Manning’s (2004) dependency model with valence (DMV) could be imple—  —  1Unrestricted head-outward automata are strictly more powerful (e. [sent-45, score-0.227]
</p><p>21 , the ordered sequence ofall already-generated descendants, on the side of the head that is in the process of spawning off an additional child is not only known but also readily accessible. [sent-56, score-0.178]
</p><p>22 Taking advantage of this availability, we designed three new models for dependency grammar induction. [sent-57, score-0.281]
</p><p>23 1 Dependency and Boundary Model One DBM-1 conditions all stopping decisions on adjacency and the identity of the fringe word ce the currently-farthest descendant (edge) derived by head ch in the given head-outward direction (dir ∈ {L, R}): —  PSTOP( · | dir; adj, ce). [sent-59, score-0.7]
</p><p>24 In the adjacent case (adj = T), ch is deciding whether to have any children on a given side: a first child’s subtree would be right next to the head, so the head  × ×  and the fringe words coincide (ch = ce). [sent-60, score-0.415]
</p><p>25 In the nonadjacent case (adj = F), these will be different words and their classes will, in general, not be the Thus, non-adjacent stopping decisions will be made independently of a head word’s identity. [sent-61, score-0.271]
</p><p>26 Therefore, all word classes will be equally likely to continue to grow or not, for a specific proposed fringe boundary. [sent-62, score-0.173]
</p><p>27 For example, production of The check is involves two non-adjacent stopping decisions on the left: one by the noun check and one by the verb is, both of which stop after generating a first child. [sent-63, score-0.248]
</p><p>28 Figure 2: Our running example a simple sentence and its unlabeled dependency parse structure’s probability, as factored by DBM-1 ; highlighted comments specify heads associated to non-adjacent stopping probability factors. [sent-69, score-0.329]
</p><p>29 2 Dependency and Boundary Model Two DBM-2 allows different but related grammars to coexist in a single model. [sent-74, score-0.147]
</p><p>30 Specifically, we presuppose that all sentences are assigned to one of two classes: complete and incomplete (comp ∈ {T, F}, for now  taken as exogenous). [sent-75, score-0.175]
</p><p>31 However, sentence lengths for which stopping probabilities are responsible and distributions of root words may be different. [sent-79, score-0.208]
</p><p>32 Consequently, an additional comp parameter is added to the context of two relevant types of factors: PSTOP ( · | dir; adj, ce , comp) ; and PATTACH(cr | ⋄; L, comp) . [sent-80, score-0.174]
</p><p>33 For example, the new stopping factors could capture the fact that incomplete fragments such as the noun-phrases George Morton, headlines Energy and Odds and Ends, a line item c - Domestic car, dollar  —  —  —  690 quantity Revenue: $3. [sent-81, score-0.401]
</p><p>34 The new root-attachment factors could further track that incomplete sentences generally lack verbs, in contrast to other short sentences, e. [sent-83, score-0.175]
</p><p>35 3 Dependency and Boundary Model Three DBM-3 adds further conditioning on punctuation context. [sent-103, score-0.273]
</p><p>36 We introduce another boolean parameter, cross, which indicates the presence of intervening punctuation between a proposed head word ch and its dependent cd. [sent-104, score-0.533]
</p><p>37 Conditioning on (the absence of) intervening punctuation could help tell true long-distance relations from impostors. [sent-108, score-0.253]
</p><p>38 Table 1 lists some parameterizations that have since been used by unsupervised dependency grammar inducers sharing their backbone split-head process. [sent-113, score-0.613]
</p><p>39 In the grammar induction experiments that follow,  we will test each model’s incremental contribution to accuracies empirically, across many disparate languages. [sent-120, score-0.306]
</p><p>40 4 For each data set, we induced a baseline grammar using the DMV. [sent-123, score-0.171]
</p><p>41 Grammar inducers were initialized using (the same) uniformly-at-random chosen parse trees of training sentences (Cohen and Smith, 2010); thereafter, we applied “add one” smoothing at every training step. [sent-126, score-0.317]
</p><p>42 We did not test DBM-3 in this set-up because most sentence-internal punctuation occurs in longer sentences; instead, DBM-3 will be tested later (see §7), using most in the final training step §of7 a c uusirrnigcu mluomst strategy (Bengio et al. [sent-140, score-0.208]
</p><p>43 -792  Table 2: Directed dependency accuracies, averaged over all 2006/7 CoNLL evaluation sets (all sentences), for the DMV and two new dependency-and-boundary grammar inducers (DBM-1,2) using two termination strategies. [sent-147, score-0.578]
</p><p>44 6 —  4  Dependency and Boundary Model One  The primary difference between DBM-1 and traditional models, such as the DMV, is that DBM-1 conditions non-adjacent stopping decisions on the identities of fringe words in partial yields (see §2. [sent-148, score-0.399]
</p><p>45 1 Analytical Motivation Treebank data suggests that the class of the fringe word its part-of-speech, ce is a better predictor of (non-adjacent) stopping decisions, in a given direction dir, than the head’s own class ch. [sent-151, score-0.384]
</p><p>46 In contrast, using ce in place of ch boosts explanatory power to 24%, keeping the number ofparameters the same. [sent-159, score-0.294]
</p><p>47 Moreover, every sentence exposes two true edges (H¨ anig, 2010): integrated over many sample sentence beginnings and ends, cumulative knowledge about such markers can guide a grammar inducer inside long inputs, where structure is murky. [sent-163, score-0.171]
</p><p>48 692  Table 4: Empirical distributions for non-punctuationpartof-speech tags in WSJ, ordered by overall frequency, as well as distributions for sentence boundaries and for the roots of complete and incomplete sentences. [sent-175, score-0.359]
</p><p>49 )  Figure  5  3: Histograms  of lengths (in tokens) for  2,261 non-clausal  fragments (red) and other sentences (blue) in WSJ. [sent-188, score-0.185]
</p><p>50 Dependency and Boundary Model Two  DBM-2 adapts DBM-1 grammars to two classes of inputs (complete sentences and incomplete fragments) by forking off new, separate multinomials for stopping decisions and root-distributions (see §2. [sent-189, score-0.499]
</p><p>51 1 Analytical Motivation Unrepresentative short sentences such as headlines and titles are common in news-style data and pose a known nuisance to grammar inducers. [sent-192, score-0.266]
</p><p>52 8 Table 4 shows that roots of incomplete sentences, which are dominated by nouns, barely resemble the other roots, drawn from more traditional verb and modal types. [sent-196, score-0.196]
</p><p>53 693 (complete) roots, suggesting that heads of fragments too may warrant their own multinomial in the model. [sent-206, score-0.152]
</p><p>54 Further, incomplete sentences are uncharacteristically short (see Figure 3). [sent-207, score-0.175]
</p><p>55 It is this property that makes them particularly treacherous to grammar inducers, since by offering few options of root positions they increase the chances that a learner will incorrectly induce nouns to be heads. [sent-208, score-0.171]
</p><p>56 Given that expected lengths are directly related to stopping decisions, it could make sense to also model the stopping probabilities of incomplete sentences separately. [sent-209, score-0.469]
</p><p>57 2 Experimental Results Since it is not possible to consult parse trees during grammar induction (to check whether an input sentence is clausal), we opted for a proxy: presence of sentence-final punctuation. [sent-211, score-0.377]
</p><p>58 Using punctuation to divide input sentences into two groups, DBM-2 scored higher: 40. [sent-212, score-0.256]
</p><p>59 3o tan 1lC4 la6 u,891s421a978lno -Cl12a,u932s362a615l4 89T,7o24t604a583l Table 6: A contingency table for clausal sentences and trailing punctuation in WSJ; the mean square contingency coefficient rφ signifies a low degree of correlation. [sent-218, score-0.401]
</p><p>60 We suspect that identities of punctuation marks (Collins, 2003, Footnote 13) both sentence-final and sentence-initial could be of extra assistance in grammar induction, specifically for grouping imperatives, questions, and so forth. [sent-225, score-0.437]
</p><p>61 —  —  6  Dependency and Boundary Model Three  DBM-3 exploits sentence-internal punctuation contexts by modeling punctuation-crossing dependency  arcs separately from other attachments (see §2. [sent-226, score-0.318]
</p><p>62 Sometimes longer-distance dependencies can be vetted using sentence-internal punctuation marks. [sent-235, score-0.208]
</p><p>63 It happens that the presence of punctuation between such conjunction (IN) and verb (MD) types serves as a clue that they are not connected (see Table 7a); by contrast, a simpler cue whether these words are adjacent is, in this case, hardly of any use (see Table 7b). [sent-236, score-0.246]
</p><p>64 Conditioning on crossing punctuation could be of help then, playing a role similar to that of comma-counting (Collins, 1997, §2. [sent-237, score-0.208]
</p><p>65 04na tlA a2 ,c4h3178e41d38notA 1a74c,h60e4817d50231746T,91o 86t5a241l Table 7: Contingency tables for IN right-attaching MD, among closest ordered pairs of these tokens in WSJ sentences with punctuation, versus: (a) presence of intervening punctuation; and (b) presence of intermediate words. [sent-241, score-0.205]
</p><p>66 2  Experimental Results Postponed  As we mentioned earlier (see §3), there is little point iAns testing DntiBoMne-d3 weairtlhie srh (osreteer § sentences, s li nttlcee pmooinstt sentence-internal punctuation occurs in longer inputs. [sent-243, score-0.208]
</p><p>67 Our grammar inducers will thus be “starting small” in both senses suggested by Elman (1993): simultaneously scaffolding on model- and data-complexity. [sent-251, score-0.468]
</p><p>68 1 Scaffolding Stage #1: DBM-1 We begin by training DBM-1 on sentences without sentence-internal punctuation but with at least one trailing punctuation mark. [sent-253, score-0.464]
</p><p>69 Table 8: Average accuracies over CoNLL evaluation sets (all sentences), for the DMV baseline and DBM1–3 trained with a curriculum strategy, and state-of-the-art results for systems that: (i) are also POS-agnostic and monolingual, including SCAJ (Spitkovsky et al. [sent-264, score-0.161]
</p><p>70 , 2011b); (ii) rely on gold POS-tag identities to discourage noun roots (Mare ˇcek and Zabokrtsk y´, 2011, MZ) or to encourage verbs (Rasooli and Faili,  2012, RF); and (iii) transfer delexicalized parsers (Søgaard, 2011a, S) from resource-rich languages with translations (McDonald et al. [sent-266, score-0.169]
</p><p>71 DMV and DBM-1 trained on simple sentences, from uniform; DBM-2 and 3 trained on most sentences, from DBM-1 and 2, respectively; +inference is DBM-3 with punctuation constraints. [sent-268, score-0.208]
</p><p>72 1), plus —  695 uniformly-at-random chosen dependency trees for the new complex and incomplete sentences, subject  to punctuation-induced constraints. [sent-276, score-0.237]
</p><p>73 Here we used the sprawl method a more relaxed approach than in training, allowing arbitrary words to attach inter-punctuation fragments (provided that each entire fragment still be derived by one of its words) as suggested by Spitkovsky et al. [sent-291, score-0.131]
</p><p>74 10 —  —  8  Discussion and the State-of-the-Art  DBMs come from a long line of head-outward mod-  els for dependency grammar induction yet their generative processes feature important novelties. [sent-297, score-0.371]
</p><p>75 , of complete and incomplete sentences to coexist in a single model. [sent-301, score-0.208]
</p><p>76 The second part of our work the use of a curriculum strategy to train DBM-1 through 3 eliminates having to know tuned cut-offs, such as sentences with up to a predetermined number of tokens. [sent-304, score-0.164]
</p><p>77 : stage one’s data is dictated by DBM-1 (which ignores punctuation); subsequent stages initialize additional pieces uniformly: uniform-at-random parses for new data and uniform multinomials for new parameters. [sent-306, score-0.132]
</p><p>78 Other orthogonal dependency grammar induction techniques including ones based on universal rules (Naseem et al. [sent-313, score-0.371]
</p><p>79 1 Monolingual POS-Agnostic Inducers The first type of grammar inducers, including our own approach, uses standard training and test data sets for each language, with gold part-of-speech tags as anonymized word classes. [sent-318, score-0.171]
</p><p>80 The progression of scores for DBM-1 through 3 without using punctuation constraints in inference 40. [sent-328, score-0.208]
</p><p>81 2% fell entirely above this previous state-of-the-art result as well; the DMV baseline also trained on sentences without internal but with final punctuation averaged 33. [sent-331, score-0.256]
</p><p>82 Such grammar inducers generally do better than the first kind e. [sent-338, score-0.402]
</p><p>83 Of the 10 languages for which we found results in the literature, transferred parsers underperformed the grammar inducers in only one case: on English (see Table 8). [sent-347, score-0.402]
</p><p>84 For example, modeling of incomplete sentences could help in incremental initialization strategies like baby steps (Spitkovsky et al. [sent-352, score-0.208]
</p><p>85 Since redundant views of data can make learning easier (Blum and Mitchell, 1998), integrating aspects of both constituency and dependency ought to be able to help grammar induction. [sent-362, score-0.322]
</p><p>86 We have shown that this insight is correct: dependency grammar inducers can gain from modeling boundary information that is fundamental to constituency (i. [sent-363, score-0.641]
</p><p>87 DBMs are a step in the direction towards modeling constituent boundaries jointly with head dependencies. [sent-366, score-0.217]
</p><p>88 Further steps must involve more tightly 697 coupling the two frameworks, as well as showing ways to incorporate both kinds of information in other state-of-the art grammar induction paradigms. [sent-367, score-0.261]
</p><p>89 Learning dependency translation models as collections of finite-state head transducers. [sent-378, score-0.213]
</p><p>90 Boosting unsupervised grammar induction by splitting complex sentences on function words. [sent-412, score-0.371]
</p><p>91 Two experiments on learning probabilistic dependency grammars from corpora. [sent-475, score-0.224]
</p><p>92 Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. [sent-483, score-0.275]
</p><p>93 Concavity and initialization  for unsupervised dependency grammar induction. [sent-577, score-0.343]
</p><p>94 Improving unsupervised dependency parsing with richer contexts and smoothing. [sent-590, score-0.172]
</p><p>95 Corpus-based induction of syntactic structure: Models of dependency and constituency. [sent-597, score-0.2]
</p><p>96 Gibbs sampling with treeness constraint in unsupervised dependency parsing. [sent-619, score-0.172]
</p><p>97 From ranked words to dependency trees: two-stage unsupervised non-projective dependency parsing. [sent-715, score-0.282]
</p><p>98 Baby Steps: How “Less is More” in unsupervised dependency parsing. [sent-723, score-0.172]
</p><p>99 Lateen EM: Unsupervised training with multiple objectives, applied to dependency grammar induction. [sent-731, score-0.281]
</p><p>100 Bootstrapping dependency grammar inducers from incomplete sentence fragments via austere models the “wabi-sabi” of unsupervised parsing. [sent-747, score-0.8]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pstop', 0.366), ('spitkovsky', 0.258), ('inducers', 0.231), ('pattach', 0.212), ('punctuation', 0.208), ('dbms', 0.193), ('fringe', 0.173), ('grammar', 0.171), ('alshawi', 0.165), ('dmv', 0.165), ('ch', 0.139), ('dir', 0.138), ('stopping', 0.128), ('incomplete', 0.127), ('vbz', 0.117), ('curriculum', 0.116), ('grammars', 0.114), ('dependency', 0.11), ('head', 0.103), ('fragments', 0.099), ('wsj', 0.092), ('comp', 0.091), ('induction', 0.09), ('boundary', 0.088), ('ce', 0.083), ('nn', 0.08), ('boundaries', 0.079), ('automata', 0.078), ('zabokrtsk', 0.077), ('dt', 0.072), ('roots', 0.069), ('termination', 0.066), ('lateen', 0.066), ('mare', 0.066), ('scaffolding', 0.066), ('em', 0.066), ('conditioning', 0.065), ('unsupervised', 0.062), ('adj', 0.058), ('identities', 0.058), ('mail', 0.058), ('paskin', 0.058), ('rasooli', 0.058), ('heads', 0.053), ('contingency', 0.05), ('cd', 0.05), ('determiners', 0.049), ('sentences', 0.048), ('stage', 0.047), ('headlines', 0.047), ('naseem', 0.047), ('clausal', 0.045), ('accuracies', 0.045), ('intervening', 0.045), ('uniform', 0.043), ('distributions', 0.042), ('side', 0.042), ('bengio', 0.042), ('multinomials', 0.042), ('delexicalized', 0.042), ('determiner', 0.041), ('constituency', 0.041), ('gillenwater', 0.041), ('check', 0.04), ('decisions', 0.04), ('valence', 0.039), ('descendants', 0.039), ('faili', 0.039), ('krueger', 0.039), ('ofparameters', 0.039), ('parameterizations', 0.039), ('ponvert', 0.039), ('steer', 0.039), ('unrepresentative', 0.039), ('presence', 0.038), ('lengths', 0.038), ('parse', 0.038), ('cr', 0.037), ('analytical', 0.037), ('cohen', 0.037), ('tokens', 0.036), ('collins', 0.036), ('constituent', 0.035), ('alia', 0.035), ('inter', 0.035), ('eisner', 0.034), ('identity', 0.034), ('siblings', 0.033), ('gaard', 0.033), ('blum', 0.033), ('dots', 0.033), ('cek', 0.033), ('brent', 0.033), ('baby', 0.033), ('samdani', 0.033), ('explanatory', 0.033), ('hellinger', 0.033), ('coexist', 0.033), ('child', 0.033), ('fragment', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000013 <a title="124-tfidf-1" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: We present a new family of models for unsupervised parsing, Dependency and Boundary models, that use cues at constituent boundaries to inform head-outward dependency tree generation. We build on three intuitions that are explicit in phrase-structure grammars but only implicit in standard dependency formulations: (i) Distributions of words that occur at sentence boundaries such as English determiners resemble constituent edges. (ii) Punctuation at sentence boundaries further helps distinguish full sentences from fragments like headlines and titles, allowing us to model grammatical differences between complete and incomplete sentences. (iii) Sentence-internal punctuation boundaries help with longer-distance dependencies, since punctuation correlates with constituent edges. Our models induce state-of-the-art dependency grammars for many languages without — — special knowledge of optimal input sentence lengths or biased, manually-tuned initializers.</p><p>2 0.1531741 <a title="124-tfidf-2" href="./emnlp-2012-Exploiting_Reducibility_in_Unsupervised_Dependency_Parsing.html">46 emnlp-2012-Exploiting Reducibility in Unsupervised Dependency Parsing</a></p>
<p>Author: David Marecek ; Zdene20 ek Zabokrtsky</p><p>Abstract: The possibility of deleting a word from a sentence without violating its syntactic correctness belongs to traditionally known manifestations of syntactic dependency. We introduce a novel unsupervised parsing approach that is based on a new n-gram reducibility measure. We perform experiments across 18 languages available in CoNLL data and we show that our approach achieves better accuracy for the majority of the languages then previously reported results.</p><p>3 0.13525885 <a title="124-tfidf-3" href="./emnlp-2012-Unambiguity_Regularization_for_Unsupervised_Learning_of_Probabilistic_Grammars.html">130 emnlp-2012-Unambiguity Regularization for Unsupervised Learning of Probabilistic Grammars</a></p>
<p>Author: Kewei Tu ; Vasant Honavar</p><p>Abstract: We introduce a novel approach named unambiguity regularization for unsupervised learning of probabilistic natural language grammars. The approach is based on the observation that natural language is remarkably unambiguous in the sense that only a tiny portion of the large number of possible parses of a natural language sentence are syntactically valid. We incorporate an inductive bias into grammar learning in favor of grammars that lead to unambiguous parses on natural language sentences. The resulting family of algorithms includes the expectation-maximization algorithm (EM) and its variant, Viterbi EM, as well as a so-called softmax-EM algorithm. The softmax-EM algorithm can be implemented with a simple and computationally efficient extension to standard EM. In our experiments of unsupervised dependency grammar learn- ing, we show that unambiguity regularization is beneficial to learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art.</p><p>4 0.11596425 <a title="124-tfidf-4" href="./emnlp-2012-Training_Factored_PCFGs_with_Expectation_Propagation.html">126 emnlp-2012-Training Factored PCFGs with Expectation Propagation</a></p>
<p>Author: David Hall ; Dan Klein</p><p>Abstract: PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar. We present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion. Our method works with linguisticallymotivated annotations, induced latent structure, lexicalization, or any mix of the three. We use a structured expectation propagation algorithm that makes use of the factored structure in two ways. First, by partitioning the factors, it speeds up parsing exponentially over the unfactored approach. Second, it minimizes the redundancy of the factors during training, improving accuracy over an independent approach. Using purely latent variable annotations, we can efficiently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the na¨ ıve approach. Combining latent, lexicalized, and unlexicalized anno- tations, our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank.</p><p>5 0.10396685 <a title="124-tfidf-5" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>Author: Greg Durrett ; Adam Pauls ; Dan Klein</p><p>Abstract: We consider the problem of using a bilingual dictionary to transfer lexico-syntactic information from a resource-rich source language to a resource-poor target language. In contrast to past work that used bitexts to transfer analyses of specific sentences at the token level, we instead use features to transfer the behavior of words at a type level. In a discriminative dependency parsing framework, our approach produces gains across a range of target languages, using two different lowresource training methodologies (one weakly supervised and one indirectly supervised) and two different dictionary sources (one manually constructed and one automatically constructed).</p><p>6 0.099595867 <a title="124-tfidf-6" href="./emnlp-2012-Generalized_Higher-Order_Dependency_Parsing_with_Cube_Pruning.html">57 emnlp-2012-Generalized Higher-Order Dependency Parsing with Cube Pruning</a></p>
<p>7 0.090760231 <a title="124-tfidf-7" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>8 0.082348503 <a title="124-tfidf-8" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>9 0.079272591 <a title="124-tfidf-9" href="./emnlp-2012-Dynamic_Programming_for_Higher_Order_Parsing_of_Gap-Minding_Trees.html">37 emnlp-2012-Dynamic Programming for Higher Order Parsing of Gap-Minding Trees</a></p>
<p>10 0.078276142 <a title="124-tfidf-10" href="./emnlp-2012-Minimal_Dependency_Length_in_Realization_Ranking.html">88 emnlp-2012-Minimal Dependency Length in Realization Ranking</a></p>
<p>11 0.076966785 <a title="124-tfidf-11" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<p>12 0.074596159 <a title="124-tfidf-12" href="./emnlp-2012-A_Bayesian_Model_for_Learning_SCFGs_with_Discontiguous_Rules.html">1 emnlp-2012-A Bayesian Model for Learning SCFGs with Discontiguous Rules</a></p>
<p>13 0.071633279 <a title="124-tfidf-13" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>14 0.068443671 <a title="124-tfidf-14" href="./emnlp-2012-Exploring_Adaptor_Grammars_for_Native_Language_Identification.html">48 emnlp-2012-Exploring Adaptor Grammars for Native Language Identification</a></p>
<p>15 0.06688977 <a title="124-tfidf-15" href="./emnlp-2012-Improving_NLP_through_Marginalization_of_Hidden_Syntactic_Structure.html">65 emnlp-2012-Improving NLP through Marginalization of Hidden Syntactic Structure</a></p>
<p>16 0.061452854 <a title="124-tfidf-16" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>17 0.059173685 <a title="124-tfidf-17" href="./emnlp-2012-Type-Supervised_Hidden_Markov_Models_for_Part-of-Speech_Tagging_with_Incomplete_Tag_Dictionaries.html">129 emnlp-2012-Type-Supervised Hidden Markov Models for Part-of-Speech Tagging with Incomplete Tag Dictionaries</a></p>
<p>18 0.058831077 <a title="124-tfidf-18" href="./emnlp-2012-Spectral_Dependency_Parsing_with_Latent_Variables.html">119 emnlp-2012-Spectral Dependency Parsing with Latent Variables</a></p>
<p>19 0.058464315 <a title="124-tfidf-19" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>20 0.057662811 <a title="124-tfidf-20" href="./emnlp-2012-Towards_Efficient_Named-Entity_Rule_Induction_for_Customizability.html">125 emnlp-2012-Towards Efficient Named-Entity Rule Induction for Customizability</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.23), (1, -0.098), (2, 0.11), (3, -0.041), (4, 0.031), (5, 0.072), (6, -0.023), (7, 0.023), (8, -0.0), (9, 0.143), (10, 0.1), (11, 0.141), (12, 0.016), (13, 0.104), (14, -0.131), (15, -0.044), (16, -0.01), (17, 0.004), (18, -0.151), (19, -0.107), (20, 0.071), (21, -0.037), (22, 0.019), (23, 0.139), (24, -0.052), (25, 0.137), (26, 0.064), (27, 0.141), (28, 0.007), (29, 0.063), (30, 0.122), (31, -0.192), (32, -0.016), (33, -0.056), (34, 0.142), (35, -0.042), (36, -0.038), (37, 0.186), (38, -0.132), (39, 0.119), (40, -0.052), (41, 0.004), (42, 0.005), (43, 0.068), (44, -0.028), (45, 0.001), (46, -0.037), (47, 0.046), (48, -0.043), (49, 0.071)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94625074 <a title="124-lsi-1" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: We present a new family of models for unsupervised parsing, Dependency and Boundary models, that use cues at constituent boundaries to inform head-outward dependency tree generation. We build on three intuitions that are explicit in phrase-structure grammars but only implicit in standard dependency formulations: (i) Distributions of words that occur at sentence boundaries such as English determiners resemble constituent edges. (ii) Punctuation at sentence boundaries further helps distinguish full sentences from fragments like headlines and titles, allowing us to model grammatical differences between complete and incomplete sentences. (iii) Sentence-internal punctuation boundaries help with longer-distance dependencies, since punctuation correlates with constituent edges. Our models induce state-of-the-art dependency grammars for many languages without — — special knowledge of optimal input sentence lengths or biased, manually-tuned initializers.</p><p>2 0.74216664 <a title="124-lsi-2" href="./emnlp-2012-Exploiting_Reducibility_in_Unsupervised_Dependency_Parsing.html">46 emnlp-2012-Exploiting Reducibility in Unsupervised Dependency Parsing</a></p>
<p>Author: David Marecek ; Zdene20 ek Zabokrtsky</p><p>Abstract: The possibility of deleting a word from a sentence without violating its syntactic correctness belongs to traditionally known manifestations of syntactic dependency. We introduce a novel unsupervised parsing approach that is based on a new n-gram reducibility measure. We perform experiments across 18 languages available in CoNLL data and we show that our approach achieves better accuracy for the majority of the languages then previously reported results.</p><p>3 0.72772259 <a title="124-lsi-3" href="./emnlp-2012-Unambiguity_Regularization_for_Unsupervised_Learning_of_Probabilistic_Grammars.html">130 emnlp-2012-Unambiguity Regularization for Unsupervised Learning of Probabilistic Grammars</a></p>
<p>Author: Kewei Tu ; Vasant Honavar</p><p>Abstract: We introduce a novel approach named unambiguity regularization for unsupervised learning of probabilistic natural language grammars. The approach is based on the observation that natural language is remarkably unambiguous in the sense that only a tiny portion of the large number of possible parses of a natural language sentence are syntactically valid. We incorporate an inductive bias into grammar learning in favor of grammars that lead to unambiguous parses on natural language sentences. The resulting family of algorithms includes the expectation-maximization algorithm (EM) and its variant, Viterbi EM, as well as a so-called softmax-EM algorithm. The softmax-EM algorithm can be implemented with a simple and computationally efficient extension to standard EM. In our experiments of unsupervised dependency grammar learn- ing, we show that unambiguity regularization is beneficial to learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art.</p><p>4 0.50189149 <a title="124-lsi-4" href="./emnlp-2012-Training_Factored_PCFGs_with_Expectation_Propagation.html">126 emnlp-2012-Training Factored PCFGs with Expectation Propagation</a></p>
<p>Author: David Hall ; Dan Klein</p><p>Abstract: PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar. We present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion. Our method works with linguisticallymotivated annotations, induced latent structure, lexicalization, or any mix of the three. We use a structured expectation propagation algorithm that makes use of the factored structure in two ways. First, by partitioning the factors, it speeds up parsing exponentially over the unfactored approach. Second, it minimizes the redundancy of the factors during training, improving accuracy over an independent approach. Using purely latent variable annotations, we can efficiently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the na¨ ıve approach. Combining latent, lexicalized, and unlexicalized anno- tations, our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank.</p><p>5 0.44190285 <a title="124-lsi-5" href="./emnlp-2012-Minimal_Dependency_Length_in_Realization_Ranking.html">88 emnlp-2012-Minimal Dependency Length in Realization Ranking</a></p>
<p>Author: Michael White ; Rajakrishnan Rajkumar</p><p>Abstract: Comprehension and corpus studies have found that the tendency to minimize dependency length has a strong influence on constituent ordering choices. In this paper, we investigate dependency length minimization in the context of discriminative realization ranking, focusing on its potential to eliminate egregious ordering errors as well as better match the distributional characteristics of sentence orderings in news text. We find that with a stateof-the-art, comprehensive realization ranking model, dependency length minimization yields statistically significant improvements in BLEU scores and significantly reduces the number of heavy/light ordering errors. Through distributional analyses, we also show that with simpler ranking models, dependency length minimization can go overboard, too often sacrificing canonical word order to shorten dependencies, while richer models manage to better counterbalance the dependency length minimization preference against (sometimes) competing canonical word order preferences.</p><p>6 0.42998585 <a title="124-lsi-6" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>7 0.42632729 <a title="124-lsi-7" href="./emnlp-2012-Exploring_Adaptor_Grammars_for_Native_Language_Identification.html">48 emnlp-2012-Exploring Adaptor Grammars for Native Language Identification</a></p>
<p>8 0.37945575 <a title="124-lsi-8" href="./emnlp-2012-A_Bayesian_Model_for_Learning_SCFGs_with_Discontiguous_Rules.html">1 emnlp-2012-A Bayesian Model for Learning SCFGs with Discontiguous Rules</a></p>
<p>9 0.35132986 <a title="124-lsi-9" href="./emnlp-2012-Generalized_Higher-Order_Dependency_Parsing_with_Cube_Pruning.html">57 emnlp-2012-Generalized Higher-Order Dependency Parsing with Cube Pruning</a></p>
<p>10 0.34390378 <a title="124-lsi-10" href="./emnlp-2012-Learning_Syntactic_Categories_Using_Paradigmatic_Representations_of_Word_Context.html">79 emnlp-2012-Learning Syntactic Categories Using Paradigmatic Representations of Word Context</a></p>
<p>11 0.33719951 <a title="124-lsi-11" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<p>12 0.3074255 <a title="124-lsi-12" href="./emnlp-2012-Generating_Non-Projective_Word_Order_in_Statistical_Linearization.html">59 emnlp-2012-Generating Non-Projective Word Order in Statistical Linearization</a></p>
<p>13 0.30579269 <a title="124-lsi-13" href="./emnlp-2012-Spectral_Dependency_Parsing_with_Latent_Variables.html">119 emnlp-2012-Spectral Dependency Parsing with Latent Variables</a></p>
<p>14 0.30167758 <a title="124-lsi-14" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>15 0.29811063 <a title="124-lsi-15" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>16 0.29113442 <a title="124-lsi-16" href="./emnlp-2012-Towards_Efficient_Named-Entity_Rule_Induction_for_Customizability.html">125 emnlp-2012-Towards Efficient Named-Entity Rule Induction for Customizability</a></p>
<p>17 0.2898345 <a title="124-lsi-17" href="./emnlp-2012-Language_Model_Rest_Costs_and_Space-Efficient_Storage.html">74 emnlp-2012-Language Model Rest Costs and Space-Efficient Storage</a></p>
<p>18 0.28554693 <a title="124-lsi-18" href="./emnlp-2012-Dynamic_Programming_for_Higher_Order_Parsing_of_Gap-Minding_Trees.html">37 emnlp-2012-Dynamic Programming for Higher Order Parsing of Gap-Minding Trees</a></p>
<p>19 0.27239189 <a title="124-lsi-19" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>20 0.2658287 <a title="124-lsi-20" href="./emnlp-2012-A_Novel_Discriminative_Framework_for_Sentence-Level_Discourse_Analysis.html">7 emnlp-2012-A Novel Discriminative Framework for Sentence-Level Discourse Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.034), (11, 0.024), (16, 0.055), (25, 0.022), (29, 0.014), (34, 0.048), (39, 0.012), (45, 0.026), (60, 0.066), (63, 0.056), (64, 0.024), (65, 0.035), (70, 0.02), (73, 0.031), (74, 0.072), (76, 0.072), (80, 0.023), (82, 0.011), (86, 0.027), (89, 0.238), (95, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.85280615 <a title="124-lda-1" href="./emnlp-2012-Word_Salad%3A_Relating_Food_Prices_and_Descriptions.html">139 emnlp-2012-Word Salad: Relating Food Prices and Descriptions</a></p>
<p>Author: Victor Chahuneau ; Kevin Gimpel ; Bryan R. Routledge ; Lily Scherlis ; Noah A. Smith</p><p>Abstract: We investigate the use of language in food writing, specifically on restaurant menus and in customer reviews. Our approach is to build predictive models of concrete external variables, such as restaurant menu prices. We make use of a dataset of menus and customer reviews for thousands of restaurants in several U.S. cities. By focusing on prediction tasks and doing our analysis at scale, our methodology allows quantitative, objective measurements of the words and phrases used to de- scribe food in restaurants. We also explore interactions in language use between menu prices and sentiment as expressed in user reviews.</p><p>same-paper 2 0.78946549 <a title="124-lda-2" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: We present a new family of models for unsupervised parsing, Dependency and Boundary models, that use cues at constituent boundaries to inform head-outward dependency tree generation. We build on three intuitions that are explicit in phrase-structure grammars but only implicit in standard dependency formulations: (i) Distributions of words that occur at sentence boundaries such as English determiners resemble constituent edges. (ii) Punctuation at sentence boundaries further helps distinguish full sentences from fragments like headlines and titles, allowing us to model grammatical differences between complete and incomplete sentences. (iii) Sentence-internal punctuation boundaries help with longer-distance dependencies, since punctuation correlates with constituent edges. Our models induce state-of-the-art dependency grammars for many languages without — — special knowledge of optimal input sentence lengths or biased, manually-tuned initializers.</p><p>3 0.53205162 <a title="124-lda-3" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms ofweak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.</p><p>4 0.50558579 <a title="124-lda-4" href="./emnlp-2012-Unambiguity_Regularization_for_Unsupervised_Learning_of_Probabilistic_Grammars.html">130 emnlp-2012-Unambiguity Regularization for Unsupervised Learning of Probabilistic Grammars</a></p>
<p>Author: Kewei Tu ; Vasant Honavar</p><p>Abstract: We introduce a novel approach named unambiguity regularization for unsupervised learning of probabilistic natural language grammars. The approach is based on the observation that natural language is remarkably unambiguous in the sense that only a tiny portion of the large number of possible parses of a natural language sentence are syntactically valid. We incorporate an inductive bias into grammar learning in favor of grammars that lead to unambiguous parses on natural language sentences. The resulting family of algorithms includes the expectation-maximization algorithm (EM) and its variant, Viterbi EM, as well as a so-called softmax-EM algorithm. The softmax-EM algorithm can be implemented with a simple and computationally efficient extension to standard EM. In our experiments of unsupervised dependency grammar learn- ing, we show that unambiguity regularization is beneficial to learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art.</p><p>5 0.50210673 <a title="124-lda-5" href="./emnlp-2012-Extracting_Opinion_Expressions_with_semi-Markov_Conditional_Random_Fields.html">51 emnlp-2012-Extracting Opinion Expressions with semi-Markov Conditional Random Fields</a></p>
<p>Author: Bishan Yang ; Claire Cardie</p><p>Abstract: Extracting opinion expressions from text is usually formulated as a token-level sequence labeling task tackled using Conditional Random Fields (CRFs). CRFs, however, do not readily model potentially useful segment-level information like syntactic constituent structure. Thus, we propose a semi-CRF-based approach to the task that can perform sequence labeling at the segment level. We extend the original semi-CRF model (Sarawagi and Cohen, 2004) to allow the modeling of arbitrarily long expressions while accounting for their likely syntactic structure when modeling segment boundaries. We evaluate performance on two opinion extraction tasks, and, in contrast to previous sequence labeling approaches to the task, explore the usefulness of segment- level syntactic parse features. Experimental results demonstrate that our approach outperforms state-of-the-art methods for both opinion expression tasks.</p><p>6 0.49405488 <a title="124-lda-6" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>7 0.49359947 <a title="124-lda-7" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>8 0.49317989 <a title="124-lda-8" href="./emnlp-2012-Left-to-Right_Tree-to-String_Decoding_with_Prediction.html">82 emnlp-2012-Left-to-Right Tree-to-String Decoding with Prediction</a></p>
<p>9 0.4916448 <a title="124-lda-9" href="./emnlp-2012-Exploiting_Reducibility_in_Unsupervised_Dependency_Parsing.html">46 emnlp-2012-Exploiting Reducibility in Unsupervised Dependency Parsing</a></p>
<p>10 0.48914969 <a title="124-lda-10" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>11 0.48888952 <a title="124-lda-11" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>12 0.48709419 <a title="124-lda-12" href="./emnlp-2012-Besting_the_Quiz_Master%3A_Crowdsourcing_Incremental_Classification_Games.html">23 emnlp-2012-Besting the Quiz Master: Crowdsourcing Incremental Classification Games</a></p>
<p>13 0.48684335 <a title="124-lda-13" href="./emnlp-2012-A_Comparison_of_Vector-based_Representations_for_Semantic_Composition.html">4 emnlp-2012-A Comparison of Vector-based Representations for Semantic Composition</a></p>
<p>14 0.48532054 <a title="124-lda-14" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<p>15 0.48505211 <a title="124-lda-15" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>16 0.48291609 <a title="124-lda-16" href="./emnlp-2012-Transforming_Trees_to_Improve_Syntactic_Convergence.html">127 emnlp-2012-Transforming Trees to Improve Syntactic Convergence</a></p>
<p>17 0.48253965 <a title="124-lda-17" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>18 0.48235515 <a title="124-lda-18" href="./emnlp-2012-A_Novel_Discriminative_Framework_for_Sentence-Level_Discourse_Analysis.html">7 emnlp-2012-A Novel Discriminative Framework for Sentence-Level Discourse Analysis</a></p>
<p>19 0.48144755 <a title="124-lda-19" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>20 0.48110953 <a title="124-lda-20" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
