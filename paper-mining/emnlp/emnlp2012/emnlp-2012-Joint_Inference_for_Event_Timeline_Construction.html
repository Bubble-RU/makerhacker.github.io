<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>72 emnlp-2012-Joint Inference for Event Timeline Construction</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-72" href="#">emnlp2012-72</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>72 emnlp-2012-Joint Inference for Event Timeline Construction</h1>
<br/><p>Source: <a title="emnlp-2012-72-pdf" href="http://aclweb.org/anthology//D/D12/D12-1062.pdf">pdf</a></p><p>Author: Quang Do ; Wei Lu ; Dan Roth</p><p>Abstract: This paper addresses the task of constructing a timeline of events mentioned in a given text. To accomplish that, we present a novel representation of the temporal structure of a news article based on time intervals. We then present an algorithmic approach that jointly optimizes the temporal structure by coupling local classifiers that predict associations and temporal relations between pairs of temporal entities with global constraints. Moreover, we present ways to leverage knowledge provided by event coreference to further improve the system performance. Overall, our experiments show that the joint inference model significantly outperformed the local classifiers by 9.2% of relative improvement in F1. The experiments also suggest that good event coreference could make remarkable contribution to a robust event timeline construction system.</p><p>Reference: <a title="emnlp-2012-72-reference" href="../emnlp2012_reference/emnlp-2012-Joint_Inference_for_Event_Timeline_Construction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We then present an algorithmic approach that jointly optimizes the temporal structure by coupling local classifiers that predict associations and temporal relations between pairs of temporal entities with global constraints. [sent-4, score-1.326]
</p><p>2 Moreover, we present ways to leverage knowledge provided by event coreference to further improve the system performance. [sent-5, score-0.814]
</p><p>3 The experiments also suggest that good event coreference could make remarkable contribution to a robust event timeline construction system. [sent-8, score-1.701]
</p><p>4 Most previous works devoted much efforts to the task of identifying relative temporal relations (such as before, or overlap) amongst events (Chambers 677  Time  t1e1e3/te25e4t3t4−→ e2  −∞  | |  e7 − e6  I1  | I2  |  I3  +•∞ |  |  Figure 1: A graphical illustration of our timeline representation. [sent-14, score-0.849]
</p><p>5 In practice, however, being able to automatically infer the correct time of occurrence associated with each event is crucial. [sent-20, score-0.685]
</p><p>6 Besides inferring the relative temporal relations amongst the events, we would also like to automatically infer a specific absolute time of occurrence for each event mentioned in the text. [sent-23, score-1.124]
</p><p>7 Unlike previous work, we associate each event with a specific absolute time interval inferred from the text. [sent-24, score-0.88]
</p><p>8 Moreover, we observed that event coreference can reveal important information for such a task. [sent-35, score-0.814]
</p><p>9 We propose that different event mentions that refer to the same event can be grouped together before classification and performing global inference. [sent-36, score-1.456]
</p><p>10 To the best of our knowl-  edge, our proposal of leveraging event coreference to support event timeline construction is novel. [sent-38, score-1.701]
</p><p>11 2  Background  We focus on the task of mapping event mentions in a news article to a timeline. [sent-42, score-0.84]
</p><p>12 We also distinguish between events and event mentions, where a unique event can be core-  ferred to by a set of explicit event mentions in an article. [sent-46, score-2.148]
</p><p>13 Formally, an event Ei is co-referred to by 678 a set of event mentions (ei1, ei2, . [sent-47, score-1.409]
</p><p>14 Each event mention e can be written as p(a1, a2, . [sent-51, score-0.75]
</p><p>15 In this work we focus on four temporal relations between two event mentions including before, after, overlap and no relation. [sent-58, score-1.224]
</p><p>16 We say that the time interval Ii precedes the time interval Ij on a timeline if and only if ti+ ≤ tj−, which also implies that Ii succeeds Ij if and only if ti− ≥ tj+. [sent-78, score-0.823]
</p><p>17 The event e4 is associated with the interval (−∞, +∞), indicating there is no knowledge aablo (u−t ∞its ,t+im∞e )o,f occurrence. [sent-91, score-0.826]
</p><p>18 We believe that such a timeline representation for temporally ordering events has several advantages over the temporal graph representations used in previous works (Chambers and Jurafsky, 2008; Yoshikawa et al. [sent-92, score-0.742]
</p><p>19 Unlike previous works, in our model the events are partially ordered in a single timeline, where each event is associated with a precise time interval. [sent-94, score-0.793]
</p><p>20 Furthermore, as we will show later, the use of time intervals within the timeline representation simplifies the global inference formulation and thus the inference process. [sent-97, score-0.746]
</p><p>21 2 shows a simplified temporal structure of event mentions and time intervals of an article in our model. [sent-102, score-1.425]
</p><p>22 , 2009; Denis and Muller, 2011), where such classifiers were trained to identify temporal relations between event mentions and a temporal expression. [sent-104, score-1.629]
</p><p>23 In our work, in order to construct absolute timeline of event mentions, temporal expressions are captured and normalized as absolute time intervals. [sent-105, score-1.32]
</p><p>24 The E–T classifiers are then used to assign event mentions to their contextually corresponding time intervals. [sent-106, score-0.948]
</p><p>25 Specifically, we do not require that event mentions and time expressions have to appear in the same sentence, and we do not requiretwo eventmentions haveto appearvery close to each other (e. [sent-111, score-0.858]
</p><p>26 , main event mentions in adjacent sentences) in order to be considered as candidate pairs for classification. [sent-113, score-0.819]
</p><p>27 Instead, we performed classifications over all pairs of event mentions and time intervals as well as over all pairs of event mentions. [sent-114, score-1.691]
</p><p>28 Specifically, our global inference model jointly optimizes the E-E relations amongst event mentions and their associations, E-T, with temporal information (intervals in our case). [sent-118, score-1.36]
</p><p>29 1 The Pairwise Classifiers We first describe our local classifiers that associate  event mention with time interval and classify temporal relations between event mentions, respectively. [sent-121, score-2.153]
</p><p>30 CE−T: is the E–T classifier that associates an event mention with a time interval. [sent-122, score-0.873]
</p><p>31 Given an event mention and a time interval, the classifier predicts  e1I e2eI23e4I3e5• • • en-1Imen Figure 2: A simplified temporal structure of an article. [sent-123, score-1.175]
</p><p>32 There are m time intervals I1 · · · Im and n event mentions e1 · · · en. [sent-124, score-1.05]
</p><p>33 A solid edge indicates an ·aIssociation between an interv·a·l aend an event mention, whereas a dash edge illustrates a temporal relation between two event mentions. [sent-125, score-1.609]
</p><p>34 CE−T(ei, Ij)  →  {0, 1},  ∀i, j, 1 ≤ i≤ n, 1≤ j ≤ m,  (1)  where n and m are the number of event mentions and time intervals in an article, respectively. [sent-127, score-1.05]
</p><p>35 CE−E: is the E–E classifier that identifies the temporal relation between two event mentions. [sent-128, score-1.034]
</p><p>36 We use the term temporal entity (or entity, for short) to refer to either an event mention or a time interval. [sent-134, score-1.143]
</p><p>37 Semantic Features‡: A set of semantic features, mostly related to the input event mentions: (i) whether the input event mentions have a common synonym from their synsets in WordNet (Fellbaum, 1998); (ii) whether the input event mentions have a common derivational form derived from WordNet. [sent-138, score-2.211]
</p><p>38 2 Joint Inference for Event Timeline To exploit the interaction among the temporal entities in an article, we optimize the predicted temporal structure, formed by predictions from CE−T and CE−E, w. [sent-152, score-0.695]
</p><p>39 fW ael la plsaoi dsen oofte ev tehnet set of event mention pairs by EE = {(ei, ej) ∈ sEe E|ei ∈ E, ej ∈ E, ip j}. [sent-166, score-0.882]
</p><p>40 o T tehme prediction probability of an event mention pair ee ∈ EE that takes temporal a rnela etvieonnt r, given by CE−E, iEs Ede tnhoatte tad by phee,ri . [sent-169, score-1.203]
</p><p>41 Similarly, we define a binary indicator variable yhee,ri of a pair of event mentions ee that takes on the value 1 iff ee is predicted to hold the relation r. [sent-171, score-1.069]
</p><p>42 The equality constraint (6) ensures that exactly one particular relation can be assigned to each event mention pair. [sent-176, score-0.838]
</p><p>43 In addition, we also require that each event is associated with only one time interval. [sent-177, score-0.685]
</p><p>44 closure of relations between event mentions with inequality constraints in (9), which states that if the pair (ei, ej) has a certain relation r1, and the pair (ej , ek) has the relation r2, then the relation r3 must be satisfied between ei and ek. [sent-188, score-1.262]
</p><p>45 Specifically, if two event mentions ei and ej are associated with two time intervals Ik and Il respectively, and Ik precedes Il in the timeline, then ei must happen before ej. [sent-191, score-1.453]
</p><p>46 4  Incorporating Knowledge from Event Coreference  One of the key contributions of our work is using event coreference information to enhance the timeline construction performance. [sent-200, score-1.094]
</p><p>47 -  The example below, extracted from an article published on 03/11/2003 in the Automatic Content Extraction (ACE), 2005, corpus6 serves to illustrate the significance of event coreference to our task. [sent-203, score-0.852]
</p><p>48 Now if we consider the event mention e21, it actually belongs to the implicit future interval I2 = [2003-03-1 1 23:59:59, +∞). [sent-209, score-0.99]
</p><p>49 Fortunately, precise knowledge from event coreference may help alleviate such a problem. [sent-212, score-0.814]
</p><p>50 The knowledge reveals that the 4 event mentions can be grouped into 2 distinct events: E1 = {e11 , e21, e31}, E2 = {e21}. [sent-213, score-0.802]
</p><p>51 If CE−T can make a strong prediction in associating fth Ce event mention e11 (or e31) to I2, instead of I1, the system will have a high chance to re-assign e21 to I2 based on principle (P1). [sent-214, score-0.775]
</p><p>52 Similarly,  if CE−E is effective in figuring out that some mention of event E1 occurs after some mention of E2, then all the mentions of E1 would be predicted to occur after all mentions in E2 according to (P2). [sent-215, score-1.283]
</p><p>53 We note that ifwe stop at step (2), we get the outputs of the local classifiers enhanced by event coreference knowledge. [sent-217, score-0.957]
</p><p>54 ossible event mention pairs and relations, we first pick the pair who globally obtains the highest probability for some relation. [sent-225, score-0.79]
</p><p>55 Next, we simply take the probability distribution of that event mention pair as the distribution over the relations, for the event pair. [sent-226, score-1.38]
</p><p>56 roups of event mentions, we first compute the total score of each relation, and select the relation which has the highest score. [sent-234, score-0.665]
</p><p>57 Next from the list of pairs of event mentions from the two groups, we select the pair which has the relation r* with highest score compared to all other pairs. [sent-235, score-0.9]
</p><p>58 The probability distribution of this pair will be used as the probability distribution of all event mention pairs between the two events. [sent-236, score-0.79]
</p><p>59 In both approaches, we assign the overlap relations to all pairs of event mentions in the same event with probability 1. [sent-237, score-1.511]
</p><p>60 The corpus contains a fairly diverse collection of annotated event mentions, without any specific focus on certain event types. [sent-242, score-1.214]
</p><p>61 Furthermore, event mentions in TimeBank are annotated with neither event arguments nor event coreference information. [sent-245, score-2.223]
</p><p>62 The corpus consists of articles annotated with event mentions (with event triggers and arguments) and event coreference information. [sent-247, score-2.269]
</p><p>63 We then hired an anno-  tator with expertise in the field to annotate the data with the following information: (i) event mention and time interval association, and (ii) the temporal relations between event mentions, including {b¯, a, ¯o}. [sent-254, score-2.01]
</p><p>64 There are totally 8312 event pairs from 20 documents, including no relation pairs. [sent-258, score-0.682]
</p><p>65 It associates an event mention with the closest time interval found in the same sentence. [sent-269, score-1.038]
</p><p>66 For the temporal rela-  tion between a pair of event mentions, the baseline treats the event mention that appears earlier in the text as temporally happening before the other mention. [sent-273, score-1.734]
</p><p>67 Next, we integrated event coreference knowledge into our systems (as described in Sec. [sent-285, score-0.814]
</p><p>68 Our observations showed that event mentions of an event may appear in close proximity with multiple time intervals in the text, making CE−T produce high prediction scores for many event mention-interval pairs. [sent-288, score-2.264]
</p><p>69 This, consequently, confuses MaxScore on the best association of the event and the time intervals, whereas SumScore overcomes the problem by averaging out the association scores. [sent-289, score-0.663]
</p><p>70 On the other hand, CE−E gets more benefit from MaxScore because CE−E works better on pairs of event mentions that appear closely in the text, which activate more valuable learning features. [sent-290, score-0.819]
</p><p>71 To evaluate our systems with event coreference knowledge, we first experimented our systems with gold event coreference as given by the ACE 2005 corpus. [sent-292, score-1.628]
</p><p>72 Table 2 shows the contribution of event coreference to our systems in the third group of the results. [sent-293, score-0.814]
</p><p>73 The results show that injecting knowledge from event coreference remarkably improved both the local classifiers and the joint inference model. [sent-294, score-1.045]
</p><p>74 Overall, the system that combined event coreference and the global inference model achieved the best performance, which significantly overtook all other compared systems. [sent-295, score-0.931]
</p><p>75 Specifically, it outperformed the baseline system, the local classifiers, and the joint inference model without event coreference with 80%, 25%, and 14% of relative improvement in F1, respectively. [sent-296, score-0.979]
</p><p>76 It also consistently outperformed the local classifiers enhanced with event coreference. [sent-297, score-0.774]
</p><p>77 We note that the precision and recall of CE−T in the joint inference model are the same because the inference model enforced each event mention to be associated with exactly one time interval. [sent-298, score-0.986]
</p><p>78 This is also true for the systems integrated with event coreference because our integration approaches assign only one time interval to an event mention. [sent-299, score-1.674]
</p><p>79 We next move to experimenting with automatically learned event coreference systems. [sent-300, score-0.814]
</p><p>80 periment, we re-trained the event coreference system described in Chen et al. [sent-309, score-0.814]
</p><p>81 The results show that by using a learned event coreference system, we achieved the same improvement trends as with gold event coreference. [sent-312, score-1.421]
</p><p>82 However, we did not obtain significant improvement when comparing with global inference without event coreference information. [sent-313, score-0.931]
</p><p>83 This result shows that the performance of an event coreference system can have a significant impact on the overall performance. [sent-314, score-0.814]
</p><p>84 While this suggests that a better event coreference system could potentially help the task more, it also opens the question whether event coreference can be benefited from our local classifiers through the use of a joint inference framework. [sent-315, score-1.859]
</p><p>85 We followed the settings of Chambers and Jurafsky (2008) to extract all event mention pairs that were annotated with before (or ibefore, “immediately before”) and after (or iafter) relations in 183 news articles in the corpus. [sent-332, score-0.859]
</p><p>86 We next injected the knowledge of an event coreference system trained on the ACE2005 corpus into our CE−E, and obtained a micro-averaged accuracy of 73. [sent-338, score-0.836]
</p><p>87 It was not surprising that event coreference did not help in this dataset because: (i) different domains the event coreference was trained on ACE 05 but applied on TimeBank, and (ii) different annotation guidelines on events in ACE 2005 and TimeBank. [sent-340, score-1.753]
</p><p>88 To do this, we first converted our data in Table 1 from intervals to time points and infer the temporal relations between the annotated event mentions and the time points: before, after, overlap, and unknown. [sent-342, score-1.506]
</p><p>89 We also made several changes to the constraints, including removing those in (7) since they are no longer required, and adding constraints that ensure –  the relation between a time point and an event mention takes exactly one value. [sent-344, score-0.906]
</p><p>90 In these challenges, several temporal-related tasks were defined including the tasks of identifying the temporal relation between an event mention and a temporal expression in the same  sentence, and recognizing temporal relations ofpairs of event mentions in adjacent sentences. [sent-356, score-2.684]
</p><p>91 Recently, there has been much work attempting to leverage Allen’s interval algebra of temporal relations to enforce global constraints on local predictions. [sent-358, score-0.739]
</p><p>92 They used greedy search to select the most appropriate configuration of temporal relations among events and temporal expressions. [sent-360, score-0.845]
</p><p>93 Our work, on the other hand, focuses on constructing an event timeline with time intervals, taking multiple local pairwise predictions into ajoint inference model and removing the restrictions on the positions of the temporal entities. [sent-368, score-1.461]
</p><p>94 Furthermore, we propose for the first time to use event coreference and evaluate the importance of its role in the task of event timeline construction. [sent-369, score-1.757]
</p><p>95 7  Conclusions and Future Work  We proposed an interval-based representation of the timeline of event mentions in an article. [sent-370, score-1.082]
</p><p>96 Our representation allowed us to formalize the joint inference model that can be solved efficiently, compared to a time point-based inference model, thus opening up the possibility of building more practical event temporal inference systems. [sent-371, score-1.228]
</p><p>97 We also showed that event coreference can naturally support timeline construction, and good event coreference led to significant im-  provement in the system performance. [sent-373, score-1.908]
</p><p>98 Specifically, when such gold event coreference knowledge was injected into the model, a significant improvement in the overall performance could be obtained. [sent-374, score-0.836]
</p><p>99 While our experiments suggest that the temporal classifiers can potentially help enhance the performance of event coreference, in future work we would like to investigate into coupling event coreference with other components in a global inference framework. [sent-375, score-1.965]
</p><p>100 A pairwise event coreference model, feature impact and evaluation for event coreference resolution. [sent-403, score-1.655]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('event', 0.607), ('temporal', 0.337), ('timeline', 0.28), ('coreference', 0.207), ('interval', 0.197), ('mentions', 0.195), ('intervals', 0.192), ('ce', 0.185), ('mention', 0.143), ('ei', 0.118), ('events', 0.108), ('muller', 0.103), ('timebank', 0.103), ('yoshikawa', 0.1), ('ee', 0.093), ('classifiers', 0.09), ('ej', 0.089), ('chambers', 0.08), ('verhagen', 0.08), ('dct', 0.077), ('inference', 0.07), ('denis', 0.07), ('relations', 0.063), ('ace', 0.059), ('relation', 0.058), ('time', 0.056), ('local', 0.053), ('maxscore', 0.051), ('jurafsky', 0.051), ('global', 0.047), ('tempeval', 0.044), ('implicit', 0.043), ('constraints', 0.042), ('amongst', 0.041), ('ij', 0.039), ('eiej', 0.039), ('sumscore', 0.039), ('article', 0.038), ('reasoning', 0.037), ('precedes', 0.037), ('mani', 0.037), ('ii', 0.037), ('associates', 0.035), ('endpoint', 0.033), ('bethard', 0.033), ('ilp', 0.033), ('classifier', 0.032), ('il', 0.031), ('restrictions', 0.031), ('formulation', 0.031), ('pustejovsky', 0.03), ('equality', 0.03), ('articles', 0.029), ('pairwise', 0.027), ('ik', 0.026), ('ofevent', 0.026), ('oofte', 0.026), ('overwrite', 0.026), ('tatu', 0.026), ('vef', 0.026), ('imposed', 0.026), ('january', 0.025), ('associating', 0.025), ('explicit', 0.024), ('associations', 0.024), ('outperformed', 0.024), ('im', 0.023), ('pair', 0.023), ('overlap', 0.022), ('rt', 0.022), ('associated', 0.022), ('bramsen', 0.022), ('sauri', 0.022), ('coherency', 0.022), ('injected', 0.022), ('saturated', 0.022), ('entities', 0.021), ('february', 0.02), ('timelines', 0.02), ('absolute', 0.02), ('illustration', 0.02), ('tense', 0.02), ('ti', 0.02), ('happen', 0.019), ('xr', 0.018), ('freund', 0.018), ('arl', 0.018), ('talukdar', 0.018), ('enforces', 0.018), ('gaizauskas', 0.018), ('allen', 0.018), ('joint', 0.018), ('guidelines', 0.017), ('marc', 0.017), ('connectives', 0.017), ('modal', 0.017), ('inequality', 0.017), ('triggers', 0.017), ('temporally', 0.017), ('pairs', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999887 <a title="72-tfidf-1" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>Author: Quang Do ; Wei Lu ; Dan Roth</p><p>Abstract: This paper addresses the task of constructing a timeline of events mentioned in a given text. To accomplish that, we present a novel representation of the temporal structure of a news article based on time intervals. We then present an algorithmic approach that jointly optimizes the temporal structure by coupling local classifiers that predict associations and temporal relations between pairs of temporal entities with global constraints. Moreover, we present ways to leverage knowledge provided by event coreference to further improve the system performance. Overall, our experiments show that the joint inference model significantly outperformed the local classifiers by 9.2% of relative improvement in F1. The experiments also suggest that good event coreference could make remarkable contribution to a robust event timeline construction system.</p><p>2 0.35611707 <a title="72-tfidf-2" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>Author: Heeyoung Lee ; Marta Recasens ; Angel Chang ; Mihai Surdeanu ; Dan Jurafsky</p><p>Abstract: We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable documents, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.</p><p>3 0.25741804 <a title="72-tfidf-3" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>Author: David McClosky ; Christopher D. Manning</p><p>Abstract: We present a distantly supervised system for extracting the temporal bounds of fluents (relations which only hold during certain times, such as attends school). Unlike previous pipelined approaches, our model does not assume independence between each fluent or even between named entities with known connections (parent, spouse, employer, etc.). Instead, we model what makes timelines of fluents consistent by learning cross-fluent constraints, potentially spanning entities as well. For example, our model learns that someone is unlikely to start a job at age two or to marry someone who hasn’t been born yet. Our system achieves a 36% error reduction over a pipelined baseline.</p><p>4 0.17788798 <a title="72-tfidf-4" href="./emnlp-2012-Joint_Learning_for_Coreference_Resolution_with_Markov_Logic.html">73 emnlp-2012-Joint Learning for Coreference Resolution with Markov Logic</a></p>
<p>Author: Yang Song ; Jing Jiang ; Wayne Xin Zhao ; Sujian Li ; Houfeng Wang</p><p>Abstract: Pairwise coreference resolution models must merge pairwise coreference decisions to generate final outputs. Traditional merging methods adopt different strategies such as the bestfirst method and enforcing the transitivity constraint, but most of these methods are used independently of the pairwise learning methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-201 1shared task on the same dataset. Compared with the best system from CoNLL2011, which employs a rule-based method, our system shows competitive performance.</p><p>5 0.16611141 <a title="72-tfidf-5" href="./emnlp-2012-Employing_Compositional_Semantics_and_Discourse_Consistency_in_Chinese_Event_Extraction.html">38 emnlp-2012-Employing Compositional Semantics and Discourse Consistency in Chinese Event Extraction</a></p>
<p>Author: Peifeng Li ; Guodong Zhou ; Qiaoming Zhu ; Libin Hou</p><p>Abstract: Current Chinese event extraction systems suffer much from two problems in trigger identification: unknown triggers and word segmentation errors to known triggers. To resolve these problems, this paper proposes two novel inference mechanisms to explore special characteristics in Chinese via compositional semantics inside Chinese triggers and discourse consistency between Chinese trigger mentions. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our approach over a strong baseline. 1</p><p>6 0.14161128 <a title="72-tfidf-6" href="./emnlp-2012-Learning-based_Multi-Sieve_Co-reference_Resolution_with_Knowledge.html">76 emnlp-2012-Learning-based Multi-Sieve Co-reference Resolution with Knowledge</a></p>
<p>7 0.12023105 <a title="72-tfidf-7" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>8 0.1075571 <a title="72-tfidf-8" href="./emnlp-2012-Resolving_Complex_Cases_of_Definite_Pronouns%3A_The_Winograd_Schema_Challenge.html">112 emnlp-2012-Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge</a></p>
<p>9 0.10223636 <a title="72-tfidf-9" href="./emnlp-2012-Monte_Carlo_MCMC%3A_Efficient_Inference_by_Approximate_Sampling.html">91 emnlp-2012-Monte Carlo MCMC: Efficient Inference by Approximate Sampling</a></p>
<p>10 0.092849538 <a title="72-tfidf-10" href="./emnlp-2012-Identifying_Event-related_Bursts_via_Social_Media_Activities.html">63 emnlp-2012-Identifying Event-related Bursts via Social Media Activities</a></p>
<p>11 0.091553375 <a title="72-tfidf-11" href="./emnlp-2012-Dynamic_Programming_for_Higher_Order_Parsing_of_Gap-Minding_Trees.html">37 emnlp-2012-Dynamic Programming for Higher Order Parsing of Gap-Minding Trees</a></p>
<p>12 0.089226298 <a title="72-tfidf-12" href="./emnlp-2012-Domain_Adaptation_for_Coreference_Resolution%3A_An_Adaptive_Ensemble_Approach.html">36 emnlp-2012-Domain Adaptation for Coreference Resolution: An Adaptive Ensemble Approach</a></p>
<p>13 0.08160165 <a title="72-tfidf-13" href="./emnlp-2012-An_Entity-Topic_Model_for_Entity_Linking.html">19 emnlp-2012-An Entity-Topic Model for Entity Linking</a></p>
<p>14 0.076512173 <a title="72-tfidf-14" href="./emnlp-2012-Using_Discourse_Information_for_Paraphrase_Extraction.html">135 emnlp-2012-Using Discourse Information for Paraphrase Extraction</a></p>
<p>15 0.075539373 <a title="72-tfidf-15" href="./emnlp-2012-Identifying_Constant_and_Unique_Relations_by_using_Time-Series_Text.html">62 emnlp-2012-Identifying Constant and Unique Relations by using Time-Series Text</a></p>
<p>16 0.075461775 <a title="72-tfidf-16" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>17 0.073287494 <a title="72-tfidf-17" href="./emnlp-2012-Generalizing_Sub-sentential_Paraphrase_Acquisition_across_Original_Signal_Type_of_Text_Pairs.html">58 emnlp-2012-Generalizing Sub-sentential Paraphrase Acquisition across Original Signal Type of Text Pairs</a></p>
<p>18 0.070783533 <a title="72-tfidf-18" href="./emnlp-2012-Discovering_Diverse_and_Salient_Threads_in_Document_Collections.html">33 emnlp-2012-Discovering Diverse and Salient Threads in Document Collections</a></p>
<p>19 0.06758377 <a title="72-tfidf-19" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>20 0.066066138 <a title="72-tfidf-20" href="./emnlp-2012-Learning_Verb_Inference_Rules_from_Linguistically-Motivated_Evidence.html">80 emnlp-2012-Learning Verb Inference Rules from Linguistically-Motivated Evidence</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.21), (1, 0.282), (2, -0.076), (3, -0.341), (4, 0.034), (5, -0.066), (6, -0.145), (7, -0.198), (8, 0.045), (9, 0.065), (10, -0.022), (11, -0.042), (12, 0.048), (13, -0.098), (14, -0.079), (15, -0.049), (16, -0.058), (17, -0.071), (18, 0.242), (19, -0.278), (20, -0.083), (21, -0.043), (22, 0.129), (23, 0.18), (24, 0.107), (25, 0.007), (26, -0.038), (27, 0.009), (28, -0.084), (29, -0.146), (30, 0.066), (31, -0.093), (32, -0.003), (33, -0.077), (34, -0.027), (35, -0.049), (36, 0.042), (37, -0.056), (38, 0.03), (39, -0.047), (40, 0.009), (41, 0.037), (42, -0.038), (43, -0.009), (44, 0.052), (45, -0.005), (46, -0.066), (47, 0.035), (48, 0.003), (49, -0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98784274 <a title="72-lsi-1" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>Author: Quang Do ; Wei Lu ; Dan Roth</p><p>Abstract: This paper addresses the task of constructing a timeline of events mentioned in a given text. To accomplish that, we present a novel representation of the temporal structure of a news article based on time intervals. We then present an algorithmic approach that jointly optimizes the temporal structure by coupling local classifiers that predict associations and temporal relations between pairs of temporal entities with global constraints. Moreover, we present ways to leverage knowledge provided by event coreference to further improve the system performance. Overall, our experiments show that the joint inference model significantly outperformed the local classifiers by 9.2% of relative improvement in F1. The experiments also suggest that good event coreference could make remarkable contribution to a robust event timeline construction system.</p><p>2 0.73141026 <a title="72-lsi-2" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>Author: David McClosky ; Christopher D. Manning</p><p>Abstract: We present a distantly supervised system for extracting the temporal bounds of fluents (relations which only hold during certain times, such as attends school). Unlike previous pipelined approaches, our model does not assume independence between each fluent or even between named entities with known connections (parent, spouse, employer, etc.). Instead, we model what makes timelines of fluents consistent by learning cross-fluent constraints, potentially spanning entities as well. For example, our model learns that someone is unlikely to start a job at age two or to marry someone who hasn’t been born yet. Our system achieves a 36% error reduction over a pipelined baseline.</p><p>3 0.68418843 <a title="72-lsi-3" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>Author: Heeyoung Lee ; Marta Recasens ; Angel Chang ; Mihai Surdeanu ; Dan Jurafsky</p><p>Abstract: We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable documents, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.</p><p>4 0.67081481 <a title="72-lsi-4" href="./emnlp-2012-Employing_Compositional_Semantics_and_Discourse_Consistency_in_Chinese_Event_Extraction.html">38 emnlp-2012-Employing Compositional Semantics and Discourse Consistency in Chinese Event Extraction</a></p>
<p>Author: Peifeng Li ; Guodong Zhou ; Qiaoming Zhu ; Libin Hou</p><p>Abstract: Current Chinese event extraction systems suffer much from two problems in trigger identification: unknown triggers and word segmentation errors to known triggers. To resolve these problems, this paper proposes two novel inference mechanisms to explore special characteristics in Chinese via compositional semantics inside Chinese triggers and discourse consistency between Chinese trigger mentions. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our approach over a strong baseline. 1</p><p>5 0.51256675 <a title="72-lsi-5" href="./emnlp-2012-Joint_Learning_for_Coreference_Resolution_with_Markov_Logic.html">73 emnlp-2012-Joint Learning for Coreference Resolution with Markov Logic</a></p>
<p>Author: Yang Song ; Jing Jiang ; Wayne Xin Zhao ; Sujian Li ; Houfeng Wang</p><p>Abstract: Pairwise coreference resolution models must merge pairwise coreference decisions to generate final outputs. Traditional merging methods adopt different strategies such as the bestfirst method and enforcing the transitivity constraint, but most of these methods are used independently of the pairwise learning methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-201 1shared task on the same dataset. Compared with the best system from CoNLL2011, which employs a rule-based method, our system shows competitive performance.</p><p>6 0.37109464 <a title="72-lsi-6" href="./emnlp-2012-Learning-based_Multi-Sieve_Co-reference_Resolution_with_Knowledge.html">76 emnlp-2012-Learning-based Multi-Sieve Co-reference Resolution with Knowledge</a></p>
<p>7 0.34721589 <a title="72-lsi-7" href="./emnlp-2012-Identifying_Event-related_Bursts_via_Social_Media_Activities.html">63 emnlp-2012-Identifying Event-related Bursts via Social Media Activities</a></p>
<p>8 0.32208812 <a title="72-lsi-8" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>9 0.28520542 <a title="72-lsi-9" href="./emnlp-2012-Discovering_Diverse_and_Salient_Threads_in_Document_Collections.html">33 emnlp-2012-Discovering Diverse and Salient Threads in Document Collections</a></p>
<p>10 0.27354521 <a title="72-lsi-10" href="./emnlp-2012-Monte_Carlo_MCMC%3A_Efficient_Inference_by_Approximate_Sampling.html">91 emnlp-2012-Monte Carlo MCMC: Efficient Inference by Approximate Sampling</a></p>
<p>11 0.25131494 <a title="72-lsi-11" href="./emnlp-2012-Resolving_Complex_Cases_of_Definite_Pronouns%3A_The_Winograd_Schema_Challenge.html">112 emnlp-2012-Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge</a></p>
<p>12 0.23800273 <a title="72-lsi-12" href="./emnlp-2012-Identifying_Constant_and_Unique_Relations_by_using_Time-Series_Text.html">62 emnlp-2012-Identifying Constant and Unique Relations by using Time-Series Text</a></p>
<p>13 0.23395169 <a title="72-lsi-13" href="./emnlp-2012-Domain_Adaptation_for_Coreference_Resolution%3A_An_Adaptive_Ensemble_Approach.html">36 emnlp-2012-Domain Adaptation for Coreference Resolution: An Adaptive Ensemble Approach</a></p>
<p>14 0.21909995 <a title="72-lsi-14" href="./emnlp-2012-Using_Discourse_Information_for_Paraphrase_Extraction.html">135 emnlp-2012-Using Discourse Information for Paraphrase Extraction</a></p>
<p>15 0.20345049 <a title="72-lsi-15" href="./emnlp-2012-Dynamic_Programming_for_Higher_Order_Parsing_of_Gap-Minding_Trees.html">37 emnlp-2012-Dynamic Programming for Higher Order Parsing of Gap-Minding Trees</a></p>
<p>16 0.20045546 <a title="72-lsi-16" href="./emnlp-2012-Learning_Verb_Inference_Rules_from_Linguistically-Motivated_Evidence.html">80 emnlp-2012-Learning Verb Inference Rules from Linguistically-Motivated Evidence</a></p>
<p>17 0.19992653 <a title="72-lsi-17" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>18 0.17710899 <a title="72-lsi-18" href="./emnlp-2012-An_Entity-Topic_Model_for_Entity_Linking.html">19 emnlp-2012-An Entity-Topic Model for Entity Linking</a></p>
<p>19 0.1519897 <a title="72-lsi-19" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>20 0.14950667 <a title="72-lsi-20" href="./emnlp-2012-Local_and_Global_Context_for_Supervised_and_Unsupervised_Metonymy_Resolution.html">85 emnlp-2012-Local and Global Context for Supervised and Unsupervised Metonymy Resolution</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.027), (16, 0.026), (25, 0.023), (34, 0.081), (45, 0.013), (60, 0.125), (63, 0.054), (64, 0.024), (65, 0.049), (71, 0.27), (73, 0.025), (74, 0.03), (76, 0.045), (80, 0.043), (86, 0.029), (95, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8059786 <a title="72-lda-1" href="./emnlp-2012-Joint_Inference_for_Event_Timeline_Construction.html">72 emnlp-2012-Joint Inference for Event Timeline Construction</a></p>
<p>Author: Quang Do ; Wei Lu ; Dan Roth</p><p>Abstract: This paper addresses the task of constructing a timeline of events mentioned in a given text. To accomplish that, we present a novel representation of the temporal structure of a news article based on time intervals. We then present an algorithmic approach that jointly optimizes the temporal structure by coupling local classifiers that predict associations and temporal relations between pairs of temporal entities with global constraints. Moreover, we present ways to leverage knowledge provided by event coreference to further improve the system performance. Overall, our experiments show that the joint inference model significantly outperformed the local classifiers by 9.2% of relative improvement in F1. The experiments also suggest that good event coreference could make remarkable contribution to a robust event timeline construction system.</p><p>2 0.77655715 <a title="72-lda-2" href="./emnlp-2012-Regularized_Interlingual_Projections%3A_Evaluation_on_Multilingual_Transliteration.html">111 emnlp-2012-Regularized Interlingual Projections: Evaluation on Multilingual Transliteration</a></p>
<p>Author: Jagadeesh Jagarlamudi ; Hal Daume III</p><p>Abstract: In this paper, we address the problem of building a multilingual transliteration system using an interlingual representation. Our approach uses international phonetic alphabet (IPA) to learn the interlingual representation and thus allows us to use any word and its IPA representation as a training example. Thus, our approach requires only monolingual resources: a phoneme dictionary that lists words and their IPA representations.1 By adding a phoneme dictionary of a new language, we can readily build a transliteration system into any of the existing previous languages, without the expense of all-pairs data or computation. We also propose a regularization framework for learning the interlingual representation, which accounts for language specific phonemic variability, and thus it can find better mappings between languages. Experimental results on the name transliteration task in five diverse languages show a maximum improvement of 29% accuracy and an average improvement of 17% accuracy compared to a state-of-the-art baseline system.</p><p>3 0.56005353 <a title="72-lda-3" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>Author: Heeyoung Lee ; Marta Recasens ; Angel Chang ; Mihai Surdeanu ; Dan Jurafsky</p><p>Abstract: We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable documents, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.</p><p>4 0.53938437 <a title="72-lda-4" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>Author: Taylor Berg-Kirkpatrick ; David Burkett ; Dan Klein</p><p>Abstract: We investigate two aspects of the empirical behavior of paired significance tests for NLP systems. First, when one system appears to outperform another, how does significance level relate in practice to the magnitude of the gain, to the size of the test set, to the similarity of the systems, and so on? Is it true that for each task there is a gain which roughly implies significance? We explore these issues across a range of NLP tasks using both large collections of past systems’ outputs and variants of single systems. Next, once significance levels are computed, how well does the standard i.i.d. notion of significance hold up in practical settings where future distributions are neither independent nor identically distributed, such as across domains? We explore this question using a range of test set variations for constituency parsing.</p><p>5 0.53726268 <a title="72-lda-5" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms ofweak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.</p><p>6 0.53324288 <a title="72-lda-6" href="./emnlp-2012-Multi-instance_Multi-label_Learning_for_Relation_Extraction.html">93 emnlp-2012-Multi-instance Multi-label Learning for Relation Extraction</a></p>
<p>7 0.53276432 <a title="72-lda-7" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>8 0.53230351 <a title="72-lda-8" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>9 0.5317291 <a title="72-lda-9" href="./emnlp-2012-Joint_Learning_for_Coreference_Resolution_with_Markov_Logic.html">73 emnlp-2012-Joint Learning for Coreference Resolution with Markov Logic</a></p>
<p>10 0.53023779 <a title="72-lda-10" href="./emnlp-2012-Multi-Domain_Learning%3A_When_Do_Domains_Matter%3F.html">92 emnlp-2012-Multi-Domain Learning: When Do Domains Matter?</a></p>
<p>11 0.5278005 <a title="72-lda-11" href="./emnlp-2012-Besting_the_Quiz_Master%3A_Crowdsourcing_Incremental_Classification_Games.html">23 emnlp-2012-Besting the Quiz Master: Crowdsourcing Incremental Classification Games</a></p>
<p>12 0.52518111 <a title="72-lda-12" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>13 0.52481687 <a title="72-lda-13" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>14 0.52463633 <a title="72-lda-14" href="./emnlp-2012-Explore_Person_Specific_Evidence_in_Web_Person_Name_Disambiguation.html">47 emnlp-2012-Explore Person Specific Evidence in Web Person Name Disambiguation</a></p>
<p>15 0.5235126 <a title="72-lda-15" href="./emnlp-2012-Enlarging_Paraphrase_Collections_through_Generalization_and_Instantiation.html">39 emnlp-2012-Enlarging Paraphrase Collections through Generalization and Instantiation</a></p>
<p>16 0.52348816 <a title="72-lda-16" href="./emnlp-2012-Wiki-ly_Supervised_Part-of-Speech_Tagging.html">138 emnlp-2012-Wiki-ly Supervised Part-of-Speech Tagging</a></p>
<p>17 0.52318758 <a title="72-lda-17" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>18 0.5231505 <a title="72-lda-18" href="./emnlp-2012-Probabilistic_Finite_State_Machines_for_Regression-based_MT_Evaluation.html">108 emnlp-2012-Probabilistic Finite State Machines for Regression-based MT Evaluation</a></p>
<p>19 0.52299285 <a title="72-lda-19" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>20 0.52285755 <a title="72-lda-20" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
