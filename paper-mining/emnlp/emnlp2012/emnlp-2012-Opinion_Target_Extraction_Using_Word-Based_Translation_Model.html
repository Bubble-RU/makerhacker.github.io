<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>101 emnlp-2012-Opinion Target Extraction Using Word-Based Translation Model</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-101" href="#">emnlp2012-101</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>101 emnlp-2012-Opinion Target Extraction Using Word-Based Translation Model</h1>
<br/><p>Source: <a title="emnlp-2012-101-pdf" href="http://aclweb.org/anthology//D/D12/D12-1123.pdf">pdf</a></p><p>Author: Kang Liu ; Liheng Xu ; Jun Zhao</p><p>Abstract: This paper proposes a novel approach to extract opinion targets based on wordbased translation model (WTM). At first, we apply WTM in a monolingual scenario to mine the associations between opinion targets and opinion words. Then, a graphbased algorithm is exploited to extract opinion targets, where candidate opinion relevance estimated from the mined associations, is incorporated with candidate importance to generate a global measure. By using WTM, our method can capture opinion relations more precisely, especially for long-span relations. In particular, compared with previous syntax-based methods, our method can effectively avoid noises from parsing errors when dealing with informal texts in large Web corpora. By using graph-based algorithm, opinion targets are extracted in a global process, which can effectively alleviate the problem of error propagation in traditional bootstrap-based methods, such as Double Propagation. The experimental results on three real world datasets in different sizes and languages show that our approach is more effective and robust than state-of-art methods. 1</p><p>Reference: <a title="emnlp-2012-101-reference" href="../emnlp2012_reference/emnlp-2012-Opinion_Target_Extraction_Using_Word-Based_Translation_Model_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn  Abstract This paper proposes a novel approach to extract opinion targets based on wordbased translation model (WTM). [sent-4, score-1.186]
</p><p>2 At first, we apply WTM in a monolingual scenario to mine the associations between opinion targets and opinion words. [sent-5, score-2.091]
</p><p>3 Then, a graphbased algorithm is exploited to extract opinion targets, where candidate opinion relevance estimated from the mined associations, is incorporated with candidate importance to generate a global measure. [sent-6, score-2.039]
</p><p>4 By using WTM, our method can capture opinion relations more precisely, especially for long-span relations. [sent-7, score-0.879]
</p><p>5 By using graph-based algorithm, opinion targets are extracted in a global process,  which can effectively alleviate the problem of error propagation in traditional bootstrap-based methods, such as Double Propagation. [sent-9, score-1.187]
</p><p>6 In opinion mining, one fundamental problem is opinion target extraction. [sent-14, score-1.728]
</p><p>7 For example, in the sentence of “The phone has a colorful and even amazing screen”, “screen” is an opinion target. [sent-17, score-0.982]
</p><p>8 In online product reviews, opinion targets often are products or product features, so this task is also named as product feature extraction in previous work (Hu et al. [sent-18, score-1.24]
</p><p>9 To extract opinion targets, many studies regarded opinion words as strong indicators (Hu et al. [sent-25, score-1.73]
</p><p>10 , 2010), which is based on the observation that opinion words are usually located around opinion targets, and there are associations between them. [sent-30, score-1.792]
</p><p>11 Therefore, most pervious methods iteratively extracted opinion targets depending upon the associations between opinion words and opinion targets (Qiu et al. [sent-31, score-3.202]
</p><p>12 If “colorful” and “amazing” had been known to be opinion words, “screen” is likely to be an opinion target in this domain. [sent-35, score-1.728]
</p><p>13 In addition, the extracted opinion targets can be used to expand more opinion words  according to their associations. [sent-36, score-1.974]
</p><p>14 Therefore, mining associations between opinion targets and opinion words is a key for opinion target extraction (Wu et al. [sent-38, score-3.016]
</p><p>15 , 2008), named as adjacent methods, employed the adjacent rule, where an opinion target was regarded to have opinion relations with the surrounding opinion words in a given window. [sent-43, score-2.708]
</p><p>16 However, because of the limitation of window size, opinion relations cannot be captured precisely, especially for long-span relations, which would hurt estimating associations between opinion targets and opinion words. [sent-44, score-2.967]
</p><p>17 If the syntactic relation between an opinion word and an opinion target satisfied a designed pattern, then there was an opinion relation between them. [sent-51, score-2.598]
</p><p>18 To overcome the weakness of the two kinds of methods mentioned above, we propose a novel unsupervised approach to extract opinion targets by using word-based translation model (WTM). [sent-60, score-1.167]
</p><p>19 We formulate identifying opinion relations between opinion targets and opinion words as a word alignment task. [sent-61, score-2.88]
</p><p>20 We argue that an opinion target can find its corresponding modifier through monolingual word alignment. [sent-62, score-0.911]
</p><p>21 For example in Figure 1, the opinion words “colorful” and “amazing” are aligned with the target “screen” through word alignment. [sent-63, score-0.907]
</p><p>22 To this end, we use WTM to perform monolingual word alignment for mining associations between opinion targets and opinion words. [sent-64, score-2.169]
</p><p>23 Compared with adjacent methods, WTM doesn’t identify opinion relations between words in a given window, so long-span relations can be effectively captured (Liu et al. [sent-67, score-0.986]
</p><p>24 In addition, by using WTM, our method can capture the “one-to-many” or “many-to-one” relations (“one-to-many” means that, in a sentence one opinion word modifies several opinion targets, and “many-to-one” means several opinion words modify one opinion target). [sent-71, score-3.418]
</p><p>25 Thus, it’s reasonable to expect that WTM is likely to yield better performance than traditional methods for mining associations between opinion targets and opinion words. [sent-72, score-2.101]
</p><p>26 Based on the mined associations, we extract opinion targets in a ranking framework. [sent-73, score-1.153]
</p><p>27 All nouns/noun phrases are regarded as opinion target candidates. [sent-74, score-0.933]
</p><p>28 Then a graph-based algorithm is exploited to assign confidences to each candidate, in which candidate opinion relevance and  importance are incorporated to generate a global measure. [sent-75, score-1.091]
</p><p>29 At last, the candidates with higher ranks are extracted as opinion targets. [sent-76, score-0.879]
</p><p>30 , 2011), we don’t extract opinion targets iteratively based on the bootstrapping strategy, such as Double Propagation (Qiu et al. [sent-80, score-1.152]
</p><p>31 1) We formulate the opinion relation identification between opinion targets and opinion words as a word alignment task. [sent-84, score-2.856]
</p><p>32 2) We propose a graph-based algorithm for opinion target extraction in which candidate opinion relevance and importance are incorporated into a unified graph to estimate candidate confidence. [sent-87, score-2.048]
</p><p>33 Then the candidates with higher confidence scores are extracted as opinion targets (in Section 3. [sent-88, score-1.238]
</p><p>34 2  Related Work  Many studies have focused on the task of opinion target extraction, such as (Hu et al. [sent-99, score-0.888]
</p><p>35 In supervised approaches, the opinion target extraction task was usually regarded as a sequence labeling task (Jin et al. [sent-110, score-0.944]
</p><p>36 (2010) proposed a Skip-Tree CRF model for opinion target extraction. [sent-119, score-0.888]
</p><p>37 (2009) utilized a SVM classifier to identify relations between opinion targets and opinion expressions by leveraging phrase dependency parsing. [sent-122, score-2.032]
</p><p>38 In unsupervised methods, most approaches regarded opinion words as the important indicators for opinion targets (Hu et al. [sent-124, score-1.985]
</p><p>39 The basic idea was that reviewers often 1348  use the same opinion words when they comment on the similar opinion targets. [sent-130, score-1.68]
</p><p>40 The extraction procedure was often a bootstrapping process which extracted opinion words and opinion targets iteratively, depending upon their associations. [sent-131, score-2.016]
</p><p>41 (2005) used syntactic patterns to extract opinion target candidates. [sent-133, score-0.934]
</p><p>42 The adjective nearest to the frequent explicit feature was extracted as an opinion word. [sent-137, score-0.906]
</p><p>43 Then the extracted opinion words were used to extract infrequent opinion targets. [sent-138, score-1.719]
</p><p>44 (2009, 2011) proposed a Double Propagation method to expand a domain sentiment lexicon and an opinion target set iteratively. [sent-142, score-0.908]
</p><p>45 They exploited direct dependency relations between words to extract opinion targets and opinion words iteratively. [sent-143, score-2.045]
</p><p>46 , 1999) algorithm to compute the feature relevance scores, which were simply multiplied by the log of feature frequencies to rank the extracted opinion targets. [sent-151, score-0.919]
</p><p>47 2) Candidate confidence estimation: Based on these associations, we exploit a graph-based algorithm to compute the confidence of each opinion target candidate. [sent-155, score-1.054]
</p><p>48 Then the candidates with higher confidence scores are extracted as opinion targets. [sent-156, score-0.962]
</p><p>49 2 Mining associations between opinion targets and opinion words using Wordbased Translation Model This component is to identify potential opinion relations in sentences and estimate associations between opinion targets and opinion words. [sent-158, score-5.032]
</p><p>50 We assume opinion targets and opinion words respectively to be nouns/noun phrases and adjectives, which have been widely adopted in previous work (Hu et al. [sent-159, score-1.986]
</p><p>51 Thus, our aim is to find potential opinion relations between nouns/noun phrases and adjectives in sentences, and calculate the associations between them. [sent-164, score-1.035]
</p><p>52 As mentioned in the first section, we formulate opinion relation identification as a word alignment task. [sent-165, score-0.9]
</p><p>53 Moreover, these models may capture “one-to-many” or “many-to-one” opinion relations (mentioned in the first section). [sent-194, score-0.879]
</p><p>54 We can see that our method using WTM can successfully capture associations between opinion targets and opinion words. [sent-206, score-2.068]
</p><p>55 0 f0 t2w0 6a re  Table 1: Examples of associations between opinion targets and opinion words. [sent-210, score-2.068]
</p><p>56 3 Candidate Confidence Estimation  In this component, we compute the confidence of each opinion target candidate and rank them. [sent-212, score-1.059]
</p><p>57 The candidates with higher confidence are regarded as the opinion targets. [sent-213, score-0.973]
</p><p>58 Opinion Relevance reflects the degree that a candidate is associated to opinion words. [sent-215, score-0.928]
</p><p>59 If an adjective has higher confidence to be an opinion word, the noun/noun phrase it modifies will have higher confidence to be an opinion target. [sent-216, score-1.933]
</p><p>60 1350 Similarly, if a noun/noun phrase has higher confidence to be an opinion target, the adjective which modifies it will be highly possible to be an opinion word. [sent-217, score-1.85]
</p><p>61 We assign an importance score to an opinion target candidate f according to its tf- idf score, which is further normalized by the sum of tf- idf scores of all candidates. [sent-220, score-1.017]
</p><p>62 An edge between a noun/noun phrase and an adjective represents that there is an opinion relation between them. [sent-224, score-0.923]
</p><p>63 Opinion Target Candidates (nouns/noun phrases)  Opinion Word Candidates (adjectives)  Figure 2: Bipartite graph for modeling relations between opinion targets and opinion words 1 http://books. [sent-237, score-1.995]
</p><p>64 M is matrix, a m n matrix,  (6)  confidence vector at candidate confidence an opinion relevance where M i, j is the  associated weight between a noun/noun phrase iand an adjective j . [sent-241, score-1.223]
</p><p>65 To consider the candidate importance scores, we introduce a reallocate condition: combining the candidate opinion relevance with the candidate importance at each step. [sent-242, score-1.247]
</p><p>66 When   1 , the candidate confidence is completely determined by the candidate importance; and when   0 , the candidate confidence is determined by the candidate opinion relevance. [sent-245, score-1.358]
</p><p>67 Using this equation, we estimate confidences for opinion target candidates. [sent-254, score-0.905]
</p><p>68 The candidates with higher confidence scores than the threshold will be extracted as the opinion targets. [sent-255, score-0.962]
</p><p>69 Then the opinion targets in Large were manually annotated as the gold standard for evaluations. [sent-275, score-1.116]
</p><p>70 Then two annotators were required to judge whether every noun/noun phrase is opinion target or not. [sent-278, score-0.908]
</p><p>71 In total, we respectively obtain 1,112, 1,241 and 1,850 opinion targets in Hotel, MP3 and Restaurant. [sent-282, score-1.116]
</p><p>72 , 2004), which extracted opinion targets by using adjacent rule. [sent-347, score-1.17]
</p><p>73 , 2011), which used Double Propagation algorithm to extract opinion targets depending on syntactic relations between words. [sent-349, score-1.176]
</p><p>74 They extracted opinion targets candidates using syntactic patterns and other specific patterns. [sent-352, score-1.18]
</p><p>75 Then HITS (Kleinberg  1999) algorithm combined with candidate frequency is employed to rank the results for opinion target extraction. [sent-353, score-0.976]
</p><p>76 Hu is selected to represent adjacent methods for opinion target extraction. [sent-354, score-0.924]
</p><p>77 Ours denotes full model of our method, in which we use IBM-3 model for identifying opinion relations between words. [sent-362, score-0.894]
</p><p>78 This indicates that our method based on word-based translation model is  effective for opinion target extraction. [sent-369, score-0.918]
</p><p>79 The reason is that graph-based methods extract opinion targets in a global framework and they can effectively avoid the error propagation made by traditional methods based on Double Propagation. [sent-373, score-1.19]
</p><p>80 We believe the reason is that Ours consider the opinion relevance and the candidate importance in a unified graph-based framework. [sent-375, score-1.03]
</p><p>81 By contrast, Zhang only simply plus opinion relevance with frequency to determine the candidate confidence. [sent-376, score-0.989]
</p><p>82 This indicates that our method is more effective for opinion target extraction than state-of-art methods, especially for large corpora. [sent-382, score-0.915]
</p><p>83 On the other side, Ours uses WTM other than parsing to identify opinion relations between words, and the noises made by inaccurate parsing can be avoided. [sent-384, score-0.966]
</p><p>84 An Example In Table 6, we show top 10 opinion targets extracted by Hu, DP, Zhang and Ours in MP3 of Large. [sent-390, score-1.134]
</p><p>85 From these examples, we can see Ours extracts more correct opinion targets than others. [sent-393, score-1.116]
</p><p>86 Moreover, Ours considers candidate importance besides opinion relevance, so some specific 1353 opinion targets are ranked to the fore, such as “voice recorder”, “fm radio” and “lcd screen”. [sent-396, score-2.085]
</p><p>87 3  Effect of Word-based Translation Model  In this subsection, we aim to prove the effectiveness of our WTM for estimating associations between opinion targets and opinion words. [sent-398, score-2.082]
</p><p>88 4) is used to estimate associations between opinion targets and opinion words. [sent-406, score-2.068]
</p><p>89 It indicates that WTM is effective for identifying opinion relations, which makes the estimation of the associations be more precise. [sent-412, score-0.952]
</p><p>90 4 Effect of Our Graph-based Method In this subsection, we aim to prove the effectiveness of our graph-based method for opinion target extraction. [sent-414, score-0.902]
</p><p>91 Both WTM_DP and WTM_HITS use WTM to mine associations between opinion targets and opinion words. [sent-416, score-2.068]
</p><p>92 2009) to  extract opinion targets, which only consider the candidate opinion relevance. [sent-419, score-1.789]
</p><p>93 (2010) to extract opinion targets, which consider both candidate opinion relevance and frequency. [sent-421, score-1.85]
</p><p>94 It indicates that candidate importance and candidate opinion relevance are both important for candidate confidence estimation. [sent-442, score-1.289]
</p><p>95 The performance of opinion target extraction benefits from their combination. [sent-443, score-0.915]
</p><p>96 Experimental results when varying  5 Conclusions and Future Work This paper proposes a novel graph-based approach to extract opinion targets using WTM. [sent-447, score-1.137]
</p><p>97 Compared with previous adjacent methods and syntax-based methods, by using WTM, our method can capture opinion relations more precisely and therefore be more effective for opinion target extraction, especially for large informal Web corpora. [sent-448, score-1.85]
</p><p>98 Meanwhile, we will add some syntactic information into WTM to constrain the word alignment process, in order to identify opinion relations between words more precisely. [sent-451, score-0.941]
</p><p>99 Moreover, we believe that there are  some verbs or nouns can be opinion words and they may be helpful for opinion target extraction. [sent-452, score-1.728]
</p><p>100 And we think that it’s useful to add some prior knowledge of opinion words (sentiment lexicon) in our model for estimating candidate opinion relevance. [sent-453, score-1.768]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('opinion', 0.84), ('targets', 0.276), ('wtm', 0.208), ('qiu', 0.126), ('associations', 0.112), ('reviews', 0.107), ('hu', 0.103), ('candidate', 0.088), ('confidence', 0.083), ('amazing', 0.066), ('pibm', 0.066), ('double', 0.064), ('screen', 0.063), ('relevance', 0.061), ('zhang', 0.055), ('bing', 0.054), ('dp', 0.049), ('adjective', 0.048), ('target', 0.048), ('waj', 0.047), ('alignment', 0.045), ('wtms', 0.044), ('ding', 0.042), ('liu', 0.042), ('importance', 0.041), ('wu', 0.04), ('relations', 0.039), ('propagation', 0.038), ('phone', 0.038), ('colorful', 0.038), ('customer', 0.036), ('chinese', 0.036), ('adjacent', 0.036), ('popescu', 0.033), ('aj', 0.033), ('mining', 0.033), ('informal', 0.033), ('translation', 0.03), ('noises', 0.03), ('hotel', 0.03), ('regarded', 0.029), ('exploited', 0.029), ('adjectives', 0.028), ('extraction', 0.027), ('wang', 0.027), ('product', 0.026), ('review', 0.026), ('patterns', 0.025), ('wn', 0.025), ('wj', 0.024), ('monolingual', 0.023), ('wa', 0.023), ('popsecu', 0.022), ('ct', 0.022), ('extract', 0.021), ('candidates', 0.021), ('customers', 0.021), ('bipartite', 0.021), ('sentiment', 0.02), ('limitation', 0.02), ('parsing', 0.02), ('phrase', 0.02), ('opinions', 0.02), ('products', 0.019), ('modifies', 0.019), ('chun', 0.019), ('guang', 0.019), ('fertility', 0.019), ('yuanbin', 0.019), ('wordbased', 0.019), ('aligned', 0.019), ('datasets', 0.018), ('jin', 0.018), ('li', 0.018), ('extracted', 0.018), ('subsection', 0.018), ('identify', 0.017), ('jiajun', 0.017), ('xiaowen', 0.017), ('confidences', 0.017), ('restaurant', 0.016), ('meanwhile', 0.016), ('vertices', 0.016), ('baselines', 0.016), ('qi', 0.016), ('mined', 0.016), ('phrases', 0.016), ('effectively', 0.015), ('relation', 0.015), ('moreover', 0.015), ('incorporated', 0.015), ('syntaxbased', 0.015), ('denotes', 0.015), ('bootstrapping', 0.015), ('align', 0.014), ('adopted', 0.014), ('precisely', 0.014), ('wi', 0.014), ('prove', 0.014), ('hits', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="101-tfidf-1" href="./emnlp-2012-Opinion_Target_Extraction_Using_Word-Based_Translation_Model.html">101 emnlp-2012-Opinion Target Extraction Using Word-Based Translation Model</a></p>
<p>Author: Kang Liu ; Liheng Xu ; Jun Zhao</p><p>Abstract: This paper proposes a novel approach to extract opinion targets based on wordbased translation model (WTM). At first, we apply WTM in a monolingual scenario to mine the associations between opinion targets and opinion words. Then, a graphbased algorithm is exploited to extract opinion targets, where candidate opinion relevance estimated from the mined associations, is incorporated with candidate importance to generate a global measure. By using WTM, our method can capture opinion relations more precisely, especially for long-span relations. In particular, compared with previous syntax-based methods, our method can effectively avoid noises from parsing errors when dealing with informal texts in large Web corpora. By using graph-based algorithm, opinion targets are extracted in a global process, which can effectively alleviate the problem of error propagation in traditional bootstrap-based methods, such as Double Propagation. The experimental results on three real world datasets in different sizes and languages show that our approach is more effective and robust than state-of-art methods. 1</p><p>2 0.42927575 <a title="101-tfidf-2" href="./emnlp-2012-Extracting_Opinion_Expressions_with_semi-Markov_Conditional_Random_Fields.html">51 emnlp-2012-Extracting Opinion Expressions with semi-Markov Conditional Random Fields</a></p>
<p>Author: Bishan Yang ; Claire Cardie</p><p>Abstract: Extracting opinion expressions from text is usually formulated as a token-level sequence labeling task tackled using Conditional Random Fields (CRFs). CRFs, however, do not readily model potentially useful segment-level information like syntactic constituent structure. Thus, we propose a semi-CRF-based approach to the task that can perform sequence labeling at the segment level. We extend the original semi-CRF model (Sarawagi and Cohen, 2004) to allow the modeling of arbitrarily long expressions while accounting for their likely syntactic structure when modeling segment boundaries. We evaluate performance on two opinion extraction tasks, and, in contrast to previous sequence labeling approaches to the task, explore the usefulness of segment- level syntactic parse features. Experimental results demonstrate that our approach outperforms state-of-the-art methods for both opinion expression tasks.</p><p>3 0.31066528 <a title="101-tfidf-3" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>Author: Jianxing Yu ; Zheng-Jun Zha ; Tat-Seng Chua</p><p>Abstract: This paper proposes to generate appropriate answers for opinion questions about products by exploiting the hierarchical organization of consumer reviews. The hierarchy organizes product aspects as nodes following their parent-child relations. For each aspect, the reviews and corresponding opinions on this aspect are stored. We develop a new framework for opinion Questions Answering, which enables accurate question analysis and effective answer generation by making use the hierarchy. In particular, we first identify the (explicit/implicit) product aspects asked in the questions and their sub-aspects by referring to the hierarchy. We then retrieve the corresponding review fragments relevant to the aspects from the hierarchy. In order to gener- ate appropriate answers from the review fragments, we develop a multi-criteria optimization approach for answer generation by simultaneously taking into account review salience, coherence, diversity, and parent-child relations among the aspects. We conduct evaluations on 11 popular products in four domains. The evaluated corpus contains 70,359 consumer reviews and 220 questions on these products. Experimental results demonstrate the effectiveness of our approach.</p><p>4 0.13205442 <a title="101-tfidf-4" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>Author: Lizhen Qu ; Rainer Gemulla ; Gerhard Weikum</p><p>Abstract: We propose the weakly supervised MultiExperts Model (MEM) for analyzing the semantic orientation of opinions expressed in natural language reviews. In contrast to most prior work, MEM predicts both opinion polarity and opinion strength at the level of individual sentences; such fine-grained analysis helps to understand better why users like or dislike the entity under review. A key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength. For this reason, MEM is weakly supervised: It starts with potentially noisy indicators obtained from coarse-grained training data (i.e., document-level ratings), a small set of diverse base predictors, and, if available, small amounts of fine-grained training data. We integrate these noisy indicators into a unified probabilistic framework using ideas from ensemble learning and graph-based semi-supervised learning. Our experiments indicate that MEM outperforms state-of-the-art methods by a significant margin.</p><p>5 0.10694923 <a title="101-tfidf-5" href="./emnlp-2012-Collocation_Polarity_Disambiguation_Using_Web-based_Pseudo_Contexts.html">28 emnlp-2012-Collocation Polarity Disambiguation Using Web-based Pseudo Contexts</a></p>
<p>Author: Yanyan Zhao ; Bing Qin ; Ting Liu</p><p>Abstract: This paper focuses on the task of collocation polarity disambiguation. The collocation refers to a binary tuple of a polarity word and a target (such as ⟨long, battery life⟩ or ⟨long, ast atratrguep⟩t) (, siunc whh aisch ⟨ ltohneg s,en btatitmeernyt l iofrei⟩en otrat ⟨iolonn gof, tshtaer polarity wwohirdch (“long”) changes along owniothf different targets (“battery life” or “startup”). To disambiguate a collocation’s polarity, previous work always turned to investigate the polarities of its surrounding contexts, and then assigned the majority polarity to the collocation. However, these contexts are limited, thus the resulting polarity is insufficient to be reliable. We therefore propose an unsupervised three-component framework to expand some pseudo contexts from web, to help disambiguate a collocation’s polarity.Without using any additional labeled data, experiments , show that our method is effective.</p><p>6 0.096132211 <a title="101-tfidf-6" href="./emnlp-2012-Why_Question_Answering_using_Sentiment_Analysis_and_Word_Classes.html">137 emnlp-2012-Why Question Answering using Sentiment Analysis and Word Classes</a></p>
<p>7 0.06650617 <a title="101-tfidf-7" href="./emnlp-2012-Word_Salad%3A_Relating_Food_Prices_and_Descriptions.html">139 emnlp-2012-Word Salad: Relating Food Prices and Descriptions</a></p>
<p>8 0.062854685 <a title="101-tfidf-8" href="./emnlp-2012-Do_Neighbours_Help%3F_An_Exploration_of_Graph-based_Algorithms_for_Cross-domain_Sentiment_Classification.html">34 emnlp-2012-Do Neighbours Help? An Exploration of Graph-based Algorithms for Cross-domain Sentiment Classification</a></p>
<p>9 0.049757876 <a title="101-tfidf-9" href="./emnlp-2012-Resolving_Complex_Cases_of_Definite_Pronouns%3A_The_Winograd_Schema_Challenge.html">112 emnlp-2012-Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge</a></p>
<p>10 0.041389722 <a title="101-tfidf-10" href="./emnlp-2012-Active_Learning_for_Imbalanced_Sentiment_Classification.html">15 emnlp-2012-Active Learning for Imbalanced Sentiment Classification</a></p>
<p>11 0.040879786 <a title="101-tfidf-11" href="./emnlp-2012-Streaming_Analysis_of_Discourse_Participants.html">120 emnlp-2012-Streaming Analysis of Discourse Participants</a></p>
<p>12 0.040204883 <a title="101-tfidf-12" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>13 0.038409796 <a title="101-tfidf-13" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>14 0.038161315 <a title="101-tfidf-14" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>15 0.037322041 <a title="101-tfidf-15" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>16 0.036577236 <a title="101-tfidf-16" href="./emnlp-2012-Document-Wide_Decoding_for_Phrase-Based_Statistical_Machine_Translation.html">35 emnlp-2012-Document-Wide Decoding for Phrase-Based Statistical Machine Translation</a></p>
<p>17 0.034587175 <a title="101-tfidf-17" href="./emnlp-2012-Part-of-Speech_Tagging_for_Chinese-English_Mixed_Texts_with_Dynamic_Features.html">106 emnlp-2012-Part-of-Speech Tagging for Chinese-English Mixed Texts with Dynamic Features</a></p>
<p>18 0.032554097 <a title="101-tfidf-18" href="./emnlp-2012-Bilingual_Lexicon_Extraction_from_Comparable_Corpora_Using_Label_Propagation.html">25 emnlp-2012-Bilingual Lexicon Extraction from Comparable Corpora Using Label Propagation</a></p>
<p>19 0.032391232 <a title="101-tfidf-19" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>20 0.029255068 <a title="101-tfidf-20" href="./emnlp-2012-Translation_Model_Based_Cross-Lingual_Language_Model_Adaptation%3A_from_Word_Models_to_Phrase_Models.html">128 emnlp-2012-Translation Model Based Cross-Lingual Language Model Adaptation: from Word Models to Phrase Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.157), (1, 0.077), (2, 0.005), (3, 0.33), (4, 0.26), (5, -0.196), (6, -0.258), (7, -0.067), (8, -0.201), (9, -0.043), (10, 0.005), (11, 0.054), (12, 0.113), (13, -0.043), (14, -0.117), (15, 0.093), (16, 0.009), (17, -0.462), (18, -0.011), (19, 0.041), (20, 0.037), (21, -0.046), (22, -0.146), (23, -0.096), (24, -0.089), (25, 0.011), (26, -0.147), (27, -0.018), (28, -0.081), (29, 0.036), (30, 0.006), (31, -0.036), (32, 0.002), (33, 0.022), (34, -0.002), (35, -0.009), (36, 0.004), (37, -0.02), (38, -0.006), (39, 0.013), (40, -0.008), (41, -0.015), (42, -0.035), (43, 0.006), (44, 0.065), (45, -0.014), (46, -0.049), (47, -0.005), (48, 0.0), (49, 0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98714781 <a title="101-lsi-1" href="./emnlp-2012-Opinion_Target_Extraction_Using_Word-Based_Translation_Model.html">101 emnlp-2012-Opinion Target Extraction Using Word-Based Translation Model</a></p>
<p>Author: Kang Liu ; Liheng Xu ; Jun Zhao</p><p>Abstract: This paper proposes a novel approach to extract opinion targets based on wordbased translation model (WTM). At first, we apply WTM in a monolingual scenario to mine the associations between opinion targets and opinion words. Then, a graphbased algorithm is exploited to extract opinion targets, where candidate opinion relevance estimated from the mined associations, is incorporated with candidate importance to generate a global measure. By using WTM, our method can capture opinion relations more precisely, especially for long-span relations. In particular, compared with previous syntax-based methods, our method can effectively avoid noises from parsing errors when dealing with informal texts in large Web corpora. By using graph-based algorithm, opinion targets are extracted in a global process, which can effectively alleviate the problem of error propagation in traditional bootstrap-based methods, such as Double Propagation. The experimental results on three real world datasets in different sizes and languages show that our approach is more effective and robust than state-of-art methods. 1</p><p>2 0.91249681 <a title="101-lsi-2" href="./emnlp-2012-Extracting_Opinion_Expressions_with_semi-Markov_Conditional_Random_Fields.html">51 emnlp-2012-Extracting Opinion Expressions with semi-Markov Conditional Random Fields</a></p>
<p>Author: Bishan Yang ; Claire Cardie</p><p>Abstract: Extracting opinion expressions from text is usually formulated as a token-level sequence labeling task tackled using Conditional Random Fields (CRFs). CRFs, however, do not readily model potentially useful segment-level information like syntactic constituent structure. Thus, we propose a semi-CRF-based approach to the task that can perform sequence labeling at the segment level. We extend the original semi-CRF model (Sarawagi and Cohen, 2004) to allow the modeling of arbitrarily long expressions while accounting for their likely syntactic structure when modeling segment boundaries. We evaluate performance on two opinion extraction tasks, and, in contrast to previous sequence labeling approaches to the task, explore the usefulness of segment- level syntactic parse features. Experimental results demonstrate that our approach outperforms state-of-the-art methods for both opinion expression tasks.</p><p>3 0.56010556 <a title="101-lsi-3" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>Author: Jianxing Yu ; Zheng-Jun Zha ; Tat-Seng Chua</p><p>Abstract: This paper proposes to generate appropriate answers for opinion questions about products by exploiting the hierarchical organization of consumer reviews. The hierarchy organizes product aspects as nodes following their parent-child relations. For each aspect, the reviews and corresponding opinions on this aspect are stored. We develop a new framework for opinion Questions Answering, which enables accurate question analysis and effective answer generation by making use the hierarchy. In particular, we first identify the (explicit/implicit) product aspects asked in the questions and their sub-aspects by referring to the hierarchy. We then retrieve the corresponding review fragments relevant to the aspects from the hierarchy. In order to gener- ate appropriate answers from the review fragments, we develop a multi-criteria optimization approach for answer generation by simultaneously taking into account review salience, coherence, diversity, and parent-child relations among the aspects. We conduct evaluations on 11 popular products in four domains. The evaluated corpus contains 70,359 consumer reviews and 220 questions on these products. Experimental results demonstrate the effectiveness of our approach.</p><p>4 0.26892853 <a title="101-lsi-4" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>Author: Lizhen Qu ; Rainer Gemulla ; Gerhard Weikum</p><p>Abstract: We propose the weakly supervised MultiExperts Model (MEM) for analyzing the semantic orientation of opinions expressed in natural language reviews. In contrast to most prior work, MEM predicts both opinion polarity and opinion strength at the level of individual sentences; such fine-grained analysis helps to understand better why users like or dislike the entity under review. A key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength. For this reason, MEM is weakly supervised: It starts with potentially noisy indicators obtained from coarse-grained training data (i.e., document-level ratings), a small set of diverse base predictors, and, if available, small amounts of fine-grained training data. We integrate these noisy indicators into a unified probabilistic framework using ideas from ensemble learning and graph-based semi-supervised learning. Our experiments indicate that MEM outperforms state-of-the-art methods by a significant margin.</p><p>5 0.22865163 <a title="101-lsi-5" href="./emnlp-2012-Word_Salad%3A_Relating_Food_Prices_and_Descriptions.html">139 emnlp-2012-Word Salad: Relating Food Prices and Descriptions</a></p>
<p>Author: Victor Chahuneau ; Kevin Gimpel ; Bryan R. Routledge ; Lily Scherlis ; Noah A. Smith</p><p>Abstract: We investigate the use of language in food writing, specifically on restaurant menus and in customer reviews. Our approach is to build predictive models of concrete external variables, such as restaurant menu prices. We make use of a dataset of menus and customer reviews for thousands of restaurants in several U.S. cities. By focusing on prediction tasks and doing our analysis at scale, our methodology allows quantitative, objective measurements of the words and phrases used to de- scribe food in restaurants. We also explore interactions in language use between menu prices and sentiment as expressed in user reviews.</p><p>6 0.22417463 <a title="101-lsi-6" href="./emnlp-2012-Why_Question_Answering_using_Sentiment_Analysis_and_Word_Classes.html">137 emnlp-2012-Why Question Answering using Sentiment Analysis and Word Classes</a></p>
<p>7 0.19902356 <a title="101-lsi-7" href="./emnlp-2012-Collocation_Polarity_Disambiguation_Using_Web-based_Pseudo_Contexts.html">28 emnlp-2012-Collocation Polarity Disambiguation Using Web-based Pseudo Contexts</a></p>
<p>8 0.13395399 <a title="101-lsi-8" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>9 0.13254517 <a title="101-lsi-9" href="./emnlp-2012-Translation_Model_Based_Cross-Lingual_Language_Model_Adaptation%3A_from_Word_Models_to_Phrase_Models.html">128 emnlp-2012-Translation Model Based Cross-Lingual Language Model Adaptation: from Word Models to Phrase Models</a></p>
<p>10 0.12772073 <a title="101-lsi-10" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>11 0.12619114 <a title="101-lsi-11" href="./emnlp-2012-Bilingual_Lexicon_Extraction_from_Comparable_Corpora_Using_Label_Propagation.html">25 emnlp-2012-Bilingual Lexicon Extraction from Comparable Corpora Using Label Propagation</a></p>
<p>12 0.12413663 <a title="101-lsi-12" href="./emnlp-2012-Streaming_Analysis_of_Discourse_Participants.html">120 emnlp-2012-Streaming Analysis of Discourse Participants</a></p>
<p>13 0.12181208 <a title="101-lsi-13" href="./emnlp-2012-Resolving_Complex_Cases_of_Definite_Pronouns%3A_The_Winograd_Schema_Challenge.html">112 emnlp-2012-Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge</a></p>
<p>14 0.11812683 <a title="101-lsi-14" href="./emnlp-2012-Open_Language_Learning_for_Information_Extraction.html">100 emnlp-2012-Open Language Learning for Information Extraction</a></p>
<p>15 0.11457136 <a title="101-lsi-15" href="./emnlp-2012-Building_a_Lightweight_Semantic_Model_for_Unsupervised_Information_Extraction_on_Short_Listings.html">26 emnlp-2012-Building a Lightweight Semantic Model for Unsupervised Information Extraction on Short Listings</a></p>
<p>16 0.11308087 <a title="101-lsi-16" href="./emnlp-2012-Exploiting_Chunk-level_Features_to_Improve_Phrase_Chunking.html">45 emnlp-2012-Exploiting Chunk-level Features to Improve Phrase Chunking</a></p>
<p>17 0.10991342 <a title="101-lsi-17" href="./emnlp-2012-Forced_Derivation_Tree_based_Model_Training_to_Statistical_Machine_Translation.html">54 emnlp-2012-Forced Derivation Tree based Model Training to Statistical Machine Translation</a></p>
<p>18 0.10551125 <a title="101-lsi-18" href="./emnlp-2012-Do_Neighbours_Help%3F_An_Exploration_of_Graph-based_Algorithms_for_Cross-domain_Sentiment_Classification.html">34 emnlp-2012-Do Neighbours Help? An Exploration of Graph-based Algorithms for Cross-domain Sentiment Classification</a></p>
<p>19 0.10491489 <a title="101-lsi-19" href="./emnlp-2012-Locally_Training_the_Log-Linear_Model_for_SMT.html">86 emnlp-2012-Locally Training the Log-Linear Model for SMT</a></p>
<p>20 0.10452515 <a title="101-lsi-20" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.022), (16, 0.049), (25, 0.011), (34, 0.057), (60, 0.085), (63, 0.05), (64, 0.034), (65, 0.04), (70, 0.028), (73, 0.012), (74, 0.035), (76, 0.043), (79, 0.015), (80, 0.017), (86, 0.042), (90, 0.285), (95, 0.069)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76550376 <a title="101-lda-1" href="./emnlp-2012-Opinion_Target_Extraction_Using_Word-Based_Translation_Model.html">101 emnlp-2012-Opinion Target Extraction Using Word-Based Translation Model</a></p>
<p>Author: Kang Liu ; Liheng Xu ; Jun Zhao</p><p>Abstract: This paper proposes a novel approach to extract opinion targets based on wordbased translation model (WTM). At first, we apply WTM in a monolingual scenario to mine the associations between opinion targets and opinion words. Then, a graphbased algorithm is exploited to extract opinion targets, where candidate opinion relevance estimated from the mined associations, is incorporated with candidate importance to generate a global measure. By using WTM, our method can capture opinion relations more precisely, especially for long-span relations. In particular, compared with previous syntax-based methods, our method can effectively avoid noises from parsing errors when dealing with informal texts in large Web corpora. By using graph-based algorithm, opinion targets are extracted in a global process, which can effectively alleviate the problem of error propagation in traditional bootstrap-based methods, such as Double Propagation. The experimental results on three real world datasets in different sizes and languages show that our approach is more effective and robust than state-of-art methods. 1</p><p>2 0.69892621 <a title="101-lda-2" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>Author: Lizhen Qu ; Rainer Gemulla ; Gerhard Weikum</p><p>Abstract: We propose the weakly supervised MultiExperts Model (MEM) for analyzing the semantic orientation of opinions expressed in natural language reviews. In contrast to most prior work, MEM predicts both opinion polarity and opinion strength at the level of individual sentences; such fine-grained analysis helps to understand better why users like or dislike the entity under review. A key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength. For this reason, MEM is weakly supervised: It starts with potentially noisy indicators obtained from coarse-grained training data (i.e., document-level ratings), a small set of diverse base predictors, and, if available, small amounts of fine-grained training data. We integrate these noisy indicators into a unified probabilistic framework using ideas from ensemble learning and graph-based semi-supervised learning. Our experiments indicate that MEM outperforms state-of-the-art methods by a significant margin.</p><p>3 0.45753339 <a title="101-lda-3" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>Author: Heeyoung Lee ; Marta Recasens ; Angel Chang ; Mihai Surdeanu ; Dan Jurafsky</p><p>Abstract: We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable documents, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.</p><p>4 0.45628268 <a title="101-lda-4" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms ofweak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.</p><p>5 0.45463958 <a title="101-lda-5" href="./emnlp-2012-A_Discriminative_Model_for_Query_Spelling_Correction_with_Latent_Structural_SVM.html">5 emnlp-2012-A Discriminative Model for Query Spelling Correction with Latent Structural SVM</a></p>
<p>Author: Huizhong Duan ; Yanen Li ; ChengXiang Zhai ; Dan Roth</p><p>Abstract: Discriminative training in query spelling correction is difficult due to the complex internal structures of the data. Recent work on query spelling correction suggests a two stage approach a noisy channel model that is used to retrieve a number of candidate corrections, followed by discriminatively trained ranker applied to these candidates. The ranker, however, suffers from the fact the low recall of the first, suboptimal, search stage. This paper proposes to directly optimize the search stage with a discriminative model based on latent structural SVM. In this model, we treat query spelling correction as a multiclass classification problem with structured input and output. The latent structural information is used to model the alignment of words in the spelling correction process. Experiment results show that as a standalone speller, our model outperforms all the baseline systems. It also attains a higher recall compared with the noisy channel model, and can therefore serve as a better filtering stage when combined with a ranker.</p><p>6 0.44690406 <a title="101-lda-6" href="./emnlp-2012-Lexical_Differences_in_Autobiographical_Narratives_from_Schizophrenic_Patients_and_Healthy_Controls.html">83 emnlp-2012-Lexical Differences in Autobiographical Narratives from Schizophrenic Patients and Healthy Controls</a></p>
<p>7 0.44648004 <a title="101-lda-7" href="./emnlp-2012-Besting_the_Quiz_Master%3A_Crowdsourcing_Incremental_Classification_Games.html">23 emnlp-2012-Besting the Quiz Master: Crowdsourcing Incremental Classification Games</a></p>
<p>8 0.44626597 <a title="101-lda-8" href="./emnlp-2012-Learning_Lexicon_Models_from_Search_Logs_for_Query_Expansion.html">78 emnlp-2012-Learning Lexicon Models from Search Logs for Query Expansion</a></p>
<p>9 0.44540632 <a title="101-lda-9" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>10 0.44408327 <a title="101-lda-10" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>11 0.44340587 <a title="101-lda-11" href="./emnlp-2012-Fast_Large-Scale_Approximate_Graph_Construction_for_NLP.html">52 emnlp-2012-Fast Large-Scale Approximate Graph Construction for NLP</a></p>
<p>12 0.44299978 <a title="101-lda-12" href="./emnlp-2012-Explore_Person_Specific_Evidence_in_Web_Person_Name_Disambiguation.html">47 emnlp-2012-Explore Person Specific Evidence in Web Person Name Disambiguation</a></p>
<p>13 0.44231611 <a title="101-lda-13" href="./emnlp-2012-Exploring_Topic_Coherence_over_Many_Models_and_Many_Topics.html">49 emnlp-2012-Exploring Topic Coherence over Many Models and Many Topics</a></p>
<p>14 0.44148406 <a title="101-lda-14" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>15 0.44123369 <a title="101-lda-15" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>16 0.44111183 <a title="101-lda-16" href="./emnlp-2012-Multi-Domain_Learning%3A_When_Do_Domains_Matter%3F.html">92 emnlp-2012-Multi-Domain Learning: When Do Domains Matter?</a></p>
<p>17 0.44109687 <a title="101-lda-17" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>18 0.4389191 <a title="101-lda-18" href="./emnlp-2012-Building_a_Lightweight_Semantic_Model_for_Unsupervised_Information_Extraction_on_Short_Listings.html">26 emnlp-2012-Building a Lightweight Semantic Model for Unsupervised Information Extraction on Short Listings</a></p>
<p>19 0.4388338 <a title="101-lda-19" href="./emnlp-2012-Why_Question_Answering_using_Sentiment_Analysis_and_Word_Classes.html">137 emnlp-2012-Why Question Answering using Sentiment Analysis and Word Classes</a></p>
<p>20 0.4384383 <a title="101-lda-20" href="./emnlp-2012-Revisiting_the_Predictability_of_Language%3A_Response_Completion_in_Social_Media.html">114 emnlp-2012-Revisiting the Predictability of Language: Response Completion in Social Media</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
