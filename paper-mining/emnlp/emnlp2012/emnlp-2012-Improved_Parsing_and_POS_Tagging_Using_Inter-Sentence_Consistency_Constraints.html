<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-64" href="#">emnlp2012-64</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</h1>
<br/><p>Source: <a title="emnlp-2012-64-pdf" href="http://aclweb.org/anthology//D/D12/D12-1131.pdf">pdf</a></p><p>Author: Alexander Rush ; Roi Reichart ; Michael Collins ; Amir Globerson</p><p>Abstract: State-of-the-art statistical parsers and POS taggers perform very well when trained with large amounts of in-domain data. When training data is out-of-domain or limited, accuracy degrades. In this paper, we aim to compensate for the lack of available training data by exploiting similarities between test set sentences. We show how to augment sentencelevel models for parsing and POS tagging with inter-sentence consistency constraints. To deal with the resulting global objective, we present an efficient and exact dual decomposition decoding algorithm. In experiments, we add consistency constraints to the MST parser and the Stanford part-of-speech tagger and demonstrate significant error reduction in the domain adaptation and the lightly supervised settings across five languages.</p><p>Reference: <a title="emnlp-2012-64-reference" href="../emnlp2012_reference/emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We show how to augment sentencelevel models for parsing and POS tagging with inter-sentence consistency constraints. [sent-9, score-0.499]
</p><p>2 To deal with the resulting global objective, we present an efficient and exact dual decomposition decoding algorithm. [sent-10, score-0.709]
</p><p>3 In experiments, we add consistency constraints to the MST parser and the Stanford part-of-speech tagger and demonstrate significant error reduction in the domain adaptation and the lightly supervised  settings across five languages. [sent-11, score-0.569]
</p><p>4 Most parsing and tagging models are defined at the sentence-level, which makes such inter-sentence information sharing difficult. [sent-15, score-0.228]
</p><p>5 We show how to augment sentence-level models with inter-sentence constraints to encourage consistent descisions in similar ∗ Both authors contributed equally to this work. [sent-16, score-0.264]
</p><p>6 l contexts, and we give an efficient algorithm with formal guarantees for decoding such models. [sent-20, score-0.251]
</p><p>7 With a global objective, we can include constraints that encourage a consistent tag across all occurrences of an unknown word type to improve accuracy. [sent-22, score-0.465]
</p><p>8 Using a global objective, we can add constraints that encourage similar surface-level contexts to exhibit similar syntactic behaviour. [sent-24, score-0.467]
</p><p>9 The first contribution of this work is the use of Markov random fields (MRFs) to model global constraints between sentences in dependency parsing and POS tagging. [sent-25, score-0.525]
</p><p>10 We represent each word as a node, the tagging or parse decision as its label, and add constraints through edges. [sent-26, score-0.297]
</p><p>11 MRFs allow us to include global constraints tailored to these problems, and to reason about inference in the corresponding global models. [sent-27, score-0.52]
</p><p>12 The second contribution is an efficient dual decomposition algorithm for decoding a global objective with inter-sentence constraints. [sent-28, score-0.789]
</p><p>13 These constraints generally make direct inference challenging since they tie together the entire test corpus. [sent-29, score-0.198]
</p><p>14 To alleviate this issue, our algorithm splits the global inference problem into subproblems - decoding of indi-  vidual sentences, and decoding of the global MRF. [sent-30, score-0.687]
</p><p>15 These subproblems can be solved efficiently through known methods. [sent-31, score-0.168]
</p><p>16 Lc a2n0g1u2ag Aes Psorcoicaetsiosin fgo arn Cdo Cmopmutpauti oantiaoln Lailn Ngautiustriacls exact solution to the global model. [sent-34, score-0.25]
</p><p>17 We experiment with domain adaptation and lightly supervised training. [sent-35, score-0.154]
</p><p>18 We demonstrate that global models with consistency constraints can improve upon sentence-level models for dependency parsing and part-of-speech tagging. [sent-36, score-0.707]
</p><p>19 For lightly supervised learning, we show an error reduction of up to 12. [sent-40, score-0.141]
</p><p>20 7 times the costs of sentence-level inference and finds the exact solution on the vast majority of sentences. [sent-45, score-0.129]
</p><p>21 , 2002), and collective inference with symmetric clique potentials (Gupta et al. [sent-48, score-0.191]
</p><p>22 These models use a linear-chain CRF or MRF objective modified by potentials defined over pairs of nodes or clique templates. [sent-50, score-0.286]
</p><p>23 In particular, we focus on dependency parsing which is characterized by a more complicated structure compared to the IE tasks addressed by previous work. [sent-56, score-0.206]
</p><p>24 The constraints used by these works differ from ours in that they encourage the posterior label distribution to have desired properties such as sparsity (e. [sent-62, score-0.319]
</p><p>25 In addition, these methods use global information during training as opposed to our approach which applies test-time inference global constraints. [sent-65, score-0.396]
</p><p>26 The application of dual decomposition for inference in MRFs has been explored by Wainwright et al. [sent-66, score-0.367]
</p><p>27 (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. [sent-71, score-0.462]
</p><p>28 Work on dual decomposition for NLP is related to the work  of Smith and Eisner (2008) who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. [sent-72, score-0.596]
</p><p>29 Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al. [sent-77, score-0.135]
</p><p>30 Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. [sent-85, score-0.121]
</p><p>31 Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. [sent-86, score-0.182]
</p><p>32 We complement these works, which focus on consistency between consecutive sentences, and explore corpus level consistency. [sent-93, score-0.182]
</p><p>33 –  –  3  Structured Models  We begin by introducing notation for sentencelevel dependency parsing as a structured prediction  problem. [sent-94, score-0.331]
</p><p>34 The goal of dependency parsing is to find the best parse y for a tagged sentence x = (w1/t1 , . [sent-95, score-0.252]
</p><p>35 Define the index set for dependency parsing as I(x) = {(m, h) : m ∈ {1. [sent-99, score-0.279]
</p><p>36 f W Walel dveafliind dependency parses 0f,o1r a sentence x. [sent-108, score-0.131]
</p><p>37 In this work, we use projective dependency parses, but the method also applies to the set of nonprojective parse trees. [sent-109, score-0.203]
</p><p>38 This sentencelevel decoding problem can often be solved efficiently. [sent-112, score-0.23]
</p><p>39 For example in commonly used projective dependency parsing models (McDonald et al. [sent-113, score-0.258]
</p><p>40 where u is a vector in In practice, u will be a vector of Lagrange multipliers associated with the dependencies of y in our dual decomposition algorithm given in Section 6. [sent-116, score-0.385]
</p><p>41 We can construct a very similar setting for POS tagging where the goal is to find the best tagging y for a sentence x = (w1, . [sent-117, score-0.218]
</p><p>42 |V |}, l ∈ Li} ∪ {((i, j), li, lj) : (i, j) ∈ E, li ∈ Li, lj ∈ Lj} A label assignment in the MRF is a binary vector z with z(i, l) = 1if the label lis selected at node i  and z((i, j), li, lj) = 1if the labels li, lj are selected for the nodes i,j. [sent-133, score-0.763]
</p><p>43 In applications such as parsing and POS tagging, some of the label assignments are not allowed. [sent-134, score-0.26]
</p><p>44 For example, in dependency parsing the resulting structure must be a tree. [sent-135, score-0.206]
</p><p>45 Consequently, if every node in the MRF corresponds to a word in a document and its label corresponds to the index of its head word, the resulting dependency structure for each sentence must be acyclic. [sent-136, score-0.416]
</p><p>46 one label per node) is given by W⊂e { score label assignments in the MRF with a scoring function g : Z → R. [sent-138, score-0.281]
</p><p>47 The constraint applies to all modifiers in the context DT JJ. [sent-144, score-0.139]
</p><p>48 The white node corresponds to the consensus POS tag of the head word of these modifiers. [sent-145, score-0.429]
</p><p>49 4  A Parsing Example  In this section we give a detailed example of global constraints for dependency parsing. [sent-146, score-0.39]
</p><p>50 The aim is to construct a global objective that encourages similar contexts across the corpus to exhibit similar syntactic behaviour. [sent-147, score-0.42]
</p><p>51 We implement this objective using an MRF with a node for each word in the test set. [sent-148, score-0.237]
</p><p>52 The label of each node is the index of the word it modifies. [sent-149, score-0.274]
</p><p>53 We add edges to this MRF to reward consistency among similar contexts. [sent-150, score-0.227]
</p><p>54 Furthermore, we add nodes with a fixed label to incorporate contexts seen in the training data. [sent-151, score-0.206]
</p><p>55 Specifically, we say that the context of a word is its POS tag and the POS tags of some set of the words around it. [sent-152, score-0.145]
</p><p>56 Our constraints are designed to bias words in the same context to modify words with similar POS tags. [sent-154, score-0.189]
</p><p>57 Figure 1shows a global MRF over a small parsing example with one training sentence and two test sentences. [sent-155, score-0.296]
</p><p>58 The MRF contains a node associated with each word instance, where the label of the node is the index of the word it modifies. [sent-156, score-0.39]
</p><p>59 We hope to choose head words with similar 1437 POS tags for these two test contexts biased by the observed training context. [sent-158, score-0.187]
</p><p>60 We then define our MRF to include one consensus node for each set Sc as well as a word node for each instance in the set Sc ∪ Oc. [sent-169, score-0.404]
</p><p>61 an edge (fSrom each∪ n Oode  (ScC=1  i∈ Sc∪Oc to its consensus node c, ES = {(i, c) : c ∈ {1, . [sent-175, score-0.288]
</p><p>62 The consensus nodes have the label set Lc = T ∪ {NULL} where T is the set of POS tags and= =th Te N ∪U {LNLU symbwohle represents et sheet tc oofn PsOtraSin tat being dttu hrnee Nd UoLff. [sent-190, score-0.402]
</p><p>63 The scoring function aims to reward consistency among the head POS tag at each word and the consensus node  wθh(eri,cpo)slim,acp)s=aw ordδ 1203indleocfxtph=toersNiwt(Ulsi P)LeOi=sScltaogse. [sent-192, score-0.727]
</p><p>64 The MRF graph is identical to the parsing case with Tc replacing Sc and we no longer have Oc. [sent-196, score-0.135]
</p><p>65 The label sets for the word nodes are now L(s,m) = T where the label is the POS tag chosen at that word, and the label set for the consensus node is Lc = T ∪ {NULL}. [sent-197, score-0.662]
</p><p>66 We use the same scoring function= as i n∪ parsing }to. [sent-198, score-0.206]
</p><p>67 Wenefo urcsee consistency between word nodes and the consensus node. [sent-199, score-0.403]
</p><p>68 5 Global Objective Recall the definition of sentence-level parsing, where the optimal parse y∗ for a single sentence x under a scoring function f is given by: y∗ = arg maxy∈Y(x) f(y) . [sent-200, score-0.182]
</p><p>69 We apply this objective to a set ofy sentences, specified by the tuple X = (x1, . [sent-201, score-0.121]
</p><p>70 to ×fin Yd( xthe optimal dependency parses Y∗ = (Y1∗ , . [sent-210, score-0.131]
</p><p>71 , Yr∗) ∈ Y(X) under a global objective Xr  Y∗= aYrg ∈Ym(Xax)F(Y ) = aYrg ∈Ym(Xax)sX=1f(Ys) where F : Y(X)  →  R is the global scoring func-  wtiohne. [sent-213, score-0.514]
</p><p>72 r We now consider scoring functions where the global objective includes inter-sentence constraints. [sent-214, score-0.353]
</p><p>73 Objectives of this form will not factor directly into individual parsing problems; however, we can choose to write them as the sum of two convenient terms: (1) A simple sum of sentence-level objectives; and (2) A global MRF that connects the local structures. [sent-215, score-0.326]
</p><p>74 We say the parses Ys are consistent with a label assignment z if for all (s, m, h) ∈ J(X) we have that z((s, m) , h) = Ys(m, h). [sent-221, score-0.192]
</p><p>75 With this notation we can write the full global decoding objective as (Y∗, z∗) =  arg max F(Y ) Y ∈Y(X), z∈Z  + g(z)  (1)  s. [sent-223, score-0.535]
</p><p>76 ∀(s, m, h)  ∈  J(X) , z((s, m) , h) = Ys (m, h) 1438  parsing models. [sent-225, score-0.135]
</p><p>77 The solution to this objective maximizes the local  models as well as the global MRF, while maintaining consistency among the models. [sent-226, score-0.542]
</p><p>78 Specifically, the MRF we use in the experiments has a simple naive Bayes structure with the consensus node connected to all relevant word nodes. [sent-227, score-0.288]
</p><p>79 The global objective for POS tagging has a similar form. [sent-228, score-0.375]
</p><p>80 The index set contains an element for each possible tag at each word instance in the corpus. [sent-231, score-0.143]
</p><p>81 6  A Global Decoding Algorithm  We now consider the decoding question: how to find the structure Y∗ that maximizes the global objective. [sent-232, score-0.314]
</p><p>82 We aim for an efficient solution that makes use of the individual solvers at the sentence-level. [sent-233, score-0.139]
</p><p>83 Before we describe our dual decomposition algorithm, we consider the difficulty of solving the  global objective directly. [sent-237, score-0.678]
</p><p>84 We have an efficient dynamic programming algorithm for solving dependency parsing at the sentence-level, and efficient algorithms for solving the MRF. [sent-238, score-0.489]
</p><p>85 Solving the intersected dynamic program requires decoding simultaneously over the entire corpus, with an additional multiplicative factor for solving the MRF. [sent-241, score-0.275]
</p><p>86 In contrast, we can construct a dual decomposition algorithm which is efficient, produces a certificate when it finds an exact solution, and directly uses the sentence-level parsing models. [sent-243, score-0.535]
</p><p>87 Considering again the global objective of equation 1, we note that the difficulty in decoding this objective comes entirely from the constraints z((s, m) , h) = Ys (m, h). [sent-244, score-0.684]
</p><p>88 ev Tehle parsing problems oanrsd n tahte-  second can be solved efficiently given our assumptions on the MRF topology G. [sent-247, score-0.224]
</p><p>89 Recent work has shown that a relaxation based on dual decomposition often produces an exact solution for such problems (Koo et al. [sent-248, score-0.416]
</p><p>90 To apply dual decomposition, we introduce Lagrange multipliers u(s, m, h) for the agreement constraints between the sentence-level models and the global MRF. [sent-250, score-0.568]
</p><p>91 The Lagrangian dual is the function L(u) = maxz g(z, u) + maxy F(y, u) where  X  g(z,u) = g(z) + u(s,m,h)z((s,m),h)), (s,mX,h)∈J(X) F(y,u) = F(Y ) −X u(s,m,h)Ys(m,h) (s,mX,h)∈J(X) In order to find minu L(u), we use subgradient descent. [sent-251, score-0.336]
</p><p>92 At each iteration k, we find the best set of parses Y(k) over the entire corpus and the best MRF assignment z(k) . [sent-255, score-0.138]
</p><p>93 On the next iteration, we solve the same decoding prob1439  HAealdl iCno Cntoexnt sext≥676 0. [sent-257, score-0.123]
</p><p>94 The table shows the percentage ofcontext types for which the probability of the most frequent head tag is at least p. [sent-262, score-0.141]
</p><p>95 Head in Context refers to the subset of contexts where the most frequent head is within the context itself. [sent-263, score-0.174]
</p><p>96 If at any point the current solutions and satisfy the consistency constraint, we return their current values. [sent-266, score-0.182]
</p><p>97 7  Consistency Constraints  In this section we describe the consistency constraints used for the global models of parsing and tagging. [sent-276, score-0.636]
</p><p>98 Recall from Section 4 that we choose parsing constraints based on the word context. [sent-278, score-0.293]
</p><p>99 We encourage words in similar contexts to choose head words with similar POS tags. [sent-279, score-0.219]
</p><p>100 We use a simple procedure to select which constraints to add. [sent-280, score-0.158]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mrf', 0.522), ('dual', 0.191), ('consistency', 0.182), ('pos', 0.173), ('consensus', 0.172), ('lj', 0.169), ('global', 0.161), ('constraints', 0.158), ('ys', 0.15), ('mrfs', 0.145), ('decomposition', 0.136), ('parsing', 0.135), ('decoding', 0.123), ('objective', 0.121), ('node', 0.116), ('sc', 0.112), ('oc', 0.107), ('lightly', 0.105), ('imrf', 0.101), ('tagging', 0.093), ('maxy', 0.087), ('label', 0.085), ('subproblems', 0.079), ('encourage', 0.076), ('index', 0.073), ('xr', 0.073), ('contexts', 0.072), ('dependency', 0.071), ('scoring', 0.071), ('head', 0.071), ('tag', 0.07), ('solving', 0.069), ('ayrg', 0.067), ('xax', 0.067), ('arg', 0.065), ('rush', 0.064), ('mcclosky', 0.061), ('parses', 0.06), ('sentencelevel', 0.059), ('compensate', 0.058), ('clique', 0.058), ('lagrange', 0.058), ('maxz', 0.058), ('multipliers', 0.058), ('potentials', 0.058), ('efficient', 0.057), ('taggers', 0.054), ('crfs', 0.054), ('intersected', 0.052), ('tc', 0.052), ('projective', 0.052), ('adaptation', 0.049), ('nodes', 0.049), ('objectives', 0.049), ('lagrangian', 0.049), ('solved', 0.048), ('solution', 0.048), ('assignment', 0.047), ('parse', 0.046), ('jj', 0.046), ('reward', 0.045), ('roi', 0.045), ('tags', 0.044), ('proof', 0.043), ('li', 0.043), ('constraint', 0.042), ('exact', 0.041), ('efficiently', 0.041), ('ins', 0.041), ('inference', 0.04), ('assignments', 0.04), ('mst', 0.039), ('guarantees', 0.039), ('tagger', 0.039), ('ym', 0.037), ('theorem', 0.037), ('reduction', 0.036), ('notation', 0.035), ('tr', 0.035), ('koo', 0.035), ('collective', 0.035), ('applies', 0.034), ('mccallum', 0.034), ('aim', 0.034), ('argmax', 0.034), ('bayes', 0.034), ('null', 0.034), ('mann', 0.032), ('modifiers', 0.032), ('construct', 0.032), ('formal', 0.032), ('dynamic', 0.031), ('structured', 0.031), ('iteration', 0.031), ('context', 0.031), ('nlp', 0.03), ('augment', 0.03), ('ns', 0.03), ('maximizes', 0.03), ('write', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="64-tfidf-1" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<p>Author: Alexander Rush ; Roi Reichart ; Michael Collins ; Amir Globerson</p><p>Abstract: State-of-the-art statistical parsers and POS taggers perform very well when trained with large amounts of in-domain data. When training data is out-of-domain or limited, accuracy degrades. In this paper, we aim to compensate for the lack of available training data by exploiting similarities between test set sentences. We show how to augment sentencelevel models for parsing and POS tagging with inter-sentence consistency constraints. To deal with the resulting global objective, we present an efficient and exact dual decomposition decoding algorithm. In experiments, we add consistency constraints to the MST parser and the Stanford part-of-speech tagger and demonstrate significant error reduction in the domain adaptation and the lightly supervised settings across five languages.</p><p>2 0.1537033 <a title="64-tfidf-2" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>Author: Xian Qian ; Yang Liu</p><p>Abstract: In this paper, we propose a novel decoding algorithm for discriminative joint Chinese word segmentation, part-of-speech (POS) tagging, and parsing. Previous work often used a pipeline method Chinese word segmentation followed by POS tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components. In our approach, we train the three individual models separately during training, and incorporate them together in a unified framework during decoding. We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features. As far as we know, this is the first work on joint Chinese word segmentation, POS tagging and parsing. Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system. –</p><p>3 0.14372796 <a title="64-tfidf-3" href="./emnlp-2012-Generalized_Higher-Order_Dependency_Parsing_with_Cube_Pruning.html">57 emnlp-2012-Generalized Higher-Order Dependency Parsing with Cube Pruning</a></p>
<p>Author: Hao Zhang ; Ryan McDonald</p><p>Abstract: State-of-the-art graph-based parsers use features over higher-order dependencies that rely on decoding algorithms that are slow and difficult to generalize. On the other hand, transition-based dependency parsers can easily utilize such features without increasing the linear complexity of the shift-reduce system beyond a constant. In this paper, we attempt to address this imbalance for graph-based parsing by generalizing the Eisner (1996) algorithm to handle arbitrary features over higherorder dependencies. The generalization is at the cost of asymptotic efficiency. To account for this, cube pruning for decoding is utilized (Chiang, 2007). For the first time, label tuple and structural features such as valencies can be scored efficiently with third-order features in a graph-based parser. Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation ofthe third-ordermodel of Koo et al. (2010).</p><p>4 0.13748047 <a title="64-tfidf-4" href="./emnlp-2012-Parse%2C_Price_and_Cut-Delayed_Column_and_Row_Generation_for_Graph_Based_Parsers.html">104 emnlp-2012-Parse, Price and Cut-Delayed Column and Row Generation for Graph Based Parsers</a></p>
<p>Author: Sebastian Riedel ; David Smith ; Andrew McCallum</p><p>Abstract: Graph-based dependency parsers suffer from the sheer number of higher order edges they need to (a) score and (b) consider during optimization. Here we show that when working with LP relaxations, large fractions of these edges can be pruned before they are fully scored—without any loss of optimality guarantees and, hence, accuracy. This is achieved by iteratively parsing with a subset of higherorder edges, adding higher-order edges that may improve the score of the current solution, and adding higher-order edges that are implied by the current best first order edges. This amounts to delayed column and row generation in the LP relaxation and is guaranteed to provide the optimal LP solution. For second order grandparent models, our method considers, or scores, no more than 6–13% of the second order edges of the full model. This yields up to an eightfold parsing speedup, while pro- viding the same empirical accuracy and certificates of optimality as working with the full LP relaxation. We also provide a tighter LP formulation for grandparent models that leads to a smaller integrality gap and higher speed.</p><p>5 0.13338521 <a title="64-tfidf-5" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>Author: Bernd Bohnet ; Joakim Nivre</p><p>Abstract: Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins. We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees. Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages.</p><p>6 0.12028338 <a title="64-tfidf-6" href="./emnlp-2012-Part-of-Speech_Tagging_for_Chinese-English_Mixed_Texts_with_Dynamic_Features.html">106 emnlp-2012-Part-of-Speech Tagging for Chinese-English Mixed Texts with Dynamic Features</a></p>
<p>7 0.11667065 <a title="64-tfidf-7" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<p>8 0.10332903 <a title="64-tfidf-8" href="./emnlp-2012-Learning_Lexicon_Models_from_Search_Logs_for_Query_Expansion.html">78 emnlp-2012-Learning Lexicon Models from Search Logs for Query Expansion</a></p>
<p>9 0.10319725 <a title="64-tfidf-9" href="./emnlp-2012-On_Amortizing_Inference_Cost_for_Structured_Prediction.html">99 emnlp-2012-On Amortizing Inference Cost for Structured Prediction</a></p>
<p>10 0.09860871 <a title="64-tfidf-10" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>11 0.091666222 <a title="64-tfidf-11" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>12 0.08260601 <a title="64-tfidf-12" href="./emnlp-2012-Type-Supervised_Hidden_Markov_Models_for_Part-of-Speech_Tagging_with_Incomplete_Tag_Dictionaries.html">129 emnlp-2012-Type-Supervised Hidden Markov Models for Part-of-Speech Tagging with Incomplete Tag Dictionaries</a></p>
<p>13 0.077827781 <a title="64-tfidf-13" href="./emnlp-2012-Dynamic_Programming_for_Higher_Order_Parsing_of_Gap-Minding_Trees.html">37 emnlp-2012-Dynamic Programming for Higher Order Parsing of Gap-Minding Trees</a></p>
<p>14 0.077442311 <a title="64-tfidf-14" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>15 0.076720215 <a title="64-tfidf-15" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>16 0.066375233 <a title="64-tfidf-16" href="./emnlp-2012-Assessment_of_ESL_Learners%27_Syntactic_Competence_Based_on_Similarity_Measures.html">21 emnlp-2012-Assessment of ESL Learners' Syntactic Competence Based on Similarity Measures</a></p>
<p>17 0.066348396 <a title="64-tfidf-17" href="./emnlp-2012-Spectral_Dependency_Parsing_with_Latent_Variables.html">119 emnlp-2012-Spectral Dependency Parsing with Latent Variables</a></p>
<p>18 0.064037919 <a title="64-tfidf-18" href="./emnlp-2012-Wiki-ly_Supervised_Part-of-Speech_Tagging.html">138 emnlp-2012-Wiki-ly Supervised Part-of-Speech Tagging</a></p>
<p>19 0.06260033 <a title="64-tfidf-19" href="./emnlp-2012-Improving_NLP_through_Marginalization_of_Hidden_Syntactic_Structure.html">65 emnlp-2012-Improving NLP through Marginalization of Hidden Syntactic Structure</a></p>
<p>20 0.061861996 <a title="64-tfidf-20" href="./emnlp-2012-Exploiting_Reducibility_in_Unsupervised_Dependency_Parsing.html">46 emnlp-2012-Exploiting Reducibility in Unsupervised Dependency Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.248), (1, -0.128), (2, 0.198), (3, -0.078), (4, 0.057), (5, 0.003), (6, -0.01), (7, -0.011), (8, 0.014), (9, -0.004), (10, 0.06), (11, 0.001), (12, 0.073), (13, -0.111), (14, 0.007), (15, -0.013), (16, -0.073), (17, 0.109), (18, 0.196), (19, -0.033), (20, 0.052), (21, 0.098), (22, -0.031), (23, -0.239), (24, 0.022), (25, 0.035), (26, -0.035), (27, 0.017), (28, -0.117), (29, -0.031), (30, -0.131), (31, -0.101), (32, 0.175), (33, 0.014), (34, -0.006), (35, 0.155), (36, -0.044), (37, 0.081), (38, 0.116), (39, 0.002), (40, -0.01), (41, 0.031), (42, 0.029), (43, 0.025), (44, -0.064), (45, 0.039), (46, -0.054), (47, -0.09), (48, 0.031), (49, 0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95566392 <a title="64-lsi-1" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<p>Author: Alexander Rush ; Roi Reichart ; Michael Collins ; Amir Globerson</p><p>Abstract: State-of-the-art statistical parsers and POS taggers perform very well when trained with large amounts of in-domain data. When training data is out-of-domain or limited, accuracy degrades. In this paper, we aim to compensate for the lack of available training data by exploiting similarities between test set sentences. We show how to augment sentencelevel models for parsing and POS tagging with inter-sentence consistency constraints. To deal with the resulting global objective, we present an efficient and exact dual decomposition decoding algorithm. In experiments, we add consistency constraints to the MST parser and the Stanford part-of-speech tagger and demonstrate significant error reduction in the domain adaptation and the lightly supervised settings across five languages.</p><p>2 0.70726228 <a title="64-lsi-2" href="./emnlp-2012-Parse%2C_Price_and_Cut-Delayed_Column_and_Row_Generation_for_Graph_Based_Parsers.html">104 emnlp-2012-Parse, Price and Cut-Delayed Column and Row Generation for Graph Based Parsers</a></p>
<p>Author: Sebastian Riedel ; David Smith ; Andrew McCallum</p><p>Abstract: Graph-based dependency parsers suffer from the sheer number of higher order edges they need to (a) score and (b) consider during optimization. Here we show that when working with LP relaxations, large fractions of these edges can be pruned before they are fully scored—without any loss of optimality guarantees and, hence, accuracy. This is achieved by iteratively parsing with a subset of higherorder edges, adding higher-order edges that may improve the score of the current solution, and adding higher-order edges that are implied by the current best first order edges. This amounts to delayed column and row generation in the LP relaxation and is guaranteed to provide the optimal LP solution. For second order grandparent models, our method considers, or scores, no more than 6–13% of the second order edges of the full model. This yields up to an eightfold parsing speedup, while pro- viding the same empirical accuracy and certificates of optimality as working with the full LP relaxation. We also provide a tighter LP formulation for grandparent models that leads to a smaller integrality gap and higher speed.</p><p>3 0.64036888 <a title="64-lsi-3" href="./emnlp-2012-On_Amortizing_Inference_Cost_for_Structured_Prediction.html">99 emnlp-2012-On Amortizing Inference Cost for Structured Prediction</a></p>
<p>Author: Vivek Srikumar ; Gourab Kundu ; Dan Roth</p><p>Abstract: This paper deals with the problem of predicting structures in the context of NLP. Typically, in structured prediction, an inference procedure is applied to each example independently of the others. In this paper, we seek to optimize the time complexity of inference over entire datasets, rather than individual examples. By considering the general inference representation provided by integer linear programs, we propose three exact inference theorems which allow us to re-use earlier solutions for certain instances, thereby completely avoiding possibly expensive calls to the inference procedure. We also identify several approximation schemes which can provide further speedup. We instantiate these ideas to the structured prediction task of semantic role labeling and show that we can achieve a speedup of over 2.5 using our approach while retaining the guarantees of exactness and a further speedup of over 3 using approximations that do not degrade performance.</p><p>4 0.55542201 <a title="64-lsi-4" href="./emnlp-2012-Generalized_Higher-Order_Dependency_Parsing_with_Cube_Pruning.html">57 emnlp-2012-Generalized Higher-Order Dependency Parsing with Cube Pruning</a></p>
<p>Author: Hao Zhang ; Ryan McDonald</p><p>Abstract: State-of-the-art graph-based parsers use features over higher-order dependencies that rely on decoding algorithms that are slow and difficult to generalize. On the other hand, transition-based dependency parsers can easily utilize such features without increasing the linear complexity of the shift-reduce system beyond a constant. In this paper, we attempt to address this imbalance for graph-based parsing by generalizing the Eisner (1996) algorithm to handle arbitrary features over higherorder dependencies. The generalization is at the cost of asymptotic efficiency. To account for this, cube pruning for decoding is utilized (Chiang, 2007). For the first time, label tuple and structural features such as valencies can be scored efficiently with third-order features in a graph-based parser. Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation ofthe third-ordermodel of Koo et al. (2010).</p><p>5 0.51569736 <a title="64-lsi-5" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>Author: Xian Qian ; Yang Liu</p><p>Abstract: In this paper, we propose a novel decoding algorithm for discriminative joint Chinese word segmentation, part-of-speech (POS) tagging, and parsing. Previous work often used a pipeline method Chinese word segmentation followed by POS tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components. In our approach, we train the three individual models separately during training, and incorporate them together in a unified framework during decoding. We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features. As far as we know, this is the first work on joint Chinese word segmentation, POS tagging and parsing. Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system. –</p><p>6 0.47376749 <a title="64-lsi-6" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<p>7 0.4615252 <a title="64-lsi-7" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>8 0.42847332 <a title="64-lsi-8" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>9 0.42524114 <a title="64-lsi-9" href="./emnlp-2012-Part-of-Speech_Tagging_for_Chinese-English_Mixed_Texts_with_Dynamic_Features.html">106 emnlp-2012-Part-of-Speech Tagging for Chinese-English Mixed Texts with Dynamic Features</a></p>
<p>10 0.40050548 <a title="64-lsi-10" href="./emnlp-2012-Exploiting_Chunk-level_Features_to_Improve_Phrase_Chunking.html">45 emnlp-2012-Exploiting Chunk-level Features to Improve Phrase Chunking</a></p>
<p>11 0.39408925 <a title="64-lsi-11" href="./emnlp-2012-Forest_Reranking_through_Subtree_Ranking.html">55 emnlp-2012-Forest Reranking through Subtree Ranking</a></p>
<p>12 0.39102918 <a title="64-lsi-12" href="./emnlp-2012-Learning_Lexicon_Models_from_Search_Logs_for_Query_Expansion.html">78 emnlp-2012-Learning Lexicon Models from Search Logs for Query Expansion</a></p>
<p>13 0.36376646 <a title="64-lsi-13" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>14 0.3400968 <a title="64-lsi-14" href="./emnlp-2012-A_Novel_Discriminative_Framework_for_Sentence-Level_Discourse_Analysis.html">7 emnlp-2012-A Novel Discriminative Framework for Sentence-Level Discourse Analysis</a></p>
<p>15 0.33970234 <a title="64-lsi-15" href="./emnlp-2012-Assessment_of_ESL_Learners%27_Syntactic_Competence_Based_on_Similarity_Measures.html">21 emnlp-2012-Assessment of ESL Learners' Syntactic Competence Based on Similarity Measures</a></p>
<p>16 0.3281118 <a title="64-lsi-16" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>17 0.32670674 <a title="64-lsi-17" href="./emnlp-2012-Spectral_Dependency_Parsing_with_Latent_Variables.html">119 emnlp-2012-Spectral Dependency Parsing with Latent Variables</a></p>
<p>18 0.30117467 <a title="64-lsi-18" href="./emnlp-2012-Dynamic_Programming_for_Higher_Order_Parsing_of_Gap-Minding_Trees.html">37 emnlp-2012-Dynamic Programming for Higher Order Parsing of Gap-Minding Trees</a></p>
<p>19 0.29635584 <a title="64-lsi-19" href="./emnlp-2012-Type-Supervised_Hidden_Markov_Models_for_Part-of-Speech_Tagging_with_Incomplete_Tag_Dictionaries.html">129 emnlp-2012-Type-Supervised Hidden Markov Models for Part-of-Speech Tagging with Incomplete Tag Dictionaries</a></p>
<p>20 0.29392788 <a title="64-lsi-20" href="./emnlp-2012-Exploiting_Reducibility_in_Unsupervised_Dependency_Parsing.html">46 emnlp-2012-Exploiting Reducibility in Unsupervised Dependency Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.016), (11, 0.01), (16, 0.069), (25, 0.04), (34, 0.094), (51, 0.279), (60, 0.064), (63, 0.05), (64, 0.033), (65, 0.03), (70, 0.036), (73, 0.014), (74, 0.069), (76, 0.033), (80, 0.041), (86, 0.03), (95, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74401826 <a title="64-lda-1" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<p>Author: Alexander Rush ; Roi Reichart ; Michael Collins ; Amir Globerson</p><p>Abstract: State-of-the-art statistical parsers and POS taggers perform very well when trained with large amounts of in-domain data. When training data is out-of-domain or limited, accuracy degrades. In this paper, we aim to compensate for the lack of available training data by exploiting similarities between test set sentences. We show how to augment sentencelevel models for parsing and POS tagging with inter-sentence consistency constraints. To deal with the resulting global objective, we present an efficient and exact dual decomposition decoding algorithm. In experiments, we add consistency constraints to the MST parser and the Stanford part-of-speech tagger and demonstrate significant error reduction in the domain adaptation and the lightly supervised settings across five languages.</p><p>2 0.48594919 <a title="64-lda-2" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms ofweak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.</p><p>3 0.48194689 <a title="64-lda-3" href="./emnlp-2012-Left-to-Right_Tree-to-String_Decoding_with_Prediction.html">82 emnlp-2012-Left-to-Right Tree-to-String Decoding with Prediction</a></p>
<p>Author: Yang Feng ; Yang Liu ; Qun Liu ; Trevor Cohn</p><p>Abstract: Decoding algorithms for syntax based machine translation suffer from high computational complexity, a consequence of intersecting a language model with a context free grammar. Left-to-right decoding, which generates the target string in order, can improve decoding efficiency by simplifying the language model evaluation. This paper presents a novel left to right decoding algorithm for tree-to-string translation, using a bottom-up parsing strategy and dynamic future cost estimation for each partial translation. Our method outperforms previously published tree-to-string decoders, including a competing left-to-right method.</p><p>4 0.48003587 <a title="64-lda-4" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: We present a new family of models for unsupervised parsing, Dependency and Boundary models, that use cues at constituent boundaries to inform head-outward dependency tree generation. We build on three intuitions that are explicit in phrase-structure grammars but only implicit in standard dependency formulations: (i) Distributions of words that occur at sentence boundaries such as English determiners resemble constituent edges. (ii) Punctuation at sentence boundaries further helps distinguish full sentences from fragments like headlines and titles, allowing us to model grammatical differences between complete and incomplete sentences. (iii) Sentence-internal punctuation boundaries help with longer-distance dependencies, since punctuation correlates with constituent edges. Our models induce state-of-the-art dependency grammars for many languages without — — special knowledge of optimal input sentence lengths or biased, manually-tuned initializers.</p><p>5 0.48003012 <a title="64-lda-5" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>Author: Greg Durrett ; Adam Pauls ; Dan Klein</p><p>Abstract: We consider the problem of using a bilingual dictionary to transfer lexico-syntactic information from a resource-rich source language to a resource-poor target language. In contrast to past work that used bitexts to transfer analyses of specific sentences at the token level, we instead use features to transfer the behavior of words at a type level. In a discriminative dependency parsing framework, our approach produces gains across a range of target languages, using two different lowresource training methodologies (one weakly supervised and one indirectly supervised) and two different dictionary sources (one manually constructed and one automatically constructed).</p><p>6 0.47953486 <a title="64-lda-6" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>7 0.47503296 <a title="64-lda-7" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>8 0.4746643 <a title="64-lda-8" href="./emnlp-2012-A_Discriminative_Model_for_Query_Spelling_Correction_with_Latent_Structural_SVM.html">5 emnlp-2012-A Discriminative Model for Query Spelling Correction with Latent Structural SVM</a></p>
<p>9 0.47407573 <a title="64-lda-9" href="./emnlp-2012-Exploiting_Chunk-level_Features_to_Improve_Phrase_Chunking.html">45 emnlp-2012-Exploiting Chunk-level Features to Improve Phrase Chunking</a></p>
<p>10 0.4705916 <a title="64-lda-10" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>11 0.46990603 <a title="64-lda-11" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>12 0.46894619 <a title="64-lda-12" href="./emnlp-2012-Joint_Entity_and_Event_Coreference_Resolution_across_Documents.html">71 emnlp-2012-Joint Entity and Event Coreference Resolution across Documents</a></p>
<p>13 0.46649092 <a title="64-lda-13" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>14 0.46446159 <a title="64-lda-14" href="./emnlp-2012-Type-Supervised_Hidden_Markov_Models_for_Part-of-Speech_Tagging_with_Incomplete_Tag_Dictionaries.html">129 emnlp-2012-Type-Supervised Hidden Markov Models for Part-of-Speech Tagging with Incomplete Tag Dictionaries</a></p>
<p>15 0.46409491 <a title="64-lda-15" href="./emnlp-2012-An_Empirical_Investigation_of_Statistical_Significance_in_NLP.html">18 emnlp-2012-An Empirical Investigation of Statistical Significance in NLP</a></p>
<p>16 0.46379447 <a title="64-lda-16" href="./emnlp-2012-Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">70 emnlp-2012-Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>17 0.46332875 <a title="64-lda-17" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>18 0.4624179 <a title="64-lda-18" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>19 0.46241596 <a title="64-lda-19" href="./emnlp-2012-Extracting_Opinion_Expressions_with_semi-Markov_Conditional_Random_Fields.html">51 emnlp-2012-Extracting Opinion Expressions with semi-Markov Conditional Random Fields</a></p>
<p>20 0.46083516 <a title="64-lda-20" href="./emnlp-2012-Document-Wide_Decoding_for_Phrase-Based_Statistical_Machine_Translation.html">35 emnlp-2012-Document-Wide Decoding for Phrase-Based Statistical Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
