<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>5 emnlp-2012-A Discriminative Model for Query Spelling Correction with Latent Structural SVM</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-5" href="#">emnlp2012-5</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>5 emnlp-2012-A Discriminative Model for Query Spelling Correction with Latent Structural SVM</h1>
<br/><p>Source: <a title="emnlp-2012-5-pdf" href="http://aclweb.org/anthology//D/D12/D12-1138.pdf">pdf</a></p><p>Author: Huizhong Duan ; Yanen Li ; ChengXiang Zhai ; Dan Roth</p><p>Abstract: Discriminative training in query spelling correction is difficult due to the complex internal structures of the data. Recent work on query spelling correction suggests a two stage approach a noisy channel model that is used to retrieve a number of candidate corrections, followed by discriminatively trained ranker applied to these candidates. The ranker, however, suffers from the fact the low recall of the first, suboptimal, search stage. This paper proposes to directly optimize the search stage with a discriminative model based on latent structural SVM. In this model, we treat query spelling correction as a multiclass classification problem with structured input and output. The latent structural information is used to model the alignment of words in the spelling correction process. Experiment results show that as a standalone speller, our model outperforms all the baseline systems. It also attains a higher recall compared with the noisy channel model, and can therefore serve as a better filtering stage when combined with a ranker.</p><p>Reference: <a title="emnlp-2012-5-reference" href="../emnlp2012_reference/emnlp-2012-A_Discriminative_Model_for_Query_Spelling_Correction_with_Latent_Structural_SVM_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu 2 l ino s  Abstract Discriminative training in query spelling correction is difficult due to the complex internal structures of the data. [sent-2, score-1.454]
</p><p>2 Recent work on query spelling correction suggests a two stage approach a noisy channel model that is used to retrieve a number of candidate corrections, followed by discriminatively trained ranker applied to these candidates. [sent-3, score-1.892]
</p><p>3 This paper proposes to directly optimize the search stage with a discriminative model based on latent structural SVM. [sent-5, score-0.38]
</p><p>4 In this model, we treat query spelling correction as a multiclass classification problem with structured input and output. [sent-6, score-1.454]
</p><p>5 The latent structural information is used to model the alignment of words in the spelling correction process. [sent-7, score-1.23]
</p><p>6 It  also attains a higher recall compared with the noisy channel model, and can therefore serve as a better filtering stage when combined with a ranker. [sent-9, score-0.333]
</p><p>7 1 Introduction Query spelling correction has become a crucial component in modern information systems. [sent-10, score-1.041]
</p><p>8 Particularly, search engine users rely heavily on the query correction mechanism to formulate effective queries. [sent-11, score-1.015]
</p><p>9 Given a user query q, which is potentially misspelled, the goal of query spelling correction is to find a correction of the query c that could lead to a 1511 better search experience. [sent-12, score-2.916]
</p><p>10 A typical query spelling correction system employs a noisy channel model (Kernighan et al. [sent-13, score-1.709]
</p><p>11 The model assumes that the correct query c is formed in the user’s mind before entering the noisy channels, e. [sent-15, score-0.499]
</p><p>12 The prior probability p(c) represents how likely it is that c is the original correct query in the user’s mind. [sent-21, score-0.413]
</p><p>13 One problem with the noisy channel model is that there is no weighting for the two kinds of probabilities, and since they are estimated from different sources, there are usually issues regarding their scale and comparability, resulting in suboptimal performance (Gao et al. [sent-25, score-0.283]
</p><p>14 The difficulty is that the output space of query correction is enormous, as the candidate corrections for each a query term could be the entire vocabulary. [sent-32, score-1.472]
</p><p>15 In this approach, a ranker is trained to score each candidate correction of a query. [sent-39, score-0.664]
</p><p>16 When a query is issued, the system first uses the noisy channel model with a standard search algorithm to find the 20 best candidates. [sent-40, score-0.741]
</p><p>17 Then the ranker is used to re-rank these candidates and find the best correction for the query. [sent-41, score-0.649]
</p><p>18 Because query spelling correction is an online operation, only a small number of candidates can enter the ranker due to efficiency concerns, thus limiting  the ability of the ranker to the ceiling of recall set by the suboptimal search phase. [sent-44, score-1.772]
</p><p>19 The research question we address here is whether we can directly optimize the search phase of query spelling correction using a discriminative model without loss of efficiency. [sent-45, score-1.76]
</p><p>20 More specifically, we want 1) a learning process that is aware of the search phase and interacts with its result; 2) an efficient search algorithm that is able to incorporate the learned model and guide the search to the target spelling correction. [sent-46, score-0.762]
</p><p>21 In this paper, we propose a new discriminative model for query correction that maintains the advantage of a discriminative model in accommodating flexible combination of features and naturally incorporates an efficient search algorithm in learning and inference. [sent-47, score-1.225]
</p><p>22 We formulate the problem query spelling correction as a multi-  1512 class classification problem on structured inputs and outputs. [sent-51, score-1.454]
</p><p>23 This allows us to adapt the model to make it work directly with the search algorithm we use for finding the best correction of the query. [sent-53, score-0.602]
</p><p>24 To account for word boundary errors, we model the word alignment between the query and the correction as a latent structural variable. [sent-54, score-1.169]
</p><p>25 As the inference algorithm in the proposed discriminative model we use an algorithm that resembles a traditional noisy channel model. [sent-56, score-0.36]
</p><p>26 To adapt the LS-SVM model to enable the efficient search of query spelling correction, we study how features can be designed. [sent-57, score-0.998]
</p><p>27 We demonstrate the use of the criteria by designing separate features for different types of spelling errors (e. [sent-59, score-0.562]
</p><p>28 With the proposed discriminative model, we can directly optimize the  search phase of query spelling correction without loss of efficiency. [sent-62, score-1.76]
</p><p>29 Our model can be used not only as a standalone speller with high accuracy, but also as a high recall candidate generation stage for a ranker based system. [sent-63, score-0.377]
</p><p>30 Experiments verify the effectiveness of the discriminative model, as the accuracy of correction can be improved significantly over baseline systems including an award winning query spelling system. [sent-64, score-1.581]
</p><p>31 The improvement in recall at different levels over the noisy channel model demonstrates that our model is superior even when used in the two-stage approach. [sent-66, score-0.255]
</p><p>32 2  Related Work  Spelling correction has a long history (Levenshtein, 1966). [sent-68, score-0.529]
</p><p>33 Later, statistical generative models were shown to be effective in spelling correction, where a source language model and an er-  ror model were identified as two major components (Brill and Moore, 2000). [sent-70, score-0.512]
</p><p>34 Note that we are not dealing here with the standard models in context sensitive spelling (Golding and Roth, 1999) where the set of candidate correction is a known “confusion set”. [sent-71, score-1.079]
</p><p>35 Query spelling correction, a special form of the problem, has received much attention in recent years. [sent-72, score-0.512]
</p><p>36 Compared with traditional spelling correction task, query spelling deals with more complex types of misspellings and a much larger scale of language. [sent-73, score-2.015]
</p><p>37 Research in this direction includes utilizing large web corpora and query log (Chen et al. [sent-74, score-0.413]
</p><p>38 Query alteration/refinement is a very relevant topic to query spelling correction. [sent-78, score-0.925]
</p><p>39 The goal of query alteration/refinement is to modify the ineffective query so that it could . [sent-79, score-0.826]
</p><p>40 Researches on this track include query expansion (Xu and Croft, 1996; Qiu and Frei, 1993; Mitra et al. [sent-80, score-0.413]
</p><p>41 , 1998), query contraction(Kumaran and Allan, 2008; Bendersky and Croft, 2008; Kumaran and Carvalho, 2009) and  other types of query reformulations for bridging the vocabulary gap (Wang and Zhai, 2008). [sent-81, score-0.87]
</p><p>42 , 2008) proposed a unified model to perform a broad set of query refinements including correction, segmentation and stemming. [sent-83, score-0.413]
</p><p>43 However, it has very limited ability in query correction. [sent-84, score-0.413]
</p><p>44 In this paper, we study the discriminative training of query spelling correction, which is potentially beneficial to many existing studies. [sent-85, score-1.03]
</p><p>45 Noisy channel model (or source channel model) has been widely used in NLP. [sent-86, score-0.338]
</p><p>46 In spelling correction, however, the search space is much bigger and the existing approaches featuring dynamic programming are difficult to be applied. [sent-93, score-0.585]
</p><p>47 Query spelling correction also shares many similarities with statistical machine translation (SMT). [sent-107, score-1.041]
</p><p>48 However, SMT usually involves more complex alignments, while in query spelling correction search is the more challenging part. [sent-110, score-1.527]
</p><p>49 Our main contribution in this paper is a novel unified way to directly optimize the search phase of  query spelling correction with the use of LS-SVM. [sent-111, score-1.582]
</p><p>50 3  Discriminative Model for Query Spelling Correction Based on LS-SVM  In this section, we first present the discriminative formulation of the problem of query spelling correction. [sent-112, score-1.071]
</p><p>51 1 The Discriminative Form of Query Spelling Correction In query spelling correction, given a user entered query q, which is potentially misspelled, the goal is to find a correction c, such that it could be a more effective query which improves the quality of search results. [sent-115, score-2.387]
</p><p>52 This discriminative formulation is more general compared to the noisy channel model. [sent-117, score-0.401]
</p><p>53 The noisy channel model is a special case of the discriminative form where only two features, the source probability and the transformation probability, are used and uniform weightings are applied. [sent-119, score-0.417]
</p><p>54 To enhance the formulation, we explore the fact that spelling correction follows a word-by-word procedure. [sent-122, score-1.041]
</p><p>55 In this scenario, each query term matches and only matches to a single term in the correction. [sent-124, score-0.469]
</p><p>56 t Wh hqen an no w haorvde boundary error exists, |c| = |q| holds for any candidate correction c. [sent-134, score-0.629]
</p><p>57 rIenc ttihoins case, we have a more detailed discriminative form:  f(q) = arc∈gVm|qa|x[w · (Ψ0+Xi|=q|1Ψ1(qi,ci) ],  (4)  where Ψ0 is a vector of normalizing factors, Ψ1 (qi, ci) is the decomposed computation of Ψ(q, c) for each query term qi and ci, for i= 1to |q| . [sent-136, score-0.575]
</p><p>58 To account for these word boundary errors and enhance the discriminative formulation, we introduce a latent variable a to model the unobserved structural information. [sent-143, score-0.322]
</p><p>59 2 Latent Structural SVM We employ the latent structural SVM (LS-SVM) model for learning the discriminative model ofquery spelling correction. [sent-155, score-0.747]
</p><p>60 We will show in detail how we solve these maximization problems to make LSSVM work for query spelling correction in the following subsection. [sent-174, score-1.535]
</p><p>61 3 Solving the Inference Problems The essential inference problem is to find the correction that maximizes the scoring function according  to the model (i. [sent-179, score-0.529]
</p><p>62 For this purpose we design a best first search algorithm similar to the standard search algorithm in the noisy channel model. [sent-182, score-0.437]
</p><p>63 Splitting errors are dealt with in Algorithm 1 by “looking forward” m words in the query when generating candidate words. [sent-190, score-0.477]
</p><p>64 Given query q, for any alignment At = At−1 ∪ {at} at time t, ψ(qAt, cAt, At) ≤ ψ(qA cAt−1 At−1), wmheer te, qAt is the concate≤nation of qa0 to qat and cAt is the concatenation of ca0 to cat . [sent-197, score-0.622]
</p><p>65 In spelling correction, usually only one correction is valid for an input query. [sent-205, score-1.041]
</p><p>66 During the search procedure, we check if the loss decreases to 0 given the correction string so far. [sent-209, score-0.675]
</p><p>67 The inference of the latent alignment variable can be solved with dynamic programming, as the number of possible alignments is limited given the query and the correction. [sent-214, score-0.554]
</p><p>68 1 Source Probability and Transformation Probability We know from empirical experience that the source probability and the transformation probability are the two most important features in query spelling correction. [sent-217, score-0.982]
</p><p>69 This allows fine tuning of the query spelling correction system, making it more adaptive to environments where the ratio of different types of errors may vary. [sent-227, score-1.48]
</p><p>70 Moreover, the model also allows us to include language models trained over different resources, such as query log, title of webpages or anchor texts. [sent-228, score-0.413]
</p><p>71 2  Local Heuristic Features  Despite the goal of query spelling correction is to deal with misspellings, in real world most queries are correctly spelled. [sent-230, score-1.51]
</p><p>72 A good query spelling correction system shall prevent as much as possible from misjudging an correctly spelled query as misspelled. [sent-231, score-1.891]
</p><p>73 When a query term is matched against trustable vocabulary, it increases the chance that the term is already in its correct form. [sent-237, score-0.469]
</p><p>74 3 Global Heuristic Features Some global heuristics are also important in query spelling correction. [sent-250, score-0.925]
</p><p>75 org of words being corrected in the query may be an indicator of whether the system has leaned towards overcorrecting. [sent-253, score-0.436]
</p><p>76 5  Experiments  In order to test the effectiveness and efficiency ofour proposed discriminative training method, in this section we conduct extensive experiments on two web query spelling datasets. [sent-258, score-1.03]
</p><p>77 Below we first present the dataset and evaluation metrics, followed by the experiment results on query spelling correction. [sent-259, score-0.925]
</p><p>78 1 Dataset Preparation The experiments are conducted on two query spelling correction datasets. [sent-261, score-1.454]
</p><p>79 We have also annotated another dataset that contains 4926 MSN queries, where for each query there is at most one correction. [sent-267, score-0.413]
</p><p>80 If they agree on the returned results (including the case if the query is just unchanged), we take it as the corrected form of the input query. [sent-272, score-0.436]
</p><p>81 Let q be a user query and C(q) = (c1, c2, , ck) be the set of system output with posterior probabil-  ities P(ci |q). [sent-282, score-0.447]
</p><p>82 Let S(q) denote the set of plausible spelling va|qri)a. [sent-283, score-0.512]
</p><p>83 The echo system is usually considered as a strong baseline in query spelling correction as the majority of the queries are correctly spelled queries. [sent-291, score-1.56]
</p><p>84 461†  We show performances for the entire query sets as well as the query sets consisting only the misspelled queries. [sent-330, score-0.884]
</p><p>85 to achieve statistical significance in the entire query set as majority queries have almost identical performance in different systems due to the large amount of correct queries. [sent-333, score-0.469]
</p><p>86 Despite we are primarily focused on optimizing the top correction in our discriminative model, we can also use the trained system to output multiple candidate corrections. [sent-337, score-0.672]
</p><p>87 Table 2 compare our system with the noisy channel model (N-C) in terms of recall at different levels of cutoff. [sent-338, score-0.255]
</p><p>88 Since the ranker makes use of arbitrary features, it has the potential of further improving the accuracy of query spelling correction. [sent-341, score-1.022]
</p><p>89 960  the precision of query spelling correction can benefits from the use of rich features. [sent-356, score-1.454]
</p><p>90 This is reasonable as the additional features are primarily designed to improve the accuracy of the top correction generated by the system. [sent-358, score-0.529]
</p><p>91 928  Conclusions  In this paper, we present a novel discriminative model for query spelling correction. [sent-374, score-1.03]
</p><p>92 The paper made the following contributions: First, to the best of our knowledge, this is a novel exploration of directly optimizing the search phase in query spelling correction with a discriminative model. [sent-375, score-1.663]
</p><p>93 By modeling word alignment as the latent structural information, our formulation also deals with word boundary errors. [sent-376, score-0.293]
</p><p>94 When used in a two stage approach, it attains higher recall than the noisy channel model and can thus serve as a superior method for candidate generation. [sent-383, score-0.371]
</p><p>95 We also verify that through the use of rich features, we can further improve the accuracy of our query spelling correction system. [sent-384, score-1.454]
</p><p>96 Learning a spelling error model from search query logs. [sent-390, score-1.022]
</p><p>97 An improved error model for noisy channel spelling correction. [sent-405, score-0.791]
</p><p>98 Spelling correction as an iterative process that exploits the collective knowledge of web users. [sent-426, score-0.529]
</p><p>99 A large scale ranker-based system for search query spelling correction. [sent-475, score-0.998]
</p><p>100 A spelling correction program based on a noisy channel model. [sent-506, score-1.296]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('correction', 0.529), ('spelling', 0.512), ('query', 0.413), ('channel', 0.169), ('speller', 0.118), ('discriminative', 0.105), ('ranker', 0.097), ('qat', 0.091), ('sigir', 0.091), ('noisy', 0.086), ('lssvm', 0.076), ('monotonicity', 0.076), ('standalone', 0.076), ('loss', 0.073), ('search', 0.073), ('structural', 0.071), ('freitag', 0.061), ('latent', 0.059), ('cat', 0.059), ('alignment', 0.059), ('misspelled', 0.058), ('transformation', 0.057), ('queries', 0.056), ('maximization', 0.053), ('kumaran', 0.052), ('corrections', 0.051), ('qa', 0.051), ('heuristic', 0.05), ('splitting', 0.049), ('trec', 0.049), ('stage', 0.048), ('gao', 0.044), ('vocabulary', 0.044), ('acm', 0.043), ('merging', 0.043), ('formulation', 0.041), ('candidate', 0.038), ('svm', 0.038), ('boundary', 0.038), ('duan', 0.037), ('design', 0.036), ('yu', 0.036), ('joachims', 0.034), ('ny', 0.034), ('user', 0.034), ('pq', 0.033), ('priority', 0.033), ('logp', 0.033), ('path', 0.032), ('daume', 0.031), ('phase', 0.031), ('ahmad', 0.03), ('attains', 0.03), ('cxi', 0.03), ('glopg', 0.03), ('golding', 0.03), ('khadivi', 0.03), ('mitra', 0.03), ('msn', 0.03), ('wcmax', 0.03), ('qi', 0.029), ('solve', 0.028), ('brill', 0.028), ('suboptimal', 0.028), ('term', 0.028), ('microsoft', 0.028), ('york', 0.027), ('correcting', 0.026), ('enumerate', 0.026), ('bendersky', 0.026), ('cccp', 0.026), ('customizable', 0.026), ('echo', 0.026), ('kernighan', 0.026), ('searn', 0.026), ('errors', 0.026), ('deals', 0.025), ('zhai', 0.025), ('roth', 0.025), ('designing', 0.024), ('error', 0.024), ('optimize', 0.024), ('micol', 0.024), ('misspellings', 0.024), ('quadruple', 0.024), ('serving', 0.024), ('spelled', 0.024), ('trie', 0.024), ('yessenalina', 0.024), ('variable', 0.023), ('candidates', 0.023), ('corrected', 0.023), ('ci', 0.022), ('arg', 0.022), ('dreyer', 0.022), ('guidance', 0.022), ('queue', 0.022), ('qx', 0.022), ('winning', 0.022), ('max', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="5-tfidf-1" href="./emnlp-2012-A_Discriminative_Model_for_Query_Spelling_Correction_with_Latent_Structural_SVM.html">5 emnlp-2012-A Discriminative Model for Query Spelling Correction with Latent Structural SVM</a></p>
<p>Author: Huizhong Duan ; Yanen Li ; ChengXiang Zhai ; Dan Roth</p><p>Abstract: Discriminative training in query spelling correction is difficult due to the complex internal structures of the data. Recent work on query spelling correction suggests a two stage approach a noisy channel model that is used to retrieve a number of candidate corrections, followed by discriminatively trained ranker applied to these candidates. The ranker, however, suffers from the fact the low recall of the first, suboptimal, search stage. This paper proposes to directly optimize the search stage with a discriminative model based on latent structural SVM. In this model, we treat query spelling correction as a multiclass classification problem with structured input and output. The latent structural information is used to model the alignment of words in the spelling correction process. Experiment results show that as a standalone speller, our model outperforms all the baseline systems. It also attains a higher recall compared with the noisy channel model, and can therefore serve as a better filtering stage when combined with a ranker.</p><p>2 0.49244875 <a title="5-tfidf-2" href="./emnlp-2012-A_Unified_Approach_to_Transliteration-based_Text_Input_with_Online_Spelling_Correction.html">13 emnlp-2012-A Unified Approach to Transliteration-based Text Input with Online Spelling Correction</a></p>
<p>Author: Hisami Suzuki ; Jianfeng Gao</p><p>Abstract: This paper presents an integrated, end-to-end approach to online spelling correction for text input. Online spelling correction refers to the spelling correction as you type, as opposed to post-editing. The online scenario is particularly important for languages that routinely use transliteration-based text input methods, such as Chinese and Japanese, because the desired target characters cannot be input at all unless they are in the list of candidates provided by an input method, and spelling errors prevent them from appearing in the list. For example, a user might type suesheng by mistake to mean xuesheng 学生 'student' in Chinese; existing input methods fail to convert this misspelled input to the desired target Chinese characters. In this paper, we propose a unified approach to the problem of spelling correction and transliteration-based character conversion using an approach inspired by the phrasebased statistical machine translation framework. At the phrase (substring) level, k most probable pinyin (Romanized Chinese) corrections are generated using a monotone decoder; at the sentence level, input pinyin strings are directly transliterated into target Chinese characters by a decoder using a loglinear model that refer to the features of both levels. A new method of automatically deriving parallel training data from user keystroke logs is also presented. Experiments on Chinese pinyin conversion show that our integrated method reduces the character error rate by 20% (from 8.9% to 7. 12%) over the previous state-of-the art based on a noisy channel model. 609 1</p><p>3 0.34469983 <a title="5-tfidf-3" href="./emnlp-2012-A_Beam-Search_Decoder_for_Grammatical_Error_Correction.html">2 emnlp-2012-A Beam-Search Decoder for Grammatical Error Correction</a></p>
<p>Author: Daniel Dahlmeier ; Hwee Tou Ng</p><p>Abstract: We present a novel beam-search decoder for grammatical error correction. The decoder iteratively generates new hypothesis corrections from current hypotheses and scores them based on features of grammatical correctness and fluency. These features include scores from discriminative classifiers for specific error categories, such as articles and prepositions. Unlike all previous approaches, our method is able to perform correction of whole sentences with multiple and interacting errors while still taking advantage of powerful existing classifier approaches. Our decoder achieves an F1 correction score significantly higher than all previous published scores on the Helping Our Own (HOO) shared task data set.</p><p>4 0.21219678 <a title="5-tfidf-4" href="./emnlp-2012-Learning_Lexicon_Models_from_Search_Logs_for_Query_Expansion.html">78 emnlp-2012-Learning Lexicon Models from Search Logs for Query Expansion</a></p>
<p>Author: Jianfeng Gao ; Shasha Xie ; Xiaodong He ; Alnur Ali</p><p>Abstract: This paper explores log-based query expansion (QE) models for Web search. Three lexicon models are proposed to bridge the lexical gap between Web documents and user queries. These models are trained on pairs of user queries and titles of clicked documents. Evaluations on a real world data set show that the lexicon models, integrated into a ranker-based QE system, not only significantly improve the document retrieval performance but also outperform two state-of-the-art log-based QE methods.</p><p>5 0.099724203 <a title="5-tfidf-5" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>Author: Mohamed Yahya ; Klaus Berberich ; Shady Elbassuoni ; Maya Ramanath ; Volker Tresp ; Gerhard Weikum</p><p>Abstract: The Linked Data initiative comprises structured databases in the Semantic-Web data model RDF. Exploring this heterogeneous data by structured query languages is tedious and error-prone even for skilled users. To ease the task, this paper presents a methodology for translating natural language questions into structured SPARQL queries over linked-data sources. Our method is based on an integer linear program to solve several disambiguation tasks jointly: the segmentation of questions into phrases; the mapping of phrases to semantic entities, classes, and relations; and the construction of SPARQL triple patterns. Our solution harnesses the rich type system provided by knowledge bases in the web of linked data, to constrain our semantic-coherence objective function. We present experiments on both the . in question translation and the resulting query answering.</p><p>6 0.087008819 <a title="5-tfidf-6" href="./emnlp-2012-Collocation_Polarity_Disambiguation_Using_Web-based_Pseudo_Contexts.html">28 emnlp-2012-Collocation Polarity Disambiguation Using Web-based Pseudo Contexts</a></p>
<p>7 0.084922239 <a title="5-tfidf-7" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>8 0.075309314 <a title="5-tfidf-8" href="./emnlp-2012-Entity_based_QA_Retrieval.html">41 emnlp-2012-Entity based QA Retrieval</a></p>
<p>9 0.074398972 <a title="5-tfidf-9" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>10 0.058088537 <a title="5-tfidf-10" href="./emnlp-2012-Fast_Large-Scale_Approximate_Graph_Construction_for_NLP.html">52 emnlp-2012-Fast Large-Scale Approximate Graph Construction for NLP</a></p>
<p>11 0.056415401 <a title="5-tfidf-11" href="./emnlp-2012-Forest_Reranking_through_Subtree_Ranking.html">55 emnlp-2012-Forest Reranking through Subtree Ranking</a></p>
<p>12 0.054186158 <a title="5-tfidf-12" href="./emnlp-2012-Iterative_Annotation_Transformation_with_Predict-Self_Reestimation_for_Chinese_Word_Segmentation.html">68 emnlp-2012-Iterative Annotation Transformation with Predict-Self Reestimation for Chinese Word Segmentation</a></p>
<p>13 0.048338402 <a title="5-tfidf-13" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>14 0.044388607 <a title="5-tfidf-14" href="./emnlp-2012-Identifying_Event-related_Bursts_via_Social_Media_Activities.html">63 emnlp-2012-Identifying Event-related Bursts via Social Media Activities</a></p>
<p>15 0.044100162 <a title="5-tfidf-15" href="./emnlp-2012-Resolving_Complex_Cases_of_Definite_Pronouns%3A_The_Winograd_Schema_Challenge.html">112 emnlp-2012-Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge</a></p>
<p>16 0.043868389 <a title="5-tfidf-16" href="./emnlp-2012-Locally_Training_the_Log-Linear_Model_for_SMT.html">86 emnlp-2012-Locally Training the Log-Linear Model for SMT</a></p>
<p>17 0.042303976 <a title="5-tfidf-17" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>18 0.041199293 <a title="5-tfidf-18" href="./emnlp-2012-Revisiting_the_Predictability_of_Language%3A_Response_Completion_in_Social_Media.html">114 emnlp-2012-Revisiting the Predictability of Language: Response Completion in Social Media</a></p>
<p>19 0.041072827 <a title="5-tfidf-19" href="./emnlp-2012-Inducing_a_Discriminative_Parser_to_Optimize_Machine_Translation_Reordering.html">67 emnlp-2012-Inducing a Discriminative Parser to Optimize Machine Translation Reordering</a></p>
<p>20 0.040484548 <a title="5-tfidf-20" href="./emnlp-2012-Optimising_Incremental_Dialogue_Decisions_Using_Information_Density_for_Interactive_Systems.html">102 emnlp-2012-Optimising Incremental Dialogue Decisions Using Information Density for Interactive Systems</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.193), (1, -0.053), (2, -0.048), (3, 0.092), (4, -0.106), (5, -0.411), (6, 0.573), (7, -0.293), (8, -0.033), (9, 0.044), (10, -0.022), (11, 0.114), (12, 0.114), (13, 0.02), (14, -0.053), (15, 0.016), (16, 0.024), (17, -0.011), (18, 0.017), (19, -0.099), (20, 0.03), (21, -0.014), (22, -0.009), (23, -0.063), (24, 0.053), (25, 0.031), (26, 0.02), (27, -0.038), (28, 0.019), (29, 0.041), (30, -0.011), (31, 0.023), (32, -0.027), (33, -0.045), (34, -0.021), (35, -0.005), (36, -0.043), (37, 0.003), (38, -0.039), (39, 0.013), (40, -0.002), (41, 0.012), (42, 0.025), (43, 0.017), (44, 0.001), (45, 0.016), (46, 0.013), (47, -0.011), (48, -0.01), (49, -0.002)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96610767 <a title="5-lsi-1" href="./emnlp-2012-A_Discriminative_Model_for_Query_Spelling_Correction_with_Latent_Structural_SVM.html">5 emnlp-2012-A Discriminative Model for Query Spelling Correction with Latent Structural SVM</a></p>
<p>Author: Huizhong Duan ; Yanen Li ; ChengXiang Zhai ; Dan Roth</p><p>Abstract: Discriminative training in query spelling correction is difficult due to the complex internal structures of the data. Recent work on query spelling correction suggests a two stage approach a noisy channel model that is used to retrieve a number of candidate corrections, followed by discriminatively trained ranker applied to these candidates. The ranker, however, suffers from the fact the low recall of the first, suboptimal, search stage. This paper proposes to directly optimize the search stage with a discriminative model based on latent structural SVM. In this model, we treat query spelling correction as a multiclass classification problem with structured input and output. The latent structural information is used to model the alignment of words in the spelling correction process. Experiment results show that as a standalone speller, our model outperforms all the baseline systems. It also attains a higher recall compared with the noisy channel model, and can therefore serve as a better filtering stage when combined with a ranker.</p><p>2 0.92089164 <a title="5-lsi-2" href="./emnlp-2012-A_Unified_Approach_to_Transliteration-based_Text_Input_with_Online_Spelling_Correction.html">13 emnlp-2012-A Unified Approach to Transliteration-based Text Input with Online Spelling Correction</a></p>
<p>Author: Hisami Suzuki ; Jianfeng Gao</p><p>Abstract: This paper presents an integrated, end-to-end approach to online spelling correction for text input. Online spelling correction refers to the spelling correction as you type, as opposed to post-editing. The online scenario is particularly important for languages that routinely use transliteration-based text input methods, such as Chinese and Japanese, because the desired target characters cannot be input at all unless they are in the list of candidates provided by an input method, and spelling errors prevent them from appearing in the list. For example, a user might type suesheng by mistake to mean xuesheng 学生 'student' in Chinese; existing input methods fail to convert this misspelled input to the desired target Chinese characters. In this paper, we propose a unified approach to the problem of spelling correction and transliteration-based character conversion using an approach inspired by the phrasebased statistical machine translation framework. At the phrase (substring) level, k most probable pinyin (Romanized Chinese) corrections are generated using a monotone decoder; at the sentence level, input pinyin strings are directly transliterated into target Chinese characters by a decoder using a loglinear model that refer to the features of both levels. A new method of automatically deriving parallel training data from user keystroke logs is also presented. Experiments on Chinese pinyin conversion show that our integrated method reduces the character error rate by 20% (from 8.9% to 7. 12%) over the previous state-of-the art based on a noisy channel model. 609 1</p><p>3 0.68958884 <a title="5-lsi-3" href="./emnlp-2012-A_Beam-Search_Decoder_for_Grammatical_Error_Correction.html">2 emnlp-2012-A Beam-Search Decoder for Grammatical Error Correction</a></p>
<p>Author: Daniel Dahlmeier ; Hwee Tou Ng</p><p>Abstract: We present a novel beam-search decoder for grammatical error correction. The decoder iteratively generates new hypothesis corrections from current hypotheses and scores them based on features of grammatical correctness and fluency. These features include scores from discriminative classifiers for specific error categories, such as articles and prepositions. Unlike all previous approaches, our method is able to perform correction of whole sentences with multiple and interacting errors while still taking advantage of powerful existing classifier approaches. Our decoder achieves an F1 correction score significantly higher than all previous published scores on the Helping Our Own (HOO) shared task data set.</p><p>4 0.50596583 <a title="5-lsi-4" href="./emnlp-2012-Learning_Lexicon_Models_from_Search_Logs_for_Query_Expansion.html">78 emnlp-2012-Learning Lexicon Models from Search Logs for Query Expansion</a></p>
<p>Author: Jianfeng Gao ; Shasha Xie ; Xiaodong He ; Alnur Ali</p><p>Abstract: This paper explores log-based query expansion (QE) models for Web search. Three lexicon models are proposed to bridge the lexical gap between Web documents and user queries. These models are trained on pairs of user queries and titles of clicked documents. Evaluations on a real world data set show that the lexicon models, integrated into a ranker-based QE system, not only significantly improve the document retrieval performance but also outperform two state-of-the-art log-based QE methods.</p><p>5 0.25091791 <a title="5-lsi-5" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>Author: Ni Lao ; Amarnag Subramanya ; Fernando Pereira ; William W. Cohen</p><p>Abstract: We study how to extend a large knowledge base (Freebase) by reading relational information from a large Web text corpus. Previous studies on extracting relational knowledge from text show the potential of syntactic patterns for extraction, but they do not exploit background knowledge of other relations in the knowledge base. We describe a distributed, Web-scale implementation of a path-constrained random walk model that learns syntactic-semantic inference rules for binary relations from a graph representation of the parsed text and the knowledge base. Experiments show significant accuracy improvements in binary relation prediction over methods that consider only text, or only the existing knowledge base.</p><p>6 0.24653308 <a title="5-lsi-6" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>7 0.21945445 <a title="5-lsi-7" href="./emnlp-2012-Entity_based_QA_Retrieval.html">41 emnlp-2012-Entity based QA Retrieval</a></p>
<p>8 0.21155955 <a title="5-lsi-8" href="./emnlp-2012-Collocation_Polarity_Disambiguation_Using_Web-based_Pseudo_Contexts.html">28 emnlp-2012-Collocation Polarity Disambiguation Using Web-based Pseudo Contexts</a></p>
<p>9 0.18629254 <a title="5-lsi-9" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>10 0.16996747 <a title="5-lsi-10" href="./emnlp-2012-Fast_Large-Scale_Approximate_Graph_Construction_for_NLP.html">52 emnlp-2012-Fast Large-Scale Approximate Graph Construction for NLP</a></p>
<p>11 0.15203594 <a title="5-lsi-11" href="./emnlp-2012-Sketch_Algorithms_for_Estimating_Point_Queries_in_NLP.html">117 emnlp-2012-Sketch Algorithms for Estimating Point Queries in NLP</a></p>
<p>12 0.14959311 <a title="5-lsi-12" href="./emnlp-2012-Revisiting_the_Predictability_of_Language%3A_Response_Completion_in_Social_Media.html">114 emnlp-2012-Revisiting the Predictability of Language: Response Completion in Social Media</a></p>
<p>13 0.14718015 <a title="5-lsi-13" href="./emnlp-2012-Identifying_Event-related_Bursts_via_Social_Media_Activities.html">63 emnlp-2012-Identifying Event-related Bursts via Social Media Activities</a></p>
<p>14 0.14341615 <a title="5-lsi-14" href="./emnlp-2012-Large_Scale_Decipherment_for_Out-of-Domain_Machine_Translation.html">75 emnlp-2012-Large Scale Decipherment for Out-of-Domain Machine Translation</a></p>
<p>15 0.13953973 <a title="5-lsi-15" href="./emnlp-2012-Parser_Showdown_at_the_Wall_Street_Corral%3A_An_Empirical_Investigation_of_Error_Types_in_Parser_Output.html">105 emnlp-2012-Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</a></p>
<p>16 0.13872403 <a title="5-lsi-16" href="./emnlp-2012-Document-Wide_Decoding_for_Phrase-Based_Statistical_Machine_Translation.html">35 emnlp-2012-Document-Wide Decoding for Phrase-Based Statistical Machine Translation</a></p>
<p>17 0.13754705 <a title="5-lsi-17" href="./emnlp-2012-Automatically_Constructing_a_Normalisation_Dictionary_for_Microblogs.html">22 emnlp-2012-Automatically Constructing a Normalisation Dictionary for Microblogs</a></p>
<p>18 0.13453442 <a title="5-lsi-18" href="./emnlp-2012-Forest_Reranking_through_Subtree_Ranking.html">55 emnlp-2012-Forest Reranking through Subtree Ranking</a></p>
<p>19 0.13298355 <a title="5-lsi-19" href="./emnlp-2012-Name_Phylogeny%3A_A_Generative_Model_of_String_Variation.html">96 emnlp-2012-Name Phylogeny: A Generative Model of String Variation</a></p>
<p>20 0.13199347 <a title="5-lsi-20" href="./emnlp-2012-Exploiting_Chunk-level_Features_to_Improve_Phrase_Chunking.html">45 emnlp-2012-Exploiting Chunk-level Features to Improve Phrase Chunking</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.012), (16, 0.056), (25, 0.015), (34, 0.091), (39, 0.016), (52, 0.251), (60, 0.085), (63, 0.045), (64, 0.016), (65, 0.026), (70, 0.035), (74, 0.045), (76, 0.053), (80, 0.03), (86, 0.028), (95, 0.094)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77690679 <a title="5-lda-1" href="./emnlp-2012-A_Discriminative_Model_for_Query_Spelling_Correction_with_Latent_Structural_SVM.html">5 emnlp-2012-A Discriminative Model for Query Spelling Correction with Latent Structural SVM</a></p>
<p>Author: Huizhong Duan ; Yanen Li ; ChengXiang Zhai ; Dan Roth</p><p>Abstract: Discriminative training in query spelling correction is difficult due to the complex internal structures of the data. Recent work on query spelling correction suggests a two stage approach a noisy channel model that is used to retrieve a number of candidate corrections, followed by discriminatively trained ranker applied to these candidates. The ranker, however, suffers from the fact the low recall of the first, suboptimal, search stage. This paper proposes to directly optimize the search stage with a discriminative model based on latent structural SVM. In this model, we treat query spelling correction as a multiclass classification problem with structured input and output. The latent structural information is used to model the alignment of words in the spelling correction process. Experiment results show that as a standalone speller, our model outperforms all the baseline systems. It also attains a higher recall compared with the noisy channel model, and can therefore serve as a better filtering stage when combined with a ranker.</p><p>2 0.55150747 <a title="5-lda-2" href="./emnlp-2012-Learning_Lexicon_Models_from_Search_Logs_for_Query_Expansion.html">78 emnlp-2012-Learning Lexicon Models from Search Logs for Query Expansion</a></p>
<p>Author: Jianfeng Gao ; Shasha Xie ; Xiaodong He ; Alnur Ali</p><p>Abstract: This paper explores log-based query expansion (QE) models for Web search. Three lexicon models are proposed to bridge the lexical gap between Web documents and user queries. These models are trained on pairs of user queries and titles of clicked documents. Evaluations on a real world data set show that the lexicon models, integrated into a ranker-based QE system, not only significantly improve the document retrieval performance but also outperform two state-of-the-art log-based QE methods.</p><p>3 0.55146706 <a title="5-lda-3" href="./emnlp-2012-Lexical_Differences_in_Autobiographical_Narratives_from_Schizophrenic_Patients_and_Healthy_Controls.html">83 emnlp-2012-Lexical Differences in Autobiographical Narratives from Schizophrenic Patients and Healthy Controls</a></p>
<p>Author: Kai Hong ; Christian G. Kohler ; Mary E. March ; Amber A. Parker ; Ani Nenkova</p><p>Abstract: We present a system for automatic identification of schizophrenic patients and healthy controls based on narratives the subjects recounted about emotional experiences in their own life. The focus of the study is to identify the lexical features that distinguish the two populations. We report the results of feature selection experiments that demonstrate that the classifier can achieve accuracy on patient level prediction as high as 76.9% with only a small set of features. We provide an in-depth discussion of the lexical features that distinguish the two groups and the unexpected relationship between emotion types of the narratives and the accuracy of patient status prediction.</p><p>4 0.54886585 <a title="5-lda-4" href="./emnlp-2012-Collocation_Polarity_Disambiguation_Using_Web-based_Pseudo_Contexts.html">28 emnlp-2012-Collocation Polarity Disambiguation Using Web-based Pseudo Contexts</a></p>
<p>Author: Yanyan Zhao ; Bing Qin ; Ting Liu</p><p>Abstract: This paper focuses on the task of collocation polarity disambiguation. The collocation refers to a binary tuple of a polarity word and a target (such as ⟨long, battery life⟩ or ⟨long, ast atratrguep⟩t) (, siunc whh aisch ⟨ ltohneg s,en btatitmeernyt l iofrei⟩en otrat ⟨iolonn gof, tshtaer polarity wwohirdch (“long”) changes along owniothf different targets (“battery life” or “startup”). To disambiguate a collocation’s polarity, previous work always turned to investigate the polarities of its surrounding contexts, and then assigned the majority polarity to the collocation. However, these contexts are limited, thus the resulting polarity is insufficient to be reliable. We therefore propose an unsupervised three-component framework to expand some pseudo contexts from web, to help disambiguate a collocation’s polarity.Without using any additional labeled data, experiments , show that our method is effective.</p><p>5 0.5438875 <a title="5-lda-5" href="./emnlp-2012-Exploring_Topic_Coherence_over_Many_Models_and_Many_Topics.html">49 emnlp-2012-Exploring Topic Coherence over Many Models and Many Topics</a></p>
<p>Author: Keith Stevens ; Philip Kegelmeyer ; David Andrzejewski ; David Buttler</p><p>Abstract: We apply two new automated semantic evaluations to three distinct latent topic models. Both metrics have been shown to align with human evaluations and provide a balance between internal measures of information gain and comparisons to human ratings of coherent topics. We improve upon the measures by introducing new aggregate measures that allows for comparing complete topic models. We further compare the automated measures to other metrics for topic models, comparison to manually crafted semantic tests and document classification. Our experiments reveal that LDA and LSA each have different strengths; LDA best learns descriptive topics while LSA is best at creating a compact semantic representation ofdocuments and words in a corpus.</p><p>6 0.54245484 <a title="5-lda-6" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>7 0.53206849 <a title="5-lda-7" href="./emnlp-2012-A_Unified_Approach_to_Transliteration-based_Text_Input_with_Online_Spelling_Correction.html">13 emnlp-2012-A Unified Approach to Transliteration-based Text Input with Online Spelling Correction</a></p>
<p>8 0.52969784 <a title="5-lda-8" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>9 0.52850282 <a title="5-lda-9" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>10 0.52663779 <a title="5-lda-10" href="./emnlp-2012-Fast_Large-Scale_Approximate_Graph_Construction_for_NLP.html">52 emnlp-2012-Fast Large-Scale Approximate Graph Construction for NLP</a></p>
<p>11 0.52237988 <a title="5-lda-11" href="./emnlp-2012-Streaming_Analysis_of_Discourse_Participants.html">120 emnlp-2012-Streaming Analysis of Discourse Participants</a></p>
<p>12 0.52111471 <a title="5-lda-12" href="./emnlp-2012-Explore_Person_Specific_Evidence_in_Web_Person_Name_Disambiguation.html">47 emnlp-2012-Explore Person Specific Evidence in Web Person Name Disambiguation</a></p>
<p>13 0.52025473 <a title="5-lda-13" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>14 0.520217 <a title="5-lda-14" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>15 0.51740909 <a title="5-lda-15" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<p>16 0.51723671 <a title="5-lda-16" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>17 0.5167793 <a title="5-lda-17" href="./emnlp-2012-Improved_Parsing_and_POS_Tagging_Using_Inter-Sentence_Consistency_Constraints.html">64 emnlp-2012-Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints</a></p>
<p>18 0.51576447 <a title="5-lda-18" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>19 0.51455408 <a title="5-lda-19" href="./emnlp-2012-Exploiting_Chunk-level_Features_to_Improve_Phrase_Chunking.html">45 emnlp-2012-Exploiting Chunk-level Features to Improve Phrase Chunking</a></p>
<p>20 0.51352465 <a title="5-lda-20" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
