<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>83 emnlp-2012-Lexical Differences in Autobiographical Narratives from Schizophrenic Patients and Healthy Controls</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-83" href="#">emnlp2012-83</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>83 emnlp-2012-Lexical Differences in Autobiographical Narratives from Schizophrenic Patients and Healthy Controls</h1>
<br/><p>Source: <a title="emnlp-2012-83-pdf" href="http://aclweb.org/anthology//D/D12/D12-1004.pdf">pdf</a></p><p>Author: Kai Hong ; Christian G. Kohler ; Mary E. March ; Amber A. Parker ; Ani Nenkova</p><p>Abstract: We present a system for automatic identification of schizophrenic patients and healthy controls based on narratives the subjects recounted about emotional experiences in their own life. The focus of the study is to identify the lexical features that distinguish the two populations. We report the results of feature selection experiments that demonstrate that the classifier can achieve accuracy on patient level prediction as high as 76.9% with only a small set of features. We provide an in-depth discussion of the lexical features that distinguish the two groups and the unexpected relationship between emotion types of the narratives and the accuracy of patient status prediction.</p><p>Reference: <a title="emnlp-2012-83-reference" href="../emnlp2012_reference/emnlp-2012-Lexical_Differences_in_Autobiographical_Narratives_from_Schizophrenic_Patients_and_Healthy_Controls_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 duup  Abstract We present a system for automatic identification of schizophrenic patients and healthy controls based on narratives the subjects recounted about emotional experiences in their own life. [sent-15, score-1.275]
</p><p>2 We report the results of feature selection experiments that demonstrate that the classifier can achieve accuracy on patient level prediction as high as 76. [sent-17, score-0.319]
</p><p>3 We provide an in-depth discussion of the lexical features that distinguish the two groups and the unexpected relationship between emotion types of the narratives and the accuracy of patient status prediction. [sent-19, score-0.723]
</p><p>4 1 Introduction  Recent studies have shown that automatic language analysis can be successfully applied to detect cognitive impairment and language disorders. [sent-20, score-0.156]
</p><p>5 Our work further extends this line of investigation with analysis of the lexical differences between patients suffering from schizophrenia and healthy controls. [sent-21, score-1.013]
</p><p>6 Prior work has reported on characteristic language peculiarities exhibited by schizophrenia patients. [sent-22, score-0.304]
</p><p>7 There are more repetitions in speech of patients compared to controls (Manschreck et al. [sent-23, score-0.746]
</p><p>8 Deviations from normal language use in patients on different levels, including phonetics and syntax, have been documented (Covington et al. [sent-27, score-0.536]
</p><p>9 In this paper we introduce a dataset of autobiographical narratives told by schizophrenic patients and by healthy controls. [sent-29, score-1.109]
</p><p>10 The narratives are related to emotional personal experiences of the subjects for five basic emotions: ANGER, SAD,  HAPPY, DISGUST, FEAR. [sent-30, score-0.425]
</p><p>11 An automatic system for predicting patient status from autobiographical narratives can aid psychiatrists in tracking patients over time and can serve as an easy way to administer large scale screening. [sent-33, score-1.043]
</p><p>12 We study a range of lexical features including individual words, repetitions as well as classes of words defined in specialized dictionaries compiled by psychologists (Section 4). [sent-35, score-0.226]
</p><p>13 Through feature selection we are able to obtain a small set of 25 highly predictive features which lead to status classification accuracy significantly better than chance (Section 6. [sent-38, score-0.296]
</p><p>14 We also show that differences between patients and controls are revealed best in stories related to SAD and ANGRY  narratives, they are decent in HAPPY stories, and that distinctions are poor for DISGUST and FEAR (Section 6. [sent-40, score-1.0]
</p><p>15 In the view of therapy, Pennebaker discovered that writing emotional experiences can be helpful in therapeutic process (Pennebaker, 1997). [sent-49, score-0.211]
</p><p>16 It has been shown that features from language models (LMs) can be used to detect impairment in monolingual and bilingual children (Gabani et al. [sent-52, score-0.154]
</p><p>17 Similarly, studies on child language development and autism have shown that n-gram cross-entropy from LMs representative of healthy and impaired subjects is a highly significant feature predictive of language impairment (Prud’hommeaux et al. [sent-55, score-0.398]
</p><p>18 Speech-related features and interactional aspects of dialog behavior such as pauses, fillers, etc, have also been found helpful in identifying autistic patients (Heeman et al. [sent-65, score-0.612]
</p><p>19 Similarly to prior work, we present the most significant features related to differences between schizophrenic patients and healthy controls. [sent-80, score-0.803]
</p><p>20 Unlike prior work, instead of doing class ablation studies we perform feature selection from the full set of available features and identify a small set of highly predictive features which are sufficient to achieve the top performance we report. [sent-81, score-0.258]
</p><p>21 Such targeted analysis is more helpful for medical professionals as they search to develop new therapies and ways to track patient status between visits. [sent-82, score-0.235]
</p><p>22 3  Data  For our experiments we collected autobiographical narratives from 39 speakers. [sent-83, score-0.272]
</p><p>23 The speakers are asked to tell their experience involving the following emotions: HAPPY, ANGER, SAD, FEAR and DISGUST, which comprise the set of the five basic  emotions (Cowie, 2000). [sent-84, score-0.151]
</p><p>24 Most subjects told a single story for each of the emotions, some told two. [sent-85, score-0.375]
</p><p>25 The total number of stories in the dataset is 201. [sent-86, score-0.258]
</p><p>26 The recordings of the narratives were manually transcribed in plain text format. [sent-88, score-0.192]
</p><p>27 We show age and length in words of the told stories for the two groups in Table 1. [sent-89, score-0.428]
</p><p>28 There are 23 patients with schizophrenia and 16 healthy controls, telling 120 and 81 stories respectively. [sent-90, score-1.224]
</p><p>29 4  Features  Here we introduce the large set of lexical features that we group in three classes: a large class of features computed for individual lexical items, basic features, features derived on the basis of pre-existing dictionaries and language model features. [sent-91, score-0.271]
</p><p>30 We also detail the way we performed feature normalization and feature selection. [sent-92, score-0.156]
</p><p>31 Of particular interest we track the use of pronouns because early research has reported that people with cognitive impairment have a tendency to use subjective words or referring to themselves (Rude et al. [sent-111, score-0.156]
</p><p>32 Thus in the experiments we report later we train one model for patients and one for controls and use the perplexity of a given text according to the bigram language models on word and POS as features in prediction. [sent-121, score-0.781]
</p><p>33 com Because of the elaborate development of dictionaries and categories, LIWC has been used for predicting emotional and cognitive problems from subject’s spoken and written samples. [sent-138, score-0.201]
</p><p>34 Representative applications include studying attention focus through personal pronouns, studying honesty and deception by emotion words and exclusive words and identifying thinking styles (Tausczik and Pennebaker, 2010). [sent-139, score-0.174]
</p><p>35 Thus it is reasonable to expect that LIWC derived features would be helpful in identifying schizophrenia patients. [sent-140, score-0.348]
</p><p>36 4 we discuss in more detail the features which turned out to be significantly different between patients and controls within LIWC. [sent-142, score-0.715]
</p><p>37 3 Feature normalization We use two feature normalization approaches: projection normalization and binary normalization. [sent-160, score-0.228]
</p><p>38 Thus for each feature j, we have: averagej = ∑in=1 vij minj = mini{vij}, maxj = maxi{vij}. [sent-165, score-0.187]
</p><p>39 Then we could have pij =  n1  mvaijx−j−mminijnj,  where  pij  is the feature value after  norm−almiziantion. [sent-169, score-0.158]
</p><p>40 2 Binary normalization Here all features are converted to binary values, reflecting whether the value falls below or above the average value for that feature observed in training. [sent-172, score-0.152]
</p><p>41 The value pij of j-th feature for the i-th instance is as below:  pij={ 10 voitjhe < 0, we change pj into 0; if pj > 1, we change pj into 1. [sent-173, score-0.187]
</p><p>42 In the medical domain this problem is even more acute because collecting patient data is difficult. [sent-177, score-0.153]
</p><p>43 Therefore, in our classification procedure, we perform feature selection by doing two-sided T-test to compare the values of features in the patient and control groups. [sent-179, score-0.399]
</p><p>44 Note however that we don’t use the features selected on the full dataset for machine learning experiments because when T-tests are applied on the full dataset feature selection decisions would include information about the test set as well. [sent-184, score-0.175]
</p><p>45 We also explore alternative feature ranking and feature selection procedures in Section 6. [sent-193, score-0.179]
</p><p>46 41 5  Our approach  The goal of our system is to classify the person who told a story in one of two categories: Schizophrenia group (SC) and Control group (CO). [sent-197, score-0.267]
</p><p>47 In order to do this, we give labels to the stories told by each subject. [sent-198, score-0.38]
</p><p>48 Therefore we could use our model to identify the status of the person who told each individual story, the task is to answer the question “Was the subject who told this story a patient or control? [sent-199, score-0.613]
</p><p>49 Then we combine the predictions for stories to predict status of each subject, and the task becomes answering the question “Is this subject a patient or control given that they told these five stories? [sent-201, score-0.743]
</p><p>50 Thus in story level prediction we use no information about the fact that subjects told more than one story, while in subject-level prediction we do use this information. [sent-203, score-0.323]
</p><p>51 Patients with speaking disorder or cognitive impairment express themselves in atypical ways. [sent-211, score-0.188]
</p><p>52 We expect that the approach would be useful for the study of schizophrenia as well and so start with a description of the LM experiments. [sent-213, score-0.304]
</p><p>53 We use LMs on words to recognize the difference between patients and controls in vocabulary use. [sent-214, score-0.671]
</p><p>54 Two separate LMs are trained on transcripts of schizophrenia and controls respectively, using leave-one-subject-out protocol. [sent-216, score-0.476]
</p><p>55 r61o6-F Table 2: Language model performance Here t means a transcript from a subject, while PERSC and PERCO are perplexities for patients and controls, respectively. [sent-225, score-0.573]
</p><p>56 Moreover, we would like to analyze more specific differences between the patient and control group and this would be more appropriately done using a larger set of features. [sent-231, score-0.302]
</p><p>57 We have described our features and feature selection process in Section 4. [sent-232, score-0.175]
</p><p>58 The most intuitive way to obtain a subject-level prediction is by voting from story-level predictions between the stories told by the particular subject. [sent-237, score-0.415]
</p><p>59 On the few occasions where there are equal votes for schizophrenia and control, the system makes a preference towards schizophrenia, because it is more 42  P-va0lu . [sent-239, score-0.304]
</p><p>60 94176ject#F41e3 56a428t091ures  Table 3: Performance by subject after T-test feature selection in different confidence levels. [sent-243, score-0.188]
</p><p>61 We report precision, recall and F-measure for both patient and control groups, as well as overall accuracy and Macro-F value. [sent-247, score-0.224]
</p><p>62 Narrowing the feature set as much as possible will be most useful for clinicians as they understand the differences between the groups and look for indicators of the illness they need to track during regular patient visits. [sent-280, score-0.293]
</p><p>63 ‘+’ and ‘-’ means more prevalent for patient and control, while  ‘prj’ and ‘01’ correspond to the two normalization approaches in Section 4. [sent-297, score-0.213]
</p><p>64 4 Analysis of Significant Features In this section we discuss the specific features that were revealed as most predictive by the feature selection methods that we employed. [sent-300, score-0.241]
</p><p>65 4 We group the significant features according to the feature classes we introduced in 4LM1 is defined as the ratio of CO perplexity and SC perplexity from LMs, LM7 comes from projection normalization of LM1. [sent-305, score-0.318]
</p><p>66 This finding conforms with prior research that patients with mental disorders refer to themselves more often than regular people. [sent-311, score-0.536]
</p><p>67 In terms of words, patients talked more about  money, trouble, and used adverbs like moderately and basically. [sent-315, score-0.584]
</p><p>68 Repetition in language is also a revealing characteristic of the patient narratives. [sent-316, score-0.153]
</p><p>69 There is a substantial difference in the appearance of repetitions between the two groups, as well as repetition of specific words: I, and, and repetition of filled pauses um. [sent-317, score-0.306]
</p><p>70 As patients focus more on their own feelings, they talked a lot about their family, using words such as son, grandfather and even dogs. [sent-318, score-0.584]
</p><p>71 The schizophrenia group scores higher in the Self, Cognition, Past, Insistence and Satisfaction categories. [sent-320, score-0.338]
</p><p>72 This indicates that they are more likely to talk about past experience, using cognitive terms and having a repetition of key 44 terms. [sent-321, score-0.181]
</p><p>73 We were particularly curious to understand why patients score higher on Satisfaction ratings. [sent-322, score-0.536]
</p><p>74 On closer inspection we discovered that patients’ stories were rated higher in Satisfaction when they were telling SAD stories. [sent-323, score-0.285]
</p><p>75 This finding has important clinical implications because one of the  diagnostic elements for the disease is inappropriate emotion expression. [sent-324, score-0.174]
</p><p>76 Prompted by this discovery, we take a closer look at the interaction between the emotion expressed in a story and the accuracy of status prediction in the next section. [sent-326, score-0.368]
</p><p>77 Accuracy per emotion with three feature selection methods is shown in Table 6. [sent-334, score-0.305]
</p><p>78 When using signal-to-noise, we can see that on SAD stories the two groups can be distinguished better. [sent-335, score-0.306]
</p><p>79 5%, and that the accuracy on HAPPY stories is the next highest one. [sent-337, score-0.258]
</p><p>80 05 p-value  cut-off to select significant features, ANGER stories become the ones for which the status of a subject  AcDHuAFSirnsae gap cadpuyer s(t%)s26 7 6n01326(. [sent-339, score-0.397]
</p><p>81 047 Table 6: Accuracy per emotion by different feature-sets can be predicted most accurately. [sent-347, score-0.174]
</p><p>82 The changes in the recognition accuracy depending on feature selection suggests that in future studies it may be more beneficial to perform feature selection only on stories from a given type because obviously indicative features exist at least for the SAD, ANGER and HAPPY stories. [sent-351, score-0.597]
</p><p>83 Regardless of the feature selection approach, it  is more difficult to tell the two groups apart when they tell DISGUST and FEAR stories. [sent-352, score-0.179]
</p><p>84 These results seem to indicate that when talking about certain emotions patients and controls look much more alike than when other emotions are concerned. [sent-353, score-0.96]
</p><p>85 Future data acquisition efforts can focus only on collecting autobiographical narratives relevant to the emotions for which patients and controls differ most. [sent-354, score-1.067]
</p><p>86 Figure 2: Number of significant features by P-value selection on different thresholds (per emotion)  In future work we would like to use only stories from a given emotion to classify between patients 45  Table 7: Significant features (p-value ≤ 0. [sent-355, score-1.139]
</p><p>87 Therefore, we use our data to identify significant features that distinguish patients from controls only on narratives from a particular emotion. [sent-358, score-0.907]
</p><p>88 For example, we compare the differences of SAD stories told by patients and controls. [sent-359, score-0.96]
</p><p>89 We count the number of significant features between patients and controls with 11 different p-value cut-offs, and provide a plot that visualizes the results in Figure 2. [sent-360, score-0.715]
</p><p>90 The feature analysis performed by emotion reveals more differences between patients and controls, beyond common features such as self, I, etc. [sent-364, score-0.846]
</p><p>91 For HAPPY stories, patients talk more about their friends and relatives; they also have a higher tendency of being ambivalent. [sent-365, score-0.569]
</p><p>92 For DISGUST stories, patients are more disgusted with dogs, and they talk more about health. [sent-366, score-0.569]
</p><p>93 ANGER is one of the emotions that best reveals the differences between groups, and schizophrenia patients show more aggression and cognition while talking, according to features derived from Diction. [sent-368, score-1.082]
</p><p>94 In FEAR stories patients talk about money more often than controls. [sent-370, score-0.827]
</p><p>95 When talking about sad experiences, patients sometimes show satisfaction and insistence, while the controls talked more about working experiences. [sent-373, score-0.954]
</p><p>96 7  Conclusion  In this paper, we analyzed the predictive power of different kinds of features for distinguishing schizophrenia patients from healthy controls. [sent-374, score-1.022]
</p><p>97 We provided an in-depth analysis of features that distinguish patients from controls and showed that the type of emotion conveyed by the personal narratives is important for the distinction and that stories for different emotions give different sets indicators for subject status. [sent-375, score-1.52]
</p><p>98 We are currently collecting and transcribing additional stories from the two groups which we would like to use as a definitive test set to verify the stability of our findings. [sent-381, score-0.306]
</p><p>99 We plan to explore syntactic and coherence models to  analyze the stories, as well as emotion analysis of the narratives. [sent-382, score-0.174]
</p><p>100 A corpus-based approach for the prediction of language impairment in monolingual english and spanish-english bilingual children. [sent-400, score-0.145]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('patients', 0.536), ('schizophrenia', 0.304), ('stories', 0.258), ('narratives', 0.192), ('emotion', 0.174), ('liwc', 0.165), ('patient', 0.153), ('happy', 0.137), ('sad', 0.137), ('controls', 0.135), ('diction', 0.128), ('emotions', 0.124), ('told', 0.122), ('impairment', 0.11), ('anger', 0.11), ('emotional', 0.11), ('repetition', 0.102), ('healthy', 0.099), ('lms', 0.097), ('pennebaker', 0.087), ('selection', 0.083), ('disgust', 0.082), ('status', 0.082), ('autobiographical', 0.08), ('schizophrenic', 0.08), ('solorio', 0.08), ('story', 0.077), ('repetitions', 0.075), ('personality', 0.075), ('control', 0.071), ('experiences', 0.069), ('perplexity', 0.066), ('gabani', 0.064), ('insistence', 0.064), ('normalization', 0.06), ('satisfaction', 0.057), ('subject', 0.057), ('certainty', 0.055), ('guyon', 0.055), ('gill', 0.055), ('pij', 0.055), ('subjects', 0.054), ('fear', 0.05), ('self', 0.05), ('groups', 0.048), ('autism', 0.048), ('hommeaux', 0.048), ('prud', 0.048), ('rude', 0.048), ('talked', 0.048), ('tausczik', 0.048), ('vij', 0.048), ('feature', 0.048), ('cognitive', 0.046), ('dictionaries', 0.045), ('features', 0.044), ('differences', 0.044), ('realism', 0.041), ('talking', 0.041), ('lm', 0.04), ('predictive', 0.039), ('transcript', 0.037), ('transcripts', 0.037), ('prediction', 0.035), ('dominance', 0.034), ('elizabeth', 0.034), ('group', 0.034), ('talk', 0.033), ('indicative', 0.033), ('alastair', 0.032), ('atypical', 0.032), ('averagej', 0.032), ('awareness', 0.032), ('bedore', 0.032), ('clop', 0.032), ('interactional', 0.032), ('lois', 0.032), ('loso', 0.032), ('manschreck', 0.032), ('melissa', 0.032), ('minj', 0.032), ('perco', 0.032), ('psychologists', 0.032), ('sherman', 0.032), ('thamar', 0.032), ('therapeutic', 0.032), ('lexical', 0.03), ('cognition', 0.03), ('pj', 0.028), ('fillers', 0.027), ('covington', 0.027), ('psychometric', 0.027), ('cried', 0.027), ('familiarity', 0.027), ('heeman', 0.027), ('maxj', 0.027), ('pauses', 0.027), ('telling', 0.027), ('revealed', 0.027), ('experience', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="83-tfidf-1" href="./emnlp-2012-Lexical_Differences_in_Autobiographical_Narratives_from_Schizophrenic_Patients_and_Healthy_Controls.html">83 emnlp-2012-Lexical Differences in Autobiographical Narratives from Schizophrenic Patients and Healthy Controls</a></p>
<p>Author: Kai Hong ; Christian G. Kohler ; Mary E. March ; Amber A. Parker ; Ani Nenkova</p><p>Abstract: We present a system for automatic identification of schizophrenic patients and healthy controls based on narratives the subjects recounted about emotional experiences in their own life. The focus of the study is to identify the lexical features that distinguish the two populations. We report the results of feature selection experiments that demonstrate that the classifier can achieve accuracy on patient level prediction as high as 76.9% with only a small set of features. We provide an in-depth discussion of the lexical features that distinguish the two groups and the unexpected relationship between emotion types of the narratives and the accuracy of patient status prediction.</p><p>2 0.14656518 <a title="83-tfidf-2" href="./emnlp-2012-Lyrics%2C_Music%2C_and_Emotions.html">87 emnlp-2012-Lyrics, Music, and Emotions</a></p>
<p>Author: Rada Mihalcea ; Carlo Strapparava</p><p>Abstract: In this paper, we explore the classification of emotions in songs, using the music and the lyrics representation of the songs. We introduce a novel corpus of music and lyrics, consisting of 100 songs annotated for emotions. We show that textual and musical features can both be successfully used for emotion recognition in songs. Moreover, through comparative experiments, we show that the joint use of lyrics and music brings significant improvements over each of the individual textual and musical classifiers, with error rate reductions of up to 31%.</p><p>3 0.052290775 <a title="83-tfidf-3" href="./emnlp-2012-Translation_Model_Based_Cross-Lingual_Language_Model_Adaptation%3A_from_Word_Models_to_Phrase_Models.html">128 emnlp-2012-Translation Model Based Cross-Lingual Language Model Adaptation: from Word Models to Phrase Models</a></p>
<p>Author: Shixiang Lu ; Wei Wei ; Xiaoyin Fu ; Bo Xu</p><p>Abstract: In this paper, we propose a novel translation model (TM) based cross-lingual data selection model for language model (LM) adaptation in statistical machine translation (SMT), from word models to phrase models. Given a source sentence in the translation task, this model directly estimates the probability that a sentence in the target LM training corpus is similar. Compared with the traditional approaches which utilize the first pass translation hypotheses, cross-lingual data selection model avoids the problem of noisy proliferation. Furthermore, phrase TM based cross-lingual data selection model is more effective than the traditional approaches based on bag-ofwords models and word-based TM, because it captures contextual information in modeling the selection of phrase as a whole. Experiments conducted on large-scale data sets demonstrate that our approach significantly outperforms the state-of-the-art approaches on both LM perplexity and SMT performance.</p><p>4 0.049512617 <a title="83-tfidf-4" href="./emnlp-2012-Assessment_of_ESL_Learners%27_Syntactic_Competence_Based_on_Similarity_Measures.html">21 emnlp-2012-Assessment of ESL Learners' Syntactic Competence Based on Similarity Measures</a></p>
<p>Author: Su-Youn Yoon ; Suma Bhat</p><p>Abstract: This study presents a novel method that measures English language learners’ syntactic competence towards improving automated speech scoring systems. In contrast to most previous studies which focus on the length of production units such as the mean length of clauses, we focused on capturing the differences in the distribution of morpho-syntactic features or grammatical expressions across proficiency. We estimated the syntactic competence through the use of corpus-based NLP techniques. Assuming that the range and so- phistication of grammatical expressions can be captured by the distribution of Part-ofSpeech (POS) tags, vector space models of POS tags were constructed. We use a large corpus of English learners’ responses that are classified into four proficiency levels by human raters. Our proposed feature measures the similarity of a given response with the most proficient group and is then estimates the learner’s syntactic competence level. Widely outperforming the state-of-the-art measures of syntactic complexity, our method attained a significant correlation with humanrated scores. The correlation between humanrated scores and features based on manual transcription was 0.43 and the same based on ASR-hypothesis was slightly lower, 0.42. An important advantage of our method is its robustness against speech recognition errors not to mention the simplicity of feature generation that captures a reasonable set of learnerspecific syntactic errors. 600 Measures Suma Bhat Beckman Institute, Urbana, IL 61801 . spbhat 2 @ i l l ino i edu s</p><p>5 0.039823208 <a title="83-tfidf-5" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>Author: Annie Louis ; Ani Nenkova</p><p>Abstract: We introduce a model of coherence which captures the intentional discourse structure in text. Our work is based on the hypothesis that syntax provides a proxy for the communicative goal of a sentence and therefore the sequence of sentences in a coherent discourse should exhibit detectable structural patterns. Results show that our method has high discriminating power for separating out coherent and incoherent news articles reaching accuracies of up to 90%. We also show that our syntactic patterns are correlated with manual annotations of intentional structure for academic conference articles and can successfully predict the coherence of abstract, introduction and related work sections of these articles. 59.3 (100.0) Intro 50.3 (100.0) 1166 Rel wk 55.4 (100.0) >= 0.663.8 (67.2)50.8 (71.1)58.6 (75.9) >= 0.7 67.2 (32.0) 54.4 (38.6) 63.3 (52.8) >= 0.8 74.0 (10.0) 51.6 (22.0) 63.0 (25.7) >= 0.9 91.7 (2.0) 30.6 (5.0) 68.1 (7.2) Table 9: Accuracy (% examples) above each confidence level for the conference versus workshop task. These results are shown in Table 9. The proportion of examples under each setting is also indicated. When only examples above 0.6 confidence are examined, the classifier has a higher accuracy of63.8% for abstracts and covers close to 70% of the examples. Similarly, when a cutoff of 0.7 is applied to the confidence for predicting related work sections, we achieve 63.3% accuracy for 53% of examples. So we can consider that 30 to 47% of the examples in the two sections respectively are harder to tell apart. Interestingly however even high confidence predictions on introductions remain incorrect. These results show that our model can successfully distinguish the structure of articles beyond just clearly incoherent permutation examples. 7 Conclusion Our work is the first to develop an unsupervised model for intentional structure and to show that it has good accuracy for coherence prediction and also complements entity and lexical structure of discourse. This result raises interesting questions about how patterns captured by these different coherence metrics vary and how they can be combined usefully for predicting coherence. We plan to explore these ideas in future work. We also want to analyze genre differences to understand if the strength of these coherence dimensions varies with genre. Acknowledgements This work is partially supported by a Google research grant and NSF CAREER 0953445 award. References Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Computa- tional Linguistics, 34(1): 1–34. Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In Proceedings of NAACL-HLT, pages 113–120. Xavier Carreras, Michael Collins, and Terry Koo. 2008. Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing. In Proceedings of CoNLL, pages 9–16. Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proceedings of ACL, pages 173–180. Jackie C.K. Cheung and Gerald Penn. 2010. Utilizing extra-sentential context for parsing. In Proceedings of EMNLP, pages 23–33. Christelle Cocco, Rapha ¨el Pittier, Fran ¸cois Bavaud, and Aris Xanthos. 2011. Segmentation and clustering of textual sequences: a typological approach. In Proceedings of RANLP, pages 427–433. Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 3 1:25–70. Isaac G. Councill, C. Lee Giles, and Min-Yen Kan. 2008. Parscit: An open-source crf reference string parsing package. In Proceedings of LREC, pages 661–667. Micha Elsner and Eugene Charniak. 2008. Coreferenceinspired coherence modeling. In Proceedings of ACLHLT, Short Papers, pages 41–44. Micha Elsner and Eugene Charniak. 2011. Extending the entity grid with entity-specific features. In Proceedings of ACL-HLT, pages 125–129. Micha Elsner, Joseph Austerweil, and Eugene Charniak. 2007. A unified local and global model for discourse coherence. In Proceedings of NAACL-HLT, pages 436–443. Pascale Fung and Grace Ngai. 2006. One story, one flow: Hidden markov story models for multilingual multidocument summarization. ACM Transactions on Speech and Language Processing, 3(2): 1–16. Barbara J. Grosz and Candace L. Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 3(12): 175–204. Yufan Guo, Anna Korhonen, and Thierry Poibeau. 2011. A weakly-supervised approach to argumentative zoning of scientific documents. In Proceedings of EMNLP, pages 273–283. Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of ACL-HLT, pages 586–594, June. 1167 Nikiforos Karamanis, Chris Mellish, Massimo Poesio, and Jon Oberlander. 2009. Evaluating centering for information ordering using corpora. Computational Linguistics, 35(1):29–46. Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL, pages 423–430. Mirella Lapata and Regina Barzilay. 2005. Automatic evaluation of text coherence: Models and representations. In Proceedings of IJCAI. Mirella Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings of ACL, pages 545–552. Maria Liakata and Larisa Soldatova. 2008. Guidelines for the annotation of general scientific concepts. JISC Project Report. Maria Liakata, Simone Teufel, Advaith Siddharthan, and Colin Batchelor. 2010. Corpora for the conceptualisation and zoning of scientific papers. In Proceedings of LREC. Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing implicit discourse relations in the Penn Discourse Treebank. In Proceedings of EMNLP, pages 343–351. Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically evaluating text coherence using discourse relations. In Proceedings of ACL-HLT, pages 997– 1006. Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330. Emily Pitler and Ani Nenkova. 2008. Revisiting readability: A unified framework for predicting text quality. In Proceedings of EMNLP, pages 186–195. Dragomir R. Radev, Mark Thomas Joseph, Bryan Gibson, and Pradeep Muthukrishnan. 2009. A Bibliometric and Network Analysis ofthe field of Computational Linguistics. Journal of the American Society for Information Science and Technology. David Reitter, Johanna D. Moore, and Frank Keller. 2006. Priming of Syntactic Rules in Task-Oriented Dialogue and Spontaneous Conversation. In Proceedings of the 28th Annual Conference of the Cognitive Science Society, pages 685–690. Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In Proceedings of the fifth conference on Applied natural language processing, pages 16–19. Radu Soricut and Daniel Marcu. 2006. Discourse generation using utility-trained coherence models. In Proceedings of COLING-ACL, pages 803–810. John Swales. 1990. Genre analysis: English in academic and research settings, volume 11. Cambridge University Press. Simone Teufel and Marc Moens. 2000. What’s yours and what’s mine: determining intellectual attribution in scientific text. In Proceedings of EMNLP, pages 9– 17. Simone Teufel, Jean Carletta, and Marc Moens. 1999. An annotation scheme for discourse-level argumentation in research articles. In Proceedings of EACL, pages 110–1 17. Ying Zhao, George Karypis, and Usama Fayyad. 2005. Hierarchical clustering algorithms for document datasets. Data Mining and Knowledge Discovery, 10: 141–168. 1168</p><p>6 0.037698146 <a title="83-tfidf-6" href="./emnlp-2012-Resolving_This-issue_Anaphora.html">113 emnlp-2012-Resolving This-issue Anaphora</a></p>
<p>7 0.037445117 <a title="83-tfidf-7" href="./emnlp-2012-Concurrent_Acquisition_of_Word_Meaning_and_Lexical_Categories.html">29 emnlp-2012-Concurrent Acquisition of Word Meaning and Lexical Categories</a></p>
<p>8 0.035257928 <a title="83-tfidf-8" href="./emnlp-2012-Semantic_Compositionality_through_Recursive_Matrix-Vector_Spaces.html">116 emnlp-2012-Semantic Compositionality through Recursive Matrix-Vector Spaces</a></p>
<p>9 0.034595992 <a title="83-tfidf-9" href="./emnlp-2012-Learning_Verb_Inference_Rules_from_Linguistically-Motivated_Evidence.html">80 emnlp-2012-Learning Verb Inference Rules from Linguistically-Motivated Evidence</a></p>
<p>10 0.032082986 <a title="83-tfidf-10" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>11 0.031781998 <a title="83-tfidf-11" href="./emnlp-2012-An_%22AI_readability%22_Formula_for_French_as_a_Foreign_Language.html">17 emnlp-2012-An "AI readability" Formula for French as a Foreign Language</a></p>
<p>12 0.031577475 <a title="83-tfidf-12" href="./emnlp-2012-Multiple_Aspect_Summarization_Using_Integer_Linear_Programming.html">94 emnlp-2012-Multiple Aspect Summarization Using Integer Linear Programming</a></p>
<p>13 0.031545762 <a title="83-tfidf-13" href="./emnlp-2012-Mixed_Membership_Markov_Models_for_Unsupervised_Conversation_Modeling.html">89 emnlp-2012-Mixed Membership Markov Models for Unsupervised Conversation Modeling</a></p>
<p>14 0.031114526 <a title="83-tfidf-14" href="./emnlp-2012-Active_Learning_for_Imbalanced_Sentiment_Classification.html">15 emnlp-2012-Active Learning for Imbalanced Sentiment Classification</a></p>
<p>15 0.030474529 <a title="83-tfidf-15" href="./emnlp-2012-A_Statistical_Relational_Learning_Approach_to_Identifying_Evidence_Based_Medicine_Categories.html">10 emnlp-2012-A Statistical Relational Learning Approach to Identifying Evidence Based Medicine Categories</a></p>
<p>16 0.029429693 <a title="83-tfidf-16" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>17 0.029125746 <a title="83-tfidf-17" href="./emnlp-2012-A_Sequence_Labelling_Approach_to_Quote_Attribution.html">9 emnlp-2012-A Sequence Labelling Approach to Quote Attribution</a></p>
<p>18 0.028456993 <a title="83-tfidf-18" href="./emnlp-2012-Revisiting_the_Predictability_of_Language%3A_Response_Completion_in_Social_Media.html">114 emnlp-2012-Revisiting the Predictability of Language: Response Completion in Social Media</a></p>
<p>19 0.027497126 <a title="83-tfidf-19" href="./emnlp-2012-User_Demographics_and_Language_in_an_Implicit_Social_Network.html">134 emnlp-2012-User Demographics and Language in an Implicit Social Network</a></p>
<p>20 0.026975829 <a title="83-tfidf-20" href="./emnlp-2012-Generative_Goal-Driven_User_Simulation_for_Dialog_Management.html">60 emnlp-2012-Generative Goal-Driven User Simulation for Dialog Management</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.11), (1, 0.012), (2, 0.017), (3, 0.047), (4, -0.0), (5, 0.029), (6, 0.007), (7, -0.022), (8, 0.035), (9, -0.015), (10, 0.006), (11, -0.034), (12, -0.138), (13, 0.036), (14, -0.023), (15, -0.1), (16, -0.031), (17, -0.05), (18, -0.031), (19, 0.055), (20, -0.038), (21, -0.033), (22, 0.091), (23, 0.015), (24, -0.037), (25, 0.129), (26, 0.303), (27, -0.16), (28, -0.491), (29, 0.096), (30, -0.03), (31, 0.148), (32, -0.141), (33, -0.125), (34, 0.043), (35, 0.056), (36, 0.084), (37, 0.012), (38, 0.062), (39, -0.077), (40, -0.058), (41, -0.04), (42, -0.047), (43, 0.032), (44, -0.073), (45, 0.01), (46, 0.078), (47, -0.008), (48, -0.059), (49, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93505633 <a title="83-lsi-1" href="./emnlp-2012-Lexical_Differences_in_Autobiographical_Narratives_from_Schizophrenic_Patients_and_Healthy_Controls.html">83 emnlp-2012-Lexical Differences in Autobiographical Narratives from Schizophrenic Patients and Healthy Controls</a></p>
<p>Author: Kai Hong ; Christian G. Kohler ; Mary E. March ; Amber A. Parker ; Ani Nenkova</p><p>Abstract: We present a system for automatic identification of schizophrenic patients and healthy controls based on narratives the subjects recounted about emotional experiences in their own life. The focus of the study is to identify the lexical features that distinguish the two populations. We report the results of feature selection experiments that demonstrate that the classifier can achieve accuracy on patient level prediction as high as 76.9% with only a small set of features. We provide an in-depth discussion of the lexical features that distinguish the two groups and the unexpected relationship between emotion types of the narratives and the accuracy of patient status prediction.</p><p>2 0.92110568 <a title="83-lsi-2" href="./emnlp-2012-Lyrics%2C_Music%2C_and_Emotions.html">87 emnlp-2012-Lyrics, Music, and Emotions</a></p>
<p>Author: Rada Mihalcea ; Carlo Strapparava</p><p>Abstract: In this paper, we explore the classification of emotions in songs, using the music and the lyrics representation of the songs. We introduce a novel corpus of music and lyrics, consisting of 100 songs annotated for emotions. We show that textual and musical features can both be successfully used for emotion recognition in songs. Moreover, through comparative experiments, we show that the joint use of lyrics and music brings significant improvements over each of the individual textual and musical classifiers, with error rate reductions of up to 31%.</p><p>3 0.22311956 <a title="83-lsi-3" href="./emnlp-2012-Translation_Model_Based_Cross-Lingual_Language_Model_Adaptation%3A_from_Word_Models_to_Phrase_Models.html">128 emnlp-2012-Translation Model Based Cross-Lingual Language Model Adaptation: from Word Models to Phrase Models</a></p>
<p>Author: Shixiang Lu ; Wei Wei ; Xiaoyin Fu ; Bo Xu</p><p>Abstract: In this paper, we propose a novel translation model (TM) based cross-lingual data selection model for language model (LM) adaptation in statistical machine translation (SMT), from word models to phrase models. Given a source sentence in the translation task, this model directly estimates the probability that a sentence in the target LM training corpus is similar. Compared with the traditional approaches which utilize the first pass translation hypotheses, cross-lingual data selection model avoids the problem of noisy proliferation. Furthermore, phrase TM based cross-lingual data selection model is more effective than the traditional approaches based on bag-ofwords models and word-based TM, because it captures contextual information in modeling the selection of phrase as a whole. Experiments conducted on large-scale data sets demonstrate that our approach significantly outperforms the state-of-the-art approaches on both LM perplexity and SMT performance.</p><p>4 0.21810029 <a title="83-lsi-4" href="./emnlp-2012-An_%22AI_readability%22_Formula_for_French_as_a_Foreign_Language.html">17 emnlp-2012-An "AI readability" Formula for French as a Foreign Language</a></p>
<p>Author: Thomas Francois ; Cedrick Fairon</p><p>Abstract: This paper present a new readability formula for French as a foreign language (FFL), which relies on 46 textual features representative of the lexical, syntactic, and semantic levels as well as some of the specificities of the FFL context. We report comparisons between several techniques for feature selection and various learning algorithms. Our best model, based on support vector machines (SVM), significantly outperforms previous FFL formulas. We also found that semantic features behave poorly in our case, in contrast with some previous readability studies on English as a first language.</p><p>5 0.21409893 <a title="83-lsi-5" href="./emnlp-2012-Active_Learning_for_Imbalanced_Sentiment_Classification.html">15 emnlp-2012-Active Learning for Imbalanced Sentiment Classification</a></p>
<p>Author: Shoushan Li ; Shengfeng Ju ; Guodong Zhou ; Xiaojun Li</p><p>Abstract: Active learning is a promising way for sentiment classification to reduce the annotation cost. In this paper, we focus on the imbalanced class distribution scenario for sentiment classification, wherein the number of positive samples is quite different from that of negative samples. This scenario posits new challenges to active learning. To address these challenges, we propose a novel active learning approach, named co-selecting, by taking both the imbalanced class distribution issue and uncertainty into account. Specifically, our co-selecting approach employs two feature subspace classifiers to collectively select most informative minority-class samples for manual annotation by leveraging a certainty measurement and an uncertainty measurement, and in the meanwhile, automatically label most informative majority-class samples, to reduce humanannotation efforts. Extensive experiments across four domains demonstrate great potential and effectiveness of our proposed co-selecting approach to active learning for imbalanced sentiment classification. 1</p><p>6 0.20871221 <a title="83-lsi-6" href="./emnlp-2012-A_Statistical_Relational_Learning_Approach_to_Identifying_Evidence_Based_Medicine_Categories.html">10 emnlp-2012-A Statistical Relational Learning Approach to Identifying Evidence Based Medicine Categories</a></p>
<p>7 0.19696495 <a title="83-lsi-7" href="./emnlp-2012-Assessment_of_ESL_Learners%27_Syntactic_Competence_Based_on_Similarity_Measures.html">21 emnlp-2012-Assessment of ESL Learners' Syntactic Competence Based on Similarity Measures</a></p>
<p>8 0.19220594 <a title="83-lsi-8" href="./emnlp-2012-Concurrent_Acquisition_of_Word_Meaning_and_Lexical_Categories.html">29 emnlp-2012-Concurrent Acquisition of Word Meaning and Lexical Categories</a></p>
<p>9 0.16000572 <a title="83-lsi-9" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>10 0.15768227 <a title="83-lsi-10" href="./emnlp-2012-A_Sequence_Labelling_Approach_to_Quote_Attribution.html">9 emnlp-2012-A Sequence Labelling Approach to Quote Attribution</a></p>
<p>11 0.15381089 <a title="83-lsi-11" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>12 0.15349466 <a title="83-lsi-12" href="./emnlp-2012-Universal_Grapheme-to-Phoneme_Prediction_Over_Latin_Alphabets.html">132 emnlp-2012-Universal Grapheme-to-Phoneme Prediction Over Latin Alphabets</a></p>
<p>13 0.145208 <a title="83-lsi-13" href="./emnlp-2012-Resolving_This-issue_Anaphora.html">113 emnlp-2012-Resolving This-issue Anaphora</a></p>
<p>14 0.13629836 <a title="83-lsi-14" href="./emnlp-2012-Revisiting_the_Predictability_of_Language%3A_Response_Completion_in_Social_Media.html">114 emnlp-2012-Revisiting the Predictability of Language: Response Completion in Social Media</a></p>
<p>15 0.13138098 <a title="83-lsi-15" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>16 0.13092583 <a title="83-lsi-16" href="./emnlp-2012-Learning_Syntactic_Categories_Using_Paradigmatic_Representations_of_Word_Context.html">79 emnlp-2012-Learning Syntactic Categories Using Paradigmatic Representations of Word Context</a></p>
<p>17 0.12846008 <a title="83-lsi-17" href="./emnlp-2012-A_Novel_Discriminative_Framework_for_Sentence-Level_Discourse_Analysis.html">7 emnlp-2012-A Novel Discriminative Framework for Sentence-Level Discourse Analysis</a></p>
<p>18 0.12804672 <a title="83-lsi-18" href="./emnlp-2012-Characterizing_Stylistic_Elements_in_Syntactic_Structure.html">27 emnlp-2012-Characterizing Stylistic Elements in Syntactic Structure</a></p>
<p>19 0.12442339 <a title="83-lsi-19" href="./emnlp-2012-Minimal_Dependency_Length_in_Realization_Ranking.html">88 emnlp-2012-Minimal Dependency Length in Realization Ranking</a></p>
<p>20 0.12426497 <a title="83-lsi-20" href="./emnlp-2012-Do_Neighbours_Help%3F_An_Exploration_of_Graph-based_Algorithms_for_Cross-domain_Sentiment_Classification.html">34 emnlp-2012-Do Neighbours Help? An Exploration of Graph-based Algorithms for Cross-domain Sentiment Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.01), (16, 0.024), (34, 0.039), (45, 0.01), (60, 0.051), (63, 0.045), (64, 0.022), (65, 0.014), (70, 0.014), (73, 0.011), (74, 0.052), (76, 0.032), (79, 0.031), (80, 0.012), (86, 0.021), (95, 0.54)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86222923 <a title="83-lda-1" href="./emnlp-2012-Collocation_Polarity_Disambiguation_Using_Web-based_Pseudo_Contexts.html">28 emnlp-2012-Collocation Polarity Disambiguation Using Web-based Pseudo Contexts</a></p>
<p>Author: Yanyan Zhao ; Bing Qin ; Ting Liu</p><p>Abstract: This paper focuses on the task of collocation polarity disambiguation. The collocation refers to a binary tuple of a polarity word and a target (such as ⟨long, battery life⟩ or ⟨long, ast atratrguep⟩t) (, siunc whh aisch ⟨ ltohneg s,en btatitmeernyt l iofrei⟩en otrat ⟨iolonn gof, tshtaer polarity wwohirdch (“long”) changes along owniothf different targets (“battery life” or “startup”). To disambiguate a collocation’s polarity, previous work always turned to investigate the polarities of its surrounding contexts, and then assigned the majority polarity to the collocation. However, these contexts are limited, thus the resulting polarity is insufficient to be reliable. We therefore propose an unsupervised three-component framework to expand some pseudo contexts from web, to help disambiguate a collocation’s polarity.Without using any additional labeled data, experiments , show that our method is effective.</p><p>same-paper 2 0.84898752 <a title="83-lda-2" href="./emnlp-2012-Lexical_Differences_in_Autobiographical_Narratives_from_Schizophrenic_Patients_and_Healthy_Controls.html">83 emnlp-2012-Lexical Differences in Autobiographical Narratives from Schizophrenic Patients and Healthy Controls</a></p>
<p>Author: Kai Hong ; Christian G. Kohler ; Mary E. March ; Amber A. Parker ; Ani Nenkova</p><p>Abstract: We present a system for automatic identification of schizophrenic patients and healthy controls based on narratives the subjects recounted about emotional experiences in their own life. The focus of the study is to identify the lexical features that distinguish the two populations. We report the results of feature selection experiments that demonstrate that the classifier can achieve accuracy on patient level prediction as high as 76.9% with only a small set of features. We provide an in-depth discussion of the lexical features that distinguish the two groups and the unexpected relationship between emotion types of the narratives and the accuracy of patient status prediction.</p><p>3 0.82939047 <a title="83-lda-3" href="./emnlp-2012-Exploring_Topic_Coherence_over_Many_Models_and_Many_Topics.html">49 emnlp-2012-Exploring Topic Coherence over Many Models and Many Topics</a></p>
<p>Author: Keith Stevens ; Philip Kegelmeyer ; David Andrzejewski ; David Buttler</p><p>Abstract: We apply two new automated semantic evaluations to three distinct latent topic models. Both metrics have been shown to align with human evaluations and provide a balance between internal measures of information gain and comparisons to human ratings of coherent topics. We improve upon the measures by introducing new aggregate measures that allows for comparing complete topic models. We further compare the automated measures to other metrics for topic models, comparison to manually crafted semantic tests and document classification. Our experiments reveal that LDA and LSA each have different strengths; LDA best learns descriptive topics while LSA is best at creating a compact semantic representation ofdocuments and words in a corpus.</p><p>4 0.77677608 <a title="83-lda-4" href="./emnlp-2012-Learning_Lexicon_Models_from_Search_Logs_for_Query_Expansion.html">78 emnlp-2012-Learning Lexicon Models from Search Logs for Query Expansion</a></p>
<p>Author: Jianfeng Gao ; Shasha Xie ; Xiaodong He ; Alnur Ali</p><p>Abstract: This paper explores log-based query expansion (QE) models for Web search. Three lexicon models are proposed to bridge the lexical gap between Web documents and user queries. These models are trained on pairs of user queries and titles of clicked documents. Evaluations on a real world data set show that the lexicon models, integrated into a ranker-based QE system, not only significantly improve the document retrieval performance but also outperform two state-of-the-art log-based QE methods.</p><p>5 0.41323358 <a title="83-lda-5" href="./emnlp-2012-A_Discriminative_Model_for_Query_Spelling_Correction_with_Latent_Structural_SVM.html">5 emnlp-2012-A Discriminative Model for Query Spelling Correction with Latent Structural SVM</a></p>
<p>Author: Huizhong Duan ; Yanen Li ; ChengXiang Zhai ; Dan Roth</p><p>Abstract: Discriminative training in query spelling correction is difficult due to the complex internal structures of the data. Recent work on query spelling correction suggests a two stage approach a noisy channel model that is used to retrieve a number of candidate corrections, followed by discriminatively trained ranker applied to these candidates. The ranker, however, suffers from the fact the low recall of the first, suboptimal, search stage. This paper proposes to directly optimize the search stage with a discriminative model based on latent structural SVM. In this model, we treat query spelling correction as a multiclass classification problem with structured input and output. The latent structural information is used to model the alignment of words in the spelling correction process. Experiment results show that as a standalone speller, our model outperforms all the baseline systems. It also attains a higher recall compared with the noisy channel model, and can therefore serve as a better filtering stage when combined with a ranker.</p><p>6 0.39161649 <a title="83-lda-6" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>7 0.35488522 <a title="83-lda-7" href="./emnlp-2012-Fast_Large-Scale_Approximate_Graph_Construction_for_NLP.html">52 emnlp-2012-Fast Large-Scale Approximate Graph Construction for NLP</a></p>
<p>8 0.35005814 <a title="83-lda-8" href="./emnlp-2012-Opinion_Target_Extraction_Using_Word-Based_Translation_Model.html">101 emnlp-2012-Opinion Target Extraction Using Word-Based Translation Model</a></p>
<p>9 0.34941715 <a title="83-lda-9" href="./emnlp-2012-Explore_Person_Specific_Evidence_in_Web_Person_Name_Disambiguation.html">47 emnlp-2012-Explore Person Specific Evidence in Web Person Name Disambiguation</a></p>
<p>10 0.3447547 <a title="83-lda-10" href="./emnlp-2012-Why_Question_Answering_using_Sentiment_Analysis_and_Word_Classes.html">137 emnlp-2012-Why Question Answering using Sentiment Analysis and Word Classes</a></p>
<p>11 0.34015507 <a title="83-lda-11" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>12 0.3320986 <a title="83-lda-12" href="./emnlp-2012-Streaming_Analysis_of_Discourse_Participants.html">120 emnlp-2012-Streaming Analysis of Discourse Participants</a></p>
<p>13 0.32872495 <a title="83-lda-13" href="./emnlp-2012-Identifying_Event-related_Bursts_via_Social_Media_Activities.html">63 emnlp-2012-Identifying Event-related Bursts via Social Media Activities</a></p>
<p>14 0.32855952 <a title="83-lda-14" href="./emnlp-2012-Extending_Machine_Translation_Evaluation_Metrics_with_Lexical_Cohesion_to_Document_Level.html">50 emnlp-2012-Extending Machine Translation Evaluation Metrics with Lexical Cohesion to Document Level</a></p>
<p>15 0.3230831 <a title="83-lda-15" href="./emnlp-2012-A_Phrase-Discovering_Topic_Model_Using_Hierarchical_Pitman-Yor_Processes.html">8 emnlp-2012-A Phrase-Discovering Topic Model Using Hierarchical Pitman-Yor Processes</a></p>
<p>16 0.3157286 <a title="83-lda-16" href="./emnlp-2012-Reading_The_Web_with_Learned_Syntactic-Semantic_Inference_Rules.html">110 emnlp-2012-Reading The Web with Learned Syntactic-Semantic Inference Rules</a></p>
<p>17 0.31477407 <a title="83-lda-17" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>18 0.30724427 <a title="83-lda-18" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>19 0.30565363 <a title="83-lda-19" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>20 0.30474579 <a title="83-lda-20" href="./emnlp-2012-Discovering_Diverse_and_Salient_Threads_in_Document_Collections.html">33 emnlp-2012-Discovering Diverse and Salient Threads in Document Collections</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
