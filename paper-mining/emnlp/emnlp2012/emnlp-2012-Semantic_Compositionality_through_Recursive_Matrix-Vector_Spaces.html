<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>116 emnlp-2012-Semantic Compositionality through Recursive Matrix-Vector Spaces</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-116" href="#">emnlp2012-116</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>116 emnlp-2012-Semantic Compositionality through Recursive Matrix-Vector Spaces</h1>
<br/><p>Source: <a title="emnlp-2012-116-pdf" href="http://aclweb.org/anthology//D/D12/D12-1110.pdf">pdf</a></p><p>Author: Richard Socher ; Brody Huval ; Christopher D. Manning ; Andrew Y. Ng</p><p>Abstract: Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.</p><p>Reference: <a title="emnlp-2012-116-reference" href="../emnlp2012_reference/emnlp-2012-Semantic_Compositionality_through_Recursive_Matrix-Vector_Spaces_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. [sent-7, score-0.338]
</p><p>2 We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. [sent-8, score-0.858]
</p><p>3 Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. [sent-9, score-0.758]
</p><p>4 This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. [sent-10, score-0.555]
</p><p>5 In these models the meaning of a word is encoded as a vector computed from co-occurrence statistics of a word and its neighboring words. [sent-16, score-0.226]
</p><p>6 vmeactroix  Figure 1: A recursive neural network which learns semantic vector representations of phrases in a tree structure. [sent-21, score-0.732]
</p><p>7 Recently, there has been much progress  in capturing compositionality in vector spaces, e. [sent-29, score-0.277]
</p><p>8 We present a novel recursive neural network model for semantic compositionality. [sent-35, score-0.402]
</p><p>9 In our context, compositionality is the ability to learn compositional vector representations for various types of phrases and sentences of arbitrary length. [sent-36, score-0.667]
</p><p>10 The matrix captures how it modifies the meaning of the other word that it combines with. [sent-42, score-0.225]
</p><p>11 Since the model uses the MV representation with a neural network as the final merging function, we call our model a matrix-vector recursive neural network  (MV-RNN). [sent-44, score-0.472]
</p><p>12 We show that the ability to capture semantic compositionality in a syntactically plausible way translates into state of the art performance on various tasks. [sent-45, score-0.3]
</p><p>13 The task is to predict a sentiment distribution over movie reviews of adverb-adjective pairs such as unbelievably sad or really awesome. [sent-47, score-0.592]
</p><p>14 The MV-RNN is the only model that is able to properly negate sentiment when adjectives are combined with not. [sent-48, score-0.252]
</p><p>15 The MV-RNN outperforms previous state of the art models on full sentence sentiment prediction of movie reviews. [sent-49, score-0.379]
</p><p>16 2  MV-RNN: A Recursive Matrix-Vector Model  The dominant approach for building representations of multi-word units from single word vector representations has been to form a linear combination of the single word representations, such as a sum or weighted average. [sent-59, score-0.461]
</p><p>17 These approaches can work well when the meaning of a text is literally “the sum of its parts”, but fails when words function as operators that modify the meaning of another word: the meaning of “extremely strong” cannot be captured as the sum of word representations for “extremely” and “strong. [sent-61, score-0.673]
</p><p>18 (201 1c) provided a  new possibility for moving beyond a linear combination, through use of a matrix W that multiplied the word vectors (a, b), and a nonlinearity function g (such as a sigmoid or tanh). [sent-63, score-0.449]
</p><p>19 (1)  and apply this function recursively inside a binarized parse tree so that it can compute vectors for multiword sequences. [sent-68, score-0.324]
</p><p>20 Even though the nonlinearity allows to express a wider range of functions, it is almost certainly too much to expect a single fixed W matrix to be able to capture the meaning combination effects of all natural language operators. [sent-69, score-0.369]
</p><p>21 Recent work has started to capture the behavior of natural language operators inside semantic vector spaces by modeling them as matrices, which would allow a matrix for “extremely” to appropriately modify vectors for “smelly” or “strong” (Baroni and Zamparelli, 2010; Zanzotto et al. [sent-71, score-0.769]
</p><p>22 These approaches are along the right lines but so  far have been restricted to capture linear functions of pairs of words whereas we would like nonlinear functions to compute compositional meaning representations for multi-word phrases or full sentences. [sent-73, score-0.635]
</p><p>23 The MV-RNN combines the strengths of both of these ideas by (i) assigning a vector and a matrix to every word and (ii) learning an input-specific, nonlinear, compositional function for computing vector and matrix representations for multi-word sequences of any syntactic type. [sent-74, score-0.954]
</p><p>24 If a word lacks operator semantics, its matrix can be an identity matrix. [sent-76, score-0.254]
</p><p>25 However, if a word acts mainly as an operator, such as “extremely”, its vector can become close to zero, while its matrix gains a clear operator meaning, here magnifying the meaning of the modified word in both positive and negative directions. [sent-77, score-0.415]
</p><p>26 1 Matrix-Vector Neural Word Representation  We represent a word as both a continuous vector and a matrix of parameters. [sent-81, score-0.265]
</p><p>27 Similar to other local co-occurrence based vector space models, the resulting word vectors capture syntactic and semantic information. [sent-84, score-0.404]
</p><p>28 If the vectors have dimensionality n, then each word’s matrix has dimensionality X ∈ Rn×n. [sent-90, score-0.36]
</p><p>29 While the initialization is random, athliety vectors and matrices will subsequently be modified to enable a sequence of words to compose a vector that can predict a distribution over semantic labels. [sent-91, score-0.522]
</p><p>30 In order to compute a parent vector p from two consecutive words and their respective vectors a and b, Mitchell and Lapata (2010) give as their most general function: p = f(a, b, R, K),where R is the a-priori known syntactic relation and K is background knowledge. [sent-98, score-0.36]
</p><p>31 (2) where A, B are matrices for single words, the global W ∈ Rn×2n is a matrix that maps both transformed wWor ∈ds R Rback into the same n-dimensional space. [sent-112, score-0.324]
</p><p>32 The element-wise function g could be simply the identity function but we use instead a nonlinearity such as the sigmoid or hyperbolic tangent tanh. [sent-113, score-0.244]
</p><p>33 Rewriting the two transformed vectors as one vector z, we get p = g(Wz) which is a single layer neural network. [sent-116, score-0.356]
</p><p>34 In this model, the word matrices can capture compositional effects specific to each word, whereas W captures a general composition function. [sent-117, score-0.525]
</p><p>35 Baroni and Zamparelli (2010) computed the parent vector of adjective-noun pairs by p = Ab, where A is an adjective matrix and b is a vector for a noun. [sent-125, score-0.456]
</p><p>36 3  Recursive Compositions of Multiple Words and Phrases This section describes how we extend a word-pair matrix-vector-based compositional model to learn vectors and matrices for longer sequences of words. [sent-137, score-0.571]
</p><p>37 For this to work, we need to take as input a binary parse tree of a phrase or sentence and also compute matrices at each nonterminal parent node. [sent-139, score-0.383]
</p><p>38 The function f can be readily used for phrase vectors since it is recursively compatible (p has the same dimensionality as its children). [sent-140, score-0.313]
</p><p>39 For instance, to compute the vectors and matrices depicted in Fig. [sent-145, score-0.324]
</p><p>40 The model computes vectors and matrices in a bottom-up fashion, applying the functions f,fM to its own previous output (i. [sent-148, score-0.38]
</p><p>41 4 Objective Functions for Training One of the advantages of RNN-based models is that each node of a tree has associated with it a distributed vector representation (the parent vector p) which can also be seen as features describing that phrase. [sent-156, score-0.378]
</p><p>42 We train these representations by adding on  top of each parent node a simple softmax classifier to predict a class distribution over, e. [sent-157, score-0.324]
</p><p>43 The main difference is that the MV-RNN has more flexibility since it has an input specific recursive function fA,B to compute each parent. [sent-166, score-0.238]
</p><p>44 In the following applications, we will use the softmax classifier to predict both sentiment distributions and noun-noun relationships. [sent-167, score-0.338]
</p><p>45 (4)  To compute this gradient, we first compute all tree nodes (pi, Pi) from the bottom-up and then take derivatives of the softmax classifiers at each node in the tree from the top down. [sent-172, score-0.295]
</p><p>46 6 Low-Rank Matrix Approximations If every word is represented by an n-dimensional vector and additionally by an n n matrix, the dimensionality odift itohnea wllyho blye amno nde ×l may a btericxo,m thee t doiolarge with commonly used vector sizes of n = 100. [sent-178, score-0.314]
</p><p>47 7 Discussion: Evaluation and Generality Evaluation of compositional vector spaces is a complex task. [sent-181, score-0.396]
</p><p>48 However, even with good correlation the question remains how these models would perform on downstream NLP tasks such as sentiment detection. [sent-184, score-0.215]
</p><p>49 For sentiment analysis, this is not surprising since antonyms often get similar vectors during unsupervised learning from co-occurrences due  to high similarity of local syntactic contexts. [sent-187, score-0.384]
</p><p>50 If a model cannot correctly capture how an adverb operates on the meaning of adjectives, then there’s little chance it can learn operators for more complex relationships. [sent-194, score-0.384]
</p><p>51 The second study analyzes whether the MV-RNN can learn simple boolean operators of propositional logic such as conjunctives or negation from truth values. [sent-195, score-0.585]
</p><p>52 1 Predicting Sentiment Distributions of Adverb-Adjective Pairs The first study considers the prediction of finegrained sentiment distributions of adverb-adjective pairs and analyzes different possibilities for computing the parent vector p. [sent-198, score-0.443]
</p><p>53 The results show that the MV-RNN operators are powerful enough to capture the operational meanings of various types of adverbs. [sent-199, score-0.252]
</p><p>54 We never give the algorithm sentiment distributions for single words, and, while single words overlap between training and testing, the test set consists of never before seen word pairs. [sent-207, score-0.215]
</p><p>55 unbelievaMRbNlVyN−aRnNnNoying 01 2345678910 fairly sad  fairly awesome  0 4. [sent-231, score-0.305]
</p><p>56 20 1  2  3  4  5  6  7  8  9  10  1  2  3  4  not awesome  5  6  7  8  9  not sad  0 0 02310. [sent-239, score-0.227]
</p><p>57 RMNVN−RN 1  2  3  4  5  6  7  8  9  10  1  2  3  unbelievably awesome  1  2  3  4  5  6  7  8  4  5  6  7  8  9  10  8  9  10  unbelievably sad  9  10  1  2  3  4  5  6  7  Figure 3: Left: Average KL-divergence for predicting sentiment distributions of unseen adverb-adjective pairs of the test set. [sent-250, score-0.741]
</p><p>58 Right: Predicting sentiment distributions (over 1-10 stars on the x-axis) of adverbadjective pairs. [sent-254, score-0.215]
</p><p>59 The RNN and linear MVR are not able to modify the sentiment correctly: not awesome is more positive than fairly awesome and not annoying has a similar shape as unbelievably annoying. [sent-257, score-0.723]
</p><p>60 An (adverb,adjective) pair is described by its vectors (a, b) and matrices (A, B). [sent-261, score-0.324]
</p><p>61 We cross-validated all models over regularization parameters for word vectors, the softmax classifier, the RNN parameter W and the word operators (10−4, 10−3) and word vector sizes (n = 6, 8, 10, 12, 15, 20). [sent-276, score-0.471]
</p><p>62 It shows that the idea of matrix-vector representations for all words and having a nonlinearity are both important. [sent-281, score-0.25]
</p><p>63 The MV-RNN which combines these two ideas is best able to learn the various compositional effects. [sent-282, score-0.247]
</p><p>64 However, only the MV-RNN has enough expressive power to allow negation to completely shift the sentiment with respect to an adjective. [sent-287, score-0.301]
</p><p>65 A negated adjective carrying negative sentiment becomes slightly positive, whereas not awesome is correctly attenuated. [sent-288, score-0.343]
</p><p>66 false  falsefalse  false  truefalse  false  falsetrue  true  truetrue  true  ¬false  false  ¬true  Figure 4: Training trees for the MV-RNN to learn propositional operators. [sent-293, score-0.41]
</p><p>67 The model learns vectors and operators for ∧ (and) and ¬(negation). [sent-294, score-0.347]
</p><p>68 In other words, can it learn some of the propositional logic operators such as and, or, not in terms of vectors and matrices from a few examples. [sent-300, score-0.786]
</p><p>69 There have been many attempts at automatically parsing natural language to a logi-  cal form using recursive compositional rules. [sent-305, score-0.41]
</p><p>70 To this end, we illustrate in a simple example that our MV-RNN model and its learned word matrices (operators) have the ability to learn propositional logic operators such as ∧, ∨, ¬ (and, or, not). [sent-313, score-0.654]
</p><p>71 ion T hfiosr tshe a ability to pick up these phenomena in real datasets 1207 and tasks such as sentiment detection which we focus on in the subsequent sections. [sent-315, score-0.215]
</p><p>72 Fixing the operators to the 1 1 identity matrix 1 Fisi essentially ignoring ot htheme. [sent-320, score-0.412]
</p><p>73 Thus, we can combine these operators to construct any propositional logic function of any number of inputs (including xor). [sent-329, score-0.459]
</p><p>74 Table 2: Hard movie review examples of positive (1) and negative (0) sentiment (S. [sent-357, score-0.325]
</p><p>75 The state of the art recursive autoencoder model of Socher et al. [sent-361, score-0.293]
</p><p>76 In our last experiment we show that the MV-RNN can also learn how a syntactic context composes an aggregate meaning of the semantic relationships between words. [sent-370, score-0.302]
</p><p>77 We show that by building a single compositional semantics for the minimal constituent including both terms one can achieve a higher performance. [sent-378, score-0.247]
</p><p>78 We apply the same type of MV-RNN model as in sentiment to the subtree spanned by the two words. [sent-384, score-0.215]
</p><p>79 Note that the final classifier is a recursive, compositional function of all the words in the syntactic path between the bracketed  words. [sent-408, score-0.32]
</p><p>80 maining inputs and the training setup are the same as in previous sentiment experiments. [sent-427, score-0.215]
</p><p>81 In order to see whether our system can improve over this system, we added three features to the MV-RNN vector and trained another softmax classifier. [sent-430, score-0.256]
</p><p>82 There are several sophisticated ideas for compositionality in vector spaces. [sent-444, score-0.277]
</p><p>83 Mitchell and Lapata (2010) present an overview of the most important compositional models, from simple vector addition and component-wise multiplication to tensor products, and convolution (Metcalfe, 1990). [sent-445, score-0.468]
</p><p>84 net/projects/supersensetag/ Other important models are tensor products (Clark and Pulman, 2007), quantum logic (Widdows, 2008), holographic reduced representations (Plate, 1995) and the Compositional Matrix Space model (Rudolph and Giesbrecht, 2010). [sent-449, score-0.42]
</p><p>85 RNNs are related to autoencoder models such as the recursive autoas-  sociative memory (RAAM) (Pollack, 1990) or recurrent neural networks (Elman, 1991). [sent-450, score-0.42]
</p><p>86 Bottou (201 1) and Hinton (1990) discussed related models such as recursive autoencoders for text understanding. [sent-451, score-0.248]
</p><p>87 Yessenalina and Cardie (201 1) introduce a sentiment analysis model that describes words as matrices and composition as matrix multiplication. [sent-458, score-0.627]
</p><p>88 Since matrix multiplication is associative, this cannot capture different scopes of negation or syntactic differences. [sent-459, score-0.369]
</p><p>89 Their model, is a special case of our encoding model (when you ignore vectors, fix the tree to be strictly branching in one direction and use as the matrix composition function P = AB). [sent-460, score-0.31]
</p><p>90 Grefenstette and Sadrzadeh (201 1) learn matrices for verbs in a categorical model. [sent-462, score-0.231]
</p><p>91 The trained matrices improve correlation with human judgments on the task of identifying relatedness of subjectverb-object triplets. [sent-463, score-0.247]
</p><p>92 7  Conclusion  We introduced a new model towards a complete treatment of compositionality in word vector spaces. [sent-464, score-0.277]
</p><p>93 Our model builds on a syntactically plausible parse tree and can handle compositional phenomena. [sent-465, score-0.304]
</p><p>94 The main novelty of our model is the combination of matrix-vector representations with a recursive neural network. [sent-466, score-0.436]
</p><p>95 It can learn both the meaning vectors of a word and how that word modifies its neighbors (via its matrix). [sent-467, score-0.264]
</p><p>96 It generalizes several models in the literature, can learn propositional logic, accurately predicts sentiment and can be used to classify semantic relationships between nouns in a sentence. [sent-469, score-0.563]
</p><p>97 Experimental support for a categorical compositional distributional model of meaning. [sent-544, score-0.265]
</p><p>98 Dependency tree-based sentiment classification using CRFs with hidden variables. [sent-617, score-0.215]
</p><p>99 Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. [sent-638, score-0.283]
</p><p>100 Learning continuous phrase representations and syntactic parsing with recursive neural networks. [sent-690, score-0.51]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('socher', 0.322), ('sentiment', 0.215), ('operators', 0.215), ('compositional', 0.208), ('recursive', 0.202), ('matrices', 0.192), ('mvr', 0.149), ('rnn', 0.149), ('compositionality', 0.144), ('representations', 0.143), ('vector', 0.133), ('matrix', 0.132), ('vectors', 0.132), ('awesome', 0.128), ('unbelievably', 0.128), ('propositional', 0.123), ('softmax', 0.123), ('baroni', 0.115), ('ba', 0.115), ('zanzotto', 0.11), ('movie', 0.11), ('nonlinearity', 0.107), ('rn', 0.106), ('ab', 0.106), ('sad', 0.099), ('meaning', 0.093), ('neural', 0.091), ('composition', 0.088), ('negation', 0.086), ('logic', 0.085), ('multiplication', 0.077), ('hendrickx', 0.073), ('relationships', 0.068), ('zamparelli', 0.066), ('identity', 0.065), ('lapata', 0.065), ('mitchell', 0.065), ('semantic', 0.065), ('derivatives', 0.064), ('mv', 0.064), ('nrnn', 0.064), ('rmnv', 0.064), ('svmpos', 0.064), ('false', 0.062), ('recursively', 0.06), ('parent', 0.058), ('distributional', 0.057), ('operator', 0.057), ('functions', 0.056), ('holographic', 0.055), ('judgments', 0.055), ('spaces', 0.055), ('nakagawa', 0.054), ('art', 0.054), ('tree', 0.054), ('nouns', 0.053), ('quantum', 0.05), ('tensor', 0.05), ('yessenalina', 0.05), ('pretty', 0.05), ('wm', 0.05), ('wordnet', 0.048), ('dimensionality', 0.048), ('networks', 0.047), ('autoencoders', 0.046), ('network', 0.044), ('predicting', 0.043), ('annoying', 0.043), ('backpropagation', 0.043), ('goller', 0.043), ('mvrnn', 0.043), ('nella', 0.043), ('potts', 0.043), ('ptop', 0.043), ('recurrent', 0.043), ('rink', 0.043), ('rnns', 0.043), ('rudolph', 0.043), ('classifying', 0.042), ('linear', 0.042), ('parse', 0.042), ('reviews', 0.04), ('semantics', 0.039), ('path', 0.039), ('fairly', 0.039), ('learn', 0.039), ('extremely', 0.038), ('products', 0.037), ('analyzes', 0.037), ('fm', 0.037), ('phrase', 0.037), ('autoencoder', 0.037), ('influenza', 0.037), ('apartment', 0.037), ('negate', 0.037), ('stemming', 0.037), ('und', 0.037), ('syntactic', 0.037), ('capture', 0.037), ('function', 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000011 <a title="116-tfidf-1" href="./emnlp-2012-Semantic_Compositionality_through_Recursive_Matrix-Vector_Spaces.html">116 emnlp-2012-Semantic Compositionality through Recursive Matrix-Vector Spaces</a></p>
<p>Author: Richard Socher ; Brody Huval ; Christopher D. Manning ; Andrew Y. Ng</p><p>Abstract: Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.</p><p>2 0.42117533 <a title="116-tfidf-2" href="./emnlp-2012-A_Comparison_of_Vector-based_Representations_for_Semantic_Composition.html">4 emnlp-2012-A Comparison of Vector-based Representations for Semantic Composition</a></p>
<p>Author: William Blacoe ; Mirella Lapata</p><p>Abstract: In this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods. We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication. Some are shallow while others operate over syntactic structure, rely on parameter learning, or require access to very large corpora. We find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection. The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method.</p><p>3 0.13704175 <a title="116-tfidf-3" href="./emnlp-2012-First_Order_vs._Higher_Order_Modification_in_Distributional_Semantics.html">53 emnlp-2012-First Order vs. Higher Order Modification in Distributional Semantics</a></p>
<p>Author: Gemma Boleda ; Eva Maria Vecchi ; Miquel Cornudella ; Louise McNally</p><p>Abstract: Adjectival modification, particularly by expressions that have been treated as higherorder modifiers in the formal semantics tradition, raises interesting challenges for semantic composition in distributional semantic models. We contrast three types of adjectival modifiers intersectively used color terms (as in white towel, clearly first-order), subsectively used color terms (white wine, which have been modeled as both first- and higher-order), and intensional adjectives (former bassist, clearly higher-order) and test the ability of different composition strategies to model their behavior. In addition to opening up a new empirical domain for research on distributional semantics, our observations concerning the attested vectors for the different types of adjectives, the nouns they modify, and the resulting – – noun phrases yield insights into modification that have been little evident in the formal semantics literature to date.</p><p>4 0.13232686 <a title="116-tfidf-4" href="./emnlp-2012-Do_Neighbours_Help%3F_An_Exploration_of_Graph-based_Algorithms_for_Cross-domain_Sentiment_Classification.html">34 emnlp-2012-Do Neighbours Help? An Exploration of Graph-based Algorithms for Cross-domain Sentiment Classification</a></p>
<p>Author: Natalia Ponomareva ; Mike Thelwall</p><p>Abstract: This paper presents a comparative study of graph-based approaches for cross-domain sentiment classification. In particular, the paper analyses two existing methods: an optimisation problem and a ranking algorithm. We compare these graph-based methods with each other and with the other state-ofthe-art approaches and conclude that graph domain representations offer a competitive solution to the domain adaptation problem. Analysis of the best parameters for graphbased algorithms reveals that there are no optimal values valid for all domain pairs and that these values are dependent on the characteristics of corresponding domains.</p><p>5 0.10750264 <a title="116-tfidf-5" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>Author: Fei Huang ; Alexander Yates</p><p>Abstract: Representation learning is a promising technique for discovering features that allow supervised classifiers to generalize from a source domain dataset to arbitrary new domains. We present a novel, formal statement of the representation learning task. We argue that because the task is computationally intractable in general, it is important for a representation learner to be able to incorporate expert knowledge during its search for helpful features. Leveraging the Posterior Regularization framework, we develop an architecture for incorporating biases into representation learning. We investigate three types of biases, and experiments on two domain adaptation tasks show that our biased learners identify significantly better sets of features than unbiased learners, resulting in a relative reduction in error of more than 16% for both tasks, with respect to existing state-of-the-art representation learning techniques.</p><p>6 0.10155846 <a title="116-tfidf-6" href="./emnlp-2012-Why_Question_Answering_using_Sentiment_Analysis_and_Word_Classes.html">137 emnlp-2012-Why Question Answering using Sentiment Analysis and Word Classes</a></p>
<p>7 0.088364854 <a title="116-tfidf-7" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>8 0.085561194 <a title="116-tfidf-8" href="./emnlp-2012-Learning_Syntactic_Categories_Using_Paradigmatic_Representations_of_Word_Context.html">79 emnlp-2012-Learning Syntactic Categories Using Paradigmatic Representations of Word Context</a></p>
<p>9 0.083998658 <a title="116-tfidf-9" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>10 0.079240501 <a title="116-tfidf-10" href="./emnlp-2012-Active_Learning_for_Imbalanced_Sentiment_Classification.html">15 emnlp-2012-Active Learning for Imbalanced Sentiment Classification</a></p>
<p>11 0.078774579 <a title="116-tfidf-11" href="./emnlp-2012-Spectral_Dependency_Parsing_with_Latent_Variables.html">119 emnlp-2012-Spectral Dependency Parsing with Latent Variables</a></p>
<p>12 0.072624125 <a title="116-tfidf-12" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>13 0.070550457 <a title="116-tfidf-13" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>14 0.070363812 <a title="116-tfidf-14" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>15 0.066668853 <a title="116-tfidf-15" href="./emnlp-2012-Improving_NLP_through_Marginalization_of_Hidden_Syntactic_Structure.html">65 emnlp-2012-Improving NLP through Marginalization of Hidden Syntactic Structure</a></p>
<p>16 0.066638038 <a title="116-tfidf-16" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>17 0.064895853 <a title="116-tfidf-17" href="./emnlp-2012-Concurrent_Acquisition_of_Word_Meaning_and_Lexical_Categories.html">29 emnlp-2012-Concurrent Acquisition of Word Meaning and Lexical Categories</a></p>
<p>18 0.063793972 <a title="116-tfidf-18" href="./emnlp-2012-Word_Salad%3A_Relating_Food_Prices_and_Descriptions.html">139 emnlp-2012-Word Salad: Relating Food Prices and Descriptions</a></p>
<p>19 0.060845386 <a title="116-tfidf-19" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>20 0.05965846 <a title="116-tfidf-20" href="./emnlp-2012-Fast_Large-Scale_Approximate_Graph_Construction_for_NLP.html">52 emnlp-2012-Fast Large-Scale Approximate Graph Construction for NLP</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.261), (1, 0.07), (2, -0.007), (3, 0.171), (4, 0.137), (5, 0.138), (6, 0.085), (7, 0.118), (8, 0.261), (9, 0.077), (10, -0.417), (11, 0.176), (12, 0.01), (13, -0.137), (14, 0.087), (15, -0.117), (16, 0.19), (17, 0.015), (18, 0.047), (19, -0.057), (20, -0.055), (21, -0.009), (22, -0.117), (23, 0.015), (24, 0.019), (25, 0.053), (26, -0.052), (27, 0.065), (28, -0.07), (29, -0.006), (30, 0.004), (31, 0.034), (32, -0.037), (33, 0.028), (34, 0.015), (35, -0.03), (36, -0.08), (37, -0.103), (38, 0.011), (39, -0.022), (40, -0.002), (41, -0.023), (42, 0.005), (43, 0.061), (44, -0.028), (45, -0.089), (46, 0.055), (47, 0.038), (48, 0.064), (49, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95575559 <a title="116-lsi-1" href="./emnlp-2012-Semantic_Compositionality_through_Recursive_Matrix-Vector_Spaces.html">116 emnlp-2012-Semantic Compositionality through Recursive Matrix-Vector Spaces</a></p>
<p>Author: Richard Socher ; Brody Huval ; Christopher D. Manning ; Andrew Y. Ng</p><p>Abstract: Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.</p><p>2 0.89708728 <a title="116-lsi-2" href="./emnlp-2012-A_Comparison_of_Vector-based_Representations_for_Semantic_Composition.html">4 emnlp-2012-A Comparison of Vector-based Representations for Semantic Composition</a></p>
<p>Author: William Blacoe ; Mirella Lapata</p><p>Abstract: In this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods. We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication. Some are shallow while others operate over syntactic structure, rely on parameter learning, or require access to very large corpora. We find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection. The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method.</p><p>3 0.78831369 <a title="116-lsi-3" href="./emnlp-2012-First_Order_vs._Higher_Order_Modification_in_Distributional_Semantics.html">53 emnlp-2012-First Order vs. Higher Order Modification in Distributional Semantics</a></p>
<p>Author: Gemma Boleda ; Eva Maria Vecchi ; Miquel Cornudella ; Louise McNally</p><p>Abstract: Adjectival modification, particularly by expressions that have been treated as higherorder modifiers in the formal semantics tradition, raises interesting challenges for semantic composition in distributional semantic models. We contrast three types of adjectival modifiers intersectively used color terms (as in white towel, clearly first-order), subsectively used color terms (white wine, which have been modeled as both first- and higher-order), and intensional adjectives (former bassist, clearly higher-order) and test the ability of different composition strategies to model their behavior. In addition to opening up a new empirical domain for research on distributional semantics, our observations concerning the attested vectors for the different types of adjectives, the nouns they modify, and the resulting – – noun phrases yield insights into modification that have been little evident in the formal semantics literature to date.</p><p>4 0.52057379 <a title="116-lsi-4" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>Author: Wen-tau Yih ; Geoffrey Zweig ; John Platt</p><p>Abstract: Existing vector space models typically map synonyms and antonyms to similar word vectors, and thus fail to represent antonymy. We introduce a new vector space representation where antonyms lie on opposite sides of a sphere: in the word vector space, synonyms have cosine similarities close to one, while antonyms are close to minus one. We derive this representation with the aid of a thesaurus and latent semantic analysis (LSA). Each entry in the thesaurus a word sense along with its synonyms and antonyms is treated as a “document,” and the resulting document collection is subjected to LSA. The key contribution of this work is to show how to assign signs to the entries in the co-occurrence matrix on which LSA operates, so as to induce a subspace with the desired property. – – We evaluate this procedure with the Graduate Record Examination questions of (Mohammed et al., 2008) and find that the method improves on the results of that study. Further improvements result from refining the subspace representation with discriminative training, and augmenting the training data with general newspaper text. Altogether, we improve on the best previous results by 11points absolute in F measure.</p><p>5 0.44940788 <a title="116-lsi-5" href="./emnlp-2012-Learning_Syntactic_Categories_Using_Paradigmatic_Representations_of_Word_Context.html">79 emnlp-2012-Learning Syntactic Categories Using Paradigmatic Representations of Word Context</a></p>
<p>Author: Mehmet Ali Yatbaz ; Enis Sert ; Deniz Yuret</p><p>Abstract: We investigate paradigmatic representations of word context in the domain of unsupervised syntactic category acquisition. Paradigmatic representations of word context are based on potential substitutes of a word in contrast to syntagmatic representations based on properties of neighboring words. We compare a bigram based baseline model with several paradigmatic models and demonstrate significant gains in accuracy. Our best model based on Euclidean co-occurrence embedding combines the paradigmatic context representation with morphological and orthographic features and achieves 80% many-to-one accuracy on a 45-tag 1M word corpus.</p><p>6 0.40208367 <a title="116-lsi-6" href="./emnlp-2012-Do_Neighbours_Help%3F_An_Exploration_of_Graph-based_Algorithms_for_Cross-domain_Sentiment_Classification.html">34 emnlp-2012-Do Neighbours Help? An Exploration of Graph-based Algorithms for Cross-domain Sentiment Classification</a></p>
<p>7 0.35665458 <a title="116-lsi-7" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>8 0.35545775 <a title="116-lsi-8" href="./emnlp-2012-Grounded_Models_of_Semantic_Representation.html">61 emnlp-2012-Grounded Models of Semantic Representation</a></p>
<p>9 0.34832826 <a title="116-lsi-9" href="./emnlp-2012-Spectral_Dependency_Parsing_with_Latent_Variables.html">119 emnlp-2012-Spectral Dependency Parsing with Latent Variables</a></p>
<p>10 0.29342765 <a title="116-lsi-10" href="./emnlp-2012-Word_Salad%3A_Relating_Food_Prices_and_Descriptions.html">139 emnlp-2012-Word Salad: Relating Food Prices and Descriptions</a></p>
<p>11 0.29080302 <a title="116-lsi-11" href="./emnlp-2012-Active_Learning_for_Imbalanced_Sentiment_Classification.html">15 emnlp-2012-Active Learning for Imbalanced Sentiment Classification</a></p>
<p>12 0.27503809 <a title="116-lsi-12" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>13 0.26104087 <a title="116-lsi-13" href="./emnlp-2012-Why_Question_Answering_using_Sentiment_Analysis_and_Word_Classes.html">137 emnlp-2012-Why Question Answering using Sentiment Analysis and Word Classes</a></p>
<p>14 0.25614509 <a title="116-lsi-14" href="./emnlp-2012-Concurrent_Acquisition_of_Word_Meaning_and_Lexical_Categories.html">29 emnlp-2012-Concurrent Acquisition of Word Meaning and Lexical Categories</a></p>
<p>15 0.24792986 <a title="116-lsi-15" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>16 0.24345247 <a title="116-lsi-16" href="./emnlp-2012-Aligning_Predicates_across_Monolingual_Comparable_Texts_using_Graph-based_Clustering.html">16 emnlp-2012-Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering</a></p>
<p>17 0.23511058 <a title="116-lsi-17" href="./emnlp-2012-Fast_Large-Scale_Approximate_Graph_Construction_for_NLP.html">52 emnlp-2012-Fast Large-Scale Approximate Graph Construction for NLP</a></p>
<p>18 0.23209865 <a title="116-lsi-18" href="./emnlp-2012-Characterizing_Stylistic_Elements_in_Syntactic_Structure.html">27 emnlp-2012-Characterizing Stylistic Elements in Syntactic Structure</a></p>
<p>19 0.22534589 <a title="116-lsi-19" href="./emnlp-2012-Employing_Compositional_Semantics_and_Discourse_Consistency_in_Chinese_Event_Extraction.html">38 emnlp-2012-Employing Compositional Semantics and Discourse Consistency in Chinese Event Extraction</a></p>
<p>20 0.22453168 <a title="116-lsi-20" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.013), (16, 0.021), (25, 0.016), (34, 0.044), (45, 0.048), (60, 0.066), (63, 0.043), (64, 0.023), (65, 0.028), (70, 0.012), (74, 0.034), (76, 0.537), (80, 0.012), (86, 0.019), (95, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94048566 <a title="116-lda-1" href="./emnlp-2012-Training_Factored_PCFGs_with_Expectation_Propagation.html">126 emnlp-2012-Training Factored PCFGs with Expectation Propagation</a></p>
<p>Author: David Hall ; Dan Klein</p><p>Abstract: PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar. We present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion. Our method works with linguisticallymotivated annotations, induced latent structure, lexicalization, or any mix of the three. We use a structured expectation propagation algorithm that makes use of the factored structure in two ways. First, by partitioning the factors, it speeds up parsing exponentially over the unfactored approach. Second, it minimizes the redundancy of the factors during training, improving accuracy over an independent approach. Using purely latent variable annotations, we can efficiently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the na¨ ıve approach. Combining latent, lexicalized, and unlexicalized anno- tations, our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank.</p><p>same-paper 2 0.92409533 <a title="116-lda-2" href="./emnlp-2012-Semantic_Compositionality_through_Recursive_Matrix-Vector_Spaces.html">116 emnlp-2012-Semantic Compositionality through Recursive Matrix-Vector Spaces</a></p>
<p>Author: Richard Socher ; Brody Huval ; Christopher D. Manning ; Andrew Y. Ng</p><p>Abstract: Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.</p><p>3 0.9062742 <a title="116-lda-3" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>Author: Ahmed Hassan ; Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: A mixture of positive (friendly) and negative (antagonistic) relations exist among users in most social media applications. However, many such applications do not allow users to explicitly express the polarity of their interactions. As a result most research has either ignored negative links or was limited to the few domains where such relations are explicitly expressed (e.g. Epinions trust/distrust). We study text exchanged between users in online communities. We find that the polarity of the links between users can be predicted with high accuracy given the text they exchange. This allows us to build a signed network representation of discussions; where every edge has a sign: positive to denote a friendly relation, or negative to denote an antagonistic relation. We also connect our analysis to social psychology theories of balance. We show that the automatically predicted networks are consistent with those theories. Inspired by that, we present a technique for identifying subgroups in discussions by partitioning singed networks representing them.</p><p>4 0.58850539 <a title="116-lda-4" href="./emnlp-2012-A_Comparison_of_Vector-based_Representations_for_Semantic_Composition.html">4 emnlp-2012-A Comparison of Vector-based Representations for Semantic Composition</a></p>
<p>Author: William Blacoe ; Mirella Lapata</p><p>Abstract: In this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods. We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication. Some are shallow while others operate over syntactic structure, rely on parameter learning, or require access to very large corpora. We find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection. The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method.</p><p>5 0.47969106 <a title="116-lda-5" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms ofweak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.</p><p>6 0.47283483 <a title="116-lda-6" href="./emnlp-2012-Re-training_Monolingual_Parser_Bilingually_for_Syntactic_SMT.html">109 emnlp-2012-Re-training Monolingual Parser Bilingually for Syntactic SMT</a></p>
<p>7 0.46942273 <a title="116-lda-7" href="./emnlp-2012-Constructing_Task-Specific_Taxonomies_for_Document_Collection_Browsing.html">30 emnlp-2012-Constructing Task-Specific Taxonomies for Document Collection Browsing</a></p>
<p>8 0.46333489 <a title="116-lda-8" href="./emnlp-2012-User_Demographics_and_Language_in_an_Implicit_Social_Network.html">134 emnlp-2012-User Demographics and Language in an Implicit Social Network</a></p>
<p>9 0.46099621 <a title="116-lda-9" href="./emnlp-2012-Answering_Opinion_Questions_on_Products_by_Exploiting_Hierarchical_Organization_of_Consumer_Reviews.html">20 emnlp-2012-Answering Opinion Questions on Products by Exploiting Hierarchical Organization of Consumer Reviews</a></p>
<p>10 0.44946185 <a title="116-lda-10" href="./emnlp-2012-Fast_Large-Scale_Approximate_Graph_Construction_for_NLP.html">52 emnlp-2012-Fast Large-Scale Approximate Graph Construction for NLP</a></p>
<p>11 0.4486838 <a title="116-lda-11" href="./emnlp-2012-First_Order_vs._Higher_Order_Modification_in_Distributional_Semantics.html">53 emnlp-2012-First Order vs. Higher Order Modification in Distributional Semantics</a></p>
<p>12 0.44521892 <a title="116-lda-12" href="./emnlp-2012-Streaming_Analysis_of_Discourse_Participants.html">120 emnlp-2012-Streaming Analysis of Discourse Participants</a></p>
<p>13 0.44482896 <a title="116-lda-13" href="./emnlp-2012-Three_Dependency-and-Boundary_Models_for_Grammar_Induction.html">124 emnlp-2012-Three Dependency-and-Boundary Models for Grammar Induction</a></p>
<p>14 0.43931857 <a title="116-lda-14" href="./emnlp-2012-Polarity_Inducing_Latent_Semantic_Analysis.html">107 emnlp-2012-Polarity Inducing Latent Semantic Analysis</a></p>
<p>15 0.43207231 <a title="116-lda-15" href="./emnlp-2012-A_Weakly_Supervised_Model_for_Sentence-Level_Semantic_Orientation_Analysis_with_Multiple_Experts.html">14 emnlp-2012-A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts</a></p>
<p>16 0.43199754 <a title="116-lda-16" href="./emnlp-2012-Extracting_Opinion_Expressions_with_semi-Markov_Conditional_Random_Fields.html">51 emnlp-2012-Extracting Opinion Expressions with semi-Markov Conditional Random Fields</a></p>
<p>17 0.4297871 <a title="116-lda-17" href="./emnlp-2012-Word_Salad%3A_Relating_Food_Prices_and_Descriptions.html">139 emnlp-2012-Word Salad: Relating Food Prices and Descriptions</a></p>
<p>18 0.40971851 <a title="116-lda-18" href="./emnlp-2012-Learning_Constraints_for_Consistent_Timeline_Extraction.html">77 emnlp-2012-Learning Constraints for Consistent Timeline Extraction</a></p>
<p>19 0.40529034 <a title="116-lda-19" href="./emnlp-2012-Unsupervised_PCFG_Induction_for_Grounded_Language_Learning_with_Highly_Ambiguous_Supervision.html">133 emnlp-2012-Unsupervised PCFG Induction for Grounded Language Learning with Highly Ambiguous Supervision</a></p>
<p>20 0.39538038 <a title="116-lda-20" href="./emnlp-2012-Syntactic_Surprisal_Affects_Spoken_Word_Duration_in_Conversational_Contexts.html">122 emnlp-2012-Syntactic Surprisal Affects Spoken Word Duration in Conversational Contexts</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
