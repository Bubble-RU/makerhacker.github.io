<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>122 emnlp-2012-Syntactic Surprisal Affects Spoken Word Duration in Conversational Contexts</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-122" href="#">emnlp2012-122</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>122 emnlp-2012-Syntactic Surprisal Affects Spoken Word Duration in Conversational Contexts</h1>
<br/><p>Source: <a title="emnlp-2012-122-pdf" href="http://aclweb.org/anthology//D/D12/D12-1033.pdf">pdf</a></p><p>Author: Vera Demberg ; Asad Sayeed ; Philip Gorinski ; Nikolaos Engonopoulos</p><p>Abstract: We present results of a novel experiment to investigate speech production in conversational data that links speech rate to information density. We provide the first evidence for an association between syntactic surprisal and word duration in recorded speech. Using the AMI corpus which contains transcriptions of focus group meetings with precise word durations, we show that word durations correlate with syntactic surprisal estimated from the incremental Roark parser over and above simpler measures, such as word duration estimated from a state-of-the-art text-to-speech system and word frequencies, and that the syntactic surprisal estimates are better predictors of word durations than a simpler version of surprisal based on trigram probabilities. This result supports the uniform information density (UID) hypothesis and points a way to more realistic artificial speech generation.</p><p>Reference: <a title="emnlp-2012-122-reference" href="../emnlp2012_reference/emnlp-2012-Syntactic_Surprisal_Affects_Spoken_Word_Duration_in_Conversational_Contexts_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dur', 0.707), ('surpr', 0.346), ('roark', 0.24), ('jaeg', 0.129), ('uid', 0.129), ('gigaword', 0.128), ('speak', 0.109), ('slop', 0.1), ('tts', 0.1), ('speech', 0.099), ('mary', 0.097), ('resid', 0.096), ('canon', 0.096), ('prefix', 0.08), ('spok', 0.079), ('am', 0.073), ('syntact', 0.073), ('dens', 0.072), ('syl', 0.072), ('earley', 0.072)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="122-tfidf-1" href="./emnlp-2012-Syntactic_Surprisal_Affects_Spoken_Word_Duration_in_Conversational_Contexts.html">122 emnlp-2012-Syntactic Surprisal Affects Spoken Word Duration in Conversational Contexts</a></p>
<p>Author: Vera Demberg ; Asad Sayeed ; Philip Gorinski ; Nikolaos Engonopoulos</p><p>Abstract: We present results of a novel experiment to investigate speech production in conversational data that links speech rate to information density. We provide the first evidence for an association between syntactic surprisal and word duration in recorded speech. Using the AMI corpus which contains transcriptions of focus group meetings with precise word durations, we show that word durations correlate with syntactic surprisal estimated from the incremental Roark parser over and above simpler measures, such as word duration estimated from a state-of-the-art text-to-speech system and word frequencies, and that the syntactic surprisal estimates are better predictors of word durations than a simpler version of surprisal based on trigram probabilities. This result supports the uniform information density (UID) hypothesis and points a way to more realistic artificial speech generation.</p><p>2 0.095153108 <a title="122-tfidf-2" href="./emnlp-2012-Optimising_Incremental_Dialogue_Decisions_Using_Information_Density_for_Interactive_Systems.html">102 emnlp-2012-Optimising Incremental Dialogue Decisions Using Information Density for Interactive Systems</a></p>
<p>Author: Nina Dethlefs ; Helen Hastie ; Verena Rieser ; Oliver Lemon</p><p>Abstract: Incremental processing allows system designers to address several discourse phenomena that have previously been somewhat neglected in interactive systems, such as backchannels or barge-ins, but that can enhance the responsiveness and naturalness of systems. Unfortunately, prior work has focused largely on deterministic incremental decision making, rendering system behaviour less flexible and adaptive than is desirable. We present a novel approach to incremental decision making that is based on Hierarchical Reinforcement Learning to achieve an interactive optimisation of Information Presentation (IP) strategies, allowing the system to generate and comprehend backchannels and barge-ins, by employing the recent psycholinguistic hypothesis of information density (ID) (Jaeger, 2010). Results in terms of average rewards and a human rating study show that our learnt strategy outperforms several baselines that are | v not sensitive to ID by more than 23%.</p><p>3 0.093167365 <a title="122-tfidf-3" href="./emnlp-2012-Assessment_of_ESL_Learners%27_Syntactic_Competence_Based_on_Similarity_Measures.html">21 emnlp-2012-Assessment of ESL Learners' Syntactic Competence Based on Similarity Measures</a></p>
<p>Author: Su-Youn Yoon ; Suma Bhat</p><p>Abstract: This study presents a novel method that measures English language learners’ syntactic competence towards improving automated speech scoring systems. In contrast to most previous studies which focus on the length of production units such as the mean length of clauses, we focused on capturing the differences in the distribution of morpho-syntactic features or grammatical expressions across proficiency. We estimated the syntactic competence through the use of corpus-based NLP techniques. Assuming that the range and so- phistication of grammatical expressions can be captured by the distribution of Part-ofSpeech (POS) tags, vector space models of POS tags were constructed. We use a large corpus of English learners’ responses that are classified into four proficiency levels by human raters. Our proposed feature measures the similarity of a given response with the most proficient group and is then estimates the learner’s syntactic competence level. Widely outperforming the state-of-the-art measures of syntactic complexity, our method attained a significant correlation with humanrated scores. The correlation between humanrated scores and features based on manual transcription was 0.43 and the same based on ASR-hypothesis was slightly lower, 0.42. An important advantage of our method is its robustness against speech recognition errors not to mention the simplicity of feature generation that captures a reasonable set of learnerspecific syntactic errors. 600 Measures Suma Bhat Beckman Institute, Urbana, IL 61801 . spbhat 2 @ i l l ino i edu s</p><p>4 0.062164456 <a title="122-tfidf-4" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>Author: Zhongguo Li ; Guodong Zhou</p><p>Abstract: Most previous approaches to syntactic parsing of Chinese rely on a preprocessing step of word segmentation, thereby assuming there was a clearly defined boundary between morphology and syntax in Chinese. We show how this assumption can fail badly, leading to many out-of-vocabulary words and incompatible annotations. Hence in practice the strict separation of morphology and syntax in the Chinese language proves to be untenable. We present a unified dependency parsing approach for Chinese which takes unsegmented sentences as input and outputs both morphological and syntactic structures with a single model and algorithm. By removing the intermediate word segmentation, the unified parser no longer needs separate notions for words and phrases. Evaluation proves the effectiveness of the unified model and algorithm in parsing structures of words, phrases and sen- tences simultaneously. 1</p><p>5 0.057133075 <a title="122-tfidf-5" href="./emnlp-2012-Generative_Goal-Driven_User_Simulation_for_Dialog_Management.html">60 emnlp-2012-Generative Goal-Driven User Simulation for Dialog Management</a></p>
<p>Author: Aciel Eshky ; Ben Allison ; Mark Steedman</p><p>Abstract: User simulation is frequently used to train statistical dialog managers for task-oriented domains. At present, goal-driven simulators (those that have a persistent notion of what they wish to achieve in the dialog) require some task-specific engineering, making them impossible to evaluate intrinsically. Instead, they have been evaluated extrinsically by means of the dialog managers they are intended to train, leading to circularity of argument. In this paper, we propose the first fully generative goal-driven simulator that is fully induced from data, without hand-crafting or goal annotation. Our goals are latent, and take the form of topics in a topic model, clustering together semantically equivalent and phonetically confusable strings, implicitly modelling synonymy and speech recognition noise. We evaluate on two standard dialog resources, the Communicator and Let’s Go datasets, and demonstrate that our model has substantially better fit to held out data than competing approaches. We also show that features derived from our model allow significantly greater improvement over a baseline at distinguishing real from randomly permuted dialogs.</p><p>6 0.054770689 <a title="122-tfidf-6" href="./emnlp-2012-Concurrent_Acquisition_of_Word_Meaning_and_Lexical_Categories.html">29 emnlp-2012-Concurrent Acquisition of Word Meaning and Lexical Categories</a></p>
<p>7 0.051303867 <a title="122-tfidf-7" href="./emnlp-2012-Cross-Lingual_Language_Modeling_with_Syntactic_Reordering_for_Low-Resource_Speech_Recognition.html">31 emnlp-2012-Cross-Lingual Language Modeling with Syntactic Reordering for Low-Resource Speech Recognition</a></p>
<p>8 0.05019737 <a title="122-tfidf-8" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>9 0.04943138 <a title="122-tfidf-9" href="./emnlp-2012-Generalized_Higher-Order_Dependency_Parsing_with_Cube_Pruning.html">57 emnlp-2012-Generalized Higher-Order Dependency Parsing with Cube Pruning</a></p>
<p>10 0.049017023 <a title="122-tfidf-10" href="./emnlp-2012-Learning_Syntactic_Categories_Using_Paradigmatic_Representations_of_Word_Context.html">79 emnlp-2012-Learning Syntactic Categories Using Paradigmatic Representations of Word Context</a></p>
<p>11 0.048895236 <a title="122-tfidf-11" href="./emnlp-2012-A_Transition-Based_System_for_Joint_Part-of-Speech_Tagging_and_Labeled_Non-Projective_Dependency_Parsing.html">12 emnlp-2012-A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</a></p>
<p>12 0.048231725 <a title="122-tfidf-12" href="./emnlp-2012-A_Sequence_Labelling_Approach_to_Quote_Attribution.html">9 emnlp-2012-A Sequence Labelling Approach to Quote Attribution</a></p>
<p>13 0.047665562 <a title="122-tfidf-13" href="./emnlp-2012-A_Novel_Discriminative_Framework_for_Sentence-Level_Discourse_Analysis.html">7 emnlp-2012-A Novel Discriminative Framework for Sentence-Level Discourse Analysis</a></p>
<p>14 0.045120388 <a title="122-tfidf-14" href="./emnlp-2012-An_%22AI_readability%22_Formula_for_French_as_a_Foreign_Language.html">17 emnlp-2012-An "AI readability" Formula for French as a Foreign Language</a></p>
<p>15 0.044882752 <a title="122-tfidf-15" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<p>16 0.043214779 <a title="122-tfidf-16" href="./emnlp-2012-No_Noun_Phrase_Left_Behind%3A_Detecting_and_Typing_Unlinkable_Entities.html">98 emnlp-2012-No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</a></p>
<p>17 0.042231385 <a title="122-tfidf-17" href="./emnlp-2012-Syntactic_Transfer_Using_a_Bilingual_Lexicon.html">123 emnlp-2012-Syntactic Transfer Using a Bilingual Lexicon</a></p>
<p>18 0.042164415 <a title="122-tfidf-18" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>19 0.041656714 <a title="122-tfidf-19" href="./emnlp-2012-Learning_to_Map_into_a_Universal_POS_Tagset.html">81 emnlp-2012-Learning to Map into a Universal POS Tagset</a></p>
<p>20 0.040061958 <a title="122-tfidf-20" href="./emnlp-2012-Improving_NLP_through_Marginalization_of_Hidden_Syntactic_Structure.html">65 emnlp-2012-Improving NLP through Marginalization of Hidden Syntactic Structure</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.149), (1, -0.02), (2, 0.035), (3, -0.008), (4, 0.015), (5, 0.057), (6, -0.084), (7, -0.007), (8, -0.015), (9, -0.027), (10, -0.113), (11, -0.026), (12, 0.109), (13, -0.123), (14, 0.023), (15, -0.103), (16, 0.071), (17, 0.059), (18, 0.062), (19, 0.041), (20, -0.016), (21, -0.02), (22, -0.005), (23, -0.048), (24, 0.033), (25, -0.026), (26, -0.048), (27, -0.017), (28, -0.029), (29, -0.168), (30, -0.141), (31, 0.001), (32, -0.106), (33, 0.111), (34, -0.024), (35, -0.064), (36, 0.097), (37, -0.191), (38, 0.058), (39, -0.084), (40, -0.066), (41, -0.049), (42, -0.044), (43, -0.072), (44, 0.109), (45, -0.008), (46, -0.214), (47, 0.067), (48, -0.152), (49, -0.114)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92661452 <a title="122-lsi-1" href="./emnlp-2012-Syntactic_Surprisal_Affects_Spoken_Word_Duration_in_Conversational_Contexts.html">122 emnlp-2012-Syntactic Surprisal Affects Spoken Word Duration in Conversational Contexts</a></p>
<p>Author: Vera Demberg ; Asad Sayeed ; Philip Gorinski ; Nikolaos Engonopoulos</p><p>Abstract: We present results of a novel experiment to investigate speech production in conversational data that links speech rate to information density. We provide the first evidence for an association between syntactic surprisal and word duration in recorded speech. Using the AMI corpus which contains transcriptions of focus group meetings with precise word durations, we show that word durations correlate with syntactic surprisal estimated from the incremental Roark parser over and above simpler measures, such as word duration estimated from a state-of-the-art text-to-speech system and word frequencies, and that the syntactic surprisal estimates are better predictors of word durations than a simpler version of surprisal based on trigram probabilities. This result supports the uniform information density (UID) hypothesis and points a way to more realistic artificial speech generation.</p><p>2 0.52243984 <a title="122-lsi-2" href="./emnlp-2012-Assessment_of_ESL_Learners%27_Syntactic_Competence_Based_on_Similarity_Measures.html">21 emnlp-2012-Assessment of ESL Learners' Syntactic Competence Based on Similarity Measures</a></p>
<p>Author: Su-Youn Yoon ; Suma Bhat</p><p>Abstract: This study presents a novel method that measures English language learners’ syntactic competence towards improving automated speech scoring systems. In contrast to most previous studies which focus on the length of production units such as the mean length of clauses, we focused on capturing the differences in the distribution of morpho-syntactic features or grammatical expressions across proficiency. We estimated the syntactic competence through the use of corpus-based NLP techniques. Assuming that the range and so- phistication of grammatical expressions can be captured by the distribution of Part-ofSpeech (POS) tags, vector space models of POS tags were constructed. We use a large corpus of English learners’ responses that are classified into four proficiency levels by human raters. Our proposed feature measures the similarity of a given response with the most proficient group and is then estimates the learner’s syntactic competence level. Widely outperforming the state-of-the-art measures of syntactic complexity, our method attained a significant correlation with humanrated scores. The correlation between humanrated scores and features based on manual transcription was 0.43 and the same based on ASR-hypothesis was slightly lower, 0.42. An important advantage of our method is its robustness against speech recognition errors not to mention the simplicity of feature generation that captures a reasonable set of learnerspecific syntactic errors. 600 Measures Suma Bhat Beckman Institute, Urbana, IL 61801 . spbhat 2 @ i l l ino i edu s</p><p>3 0.5052076 <a title="122-lsi-3" href="./emnlp-2012-A_Sequence_Labelling_Approach_to_Quote_Attribution.html">9 emnlp-2012-A Sequence Labelling Approach to Quote Attribution</a></p>
<p>Author: Timothy O'Keefe ; Silvia Pareti ; James R. Curran ; Irena Koprinska ; Matthew Honnibal</p><p>Abstract: Quote extraction and attribution is the task of automatically extracting quotes from text and attributing each quote to its correct speaker. The present state-of-the-art system uses gold standard information from previous decisions in its features, which, when removed, results in a large drop in performance. We treat the problem as a sequence labelling task, which allows us to incorporate sequence features without using gold standard information. We present results on two new corpora and an augmented version of a third, achieving a new state-of-the-art for systems using only realistic features.</p><p>4 0.43177998 <a title="122-lsi-4" href="./emnlp-2012-Improving_NLP_through_Marginalization_of_Hidden_Syntactic_Structure.html">65 emnlp-2012-Improving NLP through Marginalization of Hidden Syntactic Structure</a></p>
<p>Author: Jason Naradowsky ; Sebastian Riedel ; David Smith</p><p>Abstract: Many NLP tasks make predictions that are inherently coupled to syntactic relations, but for many languages the resources required to provide such syntactic annotations are unavailable. For others it is unclear exactly how much of the syntactic annotations can be effectively leveraged with current models, and what structures in the syntactic trees are most relevant to the current task. We propose a novel method which avoids the need for any syntactically annotated data when predicting a related NLP task. Our method couples latent syntactic representations, constrained to form valid dependency graphs or constituency parses, with the prediction task via specialized factors in a Markov random field. At both training and test time we marginalize over this hidden structure, learning the optimal latent representations for the problem. Results show that this approach provides significant gains over a syntactically uninformed baseline, outperforming models that observe syntax on an English relation extraction task, and performing comparably to them in semantic role labeling.</p><p>5 0.39925778 <a title="122-lsi-5" href="./emnlp-2012-Grounded_Models_of_Semantic_Representation.html">61 emnlp-2012-Grounded Models of Semantic Representation</a></p>
<p>Author: Carina Silberer ; Mirella Lapata</p><p>Abstract: A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two.</p><p>6 0.39784417 <a title="122-lsi-6" href="./emnlp-2012-Concurrent_Acquisition_of_Word_Meaning_and_Lexical_Categories.html">29 emnlp-2012-Concurrent Acquisition of Word Meaning and Lexical Categories</a></p>
<p>7 0.37675139 <a title="122-lsi-7" href="./emnlp-2012-Optimising_Incremental_Dialogue_Decisions_Using_Information_Density_for_Interactive_Systems.html">102 emnlp-2012-Optimising Incremental Dialogue Decisions Using Information Density for Interactive Systems</a></p>
<p>8 0.36865738 <a title="122-lsi-8" href="./emnlp-2012-An_%22AI_readability%22_Formula_for_French_as_a_Foreign_Language.html">17 emnlp-2012-An "AI readability" Formula for French as a Foreign Language</a></p>
<p>9 0.34647962 <a title="122-lsi-9" href="./emnlp-2012-Generative_Goal-Driven_User_Simulation_for_Dialog_Management.html">60 emnlp-2012-Generative Goal-Driven User Simulation for Dialog Management</a></p>
<p>10 0.34453285 <a title="122-lsi-10" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<p>11 0.34088594 <a title="122-lsi-11" href="./emnlp-2012-Learning_Syntactic_Categories_Using_Paradigmatic_Representations_of_Word_Context.html">79 emnlp-2012-Learning Syntactic Categories Using Paradigmatic Representations of Word Context</a></p>
<p>12 0.31509525 <a title="122-lsi-12" href="./emnlp-2012-Cross-Lingual_Language_Modeling_with_Syntactic_Reordering_for_Low-Resource_Speech_Recognition.html">31 emnlp-2012-Cross-Lingual Language Modeling with Syntactic Reordering for Low-Resource Speech Recognition</a></p>
<p>13 0.31164929 <a title="122-lsi-13" href="./emnlp-2012-Language_Model_Rest_Costs_and_Space-Efficient_Storage.html">74 emnlp-2012-Language Model Rest Costs and Space-Efficient Storage</a></p>
<p>14 0.30462646 <a title="122-lsi-14" href="./emnlp-2012-Revisiting_the_Predictability_of_Language%3A_Response_Completion_in_Social_Media.html">114 emnlp-2012-Revisiting the Predictability of Language: Response Completion in Social Media</a></p>
<p>15 0.27890077 <a title="122-lsi-15" href="./emnlp-2012-Minimal_Dependency_Length_in_Realization_Ranking.html">88 emnlp-2012-Minimal Dependency Length in Realization Ranking</a></p>
<p>16 0.26666573 <a title="122-lsi-16" href="./emnlp-2012-Lexical_Differences_in_Autobiographical_Narratives_from_Schizophrenic_Patients_and_Healthy_Controls.html">83 emnlp-2012-Lexical Differences in Autobiographical Narratives from Schizophrenic Patients and Healthy Controls</a></p>
<p>17 0.25834638 <a title="122-lsi-17" href="./emnlp-2012-Unified_Dependency_Parsing_of_Chinese_Morphological_and_Syntactic_Structures.html">131 emnlp-2012-Unified Dependency Parsing of Chinese Morphological and Syntactic Structures</a></p>
<p>18 0.25124571 <a title="122-lsi-18" href="./emnlp-2012-Spectral_Dependency_Parsing_with_Latent_Variables.html">119 emnlp-2012-Spectral Dependency Parsing with Latent Variables</a></p>
<p>19 0.2440937 <a title="122-lsi-19" href="./emnlp-2012-Training_Factored_PCFGs_with_Expectation_Propagation.html">126 emnlp-2012-Training Factored PCFGs with Expectation Propagation</a></p>
<p>20 0.23886447 <a title="122-lsi-20" href="./emnlp-2012-Weakly_Supervised_Training_of_Semantic_Parsers.html">136 emnlp-2012-Weakly Supervised Training of Semantic Parsers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(9, 0.033), (21, 0.039), (30, 0.027), (31, 0.035), (38, 0.02), (43, 0.436), (48, 0.04), (55, 0.045), (59, 0.045), (61, 0.09), (80, 0.019), (82, 0.021), (85, 0.041), (91, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.79235029 <a title="122-lda-1" href="./emnlp-2012-Lexical_Differences_in_Autobiographical_Narratives_from_Schizophrenic_Patients_and_Healthy_Controls.html">83 emnlp-2012-Lexical Differences in Autobiographical Narratives from Schizophrenic Patients and Healthy Controls</a></p>
<p>Author: Kai Hong ; Christian G. Kohler ; Mary E. March ; Amber A. Parker ; Ani Nenkova</p><p>Abstract: We present a system for automatic identification of schizophrenic patients and healthy controls based on narratives the subjects recounted about emotional experiences in their own life. The focus of the study is to identify the lexical features that distinguish the two populations. We report the results of feature selection experiments that demonstrate that the classifier can achieve accuracy on patient level prediction as high as 76.9% with only a small set of features. We provide an in-depth discussion of the lexical features that distinguish the two groups and the unexpected relationship between emotion types of the narratives and the accuracy of patient status prediction.</p><p>same-paper 2 0.74295503 <a title="122-lda-2" href="./emnlp-2012-Syntactic_Surprisal_Affects_Spoken_Word_Duration_in_Conversational_Contexts.html">122 emnlp-2012-Syntactic Surprisal Affects Spoken Word Duration in Conversational Contexts</a></p>
<p>Author: Vera Demberg ; Asad Sayeed ; Philip Gorinski ; Nikolaos Engonopoulos</p><p>Abstract: We present results of a novel experiment to investigate speech production in conversational data that links speech rate to information density. We provide the first evidence for an association between syntactic surprisal and word duration in recorded speech. Using the AMI corpus which contains transcriptions of focus group meetings with precise word durations, we show that word durations correlate with syntactic surprisal estimated from the incremental Roark parser over and above simpler measures, such as word duration estimated from a state-of-the-art text-to-speech system and word frequencies, and that the syntactic surprisal estimates are better predictors of word durations than a simpler version of surprisal based on trigram probabilities. This result supports the uniform information density (UID) hypothesis and points a way to more realistic artificial speech generation.</p><p>3 0.50667691 <a title="122-lda-3" href="./emnlp-2012-Entropy-based_Pruning_for_Phrase-based_Machine_Translation.html">42 emnlp-2012-Entropy-based Pruning for Phrase-based Machine Translation</a></p>
<p>Author: Wang Ling ; Joao Graca ; Isabel Trancoso ; Alan Black</p><p>Abstract: Phrase-based machine translation models have shown to yield better translations than Word-based models, since phrase pairs encode the contextual information that is needed for a more accurate translation. However, many phrase pairs do not encode any relevant context, which means that the translation event encoded in that phrase pair is led by smaller translation events that are independent from each other, and can be found on smaller phrase pairs, with little or no loss in translation accuracy. In this work, we propose a relative entropy model for translation models, that measures how likely a phrase pair encodes a translation event that is derivable using smaller translation events with similar probabilities. This model is then applied to phrase table pruning. Tests show that considerable amounts of phrase pairs can be excluded, without much impact on the transla- . tion quality. In fact, we show that better translations can be obtained using our pruned models, due to the compression of the search space during decoding.</p><p>4 0.34888357 <a title="122-lda-4" href="./emnlp-2012-Assessment_of_ESL_Learners%27_Syntactic_Competence_Based_on_Similarity_Measures.html">21 emnlp-2012-Assessment of ESL Learners' Syntactic Competence Based on Similarity Measures</a></p>
<p>Author: Su-Youn Yoon ; Suma Bhat</p><p>Abstract: This study presents a novel method that measures English language learners’ syntactic competence towards improving automated speech scoring systems. In contrast to most previous studies which focus on the length of production units such as the mean length of clauses, we focused on capturing the differences in the distribution of morpho-syntactic features or grammatical expressions across proficiency. We estimated the syntactic competence through the use of corpus-based NLP techniques. Assuming that the range and so- phistication of grammatical expressions can be captured by the distribution of Part-ofSpeech (POS) tags, vector space models of POS tags were constructed. We use a large corpus of English learners’ responses that are classified into four proficiency levels by human raters. Our proposed feature measures the similarity of a given response with the most proficient group and is then estimates the learner’s syntactic competence level. Widely outperforming the state-of-the-art measures of syntactic complexity, our method attained a significant correlation with humanrated scores. The correlation between humanrated scores and features based on manual transcription was 0.43 and the same based on ASR-hypothesis was slightly lower, 0.42. An important advantage of our method is its robustness against speech recognition errors not to mention the simplicity of feature generation that captures a reasonable set of learnerspecific syntactic errors. 600 Measures Suma Bhat Beckman Institute, Urbana, IL 61801 . spbhat 2 @ i l l ino i edu s</p><p>5 0.3409428 <a title="122-lda-5" href="./emnlp-2012-Optimising_Incremental_Dialogue_Decisions_Using_Information_Density_for_Interactive_Systems.html">102 emnlp-2012-Optimising Incremental Dialogue Decisions Using Information Density for Interactive Systems</a></p>
<p>Author: Nina Dethlefs ; Helen Hastie ; Verena Rieser ; Oliver Lemon</p><p>Abstract: Incremental processing allows system designers to address several discourse phenomena that have previously been somewhat neglected in interactive systems, such as backchannels or barge-ins, but that can enhance the responsiveness and naturalness of systems. Unfortunately, prior work has focused largely on deterministic incremental decision making, rendering system behaviour less flexible and adaptive than is desirable. We present a novel approach to incremental decision making that is based on Hierarchical Reinforcement Learning to achieve an interactive optimisation of Information Presentation (IP) strategies, allowing the system to generate and comprehend backchannels and barge-ins, by employing the recent psycholinguistic hypothesis of information density (ID) (Jaeger, 2010). Results in terms of average rewards and a human rating study show that our learnt strategy outperforms several baselines that are | v not sensitive to ID by more than 23%.</p><p>6 0.33832708 <a title="122-lda-6" href="./emnlp-2012-Lyrics%2C_Music%2C_and_Emotions.html">87 emnlp-2012-Lyrics, Music, and Emotions</a></p>
<p>7 0.31634328 <a title="122-lda-7" href="./emnlp-2012-A_Sequence_Labelling_Approach_to_Quote_Attribution.html">9 emnlp-2012-A Sequence Labelling Approach to Quote Attribution</a></p>
<p>8 0.31429532 <a title="122-lda-8" href="./emnlp-2012-PATTY%3A_A_Taxonomy_of_Relational_Patterns_with_Semantic_Types.html">103 emnlp-2012-PATTY: A Taxonomy of Relational Patterns with Semantic Types</a></p>
<p>9 0.31318906 <a title="122-lda-9" href="./emnlp-2012-Word_Salad%3A_Relating_Food_Prices_and_Descriptions.html">139 emnlp-2012-Word Salad: Relating Food Prices and Descriptions</a></p>
<p>10 0.30496821 <a title="122-lda-10" href="./emnlp-2012-Natural_Language_Questions_for_the_Web_of_Data.html">97 emnlp-2012-Natural Language Questions for the Web of Data</a></p>
<p>11 0.30083275 <a title="122-lda-11" href="./emnlp-2012-Exploring_Adaptor_Grammars_for_Native_Language_Identification.html">48 emnlp-2012-Exploring Adaptor Grammars for Native Language Identification</a></p>
<p>12 0.29828048 <a title="122-lda-12" href="./emnlp-2012-Framework_of_Automatic_Text_Summarization_Using_Reinforcement_Learning.html">56 emnlp-2012-Framework of Automatic Text Summarization Using Reinforcement Learning</a></p>
<p>13 0.2975038 <a title="122-lda-13" href="./emnlp-2012-A_Novel_Discriminative_Framework_for_Sentence-Level_Discourse_Analysis.html">7 emnlp-2012-A Novel Discriminative Framework for Sentence-Level Discourse Analysis</a></p>
<p>14 0.29643998 <a title="122-lda-14" href="./emnlp-2012-Do_Neighbours_Help%3F_An_Exploration_of_Graph-based_Algorithms_for_Cross-domain_Sentiment_Classification.html">34 emnlp-2012-Do Neighbours Help? An Exploration of Graph-based Algorithms for Cross-domain Sentiment Classification</a></p>
<p>15 0.29595503 <a title="122-lda-15" href="./emnlp-2012-Detecting_Subgroups_in_Online_Discussions_by_Modeling_Positive_and_Negative_Relations_among_Participants.html">32 emnlp-2012-Detecting Subgroups in Online Discussions by Modeling Positive and Negative Relations among Participants</a></p>
<p>16 0.29583153 <a title="122-lda-16" href="./emnlp-2012-Biased_Representation_Learning_for_Domain_Adaptation.html">24 emnlp-2012-Biased Representation Learning for Domain Adaptation</a></p>
<p>17 0.2948311 <a title="122-lda-17" href="./emnlp-2012-Generating_Non-Projective_Word_Order_in_Statistical_Linearization.html">59 emnlp-2012-Generating Non-Projective Word Order in Statistical Linearization</a></p>
<p>18 0.29477379 <a title="122-lda-18" href="./emnlp-2012-Forest_Reranking_through_Subtree_Ranking.html">55 emnlp-2012-Forest Reranking through Subtree Ranking</a></p>
<p>19 0.29257634 <a title="122-lda-19" href="./emnlp-2012-Multi-Domain_Learning%3A_When_Do_Domains_Matter%3F.html">92 emnlp-2012-Multi-Domain Learning: When Do Domains Matter?</a></p>
<p>20 0.29236317 <a title="122-lda-20" href="./emnlp-2012-A_Coherence_Model_Based_on_Syntactic_Patterns.html">3 emnlp-2012-A Coherence Model Based on Syntactic Patterns</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
