<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>102 emnlp-2013-Improving Learning and Inference in a Large Knowledge-Base using Latent Syntactic Cues</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-102" href="#">emnlp2013-102</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>102 emnlp-2013-Improving Learning and Inference in a Large Knowledge-Base using Latent Syntactic Cues</h1>
<br/><p>Source: <a title="emnlp-2013-102-pdf" href="http://aclweb.org/anthology//D/D13/D13-1080.pdf">pdf</a></p><p>Author: Matt Gardner ; Partha Pratim Talukdar ; Bryan Kisiel ; Tom Mitchell</p><p>Abstract: Automatically constructed Knowledge Bases (KBs) are often incomplete and there is a genuine need to improve their coverage. Path Ranking Algorithm (PRA) is a recently proposed method which aims to improve KB coverage by performing inference directly over the KB graph. For the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRAbased approaches on the KB inference task. We present extensive experimental results validating this finding. The resources presented in this paper are publicly available.</p><p>Reference: <a title="emnlp-2013-102-reference" href="../emnlp2013_reference/emnlp-2013-Improving_Learning_and_Inference_in_a_Large_Knowledge-Base_using_Latent_Syntactic_Cues_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract Automatically constructed Knowledge Bases (KBs) are often incomplete and there is a genuine need to improve their coverage. [sent-2, score-0.06]
</p><p>2 Path Ranking Algorithm (PRA) is a recently proposed method which aims to improve KB coverage by performing inference directly over the KB graph. [sent-3, score-0.144]
</p><p>3 For the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRAbased approaches on the KB inference task. [sent-4, score-0.54]
</p><p>4 1 Introduction Over the last few years, several large scale Knowl-  edge Bases (KBs) such as Freebase (Bollacker et al. [sent-7, score-0.19]
</p><p>5 Unfortunately, these KBs are often incomplete and there is a need to increase their coverage of facts to make them useful in practical applications. [sent-14, score-0.181]
</p><p>6 A strategy to increase coverage might be to perform inference directly over the KB represented as a graph. [sent-15, score-0.179]
</p><p>7 For example, if the KB contained the following facts, (Tiger Woods, participatesIn, PGA Tour)) and (Golf, sportOfTournament, PGA Tour), then by putting these two facts together, we could potentially infer that (Tiger Woods, playsSport, Golf). [sent-16, score-0.101]
</p><p>8 edu l}@  Figure 1: Example demonstrating how lexicalized syntactic edges can improve connectivity in the KB enabling PRA (Lao and Cohen, 2010) to discover relationships between Alex Rodriguez and World Series. [sent-20, score-0.318]
</p><p>9 Edges with latent labels can improve inference performance by reducing data sparsity. [sent-21, score-0.259]
</p><p>10 recently proposed Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) performs such inference by automatically learning semantic inference rules over the KB (Lao et al. [sent-24, score-0.188]
</p><p>11 PRA uses features based off of sequences of edge types, e. [sent-26, score-0.19]
</p><p>12 , 2012) to perform inference over a KB augmented with dependency parsed sentences. [sent-30, score-0.296]
</p><p>13 While this opens up the possibility of learning syntactic-semantic inference rules, the set of syntactic edge labels used are just the unlexicalized dependency role labels (e. [sent-31, score-0.551]
</p><p>14 , without the corresponding words), thereby limiting overall expressitivity of the learned inference rules. [sent-34, score-0.126]
</p><p>15 To overcome this limitation, in this paper we augment the KB graph by adding edges with more expressive lexicalized syntactic labels  (where the labels are words instead of dependen833  ProceSe datintlges, o Wfa tsh ein 2g01to3n, C UoSnfAe,re 1n8c-e2 o1n O Ecmtopbier ic 2a0l1 M3. [sent-35, score-0.528]
</p><p>16 , (Alex Rodriguez, “plays for”, NY Yankees), are mined by extracting 600 million Subject-Verb-Object (SVO) triples from a large corpus of 500m dependency parsed documents, which would have been prohibitively expensive to add directly as in (Lao et al. [sent-40, score-0.281]
</p><p>17 In order to overcome the explosion of path features and data sparsity, we derive edge labels by learning latent embeddings of the lexicalized edges. [sent-42, score-0.794]
</p><p>18 Through extensive experiments on real world datasets, we demonstrate effectiveness of the proposed approach. [sent-43, score-0.06]
</p><p>19 1 Motivating Example In Figure 1, the KB graph (only solid edges) is disconnected, thereby making it impossible for PRA to discover any relationship between Alex Rodriguez and World Series. [sent-45, score-0.136]
</p><p>20 However, addition of the two edges with SVO-based lexicalized syntactic edges (e. [sent-46, score-0.397]
</p><p>21 , (Alex Rodriguez, plays for, NY Yankees)) restores this inference possibility. [sent-48, score-0.161]
</p><p>22 For example, PRA might use the edge sequence h “plays for”, team-  PlaysIni as heevi deedngece feoqru predicting ythse roerl”at,io tena min-sPtalanycseI (Alex Rodriguez, athleteWonChampionship, World Series). [sent-49, score-0.19]
</p><p>23 Unfortunately, such na¨ ıve addition of lexicalized edges may result in significant data sparsity, which can be overcome by mapping lexicalized edge labels to some latent embedding (e. [sent-50, score-0.76]
</p><p>24 , (Alex Rodriguez, LatentFeat#5, NY Yankees) and running PRA over this augmented graph. [sent-52, score-0.058]
</p><p>25 Using latent embeddings, PRA could then use the following edge sequence as a feature in its prediction models: hLatentFeat#5, teamPlaysIni . [sent-53, score-0.279]
</p><p>26 We find this strategy htoL ab ete very et#ff5ec,t teivaem as ldaeysscIrniib. [sent-54, score-0.063]
</p><p>27 t 2  Related Work  There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al. [sent-56, score-0.101]
</p><p>28 Syntactic information in the form of dependency paths have been explored in (Snow et al. [sent-58, score-0.086]
</p><p>29 A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al. [sent-61, score-0.142]
</p><p>30 834 Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al. [sent-64, score-0.094]
</p><p>31 , 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text. [sent-67, score-0.221]
</p><p>32 In contrast to these previous PRA-based approaches where all edge labels are either KB labels or at surface-level, in this paper we explore using latent edge labels in addition to surface-level labels in the graph over which PRA is applied. [sent-68, score-0.846]
</p><p>33 In particular, we focus on the problem of performing inference over a large KB and learn latent edge labels by mining dependency syntax statistics from a large text corpus. [sent-69, score-0.5]
</p><p>34 Though we use Principal Components Analysis (PCA) for dimensionality reduction for the experiments in this paper, this is by no means the only choice. [sent-70, score-0.037]
</p><p>35 Various other dimensionality reduction techniques, and in particular, other verb clustering tech-  niques (Korhonen et al. [sent-71, score-0.037]
</p><p>36 , 2011) also extract verb-anchored dependency triples from large text corpus. [sent-74, score-0.115]
</p><p>37 In contrast to such approaches, we focus on how latent embedding of verbs in such triples can be combined with explicit background knowledge to improve coverage of existing KBs. [sent-75, score-0.285]
</p><p>38 This has the added capability of inferring facts which are not explicitly mentioned in text. [sent-76, score-0.141]
</p><p>39 , 2013) also demonstrates the benefit of using latent features for increasing coverage of KBs. [sent-78, score-0.139]
</p><p>40 Let G = (V, E, T) be the graph, where V is the set of vertices, E is the set of edges, and T is the set of edge types. [sent-84, score-0.19]
</p><p>41 For each edge (v1, t,v2) ∈ E, we have v1, v2 ∈ V and t ∈ T. [sent-85, score-0.19]
</p><p>42 Let R ⊂ T be the set of types predicted by nPdR tA ∈. [sent-86, score-0.034]
</p><p>43 c Louetld R Rin ⊂ principal equal T, pb uest in this paper we restrict prediction to KB relations, while T also includes types derived from surface text and latent embeddings. [sent-89, score-0.169]
</p><p>44 ,twi be a path type mofb length w over graph G, where ti ∈ bTe is the type of the ith edge in the path. [sent-93, score-0.526]
</p><p>45 Each s∈uc Th path type is also a feature in the PRA model. [sent-94, score-0.233]
</p><p>46 For a given source and target node pair s, t ∈ V , let P(s → t; π) eb e a nthde tvaarglueet o nfo dthee pfaeiartu sr,et π specifying sth →e probability eof v reaching neo fdeea t u starting fcriofmynode s and following a path constrained by path type π. [sent-95, score-0.642]
</p><p>47 A value of 0 indicates unreachability from s to t using path type π. [sent-97, score-0.233]
</p><p>48 {Tπhe score tha}t b reela thteio nse r ohfold alsl b feetawtuereens node s and node t is given by the following function: ScorePRA(s,t,r)  =  XP(s → t;π) Xπ∈B  θπr  where θπr is the weight of feature π in class r ∈ R. [sent-102, score-0.137]
</p><p>49 Featureis S thelee wcteioignh: oTfh fee stuetr eB π o inf possible path types grows exponentially in the length of the paths that are considered. [sent-103, score-0.3]
</p><p>50 In order to have a manageable set of features to compute, we first perform a feature selection step. [sent-104, score-0.03]
</p><p>51 The goal of this step is to select for computation only those path types that commonly connect sources and targets of relation r. [sent-105, score-0.237]
</p><p>52 We perform this feature selection by doing length-bounded random walks from a given list of source and target nodes, keeping track of how frequently each path type leads from a source node to a target node. [sent-106, score-0.27]
</p><p>53 The most common m path types are selected for the set B. [sent-107, score-0.237]
</p><p>54 We follow the strategy in (Lao and Cohen, 2010) to generate positive and negative training instances. [sent-109, score-0.035]
</p><p>55 2  PRAsyntactic  In this section, we shall extend the knowledge graph G = (V, E, T) from the previous section with an augmented graph = (V, , where E ⊂ and T ⊂ with the set of vertice)s, unchanged. [sent-111, score-0.247]
</p><p>56 T0,  G0  E0 T0),  E0  E0  Idn T o ⊂rde Tr to get the edges in − E, we first collect a set of Subject-Verb-Object (SVO) triples D = {(s, v, o, c)} from a large dependency parsed 835 text corpus, with c ∈ R+ denoting the frequency otefx tth ciso triple i nth hth ce corpus. [sent-112, score-0.376]
</p><p>57 The additional edge set is then defined as Esyntactic = −E = {(s, v, o) | ∃(s, v, o, c) ∈ D, s, o ∈ V }. [sent-113, score-0.19]
</p><p>58 | ∃I(ns ,ovth,eor) words, for eac}h pair oeft directly con∪nec Ste. [sent-115, score-0.033]
</p><p>59 d Inno odtehs einr twhoer KdsB, graph cGh, we rad odf an additional edge between those two nodes for each verb which takes the NPs represented by two nodes as subjects and objects (or vice versa) as observed in a text corpus. [sent-116, score-0.291]
</p><p>60 PRA is then applied over this augmented graph over the same set of prediction types R as before. [sent-118, score-0.165]
</p><p>61 We shall refer to this version of PRA as PRAsyntactic. [sent-119, score-0.043]
</p><p>62 For the experiments in this paper, we collected |D| = 600 million SVO triples1 from the ecnoltilreec Cedlu |eDW|eb = corpus (Callan VeOt al. [sent-120, score-0.029]
</p><p>63 , 2009), parsed using the Malt parser (Nivre et al. [sent-121, score-0.093]
</p><p>64 3 PRAlatent In this section we construct = (V, E00, another syntactic-information-induced extension of the knowledge graph G, but instead of using the surface forms of verbs in S (see previous section) as edge types, we derive those edges types based on latent embeddings of those verbs. [sent-125, score-0.662]
</p><p>65 We note that E⊂ and T ⊂ In order to learn the latent or low dimensional embeddings of the verbs in S, we first define QS = {(s, o) | ∃(s, v, o, c) ∈ D, v ∈ S}, the set of subject-object tuples )in ∈ ∈D D w,h vich ∈ are }co,n tnheect seedt by at least one verb in S. [sent-126, score-0.296]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pra', 0.556), ('lao', 0.306), ('kb', 0.301), ('rodriguez', 0.229), ('path', 0.203), ('edge', 0.19), ('kbs', 0.164), ('edges', 0.14), ('yankees', 0.131), ('qs', 0.119), ('golf', 0.113), ('embeddings', 0.107), ('facts', 0.101), ('inference', 0.094), ('alex', 0.093), ('parsed', 0.093), ('svo', 0.09), ('latent', 0.089), ('woods', 0.083), ('lexicalized', 0.083), ('tiger', 0.079), ('labels', 0.076), ('esyntactic', 0.075), ('pga', 0.075), ('playssport', 0.075), ('tour', 0.075), ('graph', 0.073), ('cohen', 0.072), ('eb', 0.069), ('plays', 0.067), ('triples', 0.064), ('suchanek', 0.06), ('augmented', 0.058), ('embedding', 0.053), ('dependency', 0.051), ('coverage', 0.05), ('bases', 0.048), ('overcome', 0.046), ('principal', 0.046), ('ranking', 0.045), ('mined', 0.044), ('shall', 0.043), ('ny', 0.041), ('capability', 0.04), ('dimensional', 0.038), ('node', 0.037), ('dimensionality', 0.037), ('etzioni', 0.036), ('paths', 0.035), ('riedel', 0.035), ('strategy', 0.035), ('syntactic', 0.034), ('let', 0.034), ('types', 0.034), ('rde', 0.033), ('nec', 0.033), ('malt', 0.033), ('reela', 0.033), ('dow', 0.033), ('openie', 0.033), ('bollacker', 0.033), ('eof', 0.033), ('forbes', 0.033), ('gravano', 0.033), ('inno', 0.033), ('nthde', 0.033), ('oeft', 0.033), ('seedt', 0.033), ('thereby', 0.032), ('world', 0.032), ('discover', 0.031), ('incomplete', 0.03), ('nse', 0.03), ('cies', 0.03), ('validating', 0.03), ('kisiel', 0.03), ('settles', 0.03), ('ravichandran', 0.03), ('manageable', 0.03), ('genuine', 0.03), ('pca', 0.03), ('hof', 0.03), ('callan', 0.03), ('connectivity', 0.03), ('eac', 0.03), ('opens', 0.03), ('pratim', 0.03), ('type', 0.03), ('verbs', 0.029), ('million', 0.029), ('extensive', 0.028), ('einr', 0.028), ('fee', 0.028), ('bte', 0.028), ('ciso', 0.028), ('ete', 0.028), ('dobj', 0.028), ('fin', 0.028), ('disconnected', 0.028), ('yago', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="102-tfidf-1" href="./emnlp-2013-Improving_Learning_and_Inference_in_a_Large_Knowledge-Base_using_Latent_Syntactic_Cues.html">102 emnlp-2013-Improving Learning and Inference in a Large Knowledge-Base using Latent Syntactic Cues</a></p>
<p>Author: Matt Gardner ; Partha Pratim Talukdar ; Bryan Kisiel ; Tom Mitchell</p><p>Abstract: Automatically constructed Knowledge Bases (KBs) are often incomplete and there is a genuine need to improve their coverage. Path Ranking Algorithm (PRA) is a recently proposed method which aims to improve KB coverage by performing inference directly over the KB graph. For the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRAbased approaches on the KB inference task. We present extensive experimental results validating this finding. The resources presented in this paper are publicly available.</p><p>2 0.17493026 <a title="102-tfidf-2" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>Author: Jason Weston ; Antoine Bordes ; Oksana Yakhnenko ; Nicolas Usunier</p><p>Abstract: This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over methods that rely on text features alone.</p><p>3 0.10474524 <a title="102-tfidf-3" href="./emnlp-2013-Dynamic_Feature_Selection_for_Dependency_Parsing.html">66 emnlp-2013-Dynamic Feature Selection for Dependency Parsing</a></p>
<p>Author: He He ; Hal Daume III ; Jason Eisner</p><p>Abstract: Feature computation and exhaustive search have significantly restricted the speed of graph-based dependency parsing. We propose a faster framework of dynamic feature selection, where features are added sequentially as needed, edges are pruned early, and decisions are made online for each sentence. We model this as a sequential decision-making problem and solve it by imitation learning techniques. We test our method on 7 languages. Our dynamic parser can achieve accuracies comparable or even superior to parsers using a full set of features, while computing fewer than 30% of the feature templates.</p><p>4 0.088990264 <a title="102-tfidf-4" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>Author: Will Y. Zou ; Richard Socher ; Daniel Cer ; Christopher D. Manning</p><p>Abstract: We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence. The new embeddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task.</p><p>5 0.086278498 <a title="102-tfidf-5" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>Author: Oier Lopez de Lacalle ; Mirella Lapata</p><p>Abstract: In this paper we present an unsupervised approach to relational information extraction. Our model partitions tuples representing an observed syntactic relationship between two named entities (e.g., “X was born in Y” and “X is from Y”) into clusters corresponding to underlying semantic relation types (e.g., BornIn, Located). Our approach incorporates general domain knowledge which we encode as First Order Logic rules and automatically combine with a topic model developed specifically for the relation extraction task. Evaluation results on the ACE 2007 English Relation Detection and Categorization (RDC) task show that our model outperforms competitive unsupervised approaches by a wide margin and is able to produce clusters shaped by both the data and the rules.</p><p>6 0.071441323 <a title="102-tfidf-6" href="./emnlp-2013-The_Topology_of_Semantic_Knowledge.html">182 emnlp-2013-The Topology of Semantic Knowledge</a></p>
<p>7 0.062958546 <a title="102-tfidf-7" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>8 0.055801112 <a title="102-tfidf-8" href="./emnlp-2013-Overcoming_the_Lack_of_Parallel_Data_in_Sentence_Compression.html">149 emnlp-2013-Overcoming the Lack of Parallel Data in Sentence Compression</a></p>
<p>9 0.054109961 <a title="102-tfidf-9" href="./emnlp-2013-Semantic_Parsing_on_Freebase_from_Question-Answer_Pairs.html">166 emnlp-2013-Semantic Parsing on Freebase from Question-Answer Pairs</a></p>
<p>10 0.052050944 <a title="102-tfidf-10" href="./emnlp-2013-Combining_Generative_and_Discriminative_Model_Scores_for_Distant_Supervision.html">49 emnlp-2013-Combining Generative and Discriminative Model Scores for Distant Supervision</a></p>
<p>11 0.051952604 <a title="102-tfidf-11" href="./emnlp-2013-Dependency_Language_Models_for_Sentence_Completion.html">58 emnlp-2013-Dependency Language Models for Sentence Completion</a></p>
<p>12 0.050980486 <a title="102-tfidf-12" href="./emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations.html">17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</a></p>
<p>13 0.048913423 <a title="102-tfidf-13" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>14 0.046020906 <a title="102-tfidf-14" href="./emnlp-2013-A_Constrained_Latent_Variable_Model_for_Coreference_Resolution.html">1 emnlp-2013-A Constrained Latent Variable Model for Coreference Resolution</a></p>
<p>15 0.045114607 <a title="102-tfidf-15" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>16 0.042328697 <a title="102-tfidf-16" href="./emnlp-2013-Open-Domain_Fine-Grained_Class_Extraction_from_Web_Search_Queries.html">142 emnlp-2013-Open-Domain Fine-Grained Class Extraction from Web Search Queries</a></p>
<p>17 0.038235601 <a title="102-tfidf-17" href="./emnlp-2013-Learning_Biological_Processes_with_Global_Constraints.html">118 emnlp-2013-Learning Biological Processes with Global Constraints</a></p>
<p>18 0.037994031 <a title="102-tfidf-18" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>19 0.036795061 <a title="102-tfidf-19" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>20 0.036315996 <a title="102-tfidf-20" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.135), (1, 0.031), (2, 0.015), (3, 0.029), (4, -0.013), (5, 0.06), (6, -0.004), (7, 0.03), (8, 0.026), (9, 0.032), (10, 0.096), (11, -0.079), (12, -0.078), (13, 0.013), (14, -0.114), (15, -0.054), (16, -0.067), (17, 0.108), (18, 0.037), (19, 0.077), (20, -0.014), (21, -0.072), (22, 0.143), (23, 0.185), (24, -0.204), (25, 0.068), (26, 0.087), (27, -0.031), (28, -0.215), (29, 0.025), (30, -0.005), (31, 0.019), (32, -0.169), (33, 0.12), (34, -0.128), (35, -0.107), (36, -0.006), (37, 0.079), (38, 0.064), (39, -0.019), (40, -0.034), (41, 0.017), (42, 0.221), (43, -0.071), (44, 0.09), (45, 0.034), (46, -0.131), (47, 0.029), (48, -0.046), (49, -0.075)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96209335 <a title="102-lsi-1" href="./emnlp-2013-Improving_Learning_and_Inference_in_a_Large_Knowledge-Base_using_Latent_Syntactic_Cues.html">102 emnlp-2013-Improving Learning and Inference in a Large Knowledge-Base using Latent Syntactic Cues</a></p>
<p>Author: Matt Gardner ; Partha Pratim Talukdar ; Bryan Kisiel ; Tom Mitchell</p><p>Abstract: Automatically constructed Knowledge Bases (KBs) are often incomplete and there is a genuine need to improve their coverage. Path Ranking Algorithm (PRA) is a recently proposed method which aims to improve KB coverage by performing inference directly over the KB graph. For the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRAbased approaches on the KB inference task. We present extensive experimental results validating this finding. The resources presented in this paper are publicly available.</p><p>2 0.66322178 <a title="102-lsi-2" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>Author: Jason Weston ; Antoine Bordes ; Oksana Yakhnenko ; Nicolas Usunier</p><p>Abstract: This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over methods that rely on text features alone.</p><p>3 0.52890027 <a title="102-lsi-3" href="./emnlp-2013-The_Topology_of_Semantic_Knowledge.html">182 emnlp-2013-The Topology of Semantic Knowledge</a></p>
<p>Author: Jimmy Dubuisson ; Jean-Pierre Eckmann ; Christian Scheible ; Hinrich Schutze</p><p>Abstract: Studies of the graph of dictionary definitions (DD) (Picard et al., 2009; Levary et al., 2012) have revealed strong semantic coherence of local topological structures. The techniques used in these papers are simple and the main results are found by understanding the structure of cycles in the directed graph (where words point to definitions). Based on our earlier work (Levary et al., 2012), we study a different class of word definitions, namely those of the Free Association (FA) dataset (Nelson et al., 2004). These are responses by subjects to a cue word, which are then summarized by a directed, free association graph. We find that the structure of this network is quite different from both the Wordnet and the dictionary networks. This difference can be explained by the very nature of free association as compared to the more “logical” construction of dictionaries. It thus sheds some (quantitative) light on the psychology of free association. In NLP, semantic groups or clusters are interesting for various applications such as word sense disambiguation. The FA graph is tighter than the DD graph, because of the large number of triangles. This also makes drift of meaning quite measurable so that FA graphs provide a quantitative measure of the semantic coherence of small groups of words.</p><p>4 0.43269578 <a title="102-lsi-4" href="./emnlp-2013-Dynamic_Feature_Selection_for_Dependency_Parsing.html">66 emnlp-2013-Dynamic Feature Selection for Dependency Parsing</a></p>
<p>Author: He He ; Hal Daume III ; Jason Eisner</p><p>Abstract: Feature computation and exhaustive search have significantly restricted the speed of graph-based dependency parsing. We propose a faster framework of dynamic feature selection, where features are added sequentially as needed, edges are pruned early, and decisions are made online for each sentence. We model this as a sequential decision-making problem and solve it by imitation learning techniques. We test our method on 7 languages. Our dynamic parser can achieve accuracies comparable or even superior to parsers using a full set of features, while computing fewer than 30% of the feature templates.</p><p>5 0.3789306 <a title="102-lsi-5" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>Author: Will Y. Zou ; Richard Socher ; Daniel Cer ; Christopher D. Manning</p><p>Abstract: We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence. The new embeddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task.</p><p>6 0.34842491 <a title="102-lsi-6" href="./emnlp-2013-Open-Domain_Fine-Grained_Class_Extraction_from_Web_Search_Queries.html">142 emnlp-2013-Open-Domain Fine-Grained Class Extraction from Web Search Queries</a></p>
<p>7 0.30780098 <a title="102-lsi-7" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>8 0.29435742 <a title="102-lsi-8" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>9 0.27861983 <a title="102-lsi-9" href="./emnlp-2013-With_Blinkers_on%3A_Robust_Prediction_of_Eye_Movements_across_Readers.html">203 emnlp-2013-With Blinkers on: Robust Prediction of Eye Movements across Readers</a></p>
<p>10 0.27598542 <a title="102-lsi-10" href="./emnlp-2013-This_Text_Has_the_Scent_of_Starbucks%3A_A_Laplacian_Structured_Sparsity_Model_for_Computational_Branding_Analytics.html">184 emnlp-2013-This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</a></p>
<p>11 0.26494342 <a title="102-lsi-11" href="./emnlp-2013-Effectiveness_and_Efficiency_of_Open_Relation_Extraction.html">68 emnlp-2013-Effectiveness and Efficiency of Open Relation Extraction</a></p>
<p>12 0.26344112 <a title="102-lsi-12" href="./emnlp-2013-Dependency_Language_Models_for_Sentence_Completion.html">58 emnlp-2013-Dependency Language Models for Sentence Completion</a></p>
<p>13 0.25795293 <a title="102-lsi-13" href="./emnlp-2013-Combining_Generative_and_Discriminative_Model_Scores_for_Distant_Supervision.html">49 emnlp-2013-Combining Generative and Discriminative Model Scores for Distant Supervision</a></p>
<p>14 0.24484293 <a title="102-lsi-14" href="./emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations.html">17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</a></p>
<p>15 0.21215262 <a title="102-lsi-15" href="./emnlp-2013-Overcoming_the_Lack_of_Parallel_Data_in_Sentence_Compression.html">149 emnlp-2013-Overcoming the Lack of Parallel Data in Sentence Compression</a></p>
<p>16 0.21213479 <a title="102-lsi-16" href="./emnlp-2013-Animacy_Detection_with_Voting_Models.html">23 emnlp-2013-Animacy Detection with Voting Models</a></p>
<p>17 0.2017771 <a title="102-lsi-17" href="./emnlp-2013-Measuring_Ideological_Proportions_in_Political_Speeches.html">129 emnlp-2013-Measuring Ideological Proportions in Political Speeches</a></p>
<p>18 0.2012381 <a title="102-lsi-18" href="./emnlp-2013-Modeling_Scientific_Impact_with_Topical_Influence_Regression.html">133 emnlp-2013-Modeling Scientific Impact with Topical Influence Regression</a></p>
<p>19 0.200333 <a title="102-lsi-19" href="./emnlp-2013-Building_Event_Threads_out_of_Multiple_News_Articles.html">41 emnlp-2013-Building Event Threads out of Multiple News Articles</a></p>
<p>20 0.19774339 <a title="102-lsi-20" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(9, 0.473), (18, 0.051), (22, 0.035), (30, 0.071), (50, 0.017), (51, 0.135), (66, 0.028), (71, 0.016), (75, 0.024), (77, 0.023), (90, 0.014), (95, 0.01), (96, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81278926 <a title="102-lda-1" href="./emnlp-2013-Improving_Learning_and_Inference_in_a_Large_Knowledge-Base_using_Latent_Syntactic_Cues.html">102 emnlp-2013-Improving Learning and Inference in a Large Knowledge-Base using Latent Syntactic Cues</a></p>
<p>Author: Matt Gardner ; Partha Pratim Talukdar ; Bryan Kisiel ; Tom Mitchell</p><p>Abstract: Automatically constructed Knowledge Bases (KBs) are often incomplete and there is a genuine need to improve their coverage. Path Ranking Algorithm (PRA) is a recently proposed method which aims to improve KB coverage by performing inference directly over the KB graph. For the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRAbased approaches on the KB inference task. We present extensive experimental results validating this finding. The resources presented in this paper are publicly available.</p><p>2 0.78075898 <a title="102-lda-2" href="./emnlp-2013-The_Topology_of_Semantic_Knowledge.html">182 emnlp-2013-The Topology of Semantic Knowledge</a></p>
<p>Author: Jimmy Dubuisson ; Jean-Pierre Eckmann ; Christian Scheible ; Hinrich Schutze</p><p>Abstract: Studies of the graph of dictionary definitions (DD) (Picard et al., 2009; Levary et al., 2012) have revealed strong semantic coherence of local topological structures. The techniques used in these papers are simple and the main results are found by understanding the structure of cycles in the directed graph (where words point to definitions). Based on our earlier work (Levary et al., 2012), we study a different class of word definitions, namely those of the Free Association (FA) dataset (Nelson et al., 2004). These are responses by subjects to a cue word, which are then summarized by a directed, free association graph. We find that the structure of this network is quite different from both the Wordnet and the dictionary networks. This difference can be explained by the very nature of free association as compared to the more “logical” construction of dictionaries. It thus sheds some (quantitative) light on the psychology of free association. In NLP, semantic groups or clusters are interesting for various applications such as word sense disambiguation. The FA graph is tighter than the DD graph, because of the large number of triangles. This also makes drift of meaning quite measurable so that FA graphs provide a quantitative measure of the semantic coherence of small groups of words.</p><p>3 0.7160607 <a title="102-lda-3" href="./emnlp-2013-Improvements_to_the_Bayesian_Topic_N-Gram_Models.html">100 emnlp-2013-Improvements to the Bayesian Topic N-Gram Models</a></p>
<p>Author: Hiroshi Noji ; Daichi Mochihashi ; Yusuke Miyao</p><p>Abstract: One of the language phenomena that n-gram language model fails to capture is the topic information of a given situation. We advance the previous study of the Bayesian topic language model by Wallach (2006) in two directions: one, investigating new priors to alleviate the sparseness problem caused by dividing all ngrams into exclusive topics, and two, developing a novel Gibbs sampler that enables moving multiple n-grams across different documents to another topic. Our blocked sampler can efficiently search for higher probability space even with higher order n-grams. In terms of modeling assumption, we found it is effective to assign a topic to only some parts of a document.</p><p>4 0.41009563 <a title="102-lda-4" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>Author: Jason Weston ; Antoine Bordes ; Oksana Yakhnenko ; Nicolas Usunier</p><p>Abstract: This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over methods that rely on text features alone.</p><p>5 0.4039748 <a title="102-lda-5" href="./emnlp-2013-Dynamic_Feature_Selection_for_Dependency_Parsing.html">66 emnlp-2013-Dynamic Feature Selection for Dependency Parsing</a></p>
<p>Author: He He ; Hal Daume III ; Jason Eisner</p><p>Abstract: Feature computation and exhaustive search have significantly restricted the speed of graph-based dependency parsing. We propose a faster framework of dynamic feature selection, where features are added sequentially as needed, edges are pruned early, and decisions are made online for each sentence. We model this as a sequential decision-making problem and solve it by imitation learning techniques. We test our method on 7 languages. Our dynamic parser can achieve accuracies comparable or even superior to parsers using a full set of features, while computing fewer than 30% of the feature templates.</p><p>6 0.3720893 <a title="102-lda-6" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>7 0.36837766 <a title="102-lda-7" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>8 0.36631078 <a title="102-lda-8" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>9 0.36596531 <a title="102-lda-9" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>10 0.36551833 <a title="102-lda-10" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>11 0.3649056 <a title="102-lda-11" href="./emnlp-2013-Modeling_Scientific_Impact_with_Topical_Influence_Regression.html">133 emnlp-2013-Modeling Scientific Impact with Topical Influence Regression</a></p>
<p>12 0.36316463 <a title="102-lda-12" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>13 0.36253226 <a title="102-lda-13" href="./emnlp-2013-Automatically_Determining_a_Proper_Length_for_Multi-Document_Summarization%3A_A_Bayesian_Nonparametric_Approach.html">36 emnlp-2013-Automatically Determining a Proper Length for Multi-Document Summarization: A Bayesian Nonparametric Approach</a></p>
<p>14 0.3600381 <a title="102-lda-14" href="./emnlp-2013-Measuring_Ideological_Proportions_in_Political_Speeches.html">129 emnlp-2013-Measuring Ideological Proportions in Political Speeches</a></p>
<p>15 0.35986051 <a title="102-lda-15" href="./emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM.html">99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</a></p>
<p>16 0.35594308 <a title="102-lda-16" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>17 0.35151738 <a title="102-lda-17" href="./emnlp-2013-Noise-Aware_Character_Alignment_for_Bootstrapping_Statistical_Machine_Transliteration_from_Bilingual_Corpora.html">139 emnlp-2013-Noise-Aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora</a></p>
<p>18 0.35013396 <a title="102-lda-18" href="./emnlp-2013-Overcoming_the_Lack_of_Parallel_Data_in_Sentence_Compression.html">149 emnlp-2013-Overcoming the Lack of Parallel Data in Sentence Compression</a></p>
<p>19 0.34934983 <a title="102-lda-19" href="./emnlp-2013-Joint_Learning_of_Phonetic_Units_and_Word_Pronunciations_for_ASR.html">115 emnlp-2013-Joint Learning of Phonetic Units and Word Pronunciations for ASR</a></p>
<p>20 0.34834504 <a title="102-lda-20" href="./emnlp-2013-Learning_Biological_Processes_with_Global_Constraints.html">118 emnlp-2013-Learning Biological Processes with Global Constraints</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
