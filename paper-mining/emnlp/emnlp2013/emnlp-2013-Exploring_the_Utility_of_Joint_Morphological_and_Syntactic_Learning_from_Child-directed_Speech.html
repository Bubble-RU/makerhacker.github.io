<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-83" href="#">emnlp2013-83</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</h1>
<br/><p>Source: <a title="emnlp-2013-83-pdf" href="http://aclweb.org/anthology//D/D13/D13-1004.pdf">pdf</a></p><p>Author: Stella Frank ; Frank Keller ; Sharon Goldwater</p><p>Abstract: Frank Keller keller@ inf .ed .ac .uk Sharon Goldwater sgwater@ inf .ed .ac .uk ILCC, School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK interactions are often (but not necessarily) synergisChildren learn various levels of linguistic structure concurrently, yet most existing models of language acquisition deal with only a single level of structure, implicitly assuming a sequential learning process. Developing models that learn multiple levels simultaneously can provide important insights into how these levels might interact synergistically dur- ing learning. Here, we present a model that jointly induces syntactic categories and morphological segmentations by combining two well-known models for the individual tasks. We test on child-directed utterances in English and Spanish and compare to single-task baselines. In the morphologically poorer language (English), the model improves morphological segmentation, while in the morphologically richer language (Spanish), it leads to better syntactic categorization. These results provide further evidence that joint learning is useful, but also suggest that the benefits may be different for typologically different languages.</p><p>Reference: <a title="emnlp-2013-83-reference" href="../emnlp2013_reference/emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Exploring the utility of joint morphological and syntactic learning from child-directed speech  Stella Frank s frank@ inf . [sent-1, score-0.425]
</p><p>2 Here, we present a model that jointly induces syntactic categories and morphological segmentations by combining two well-known models for the individual tasks. [sent-12, score-0.468]
</p><p>3 In the morphologically poorer language (English), the model improves morphological segmentation, while in the morphologically richer language (Spanish), it leads to better syntactic categorization. [sent-14, score-0.488]
</p><p>4 Recent models using joint learning to model language acquisition have spanned various domains including phonology, word segmentation, syntax and semantics (Feldman et al. [sent-21, score-0.146]
</p><p>5 In this paper we examine the joint learning of syntactic categories and morphology, which are acquired by children at roughly the same age (Clark, 2003b), implying possible interactions in the learning process. [sent-25, score-0.237]
</p><p>6 Both morphology and word order depend on categorising words based on their morphosyntactic function. [sent-26, score-0.458]
</p><p>7 However, previous models of syntactic category learning have relied principally on surrounding context, i. [sent-27, score-0.163]
</p><p>8 , word order constraints, whereas models of morphology use word-internal cues. [sent-29, score-0.458]
</p><p>9 Languages differ in the richness of their morphology and strictness of word order. [sent-31, score-0.505]
</p><p>10 These characteristics appear to be (anti)correlated, with rich morphology co-occurring with free word order and vice versa (Blake, 2001; McFadden, 2003). [sent-32, score-0.458]
</p><p>11 The timecourse of acquisition is also influenced by language typology: learners of morphologically rich languages become productive in morphology earlier (Xanthos et al. [sent-33, score-0.747]
</p><p>12 , 2011), suggesting that richer morphology may be more salient for learners than impoverished morphology. [sent-34, score-0.553]
</p><p>13 These interactions between morphology and word order suggest that a joint model will be better able to support the differences in cue strength (rich morphology versus strict word order),  and thus be more language-general, than single-task models. [sent-39, score-0.975]
</p><p>14 Both syntactic category and morphology induction have been the focus of much recent work. [sent-40, score-0.665]
</p><p>15 (See Hammarstr o¨m and Borin (201 1) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al. [sent-41, score-0.495]
</p><p>16 (2010) for a comparison of part of speech/syntactic category induction systems. [sent-42, score-0.136]
</p><p>17 ) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. [sent-43, score-0.588]
</p><p>18 Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al. [sent-44, score-0.171]
</p><p>19 , 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. [sent-48, score-0.788]
</p><p>20 Models of morphology induction generally operate over a lexicon, i. [sent-49, score-0.502]
</p><p>21 These models find morphological categories on the basis of wordinternal features, without taking syntactic context into account (which is of course not available in a lexicon). [sent-53, score-0.346]
</p><p>22 (201 1) and Sirts and Alum a¨e (2012) present models that infer morphological segmentations and syntactic categories jointly, although Lee et al. [sent-55, score-0.468]
</p><p>23 , all instances of ducks are assigned to a single category and will have the same morpheme analysis, ignoring the gold standard distinction between a plural noun and third person singular verb). [sent-59, score-0.242]
</p><p>24 This can make inference more tractable, and often increases performance, but does not respect the ambiguity in31 herent in natural language, both over syntactic categories and morphological analyses. [sent-60, score-0.346]
</p><p>25 , likelihood scaling (Sirts and Alum a¨e, 2012), sequential suffix matching (Lee et al. [sent-64, score-0.254]
</p><p>26 We test ourjoint model on child-directed utterances in English (a morphologically poor language) and Spanish (with richer morphology)1 . [sent-70, score-0.168]
</p><p>27 Our results indicate that our joint model is able to flexibly accommodate languages with differing levels of morphological richness. [sent-71, score-0.419]
</p><p>28 2  Model  The task is to assign word tokens to part of speech categories and simultaneously segment the tokens into morphemes. [sent-77, score-0.226]
</p><p>29 We assume a relatively simple yet commonly used concatenative morphology which models a word as a stem plus (possibly null) suffix2. [sent-78, score-0.603]
</p><p>30 1There are languages with much richer morphology than Spanish, but none with a child-directed corpus suitably annotated for evaluation. [sent-79, score-0.568]
</p><p>31 2Fullwood and O’Donnell (2013) recently presented a model of non-concatenative morphology that could be integrated into this model; however, it does not perform well on English (and presumably other mostly concatenative languages). [sent-80, score-0.493]
</p><p>32 Since this is an unsupervised model, the inferred categories and morphemes lack meaningful labels, but ideally will correspond to gold standard categories and morphemes. [sent-81, score-0.277]
</p><p>33 The tag sequence is generated by a trigram Dirichlet-multinomial distribution, where transition parameters τ are drawn from a symmetric Dirichlet distribution with the hyperparameter αt. [sent-85, score-0.31]
</p><p>34 This part of the model is parametric, operating over a fixed number of tags T, and is identical to the formulation of tag transitions in the Bayesian HMM (Goldwater and Griffiths, 2007). [sent-90, score-0.167]
</p><p>35 However, we replace the BHMM’s emission distribution with the morphologically-informed distributions described below. [sent-91, score-0.148]
</p><p>36 As in the BHMM, the emission distributions are conditioned on the tag, i. [sent-92, score-0.145]
</p><p>37 2 Morphology The morphology model introduced by Goldwater et al. [sent-96, score-0.458]
</p><p>38 (2006) generates morphological analyses for a set of tokens. [sent-97, score-0.254]
</p><p>39 These analyses consist of a tag plus a stem and suffix pair, which are concatenated to form the observed words. [sent-98, score-0.501]
</p><p>40 Both stem s and suffix f are 32 generated from Dirichlet-multinomials conditioned on the tag t: κ ∼  Dir(ακ)  t|κ σ s| t,σ  φ f|t, φ  ∼ ∼ ∼ ∼ ∼  Mult(κ) Dir(αs) Mult(σt) Dir(αf) Mult(φt)  The αs are hyperparameters governing the Dirichlet distributions from which the multinomials κ, σ, are drawn. [sent-99, score-0.594]
</p><p>41 On its own, this distribution over morphological analyses makes independence assumptions that are too strong: most word tokens of a word type have the same analysis, but P0 will re-generate that analysis for every token. [sent-104, score-0.355]
</p><p>42 The distribution of draws from a Pitman-Yor process (which, in our case, determines the distribution of word tokens with each morphological analysis) is commonly described using the metaphor of a Chinese restaurant. [sent-107, score-0.349]
</p><p>43 zi−1 ,a, b) =  (i−n Kka1 −+ +ab i f 1k ≤ = Kk ≤+1 K  (2)  tfkklskkKwziiN  Figure 1: Plate diagram depicting the morphology model (adapted from Goldwater et al. [sent-115, score-0.458]
</p><p>44 The left-hand plate depicts the base distribution P0; note that the morphological analyses lk are generated deterministically as (tk, sk, fk). [sent-118, score-0.294]
</p><p>45 A model that relies on word order cannot learn syntactic categories from a morphologically complex language with free word order; likewise a model attempting to categorise words using morphology alone will fail on a language without morphology. [sent-123, score-0.679]
</p><p>46 ts npop  Language A fefh pomo rtut us st bcba gghh npop npoo aaaa fefh hfeg pnon Language B . [sent-127, score-0.176]
</p><p>47 In order to test the proposed model, we run two experiments on synthetic languages, which simulate languages in which either word order or morphology is the sole cue. [sent-144, score-0.525]
</p><p>48 Words within a category may thus share beginning or ending characters, which could be posited as stems or suffixes by the model, but since only 50 of 256 possible strings are used, there will be  no strong evidence for consistent stem and suffixes (i. [sent-149, score-0.695]
</p><p>49 In these sequences, each category is either followed by itself or the next category (i. [sent-153, score-0.184]
</p><p>50 ated by the concatenation of a stem and a suffix, where the stems are the same as the words in language A (50 stems in each of four categories). [sent-161, score-0.312]
</p><p>51 One of six category-specific suffixes is appended to each stem, resulting in 300 word types per category. [sent-162, score-0.196]
</p><p>52 Each suffix is two letters long, created by combining three possible letters (the same letters used to create the stems), thus making mis-segmentation possible (for  instance, up to three of the suffixes could have the same final letter). [sent-163, score-0.582]
</p><p>53 Figure 4 shows that the morphology component continues to increase the log probability by increasing the number of tokens seated at a table. [sent-170, score-0.563]
</p><p>54 Note that the correct solution in Language A involves learning a very peaked transition distribution as well as an even more extreme distribution over suffixes (where only the null suffix has high probability), whereas the same distributions in Language B are much flatter. [sent-171, score-0.672]
</p><p>55 These experiments demonstrate that our joint model is able to learn correctly even when only either morphology or word order is informative in a language. [sent-179, score-0.517]
</p><p>56 We now turn to acquisition data from natural languages in which both morphology and word order are useful cues but to varying degrees. [sent-180, score-0.648]
</p><p>57 However, the English corpus has only 17 gold suffix types, while Spanish has 83. [sent-187, score-0.316]
</p><p>58 The increased richness of Spanish morphology also has an effect on the number ofword types in the corpus: the Spanish dataset has 3046 word types, whereas the larger English dataset has only 1957. [sent-188, score-0.505]
</p><p>59 Syncretic suffixes (sharing an identical surface form) are disambiguated: sings is annotated as sing-3S, plums as plum-PL. [sent-195, score-0.196]
</p><p>60 was, annotated as be&PAST;) and use only hyphen-separated suffixes to evaluate. [sent-200, score-0.196]
</p><p>61 , dog-DIM-PL) we treat this as a single suffix (-DIM-PL) for evaluation purposes. [sent-203, score-0.254]
</p><p>62 In Spanish, many words are annotated as having a suffix of effectively zero length, e. [sent-204, score-0.254]
</p><p>63 We replace these suffixes (where the stem is equal to the word) with a null suffix, excluding them from evaluation, as they are impossible for a segmentationbased model to find. [sent-207, score-0.349]
</p><p>64 We also use VM to evaluate the morphological segmentation: all tokens with a common suffix are  clustered together, and these clusters are compared against the gold suffix clusters6. [sent-212, score-0.872]
</p><p>65 Tag membership is added to the non-null model suffixes, so that a final -s suffix found in tag 2 is distinguished from the same suffix found in tag 8 (creating suffixes -s-T8 and -s-T2), analogous to the gold annotation distinction between syncretic morphemes -PL and -3S. [sent-214, score-1.031]
</p><p>66 Note that ceiling performance of our model on Suffix VM will be below 100, since our model cannot cluster allomorphs, which are represented by a single abstract morpheme in the gold standard. [sent-215, score-0.15]
</p><p>67 6We also evaluated stem morpheme clusters and found nearceiling performance due to the high number of null-suffix words in both corpora. [sent-219, score-0.231]
</p><p>68 37 MORTAGNOTRANS is the full model without transitions between tag tokens; morphology PYP draws remain conditioned on token tags. [sent-220, score-0.586]
</p><p>69 1) to encourage tag sparsity (analogous to the transition distribution in the full model). [sent-222, score-0.195]
</p><p>70 (2006), in which tags (called clusters in the original) are drawn by P0. [sent-224, score-0.148]
</p><p>71 MORTAGNOSEG is a variant in which the only available suffix is the null suffix; thus segmentations are trivial and only tags are inferred. [sent-225, score-0.495]
</p><p>72 We also evaluate against tags found by the BHMM, with a Dirichlet-multinomial emission distribution and no morphology. [sent-227, score-0.189]
</p><p>73 MORTAGTRUETAGS is the full model but with all tags fixed to their gold values. [sent-228, score-0.138]
</p><p>74 (Due to the annotation scheme used in CHILDES, oracle morphological segmentations are unavailable, so we were unable to test a model with gold morphology and in-  ferred tags. [sent-230, score-0.884]
</p><p>75 We set the hyperparameters for the stem and suffix distributions in the morphology base distribution P0 to 0. [sent-237, score-0.964]
</p><p>76 The number of possible stems and suffixes is given by the dataset: in the Eve dataset there are 5339 candidate stems and 6617 candidate suffixes; in the Ornat dataset these numbers are 8649 and 6598, respectively. [sent-240, score-0.398]
</p><p>77 The num-  ber oftags available to the model is set to the number of gold tags in the data. [sent-241, score-0.138]
</p><p>78 Our results show a clear improvement in the morphological segmentations found by the joint model and stable tagging performance across all models with context information. [sent-271, score-0.389]
</p><p>79 The syntactic clusters found by models using only morphological patterns, MORTAGNOTRANS and MORCLUSTERS, are clearly inferior and lead to low Tag VM results. [sent-272, score-0.312]
</p><p>80 Here we see a statistically significant improvement in tagging performance of the full joint model over both models without morphology (MORTAGNOSEG and BHMM). [sent-306, score-0.517]
</p><p>81 However, the full model does not find better morphological segmentations than the MORCLUSTERS model, despite better tags (the two models’ Suffix VM scores are not statistically significantly different). [sent-308, score-0.406]
</p><p>82 We also see that the difference between the segmentations found by the model using gold tags and estimated tags is quite large. [sent-309, score-0.336]
</p><p>83 This is due to the oracle model finding the rarer suffixes which were not distinguished by the models with noisier tags. [sent-310, score-0.23]
</p><p>84 This demonstrates the importance of syntactic categorisa-  tion for the morpheme induction task, and suggests that a more sophisticated tagging model (with better performance) may yet improve morpheme segmentation performance in Spanish. [sent-311, score-0.348]
</p><p>85 6  Conclusion  We have presented a model of joint syntactic category and morphology induction. [sent-312, score-0.68]
</p><p>86 To our knowledge, this is the first joint model to be tested on child-directed speech data, which is less complex than the newswire corpora used by previous joint models. [sent-315, score-0.155]
</p><p>87 Child-directed speech may be simple enough for joint learning not to be necessary: our results indicate the contrary, namely that joint learning is indeed helpful when learning from realistic acquisition data. [sent-316, score-0.242]
</p><p>88 We tested this model on two languages with different morphological characteristics. [sent-317, score-0.275]
</p><p>89 On English, a language with relatively little morphology, especially in child directed speech, we found that better categorisation of words yielded much better mor-  phology in terms of suffixes learned. [sent-318, score-0.196]
</p><p>90 Conversely, in Spanish we saw less difference on the morphology task between models with categories inferred solely from morphemic patterns and models that also used local syntactic context for categorisation. [sent-319, score-0.635]
</p><p>91 However, in Spanish we saw an improvement in the tagging task when morphology information was included. [sent-320, score-0.458]
</p><p>92 This suggests that English and Spanish make different word-order and morphology trade-offs. [sent-321, score-0.458]
</p><p>93 In English, local context provides at least as much information as morphology in terms of determining the correct syntactic category, but knowing a good estimate of the correct syntactic category is useful for determining a word’s morphology. [sent-322, score-0.692]
</p><p>94 In Spanish, a word’s morphology can more easily be determined simply by looking at frequent suffixes within a purely morphological system. [sent-323, score-0.862]
</p><p>95 On the other hand, word order is freer, making local syntactic context unreliable, so taking morphological information into account can improve tagging. [sent-324, score-0.279]
</p><p>96 These differences between languages demonstrate the benefits of joint learning, which enables the learner to more flexibly utilise the information available in the input data. [sent-325, score-0.179]
</p><p>97 Combining distributional and morphological information for part of speech induction. [sent-346, score-0.245]
</p><p>98 A probabilistic model of syntactic and semantic acquisition from child-directed utterances and their meanings. [sent-397, score-0.2]
</p><p>99 A hierarchical Dirichlet process model for joint part-of-speech and morphology induction. [sent-422, score-0.517]
</p><p>100 On the role of morphological richness in the early development of noun and verb inflection. [sent-440, score-0.255]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('morphology', 0.458), ('suffix', 0.254), ('morphological', 0.208), ('vm', 0.202), ('suffixes', 0.196), ('spanish', 0.181), ('bhmm', 0.178), ('goldwater', 0.162), ('morclusters', 0.156), ('eve', 0.135), ('ornat', 0.133), ('segmentations', 0.122), ('christodoulopoulos', 0.116), ('mortag', 0.111), ('mortagnoseg', 0.111), ('stem', 0.11), ('stems', 0.101), ('pyp', 0.097), ('category', 0.092), ('tag', 0.091), ('mortagnotrans', 0.089), ('morpheme', 0.088), ('acquisition', 0.087), ('sharon', 0.085), ('morphologically', 0.083), ('childes', 0.077), ('tags', 0.076), ('hyperparameter', 0.076), ('emission', 0.073), ('syntactic', 0.071), ('hmm', 0.068), ('categories', 0.067), ('alum', 0.067), ('mortagtruetags', 0.067), ('hyperparameters', 0.067), ('languages', 0.067), ('transition', 0.064), ('gold', 0.062), ('tokens', 0.061), ('bayesian', 0.06), ('mult', 0.059), ('joint', 0.059), ('macwhinney', 0.058), ('sirts', 0.058), ('segmentation', 0.057), ('flexibly', 0.053), ('learners', 0.052), ('inf', 0.05), ('dir', 0.05), ('richness', 0.047), ('english', 0.046), ('analyses', 0.046), ('allomorphs', 0.044), ('cbab', 0.044), ('doyle', 0.044), ('emseosd', 0.044), ('extremes', 0.044), ('fefh', 0.044), ('npop', 0.044), ('parrteantgh', 0.044), ('seated', 0.044), ('tahree', 0.044), ('xanthos', 0.044), ('morphemes', 0.044), ('customers', 0.044), ('letters', 0.044), ('griffiths', 0.044), ('induction', 0.044), ('richer', 0.043), ('null', 0.043), ('utterances', 0.042), ('haghighi', 0.041), ('distribution', 0.04), ('children', 0.04), ('drawn', 0.039), ('uu', 0.039), ('kurimo', 0.039), ('morphemic', 0.039), ('christos', 0.039), ('syncretic', 0.039), ('conditioned', 0.037), ('speech', 0.037), ('unsupervised', 0.037), ('cues', 0.036), ('distributions', 0.035), ('concatenative', 0.035), ('creutz', 0.035), ('aux', 0.035), ('rosenberg', 0.035), ('oracle', 0.034), ('clusters', 0.033), ('characters', 0.033), ('hammarstr', 0.033), ('keok', 0.033), ('yoong', 0.033), ('dasgupta', 0.033), ('kwiatkowski', 0.033), ('blunsom', 0.033), ('levels', 0.032), ('lee', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="83-tfidf-1" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>Author: Stella Frank ; Frank Keller ; Sharon Goldwater</p><p>Abstract: Frank Keller keller@ inf .ed .ac .uk Sharon Goldwater sgwater@ inf .ed .ac .uk ILCC, School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK interactions are often (but not necessarily) synergisChildren learn various levels of linguistic structure concurrently, yet most existing models of language acquisition deal with only a single level of structure, implicitly assuming a sequential learning process. Developing models that learn multiple levels simultaneously can provide important insights into how these levels might interact synergistically dur- ing learning. Here, we present a model that jointly induces syntactic categories and morphological segmentations by combining two well-known models for the individual tasks. We test on child-directed utterances in English and Spanish and compare to single-task baselines. In the morphologically poorer language (English), the model improves morphological segmentation, while in the morphologically richer language (Spanish), it leads to better syntactic categorization. These results provide further evidence that joint learning is useful, but also suggest that the benefits may be different for typologically different languages.</p><p>2 0.36173484 <a title="83-tfidf-2" href="./emnlp-2013-The_Effects_of_Syntactic_Features_in_Automatic_Prediction_of_Morphology.html">181 emnlp-2013-The Effects of Syntactic Features in Automatic Prediction of Morphology</a></p>
<p>Author: Wolfgang Seeker ; Jonas Kuhn</p><p>Abstract: Morphology and syntax interact considerably in many languages and language processing should pay attention to these interdependencies. We analyze the effect of syntactic features when used in automatic morphology prediction on four typologically different languages. We show that predicting morphology for languages with highly ambiguous word forms profits from taking the syntactic context of words into account and results in state-ofthe-art models.</p><p>3 0.28436476 <a title="83-tfidf-3" href="./emnlp-2013-Adaptor_Grammars_for_Learning_Non-Concatenative_Morphology.html">19 emnlp-2013-Adaptor Grammars for Learning Non-Concatenative Morphology</a></p>
<p>Author: Jan A. Botha ; Phil Blunsom</p><p>Abstract: This paper contributes an approach for expressing non-concatenative morphological phenomena, such as stem derivation in Semitic languages, in terms of a mildly context-sensitive grammar formalism. This offers a convenient level of modelling abstraction while remaining computationally tractable. The nonparametric Bayesian framework of adaptor grammars is extended to this richer grammar formalism to propose a probabilistic model that can learn word segmentation and morpheme lexicons, including ones with discontiguous strings as elements, from unannotated data. Our experiments on Hebrew and three variants of Arabic data find that the additional expressiveness to capture roots and templates as atomic units improves the quality of concatenative segmentation and stem identification. We obtain 74% accuracy in identifying triliteral Hebrew roots, while performing morphological segmentation with an F1-score of 78. 1.</p><p>4 0.25123927 <a title="83-tfidf-4" href="./emnlp-2013-Translating_into_Morphologically_Rich_Languages_with_Synthetic_Phrases.html">186 emnlp-2013-Translating into Morphologically Rich Languages with Synthetic Phrases</a></p>
<p>Author: Victor Chahuneau ; Eva Schlinger ; Noah A. Smith ; Chris Dyer</p><p>Abstract: Translation into morphologically rich languages is an important but recalcitrant problem in MT. We present a simple and effective approach that deals with the problem in two phases. First, a discriminative model is learned to predict inflections of target words from rich source-side annotations. Then, this model is used to create additional sentencespecific word- and phrase-level translations that are added to a standard translation model as “synthetic” phrases. Our approach relies on morphological analysis of the target language, but we show that an unsupervised Bayesian model of morphology can successfully be used in place of a supervised analyzer. We report significant improvements in translation quality when translating from English to Russian, Hebrew and Swahili.</p><p>5 0.23457213 <a title="83-tfidf-5" href="./emnlp-2013-Automatic_Extraction_of_Morphological_Lexicons_from_Morphologically_Annotated_Corpora.html">30 emnlp-2013-Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora</a></p>
<p>Author: Ramy Eskander ; Nizar Habash ; Owen Rambow</p><p>Abstract: We present a method for automatically learning inflectional classes and associated lemmas from morphologically annotated corpora. The method consists of a core languageindependent algorithm, which can be optimized for specific languages. The method is demonstrated on Egyptian Arabic and German, two morphologically rich languages. Our best method for Egyptian Arabic provides an error reduction of 55.6% over a simple baseline; our best method for German achieves a 66.7% error reduction.</p><p>6 0.13301955 <a title="83-tfidf-6" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>7 0.12540019 <a title="83-tfidf-7" href="./emnlp-2013-Efficient_Higher-Order_CRFs_for_Morphological_Tagging.html">70 emnlp-2013-Efficient Higher-Order CRFs for Morphological Tagging</a></p>
<p>8 0.11233096 <a title="83-tfidf-8" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>9 0.10967216 <a title="83-tfidf-9" href="./emnlp-2013-Structured_Penalties_for_Log-Linear_Language_Models.html">176 emnlp-2013-Structured Penalties for Log-Linear Language Models</a></p>
<p>10 0.100346 <a title="83-tfidf-10" href="./emnlp-2013-Improvements_to_the_Bayesian_Topic_N-Gram_Models.html">100 emnlp-2013-Improvements to the Bayesian Topic N-Gram Models</a></p>
<p>11 0.076677047 <a title="83-tfidf-11" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>12 0.074351855 <a title="83-tfidf-12" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>13 0.068132624 <a title="83-tfidf-13" href="./emnlp-2013-An_Efficient_Language_Model_Using_Double-Array_Structures.html">20 emnlp-2013-An Efficient Language Model Using Double-Array Structures</a></p>
<p>14 0.06394726 <a title="83-tfidf-14" href="./emnlp-2013-Ubertagging%3A_Joint_Segmentation_and_Supertagging_for_English.html">190 emnlp-2013-Ubertagging: Joint Segmentation and Supertagging for English</a></p>
<p>15 0.060154073 <a title="83-tfidf-15" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>16 0.060086474 <a title="83-tfidf-16" href="./emnlp-2013-Russian_Stress_Prediction_using_Maximum_Entropy_Ranking.html">162 emnlp-2013-Russian Stress Prediction using Maximum Entropy Ranking</a></p>
<p>17 0.059342168 <a title="83-tfidf-17" href="./emnlp-2013-Joint_Learning_of_Phonetic_Units_and_Word_Pronunciations_for_ASR.html">115 emnlp-2013-Joint Learning of Phonetic Units and Word Pronunciations for ASR</a></p>
<p>18 0.05861026 <a title="83-tfidf-18" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>19 0.057618711 <a title="83-tfidf-19" href="./emnlp-2013-An_Empirical_Study_Of_Semi-Supervised_Chinese_Word_Segmentation_Using_Co-Training.html">21 emnlp-2013-An Empirical Study Of Semi-Supervised Chinese Word Segmentation Using Co-Training</a></p>
<p>20 0.053365231 <a title="83-tfidf-20" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.219), (1, -0.074), (2, -0.021), (3, -0.167), (4, -0.488), (5, -0.146), (6, -0.16), (7, -0.075), (8, 0.056), (9, -0.142), (10, 0.021), (11, -0.054), (12, -0.045), (13, -0.04), (14, -0.016), (15, -0.026), (16, 0.092), (17, -0.002), (18, 0.018), (19, -0.021), (20, -0.02), (21, -0.033), (22, -0.072), (23, -0.016), (24, 0.004), (25, 0.019), (26, -0.007), (27, -0.032), (28, -0.041), (29, -0.005), (30, 0.044), (31, -0.056), (32, -0.004), (33, 0.021), (34, -0.044), (35, -0.039), (36, 0.024), (37, -0.027), (38, -0.102), (39, 0.067), (40, -0.061), (41, 0.044), (42, -0.084), (43, -0.003), (44, 0.007), (45, -0.004), (46, -0.117), (47, -0.008), (48, 0.021), (49, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94597858 <a title="83-lsi-1" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>Author: Stella Frank ; Frank Keller ; Sharon Goldwater</p><p>Abstract: Frank Keller keller@ inf .ed .ac .uk Sharon Goldwater sgwater@ inf .ed .ac .uk ILCC, School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK interactions are often (but not necessarily) synergisChildren learn various levels of linguistic structure concurrently, yet most existing models of language acquisition deal with only a single level of structure, implicitly assuming a sequential learning process. Developing models that learn multiple levels simultaneously can provide important insights into how these levels might interact synergistically dur- ing learning. Here, we present a model that jointly induces syntactic categories and morphological segmentations by combining two well-known models for the individual tasks. We test on child-directed utterances in English and Spanish and compare to single-task baselines. In the morphologically poorer language (English), the model improves morphological segmentation, while in the morphologically richer language (Spanish), it leads to better syntactic categorization. These results provide further evidence that joint learning is useful, but also suggest that the benefits may be different for typologically different languages.</p><p>2 0.84399307 <a title="83-lsi-2" href="./emnlp-2013-Automatic_Extraction_of_Morphological_Lexicons_from_Morphologically_Annotated_Corpora.html">30 emnlp-2013-Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora</a></p>
<p>Author: Ramy Eskander ; Nizar Habash ; Owen Rambow</p><p>Abstract: We present a method for automatically learning inflectional classes and associated lemmas from morphologically annotated corpora. The method consists of a core languageindependent algorithm, which can be optimized for specific languages. The method is demonstrated on Egyptian Arabic and German, two morphologically rich languages. Our best method for Egyptian Arabic provides an error reduction of 55.6% over a simple baseline; our best method for German achieves a 66.7% error reduction.</p><p>3 0.7895664 <a title="83-lsi-3" href="./emnlp-2013-Adaptor_Grammars_for_Learning_Non-Concatenative_Morphology.html">19 emnlp-2013-Adaptor Grammars for Learning Non-Concatenative Morphology</a></p>
<p>Author: Jan A. Botha ; Phil Blunsom</p><p>Abstract: This paper contributes an approach for expressing non-concatenative morphological phenomena, such as stem derivation in Semitic languages, in terms of a mildly context-sensitive grammar formalism. This offers a convenient level of modelling abstraction while remaining computationally tractable. The nonparametric Bayesian framework of adaptor grammars is extended to this richer grammar formalism to propose a probabilistic model that can learn word segmentation and morpheme lexicons, including ones with discontiguous strings as elements, from unannotated data. Our experiments on Hebrew and three variants of Arabic data find that the additional expressiveness to capture roots and templates as atomic units improves the quality of concatenative segmentation and stem identification. We obtain 74% accuracy in identifying triliteral Hebrew roots, while performing morphological segmentation with an F1-score of 78. 1.</p><p>4 0.75903928 <a title="83-lsi-4" href="./emnlp-2013-Translating_into_Morphologically_Rich_Languages_with_Synthetic_Phrases.html">186 emnlp-2013-Translating into Morphologically Rich Languages with Synthetic Phrases</a></p>
<p>Author: Victor Chahuneau ; Eva Schlinger ; Noah A. Smith ; Chris Dyer</p><p>Abstract: Translation into morphologically rich languages is an important but recalcitrant problem in MT. We present a simple and effective approach that deals with the problem in two phases. First, a discriminative model is learned to predict inflections of target words from rich source-side annotations. Then, this model is used to create additional sentencespecific word- and phrase-level translations that are added to a standard translation model as “synthetic” phrases. Our approach relies on morphological analysis of the target language, but we show that an unsupervised Bayesian model of morphology can successfully be used in place of a supervised analyzer. We report significant improvements in translation quality when translating from English to Russian, Hebrew and Swahili.</p><p>5 0.7517339 <a title="83-lsi-5" href="./emnlp-2013-The_Effects_of_Syntactic_Features_in_Automatic_Prediction_of_Morphology.html">181 emnlp-2013-The Effects of Syntactic Features in Automatic Prediction of Morphology</a></p>
<p>Author: Wolfgang Seeker ; Jonas Kuhn</p><p>Abstract: Morphology and syntax interact considerably in many languages and language processing should pay attention to these interdependencies. We analyze the effect of syntactic features when used in automatic morphology prediction on four typologically different languages. We show that predicting morphology for languages with highly ambiguous word forms profits from taking the syntactic context of words into account and results in state-ofthe-art models.</p><p>6 0.47599047 <a title="83-lsi-6" href="./emnlp-2013-Efficient_Higher-Order_CRFs_for_Morphological_Tagging.html">70 emnlp-2013-Efficient Higher-Order CRFs for Morphological Tagging</a></p>
<p>7 0.41581252 <a title="83-lsi-7" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>8 0.36156449 <a title="83-lsi-8" href="./emnlp-2013-Structured_Penalties_for_Log-Linear_Language_Models.html">176 emnlp-2013-Structured Penalties for Log-Linear Language Models</a></p>
<p>9 0.34783897 <a title="83-lsi-9" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>10 0.34491435 <a title="83-lsi-10" href="./emnlp-2013-Improvements_to_the_Bayesian_Topic_N-Gram_Models.html">100 emnlp-2013-Improvements to the Bayesian Topic N-Gram Models</a></p>
<p>11 0.33720809 <a title="83-lsi-11" href="./emnlp-2013-Ubertagging%3A_Joint_Segmentation_and_Supertagging_for_English.html">190 emnlp-2013-Ubertagging: Joint Segmentation and Supertagging for English</a></p>
<p>12 0.3179633 <a title="83-lsi-12" href="./emnlp-2013-Assembling_the_Kazakh_Language_Corpus.html">26 emnlp-2013-Assembling the Kazakh Language Corpus</a></p>
<p>13 0.31393671 <a title="83-lsi-13" href="./emnlp-2013-An_Efficient_Language_Model_Using_Double-Array_Structures.html">20 emnlp-2013-An Efficient Language Model Using Double-Array Structures</a></p>
<p>14 0.29808208 <a title="83-lsi-14" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>15 0.2768411 <a title="83-lsi-15" href="./emnlp-2013-Dependency_Language_Models_for_Sentence_Completion.html">58 emnlp-2013-Dependency Language Models for Sentence Completion</a></p>
<p>16 0.27628022 <a title="83-lsi-16" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>17 0.27561679 <a title="83-lsi-17" href="./emnlp-2013-Joint_Learning_of_Phonetic_Units_and_Word_Pronunciations_for_ASR.html">115 emnlp-2013-Joint Learning of Phonetic Units and Word Pronunciations for ASR</a></p>
<p>18 0.27406922 <a title="83-lsi-18" href="./emnlp-2013-With_Blinkers_on%3A_Robust_Prediction_of_Eye_Movements_across_Readers.html">203 emnlp-2013-With Blinkers on: Robust Prediction of Eye Movements across Readers</a></p>
<p>19 0.27054834 <a title="83-lsi-19" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>20 0.2624118 <a title="83-lsi-20" href="./emnlp-2013-An_Empirical_Study_Of_Semi-Supervised_Chinese_Word_Segmentation_Using_Co-Training.html">21 emnlp-2013-An Empirical Study Of Semi-Supervised Chinese Word Segmentation Using Co-Training</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.043), (9, 0.03), (18, 0.038), (22, 0.028), (30, 0.097), (50, 0.038), (51, 0.144), (52, 0.298), (66, 0.074), (71, 0.025), (75, 0.031), (77, 0.037), (96, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75219566 <a title="83-lda-1" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>Author: Stella Frank ; Frank Keller ; Sharon Goldwater</p><p>Abstract: Frank Keller keller@ inf .ed .ac .uk Sharon Goldwater sgwater@ inf .ed .ac .uk ILCC, School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK interactions are often (but not necessarily) synergisChildren learn various levels of linguistic structure concurrently, yet most existing models of language acquisition deal with only a single level of structure, implicitly assuming a sequential learning process. Developing models that learn multiple levels simultaneously can provide important insights into how these levels might interact synergistically dur- ing learning. Here, we present a model that jointly induces syntactic categories and morphological segmentations by combining two well-known models for the individual tasks. We test on child-directed utterances in English and Spanish and compare to single-task baselines. In the morphologically poorer language (English), the model improves morphological segmentation, while in the morphologically richer language (Spanish), it leads to better syntactic categorization. These results provide further evidence that joint learning is useful, but also suggest that the benefits may be different for typologically different languages.</p><p>2 0.7473219 <a title="83-lda-2" href="./emnlp-2013-Automated_Essay_Scoring_by_Maximizing_Human-Machine_Agreement.html">28 emnlp-2013-Automated Essay Scoring by Maximizing Human-Machine Agreement</a></p>
<p>Author: Hongbo Chen ; Ben He</p><p>Abstract: Previous approaches for automated essay scoring (AES) learn a rating model by minimizing either the classification, regression, or pairwise classification loss, depending on the learning algorithm used. In this paper, we argue that the current AES systems can be further improved by taking into account the agreement between human and machine raters. To this end, we propose a rankbased approach that utilizes listwise learning to rank algorithms for learning a rating model, where the agreement between the human and machine raters is directly incorporated into the loss function. Various linguistic and statistical features are utilized to facilitate the learning algorithms. Experiments on the publicly available English essay dataset, Automated Student Assessment Prize (ASAP), show that our proposed approach outperforms the state-of-the-art algorithms, and achieves performance comparable to professional human raters, which suggests the effectiveness of our proposed method for automated essay scoring.</p><p>3 0.72759986 <a title="83-lda-3" href="./emnlp-2013-Noise-Aware_Character_Alignment_for_Bootstrapping_Statistical_Machine_Transliteration_from_Bilingual_Corpora.html">139 emnlp-2013-Noise-Aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora</a></p>
<p>Author: Katsuhito Sudoh ; Shinsuke Mori ; Masaaki Nagata</p><p>Abstract: This paper proposes a novel noise-aware character alignment method for bootstrapping statistical machine transliteration from automatically extracted phrase pairs. The model is an extension of a Bayesian many-to-many alignment method for distinguishing nontransliteration (noise) parts in phrase pairs. It worked effectively in the experiments of bootstrapping Japanese-to-English statistical machine transliteration in patent domain using patent bilingual corpora.</p><p>4 0.55462712 <a title="83-lda-4" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>Author: Micha Elsner ; Sharon Goldwater ; Naomi Feldman ; Frank Wood</p><p>Abstract: We present a cognitive model of early lexical acquisition which jointly performs word segmentation and learns an explicit model of phonetic variation. We define the model as a Bayesian noisy channel; we sample segmentations and word forms simultaneously from the posterior, using beam sampling to control the size of the search space. Compared to a pipelined approach in which segmentation is performed first, our model is qualitatively more similar to human learners. On data with vari- able pronunciations, the pipelined approach learns to treat syllables or morphemes as words. In contrast, our joint model, like infant learners, tends to learn multiword collocations. We also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors, and relate these to developmental evidence.</p><p>5 0.54779226 <a title="83-lda-5" href="./emnlp-2013-Adaptor_Grammars_for_Learning_Non-Concatenative_Morphology.html">19 emnlp-2013-Adaptor Grammars for Learning Non-Concatenative Morphology</a></p>
<p>Author: Jan A. Botha ; Phil Blunsom</p><p>Abstract: This paper contributes an approach for expressing non-concatenative morphological phenomena, such as stem derivation in Semitic languages, in terms of a mildly context-sensitive grammar formalism. This offers a convenient level of modelling abstraction while remaining computationally tractable. The nonparametric Bayesian framework of adaptor grammars is extended to this richer grammar formalism to propose a probabilistic model that can learn word segmentation and morpheme lexicons, including ones with discontiguous strings as elements, from unannotated data. Our experiments on Hebrew and three variants of Arabic data find that the additional expressiveness to capture roots and templates as atomic units improves the quality of concatenative segmentation and stem identification. We obtain 74% accuracy in identifying triliteral Hebrew roots, while performing morphological segmentation with an F1-score of 78. 1.</p><p>6 0.54499042 <a title="83-lda-6" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>7 0.54321396 <a title="83-lda-7" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>8 0.53850389 <a title="83-lda-8" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>9 0.5370096 <a title="83-lda-9" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>10 0.5365591 <a title="83-lda-10" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>11 0.53242636 <a title="83-lda-11" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>12 0.53150403 <a title="83-lda-12" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>13 0.53133976 <a title="83-lda-13" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>14 0.53077537 <a title="83-lda-14" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>15 0.52912861 <a title="83-lda-15" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>16 0.52827621 <a title="83-lda-16" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>17 0.52824807 <a title="83-lda-17" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>18 0.52645874 <a title="83-lda-18" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>19 0.52585143 <a title="83-lda-19" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>20 0.525702 <a title="83-lda-20" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
