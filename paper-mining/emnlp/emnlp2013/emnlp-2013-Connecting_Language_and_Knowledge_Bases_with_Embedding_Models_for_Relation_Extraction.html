<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-51" href="#">emnlp2013-51</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</h1>
<br/><p>Source: <a title="emnlp-2013-51-pdf" href="http://aclweb.org/anthology//D/D13/D13-1136.pdf">pdf</a></p><p>Author: Jason Weston ; Antoine Bordes ; Oksana Yakhnenko ; Nicolas Usunier</p><p>Abstract: This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over methods that rely on text features alone.</p><p>Reference: <a title="emnlp-2013-51-reference" href="../emnlp2013_reference/emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction  Jason Weston  Antoine Bordes  Google  Heudiasyc  111 8th avenue  UT de Compiègne  New York, NY, USA  & CNRS  jweston@google . [sent-1, score-0.032]
</p><p>2 fr  Abstract This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. [sent-3, score-0.205]
</p><p>3 Our model is based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base. [sent-4, score-0.546]
</p><p>4 We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over methods that rely on text features alone. [sent-5, score-0.04]
</p><p>5 1 Introduction Information extraction (IE) aims at generating structured data from free text in order to populate Knowledge Bases (KBs). [sent-6, score-0.057]
</p><p>6 Hence, one is given an incomplete KB composed of a set of triples of the form (h, r, t); h is the left-hand side entity (or head), t the right-hand side entity (or tail) and r the relationship linking them. [sent-7, score-0.392]
</p><p>7 This paper focuses on the problem of learning to perform relation extraction (RE) under weak supervision from a KB. [sent-9, score-0.335]
</p><p>8 RE is sub-task of IE that considers that entities have already been detected by a different process, such as a named-entity recognizer. [sent-10, score-0.156]
</p><p>9 RE then aims at assigning to a relation mention m 1www. [sent-11, score-0.198]
</p><p>10 com  Oksana Yakhnenko Google 111 8th avenue  1366 Nicolas Usunier Heudiasyc UT de Compiègne  New York, NY, USA oksana@google . [sent-13, score-0.032]
</p><p>11 a sequence of text which states that some relation is true) the corresponding relationship from the KB, given a pair of extracted entities (h, t) as context. [sent-17, score-0.371]
</p><p>12 For example, given the triple (/m/2d3rf ,“wrote and directed", /m/3/324), a system should predict  . [sent-18, score-0.063]
</p><p>13 The task is said to be weakly supervised because for each pair of entities (h, t) detected in the text, all relation mentions m associated with them are labeled with all the relationships connecting h and t in the KB, whether they are actually expressed by m or not. [sent-19, score-0.712]
</p><p>14 Our key contribution is a novel model that employs not only weakly labeled text mention data, as most approaches do, but also leverages triples from the known KB. [sent-20, score-0.261]
</p><p>15 The model thus learns the plausibility of new (h, r, t) triples by generalizing from the KB, even though this triple is not present. [sent-21, score-0.181]
</p><p>16 A ranking-based embedding framework is used to train our model. [sent-22, score-0.152]
</p><p>17 Thereby, relation mentions, entities and relationships are all embedded into a common low-  dimensional vector space, where scores are computed. [sent-23, score-0.407]
</p><p>18 2  Previous Work  Learning under weak supervision is common in natural language processing, especially for tasks where the annotation costs are significant such as in seProce Sdeiantgtlse o,f W thaesh 2i0n1gt3o nC,o UnSfeAre,n 1c8e- o2n1 E Omctpoibriecra 2l0 M13et. [sent-26, score-0.165]
</p><p>19 It was also used to train open extractors based on  Wikipedia infoboxes and corresponding sentences (Wu and Weld, 2007; Wu and Weld, 2010). [sent-35, score-0.038]
</p><p>20 , 2007)) also rely on weak supervision, since they learn models from a seed KB in order to extend it. [sent-39, score-0.066]
</p><p>21 Weak supervision is also a popular option for RE: Mintz et al. [sent-40, score-0.099]
</p><p>22 (2009) used Freebase to train weakly supervised relational extractors on Wikipedia, an approach generalized by the multi-instance learning frameworks (Riedel et al. [sent-41, score-0.127]
</p><p>23 (2013) proposed another joint approach based on collaborative filtering for learning entity embeddings. [sent-49, score-0.109]
</p><p>24 This approach connects text with Freebase by learning shared embeddings of entities through weak supervision, in contrast to our method where no joint learning is performed. [sent-50, score-0.442]
</p><p>25 Nevertheless, our method is easier to integrate into exist-  ing systems than those, since KB data is used via the addition of a scoring term, which is trained separately beforehand (with no shared embeddings). [sent-52, score-0.038]
</p><p>26 In this framework, we learn two models: one for predicting relationships given re1367 lation mentions and another one to encode the interactions among entities and relationships from the KB. [sent-58, score-0.597]
</p><p>27 The joint action of both models in prediction allows us to use the connection between the KB and text to perform relation extraction. [sent-59, score-0.145]
</p><p>28 Both submodels end up learning vector embeddings of symbols, either for entities or relationships in the KB, or for each word/feature ofthe vocabulary (denoted V). [sent-63, score-0.508]
</p><p>29 The set of entities and relationships in t(dhee nKotBe are d. [sent-64, score-0.294]
</p><p>30 e Tnhoete sde by E e natnitdie R, nadnd re nv, ne ahnipds nr dtheeno KteB th aree s diezen ootef V, yE Ean adn dR R respectively. [sent-65, score-0.073]
</p><p>31 Given a triple (h, r, t) othfe V embeddings eosfp tehctei veenltyit. [sent-66, score-0.277]
</p><p>32 ie Gs avendn the relationship (vectors in Rk) are denoted with the same letter, in boldface characters (i. [sent-67, score-0.167]
</p><p>33 1 Connecting Text and Relationships The first part of the framework concerns the learning of a function Sm2r(m, r), based on embeddings, that is designed to score the similarity of a relation mention m and a relationship r. [sent-71, score-0.382]
</p><p>34 Our scoring approach is inspired by previous work for connecting word labels and images (Weston et al. [sent-72, score-0.114]
</p><p>35 , 2010), which we adapted, replacing images by mentions and word labels by relationships. [sent-73, score-0.141]
</p><p>36 Intuitively, it consists of first projecting words and features into the embedding space and then computing a similarity measure (the dot product in this paper) between this projection and a relationship em-  bedding. [sent-74, score-0.254]
</p><p>37 Since this type of learning problem is weakly supervised, Bordes et al. [sent-77, score-0.089]
</p><p>38 (2010) showed that a convenient way to train it is by using a ranking loss. [sent-78, score-0.045]
</p><p>39 , |D| } consisting of (mention, relationship) training pairs, one icstoiunlgd loefa r(nm tehneembeddings using constraints of the form:  W>Φ(m). [sent-82, score-0.037]
</p><p>40 That is, we want the relation that (weakly) labels a given mention to be scored higher than other relation by a margin of 1. [sent-84, score-0.311]
</p><p>41 Then, given any mention m one can predict the corresponding relationship r(m) with:  r(m) = argmr0axSm2r(m,r0)  = argmr0ax? [sent-85, score-0.213]
</p><p>42 Learning Sm2r(·) under constraints (1) is well suited when one ·is) uinntdeerers cteodn itnra building a permention prediction system. [sent-88, score-0.037]
</p><p>43 However, performance metrics of relation extraction are sometimes measured using precision recall curves aggregated for all mentions concerning the same pair of entities, as in (Riedel et al. [sent-89, score-0.369]
</p><p>44 In that case the scores across predictions for different mentions need to be calibrated so that the most confident ones have the higher scores. [sent-91, score-0.113]
</p><p>45 This can be better encoded with constraints of the following form: ∀i,j,  ∀r0  =  ri, rj,  f(mi)>ri  > 1+  f(mj)>r0  . [sent-92, score-0.037]
</p><p>46 In practice, we use “soft” ranking constraints (optimizing the hinge loss), i. [sent-94, score-0.12]
</p><p>47 ×  Finally, we also enforce a (hard) constraint on the norms of the columns of W and r, i. [sent-97, score-0.043]
</p><p>48 That is, at the start of training the parameters to be learnt (the nv k word/feature embeddings in W and the nr k× × re kla wtioornd e/fmeabtuerdedings r) are initialized to rand×om k weights. [sent-106, score-0.323]
</p><p>49 Wemeb iendi-tialize each k-dimensional embedding vector randomly with mean 0, standard deviation k1. [sent-107, score-0.126]
</p><p>50 Select at random a secondary training pair (mj, rj), used to calibrate the scores. [sent-111, score-0.043]
</p><p>51 Select at random a negative relation r0 such that r0 ri and r0 rj. [sent-113, score-0.287]
</p><p>52 Enforce the constraint that each embedding vector is normalized, i. [sent-117, score-0.126]
</p><p>53 2 Encoding Structured Data of KBs Using only weakly labeled text mentions for training ignores much of the prior knowledge we can leverage from a large KB such as Freebase. [sent-121, score-0.231]
</p><p>54 In order to connect this relational data with our model, we propose to encode its information into entity and relationship embeddings. [sent-122, score-0.231]
</p><p>55 This allows us to build a model which can score the plausibility of new entity relationship triples which are missing from Freebase. [sent-123, score-0.323]
</p><p>56 e,l| Sle|}arn osf v reeclattoiro embeddings ofrfo tmhe t ehent iKtiBes, tahnids of the relationships using the idea that the func-  tional relation induced by the r-labeled arcs of the KB should correspond to a translation of the embeddings. [sent-137, score-0.491]
</p><p>57 That is, given a k-dimensional embedding of the left-hand side (head) entity, adding the k-dimensional embedding of a given relation should yield the point (or close to the point) of the kdimensional embedding of the right-hand side (tail) entity. [sent-138, score-0.543]
</p><p>58 The model thus gives the following score for the plausibility of a relation: Skb(h, r, t) = −||h + r − t||22 . [sent-140, score-0.06]
</p><p>59 The ranking objective is designed to assign higher scores to existing relations versus any other possibility: ∀i, ∀h0 ∀i, ∀r0 ∀i, ∀t0  = hi, = ri, = ti,  + + +  Skb(hi, ri, ti) ≥ 1 Skb(h0, ri, ti), Skb(hi, ri, ti) ≥ 1 Skb(hi, r0, ti), Skb(hi, ri, ti) ≥ 1 Skb(hi, ri, t0). [sent-142, score-0.085]
</p><p>60 That is, for each known triple (h, r, t), if we replaced the (i) head, (ii) relation or (iii) tail with some other possibility, the modified triple should have a  lower score (i. [sent-143, score-0.287]
</p><p>61 The three sets of constraints defined above encode the three types of modification. [sent-146, score-0.063]
</p><p>62 1 we use soft constraints via the hinge loss, enforce constraints on the norm of embeddings, i. [sent-148, score-0.155]
</p><p>63 At test time, one may again need to calibrate the scores Skb across entity pairs. [sent-152, score-0.12]
</p><p>64 We propose a simple approach: we convert the scores by ranking all relationships R by Skb and instead output:  S˜kb(h,r,t)=Φ? [sent-153, score-0.209]
</p><p>65 3 Implementation for Relation Extraction Our framework can be used for relation extraction in the following way. [sent-160, score-0.196]
</p><p>66 First, for each pair of entities (h, t) that appear in the test set, all the corresponding mentions Mh,t in the test set are collected and a prediction niss performed with:  rh,t= arrg∈mRaxm∈XMh,tSm2r(m,r). [sent-161, score-0.243]
</p><p>67 The predicted relationship can either be a valid relationship or NA a marker that means that there is no relation between h and t (NA is added to R during training aetnwd iesn nt rhea atnedd tli (kNe Aot ihse ar relationships). [sent-162, score-0.369]
</p><p>68 If rh,t is a relationship, a composite score is defined: –  Sm2r+kb(h, rh,t,  t)=XSm2r(m, rh,t)+˜Skb(h,  rh,t,  t)  m∈XMh,t That is, only the top scoring non-NA predictions are re-scored. [sent-163, score-0.07]
</p><p>69 Hence, our final composite model favors predictions that agree with both the mentions and the KB. [sent-164, score-0.145]
</p><p>70 , 2010), aligns Freebase relations with the New  York Times corpus. [sent-171, score-0.04]
</p><p>71 Entities were found using the Stanford named entity tagger (Finkel et al. [sent-172, score-0.077]
</p><p>72 For each mention, sentence level features are extracted which include part of speech, named entity and dependency tree path properties. [sent-174, score-0.077]
</p><p>73 Unlike some of the previous methods, we do not use features that aggregate properties across multiple mentions. [sent-175, score-0.038]
</p><p>74 There are 52 possible relationships and 121,034 training mentions of which most are labeled as no relation (labeled “NA”) there are 4700 Freebase relations mentioned in the training set, and 1950 in the test set. [sent-177, score-0.459]
</p><p>75 –  Freebase Freebase is a large-scale KB that has around 80M entities, 23k relationships and 1. [sent-178, score-0.164]
</p><p>76 We used a subset restricted to the top 4M entities for scalability reasons where top is defined as the ones with the largest number of relations to other entities. [sent-180, score-0.17]
</p><p>77 We used all the 23k possible relationships in Freebase. [sent-181, score-0.164]
</p><p>78 To make a realistic setting, we did not choose the entity set using the NYT+FB data set, so it may not overlap completely. [sent-182, score-0.077]
</p><p>79 Keeping –  the top 4M entities gives an overlap of 80% with the entities in the NYT+FB test set. [sent-184, score-0.26]
</p><p>80 Most importantly, we then removed all the entity pairs present in the NYT+FB test set from Freebase, i. [sent-185, score-0.077]
</p><p>81 all relations they are involved in independent of the relationship. [sent-187, score-0.04]
</p><p>82 This ensures that we cannot just memorize the true relations for an entity pair we have to learn to generalize from other entities and relations. [sent-188, score-0.247]
</p><p>83 As the NYT+FB dataset was built on an earlier version of Freebase we also had to translate the deprecated relationships into their new variants (e. [sent-189, score-0.164]
</p><p>84 “/p/business/company/place_founded ” → “/organization/organization/place_founded”) ”to → →ma “k/oe tghaetwo datasets link (then, the 52 relationships in NYT+FB are now a subset of the 23k from Freebase). [sent-191, score-0.164]
</p><p>85 –  Figure 1: Top: Aggregate extraction precision/recall curves for a variety of methods. [sent-193, score-0.09]
</p><p>86 For the calibration of τ = 10 (note, here we are ranking  Sˆkb,  all 23k Freebase relationships). [sent-202, score-0.045]
</p><p>87 Results Figure 1 displays the aggregate precision / recall curves of our approach WSABIEM2R+FB which uses the combination of Sm2r + Skb, as well as WSABIEM2R , which only uses Sm2r, and existing state-of-the-art approaches: HOFFMANN (Hoffmann 1370 et al. [sent-204, score-0.124]
</p><p>88 However, the addition of extra knowledge from other Freebase entities in WSABIEM2R+FB provides superior performance to all other methods, by a wide margin, at least between 0 and 0. [sent-213, score-0.13]
</p><p>89 This is also  caused by the simplifications of WSABIEM2R that prevent it from reaching high precision when the recall is greater than 0. [sent-217, score-0.053]
</p><p>90 We recall that Freebase data is not used to detect relationships i. [sent-219, score-0.192]
</p><p>91 to discriminate between NA and the rest, but only to select the best relationship in case of detection. [sent-221, score-0.128]
</p><p>92 5  Conclusion  In this paper we described a framework for leveraging large scale knowledge bases to improve relation extraction by training not only on (mention, relationship) pairs but using all other KB triples as well. [sent-230, score-0.322]
</p><p>93 2There is an error in the plot from (Hoffmann et al. [sent-236, score-0.034]
</p><p>94 Label ranking under ambiguous supervision for learning semantic correspondences. [sent-245, score-0.144]
</p><p>95 Constructing biological knowledge bases by extracting information from text sources. [sent-272, score-0.068]
</p><p>96 Incorporating non-local information into information extraction systems by gibbs sampling. [sent-276, score-0.057]
</p><p>97 Knowledgebased weak supervision for information extraction of overlapping relations. [sent-281, score-0.222]
</p><p>98 In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, volume 1, pages 541–550. [sent-282, score-0.059]
</p><p>99 A joint model of language and perception for grounded attribute learning. [sent-299, score-0.032]
</p><p>100 Large scale image annotation: learning to rank with joint word-image embeddings. [sent-326, score-0.032]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('skb', 0.405), ('bordes', 0.3), ('freebase', 0.282), ('kb', 0.259), ('embeddings', 0.214), ('fb', 0.207), ('ri', 0.174), ('relationships', 0.164), ('hoffmann', 0.148), ('entities', 0.13), ('relationship', 0.128), ('embedding', 0.126), ('riedel', 0.126), ('nyt', 0.12), ('mentions', 0.113), ('relation', 0.113), ('compi', 0.108), ('gne', 0.108), ('antoine', 0.1), ('supervision', 0.099), ('weston', 0.09), ('weakly', 0.089), ('mention', 0.085), ('oksana', 0.081), ('mintz', 0.08), ('hi', 0.079), ('entity', 0.077), ('nicolas', 0.075), ('rj', 0.075), ('mj', 0.072), ('mimlre', 0.07), ('usunier', 0.07), ('bases', 0.068), ('weak', 0.066), ('surdeanu', 0.065), ('triple', 0.063), ('plausibility', 0.06), ('lao', 0.06), ('triples', 0.058), ('mi', 0.057), ('extraction', 0.057), ('ti', 0.056), ('heudiasyc', 0.054), ('nickel', 0.054), ('rnv', 0.054), ('ut', 0.052), ('sgd', 0.049), ('connecting', 0.048), ('tail', 0.048), ('kbs', 0.047), ('jason', 0.047), ('rk', 0.046), ('na', 0.045), ('ranking', 0.045), ('enforce', 0.043), ('re', 0.043), ('calibrate', 0.043), ('cnrs', 0.043), ('craven', 0.04), ('relations', 0.04), ('ie', 0.039), ('scoring', 0.038), ('aggregate', 0.038), ('hinge', 0.038), ('kate', 0.038), ('matuszek', 0.038), ('extractors', 0.038), ('constraints', 0.037), ('limin', 0.036), ('nv', 0.036), ('fr', 0.035), ('banko', 0.034), ('wu', 0.034), ('plot', 0.034), ('curves', 0.033), ('weld', 0.033), ('joint', 0.032), ('composite', 0.032), ('avenue', 0.032), ('pages', 0.031), ('wi', 0.03), ('concerns', 0.03), ('nr', 0.03), ('labeled', 0.029), ('fei', 0.029), ('luke', 0.029), ('yoshua', 0.029), ('volume', 0.028), ('images', 0.028), ('zettlemoyer', 0.028), ('recall', 0.028), ('yao', 0.027), ('york', 0.027), ('france', 0.026), ('framework', 0.026), ('detected', 0.026), ('encode', 0.026), ('side', 0.026), ('precision', 0.025), ('google', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="51-tfidf-1" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>Author: Jason Weston ; Antoine Bordes ; Oksana Yakhnenko ; Nicolas Usunier</p><p>Abstract: This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over methods that rely on text features alone.</p><p>2 0.19582884 <a title="51-tfidf-2" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>Author: Hrushikesh Mohapatra ; Siddhanth Jain ; Soumen Chakrabarti</p><p>Abstract: Web search can be enhanced in powerful ways if token spans in Web text are annotated with disambiguated entities from large catalogs like Freebase. Entity annotators need to be trained on sample mention snippets. Wikipedia entities and annotated pages offer high-quality labeled data for training and evaluation. Unfortunately, Wikipedia features only one-ninth the number of entities as Freebase, and these are a highly biased sample of well-connected, frequently mentioned “head” entities. To bring hope to “tail” entities, we broaden our goal to a second task: assigning types to entities in Freebase but not Wikipedia. The two tasks are synergistic: knowing the types of unfamiliar entities helps disambiguate mentions, and words in mention contexts help assign types to entities. We present TMI, a bipartite graphical model for joint type-mention inference. TMI attempts no schema integration or entity resolution, but exploits the above-mentioned synergy. In experiments involving 780,000 people in Wikipedia, 2.3 million people in Freebase, 700 million Web pages, and over 20 professional editors, TMI shows considerable annotation accuracy improvement (e.g., 70%) compared to baselines (e.g., 46%), especially for “tail” and emerging entities. We also compare with Google’s recent annotations of the same corpus with Freebase entities, and report considerable improvements within the people domain.</p><p>3 0.18755651 <a title="51-tfidf-3" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>Author: Will Y. Zou ; Richard Socher ; Daniel Cer ; Christopher D. Manning</p><p>Abstract: We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence. The new embeddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task.</p><p>4 0.17493026 <a title="51-tfidf-4" href="./emnlp-2013-Improving_Learning_and_Inference_in_a_Large_Knowledge-Base_using_Latent_Syntactic_Cues.html">102 emnlp-2013-Improving Learning and Inference in a Large Knowledge-Base using Latent Syntactic Cues</a></p>
<p>Author: Matt Gardner ; Partha Pratim Talukdar ; Bryan Kisiel ; Tom Mitchell</p><p>Abstract: Automatically constructed Knowledge Bases (KBs) are often incomplete and there is a genuine need to improve their coverage. Path Ranking Algorithm (PRA) is a recently proposed method which aims to improve KB coverage by performing inference directly over the KB graph. For the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRAbased approaches on the KB inference task. We present extensive experimental results validating this finding. The resources presented in this paper are publicly available.</p><p>5 0.14198449 <a title="51-tfidf-5" href="./emnlp-2013-Combining_Generative_and_Discriminative_Model_Scores_for_Distant_Supervision.html">49 emnlp-2013-Combining Generative and Discriminative Model Scores for Distant Supervision</a></p>
<p>Author: Benjamin Roth ; Dietrich Klakow</p><p>Abstract: Distant supervision is a scheme to generate noisy training data for relation extraction by aligning entities of a knowledge base with text. In this work we combine the output of a discriminative at-least-one learner with that of a generative hierarchical topic model to reduce the noise in distant supervision data. The combination significantly increases the ranking quality of extracted facts and achieves state-of-the-art extraction performance in an end-to-end setting. A simple linear interpolation of the model scores performs better than a parameter-free scheme based on nondominated sorting.</p><p>6 0.1379769 <a title="51-tfidf-6" href="./emnlp-2013-Semantic_Parsing_on_Freebase_from_Question-Answer_Pairs.html">166 emnlp-2013-Semantic Parsing on Freebase from Question-Answer Pairs</a></p>
<p>7 0.12768695 <a title="51-tfidf-7" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>8 0.12513471 <a title="51-tfidf-8" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>9 0.10588652 <a title="51-tfidf-9" href="./emnlp-2013-Scaling_Semantic_Parsers_with_On-the-Fly_Ontology_Matching.html">164 emnlp-2013-Scaling Semantic Parsers with On-the-Fly Ontology Matching</a></p>
<p>10 0.10580499 <a title="51-tfidf-10" href="./emnlp-2013-Joint_Coreference_Resolution_and_Named-Entity_Linking_with_Multi-Pass_Sieves.html">112 emnlp-2013-Joint Coreference Resolution and Named-Entity Linking with Multi-Pass Sieves</a></p>
<p>11 0.10315151 <a title="51-tfidf-11" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>12 0.097861908 <a title="51-tfidf-12" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>13 0.096163161 <a title="51-tfidf-13" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>14 0.089281358 <a title="51-tfidf-14" href="./emnlp-2013-Easy_Victories_and_Uphill_Battles_in_Coreference_Resolution.html">67 emnlp-2013-Easy Victories and Uphill Battles in Coreference Resolution</a></p>
<p>15 0.080676027 <a title="51-tfidf-15" href="./emnlp-2013-Effectiveness_and_Efficiency_of_Open_Relation_Extraction.html">68 emnlp-2013-Effectiveness and Efficiency of Open Relation Extraction</a></p>
<p>16 0.073137693 <a title="51-tfidf-16" href="./emnlp-2013-A_Constrained_Latent_Variable_Model_for_Coreference_Resolution.html">1 emnlp-2013-A Constrained Latent Variable Model for Coreference Resolution</a></p>
<p>17 0.070913136 <a title="51-tfidf-17" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>18 0.067698941 <a title="51-tfidf-18" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>19 0.066706367 <a title="51-tfidf-19" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>20 0.06019618 <a title="51-tfidf-20" href="./emnlp-2013-Learning_Biological_Processes_with_Global_Constraints.html">118 emnlp-2013-Learning Biological Processes with Global Constraints</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.21), (1, 0.111), (2, 0.112), (3, 0.017), (4, 0.036), (5, 0.163), (6, -0.039), (7, 0.105), (8, 0.09), (9, 0.034), (10, 0.158), (11, -0.103), (12, -0.041), (13, -0.063), (14, -0.219), (15, -0.115), (16, 0.022), (17, 0.086), (18, 0.044), (19, 0.095), (20, -0.142), (21, -0.089), (22, 0.08), (23, 0.159), (24, -0.139), (25, 0.129), (26, 0.103), (27, -0.08), (28, -0.207), (29, 0.004), (30, -0.081), (31, 0.068), (32, -0.057), (33, 0.106), (34, -0.093), (35, -0.131), (36, -0.012), (37, 0.02), (38, 0.062), (39, -0.023), (40, -0.045), (41, -0.055), (42, 0.036), (43, -0.019), (44, -0.011), (45, -0.008), (46, -0.004), (47, 0.024), (48, -0.015), (49, -0.092)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93875575 <a title="51-lsi-1" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>Author: Jason Weston ; Antoine Bordes ; Oksana Yakhnenko ; Nicolas Usunier</p><p>Abstract: This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over methods that rely on text features alone.</p><p>2 0.73629099 <a title="51-lsi-2" href="./emnlp-2013-Improving_Learning_and_Inference_in_a_Large_Knowledge-Base_using_Latent_Syntactic_Cues.html">102 emnlp-2013-Improving Learning and Inference in a Large Knowledge-Base using Latent Syntactic Cues</a></p>
<p>Author: Matt Gardner ; Partha Pratim Talukdar ; Bryan Kisiel ; Tom Mitchell</p><p>Abstract: Automatically constructed Knowledge Bases (KBs) are often incomplete and there is a genuine need to improve their coverage. Path Ranking Algorithm (PRA) is a recently proposed method which aims to improve KB coverage by performing inference directly over the KB graph. For the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRAbased approaches on the KB inference task. We present extensive experimental results validating this finding. The resources presented in this paper are publicly available.</p><p>3 0.64656883 <a title="51-lsi-3" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>Author: Hrushikesh Mohapatra ; Siddhanth Jain ; Soumen Chakrabarti</p><p>Abstract: Web search can be enhanced in powerful ways if token spans in Web text are annotated with disambiguated entities from large catalogs like Freebase. Entity annotators need to be trained on sample mention snippets. Wikipedia entities and annotated pages offer high-quality labeled data for training and evaluation. Unfortunately, Wikipedia features only one-ninth the number of entities as Freebase, and these are a highly biased sample of well-connected, frequently mentioned “head” entities. To bring hope to “tail” entities, we broaden our goal to a second task: assigning types to entities in Freebase but not Wikipedia. The two tasks are synergistic: knowing the types of unfamiliar entities helps disambiguate mentions, and words in mention contexts help assign types to entities. We present TMI, a bipartite graphical model for joint type-mention inference. TMI attempts no schema integration or entity resolution, but exploits the above-mentioned synergy. In experiments involving 780,000 people in Wikipedia, 2.3 million people in Freebase, 700 million Web pages, and over 20 professional editors, TMI shows considerable annotation accuracy improvement (e.g., 70%) compared to baselines (e.g., 46%), especially for “tail” and emerging entities. We also compare with Google’s recent annotations of the same corpus with Freebase entities, and report considerable improvements within the people domain.</p><p>4 0.52933878 <a title="51-lsi-4" href="./emnlp-2013-Combining_Generative_and_Discriminative_Model_Scores_for_Distant_Supervision.html">49 emnlp-2013-Combining Generative and Discriminative Model Scores for Distant Supervision</a></p>
<p>Author: Benjamin Roth ; Dietrich Klakow</p><p>Abstract: Distant supervision is a scheme to generate noisy training data for relation extraction by aligning entities of a knowledge base with text. In this work we combine the output of a discriminative at-least-one learner with that of a generative hierarchical topic model to reduce the noise in distant supervision data. The combination significantly increases the ranking quality of extracted facts and achieves state-of-the-art extraction performance in an end-to-end setting. A simple linear interpolation of the model scores performs better than a parameter-free scheme based on nondominated sorting.</p><p>5 0.49204889 <a title="51-lsi-5" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>Author: Xiao Cheng ; Dan Roth</p><p>Abstract: Wikification, commonly referred to as Disambiguation to Wikipedia (D2W), is the task of identifying concepts and entities in text and disambiguating them into the most specific corresponding Wikipedia pages. Previous approaches to D2W focused on the use of local and global statistics over the given text, Wikipedia articles and its link structures, to evaluate context compatibility among a list of probable candidates. However, these methods fail (often, embarrassingly), when some level of text understanding is needed to support Wikification. In this paper we introduce a novel approach to Wikification by incorporating, along with statistical methods, richer relational analysis of the text. We provide an extensible, efficient and modular Integer Linear Programming (ILP) formulation of Wikification that incorporates the entity-relation inference problem, and show that the ability to identify relations in text helps both candi- date generation and ranking Wikipedia titles considerably. Our results show significant improvements in both Wikification and the TAC Entity Linking task.</p><p>6 0.48150533 <a title="51-lsi-6" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>7 0.43332505 <a title="51-lsi-7" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>8 0.42695102 <a title="51-lsi-8" href="./emnlp-2013-Effectiveness_and_Efficiency_of_Open_Relation_Extraction.html">68 emnlp-2013-Effectiveness and Efficiency of Open Relation Extraction</a></p>
<p>9 0.41962487 <a title="51-lsi-9" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>10 0.41647369 <a title="51-lsi-10" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>11 0.38036585 <a title="51-lsi-11" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>12 0.35999325 <a title="51-lsi-12" href="./emnlp-2013-Semantic_Parsing_on_Freebase_from_Question-Answer_Pairs.html">166 emnlp-2013-Semantic Parsing on Freebase from Question-Answer Pairs</a></p>
<p>13 0.34190896 <a title="51-lsi-13" href="./emnlp-2013-Joint_Coreference_Resolution_and_Named-Entity_Linking_with_Multi-Pass_Sieves.html">112 emnlp-2013-Joint Coreference Resolution and Named-Entity Linking with Multi-Pass Sieves</a></p>
<p>14 0.32649666 <a title="51-lsi-14" href="./emnlp-2013-Scaling_Semantic_Parsers_with_On-the-Fly_Ontology_Matching.html">164 emnlp-2013-Scaling Semantic Parsers with On-the-Fly Ontology Matching</a></p>
<p>15 0.31124386 <a title="51-lsi-15" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>16 0.28228882 <a title="51-lsi-16" href="./emnlp-2013-Rule-Based_Information_Extraction_is_Dead%21_Long_Live_Rule-Based_Information_Extraction_Systems%21.html">161 emnlp-2013-Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!</a></p>
<p>17 0.28209254 <a title="51-lsi-17" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>18 0.27017897 <a title="51-lsi-18" href="./emnlp-2013-The_Topology_of_Semantic_Knowledge.html">182 emnlp-2013-The Topology of Semantic Knowledge</a></p>
<p>19 0.25504482 <a title="51-lsi-19" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>20 0.25416514 <a title="51-lsi-20" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.03), (9, 0.062), (18, 0.094), (22, 0.052), (30, 0.07), (50, 0.018), (51, 0.177), (65, 0.057), (66, 0.05), (71, 0.026), (75, 0.052), (77, 0.02), (78, 0.109), (90, 0.025), (96, 0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8946963 <a title="51-lda-1" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>Author: Jason Weston ; Antoine Bordes ; Oksana Yakhnenko ; Nicolas Usunier</p><p>Abstract: This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over methods that rely on text features alone.</p><p>2 0.83034056 <a title="51-lda-2" href="./emnlp-2013-Modeling_Scientific_Impact_with_Topical_Influence_Regression.html">133 emnlp-2013-Modeling Scientific Impact with Topical Influence Regression</a></p>
<p>Author: James Foulds ; Padhraic Smyth</p><p>Abstract: When reviewing scientific literature, it would be useful to have automatic tools that identify the most influential scientific articles as well as how ideas propagate between articles. In this context, this paper introduces topical influence, a quantitative measure of the extent to which an article tends to spread its topics to the articles that cite it. Given the text of the articles and their citation graph, we show how to learn a probabilistic model to recover both the degree of topical influence of each article and the influence relationships between articles. Experimental results on corpora from two well-known computer science conferences are used to illustrate and validate the proposed approach.</p><p>3 0.82592648 <a title="51-lda-3" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>Author: Zhongqing Wang ; Shoushan LI ; Fang Kong ; Guodong Zhou</p><p>Abstract: Personal profile information on social media like LinkedIn.com and Facebook.com is at the core of many interesting applications, such as talent recommendation and contextual advertising. However, personal profiles usually lack organization confronted with the large amount of available information. Therefore, it is always a challenge for people to find desired information from them. In this paper, we address the task of personal profile summarization by leveraging both personal profile textual information and social networks. Here, using social networks is motivated by the intuition that, people with similar academic, business or social connections (e.g. co-major, co-university, and cocorporation) tend to have similar experience and summaries. To achieve the learning process, we propose a collective factor graph (CoFG) model to incorporate all these resources of knowledge to summarize personal profiles with local textual attribute functions and social connection factors. Extensive evaluation on a large-scale dataset from LinkedIn.com demonstrates the effectiveness of the proposed approach. 1</p><p>4 0.82176161 <a title="51-lda-4" href="./emnlp-2013-Interpreting_Anaphoric_Shell_Nouns_using_Antecedents_of_Cataphoric_Shell_Nouns_as_Training_Data.html">108 emnlp-2013-Interpreting Anaphoric Shell Nouns using Antecedents of Cataphoric Shell Nouns as Training Data</a></p>
<p>Author: Varada Kolhatkar ; Heike Zinsmeister ; Graeme Hirst</p><p>Abstract: Interpreting anaphoric shell nouns (ASNs) such as this issue and this fact is essential to understanding virtually any substantial natural language text. One obstacle in developing methods for automatically interpreting ASNs is the lack of annotated data. We tackle this challenge by exploiting cataphoric shell nouns (CSNs) whose construction makes them particularly easy to interpret (e.g., the fact that X). We propose an approach that uses automatically extracted antecedents of CSNs as training data to interpret ASNs. We achieve precisions in the range of 0.35 (baseline = 0.21) to 0.72 (baseline = 0.44), depending upon the shell noun. 1</p><p>5 0.82155693 <a title="51-lda-5" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>Author: Yangfeng Ji ; Jacob Eisenstein</p><p>Abstract: Matrix and tensor factorization have been applied to a number of semantic relatedness tasks, including paraphrase identification. The key idea is that similarity in the latent space implies semantic relatedness. We describe three ways in which labeled data can improve the accuracy of these approaches on paraphrase classification. First, we design a new discriminative term-weighting metric called TF-KLD, which outperforms TF-IDF. Next, we show that using the latent representation from matrix factorization as features in a classification algorithm substantially improves accuracy. Finally, we combine latent features with fine-grained n-gram overlap features, yielding performance that is 3% more accurate than the prior state-of-the-art.</p><p>6 0.82063198 <a title="51-lda-6" href="./emnlp-2013-Scaling_Semantic_Parsers_with_On-the-Fly_Ontology_Matching.html">164 emnlp-2013-Scaling Semantic Parsers with On-the-Fly Ontology Matching</a></p>
<p>7 0.81897461 <a title="51-lda-7" href="./emnlp-2013-Improvements_to_the_Bayesian_Topic_N-Gram_Models.html">100 emnlp-2013-Improvements to the Bayesian Topic N-Gram Models</a></p>
<p>8 0.81466484 <a title="51-lda-8" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>9 0.80406177 <a title="51-lda-9" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>10 0.80294859 <a title="51-lda-10" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>11 0.80271423 <a title="51-lda-11" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>12 0.80022955 <a title="51-lda-12" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>13 0.79973757 <a title="51-lda-13" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>14 0.79946625 <a title="51-lda-14" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>15 0.79817015 <a title="51-lda-15" href="./emnlp-2013-Unsupervised_Induction_of_Cross-Lingual_Semantic_Relations.html">193 emnlp-2013-Unsupervised Induction of Cross-Lingual Semantic Relations</a></p>
<p>16 0.79731411 <a title="51-lda-16" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>17 0.79645395 <a title="51-lda-17" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>18 0.79624933 <a title="51-lda-18" href="./emnlp-2013-Exploiting_Zero_Pronouns_to_Improve_Chinese_Coreference_Resolution.html">80 emnlp-2013-Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</a></p>
<p>19 0.79624051 <a title="51-lda-19" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>20 0.79561281 <a title="51-lda-20" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
