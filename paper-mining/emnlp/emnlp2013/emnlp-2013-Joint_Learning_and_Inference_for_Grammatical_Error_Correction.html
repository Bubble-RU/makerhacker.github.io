<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-114" href="#">emnlp2013-114</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</h1>
<br/><p>Source: <a title="emnlp-2013-114-pdf" href="http://aclweb.org/anthology//D/D13/D13-1074.pdf">pdf</a></p><p>Author: Alla Rozovskaya ; Dan Roth</p><p>Abstract: State-of-the-art systems for grammatical error correction are based on a collection of independently-trained models for specific errors. Such models ignore linguistic interactions at the sentence level and thus do poorly on mistakes that involve grammatical dependencies among several words. In this paper, we identify linguistic structures with interacting grammatical properties and propose to address such dependencies via joint inference and joint learning. We show that it is possible to identify interactions well enough to facilitate a joint approach and, consequently, that joint methods correct incoherent predictions that independentlytrained classifiers tend to produce. Furthermore, because the joint learning model considers interacting phenomena during training, it is able to identify mistakes that require mak- ing multiple changes simultaneously and that standard approaches miss. Overall, our model significantly outperforms the Illinois system that placed first in the CoNLL-2013 shared task on grammatical error correction.</p><p>Reference: <a title="emnlp-2013-114-reference" href="../emnlp2013_reference/emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu l ino s  Abstract State-of-the-art systems for grammatical error correction are based on a collection of independently-trained models for specific errors. [sent-3, score-0.477]
</p><p>2 Such models ignore linguistic interactions at the sentence level and thus do poorly on mistakes that involve grammatical dependencies among several words. [sent-4, score-0.396]
</p><p>3 In this paper, we identify linguistic structures with interacting grammatical properties and propose to address such dependencies via joint inference and joint learning. [sent-5, score-1.349]
</p><p>4 We show that it is possible to identify interactions well enough to facilitate a joint approach and, consequently, that joint methods correct incoherent predictions that independentlytrained classifiers tend to produce. [sent-6, score-0.746]
</p><p>5 Furthermore, because the joint learning model considers interacting phenomena during training, it is able to identify mistakes that require mak-  ing multiple changes simultaneously and that standard approaches miss. [sent-7, score-0.786]
</p><p>6 Overall, our model significantly outperforms the Illinois system that placed first in the CoNLL-2013 shared task on grammatical error correction. [sent-8, score-0.404]
</p><p>7 In the past two years, three competitions devoted to grammatical error correction for nonnative writers took place: HOO-201 1 (Dale and Kilgarriff, 2011), HOO-2012 (Dale et al. [sent-10, score-0.505]
</p><p>8 A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al. [sent-17, score-0.468]
</p><p>9 The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. [sent-25, score-0.504]
</p><p>10 In the example shown in Figure 1, the agreement error on the verb “have”  interacts with the noun number error: a correction system that takes into account the context may infer, because of the word “phone”, that the verb number is correct. [sent-27, score-0.781]
</p><p>11 hc o2d0s1 i3n A Nsastoucria lti Loan fgoura Cgoem Ppruotcaetsiosin agl, L piang eusis 7t9ic1s–802, ers noun and agreement errors separately will fail to identify and correct the interacting errors shown in Fig. [sent-30, score-0.722]
</p><p>12 We believe that the reasons for that are three-fold: (1) Data: until very recently we did not have data that jointly annotates sufficiently many errors of interacting phenomena (see Sec. [sent-34, score-0.504]
</p><p>13 (2) Conceptual: Correcting errors in interacting linguistic phenomena requires that one identifies those phenomena and, more importantly, can recognize reliably the interacting components (e. [sent-36, score-1.022]
</p><p>14 (3) Technical:  The NLP community has started to better understand joint learning and inference and apply it to various phenomena (Roth and Yih, 2004; Punyakanok et al. [sent-41, score-0.473]
</p><p>15 Specifically: • We identify two pairs of interacting phenomena, subject-verb yan twd oa prtaiicrlse- oNfP ihnetearadc agreements; we show how to reliably identify these pairs in noisy ESL data, thereby facilitating the joint correction of these phenomena. [sent-46, score-0.85]
</p><p>16 • We propose two joint approaches: (1) a joint inference approach implemented on top 1of) individually learned models using an integer linear programming formulation (ILP, (Roth and Yih, 2004)), and (2) a model that jointly learns each pair of these phenomena. [sent-47, score-0.638]
</p><p>17 We show that each of these methods has its advantages, and that both solve the two challenges outlined above: the joint models exclude inconsistent predictions that violate linguistic constraints. [sent-48, score-0.368]
</p><p>18 The joint learning model exhibits superior performance, as it is also able to overcome the problem of the noisy context encountered by the individual mod-  els and to identify errors in contexts, where multiple changes need to be applied at the same time. [sent-49, score-0.422]
</p><p>19 We show that our joint models produce state-ofthe-art performance and, in particular, significantly outperform the University of Illinois system that 792 placed first in the CoNLL-2013 shared task, increasing the F1 score by 2 and 4 points in different evaluation settings. [sent-50, score-0.339]
</p><p>20 2  Task Description and Motivation  To illustrate the utility of jointly addressing interacting grammatical phenomena, we consider the corpus of the CoNLL-2013 shared task on grammatical error correction (Ng et al. [sent-51, score-1.089]
</p><p>21 The task focuses on the following five common mistakes made by ESL writers: article, preposition, noun number, subject-verb agreement, and verb form, and we address two interactions: article-NPhead and subjectverb. [sent-53, score-0.333]
</p><p>22 contains a good number of interacting errors article, noun, and verb agreement mistakes makes the data set well-suited for studying which approach works best for addressing interacting phenomena. [sent-76, score-1.06]
</p><p>23 The HOO-201 1 shared task collection (Dale and Kilgarriff, 2011) contains a very small number of noun and agreement errors (41 and 11 in test, respectively), while the HOO-2012 competition (Dale et al. [sent-77, score-0.401]
</p><p>24 First, the HOO-201 1 data set which they used does not contain a good number of errors in interacting structures. [sent-81, score-0.378]
</p><p>25 In contrast, we show how to identify the interacting structures’ components in a reliable way, and this plays a key role in the joint modeling improvements. [sent-83, score-0.61]
</p><p>26 Lack of data hindered other earlier efforts for error correction beyond individual language phenomena. [sent-84, score-0.361]
</p><p>27 (2006) applied machinetranslation techniques to correct noun number errors on mass nouns and article usage but their application was restricted to a small set of constructions. [sent-86, score-0.387]
</p><p>28 Park and Levy (201 1) proposed a language-modeling approach to whole sentence error correction but their model is not competitive with individually trained models. [sent-87, score-0.345]
</p><p>29 The Illinois system implements five machinelearning independently-trained classifiers that follow the popular approach to ESL error correction borrowed from the context-sensitive spelling correction task (Golding and Roth, 1999; Carlson et al. [sent-94, score-0.64]
</p><p>30 Article and preposition candidates are identified with a closed list of words; noun-phrase-initial contexts for the article classifier are determined using a shallow parser3 (Punyakanok and Roth, 2001). [sent-117, score-0.325]
</p><p>31 In the former, either the article or the noun number should be changed; in the latter, either the noun number or the verb agreement marker5. [sent-132, score-0.576]
</p><p>32 Since each ofthe independent classifiers (for nouns and for verb agreement) takes into account the other word as part of its features, they both infer that the verb number is correct and that the grammatical subject “student” should be plural. [sent-142, score-0.585]
</p><p>33 1 Structures for Joint Modeling We address two linguistic structures that are relevant for the grammatical phenomena considered: article-  NPhead and subject-verb. [sent-146, score-0.546]
</p><p>34 Article-NPhead structures are pairs of words, such that the first word is a candidate of type article, while the second word is a noun candidate. [sent-151, score-0.343]
</p><p>35 Given an article candidate, the head of its NP is determined using the POS information (this information is obtained from the article feature vector because the NP head is a feature used by the article system)6. [sent-152, score-0.605]
</p><p>36 Based on the accuracy results for identifying the structure components, we select those structures where the components are reliably identified. [sent-166, score-0.405]
</p><p>37 For article-NPhead, valid structures are those where the distance is at most three words. [sent-167, score-0.36]
</p><p>38 For subject-verb, we consider as valid those structures where the identi-  fied subject is located within two words to the left or three words to the right of the verb. [sent-168, score-0.414]
</p><p>39 The valid structures are selected as input to the joint model (Sec. [sent-169, score-0.518]
</p><p>40 The joint learning model considers only those valid structures whose components are adjacent. [sent-171, score-0.667]
</p><p>41 In adjacent structures the NP head immediately follows the article, and the verb immediately follows the subject. [sent-172, score-0.534]
</p><p>42 Note also that because a noun may belong both to an article-NPhead and a subject-verb  structure, the  structures contain an overlap. [sent-184, score-0.372]
</p><p>43 5  The Joint Model  In this section, we present the joint inference and the joint learning approaches. [sent-202, score-0.581]
</p><p>44 In the joint inference approach, we use the independently-learned models from the Illinois system, and the interacting target words identified earlier are considered only at inference stage. [sent-203, score-0.858]
</p><p>45 In the joint learning method, we jointly learn a model for the interacting phenomena. [sent-204, score-0.515]
</p><p>46 NInPhvaelaidd 7“sing” and “pl” refer to the grammatical number of noun structures, such as pl-sing are excluded via hard constraints (when we run joint inference) or via implicit  soft constraints (when we use joint learning). [sent-206, score-0.796]
</p><p>47 The purpose of joint inference is to include linguistic (i. [sent-209, score-0.416]
</p><p>48 The inference approach we develop in this paper follows the one proposed by Roth and Yih (2004) of training individual models and combining them at decision time via joint inference. [sent-213, score-0.42]
</p><p>49 The joint model thus selects a hypothesis that both obtains the best score according to the individual models and satisfies the constraints that reflect the interactions among the grammatical phenomena at the level of linguistic structures, as defined in Sec. [sent-218, score-0.677]
</p><p>50 Inference The joint inference is enforced at the level of structures, and each structure corresponds to one ILP instance. [sent-220, score-0.376]
</p><p>51 All structures consist of two or three words: when an article-NPhead structure and a subject-verb structure include the same noun, the structure input to the ILP consists of an article-nounand verb agreement candidates. [sent-221, score-0.472]
</p><p>52 Note that a subject in subject-verb structures is always third person, since all subjects in subject-verb structures are common nouns; other subjects, including pronouns, are excluded. [sent-223, score-0.657]
</p><p>53 Adjacent denotes a setting, where the joint inference is applied to structures with consecutive subject-verb). [sent-265, score-0.667]
</p><p>54 In all cases, the candidates that are not part of the structures are handled by the respective components of the Illinois system. [sent-270, score-0.381]
</p><p>55 Since we use features that can be computed from the small windows in the Google corpus, the joint learning model handles only adjacent structures (Sec. [sent-275, score-0.518]
</p><p>56 One reason for this is that the NP head and subject predictions are not 100% accurate, so input structures will have noise. [sent-279, score-0.485]
</p><p>57 In the joint approach, the joint components presented in Sec. [sent-283, score-0.496]
</p><p>58 Joint Learning: we compare the Illinois system with a model that incorporates jointly-trained components for the two linguistic structures that we described in Sec. [sent-292, score-0.414]
</p><p>59 Joint Learning and Inference: we apply joint inference to the output of the joint learning system to account for dependencies not covered by the joint learning model. [sent-296, score-0.825]
</p><p>60 1 Joint Inference Results Table 7 shows the results of applying joint inference to the Illinois system. [sent-301, score-0.376]
</p><p>61 The results for the joint inference are shown in two settings, adjacent and all structures, so that  later we can compare joint inference with the joint learning model that handles only adjacent structures. [sent-303, score-1.085]
</p><p>62 Illinois-NBArticle denotes the Illinois system, where the discriminative denotes a setting, where the structure components  are consecutive  article model is replaced with a NB classifier. [sent-318, score-0.335]
</p><p>63 In all cases, the candidates that are not part of the structures are handled by the respective components of the Illinois system. [sent-323, score-0.381]
</p><p>64 It is also interesting to note that the key improvement comes from considering structures whose components are adjacent. [sent-326, score-0.364]
</p><p>65 The heuristics differ from the joint inference in that they enforce agreement by always changing either the noun (Na ı¨veNoun) or the verb (Na ı¨veVerb), while the joint inference does this using the scores produced by the independent models. [sent-330, score-1.069]
</p><p>66 Recall that valid structures include only those whose components can be identified in a reliable way (Sec. [sent-333, score-0.458]
</p><p>67 To evaluate the impact of that filtering, we perform two experiments with subject-verb structures (long-distance dependencies are more common in those constructions than in the articleNPhead structures): first, we apply joint inference to all subject-verb structures. [sent-336, score-0.625]
</p><p>68 28, on original and revised gold data, respectively, which is significantly worse than the results on subject-verb structures in Table 7 (3 1. [sent-339, score-0.33]
</p><p>69 Furthermore, when we apply joint inference to those structures which were excluded by filtering in Sec. [sent-342, score-0.625]
</p><p>70 These results demonstrate that the joint inference improvements are due to structures whose components can be identified with high accuracy and that it is essential to identify these structures; bad structures, on the other hand, hurt performance. [sent-347, score-0.835]
</p><p>71 3 makes use of a discriminative article model, while the joint model uses NB, we also show results, where the article model is replaced by a NB classifier trained on the Google corpus. [sent-352, score-0.535]
</p><p>72 3 Joint Learning and Inference Results Finally, we apply joint inference to the output of the joint learning system in Sec. [sent-355, score-0.62]
</p><p>73 Table 9 shows the results of the Illinois model, the model that applies joint inference and joint learning separately, and both. [sent-358, score-0.581]
</p><p>74 Even though the joint learning performs better than the joint inference, the joint learning  covers only adjacent structures. [sent-359, score-0.679]
</p><p>75 Furthermore, joint  learning does not address overlapping  structures of  triples that consist of article, subject, and verb (6% of all structures). [sent-360, score-0.562]
</p><p>76 Joint inference allows us to ensure consistent predictions in cases not addressed by the  obtained by the  top CoNLL-2013  shared task  system  from the University of Illinois. [sent-361, score-0.421]
</p><p>77 JL  and JI stand for joint learning and joint  inference, respectively. [sent-362, score-0.41]
</p><p>78 Results of the joint models that include the joint inference component are shown for structures of all distances. [sent-377, score-0.83]
</p><p>79 All joint systems demonstrate a statistically significant improvement over the Illinois system; joint learning improvements are also statistically significant compared to the joint inference results (McNemar’s test, p < 0. [sent-379, score-0.786]
</p><p>80 Indeed, we can get a small improvement by adding joint inference on top of the joint learning on original annotations. [sent-382, score-0.581]
</p><p>81 7  Discussion and Error Analysis  In the previous section, we evaluated the proposed joint inference and joint learning models that handle interacting grammatical phenomena. [sent-385, score-1.022]
</p><p>82 These results are interesting from the point of view of developing a practical error correction system. [sent-387, score-0.317]
</p><p>83 However, recall that the errors in the interact-  799  ing structures are only a subset of mistakes in the CoNLL-2013 data set but the evaluation in Sec. [sent-388, score-0.477]
</p><p>84 From a scientific point of view, it is interesting to evaluate the impact of the joint models more precisely by considering the improvements on the relevant structures only. [sent-390, score-0.454]
</p><p>85 In Table 10 we show examples of mistakes that the model that uses joint learning and inference is able to identify correctly, along with the original predictions made by the Illinois system. [sent-400, score-0.625]
</p><p>86 The joint inference approach does this by enforcing linguistic constraints on the output. [sent-403, score-0.518]
</p><p>87 Joint learning, on the other hand, is superior to joint inference, since it is better at modeling interactions where multiple errors occur simultaneously it eliminates the noisy context present when learning the independent classifiers. [sent-406, score-0.405]
</p><p>88 Consider the first example from Table 10, where both the noun and the agreement classifiers receive noisy input: the verb “help” and –  the noun “technologies” act as part of input features for the noun and agreement classifiers, respectively. [sent-407, score-0.735]
</p><p>89 Finally, an important distinction of the joint learning method is that it considers all possible output sequences in training, and thus it is able to better identify errors that require multiple changes, such as the last example in Table 10, where the Illinois system proposes no changes. [sent-409, score-0.413]
</p><p>90 1 Error Correction: Challenges We finalize our discussion with a few comments on the challenges of the error correction task. [sent-411, score-0.317]
</p><p>91 The low error rates are the key reason the error correction task is so difficult: it is quite challenging for a system to improve over a writer that already performs at the level of over 90%. [sent-414, score-0.466]
</p><p>92 Practical error correction systems, however, should be tuned to minimize recall to guarantee that the overall quality of the text does not go down. [sent-417, score-0.317]
</p><p>93 Indeed, the error sparsity makes it very challenging to identify mistakes accurately, and no system in the shared task achieves a precision over 50%. [sent-418, score-0.413]
</p><p>94 We addressed two pairs of interacting phenomena and showed that it is possible to reliably identify their components, thereby facilitating the joint approach. [sent-434, score-0.7]
</p><p>95 We described two joint methods: a joint inference approach implemented via ILP and a joint learning model. [sent-435, score-0.786]
</p><p>96 The joint inference enforces constraints using the scores produced by the independently-trained models. [sent-436, score-0.442]
</p><p>97 The joint learning model learns the interacting phenomena as structures. [sent-437, score-0.583]
</p><p>98 A report on the preposition and determiner error correction shared task. [sent-512, score-0.496]
</p><p>99 A classifier-based approach to preposition and determiner error correction in L2 English. [sent-520, score-0.401]
</p><p>100 University of Illinois system in HOO text correction shared task. [sent-671, score-0.341]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('illinois', 0.41), ('interacting', 0.281), ('structures', 0.249), ('correction', 0.207), ('joint', 0.205), ('esl', 0.196), ('rozovskaya', 0.196), ('inference', 0.171), ('article', 0.165), ('grammatical', 0.16), ('mistakes', 0.131), ('sing', 0.121), ('dahlmeier', 0.116), ('agreement', 0.115), ('error', 0.11), ('verb', 0.108), ('roth', 0.106), ('subject', 0.101), ('phenomena', 0.097), ('errors', 0.097), ('shared', 0.095), ('noun', 0.094), ('pl', 0.089), ('correcting', 0.088), ('components', 0.086), ('np', 0.086), ('nb', 0.084), ('preposition', 0.084), ('revised', 0.081), ('predictions', 0.08), ('classifiers', 0.077), ('independentlytrained', 0.076), ('confusion', 0.071), ('dale', 0.067), ('constraints', 0.066), ('interactions', 0.065), ('adjacent', 0.064), ('valid', 0.064), ('ilp', 0.061), ('gamon', 0.06), ('punyakanok', 0.06), ('subjects', 0.058), ('hoo', 0.057), ('kilgarriff', 0.057), ('veverb', 0.057), ('sammons', 0.056), ('yih', 0.056), ('head', 0.055), ('na', 0.051), ('corrections', 0.05), ('distance', 0.047), ('addressing', 0.047), ('candidates', 0.046), ('tetreault', 0.045), ('learners', 0.045), ('mcnemar', 0.045), ('individual', 0.044), ('reliably', 0.043), ('inconsistent', 0.043), ('ng', 0.043), ('indefinite', 0.042), ('chodorow', 0.042), ('denotes', 0.042), ('brockett', 0.04), ('linguistic', 0.04), ('system', 0.039), ('noisy', 0.038), ('identify', 0.038), ('confusable', 0.038), ('gioja', 0.038), ('golding', 0.038), ('izumi', 0.038), ('venoun', 0.038), ('enforcing', 0.036), ('innovative', 0.036), ('addressed', 0.036), ('educational', 0.035), ('native', 0.035), ('considers', 0.034), ('felice', 0.033), ('manchester', 0.033), ('cogcomp', 0.033), ('interact', 0.032), ('nouns', 0.031), ('identified', 0.03), ('jointly', 0.029), ('whose', 0.029), ('decoder', 0.029), ('belong', 0.029), ('immediately', 0.029), ('declarative', 0.028), ('erroneous', 0.028), ('competitions', 0.028), ('inoi', 0.028), ('individually', 0.028), ('percentage', 0.027), ('heads', 0.027), ('accuracy', 0.027), ('independently', 0.027), ('plural', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="114-tfidf-1" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>Author: Alla Rozovskaya ; Dan Roth</p><p>Abstract: State-of-the-art systems for grammatical error correction are based on a collection of independently-trained models for specific errors. Such models ignore linguistic interactions at the sentence level and thus do poorly on mistakes that involve grammatical dependencies among several words. In this paper, we identify linguistic structures with interacting grammatical properties and propose to address such dependencies via joint inference and joint learning. We show that it is possible to identify interactions well enough to facilitate a joint approach and, consequently, that joint methods correct incoherent predictions that independentlytrained classifiers tend to produce. Furthermore, because the joint learning model considers interacting phenomena during training, it is able to identify mistakes that require mak- ing multiple changes simultaneously and that standard approaches miss. Overall, our model significantly outperforms the Illinois system that placed first in the CoNLL-2013 shared task on grammatical error correction.</p><p>2 0.11809659 <a title="114-tfidf-2" href="./emnlp-2013-Using_Soft_Constraints_in_Joint_Inference_for_Clinical_Concept_Recognition.html">198 emnlp-2013-Using Soft Constraints in Joint Inference for Clinical Concept Recognition</a></p>
<p>Author: Prateek Jindal ; Dan Roth</p><p>Abstract: This paper introduces IQPs (Integer Quadratic Programs) as a way to model joint inference for the task of concept recognition in clinical domain. IQPs make it possible to easily incorporate soft constraints in the optimization framework and still support exact global inference. We show that soft constraints give statistically significant performance improvements when compared to hard constraints.</p><p>3 0.1109192 <a title="114-tfidf-3" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>Author: Jonathan K. Kummerfeld ; Dan Klein</p><p>Abstract: Coreference resolution metrics quantify errors but do not analyze them. Here, we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types. Using this tool, we first compare the error distributions across a large set of systems, then analyze common errors across the top ten systems, empirically characterizing the major unsolved challenges of the coreference resolution task.</p><p>4 0.10245173 <a title="114-tfidf-4" href="./emnlp-2013-A_Constrained_Latent_Variable_Model_for_Coreference_Resolution.html">1 emnlp-2013-A Constrained Latent Variable Model for Coreference Resolution</a></p>
<p>Author: Kai-Wei Chang ; Rajhans Samdani ; Dan Roth</p><p>Abstract: Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L3M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L3M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L3M and its constrained version, CL3M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.</p><p>5 0.082291208 <a title="114-tfidf-5" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>Author: Xiao Cheng ; Dan Roth</p><p>Abstract: Wikification, commonly referred to as Disambiguation to Wikipedia (D2W), is the task of identifying concepts and entities in text and disambiguating them into the most specific corresponding Wikipedia pages. Previous approaches to D2W focused on the use of local and global statistics over the given text, Wikipedia articles and its link structures, to evaluate context compatibility among a list of probable candidates. However, these methods fail (often, embarrassingly), when some level of text understanding is needed to support Wikification. In this paper we introduce a novel approach to Wikification by incorporating, along with statistical methods, richer relational analysis of the text. We provide an extensible, efficient and modular Integer Linear Programming (ILP) formulation of Wikification that incorporates the entity-relation inference problem, and show that the ability to identify relations in text helps both candi- date generation and ranking Wikipedia titles considerably. Our results show significant improvements in both Wikification and the TAC Entity Linking task.</p><p>6 0.065775201 <a title="114-tfidf-6" href="./emnlp-2013-Easy_Victories_and_Uphill_Battles_in_Coreference_Resolution.html">67 emnlp-2013-Easy Victories and Uphill Battles in Coreference Resolution</a></p>
<p>7 0.06175657 <a title="114-tfidf-7" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>8 0.061377063 <a title="114-tfidf-8" href="./emnlp-2013-Identifying_Phrasal_Verbs_Using_Many_Bilingual_Corpora.html">96 emnlp-2013-Identifying Phrasal Verbs Using Many Bilingual Corpora</a></p>
<p>9 0.059917822 <a title="114-tfidf-9" href="./emnlp-2013-The_Effects_of_Syntactic_Features_in_Automatic_Prediction_of_Morphology.html">181 emnlp-2013-The Effects of Syntactic Features in Automatic Prediction of Morphology</a></p>
<p>10 0.059340261 <a title="114-tfidf-10" href="./emnlp-2013-Gender_Inference_of_Twitter_Users_in_Non-English_Contexts.html">89 emnlp-2013-Gender Inference of Twitter Users in Non-English Contexts</a></p>
<p>11 0.056352474 <a title="114-tfidf-11" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>12 0.055366635 <a title="114-tfidf-12" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<p>13 0.055189777 <a title="114-tfidf-13" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>14 0.054054208 <a title="114-tfidf-14" href="./emnlp-2013-Fast_Joint_Compression_and_Summarization_via_Graph_Cuts.html">85 emnlp-2013-Fast Joint Compression and Summarization via Graph Cuts</a></p>
<p>15 0.053365048 <a title="114-tfidf-15" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>16 0.052793 <a title="114-tfidf-16" href="./emnlp-2013-Dependency_Language_Models_for_Sentence_Completion.html">58 emnlp-2013-Dependency Language Models for Sentence Completion</a></p>
<p>17 0.052366916 <a title="114-tfidf-17" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>18 0.051918309 <a title="114-tfidf-18" href="./emnlp-2013-Modeling_Scientific_Impact_with_Topical_Influence_Regression.html">133 emnlp-2013-Modeling Scientific Impact with Topical Influence Regression</a></p>
<p>19 0.051888485 <a title="114-tfidf-19" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>20 0.050905496 <a title="114-tfidf-20" href="./emnlp-2013-Effectiveness_and_Efficiency_of_Open_Relation_Extraction.html">68 emnlp-2013-Effectiveness and Efficiency of Open Relation Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.207), (1, 0.048), (2, 0.063), (3, -0.016), (4, -0.062), (5, -0.0), (6, 0.023), (7, -0.048), (8, -0.012), (9, -0.019), (10, -0.004), (11, 0.032), (12, -0.028), (13, 0.087), (14, 0.019), (15, 0.003), (16, -0.021), (17, -0.036), (18, 0.082), (19, 0.039), (20, 0.007), (21, 0.105), (22, 0.049), (23, 0.053), (24, 0.108), (25, 0.052), (26, 0.011), (27, -0.064), (28, -0.051), (29, 0.053), (30, -0.034), (31, -0.061), (32, -0.09), (33, -0.04), (34, -0.016), (35, 0.138), (36, -0.037), (37, 0.002), (38, 0.06), (39, 0.039), (40, 0.06), (41, 0.015), (42, -0.003), (43, 0.024), (44, -0.111), (45, 0.039), (46, -0.16), (47, -0.139), (48, 0.065), (49, -0.175)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96331084 <a title="114-lsi-1" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>Author: Alla Rozovskaya ; Dan Roth</p><p>Abstract: State-of-the-art systems for grammatical error correction are based on a collection of independently-trained models for specific errors. Such models ignore linguistic interactions at the sentence level and thus do poorly on mistakes that involve grammatical dependencies among several words. In this paper, we identify linguistic structures with interacting grammatical properties and propose to address such dependencies via joint inference and joint learning. We show that it is possible to identify interactions well enough to facilitate a joint approach and, consequently, that joint methods correct incoherent predictions that independentlytrained classifiers tend to produce. Furthermore, because the joint learning model considers interacting phenomena during training, it is able to identify mistakes that require mak- ing multiple changes simultaneously and that standard approaches miss. Overall, our model significantly outperforms the Illinois system that placed first in the CoNLL-2013 shared task on grammatical error correction.</p><p>2 0.76029003 <a title="114-lsi-2" href="./emnlp-2013-Using_Soft_Constraints_in_Joint_Inference_for_Clinical_Concept_Recognition.html">198 emnlp-2013-Using Soft Constraints in Joint Inference for Clinical Concept Recognition</a></p>
<p>Author: Prateek Jindal ; Dan Roth</p><p>Abstract: This paper introduces IQPs (Integer Quadratic Programs) as a way to model joint inference for the task of concept recognition in clinical domain. IQPs make it possible to easily incorporate soft constraints in the optimization framework and still support exact global inference. We show that soft constraints give statistically significant performance improvements when compared to hard constraints.</p><p>3 0.46874955 <a title="114-lsi-3" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>Author: Xiao Cheng ; Dan Roth</p><p>Abstract: Wikification, commonly referred to as Disambiguation to Wikipedia (D2W), is the task of identifying concepts and entities in text and disambiguating them into the most specific corresponding Wikipedia pages. Previous approaches to D2W focused on the use of local and global statistics over the given text, Wikipedia articles and its link structures, to evaluate context compatibility among a list of probable candidates. However, these methods fail (often, embarrassingly), when some level of text understanding is needed to support Wikification. In this paper we introduce a novel approach to Wikification by incorporating, along with statistical methods, richer relational analysis of the text. We provide an extensible, efficient and modular Integer Linear Programming (ILP) formulation of Wikification that incorporates the entity-relation inference problem, and show that the ability to identify relations in text helps both candi- date generation and ranking Wikipedia titles considerably. Our results show significant improvements in both Wikification and the TAC Entity Linking task.</p><p>4 0.46499702 <a title="114-lsi-4" href="./emnlp-2013-Detecting_Promotional_Content_in_Wikipedia.html">61 emnlp-2013-Detecting Promotional Content in Wikipedia</a></p>
<p>Author: Shruti Bhosale ; Heath Vinicombe ; Raymond Mooney</p><p>Abstract: This paper presents an approach for detecting promotional content in Wikipedia. By incorporating stylometric features, including features based on n-gram and PCFG language models, we demonstrate improved accuracy at identifying promotional articles, compared to using only lexical information and metafeatures.</p><p>5 0.44455701 <a title="114-lsi-5" href="./emnlp-2013-Combining_Generative_and_Discriminative_Model_Scores_for_Distant_Supervision.html">49 emnlp-2013-Combining Generative and Discriminative Model Scores for Distant Supervision</a></p>
<p>Author: Benjamin Roth ; Dietrich Klakow</p><p>Abstract: Distant supervision is a scheme to generate noisy training data for relation extraction by aligning entities of a knowledge base with text. In this work we combine the output of a discriminative at-least-one learner with that of a generative hierarchical topic model to reduce the noise in distant supervision data. The combination significantly increases the ranking quality of extracted facts and achieves state-of-the-art extraction performance in an end-to-end setting. A simple linear interpolation of the model scores performs better than a parameter-free scheme based on nondominated sorting.</p><p>6 0.43132907 <a title="114-lsi-6" href="./emnlp-2013-Two-Stage_Method_for_Large-Scale_Acquisition_of_Contradiction_Pattern_Pairs_using_Entailment.html">189 emnlp-2013-Two-Stage Method for Large-Scale Acquisition of Contradiction Pattern Pairs using Entailment</a></p>
<p>7 0.42849454 <a title="114-lsi-7" href="./emnlp-2013-A_Constrained_Latent_Variable_Model_for_Coreference_Resolution.html">1 emnlp-2013-A Constrained Latent Variable Model for Coreference Resolution</a></p>
<p>8 0.4205839 <a title="114-lsi-8" href="./emnlp-2013-Automatically_Classifying_Edit_Categories_in_Wikipedia_Revisions.html">34 emnlp-2013-Automatically Classifying Edit Categories in Wikipedia Revisions</a></p>
<p>9 0.41762003 <a title="114-lsi-9" href="./emnlp-2013-Animacy_Detection_with_Voting_Models.html">23 emnlp-2013-Animacy Detection with Voting Models</a></p>
<p>10 0.41750944 <a title="114-lsi-10" href="./emnlp-2013-Using_Topic_Modeling_to_Improve_Prediction_of_Neuroticism_and_Depression_in_College_Students.html">199 emnlp-2013-Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students</a></p>
<p>11 0.4016231 <a title="114-lsi-11" href="./emnlp-2013-The_VerbCorner_Project%3A_Toward_an_Empirically-Based_Semantic_Decomposition_of_Verbs.html">183 emnlp-2013-The VerbCorner Project: Toward an Empirically-Based Semantic Decomposition of Verbs</a></p>
<p>12 0.39270541 <a title="114-lsi-12" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>13 0.38678002 <a title="114-lsi-13" href="./emnlp-2013-Sentiment_Analysis%3A_How_to_Derive_Prior_Polarities_from_SentiWordNet.html">170 emnlp-2013-Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet</a></p>
<p>14 0.38565511 <a title="114-lsi-14" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>15 0.3732951 <a title="114-lsi-15" href="./emnlp-2013-Effectiveness_and_Efficiency_of_Open_Relation_Extraction.html">68 emnlp-2013-Effectiveness and Efficiency of Open Relation Extraction</a></p>
<p>16 0.37009397 <a title="114-lsi-16" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>17 0.36827803 <a title="114-lsi-17" href="./emnlp-2013-Learning_to_Freestyle%3A_Hip_Hop_Challenge-Response_Induction_via_Transduction_Rule_Segmentation.html">122 emnlp-2013-Learning to Freestyle: Hip Hop Challenge-Response Induction via Transduction Rule Segmentation</a></p>
<p>18 0.36723238 <a title="114-lsi-18" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>19 0.36620435 <a title="114-lsi-19" href="./emnlp-2013-Joint_Parsing_and_Disfluency_Detection_in_Linear_Time.html">116 emnlp-2013-Joint Parsing and Disfluency Detection in Linear Time</a></p>
<p>20 0.3598032 <a title="114-lsi-20" href="./emnlp-2013-Easy_Victories_and_Uphill_Battles_in_Coreference_Resolution.html">67 emnlp-2013-Easy Victories and Uphill Battles in Coreference Resolution</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.049), (18, 0.034), (22, 0.063), (30, 0.084), (45, 0.018), (50, 0.034), (51, 0.176), (66, 0.058), (71, 0.029), (72, 0.198), (75, 0.05), (77, 0.043), (90, 0.013), (95, 0.031), (96, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83156234 <a title="114-lda-1" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>Author: Alla Rozovskaya ; Dan Roth</p><p>Abstract: State-of-the-art systems for grammatical error correction are based on a collection of independently-trained models for specific errors. Such models ignore linguistic interactions at the sentence level and thus do poorly on mistakes that involve grammatical dependencies among several words. In this paper, we identify linguistic structures with interacting grammatical properties and propose to address such dependencies via joint inference and joint learning. We show that it is possible to identify interactions well enough to facilitate a joint approach and, consequently, that joint methods correct incoherent predictions that independentlytrained classifiers tend to produce. Furthermore, because the joint learning model considers interacting phenomena during training, it is able to identify mistakes that require mak- ing multiple changes simultaneously and that standard approaches miss. Overall, our model significantly outperforms the Illinois system that placed first in the CoNLL-2013 shared task on grammatical error correction.</p><p>2 0.73856473 <a title="114-lda-2" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>Author: Xiaoqing Zheng ; Hanyang Chen ; Tianyu Xu</p><p>Abstract: This study explores the feasibility of performing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representation of Chinese characters, and use these improved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-theart performance with minimal computational cost. We also describe a perceptron-style algorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented.</p><p>3 0.73186749 <a title="114-lda-3" href="./emnlp-2013-Document_Summarization_via_Guided_Sentence_Compression.html">65 emnlp-2013-Document Summarization via Guided Sentence Compression</a></p>
<p>Author: Chen Li ; Fei Liu ; Fuliang Weng ; Yang Liu</p><p>Abstract: Joint compression and summarization has been used recently to generate high quality summaries. However, such word-based joint optimization is computationally expensive. In this paper we adopt the ‘sentence compression + sentence selection’ pipeline approach for compressive summarization, but propose to perform summary guided compression, rather than generic sentence-based compression. To create an annotated corpus, the human annotators were asked to compress sentences while explicitly given the important summary words in the sentences. Using this corpus, we train a supervised sentence compression model using a set of word-, syntax-, and documentlevel features. During summarization, we use multiple compressed sentences in the integer linear programming framework to select . salient summary sentences. Our results on the TAC 2008 and 2011 summarization data sets show that by incorporating the guided sentence compression model, our summarization system can yield significant performance gain as compared to the state-of-the-art.</p><p>4 0.72818398 <a title="114-lda-4" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>Author: Zhongqing Wang ; Shoushan LI ; Fang Kong ; Guodong Zhou</p><p>Abstract: Personal profile information on social media like LinkedIn.com and Facebook.com is at the core of many interesting applications, such as talent recommendation and contextual advertising. However, personal profiles usually lack organization confronted with the large amount of available information. Therefore, it is always a challenge for people to find desired information from them. In this paper, we address the task of personal profile summarization by leveraging both personal profile textual information and social networks. Here, using social networks is motivated by the intuition that, people with similar academic, business or social connections (e.g. co-major, co-university, and cocorporation) tend to have similar experience and summaries. To achieve the learning process, we propose a collective factor graph (CoFG) model to incorporate all these resources of knowledge to summarize personal profiles with local textual attribute functions and social connection factors. Extensive evaluation on a large-scale dataset from LinkedIn.com demonstrates the effectiveness of the proposed approach. 1</p><p>5 0.72637761 <a title="114-lda-5" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>Author: Jesus Gonzalez-Rubio ; Daniel Ortiz-Martinez ; Jose-Miguel Benedi ; Francisco Casacuberta</p><p>Abstract: Current automatic machine translation systems are not able to generate error-free translations and human intervention is often required to correct their output. Alternatively, an interactive framework that integrates the human knowledge into the translation process has been presented in previous works. Here, we describe a new interactive machine translation approach that is able to work with phrase-based and hierarchical translation models, and integrates error-correction all in a unified statistical framework. In our experiments, our approach outperforms previous interactive translation systems, and achieves estimated effort reductions of as much as 48% relative over a traditional post-edition system.</p><p>6 0.72598082 <a title="114-lda-6" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>7 0.72517568 <a title="114-lda-7" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>8 0.72410631 <a title="114-lda-8" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>9 0.72285056 <a title="114-lda-9" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>10 0.72240144 <a title="114-lda-10" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>11 0.7219882 <a title="114-lda-11" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>12 0.72155434 <a title="114-lda-12" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>13 0.72003758 <a title="114-lda-13" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>14 0.71980822 <a title="114-lda-14" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>15 0.71964359 <a title="114-lda-15" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>16 0.71944952 <a title="114-lda-16" href="./emnlp-2013-Automatically_Determining_a_Proper_Length_for_Multi-Document_Summarization%3A_A_Bayesian_Nonparametric_Approach.html">36 emnlp-2013-Automatically Determining a Proper Length for Multi-Document Summarization: A Bayesian Nonparametric Approach</a></p>
<p>17 0.7182833 <a title="114-lda-17" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>18 0.71686953 <a title="114-lda-18" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>19 0.71581972 <a title="114-lda-19" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>20 0.71473306 <a title="114-lda-20" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
