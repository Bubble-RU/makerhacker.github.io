<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1 emnlp-2013-A Constrained Latent Variable Model for Coreference Resolution</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-1" href="#">emnlp2013-1</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1 emnlp-2013-A Constrained Latent Variable Model for Coreference Resolution</h1>
<br/><p>Source: <a title="emnlp-2013-1-pdf" href="http://aclweb.org/anthology//D/D13/D13-1057.pdf">pdf</a></p><p>Author: Kai-Wei Chang ; Rajhans Samdani ; Dan Roth</p><p>Abstract: Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L3M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L3M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L3M and its constrained version, CL3M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.</p><p>Reference: <a title="emnlp-2013-1-reference" href="../emnlp2013_reference/emnlp-2013-A_Constrained_Latent_Variable_Model_for_Coreference_Resolution_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu l ino s  Abstract Coreference resolution is a well known clustering task in Natural Language Processing. [sent-2, score-0.347]
</p><p>2 In this paper, we describe the Latent Left Linking model (L3M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. [sent-3, score-0.668]
</p><p>3 We show that L3M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. [sent-4, score-0.231]
</p><p>4 Experiments on ACE and Ontonotes data show that L3M and its constrained version, CL3M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature. [sent-5, score-0.231]
</p><p>5 1 Introduction Coreference resolution is a challenging task, that involves identification and clustering of noun phrases mentions that refer to the same real-world entity. [sent-6, score-0.482]
</p><p>6 Most machine learning approaches to coreference resolution learn a scoring function to estimate the compatibility between two mentions or two sets of previously clustered mentions. [sent-7, score-0.892]
</p><p>7 Then, a decoding algorithm is designed to aggregate these scores and find an optimal clustering assignment. [sent-8, score-0.16]
</p><p>8 The most popular of these frameworks is the pairwise mention model (Soon et al. [sent-9, score-0.593]
</p><p>9 , 2001 ; Ng and Cardie, 2002; Bengtson and Roth, 2008), which learns a compatibility score of mention-pairs and uses these pairwise scores to obtain a global clustering. [sent-10, score-0.362]
</p><p>10 Recently, efforts have been made (Haghighi and Klein, 2010; Rahman and Ng, 2011b; Rahman and Ng, 2011c) to consider models that capture higher order interactions, in particular, between mentions 601 and previously identified entities (that is, between mentions and clusters). [sent-11, score-0.393]
</p><p>11 This paper focuses on a novel and principled machine learning framework that pushes the state-ofthe-art while operating at a mention-pair granularity. [sent-13, score-0.234]
</p><p>12 We present two models the Latent Left-Linking Model (L3M), and a version of that is augmented with domain knowledge-based constraints, the Constrained Latent Left-Linking Model (CL3M). [sent-14, score-0.043]
</p><p>13 L3M admits efficient inference, linking each mention to a previously occurring mention to its left, much like the existing best-left-link inference models (Ng and Cardie, 2002; Bengtson and Roth, 2008). [sent-15, score-0.941]
</p><p>14 However, unlike previous best-link techniques, learning in our case is performed jointly with decoding we present a novel latent structural SVM approach, optimized using a fast stochastic gradient-based technique. [sent-16, score-0.149]
</p><p>15 Furthermore, we present a probabilistic generalization of L3M that is more expressive in that it is capable of considering mention-entity interactions using scores at the mention-pair granularity. [sent-17, score-0.116]
</p><p>16 CL3M augments L3M with knowledge-based constraints following (Roth and Yih, 2004; Denis and Baldridge, 2007). [sent-20, score-0.15]
</p><p>17 In L3M, domain-specific constraints are incorporated into learning and inference in a straightforward way. [sent-24, score-0.241]
</p><p>18 CL3M scores a mention’s contribution to its cluster by combining the corresponding score ProceSe datintlges, o Wfa tsh ein 2g01to3n, C UoSnfAe,re 1n8c-e2 o1n O Ecmtopbier ic 2a0l1 M3. [sent-25, score-0.085]
</p><p>19 Most importantly, in our experiments on benchmark coreference datasets, we show that CL3M, with just five constraints, compares favorably with other, more complicated, state-of-the-art algorithms on a variety of evaluation metrics. [sent-28, score-0.454]
</p><p>20 Overall, the main contribution of this paper is a principled machine learning model operating at mention-pair granularity, using easy to implement constraint-augmented inference and learning, that yields competitive results on coreference resolution on Ontonotes-5. [sent-29, score-0.89]
</p><p>21 2  Related Work  The idea of Latent Left-linking Model (L3M) is inspired by a popular inference approach to coreference which we call the Best-Left-Link approach (Ng and Cardie, 2002; Bengtson and Roth, 2008). [sent-32, score-0.597]
</p><p>22 In the best-left-link strategy, each mention iis connected to the best antecedent mention j with j < i(i. [sent-33, score-0.8]
</p><p>23 a mention occurring to the left of i, assuming a leftto-right reading order), thereby creating a left-link. [sent-35, score-0.404]
</p><p>24 The “best” antecedent mention is the one with the highest pairwise score, wij ; furthermore, if wij is below some threshold, say 0, then iis not connected to any antecedent mention. [sent-36, score-0.998]
</p><p>25 The final clustering is a transitive closure of these “best” links. [sent-37, score-0.198]
</p><p>26 The intuition behind best-left-link strategy is based on how humans read and decipher coreference links they mostly rely on information to the left of the mention when deciding whether to add it to a previously constructed cluster or not. [sent-38, score-0.958]
</p><p>27 This strategy has been successful and commonly used in coreference resolution (Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al. [sent-39, score-0.606]
</p><p>28 However, most works have developed ad-hoc approaches to implement this idea. [sent-41, score-0.038]
</p><p>29 For instance, Bengtson and Roth (2008) train a model w on binary training data generated by tak–  ing for each mention, the closest antecedent coreferent mention as a positive example, and all the other mentions as negative examples. [sent-42, score-0.662]
</p><p>30 Similar approaches to training and, additionally, decoupling the training stage from the clustering stage were used by other systems. [sent-43, score-0.284]
</p><p>31 In this paper, we formalize the learning problem of the best-left-link model as a structured 602 prediction problem and analyze our system with detailed experiments. [sent-44, score-0.185]
</p><p>32 Furthermore, we generalize this approach by considering multiple pairwise left-links instead of just the best link, efficiently capturing the notion of a mention-to-cluster link. [sent-45, score-0.253]
</p><p>33 Many techniques in the coreference literature break away from the mention pair-based, best-leftlink paradigm. [sent-46, score-0.713]
</p><p>34 Denis and Baldridge (2008) and Ng (2005) learn a local ranker to rank the mention pairs based on their compatibility. [sent-47, score-0.307]
</p><p>35 While these approaches achieve decent empirical performance, it is unclear why these are the right ways to train the model. [sent-48, score-0.041]
</p><p>36 Some techniques consider a more expressive model by using features defined over mentioncluster or cluster-cluster (Rahman and Ng, 2011c;  Stoyanov and Eisner, 2012; Haghighi and Klein, 2010). [sent-49, score-0.074]
</p><p>37 For these models, the inference and learning algorithms are usually complicated. [sent-50, score-0.101]
</p><p>38 (2013) propose a probabilistic model which enforces structural agreement constraints between specified properties of mention cluster when using a mention-pair model. [sent-52, score-0.534]
</p><p>39 We believe that this is due to our novel and principled structured prediction framework which results in accurate (and efficient) training. [sent-59, score-0.258]
</p><p>40 Several structured prediction techniques have been applied to coreference resolution in the machine learning literature. [sent-60, score-0.705]
</p><p>41 For example, McCallum  and Wellner (2003) and Finley and Joachims (2005) model coreference as a correlational clustering problem (Bansal et al. [sent-61, score-0.731]
</p><p>42 , 2002) on a complete graph over the mentions with edge weights given by the pairwise classifier. [sent-62, score-0.383]
</p><p>43 However, correlational clustering is known to be NP Hard (Bansal et al. [sent-63, score-0.325]
</p><p>44 , 2002); nonetheless, an ILP solver or an approximate inference algorithm can be used to solve this problem. [sent-64, score-0.139]
</p><p>45 Another approach proposed by Yu and Joachims (2009) formulates coreference with latent spanning trees. [sent-65, score-0.623]
</p><p>46 However, their approach has no directionality between mentions, whereas our latent structure captures the natural left-to-right ordering of mentions. [sent-66, score-0.112]
</p><p>47 5), we show that our technique vastly outperforms both the spanning tree and the correlational clustering techniques. [sent-68, score-0.419]
</p><p>48 , 2012) and the publicly available Stanford coreference system (Raghunathan et al. [sent-70, score-0.406]
</p><p>49 Finally, some research (Ratinov and Roth, 2012; Bansal and Klein, 2012; Rahman and Ng, 2011a) has tried to integrate world knowledge from webbased statistics or knowledge bases into a corefer-  ence system. [sent-73, score-0.083]
</p><p>50 World knowledge is potentially useful for resolving coreference and can be injected into our system in a straightforward way via the constraints framework. [sent-74, score-0.594]
</p><p>51 Including massive amount of information from knowledge resources is not the focus of this paper and may distort the comparison with other relevant models but our results indicate that this is doable in our model, and may provide significant improvements. [sent-78, score-0.041]
</p><p>52 We first introduce the notion of a pairwise mention-scorer, then introduce our Left-Linking Model (L3M), and finally describe how to inject constraints into our model. [sent-84, score-0.4]
</p><p>53 A coreference clustering C for document 603 d is a collection of disjoint sets partitioning the set {1, . [sent-87, score-0.566]
</p><p>54 C(i, j) = W1 eif r mepernetsieonnts Ci aa snd a j are coreferent, wotihtehr Cw(isie,j C(i, j) = 0e. [sent-92, score-0.044]
</p><p>55 n tLioent s(C; w, d) e be c othreef score ootfh a given clustering 0C. [sent-93, score-0.243]
</p><p>56 f Lore a given ,ddo)c bume ethnet a scndor a given pairwise weight vector w. [sent-94, score-0.21]
</p><p>57 Then, during ainnfder aence, a clustering C is predicted by maximizing the scoring cfluunsctteiroinng s(C; w, d), over ayll m vaaxliidm (i. [sent-95, score-0.16]
</p><p>58 s tahteisfying symmetry a(Cnd; transitivity) clustering binary functions C : {1, . [sent-97, score-0.204]
</p><p>59 1 Mention Pair Scorer  We model the task of coreference resolution using a pairwise scorer which indicates the compatibility of  a pair of mentions. [sent-105, score-0.955]
</p><p>60 The inference routine then predicts the final clustering a structured prediction problem using these pairwise scores. [sent-106, score-0.665]
</p><p>61 j < i), we produce a pairwise compatibility score wji using extracted features φ(j, i) as —  —  wji  = w · φ(j, i) ,  (1)  where w is a weight parameter that is learned. [sent-111, score-0.536]
</p><p>62 2 Latent Left-Linking Model Our inference algorithm is inspired by the best-leftlink approach. [sent-113, score-0.15]
</p><p>63 In particular, the score s(C; d, w) is dlienfkin aepdp sroo athchat. [sent-114, score-0.035]
</p><p>64 e Inach pa mrteicnutiloarn, ltihneks s toor teh se( Can;tdec,ewd)en ist mention (to its left) with the highest score (as long as the score is above some threshold, say, 0). [sent-115, score-0.463]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('coreference', 0.406), ('mention', 0.307), ('bengtson', 0.262), ('roth', 0.236), ('pairwise', 0.21), ('mentions', 0.173), ('correlational', 0.165), ('clustering', 0.16), ('resolution', 0.149), ('rahman', 0.14), ('cardie', 0.129), ('antecedent', 0.121), ('compatibility', 0.117), ('md', 0.117), ('ng', 0.112), ('latent', 0.112), ('bansal', 0.11), ('principled', 0.108), ('constraints', 0.106), ('baldridge', 0.105), ('inference', 0.101), ('raghunathan', 0.096), ('operating', 0.088), ('admits', 0.087), ('samdani', 0.087), ('wij', 0.087), ('wji', 0.087), ('constrained', 0.081), ('pradhan', 0.077), ('stoyanov', 0.077), ('prediction', 0.076), ('expressive', 0.074), ('structured', 0.074), ('scorer', 0.073), ('denis', 0.067), ('iis', 0.065), ('yih', 0.065), ('coreferent', 0.061), ('ace', 0.059), ('spanning', 0.057), ('left', 0.053), ('strategy', 0.051), ('haghighi', 0.05), ('joachims', 0.05), ('cluster', 0.05), ('inspired', 0.049), ('importantly', 0.048), ('linking', 0.048), ('cnd', 0.048), ('danr', 0.048), ('decoupling', 0.048), ('favorably', 0.048), ('finley', 0.048), ('formulates', 0.048), ('injected', 0.048), ('ixm', 0.048), ('ootfh', 0.048), ('rajhans', 0.048), ('tak', 0.048), ('toor', 0.048), ('webbased', 0.048), ('previously', 0.047), ('klein', 0.047), ('occurring', 0.044), ('augments', 0.044), ('decipher', 0.044), ('eif', 0.044), ('injection', 0.044), ('routine', 0.044), ('symmetry', 0.044), ('augmented', 0.043), ('notion', 0.043), ('interactions', 0.042), ('popular', 0.041), ('decent', 0.041), ('distort', 0.041), ('inject', 0.041), ('wellner', 0.041), ('furthermore', 0.04), ('closure', 0.038), ('ino', 0.038), ('ist', 0.038), ('pushes', 0.038), ('solver', 0.038), ('implement', 0.038), ('stage', 0.038), ('structural', 0.037), ('fernandes', 0.037), ('ganchev', 0.037), ('vastly', 0.037), ('score', 0.035), ('bases', 0.035), ('durrett', 0.035), ('formalize', 0.035), ('frameworks', 0.035), ('illinois', 0.035), ('straightforward', 0.034), ('cw', 0.034), ('enforces', 0.034), ('ontonotes', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="1-tfidf-1" href="./emnlp-2013-A_Constrained_Latent_Variable_Model_for_Coreference_Resolution.html">1 emnlp-2013-A Constrained Latent Variable Model for Coreference Resolution</a></p>
<p>Author: Kai-Wei Chang ; Rajhans Samdani ; Dan Roth</p><p>Abstract: Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L3M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L3M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L3M and its constrained version, CL3M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.</p><p>2 0.51460278 <a title="1-tfidf-2" href="./emnlp-2013-Easy_Victories_and_Uphill_Battles_in_Coreference_Resolution.html">67 emnlp-2013-Easy Victories and Uphill Battles in Coreference Resolution</a></p>
<p>Author: Greg Durrett ; Dan Klein</p><p>Abstract: Classical coreference systems encode various syntactic, discourse, and semantic phenomena explicitly, using heterogenous features computed from hand-crafted heuristics. In contrast, we present a state-of-the-art coreference system that captures such phenomena implicitly, with a small number of homogeneous feature templates examining shallow properties of mentions. Surprisingly, our features are actually more effective than the corresponding hand-engineered ones at modeling these key linguistic phenomena, allowing us to win “easy victories” without crafted heuristics. These features are successful on syntax and discourse; however, they do not model semantic compatibility well, nor do we see gains from experiments with shallow semantic features from the literature, suggesting that this approach to semantics is an “uphill battle.” Nonetheless, our final system1 outperforms the Stanford system (Lee et al. (201 1), the winner of the CoNLL 2011 shared task) by 3.5% absolute on the CoNLL metric and outperforms the IMS system (Bj o¨rkelund and Farkas (2012), the best publicly available English coreference system) by 1.9% absolute.</p><p>3 0.3885217 <a title="1-tfidf-3" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>Author: Jonathan K. Kummerfeld ; Dan Klein</p><p>Abstract: Coreference resolution metrics quantify errors but do not analyze them. Here, we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types. Using this tool, we first compare the error distributions across a large set of systems, then analyze common errors across the top ten systems, empirically characterizing the major unsolved challenges of the coreference resolution task.</p><p>4 0.29081771 <a title="1-tfidf-4" href="./emnlp-2013-Joint_Coreference_Resolution_and_Named-Entity_Linking_with_Multi-Pass_Sieves.html">112 emnlp-2013-Joint Coreference Resolution and Named-Entity Linking with Multi-Pass Sieves</a></p>
<p>Author: Hannaneh Hajishirzi ; Leila Zilles ; Daniel S. Weld ; Luke Zettlemoyer</p><p>Abstract: Many errors in coreference resolution come from semantic mismatches due to inadequate world knowledge. Errors in named-entity linking (NEL), on the other hand, are often caused by superficial modeling of entity context. This paper demonstrates that these two tasks are complementary. We introduce NECO, a new model for named entity linking and coreference resolution, which solves both problems jointly, reducing the errors made on each. NECO extends the Stanford deterministic coreference system by automatically linking mentions to Wikipedia and introducing new NEL-informed mention-merging sieves. Linking improves mention-detection and enables new semantic attributes to be incorporated from Freebase, while coreference provides better context modeling by propagating named-entity links within mention clusters. Experiments show consistent improve- ments across a number of datasets and experimental conditions, including over 11% reduction in MUC coreference error and nearly 21% reduction in F1 NEL error on ACE 2004 newswire data.</p><p>5 0.28581616 <a title="1-tfidf-5" href="./emnlp-2013-Exploiting_Zero_Pronouns_to_Improve_Chinese_Coreference_Resolution.html">80 emnlp-2013-Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</a></p>
<p>Author: Fang Kong ; Hwee Tou Ng</p><p>Abstract: Coreference resolution plays a critical role in discourse analysis. This paper focuses on exploiting zero pronouns to improve Chinese coreference resolution. In particular, a simplified semantic role labeling framework is proposed to identify clauses and to detect zero pronouns effectively, and two effective methods (refining syntactic parser and refining learning example generation) are employed to exploit zero pronouns for Chinese coreference resolution. Evaluation on the CoNLL-2012 shared task data set shows that zero pronouns can significantly improve Chinese coreference resolution.</p><p>6 0.18263045 <a title="1-tfidf-6" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>7 0.137713 <a title="1-tfidf-7" href="./emnlp-2013-Latent_Anaphora_Resolution_for_Cross-Lingual_Pronoun_Prediction.html">117 emnlp-2013-Latent Anaphora Resolution for Cross-Lingual Pronoun Prediction</a></p>
<p>8 0.11437686 <a title="1-tfidf-8" href="./emnlp-2013-Using_Soft_Constraints_in_Joint_Inference_for_Clinical_Concept_Recognition.html">198 emnlp-2013-Using Soft Constraints in Joint Inference for Clinical Concept Recognition</a></p>
<p>9 0.10894901 <a title="1-tfidf-9" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>10 0.10245173 <a title="1-tfidf-10" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>11 0.094918989 <a title="1-tfidf-11" href="./emnlp-2013-Learning_Biological_Processes_with_Global_Constraints.html">118 emnlp-2013-Learning Biological Processes with Global Constraints</a></p>
<p>12 0.07909742 <a title="1-tfidf-12" href="./emnlp-2013-Chinese_Zero_Pronoun_Resolution%3A_Some_Recent_Advances.html">45 emnlp-2013-Chinese Zero Pronoun Resolution: Some Recent Advances</a></p>
<p>13 0.073137693 <a title="1-tfidf-13" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>14 0.065207593 <a title="1-tfidf-14" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>15 0.059313685 <a title="1-tfidf-15" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>16 0.059224498 <a title="1-tfidf-16" href="./emnlp-2013-Interpreting_Anaphoric_Shell_Nouns_using_Antecedents_of_Cataphoric_Shell_Nouns_as_Training_Data.html">108 emnlp-2013-Interpreting Anaphoric Shell Nouns using Antecedents of Cataphoric Shell Nouns as Training Data</a></p>
<p>17 0.058740728 <a title="1-tfidf-17" href="./emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model.html">75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</a></p>
<p>18 0.057489492 <a title="1-tfidf-18" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>19 0.055469025 <a title="1-tfidf-19" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>20 0.055359777 <a title="1-tfidf-20" href="./emnlp-2013-Event-Based_Time_Label_Propagation_for_Automatic_Dating_of_News_Articles.html">74 emnlp-2013-Event-Based Time Label Propagation for Automatic Dating of News Articles</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.22), (1, 0.284), (2, 0.497), (3, -0.159), (4, 0.091), (5, -0.081), (6, 0.069), (7, -0.102), (8, -0.067), (9, -0.05), (10, -0.07), (11, -0.053), (12, -0.032), (13, 0.014), (14, 0.086), (15, -0.051), (16, 0.005), (17, 0.013), (18, -0.005), (19, -0.005), (20, -0.023), (21, -0.073), (22, 0.019), (23, -0.061), (24, 0.039), (25, 0.021), (26, -0.082), (27, -0.066), (28, 0.021), (29, 0.07), (30, -0.0), (31, -0.033), (32, -0.018), (33, -0.031), (34, -0.096), (35, 0.037), (36, 0.027), (37, -0.016), (38, 0.048), (39, 0.017), (40, -0.005), (41, -0.038), (42, 0.03), (43, -0.002), (44, 0.043), (45, 0.026), (46, -0.034), (47, -0.07), (48, -0.011), (49, -0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98204696 <a title="1-lsi-1" href="./emnlp-2013-A_Constrained_Latent_Variable_Model_for_Coreference_Resolution.html">1 emnlp-2013-A Constrained Latent Variable Model for Coreference Resolution</a></p>
<p>Author: Kai-Wei Chang ; Rajhans Samdani ; Dan Roth</p><p>Abstract: Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L3M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L3M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L3M and its constrained version, CL3M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.</p><p>2 0.93031836 <a title="1-lsi-2" href="./emnlp-2013-Easy_Victories_and_Uphill_Battles_in_Coreference_Resolution.html">67 emnlp-2013-Easy Victories and Uphill Battles in Coreference Resolution</a></p>
<p>Author: Greg Durrett ; Dan Klein</p><p>Abstract: Classical coreference systems encode various syntactic, discourse, and semantic phenomena explicitly, using heterogenous features computed from hand-crafted heuristics. In contrast, we present a state-of-the-art coreference system that captures such phenomena implicitly, with a small number of homogeneous feature templates examining shallow properties of mentions. Surprisingly, our features are actually more effective than the corresponding hand-engineered ones at modeling these key linguistic phenomena, allowing us to win “easy victories” without crafted heuristics. These features are successful on syntax and discourse; however, they do not model semantic compatibility well, nor do we see gains from experiments with shallow semantic features from the literature, suggesting that this approach to semantics is an “uphill battle.” Nonetheless, our final system1 outperforms the Stanford system (Lee et al. (201 1), the winner of the CoNLL 2011 shared task) by 3.5% absolute on the CoNLL metric and outperforms the IMS system (Bj o¨rkelund and Farkas (2012), the best publicly available English coreference system) by 1.9% absolute.</p><p>3 0.88973331 <a title="1-lsi-3" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>Author: Jonathan K. Kummerfeld ; Dan Klein</p><p>Abstract: Coreference resolution metrics quantify errors but do not analyze them. Here, we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types. Using this tool, we first compare the error distributions across a large set of systems, then analyze common errors across the top ten systems, empirically characterizing the major unsolved challenges of the coreference resolution task.</p><p>4 0.88508815 <a title="1-lsi-4" href="./emnlp-2013-Joint_Coreference_Resolution_and_Named-Entity_Linking_with_Multi-Pass_Sieves.html">112 emnlp-2013-Joint Coreference Resolution and Named-Entity Linking with Multi-Pass Sieves</a></p>
<p>Author: Hannaneh Hajishirzi ; Leila Zilles ; Daniel S. Weld ; Luke Zettlemoyer</p><p>Abstract: Many errors in coreference resolution come from semantic mismatches due to inadequate world knowledge. Errors in named-entity linking (NEL), on the other hand, are often caused by superficial modeling of entity context. This paper demonstrates that these two tasks are complementary. We introduce NECO, a new model for named entity linking and coreference resolution, which solves both problems jointly, reducing the errors made on each. NECO extends the Stanford deterministic coreference system by automatically linking mentions to Wikipedia and introducing new NEL-informed mention-merging sieves. Linking improves mention-detection and enables new semantic attributes to be incorporated from Freebase, while coreference provides better context modeling by propagating named-entity links within mention clusters. Experiments show consistent improve- ments across a number of datasets and experimental conditions, including over 11% reduction in MUC coreference error and nearly 21% reduction in F1 NEL error on ACE 2004 newswire data.</p><p>5 0.81381273 <a title="1-lsi-5" href="./emnlp-2013-Exploiting_Zero_Pronouns_to_Improve_Chinese_Coreference_Resolution.html">80 emnlp-2013-Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</a></p>
<p>Author: Fang Kong ; Hwee Tou Ng</p><p>Abstract: Coreference resolution plays a critical role in discourse analysis. This paper focuses on exploiting zero pronouns to improve Chinese coreference resolution. In particular, a simplified semantic role labeling framework is proposed to identify clauses and to detect zero pronouns effectively, and two effective methods (refining syntactic parser and refining learning example generation) are employed to exploit zero pronouns for Chinese coreference resolution. Evaluation on the CoNLL-2012 shared task data set shows that zero pronouns can significantly improve Chinese coreference resolution.</p><p>6 0.48013523 <a title="1-lsi-6" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>7 0.42135683 <a title="1-lsi-7" href="./emnlp-2013-Chinese_Zero_Pronoun_Resolution%3A_Some_Recent_Advances.html">45 emnlp-2013-Chinese Zero Pronoun Resolution: Some Recent Advances</a></p>
<p>8 0.40509054 <a title="1-lsi-8" href="./emnlp-2013-Using_Soft_Constraints_in_Joint_Inference_for_Clinical_Concept_Recognition.html">198 emnlp-2013-Using Soft Constraints in Joint Inference for Clinical Concept Recognition</a></p>
<p>9 0.40002534 <a title="1-lsi-9" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>10 0.38395166 <a title="1-lsi-10" href="./emnlp-2013-Latent_Anaphora_Resolution_for_Cross-Lingual_Pronoun_Prediction.html">117 emnlp-2013-Latent Anaphora Resolution for Cross-Lingual Pronoun Prediction</a></p>
<p>11 0.28826961 <a title="1-lsi-11" href="./emnlp-2013-Animacy_Detection_with_Voting_Models.html">23 emnlp-2013-Animacy Detection with Voting Models</a></p>
<p>12 0.28719339 <a title="1-lsi-12" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>13 0.25566003 <a title="1-lsi-13" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>14 0.22957304 <a title="1-lsi-14" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>15 0.21948464 <a title="1-lsi-15" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>16 0.21903864 <a title="1-lsi-16" href="./emnlp-2013-Learning_Biological_Processes_with_Global_Constraints.html">118 emnlp-2013-Learning Biological Processes with Global Constraints</a></p>
<p>17 0.21241665 <a title="1-lsi-17" href="./emnlp-2013-Event-Based_Time_Label_Propagation_for_Automatic_Dating_of_News_Articles.html">74 emnlp-2013-Event-Based Time Label Propagation for Automatic Dating of News Articles</a></p>
<p>18 0.2090507 <a title="1-lsi-18" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>19 0.20286989 <a title="1-lsi-19" href="./emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model.html">75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</a></p>
<p>20 0.20253755 <a title="1-lsi-20" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.011), (9, 0.012), (18, 0.041), (22, 0.036), (30, 0.091), (50, 0.013), (51, 0.131), (58, 0.017), (66, 0.014), (71, 0.025), (75, 0.061), (77, 0.013), (95, 0.4), (96, 0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84362429 <a title="1-lda-1" href="./emnlp-2013-Fast_Joint_Compression_and_Summarization_via_Graph_Cuts.html">85 emnlp-2013-Fast Joint Compression and Summarization via Graph Cuts</a></p>
<p>Author: Xian Qian ; Yang Liu</p><p>Abstract: Extractive summarization typically uses sentences as summarization units. In contrast, joint compression and summarization can use smaller units such as words and phrases, resulting in summaries containing more information. The goal of compressive summarization is to find a subset of words that maximize the total score of concepts and cutting dependency arcs under the grammar constraints and summary length constraint. We propose an efficient decoding algorithm for fast compressive summarization using graph cuts. Our approach first relaxes the length constraint using Lagrangian relaxation. Then we propose to bound the relaxed objective function by the supermodular binary quadratic programming problem, which can be solved efficiently using graph max-flow/min-cut. Since finding the tightest lower bound suffers from local optimality, we use convex relaxation for initialization. Experimental results on TAC2008 dataset demonstrate our method achieves competitive ROUGE score and has good readability, while is much faster than the integer linear programming (ILP) method.</p><p>same-paper 2 0.78691375 <a title="1-lda-2" href="./emnlp-2013-A_Constrained_Latent_Variable_Model_for_Coreference_Resolution.html">1 emnlp-2013-A Constrained Latent Variable Model for Coreference Resolution</a></p>
<p>Author: Kai-Wei Chang ; Rajhans Samdani ; Dan Roth</p><p>Abstract: Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L3M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L3M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L3M and its constrained version, CL3M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.</p><p>3 0.58164006 <a title="1-lda-3" href="./emnlp-2013-Document_Summarization_via_Guided_Sentence_Compression.html">65 emnlp-2013-Document Summarization via Guided Sentence Compression</a></p>
<p>Author: Chen Li ; Fei Liu ; Fuliang Weng ; Yang Liu</p><p>Abstract: Joint compression and summarization has been used recently to generate high quality summaries. However, such word-based joint optimization is computationally expensive. In this paper we adopt the ‘sentence compression + sentence selection’ pipeline approach for compressive summarization, but propose to perform summary guided compression, rather than generic sentence-based compression. To create an annotated corpus, the human annotators were asked to compress sentences while explicitly given the important summary words in the sentences. Using this corpus, we train a supervised sentence compression model using a set of word-, syntax-, and documentlevel features. During summarization, we use multiple compressed sentences in the integer linear programming framework to select . salient summary sentences. Our results on the TAC 2008 and 2011 summarization data sets show that by incorporating the guided sentence compression model, our summarization system can yield significant performance gain as compared to the state-of-the-art.</p><p>4 0.57755816 <a title="1-lda-4" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>Author: Hrushikesh Mohapatra ; Siddhanth Jain ; Soumen Chakrabarti</p><p>Abstract: Web search can be enhanced in powerful ways if token spans in Web text are annotated with disambiguated entities from large catalogs like Freebase. Entity annotators need to be trained on sample mention snippets. Wikipedia entities and annotated pages offer high-quality labeled data for training and evaluation. Unfortunately, Wikipedia features only one-ninth the number of entities as Freebase, and these are a highly biased sample of well-connected, frequently mentioned “head” entities. To bring hope to “tail” entities, we broaden our goal to a second task: assigning types to entities in Freebase but not Wikipedia. The two tasks are synergistic: knowing the types of unfamiliar entities helps disambiguate mentions, and words in mention contexts help assign types to entities. We present TMI, a bipartite graphical model for joint type-mention inference. TMI attempts no schema integration or entity resolution, but exploits the above-mentioned synergy. In experiments involving 780,000 people in Wikipedia, 2.3 million people in Freebase, 700 million Web pages, and over 20 professional editors, TMI shows considerable annotation accuracy improvement (e.g., 70%) compared to baselines (e.g., 46%), especially for “tail” and emerging entities. We also compare with Google’s recent annotations of the same corpus with Freebase entities, and report considerable improvements within the people domain.</p><p>5 0.52538282 <a title="1-lda-5" href="./emnlp-2013-Using_Soft_Constraints_in_Joint_Inference_for_Clinical_Concept_Recognition.html">198 emnlp-2013-Using Soft Constraints in Joint Inference for Clinical Concept Recognition</a></p>
<p>Author: Prateek Jindal ; Dan Roth</p><p>Abstract: This paper introduces IQPs (Integer Quadratic Programs) as a way to model joint inference for the task of concept recognition in clinical domain. IQPs make it possible to easily incorporate soft constraints in the optimization framework and still support exact global inference. We show that soft constraints give statistically significant performance improvements when compared to hard constraints.</p><p>6 0.46059951 <a title="1-lda-6" href="./emnlp-2013-Automatically_Determining_a_Proper_Length_for_Multi-Document_Summarization%3A_A_Bayesian_Nonparametric_Approach.html">36 emnlp-2013-Automatically Determining a Proper Length for Multi-Document Summarization: A Bayesian Nonparametric Approach</a></p>
<p>7 0.45111325 <a title="1-lda-7" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>8 0.44387218 <a title="1-lda-8" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>9 0.44151881 <a title="1-lda-9" href="./emnlp-2013-Overcoming_the_Lack_of_Parallel_Data_in_Sentence_Compression.html">149 emnlp-2013-Overcoming the Lack of Parallel Data in Sentence Compression</a></p>
<p>10 0.42788124 <a title="1-lda-10" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>11 0.42718604 <a title="1-lda-11" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>12 0.42603019 <a title="1-lda-12" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>13 0.42126289 <a title="1-lda-13" href="./emnlp-2013-Joint_Coreference_Resolution_and_Named-Entity_Linking_with_Multi-Pass_Sieves.html">112 emnlp-2013-Joint Coreference Resolution and Named-Entity Linking with Multi-Pass Sieves</a></p>
<p>14 0.42063552 <a title="1-lda-14" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>15 0.41822562 <a title="1-lda-15" href="./emnlp-2013-Single-Document_Summarization_as_a_Tree_Knapsack_Problem.html">174 emnlp-2013-Single-Document Summarization as a Tree Knapsack Problem</a></p>
<p>16 0.4178409 <a title="1-lda-16" href="./emnlp-2013-Easy_Victories_and_Uphill_Battles_in_Coreference_Resolution.html">67 emnlp-2013-Easy Victories and Uphill Battles in Coreference Resolution</a></p>
<p>17 0.41465402 <a title="1-lda-17" href="./emnlp-2013-Unsupervised_Spectral_Learning_of_WCFG_as_Low-rank_Matrix_Completion.html">195 emnlp-2013-Unsupervised Spectral Learning of WCFG as Low-rank Matrix Completion</a></p>
<p>18 0.41314375 <a title="1-lda-18" href="./emnlp-2013-Feature_Noising_for_Log-Linear_Structured_Prediction.html">86 emnlp-2013-Feature Noising for Log-Linear Structured Prediction</a></p>
<p>19 0.410018 <a title="1-lda-19" href="./emnlp-2013-Exploiting_Zero_Pronouns_to_Improve_Chinese_Coreference_Resolution.html">80 emnlp-2013-Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</a></p>
<p>20 0.4083572 <a title="1-lda-20" href="./emnlp-2013-A_Unified_Model_for_Topics%2C_Events_and_Users_on_Twitter.html">16 emnlp-2013-A Unified Model for Topics, Events and Users on Twitter</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
