<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>67 emnlp-2013-Easy Victories and Uphill Battles in Coreference Resolution</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-67" href="#">emnlp2013-67</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>67 emnlp-2013-Easy Victories and Uphill Battles in Coreference Resolution</h1>
<br/><p>Source: <a title="emnlp-2013-67-pdf" href="http://aclweb.org/anthology//D/D13/D13-1203.pdf">pdf</a></p><p>Author: Greg Durrett ; Dan Klein</p><p>Abstract: Classical coreference systems encode various syntactic, discourse, and semantic phenomena explicitly, using heterogenous features computed from hand-crafted heuristics. In contrast, we present a state-of-the-art coreference system that captures such phenomena implicitly, with a small number of homogeneous feature templates examining shallow properties of mentions. Surprisingly, our features are actually more effective than the corresponding hand-engineered ones at modeling these key linguistic phenomena, allowing us to win “easy victories” without crafted heuristics. These features are successful on syntax and discourse; however, they do not model semantic compatibility well, nor do we see gains from experiments with shallow semantic features from the literature, suggesting that this approach to semantics is an “uphill battle.” Nonetheless, our final system1 outperforms the Stanford system (Lee et al. (201 1), the winner of the CoNLL 2011 shared task) by 3.5% absolute on the CoNLL metric and outperforms the IMS system (Bj o¨rkelund and Farkas (2012), the best publicly available English coreference system) by 1.9% absolute.</p><p>Reference: <a title="emnlp-2013-67-reference" href="../emnlp2013_reference/emnlp-2013-Easy_Victories_and_Uphill_Battles_in_Coreference_Resolution_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Classical coreference systems encode various syntactic, discourse, and semantic phenomena explicitly, using heterogenous features computed from hand-crafted heuristics. [sent-3, score-0.696]
</p><p>2 In contrast, we present a state-of-the-art coreference system that captures such phenomena implicitly, with a small number of homogeneous feature templates examining shallow properties of mentions. [sent-4, score-0.789]
</p><p>3 These features are successful on syntax and discourse; however, they do not model semantic compatibility well, nor do we see gains from experiments with shallow semantic features from the literature, suggesting that this approach to semantics is an “uphill battle. [sent-6, score-0.257]
</p><p>4 5% absolute on the CoNLL metric and outperforms the IMS system (Bj o¨rkelund and Farkas (2012), the best publicly available English coreference system) by 1. [sent-9, score-0.594]
</p><p>5 1 Introduction Coreference resolution is a multi-faceted task: humans resolve references by exploiting contextual and grammatical clues, as well as semantic information and world knowledge, so capturing each of 1The Berkeley Coreference Resolution System is available at http : / /nlp . [sent-11, score-0.252]
</p><p>6 Acknowledging this complexity, coreference systems, either learning-based (Bengtson and Roth, 2008; Stoyanov et al. [sent-16, score-0.525]
</p><p>7 We build a learning-based, mention-synchronous coreference system that aims to use the simplest possible set of features to tackle the various aspects of coreference resolution. [sent-20, score-1.171]
</p><p>8 Our base system uses only two recall-oriented features on nominal and proper mentions: head match and exact string match. [sent-25, score-0.346]
</p><p>9 However, these features are beneficial when gold mentions are provided to our system, leading us to conclude that the large number of system mentions extracted by most coreference systems (Lee et al. [sent-29, score-1.413]
</p><p>10 , 2012) means that weak indicators cannot overcome the bias against making coreference links. [sent-31, score-0.563]
</p><p>11 Capturing semantic information in this shallow way is an “uphill battle” due to this structural property of coreference resolution. [sent-32, score-0.629]
</p><p>12 Nevertheless, using a simple architecture and feature set, our final system outperforms the two best publicly available English coreference systems, the Stanford system (Lee et al. [sent-33, score-0.7]
</p><p>13 All experiments use system mentions except where otherwise indicated. [sent-41, score-0.413]
</p><p>14 3  3  A Mention-Synchronous Framework  We first present the basic architecture of our coreference system, independent of a feature set. [sent-45, score-0.562]
</p><p>15 Unlike binary classification-based coreference systems where independent binary decisions are made about each pair (Soon et al. [sent-46, score-0.564]
</p><p>16 1972 each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). [sent-53, score-0.281]
</p><p>17 In this mentionranking or mention-synchronous framework, features examine single mentions to evaluate whether or not they are anaphoric and pairs of mentions to evaluate whether or not they corefer. [sent-54, score-0.858]
</p><p>18 1 Mention Detection Our system first identifies a set of predicted mentions from text annotated with parses and named entity tags. [sent-59, score-0.413]
</p><p>19 We extract three distinct types of mentions: proper mentions from all named entity chunks except for those labeled as QUANTITY, CARDINAL, or PERCENT, pronominal mentions from single words tagged with PRP or PRP$, and nominal mentions from all other maximal NP projections. [sent-60, score-1.185]
</p><p>20 2 Coreference Model Figure 1 shows the mention-ranking architecture that serves as the backbone of our coreference system. [sent-65, score-0.525]
</p><p>21 Assume we have extracted n mentions from a document x, where x denotes the surface properties of a document and any precomputed information. [sent-66, score-0.449]
</p><p>22 The ith mention in a document has an associated random variable ai taking values in the set {1, . [sent-67, score-0.413]
</p><p>23 , i−1, NEW}; this variable specifies mention {i’1s s. [sent-70, score-0.281]
</p><p>24 e,cite−d1 a,ntece}d;en thti or ainrdiaibclaete ssp ethciatfi eits begins a new coreference chain. [sent-73, score-0.525]
</p><p>25 , an), implies a unique set of coreference chains C that serve as our system output. [sent-77, score-0.594]
</p><p>26 The ith mention in a document has ipossible antecedence choices: link to one of the i 1preceding mentions or begin a new cluster. [sent-82, score-0.662]
</p><p>27 where f(i, ai, x) is a feature function that examines the coreference decision ai for mention iwith document context x. [sent-86, score-0.975]
</p><p>28 Inference in this model is efficient: because  log P(a|x) decomposes linearly over mentions, we can compute ai = arg maxai P(ai |x) separately for each mention and return the set o|xf )co sreepfaerraetneclye chains implied by these decisions. [sent-88, score-0.413]
</p><p>29 The main complicating factor in this process is that the supervision in coreference consists of a gold clustering C∗ defined over gold mentions. [sent-92, score-0.683]
</p><p>30 This is problematic for two reasons: first, because the clustering is defined over gold mentions rather than our system mentions, and second, because a clustering does not specify a full antecedent structure of the sort our model produces. [sent-93, score-0.633]
</p><p>31 We can address the first of these problems by imputing singleton clusters for mentions that do 1973 not appear in the gold standard; our system will then simply learn to put spurious mentions in their own clusters. [sent-94, score-1.006]
</p><p>32 To address the lack of explicit antecedents in C∗, we simply sum over all possible antecedent structures licensed by the gold clusters. [sent-96, score-0.266]
</p><p>33 Formally, we will maximize the conditional loglikelihood of the set A(C∗) of antecedent vectors a feolirh a oddoc oufm tehent sthetat A are consistent with the gold annotation. [sent-97, score-0.22]
</p><p>34 4 Consistency for an antecedent choice ai under gold clusters C∗ is defined as follows: 1. [sent-98, score-0.39]
</p><p>35 If ai = j, ai is consistent iff mentions iand j are present in C∗ and are in the same cluster. [sent-99, score-0.608]
</p><p>36 If ai = NEW, ai is consistent off mention iis not present in C∗, or it is present in C∗ and has no gold antecedents, or it is present in C∗ and none of its gold antecedents are among the set of system predicted mentions. [sent-101, score-0.818]
</p><p>37 A false anaphor (FA) error occurs when ai is chosen to be anaphoric when it should start a new cluster. [sent-106, score-0.283]
</p><p>38 Finally, a wrong link (WL) error occurs when the antecedent chosen for ai is the wrong antecedent (but ai is indeed anaphoric). [sent-108, score-0.546]
</p><p>39 By setting αFA low and αFN high relative to αWL, we can counterbalance the high number of singleton mentions and bias the system towards making more coreference linkages. [sent-110, score-1.016]
</p><p>40 4  Easy Victories from Surface Features  Our primary goal with this work is to show that a high-performance coreference system is attainable with a small number of feature templates that use only surface-level information sources. [sent-117, score-0.631]
</p><p>41 •  •  •  The word immediately preceding and the word immediately following a mention Mention length, in words Two distance measures between mentions (number of sentences and number of mentions)  Table 1 shows the SURFACE feature set. [sent-122, score-0.662]
</p><p>42 Features that look only at the current mention fire on all decisions (ai = j or ai = NEW), whereas features that look at the antecedent in any way (the latter two groups of features) only fire on pairwise linkages (ai NEW). [sent-123, score-0.778]
</p><p>43 Two conjunctions of each feature are also included: first with the “type” of the mention be-  =  ing resolved (either NOMINAL, PROPER, or, if it is pronominal, the citation form of the pronoun), and then additionally with the antecedent type (only if the feature is over a pairwise link). [sent-124, score-0.601]
</p><p>44 Note that features that just examine the antecedent will end up with  [Voters]1 generally agree when [they]1 . [sent-126, score-0.193]
</p><p>45 Each feature on anaphoricity is conjoined with the type (NOMINAL, PROPER, or the citation form if it is a pronoun) of the mention being resolved. [sent-130, score-0.454]
</p><p>46 Each feature on a mention pair is additionally conjoined with the types of the current and antecedent mentions. [sent-131, score-0.495]
</p><p>47 conjunctions that examine properties of the current mention as well, as shown with the ANT. [sent-132, score-0.34]
</p><p>48 (201 1), the winner of the CoNLL 2011 shared task) and the IMS system (Bj o¨rkelund and Farkas (2012), the best publicly available English coreference system). [sent-137, score-0.658]
</p><p>49 2  Data-Driven versus Heuristic-Driven Features  Why are the SURFACE features sufficient to give high coreference performance, when they do not make apparent reference to important linguistic phenomena? [sent-140, score-0.577]
</p><p>50 Despite using limited information sources, our system is able to substantially outperform the other two, the two best publicly-available English coreference systems. [sent-148, score-0.594]
</p><p>51 For example, rather than having rules targeting person, number, gender, or animacy of mentions, we use conjunctions with  pronoun identity, which contains this information. [sent-152, score-0.29]
</p><p>52 Rather than explicitly writing a feature targeting definiteness, our indicators on the first word of a mention will capture this and other effects. [sent-153, score-0.415]
</p><p>53 , 2010; Haghighi and Klein, 2010), our features on word context can identify configurational clues like whether a mention is preceded or followed by a verb, and therefore whether it is likely in subject or object position. [sent-156, score-0.37]
</p><p>54 Instead of using an indicator on the first word of a mention (1STWORD), we instead fire a feature based on that mention’s manuallycomputed definiteness (DEF). [sent-159, score-0.454]
</p><p>55 Instead of conjoining features on pronominalpronominal linkages with the citation form of 5Heuristic-driven approaches were historically more appropriate, since past coreference corpora such as MUC and ACE were smaller and therefore more prone to overfitting featurerich models. [sent-161, score-0.678]
</p><p>56 each pronoun (PRONCONJ), we only conjoin with a PRONOUN indicator and add features targeting the person, number, gender, and animacy of the two pronouns (AGR). [sent-168, score-0.354]
</p><p>57 Instead of using our context features on the preceding and following word (CONTEXT), we use manual determinations of when mentions are in subject, direct object, indirect objection, or oblique position (POSN). [sent-170, score-0.396]
</p><p>58 1976 5  Uphill Battles on Semantics  In Section 4, we gave a simple set of features that yielded a high-performance coreference system; this high performance is possible because features targeting only superficial properties in a fine-grained way can actually model complex linguistic constraints. [sent-177, score-0.726]
</p><p>59 However, while our existing features capture syntactic and discourse-level phenomena surprisingly well, they are not effective at capturing semantic phenomena like type compatibility. [sent-178, score-0.241]
</p><p>60 We will show that due to structural aspects of the coreference resolution problem, even a combination of several shallow semantic features from the literature fails to adequately model semantics. [sent-179, score-0.823]
</p><p>61 To answer this question, we will split mentions into several categories based on their observable properties and the gold standard coreference information, and examine our system’s accuracy on each mention subclass in order to more thoroughly characterize its performance. [sent-182, score-1.229]
</p><p>62 6 These categories represent important distinctions in terms of the difficulty of mention resolution for our system. [sent-183, score-0.423]
</p><p>63 We first split mentions into three categories by their status in the gold standard: singleton (unannotated in the OntoNotes corpus), starting a new entity with at least two mentions, or anaphoric. [sent-184, score-0.501]
</p><p>64 (2009) and Rahman and Ng (201 1b), though we split our mentions along different axes, and can simply evaluate on accuracy because our decisions do not directly imply multiple links, as they do in binary classification-based systems (Stoyanov et al. [sent-187, score-0.383]
</p><p>65 We characterize each predicted mention by its status in the gold standard (singleton, starting a new entity, or anaphoric), its type (pronominal or nominal/proper), and by whether its head has appeared as the head of a previous mention. [sent-196, score-0.586]
</p><p>66 Each cell shows our system’s accuracy on that mention class as well as the size of the class. [sent-197, score-0.317]
</p><p>67 The biggest weakness of our system appears to be its inability to resolve anaphoric mentions with new heads (bottom-left cell). [sent-198, score-0.592]
</p><p>68 Second,  we  divide  mentions  by  their  type,  pronominal versus nominal/proper; we then further subdivide nominals and propers based on whether or not the head word of the mention has appeared as the head of a previous mention in the document. [sent-200, score-1.296]
</p><p>69 In each cell, we show the fraction of mentions that we correctly resolve (i. [sent-202, score-0.405]
</p><p>70 , for which we make an antecedence decision consistent with the gold standard), as well as the total number of mentions falling into that cell. [sent-204, score-0.46]
</p><p>71 First, we observe that there are a surprisingly large number of singleton mentions with misleading head matches to previous mentions (often recurring temporal nouns phrases, like July). [sent-205, score-0.858]
</p><p>72 The features in our system targeting anaphoricity are useful for exactly this reason: the more bad head matches we can rule out based on other criteria, the more strongly we can rely on head match to make correct linkages. [sent-206, score-0.494]
</p><p>73 Our system is most noticeably poor at resolving anaphoric mentions whose heads have not appeared before. [sent-207, score-0.573]
</p><p>74 The fact that exact match and head match are our only recall-oriented features on nominals and propers is starkly apparent here: when we cannot rely on head match, as is true for this mention class, we only resolve 7. [sent-208, score-0.76]
</p><p>75 7  Many of the mentions in this category  7There are an additional 346 anaphoric nominal/proper mentions in the 2nd+ category whose heads only appeared previously as part of a different cluster; we only resolve 1. [sent-210, score-0.909]
</p><p>76 However, the semantic information contained even in a coreference corpus of thousands of documents is insufficient to generalize to unseen data,8 so system designers have turned to external resources such as semantic classes derived from WordNet (Soon et al. [sent-214, score-0.692]
</p><p>77 , 2010), semantic similarity computed from online resources (Ponzetto and Strube, 2006), named entity type features, gender and number match using the dataset of Bergsma and Lin (2006), and features from unsupervised clus-  ters (Hendrickx and Daelemans, 2007; Durrett et al. [sent-216, score-0.202]
</p><p>78 In this section, we consider the following subset of these information sources: •  •  •  •  WordNet hypernymy and synonymy Number and gender data for nominals propers from Bergsma and Lin (2006)  and  Named entity types Latent clusters computed from English Gigaword (Graff et al. [sent-218, score-0.207]
</p><p>79 , 2007), where a latent cluster label generates each nominal head (excluding pronouns) and a conjunction of its verbal governor and semantic role, if any (Durrett et al. [sent-219, score-0.241]
</p><p>80 We experiment on both system mentions and gold mentions. [sent-242, score-0.492]
</p><p>81 Surprisingly, despite the fact that absolute performance numbers are much higher on gold mentions and there is less room for improvement, the semantic features help much more than they do on system mentions. [sent-243, score-0.593]
</p><p>82 3 Analysis of Semantic Features The main reason that weak semantic cues are not more effective is the small fraction of positive coreference links present in the training data. [sent-245, score-0.612]
</p><p>83 , 2011), would introduce many spurious coreference arcs if applied too liberally (see Table 4). [sent-249, score-0.618]
</p><p>84 To confirm this intuition, we show in the bottom part of Table 5 results when we apply these semantic features on top of our SURFACE system on gold mentions, where there are no singletons. [sent-251, score-0.249]
</p><p>85 In the gold mention setting, we see that the semantic features give a consistent improvement on every metric. [sent-252, score-0.461]
</p><p>86 , 2012), opt for high mention recall and resolve a relatively large number of system mentions. [sent-255, score-0.411]
</p><p>87 1978 match: accuracy on the 1601 mentions that fall into this category improves from 28. [sent-256, score-0.344]
</p><p>88 6 FINAL System and Results While semantic features ended up giving only marginal benefit, we have demonstrated that nevertheless our SURFACE system is a state-of-the-art English coreference system. [sent-266, score-0.695]
</p><p>89 Before giving final results, we will present a small set of additional features that consider four additional mention properties beyond those in Section 4. [sent-268, score-0.333]
</p><p>90 semantic information and the speaker information can apply in a fine-grained way to different pronouns, and can therefore improve pronoun resolution substantially; however, these features generally only improve pronoun resolution. [sent-282, score-0.415]
</p><p>91 7  Related Work  Many ofthe individual features we employ in the FINAL feature set have appeared in other coreference systems (Bj o¨rkelund and Nugues, 2011; Rahman and Ng, 2011b; Fernandes et al. [sent-287, score-0.656]
</p><p>92 However, other authors have often emphasized bilexical features on head pairs, whereas our features are heavily monolexical. [sent-289, score-0.228]
</p><p>93 As with the features of the SURFACE system, two conjoined variants of each feature are included: first with the type of the current mention (NOMINAL, PROPER, or the citation form of the pronoun), then with the types of both mentions in the pair. [sent-300, score-0.796]
</p><p>94 These conjunctions allow antecedent features on gender and number to impact pronoun resolution, and they allow speaker match to capture effects like I you being and coreferent when the speakers differ. [sent-301, score-0.499]
</p><p>95 features with regularization also means that we organically get distinctions among different mention types without having to choose a level of granularity a priori, unlike the distinct classifiers employed by Denis and Baldridge (2008). [sent-302, score-0.333]
</p><p>96 In terms of architecture, many coreference systems operate in a pipelined fashion, making partial decisions about coreference or pruning arcs before full resolution. [sent-303, score-1.128]
</p><p>97 , 2012), a series of learning-based passes (Bj o¨rkelund and Farkas, 2012), or referentiality classifiers that prune the set of mentions before resolution (Rahman and Ng, 2009; Bj ¨orkelund and Farkas, 2012; Recasens et al. [sent-305, score-0.523]
</p><p>98 Conclusion  We have presented a coreference system that uses a simple, homogeneous set of features in a discriminative learning framework to achieve high performance. [sent-314, score-0.679]
</p><p>99 Large numbers of lexicalized, data-driven features implicitly model linguistic phenomena such as definiteness and centering, obviating the need for heuristic-driven rules explicitly targeting these same phenomena. [sent-315, score-0.283]
</p><p>100 Additional semantic features give only slight benefit beyond head match because they do not provide strong enough signals of coreference to improve performance in the system mention setting;  modeling semantic similarity still requires complex outside information and deep heuristics. [sent-316, score-1.155]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('coreference', 0.525), ('mentions', 0.344), ('mention', 0.281), ('resolution', 0.142), ('antecedent', 0.141), ('ai', 0.132), ('rahman', 0.128), ('conll', 0.128), ('anaphoric', 0.118), ('bj', 0.118), ('surface', 0.105), ('stoyanov', 0.102), ('farkas', 0.102), ('targeting', 0.097), ('head', 0.092), ('pronoun', 0.086), ('fernandes', 0.085), ('orkelund', 0.081), ('gold', 0.079), ('ontonotes', 0.078), ('singleton', 0.078), ('rkelund', 0.073), ('phenomena', 0.07), ('durrett', 0.07), ('system', 0.069), ('uphill', 0.068), ('pradhan', 0.064), ('definiteness', 0.064), ('shared', 0.064), ('gender', 0.063), ('lee', 0.061), ('resolve', 0.061), ('coreferent', 0.06), ('nominal', 0.06), ('conjunctions', 0.059), ('victories', 0.058), ('pronominal', 0.058), ('linkages', 0.055), ('propers', 0.055), ('shallow', 0.055), ('anaphoricity', 0.054), ('spurious', 0.054), ('features', 0.052), ('ims', 0.051), ('nominals', 0.051), ('recasens', 0.051), ('haghighi', 0.05), ('luo', 0.049), ('semantic', 0.049), ('animacy', 0.048), ('fn', 0.046), ('citation', 0.046), ('antecedents', 0.046), ('denis', 0.045), ('bengtson', 0.044), ('singletons', 0.044), ('appeared', 0.042), ('prp', 0.041), ('conjunction', 0.04), ('decisions', 0.039), ('fa', 0.039), ('fire', 0.039), ('arcs', 0.039), ('pronouns', 0.038), ('president', 0.038), ('match', 0.038), ('sameer', 0.038), ('clusters', 0.038), ('bergsma', 0.038), ('weak', 0.038), ('ng', 0.037), ('feature', 0.037), ('xk', 0.037), ('wl', 0.037), ('ancestry', 0.037), ('antecedence', 0.037), ('ceafe', 0.037), ('configurational', 0.037), ('posn', 0.037), ('referentiality', 0.037), ('cell', 0.036), ('altaf', 0.036), ('centering', 0.036), ('conjoined', 0.036), ('proper', 0.035), ('berkeley', 0.035), ('ponzetto', 0.033), ('anaphor', 0.033), ('cai', 0.033), ('homogeneous', 0.033), ('choices', 0.033), ('indicator', 0.033), ('stanford', 0.033), ('agr', 0.032), ('lassalle', 0.032), ('daelemans', 0.032), ('bilexical', 0.032), ('martschat', 0.032), ('klein', 0.031), ('loss', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="67-tfidf-1" href="./emnlp-2013-Easy_Victories_and_Uphill_Battles_in_Coreference_Resolution.html">67 emnlp-2013-Easy Victories and Uphill Battles in Coreference Resolution</a></p>
<p>Author: Greg Durrett ; Dan Klein</p><p>Abstract: Classical coreference systems encode various syntactic, discourse, and semantic phenomena explicitly, using heterogenous features computed from hand-crafted heuristics. In contrast, we present a state-of-the-art coreference system that captures such phenomena implicitly, with a small number of homogeneous feature templates examining shallow properties of mentions. Surprisingly, our features are actually more effective than the corresponding hand-engineered ones at modeling these key linguistic phenomena, allowing us to win “easy victories” without crafted heuristics. These features are successful on syntax and discourse; however, they do not model semantic compatibility well, nor do we see gains from experiments with shallow semantic features from the literature, suggesting that this approach to semantics is an “uphill battle.” Nonetheless, our final system1 outperforms the Stanford system (Lee et al. (201 1), the winner of the CoNLL 2011 shared task) by 3.5% absolute on the CoNLL metric and outperforms the IMS system (Bj o¨rkelund and Farkas (2012), the best publicly available English coreference system) by 1.9% absolute.</p><p>2 0.60092551 <a title="67-tfidf-2" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>Author: Jonathan K. Kummerfeld ; Dan Klein</p><p>Abstract: Coreference resolution metrics quantify errors but do not analyze them. Here, we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types. Using this tool, we first compare the error distributions across a large set of systems, then analyze common errors across the top ten systems, empirically characterizing the major unsolved challenges of the coreference resolution task.</p><p>3 0.51460278 <a title="67-tfidf-3" href="./emnlp-2013-A_Constrained_Latent_Variable_Model_for_Coreference_Resolution.html">1 emnlp-2013-A Constrained Latent Variable Model for Coreference Resolution</a></p>
<p>Author: Kai-Wei Chang ; Rajhans Samdani ; Dan Roth</p><p>Abstract: Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L3M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L3M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L3M and its constrained version, CL3M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.</p><p>4 0.43323615 <a title="67-tfidf-4" href="./emnlp-2013-Exploiting_Zero_Pronouns_to_Improve_Chinese_Coreference_Resolution.html">80 emnlp-2013-Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</a></p>
<p>Author: Fang Kong ; Hwee Tou Ng</p><p>Abstract: Coreference resolution plays a critical role in discourse analysis. This paper focuses on exploiting zero pronouns to improve Chinese coreference resolution. In particular, a simplified semantic role labeling framework is proposed to identify clauses and to detect zero pronouns effectively, and two effective methods (refining syntactic parser and refining learning example generation) are employed to exploit zero pronouns for Chinese coreference resolution. Evaluation on the CoNLL-2012 shared task data set shows that zero pronouns can significantly improve Chinese coreference resolution.</p><p>5 0.37986144 <a title="67-tfidf-5" href="./emnlp-2013-Joint_Coreference_Resolution_and_Named-Entity_Linking_with_Multi-Pass_Sieves.html">112 emnlp-2013-Joint Coreference Resolution and Named-Entity Linking with Multi-Pass Sieves</a></p>
<p>Author: Hannaneh Hajishirzi ; Leila Zilles ; Daniel S. Weld ; Luke Zettlemoyer</p><p>Abstract: Many errors in coreference resolution come from semantic mismatches due to inadequate world knowledge. Errors in named-entity linking (NEL), on the other hand, are often caused by superficial modeling of entity context. This paper demonstrates that these two tasks are complementary. We introduce NECO, a new model for named entity linking and coreference resolution, which solves both problems jointly, reducing the errors made on each. NECO extends the Stanford deterministic coreference system by automatically linking mentions to Wikipedia and introducing new NEL-informed mention-merging sieves. Linking improves mention-detection and enables new semantic attributes to be incorporated from Freebase, while coreference provides better context modeling by propagating named-entity links within mention clusters. Experiments show consistent improve- ments across a number of datasets and experimental conditions, including over 11% reduction in MUC coreference error and nearly 21% reduction in F1 NEL error on ACE 2004 newswire data.</p><p>6 0.22173071 <a title="67-tfidf-6" href="./emnlp-2013-Latent_Anaphora_Resolution_for_Cross-Lingual_Pronoun_Prediction.html">117 emnlp-2013-Latent Anaphora Resolution for Cross-Lingual Pronoun Prediction</a></p>
<p>7 0.20514333 <a title="67-tfidf-7" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>8 0.12489675 <a title="67-tfidf-8" href="./emnlp-2013-Chinese_Zero_Pronoun_Resolution%3A_Some_Recent_Advances.html">45 emnlp-2013-Chinese Zero Pronoun Resolution: Some Recent Advances</a></p>
<p>9 0.11431967 <a title="67-tfidf-9" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>10 0.10306189 <a title="67-tfidf-10" href="./emnlp-2013-Learning_Biological_Processes_with_Global_Constraints.html">118 emnlp-2013-Learning Biological Processes with Global Constraints</a></p>
<p>11 0.10184333 <a title="67-tfidf-11" href="./emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model.html">75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</a></p>
<p>12 0.10152715 <a title="67-tfidf-12" href="./emnlp-2013-Interpreting_Anaphoric_Shell_Nouns_using_Antecedents_of_Cataphoric_Shell_Nouns_as_Training_Data.html">108 emnlp-2013-Interpreting Anaphoric Shell Nouns using Antecedents of Cataphoric Shell Nouns as Training Data</a></p>
<p>13 0.089281358 <a title="67-tfidf-13" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>14 0.080869868 <a title="67-tfidf-14" href="./emnlp-2013-Animacy_Detection_with_Voting_Models.html">23 emnlp-2013-Animacy Detection with Voting Models</a></p>
<p>15 0.079144098 <a title="67-tfidf-15" href="./emnlp-2013-Cascading_Collective_Classification_for_Bridging_Anaphora_Recognition_using_a_Rich_Linguistic_Feature_Set.html">43 emnlp-2013-Cascading Collective Classification for Bridging Anaphora Recognition using a Rich Linguistic Feature Set</a></p>
<p>16 0.078388453 <a title="67-tfidf-16" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>17 0.07344944 <a title="67-tfidf-17" href="./emnlp-2013-Event-Based_Time_Label_Propagation_for_Automatic_Dating_of_News_Articles.html">74 emnlp-2013-Event-Based Time Label Propagation for Automatic Dating of News Articles</a></p>
<p>18 0.071008913 <a title="67-tfidf-18" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>19 0.065775201 <a title="67-tfidf-19" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>20 0.062522799 <a title="67-tfidf-20" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.278), (1, 0.35), (2, 0.609), (3, -0.21), (4, 0.077), (5, -0.093), (6, 0.028), (7, -0.143), (8, -0.08), (9, -0.057), (10, -0.093), (11, -0.009), (12, 0.019), (13, 0.001), (14, 0.095), (15, 0.005), (16, 0.008), (17, -0.009), (18, -0.011), (19, -0.027), (20, -0.013), (21, -0.034), (22, 0.009), (23, -0.023), (24, 0.018), (25, -0.011), (26, -0.053), (27, -0.02), (28, 0.025), (29, 0.036), (30, 0.005), (31, -0.01), (32, -0.011), (33, -0.014), (34, -0.064), (35, -0.015), (36, 0.003), (37, -0.032), (38, 0.004), (39, -0.012), (40, -0.012), (41, -0.008), (42, 0.019), (43, -0.024), (44, 0.03), (45, 0.009), (46, 0.009), (47, -0.006), (48, -0.01), (49, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97040033 <a title="67-lsi-1" href="./emnlp-2013-Easy_Victories_and_Uphill_Battles_in_Coreference_Resolution.html">67 emnlp-2013-Easy Victories and Uphill Battles in Coreference Resolution</a></p>
<p>Author: Greg Durrett ; Dan Klein</p><p>Abstract: Classical coreference systems encode various syntactic, discourse, and semantic phenomena explicitly, using heterogenous features computed from hand-crafted heuristics. In contrast, we present a state-of-the-art coreference system that captures such phenomena implicitly, with a small number of homogeneous feature templates examining shallow properties of mentions. Surprisingly, our features are actually more effective than the corresponding hand-engineered ones at modeling these key linguistic phenomena, allowing us to win “easy victories” without crafted heuristics. These features are successful on syntax and discourse; however, they do not model semantic compatibility well, nor do we see gains from experiments with shallow semantic features from the literature, suggesting that this approach to semantics is an “uphill battle.” Nonetheless, our final system1 outperforms the Stanford system (Lee et al. (201 1), the winner of the CoNLL 2011 shared task) by 3.5% absolute on the CoNLL metric and outperforms the IMS system (Bj o¨rkelund and Farkas (2012), the best publicly available English coreference system) by 1.9% absolute.</p><p>2 0.93585789 <a title="67-lsi-2" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>Author: Jonathan K. Kummerfeld ; Dan Klein</p><p>Abstract: Coreference resolution metrics quantify errors but do not analyze them. Here, we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types. Using this tool, we first compare the error distributions across a large set of systems, then analyze common errors across the top ten systems, empirically characterizing the major unsolved challenges of the coreference resolution task.</p><p>3 0.93058795 <a title="67-lsi-3" href="./emnlp-2013-A_Constrained_Latent_Variable_Model_for_Coreference_Resolution.html">1 emnlp-2013-A Constrained Latent Variable Model for Coreference Resolution</a></p>
<p>Author: Kai-Wei Chang ; Rajhans Samdani ; Dan Roth</p><p>Abstract: Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L3M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L3M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L3M and its constrained version, CL3M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.</p><p>4 0.91088265 <a title="67-lsi-4" href="./emnlp-2013-Exploiting_Zero_Pronouns_to_Improve_Chinese_Coreference_Resolution.html">80 emnlp-2013-Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</a></p>
<p>Author: Fang Kong ; Hwee Tou Ng</p><p>Abstract: Coreference resolution plays a critical role in discourse analysis. This paper focuses on exploiting zero pronouns to improve Chinese coreference resolution. In particular, a simplified semantic role labeling framework is proposed to identify clauses and to detect zero pronouns effectively, and two effective methods (refining syntactic parser and refining learning example generation) are employed to exploit zero pronouns for Chinese coreference resolution. Evaluation on the CoNLL-2012 shared task data set shows that zero pronouns can significantly improve Chinese coreference resolution.</p><p>5 0.89461142 <a title="67-lsi-5" href="./emnlp-2013-Joint_Coreference_Resolution_and_Named-Entity_Linking_with_Multi-Pass_Sieves.html">112 emnlp-2013-Joint Coreference Resolution and Named-Entity Linking with Multi-Pass Sieves</a></p>
<p>Author: Hannaneh Hajishirzi ; Leila Zilles ; Daniel S. Weld ; Luke Zettlemoyer</p><p>Abstract: Many errors in coreference resolution come from semantic mismatches due to inadequate world knowledge. Errors in named-entity linking (NEL), on the other hand, are often caused by superficial modeling of entity context. This paper demonstrates that these two tasks are complementary. We introduce NECO, a new model for named entity linking and coreference resolution, which solves both problems jointly, reducing the errors made on each. NECO extends the Stanford deterministic coreference system by automatically linking mentions to Wikipedia and introducing new NEL-informed mention-merging sieves. Linking improves mention-detection and enables new semantic attributes to be incorporated from Freebase, while coreference provides better context modeling by propagating named-entity links within mention clusters. Experiments show consistent improve- ments across a number of datasets and experimental conditions, including over 11% reduction in MUC coreference error and nearly 21% reduction in F1 NEL error on ACE 2004 newswire data.</p><p>6 0.56512421 <a title="67-lsi-6" href="./emnlp-2013-Chinese_Zero_Pronoun_Resolution%3A_Some_Recent_Advances.html">45 emnlp-2013-Chinese Zero Pronoun Resolution: Some Recent Advances</a></p>
<p>7 0.50948942 <a title="67-lsi-7" href="./emnlp-2013-Latent_Anaphora_Resolution_for_Cross-Lingual_Pronoun_Prediction.html">117 emnlp-2013-Latent Anaphora Resolution for Cross-Lingual Pronoun Prediction</a></p>
<p>8 0.46152267 <a title="67-lsi-8" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>9 0.37162191 <a title="67-lsi-9" href="./emnlp-2013-Animacy_Detection_with_Voting_Models.html">23 emnlp-2013-Animacy Detection with Voting Models</a></p>
<p>10 0.33475959 <a title="67-lsi-10" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>11 0.31665233 <a title="67-lsi-11" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>12 0.29713246 <a title="67-lsi-12" href="./emnlp-2013-Using_Soft_Constraints_in_Joint_Inference_for_Clinical_Concept_Recognition.html">198 emnlp-2013-Using Soft Constraints in Joint Inference for Clinical Concept Recognition</a></p>
<p>13 0.28903472 <a title="67-lsi-13" href="./emnlp-2013-Interpreting_Anaphoric_Shell_Nouns_using_Antecedents_of_Cataphoric_Shell_Nouns_as_Training_Data.html">108 emnlp-2013-Interpreting Anaphoric Shell Nouns using Antecedents of Cataphoric Shell Nouns as Training Data</a></p>
<p>14 0.27711219 <a title="67-lsi-14" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>15 0.24921252 <a title="67-lsi-15" href="./emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model.html">75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</a></p>
<p>16 0.24041799 <a title="67-lsi-16" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>17 0.23698048 <a title="67-lsi-17" href="./emnlp-2013-Cascading_Collective_Classification_for_Bridging_Anaphora_Recognition_using_a_Rich_Linguistic_Feature_Set.html">43 emnlp-2013-Cascading Collective Classification for Bridging Anaphora Recognition using a Rich Linguistic Feature Set</a></p>
<p>18 0.21546145 <a title="67-lsi-18" href="./emnlp-2013-Automatically_Detecting_and_Attributing_Indirect_Quotations.html">35 emnlp-2013-Automatically Detecting and Attributing Indirect Quotations</a></p>
<p>19 0.21311888 <a title="67-lsi-19" href="./emnlp-2013-Predicting_the_Resolution_of_Referring_Expressions_from_User_Behavior.html">153 emnlp-2013-Predicting the Resolution of Referring Expressions from User Behavior</a></p>
<p>20 0.21024068 <a title="67-lsi-20" href="./emnlp-2013-Learning_Biological_Processes_with_Global_Constraints.html">118 emnlp-2013-Learning Biological Processes with Global Constraints</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.033), (17, 0.017), (18, 0.024), (22, 0.029), (30, 0.074), (50, 0.019), (51, 0.186), (58, 0.289), (66, 0.037), (71, 0.02), (75, 0.071), (77, 0.021), (95, 0.025), (96, 0.051), (97, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83122551 <a title="67-lda-1" href="./emnlp-2013-Mining_New_Business_Opportunities%3A_Identifying_Trend_related_Products_by_Leveraging_Commercial_Intents_from_Microblogs.html">131 emnlp-2013-Mining New Business Opportunities: Identifying Trend related Products by Leveraging Commercial Intents from Microblogs</a></p>
<p>Author: Jinpeng Wang ; Wayne Xin Zhao ; Haitian Wei ; Hongfei Yan ; Xiaoming Li</p><p>Abstract: Hot trends are likely to bring new business opportunities. For example, “Air Pollution” might lead to a significant increase of the sales of related products, e.g., mouth mask. For ecommerce companies, it is very important to make rapid and correct response to these hot trends in order to improve product sales. In this paper, we take the initiative to study the task of how to identify trend related products. The major novelty of our work is that we automatically learn commercial intents revealed from microblogs. We carefully construct a data collection for this task and present quite a few insightful findings. In order to solve this problem, we further propose a graph based method, which jointly models relevance and associativity. We perform extensive experiments and the results showed that our methods are very effective.</p><p>same-paper 2 0.79315019 <a title="67-lda-2" href="./emnlp-2013-Easy_Victories_and_Uphill_Battles_in_Coreference_Resolution.html">67 emnlp-2013-Easy Victories and Uphill Battles in Coreference Resolution</a></p>
<p>Author: Greg Durrett ; Dan Klein</p><p>Abstract: Classical coreference systems encode various syntactic, discourse, and semantic phenomena explicitly, using heterogenous features computed from hand-crafted heuristics. In contrast, we present a state-of-the-art coreference system that captures such phenomena implicitly, with a small number of homogeneous feature templates examining shallow properties of mentions. Surprisingly, our features are actually more effective than the corresponding hand-engineered ones at modeling these key linguistic phenomena, allowing us to win “easy victories” without crafted heuristics. These features are successful on syntax and discourse; however, they do not model semantic compatibility well, nor do we see gains from experiments with shallow semantic features from the literature, suggesting that this approach to semantics is an “uphill battle.” Nonetheless, our final system1 outperforms the Stanford system (Lee et al. (201 1), the winner of the CoNLL 2011 shared task) by 3.5% absolute on the CoNLL metric and outperforms the IMS system (Bj o¨rkelund and Farkas (2012), the best publicly available English coreference system) by 1.9% absolute.</p><p>3 0.75785547 <a title="67-lda-3" href="./emnlp-2013-Improving_Web_Search_Ranking_by_Incorporating_Structured_Annotation_of_Queries.html">105 emnlp-2013-Improving Web Search Ranking by Incorporating Structured Annotation of Queries</a></p>
<p>Author: Xiao Ding ; Zhicheng Dou ; Bing Qin ; Ting Liu ; Ji-rong Wen</p><p>Abstract: Web users are increasingly looking for structured data, such as lyrics, job, or recipes, using unstructured queries on the web. However, retrieving relevant results from such data is a challenging problem due to the unstructured language of the web queries. In this paper, we propose a method to improve web search ranking by detecting Structured Annotation of queries based on top search results. In a structured annotation, the original query is split into different units that are associated with semantic attributes in the corresponding domain. We evaluate our techniques using real world queries and achieve significant improvement. . 1</p><p>4 0.67645699 <a title="67-lda-4" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>Author: Jonathan K. Kummerfeld ; Dan Klein</p><p>Abstract: Coreference resolution metrics quantify errors but do not analyze them. Here, we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types. Using this tool, we first compare the error distributions across a large set of systems, then analyze common errors across the top ten systems, empirically characterizing the major unsolved challenges of the coreference resolution task.</p><p>5 0.65192109 <a title="67-lda-5" href="./emnlp-2013-Exploiting_Zero_Pronouns_to_Improve_Chinese_Coreference_Resolution.html">80 emnlp-2013-Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</a></p>
<p>Author: Fang Kong ; Hwee Tou Ng</p><p>Abstract: Coreference resolution plays a critical role in discourse analysis. This paper focuses on exploiting zero pronouns to improve Chinese coreference resolution. In particular, a simplified semantic role labeling framework is proposed to identify clauses and to detect zero pronouns effectively, and two effective methods (refining syntactic parser and refining learning example generation) are employed to exploit zero pronouns for Chinese coreference resolution. Evaluation on the CoNLL-2012 shared task data set shows that zero pronouns can significantly improve Chinese coreference resolution.</p><p>6 0.62632138 <a title="67-lda-6" href="./emnlp-2013-Chinese_Zero_Pronoun_Resolution%3A_Some_Recent_Advances.html">45 emnlp-2013-Chinese Zero Pronoun Resolution: Some Recent Advances</a></p>
<p>7 0.60114634 <a title="67-lda-7" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>8 0.60042524 <a title="67-lda-8" href="./emnlp-2013-Joint_Coreference_Resolution_and_Named-Entity_Linking_with_Multi-Pass_Sieves.html">112 emnlp-2013-Joint Coreference Resolution and Named-Entity Linking with Multi-Pass Sieves</a></p>
<p>9 0.59969932 <a title="67-lda-9" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>10 0.59670931 <a title="67-lda-10" href="./emnlp-2013-Unsupervised_Induction_of_Cross-Lingual_Semantic_Relations.html">193 emnlp-2013-Unsupervised Induction of Cross-Lingual Semantic Relations</a></p>
<p>11 0.59555084 <a title="67-lda-11" href="./emnlp-2013-Document_Summarization_via_Guided_Sentence_Compression.html">65 emnlp-2013-Document Summarization via Guided Sentence Compression</a></p>
<p>12 0.59446108 <a title="67-lda-12" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>13 0.59259444 <a title="67-lda-13" href="./emnlp-2013-A_Constrained_Latent_Variable_Model_for_Coreference_Resolution.html">1 emnlp-2013-A Constrained Latent Variable Model for Coreference Resolution</a></p>
<p>14 0.5924902 <a title="67-lda-14" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>15 0.59161961 <a title="67-lda-15" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>16 0.59024107 <a title="67-lda-16" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>17 0.5894953 <a title="67-lda-17" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>18 0.58894217 <a title="67-lda-18" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>19 0.58801973 <a title="67-lda-19" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>20 0.58654529 <a title="67-lda-20" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
