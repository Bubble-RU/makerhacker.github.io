<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-191" href="#">emnlp2013-191</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</h1>
<br/><p>Source: <a title="emnlp-2013-191-pdf" href="http://aclweb.org/anthology//D/D13/D13-1124.pdf">pdf</a></p><p>Author: Polina Kuznetsova ; Jianfu Chen ; Yejin Choi</p><p>Abstract: Why do certain combinations of words such as “disadvantageous peace ” or “metal to the petal” appeal to our minds as interesting expressions with a sense of creativity, while other phrases such as “quiet teenager”, or “geometrical base ” not as much? We present statistical explorations to understand the characteristics of lexical compositions that give rise to the perception of being original, interesting, and at times even artistic. We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learning that give us further insights on how different aspects of lexical composition collectively contribute to the perceived creativity.</p><p>Reference: <a title="emnlp-2013-191-reference" href="../emnlp2013_reference/emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu }@  ,  ,  Abstract Why do certain combinations of words such as “disadvantageous peace ” or “metal to the petal” appeal to our minds as interesting expressions with a sense of creativity, while other phrases such as “quiet teenager”, or “geometrical base ” not as much? [sent-3, score-0.178]
</p><p>2 1 Introduction  An essential property of natural language is the generative capacity that makes it possible for people to express indefinitely many thoughts through indefinitely many different ways of composing phrases and sentences (Chomsky, 1965). [sent-6, score-0.092]
</p><p>3 The possibility of novel, creative expressions never seems to exhaust. [sent-7, score-0.613]
</p><p>4 Various types of writers, such as novelists, journalists, movie script writers, and creatives in advertising, continue creating novel phrases and expressions that are original while befitting in expressing the desired meaning in the given situation. [sent-8, score-0.094]
</p><p>5 Writers put significant effort in choosing the perfect words in completing their compositions, as a well-chosen combination of words is impactful in readers’ minds for rendering the precise intended meaning, as well as stimulating an increased level of cognitive responses and attention. [sent-11, score-0.099]
</p><p>6 Moreover, recent studies based on fMRI begin to discover biological evidences that support the impact of creative phrases on people’s minds. [sent-17, score-0.591]
</p><p>7 These studies report that unconventional metaphoric expressions elicit significantly increased involvement of brain processing when compared against the effect of conventional metaphors or literal expressions (e. [sent-18, score-0.301]
</p><p>8 In this paper, as a small step toward quantitative understanding of linguistic creativity, we present a focused study on lexical composition two content words. [sent-27, score-0.132]
</p><p>9 Different studies assumed different definitions of linguistic creativity depending on their context and end goals (e. [sent-41, score-0.57]
</p><p>10 In this paper, as an operational definition, we consider a phrase creative if it is (a) unconventional or uncommon, and (b) expressive in an interesting, imaginative, or inspirational way. [sent-45, score-0.63]
</p><p>11 A system that can recognize creative expressions could be of practical use for many aspiring writers who are often in need of inspirational help in searching for the optimal choice of words. [sent-46, score-0.702]
</p><p>12 With these practical goals in mind, we aim to understand phrases with linguistic creativity in a broad scope. [sent-48, score-0.606]
</p><p>13 (2009), our study encompasses phrases that evoke the sense of interestingness and creativity in readers’ minds, rather than focusing exclusively on clearly but narrowly defined figure of speeches such as metaphors (e. [sent-50, score-0.749]
</p><p>14 2  Theories of Creativity and Hypotheses  Many researchers, from the ancient philosophers to the modern time scientists, have proposed theories that attempt to explain the mechanism of creative process. [sent-62, score-0.594]
</p><p>15 In this section, we draw connections from some of these theories developed for general human creativity to the problem of quantitatively interpreting linguistic creativity in lexical composition. [sent-63, score-1.179]
</p><p>16 , McCrae (1987)), which seeks to generate multiple unstereotypical solutions to an open ended problem has been considered as the key element in creative process, which contrasts with convergent thinking that find a single, correct solution (e. [sent-67, score-0.607]
</p><p>17 Applying the same high-level idea to lexical composition, divergent composition that explores an unusual, uncon-  ventional set of words is more likely to be creative. [sent-70, score-0.205]
</p><p>18 Note that the key novelty then lies in the compositional operation itself, i. [sent-71, score-0.114]
</p><p>19 In recent years there has been a swell of work on compositional distributional semantics that captures the compositional aspects of language understanding, such as sentiment analysis (e. [sent-74, score-0.319]
</p><p>20 However, none has examined the compositional nature in quantifying creativity in lexical composition. [sent-80, score-0.723]
</p><p>21 We consider two computational approaches to capture the notion of creative composition. [sent-81, score-0.555]
</p><p>22 , relative entropy reduction, to measure the surprisal of seeing the next word given the previous word. [sent-84, score-0.142]
</p><p>23 The second is via supervised learning, where we explore different modeling techniques to capture the statistical regularities in creative compositional operations. [sent-85, score-0.669]
</p><p>24 In particular, we will explore (1) compo-  sitional operations of vector space models, (2) kernels capturing the non-linear composition of different dimensions in the meaning space, (3) the use of neural networks as an alternative to incorporate nonlinearity in vector composition. [sent-86, score-0.221]
</p><p>25 2 Therefore, we must consider additional conditions that give rise to creative phrases. [sent-92, score-0.555]
</p><p>26 3 Then we hypothesize that some subsets of semantic space {Si |Si ⊂ S} are semantically futile regions cfo srp appreciable linguistic creativity, regardless of how novel the composition in itself might be. [sent-97, score-0.309]
</p><p>27 Similarly, we expect semantically fruitful subsets of semantic space where cre-  ative expressions are more frequently found. [sent-99, score-0.154]
</p><p>28 For instance, phrases such as “guns and roses ” and “metal to the petal” are semantically close to each other and yet both can be considered as interesting and creative (as opposed to one of them losing the sense of creativity due to its semantic proximity to the other). [sent-100, score-1.195]
</p><p>29 This notion of creative semantic subspace connects to theories that suggest that latent memories serve as motives for creative ideas and that one’s creativity is largely depending on prior experience and knowledge one has been exposed to (e. [sent-101, score-1.805]
</p><p>30 , Freud (1908), Necka (1999), Glaskin (201 1), Cohen and Levinthal (1990), Amabile (1997)), a point also made by Einstein: “The secret to creativity is knowing how to hide your sources. [sent-103, score-0.57]
</p><p>31 3Investigation on recursive composition of more than two content words and the influence of syntactic packaging is left as future research. [sent-105, score-0.132]
</p><p>32 “kingdom ” and “power” is relatively more fruitful for composing creative (i. [sent-112, score-0.617]
</p><p>33 , unique and uncommon while being imaginative and interesting, per our operational definition of creativity given in § 1) word pairs, e. [sent-114, score-0.609]
</p><p>34 yIn g our empirical oirndvestigation, this notion of semantically fruitful and futile semantic subspaces are captured using dis-  tributional semantic space models under supervised learning framework (§5). [sent-117, score-0.2]
</p><p>35 3 Affective Language Another angle we probe is the connection between creative expressions and the use of affective language. [sent-119, score-0.658]
</p><p>36 This idea is supported in part by previous research that explored the connection between figurative languages such as metaphors and sentiment (e. [sent-120, score-0.251]
</p><p>37 The focus of previous work was either on interpretation of the sentiment in metaphors, or the use of metaphors in the description of affect. [sent-125, score-0.173]
</p><p>38 In contrast, we aim to quantify the correlation between creative expressions (beyond metaphors) and the use of sentimentladen words in a more systematic way. [sent-126, score-0.613]
</p><p>39 This exploration has a connection to the creative semantic subspace discussed earlier (§2. [sent-127, score-0.686]
</p><p>40 3 Creative Language Dataset We start our investigation by considering two types of naturally existing collection of sentences: (1) quotes and (2) dictionary glosses. [sent-130, score-0.146]
</p><p>41 QUOTESraw: We crawled inspirational quotes from “Brainy GLOSSESraw: We collected glosses from Oxford Dictionary and Merriam-Webster Overall we crawled about 8K definitions. [sent-134, score-0.248]
</p><p>42 Not all pairs from QUOTESraw are creative, and likewise, not all pairs from GLOSSESraw are uncreative. [sent-154, score-0.114]
</p><p>43 We ask three turkers to score each pair in 1-5 scale, where 1is the least creative and 5 is the most creative. [sent-161, score-0.555]
</p><p>44 We then obtain the final creativity scale score by averaging  the scores over 3 users. [sent-162, score-0.57]
</p><p>45 In addition, we ask turkers a series of yes/no questions to help turkers to determine whether the given pair is creative or not. [sent-163, score-0.555]
</p><p>46 8 We determine the final label of a word pair based on two scores, creativity scale score and yes/no questionbased score. [sent-164, score-0.57]
</p><p>47 If creativity scale score is 4 or 5 and question-based score is positive, we label the pair as creative. [sent-165, score-0.57]
</p><p>48 Similarly, if creativity scale score is 1 or 2 and question-based score is negative, we label the pair as common. [sent-166, score-0.57]
</p><p>49 This filtering process is akin to the removal of neural sentiment in the early work of sentiment analysis (e. [sent-168, score-0.126]
</p><p>50 As expected, word pairs with high frequencies are much more likely to be common, while word pairs with  low frequencies can be either of the two. [sent-176, score-0.188]
</p><p>51 Also as expected, pairs extracted from QUOTES are relatively more likely to be creative than those from GLOSSES. [sent-177, score-0.612]
</p><p>52 Final Dataset: From our initial annotation study, it became apparent to us that creative pairs are very rare, perhaps not surprisingly, even among infrequent pairs. [sent-193, score-0.612]
</p><p>53 In order to build the word pair corpus with as many creative pairs as possible, we focus on infrequent word pairs for further annotation, from which we construct a larger and balanced set of creative and common word pairs, with 394 word pairs for each class. [sent-194, score-1.281]
</p><p>54 1 Information Measures In this section we explore information theoretic measures to quantify the surprisal aspect of creative word pairs, relating to the divergent, compositional nature of creativity discussed in §2. [sent-199, score-1.436]
</p><p>55 For instance, the entropy after seeing “very” would be higher than that after seeing “inglorious”, as the former can be used in a wider variety of context than the later. [sent-208, score-0.117]
</p><p>56 12We also compute KL(w1 , w2) in a similar manner as KL(w1w2 , w1) the effective measures in capturing creative pairs. [sent-216, score-0.599]
</p><p>57 Interestingly, information theoretic measures that compare the distribution of word’s context, such as RH(w1 , w2), KL(w1w2 , w1) and MI(w1 , w2), capture the surprisal aspect of creativity better than simple frequencies or PMI scores that do not consider contextual changes. [sent-218, score-0.804]
</p><p>58 Second, these measures only capture the surprisal aspect of creativity, missing the other important qualities: interestingness or imaginativeness. [sent-222, score-0.169]
</p><p>59 2 Sentiment and Connotation Next we investigate the connection between creativity and sentiment, as illustrated in §2. [sent-224, score-0.615]
</p><p>60 14 When wi has a negative polarity L(wi) is assigned a value of -1, and when wi is positive L(wi) is equal to 1. [sent-235, score-0.121]
</p><p>61 14We denote polarity from OpinionFinder as Lsubj and connotation as Lconn  1252  MeasureCorr Coeffp-value∗adj p-value∗∗  PFMreIq( w 1 ,w 2 ) pointw0 . [sent-241, score-0.103]
</p><p>62 0786e34-7085  Table 4: Pearson correlation between various measures and creativity of word pairs. [sent-254, score-0.614]
</p><p>63 sniogtnei *fic: aTnwceo- (tpail ≤ed 0 p-value, 394 word pairs per class note **: We used Benjamini-Hochberg method to adjust  p-values for multiple tests Table 4 shows Pearson coefficient for sentiment and connotation based measures. [sent-257, score-0.188]
</p><p>64 It turns out that polarity of each word on its own does not have a high impact on the creativity of a word pair. [sent-258, score-0.605]
</p><p>65 However, in order to pursue the conceptual aspect of creativity illustrated in §2. [sent-268, score-0.606]
</p><p>66 2, that is, the tnuoatilo ansp poefc ts eomf carnetaitci subspaces ttheadt are inherently fthuetile or fruitful for creativity, we need to incorporate semantic representations more directly. [sent-269, score-0.124]
</p><p>67 Another goal of this section will be additional learning-based investigation to the compositional nature of creative word pairs, complementing the investigation in §4, which focused on the compositional aspect ionf creativity hd feosccurisbeedd oinn §2. [sent-271, score-1.447]
</p><p>68 sWitiiothn aabl aosvpe goals icnre mind, yin d weschratib follows, we explore three different ways to learn compositional aspect of creative word pairs: (1) learning with explicit compositional vector operations (§5. [sent-273, score-0.88]
</p><p>69 1), (2) learning ncoomnlipnoesairti composition pveiraa tkieornnsel (s§ (§5. [sent-274, score-0.132]
</p><p>70 2), (3) alernairnnging lnionnelairn ecoamr composition vkeiar deep learning (§5. [sent-275, score-0.132]
</p><p>71 iNngote n othnlaitn iena arl cl othmepsoes approaches, tph lee naorntiionng o(§f5 cre-  ative semantic subspace is integrated indirectly, as the feature representation always incorporates the resulting (composed) vector representations. [sent-277, score-0.114]
</p><p>72 Since the size of creative pair dataset is not at scale yet, we choose to work with vector space models that are in reduced dimensions. [sent-279, score-0.583]
</p><p>73 1 Compositional Vector Operations We consider the following compositional vector operations inspired by recent studies for compositional distributional semantics (e. [sent-284, score-0.317]
</p><p>74 w~∗1 w~ , w~ 2} •• MAX:: mminax{ w{~ w~1 , w~ }2} All operations take two input vectors ∈ Rn, and output a vector ∈ eR tnw. [sent-290, score-0.096]
</p><p>75 Besides using features based on the composed vectors, we also experiment with features based on concatenating multiple composed vectors, in the hope to capture more diverse compositional operations. [sent-293, score-0.114]
</p><p>76 2 Learning Nonlinear Composition via Kernels As an alternative to explicit vector compositions, we also probe implicit operations based on non-linear combinations of semantic dimensions using kernels (e. [sent-296, score-0.095]
</p><p>77 3  Learning Non-linear Composition via Deep Learning Yet another alternative to model non-linear composition is deep learning. [sent-299, score-0.132]
</p><p>78 We follow the formulation of vector composition proposed by Socher et al. [sent-303, score-0.16]
</p><p>79 (201 1) models the composition of a word pair as a non-linear transformation of their concatenation [ w~1 ; w~ 2] :  p~  p~ = f(M1[ w~1; w~ 2] + ~b1)  (4)  where M1 ∈ Rn×2n. [sent-307, score-0.167]
</p><p>80 and a softmax layer to predict the probability of the word pair being creative and not creative. [sent-313, score-0.555]
</p><p>81 We see that simple vector composition 1254 alone does not perform better than vector concatenation [ w~1 ; w~ 2] . [sent-319, score-0.188]
</p><p>82 K}e wrniethls [ w~with non-linear transformation of feature space generally improve performance over linear SVM, suggesting that kernels capture some of the interesting compositional aspect of creativity that is not covered by some of the explicit vector compositions considered in §5. [sent-321, score-0.824]
</p><p>83 Unfortunately learning nonlinear composition with deep learning did not yield better results. [sent-326, score-0.132]
</p><p>84 Figure 5 shows some of the interesting regions of the projection: some regions are relatively futile in having creative phrases (e. [sent-332, score-0.779]
</p><p>85 , regions involving simple adjectives such as “good”, “bad”, regions corresponding to legal terms), while some regions are relatively more fruitful (e. [sent-334, score-0.324]
</p><p>86 , in the vicinity of “true ”, “perfect” or “intelligent” in Figure 5) where the separation between creative and  noncreative phrases are not as prominent. [sent-339, score-0.591]
</p><p>87 In those regions, compositional aspects would play a bigger role in determining creativity than memorizing fruitful semantic subspaces. [sent-340, score-0.78]
</p><p>88 Other linguistic devices and phenomena related to creativity include irony (e. [sent-349, score-0.57]
</p><p>89 Veale (201 1) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. [sent-360, score-0.669]
</p><p>90 Our work contributes to the retrieval process of recognizing more creative phrases. [sent-361, score-0.555]
</p><p>91 Ozbal and Strapparava (2012) explored automatic creative naming of commercial products and services, focusing on the generation of creative phrases within a specific domain. [sent-362, score-1.146]
</p><p>92 In contrast, we present a datadriven investigation to quantifying creativity in lexical composition. [sent-364, score-0.638]
</p><p>93 Memorability is loosely related to 1255 linguistic creativity (Danescu-Niculescu-Mizil et al. [sent-365, score-0.57]
</p><p>94 (2012)) as some of the creative quotes may be more memorable, but not all creative phrases are memorable and vice versa. [sent-366, score-1.291]
</p><p>95 Our experimental results suggest the viability of learning creative language, and point to promising directions for future research. [sent-368, score-0.555]
</p><p>96 Motivating creativity in organizations: On doing what you love and loving what you do. [sent-374, score-0.57]
</p><p>97 Investigating creative language: People’s choice of words in the production of novel noun-noun compounds. [sent-425, score-0.555]
</p><p>98 An fmri investigation of the neural correlates  underlying the processing of novel metaphoric expressions. [sent-525, score-0.108]
</p><p>99 A computational approach to the automation of creative naming. [sent-570, score-0.555]
</p><p>100 a linguistic creativity mea-  sure from computer science and cognitive psychology perspectives. [sent-670, score-0.613]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('creativity', 0.57), ('creative', 0.555), ('composition', 0.132), ('quotes', 0.117), ('compositional', 0.114), ('metaphors', 0.11), ('freq', 0.098), ('glosses', 0.089), ('mashal', 0.084), ('kl', 0.082), ('val', 0.073), ('divergent', 0.073), ('regions', 0.073), ('veale', 0.07), ('connotation', 0.068), ('bucket', 0.068), ('sentiment', 0.063), ('fruitful', 0.062), ('theoretic', 0.061), ('expressions', 0.058), ('pairs', 0.057), ('glossesraw', 0.056), ('inglorious', 0.056), ('minds', 0.056), ('quotesraw', 0.056), ('surprisal', 0.056), ('unusual', 0.056), ('entropy', 0.055), ('subspace', 0.052), ('thinking', 0.052), ('pmi', 0.049), ('writers', 0.047), ('rh', 0.045), ('connection', 0.045), ('jj', 0.045), ('measures', 0.044), ('legal', 0.043), ('wi', 0.043), ('cognitive', 0.043), ('blue', 0.042), ('futile', 0.042), ('humor', 0.042), ('inspirational', 0.042), ('metaphoric', 0.042), ('purandare', 0.042), ('rumbell', 0.042), ('metaphor', 0.041), ('compositions', 0.041), ('quantifying', 0.039), ('uncommon', 0.039), ('pearson', 0.039), ('theories', 0.039), ('frequencies', 0.037), ('quiet', 0.037), ('fmri', 0.037), ('rentoumi', 0.037), ('phrases', 0.036), ('aspect', 0.036), ('perceived', 0.036), ('polarity', 0.035), ('vectors', 0.035), ('transformation', 0.035), ('semantic', 0.034), ('unconventional', 0.033), ('opinionfinder', 0.033), ('figurative', 0.033), ('strapparava', 0.033), ('brook', 0.033), ('interestingness', 0.033), ('stony', 0.033), ('operations', 0.033), ('hao', 0.032), ('seeing', 0.031), ('buckets', 0.029), ('red', 0.029), ('zhu', 0.029), ('investigation', 0.029), ('nn', 0.029), ('vector', 0.028), ('distributional', 0.028), ('abs', 0.028), ('appreciable', 0.028), ('disadvantageous', 0.028), ('empire', 0.028), ('faust', 0.028), ('fussell', 0.028), ('glycosides', 0.028), ('hendler', 0.028), ('imagination', 0.028), ('indefinitely', 0.028), ('krishnakumaran', 0.028), ('maybin', 0.028), ('mccurry', 0.028), ('memorable', 0.028), ('ozbal', 0.028), ('peace', 0.028), ('petal', 0.028), ('similes', 0.028), ('subspaces', 0.028), ('voegtlin', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="191-tfidf-1" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>Author: Polina Kuznetsova ; Jianfu Chen ; Yejin Choi</p><p>Abstract: Why do certain combinations of words such as “disadvantageous peace ” or “metal to the petal” appeal to our minds as interesting expressions with a sense of creativity, while other phrases such as “quiet teenager”, or “geometrical base ” not as much? We present statistical explorations to understand the characteristics of lexical compositions that give rise to the perception of being original, interesting, and at times even artistic. We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learning that give us further insights on how different aspects of lexical composition collectively contribute to the perceived creativity.</p><p>2 0.10597286 <a title="191-tfidf-2" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>Author: Angeliki Lazaridou ; Eva Maria Vecchi ; Marco Baroni</p><p>Abstract: In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the correct parsing of NPs (e.g., (live fish) transporter rather than live (fish transporter)). We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features.</p><p>3 0.10115843 <a title="191-tfidf-3" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>Author: Richard Socher ; Alex Perelygin ; Jean Wu ; Jason Chuang ; Christopher D. Manning ; Andrew Ng ; Christopher Potts</p><p>Abstract: Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.</p><p>4 0.099548489 <a title="191-tfidf-4" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>Author: Eva Maria Vecchi ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: In this study, we use compositional distributional semantic methods to investigate restrictions in adjective ordering. Specifically, we focus on properties distinguishing AdjectiveAdjective-Noun phrases in which there is flexibility in the adjective ordering from those bound to a rigid order. We explore a number of measures extracted from the distributional representation of AAN phrases which may indicate a word order restriction. We find that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modification of the adjectives. Our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics.</p><p>5 0.077531077 <a title="191-tfidf-5" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>Author: Masashi Tsubaki ; Kevin Duh ; Masashi Shimbo ; Yuji Matsumoto</p><p>Abstract: We present a novel vector space model for semantic co-compositionality. Inspired by Generative Lexicon Theory (Pustejovsky, 1995), our goal is a compositional model where both predicate and argument are allowed to modify each others’ meaning representations while generating the overall semantics. This readily addresses some major challenges with current vector space models, notably the polysemy issue and the use of one representation per word type. We implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations. We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality. The model achieves the best result to date (ρ = 0.47) on the semantic similarity task of transitive verbs (Grefenstette and Sadrzadeh, 2011).</p><p>6 0.072385803 <a title="191-tfidf-6" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>7 0.070647165 <a title="191-tfidf-7" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>8 0.06732548 <a title="191-tfidf-8" href="./emnlp-2013-Success_with_Style%3A_Using_Writing_Style_to_Predict_the_Success_of_Novels.html">178 emnlp-2013-Success with Style: Using Writing Style to Predict the Success of Novels</a></p>
<p>9 0.065489121 <a title="191-tfidf-9" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>10 0.06412939 <a title="191-tfidf-10" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>11 0.06139515 <a title="191-tfidf-11" href="./emnlp-2013-Sarcasm_as_Contrast_between_a_Positive_Sentiment_and_Negative_Situation.html">163 emnlp-2013-Sarcasm as Contrast between a Positive Sentiment and Negative Situation</a></p>
<p>12 0.058529813 <a title="191-tfidf-12" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<p>13 0.054169618 <a title="191-tfidf-13" href="./emnlp-2013-Sentiment_Analysis%3A_How_to_Derive_Prior_Polarities_from_SentiWordNet.html">170 emnlp-2013-Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet</a></p>
<p>14 0.051756345 <a title="191-tfidf-14" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>15 0.051184203 <a title="191-tfidf-15" href="./emnlp-2013-Learning_Distributions_over_Logical_Forms_for_Referring_Expression_Generation.html">119 emnlp-2013-Learning Distributions over Logical Forms for Referring Expression Generation</a></p>
<p>16 0.048972398 <a title="191-tfidf-16" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>17 0.048811045 <a title="191-tfidf-17" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>18 0.048766237 <a title="191-tfidf-18" href="./emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations.html">17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</a></p>
<p>19 0.0459208 <a title="191-tfidf-19" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>20 0.045620583 <a title="191-tfidf-20" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.153), (1, 0.035), (2, -0.096), (3, -0.076), (4, 0.038), (5, 0.109), (6, -0.017), (7, -0.1), (8, -0.088), (9, 0.029), (10, 0.009), (11, 0.058), (12, -0.051), (13, 0.037), (14, 0.009), (15, -0.053), (16, 0.067), (17, -0.085), (18, -0.033), (19, -0.035), (20, -0.011), (21, -0.007), (22, -0.072), (23, -0.033), (24, 0.053), (25, -0.024), (26, 0.006), (27, -0.041), (28, -0.092), (29, -0.136), (30, -0.041), (31, -0.088), (32, 0.036), (33, 0.094), (34, -0.067), (35, -0.009), (36, 0.121), (37, -0.082), (38, 0.034), (39, 0.024), (40, 0.044), (41, 0.065), (42, 0.039), (43, -0.012), (44, -0.007), (45, -0.014), (46, 0.03), (47, -0.013), (48, -0.013), (49, -0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91359252 <a title="191-lsi-1" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>Author: Polina Kuznetsova ; Jianfu Chen ; Yejin Choi</p><p>Abstract: Why do certain combinations of words such as “disadvantageous peace ” or “metal to the petal” appeal to our minds as interesting expressions with a sense of creativity, while other phrases such as “quiet teenager”, or “geometrical base ” not as much? We present statistical explorations to understand the characteristics of lexical compositions that give rise to the perception of being original, interesting, and at times even artistic. We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learning that give us further insights on how different aspects of lexical composition collectively contribute to the perceived creativity.</p><p>2 0.82622212 <a title="191-lsi-2" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>Author: Angeliki Lazaridou ; Eva Maria Vecchi ; Marco Baroni</p><p>Abstract: In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the correct parsing of NPs (e.g., (live fish) transporter rather than live (fish transporter)). We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features.</p><p>3 0.82012331 <a title="191-lsi-3" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>Author: Eva Maria Vecchi ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: In this study, we use compositional distributional semantic methods to investigate restrictions in adjective ordering. Specifically, we focus on properties distinguishing AdjectiveAdjective-Noun phrases in which there is flexibility in the adjective ordering from those bound to a rigid order. We explore a number of measures extracted from the distributional representation of AAN phrases which may indicate a word order restriction. We find that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modification of the adjectives. Our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics.</p><p>4 0.66178691 <a title="191-lsi-4" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<p>Author: Om P. Damani ; Shweta Ghonge</p><p>Abstract: Two recent measures incorporate the notion of statistical significance in basic PMI formulation. In some tasks, we find that the new measures perform worse than the PMI. Our analysis shows that while the basic ideas in incorporating statistical significance in PMI are reasonable, they have been applied slightly inappropriately. By fixing this, we get new measures that improve performance over not just PMI but on other popular co-occurrence measures as well. In fact, the revised measures perform reasonably well compared with more resource intensive non co-occurrence based methods also.</p><p>5 0.58242732 <a title="191-lsi-5" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>Author: Douwe Kiela ; Stephen Clark</p><p>Abstract: We present a novel unsupervised approach to detecting the compositionality of multi-word expressions. We compute the compositionality of a phrase through substituting the constituent words with their “neighbours” in a semantic vector space and averaging over the distance between the original phrase and the substituted neighbour phrases. Several methods of obtaining neighbours are presented. The results are compared to existing supervised results and achieve state-of-the-art performance on a verb-object dataset of human compositionality ratings.</p><p>6 0.56472844 <a title="191-lsi-6" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>7 0.55923176 <a title="191-lsi-7" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>8 0.43410945 <a title="191-lsi-8" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>9 0.4139027 <a title="191-lsi-9" href="./emnlp-2013-Using_Crowdsourcing_to_get_Representations_based_on_Regular_Expressions.html">196 emnlp-2013-Using Crowdsourcing to get Representations based on Regular Expressions</a></p>
<p>10 0.40101883 <a title="191-lsi-10" href="./emnlp-2013-Sarcasm_as_Contrast_between_a_Positive_Sentiment_and_Negative_Situation.html">163 emnlp-2013-Sarcasm as Contrast between a Positive Sentiment and Negative Situation</a></p>
<p>11 0.38622722 <a title="191-lsi-11" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>12 0.38197434 <a title="191-lsi-12" href="./emnlp-2013-Scaling_to_Large3_Data%3A_An_Efficient_and_Effective_Method_to_Compute_Distributional_Thesauri.html">165 emnlp-2013-Scaling to Large3 Data: An Efficient and Effective Method to Compute Distributional Thesauri</a></p>
<p>13 0.38165659 <a title="191-lsi-13" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>14 0.37112671 <a title="191-lsi-14" href="./emnlp-2013-Success_with_Style%3A_Using_Writing_Style_to_Predict_the_Success_of_Novels.html">178 emnlp-2013-Success with Style: Using Writing Style to Predict the Success of Novels</a></p>
<p>15 0.36096361 <a title="191-lsi-15" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>16 0.34435809 <a title="191-lsi-16" href="./emnlp-2013-Opinion_Mining_in_Newspaper_Articles_by_Entropy-Based_Word_Connections.html">144 emnlp-2013-Opinion Mining in Newspaper Articles by Entropy-Based Word Connections</a></p>
<p>17 0.33947653 <a title="191-lsi-17" href="./emnlp-2013-Sentiment_Analysis%3A_How_to_Derive_Prior_Polarities_from_SentiWordNet.html">170 emnlp-2013-Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet</a></p>
<p>18 0.32615763 <a title="191-lsi-18" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>19 0.32356313 <a title="191-lsi-19" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>20 0.32238555 <a title="191-lsi-20" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.032), (6, 0.045), (9, 0.01), (18, 0.029), (22, 0.042), (30, 0.068), (47, 0.012), (50, 0.012), (51, 0.178), (57, 0.305), (66, 0.041), (71, 0.036), (75, 0.02), (77, 0.011), (90, 0.018), (96, 0.03), (97, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84480715 <a title="191-lda-1" href="./emnlp-2013-Centering_Similarity_Measures_to_Reduce_Hubs.html">44 emnlp-2013-Centering Similarity Measures to Reduce Hubs</a></p>
<p>Author: Ikumi Suzuki ; Kazuo Hara ; Masashi Shimbo ; Marco Saerens ; Kenji Fukumizu</p><p>Abstract: The performance of nearest neighbor methods is degraded by the presence of hubs, i.e., objects in the dataset that are similar to many other objects. In this paper, we show that the classical method of centering, the transformation that shifts the origin of the space to the data centroid, provides an effective way to reduce hubs. We show analytically why hubs emerge and why they are suppressed by centering, under a simple probabilistic model of data. To further reduce hubs, we also move the origin more aggressively towards hubs, through weighted centering. Our experimental results show that (weighted) centering is effective for natural language data; it improves the performance of the k-nearest neighbor classi- fiers considerably in word sense disambiguation and document classification tasks.</p><p>same-paper 2 0.77871788 <a title="191-lda-2" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>Author: Polina Kuznetsova ; Jianfu Chen ; Yejin Choi</p><p>Abstract: Why do certain combinations of words such as “disadvantageous peace ” or “metal to the petal” appeal to our minds as interesting expressions with a sense of creativity, while other phrases such as “quiet teenager”, or “geometrical base ” not as much? We present statistical explorations to understand the characteristics of lexical compositions that give rise to the perception of being original, interesting, and at times even artistic. We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learning that give us further insights on how different aspects of lexical composition collectively contribute to the perceived creativity.</p><p>3 0.56356442 <a title="191-lda-3" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>Author: Richard Socher ; Alex Perelygin ; Jean Wu ; Jason Chuang ; Christopher D. Manning ; Andrew Ng ; Christopher Potts</p><p>Abstract: Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.</p><p>4 0.55816561 <a title="191-lda-4" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>Author: Masashi Tsubaki ; Kevin Duh ; Masashi Shimbo ; Yuji Matsumoto</p><p>Abstract: We present a novel vector space model for semantic co-compositionality. Inspired by Generative Lexicon Theory (Pustejovsky, 1995), our goal is a compositional model where both predicate and argument are allowed to modify each others’ meaning representations while generating the overall semantics. This readily addresses some major challenges with current vector space models, notably the polysemy issue and the use of one representation per word type. We implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations. We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality. The model achieves the best result to date (ρ = 0.47) on the semantic similarity task of transitive verbs (Grefenstette and Sadrzadeh, 2011).</p><p>5 0.55242646 <a title="191-lda-5" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>Author: Dimitri Kartsaklis ; Mehrnoosh Sadrzadeh</p><p>Abstract: Recent work has shown that compositionaldistributional models using element-wise operations on contextual word vectors benefit from the introduction of a prior disambiguation step. The purpose of this paper is to generalise these ideas to tensor-based models, where relational words such as verbs and adjectives are represented by linear maps (higher order tensors) acting on a number of arguments (vectors). We propose disambiguation algorithms for a number of tensor-based models, which we then test on a variety of tasks. The results show that disambiguation can provide better compositional representation even for the case of tensor-based models. Further- more, we confirm previous findings regarding the positive effect of disambiguation on vector mixture models, and we compare the effectiveness of the two approaches.</p><p>6 0.5498572 <a title="191-lda-6" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>7 0.54966766 <a title="191-lda-7" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>8 0.54865295 <a title="191-lda-8" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>9 0.548078 <a title="191-lda-9" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>10 0.5476234 <a title="191-lda-10" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>11 0.54728895 <a title="191-lda-11" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<p>12 0.54583991 <a title="191-lda-12" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>13 0.54550749 <a title="191-lda-13" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>14 0.54547375 <a title="191-lda-14" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>15 0.54504073 <a title="191-lda-15" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>16 0.54424989 <a title="191-lda-16" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>17 0.54391325 <a title="191-lda-17" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>18 0.5436464 <a title="191-lda-18" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>19 0.54327506 <a title="191-lda-19" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>20 0.54287273 <a title="191-lda-20" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
