<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>30 emnlp-2013-Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-30" href="#">emnlp2013-30</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>30 emnlp-2013-Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora</h1>
<br/><p>Source: <a title="emnlp-2013-30-pdf" href="http://aclweb.org/anthology//D/D13/D13-1105.pdf">pdf</a></p><p>Author: Ramy Eskander ; Nizar Habash ; Owen Rambow</p><p>Abstract: We present a method for automatically learning inflectional classes and associated lemmas from morphologically annotated corpora. The method consists of a core languageindependent algorithm, which can be optimized for specific languages. The method is demonstrated on Egyptian Arabic and German, two morphologically rich languages. Our best method for Egyptian Arabic provides an error reduction of 55.6% over a simple baseline; our best method for German achieves a 66.7% error reduction.</p><p>Reference: <a title="emnlp-2013-30-reference" href="../emnlp2013_reference/emnlp-2013-Automatic_Extraction_of_Morphological_Lexicons_from_Morphologically_Annotated_Corpora_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We present a method for automatically learning inflectional classes and associated lemmas from morphologically annotated corpora. [sent-3, score-0.471]
</p><p>2 For example, a morphological lexicon is an important component of a morphological tagger  or of a part-of-speech (POS) tagger for languages with rich morphology. [sent-10, score-0.488]
</p><p>3 1 Traditionally, a morphological lexicon has been created through painstaking lexicographic and morphological analysis of the language, drawing on unannotated corpora. [sent-11, score-0.515]
</p><p>4 Fully or largely unsupervised approaches cannot link surface forms to morphosyntactic features and are thus not suited for building morphological lexicons. [sent-13, score-0.414]
</p><p>5 While there has been much work on computational morphology, to our knowledge this is the first paper to study the question of how to extract morphological lexicons from morphologically annotated corpora, and how to determine how much annotation is needed. [sent-18, score-0.315]
</p><p>6 In  this paper, we assume a corpus with each word annotated with morphosyntactic features and with a lemma which tells us what lexeme the word form is part of. [sent-19, score-0.492]
</p><p>7 The task is to predict the correspondence between a word form and its lemma and morphological features. [sent-20, score-0.402]
</p><p>8 It incrementally merges complementary paradigm information about different lexemes into more abstract and more informative inflectional classes. [sent-24, score-0.3]
</p><p>9 By adding language-specific modeling of the stem using templates, we obtain further  error reductions for EGY (up to 55. [sent-32, score-0.441]
</p><p>10 We present our basic language-independent method in Section 4, and our language-specific modeling of stem variation in Section 5. [sent-35, score-0.375]
</p><p>11 The work presented in this paper falls in the middle of this continuum: we are interested in learning complete morphological models using rich morphological annotations and, optionally, limited linguistic knowledge. [sent-45, score-0.484]
</p><p>12 Morphological Paradigms Many traditional and modern theories of inflectional morphology organize natural language morphology by paradigms (Stump, 2001 ; Walther, 2011; Camilleri, 2011). [sent-47, score-0.695]
</p><p>13 Within the continuum we discussed above, we find hierarchical representations of paradigm knowledge that have been used in manually constructed morphological models (Finkel and Stump, 2002; Habash et al. [sent-48, score-0.344]
</p><p>14 Furthermore, D ´etrez and Ranta (2012) introduce an implementation of Smart Paradigms heuristically organized paradigms minimizing the number of forms needed to predict the full paradigm –  of a particular lexeme. [sent-50, score-0.298]
</p><p>15 (2004) describe methods for automatically populating a lexicon from raw data given a set of morphological inflectional classes in a language. [sent-53, score-0.516]
</p><p>16 Our work differs in that we use annotated data, but do not start with a complete set of inflectional classes; thus, our work is exactly complementary to this work. [sent-54, score-0.278]
</p><p>17 (2002) (and later on Can and Manandhar (2012)) define a paradigm as “a set of suffixes and the stems that attach to those suffixes and no others”. [sent-57, score-0.289]
</p><p>18 Chan (2006) defines a simpler concept of paradigms in his probabilistic paradigm model, which has many limitations, such as not handling syncretism or irregular morphology, nor distinguishing inflection and derivation. [sent-59, score-0.37]
</p><p>19 Dreyer and Eisner (201 1) learn complete German verb paradigms from a small set of complete  seed paradigms (50 or 100), which they choose randomly from all verbs in the language. [sent-60, score-0.464]
</p><p>20 They model stem changes using letter-based models, and use a large unannotated corpus in addition to the seed paradigms. [sent-61, score-0.434]
</p><p>21 Instead of using unannotated text, they model explicit rules for affixes and stem changes. [sent-63, score-0.51]
</p><p>22 In this paper we target the learning and completion of inflectional classes from  morphologically annotated data. [sent-72, score-0.343]
</p><p>23 Our approach does not sacrifice details of what paradigms should include: we handle syncretism and stem changes, and allow for the prediction of new word forms from morphosyntactic features and lemmas, unlike the largely unsupervised work. [sent-73, score-0.813]
</p><p>24 Our work also differs from most previous work in that we investigate how to model stem change explicitly. [sent-74, score-0.403]
</p><p>25 Whereas other approaches model stem syncretism through letterbased models (Yarowsky and Wicentowski, 2000; Neuvel and Fulop, 2002; Dreyer and Eisner, 2011), we explore the use of abstract stems. [sent-75, score-0.487]
</p><p>26 In our previous work on the EGY morphological analyzer CALIMA, we similarly used a lexicon of annotated morphological forms and extended it automatically using a simpler approach to paradigm completion (Habash et al. [sent-76, score-0.714]
</p><p>27 There are two types of morphological processes: inflectional and derivational morphology. [sent-81, score-0.454]
</p><p>28 In inflectional morphology, a core meaning is retained and different word forms reflect different types of morphosyntactic features such as person, number, or tense. [sent-82, score-0.425]
</p><p>29 We will refer to the set of all word forms that are related through inflectional morphology alone as lexeme. [sent-86, score-0.452]
</p><p>30 A paradigm of a lexeme is a list of cells, where a cell is a combination of a complete set of morphosyntactic features (properties) and the corresponding inflected form of the lexeme. [sent-90, score-0.554]
</p><p>31 A paradigm is com-  plete if there are cells for all possible morphosyntactic features (the list of possible morphosyntactic features is of course language-dependent). [sent-91, score-0.517]
</p><p>32 We can abstract from paradigms by grouping together paradigms which share the same affixes in corresponding cells, and where stems in corresponding cells differ in some restricted manner. [sent-98, score-0.664]
</p><p>33 We can define the inflectional class (IC) more formally as a set of abstract cells, where an abstract cell is a combination of a complete set of morphosyntactic features (properties) and an abstract representation of a stem (an abstract stem) along with fully specified affixes. [sent-99, score-0.788]
</p><p>34 The abstract stems (and thus the abstract cells) must have the property that, given a single instantiated word form of a lexeme along with its associated IC, we can derive the complete paradigm of the lexeme deterministically. [sent-100, score-0.598]
</p><p>35 In the first results  we present, we simply assume that the stem is either shared entirely with other abstract cells, or it is entirely lexically instantiated. [sent-101, score-0.375]
</p><p>36 We explore languagespecific approaches to defining an abstract stem in Section 5, where we also discuss relevant morphological facts of EGY and GER. [sent-102, score-0.596]
</p><p>37 Prefixes, suffixes, and stems can be the same for different cells of a single paradigm or IC. [sent-103, score-0.386]
</p><p>38 We will refer to the cells which share a stem as a stem syncretism zone or “zone” for short. [sent-105, score-1.086]
</p><p>39 The following is an input example specifying the inflected form of the person plural imperfective inflection for the EGY lemma ‘write’ : hy+iktib+uwA, katab, I3UPi. [sent-110, score-0.301]
</p><p>40 Since the lemma itself is an inflected form with an a priori fixed feature combination, we also use the lemma to construct the initial IC, even if it did not occur as a word form in the corpus. [sent-120, score-0.456]
</p><p>41 If all inflected forms of a lemma appear in the training data, the IC will be complete. [sent-121, score-0.344]
</p><p>42 If all seen forms related to one lexeme have the same stem, the abstract stem chosen for the IC is a single stem variable; the abstract cells of the new IC can of course differ in terms of affixes (which are always fully specified rather than represented as variables). [sent-123, score-1.255]
</p><p>43 Two ICs are mergeable if neither of the ICs is suppletive, if they share at least one abstract cell for some morphological feature, and if no morphological features are associated with different abstract cells, i. [sent-132, score-0.518]
</p><p>44 3 Completing Stems and Affixes Soft Stem Syncretism Zones We extend the concept of stem-syncretism zones into a statistical model that computes the probability that the abstract stems in two abstract cells are the same given their feature combinations, i. [sent-154, score-0.413]
</p><p>45 The probabilities are computed for each feature-combination pair as the ratio of the times the abstract stem for the feature-combination pair are equal divided by the times the abstract stem for the feature-combination pair are not empty. [sent-158, score-0.75]
</p><p>46 When applying the soft zones to determine the abstract stems of empty cells, we consider all filled cells in the same IC and select the abstract stem from the cell of the feature combination that has the highest soft-zone probability with the feature combination of the empty cell. [sent-164, score-0.869]
</p><p>47 Note that the copied abstract stem can be either a stem variable, or, for a suppletive IC, a lexical form. [sent-165, score-0.827]
</p><p>48 4 Model Application The complete ICs produced by the completion algorithm, associated with their lemmas in the lexicon, can now be used to predict the surface form of a given lemma and morphological features. [sent-171, score-0.572]
</p><p>49 This may require instantiating a stem variable (in case the IC is not suppletive), but this can be done deterministically from the lemma. [sent-174, score-0.375]
</p><p>50 If such an IC does not exist, a backup mode returns the stem of the lemma associated with the most frequent affixes of the queried features. [sent-180, score-0.697]
</p><p>51 The lemma we use is the Arabic citation form for verbs, which is the perfective third person masculine sin4The corpus was automatically annotated using information from the CHE transcripts (Gadalla et al. [sent-193, score-0.316]
</p><p>52 We evaluate the accuracy of the automatically generated bidirectional lexicon by generating surface forms from lemmas and morphosyntactic features. [sent-201, score-0.334]
</p><p>53 All cases of citation form features (in the case of EGY, P3MS) return the lemma as the inflected form. [sent-206, score-0.308]
</p><p>54 Otherwise, we look up the lemma and feature pair among all triplets in the training data and return the inflected form if such a triplet was found. [sent-207, score-0.313]
</p><p>55 Results The results on tokens are summarized in Table 2 under the column heading BL (baseline) and NOTMP (LICA with no stem template). [sent-209, score-0.375]
</p><p>56 Additional 35% of the cases are due to stem templates that are unseen in the complete ICs although their corresponding lemmas are seen. [sent-213, score-0.559]
</p><p>57 We disregard any verbs with separable prefixes (as is common in work in morphology learning). [sent-221, score-0.299]
</p><p>58 1 General Approach We model the abstract stems using the notions of orthographic template and orthographic root. [sent-234, score-0.416]
</p><p>59 We define them simply in terms of sets of letters: the set of letters of the alphabet used to write the language we are modeling is partitioned into the root letters and the pattern letters. [sent-236, score-0.582]
</p><p>60 The orthographic template is a string that specifies the template letters and that has placeholders for the root letters, which we write as ‘2’ . [sent-237, score-0.576]
</p><p>61 The orthographic root is a sequence of strings that specifies the root letters that can fill a vocalic template’s placeholders, in the order specified. [sent-238, score-0.517]
</p><p>62 This gives us another way to specify a lexeme: since a com-  plete paradigm enumerates all inflected forms of a lexeme, and since a complete IC along with a root defines a complete paradigm, we can specify a lexeme to be a pair consisting of an IC along with a root. [sent-240, score-0.649]
</p><p>63 root of the lemma is used to decompose the stem of the inflected form into a root and a template. [sent-244, score-0.848]
</p><p>64 2 Choosing Root and Pattern Letters Determining which letters count as root letters and which count as pattern letters is language-specific. [sent-246, score-0.791]
</p><p>65 No Template (NOTMP) In this approach, we do not model the stem at all, i. [sent-248, score-0.375]
</p><p>66 As a result, the ICs cannot express stem changes; any verb with a stem change will be an irregular verb and it will be modeled with a suppletive IC. [sent-251, score-0.954]
</p><p>67 Note that in EGY, every verb manifests a stem change between the perfective and imperfective forms, while in GER many verbs in fact do have the same stem for all inflected forms. [sent-252, score-1.051]
</p><p>68 These are letters 1038  which we know can change in stems within the same IC. [sent-255, score-0.354]
</p><p>69 We align the letters of all the stems in the development corpus with the stem letters of their corresponding lemmas. [sent-262, score-0.91]
</p><p>70 This allows us to identify which letters change between lemma stem and inflected stem. [sent-263, score-0.887]
</p><p>71 We order the letters by the probability that a letter changes given all occurrences of that letter, and by the probability that a letter changes given all changes that occur. [sent-264, score-0.425]
</p><p>72 For both rankings and for each letter that changes, we construct a set of letters by including all letters from the most highly ranked letter down to the letter under consideration. [sent-266, score-0.598]
</p><p>73 o-called weak root radicals, Hamzated forms and diacritics that are often used to discuss different verbal paradigms in Arabic (Gadalla, 2000). [sent-297, score-0.349]
</p><p>74 We follow the standard practice in modeling Arabic morphology of assuming that each placeholder in an orthographic template corresponds to exactly one letter from the orthographic root (i. [sent-299, score-0.637]
</p><p>75 To inflect a lemma for a particular set of features, we combine the root with the 2 slots in the corresponding inflectional class feature row, e. [sent-305, score-0.484]
</p><p>76 We also see examples of stem syncretism zones in Table 1; they are marked with horizontal lines. [sent-310, score-0.61]
</p><p>77 Results The template approaches SCHLR and EMPR consistently beat NOTMP which does not make use of any templatic stem modeling (see Table 2). [sent-341, score-0.454]
</p><p>78 Additionally, 71% of the cases in NOTMP where the stem template is unseen and the lemma is seen are solved. [sent-346, score-0.682]
</p><p>79 However, 7% of the error  1039 types in EMPR are not present in NOTMP, and they are all cases where the stem template is unseen while the lemma is seen. [sent-347, score-0.721]
</p><p>80 Errors due to multiple stem forms and gold errors remain the same in both NOTMP and EMPR, contributing to 26% and 23%, respectively, of all the error types in EMPR. [sent-348, score-0.483]
</p><p>81 , they have all the template cells for all morphosyntactic features specified. [sent-355, score-0.376]
</p><p>82 Like Arabic, German verb paradigms can show stem changes which are typically vowel changes. [sent-390, score-0.611]
</p><p>83 However, unlike Arabic, German has many verbs (called “weak verbs”) which are regular in the sense that they show no stem change at all. [sent-392, score-0.482]
</p><p>84 The irregular verbs, or “strong verbs”, show many different patterns of stem changes. [sent-393, score-0.404]
</p><p>85 In particular, the weak verbs form several inflectional classes (which, of course, differ only in affixes). [sent-395, score-0.376]
</p><p>86 The different blocks in the IC table specify different stem syncretism zones. [sent-397, score-0.513]
</p><p>87 of sequences of single letters: the strong verbs have monosyllabic stems (plus perhaps derivational morphology), with the vowel in this stem potentially undergoing changes. [sent-398, score-0.664]
</p><p>88 However, the onset and coda of the stem syllable can be any consonant cluster allowed by German phonology. [sent-399, score-0.375]
</p><p>89 Thus, in German, we model roots as pairs of strings of any length (which represent the onset and coda of the stem syllable). [sent-400, score-0.375]
</p><p>90 Since German strong verbs can have any stem vowel, we assume that all eight vowel letters of German (aeiou¨ a o¨ u¨) are pattern letters, and all other consonant letters are root letters. [sent-402, score-1.1]
</p><p>91 German weak verbs show no stem changes at all; if we tailored our templates to them, we would define all letters to be root letters, and there would be no pattern letters at all. [sent-403, score-1.116]
</p><p>92 However, since we do not know during training time whether a verb is weak or strong, all verbs will be modeled with an orthographic template, even though there is no stem change at all. [sent-406, score-0.675]
</p><p>93 The EMPR pattern letters differ from the SCHLR pattern letters by omitting five vowels, but including the ß variant of the s. [sent-409, score-0.548]
</p><p>94 6  Conclusion and Future Work  We presented a method for automatically learning inflectional classes and associated lemmas from morphologically annotated corpora. [sent-441, score-0.471]
</p><p>95 In the future, we plan to improve several aspects of our models, in particular, using more powerful languageindependent template transformations to automatically optimize for stem and affix modeling. [sent-442, score-0.485]
</p><p>96 We will explore ideas from unsupervised morphology learning to minimize the need for morphological annotations. [sent-447, score-0.4]
</p><p>97 Island morphology: Morphology’s interactions in the study of stem patterns. [sent-463, score-0.375]
</p><p>98 Learning probabilistic paradigms for morphology in a latent class model. [sent-472, score-0.312]
</p><p>99 Smart paradigms and the predictability and complexity of inflectional morphology. [sent-488, score-0.337]
</p><p>100 Discovering morphological paradigms from plain text using a dirichlet process mixture model. [sent-492, score-0.354]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stem', 0.375), ('egy', 0.245), ('ics', 0.243), ('morphological', 0.221), ('letters', 0.209), ('inflectional', 0.204), ('lemma', 0.181), ('notmp', 0.18), ('morphology', 0.179), ('cells', 0.173), ('arabic', 0.16), ('lexeme', 0.155), ('schlr', 0.142), ('paradigms', 0.133), ('empr', 0.129), ('morphosyntactic', 0.124), ('zones', 0.123), ('stems', 0.117), ('habash', 0.117), ('syncretism', 0.112), ('orthographic', 0.11), ('affixes', 0.108), ('egyptian', 0.102), ('root', 0.099), ('paradigm', 0.096), ('lemmas', 0.095), ('inflected', 0.094), ('ic', 0.089), ('nizar', 0.082), ('ger', 0.081), ('verbs', 0.079), ('template', 0.079), ('gadalla', 0.077), ('iics', 0.077), ('suppletive', 0.077), ('german', 0.072), ('forms', 0.069), ('durrett', 0.065), ('pattern', 0.065), ('morphologically', 0.062), ('letter', 0.06), ('eskander', 0.052), ('hzs', 0.052), ('dreyer', 0.051), ('zone', 0.051), ('blind', 0.049), ('weak', 0.048), ('unseen', 0.047), ('denero', 0.047), ('lexicon', 0.046), ('classes', 0.045), ('semitic', 0.045), ('cell', 0.043), ('complete', 0.042), ('prefixes', 0.041), ('error', 0.039), ('etrez', 0.039), ('forsberg', 0.039), ('kilany', 0.039), ('lica', 0.039), ('neuvel', 0.039), ('perfective', 0.039), ('ramy', 0.039), ('triplets', 0.038), ('suffixes', 0.038), ('soft', 0.038), ('vowel', 0.036), ('merge', 0.036), ('verb', 0.035), ('combinations', 0.034), ('associated', 0.033), ('citation', 0.033), ('changes', 0.032), ('graph', 0.032), ('annotated', 0.032), ('affix', 0.031), ('merging', 0.031), ('stump', 0.031), ('masculine', 0.031), ('yarowsky', 0.03), ('analyzer', 0.029), ('irregular', 0.029), ('derivational', 0.029), ('hammarstr', 0.029), ('strong', 0.028), ('core', 0.028), ('change', 0.028), ('rambow', 0.027), ('continuum', 0.027), ('infinitive', 0.027), ('reductions', 0.027), ('unannotated', 0.027), ('specify', 0.026), ('aarne', 0.026), ('arram', 0.026), ('callhome', 0.026), ('fulop', 0.026), ('hamzated', 0.026), ('imperfective', 0.026), ('katab', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999881 <a title="30-tfidf-1" href="./emnlp-2013-Automatic_Extraction_of_Morphological_Lexicons_from_Morphologically_Annotated_Corpora.html">30 emnlp-2013-Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora</a></p>
<p>Author: Ramy Eskander ; Nizar Habash ; Owen Rambow</p><p>Abstract: We present a method for automatically learning inflectional classes and associated lemmas from morphologically annotated corpora. The method consists of a core languageindependent algorithm, which can be optimized for specific languages. The method is demonstrated on Egyptian Arabic and German, two morphologically rich languages. Our best method for Egyptian Arabic provides an error reduction of 55.6% over a simple baseline; our best method for German achieves a 66.7% error reduction.</p><p>2 0.33682936 <a title="30-tfidf-2" href="./emnlp-2013-Adaptor_Grammars_for_Learning_Non-Concatenative_Morphology.html">19 emnlp-2013-Adaptor Grammars for Learning Non-Concatenative Morphology</a></p>
<p>Author: Jan A. Botha ; Phil Blunsom</p><p>Abstract: This paper contributes an approach for expressing non-concatenative morphological phenomena, such as stem derivation in Semitic languages, in terms of a mildly context-sensitive grammar formalism. This offers a convenient level of modelling abstraction while remaining computationally tractable. The nonparametric Bayesian framework of adaptor grammars is extended to this richer grammar formalism to propose a probabilistic model that can learn word segmentation and morpheme lexicons, including ones with discontiguous strings as elements, from unannotated data. Our experiments on Hebrew and three variants of Arabic data find that the additional expressiveness to capture roots and templates as atomic units improves the quality of concatenative segmentation and stem identification. We obtain 74% accuracy in identifying triliteral Hebrew roots, while performing morphological segmentation with an F1-score of 78. 1.</p><p>3 0.31054062 <a title="30-tfidf-3" href="./emnlp-2013-Translating_into_Morphologically_Rich_Languages_with_Synthetic_Phrases.html">186 emnlp-2013-Translating into Morphologically Rich Languages with Synthetic Phrases</a></p>
<p>Author: Victor Chahuneau ; Eva Schlinger ; Noah A. Smith ; Chris Dyer</p><p>Abstract: Translation into morphologically rich languages is an important but recalcitrant problem in MT. We present a simple and effective approach that deals with the problem in two phases. First, a discriminative model is learned to predict inflections of target words from rich source-side annotations. Then, this model is used to create additional sentencespecific word- and phrase-level translations that are added to a standard translation model as “synthetic” phrases. Our approach relies on morphological analysis of the target language, but we show that an unsupervised Bayesian model of morphology can successfully be used in place of a supervised analyzer. We report significant improvements in translation quality when translating from English to Russian, Hebrew and Swahili.</p><p>4 0.26360014 <a title="30-tfidf-4" href="./emnlp-2013-The_Effects_of_Syntactic_Features_in_Automatic_Prediction_of_Morphology.html">181 emnlp-2013-The Effects of Syntactic Features in Automatic Prediction of Morphology</a></p>
<p>Author: Wolfgang Seeker ; Jonas Kuhn</p><p>Abstract: Morphology and syntax interact considerably in many languages and language processing should pay attention to these interdependencies. We analyze the effect of syntactic features when used in automatic morphology prediction on four typologically different languages. We show that predicting morphology for languages with highly ambiguous word forms profits from taking the syntactic context of words into account and results in state-ofthe-art models.</p><p>5 0.23457213 <a title="30-tfidf-5" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>Author: Stella Frank ; Frank Keller ; Sharon Goldwater</p><p>Abstract: Frank Keller keller@ inf .ed .ac .uk Sharon Goldwater sgwater@ inf .ed .ac .uk ILCC, School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK interactions are often (but not necessarily) synergisChildren learn various levels of linguistic structure concurrently, yet most existing models of language acquisition deal with only a single level of structure, implicitly assuming a sequential learning process. Developing models that learn multiple levels simultaneously can provide important insights into how these levels might interact synergistically dur- ing learning. Here, we present a model that jointly induces syntactic categories and morphological segmentations by combining two well-known models for the individual tasks. We test on child-directed utterances in English and Spanish and compare to single-task baselines. In the morphologically poorer language (English), the model improves morphological segmentation, while in the morphologically richer language (Spanish), it leads to better syntactic categorization. These results provide further evidence that joint learning is useful, but also suggest that the benefits may be different for typologically different languages.</p><p>6 0.11701733 <a title="30-tfidf-6" href="./emnlp-2013-Efficient_Higher-Order_CRFs_for_Morphological_Tagging.html">70 emnlp-2013-Efficient Higher-Order CRFs for Morphological Tagging</a></p>
<p>7 0.070287414 <a title="30-tfidf-7" href="./emnlp-2013-Russian_Stress_Prediction_using_Maximum_Entropy_Ranking.html">162 emnlp-2013-Russian Stress Prediction using Maximum Entropy Ranking</a></p>
<p>8 0.058966916 <a title="30-tfidf-8" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>9 0.051179405 <a title="30-tfidf-9" href="./emnlp-2013-Identifying_Phrasal_Verbs_Using_Many_Bilingual_Corpora.html">96 emnlp-2013-Identifying Phrasal Verbs Using Many Bilingual Corpora</a></p>
<p>10 0.047204167 <a title="30-tfidf-10" href="./emnlp-2013-Gender_Inference_of_Twitter_Users_in_Non-English_Contexts.html">89 emnlp-2013-Gender Inference of Twitter Users in Non-English Contexts</a></p>
<p>11 0.045239698 <a title="30-tfidf-11" href="./emnlp-2013-Assembling_the_Kazakh_Language_Corpus.html">26 emnlp-2013-Assembling the Kazakh Language Corpus</a></p>
<p>12 0.043706786 <a title="30-tfidf-12" href="./emnlp-2013-Scaling_Semantic_Parsers_with_On-the-Fly_Ontology_Matching.html">164 emnlp-2013-Scaling Semantic Parsers with On-the-Fly Ontology Matching</a></p>
<p>13 0.043436728 <a title="30-tfidf-13" href="./emnlp-2013-Overcoming_the_Lack_of_Parallel_Data_in_Sentence_Compression.html">149 emnlp-2013-Overcoming the Lack of Parallel Data in Sentence Compression</a></p>
<p>14 0.042045038 <a title="30-tfidf-14" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>15 0.042025212 <a title="30-tfidf-15" href="./emnlp-2013-A_Log-Linear_Model_for_Unsupervised_Text_Normalization.html">9 emnlp-2013-A Log-Linear Model for Unsupervised Text Normalization</a></p>
<p>16 0.041846327 <a title="30-tfidf-16" href="./emnlp-2013-Sentiment_Analysis%3A_How_to_Derive_Prior_Polarities_from_SentiWordNet.html">170 emnlp-2013-Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet</a></p>
<p>17 0.039811455 <a title="30-tfidf-17" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>18 0.03963602 <a title="30-tfidf-18" href="./emnlp-2013-Joint_Learning_of_Phonetic_Units_and_Word_Pronunciations_for_ASR.html">115 emnlp-2013-Joint Learning of Phonetic Units and Word Pronunciations for ASR</a></p>
<p>19 0.038055155 <a title="30-tfidf-19" href="./emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model.html">75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</a></p>
<p>20 0.037630253 <a title="30-tfidf-20" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.166), (1, -0.061), (2, -0.008), (3, -0.143), (4, -0.448), (5, -0.138), (6, -0.189), (7, -0.128), (8, 0.099), (9, -0.184), (10, 0.034), (11, -0.005), (12, -0.038), (13, -0.06), (14, -0.01), (15, -0.067), (16, 0.046), (17, 0.007), (18, -0.035), (19, -0.004), (20, -0.036), (21, -0.074), (22, -0.08), (23, -0.09), (24, -0.09), (25, 0.02), (26, 0.082), (27, 0.003), (28, -0.005), (29, -0.026), (30, 0.014), (31, 0.044), (32, -0.07), (33, 0.034), (34, 0.02), (35, 0.103), (36, -0.031), (37, -0.078), (38, 0.001), (39, -0.088), (40, -0.004), (41, -0.011), (42, 0.028), (43, -0.021), (44, 0.0), (45, -0.076), (46, -0.1), (47, -0.001), (48, -0.072), (49, 0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96319407 <a title="30-lsi-1" href="./emnlp-2013-Automatic_Extraction_of_Morphological_Lexicons_from_Morphologically_Annotated_Corpora.html">30 emnlp-2013-Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora</a></p>
<p>Author: Ramy Eskander ; Nizar Habash ; Owen Rambow</p><p>Abstract: We present a method for automatically learning inflectional classes and associated lemmas from morphologically annotated corpora. The method consists of a core languageindependent algorithm, which can be optimized for specific languages. The method is demonstrated on Egyptian Arabic and German, two morphologically rich languages. Our best method for Egyptian Arabic provides an error reduction of 55.6% over a simple baseline; our best method for German achieves a 66.7% error reduction.</p><p>2 0.82464123 <a title="30-lsi-2" href="./emnlp-2013-Adaptor_Grammars_for_Learning_Non-Concatenative_Morphology.html">19 emnlp-2013-Adaptor Grammars for Learning Non-Concatenative Morphology</a></p>
<p>Author: Jan A. Botha ; Phil Blunsom</p><p>Abstract: This paper contributes an approach for expressing non-concatenative morphological phenomena, such as stem derivation in Semitic languages, in terms of a mildly context-sensitive grammar formalism. This offers a convenient level of modelling abstraction while remaining computationally tractable. The nonparametric Bayesian framework of adaptor grammars is extended to this richer grammar formalism to propose a probabilistic model that can learn word segmentation and morpheme lexicons, including ones with discontiguous strings as elements, from unannotated data. Our experiments on Hebrew and three variants of Arabic data find that the additional expressiveness to capture roots and templates as atomic units improves the quality of concatenative segmentation and stem identification. We obtain 74% accuracy in identifying triliteral Hebrew roots, while performing morphological segmentation with an F1-score of 78. 1.</p><p>3 0.78295457 <a title="30-lsi-3" href="./emnlp-2013-Translating_into_Morphologically_Rich_Languages_with_Synthetic_Phrases.html">186 emnlp-2013-Translating into Morphologically Rich Languages with Synthetic Phrases</a></p>
<p>Author: Victor Chahuneau ; Eva Schlinger ; Noah A. Smith ; Chris Dyer</p><p>Abstract: Translation into morphologically rich languages is an important but recalcitrant problem in MT. We present a simple and effective approach that deals with the problem in two phases. First, a discriminative model is learned to predict inflections of target words from rich source-side annotations. Then, this model is used to create additional sentencespecific word- and phrase-level translations that are added to a standard translation model as “synthetic” phrases. Our approach relies on morphological analysis of the target language, but we show that an unsupervised Bayesian model of morphology can successfully be used in place of a supervised analyzer. We report significant improvements in translation quality when translating from English to Russian, Hebrew and Swahili.</p><p>4 0.74257833 <a title="30-lsi-4" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>Author: Stella Frank ; Frank Keller ; Sharon Goldwater</p><p>Abstract: Frank Keller keller@ inf .ed .ac .uk Sharon Goldwater sgwater@ inf .ed .ac .uk ILCC, School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK interactions are often (but not necessarily) synergisChildren learn various levels of linguistic structure concurrently, yet most existing models of language acquisition deal with only a single level of structure, implicitly assuming a sequential learning process. Developing models that learn multiple levels simultaneously can provide important insights into how these levels might interact synergistically dur- ing learning. Here, we present a model that jointly induces syntactic categories and morphological segmentations by combining two well-known models for the individual tasks. We test on child-directed utterances in English and Spanish and compare to single-task baselines. In the morphologically poorer language (English), the model improves morphological segmentation, while in the morphologically richer language (Spanish), it leads to better syntactic categorization. These results provide further evidence that joint learning is useful, but also suggest that the benefits may be different for typologically different languages.</p><p>5 0.66604239 <a title="30-lsi-5" href="./emnlp-2013-The_Effects_of_Syntactic_Features_in_Automatic_Prediction_of_Morphology.html">181 emnlp-2013-The Effects of Syntactic Features in Automatic Prediction of Morphology</a></p>
<p>Author: Wolfgang Seeker ; Jonas Kuhn</p><p>Abstract: Morphology and syntax interact considerably in many languages and language processing should pay attention to these interdependencies. We analyze the effect of syntactic features when used in automatic morphology prediction on four typologically different languages. We show that predicting morphology for languages with highly ambiguous word forms profits from taking the syntactic context of words into account and results in state-ofthe-art models.</p><p>6 0.32524437 <a title="30-lsi-6" href="./emnlp-2013-Efficient_Higher-Order_CRFs_for_Morphological_Tagging.html">70 emnlp-2013-Efficient Higher-Order CRFs for Morphological Tagging</a></p>
<p>7 0.26390952 <a title="30-lsi-7" href="./emnlp-2013-Unsupervised_Spectral_Learning_of_WCFG_as_Low-rank_Matrix_Completion.html">195 emnlp-2013-Unsupervised Spectral Learning of WCFG as Low-rank Matrix Completion</a></p>
<p>8 0.26154789 <a title="30-lsi-8" href="./emnlp-2013-Assembling_the_Kazakh_Language_Corpus.html">26 emnlp-2013-Assembling the Kazakh Language Corpus</a></p>
<p>9 0.25586164 <a title="30-lsi-9" href="./emnlp-2013-Russian_Stress_Prediction_using_Maximum_Entropy_Ranking.html">162 emnlp-2013-Russian Stress Prediction using Maximum Entropy Ranking</a></p>
<p>10 0.23392093 <a title="30-lsi-10" href="./emnlp-2013-Animacy_Detection_with_Voting_Models.html">23 emnlp-2013-Animacy Detection with Voting Models</a></p>
<p>11 0.20214772 <a title="30-lsi-11" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>12 0.20152938 <a title="30-lsi-12" href="./emnlp-2013-Open-Domain_Fine-Grained_Class_Extraction_from_Web_Search_Queries.html">142 emnlp-2013-Open-Domain Fine-Grained Class Extraction from Web Search Queries</a></p>
<p>13 0.19384842 <a title="30-lsi-13" href="./emnlp-2013-The_VerbCorner_Project%3A_Toward_an_Empirically-Based_Semantic_Decomposition_of_Verbs.html">183 emnlp-2013-The VerbCorner Project: Toward an Empirically-Based Semantic Decomposition of Verbs</a></p>
<p>14 0.18774557 <a title="30-lsi-14" href="./emnlp-2013-Rule-Based_Information_Extraction_is_Dead%21_Long_Live_Rule-Based_Information_Extraction_Systems%21.html">161 emnlp-2013-Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!</a></p>
<p>15 0.1863803 <a title="30-lsi-15" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>16 0.18413939 <a title="30-lsi-16" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>17 0.18195583 <a title="30-lsi-17" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>18 0.18041745 <a title="30-lsi-18" href="./emnlp-2013-The_Topology_of_Semantic_Knowledge.html">182 emnlp-2013-The Topology of Semantic Knowledge</a></p>
<p>19 0.17843862 <a title="30-lsi-19" href="./emnlp-2013-A_Log-Linear_Model_for_Unsupervised_Text_Normalization.html">9 emnlp-2013-A Log-Linear Model for Unsupervised Text Normalization</a></p>
<p>20 0.17103377 <a title="30-lsi-20" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.022), (9, 0.014), (10, 0.011), (18, 0.03), (22, 0.025), (30, 0.052), (50, 0.061), (51, 0.166), (59, 0.315), (66, 0.109), (71, 0.021), (75, 0.03), (77, 0.022), (96, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83238339 <a title="30-lda-1" href="./emnlp-2013-With_Blinkers_on%3A_Robust_Prediction_of_Eye_Movements_across_Readers.html">203 emnlp-2013-With Blinkers on: Robust Prediction of Eye Movements across Readers</a></p>
<p>Author: Franz Matthies ; Anders Sgaard</p><p>Abstract: Nilsson and Nivre (2009) introduced a treebased model of persons’ eye movements in reading. The individual variation between readers reportedly made application across readers impossible. While a tree-based model seems plausible for eye movements, we show that competitive results can be obtained with a linear CRF model. Increasing the inductive bias also makes learning across readers possible. In fact we observe next-to-no performance drop when evaluating models trained on gaze records of multiple readers on new readers.</p><p>same-paper 2 0.7525785 <a title="30-lda-2" href="./emnlp-2013-Automatic_Extraction_of_Morphological_Lexicons_from_Morphologically_Annotated_Corpora.html">30 emnlp-2013-Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora</a></p>
<p>Author: Ramy Eskander ; Nizar Habash ; Owen Rambow</p><p>Abstract: We present a method for automatically learning inflectional classes and associated lemmas from morphologically annotated corpora. The method consists of a core languageindependent algorithm, which can be optimized for specific languages. The method is demonstrated on Egyptian Arabic and German, two morphologically rich languages. Our best method for Egyptian Arabic provides an error reduction of 55.6% over a simple baseline; our best method for German achieves a 66.7% error reduction.</p><p>3 0.69757861 <a title="30-lda-3" href="./emnlp-2013-Identifying_Multiple_Userids_of_the_Same_Author.html">95 emnlp-2013-Identifying Multiple Userids of the Same Author</a></p>
<p>Author: Tieyun Qian ; Bing Liu</p><p>Abstract: This paper studies the problem of identifying users who use multiple userids to post in social media. Since multiple userids may belong to the same author, it is hard to directly apply supervised learning to solve the problem. This paper proposes a new method, which still uses supervised learning but does not require training documents from the involved userids. Instead, it uses documents from other userids for classifier building. The classifier can be applied to documents of the involved userids. This is possible because we transform the document space to a similarity space and learning is performed in this new space. Our evaluation is done in the online review domain. The experimental results using a large number of userids and their reviews show that the proposed method is highly effective. 1</p><p>4 0.66330934 <a title="30-lda-4" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>Author: Margaret Mitchell ; Jacqui Aguilar ; Theresa Wilson ; Benjamin Van Durme</p><p>Abstract: We propose a novel approach to sentiment analysis for a low resource setting. The intuition behind this work is that sentiment expressed towards an entity, targeted sentiment, may be viewed as a span of sentiment expressed across the entity. This representation allows us to model sentiment detection as a sequence tagging problem, jointly discovering people and organizations along with whether there is sentiment directed towards them. We compare performance in both Spanish and English on microblog data, using only a sentiment lexicon as an external resource. By leveraging linguisticallyinformed features within conditional random fields (CRFs) trained to minimize empirical risk, our best models in Spanish significantly outperform a strong baseline, and reach around 90% accuracy on the combined task of named entity recognition and sentiment prediction. Our models in English, trained on a much smaller dataset, are not yet statistically significant against their baselines.</p><p>5 0.54606611 <a title="30-lda-5" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>Author: Jun-Ping Ng ; Min-Yen Kan ; Ziheng Lin ; Wei Feng ; Bin Chen ; Jian Su ; Chew Lim Tan</p><p>Abstract: In this paper we classify the temporal relations between pairs of events on an article-wide basis. This is in contrast to much of the existing literature which focuses on just event pairs which are found within the same or adjacent sentences. To achieve this, we leverage on discourse analysis as we believe that it provides more useful semantic information than typical lexico-syntactic features. We propose the use of several discourse analysis frameworks, including 1) Rhetorical Structure Theory (RST), 2) PDTB-styled discourse relations, and 3) topical text segmentation. We explain how features derived from these frameworks can be effectively used with support vector machines (SVM) paired with convolution kernels. Experiments show that our proposal is effective in improving on the state-of-the-art significantly by as much as 16% in terms of F1, even if we only adopt less-than-perfect automatic discourse analyzers and parsers. Making use of more accurate discourse analysis can further boost gains to 35%.</p><p>6 0.5416804 <a title="30-lda-6" href="./emnlp-2013-Translating_into_Morphologically_Rich_Languages_with_Synthetic_Phrases.html">186 emnlp-2013-Translating into Morphologically Rich Languages with Synthetic Phrases</a></p>
<p>7 0.53360689 <a title="30-lda-7" href="./emnlp-2013-What_is_Hidden_among_Translation_Rules.html">201 emnlp-2013-What is Hidden among Translation Rules</a></p>
<p>8 0.52975333 <a title="30-lda-8" href="./emnlp-2013-Adaptor_Grammars_for_Learning_Non-Concatenative_Morphology.html">19 emnlp-2013-Adaptor Grammars for Learning Non-Concatenative Morphology</a></p>
<p>9 0.52272522 <a title="30-lda-9" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>10 0.51846957 <a title="30-lda-10" href="./emnlp-2013-The_Effects_of_Syntactic_Features_in_Automatic_Prediction_of_Morphology.html">181 emnlp-2013-The Effects of Syntactic Features in Automatic Prediction of Morphology</a></p>
<p>11 0.51633847 <a title="30-lda-11" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>12 0.51590979 <a title="30-lda-12" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>13 0.51585412 <a title="30-lda-13" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>14 0.51508844 <a title="30-lda-14" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>15 0.51465493 <a title="30-lda-15" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>16 0.51353496 <a title="30-lda-16" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>17 0.51347804 <a title="30-lda-17" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>18 0.5122419 <a title="30-lda-18" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>19 0.51204205 <a title="30-lda-19" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>20 0.51200533 <a title="30-lda-20" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
