<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>111 emnlp-2013-Joint Chinese Word Segmentation and POS Tagging on Heterogeneous Annotated Corpora with Multiple Task Learning</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-111" href="#">emnlp2013-111</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>111 emnlp-2013-Joint Chinese Word Segmentation and POS Tagging on Heterogeneous Annotated Corpora with Multiple Task Learning</h1>
<br/><p>Source: <a title="emnlp-2013-111-pdf" href="http://aclweb.org/anthology//D/D13/D13-1062.pdf">pdf</a></p><p>Author: Xipeng Qiu ; Jiayi Zhao ; Xuanjing Huang</p><p>Abstract: Chinese word segmentation and part-ofspeech tagging (S&T;) are fundamental steps for more advanced Chinese language processing tasks. Recently, it has attracted more and more research interests to exploit heterogeneous annotation corpora for Chinese S&T.; In this paper, we propose a unified model for Chinese S&T; with heterogeneous annotation corpora. We first automatically construct a loose and uncertain mapping between two representative heterogeneous corpora, Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD) . Then we regard the Chinese S&T; with heterogeneous corpora as two “related” tasks and train our model on two heterogeneous corpora simultaneously. Experiments show that our method can boost the performances of both of the heterogeneous corpora by using the shared information, and achieves significant im- provements over the state-of-the-art methods.</p><p>Reference: <a title="emnlp-2013-111-reference" href="../emnlp2013_reference/emnlp-2013-Joint_Chinese_Word_Segmentation_and_POS_Tagging_on_Heterogeneous_Annotated_Corpora_with_Multiple_Task_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn  Abstract Chinese word segmentation and part-ofspeech tagging (S&T;) are fundamental steps for more advanced Chinese language processing tasks. [sent-7, score-0.237]
</p><p>2 Recently, it has attracted more and more research interests to exploit heterogeneous annotation corpora for Chinese S&T. [sent-8, score-0.677]
</p><p>3 ; In this paper, we propose a unified model for Chinese S&T; with heterogeneous annotation corpora. [sent-9, score-0.596]
</p><p>4 We first automatically construct a loose and uncertain mapping between two representative heterogeneous corpora, Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD) . [sent-10, score-1.01]
</p><p>5 Then we regard the Chinese S&T; with heterogeneous corpora as two “related” tasks and train our model on two heterogeneous corpora simultaneously. [sent-11, score-1.163]
</p><p>6 Experiments show that our method can boost the performances of both of the heterogeneous corpora by using the shared information, and achieves significant im-  provements over the state-of-the-art methods. [sent-12, score-0.665]
</p><p>7 Even worse, the existing resources are often incompatible even for a same task and the annotation guidelines are usually different for different projects, since there are many underlying linguistic theories which 658 explain the same language with different perspectives. [sent-16, score-0.197]
</p><p>8 As a result, there often exist multiple heterogeneous annotated corpora for a same task with vastly different and incompatible annotation philosophies. [sent-17, score-0.719]
</p><p>9 These heterogeneous resources are waste on some level if we cannot fully exploit them. [sent-18, score-0.483]
</p><p>10 However, though most of statistical NLP methods are not bound to specific annotation standards, almost all of them cannot deal simultaneously with the training data with different and incompatible annotation. [sent-19, score-0.191]
</p><p>11 The co-existence of heterogeneous annotation data therefore presents a new challenge to utilize these resources. [sent-20, score-0.587]
</p><p>12 The problem of incompatible annotation standards is very serious for many tasks in NLP, especially for Chinese word segmentation and part-of-speech (POS) tagging (Chinese S&T;) . [sent-21, score-0.519]
</p><p>13 In Chinese S&T;, the annotation standards are often incompatible for two main reasons. [sent-22, score-0.261]
</p><p>14 One is that there is no widely accepted segmentation standard due to the lack of a clear definition of Chinese words. [sent-23, score-0.152]
</p><p>15 , 2001) and Penn Chinese Treebank (CTB) (Xia, 2000) , use very different segmentation and POS tagging standards. [sent-26, score-0.237]
</p><p>16 hc o2d0s1 i3n A Nsastoucria lti Loan fgoura Cgoem Ppruotcaetsiosin agl, L piang eusis 6t5ic8s–6 8,  CPDTBD刘L/in刘urf翔/翔NXRi/an rg 进进re入a入c/hV/svV中 国 C/n区hsin/aN区N/n总 /b决fi赛n决a/l赛NN/vn Table 1: Incompatible word segmentation and POS tagging standards between CTB and PDD  CTB, into two words. [sent-30, score-0.335]
</p><p>17 Recently, it has attracted more and more research interests to exploit heterogeneous annotation data for Chinese word segmentation and POS tagging. [sent-34, score-0.756]
</p><p>18 (Sun and Wan, 2012) proposed a structure-based stacking model to fully utilize heterogeneous word structures. [sent-37, score-0.53]
</p><p>19 They also reported that there is no one-to-one mapping be-  tween the heterogeneous word classification and the mapping between heterogeneous tags is very uncertain. [sent-38, score-1.471]
</p><p>20 The first step is to train the preliminary taggers on heterogeneous annotations. [sent-40, score-0.528]
</p><p>21 In this paper, we propose a method for joint Chinese word segmentation and POS tagging with heterogeneous annotation corpora. [sent-43, score-0.834]
</p><p>22 We regard the Chinese S&T; with heterogeneous corpora as two “related” tasks which can improve the performance of each other. [sent-44, score-0.607]
</p><p>23 Since it is impossible to establish an exact mapping between two annotations, we first automatically construct a loose and uncertain mapping the heterogeneous tagsets of CTB and PPD. [sent-45, score-1.287]
</p><p>24 Thus we can tag a sentence in one style with the help of the “related” information in another heterogeneous style. [sent-46, score-0.54]
</p><p>25 The proposed method can improve the performances of joint Chinese S&T; on both corpora by using the shared information of each other, which is  proven effective by experiments. [sent-47, score-0.217]
</p><p>26 There are three main contributions model: •  •  •  659 of our  First, we regard these two joint S&T; tasks on different corpora as two related tasks which have interdependent and peer relationship. [sent-48, score-0.215]
</p><p>27 Second, different to the pipeline-based methods, our model can be trained simultaneously on the heterogeneous corpora. [sent-49, score-0.511]
</p><p>28 Third, our model do not depend on the exactly correct mappings between the two heterogeneous tagsets. [sent-51, score-0.522]
</p><p>29 The correct mapping relations can be automatically built in training phase. [sent-52, score-0.258]
</p><p>30 Section 4 presents an automatic method to build the loose mapping function. [sent-54, score-0.48]
</p><p>31 Then we propose our method on heterogeneous corpora in 5 and 6. [sent-55, score-0.556]
</p><p>32 2  Related Works  There are some works to exploit heterogeneous annotation data for Chinese S&T. [sent-58, score-0.562]
</p><p>33 , 2004) described a transformationbased converter to transfer a certain annotationstyle word segmentation result to another style. [sent-60, score-0.183]
</p><p>34 , 2009) proposed an automatic adaptation method of heterogeneous annotation standards, which depicts a general pipeline to integrate the knowledge of corpora with different  Figure 1: Traditional Pipeline-based Strategy for Heterogeneous POS Tagging underling annotation guidelines. [sent-63, score-0.714]
</p><p>35 They further proposed two optimization strategies, iterative training and predict-self re-estimation, to further improve the accuracy of annotation guideline transformation (Jiang et al. [sent-64, score-0.15]
</p><p>36 (Sun and Wan, 2012) proposed a structurebased stacking model to fully utilize heterogeneous word structures. [sent-66, score-0.53]
</p><p>37 These methods regard one annotation as the  main target and another annotation as the complementary/auxiliary purposes. [sent-67, score-0.188]
</p><p>38 To refine the character-based tagger, PPD-style character labels are directly incorporated as new features. [sent-69, score-0.109]
</p><p>39 Different with the multiple task learning, whose tasks are actually different labels in the same classification task, our model utilizes the shared information between the real different tasks and can produce the corresponding different styles of outputs. [sent-76, score-0.128]
</p><p>40 3  Joint Chinese Word Segmentation and POS Tagging  Currently, the mainstream method of Chinese POS tagging is joint segmentation & tagging with character-based sequence labeling models(Lafferty et al. [sent-77, score-0.421]
</p><p>41 , 2001) , which can avoid the problem of segmentation error propagation and achieve higher performance on both subtasks(Ng and Low, 2004; Jiang et al. [sent-78, score-0.152]
</p><p>42 The label of each character is the crossproduct of a segmentation label and a tagging label. [sent-81, score-0.374]
</p><p>43 A mapping function should be established to represent the relationship between two different annotation guidelines. [sent-85, score-0.347]
</p><p>44 However, the exact mapping relations are hard to establish. [sent-86, score-0.258]
</p><p>45 As reported in (Sun and Wan, 2012) , there is no one-to-one mapping between their heterogeneous word classification, and the mapping between heterogeneous tags is very uncertain. [sent-87, score-1.471]
</p><p>46 Fortunately, there is a loose mapping can be found in CTB annotation guideline1 (Xia, 2000) . [sent-88, score-0.559]
</p><p>47 edu/  ˜chi-  1 The tag set of PDD just includes the 26 broad categories in the mapping table. [sent-92, score-0.292]
</p><p>48 Table 2: Examples of mapping between CTB and PDD’s tagset mapping relations in CTB annotation guideline. [sent-94, score-0.572]
</p><p>49 For example, the mapping may be “NN/CTB↔{n,nt,nz}/PDD”, “NR/CTB↔{nr,ns}/PDD”, “v/PDD↔{VV, VA}/CTB” a{nndr so on. [sent-96, score-0.235]
</p><p>50 We define T1 and T2 as the tag sets for two difWfereen dte annotations, and t1 ∈ T1 and t2 ∈ T2  ×  are the corresponding tags in∈ t wTo tag sets∈ respectively. [sent-97, score-0.149]
</p><p>51 We first establish a loose mapping function m : T1 T2 → {0, 1} between the tags of CTB amnd : PTD×D . [sent-98, score-0.548]
</p><p>52 T  m(t1,t2) ={  01 iefls t1eand t2have mapping relation  (1) The mapping relations are automatically build from the CTB guideline (Xia, 2000) . [sent-99, score-0.556]
</p><p>53 Due to the fact that the tag set of PPD used in the CTB guideline is just broad categories, we expand the mapping relations to include the sub categories. [sent-100, score-0.411]
</p><p>54 For example, for the mapping “NR/CTB↔nr/PDD”, the relation of NR and nrf/nrg sBho↔unldr bPeD Dad”d,ed th ein r ethlaet mapping re anladtions too (nrf/nrg belong to nr) . [sent-102, score-0.491]
</p><p>55 Since we use the character-based joint S&T; model, we also need to find the mapping function between the labels of characters. [sent-103, score-0.339]
</p><p>56 pdf  661 In this paper, we employ the commonly used label set {B, I, E, S} for the segmentation part of cross-labels and the label of character can be in the form of {B-T} ( T represents POS tag) . [sent-108, score-0.328]
</p><p>57 Thus, each mapping relation t1 ↔ t2 can be automatically transformed to fou↔r fo trms: Bt1 ↔B-t2 , I-t1 ↔I-t2 , E-t1 ↔E-t2 and S-t1 ↔St2. [sent-109, score-0.256]
</p><p>58 Beside the above transformation, we also give a slight modification to adapt the different segmentation guidelines. [sent-111, score-0.152]
</p><p>59 In fact, we give segmentation alignment only to proper names due to the limitation of computing ability. [sent-115, score-0.152]
</p><p>60 言  Thus, we can easily build the loose bidirectional mapping function m˜ for the labels of characters. [sent-116, score-0.586]
</p><p>61 1 Sequence Labeling Model We first introduce the commonly used sequence labeling model in character-based joint Chinese S&T. [sent-121, score-0.118]
</p><p>62 mapping function m() between tags  mapping function m~() between labels  PCPTBD- s t y le NnrRnrfNRnrgB-nrBf- NB-Rnrg. [sent-130, score-0.607]
</p><p>63 2 The Proposed Model Different to the single task learning, the heterogeneous data have two sets of labels Y and eZro. [sent-141, score-0.519]
</p><p>64 The heterogeneous datasets Ds and Ds consist of {xi, yi}(i = 0, · · · , m) and {xi , zi} (i = 0, · · · , n) respectively. [sent-142, score-0.483]
</p><p>65 662 We rewrite the loose mapping function m˜ between two label sets into the following forms,  φ(y) = {z| m˜ (y, z) = 1}, (4) φ(z) = {y| m˜ (y, z) = 1}, (5) where φ(z) ⊂ Y and φ(y) ⊂ Z are the subsets owfh Y ean φd( zZ). [sent-154, score-0.573]
</p><p>66 ose G mapping flu yn(cotrio zn) φ r aentu arnnns tthaecorresponding mapping label set in another heterogeneous annotation. [sent-156, score-0.995]
</p><p>67 label of every character is decided by the weights of the corresponding mapping labels and itself. [sent-160, score-0.389]
</p><p>68 In this paper, we also only expand the mapping that with highest according to the current model. [sent-164, score-0.258]
</p><p>69 If given the output type of label T, we only consider the labels in T to initialize the Viterbi matrix, and the score of each node is determined by all the involved heterogeneous labels according to the loose mapping function. [sent-166, score-1.077]
</p><p>70 (15) , when we update the weight vector, the update information includes not only the features extracted from current input, but also that extracted from the loose mapping sequence of input. [sent-183, score-0.516]
</p><p>71 For each feature, the weights of its corresponding related features derived from the loose mapping function will be updated with the same magnitude as well as itself. [sent-184, score-0.536]
</p><p>72 Therefore, the two heterogeneous annotated corpora can be simultaneously used as the input of our training algorithm. [sent-186, score-0.584]
</p><p>73 Because of the tagging and training algorithm, the weights and tags of two corpora can be used separately with the only dependent part built by the loose mapping function. [sent-187, score-0.696]
</p><p>74 If the algorithm tagging a character as “n/PDD”(with help of the weight of “NN/CTB”) and the right tag isn’t one of 664  “n,nt,nz/PDD”, the weight of “NN/CTB” will also be decreased, which is reasonable since it is beneficial to distinguish the right tag. [sent-193, score-0.195]
</p><p>75 Therefore, after multiple iterations, useful features derived from the mapping function are typically receive more updates, which take relatively more responsibility for correct prediction. [sent-195, score-0.291]
</p><p>76 3 Evaluation on CTB-5 PPD The experiment results on the heterogeneous  corpora CTB-5 + PPD are shown in Table 5. [sent-225, score-0.556]
</p><p>77 Our method also gives better performance than the pipeline-based methods on heterogeneous corpora, such as (Jiang et al. [sent-229, score-0.483]
</p><p>78 The reason is that our model can utilize the information of both corpora effectively, which can boost the performance of each other. [sent-231, score-0.117]
</p><p>79 Although the loose mapping function are bidirectional between two annotation tagsets, we may also use unidirectional mapping. [sent-232, score-0.717]
</p><p>80 We just use the mapping function ψPDD→CTB , which means we obtain the PDD-style output without the information from CTB in tagging stage. [sent-234, score-0.353]
</p><p>81 12696 “ModelS ” is the model which is trained on both CTB5 and PDD training datasets with just just using the unidirectional mapping function ψPDD→CTB . [sent-275, score-0.356]
</p><p>82 Table 7: Performances of unidirectional PPD→CTB mapping on CforTmBa-n5 aensd o fP uPnDid. [sent-276, score-0.323]
</p><p>83 4 Evaluation on CTB-S PPD Table 6 shows the experiment results on the heterogeneous corpora CTB-S + PPD. [sent-278, score-0.556]
</p><p>84 5 Analysis As we can see from the above experiments, our proposed unified model can improve the performances of the two heterogeneous corpora with unidirectional or bidirectional loose mapping functions. [sent-283, score-1.254]
</p><p>85 Different to the pipeline-based methods, our model can use the shared information between two heterogeneous POS taggers. [sent-284, score-0.514]
</p><p>86 Although the mapping function is loose 666 and uncertain, it is still can boost the performances. [sent-285, score-0.532]
</p><p>87 The features derived from the wrong mapping function take relatively less responsibility for prediction after multiple updates of their weights in training stage. [sent-286, score-0.341]
</p><p>88 8  Conclusion  We proposed a method for joint Chinese word segmentation and POS tagging with heterogeneous annotation data. [sent-290, score-0.834]
</p><p>89 Different to the previous pipeline-based works, our model is learned on heterogeneous annotation data simultaneously. [sent-291, score-0.562]
</p><p>90 Our method also does not require the exact corresponding relation between the standards of heterogeneous annotations. [sent-292, score-0.602]
</p><p>91 The experimental results show our method leads to a significant improvement with heterogeneous annotations over the best performance for this task. [sent-293, score-0.508]
</p><p>92 Although our work is for a specific task on joint Chinese word segmentation and POS, the key idea to leverage heterogeneous annotations is very general and applicable to other NLP tasks. [sent-294, score-0.695]
</p><p>93 In the future, we will continue to refine the proposed model in two ways: (1) We wish to use the unsupervised method to extract the loose mapping relation between the different annotation standards, which is useful to the corpora without loose mapping guideline. [sent-295, score-1.153]
</p><p>94 (2) We will analyze the shared information (weights of the features derived from the tags which have the mapping relation) in detail and propose a more effective model. [sent-296, score-0.301]
</p><p>95 A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging. [sent-348, score-0.187]
</p><p>96 Automatic adaptation of annotation standards: Chinese word segmentation and POS tagging: a case study. [sent-356, score-0.231]
</p><p>97 Reducing approximation and estimation errors for Chinese lexical processing with heterogeneous annotations. [sent-401, score-0.483]
</p><p>98 A stacked sub-word model for joint Chinese word segmentation and part-of-speech tagging. [sent-406, score-0.187]
</p><p>99 The part-of-speech tagging guidelines for the penn Chinese treebank (3. [sent-410, score-0.171]
</p><p>100 A unified model for joint chinese word segmentation and pos tagging with heterogeneous annotation corpora. [sent-432, score-1.145]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('heterogeneous', 0.483), ('pdd', 0.457), ('ctb', 0.338), ('ppd', 0.246), ('loose', 0.245), ('mapping', 0.235), ('chinese', 0.203), ('segmentation', 0.152), ('standards', 0.098), ('unidirectional', 0.088), ('tagging', 0.085), ('incompatible', 0.084), ('annotation', 0.079), ('pos', 0.074), ('corpora', 0.073), ('jiang', 0.068), ('qiu', 0.064), ('xipeng', 0.061), ('performances', 0.059), ('sun', 0.058), ('tag', 0.057), ('wk', 0.056), ('jiayi', 0.053), ('character', 0.053), ('bakeoff', 0.052), ('xia', 0.052), ('wan', 0.05), ('xuanjing', 0.049), ('uncertain', 0.047), ('yi', 0.047), ('crammer', 0.045), ('zi', 0.042), ('tagsets', 0.042), ('guideline', 0.042), ('label', 0.042), ('pku', 0.041), ('mappings', 0.039), ('nr', 0.039), ('shanghai', 0.037), ('bidirectional', 0.037), ('reductions', 0.037), ('sequence', 0.036), ('labels', 0.036), ('tags', 0.035), ('argymax', 0.035), ('cotfi', 0.035), ('datasettest', 0.035), ('fudannlp', 0.035), ('methodtraining', 0.035), ('peer', 0.035), ('schuller', 0.035), ('sighan', 0.035), ('joint', 0.035), ('unified', 0.034), ('guidelines', 0.034), ('function', 0.033), ('zhao', 0.033), ('sub', 0.031), ('flowchart', 0.031), ('converter', 0.031), ('shared', 0.031), ('regard', 0.03), ('daily', 0.03), ('transformation', 0.029), ('simultaneously', 0.028), ('labeling', 0.028), ('slack', 0.028), ('yl', 0.028), ('penn', 0.028), ('taggers', 0.027), ('updates', 0.027), ('yu', 0.026), ('ando', 0.026), ('establishing', 0.026), ('jin', 0.025), ('annotations', 0.025), ('utilize', 0.025), ('yk', 0.025), ('treebank', 0.024), ('obtains', 0.024), ('weights', 0.023), ('responsibility', 0.023), ('expand', 0.023), ('relations', 0.023), ('interests', 0.022), ('stacking', 0.022), ('tasks', 0.021), ('templates', 0.021), ('relation', 0.021), ('represents', 0.02), ('refine', 0.02), ('attracted', 0.02), ('commonly', 0.019), ('proven', 0.019), ('boost', 0.019), ('styles', 0.019), ('yn', 0.019), ('tagged', 0.019), ('preliminary', 0.018), ('rewrite', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999899 <a title="111-tfidf-1" href="./emnlp-2013-Joint_Chinese_Word_Segmentation_and_POS_Tagging_on_Heterogeneous_Annotated_Corpora_with_Multiple_Task_Learning.html">111 emnlp-2013-Joint Chinese Word Segmentation and POS Tagging on Heterogeneous Annotated Corpora with Multiple Task Learning</a></p>
<p>Author: Xipeng Qiu ; Jiayi Zhao ; Xuanjing Huang</p><p>Abstract: Chinese word segmentation and part-ofspeech tagging (S&T;) are fundamental steps for more advanced Chinese language processing tasks. Recently, it has attracted more and more research interests to exploit heterogeneous annotation corpora for Chinese S&T.; In this paper, we propose a unified model for Chinese S&T; with heterogeneous annotation corpora. We first automatically construct a loose and uncertain mapping between two representative heterogeneous corpora, Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD) . Then we regard the Chinese S&T; with heterogeneous corpora as two “related” tasks and train our model on two heterogeneous corpora simultaneously. Experiments show that our method can boost the performances of both of the heterogeneous corpora by using the shared information, and achieves significant im- provements over the state-of-the-art methods.</p><p>2 0.22891612 <a title="111-tfidf-2" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>Author: Xiaoqing Zheng ; Hanyang Chen ; Tianyu Xu</p><p>Abstract: This study explores the feasibility of performing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representation of Chinese characters, and use these improved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-theart performance with minimal computational cost. We also describe a perceptron-style algorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented.</p><p>3 0.16338292 <a title="111-tfidf-3" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>Author: Longkai Zhang ; Houfeng Wang ; Xu Sun ; Mairgup Mansur</p><p>Abstract: Nowadays supervised sequence labeling models can reach competitive performance on the task of Chinese word segmentation. However, the ability of these models is restricted by the availability of annotated data and the design of features. We propose a scalable semi-supervised feature engineering approach. In contrast to previous works using pre-defined taskspecific features with fixed values, we dynamically extract representations of label distributions from both an in-domain corpus and an out-of-domain corpus. We update the representation values with a semi-supervised approach. Experiments on the benchmark datasets show that our approach achieve good results and reach an f-score of 0.961. The feature engineering approach proposed here is a general iterative semi-supervised method and not limited to the word segmentation task.</p><p>4 0.10383228 <a title="111-tfidf-4" href="./emnlp-2013-An_Empirical_Study_Of_Semi-Supervised_Chinese_Word_Segmentation_Using_Co-Training.html">21 emnlp-2013-An Empirical Study Of Semi-Supervised Chinese Word Segmentation Using Co-Training</a></p>
<p>Author: Fan Yang ; Paul Vozila</p><p>Abstract: In this paper we report an empirical study on semi-supervised Chinese word segmentation using co-training. We utilize two segmenters: 1) a word-based segmenter leveraging a word-level language model, and 2) a character-based segmenter using characterlevel features within a CRF-based sequence labeler. These two segmenters are initially trained with a small amount of segmented data, and then iteratively improve each other using the large amount of unlabelled data. Our experimental results show that co-training captures 20% and 31% of the performance improvement achieved by supervised training with an order of magnitude more data for the SIGHAN Bakeoff 2005 PKU and CU corpora respectively.</p><p>5 0.079061888 <a title="111-tfidf-5" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>Author: Xinjie Zhou ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style. In this paper, we study the problem of extracting opinion targets of Chinese microblog messages. Such fine-grained word-level task has not been well investigated in microblogs yet. We propose an unsupervised label propagation algorithm to address the problem. The opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets. Topics in microblogs are identified by hashtags or using clustering algorithms. Experimental results on Chinese microblogs show the effectiveness of our framework and algorithms.</p><p>6 0.072843991 <a title="111-tfidf-6" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>7 0.065407977 <a title="111-tfidf-7" href="./emnlp-2013-Semi-Supervised_Feature_Transformation_for_Dependency_Parsing.html">168 emnlp-2013-Semi-Supervised Feature Transformation for Dependency Parsing</a></p>
<p>8 0.062527396 <a title="111-tfidf-8" href="./emnlp-2013-Efficient_Higher-Order_CRFs_for_Morphological_Tagging.html">70 emnlp-2013-Efficient Higher-Order CRFs for Morphological Tagging</a></p>
<p>9 0.058174871 <a title="111-tfidf-9" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>10 0.055057846 <a title="111-tfidf-10" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>11 0.049928736 <a title="111-tfidf-11" href="./emnlp-2013-Exploiting_Zero_Pronouns_to_Improve_Chinese_Coreference_Resolution.html">80 emnlp-2013-Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</a></p>
<p>12 0.046314374 <a title="111-tfidf-12" href="./emnlp-2013-Elephant%3A_Sequence_Labeling_for_Word_and_Sentence_Segmentation.html">72 emnlp-2013-Elephant: Sequence Labeling for Word and Sentence Segmentation</a></p>
<p>13 0.045336347 <a title="111-tfidf-13" href="./emnlp-2013-Leveraging_Lexical_Cohesion_and_Disruption_for_Topic_Segmentation.html">124 emnlp-2013-Leveraging Lexical Cohesion and Disruption for Topic Segmentation</a></p>
<p>14 0.04516796 <a title="111-tfidf-14" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>15 0.043833785 <a title="111-tfidf-15" href="./emnlp-2013-Multi-Relational_Latent_Semantic_Analysis.html">137 emnlp-2013-Multi-Relational Latent Semantic Analysis</a></p>
<p>16 0.04274106 <a title="111-tfidf-16" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>17 0.042047925 <a title="111-tfidf-17" href="./emnlp-2013-Ubertagging%3A_Joint_Segmentation_and_Supertagging_for_English.html">190 emnlp-2013-Ubertagging: Joint Segmentation and Supertagging for English</a></p>
<p>18 0.042010333 <a title="111-tfidf-18" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>19 0.040207773 <a title="111-tfidf-19" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>20 0.038635138 <a title="111-tfidf-20" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.152), (1, -0.014), (2, -0.005), (3, -0.082), (4, -0.109), (5, -0.023), (6, 0.08), (7, 0.201), (8, -0.145), (9, 0.169), (10, 0.053), (11, -0.083), (12, 0.085), (13, -0.06), (14, -0.029), (15, 0.034), (16, -0.065), (17, 0.017), (18, 0.038), (19, -0.064), (20, -0.039), (21, -0.005), (22, -0.07), (23, -0.056), (24, 0.025), (25, -0.011), (26, -0.094), (27, 0.017), (28, -0.062), (29, 0.021), (30, -0.031), (31, 0.027), (32, -0.029), (33, -0.036), (34, 0.12), (35, -0.007), (36, -0.072), (37, 0.017), (38, 0.068), (39, 0.055), (40, 0.033), (41, -0.025), (42, 0.028), (43, -0.074), (44, 0.049), (45, -0.005), (46, 0.017), (47, -0.068), (48, -0.053), (49, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95857143 <a title="111-lsi-1" href="./emnlp-2013-Joint_Chinese_Word_Segmentation_and_POS_Tagging_on_Heterogeneous_Annotated_Corpora_with_Multiple_Task_Learning.html">111 emnlp-2013-Joint Chinese Word Segmentation and POS Tagging on Heterogeneous Annotated Corpora with Multiple Task Learning</a></p>
<p>Author: Xipeng Qiu ; Jiayi Zhao ; Xuanjing Huang</p><p>Abstract: Chinese word segmentation and part-ofspeech tagging (S&T;) are fundamental steps for more advanced Chinese language processing tasks. Recently, it has attracted more and more research interests to exploit heterogeneous annotation corpora for Chinese S&T.; In this paper, we propose a unified model for Chinese S&T; with heterogeneous annotation corpora. We first automatically construct a loose and uncertain mapping between two representative heterogeneous corpora, Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD) . Then we regard the Chinese S&T; with heterogeneous corpora as two “related” tasks and train our model on two heterogeneous corpora simultaneously. Experiments show that our method can boost the performances of both of the heterogeneous corpora by using the shared information, and achieves significant im- provements over the state-of-the-art methods.</p><p>2 0.85125554 <a title="111-lsi-2" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>Author: Longkai Zhang ; Houfeng Wang ; Xu Sun ; Mairgup Mansur</p><p>Abstract: Nowadays supervised sequence labeling models can reach competitive performance on the task of Chinese word segmentation. However, the ability of these models is restricted by the availability of annotated data and the design of features. We propose a scalable semi-supervised feature engineering approach. In contrast to previous works using pre-defined taskspecific features with fixed values, we dynamically extract representations of label distributions from both an in-domain corpus and an out-of-domain corpus. We update the representation values with a semi-supervised approach. Experiments on the benchmark datasets show that our approach achieve good results and reach an f-score of 0.961. The feature engineering approach proposed here is a general iterative semi-supervised method and not limited to the word segmentation task.</p><p>3 0.82964623 <a title="111-lsi-3" href="./emnlp-2013-An_Empirical_Study_Of_Semi-Supervised_Chinese_Word_Segmentation_Using_Co-Training.html">21 emnlp-2013-An Empirical Study Of Semi-Supervised Chinese Word Segmentation Using Co-Training</a></p>
<p>Author: Fan Yang ; Paul Vozila</p><p>Abstract: In this paper we report an empirical study on semi-supervised Chinese word segmentation using co-training. We utilize two segmenters: 1) a word-based segmenter leveraging a word-level language model, and 2) a character-based segmenter using characterlevel features within a CRF-based sequence labeler. These two segmenters are initially trained with a small amount of segmented data, and then iteratively improve each other using the large amount of unlabelled data. Our experimental results show that co-training captures 20% and 31% of the performance improvement achieved by supervised training with an order of magnitude more data for the SIGHAN Bakeoff 2005 PKU and CU corpora respectively.</p><p>4 0.80884409 <a title="111-lsi-4" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>Author: Xiaoqing Zheng ; Hanyang Chen ; Tianyu Xu</p><p>Abstract: This study explores the feasibility of performing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representation of Chinese characters, and use these improved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-theart performance with minimal computational cost. We also describe a perceptron-style algorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented.</p><p>5 0.51222193 <a title="111-lsi-5" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>Author: Xinjie Zhou ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style. In this paper, we study the problem of extracting opinion targets of Chinese microblog messages. Such fine-grained word-level task has not been well investigated in microblogs yet. We propose an unsupervised label propagation algorithm to address the problem. The opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets. Topics in microblogs are identified by hashtags or using clustering algorithms. Experimental results on Chinese microblogs show the effectiveness of our framework and algorithms.</p><p>6 0.50598145 <a title="111-lsi-6" href="./emnlp-2013-Elephant%3A_Sequence_Labeling_for_Word_and_Sentence_Segmentation.html">72 emnlp-2013-Elephant: Sequence Labeling for Word and Sentence Segmentation</a></p>
<p>7 0.47340602 <a title="111-lsi-7" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>8 0.46067771 <a title="111-lsi-8" href="./emnlp-2013-Semi-Supervised_Feature_Transformation_for_Dependency_Parsing.html">168 emnlp-2013-Semi-Supervised Feature Transformation for Dependency Parsing</a></p>
<p>9 0.38957816 <a title="111-lsi-9" href="./emnlp-2013-Efficient_Higher-Order_CRFs_for_Morphological_Tagging.html">70 emnlp-2013-Efficient Higher-Order CRFs for Morphological Tagging</a></p>
<p>10 0.37674251 <a title="111-lsi-10" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>11 0.35626295 <a title="111-lsi-11" href="./emnlp-2013-Feature_Noising_for_Log-Linear_Structured_Prediction.html">86 emnlp-2013-Feature Noising for Log-Linear Structured Prediction</a></p>
<p>12 0.35399157 <a title="111-lsi-12" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>13 0.35326356 <a title="111-lsi-13" href="./emnlp-2013-Ubertagging%3A_Joint_Segmentation_and_Supertagging_for_English.html">190 emnlp-2013-Ubertagging: Joint Segmentation and Supertagging for English</a></p>
<p>14 0.30672085 <a title="111-lsi-14" href="./emnlp-2013-Discourse_Level_Explanatory_Relation_Extraction_from_Product_Reviews_Using_First-Order_Logic.html">63 emnlp-2013-Discourse Level Explanatory Relation Extraction from Product Reviews Using First-Order Logic</a></p>
<p>15 0.29286999 <a title="111-lsi-15" href="./emnlp-2013-Rule-Based_Information_Extraction_is_Dead%21_Long_Live_Rule-Based_Information_Extraction_Systems%21.html">161 emnlp-2013-Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!</a></p>
<p>16 0.29105538 <a title="111-lsi-16" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>17 0.28715831 <a title="111-lsi-17" href="./emnlp-2013-Assembling_the_Kazakh_Language_Corpus.html">26 emnlp-2013-Assembling the Kazakh Language Corpus</a></p>
<p>18 0.28414679 <a title="111-lsi-18" href="./emnlp-2013-Mining_New_Business_Opportunities%3A_Identifying_Trend_related_Products_by_Leveraging_Commercial_Intents_from_Microblogs.html">131 emnlp-2013-Mining New Business Opportunities: Identifying Trend related Products by Leveraging Commercial Intents from Microblogs</a></p>
<p>19 0.27521616 <a title="111-lsi-19" href="./emnlp-2013-Chinese_Zero_Pronoun_Resolution%3A_Some_Recent_Advances.html">45 emnlp-2013-Chinese Zero Pronoun Resolution: Some Recent Advances</a></p>
<p>20 0.27317625 <a title="111-lsi-20" href="./emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM.html">99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.019), (18, 0.037), (22, 0.046), (30, 0.089), (45, 0.012), (50, 0.032), (51, 0.177), (66, 0.034), (68, 0.276), (71, 0.045), (75, 0.037), (77, 0.022), (85, 0.023), (90, 0.022), (95, 0.011), (96, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77414054 <a title="111-lda-1" href="./emnlp-2013-Joint_Chinese_Word_Segmentation_and_POS_Tagging_on_Heterogeneous_Annotated_Corpora_with_Multiple_Task_Learning.html">111 emnlp-2013-Joint Chinese Word Segmentation and POS Tagging on Heterogeneous Annotated Corpora with Multiple Task Learning</a></p>
<p>Author: Xipeng Qiu ; Jiayi Zhao ; Xuanjing Huang</p><p>Abstract: Chinese word segmentation and part-ofspeech tagging (S&T;) are fundamental steps for more advanced Chinese language processing tasks. Recently, it has attracted more and more research interests to exploit heterogeneous annotation corpora for Chinese S&T.; In this paper, we propose a unified model for Chinese S&T; with heterogeneous annotation corpora. We first automatically construct a loose and uncertain mapping between two representative heterogeneous corpora, Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD) . Then we regard the Chinese S&T; with heterogeneous corpora as two “related” tasks and train our model on two heterogeneous corpora simultaneously. Experiments show that our method can boost the performances of both of the heterogeneous corpora by using the shared information, and achieves significant im- provements over the state-of-the-art methods.</p><p>2 0.61881083 <a title="111-lda-2" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>Author: Xiaoqing Zheng ; Hanyang Chen ; Tianyu Xu</p><p>Abstract: This study explores the feasibility of performing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representation of Chinese characters, and use these improved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-theart performance with minimal computational cost. We also describe a perceptron-style algorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented.</p><p>3 0.6046713 <a title="111-lda-3" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>Author: Kuzman Ganchev ; Dipanjan Das</p><p>Abstract: We present a framework for cross-lingual transfer of sequence information from a resource-rich source language to a resourceimpoverished target language that incorporates soft constraints via posterior regularization. To this end, we use automatically word aligned bitext between the source and target language pair, and learn a discriminative conditional random field model on the target side. Our posterior regularization constraints are derived from simple intuitions about the task at hand and from cross-lingual alignment information. We show improvements over strong baselines for two tasks: part-of-speech tagging and namedentity segmentation.</p><p>4 0.60460186 <a title="111-lda-4" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>Author: Xinjie Zhou ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style. In this paper, we study the problem of extracting opinion targets of Chinese microblog messages. Such fine-grained word-level task has not been well investigated in microblogs yet. We propose an unsupervised label propagation algorithm to address the problem. The opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets. Topics in microblogs are identified by hashtags or using clustering algorithms. Experimental results on Chinese microblogs show the effectiveness of our framework and algorithms.</p><p>5 0.60452533 <a title="111-lda-5" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>Author: Alla Rozovskaya ; Dan Roth</p><p>Abstract: State-of-the-art systems for grammatical error correction are based on a collection of independently-trained models for specific errors. Such models ignore linguistic interactions at the sentence level and thus do poorly on mistakes that involve grammatical dependencies among several words. In this paper, we identify linguistic structures with interacting grammatical properties and propose to address such dependencies via joint inference and joint learning. We show that it is possible to identify interactions well enough to facilitate a joint approach and, consequently, that joint methods correct incoherent predictions that independentlytrained classifiers tend to produce. Furthermore, because the joint learning model considers interacting phenomena during training, it is able to identify mistakes that require mak- ing multiple changes simultaneously and that standard approaches miss. Overall, our model significantly outperforms the Illinois system that placed first in the CoNLL-2013 shared task on grammatical error correction.</p><p>6 0.60139877 <a title="111-lda-6" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>7 0.60035658 <a title="111-lda-7" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>8 0.60013372 <a title="111-lda-8" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>9 0.60013366 <a title="111-lda-9" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>10 0.59992838 <a title="111-lda-10" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>11 0.59967095 <a title="111-lda-11" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>12 0.59906387 <a title="111-lda-12" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>13 0.59890234 <a title="111-lda-13" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>14 0.59814894 <a title="111-lda-14" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>15 0.59788036 <a title="111-lda-15" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<p>16 0.59780449 <a title="111-lda-16" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>17 0.59654385 <a title="111-lda-17" href="./emnlp-2013-Semi-Markov_Phrase-Based_Monolingual_Alignment.html">167 emnlp-2013-Semi-Markov Phrase-Based Monolingual Alignment</a></p>
<p>18 0.59651893 <a title="111-lda-18" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>19 0.59641588 <a title="111-lda-19" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>20 0.59622949 <a title="111-lda-20" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
