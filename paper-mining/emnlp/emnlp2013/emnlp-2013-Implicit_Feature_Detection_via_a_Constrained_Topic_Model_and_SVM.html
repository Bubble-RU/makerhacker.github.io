<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-99" href="#">emnlp2013-99</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</h1>
<br/><p>Source: <a title="emnlp-2013-99-pdf" href="http://aclweb.org/anthology//D/D13/D13-1092.pdf">pdf</a></p><p>Author: Wei Wang ; Hua Xu ; Xiaoqiu Huang</p><p>Abstract: Implicit feature detection, also known as implicit feature identification, is an essential aspect of feature-specific opinion mining but previous works have often ignored it. We think, based on the explicit sentences, several Support Vector Machine (SVM) classifiers can be established to do this task. Nevertheless, we believe it is possible to do better by using a constrained topic model instead of traditional attribute selection methods. Experiments show that this method outperforms the traditional attribute selection methods by a large margin and the detection task can be completed better.</p><p>Reference: <a title="emnlp-2013-99-reference" href="../emnlp2013_reference/emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  ,  Abstract Implicit feature detection, also known as implicit feature identification, is an essential aspect of feature-specific opinion mining but previous works have often ignored it. [sent-5, score-0.557]
</p><p>2 We think, based on the explicit sentences, several Support Vector Machine (SVM) classifiers can be established to do this task. [sent-6, score-0.312]
</p><p>3 Nevertheless, we believe it is possible to do better by using a constrained topic model instead of traditional attribute selection methods. [sent-7, score-0.645]
</p><p>4 Experiments show that this method outperforms the traditional attribute selection methods by a large margin and the detection task can be completed better. [sent-8, score-0.317]
</p><p>5 1 Introduction Feature-specific  opinion mining has been well de-  fined by Ding and Liu(2008). [sent-9, score-0.069]
</p><p>6 Example 1 is a cell  phone review in which two features are mentioned. [sent-10, score-0.094]
</p><p>7 Example 1 This cell phone is fashion in appearance, and it is also very cheap. [sent-11, score-0.094]
</p><p>8 If a feature appears in a review directly, it is called an explicit feature. [sent-12, score-0.321]
</p><p>9 If a feature is only implied, it is called an implicit feature. [sent-13, score-0.373]
</p><p>10 In Example 1, appearance is an explicit feature while price is an implicit feature, which is implied by cheap. [sent-14, score-0.84]
</p><p>11 Furthermore, an explicit sentence is defined as a sentence containing at least one explicit feature, and an implicit sentence is the sentence only containing implicit features. [sent-15, score-1.06]
</p><p>12 Thus, the first sentence is an explicit sentence, while the second is an implicit one. [sent-16, score-0.53]
</p><p>13 This paper proposes an approach for implicit feature detection based on SVM and Topic Model(TM). [sent-17, score-0.419]
</p><p>14 903 The Topic Model, which incorporated into constraints based on the pre-defined product feature, is established to extract the training attributes for SVM. [sent-18, score-0.26]
</p><p>15 In the end, several SVM classifiers are constructed to train the selected attributes and utilized to detect the implicit features. [sent-19, score-0.465]
</p><p>16 2  Related Work  The definition of implicit feature comes from Liu et al. [sent-20, score-0.373]
</p><p>17 (2006) used Pointwise Mutual Information (PMI) based semantic association analysis to identify implicit features, but no quantitative experimental results were provided. [sent-23, score-0.291]
</p><p>18 (201 1) used co-occurrence association rule mining to identify implicit features. [sent-25, score-0.291]
</p><p>19 However, they only dealt with opinion words and neglected the facts. [sent-26, score-0.102]
</p><p>20 Since the inception of these works, many variations have been proposed. [sent-31, score-0.033]
</p><p>21 For example, LDA has previously been used to construct attributes for classification; it often acts to reduce data dimension(Blei and Jordan, 2003; Fei-Fei and Perona, 2005; Quelhas et al. [sent-32, score-0.096]
</p><p>22 Here, we modify LDA and adopt it to select the training attributes for SVM. [sent-34, score-0.096]
</p><p>23 1 Introduction to LDA We briefly introduce LDA, following the notation of Griffiths(Griffiths and Steyvers, 2004). [sent-36, score-0.031]
</p><p>24 For this scheme, the core process is the topic updating for each word in each document according to Equation 1. [sent-40, score-0.259]
</p><p>25 P(zi = j |z−i, w, α, β) =  (∑wW′n(−nwi(−wi,)ji,′j)++ β Wβ)(∑jTn (−d(−i d,j)i ,j)++ α Tα)  (1)  where zi = j represents the assignment of the ith word in a document to topic j, z−i represents all  n(jw′)  the topic assignments excluding the ith word. [sent-41, score-0.467]
</p><p>26 is the number of instances of word w′ assigned to topic j and is the number of words from document di assigned to topic j, the −i notation sig-  n(jdi)  nifies that tshsieg cnoedun ttos are tcak je,n th omitting athtieo nva sliugeof zi. [sent-42, score-0.443]
</p><p>27 When a specific product and the reviews are provided, the explicit sentences and corresponding features are extracted(Line 1) by word segmentation, part-ofspeech(POS) tagging and synonyms feature clustering. [sent-47, score-0.445]
</p><p>28 Then the prior knowledge are drawn from the explicit sentences automatically and integrated in-  to the constrained topic model((Line 3 - Line 5). [sent-48, score-0.76]
</p><p>29 Finally, several SVM classifiers are generated and applied to detect implicit features(Line 7 - Line 12). [sent-50, score-0.369]
</p><p>30 In our work, we use a constrained topic model to select attributes for each product features. [sent-53, score-0.558]
</p><p>31 Then two types of prior knowledge, which are derived from the pre-defined product features, are extracted automatically and incorporated: must-link/cannot-link and correlation prior knowledge. [sent-55, score-0.286]
</p><p>32 1 Must-link and Cannot-link Must-link: It specifies that two data instances must be in the same cluster. [sent-58, score-0.036]
</p><p>33 In order to mine these words, we compute the co-occurrence degree by frequency*PMI(f,w), whose formula is as following: bility of subscript occurrence in explicit sentences, f is the feature, w is the word, and f&w; means the co-occurrence of f and w. [sent-60, score-0.239]
</p><p>34 A higher value of frequency*PMI signifies that w often indicates f. [sent-61, score-0.033]
</p><p>35 For a feature fi, the top five words and fi constitute must-links. [sent-62, score-0.175]
</p><p>36 Cannot-link: It specifies that two data instances cannot be in the same cluster. [sent-64, score-0.036]
</p><p>37 If a word and a feature never co-occur in our corpus, we assume them to form a cannot-link. [sent-65, score-0.082]
</p><p>38 For example, the word lowcost has never co-occurred with the product feature screen, so they constitute a cannot-link in our corpus. [sent-66, score-0.241]
</p><p>39 In this paper, the pre-defined process, must-link, and cannot-link are derived from Andrzejewski and Zhu (2009)’s work, all must-links and cannot-links are incorporated our constrained topic model. [sent-67, score-0.414]
</p><p>40 We multiply an indicator function δ(wi, zj), which represents a hard constraint, to the Equation 1 as the final probability for topic updating (see Equation 4). [sent-68, score-0.259]
</p><p>41 P(zi = j |z−i, w, α, β) =  δ(wi,zj)(∑wWn′n(−w(i−wi,)ji′,)j++ β Wβ)(∑jTnn(−d−(i d,ij)i,)+j+ α Tα) (4) As illustrated by Equations 1 and 4, δ(wi, zj), which represents intervention or help from preexisting knowledge of must-links and cannot-links,  plays a key role in this study. [sent-69, score-0.033]
</p><p>42 In the topic updating for each word in each document, we assume that the current word is wi and its linked feature topic set is Z(wi), then for the current topic zj, δ(wi, zj) is calculated as follows: 1. [sent-70, score-1.069]
</p><p>43 If wi is constrained by must-links and the linked feature belongs to Z(wi), δ(wi, zj |zj ∈ Z(wi)) = 1and δ(wi, zj |zj ∈/ Z(wi)) = 0. [sent-71, score-1.385]
</p><p>44 If wi is constrained by cannot-links and the linked feature belongs to Z(wi), δ(wi, zj |zj ∈ Z(wi)) = 0 and δ(wi, zj |zj ∈/ Z(wi)) = 1. [sent-73, score-1.385]
</p><p>45 2 Correlation Prior Knowledge In view of the explicit product feature of each topic, the association of the word and the feature to topic-word distribution should be taken into account. [sent-81, score-0.491]
</p><p>46 Therefore, Equation 2 is revised as the following:  ϕj(wi)=∑wW(′1(1 + + C Cwwi,′j, ) ( nnj(wj(wi)′) ) + + β Wβ where Cw′,j re∑flects the correlation of  w′  (5)  with the  topic j,which is centered on the product feature fzj . [sent-82, score-0.723]
</p><p>47 The basic idea is to determine the association of w′ and fzj ,ifthey have the high relevance, Cw′,j should be set as a positive number. [sent-83, score-0.305]
</p><p>48 Otherwise, if we can determine w′ and fzj are irrelevant, Cw′,j should be 905 set as a positive number. [sent-84, score-0.305]
</p><p>49 Dependency relation judgement: If w′ as parent node in the syntax tree mainly co-occurs with fzj , Cw′,j will be set positive. [sent-87, score-0.305]
</p><p>50 If w′ mainly co-occurs with several features including fzj , Cw′,j will be set negative. [sent-88, score-0.305]
</p><p>51 PMI judgement: If w′ mainly co-occurs with fzj and PMI(w′, fzj ) is greater than the given value, Cw′,j will be set positive. [sent-91, score-0.61]
</p><p>52 4  Attribute Selection  Some words, such as ”good”, can modify several product features and should be removed. [sent-94, score-0.088]
</p><p>53 In the result of run once, if a word appears in the topics which relates to different features, it is defined as a conflicting word. [sent-95, score-0.079]
</p><p>54 If a term is thought to describe several features or indicate no features, it is defined as a noise word . [sent-96, score-0.047]
</p><p>55 When each topic has been pre-allocated, we run the explicit topic model 100 times. [sent-97, score-0.651]
</p><p>56 If a word turns into a conflicting word Tcw times(Tcw is set to 20), we assume that it is a noise word. [sent-98, score-0.095]
</p><p>57 Then the noise word collection is obtained and applied to filter the explicit sentences. [sent-99, score-0.286]
</p><p>58 The most important part to filter noise words is the correlation computation. [sent-102, score-0.089]
</p><p>59 So the experiment can work well with only estimated parameters. [sent-103, score-0.03]
</p><p>60 Next, By integrating pre-existing knowledge, the explicit topic model, which runs Titer times, severs as attribute selection for SVM. [sent-104, score-0.668]
</p><p>61 In every result for each topic cluster, we remove the least four prob-  able of word groups and merge the results by the pre-defined product feature. [sent-105, score-0.294]
</p><p>62 For a feature, if a word appears in its topic words more than Titer ∗ tratio times, it is selected as one of the training att∗ri tbutes for the feature. [sent-106, score-0.358]
</p><p>63 In the end, if an attribute associates with different features, it is deleted. [sent-107, score-0.148]
</p><p>64 IGCnafhoSinGqRauniat oire 20 30 40 50 Attrbiute Factor Number  60  70  80  90  100  (a) SVM based on traditional attribute selection method  T M + mcsPuoaynMrteIs+coal in ko+twPcselMyrndIagit(o+kswyelnadgcit) T M + mcsPuoyarnMet+Islcoani ko+twPcselMyrndIagit(o+kswyelndagcit) 01. [sent-108, score-0.271]
</p><p>65 (b) our constrained topic model by different tratio (Titer  = 20)  10  20  Ti etr  30  40  50  (c) our constrained topic model by different Titer (tratio = 0. [sent-113, score-0.9]
</p><p>66 5 Implicit Feature Detection via SVM After completing attribute selection, vector space model(VSM) is applied to the selected attributes on the explicit sentences. [sent-115, score-0.483]
</p><p>67 For each feature fi, a SVM classifier Ci is adopted. [sent-116, score-0.082]
</p><p>68 In train-set, the positive cases are the explicit sentences of fi, and the negative cases are the other explicit sentences. [sent-117, score-0.514]
</p><p>69 For a nonexplicit sentence, if the classification result of Ci is positive, it is an implicit sentence which implies fi. [sent-118, score-0.291]
</p><p>70 1 Data Sets There has no standard data set yet, we crawled the experiment data, which included reviews about a cellphone, from a famous Chinese shopping web-  site1 . [sent-120, score-0.03]
</p><p>71 The feature of each sentence was manually annotated by two research assistants. [sent-122, score-0.082]
</p><p>72 A handful of sentences which were annotated inconsistently were deleted. [sent-123, score-0.069]
</p><p>73 Other features were ignored because of their rare appearance. [sent-125, score-0.033]
</p><p>74 Here are some explanations: (1)The sentences containing several explicit features were not added to the train-set. [sent-126, score-0.275]
</p><p>75 (2) A tiny number of sentences contain both explicit and implicit features, and they can only be regarded as explicit sentences. [sent-127, score-0.805]
</p><p>76 (3) The training set contains 3 140 explicit sentences, the test set contains 7043 non-explicit sentences and more than 5500 sentences have no feature. [sent-128, score-0.311]
</p><p>77 (4) According to the ratio among the explicit sentences(6: 1:2:3: 1:2), it is reasonable that the most suitable number of topics should be 14. [sent-129, score-0.27]
</p><p>78 com/ 906 Table 1: Experiment data Features  Explicit  Implicit  Total  screen  1165  244  1409  quality  199  83  282  battery  456  205  661  price  627  561  1188  appearance  224  167  391  software  469  129  598  uct feature screen is 6, so we can assign the feature to topic 0,1,2,3,4,5. [sent-132, score-0.713]
</p><p>79 (5) Although the size of dataset is limited, out proposed is based on the constraint-based topic model, which has been widely used in different NLP fields. [sent-134, score-0.206]
</p><p>80 Of course, more high quality data will be collected to do the experiment in the future. [sent-136, score-0.03]
</p><p>81 2  Experimental Results  Figure 1a depicts the performance of using traditional attribute selection methods on SVM. [sent-138, score-0.271]
</p><p>82 In our constrained topic  model, we use different Titer and tratio. [sent-141, score-0.374]
</p><p>83 We conducted experiments by incorporating different types prior knowledge. [sent-142, score-0.078]
</p><p>84 From Figure 1b and 1c, we conclude that: (1)All these methods perform much better than the traditional feature selection methods, the improvements are more than 6%. [sent-143, score-0.205]
</p><p>85 (2)The reason for the little improvement of must-links is that the topic clusters have already obtained these linked words. [sent-144, score-0.269]
</p><p>86 (3)All the pre-existing knowledge performs best and shows 3% improvement over non prior knowledge. [sent-145, score-0.111]
</p><p>87 (4)Different types of prior knowledge have different impact on the stabilities of different parameters. [sent-146, score-0.111]
</p><p>88 (5)As we have expected, by combing allprior knowledge, the best performance can reach 77. [sent-147, score-0.03]
</p><p>89 Furthermore, as tratio or Titer changes, our constrained topic model incorporating all prior knowledge look like very stable. [sent-149, score-0.637]
</p><p>90 5  Conclusions  In this paper, we adopt a constrained topic model incorporating prior knowledge to select attribute for SVM classifiers to detect implicit features. [sent-150, score-1.002]
</p><p>91 Experiments show this method outperforms the attribute feature selection methods and detect implicit fea-  tures better. [sent-151, score-0.637]
</p><p>92 Using pointwise mutual information to identify implicit features in customer reviews. [sent-235, score-0.327]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('zj', 0.393), ('fzj', 0.305), ('implicit', 0.291), ('wi', 0.253), ('explicit', 0.239), ('titer', 0.229), ('topic', 0.206), ('cw', 0.186), ('constrained', 0.168), ('tratio', 0.152), ('attribute', 0.148), ('price', 0.121), ('pmi', 0.115), ('svm', 0.108), ('lda', 0.103), ('tcw', 0.099), ('attributes', 0.096), ('product', 0.088), ('feature', 0.082), ('prior', 0.078), ('quelhas', 0.076), ('tdi', 0.076), ('twpcselmyrndiagit', 0.076), ('griffiths', 0.075), ('selection', 0.075), ('cheap', 0.073), ('appearance', 0.07), ('opinion', 0.069), ('tsinghua', 0.066), ('ww', 0.065), ('blei', 0.063), ('linked', 0.063), ('screen', 0.061), ('judgement', 0.06), ('andrzejewski', 0.056), ('ding', 0.056), ('fi', 0.055), ('zi', 0.055), ('updating', 0.053), ('cell', 0.051), ('china', 0.051), ('conflicting', 0.048), ('ko', 0.048), ('traditional', 0.048), ('noise', 0.047), ('ci', 0.046), ('line', 0.046), ('detection', 0.046), ('hai', 0.045), ('phone', 0.043), ('equations', 0.043), ('gibbs', 0.043), ('equation', 0.042), ('correlation', 0.042), ('beijing', 0.042), ('detect', 0.041), ('steyvers', 0.041), ('incorporated', 0.04), ('wj', 0.039), ('vision', 0.038), ('constitute', 0.038), ('implied', 0.037), ('classifiers', 0.037), ('liu', 0.037), ('specifies', 0.036), ('pointwise', 0.036), ('sentences', 0.036), ('dirichlet', 0.036), ('gmai', 0.036), ('established', 0.036), ('belongs', 0.033), ('ignored', 0.033), ('tuytelaars', 0.033), ('lowcost', 0.033), ('telecommunications', 0.033), ('inconsistently', 0.033), ('seett', 0.033), ('signifies', 0.033), ('cellphone', 0.033), ('inception', 0.033), ('inghua', 0.033), ('jw', 0.033), ('neglected', 0.033), ('nnj', 0.033), ('orient', 0.033), ('tsd', 0.033), ('xiaowen', 0.033), ('knowledge', 0.033), ('national', 0.031), ('notation', 0.031), ('topics', 0.031), ('descriptors', 0.03), ('iccv', 0.03), ('battery', 0.03), ('hua', 0.03), ('combing', 0.03), ('dso', 0.03), ('peo', 0.03), ('experiment', 0.03), ('ji', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="99-tfidf-1" href="./emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM.html">99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</a></p>
<p>Author: Wei Wang ; Hua Xu ; Xiaoqiu Huang</p><p>Abstract: Implicit feature detection, also known as implicit feature identification, is an essential aspect of feature-specific opinion mining but previous works have often ignored it. We think, based on the explicit sentences, several Support Vector Machine (SVM) classifiers can be established to do this task. Nevertheless, we believe it is possible to do better by using a constrained topic model instead of traditional attribute selection methods. Experiments show that this method outperforms the traditional attribute selection methods by a large margin and the detection task can be completed better.</p><p>2 0.1540902 <a title="99-tfidf-2" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>Author: Zhiyuan Chen ; Arjun Mukherjee ; Bing Liu ; Meichun Hsu ; Malu Castellanos ; Riddhiman Ghosh</p><p>Abstract: Aspect extraction is one of the key tasks in sentiment analysis. In recent years, statistical models have been used for the task. However, such models without any domain knowledge often produce aspects that are not interpretable in applications. To tackle the issue, some knowledge-based topic models have been proposed, which allow the user to input some prior domain knowledge to generate coherent aspects. However, existing knowledge-based topic models have several major shortcomings, e.g., little work has been done to incorporate the cannot-link type of knowledge or to automatically adjust the number of topics based on domain knowledge. This paper proposes a more advanced topic model, called MC-LDA (LDA with m-set and c-set), to address these problems, which is based on an Extended generalized Pólya urn (E-GPU) model (which is also proposed in this paper). Experiments on real-life product reviews from a variety of domains show that MCLDA outperforms the existing state-of-the-art models markedly.</p><p>3 0.13977914 <a title="99-tfidf-3" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>Author: Xinjie Zhou ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style. In this paper, we study the problem of extracting opinion targets of Chinese microblog messages. Such fine-grained word-level task has not been well investigated in microblogs yet. We propose an unsupervised label propagation algorithm to address the problem. The opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets. Topics in microblogs are identified by hashtags or using clustering algorithms. Experimental results on Chinese microblogs show the effectiveness of our framework and algorithms.</p><p>4 0.12251944 <a title="99-tfidf-4" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<p>Author: John Philip McCrae ; Philipp Cimiano ; Roman Klinger</p><p>Abstract: Cross-lingual topic modelling has applications in machine translation, word sense disambiguation and terminology alignment. Multilingual extensions of approaches based on latent (LSI), generative (LDA, PLSI) as well as explicit (ESA) topic modelling can induce an interlingual topic space allowing documents in different languages to be mapped into the same space and thus to be compared across languages. In this paper, we present a novel approach that combines latent and explicit topic modelling approaches in the sense that it builds on a set of explicitly defined topics, but then computes latent relations between these. Thus, the method combines the benefits of both explicit and latent topic modelling approaches. We show that on a crosslingual mate retrieval task, our model significantly outperforms LDA, LSI, and ESA, as well as a baseline that translates every word in a document into the target language.</p><p>5 0.10438029 <a title="99-tfidf-5" href="./emnlp-2013-Improvements_to_the_Bayesian_Topic_N-Gram_Models.html">100 emnlp-2013-Improvements to the Bayesian Topic N-Gram Models</a></p>
<p>Author: Hiroshi Noji ; Daichi Mochihashi ; Yusuke Miyao</p><p>Abstract: One of the language phenomena that n-gram language model fails to capture is the topic information of a given situation. We advance the previous study of the Bayesian topic language model by Wallach (2006) in two directions: one, investigating new priors to alleviate the sparseness problem caused by dividing all ngrams into exclusive topics, and two, developing a novel Gibbs sampler that enables moving multiple n-grams across different documents to another topic. Our blocked sampler can efficiently search for higher probability space even with higher order n-grams. In terms of modeling assumption, we found it is effective to assign a topic to only some parts of a document.</p><p>6 0.093975969 <a title="99-tfidf-6" href="./emnlp-2013-Dependency_Language_Models_for_Sentence_Completion.html">58 emnlp-2013-Dependency Language Models for Sentence Completion</a></p>
<p>7 0.090125494 <a title="99-tfidf-7" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>8 0.085171297 <a title="99-tfidf-8" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>9 0.078145966 <a title="99-tfidf-9" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>10 0.078057632 <a title="99-tfidf-10" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>11 0.071985051 <a title="99-tfidf-11" href="./emnlp-2013-Learning_Topics_and_Positions_from_Debatepedia.html">121 emnlp-2013-Learning Topics and Positions from Debatepedia</a></p>
<p>12 0.071696848 <a title="99-tfidf-12" href="./emnlp-2013-Automatically_Determining_a_Proper_Length_for_Multi-Document_Summarization%3A_A_Bayesian_Nonparametric_Approach.html">36 emnlp-2013-Automatically Determining a Proper Length for Multi-Document Summarization: A Bayesian Nonparametric Approach</a></p>
<p>13 0.071428634 <a title="99-tfidf-13" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<p>14 0.067277238 <a title="99-tfidf-14" href="./emnlp-2013-Classifying_Message_Board_Posts_with_an_Extracted_Lexicon_of_Patient_Attributes.html">46 emnlp-2013-Classifying Message Board Posts with an Extracted Lexicon of Patient Attributes</a></p>
<p>15 0.065369084 <a title="99-tfidf-15" href="./emnlp-2013-Automatic_Domain_Partitioning_for_Multi-Domain_Learning.html">29 emnlp-2013-Automatic Domain Partitioning for Multi-Domain Learning</a></p>
<p>16 0.063621625 <a title="99-tfidf-16" href="./emnlp-2013-Leveraging_Lexical_Cohesion_and_Disruption_for_Topic_Segmentation.html">124 emnlp-2013-Leveraging Lexical Cohesion and Disruption for Topic Segmentation</a></p>
<p>17 0.059446316 <a title="99-tfidf-17" href="./emnlp-2013-Identifying_Manipulated_Offerings_on_Review_Portals.html">94 emnlp-2013-Identifying Manipulated Offerings on Review Portals</a></p>
<p>18 0.058571048 <a title="99-tfidf-18" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>19 0.057370357 <a title="99-tfidf-19" href="./emnlp-2013-Modeling_Scientific_Impact_with_Topical_Influence_Regression.html">133 emnlp-2013-Modeling Scientific Impact with Topical Influence Regression</a></p>
<p>20 0.057285972 <a title="99-tfidf-20" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.188), (1, 0.062), (2, -0.107), (3, -0.007), (4, 0.02), (5, -0.013), (6, 0.079), (7, 0.045), (8, -0.03), (9, -0.016), (10, -0.078), (11, -0.207), (12, -0.12), (13, 0.093), (14, -0.024), (15, 0.115), (16, 0.093), (17, 0.014), (18, -0.041), (19, -0.058), (20, -0.014), (21, 0.058), (22, -0.133), (23, 0.089), (24, -0.095), (25, -0.074), (26, -0.044), (27, 0.104), (28, -0.014), (29, 0.035), (30, -0.01), (31, -0.117), (32, 0.085), (33, -0.02), (34, -0.032), (35, 0.001), (36, -0.003), (37, -0.237), (38, 0.183), (39, 0.109), (40, 0.027), (41, 0.04), (42, 0.111), (43, 0.098), (44, 0.031), (45, -0.085), (46, 0.083), (47, -0.017), (48, -0.01), (49, 0.111)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96500999 <a title="99-lsi-1" href="./emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM.html">99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</a></p>
<p>Author: Wei Wang ; Hua Xu ; Xiaoqiu Huang</p><p>Abstract: Implicit feature detection, also known as implicit feature identification, is an essential aspect of feature-specific opinion mining but previous works have often ignored it. We think, based on the explicit sentences, several Support Vector Machine (SVM) classifiers can be established to do this task. Nevertheless, we believe it is possible to do better by using a constrained topic model instead of traditional attribute selection methods. Experiments show that this method outperforms the traditional attribute selection methods by a large margin and the detection task can be completed better.</p><p>2 0.74510682 <a title="99-lsi-2" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>Author: Zhiyuan Chen ; Arjun Mukherjee ; Bing Liu ; Meichun Hsu ; Malu Castellanos ; Riddhiman Ghosh</p><p>Abstract: Aspect extraction is one of the key tasks in sentiment analysis. In recent years, statistical models have been used for the task. However, such models without any domain knowledge often produce aspects that are not interpretable in applications. To tackle the issue, some knowledge-based topic models have been proposed, which allow the user to input some prior domain knowledge to generate coherent aspects. However, existing knowledge-based topic models have several major shortcomings, e.g., little work has been done to incorporate the cannot-link type of knowledge or to automatically adjust the number of topics based on domain knowledge. This paper proposes a more advanced topic model, called MC-LDA (LDA with m-set and c-set), to address these problems, which is based on an Extended generalized Pólya urn (E-GPU) model (which is also proposed in this paper). Experiments on real-life product reviews from a variety of domains show that MCLDA outperforms the existing state-of-the-art models markedly.</p><p>3 0.59676629 <a title="99-lsi-3" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>Author: Xinjie Zhou ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style. In this paper, we study the problem of extracting opinion targets of Chinese microblog messages. Such fine-grained word-level task has not been well investigated in microblogs yet. We propose an unsupervised label propagation algorithm to address the problem. The opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets. Topics in microblogs are identified by hashtags or using clustering algorithms. Experimental results on Chinese microblogs show the effectiveness of our framework and algorithms.</p><p>4 0.59414971 <a title="99-lsi-4" href="./emnlp-2013-Improvements_to_the_Bayesian_Topic_N-Gram_Models.html">100 emnlp-2013-Improvements to the Bayesian Topic N-Gram Models</a></p>
<p>Author: Hiroshi Noji ; Daichi Mochihashi ; Yusuke Miyao</p><p>Abstract: One of the language phenomena that n-gram language model fails to capture is the topic information of a given situation. We advance the previous study of the Bayesian topic language model by Wallach (2006) in two directions: one, investigating new priors to alleviate the sparseness problem caused by dividing all ngrams into exclusive topics, and two, developing a novel Gibbs sampler that enables moving multiple n-grams across different documents to another topic. Our blocked sampler can efficiently search for higher probability space even with higher order n-grams. In terms of modeling assumption, we found it is effective to assign a topic to only some parts of a document.</p><p>5 0.50922233 <a title="99-lsi-5" href="./emnlp-2013-Learning_Topics_and_Positions_from_Debatepedia.html">121 emnlp-2013-Learning Topics and Positions from Debatepedia</a></p>
<p>Author: Swapna Gottipati ; Minghui Qiu ; Yanchuan Sim ; Jing Jiang ; Noah A. Smith</p><p>Abstract: We explore Debatepedia, a communityauthored encyclopedia of sociopolitical debates, as evidence for inferring a lowdimensional, human-interpretable representation in the domain of issues and positions. We introduce a generative model positing latent topics and cross-cutting positions that gives special treatment to person mentions and opinion words. We evaluate the resulting representation’s usefulness in attaching opinionated documents to arguments and its consistency with human judgments about positions.</p><p>6 0.49337915 <a title="99-lsi-6" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<p>7 0.4435491 <a title="99-lsi-7" href="./emnlp-2013-Using_Topic_Modeling_to_Improve_Prediction_of_Neuroticism_and_Depression_in_College_Students.html">199 emnlp-2013-Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students</a></p>
<p>8 0.4273639 <a title="99-lsi-8" href="./emnlp-2013-Dependency_Language_Models_for_Sentence_Completion.html">58 emnlp-2013-Dependency Language Models for Sentence Completion</a></p>
<p>9 0.42406625 <a title="99-lsi-9" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>10 0.42066625 <a title="99-lsi-10" href="./emnlp-2013-Discourse_Level_Explanatory_Relation_Extraction_from_Product_Reviews_Using_First-Order_Logic.html">63 emnlp-2013-Discourse Level Explanatory Relation Extraction from Product Reviews Using First-Order Logic</a></p>
<p>11 0.41046831 <a title="99-lsi-11" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>12 0.37626415 <a title="99-lsi-12" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<p>13 0.37479344 <a title="99-lsi-13" href="./emnlp-2013-Combining_Generative_and_Discriminative_Model_Scores_for_Distant_Supervision.html">49 emnlp-2013-Combining Generative and Discriminative Model Scores for Distant Supervision</a></p>
<p>14 0.36292198 <a title="99-lsi-14" href="./emnlp-2013-Semi-Supervised_Feature_Transformation_for_Dependency_Parsing.html">168 emnlp-2013-Semi-Supervised Feature Transformation for Dependency Parsing</a></p>
<p>15 0.35310879 <a title="99-lsi-15" href="./emnlp-2013-A_Generative_Joint%2C_Additive%2C_Sequential_Model_of_Topics_and_Speech_Acts_in_Patient-Doctor_Communication.html">6 emnlp-2013-A Generative Joint, Additive, Sequential Model of Topics and Speech Acts in Patient-Doctor Communication</a></p>
<p>16 0.34631541 <a title="99-lsi-16" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>17 0.34601432 <a title="99-lsi-17" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>18 0.33446038 <a title="99-lsi-18" href="./emnlp-2013-Modeling_Scientific_Impact_with_Topical_Influence_Regression.html">133 emnlp-2013-Modeling Scientific Impact with Topical Influence Regression</a></p>
<p>19 0.32029173 <a title="99-lsi-19" href="./emnlp-2013-Leveraging_Lexical_Cohesion_and_Disruption_for_Topic_Segmentation.html">124 emnlp-2013-Leveraging Lexical Cohesion and Disruption for Topic Segmentation</a></p>
<p>20 0.3154065 <a title="99-lsi-20" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.021), (9, 0.031), (18, 0.017), (22, 0.077), (30, 0.08), (50, 0.013), (51, 0.186), (62, 0.277), (66, 0.076), (71, 0.043), (75, 0.049), (77, 0.017), (96, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79696655 <a title="99-lda-1" href="./emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM.html">99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</a></p>
<p>Author: Wei Wang ; Hua Xu ; Xiaoqiu Huang</p><p>Abstract: Implicit feature detection, also known as implicit feature identification, is an essential aspect of feature-specific opinion mining but previous works have often ignored it. We think, based on the explicit sentences, several Support Vector Machine (SVM) classifiers can be established to do this task. Nevertheless, we believe it is possible to do better by using a constrained topic model instead of traditional attribute selection methods. Experiments show that this method outperforms the traditional attribute selection methods by a large margin and the detection task can be completed better.</p><p>2 0.64129621 <a title="99-lda-2" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>Author: Jun-Ping Ng ; Min-Yen Kan ; Ziheng Lin ; Wei Feng ; Bin Chen ; Jian Su ; Chew Lim Tan</p><p>Abstract: In this paper we classify the temporal relations between pairs of events on an article-wide basis. This is in contrast to much of the existing literature which focuses on just event pairs which are found within the same or adjacent sentences. To achieve this, we leverage on discourse analysis as we believe that it provides more useful semantic information than typical lexico-syntactic features. We propose the use of several discourse analysis frameworks, including 1) Rhetorical Structure Theory (RST), 2) PDTB-styled discourse relations, and 3) topical text segmentation. We explain how features derived from these frameworks can be effectively used with support vector machines (SVM) paired with convolution kernels. Experiments show that our proposal is effective in improving on the state-of-the-art significantly by as much as 16% in terms of F1, even if we only adopt less-than-perfect automatic discourse analyzers and parsers. Making use of more accurate discourse analysis can further boost gains to 35%.</p><p>3 0.63887519 <a title="99-lda-3" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>Author: Zhiyuan Chen ; Arjun Mukherjee ; Bing Liu ; Meichun Hsu ; Malu Castellanos ; Riddhiman Ghosh</p><p>Abstract: Aspect extraction is one of the key tasks in sentiment analysis. In recent years, statistical models have been used for the task. However, such models without any domain knowledge often produce aspects that are not interpretable in applications. To tackle the issue, some knowledge-based topic models have been proposed, which allow the user to input some prior domain knowledge to generate coherent aspects. However, existing knowledge-based topic models have several major shortcomings, e.g., little work has been done to incorporate the cannot-link type of knowledge or to automatically adjust the number of topics based on domain knowledge. This paper proposes a more advanced topic model, called MC-LDA (LDA with m-set and c-set), to address these problems, which is based on an Extended generalized Pólya urn (E-GPU) model (which is also proposed in this paper). Experiments on real-life product reviews from a variety of domains show that MCLDA outperforms the existing state-of-the-art models markedly.</p><p>4 0.63138133 <a title="99-lda-4" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>Author: Zhongqing Wang ; Shoushan LI ; Fang Kong ; Guodong Zhou</p><p>Abstract: Personal profile information on social media like LinkedIn.com and Facebook.com is at the core of many interesting applications, such as talent recommendation and contextual advertising. However, personal profiles usually lack organization confronted with the large amount of available information. Therefore, it is always a challenge for people to find desired information from them. In this paper, we address the task of personal profile summarization by leveraging both personal profile textual information and social networks. Here, using social networks is motivated by the intuition that, people with similar academic, business or social connections (e.g. co-major, co-university, and cocorporation) tend to have similar experience and summaries. To achieve the learning process, we propose a collective factor graph (CoFG) model to incorporate all these resources of knowledge to summarize personal profiles with local textual attribute functions and social connection factors. Extensive evaluation on a large-scale dataset from LinkedIn.com demonstrates the effectiveness of the proposed approach. 1</p><p>5 0.6303786 <a title="99-lda-5" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>Author: Xinjie Zhou ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style. In this paper, we study the problem of extracting opinion targets of Chinese microblog messages. Such fine-grained word-level task has not been well investigated in microblogs yet. We propose an unsupervised label propagation algorithm to address the problem. The opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets. Topics in microblogs are identified by hashtags or using clustering algorithms. Experimental results on Chinese microblogs show the effectiveness of our framework and algorithms.</p><p>6 0.6300348 <a title="99-lda-6" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>7 0.62920547 <a title="99-lda-7" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>8 0.62629151 <a title="99-lda-8" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>9 0.62466502 <a title="99-lda-9" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>10 0.62223941 <a title="99-lda-10" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>11 0.62131673 <a title="99-lda-11" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>12 0.6195792 <a title="99-lda-12" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>13 0.61934322 <a title="99-lda-13" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>14 0.61926889 <a title="99-lda-14" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>15 0.61733532 <a title="99-lda-15" href="./emnlp-2013-Exploiting_Zero_Pronouns_to_Improve_Chinese_Coreference_Resolution.html">80 emnlp-2013-Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</a></p>
<p>16 0.61702365 <a title="99-lda-16" href="./emnlp-2013-Semi-Supervised_Feature_Transformation_for_Dependency_Parsing.html">168 emnlp-2013-Semi-Supervised Feature Transformation for Dependency Parsing</a></p>
<p>17 0.61699522 <a title="99-lda-17" href="./emnlp-2013-An_Empirical_Study_Of_Semi-Supervised_Chinese_Word_Segmentation_Using_Co-Training.html">21 emnlp-2013-An Empirical Study Of Semi-Supervised Chinese Word Segmentation Using Co-Training</a></p>
<p>18 0.61688471 <a title="99-lda-18" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>19 0.61667967 <a title="99-lda-19" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>20 0.61614782 <a title="99-lda-20" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
