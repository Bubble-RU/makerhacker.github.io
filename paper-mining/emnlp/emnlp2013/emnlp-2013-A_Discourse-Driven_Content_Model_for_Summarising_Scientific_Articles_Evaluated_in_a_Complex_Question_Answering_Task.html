<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>5 emnlp-2013-A Discourse-Driven Content Model for Summarising Scientific Articles Evaluated in a Complex Question Answering Task</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-5" href="#">emnlp2013-5</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>5 emnlp-2013-A Discourse-Driven Content Model for Summarising Scientific Articles Evaluated in a Complex Question Answering Task</h1>
<br/><p>Source: <a title="emnlp-2013-5-pdf" href="http://aclweb.org/anthology//D/D13/D13-1070.pdf">pdf</a></p><p>Author: Maria Liakata ; Simon Dobnik ; Shyamasree Saha ; Colin Batchelor ; Dietrich Rebholz-Schuhmann</p><p>Abstract: We present a method which exploits automatically generated scientific discourse annotations to create a content model for the summarisation of scientific articles. Full papers are first automatically annotated using the CoreSC scheme, which captures 11 contentbased concepts such as Hypothesis, Result, Conclusion etc at the sentence level. A content model which follows the sequence of CoreSC categories observed in abstracts is used to provide the skeleton of the summary, making a distinction between dependent and independent categories. Summary creation is also guided by the distribution of CoreSC categories found in the full articles, in order to adequately represent the article content. Fi- nally, we demonstrate the usefulness of the summaries by evaluating them in a complex question answering task. Results are very encouraging as summaries of papers from automatically obtained CoreSCs enable experts to answer 66% of complex content-related questions designed on the basis of paper abstracts. The questions were answered with a precision of 75%, where the upper bound for human summaries (abstracts) was 95%.</p><p>Reference: <a title="emnlp-2013-5-reference" href="../emnlp2013_reference/emnlp-2013-A_Discourse-Driven_Content_Model_for_Summarising_Scientific_Articles_Evaluated_in_a_Complex_Question_Answering_Task_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A discourse-driven content model for summarising scientific articles evaluated in a complex question answering task Maria Liakata Simon Dobnik Shyamasree Saha University of Warwick/ University of Gothenburg, Sweden EMBL-EBI, UK EMBL-EBI, UK s imon . [sent-2, score-0.402]
</p><p>2 bat che l orc @ rs c org Abstract We present a method which exploits automatically generated scientific discourse annotations to create a content model for the summarisation of scientific articles. [sent-9, score-0.542]
</p><p>3 A content model which follows the sequence of CoreSC categories observed in abstracts is used to provide the skeleton of the summary, making a distinction between dependent and independent categories. [sent-11, score-0.438]
</p><p>4 Fi-  nally, we demonstrate the usefulness of the summaries by evaluating them in a complex question answering task. [sent-13, score-0.489]
</p><p>5 Results are very encouraging as summaries of papers from automatically obtained CoreSCs enable experts to answer 66% of complex content-related questions designed on the basis of paper abstracts. [sent-14, score-0.738]
</p><p>6 The questions were answered with a precision of 75%, where the upper bound for human summaries (abstracts) was 95%. [sent-15, score-0.578]
</p><p>7 , 2011), with a view to support scientists in obtaining relevant information from scientific articles and abstracts, less work has focussed on automatically combining such information in the form of a cohesive summary which preserves the context. [sent-23, score-0.393]
</p><p>8 Teufel (2001; 2010), (Teufel and Moens, 2002) identify argumentative zones within scientific articles and use them to create use-targeted extractive summaries. [sent-26, score-0.48]
</p><p>9 A selection of various combinations of argumentative zones are chosen for the use-targeted extractive summaries (rhetorical extracts), each of which fulfills a different role. [sent-28, score-0.692]
</p><p>10 As the emphasis of this approach was the identification of the argumentative zones, less attention was given to the sentence selection criteria for the extractive summaries. [sent-30, score-0.224]
</p><p>11 , 2011) to guide the creation of extractive summaries of scientific articles. [sent-37, score-0.687]
</p><p>12 The number of sentences to include in the summary is prespecified (either directly or using a compression ratio). [sent-41, score-0.227]
</p><p>13 Our approach also makes use of the scientific discourse for summarisation purposes. [sent-42, score-0.297]
</p><p>14 We use the scientific discourse to create a content model for extractive summarisation, with a focus on representing the content of the full paper, while keeping the cohesion of the narrative. [sent-43, score-0.541]
</p><p>15 We then use the distribution of CoreSC categories observed in abstracts to create a content model which provides a skeleton for extractive summaries. [sent-52, score-0.453]
</p><p>16 The reasoning behind this is to try to preserve cohesion within the summaries and we hypothesise 748 that the sequence of CoreSC categories is a good proxy for cohesion (see section 3. [sent-53, score-0.623]
</p><p>17 In creating the summary, instantiating the content model, we identify independent categories and dependent categories, and we argue that in order to preserve the cohesion of the text the independent categories should be determined first (see section 3. [sent-55, score-0.481]
</p><p>18 We also preserve in the summary the distribution of CoreSC categories found in the corresponding full paper. [sent-57, score-0.306]
</p><p>19 Finally, we evaluate the extractive summaries in a complex real world question-answering task, in  which we assess the usefulness of the summaries as well as to what extent the generated CoreSC summaries represent the content of the original article. [sent-58, score-1.485]
</p><p>20 Experts are presented with different types of summaries and are asked to answer article-specific questions on the basis of the summaries (see section 4. [sent-59, score-1.013]
</p><p>21 Our results show that automatically generated CoreSC summaries can answer 66% of complex questions with 75% precision, outperforming a baseline of microsoft autosummarise summaries (See section 4. [sent-61, score-1.074]
</p><p>22 We have also peformed an intrinsic evaluation of the summaries using ROUGE and automatic measures for summary informativeness, such as the Jensen-Shannon divergence, yielding positive results (See section 4. [sent-63, score-0.605]
</p><p>23 Code for generating the summaries can be obtained by contacting the first author and/or visiting http://www. [sent-66, score-0.418]
</p><p>24 (2010) created a corpus of 265 full scientific articles from chemistry and biochemistry annotated with this scheme and trained classifiers using SVMs and CRFs in (Liakata et al. [sent-73, score-0.289]
</p><p>25 Summarisation for scientific articles: A lot of the work on summarising scientific articles has focussed on citation-based summaries. [sent-77, score-0.396]
</p><p>26 Sentences are clustered together creating a topic, with the combination ofclusters forming a citation summary network. [sent-79, score-0.231]
</p><p>27 , 2010) also make use of citation sentences in other scientific papers to summarize the contributions of a paper. [sent-81, score-0.286]
</p><p>28 The drawback of citation summaries is that a paper must be already cited, so this type of summary will not be useful to a paper reviewer. [sent-82, score-0.649]
</p><p>29 While we use supervised methods to annotate papers with a fixed set of topics (CoreSCs) in scientific papers, our summary content model for extracts shares similar principles such as global or-  dering of sentences and non-recurrence. [sent-88, score-0.56]
</p><p>30 , 2009) can be applied to scientific articles (over 100 sentences long), which by nature include repetition of topics. [sent-91, score-0.246]
</p><p>31 It would be interesting to make comparisons with summaries using content models learnt from our data automatically, following a similar approach to (Sauper et al. [sent-92, score-0.533]
</p><p>32 3  Extractive Summarisation using CoreSCs  In this section we describe how we use CoreSC discourse categories annotated at the sentence level to create extractive summaries of full papers, which we subsequently evaluate in a question answering task in section 4. [sent-94, score-0.732]
</p><p>33 To generate summaries we follow classic text ex-  traction techniques while making use of a document content model based on CoreSCs. [sent-95, score-0.508]
</p><p>34 While we do not consider abstracts to be adequate summaries, we at least consider them to be coherent summaries, which is why the content model reflects the distribution of CoreSCs in the abstracts. [sent-98, score-0.256]
</p><p>35 The following subsections give details about the creation of extractive summaries from CoreSC categories. [sent-102, score-0.532]
</p><p>36 However, unlike (Teufel and Moens, 2002), who take a fixed number of AZ categories of specific type to create rhetorical extracts, the number of categories used from each CoreSC category depends on their distribution in the original article. [sent-107, score-0.294]
</p><p>37 Any and all types of CoreSC category could potentially appear in a summary, as our summaries are meant to be representative of the entire content of the paper. [sent-108, score-0.551]
</p><p>38 Also, the ordering of the categories in the summary is learnt  to reflect the ordering of categories observed in abstracts of papers from the same domain. [sent-109, score-0.591]
</p><p>39 For this reason and to allow better comparison between papers of varying lengths, we fixed our summary length to 20 sentences. [sent-119, score-0.234]
</p><p>40 Ordering of CoreSC categories in the summary: According tSoC a study orfi empirical summaries (Liddy, 1991), sentences of a particular textual type appear in a particular order. [sent-127, score-0.541]
</p><p>41 Since paper abstracts were the closest approximation of human summaries available to us, CoreSC category transitions found in abstracts have been adopted in our content model for extracts. [sent-128, score-0.883]
</p><p>42 First, we extracted initial, medium and final bi-grams of categories from paper abstracts together with transition probabilities. [sent-130, score-0.249]
</p><p>43 Interestingly, our semi-empirically derived model closely follows the content model for abstracts described in (Liddy, 1991). [sent-135, score-0.256]
</p><p>44 2 Sentence extraction based on independent and dependent categories Sentence extraction involves selecting the most relevant sentences to include in a summary. [sent-138, score-0.222]
</p><p>45 1, the number of CoreSC categories in the summaries is determined according to their distribution in the paper and the order of the categories is specified in the content model. [sent-149, score-0.674]
</p><p>46 The independent categories include the categories with the lowest percentage of sentences in scientific articles as reported in (Liakata et al. [sent-162, score-0.459]
</p><p>47 Sentence extraction is driven by first identifying  the independent categories based on classifier confidence scores and then choosing the corresponding dependent categories on the basis of both relatedness to the independent categories and classifier confidence. [sent-168, score-0.449]
</p><p>48 • For a dependent category Cat, for which we need n sentences, given cthatee gsoerleyct Ceadt ,se fnorte wncheics m efr nomee dth ne corresponding independent category CatI we do the following: • If m = 0, then treat Cat as independent category fIofr m mthi =s case. [sent-173, score-0.275]
</p><p>49 Once the sentence ids are selected for each independent and each dependent category we plug them into the content model. [sent-181, score-0.256]
</p><p>50 1  Summary evaluation via question answering Task Description and experimental setup  We evaluate the extractive CoreSC summaries in terms of how well they enable 12 chemistry experts/evaluators (with at least a Masters degree in chemistry) to answer complex questions about the papers. [sent-185, score-0.81]
</p><p>51 For each of the 28 papers in the test corpus, we generated CoreSC summaries automatically using the method described in section 3. [sent-188, score-0.465]
</p><p>52 We compare the performance of the experts on a question answering (Q-A) task when given the CoreSC summaries and two other types of summary, amounting to a total of three experimental conditions (A,B,C). [sent-189, score-0.531]
</p><p>53 The other two types of summary are the original paper abstracts (summaries A), in the absence of human summaries, and summaries generated by Microsoft Office Word 2007 AutoSummarize (summaries B). [sent-190, score-0.771]
</p><p>54 For the latter reason, we considered MA to be a suitable baseline the comparison with which would illustrate the effect of using CoreSC categories on the summary and the merits of having a discourse based model for summarisation. [sent-195, score-0.319]
</p><p>55 By contrast, the current Q-A task aims to show how well the summaries represent the content of the entire paper, which means that questions are individual to each paper and required domain knowledge to create. [sent-202, score-0.604]
</p><p>56 Each of the 12 experts answered three contentbased questions per summary, where the questions were individual to each paper. [sent-203, score-0.315]
</p><p>57 They were designed by a senior chemistry expert with knowledge of linguistics, so that they could be answered based on the abstracts (A). [sent-215, score-0.325]
</p><p>58 For this purpose, the senior expert chose abstracts that were at least three sentences long. [sent-216, score-0.277]
</p><p>59 One of the merits of setting the questions on the basis of the abstracts was that the answers to be identified were deemed sufficiently important to be expressed in the humanly created abstract. [sent-219, score-0.363]
</p><p>60 However, automatic summaries created in the way proposed here could potentially 753 answer questions beyond the scope of the abstract and in cases of very short abstracts be much more informative. [sent-220, score-0.734]
</p><p>61 Experts were told that summaries were automatically generated with no details about different types of summary; it is assumed that none of them is completely familiar with the work mentioned in the 28 papers. [sent-221, score-0.418]
</p><p>62 On average, it took experts less than 10 minutes to read a summary and answer the three content-based questions. [sent-222, score-0.31]
</p><p>63 EvalutG Go134r2goups1–CAB-78–CAB-P1a4pers15(AB2–C-82)1 BCA-28  Table 1: Distribution of summaries to evaluators  4. [sent-223, score-0.418]
</p><p>64 2 Results and Discussion We compared each evaluator’s answers obtained after reading a summary against the model answers set by the senior expert, the author of the questions, based on the abstract (A) of the corresponding paper. [sent-224, score-0.381]
</p><p>65 1 above, “axial to the piperidine ring”, “gauche- to the ring nitrogen” and “The OH6 group is axial (Gauche) to the ring nitrogen” were all considered correct, fully matched answers to the question “What is the conformation of the exocyclic hydroxymethyl group? [sent-228, score-0.35]
</p><p>66 Similarly, cases where the evaluator gave “N/A” as an answer were marked as “justified” or  “unjustified” according to whether the senior expert could find the answer in the summary or not. [sent-239, score-0.417]
</p><p>67 96750F  Table 2: Matches between summary-based answers and model answers  Table 3: Precision, Recall and F-score for answering questions using the four types of summary. [sent-248, score-0.288]
</p><p>68 We report Precision, Recall and F-score (P-R-F) for answering questions given each type of summary (Table 3). [sent-250, score-0.327]
</p><p>69 Here, the standard definition of recall (TP/(TP+FN)) demonstrates how many questions can be answered using the summary (summary coverage) and Precision (TP/(TP+FP)) how well the questions are answered (summary clarity). [sent-252, score-0.441]
</p><p>70 Summaries of condition C provide answers to more questions (Recall) and with greater accuracy (Precision) than summaries B. [sent-259, score-0.588]
</p><p>71 When macro-averaging, the Recall score of summaries C is tied with that for summaries B but Precision is 6% higher. [sent-260, score-0.836]
</p><p>72 To verify the statistical significance for the difference in precision and recall for summaries B and C respectively, we performed Monte Carlo sampling 10000 times, for the populations of answers for summaries B and C. [sent-261, score-0.943]
</p><p>73 A t-test performed on the population of precision and the population of recalls showed statistical significance at 95% in both cases, with summaries C having a precision of 5% higher and a recall of 1. [sent-263, score-0.484]
</p><p>74 Therefore, we can say that CoreSC summaries C are overall better for answering questions than summaries B. [sent-266, score-0.976]
</p><p>75 Table 4: Test for statistical significance betwen summaries B (microsoft) and C (CoreSC) The difference in precision between summaries B and C shows the advantage of having a content model: summaries C are significantly clearer. [sent-267, score-1.377]
</p><p>76 We had also expected CoreSC summaries to have a much higher coverage than summaries B, and therefore significantly higher recall. [sent-268, score-0.836]
</p><p>77 Analysis using ROUGE showed that while summaries C had a slightly higher ROUGE-1 measure than summaries B (0. [sent-271, score-0.836]
</p><p>78 In table 5 we also report measurements on summary informativeness based on divergence (Kullback Leibler (KL) divergence and Jensen Shannon (JS) divergence), as in (Louis and Nenkova, 2013). [sent-275, score-0.357]
</p><p>79 JS divergence is an information-theoretic measure, reflecting the average distance of the KL divergence between summary and input (the full paper in our case) from the mean vocabulary distributions. [sent-277, score-0.357]
</p><p>80 Compared to other measures, JS divergence has been found to produce the best predictions of summary quality (Louis and Nenkova, 2013). [sent-278, score-0.272]
</p><p>81 In practice, what JS divergence tells us is how ‘different’/divergent the summary is from the original paper. [sent-279, score-0.272]
</p><p>82 Low divergence scores are indicative of greater overlap between the summaries and the original paper and are considered positive in terms of the summary information con-  tent. [sent-280, score-0.69]
</p><p>83 B: Autosummarize, C: CoreSC, random: random summaries each 20 sentences long for each paper. [sent-286, score-0.458]
</p><p>84 KLS-I: Kullback Leibler divergence between summary and input, since KL divergence is not symmetric. [sent-288, score-0.357]
</p><p>85 755 One can see the that CoreSC summaries have consistently lower divergence (both KL and JS) than microsoft autosummarise summaries and random summaries of the same length. [sent-292, score-1.4]
</p><p>86 This is a positive outcome but since such automatic measures of summary quality have not yet reached maturity and are harder to interpret, we consider the manual evaluation a more reliable indicator of summary informa-  tiveness and usefulness. [sent-293, score-0.374]
</p><p>87 Note that it is not appropriate to use divergence to assess the abstracts as this measure is influenced by the length of a text, which varies dramatically in the case of abstracts. [sent-294, score-0.251]
</p><p>88 5  Conclusions and future work  We have shown how a content model based on the scientific discourse as annotated by the CoreSC scheme can be used to produce extractive summaries. [sent-295, score-0.434]
</p><p>89 These summaries can be generated as alternatives to abstracts. [sent-296, score-0.418]
</p><p>90 We have tested the usefulness CoreSC based summaries in answering complex questions relating to the content of scientific papers. [sent-298, score-0.83]
</p><p>91 Extracts from automated CoreSCs are informative, outperform microsoft autosummarise summaries, in both intrinsic and extrinsic evaluation, and enable experts to answer 66% of complex questions with a precision of 75%. [sent-299, score-0.34]
</p><p>92 We would also like to perform comparisons with automatically induced content models and check their viability for scientific articles. [sent-301, score-0.245]
</p><p>93 We also would like to perform a human based evaluation of coherence and explore the full potential of these summaries as alternatives to author-written abstracts. [sent-302, score-0.418]
</p><p>94 This work constitutes a very important step in producing automatic summaries of scientific papers and enabling experts to extract information from the papers, a major requirement for resource curation, which is dependent on constant reviewing of the literature. [sent-303, score-0.741]
</p><p>95 We would also like to thank Mo Abrahams for the python version of the summarisation code and the cafe summary toolkit. [sent-306, score-0.28]
</p><p>96 Using argumentative zones for extractive summarization of scientific articles. [sent-348, score-0.454]
</p><p>97 A weakly-supervised approach to argumentative zoning of scientific documents. [sent-375, score-0.282]
</p><p>98 Automatic recognition of conceptualisation zones in scientific articles and two life science applications. [sent-418, score-0.303]
</p><p>99 Automatically assessing machine summary content without a gold stan-  dard. [sent-434, score-0.277]
</p><p>100 Summarizing scientific articles: experiments with relevance and rhetorical status. [sent-488, score-0.24]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('coresc', 0.62), ('summaries', 0.418), ('liakata', 0.222), ('summary', 0.187), ('abstracts', 0.166), ('scientific', 0.155), ('teufel', 0.155), ('corescs', 0.117), ('extractive', 0.114), ('questions', 0.096), ('summarisation', 0.093), ('content', 0.09), ('argumentative', 0.086), ('rhetorical', 0.085), ('divergence', 0.085), ('categories', 0.083), ('answers', 0.074), ('zones', 0.074), ('experts', 0.069), ('unjustified', 0.061), ('autosummarize', 0.058), ('obs', 0.058), ('ring', 0.058), ('chemistry', 0.057), ('met', 0.057), ('moens', 0.056), ('qazvinian', 0.056), ('answer', 0.054), ('dependent', 0.052), ('articles', 0.051), ('bac', 0.051), ('bionlp', 0.051), ('evaluator', 0.051), ('hyp', 0.051), ('discourse', 0.049), ('papers', 0.047), ('independent', 0.047), ('senior', 0.046), ('citation', 0.044), ('cj', 0.044), ('answering', 0.044), ('category', 0.043), ('cohesion', 0.043), ('louis', 0.043), ('res', 0.041), ('js', 0.041), ('extracts', 0.041), ('zoning', 0.041), ('sentences', 0.04), ('justified', 0.037), ('preserve', 0.036), ('autosummarise', 0.035), ('axial', 0.035), ('cati', 0.035), ('cjs', 0.035), ('exocyclic', 0.035), ('goa', 0.035), ('hydroxymethyl', 0.035), ('ionization', 0.035), ('mot', 0.035), ('pyysalo', 0.035), ('resonant', 0.035), ('spectroscopies', 0.035), ('summarising', 0.035), ('yufan', 0.035), ('exp', 0.035), ('simone', 0.033), ('precision', 0.033), ('matched', 0.032), ('answered', 0.031), ('kl', 0.031), ('nitrogen', 0.031), ('sauper', 0.031), ('uk', 0.028), ('kullback', 0.028), ('leibler', 0.028), ('vahed', 0.028), ('confidence', 0.027), ('complex', 0.027), ('basis', 0.027), ('tp', 0.027), ('scheme', 0.026), ('citations', 0.026), ('obj', 0.026), ('microsoft', 0.026), ('barzilay', 0.026), ('expert', 0.025), ('learnt', 0.025), ('proximity', 0.025), ('concepts', 0.025), ('summarization', 0.025), ('mod', 0.024), ('sentence', 0.024), ('dragomir', 0.024), ('ananiadou', 0.023), ('batchelor', 0.023), ('conceptualisation', 0.023), ('conformation', 0.023), ('contentbased', 0.023), ('dobnik', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="5-tfidf-1" href="./emnlp-2013-A_Discourse-Driven_Content_Model_for_Summarising_Scientific_Articles_Evaluated_in_a_Complex_Question_Answering_Task.html">5 emnlp-2013-A Discourse-Driven Content Model for Summarising Scientific Articles Evaluated in a Complex Question Answering Task</a></p>
<p>Author: Maria Liakata ; Simon Dobnik ; Shyamasree Saha ; Colin Batchelor ; Dietrich Rebholz-Schuhmann</p><p>Abstract: We present a method which exploits automatically generated scientific discourse annotations to create a content model for the summarisation of scientific articles. Full papers are first automatically annotated using the CoreSC scheme, which captures 11 contentbased concepts such as Hypothesis, Result, Conclusion etc at the sentence level. A content model which follows the sequence of CoreSC categories observed in abstracts is used to provide the skeleton of the summary, making a distinction between dependent and independent categories. Summary creation is also guided by the distribution of CoreSC categories found in the full articles, in order to adequately represent the article content. Fi- nally, we demonstrate the usefulness of the summaries by evaluating them in a complex question answering task. Results are very encouraging as summaries of papers from automatically obtained CoreSCs enable experts to answer 66% of complex content-related questions designed on the basis of paper abstracts. The questions were answered with a precision of 75%, where the upper bound for human summaries (abstracts) was 95%.</p><p>2 0.22183631 <a title="5-tfidf-2" href="./emnlp-2013-Automatically_Determining_a_Proper_Length_for_Multi-Document_Summarization%3A_A_Bayesian_Nonparametric_Approach.html">36 emnlp-2013-Automatically Determining a Proper Length for Multi-Document Summarization: A Bayesian Nonparametric Approach</a></p>
<p>Author: Tengfei Ma ; Hiroshi Nakagawa</p><p>Abstract: Document summarization is an important task in the area of natural language processing, which aims to extract the most important information from a single document or a cluster of documents. In various summarization tasks, the summary length is manually defined. However, how to find the proper summary length is quite a problem; and keeping all summaries restricted to the same length is not always a good choice. It is obviously improper to generate summaries with the same length for two clusters of documents which contain quite different quantity of information. In this paper, we propose a Bayesian nonparametric model for multidocument summarization in order to automatically determine the proper lengths of summaries. Assuming that an original document can be reconstructed from its summary, we describe the ”reconstruction” by a Bayesian framework which selects sentences to form a good summary. Experimental results on DUC2004 data sets and some expanded data demonstrate the good quality of our summaries and the rationality of the length determination.</p><p>3 0.1148776 <a title="5-tfidf-3" href="./emnlp-2013-Fast_Joint_Compression_and_Summarization_via_Graph_Cuts.html">85 emnlp-2013-Fast Joint Compression and Summarization via Graph Cuts</a></p>
<p>Author: Xian Qian ; Yang Liu</p><p>Abstract: Extractive summarization typically uses sentences as summarization units. In contrast, joint compression and summarization can use smaller units such as words and phrases, resulting in summaries containing more information. The goal of compressive summarization is to find a subset of words that maximize the total score of concepts and cutting dependency arcs under the grammar constraints and summary length constraint. We propose an efficient decoding algorithm for fast compressive summarization using graph cuts. Our approach first relaxes the length constraint using Lagrangian relaxation. Then we propose to bound the relaxed objective function by the supermodular binary quadratic programming problem, which can be solved efficiently using graph max-flow/min-cut. Since finding the tightest lower bound suffers from local optimality, we use convex relaxation for initialization. Experimental results on TAC2008 dataset demonstrate our method achieves competitive ROUGE score and has good readability, while is much faster than the integer linear programming (ILP) method.</p><p>4 0.10367059 <a title="5-tfidf-4" href="./emnlp-2013-Optimized_Event_Storyline_Generation_based_on_Mixture-Event-Aspect_Model.html">147 emnlp-2013-Optimized Event Storyline Generation based on Mixture-Event-Aspect Model</a></p>
<p>Author: Lifu Huang ; Lian'en Huang</p><p>Abstract: Recently, much research focuses on event storyline generation, which aims to produce a concise, global and temporal event summary from a collection of articles. Generally, each event contains multiple sub-events and the storyline should be composed by the component summaries of all the sub-events. However, different sub-events have different part-whole relationship with the major event, which is important to correspond to users’ interests but seldom considered in previous work. To distinguish different types of sub-events, we propose a mixture-event-aspect model which models different sub-events into local and global aspects. Combining these local/global aspects with summarization requirements together, we utilize an optimization method to generate the component summaries along the timeline. We develop experimental systems on 6 distinctively different datasets. Evaluation and comparison results indicate the effectiveness of our proposed method.</p><p>5 0.09643811 <a title="5-tfidf-5" href="./emnlp-2013-Single-Document_Summarization_as_a_Tree_Knapsack_Problem.html">174 emnlp-2013-Single-Document Summarization as a Tree Knapsack Problem</a></p>
<p>Author: Tsutomu Hirao ; Yasuhisa Yoshida ; Masaaki Nishino ; Norihito Yasuda ; Masaaki Nagata</p><p>Abstract: Recent studies on extractive text summarization formulate it as a combinatorial optimization problem such as a Knapsack Problem, a Maximum Coverage Problem or a Budgeted Median Problem. These methods successfully improved summarization quality, but they did not consider the rhetorical relations between the textual units of a source document. Thus, summaries generated by these methods may lack logical coherence. This paper proposes a single document summarization method based on the trimming of a discourse tree. This is a two-fold process. First, we propose rules for transforming a rhetorical structure theorybased discourse tree into a dependency-based discourse tree, which allows us to take a tree- . trimming approach to summarization. Second, we formulate the problem of trimming a dependency-based discourse tree as a Tree Knapsack Problem, then solve it with integer linear programming (ILP). Evaluation results showed that our method improved ROUGE scores.</p><p>6 0.09429422 <a title="5-tfidf-6" href="./emnlp-2013-Document_Summarization_via_Guided_Sentence_Compression.html">65 emnlp-2013-Document Summarization via Guided Sentence Compression</a></p>
<p>7 0.088414915 <a title="5-tfidf-7" href="./emnlp-2013-Modeling_Scientific_Impact_with_Topical_Influence_Regression.html">133 emnlp-2013-Modeling Scientific Impact with Topical Influence Regression</a></p>
<p>8 0.081823416 <a title="5-tfidf-8" href="./emnlp-2013-Automatic_Feature_Engineering_for_Answer_Selection_and_Extraction.html">31 emnlp-2013-Automatic Feature Engineering for Answer Selection and Extraction</a></p>
<p>9 0.078302547 <a title="5-tfidf-9" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>10 0.066971757 <a title="5-tfidf-10" href="./emnlp-2013-MCTest%3A_A_Challenge_Dataset_for_the_Open-Domain_Machine_Comprehension_of_Text.html">126 emnlp-2013-MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text</a></p>
<p>11 0.060363837 <a title="5-tfidf-11" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>12 0.060237955 <a title="5-tfidf-12" href="./emnlp-2013-Application_of_Localized_Similarity_for_Web_Documents.html">24 emnlp-2013-Application of Localized Similarity for Web Documents</a></p>
<p>13 0.058211811 <a title="5-tfidf-13" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<p>14 0.054616552 <a title="5-tfidf-14" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>15 0.050206121 <a title="5-tfidf-15" href="./emnlp-2013-Inducing_Document_Plans_for_Concept-to-Text_Generation.html">106 emnlp-2013-Inducing Document Plans for Concept-to-Text Generation</a></p>
<p>16 0.050166517 <a title="5-tfidf-16" href="./emnlp-2013-The_Answer_is_at_your_Fingertips%3A_Improving_Passage_Retrieval_for_Web_Question_Answering_with_Search_Behavior_Data.html">180 emnlp-2013-The Answer is at your Fingertips: Improving Passage Retrieval for Web Question Answering with Search Behavior Data</a></p>
<p>17 0.048942216 <a title="5-tfidf-17" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>18 0.047026012 <a title="5-tfidf-18" href="./emnlp-2013-Lexical_Chain_Based_Cohesion_Models_for_Document-Level_Statistical_Machine_Translation.html">125 emnlp-2013-Lexical Chain Based Cohesion Models for Document-Level Statistical Machine Translation</a></p>
<p>19 0.044410687 <a title="5-tfidf-19" href="./emnlp-2013-Question_Difficulty_Estimation_in_Community_Question_Answering_Services.html">155 emnlp-2013-Question Difficulty Estimation in Community Question Answering Services</a></p>
<p>20 0.038890511 <a title="5-tfidf-20" href="./emnlp-2013-Learning_Topics_and_Positions_from_Debatepedia.html">121 emnlp-2013-Learning Topics and Positions from Debatepedia</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.139), (1, 0.066), (2, -0.058), (3, 0.138), (4, -0.066), (5, -0.014), (6, 0.213), (7, -0.103), (8, 0.053), (9, -0.042), (10, 0.047), (11, 0.046), (12, -0.001), (13, -0.074), (14, 0.054), (15, 0.131), (16, 0.094), (17, 0.038), (18, -0.039), (19, 0.001), (20, -0.057), (21, -0.003), (22, -0.078), (23, 0.0), (24, 0.115), (25, -0.066), (26, 0.137), (27, -0.08), (28, 0.076), (29, -0.103), (30, 0.011), (31, 0.131), (32, 0.014), (33, -0.071), (34, -0.036), (35, -0.215), (36, -0.04), (37, -0.042), (38, 0.047), (39, -0.024), (40, 0.04), (41, 0.081), (42, 0.013), (43, -0.01), (44, -0.017), (45, -0.033), (46, -0.043), (47, -0.076), (48, 0.061), (49, -0.147)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95454544 <a title="5-lsi-1" href="./emnlp-2013-A_Discourse-Driven_Content_Model_for_Summarising_Scientific_Articles_Evaluated_in_a_Complex_Question_Answering_Task.html">5 emnlp-2013-A Discourse-Driven Content Model for Summarising Scientific Articles Evaluated in a Complex Question Answering Task</a></p>
<p>Author: Maria Liakata ; Simon Dobnik ; Shyamasree Saha ; Colin Batchelor ; Dietrich Rebholz-Schuhmann</p><p>Abstract: We present a method which exploits automatically generated scientific discourse annotations to create a content model for the summarisation of scientific articles. Full papers are first automatically annotated using the CoreSC scheme, which captures 11 contentbased concepts such as Hypothesis, Result, Conclusion etc at the sentence level. A content model which follows the sequence of CoreSC categories observed in abstracts is used to provide the skeleton of the summary, making a distinction between dependent and independent categories. Summary creation is also guided by the distribution of CoreSC categories found in the full articles, in order to adequately represent the article content. Fi- nally, we demonstrate the usefulness of the summaries by evaluating them in a complex question answering task. Results are very encouraging as summaries of papers from automatically obtained CoreSCs enable experts to answer 66% of complex content-related questions designed on the basis of paper abstracts. The questions were answered with a precision of 75%, where the upper bound for human summaries (abstracts) was 95%.</p><p>2 0.75749075 <a title="5-lsi-2" href="./emnlp-2013-Automatically_Determining_a_Proper_Length_for_Multi-Document_Summarization%3A_A_Bayesian_Nonparametric_Approach.html">36 emnlp-2013-Automatically Determining a Proper Length for Multi-Document Summarization: A Bayesian Nonparametric Approach</a></p>
<p>Author: Tengfei Ma ; Hiroshi Nakagawa</p><p>Abstract: Document summarization is an important task in the area of natural language processing, which aims to extract the most important information from a single document or a cluster of documents. In various summarization tasks, the summary length is manually defined. However, how to find the proper summary length is quite a problem; and keeping all summaries restricted to the same length is not always a good choice. It is obviously improper to generate summaries with the same length for two clusters of documents which contain quite different quantity of information. In this paper, we propose a Bayesian nonparametric model for multidocument summarization in order to automatically determine the proper lengths of summaries. Assuming that an original document can be reconstructed from its summary, we describe the ”reconstruction” by a Bayesian framework which selects sentences to form a good summary. Experimental results on DUC2004 data sets and some expanded data demonstrate the good quality of our summaries and the rationality of the length determination.</p><p>3 0.66246802 <a title="5-lsi-3" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>Author: Zhongqing Wang ; Shoushan LI ; Fang Kong ; Guodong Zhou</p><p>Abstract: Personal profile information on social media like LinkedIn.com and Facebook.com is at the core of many interesting applications, such as talent recommendation and contextual advertising. However, personal profiles usually lack organization confronted with the large amount of available information. Therefore, it is always a challenge for people to find desired information from them. In this paper, we address the task of personal profile summarization by leveraging both personal profile textual information and social networks. Here, using social networks is motivated by the intuition that, people with similar academic, business or social connections (e.g. co-major, co-university, and cocorporation) tend to have similar experience and summaries. To achieve the learning process, we propose a collective factor graph (CoFG) model to incorporate all these resources of knowledge to summarize personal profiles with local textual attribute functions and social connection factors. Extensive evaluation on a large-scale dataset from LinkedIn.com demonstrates the effectiveness of the proposed approach. 1</p><p>4 0.57576549 <a title="5-lsi-4" href="./emnlp-2013-Single-Document_Summarization_as_a_Tree_Knapsack_Problem.html">174 emnlp-2013-Single-Document Summarization as a Tree Knapsack Problem</a></p>
<p>Author: Tsutomu Hirao ; Yasuhisa Yoshida ; Masaaki Nishino ; Norihito Yasuda ; Masaaki Nagata</p><p>Abstract: Recent studies on extractive text summarization formulate it as a combinatorial optimization problem such as a Knapsack Problem, a Maximum Coverage Problem or a Budgeted Median Problem. These methods successfully improved summarization quality, but they did not consider the rhetorical relations between the textual units of a source document. Thus, summaries generated by these methods may lack logical coherence. This paper proposes a single document summarization method based on the trimming of a discourse tree. This is a two-fold process. First, we propose rules for transforming a rhetorical structure theorybased discourse tree into a dependency-based discourse tree, which allows us to take a tree- . trimming approach to summarization. Second, we formulate the problem of trimming a dependency-based discourse tree as a Tree Knapsack Problem, then solve it with integer linear programming (ILP). Evaluation results showed that our method improved ROUGE scores.</p><p>5 0.51611197 <a title="5-lsi-5" href="./emnlp-2013-Optimized_Event_Storyline_Generation_based_on_Mixture-Event-Aspect_Model.html">147 emnlp-2013-Optimized Event Storyline Generation based on Mixture-Event-Aspect Model</a></p>
<p>Author: Lifu Huang ; Lian'en Huang</p><p>Abstract: Recently, much research focuses on event storyline generation, which aims to produce a concise, global and temporal event summary from a collection of articles. Generally, each event contains multiple sub-events and the storyline should be composed by the component summaries of all the sub-events. However, different sub-events have different part-whole relationship with the major event, which is important to correspond to users’ interests but seldom considered in previous work. To distinguish different types of sub-events, we propose a mixture-event-aspect model which models different sub-events into local and global aspects. Combining these local/global aspects with summarization requirements together, we utilize an optimization method to generate the component summaries along the timeline. We develop experimental systems on 6 distinctively different datasets. Evaluation and comparison results indicate the effectiveness of our proposed method.</p><p>6 0.50575703 <a title="5-lsi-6" href="./emnlp-2013-Fast_Joint_Compression_and_Summarization_via_Graph_Cuts.html">85 emnlp-2013-Fast Joint Compression and Summarization via Graph Cuts</a></p>
<p>7 0.44334021 <a title="5-lsi-7" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>8 0.41310832 <a title="5-lsi-8" href="./emnlp-2013-Modeling_Scientific_Impact_with_Topical_Influence_Regression.html">133 emnlp-2013-Modeling Scientific Impact with Topical Influence Regression</a></p>
<p>9 0.4094393 <a title="5-lsi-9" href="./emnlp-2013-Application_of_Localized_Similarity_for_Web_Documents.html">24 emnlp-2013-Application of Localized Similarity for Web Documents</a></p>
<p>10 0.35370335 <a title="5-lsi-10" href="./emnlp-2013-Inducing_Document_Plans_for_Concept-to-Text_Generation.html">106 emnlp-2013-Inducing Document Plans for Concept-to-Text Generation</a></p>
<p>11 0.35025808 <a title="5-lsi-11" href="./emnlp-2013-MCTest%3A_A_Challenge_Dataset_for_the_Open-Domain_Machine_Comprehension_of_Text.html">126 emnlp-2013-MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text</a></p>
<p>12 0.31451142 <a title="5-lsi-12" href="./emnlp-2013-Document_Summarization_via_Guided_Sentence_Compression.html">65 emnlp-2013-Document Summarization via Guided Sentence Compression</a></p>
<p>13 0.3134186 <a title="5-lsi-13" href="./emnlp-2013-Automatic_Feature_Engineering_for_Answer_Selection_and_Extraction.html">31 emnlp-2013-Automatic Feature Engineering for Answer Selection and Extraction</a></p>
<p>14 0.29908583 <a title="5-lsi-14" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<p>15 0.28767461 <a title="5-lsi-15" href="./emnlp-2013-Using_Crowdsourcing_to_get_Representations_based_on_Regular_Expressions.html">196 emnlp-2013-Using Crowdsourcing to get Representations based on Regular Expressions</a></p>
<p>16 0.28442928 <a title="5-lsi-16" href="./emnlp-2013-Detecting_Promotional_Content_in_Wikipedia.html">61 emnlp-2013-Detecting Promotional Content in Wikipedia</a></p>
<p>17 0.27024424 <a title="5-lsi-17" href="./emnlp-2013-Automatically_Classifying_Edit_Categories_in_Wikipedia_Revisions.html">34 emnlp-2013-Automatically Classifying Edit Categories in Wikipedia Revisions</a></p>
<p>18 0.26978186 <a title="5-lsi-18" href="./emnlp-2013-The_Answer_is_at_your_Fingertips%3A_Improving_Passage_Retrieval_for_Web_Question_Answering_with_Search_Behavior_Data.html">180 emnlp-2013-The Answer is at your Fingertips: Improving Passage Retrieval for Web Question Answering with Search Behavior Data</a></p>
<p>19 0.26777947 <a title="5-lsi-19" href="./emnlp-2013-Assembling_the_Kazakh_Language_Corpus.html">26 emnlp-2013-Assembling the Kazakh Language Corpus</a></p>
<p>20 0.26035157 <a title="5-lsi-20" href="./emnlp-2013-Tree_Kernel-based_Negation_and_Speculation_Scope_Detection_with_Structured_Syntactic_Parse_Features.html">188 emnlp-2013-Tree Kernel-based Negation and Speculation Scope Detection with Structured Syntactic Parse Features</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.488), (9, 0.012), (18, 0.027), (22, 0.045), (30, 0.037), (50, 0.014), (51, 0.144), (66, 0.022), (71, 0.021), (75, 0.049), (95, 0.021), (96, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9259246 <a title="5-lda-1" href="./emnlp-2013-A_Generative_Joint%2C_Additive%2C_Sequential_Model_of_Topics_and_Speech_Acts_in_Patient-Doctor_Communication.html">6 emnlp-2013-A Generative Joint, Additive, Sequential Model of Topics and Speech Acts in Patient-Doctor Communication</a></p>
<p>Author: Byron C. Wallace ; Thomas A Trikalinos ; M. Barton Laws ; Ira B. Wilson ; Eugene Charniak</p><p>Abstract: We develop a novel generative model of conversation that jointly captures both the topical content and the speech act type associated with each utterance. Our model expresses both token emission and state transition probabilities as log-linear functions of separate components corresponding to topics and speech acts (and their interactions). We apply this model to a dataset comprising annotated patient-physician visits and show that the proposed joint approach outperforms a baseline univariate model.</p><p>same-paper 2 0.84267759 <a title="5-lda-2" href="./emnlp-2013-A_Discourse-Driven_Content_Model_for_Summarising_Scientific_Articles_Evaluated_in_a_Complex_Question_Answering_Task.html">5 emnlp-2013-A Discourse-Driven Content Model for Summarising Scientific Articles Evaluated in a Complex Question Answering Task</a></p>
<p>Author: Maria Liakata ; Simon Dobnik ; Shyamasree Saha ; Colin Batchelor ; Dietrich Rebholz-Schuhmann</p><p>Abstract: We present a method which exploits automatically generated scientific discourse annotations to create a content model for the summarisation of scientific articles. Full papers are first automatically annotated using the CoreSC scheme, which captures 11 contentbased concepts such as Hypothesis, Result, Conclusion etc at the sentence level. A content model which follows the sequence of CoreSC categories observed in abstracts is used to provide the skeleton of the summary, making a distinction between dependent and independent categories. Summary creation is also guided by the distribution of CoreSC categories found in the full articles, in order to adequately represent the article content. Fi- nally, we demonstrate the usefulness of the summaries by evaluating them in a complex question answering task. Results are very encouraging as summaries of papers from automatically obtained CoreSCs enable experts to answer 66% of complex content-related questions designed on the basis of paper abstracts. The questions were answered with a precision of 75%, where the upper bound for human summaries (abstracts) was 95%.</p><p>3 0.82208937 <a title="5-lda-3" href="./emnlp-2013-Detecting_Promotional_Content_in_Wikipedia.html">61 emnlp-2013-Detecting Promotional Content in Wikipedia</a></p>
<p>Author: Shruti Bhosale ; Heath Vinicombe ; Raymond Mooney</p><p>Abstract: This paper presents an approach for detecting promotional content in Wikipedia. By incorporating stylometric features, including features based on n-gram and PCFG language models, we demonstrate improved accuracy at identifying promotional articles, compared to using only lexical information and metafeatures.</p><p>4 0.79313892 <a title="5-lda-4" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>Author: Eduardo Blanco ; Dan Moldovan</p><p>Abstract: This paper presents a novel approach to determine textual similarity. A layered methodology to transform text into logic forms is proposed, and semantic features are derived from a logic prover. Experimental results show that incorporating the semantic structure of sentences is beneficial. When training data is unavailable, scores obtained from the logic prover in an unsupervised manner outperform supervised methods.</p><p>5 0.48036048 <a title="5-lda-5" href="./emnlp-2013-Automatically_Determining_a_Proper_Length_for_Multi-Document_Summarization%3A_A_Bayesian_Nonparametric_Approach.html">36 emnlp-2013-Automatically Determining a Proper Length for Multi-Document Summarization: A Bayesian Nonparametric Approach</a></p>
<p>Author: Tengfei Ma ; Hiroshi Nakagawa</p><p>Abstract: Document summarization is an important task in the area of natural language processing, which aims to extract the most important information from a single document or a cluster of documents. In various summarization tasks, the summary length is manually defined. However, how to find the proper summary length is quite a problem; and keeping all summaries restricted to the same length is not always a good choice. It is obviously improper to generate summaries with the same length for two clusters of documents which contain quite different quantity of information. In this paper, we propose a Bayesian nonparametric model for multidocument summarization in order to automatically determine the proper lengths of summaries. Assuming that an original document can be reconstructed from its summary, we describe the ”reconstruction” by a Bayesian framework which selects sentences to form a good summary. Experimental results on DUC2004 data sets and some expanded data demonstrate the good quality of our summaries and the rationality of the length determination.</p><p>6 0.47176513 <a title="5-lda-6" href="./emnlp-2013-Modeling_Scientific_Impact_with_Topical_Influence_Regression.html">133 emnlp-2013-Modeling Scientific Impact with Topical Influence Regression</a></p>
<p>7 0.46159175 <a title="5-lda-7" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>8 0.45911354 <a title="5-lda-8" href="./emnlp-2013-Single-Document_Summarization_as_a_Tree_Knapsack_Problem.html">174 emnlp-2013-Single-Document Summarization as a Tree Knapsack Problem</a></p>
<p>9 0.45737806 <a title="5-lda-9" href="./emnlp-2013-Automatically_Classifying_Edit_Categories_in_Wikipedia_Revisions.html">34 emnlp-2013-Automatically Classifying Edit Categories in Wikipedia Revisions</a></p>
<p>10 0.44733205 <a title="5-lda-10" href="./emnlp-2013-Predicting_the_Resolution_of_Referring_Expressions_from_User_Behavior.html">153 emnlp-2013-Predicting the Resolution of Referring Expressions from User Behavior</a></p>
<p>11 0.44621897 <a title="5-lda-11" href="./emnlp-2013-Elephant%3A_Sequence_Labeling_for_Word_and_Sentence_Segmentation.html">72 emnlp-2013-Elephant: Sequence Labeling for Word and Sentence Segmentation</a></p>
<p>12 0.43599349 <a title="5-lda-12" href="./emnlp-2013-Word_Level_Language_Identification_in_Online_Multilingual_Communication.html">204 emnlp-2013-Word Level Language Identification in Online Multilingual Communication</a></p>
<p>13 0.43404338 <a title="5-lda-13" href="./emnlp-2013-Document_Summarization_via_Guided_Sentence_Compression.html">65 emnlp-2013-Document Summarization via Guided Sentence Compression</a></p>
<p>14 0.43229043 <a title="5-lda-14" href="./emnlp-2013-Feature_Noising_for_Log-Linear_Structured_Prediction.html">86 emnlp-2013-Feature Noising for Log-Linear Structured Prediction</a></p>
<p>15 0.42837036 <a title="5-lda-15" href="./emnlp-2013-Opinion_Mining_in_Newspaper_Articles_by_Entropy-Based_Word_Connections.html">144 emnlp-2013-Opinion Mining in Newspaper Articles by Entropy-Based Word Connections</a></p>
<p>16 0.42772424 <a title="5-lda-16" href="./emnlp-2013-Inducing_Document_Plans_for_Concept-to-Text_Generation.html">106 emnlp-2013-Inducing Document Plans for Concept-to-Text Generation</a></p>
<p>17 0.42364955 <a title="5-lda-17" href="./emnlp-2013-Using_Topic_Modeling_to_Improve_Prediction_of_Neuroticism_and_Depression_in_College_Students.html">199 emnlp-2013-Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students</a></p>
<p>18 0.42166594 <a title="5-lda-18" href="./emnlp-2013-Using_Crowdsourcing_to_get_Representations_based_on_Regular_Expressions.html">196 emnlp-2013-Using Crowdsourcing to get Representations based on Regular Expressions</a></p>
<p>19 0.42151606 <a title="5-lda-19" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>20 0.42018414 <a title="5-lda-20" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
