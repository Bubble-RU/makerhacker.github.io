<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-177" href="#">emnlp2013-177</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</h1>
<br/><p>Source: <a title="emnlp-2013-177-pdf" href="http://aclweb.org/anthology//D/D13/D13-1015.pdf">pdf</a></p><p>Author: Eva Maria Vecchi ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: In this study, we use compositional distributional semantic methods to investigate restrictions in adjective ordering. Specifically, we focus on properties distinguishing AdjectiveAdjective-Noun phrases in which there is flexibility in the adjective ordering from those bound to a rigid order. We explore a number of measures extracted from the distributional representation of AAN phrases which may indicate a word order restriction. We find that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modification of the adjectives. Our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics.</p><p>Reference: <a title="emnlp-2013-177-reference" href="../emnlp2013_reference/emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Studying the recursive behaviour of adjectival modification with compositional distributional semantics Eva Maria Vecchi and Roberto Zamparelli and Marco Baroni Center for Mind/Brain Sciences (University of Trento, Italy) ( evamari a . [sent-1, score-0.393]
</p><p>2 it l|  Abstract In this study, we use compositional distributional semantic methods to investigate restrictions in adjective ordering. [sent-5, score-0.435]
</p><p>3 Specifically, we focus on properties distinguishing AdjectiveAdjective-Noun phrases in which there is flexibility in the adjective ordering from those bound to a rigid order. [sent-6, score-0.761]
</p><p>4 We explore a number of measures extracted from the distributional representation of AAN phrases which may indicate a word order restriction. [sent-7, score-0.197]
</p><p>5 Our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics. [sent-9, score-0.357]
</p><p>6 All of these approaches construct distributional representations for novel phrases starting from the corpusderived vectors for their lexical constituents and exploiting the geometric quality of the representation. [sent-15, score-0.269]
</p><p>7 Once we start seriously looking into recursive modification, however, the issue of modifier ordering restrictions naturally arises. [sent-24, score-0.167]
</p><p>8 new creative idea parsed as [new & creative] idea, rather than [new [creative idea]]), or to semantic category ambiguity for any adjective which appears in different orders (see Cinque (2004) for discussion). [sent-32, score-0.327]
</p><p>9 In this study, we show that the existence of both rigid and flexible order cases is robustly attested at least for adjectival modification, and that flexible ordering is unlikely to reduce to idioms, coordination or ambiguity. [sent-33, score-0.988]
</p><p>10 In the former case, we find that the same distributional semantic cues discriminate between correct and wrong orders. [sent-39, score-0.183]
</p><p>11 To achieve these goals, we consider various properties of the distributional representation of AANs (both corpus-extracted and compositionallyderived), and explore their correlation with restrictions in adjective ordering. [sent-40, score-0.34]
</p><p>12 We conclude that measures that quantify the degree to which the modifiers  have an impact on the distributional meaning of the AAN can be good predictors of ordering restrictions in AANs. [sent-41, score-0.28]
</p><p>13 1 Semantic space Our initial step was to construct a semantic space for our experiments, consisting of a matrix where each row represents the meaning of an adjective, noun, AN or AAN as a distributional vector, each column a semantic dimension ofmeaning. [sent-43, score-0.291]
</p><p>14 We first introduce the source corpus, then the vocabulary of words and phrases that we represent in the space, and finally the procedure adopted to build the vectors representing the vocabulary items from corpus statistics, and obtain the semantic space matrix. [sent-44, score-0.206]
</p><p>15 We work here with a traditional, window-based semantic space, since our focus is on the effect of different composition methods given a common semantic space. [sent-45, score-0.2]
</p><p>16 In addition, Blacoe and Lapata (2012) found that a vanilla space of this sort performed best in their composition experiments, when compared to a syntax-aware space and to neural language model vectors such as those  used for composition by Socher et al. [sent-46, score-0.343]
</p><p>17 Semantic space vocabulary The words/phrases in the semantic space must of course include the items that we need for our experiments (adjectives, nouns, ANs and AANs used for model training, as input to composition and for evaluation). [sent-52, score-0.196]
</p><p>18 Therefore, we first populate our semantic space with a core vocabulary containing the 8K most frequent nouns and the 4K most frequent adjectives from the corpus. [sent-53, score-0.237]
</p><p>19 The ANs included in the semantic space are composed of adjectives with very high frequency in the corpus so that they are generally able to combine with many classes of nouns. [sent-54, score-0.234]
</p><p>20 They are composed  of the 700 most frequent adjectives and 4K most frequent nouns in the corpus, which were manually 1http : / /wacky . [sent-55, score-0.189]
</p><p>21 Finally, we created a set of AAN phrases composed of the adjectives and nouns used to generate the ANs. [sent-67, score-0.225]
</p><p>22 , where the two adjectives ap–  –  pear separated by comma, and, or or; this addresses the objection that a flexible order AAN might be a hidden A(&)A conjunction: we would expect that such a conjunction should also appear overtly elsewhere). [sent-70, score-0.365]
</p><p>23 The set of AANs thus generated is then divided into two types of adjective ordering: 1. [sent-71, score-0.174]
</p><p>24 The preserved set resulted in 1,438 AANs: 621 flexible order and 817 rigid order. [sent-76, score-0.612]
</p><p>25 Note that there are almost as many flexible as rigid order cases; this speaks against the idea that free order is a marginal phenomenon, due to occasional ambiguities that reassign the adjective to a different semantic class. [sent-77, score-0.891]
</p><p>26 001) between cosines of corpus-extracted or model-generated AN vectors and phrase similarity ratings collected in Mitchell and Lapata (2010), as well as best reported results from Mitchell & Lapata (M&L;). [sent-92, score-0.195]
</p><p>27 Corpus-extracted vectors (corp) were computed for the ANs and for the flexible order and attested rigid order AANs, and then mapped onto the 300dimension NMF-reduced semantic space. [sent-98, score-0.907]
</p><p>28 As a sanity check, the first row of Table 1 reports the correlation between the AN phrase similarity ratings collected in Mitchell and Lapata (2010) and the cosines of corpus-extracted vectors in our space, for the same ANs. [sent-99, score-0.195]
</p><p>29 ~p  p~ = W1~ ax  + W2(W1 ~ay + W2~ n)  (3)  = W1~ ax + W2W1 a~y + W22 n~ Finally, we consider the lexical function model (LFM), first introduced in Baroni and Zamparelli (2010), in which attributive adjectives are treated as functions from noun meanings to noun meanings. [sent-119, score-0.292]
</p><p>30 This is a standard approach in Montague semantics (Thomason, 1974), except noun meanings here are distributional vectors, not denotations, and adjectives are (linear) functions learned from a large corpus. [sent-120, score-0.309]
</p><p>31 Given a weight matrix, A, for each adjective in the phrase, we apply the functions in sequence recursively as shown in Equation (4). [sent-124, score-0.205]
</p><p>32 3 Measures of adjective ordering Our general goal is to determine which linguistically-motivated factors distinguish the two types of adjective ordering. [sent-135, score-0.429]
</p><p>33 We hypothesize that in cases of flexible order, the two adjectives will have a similarly strong effect on the noun, thus transforming the meaning of the noun equivalently in the direction of both adjectives and component ANs. [sent-136, score-0.562]
</p><p>34 For example, in the phrase creative new idea, the idea is both new and creative, so we would expect a similar impact of modification by both adjectives. [sent-137, score-0.186]
</p><p>35 On the other hand, we predict that in rigid order cases, one adjective, the one closer to the noun, will  dominate the meaning of the phrase, distorting the meaning of the noun by a significant amount. [sent-138, score-0.637]
</p><p>36 First, we examine how the similarity of an AAN to its component adjectives affects the ordering, using the cosine between the AxAyN vector and each of the component A vectors as an expression of similarity (we abbreviate this as cosAx and cosAy for the first and second adjective, respectively). [sent-144, score-0.393]
</p><p>37 5 Our hypothesis predicts that flexible order AANs should remain similarly close to both component As, while rigid order AANs should remain systematically closer to their Ay than to their Ax. [sent-145, score-0.756]
</p><p>38 This measure is aimed at verifying if the degree to which the meaning of the head noun is distorted could be a property that distinguishes the two types of adjective ordering. [sent-147, score-0.272]
</p><p>39 Again, vectors for flexible order AANs should remain closer to their component nouns in the semantic space, while rigid order AANs should distort the meaning of the head noun more notably. [sent-148, score-1.008]
</p><p>40 We also inspect how the similarity of the AAN to its component AN vectors affects the type of adjective ordering (cosAxN and cosAyN). [sent-149, score-0.412]
</p><p>41 Considering the examples above, we predict that the flexible order AAN creative new idea will share many properties with both creative idea and new idea, as represented in our semantic space, while rigid order AANs, like different architectural style, should remain quite similar to the AyN, i. [sent-150, score-0.915]
</p><p>42 Based on our hypothesis described for the other measures, we expect the association in the cor-  pus of AyN to be much greater than AxN for rigid order AANs, resulting in a large negative ∆PMI values. [sent-156, score-0.48]
</p><p>43 While flexible order AANs should have similar 5In the case of LFM, we compare the similarity of the AAN with the AN centroids for each adjective, since the model does not make use of A vectors (Baroni and Zamparelli, 2010). [sent-157, score-0.305]
</p><p>44 145 association strengths for both AxN and AyN, thus we expect ∆PMI to be closer to 0 than for rigid order AANs. [sent-158, score-0.509]
</p><p>45 4  Gold standard  To our knowledge, this is the first study to use distributional representations of recursive modification; therefore we must first determine if the composed AAN vector representations are semantically coherent objects. [sent-160, score-0.264]
</p><p>46 Thus, for vector analysis, a gold standard of 320 corpus-extracted AAN vectors were selected and their quality was established by inspecting their nearest neighbors. [sent-161, score-0.187]
</p><p>47 First, we gathered a randomly selected set of 600 corpus-extracted AANs, containing 300 flexible order and 300 attested rigid order AANs. [sent-164, score-0.76]
</p><p>48 We then extracted the top 3 nearest neighbors to the corpusextracted AAN vectors as represented in the semantic space7. [sent-165, score-0.304]
</p><p>49 The rationale was that if we obtained a good distributional representation of the AAN, its nearest neighbors should be closely related words and phrases. [sent-168, score-0.196]
</p><p>50 The final set for the gold standard contains the 320 AANs (152 flexible order and 168 attested rigid order) which had a relatedness score over the mediansplit (3. [sent-170, score-0.734]
</p><p>51 standard, both flexible order (left column) and rigid order  (right column) AANs. [sent-181, score-0.667]
</p><p>52 For example, the nearest neighbors to the corpus-extracted vectors for medieval old town and rapid social change include phrases which describe quite complex associations, cf. [sent-183, score-0.301]
</p><p>53 In addition, we find that the nearest neighbors for flexible order AAN vectors are not necessarily the same for both adjective orders, as seen in the difference in neighbors of national daily newspaper and daily national newspaper. [sent-185, score-0.618]
</p><p>54 1 Quality of model-generated AAN vectors Our nearest neighbor analysis suggests that the corpus-extracted AAN vectors in the gold standard are meaningful, semantically coherent objects. [sent-188, score-0.264]
</p><p>55 For MULT and LFM, the difference between mean flexible order (FO) and rigid order (RO) cosines is also significant. [sent-206, score-0.765]
</p><p>56 We find that the performances of most composition models in approximating the vectors for the gold AANs is quite satisfactory (cf. [sent-208, score-0.226]
</p><p>57 Further, the results show that the models are able to approximate flexible order AAN vectors better than rigid order AANs, significantly so for LFM and MULT. [sent-215, score-0.764]
</p><p>58 This result is quite interesting because it suggests that flexible order AANs express a more literal (or intersective) modification by both adjectives, which is what we would expect to be better captured by compositional models. [sent-216, score-0.385]
</p><p>59 Clearly, a more complex modification process is occurring in the case of rigid order AANs, as we predicted to be the case. [sent-217, score-0.543]
</p><p>60 rigid order In the results reported below, we test how both our baseline ∆PMI measure and the distance from the AAN and its component parts changes depending on the type of adjective ordering to which the AAN belongs. [sent-220, score-0.774]
</p><p>61 The first block of Table 4 reports the t-normalized difference between  flexible order and rigid order mean cosines for the corpus-extracted vectors. [sent-222, score-0.765]
</p><p>62 t-normalized differences between flexible order (FO) and rigid order (FO) mean cosines (or mean ∆PMI values) for corpusextracted and model-generated vectors. [sent-254, score-0.852]
</p><p>63 05 after Bonferroni correction), the last column reports whether mean cosine (or ∆PMI) is larger for flexible order (FO) or rigid order (RO) class. [sent-256, score-0.707]
</p><p>64 147 In particular, rigid order AxAyNs are heavily modified by Ay, distorting the meaning of the head noun in the direction of the closest adjective quite drastically, and only undergoing a slight modification when the Ax is added. [sent-261, score-0.827]
</p><p>65 In other words, in rigid order phrases, for example rapid social change, the AyN expresses a single concept (probably a “kind”, in the terminology of formal semantics), strongly related to social, social change, which is then modified by the Ax. [sent-262, score-0.533]
</p><p>66 On the other hand, flexible order AANs maintain the semantic value of the head noun while being modified only slightly by both adjectives, almost equivalently. [sent-264, score-0.296]
</p><p>67 Most importantly, the corpus-extracted distributional representations are able to model this phenomenon inherently and can significantly distin-  guish the two adjective orders. [sent-266, score-0.31]
</p><p>68 It is worth remarking that MULT approximated the patterns observed in the corpus vectors quite well, despite producing order-insensitive representations of recursive structures. [sent-269, score-0.18]
</p><p>69 For flexible order AANs, order is indeed only slightly affecting the meaning, so it stands to reason that MULT has no problems modeling this class. [sent-270, score-0.263]
</p><p>70 For rigid order AANs, where we consider here the attested-order only, evidently the order-insensitive MULT representation is sufficient to capture their relations to their constituents. [sent-271, score-0.459]
</p><p>71 Moreover, ∆PMI does not produce a semantic representation of the phrase (see how composed distributional vectors approximate of high quality AAN vectors in Table 3). [sent-275, score-0.395]
</p><p>72 Finally, this measure will not scale up to cases where the ANs are not attested, whereas measures based on composition only need corpus-harvested representations of adjectives and nouns. [sent-276, score-0.266]
</p><p>73 We expect that the fundamental property that distinguishes the orders is again found in the degree of modification of both component adjectives. [sent-279, score-0.228]
</p><p>74 We predict that the single concept created by the AyN in attested-order rigid AANs, such as legal status in formal legal status, is an effect of the modification strength of the Ay on the head noun, and when seen in the incorrect ordering, i. [sent-280, score-0.542]
</p><p>75 legal formal status, the strong modification of legal will still dominate the meaning of the AAN. [sent-283, score-0.177]
</p><p>76 Composition models should be able to capture this effect based on the distance from both the component adjectives and ANs. [sent-284, score-0.196]
</p><p>77 Clearly, we cannot run these analyses on corpusextracted vectors since the unattested order, by definition, is not seen in our corpus, and therefore we cannot collect co-occurrence statistics for the AAN phrase. [sent-285, score-0.264]
</p><p>78 Thus, we test our measures of adjective ordering on the model-generated AAN vectors, for all gold rigid order AANs in both orders. [sent-286, score-0.743]
</p><p>79 We also consider the ∆PMI measure which was so effective in distinguishing flexible vs. [sent-287, score-0.192]
</p><p>80 t-normalized mean paired cosine (or ∆PMI) differences between attested (A) and unattested (U) AANs with their components. [sent-316, score-0.233]
</p><p>81 Across all composition models, we find that the distance between the model-generated AAN and its component adjectives, Ax and Ay, are significant indicators of attested vs. [sent-319, score-0.253]
</p><p>82 Specifically, we find that rigid order AANs in the correct order are closest to their Ay, while we can detect the unattested order when the rigid order AAN is closer to its Ax. [sent-322, score-1.157]
</p><p>83 This finding is quite interesting, since it shows that the order in which the composition functions are applied does not alter the fact that the modification of one adjective in rigid order AANs (the Ay in the case of attested-order rigid order AANs) is much stronger  than the other. [sent-323, score-1.331]
</p><p>84 Unlike the measures that differentiated flexible and rigid order AANs, here we see that the distance from the component N is not an indicator of the correct adjective ordering (trivially so for MULT, where attested and unattested AANs are identical). [sent-324, score-1.12]
</p><p>85 rigid order AANs, is the strongest indicator of correct vs wrong adjective ordering. [sent-332, score-0.633]
</p><p>86 This measure confirms that the association of one adjective (the Ay in attested-order AANs) with the head noun is indeed the most significant factor distinguishing these two classes. [sent-333, score-0.251]
</p><p>87 4  Conclusion  While AN constructions have been extensively studied within the framework of compositional distributional semantics (Baroni and Zamparelli, 2010; Boleda et al. [sent-335, score-0.207]
</p><p>88 First, we showed that composition functions applied recursively can approximate corpus-extracted AAN vectors that we know to be of high semantic quality. [sent-339, score-0.278]
</p><p>89 Next, we looked at some properties of the same high-quality corpus-extracted AAN vectors, finding that the distinction between “flexible” AANs, where the adjective order can be flipped, and “rigid” ones, where the order is fixed, is reflected in distributional cues. [sent-340, score-0.417]
</p><p>90 These results all derive from the intuition that the most embedded adjective in a rigid AAN has a very strong effect on the distributional semantic representation of the AAN. [sent-341, score-0.734]
</p><p>91 The ability to detect ordering restrictions could also help Natural Language Generation tasks (Malouf, 2000), especially for the generation of unattested combinations of As and Ns. [sent-353, score-0.214]
</p><p>92 From a theoretical point of view, we would like to extend our analysis to adjective coordination (what’s the difference between new and creative idea and new creative idea? [sent-354, score-0.296]
</p><p>93 Additionally, we could go more granular, looking at whether compositional models can help us to understand why certain classes of adjectives are more likely to precede or follow others (why is size more likely to take scope over color, so that big red car sounds more natural than red big car? [sent-356, score-0.285]
</p><p>94 ) or studying the behaviour of specific adjectives (can our approach capture the fact that strong alcoholic drink is preferable to alcoholic strong drink because strong pertains to the alcoholic properties of the drink? [sent-357, score-0.312]
</p><p>95 In the meantime, we hope that the results we reported here provide convincing evidence of the usefulness of compositional distributional semantics in tackling topics, such as recursive adjectival modification, that have been of traditional interest to theoretical linguists from a new perspective. [sent-359, score-0.309]
</p><p>96 Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. [sent-365, score-0.186]
</p><p>97 Intensionality was only alleged: On adjective-noun composition in distributional se150 mantics. [sent-383, score-0.206]
</p><p>98 General estimation and evaluation of compositional distributional semantic models. [sent-419, score-0.228]
</p><p>99 Experimental support for a categorical compositional distributional model of meaning. [sent-427, score-0.178]
</p><p>100 The order of prenominal adjectives in natural language generation. [sent-439, score-0.191]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('aans', 0.58), ('rigid', 0.404), ('aan', 0.283), ('adjective', 0.174), ('flexible', 0.153), ('adjectives', 0.136), ('lfm', 0.134), ('cosaxn', 0.112), ('cosay', 0.112), ('cosayn', 0.112), ('cosn', 0.112), ('ay', 0.109), ('distributional', 0.106), ('unattested', 0.1), ('composition', 0.1), ('vectors', 0.097), ('axn', 0.097), ('pmi', 0.096), ('attested', 0.093), ('ayn', 0.089), ('modification', 0.084), ('mult', 0.081), ('ordering', 0.081), ('axayn', 0.078), ('cosines', 0.078), ('compositional', 0.072), ('ro', 0.069), ('fo', 0.069), ('corpusextracted', 0.067), ('ans', 0.061), ('creative', 0.061), ('component', 0.06), ('axayns', 0.056), ('cinque', 0.056), ('order', 0.055), ('boleda', 0.053), ('recursive', 0.053), ('baroni', 0.051), ('semantic', 0.05), ('adjectival', 0.049), ('neighbors', 0.049), ('architectural', 0.049), ('guglielmo', 0.045), ('zamparelli', 0.044), ('vecchi', 0.044), ('orders', 0.042), ('nearest', 0.041), ('ax', 0.04), ('meaning', 0.039), ('distinguishing', 0.039), ('noun', 0.038), ('phrases', 0.036), ('compositionality', 0.035), ('marco', 0.034), ('alcoholic', 0.033), ('distorting', 0.033), ('guevara', 0.033), ('restrictions', 0.033), ('lapata', 0.032), ('recursively', 0.031), ('car', 0.031), ('representations', 0.03), ('corp', 0.029), ('semantics', 0.029), ('gold', 0.029), ('closer', 0.029), ('mitchell', 0.029), ('nouns', 0.028), ('rapid', 0.028), ('cues', 0.027), ('change', 0.027), ('legal', 0.027), ('dinu', 0.027), ('properties', 0.027), ('nghia', 0.027), ('drink', 0.025), ('composed', 0.025), ('zanzotto', 0.023), ('stacked', 0.023), ('social', 0.023), ('red', 0.023), ('space', 0.023), ('ayaxn', 0.022), ('benor', 0.022), ('carthography', 0.022), ('cartographic', 0.022), ('munro', 0.022), ('steddy', 0.022), ('thomason', 0.022), ('multiplicative', 0.022), ('expect', 0.021), ('idioms', 0.021), ('east', 0.021), ('style', 0.021), ('degree', 0.021), ('vector', 0.02), ('cosine', 0.02), ('mean', 0.02), ('socher', 0.02), ('phrase', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="177-tfidf-1" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>Author: Eva Maria Vecchi ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: In this study, we use compositional distributional semantic methods to investigate restrictions in adjective ordering. Specifically, we focus on properties distinguishing AdjectiveAdjective-Noun phrases in which there is flexibility in the adjective ordering from those bound to a rigid order. We explore a number of measures extracted from the distributional representation of AAN phrases which may indicate a word order restriction. We find that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modification of the adjectives. Our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics.</p><p>2 0.15840623 <a title="177-tfidf-2" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>Author: Angeliki Lazaridou ; Eva Maria Vecchi ; Marco Baroni</p><p>Abstract: In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the correct parsing of NPs (e.g., (live fish) transporter rather than live (fish transporter)). We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features.</p><p>3 0.099548489 <a title="177-tfidf-3" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>Author: Polina Kuznetsova ; Jianfu Chen ; Yejin Choi</p><p>Abstract: Why do certain combinations of words such as “disadvantageous peace ” or “metal to the petal” appeal to our minds as interesting expressions with a sense of creativity, while other phrases such as “quiet teenager”, or “geometrical base ” not as much? We present statistical explorations to understand the characteristics of lexical compositions that give rise to the perception of being original, interesting, and at times even artistic. We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learning that give us further insights on how different aspects of lexical composition collectively contribute to the perceived creativity.</p><p>4 0.09825249 <a title="177-tfidf-4" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>Author: Dimitri Kartsaklis ; Mehrnoosh Sadrzadeh</p><p>Abstract: Recent work has shown that compositionaldistributional models using element-wise operations on contextual word vectors benefit from the introduction of a prior disambiguation step. The purpose of this paper is to generalise these ideas to tensor-based models, where relational words such as verbs and adjectives are represented by linear maps (higher order tensors) acting on a number of arguments (vectors). We propose disambiguation algorithms for a number of tensor-based models, which we then test on a variety of tasks. The results show that disambiguation can provide better compositional representation even for the case of tensor-based models. Further- more, we confirm previous findings regarding the positive effect of disambiguation on vector mixture models, and we compare the effectiveness of the two approaches.</p><p>5 0.092876434 <a title="177-tfidf-5" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>Author: Douwe Kiela ; Stephen Clark</p><p>Abstract: We present a novel unsupervised approach to detecting the compositionality of multi-word expressions. We compute the compositionality of a phrase through substituting the constituent words with their “neighbours” in a semantic vector space and averaging over the distance between the original phrase and the substituted neighbour phrases. Several methods of obtaining neighbours are presented. The results are compared to existing supervised results and achieve state-of-the-art performance on a verb-object dataset of human compositionality ratings.</p><p>6 0.091018341 <a title="177-tfidf-6" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>7 0.075965449 <a title="177-tfidf-7" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<p>8 0.075717106 <a title="177-tfidf-8" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>9 0.070750505 <a title="177-tfidf-9" href="./emnlp-2013-Deriving_Adjectival_Scales_from_Continuous_Space_Word_Representations.html">59 emnlp-2013-Deriving Adjectival Scales from Continuous Space Word Representations</a></p>
<p>10 0.058814097 <a title="177-tfidf-10" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>11 0.052519713 <a title="177-tfidf-11" href="./emnlp-2013-Authorship_Attribution_of_Micro-Messages.html">27 emnlp-2013-Authorship Attribution of Micro-Messages</a></p>
<p>12 0.04997706 <a title="177-tfidf-12" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>13 0.044784103 <a title="177-tfidf-13" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>14 0.042212866 <a title="177-tfidf-14" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>15 0.038186383 <a title="177-tfidf-15" href="./emnlp-2013-Scaling_to_Large3_Data%3A_An_Efficient_and_Effective_Method_to_Compute_Distributional_Thesauri.html">165 emnlp-2013-Scaling to Large3 Data: An Efficient and Effective Method to Compute Distributional Thesauri</a></p>
<p>16 0.037051857 <a title="177-tfidf-16" href="./emnlp-2013-Identifying_Phrasal_Verbs_Using_Many_Bilingual_Corpora.html">96 emnlp-2013-Identifying Phrasal Verbs Using Many Bilingual Corpora</a></p>
<p>17 0.036536828 <a title="177-tfidf-17" href="./emnlp-2013-Gender_Inference_of_Twitter_Users_in_Non-English_Contexts.html">89 emnlp-2013-Gender Inference of Twitter Users in Non-English Contexts</a></p>
<p>18 0.036271524 <a title="177-tfidf-18" href="./emnlp-2013-Learning_Distributions_over_Logical_Forms_for_Referring_Expression_Generation.html">119 emnlp-2013-Learning Distributions over Logical Forms for Referring Expression Generation</a></p>
<p>19 0.036076747 <a title="177-tfidf-19" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>20 0.035462812 <a title="177-tfidf-20" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.129), (1, 0.018), (2, -0.054), (3, -0.043), (4, 0.008), (5, 0.148), (6, -0.046), (7, -0.108), (8, -0.138), (9, -0.03), (10, 0.051), (11, 0.124), (12, -0.062), (13, 0.048), (14, 0.015), (15, -0.082), (16, 0.069), (17, -0.133), (18, -0.114), (19, -0.002), (20, -0.077), (21, 0.046), (22, -0.033), (23, -0.021), (24, 0.04), (25, -0.062), (26, -0.057), (27, 0.033), (28, -0.102), (29, -0.077), (30, -0.121), (31, -0.056), (32, 0.062), (33, 0.122), (34, -0.05), (35, -0.036), (36, 0.114), (37, -0.074), (38, 0.036), (39, 0.045), (40, 0.088), (41, 0.108), (42, 0.079), (43, 0.043), (44, -0.042), (45, 0.067), (46, 0.002), (47, -0.037), (48, -0.038), (49, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94914299 <a title="177-lsi-1" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>Author: Eva Maria Vecchi ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: In this study, we use compositional distributional semantic methods to investigate restrictions in adjective ordering. Specifically, we focus on properties distinguishing AdjectiveAdjective-Noun phrases in which there is flexibility in the adjective ordering from those bound to a rigid order. We explore a number of measures extracted from the distributional representation of AAN phrases which may indicate a word order restriction. We find that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modification of the adjectives. Our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics.</p><p>2 0.87848991 <a title="177-lsi-2" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>Author: Angeliki Lazaridou ; Eva Maria Vecchi ; Marco Baroni</p><p>Abstract: In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the correct parsing of NPs (e.g., (live fish) transporter rather than live (fish transporter)). We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features.</p><p>3 0.76563346 <a title="177-lsi-3" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>Author: Polina Kuznetsova ; Jianfu Chen ; Yejin Choi</p><p>Abstract: Why do certain combinations of words such as “disadvantageous peace ” or “metal to the petal” appeal to our minds as interesting expressions with a sense of creativity, while other phrases such as “quiet teenager”, or “geometrical base ” not as much? We present statistical explorations to understand the characteristics of lexical compositions that give rise to the perception of being original, interesting, and at times even artistic. We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learning that give us further insights on how different aspects of lexical composition collectively contribute to the perceived creativity.</p><p>4 0.70198041 <a title="177-lsi-4" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<p>Author: Om P. Damani ; Shweta Ghonge</p><p>Abstract: Two recent measures incorporate the notion of statistical significance in basic PMI formulation. In some tasks, we find that the new measures perform worse than the PMI. Our analysis shows that while the basic ideas in incorporating statistical significance in PMI are reasonable, they have been applied slightly inappropriately. By fixing this, we get new measures that improve performance over not just PMI but on other popular co-occurrence measures as well. In fact, the revised measures perform reasonably well compared with more resource intensive non co-occurrence based methods also.</p><p>5 0.60753655 <a title="177-lsi-5" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>Author: Douwe Kiela ; Stephen Clark</p><p>Abstract: We present a novel unsupervised approach to detecting the compositionality of multi-word expressions. We compute the compositionality of a phrase through substituting the constituent words with their “neighbours” in a semantic vector space and averaging over the distance between the original phrase and the substituted neighbour phrases. Several methods of obtaining neighbours are presented. The results are compared to existing supervised results and achieve state-of-the-art performance on a verb-object dataset of human compositionality ratings.</p><p>6 0.58334583 <a title="177-lsi-6" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>7 0.45809147 <a title="177-lsi-7" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>8 0.3995932 <a title="177-lsi-8" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>9 0.37889218 <a title="177-lsi-9" href="./emnlp-2013-Scaling_to_Large3_Data%3A_An_Efficient_and_Effective_Method_to_Compute_Distributional_Thesauri.html">165 emnlp-2013-Scaling to Large3 Data: An Efficient and Effective Method to Compute Distributional Thesauri</a></p>
<p>10 0.35276002 <a title="177-lsi-10" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>11 0.32970929 <a title="177-lsi-11" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>12 0.30384913 <a title="177-lsi-12" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>13 0.3008374 <a title="177-lsi-13" href="./emnlp-2013-Deriving_Adjectival_Scales_from_Continuous_Space_Word_Representations.html">59 emnlp-2013-Deriving Adjectival Scales from Continuous Space Word Representations</a></p>
<p>14 0.27317017 <a title="177-lsi-14" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>15 0.24648695 <a title="177-lsi-15" href="./emnlp-2013-Unsupervised_Induction_of_Contingent_Event_Pairs_from_Film_Scenes.html">192 emnlp-2013-Unsupervised Induction of Contingent Event Pairs from Film Scenes</a></p>
<p>16 0.24616583 <a title="177-lsi-16" href="./emnlp-2013-Authorship_Attribution_of_Micro-Messages.html">27 emnlp-2013-Authorship Attribution of Micro-Messages</a></p>
<p>17 0.23733787 <a title="177-lsi-17" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>18 0.23530047 <a title="177-lsi-18" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>19 0.23395619 <a title="177-lsi-19" href="./emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations.html">17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</a></p>
<p>20 0.23123774 <a title="177-lsi-20" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.03), (6, 0.055), (18, 0.023), (22, 0.041), (30, 0.05), (50, 0.021), (51, 0.136), (64, 0.024), (66, 0.026), (71, 0.018), (75, 0.027), (77, 0.017), (90, 0.389), (96, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84936565 <a title="177-lda-1" href="./emnlp-2013-Question_Difficulty_Estimation_in_Community_Question_Answering_Services.html">155 emnlp-2013-Question Difficulty Estimation in Community Question Answering Services</a></p>
<p>Author: Jing Liu ; Quan Wang ; Chin-Yew Lin ; Hsiao-Wuen Hon</p><p>Abstract: In this paper, we address the problem of estimating question difficulty in community question answering services. We propose a competition-based model for estimating question difficulty by leveraging pairwise comparisons between questions and users. Our experimental results show that our model significantly outperforms a PageRank-based approach. Most importantly, our analysis shows that the text of question descriptions reflects the question difficulty. This implies the possibility of predicting question difficulty from the text of question descriptions.</p><p>2 0.81929624 <a title="177-lda-2" href="./emnlp-2013-Automatic_Knowledge_Acquisition_for_Case_Alternation_between_the_Passive_and_Active_Voices_in_Japanese.html">33 emnlp-2013-Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in Japanese</a></p>
<p>Author: Ryohei Sasano ; Daisuke Kawahara ; Sadao Kurohashi ; Manabu Okumura</p><p>Abstract: We present a method for automatically acquiring knowledge for case alternation between the passive and active voices in Japanese. By leveraging several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, our method aligns a case frame in the passive voice to a corresponding case frame in the active voice and finds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness.</p><p>same-paper 3 0.7982555 <a title="177-lda-3" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>Author: Eva Maria Vecchi ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: In this study, we use compositional distributional semantic methods to investigate restrictions in adjective ordering. Specifically, we focus on properties distinguishing AdjectiveAdjective-Noun phrases in which there is flexibility in the adjective ordering from those bound to a rigid order. We explore a number of measures extracted from the distributional representation of AAN phrases which may indicate a word order restriction. We find that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modification of the adjectives. Our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics.</p><p>4 0.70845836 <a title="177-lda-4" href="./emnlp-2013-Automatic_Domain_Partitioning_for_Multi-Domain_Learning.html">29 emnlp-2013-Automatic Domain Partitioning for Multi-Domain Learning</a></p>
<p>Author: Di Wang ; Chenyan Xiong ; William Yang Wang</p><p>Abstract: Chenyan Xiong School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA cx@ c s . cmu .edu William Yang Wang School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA ww@ cmu .edu might not be generalizable to other domains (BenDavid et al., 2006; Ben-David et al., 2010). Multi-Domain learning (MDL) assumes that the domain labels in the dataset are known. However, when there are multiple metadata at- tributes available, it is not always straightforward to select a single best attribute for domain partition, and it is possible that combining more than one metadata attributes (including continuous attributes) can lead to better MDL performance. In this work, we propose an automatic domain partitioning approach that aims at providing better domain identities for MDL. We use a supervised clustering approach that learns the domain distance between data instances , and then cluster the data into better domains for MDL. Our experiment on real multi-domain datasets shows that using our automatically generated domain partition improves over popular MDL methods.</p><p>5 0.66306293 <a title="177-lda-5" href="./emnlp-2013-Unsupervised_Induction_of_Contingent_Event_Pairs_from_Film_Scenes.html">192 emnlp-2013-Unsupervised Induction of Contingent Event Pairs from Film Scenes</a></p>
<p>Author: Zhichao Hu ; Elahe Rahimtoroghi ; Larissa Munishkina ; Reid Swanson ; Marilyn A. Walker</p><p>Abstract: Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning. Researchers in NLP have tackled modeling such expectations from a range of perspectives, including treating it as the inference of the CONTINGENT discourse relation, or as a type of common-sense causal reasoning. Our approach is to model likelihood between events by drawing on several of these lines of previous work. We implement and evaluate different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another. We refine event pairs that we learn from a corpus of film scene descriptions utilizing web search counts, and evaluate our results by collecting human judgments ofcontingency. Our results indicate that the use of web search counts increases the av- , erage accuracy of our best method to 85.64% over a baseline of 50%, as compared to an average accuracy of 75. 15% without web search.</p><p>6 0.43618974 <a title="177-lda-6" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>7 0.42910796 <a title="177-lda-7" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>8 0.42752624 <a title="177-lda-8" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>9 0.4242112 <a title="177-lda-9" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>10 0.42256221 <a title="177-lda-10" href="./emnlp-2013-Gender_Inference_of_Twitter_Users_in_Non-English_Contexts.html">89 emnlp-2013-Gender Inference of Twitter Users in Non-English Contexts</a></p>
<p>11 0.41832647 <a title="177-lda-11" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>12 0.41720933 <a title="177-lda-12" href="./emnlp-2013-Classifying_Message_Board_Posts_with_an_Extracted_Lexicon_of_Patient_Attributes.html">46 emnlp-2013-Classifying Message Board Posts with an Extracted Lexicon of Patient Attributes</a></p>
<p>13 0.41530159 <a title="177-lda-13" href="./emnlp-2013-Image_Description_using_Visual_Dependency_Representations.html">98 emnlp-2013-Image Description using Visual Dependency Representations</a></p>
<p>14 0.41297418 <a title="177-lda-14" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>15 0.41175073 <a title="177-lda-15" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>16 0.40915546 <a title="177-lda-16" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>17 0.40808681 <a title="177-lda-17" href="./emnlp-2013-Two-Stage_Method_for_Large-Scale_Acquisition_of_Contradiction_Pattern_Pairs_using_Entailment.html">189 emnlp-2013-Two-Stage Method for Large-Scale Acquisition of Contradiction Pattern Pairs using Entailment</a></p>
<p>18 0.40526536 <a title="177-lda-18" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>19 0.40366438 <a title="177-lda-19" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>20 0.40311903 <a title="177-lda-20" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
