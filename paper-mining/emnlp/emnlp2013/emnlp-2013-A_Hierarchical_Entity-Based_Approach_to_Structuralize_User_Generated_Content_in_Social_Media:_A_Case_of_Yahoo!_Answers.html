<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-7" href="#">emnlp2013-7</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</h1>
<br/><p>Source: <a title="emnlp-2013-7-pdf" href="http://aclweb.org/anthology//D/D13/D13-1159.pdf">pdf</a></p><p>Author: Baichuan Li ; Jing Liu ; Chin-Yew Lin ; Irwin King ; Michael R. Lyu</p><p>Abstract: Social media like forums and microblogs have accumulated a huge amount of user generated content (UGC) containing human knowledge. Currently, most of UGC is listed as a whole or in pre-defined categories. This “list-based” approach is simple, but hinders users from browsing and learning knowledge of certain topics effectively. To address this problem, we propose a hierarchical entity-based approach for structuralizing UGC in social media. By using a large-scale entity repository, we design a three-step framework to organize UGC in a novel hierarchical structure called “cluster entity tree (CET)”. With Yahoo! Answers as a test case, we conduct experiments and the results show the effectiveness of our framework in constructing CET. We further evaluate the performance of CET on UGC organization in both user and system aspects. From a user aspect, our user study demonstrates that, with CET-based structure, users perform significantly better in knowledge learning than using traditional list-based approach. From a system aspect, CET substantially boosts the performance of two information retrieval models (i.e., vector space model and query likelihood language model).</p><p>Reference: <a title="emnlp-2013-7-reference" href="../emnlp2013_reference/emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 By using a large-scale entity repository, we design a three-step framework to organize UGC in a novel hierarchical structure called “cluster entity tree (CET)”. [sent-20, score-0.839]
</p><p>2 From a user aspect, our user study demonstrates that, with CET-based structure, users perform significantly better in knowledge learning than using traditional list-based approach. [sent-24, score-0.379]
</p><p>3 The above problem calls for a new approach to structuralize UGC in social media, which facilitates users to seek knowledge (e. [sent-55, score-0.359]
</p><p>4 Thus, both topic models and social tagging are not suitable for structuralizing UGC in social media. [sent-64, score-0.283]
</p><p>5 , “cluster entity tree” or CET, to structuralize UGC in social media by leveraging an existing large-scale entity repository. [sent-67, score-0.939]
</p><p>6 In this CET, each node contains one (named) entity and a set of question IDs. [sent-69, score-0.421]
</p><p>7 With “edinburgh” as the root entity, layer 1includes all entities that co-occur with “edinburgh”. [sent-70, score-0.303]
</p><p>8 Similarly, entities on layer 2 cooccur with their parent entities on layer 1 and the root entity “edinburgh”. [sent-71, score-0.954]
</p><p>9 In addition, entities which share the same parent are clustered to different groups (see dashed rectangles in Fig. [sent-78, score-0.292]
</p><p>10 By utilizing a large-scale entity repository, CET avoids the granularity, interpretation, and sparsity problems. [sent-82, score-0.379]
</p><p>11 Entity repositories like Freebase2 provide a large number of named entities across various pre-defined topics, which avoid the granularity and sparsity problems. [sent-83, score-0.302]
</p><p>12 In this step, we extract entities from documents using an existing entity repository. [sent-89, score-0.568]
</p><p>13 we build the co-occurrence relationship between any two entities and construct hierarchical “entity trees (ETs)”. [sent-92, score-0.33]
</p><p>14 In an ET, some entities are more similar than other entities which share the same parents. [sent-95, score-0.448]
</p><p>15 Therefore, on each layer of the ET we cluster entities with the same parents (e. [sent-96, score-0.365]
</p><p>16 Answers is a popular CQA portal, where users post questions and provide answers in different categories. [sent-107, score-0.434]
</p><p>17 From a user aspect, our user study show that, with CET-based organization, users perform significantly better in  knowledge learning than using list-based approach. [sent-110, score-0.379]
</p><p>18 We propose a novel hierarchical entity-based approach to structuralize UGC in social media. [sent-115, score-0.273]
</p><p>19 To our knowledge, we are the first to utilize entities to structuralize UGC for content browsing and knowledge learning at a large scale; 2. [sent-116, score-0.437]
</p><p>20 Our hierarchical entity-based approach prevents these problems by employing a large-scale entity repository. [sent-145, score-0.409]
</p><p>21 As the entity repository provides a unified set of entities across various of pre-defined topics, and gives descriptions of entities, CET avoids the granularity, interpretation and sparsity issues. [sent-146, score-0.759]
</p><p>22 , 2013), we employ a  large-scale entity repository to extract more meaningful and interpretable key terms (entities), which make each subtopic much easier to understand. [sent-156, score-0.535]
</p><p>23 In our framework, we leverage an entity repository to extract named entities from UGC. [sent-158, score-0.759]
</p><p>24 To address the problem, (Ling and Weld, 2012) defined a fine-grained set of 112 tags based on Freebase for entity extraction. [sent-169, score-0.344]
</p><p>25 Our approach, which leverages a large-scale entity repository, addresses this issue. [sent-171, score-0.344]
</p><p>26 However, to the best of our knowledge, no previous study leverages entities to organize UGC in social media. [sent-180, score-0.364]
</p><p>27 Firstly, we provide the definitions of the entity repository and CET. [sent-182, score-0.535]
</p><p>28 1 (Entity Repository) Let ER = {R, g} be an entity repository, where R is a set of nga}m ebde e anntiti eenst taynd r g : tRo R w hise a mapping function tehdat e defines tnhde similarity of any two entities. [sent-184, score-0.403]
</p><p>29 2 (Cluster Entity Tree) Let D be a set of documents, ER = {R, g} be an entity repository, e mbee an entity, a c {lRus,tegr} entity tree CETe = (ve, V, E, C) is defined as a tree structure, with the root node ve, node set V , edge set E, and cluster set C. [sent-187, score-0.884]
</p><p>30 Each node vs ∈ V on CETe includes an entity extracted from th∈e set of CdEocTuments De ∈ D containing e, and a list L(s) which stores th∈e indexes of documents containing entity s and its superior entities. [sent-188, score-0.688]
</p><p>31 If vs is vt ’s parent node, entity t must co-occur with s and s ’s all superior entities at 1524 least once in the same document. [sent-189, score-0.606]
</p><p>32 Each element of C (one cluster) includes a set of nodes which share the same parent node, and the entities within a cluster are more similar to each other than the entities in other clusters. [sent-190, score-0.582]
</p><p>33 1 Framework This section shows our three-step framework for constructing CET: entity extraction (Section 3. [sent-192, score-0.344]
</p><p>34 1 Entity Extraction We adopt a simple entity repository based approach to extract entities, which address the “lowrecall” problem for traditional NER methods (details are given in Section 3. [sent-201, score-0.535]
</p><p>35 This approach involves two phases: candidate entity extraction and entropybased filtering. [sent-204, score-0.382]
</p><p>36 Then, we extract all noun phrases, preprocess them (including stemming), and extract the noun phrases which are included in our entity repository. [sent-207, score-0.344]
</p><p>37 In our experiments, we adopt a large-scale enterprise entity repository (anonymized for blind reviews). [sent-208, score-0.535]
</p><p>38 Given a document with a category label (or tags, which are available in most UGC sites), we get the distributions of each candidate entity over all top categories. [sent-212, score-0.445]
</p><p>39 The entropy of a candidate entity ei is  calculated as follows:  Entropy(ei) = −cX|=C1|Pc(ei)logPc(ei),  (1)  where |C| is the number of top categories and Pc(ei) iws tehree |nCum| isbe thre o nfu ei einr category c driievsid aendd by all number of candidate entities in that category. [sent-213, score-0.873]
</p><p>40 , “edinburgh”), we first search documents containing this entity and make the entity together with document ids as the root node. [sent-226, score-0.764]
</p><p>41 These entities and corresponding document ids form layer-1 nodes of the entity tree (see the example in Fig. [sent-228, score-0.643]
</p><p>42 Afterwards, for each entity in layer-1 nodes, we search entities  that co-occur with it and its superiors, combine them and corresponding document ids as new nodes, and put these new nodes under current node, which form layer-2 nodes. [sent-230, score-0.656]
</p><p>43 Iteratively, we construct the entity tree with the given entity as the root. [sent-231, score-0.762]
</p><p>44 Therefore, the final step is to hierarchically cluster entities with the same parents at different layers of entity trees. [sent-235, score-0.664]
</p><p>45 For a set of entities with the same parent, the agglomerative clustering algorithm works as follows:  1. [sent-243, score-0.295]
</p><p>46 Select one entity and create a new cluster which contains the entity; 2. [sent-244, score-0.44]
</p><p>47 Select the next entity ei, create an empty candidate list, calculate the similarity between the entity and all existing clusters. [sent-245, score-0.785]
</p><p>48 Three strategies are employed6: • AC-MAX: If the similarity between entity ei a-nMd entity ej ein s one orift yth bee tcwluesetenrs e (the 5Here we use the entity to represent the node. [sent-246, score-1.19]
</p><p>49 AC-MIN: If the similarity between entity ei a-nMd any entity ej i lna one boeft wtheee nclu esntteirtys is larger than threshold θmin, we put the cluster index and corresponding similarity in the candidate list. [sent-250, score-1.085]
</p><p>50 AC-AVG: If the mean similarity between entity ei :an Ifd any entity ej ianri one towf etehen clusters is larger than threshold θavg, we  put the cluster index and corresponding similarity in the candidate list. [sent-251, score-1.117]
</p><p>51 In our entity repository, the similarity between two entities is computed using the approach in (Shi et al. [sent-259, score-0.627]
</p><p>52 In addition, if two entities usually co-occur with a third entity (second-order co-occurrence), these two entities are likely to be similar. [sent-262, score-0.792]
</p><p>53 Some well-designed patterns are leveraged to extract similar entities from a huge repository of webpages. [sent-270, score-0.415]
</p><p>54 Given two entities ta and tb, PB calculates their similarity based on the number of RASCs containing both of them (Zhang et al. [sent-272, score-0.315]
</p><p>55 , 2010): if at least one entity is proper noun, PB is employed; otherwise DS is used. [sent-285, score-0.344]
</p><p>56 From these questions, we construct the following two test sets for evaluating entity extraction and entity clustering: Set EE. [sent-294, score-0.729]
</p><p>57 This set is employed to evaluate the  performance of entity extraction. [sent-295, score-0.377]
</p><p>58 This set is constructed to automatically evaluate hierarchical entity clustering and select the best clustering strategy. [sent-299, score-0.551]
</p><p>59 For instance, if an entity is extracted from questions in the category Computers & Internet, and it appears two or more times in Computer and Internet categories in Freebase, it will be filtered. [sent-307, score-0.603]
</p><p>60 Therefore, each entity is attached with a unique Freebase category label (i. [sent-308, score-0.407]
</p><p>61 Intuitively, entities with a same Freebase category label should be in one cluster. [sent-313, score-0.287]
</p><p>62 Note that Set EC only covers a small set of real entities and clustering on Set EC is partial clustering. [sent-314, score-0.295]
</p><p>63 For evaluating entity clustering results, we adopt Bcubed metrics. [sent-318, score-0.415]
</p><p>64 , ours and Freebase) are employed in entity extraction for comparison. [sent-341, score-0.377]
</p><p>65 , 2005) and fine-grained entity recognition (FIGER) (Ling and Weld, 2012). [sent-343, score-0.344]
</p><p>66 With the help of entity repositories, recall is significantly improved with a small decrease of precision. [sent-347, score-0.344]
</p><p>67 This observation shows the great advantage of utilizing an entity repositories in entity extraction and the effectiveness of our approach. [sent-349, score-0.763]
</p><p>68 As our ER performs better than Freebase, we adopt it as our entity repository in the following evaluations. [sent-350, score-0.535]
</p><p>69 A knowledge-learning task asks for some knowledge about a main entity from question texts. [sent-370, score-0.451]
</p><p>70 For instance, “find the games running on macbook pro” requires game names as the answer, where the main entity is “macbook pro”. [sent-371, score-0.374]
</p><p>71 A question-search task, however, asks users to find similar questions to the question in the task. [sent-372, score-0.422]
</p><p>72 The list-based program searches questions by utilizing Apache Lucene8. [sent-384, score-0.288]
</p><p>73 We extract 70,195 questions which contain at least one of the 24 main entities (see Table A. [sent-388, score-0.388]
</p><p>74 For each question, we extract the entities with the help of our entity repository. [sent-405, score-0.568]
</p><p>75 Each volunteer is asked to finish 12 tasks (6 knowledge-learning tasks and 6 question-search  tasks) using the CET-based program and 12 other tasks using the list-based program in random order. [sent-411, score-0.295]
</p><p>76 Because the CET-based program provides a series of clustered entities, it helpes users further refine queries through 1528  clicking on entities rather than reconstructing a new query. [sent-420, score-0.464]
</p><p>77 On the contrary, the list-based program returns a list of questions, and users need to find answers question-by-question. [sent-426, score-0.359]
</p><p>78 As the list-based program returns similar questions as top-ranked results, users are able to fill in answers easily. [sent-429, score-0.523]
</p><p>79 For CET-based program users, they have to find corresponding key entities in the CETs first. [sent-430, score-0.313]
</p><p>80 Therefore, they spend more time (the fourth row in Table 6) finding entities and less time filling answers. [sent-431, score-0.277]
</p><p>81 The precision of answers from CET-based program users is twice of that from list-based program users (z = 4. [sent-433, score-0.599]
</p><p>82 For questionsearch tasks, CET-based program users perform slightly worse than list-based program users, but the difference is not significant (z = 0. [sent-436, score-0.359]
</p><p>83 Since users of the CET-based program spend more time finding entities, they have limited time to check the answers. [sent-438, score-0.293]
</p><p>84 Algorithm 1CET-based search results re-ranking Input: query q, question collection Q, a ranked list of k relevant questions Qq = {q1, q2 , . [sent-455, score-0.277]
</p><p>85 , qk} to q, an entity repository ER, an empty list Θ. [sent-458, score-0.535]
</p><p>86 , 2010) such as QLLM and VSM do not capture key semantics and give more weights for entity terms. [sent-463, score-0.344]
</p><p>87 In addition, through clustering questions with similar topics, those questions which are ranked lower will be brought higher by their top-ranked neighbors. [sent-473, score-0.399]
</p><p>88 We first extract entities from each question of the whole question collection, and construct a entity co-occurrence graph (Line 1). [sent-475, score-0.763]
</p><p>89 Then, we calculate the PageRank score of each entity (Line 2). [sent-476, score-0.344]
</p><p>90 Otherwise, we identify the key entity in q (Line 6) and construct the CET cete whose root entity is e (Line 7). [sent-479, score-0.854]
</p><p>91 In Line 8, we first build an entity chain for question qi, in which entities of qi are ranked according to their PageRank scores. [sent-481, score-0.697]
</p><p>92 Afterwards, the first entity eˆ, which is not  similar to e (the similarity is calculated in Section 4. [sent-482, score-0.403]
</p><p>93 All other questions in these two categories constitute the question collection Q. [sent-490, score-0.273]
</p><p>94 Given a retrieved question by VSM  e  9These 160 questions are not used for constructing the entity co-occurrence graph. [sent-494, score-0.585]
</p><p>95 By using a largescale entity repository, we construct a three-step framework to organize knowledge in “cluster entity trees”. [sent-515, score-0.782]
</p><p>96 To our best knowledge, this work is the first attempt to utilize entities to structuralize UGC in social media, and there are some limitations to be improved in our future work. [sent-520, score-0.464]
</p><p>97 Second, our current entity extraction focuses on named entities instead of canonical entities. [sent-526, score-0.568]
</p><p>98 Searching questions by identifying question topic and question focus. [sent-575, score-0.351]
</p><p>99 Joint inference of named entity recognition and normalization for tweets. [sent-617, score-0.344]
</p><p>100 Piggyback: using search engines for robust cross-domain named entity recognition. [sent-646, score-0.344]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cet', 0.486), ('ugc', 0.356), ('entity', 0.344), ('entities', 0.224), ('repository', 0.191), ('questions', 0.164), ('users', 0.151), ('structuralize', 0.121), ('yahoo', 0.121), ('answers', 0.119), ('user', 0.114), ('cluster', 0.096), ('freebase', 0.092), ('cete', 0.091), ('cets', 0.091), ('qllm', 0.091), ('program', 0.089), ('social', 0.087), ('vsm', 0.079), ('question', 0.077), ('structuralizing', 0.076), ('clustering', 0.071), ('edinburgh', 0.069), ('cao', 0.067), ('ei', 0.067), ('hierarchical', 0.065), ('category', 0.063), ('shi', 0.062), ('yerva', 0.061), ('browsing', 0.06), ('similarity', 0.059), ('travel', 0.058), ('organization', 0.054), ('organize', 0.053), ('spend', 0.053), ('qi', 0.052), ('put', 0.046), ('knowledgelearning', 0.046), ('rascs', 0.046), ('shepitsen', 0.046), ('layer', 0.045), ('topics', 0.045), ('pagerank', 0.045), ('media', 0.043), ('boosts', 0.043), ('interface', 0.043), ('www', 0.043), ('ids', 0.042), ('construct', 0.041), ('entitybased', 0.04), ('repositories', 0.04), ('shahaf', 0.04), ('ner', 0.039), ('tasks', 0.039), ('pb', 0.039), ('granularity', 0.038), ('parent', 0.038), ('candidate', 0.038), ('stroudsburg', 0.037), ('er', 0.036), ('retrieval', 0.036), ('subtopics', 0.036), ('query', 0.036), ('tb', 0.036), ('hierarchy', 0.035), ('utilizing', 0.035), ('root', 0.034), ('cong', 0.034), ('cqa', 0.034), ('line', 0.034), ('topic', 0.033), ('searching', 0.033), ('employed', 0.033), ('tree', 0.033), ('categories', 0.032), ('ta', 0.032), ('utilize', 0.032), ('ej', 0.032), ('volunteers', 0.032), ('clusters', 0.032), ('sigir', 0.032), ('internet', 0.031), ('zhu', 0.031), ('amig', 0.03), ('baichuan', 0.03), ('easiness', 0.03), ('figer', 0.03), ('huibin', 0.03), ('huizhong', 0.03), ('katsumi', 0.03), ('macbook', 0.03), ('ohshima', 0.03), ('questionsearch', 0.03), ('rasc', 0.03), ('rectangles', 0.03), ('surender', 0.03), ('asks', 0.03), ('yong', 0.03), ('hong', 0.03), ('kong', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="7-tfidf-1" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>Author: Baichuan Li ; Jing Liu ; Chin-Yew Lin ; Irwin King ; Michael R. Lyu</p><p>Abstract: Social media like forums and microblogs have accumulated a huge amount of user generated content (UGC) containing human knowledge. Currently, most of UGC is listed as a whole or in pre-defined categories. This “list-based” approach is simple, but hinders users from browsing and learning knowledge of certain topics effectively. To address this problem, we propose a hierarchical entity-based approach for structuralizing UGC in social media. By using a large-scale entity repository, we design a three-step framework to organize UGC in a novel hierarchical structure called “cluster entity tree (CET)”. With Yahoo! Answers as a test case, we conduct experiments and the results show the effectiveness of our framework in constructing CET. We further evaluate the performance of CET on UGC organization in both user and system aspects. From a user aspect, our user study demonstrates that, with CET-based structure, users perform significantly better in knowledge learning than using traditional list-based approach. From a system aspect, CET substantially boosts the performance of two information retrieval models (i.e., vector space model and query likelihood language model).</p><p>2 0.18230504 <a title="7-tfidf-2" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>Author: Hrushikesh Mohapatra ; Siddhanth Jain ; Soumen Chakrabarti</p><p>Abstract: Web search can be enhanced in powerful ways if token spans in Web text are annotated with disambiguated entities from large catalogs like Freebase. Entity annotators need to be trained on sample mention snippets. Wikipedia entities and annotated pages offer high-quality labeled data for training and evaluation. Unfortunately, Wikipedia features only one-ninth the number of entities as Freebase, and these are a highly biased sample of well-connected, frequently mentioned “head” entities. To bring hope to “tail” entities, we broaden our goal to a second task: assigning types to entities in Freebase but not Wikipedia. The two tasks are synergistic: knowing the types of unfamiliar entities helps disambiguate mentions, and words in mention contexts help assign types to entities. We present TMI, a bipartite graphical model for joint type-mention inference. TMI attempts no schema integration or entity resolution, but exploits the above-mentioned synergy. In experiments involving 780,000 people in Wikipedia, 2.3 million people in Freebase, 700 million Web pages, and over 20 professional editors, TMI shows considerable annotation accuracy improvement (e.g., 70%) compared to baselines (e.g., 46%), especially for “tail” and emerging entities. We also compare with Google’s recent annotations of the same corpus with Freebase entities, and report considerable improvements within the people domain.</p><p>3 0.15832879 <a title="7-tfidf-3" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>Author: Zhengyan He ; Shujie Liu ; Yang Song ; Mu Li ; Ming Zhou ; Houfeng Wang</p><p>Abstract: Entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base. Recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes. We propose a fast collective disambiguation approach based on stacking. First, we train a local predictor g0 with learning to rank as base learner, to generate initial ranking list of candidates. Second, top k candidates of related instances are searched for constructing expressive global coherence features. A global predictor g1 is trained in the augmented feature space and stacking is employed to tackle the train/test mismatch problem. The proposed method is fast and easy to implement. Experiments show its effectiveness over various algorithms on several public datasets. By learning a rich semantic relatedness measure be- . tween entity categories and context document, performance is further improved.</p><p>4 0.12668136 <a title="7-tfidf-4" href="./emnlp-2013-Microblog_Entity_Linking_by_Leveraging_Extra_Posts.html">130 emnlp-2013-Microblog Entity Linking by Leveraging Extra Posts</a></p>
<p>Author: Yuhang Guo ; Bing Qin ; Ting Liu ; Sheng Li</p><p>Abstract: Linking name mentions in microblog posts to a knowledge base, namely microblog entity linking, is useful for text mining tasks on microblog. Entity linking in long text has been well studied in previous works. However few work has focused on short text such as microblog post. Microblog posts are short and noisy. Previous method can extract few features from the post context. In this paper we propose to use extra posts for the microblog entity linking task. Experimental results show that our proposed method significantly improves the linking accuracy over traditional methods by 8.3% and 7.5% respectively.</p><p>5 0.12002237 <a title="7-tfidf-5" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>Author: Jonathan K. Kummerfeld ; Dan Klein</p><p>Abstract: Coreference resolution metrics quantify errors but do not analyze them. Here, we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types. Using this tool, we first compare the error distributions across a large set of systems, then analyze common errors across the top ten systems, empirically characterizing the major unsolved challenges of the coreference resolution task.</p><p>6 0.11822904 <a title="7-tfidf-6" href="./emnlp-2013-Question_Difficulty_Estimation_in_Community_Question_Answering_Services.html">155 emnlp-2013-Question Difficulty Estimation in Community Question Answering Services</a></p>
<p>7 0.11058248 <a title="7-tfidf-7" href="./emnlp-2013-Automatic_Feature_Engineering_for_Answer_Selection_and_Extraction.html">31 emnlp-2013-Automatic Feature Engineering for Answer Selection and Extraction</a></p>
<p>8 0.10908858 <a title="7-tfidf-8" href="./emnlp-2013-The_Answer_is_at_your_Fingertips%3A_Improving_Passage_Retrieval_for_Web_Question_Answering_with_Search_Behavior_Data.html">180 emnlp-2013-The Answer is at your Fingertips: Improving Passage Retrieval for Web Question Answering with Search Behavior Data</a></p>
<p>9 0.10315151 <a title="7-tfidf-9" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>10 0.10041779 <a title="7-tfidf-10" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>11 0.095253989 <a title="7-tfidf-11" href="./emnlp-2013-Semantic_Parsing_on_Freebase_from_Question-Answer_Pairs.html">166 emnlp-2013-Semantic Parsing on Freebase from Question-Answer Pairs</a></p>
<p>12 0.094305135 <a title="7-tfidf-12" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>13 0.086009584 <a title="7-tfidf-13" href="./emnlp-2013-Identifying_Web_Search_Query_Reformulation_using_Concept_based_Matching.html">97 emnlp-2013-Identifying Web Search Query Reformulation using Concept based Matching</a></p>
<p>14 0.085603975 <a title="7-tfidf-14" href="./emnlp-2013-MCTest%3A_A_Challenge_Dataset_for_the_Open-Domain_Machine_Comprehension_of_Text.html">126 emnlp-2013-MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text</a></p>
<p>15 0.083831526 <a title="7-tfidf-15" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>16 0.083618283 <a title="7-tfidf-16" href="./emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model.html">75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</a></p>
<p>17 0.081770815 <a title="7-tfidf-17" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>18 0.080096878 <a title="7-tfidf-18" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>19 0.078909166 <a title="7-tfidf-19" href="./emnlp-2013-Well-Argued_Recommendation%3A_Adaptive_Models_Based_on_Words_in_Recommender_Systems.html">200 emnlp-2013-Well-Argued Recommendation: Adaptive Models Based on Words in Recommender Systems</a></p>
<p>20 0.076849513 <a title="7-tfidf-20" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.232), (1, 0.146), (2, -0.007), (3, -0.003), (4, 0.039), (5, 0.083), (6, 0.047), (7, 0.166), (8, 0.22), (9, -0.02), (10, 0.02), (11, 0.114), (12, -0.084), (13, -0.16), (14, -0.117), (15, -0.004), (16, 0.14), (17, 0.08), (18, 0.01), (19, -0.063), (20, -0.047), (21, -0.047), (22, -0.002), (23, 0.008), (24, 0.06), (25, 0.004), (26, -0.034), (27, 0.017), (28, -0.049), (29, -0.045), (30, -0.013), (31, 0.007), (32, 0.126), (33, -0.031), (34, 0.078), (35, -0.051), (36, -0.0), (37, 0.065), (38, -0.141), (39, -0.001), (40, 0.012), (41, -0.038), (42, 0.037), (43, -0.046), (44, -0.061), (45, -0.013), (46, -0.008), (47, 0.086), (48, 0.058), (49, 0.153)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97252202 <a title="7-lsi-1" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>Author: Baichuan Li ; Jing Liu ; Chin-Yew Lin ; Irwin King ; Michael R. Lyu</p><p>Abstract: Social media like forums and microblogs have accumulated a huge amount of user generated content (UGC) containing human knowledge. Currently, most of UGC is listed as a whole or in pre-defined categories. This “list-based” approach is simple, but hinders users from browsing and learning knowledge of certain topics effectively. To address this problem, we propose a hierarchical entity-based approach for structuralizing UGC in social media. By using a large-scale entity repository, we design a three-step framework to organize UGC in a novel hierarchical structure called “cluster entity tree (CET)”. With Yahoo! Answers as a test case, we conduct experiments and the results show the effectiveness of our framework in constructing CET. We further evaluate the performance of CET on UGC organization in both user and system aspects. From a user aspect, our user study demonstrates that, with CET-based structure, users perform significantly better in knowledge learning than using traditional list-based approach. From a system aspect, CET substantially boosts the performance of two information retrieval models (i.e., vector space model and query likelihood language model).</p><p>2 0.74278051 <a title="7-lsi-2" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>Author: Hrushikesh Mohapatra ; Siddhanth Jain ; Soumen Chakrabarti</p><p>Abstract: Web search can be enhanced in powerful ways if token spans in Web text are annotated with disambiguated entities from large catalogs like Freebase. Entity annotators need to be trained on sample mention snippets. Wikipedia entities and annotated pages offer high-quality labeled data for training and evaluation. Unfortunately, Wikipedia features only one-ninth the number of entities as Freebase, and these are a highly biased sample of well-connected, frequently mentioned “head” entities. To bring hope to “tail” entities, we broaden our goal to a second task: assigning types to entities in Freebase but not Wikipedia. The two tasks are synergistic: knowing the types of unfamiliar entities helps disambiguate mentions, and words in mention contexts help assign types to entities. We present TMI, a bipartite graphical model for joint type-mention inference. TMI attempts no schema integration or entity resolution, but exploits the above-mentioned synergy. In experiments involving 780,000 people in Wikipedia, 2.3 million people in Freebase, 700 million Web pages, and over 20 professional editors, TMI shows considerable annotation accuracy improvement (e.g., 70%) compared to baselines (e.g., 46%), especially for “tail” and emerging entities. We also compare with Google’s recent annotations of the same corpus with Freebase entities, and report considerable improvements within the people domain.</p><p>3 0.69005185 <a title="7-lsi-3" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>Author: Ruiji Fu ; Bing Qin ; Ting Liu</p><p>Abstract: Hypernym discovery aims to extract such noun pairs that one noun is a hypernym of the other. Most previous methods are based on lexical patterns but perform badly on opendomain data. Other work extracts hypernym relations from encyclopedias but has limited coverage. This paper proposes a simple yet effective distant supervision framework for Chinese open-domain hypernym discovery. Given an entity name, we try to discover its hypernyms by leveraging knowledge from multiple sources, i.e., search engine results, encyclopedias, and morphology of the entity name. First, we extract candidate hypernyms from the above sources. Then, we apply a statistical ranking model to select correct hypernyms. A set of novel features is proposed for the rank- ing model. We also present a heuristic strategy to build a large-scale noisy training data for the model without human annotation. Experimental results demonstrate that our approach outperforms the state-of-the-art methods on a manually labeled test dataset.</p><p>4 0.68404382 <a title="7-lsi-4" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>Author: Zhengyan He ; Shujie Liu ; Yang Song ; Mu Li ; Ming Zhou ; Houfeng Wang</p><p>Abstract: Entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base. Recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes. We propose a fast collective disambiguation approach based on stacking. First, we train a local predictor g0 with learning to rank as base learner, to generate initial ranking list of candidates. Second, top k candidates of related instances are searched for constructing expressive global coherence features. A global predictor g1 is trained in the augmented feature space and stacking is employed to tackle the train/test mismatch problem. The proposed method is fast and easy to implement. Experiments show its effectiveness over various algorithms on several public datasets. By learning a rich semantic relatedness measure be- . tween entity categories and context document, performance is further improved.</p><p>5 0.58078617 <a title="7-lsi-5" href="./emnlp-2013-The_Answer_is_at_your_Fingertips%3A_Improving_Passage_Retrieval_for_Web_Question_Answering_with_Search_Behavior_Data.html">180 emnlp-2013-The Answer is at your Fingertips: Improving Passage Retrieval for Web Question Answering with Search Behavior Data</a></p>
<p>Author: Mikhail Ageev ; Dmitry Lagun ; Eugene Agichtein</p><p>Abstract: Passage retrieval is a crucial first step of automatic Question Answering (QA). While existing passage retrieval algorithms are effective at selecting document passages most similar to the question, or those that contain the expected answer types, they do not take into account which parts of the document the searchers actually found useful. We propose, to the best of our knowledge, the first successful attempt to incorporate searcher examination data into passage retrieval for question answering. Specifically, we exploit detailed examination data, such as mouse cursor movements and scrolling, to infer the parts of the document the searcher found interesting, and then incorporate this signal into passage retrieval for QA. Our extensive experiments and analysis demonstrate that our method significantly improves passage retrieval, compared to using textual features alone. As an additional contribution, we make available to the research community the code and the search behavior data used in this study, with the hope of encouraging further research in this area.</p><p>6 0.56771326 <a title="7-lsi-6" href="./emnlp-2013-Question_Difficulty_Estimation_in_Community_Question_Answering_Services.html">155 emnlp-2013-Question Difficulty Estimation in Community Question Answering Services</a></p>
<p>7 0.54355133 <a title="7-lsi-7" href="./emnlp-2013-Well-Argued_Recommendation%3A_Adaptive_Models_Based_on_Words_in_Recommender_Systems.html">200 emnlp-2013-Well-Argued Recommendation: Adaptive Models Based on Words in Recommender Systems</a></p>
<p>8 0.5246501 <a title="7-lsi-8" href="./emnlp-2013-Rule-Based_Information_Extraction_is_Dead%21_Long_Live_Rule-Based_Information_Extraction_Systems%21.html">161 emnlp-2013-Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!</a></p>
<p>9 0.52022076 <a title="7-lsi-9" href="./emnlp-2013-Mining_New_Business_Opportunities%3A_Identifying_Trend_related_Products_by_Leveraging_Commercial_Intents_from_Microblogs.html">131 emnlp-2013-Mining New Business Opportunities: Identifying Trend related Products by Leveraging Commercial Intents from Microblogs</a></p>
<p>10 0.50501615 <a title="7-lsi-10" href="./emnlp-2013-Microblog_Entity_Linking_by_Leveraging_Extra_Posts.html">130 emnlp-2013-Microblog Entity Linking by Leveraging Extra Posts</a></p>
<p>11 0.47863621 <a title="7-lsi-11" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>12 0.44258869 <a title="7-lsi-12" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>13 0.4395771 <a title="7-lsi-13" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>14 0.43731451 <a title="7-lsi-14" href="./emnlp-2013-Simulating_Early-Termination_Search_for_Verbose_Spoken_Queries.html">173 emnlp-2013-Simulating Early-Termination Search for Verbose Spoken Queries</a></p>
<p>15 0.40428329 <a title="7-lsi-15" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>16 0.40393335 <a title="7-lsi-16" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>17 0.39985782 <a title="7-lsi-17" href="./emnlp-2013-Automatic_Feature_Engineering_for_Answer_Selection_and_Extraction.html">31 emnlp-2013-Automatic Feature Engineering for Answer Selection and Extraction</a></p>
<p>18 0.37923017 <a title="7-lsi-18" href="./emnlp-2013-Application_of_Localized_Similarity_for_Web_Documents.html">24 emnlp-2013-Application of Localized Similarity for Web Documents</a></p>
<p>19 0.37331215 <a title="7-lsi-19" href="./emnlp-2013-MCTest%3A_A_Challenge_Dataset_for_the_Open-Domain_Machine_Comprehension_of_Text.html">126 emnlp-2013-MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text</a></p>
<p>20 0.37228554 <a title="7-lsi-20" href="./emnlp-2013-Semantic_Parsing_on_Freebase_from_Question-Answer_Pairs.html">166 emnlp-2013-Semantic Parsing on Freebase from Question-Answer Pairs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.03), (18, 0.032), (22, 0.048), (29, 0.028), (30, 0.066), (50, 0.019), (51, 0.178), (66, 0.034), (71, 0.026), (75, 0.057), (77, 0.018), (90, 0.021), (95, 0.017), (96, 0.317)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97348964 <a title="7-lda-1" href="./emnlp-2013-Scaling_to_Large3_Data%3A_An_Efficient_and_Effective_Method_to_Compute_Distributional_Thesauri.html">165 emnlp-2013-Scaling to Large3 Data: An Efficient and Effective Method to Compute Distributional Thesauri</a></p>
<p>Author: Martin Riedl ; Chris Biemann</p><p>Abstract: We introduce a new highly scalable approach for computing Distributional Thesauri (DTs). By employing pruning techniques and a distributed framework, we make the computation for very large corpora feasible on comparably small computational resources. We demonstrate this by releasing a DT for the whole vocabulary of Google Books syntactic n-grams. Evaluating against lexical resources using two measures, we show that our approach produces higher quality DTs than previous approaches, and is thus preferable in terms of speed and quality for large corpora.</p><p>2 0.91375899 <a title="7-lda-2" href="./emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations.html">17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</a></p>
<p>Author: Shashank Srivastava ; Dirk Hovy ; Eduard Hovy</p><p>Abstract: In this paper, we propose a walk-based graph kernel that generalizes the notion of treekernels to continuous spaces. Our proposed approach subsumes a general framework for word-similarity, and in particular, provides a flexible way to incorporate distributed representations. Using vector representations, such an approach captures both distributional semantic similarities among words as well as the structural relations between them (encoded as the structure of the parse tree). We show an efficient formulation to compute this kernel using simple matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results.</p><p>3 0.90480638 <a title="7-lda-3" href="./emnlp-2013-Joint_Coreference_Resolution_and_Named-Entity_Linking_with_Multi-Pass_Sieves.html">112 emnlp-2013-Joint Coreference Resolution and Named-Entity Linking with Multi-Pass Sieves</a></p>
<p>Author: Hannaneh Hajishirzi ; Leila Zilles ; Daniel S. Weld ; Luke Zettlemoyer</p><p>Abstract: Many errors in coreference resolution come from semantic mismatches due to inadequate world knowledge. Errors in named-entity linking (NEL), on the other hand, are often caused by superficial modeling of entity context. This paper demonstrates that these two tasks are complementary. We introduce NECO, a new model for named entity linking and coreference resolution, which solves both problems jointly, reducing the errors made on each. NECO extends the Stanford deterministic coreference system by automatically linking mentions to Wikipedia and introducing new NEL-informed mention-merging sieves. Linking improves mention-detection and enables new semantic attributes to be incorporated from Freebase, while coreference provides better context modeling by propagating named-entity links within mention clusters. Experiments show consistent improve- ments across a number of datasets and experimental conditions, including over 11% reduction in MUC coreference error and nearly 21% reduction in F1 NEL error on ACE 2004 newswire data.</p><p>same-paper 4 0.87811232 <a title="7-lda-4" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>Author: Baichuan Li ; Jing Liu ; Chin-Yew Lin ; Irwin King ; Michael R. Lyu</p><p>Abstract: Social media like forums and microblogs have accumulated a huge amount of user generated content (UGC) containing human knowledge. Currently, most of UGC is listed as a whole or in pre-defined categories. This “list-based” approach is simple, but hinders users from browsing and learning knowledge of certain topics effectively. To address this problem, we propose a hierarchical entity-based approach for structuralizing UGC in social media. By using a large-scale entity repository, we design a three-step framework to organize UGC in a novel hierarchical structure called “cluster entity tree (CET)”. With Yahoo! Answers as a test case, we conduct experiments and the results show the effectiveness of our framework in constructing CET. We further evaluate the performance of CET on UGC organization in both user and system aspects. From a user aspect, our user study demonstrates that, with CET-based structure, users perform significantly better in knowledge learning than using traditional list-based approach. From a system aspect, CET substantially boosts the performance of two information retrieval models (i.e., vector space model and query likelihood language model).</p><p>5 0.87224919 <a title="7-lda-5" href="./emnlp-2013-A_Unified_Model_for_Topics%2C_Events_and_Users_on_Twitter.html">16 emnlp-2013-A Unified Model for Topics, Events and Users on Twitter</a></p>
<p>Author: Qiming Diao ; Jing Jiang</p><p>Abstract: With the rapid growth of social media, Twitter has become one of the most widely adopted platforms for people to post short and instant message. On the one hand, people tweets about their daily lives, and on the other hand, when major events happen, people also follow and tweet about them. Moreover, people’s posting behaviors on events are often closely tied to their personal interests. In this paper, we try to model topics, events and users on Twitter in a unified way. We propose a model which combines an LDA-like topic model and the Recurrent Chinese Restaurant Process to capture topics and events. We further propose a duration-based regularization component to find bursty events. We also propose to use event-topic affinity vectors to model the asso- . ciation between events and topics. Our experiments shows that our model can accurately identify meaningful events and the event-topic affinity vectors are effective for event recommendation and grouping events by topics.</p><p>6 0.70945036 <a title="7-lda-6" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>7 0.69272065 <a title="7-lda-7" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>8 0.6917991 <a title="7-lda-8" href="./emnlp-2013-Microblog_Entity_Linking_by_Leveraging_Extra_Posts.html">130 emnlp-2013-Microblog Entity Linking by Leveraging Extra Posts</a></p>
<p>9 0.68173873 <a title="7-lda-9" href="./emnlp-2013-Exploiting_Zero_Pronouns_to_Improve_Chinese_Coreference_Resolution.html">80 emnlp-2013-Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</a></p>
<p>10 0.6807732 <a title="7-lda-10" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>11 0.66815639 <a title="7-lda-11" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>12 0.6548695 <a title="7-lda-12" href="./emnlp-2013-A_temporal_model_of_text_periodicities_using_Gaussian_Processes.html">18 emnlp-2013-A temporal model of text periodicities using Gaussian Processes</a></p>
<p>13 0.65228736 <a title="7-lda-13" href="./emnlp-2013-A_Constrained_Latent_Variable_Model_for_Coreference_Resolution.html">1 emnlp-2013-A Constrained Latent Variable Model for Coreference Resolution</a></p>
<p>14 0.65181667 <a title="7-lda-14" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>15 0.64981157 <a title="7-lda-15" href="./emnlp-2013-Well-Argued_Recommendation%3A_Adaptive_Models_Based_on_Words_in_Recommender_Systems.html">200 emnlp-2013-Well-Argued Recommendation: Adaptive Models Based on Words in Recommender Systems</a></p>
<p>16 0.64367974 <a title="7-lda-16" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>17 0.6395672 <a title="7-lda-17" href="./emnlp-2013-Easy_Victories_and_Uphill_Battles_in_Coreference_Resolution.html">67 emnlp-2013-Easy Victories and Uphill Battles in Coreference Resolution</a></p>
<p>18 0.63689262 <a title="7-lda-18" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>19 0.63552922 <a title="7-lda-19" href="./emnlp-2013-Gender_Inference_of_Twitter_Users_in_Non-English_Contexts.html">89 emnlp-2013-Gender Inference of Twitter Users in Non-English Contexts</a></p>
<p>20 0.62932992 <a title="7-lda-20" href="./emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model.html">75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
