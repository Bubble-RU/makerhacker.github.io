<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-77" href="#">emnlp2013-77</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</h1>
<br/><p>Source: <a title="emnlp-2013-77-pdf" href="http://aclweb.org/anthology//D/D13/D13-1172.pdf">pdf</a></p><p>Author: Zhiyuan Chen ; Arjun Mukherjee ; Bing Liu ; Meichun Hsu ; Malu Castellanos ; Riddhiman Ghosh</p><p>Abstract: Aspect extraction is one of the key tasks in sentiment analysis. In recent years, statistical models have been used for the task. However, such models without any domain knowledge often produce aspects that are not interpretable in applications. To tackle the issue, some knowledge-based topic models have been proposed, which allow the user to input some prior domain knowledge to generate coherent aspects. However, existing knowledge-based topic models have several major shortcomings, e.g., little work has been done to incorporate the cannot-link type of knowledge or to automatically adjust the number of topics based on domain knowledge. This paper proposes a more advanced topic model, called MC-LDA (LDA with m-set and c-set), to address these problems, which is based on an Extended generalized Pólya urn (E-GPU) model (which is also proposed in this paper). Experiments on real-life product reviews from a variety of domains show that MCLDA outperforms the existing state-of-the-art models markedly.</p><p>Reference: <a title="emnlp-2013-77-reference" href="../emnlp2013_reference/emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 However, such models without any domain knowledge often produce aspects that are not interpretable in applications. [sent-6, score-0.27]
</p><p>2 To tackle the issue, some knowledge-based topic models have been proposed, which allow the user to input some prior domain knowledge to generate coherent aspects. [sent-7, score-0.465]
</p><p>3 However, existing knowledge-based topic models have several major shortcomings, e. [sent-8, score-0.251]
</p><p>4 , little work has been done to incorporate the cannot-link type of knowledge or to automatically adjust the number of topics based on domain knowledge. [sent-10, score-0.352]
</p><p>5 This paper proposes a more advanced topic model, called  MC-LDA (LDA with m-set and c-set), to address these problems, which is based on an Extended generalized Pólya urn (E-GPU) model (which is also proposed in this paper). [sent-11, score-0.77]
</p><p>6 1 Introduction In sentiment analysis and opinion mining, aspect extraction aims to extract entity aspects or features on which opinions have been expressed (Hu and Liu, 2004; Liu, 2012). [sent-13, score-0.544]
</p><p>7 ” Aspect extraction consists of two sub-tasks: (1) extracting all aspect terms (e. [sent-15, score-0.187]
</p><p>8 , cluster “picture” and “photo” into one aspect category as they mean the same in the domain “Camera”). [sent-19, score-0.252]
</p><p>9 com adopt the topic modeling approach as it can perform both sub-tasks simultaneously (see § 2). [sent-24, score-0.251]
</p><p>10 , 2003), provide an unsupervised framework for extracting latent topics in text documents. [sent-26, score-0.192]
</p><p>11 However, in recent years, researchers have found that fully unsupervised topic models may not produce topics that are very coherent for a particular application. [sent-28, score-0.464]
</p><p>12 This is because the objective functions of topic models do not always correlate well with human judgments and needs (Chang et al. [sent-29, score-0.251]
</p><p>13 To address the issue, several knowledge-based topic models have been proposed. [sent-31, score-0.251]
</p><p>14 A must-link states that two words (or terms) should belong to the same topic whereas a cannotlink indicates that two words should not be in the same topic. [sent-34, score-0.251]
</p><p>15 Furthermore, none of the existing models, including DF-LDA, is able to automatically adjust the number of topics based on domain knowledge. [sent-46, score-0.293]
</p><p>16 oc d2s0 i1n3 N Aastusorcaila Ltiaon g fuoarg Ceo Pmrpoucetastsi on ga,l p Laignegsu 1is6t5ic5s–16 7, domain “Computer”, a topic model may generate two topics Battery and Screen that represent two different aspects. [sent-51, score-0.509]
</p><p>17 A cannot-link {battery, screen} as the domain knowledge is thus consistent with the corpus. [sent-52, score-0.159]
</p><p>18 However, words Amazon and Price may appear in the same topic due to their high cooccurrences in the Amazon. [sent-53, score-0.251]
</p><p>19 In this case, the number of topics needs to be increased by 1 since the mixed topic has to be separated into two individual topics Amazon and Price. [sent-56, score-0.567]
</p><p>20 Apart from the above shortcoming, earlier knowledge-based topic models also have some major shortcomings: Incapability of handling multiple senses: A word typically has multiple meanings or senses. [sent-57, score-0.294]
</p><p>21 , 2012) allows multiple senses, it requires that each topic has at most one set of seed words (seed set), which is restrictive as the amount of knowledge should not be limited. [sent-65, score-0.345]
</p><p>22 This can harm the final topics because the attenuation of the frequent (often domain important) words can result in some irrelevant words being ranked higher (with higher probabilities). [sent-68, score-0.258]
</p><p>23 To address the above shortcomings, we define m-set (for must-set) as a set of words that should belong to the same topic and c-set (cannot-set) as a set of words that should not be in the same topic. [sent-69, score-0.251]
</p><p>24 We then propose a new topic model, called MCLDA (LDA with m-set and c-set), which is not only able to deal with c-sets and automatically adjust the number of topics, but also deal with the multiple senses and adverse effect of knowledge problems at the same time. [sent-87, score-0.583]
</p><p>25 Then, we employ the generalized Pólya urn (GPU) model (Mahmoud, 2008) to address the issue of adverse effect of knowledge (§ 4). [sent-90, score-0.666]
</p><p>26 Deviating from the standard topic modeling approaches, we propose the Extended generalized Pólya urn (E-GPU) model (§ 5). [sent-91, score-0.77]
</p><p>27 It proposed a new knowledge-based topic model called MC-LDA, which is able to use both m-sets and c-sets, as well as automatically adjust the number of topics based on domain  knowledge. [sent-98, score-0.544]
</p><p>28 It proposed the E-GPU model to enable multiurn interactions, which enables c-sets to be naturally integrated into a topic model. [sent-102, score-0.251]
</p><p>29 According to (Liu, 2012), there are three main approaches to aspect extraction: 1) Using word frequency and syntactic dependency of aspects and sentiment words for extraction (e. [sent-109, score-0.41]
</p><p>30 In this work, we focus on topic models owing to their advantage of performing both aspect extraction and clustering simultaneously. [sent-143, score-0.438]
</p><p>31 We also notice that some aspect extraction models in sentiment analysis separately discover aspect words and aspect specific sentiment words (e. [sent-151, score-0.715]
</p><p>32 Our proposed model does not separate them as most sentiment words also imply aspects and most adjectives modify specific attributes of objects. [sent-155, score-0.223]
</p><p>33 For example, sentiment words expensive and beautiful imply aspects price and appearance respectively. [sent-156, score-0.309]
</p><p>34 , 2013b), we proposed a framework (called GK-LDA) to explicitly deal with the wrong knowledge when exploring the lexical semantic relations as the general (domain independent) knowledge in topic models. [sent-164, score-0.414]
</p><p>35 The generalized Pólya urn (GPU) model (Mahmoud, 2008) was first introduced in LDA by Mimno et al. [sent-166, score-0.519]
</p><p>36 Our results in § 7 show that using domain knowledge can significantly improve aspect extraction. [sent-170, score-0.311]
</p><p>37 The GPU model was also employed in topic models in our work of (Chen et al. [sent-171, score-0.251]
</p><p>38 1 Generalized Pólya urn (GPU) Model The Pólya urn model involves an urn containing balls of different colors. [sent-370, score-1.544]
</p><p>39 At discrete time intervals, balls are added or removed from the urn according to their color distributions. [sent-371, score-0.798]
</p><p>40 In the simple Pólya urn (SPU) model, a ball is first drawn randomly from the urn and its color is recorded, then that ball is put back along with a new ball of the same color. [sent-372, score-1.886]
</p><p>41 This selection process is repeated and the contents of the urn change over time, with a self-reinforcing property sometimes expressed as “the rich get richer. [sent-373, score-0.461]
</p><p>42 The generalized Pólya urn (GPU) model differs from the SPU model in the replacement scheme during sampling. [sent-375, score-0.552]
</p><p>43 Specifically, when a ball is randomly drawn, certain numbers of additional balls of each color are returned to the urn, rather than just two balls of the same color as in SPU. [sent-376, score-0.923]
</p><p>44 2 Promoting M-sets using GPU To deal with the issue of sensitivity to the adverse effect of knowledge, MDK-LDA(b) is extended to MDK-LDA which employs the generalized Pólya urn (GPU) sampling scheme. [sent-378, score-0.761]
</p><p>45 1 Extended Generalized Pólya urn Model To handle the complex situation resulted from incorporating c-sets, we propose an Extended generalized Pólya urn (E-GPU) model. [sent-413, score-0.98]
</p><p>46 Instead of involving only one urn as in SPU and GPU, EGPU model considers a set of urns in the sampling process. [sent-414, score-0.656]
</p><p>47 The E-GPU model allows a ball to be transferred from one urn to another, enabling multi-urn interactions. [sent-415, score-0.744]
</p><p>48 Thus, during sampling, the populations of several urns will evolve even if only one ball is drawn from one urn. [sent-416, score-0.407]
</p><p>49 We define three sets of urns which will be used in the new sampling scheme in the proposed MCLDA model. [sent-418, score-0.228]
</p><p>50 colors (topics) and each ball inside has a color ? [sent-424, score-0.425]
</p><p>51 To tackle the issue, we utilize the proposed E-GPU model and incorporate c-sets handling inside the E-GPU sampling scheme, which is also designed to enable automated adjustment of the number of topics based on domain knowledge. [sent-454, score-0.379]
</p><p>52 As the E-GPU model bilities of those cannot-words under this topic while increasing their corresponding probabilities under some other topics. [sent-456, score-0.251]
</p><p>53 In order to correctly transfer a ball that represents word ? [sent-457, score-0.293]
</p><p>54 , it should be transferred to an urn which has a higher proportion of ? [sent-458, score-0.526]
</p><p>55 That is, we randomly sample an urn  that has a higher proportion of any m-set of ? [sent-463, score-0.492]
</p><p>56 For example, aspects price and amazon may be mixed under one topic (say ? [sent-467, score-0.493]
</p><p>57 In this case, according to LDA, word price has no topic with a higher proportion of it (and its related words) than topic ? [sent-470, score-0.619]
</p><p>58 To transfer it, we need to increment the number of topics by 1 and then transfer the word to this new topic urn (step 3 c below). [sent-472, score-0.992]
</p><p>59 ′} such that each urn in it satisfies the following conditions: i) ? [sent-568, score-0.499]
</p><p>60 , we perform hierarchical sampling consisting of the following three steps  (the detailed algorithms are given in Figures 2 and 3): Step 1 (Lines 1-11 in Figure 2): We jointly sample a topic ? [sent-751, score-0.329]
</p><p>61 from the corpus with topic and m-set assignments being ? [sent-834, score-0.251]
</p><p>62 Step 3 (lines 6-12 in Figure 3): For each drawn ball ? [sent-955, score-0.29]
</p><p>63 () is an indicator function, which restricts the ball to be transferred only to an urn that contains a higher proportion of its m-set. [sent-1055, score-0.775]
</p><p>64 can be successfully sampled and the current sweep (iteration) of Gibbs sampling has the same number of topic (? [sent-1057, score-0.358]
</p><p>65 Two unsupervised baseline models that we compare with are: • LDA: LDA is the basic unsupervised topic model (Blei et al. [sent-1066, score-0.251]
</p><p>66 , word camera in the domain “Camera”, since it co-occurs with most words in the corpus, leading to high similarity among topics/aspects. [sent-1098, score-0.159]
</p><p>67 Sentences as documents: As noted in (Titov and McDonald, 2008), when standard topic models are applied to reviews as documents, they tend to produce topics that correspond to global properties of products (e. [sent-1099, score-0.469]
</p><p>68 , brand name), which make topics overlapping with each other. [sent-1101, score-0.158]
</p><p>69 The reason is that all reviews of the same type of products discuss about the same aspects of these products. [sent-1102, score-0.171]
</p><p>70 Domain knowledge: User knowledge about a domain can vary a great deal. [sent-1137, score-0.159]
</p><p>71 However, the perplexity metric does not reflect the semantic coherence of individual topics learned by a topic model (Newman et al. [sent-1158, score-0.564]
</p><p>72 Also, perplexity does not really reflect our goal of finding coherent aspects with accurate semantic clustering. [sent-1162, score-0.211]
</p><p>73 , 201 1) (also called the “UMass” measure (Stevens and Buttler, 2012)) was proposed as a better alternative for assessing topic quality. [sent-1165, score-0.251]
</p><p>74 It was shown that topic coherence is highly consistent with human expert labeling by Mimno et al. [sent-1167, score-0.403]
</p><p>75 Higher topic coherence score indicates higher quality of topics, i. [sent-1169, score-0.361]
</p><p>76 This shows that the  guidance using domain knowledge is more effective than using co-document frequency. [sent-1197, score-0.159]
</p><p>77 We only say that for the task of aspect extraction and leveraging domain knowledge, these models do not generate as coherent aspects as ours because of their shortcomings discussed in § 1. [sent-1204, score-0.535]
</p><p>78 is larger than 15, aspects found by each model became more and more overlapping, with several aspects expressing the same features of products. [sent-1207, score-0.222]
</p><p>79 3  Human Evaluation  Since our aim is to make topics more interpretable and conformable to human judgments, we worked with two judges who are familiar with Amazon products and reviews to evaluate the models subjectively. [sent-1221, score-0.281]
</p><p>80 Since topics from topic models are rankings based on word probability and we do not know the number of correct topical words, a natural way to evaluate these rankings is to use Precision@n (or p@n) which was also used in  (Mukherjee and Liu, 2012; Zhao et al. [sent-1222, score-0.409]
</p><p>81 There are two steps in human evaluation: topic labeling and word labeling. [sent-1225, score-0.293]
</p><p>82 , 201 1) and asked the judges to label each topic as good or bad. [sent-1227, score-0.314]
</p><p>83 Each topic was presented as a list of 10 most probable words in descending order of their probabilities under that topic. [sent-1228, score-0.251]
</p><p>84 The models which generated the topics for labeling were obscure to the judges. [sent-1229, score-0.2]
</p><p>85 In general, each topic was annotated as good if it had more than half of its words coherently related to each other representing a semantic concept together; otherwise bad. [sent-1230, score-0.285]
</p><p>86 Agreement of human judges on topic 1663  labeling using Cohen’s Kappa yielded a score of 0. [sent-1231, score-0.356]
</p><p>87 This is reasonable as topic labeling is an easy task and semantic coherence can be judged well by humans. [sent-1233, score-0.403]
</p><p>88 Word Labeling: After topic labeling, we chose the topics, which were labeled as good by both judges, as good topics. [sent-1234, score-0.251]
</p><p>89 Since judges already had the conception of each topic in mind when they were labeling topics, labeling each word was not difficult which explains the high Kappa score for this labeling task (score = 0. [sent-1237, score-0.44]
</p><p>90 We also found that when the domain knowledge is simple with one word usually expressing only one meaning/sense (e. [sent-1248, score-0.159]
</p><p>91 The results from LDA-GPU and DF-LDA were inferior and hard for the human judges to match them with aspects found by the other models for qualitative comparison. [sent-1258, score-0.174]
</p><p>92 Table 4 shows three aspects Amazon, Price, Battery generated by each model in the domain  TCAoa#FvGmboeaprlioecdurgtas3e. [sent-1259, score-0.211]
</p><p>93 8  Conclusion  This paper proposed a new model to exploit domain knowledge in the form of m-sets and c-sets to generate coherent aspects (topics) from online reviews. [sent-1271, score-0.359]
</p><p>94 A comprehensive evaluation using real-life online reviews from multiple domains shows that MCLDA outperforms the state-of-the-art models significantly and discovers aspects with high semantic coherence. [sent-1274, score-0.205]
</p><p>95 Incorporating domain knowledge into topic modeling via Dirichlet Forest priors. [sent-1289, score-0.41]
</p><p>96 A framework for incorporating general domain knowledge into latent Dirichlet allocation using first-order logic. [sent-1293, score-0.193]
</p><p>97 ILDA: interdependent LDA model for learning latent  aspects and their ratings from online product reviews. [sent-1486, score-0.214]
</p><p>98 Labeled  LDA:  a  supervised topic model for credit attribution in multilabeled  corpora. [sent-1520, score-0.251]
</p><p>99 online  of ACL, pages  Coherence  952–961  and  stances  Topic  Veselin  pages  debates. [sent-1540, score-0.17]
</p><p>100 Constrained LDA for grouping product features in opinion mining. [sent-1612, score-0.169]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('urn', 0.461), ('topic', 0.251), ('ball', 0.249), ('lya', 0.185), ('color', 0.176), ('gpu', 0.175), ('lda', 0.163), ('balls', 0.161), ('topics', 0.158), ('aspect', 0.152), ('andrzejewski', 0.15), ('opinion', 0.134), ('mclda', 0.118), ('urns', 0.117), ('sentiment', 0.112), ('aspects', 0.111), ('coherence', 0.11), ('domain', 0.1), ('mukherjee', 0.092), ('mimno', 0.089), ('gibbs', 0.088), ('adverse', 0.088), ('price', 0.086), ('shortcomings', 0.082), ('sampling', 0.078), ('lu', 0.078), ('liu', 0.073), ('pages', 0.068), ('malu', 0.067), ('meichun', 0.067), ('riddhiman', 0.067), ('judges', 0.063), ('chengxiang', 0.062), ('senses', 0.06), ('reviews', 0.06), ('camera', 0.059), ('bing', 0.059), ('sauper', 0.059), ('spu', 0.059), ('knowledge', 0.059), ('generalized', 0.058), ('chen', 0.057), ('coherent', 0.055), ('proceedings', 0.054), ('battery', 0.054), ('titov', 0.053), ('burns', 0.051), ('castellanos', 0.051), ('gibbssampling', 0.051), ('zhiyuan', 0.051), ('jagarlamudi', 0.05), ('arjun', 0.05), ('yue', 0.049), ('jo', 0.047), ('zhai', 0.046), ('sampler', 0.046), ('amazon', 0.045), ('deal', 0.045), ('perplexity', 0.045), ('transfer', 0.044), ('handling', 0.043), ('labeling', 0.042), ('blei', 0.042), ('drawn', 0.041), ('petterson', 0.04), ('hongning', 0.04), ('hsu', 0.04), ('satisfies', 0.038), ('wiebe', 0.038), ('oh', 0.036), ('kdd', 0.036), ('shares', 0.036), ('summarization', 0.036), ('extraction', 0.035), ('seed', 0.035), ('hu', 0.035), ('product', 0.035), ('adjust', 0.035), ('wang', 0.034), ('latent', 0.034), ('online', 0.034), ('alice', 0.034), ('coherently', 0.034), ('dcomputercare', 0.034), ('edmund', 0.034), ('egpu', 0.034), ('increment', 0.034), ('ishwaran', 0.034), ('mahmoud', 0.034), ('moghaddam', 0.034), ('talley', 0.034), ('transferred', 0.034), ('scheme', 0.033), ('li', 0.032), ('cikm', 0.032), ('extended', 0.031), ('proportion', 0.031), ('emnlp', 0.03), ('xia', 0.03), ('sweep', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="77-tfidf-1" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>Author: Zhiyuan Chen ; Arjun Mukherjee ; Bing Liu ; Meichun Hsu ; Malu Castellanos ; Riddhiman Ghosh</p><p>Abstract: Aspect extraction is one of the key tasks in sentiment analysis. In recent years, statistical models have been used for the task. However, such models without any domain knowledge often produce aspects that are not interpretable in applications. To tackle the issue, some knowledge-based topic models have been proposed, which allow the user to input some prior domain knowledge to generate coherent aspects. However, existing knowledge-based topic models have several major shortcomings, e.g., little work has been done to incorporate the cannot-link type of knowledge or to automatically adjust the number of topics based on domain knowledge. This paper proposes a more advanced topic model, called MC-LDA (LDA with m-set and c-set), to address these problems, which is based on an Extended generalized Pólya urn (E-GPU) model (which is also proposed in this paper). Experiments on real-life product reviews from a variety of domains show that MCLDA outperforms the existing state-of-the-art models markedly.</p><p>2 0.16528402 <a title="77-tfidf-2" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>Author: Xinjie Zhou ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style. In this paper, we study the problem of extracting opinion targets of Chinese microblog messages. Such fine-grained word-level task has not been well investigated in microblogs yet. We propose an unsupervised label propagation algorithm to address the problem. The opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets. Topics in microblogs are identified by hashtags or using clustering algorithms. Experimental results on Chinese microblogs show the effectiveness of our framework and algorithms.</p><p>3 0.1540902 <a title="77-tfidf-3" href="./emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM.html">99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</a></p>
<p>Author: Wei Wang ; Hua Xu ; Xiaoqiu Huang</p><p>Abstract: Implicit feature detection, also known as implicit feature identification, is an essential aspect of feature-specific opinion mining but previous works have often ignored it. We think, based on the explicit sentences, several Support Vector Machine (SVM) classifiers can be established to do this task. Nevertheless, we believe it is possible to do better by using a constrained topic model instead of traditional attribute selection methods. Experiments show that this method outperforms the traditional attribute selection methods by a large margin and the detection task can be completed better.</p><p>4 0.15288669 <a title="77-tfidf-4" href="./emnlp-2013-Improvements_to_the_Bayesian_Topic_N-Gram_Models.html">100 emnlp-2013-Improvements to the Bayesian Topic N-Gram Models</a></p>
<p>Author: Hiroshi Noji ; Daichi Mochihashi ; Yusuke Miyao</p><p>Abstract: One of the language phenomena that n-gram language model fails to capture is the topic information of a given situation. We advance the previous study of the Bayesian topic language model by Wallach (2006) in two directions: one, investigating new priors to alleviate the sparseness problem caused by dividing all ngrams into exclusive topics, and two, developing a novel Gibbs sampler that enables moving multiple n-grams across different documents to another topic. Our blocked sampler can efficiently search for higher probability space even with higher order n-grams. In terms of modeling assumption, we found it is effective to assign a topic to only some parts of a document.</p><p>5 0.14661793 <a title="77-tfidf-5" href="./emnlp-2013-Learning_Topics_and_Positions_from_Debatepedia.html">121 emnlp-2013-Learning Topics and Positions from Debatepedia</a></p>
<p>Author: Swapna Gottipati ; Minghui Qiu ; Yanchuan Sim ; Jing Jiang ; Noah A. Smith</p><p>Abstract: We explore Debatepedia, a communityauthored encyclopedia of sociopolitical debates, as evidence for inferring a lowdimensional, human-interpretable representation in the domain of issues and positions. We introduce a generative model positing latent topics and cross-cutting positions that gives special treatment to person mentions and opinion words. We evaluate the resulting representation’s usefulness in attaching opinionated documents to arguments and its consistency with human judgments about positions.</p><p>6 0.13160105 <a title="77-tfidf-6" href="./emnlp-2013-Optimized_Event_Storyline_Generation_based_on_Mixture-Event-Aspect_Model.html">147 emnlp-2013-Optimized Event Storyline Generation based on Mixture-Event-Aspect Model</a></p>
<p>7 0.13057148 <a title="77-tfidf-7" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>8 0.12005487 <a title="77-tfidf-8" href="./emnlp-2013-Modeling_Scientific_Impact_with_Topical_Influence_Regression.html">133 emnlp-2013-Modeling Scientific Impact with Topical Influence Regression</a></p>
<p>9 0.1112202 <a title="77-tfidf-9" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<p>10 0.10974138 <a title="77-tfidf-10" href="./emnlp-2013-Identifying_Manipulated_Offerings_on_Review_Portals.html">94 emnlp-2013-Identifying Manipulated Offerings on Review Portals</a></p>
<p>11 0.1050544 <a title="77-tfidf-11" href="./emnlp-2013-Discourse_Level_Explanatory_Relation_Extraction_from_Product_Reviews_Using_First-Order_Logic.html">63 emnlp-2013-Discourse Level Explanatory Relation Extraction from Product Reviews Using First-Order Logic</a></p>
<p>12 0.10493808 <a title="77-tfidf-12" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>13 0.10404337 <a title="77-tfidf-13" href="./emnlp-2013-A_Unified_Model_for_Topics%2C_Events_and_Users_on_Twitter.html">16 emnlp-2013-A Unified Model for Topics, Events and Users on Twitter</a></p>
<p>14 0.10212244 <a title="77-tfidf-14" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>15 0.10084693 <a title="77-tfidf-15" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>16 0.099346735 <a title="77-tfidf-16" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>17 0.084231175 <a title="77-tfidf-17" href="./emnlp-2013-A_Multi-Teraflop_Constituency_Parser_using_GPUs.html">10 emnlp-2013-A Multi-Teraflop Constituency Parser using GPUs</a></p>
<p>18 0.078574032 <a title="77-tfidf-18" href="./emnlp-2013-Automatic_Domain_Partitioning_for_Multi-Domain_Learning.html">29 emnlp-2013-Automatic Domain Partitioning for Multi-Domain Learning</a></p>
<p>19 0.073617242 <a title="77-tfidf-19" href="./emnlp-2013-A_Generative_Joint%2C_Additive%2C_Sequential_Model_of_Topics_and_Speech_Acts_in_Patient-Doctor_Communication.html">6 emnlp-2013-A Generative Joint, Additive, Sequential Model of Topics and Speech Acts in Patient-Doctor Communication</a></p>
<p>20 0.072464004 <a title="77-tfidf-20" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.233), (1, 0.099), (2, -0.183), (3, -0.052), (4, 0.089), (5, -0.069), (6, 0.073), (7, -0.0), (8, 0.036), (9, -0.043), (10, -0.103), (11, -0.297), (12, -0.11), (13, 0.085), (14, 0.016), (15, 0.132), (16, 0.147), (17, 0.014), (18, -0.02), (19, -0.144), (20, 0.009), (21, -0.011), (22, -0.137), (23, 0.113), (24, -0.001), (25, 0.008), (26, -0.017), (27, 0.113), (28, -0.023), (29, 0.029), (30, -0.03), (31, -0.046), (32, -0.021), (33, -0.012), (34, 0.035), (35, 0.035), (36, -0.079), (37, -0.064), (38, 0.036), (39, -0.037), (40, -0.026), (41, -0.015), (42, 0.016), (43, 0.035), (44, 0.016), (45, -0.086), (46, 0.028), (47, -0.001), (48, -0.025), (49, 0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96257967 <a title="77-lsi-1" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>Author: Zhiyuan Chen ; Arjun Mukherjee ; Bing Liu ; Meichun Hsu ; Malu Castellanos ; Riddhiman Ghosh</p><p>Abstract: Aspect extraction is one of the key tasks in sentiment analysis. In recent years, statistical models have been used for the task. However, such models without any domain knowledge often produce aspects that are not interpretable in applications. To tackle the issue, some knowledge-based topic models have been proposed, which allow the user to input some prior domain knowledge to generate coherent aspects. However, existing knowledge-based topic models have several major shortcomings, e.g., little work has been done to incorporate the cannot-link type of knowledge or to automatically adjust the number of topics based on domain knowledge. This paper proposes a more advanced topic model, called MC-LDA (LDA with m-set and c-set), to address these problems, which is based on an Extended generalized Pólya urn (E-GPU) model (which is also proposed in this paper). Experiments on real-life product reviews from a variety of domains show that MCLDA outperforms the existing state-of-the-art models markedly.</p><p>2 0.79437256 <a title="77-lsi-2" href="./emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM.html">99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</a></p>
<p>Author: Wei Wang ; Hua Xu ; Xiaoqiu Huang</p><p>Abstract: Implicit feature detection, also known as implicit feature identification, is an essential aspect of feature-specific opinion mining but previous works have often ignored it. We think, based on the explicit sentences, several Support Vector Machine (SVM) classifiers can be established to do this task. Nevertheless, we believe it is possible to do better by using a constrained topic model instead of traditional attribute selection methods. Experiments show that this method outperforms the traditional attribute selection methods by a large margin and the detection task can be completed better.</p><p>3 0.73757547 <a title="77-lsi-3" href="./emnlp-2013-Improvements_to_the_Bayesian_Topic_N-Gram_Models.html">100 emnlp-2013-Improvements to the Bayesian Topic N-Gram Models</a></p>
<p>Author: Hiroshi Noji ; Daichi Mochihashi ; Yusuke Miyao</p><p>Abstract: One of the language phenomena that n-gram language model fails to capture is the topic information of a given situation. We advance the previous study of the Bayesian topic language model by Wallach (2006) in two directions: one, investigating new priors to alleviate the sparseness problem caused by dividing all ngrams into exclusive topics, and two, developing a novel Gibbs sampler that enables moving multiple n-grams across different documents to another topic. Our blocked sampler can efficiently search for higher probability space even with higher order n-grams. In terms of modeling assumption, we found it is effective to assign a topic to only some parts of a document.</p><p>4 0.7351644 <a title="77-lsi-4" href="./emnlp-2013-Learning_Topics_and_Positions_from_Debatepedia.html">121 emnlp-2013-Learning Topics and Positions from Debatepedia</a></p>
<p>Author: Swapna Gottipati ; Minghui Qiu ; Yanchuan Sim ; Jing Jiang ; Noah A. Smith</p><p>Abstract: We explore Debatepedia, a communityauthored encyclopedia of sociopolitical debates, as evidence for inferring a lowdimensional, human-interpretable representation in the domain of issues and positions. We introduce a generative model positing latent topics and cross-cutting positions that gives special treatment to person mentions and opinion words. We evaluate the resulting representation’s usefulness in attaching opinionated documents to arguments and its consistency with human judgments about positions.</p><p>5 0.68530971 <a title="77-lsi-5" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>Author: Xinjie Zhou ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style. In this paper, we study the problem of extracting opinion targets of Chinese microblog messages. Such fine-grained word-level task has not been well investigated in microblogs yet. We propose an unsupervised label propagation algorithm to address the problem. The opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets. Topics in microblogs are identified by hashtags or using clustering algorithms. Experimental results on Chinese microblogs show the effectiveness of our framework and algorithms.</p><p>6 0.58639199 <a title="77-lsi-6" href="./emnlp-2013-Identifying_Manipulated_Offerings_on_Review_Portals.html">94 emnlp-2013-Identifying Manipulated Offerings on Review Portals</a></p>
<p>7 0.56251311 <a title="77-lsi-7" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<p>8 0.54070503 <a title="77-lsi-8" href="./emnlp-2013-Using_Topic_Modeling_to_Improve_Prediction_of_Neuroticism_and_Depression_in_College_Students.html">199 emnlp-2013-Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students</a></p>
<p>9 0.50602323 <a title="77-lsi-9" href="./emnlp-2013-Modeling_Scientific_Impact_with_Topical_Influence_Regression.html">133 emnlp-2013-Modeling Scientific Impact with Topical Influence Regression</a></p>
<p>10 0.48910081 <a title="77-lsi-10" href="./emnlp-2013-Discourse_Level_Explanatory_Relation_Extraction_from_Product_Reviews_Using_First-Order_Logic.html">63 emnlp-2013-Discourse Level Explanatory Relation Extraction from Product Reviews Using First-Order Logic</a></p>
<p>11 0.45554495 <a title="77-lsi-11" href="./emnlp-2013-Optimized_Event_Storyline_Generation_based_on_Mixture-Event-Aspect_Model.html">147 emnlp-2013-Optimized Event Storyline Generation based on Mixture-Event-Aspect Model</a></p>
<p>12 0.45400426 <a title="77-lsi-12" href="./emnlp-2013-A_Generative_Joint%2C_Additive%2C_Sequential_Model_of_Topics_and_Speech_Acts_in_Patient-Doctor_Communication.html">6 emnlp-2013-A Generative Joint, Additive, Sequential Model of Topics and Speech Acts in Patient-Doctor Communication</a></p>
<p>13 0.45213956 <a title="77-lsi-13" href="./emnlp-2013-Opinion_Mining_in_Newspaper_Articles_by_Entropy-Based_Word_Connections.html">144 emnlp-2013-Opinion Mining in Newspaper Articles by Entropy-Based Word Connections</a></p>
<p>14 0.43139675 <a title="77-lsi-14" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>15 0.41981792 <a title="77-lsi-15" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>16 0.3981151 <a title="77-lsi-16" href="./emnlp-2013-Where_Not_to_Eat%3F_Improving_Public_Policy_by_Predicting_Hygiene_Inspections_Using_Online_Reviews.html">202 emnlp-2013-Where Not to Eat? Improving Public Policy by Predicting Hygiene Inspections Using Online Reviews</a></p>
<p>17 0.38558215 <a title="77-lsi-17" href="./emnlp-2013-A_Unified_Model_for_Topics%2C_Events_and_Users_on_Twitter.html">16 emnlp-2013-A Unified Model for Topics, Events and Users on Twitter</a></p>
<p>18 0.37953183 <a title="77-lsi-18" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>19 0.37647513 <a title="77-lsi-19" href="./emnlp-2013-Mining_New_Business_Opportunities%3A_Identifying_Trend_related_Products_by_Leveraging_Commercial_Intents_from_Microblogs.html">131 emnlp-2013-Mining New Business Opportunities: Identifying Trend related Products by Leveraging Commercial Intents from Microblogs</a></p>
<p>20 0.37295359 <a title="77-lsi-20" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.023), (9, 0.025), (11, 0.01), (18, 0.041), (22, 0.159), (30, 0.057), (31, 0.013), (50, 0.013), (51, 0.161), (66, 0.046), (71, 0.056), (74, 0.148), (75, 0.057), (77, 0.022), (96, 0.035), (97, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87437433 <a title="77-lda-1" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>Author: Zhiyuan Chen ; Arjun Mukherjee ; Bing Liu ; Meichun Hsu ; Malu Castellanos ; Riddhiman Ghosh</p><p>Abstract: Aspect extraction is one of the key tasks in sentiment analysis. In recent years, statistical models have been used for the task. However, such models without any domain knowledge often produce aspects that are not interpretable in applications. To tackle the issue, some knowledge-based topic models have been proposed, which allow the user to input some prior domain knowledge to generate coherent aspects. However, existing knowledge-based topic models have several major shortcomings, e.g., little work has been done to incorporate the cannot-link type of knowledge or to automatically adjust the number of topics based on domain knowledge. This paper proposes a more advanced topic model, called MC-LDA (LDA with m-set and c-set), to address these problems, which is based on an Extended generalized Pólya urn (E-GPU) model (which is also proposed in this paper). Experiments on real-life product reviews from a variety of domains show that MCLDA outperforms the existing state-of-the-art models markedly.</p><p>2 0.84118015 <a title="77-lda-2" href="./emnlp-2013-Sentiment_Analysis%3A_How_to_Derive_Prior_Polarities_from_SentiWordNet.html">170 emnlp-2013-Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet</a></p>
<p>Author: Marco Guerini ; Lorenzo Gatti ; Marco Turchi</p><p>Abstract: Assigning a positive or negative score to a word out of context (i.e. a word’s prior polarity) is a challenging task for sentiment analysis. In the literature, various approaches based on SentiWordNet have been proposed. In this paper, we compare the most often used techniques together with newly proposed ones and incorporate all of them in a learning framework to see whether blending them can further improve the estimation of prior polarity scores. Using two different versions of SentiWordNet and testing regression and classification models across tasks and datasets, our learning approach consistently outperforms the single metrics, providing a new state-ofthe-art approach in computing words’ prior polarity for sentiment analysis. We conclude our investigation showing interesting biases in calculated prior polarity scores when word Part of Speech and annotator gender are considered.</p><p>3 0.82444251 <a title="77-lda-3" href="./emnlp-2013-Building_Event_Threads_out_of_Multiple_News_Articles.html">41 emnlp-2013-Building Event Threads out of Multiple News Articles</a></p>
<p>Author: Xavier Tannier ; Veronique Moriceau</p><p>Abstract: We present an approach for building multidocument event threads from a large corpus of newswire articles. An event thread is basically a succession of events belonging to the same story. It helps the reader to contextualize the information contained in a single article, by navigating backward or forward in the thread from this article. A specific effort is also made on the detection of reactions to a particular event. In order to build these event threads, we use a cascade of classifiers and other modules, taking advantage of the redundancy of information in the newswire corpus. We also share interesting comments concerning our manual annotation procedure for building a training and testing set1.</p><p>4 0.81796956 <a title="77-lda-4" href="./emnlp-2013-Semi-Supervised_Feature_Transformation_for_Dependency_Parsing.html">168 emnlp-2013-Semi-Supervised Feature Transformation for Dependency Parsing</a></p>
<p>Author: Wenliang Chen ; Min Zhang ; Yue Zhang</p><p>Abstract: In current dependency parsing models, conventional features (i.e. base features) defined over surface words and part-of-speech tags in a relatively high-dimensional feature space may suffer from the data sparseness problem and thus exhibit less discriminative power on unseen data. In this paper, we propose a novel semi-supervised approach to addressing the problem by transforming the base features into high-level features (i.e. meta features) with the help of a large amount of automatically parsed data. The meta features are used together with base features in our final parser. Our studies indicate that our proposed approach is very effective in processing unseen data and features. Experiments on Chinese and English data sets show that the final parser achieves the best-reported accuracy on the Chinese data and comparable accuracy with the best known parsers on the English data.</p><p>5 0.81082165 <a title="77-lda-5" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>Author: Lei Cui ; Xilun Chen ; Dongdong Zhang ; Shujie Liu ; Mu Li ; Ming Zhou</p><p>Abstract: Domain adaptation for SMT usually adapts models to an individual specific domain. However, it often lacks some correlation among different domains where common knowledge could be shared to improve the overall translation quality. In this paper, we propose a novel multi-domain adaptation approach for SMT using Multi-Task Learning (MTL), with in-domain models tailored for each specific domain and a general-domain model shared by different domains. The parameters of these models are tuned jointly via MTL so that they can learn general knowledge more accurately and exploit domain knowledge better. Our experiments on a largescale English-to-Chinese translation task validate that the MTL-based adaptation approach significantly and consistently improves the translation quality compared to a non-adapted baseline. Furthermore, it also outperforms the individual adaptation of each specific domain.</p><p>6 0.80223644 <a title="77-lda-6" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<p>7 0.80181342 <a title="77-lda-7" href="./emnlp-2013-Event-Based_Time_Label_Propagation_for_Automatic_Dating_of_News_Articles.html">74 emnlp-2013-Event-Based Time Label Propagation for Automatic Dating of News Articles</a></p>
<p>8 0.78130072 <a title="77-lda-8" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>9 0.77530837 <a title="77-lda-9" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>10 0.77095461 <a title="77-lda-10" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>11 0.76243138 <a title="77-lda-11" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>12 0.75851679 <a title="77-lda-12" href="./emnlp-2013-Learning_Biological_Processes_with_Global_Constraints.html">118 emnlp-2013-Learning Biological Processes with Global Constraints</a></p>
<p>13 0.75699353 <a title="77-lda-13" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>14 0.75491107 <a title="77-lda-14" href="./emnlp-2013-An_Empirical_Study_Of_Semi-Supervised_Chinese_Word_Segmentation_Using_Co-Training.html">21 emnlp-2013-An Empirical Study Of Semi-Supervised Chinese Word Segmentation Using Co-Training</a></p>
<p>15 0.74851024 <a title="77-lda-15" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>16 0.74835527 <a title="77-lda-16" href="./emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM.html">99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</a></p>
<p>17 0.7474485 <a title="77-lda-17" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>18 0.74711376 <a title="77-lda-18" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>19 0.74291122 <a title="77-lda-19" href="./emnlp-2013-Document_Summarization_via_Guided_Sentence_Compression.html">65 emnlp-2013-Document Summarization via Guided Sentence Compression</a></p>
<p>20 0.74186808 <a title="77-lda-20" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
