<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>143 emnlp-2013-Open Domain Targeted Sentiment</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-143" href="#">emnlp2013-143</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>143 emnlp-2013-Open Domain Targeted Sentiment</h1>
<br/><p>Source: <a title="emnlp-2013-143-pdf" href="http://aclweb.org/anthology//D/D13/D13-1171.pdf">pdf</a></p><p>Author: Margaret Mitchell ; Jacqui Aguilar ; Theresa Wilson ; Benjamin Van Durme</p><p>Abstract: We propose a novel approach to sentiment analysis for a low resource setting. The intuition behind this work is that sentiment expressed towards an entity, targeted sentiment, may be viewed as a span of sentiment expressed across the entity. This representation allows us to model sentiment detection as a sequence tagging problem, jointly discovering people and organizations along with whether there is sentiment directed towards them. We compare performance in both Spanish and English on microblog data, using only a sentiment lexicon as an external resource. By leveraging linguisticallyinformed features within conditional random fields (CRFs) trained to minimize empirical risk, our best models in Spanish significantly outperform a strong baseline, and reach around 90% accuracy on the combined task of named entity recognition and sentiment prediction. Our models in English, trained on a much smaller dataset, are not yet statistically significant against their baselines.</p><p>Reference: <a title="emnlp-2013-143-reference" href="../emnlp2013_reference/emnlp-2013-Open_Domain_Targeted_Sentiment_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu s  Abstract We propose a novel approach to sentiment analysis for a low resource setting. [sent-8, score-0.734]
</p><p>2 The intuition behind this work is that sentiment expressed towards an entity, targeted sentiment, may be viewed as a span of sentiment expressed across the entity. [sent-9, score-1.998]
</p><p>3 This representation allows us to model sentiment detection as a sequence tagging problem, jointly discovering people and organizations along with whether there is sentiment directed towards them. [sent-10, score-1.633]
</p><p>4 We compare performance in both Spanish and English on microblog data, using only a sentiment lexicon as an external resource. [sent-11, score-0.772]
</p><p>5 By leveraging linguisticallyinformed features within conditional random fields (CRFs) trained to minimize empirical risk, our best models in Spanish significantly outperform a strong baseline, and reach around 90% accuracy on the combined task of named entity recognition and sentiment prediction. [sent-12, score-0.93]
</p><p>6 Determining when a positive or negative sentiment is being expressed is a large part of the challenge, but identifying other attributes, such as the target of the sentiment, is also crucial if the ultimate goal is to pinpoint and extract opinions. [sent-15, score-0.904]
</p><p>7 In (1), although there is a positive sentiment, the target of the sentiment is an event (Kentucky losing to Tennessee). [sent-23, score-0.807]
</p><p>8 However, from the positive sentiment toward this event, we can infer that the speaker has a negative sentiment toward Kentucky and a positive  sentiment toward Tennessee. [sent-24, score-2.466]
</p><p>9 In (2), the positive sentiment is toward a future event, but we are not given enough information to infer a sentiment toward the mentioned entities. [sent-25, score-1.602]
</p><p>10 We can also infer a positive sentiment toward Douglas’s Syracuse teams, and even toward Douglas himself. [sent-27, score-0.868]
</p><p>11 These examples illustrate the importance of the target when interpreting sentiment in context. [sent-28, score-0.759]
</p><p>12 However, if we are looking for sentiment toward Tennessee, we would want to identify (1) as positive, and (2) and (3) as neutral. [sent-30, score-0.777]
</p><p>13 The expression of these and other kinds of sentiment can be understood as involving three items: (1) An experiencer (2) An attitude (3) A target (optionally) Research in sentiment analysis often focuses on (2), predicting overall sentiment polarity (Agarwal et al. [sent-31, score-2.388]
</p><p>14 Recent work has begun to combine (2) with (3), examining how to automatically  predict the sentiment polarity expressed towards a target entity (Jiang et al. [sent-33, score-1.122]
</p><p>15 This topic-dependent sentiment classification requires that the target entity be Proce Sdeiantgtlse o,f W thaesh 2i0n1gt3o nC,o UnSfeAre,n 1c8e- o2n1 E Omctpoibriecra 2l0 M13et. [sent-36, score-0.878]
</p><p>16 given, and returns statements expressing sentiment towards the given entity. [sent-39, score-0.809]
</p><p>17 In this paper, we take a step towards open-domain, targeted sentiment analysis by investigating how to detect both the named entity and the sentiment expressed toward it. [sent-40, score-2.152]
</p><p>18 We focus on people and organizations (volitional named entities), which are the primary targets of sentiment in our microblog data (see Table 1). [sent-43, score-0.894]
</p><p>19 We develop such models to jointly predict the NE and the sentiment expressed towards it using minimum risk training (Stoyanov and Eisner, 2012). [sent-47, score-0.903]
</p><p>20 Our ultimate goal is to develop models that will be useful for low resource languages, where a sentiment lexicon may be known or bootstrapped, but more sophisticated linguistic tools may not be readily available. [sent-49, score-0.772]
</p><p>21 We therefore do not rely on an external part-of-speech tagger or parser, which are often used for features in fine-grained sentiment analysis; such tools are not available in many languages, and ifthey are, are not usually adapted for noisy social media. [sent-50, score-0.761]
</p><p>22 Instead, we use information from sentiment lexicons and some simple hand-written features, and otherwise use only features of the word that can be 1www. [sent-51, score-0.734]
</p><p>23 2 2  Related Work  As the scale of social media has grown, using sources such as Twitter to mine public sentiment has become increasingly promising. [sent-66, score-0.761]
</p><p>24 The majority of academic research has focused on supervised classification of message sentiment irrespective of target (Barbosa and Feng, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al. [sent-68, score-0.81]
</p><p>25 Large datasets are collected for this work by  leveraging the sentiment inherent in emoticons (e. [sent-72, score-0.734]
</p><p>26 , 2012); tracking changing sentiment during debates (Diakopoulos and Shamma, 2010); and how orthographic conventions such as word-lengthening can be used to adapt a Twitter-specific sentiment lexicon (Brody and Diakopoulos, 2011). [sent-91, score-1.506]
</p><p>27 Efforts in targeted sentiment (Bermingham and Smeaton, 2010; Jin and Ho, 2009; Li et al. [sent-92, score-1.046]
</p><p>28 In these approaches, messages are collected on a fixed set of  topics/targets, such as products or sports teams, and sentiment is learned for the given set. [sent-99, score-0.734]
</p><p>29 In contrast, we aim to predict sentiment in tweets for any named person or organization. [sent-100, score-0.867]
</p><p>30 We refer to this task as open domain targeted sentiment analysis. [sent-101, score-1.046]
</p><p>31 Within topic-dependent sentiment analysis, several approaches have explored applying CRFs or HMMs to extract sentiment and target words from text (Jin and Ho, 2009; Li et al. [sent-102, score-1.493]
</p><p>32 They do not use joint learning, but they do incorporate a number of parse-based features designed to capture relationships between sentiment terms and topic references. [sent-108, score-0.767]
</p><p>33 In contrast, we model the expression of sentiment polarity across the sentiment target itself, extracting both the sentiment target and the sentiment expressed towards it within the same span of words. [sent-112, score-3.278]
</p><p>34 This allows us to use surrounding context to determine sentiment polarity without identifying explicit opinion expressions or relying on a parser to help link expression to target. [sent-113, score-0.979]
</p><p>35 Most work in targeted sentiment outside the microblogging domain has been in relation to product review mining (e. [sent-114, score-1.046]
</p><p>36 jointly learns  targets and opinion words, and Jakob and Gurevych (2010) use CRFs to extract the targets of opinions, but do not attempt to classify the sentiment toward these targets. [sent-121, score-0.986]
</p><p>37 To the best of our knowledge, this is the first work to approach targeted sentiment in a low resource setting and to jointly predict NEs and targeted sentiment. [sent-122, score-1.358]
</p><p>38 Targeted sentiment percentages are based on expert annotations from a random sample of 10 (or all) of of each entity. [sent-127, score-0.734]
</p><p>39 Sentiment Lexicons We use two sentiment lexicon sources in each language. [sent-134, score-0.772]
</p><p>40 Annotation To collect sentiment labels, we use crowdsourcing through Amazon’s Mechanical Turk. [sent-149, score-0.734]
</p><p>41 Turkers were instructed to (1) select the sentiment being expressed towards the entity (positive, negative, or no sentiment); and (2) rate their level of confidence in their selection. [sent-151, score-0.986]
</p><p>42 com/mturk 1646 0  Figure 4: Targeted sentiment annotated for Spanish. [sent-157, score-0.734]
</p><p>43 rioMintyNEUNPTEORSGAL17 P50O27S 9NMEUajT21oR1r27iA542t19Ly6N41 E735 0G2 Table 2: Number of targeted sentiment instances where at least two of the three annotators (Majority) agreed. [sent-158, score-1.071]
</p><p>44 Common disagreements with a third annotator (Minority) were over whether no sentiment or positive sentiment was expressed, and whether no sentiment or negative sentment was expressed. [sent-159, score-2.289]
</p><p>45 The distribution of sentiment for the named entities annotated by Turkers is shown in Figure 4. [sent-165, score-0.885]
</p><p>46 Neutral (no targeted sentiment) dominates, followed by positive sentiment for both organizations and people. [sent-166, score-1.143]
</p><p>47 This is in line with  previous research showing that distinguishing positive sentiment from no sentiment (and distinguishing negative sentiment from no sentiment) is often more challenging than distinguishing between positive and negative sentiment (Wilson et al. [sent-168, score-3.11]
</p><p>48 The COLL models collapse both targeted sentiment and NE label into one node. [sent-174, score-1.073]
</p><p>49 ln), hweh perroe blia ∈ ttyhe o fse at of named entity values; and a sentiment sequence s = (s1 . [sent-181, score-0.93]
</p><p>50 ed in a CRF as a se6For the COLL models, this is instead the conditional distribution p(y|w), where entity and sentiment labels are conjoined ibnu one sequence assignment y. [sent-191, score-0.882]
</p><p>51 1647 quence of random variables for sentiment s connected to named entities l. [sent-192, score-0.906]
</p><p>52 Moving from targeted subjectivity prediction to targeted sentiment prediction is possible by changing the sentiment target (SENT-TARG) variable into two variables, one for positive targeted sentiment (POS) and one for negative (NEG). [sent-196, score-3.415]
</p><p>53 Possible values for targeted subjectivity are shown in Table 3, and possible values for targeted sentiment are shown in Table 4. [sent-197, score-1.479]
</p><p>54 In a second model, every observed volitional entity nno ade s eisc ocnodnn mecodteedl by a rfyac otobrto a sentiment label si ∈ s. [sent-199, score-1.132]
</p><p>55 ll seennttiimmeenntt nvar tihaibsl emso are treated as latent except for the sentiment connected to the volitional entity. [sent-203, score-0.986]
</p><p>56 In the collapsed models (COLL), we combine sentiment and named entity into one label sequence (e. [sent-205, score-0.993]
</p><p>57 The JOINT and PIPE models therefore predict named entity sequences, their category labels, and the sentiment expressed towards volitional named entities. [sent-209, score-1.392]
</p><p>58 7 The collapsed models predict volitional labels and targeted sentiment as combined categories. [sent-210, score-1.363]
</p><p>59 We utilize a speaker of each language to simply list word forms for sentiment features that may be indicative of sentiment, totaling less than two hours of annotation time. [sent-259, score-0.734]
</p><p>60 We compare against a baseline (BASE-NS) where we use our volitional entity labels and assign no sentiment directed towards the entity (the majority case). [sent-273, score-1.389]
</p><p>61 This is a strong baseline to isolate how our methods perform specifically for the task of identifying sentiment targeted at an entity. [sent-274, score-1.046]
</p><p>62 We report on precision, recall, and sensitivity for the tasks of NER and targeted subjectivity/sentiment prediction in isolation; and we report on accuracy for the targeted subjectivity and targeted sentiment models. [sent-275, score-1.813]
</p><p>63 For sentiment, a true positive is an instance where the label has sentiment, and a true negative is an instance where the label has no sentiment (neutral). [sent-276, score-0.875]
</p><p>64 The three systems are evaluated against one another for NER, subjectivity (entity has/does not have sentiment expressed towards it), and sentiment (positive/negative/no sentiment) using paired t-tests across folds, with a Bonferroni correction to set α to 0. [sent-278, score-1.722]
</p><p>65 Subjectivity and Sentiment Table 7 shows results for the isolated task of predicting the presence of sentiment about a volitional entity. [sent-291, score-1.015]
</p><p>66 ation data, and evaluate sentiment polarity (positive/negative) separately from subjectivity (has/does not have sentiment). [sent-301, score-0.966]
</p><p>67 Our dataset includes any entity labeled as PERSON or ORGANIZATION, and is not balanced (most targets have no sentiment expressed towards them; see Table 1), thus we can only roughly compare against their approach. [sent-302, score-1.045]
</p><p>68 Table 8 shows results for the task of predicting the polarity of the sentiment expressed about an entity. [sent-311, score-0.932]
</p><p>69 for sentiment prediction (positive/negative/no sentiment) along the target entity. [sent-325, score-0.811]
</p><p>70 perform the COLL models on sentiment recall, and the JOINT models on sentiment precision (p<. [sent-326, score-1.468]
</p><p>71 We now examine results for targeted subjectivity labeling an entity and predicting whether there is sentiment directed towards it in Table 9; and targeted sentiment labeling an entity and predicting what the sentiment directed towards it is in Table 10. [sent-331, score-3.465]
</p><p>72 We evaluate using two accuracy metrics: Acc-all, which measures the accuracy ofthe entire named entity span along with the sentiment span; and AccBsent, which measures the accuracy of identifying the start of a named entity (B- labels) along with the sentiment expressed towards it. [sent-332, score-2.08]
</p><p>73 In English, where our data is half the size, we do not see a statis–  –  –  –  tically significant difference between the predictive models and the no sentiment baselines. [sent-336, score-0.734]
</p><p>74 For the targeted sentiment task, the JOINT models again perform relatively well in Spanish (Table 10), labeling volitional entities, predicting whether or not there is sentiment targeted towards them, and  Model  Joint JBoaisnet Pipe BPaipsee Coll BCaolsel  apSAAcc c- aBlslent8329. [sent-337, score-2.448]
</p><p>75 05 Table 9: Average accuracy on Targeted Subjectivity Prediction: Identifying volitional entities and whether they are a sentiment target. [sent-364, score-1.06]
</p><p>76 05  Table 10: Average accuracy on Targeted Sentiment Prediction: Identifying volitional entities and the polarity of the sentiment expressed towards them. [sent-386, score-1.304]
</p><p>77 We find this to be the most difficult task: It may be clear that sentiment is being expressed towards an entity, but it is not always clear what the polarity of that sentiment is. [sent-390, score-1.712]
</p><p>78 In the smaller English set, the models do not outperform the no sentiment baseline. [sent-392, score-0.734]
</p><p>79 Error Analysis Because it is relatively common for there not to be sentiment targeted at a named entity, it is difficult to tease out the polarity in instances where there is targeted sentiment. [sent-395, score-1.546]
</p><p>80 In addition to lexical identity, we find that curse words and positive and negative prefixes are used to detect volitional entities and the sentiment directed towards them. [sent-399, score-1.304]
</p><p>81 an entity) with I- labels (inside an entity); and by predicting sentiment polarity when the gold annotations say there is not sentiment targeted at the entity. [sent-400, score-1.949]
</p><p>82 (b) For sentiment, most common mistakes were to predict that a positive sentiment was neutral (no sentiment), and that a neutral sentiment was negative. [sent-409, score-1.602]
</p><p>83 taO-,BNG B-OEiVe TGOs -eALTc IAkTIRe VOGENAL-ayO ndBNE -Oi Vg TuO-iLTg IAuTRr IOe GnNAL sentiment may not be clear without spelling correction: “dio” should be “dios”, meaning “God”; otherwise, “dio” is the word for “gave”. [sent-428, score-0.734]
</p><p>84 ) were likely used as indicators of positive sentiment; however, in this case the annotators marked the targeted sentiment as neutral. [sent-433, score-1.119]
</p><p>85 It was also predicted that both “Giesecke” and “Eiguiguren” had no sentiment expressed towards them; annotators disagreed, with the majority of those who annotated “Giesecke” marking negative sentiment, and the majority of those who annotated “Eiguiguren” marking no sentiment. [sent-435, score-0.981]
</p><p>86 This highlights some of the difficulty in predicting sentiment discussed in Section 3, where annotators will often disagree as to whether there is no sentiment or positive/negative sentiment. [sent-436, score-1.522]
</p><p>87 1652  8  Conclusion  We have introduced the task ofopen domain targeted sentiment: predicting sentiment directed towards an entity along with discovering the entity itself. [sent-439, score-1.454]
</p><p>88 Our approach is developed to find targeted sentiment towards both person and organization named entities by modeling sentiment as a span along the entity. [sent-440, score-2.121]
</p><p>89 We find that by modeling targeted sentiment in this way, we can reliably detect entities and whether or not they are sentiment targets above a no sentiment baseline. [sent-441, score-2.647]
</p><p>90 How best to determine the polarity of the sentiment expressed towards the entity, however, is still an open issue. [sent-442, score-0.978]
</p><p>91 Our data suggests that it is usually not clear-cut whether sentiment is being expressed or not; the strong disagreement between annotators suggests that detecting sentiment polarity in microblogs is difficult even for humans. [sent-443, score-1.662]
</p><p>92 In future work, we hope to explore further methods for teasing apart sentiment polarity expressed towards a target. [sent-444, score-0.978]
</p><p>93 This research has achieved promising results for detecting sentiment targets without relying on external supervised models, and we hope that the features and approaches developed here can aid in sentiment analysis in noisy text and languages  without rich linguistic resources. [sent-445, score-1.527]
</p><p>94 0: An enhanced lexical resource for sentiment analysis and opinion mining. [sent-458, score-0.825]
</p><p>95 Robust sentiment detection on Twitter from biased and noisy data. [sent-462, score-0.734]
</p><p>96 Extracting diverse sentiment expressions with target-dependent polarity from twitter. [sent-527, score-0.867]
</p><p>97 Combining social cognitive theories with linguistic features for multi-genre sentiment analysis. [sent-618, score-0.761]
</p><p>98 Twitter as a corpus for sentiment analysis and opinion mining. [sent-633, score-0.825]
</p><p>99 Exploring sentiment in social media: Bootstrapping subjectivity clues from multilingual twitter streams. [sent-683, score-0.956]
</p><p>100 Topic sentiment analysis in Twitter: A graph-based hashtag sentiment classification approach. [sent-687, score-1.468]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sentiment', 0.734), ('targeted', 0.312), ('volitional', 0.252), ('coll', 0.161), ('pipe', 0.16), ('spanish', 0.128), ('subjectivity', 0.121), ('entity', 0.119), ('polarity', 0.111), ('opinion', 0.091), ('kentucky', 0.08), ('named', 0.077), ('towards', 0.075), ('twitter', 0.074), ('entities', 0.074), ('syllable', 0.068), ('targets', 0.059), ('expressed', 0.058), ('ne', 0.051), ('positive', 0.048), ('jerboa', 0.046), ('sonority', 0.046), ('curse', 0.046), ('toward', 0.043), ('neutral', 0.043), ('wilson', 0.042), ('stoyanov', 0.04), ('neg', 0.04), ('jiang', 0.04), ('negative', 0.039), ('preceded', 0.038), ('lexicon', 0.038), ('organization', 0.036), ('risk', 0.036), ('collapsed', 0.036), ('directed', 0.036), ('tennessee', 0.034), ('tweets', 0.034), ('brown', 0.034), ('ner', 0.034), ('joint', 0.033), ('nes', 0.032), ('syllables', 0.03), ('exclamation', 0.03), ('along', 0.03), ('labels', 0.029), ('turkers', 0.029), ('predicting', 0.029), ('crfs', 0.027), ('diakopoulos', 0.027), ('laugh', 0.027), ('label', 0.027), ('span', 0.027), ('social', 0.027), ('message', 0.026), ('teams', 0.025), ('excellence', 0.025), ('annotators', 0.025), ('target', 0.025), ('followed', 0.025), ('majority', 0.025), ('theresa', 0.024), ('organizations', 0.024), ('endings', 0.024), ('hu', 0.024), ('li', 0.024), ('accbsent', 0.023), ('dio', 0.023), ('eiguiguren', 0.023), ('erma', 0.023), ('gaga', 0.023), ('giesecke', 0.023), ('guarenas', 0.023), ('hamala', 0.023), ('htweet', 0.023), ('ipad', 0.023), ('juancito', 0.023), ('kosinskia', 0.023), ('muy', 0.023), ('onsets', 0.023), ('positivenot', 0.023), ('sequencing', 0.023), ('syllabified', 0.023), ('syllabify', 0.023), ('syracuse', 0.023), ('variablepossible', 0.023), ('english', 0.023), ('proceedings', 0.023), ('prediction', 0.022), ('person', 0.022), ('johns', 0.022), ('agarwal', 0.022), ('capitalized', 0.022), ('expressions', 0.022), ('expression', 0.021), ('variables', 0.021), ('crf', 0.021), ('phonological', 0.021), ('tang', 0.021), ('cluster', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="143-tfidf-1" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>Author: Margaret Mitchell ; Jacqui Aguilar ; Theresa Wilson ; Benjamin Van Durme</p><p>Abstract: We propose a novel approach to sentiment analysis for a low resource setting. The intuition behind this work is that sentiment expressed towards an entity, targeted sentiment, may be viewed as a span of sentiment expressed across the entity. This representation allows us to model sentiment detection as a sequence tagging problem, jointly discovering people and organizations along with whether there is sentiment directed towards them. We compare performance in both Spanish and English on microblog data, using only a sentiment lexicon as an external resource. By leveraging linguisticallyinformed features within conditional random fields (CRFs) trained to minimize empirical risk, our best models in Spanish significantly outperform a strong baseline, and reach around 90% accuracy on the combined task of named entity recognition and sentiment prediction. Our models in English, trained on a much smaller dataset, are not yet statistically significant against their baselines.</p><p>2 0.3952238 <a title="143-tfidf-2" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>Author: Shi Feng ; Le Zhang ; Binyang Li ; Daling Wang ; Ge Yu ; Kam-Fai Wong</p><p>Abstract: Extensive experiments have validated the effectiveness of the corpus-based method for classifying the word’s sentiment polarity. However, no work is done for comparing different corpora in the polarity classification task. Nowadays, Twitter has aggregated huge amount of data that are full of people’s sentiments. In this paper, we empirically evaluate the performance of different corpora in sentiment similarity measurement, which is the fundamental task for word polarity classification. Experiment results show that the Twitter data can achieve a much better performance than the Google, Web1T and Wikipedia based methods.</p><p>3 0.31469214 <a title="143-tfidf-3" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>Author: Svitlana Volkova ; Theresa Wilson ; David Yarowsky</p><p>Abstract: Theresa Wilson Human Language Technology Center of Excellence Johns Hopkins University Baltimore, MD t aw@ j hu .edu differences may Different demographics, e.g., gender or age, can demonstrate substantial variation in their language use, particularly in informal contexts such as social media. In this paper we focus on learning gender differences in the use of subjective language in English, Spanish, and Russian Twitter data, and explore cross-cultural differences in emoticon and hashtag use for male and female users. We show that gender differences in subjective language can effectively be used to improve sentiment analysis, and in particular, polarity classification for Spanish and Russian. Our results show statistically significant relative F-measure improvement over the gender-independent baseline 1.5% and 1% for Russian, 2% and 0.5% for Spanish, and 2.5% and 5% for English for polarity and subjectivity classification.</p><p>4 0.28148776 <a title="143-tfidf-4" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>Author: Richard Socher ; Alex Perelygin ; Jean Wu ; Jason Chuang ; Christopher D. Manning ; Andrew Ng ; Christopher Potts</p><p>Abstract: Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.</p><p>5 0.21300635 <a title="143-tfidf-5" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>Author: Xinjie Zhou ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style. In this paper, we study the problem of extracting opinion targets of Chinese microblog messages. Such fine-grained word-level task has not been well investigated in microblogs yet. We propose an unsupervised label propagation algorithm to address the problem. The opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets. Topics in microblogs are identified by hashtags or using clustering algorithms. Experimental results on Chinese microblogs show the effectiveness of our framework and algorithms.</p><p>6 0.21297292 <a title="143-tfidf-6" href="./emnlp-2013-Sarcasm_as_Contrast_between_a_Positive_Sentiment_and_Negative_Situation.html">163 emnlp-2013-Sarcasm as Contrast between a Positive Sentiment and Negative Situation</a></p>
<p>7 0.16316837 <a title="143-tfidf-7" href="./emnlp-2013-Sentiment_Analysis%3A_How_to_Derive_Prior_Polarities_from_SentiWordNet.html">170 emnlp-2013-Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet</a></p>
<p>8 0.15880774 <a title="143-tfidf-8" href="./emnlp-2013-Opinion_Mining_in_Newspaper_Articles_by_Entropy-Based_Word_Connections.html">144 emnlp-2013-Opinion Mining in Newspaper Articles by Entropy-Based Word Connections</a></p>
<p>9 0.15463582 <a title="143-tfidf-9" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>10 0.14689292 <a title="143-tfidf-10" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>11 0.14238316 <a title="143-tfidf-11" href="./emnlp-2013-Discourse_Level_Explanatory_Relation_Extraction_from_Product_Reviews_Using_First-Order_Logic.html">63 emnlp-2013-Discourse Level Explanatory Relation Extraction from Product Reviews Using First-Order Logic</a></p>
<p>12 0.13057148 <a title="143-tfidf-12" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>13 0.12583579 <a title="143-tfidf-13" href="./emnlp-2013-Learning_Topics_and_Positions_from_Debatepedia.html">121 emnlp-2013-Learning Topics and Positions from Debatepedia</a></p>
<p>14 0.11116894 <a title="143-tfidf-14" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>15 0.087461993 <a title="143-tfidf-15" href="./emnlp-2013-Using_Crowdsourcing_to_get_Representations_based_on_Regular_Expressions.html">196 emnlp-2013-Using Crowdsourcing to get Representations based on Regular Expressions</a></p>
<p>16 0.083831526 <a title="143-tfidf-16" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>17 0.070647165 <a title="143-tfidf-17" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>18 0.06704282 <a title="143-tfidf-18" href="./emnlp-2013-Success_with_Style%3A_Using_Writing_Style_to_Predict_the_Success_of_Novels.html">178 emnlp-2013-Success with Style: Using Writing Style to Predict the Success of Novels</a></p>
<p>19 0.066920213 <a title="143-tfidf-19" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>20 0.065612234 <a title="143-tfidf-20" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.233), (1, 0.131), (2, -0.28), (3, -0.362), (4, 0.222), (5, -0.145), (6, -0.063), (7, -0.182), (8, 0.172), (9, 0.19), (10, 0.013), (11, -0.083), (12, 0.025), (13, -0.038), (14, -0.113), (15, -0.051), (16, -0.004), (17, -0.004), (18, 0.164), (19, 0.021), (20, 0.073), (21, -0.066), (22, -0.041), (23, -0.038), (24, 0.155), (25, -0.014), (26, 0.078), (27, -0.058), (28, 0.054), (29, -0.031), (30, 0.079), (31, -0.031), (32, -0.024), (33, 0.071), (34, 0.004), (35, -0.019), (36, 0.0), (37, 0.017), (38, -0.081), (39, -0.004), (40, -0.02), (41, -0.011), (42, 0.003), (43, 0.017), (44, -0.015), (45, -0.046), (46, -0.045), (47, 0.05), (48, -0.005), (49, -0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98954046 <a title="143-lsi-1" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>Author: Margaret Mitchell ; Jacqui Aguilar ; Theresa Wilson ; Benjamin Van Durme</p><p>Abstract: We propose a novel approach to sentiment analysis for a low resource setting. The intuition behind this work is that sentiment expressed towards an entity, targeted sentiment, may be viewed as a span of sentiment expressed across the entity. This representation allows us to model sentiment detection as a sequence tagging problem, jointly discovering people and organizations along with whether there is sentiment directed towards them. We compare performance in both Spanish and English on microblog data, using only a sentiment lexicon as an external resource. By leveraging linguisticallyinformed features within conditional random fields (CRFs) trained to minimize empirical risk, our best models in Spanish significantly outperform a strong baseline, and reach around 90% accuracy on the combined task of named entity recognition and sentiment prediction. Our models in English, trained on a much smaller dataset, are not yet statistically significant against their baselines.</p><p>2 0.80829197 <a title="143-lsi-2" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>Author: Shi Feng ; Le Zhang ; Binyang Li ; Daling Wang ; Ge Yu ; Kam-Fai Wong</p><p>Abstract: Extensive experiments have validated the effectiveness of the corpus-based method for classifying the word’s sentiment polarity. However, no work is done for comparing different corpora in the polarity classification task. Nowadays, Twitter has aggregated huge amount of data that are full of people’s sentiments. In this paper, we empirically evaluate the performance of different corpora in sentiment similarity measurement, which is the fundamental task for word polarity classification. Experiment results show that the Twitter data can achieve a much better performance than the Google, Web1T and Wikipedia based methods.</p><p>3 0.8079809 <a title="143-lsi-3" href="./emnlp-2013-Sarcasm_as_Contrast_between_a_Positive_Sentiment_and_Negative_Situation.html">163 emnlp-2013-Sarcasm as Contrast between a Positive Sentiment and Negative Situation</a></p>
<p>Author: Ellen Riloff ; Ashequl Qadir ; Prafulla Surve ; Lalindra De Silva ; Nathan Gilbert ; Ruihong Huang</p><p>Abstract: A common form of sarcasm on Twitter consists of a positive sentiment contrasted with a negative situation. For example, many sarcastic tweets include a positive sentiment, such as “love” or “enjoy”, followed by an expression that describes an undesirable activity or state (e.g., “taking exams” or “being ignored”). We have developed a sarcasm recognizer to identify this type of sarcasm in tweets. We present a novel bootstrapping algorithm that automatically learns lists of positive sentiment phrases and negative situation phrases from sarcastic tweets. We show that identifying contrasting contexts using the phrases learned through bootstrapping yields improved recall for sarcasm recognition.</p><p>4 0.74039698 <a title="143-lsi-4" href="./emnlp-2013-Opinion_Mining_in_Newspaper_Articles_by_Entropy-Based_Word_Connections.html">144 emnlp-2013-Opinion Mining in Newspaper Articles by Entropy-Based Word Connections</a></p>
<p>Author: Thomas Scholz ; Stefan Conrad</p><p>Abstract: A very valuable piece of information in newspaper articles is the tonality of extracted statements. For the analysis of tonality of newspaper articles either a big human effort is needed, when it is carried out by media analysts, or an automated approach which has to be as accurate as possible for a Media Response Analysis (MRA). To this end, we will compare several state-of-the-art approaches for Opinion Mining in newspaper articles in this paper. Furthermore, we will introduce a new technique to extract entropy-based word connections which identifies the word combinations which create a tonality. In the evaluation, we use two different corpora consisting of news articles, by which we show that the new approach achieves better results than the four state-of-the-art methods.</p><p>5 0.73643136 <a title="143-lsi-5" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>Author: Svitlana Volkova ; Theresa Wilson ; David Yarowsky</p><p>Abstract: Theresa Wilson Human Language Technology Center of Excellence Johns Hopkins University Baltimore, MD t aw@ j hu .edu differences may Different demographics, e.g., gender or age, can demonstrate substantial variation in their language use, particularly in informal contexts such as social media. In this paper we focus on learning gender differences in the use of subjective language in English, Spanish, and Russian Twitter data, and explore cross-cultural differences in emoticon and hashtag use for male and female users. We show that gender differences in subjective language can effectively be used to improve sentiment analysis, and in particular, polarity classification for Spanish and Russian. Our results show statistically significant relative F-measure improvement over the gender-independent baseline 1.5% and 1% for Russian, 2% and 0.5% for Spanish, and 2.5% and 5% for English for polarity and subjectivity classification.</p><p>6 0.67902476 <a title="143-lsi-6" href="./emnlp-2013-Sentiment_Analysis%3A_How_to_Derive_Prior_Polarities_from_SentiWordNet.html">170 emnlp-2013-Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet</a></p>
<p>7 0.65063179 <a title="143-lsi-7" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>8 0.53022349 <a title="143-lsi-8" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>9 0.51959223 <a title="143-lsi-9" href="./emnlp-2013-Using_Crowdsourcing_to_get_Representations_based_on_Regular_Expressions.html">196 emnlp-2013-Using Crowdsourcing to get Representations based on Regular Expressions</a></p>
<p>10 0.5089184 <a title="143-lsi-10" href="./emnlp-2013-Discourse_Level_Explanatory_Relation_Extraction_from_Product_Reviews_Using_First-Order_Logic.html">63 emnlp-2013-Discourse Level Explanatory Relation Extraction from Product Reviews Using First-Order Logic</a></p>
<p>11 0.40702182 <a title="143-lsi-11" href="./emnlp-2013-Learning_Topics_and_Positions_from_Debatepedia.html">121 emnlp-2013-Learning Topics and Positions from Debatepedia</a></p>
<p>12 0.34822792 <a title="143-lsi-12" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>13 0.3478522 <a title="143-lsi-13" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>14 0.34428391 <a title="143-lsi-14" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>15 0.33454788 <a title="143-lsi-15" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>16 0.33353573 <a title="143-lsi-16" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>17 0.28792301 <a title="143-lsi-17" href="./emnlp-2013-Success_with_Style%3A_Using_Writing_Style_to_Predict_the_Success_of_Novels.html">178 emnlp-2013-Success with Style: Using Writing Style to Predict the Success of Novels</a></p>
<p>18 0.27658212 <a title="143-lsi-18" href="./emnlp-2013-Rule-Based_Information_Extraction_is_Dead%21_Long_Live_Rule-Based_Information_Extraction_Systems%21.html">161 emnlp-2013-Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!</a></p>
<p>19 0.2677314 <a title="143-lsi-19" href="./emnlp-2013-Detecting_Promotional_Content_in_Wikipedia.html">61 emnlp-2013-Detecting Promotional Content in Wikipedia</a></p>
<p>20 0.26414451 <a title="143-lsi-20" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.039), (9, 0.011), (11, 0.024), (18, 0.025), (22, 0.037), (30, 0.102), (47, 0.014), (50, 0.018), (51, 0.158), (58, 0.011), (59, 0.196), (66, 0.085), (71, 0.058), (75, 0.031), (77, 0.028), (92, 0.014), (96, 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90227437 <a title="143-lda-1" href="./emnlp-2013-With_Blinkers_on%3A_Robust_Prediction_of_Eye_Movements_across_Readers.html">203 emnlp-2013-With Blinkers on: Robust Prediction of Eye Movements across Readers</a></p>
<p>Author: Franz Matthies ; Anders Sgaard</p><p>Abstract: Nilsson and Nivre (2009) introduced a treebased model of persons’ eye movements in reading. The individual variation between readers reportedly made application across readers impossible. While a tree-based model seems plausible for eye movements, we show that competitive results can be obtained with a linear CRF model. Increasing the inductive bias also makes learning across readers possible. In fact we observe next-to-no performance drop when evaluating models trained on gaze records of multiple readers on new readers.</p><p>2 0.84809405 <a title="143-lda-2" href="./emnlp-2013-Automatic_Extraction_of_Morphological_Lexicons_from_Morphologically_Annotated_Corpora.html">30 emnlp-2013-Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora</a></p>
<p>Author: Ramy Eskander ; Nizar Habash ; Owen Rambow</p><p>Abstract: We present a method for automatically learning inflectional classes and associated lemmas from morphologically annotated corpora. The method consists of a core languageindependent algorithm, which can be optimized for specific languages. The method is demonstrated on Egyptian Arabic and German, two morphologically rich languages. Our best method for Egyptian Arabic provides an error reduction of 55.6% over a simple baseline; our best method for German achieves a 66.7% error reduction.</p><p>same-paper 3 0.83171517 <a title="143-lda-3" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>Author: Margaret Mitchell ; Jacqui Aguilar ; Theresa Wilson ; Benjamin Van Durme</p><p>Abstract: We propose a novel approach to sentiment analysis for a low resource setting. The intuition behind this work is that sentiment expressed towards an entity, targeted sentiment, may be viewed as a span of sentiment expressed across the entity. This representation allows us to model sentiment detection as a sequence tagging problem, jointly discovering people and organizations along with whether there is sentiment directed towards them. We compare performance in both Spanish and English on microblog data, using only a sentiment lexicon as an external resource. By leveraging linguisticallyinformed features within conditional random fields (CRFs) trained to minimize empirical risk, our best models in Spanish significantly outperform a strong baseline, and reach around 90% accuracy on the combined task of named entity recognition and sentiment prediction. Our models in English, trained on a much smaller dataset, are not yet statistically significant against their baselines.</p><p>4 0.81956661 <a title="143-lda-4" href="./emnlp-2013-Identifying_Multiple_Userids_of_the_Same_Author.html">95 emnlp-2013-Identifying Multiple Userids of the Same Author</a></p>
<p>Author: Tieyun Qian ; Bing Liu</p><p>Abstract: This paper studies the problem of identifying users who use multiple userids to post in social media. Since multiple userids may belong to the same author, it is hard to directly apply supervised learning to solve the problem. This paper proposes a new method, which still uses supervised learning but does not require training documents from the involved userids. Instead, it uses documents from other userids for classifier building. The classifier can be applied to documents of the involved userids. This is possible because we transform the document space to a similarity space and learning is performed in this new space. Our evaluation is done in the online review domain. The experimental results using a large number of userids and their reviews show that the proposed method is highly effective. 1</p><p>5 0.72605777 <a title="143-lda-5" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>Author: Xinjie Zhou ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style. In this paper, we study the problem of extracting opinion targets of Chinese microblog messages. Such fine-grained word-level task has not been well investigated in microblogs yet. We propose an unsupervised label propagation algorithm to address the problem. The opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets. Topics in microblogs are identified by hashtags or using clustering algorithms. Experimental results on Chinese microblogs show the effectiveness of our framework and algorithms.</p><p>6 0.72350621 <a title="143-lda-6" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>7 0.70719898 <a title="143-lda-7" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>8 0.70488822 <a title="143-lda-8" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>9 0.70276409 <a title="143-lda-9" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>10 0.70219296 <a title="143-lda-10" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>11 0.69861299 <a title="143-lda-11" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>12 0.69429392 <a title="143-lda-12" href="./emnlp-2013-Paraphrasing_4_Microblog_Normalization.html">151 emnlp-2013-Paraphrasing 4 Microblog Normalization</a></p>
<p>13 0.69292837 <a title="143-lda-13" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>14 0.69222343 <a title="143-lda-14" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>15 0.69107097 <a title="143-lda-15" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>16 0.69007826 <a title="143-lda-16" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>17 0.68936825 <a title="143-lda-17" href="./emnlp-2013-A_temporal_model_of_text_periodicities_using_Gaussian_Processes.html">18 emnlp-2013-A temporal model of text periodicities using Gaussian Processes</a></p>
<p>18 0.6892978 <a title="143-lda-18" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>19 0.68919855 <a title="143-lda-19" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>20 0.68910187 <a title="143-lda-20" href="./emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM.html">99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
