<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-60" href="#">emnlp2013-60</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</h1>
<br/><p>Source: <a title="emnlp-2013-60-pdf" href="http://aclweb.org/anthology//D/D13/D13-1147.pdf">pdf</a></p><p>Author: Douwe Kiela ; Stephen Clark</p><p>Abstract: We present a novel unsupervised approach to detecting the compositionality of multi-word expressions. We compute the compositionality of a phrase through substituting the constituent words with their “neighbours” in a semantic vector space and averaging over the distance between the original phrase and the substituted neighbour phrases. Several methods of obtaining neighbours are presented. The results are compared to existing supervised results and achieve state-of-the-art performance on a verb-object dataset of human compositionality ratings.</p><p>Reference: <a title="emnlp-2013-60-reference" href="../emnlp2013_reference/emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models  Douwe Kiela University of Cambridge Computer Laboratory douwe  . [sent-1, score-0.052]
</p><p>2 uk a Abstract  We present a novel unsupervised approach to detecting the compositionality of multi-word expressions. [sent-5, score-0.45]
</p><p>3 We compute the compositionality of a phrase through substituting the constituent words with their “neighbours” in a semantic vector space and averaging over the distance between the original phrase and the substituted neighbour phrases. [sent-6, score-1.252]
</p><p>4 The results are compared to existing supervised results and achieve state-of-the-art performance on a verb-object dataset of human compositionality ratings. [sent-8, score-0.418]
</p><p>5 1 Introduction Multi-word expressions (MWEs) are defined as “id-  iosyncratic interpretations that cross word boundaries” (Sag et al. [sent-9, score-0.082]
</p><p>6 They tend to have a standard syntactic structure but are often semantically non-compositional; i. [sent-11, score-0.042]
</p><p>7 their meaning is not fully determined by their syntactic structure and the meanings of their constituents. [sent-13, score-0.04]
</p><p>8 A classic example is kick the bucket, which means to die rather than to hit a bucket with the foot. [sent-14, score-0.032]
</p><p>9 These types of expressions account for a large proportion of day-to-day language interactions (Schuler and Joshi, 2011) and present a significant problem for natural language processing systems (Sag et al. [sent-15, score-0.082]
</p><p>10 This paper presents a novel unsupervised approach to detecting the compositionality of MWEs, specifically of verb-noun collocations. [sent-17, score-0.45]
</p><p>11 The idea is 1427 Stephen Clark University of Cambridge Computer Laboratory st ephen clark@ cl cam ac uk  . [sent-18, score-0.041]
</p><p>12 that we can recognize compositional phrases by substituting related words for constituent words in the  phrase: if the result of a substitution yields a meaningful phrase, its individual constituents are likely to contribute toward the overall meaning of the phrase. [sent-22, score-0.504]
</p><p>13 Conversely, if a substitution yields a non-sensical phrase, its constituents are likely to contribute less or not at all to the overall meaning of the phrase. [sent-23, score-0.11]
</p><p>14 For the phrase eat her hat, for example, we might consider the following substituted phrases: 1. [sent-24, score-0.465]
</p><p>15 eat her trousers Both phrases are semantically anomalous, implying that eat hat is a highly non-compositional verb-noun collocation. [sent-26, score-0.593]
</p><p>16 Following a similar procedure for eat apple, however, would not lead to an anomaly: consume apple and eat pear are perfectly meaningful, leading us to believe that eat apple is compositional. [sent-27, score-0.8]
</p><p>17 In the context of distributional models, this idea can be formalised in terms of vector spaces: the average distance between a phrase vector and its substituted phrase vectors is related to its compositionality. [sent-28, score-0.765]
</p><p>18 Since we are relying on the relative distances of phrases in semantic space, we require a method for computing vectors for phrases. [sent-29, score-0.268]
</p><p>19 We experimented with a number of composition operators from Mitchell and Lapata (2010), in order to compose constituent word vectors into phrase vectors. [sent-30, score-0.377]
</p><p>20 The relation between phrase vectors and substituted phrase vectors is most pronounced in the case of Proce Sdeiantgtlse o,f W thaesh 2i0n1gt3o nC,o UnSfeAre,n 1c8e- o2n1 E Omctpoibriecra 2l0 M13et. [sent-31, score-0.645]
</p><p>21 This implies that compositional phrases are less similar to their neighbours, which is to say that the greater the average distance between a phrase vector and its substituted phrase vectors, the greater its compositionality. [sent-34, score-0.643]
</p><p>22 The contribution ofthis short focused research paper is a novel approach to detecting the compositionality of multi-word expressions that makes full use of the ability of semantic vector space models to cal-  culate distances between words and phrases. [sent-35, score-0.709]
</p><p>23 Using this unsupervised approach, we achieve state-of-theart performance in a direct comparison with existing supervised methods. [sent-36, score-0.027]
</p><p>24 2  Dataset and Vectors  The verb-noun collocation dataset from Venkatapathy and Joshi (2005), which consists of 765 verbobject pairs with human compositionality ratings, was used for evaluation. [sent-37, score-0.496]
</p><p>25 Venkatapathy & Joshi used a support vector machine (SVM) to obtain a Spearman ρs correlation of 0. [sent-38, score-0.055]
</p><p>26 They employed a variety of features ranging from frequency to LSAderived similarity measures and used 10% of the dataset as training data with tenfold cross-validation. [sent-40, score-0.068]
</p><p>27 (2007) used the same dataset and expanded on the original approach by adding WordNet and distributional prototypes to the SVM, achieving a ρs correlation of 0. [sent-42, score-0.131]
</p><p>28 The distributional vectors for our experiments were constructed from the ukWaC corpus (Baroni et al. [sent-44, score-0.218]
</p><p>29 Vectors were obtained using a standard window method (with a window size of 5) and the 50,000 most frequent context words as features,  with stopwords removed. [sent-46, score-0.08]
</p><p>30 We also experimented with syntax-based co-occurrence features extracted from a dependency-parsed version of ukWaC, but in agreement with results obtained by Schulte im Walde et al. [sent-47, score-0.078]
</p><p>31 (2013) for predicting compositionality in German, the window-based co-occurrence method produced better results. [sent-48, score-0.377]
</p><p>32 We tried several weighting schemes from the literature, such as t-test (Curran, 2004), positive mutual 1428 information (Bullinaria and Levy, 2012) and the ratio of the probability of the context word given the target word1 to the context word’s overall probability (Mitchell and Lapata, 2010). [sent-49, score-0.029]
</p><p>33 o0 cn)otne lxot egx w(tnNo wjrdo)|rd| where fij is the number of times that the target word and context word co-occur in the same window, nj  is the context word frequency, N is the total frequency and |context word| is the total number of occurrences odf | a ncotenxtetx wto wrdo|rd is. [sent-54, score-0.027]
</p><p>34 hDeis ttoatnacle n uism cbaelrcu olfa toecdusing the standard cosine measure:  dist(v1,v2) = 1 −|vv11|·|v v22| where v1 and space model. [sent-55, score-0.037]
</p><p>35 3  v2  are vectors in the semantic vector  Finding Neighbours and Computing Compositionality  We experimented with two different ways of obtaining neighbours for the constituent words in a phrase. [sent-56, score-1.023]
</p><p>36 Since vector space models lend themselves naturally to similarity computations, one way to get neighbours is to take the k-most similar vectors from a similarity matrix. [sent-57, score-0.899]
</p><p>37 This approach is straightforward, but has some potential drawbacks: it assumes that we have a large number of vectors to select neighbours from, and becomes computationally expensive when the number of neighbours is increased. [sent-58, score-1.362]
</p><p>38 An alternative source for obtaining neighbours is the lexical database WordNet (Fellbaum, 1998). [sent-59, score-0.657]
</p><p>39 We  define neighbours as siblings in the hypernym hierarchy, so that the neighbours of a word can be found by taking the hyponyms of its hypernyms. [sent-60, score-1.234]
</p><p>40 WordNet also allows us to extract only neighbours of the same grammatical type (yielding noun neighbours for nouns and verb neighbours for verbs, for example). [sent-61, score-1.898]
</p><p>41 Since not every word has the same number of neighbours in WordNet, we use only the first k 1We use target word to refer to the word for which a vector is being constructed. [sent-62, score-0.672]
</p><p>42 neighbours, which means that the neighbours have to be ranked. [sent-63, score-0.617]
</p><p>43 An obvious ranking method is to use the frequency with which each neighbour co-occurs with the other constituent(s) of the same phrase. [sent-64, score-0.104]
</p><p>44 For example, for all the WordNet neighbours of eat (for all senses of eat), we count the co-occurrences with hat in a given window size and rank them accordingly. [sent-65, score-0.914]
</p><p>45 For example, the highly ranked neighbours of apple for eat apple are likely to be items of food, and not (inedible) trees (apple is also a tree in WordNet). [sent-67, score-1.005]
</p><p>46 In order to obtain frequency-ranked neighbours,  we used the ukWaC corpus with a window size of 5. [sent-68, score-0.04]
</p><p>47 One reason for having multiple neighbours is that it allows us to correct for word sense disambiguation errors (as mentioned above), since averaging over results for several neighbours reduces the impact of including incorrect senses. [sent-69, score-1.27]
</p><p>48 As neighbours for pronouns (which are not included in WordNet), we used the other pronouns present in the dataset. [sent-72, score-0.683]
</p><p>49 1 Average distance compositionality score We considered several different ways of constructing phrasal vectors. [sent-75, score-0.497]
</p><p>50 We chose not to use the compositional models of Baroni and Zamparelli (2010) and Socher et al. [sent-76, score-0.089]
</p><p>51 (201 1) because we believe that it is important that our methods are completely unsupervised and do not require any initial learning phase. [sent-77, score-0.027]
</p><p>52 1429 Hence, we experimented with different ways of constructing phrasal vectors according to Mitchell and Lapata (2010) and found that pointwise multiplication ? [sent-78, score-0.323]
</p><p>53 Thus, we define the composed vector −e −at − h− →at as: − →  − →  −e →at ? [sent-80, score-0.055]
</p><p>54 −h →at We can now compute a compositionality score sc by averaging the distance between the original phrase vector and its substituted neighbour phrase vectors via the following formula:  sc(e−a − t− − h →at) =21k(Xi=k1dist(−e →at ? [sent-81, score-1.15]
</p><p>55 −h →at)) We also experimented with substituting only for the noun or the verb, and in fact found that only taking neighbours for the verb yields better results:  sc(−ea − t− − h →at) =k1jX=k1dist(−e →at ? [sent-85, score-0.834]
</p><p>56 −h →at) To illustrate the method, consider the collocations take breath and lend money. [sent-87, score-0.228]
</p><p>57 The annotators assigned these phrases a compositionality score of 1 out of 6 and 6 out of 6, respectively, meaning that the former is non-compositional and the latter is compositional. [sent-88, score-0.479]
</p><p>58 The distances between the first ten verbsubstituted phrases and the original phrase, together  with the average distance, are shown in Table 1 and Table 2. [sent-89, score-0.102]
</p><p>59 Substituting the verb in the non-compositional phrase yields semantically anomalous vectors, which leads to very small changes in the distance between it and the original phrase vector. [sent-90, score-0.443]
</p><p>60 Conversely, substituting the verb in the compositional phrase yields meaningful vectors, putting them in locations in semantic vector space which are sufficiently far apart to distinguish them from the non-compositional cases. [sent-92, score-0.546]
</p><p>61 4  Results  Results are given for the two methods of obtaining neighbours: via frequency-ranked WordNet neighbours and via vector space neighbours. [sent-93, score-0.749]
</p><p>62 The compositionality score was computed by using only the verb, only the noun, or both constituent neighbours in the substituted phrase vectors. [sent-94, score-1.382]
</p><p>63 e wVearlu tehsa onf k1 neighbours bereecause not enough neighbours have been found to cooccur with the other constituent, we use all of them. [sent-104, score-1.234]
</p><p>64 The dataset has an inter-annotator agreement of Kendall’s τ of 0. [sent-106, score-0.041]
</p><p>65 Note that, even though the current approach is unsupervised (in terms of not having access to compositionality ratings during training, although it does rely on WordNet), it outperforms SVMs that require an ensemble of complex feature sets (some of which are also based on WordNet). [sent-110, score-0.44]
</p><p>66 It is interesting to observe that the state-of-the-art performance is reached when only using the verb’s neighbours to compute substituted phrase vectors. [sent-111, score-0.902]
</p><p>67 eat trousers, where the noun has been substituted, does not make a lot of sense either which we would expect to be informative for determining compositionality. [sent-114, score-0.18]
</p><p>68 Head words have been found to have a higher impact on compositionality scores for compound nouns: Reddy et al. [sent-116, score-0.377]
</p><p>69 (201 1) weighted the contribution of individual constituents in such a way that the modifier’s contribution is included but is weighted less highly than the head’s contribution, which led to an improvement in performance. [sent-117, score-0.102]
</p><p>70 Our results might be improved by weighting the contribution of constituent words in a similar fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. [sent-118, score-0.57]
</p><p>71 —  5  Related Work  The past decade has seen extensive work on computational and statistical methods in detecting the com-  positionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). [sent-119, score-0.046]
</p><p>72 Many of these methods rely on distributional models and vector space models (Sch u¨tze, 1993; Turney and Pantel, 2010; Erk, 2012). [sent-120, score-0.182]
</p><p>73 Work has been done on different types of phrases, including work on particle verbs (McCarthy et al. [sent-121, score-0.027]
</p><p>74 , 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al. [sent-123, score-0.062]
</p><p>75 , 2011), as well as on languages other than English (Schulte im Walde et al. [sent-126, score-0.036]
</p><p>76 Recent developments in distributional compositional models (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al. [sent-128, score-0.179]
</p><p>77 , 2011) have opened up a number of possibilities for constructing vectors for phrases, which have also been applied to compositionality tests (Giesbrecht, 2009; Kochmar and Briscoe, 2013). [sent-130, score-0.541]
</p><p>78 6  Conclusion  We have presented a novel unsupervised approach that can be used to detect the compositionality of multi-word expressions. [sent-132, score-0.404]
</p><p>79 Our results show that the underlying intuition appears to be sound: substituting neighbours may lead to meaningful or meaningless phrases depending on whether or not the phrase 1431 is compositional. [sent-133, score-0.923]
</p><p>80 This can be formalized in vector space models to obtain compositionality scores by computing the average distance to the original phrase’s substituted neighbour phrases. [sent-134, score-0.775]
</p><p>81 In this short focused research paper, we show that, depending on how we obtain neighbours, we are able to achieve a higher performance than that achieved by supervised methods which rely on a complex feature set and support vector machines. [sent-135, score-0.055]
</p><p>82 Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. [sent-146, score-0.038]
</p><p>83 Disco11: Proceedings of the workshop on distributional se-  mantics and compositionality. [sent-154, score-0.09]
</p><p>84 Mathematical foundations for a compositional distributional model of meaning. [sent-163, score-0.179]
</p><p>85 Vector space models of word meaning and phrase meaning: A survey. [sent-176, score-0.181]
</p><p>86 Automatic identification of non-compositional multi-word expressions using latent semantic analysis. [sent-190, score-0.12]
</p><p>87 In Proceedings of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment - Volume 18, MWE ’03, pages 73–80. [sent-203, score-0.027]
</p><p>88 An empirical study on compositionality in com-  pound nouns. [sent-214, score-0.377]
</p><p>89 TF-ICF: A new term weighting scheme for clustering dynamic data streams. [sent-229, score-0.029]
</p><p>90 Is knowledge-free induction of multiword unit dictionary headwords a solved problem? [sent-241, score-0.086]
</p><p>91 Sabine Schulte im Walde, Stefan M ¨uller, and Stephen  Roller. [sent-248, score-0.036]
</p><p>92 unsupervised recognition of literal and non-literal use of idiomatic expressions. [sent-266, score-0.065]
</p><p>93 From frequency to meaning: vector space models of semantics. [sent-271, score-0.119]
</p><p>94 (linear) maps of the impossible: Capturing semantic anomalies in distributional space. [sent-279, score-0.166]
</p><p>95 Measuring the relative compositionality of verb-noun (v-n) collocations by integrating features. [sent-285, score-0.439]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('neighbours', 0.617), ('compositionality', 0.377), ('substituted', 0.181), ('eat', 0.18), ('venkatapathy', 0.136), ('vectors', 0.128), ('mccarthy', 0.127), ('breath', 0.104), ('phrase', 0.104), ('apple', 0.104), ('constituent', 0.103), ('wordnet', 0.093), ('substituting', 0.092), ('distributional', 0.09), ('compositional', 0.089), ('multiword', 0.086), ('mwe', 0.083), ('expressions', 0.082), ('baroni', 0.079), ('verbobject', 0.078), ('hat', 0.077), ('neighbour', 0.077), ('mwes', 0.068), ('diana', 0.066), ('phrases', 0.062), ('joshi', 0.062), ('lend', 0.062), ('collocations', 0.062), ('schulte', 0.062), ('walde', 0.062), ('sag', 0.062), ('anomalous', 0.062), ('eugenie', 0.062), ('vecchi', 0.062), ('vector', 0.055), ('giesbrecht', 0.054), ('douwe', 0.052), ('kochmar', 0.052), ('neighbourdist', 0.052), ('reed', 0.052), ('trousers', 0.052), ('reddy', 0.052), ('consume', 0.052), ('ukwac', 0.05), ('distance', 0.048), ('meaningful', 0.048), ('verb', 0.047), ('detecting', 0.046), ('spearman', 0.046), ('multiplication', 0.044), ('experimented', 0.042), ('semantically', 0.042), ('mitchell', 0.042), ('bullinaria', 0.041), ('cam', 0.041), ('schone', 0.041), ('sriram', 0.041), ('dataset', 0.041), ('window', 0.04), ('distances', 0.04), ('sc', 0.04), ('obtaining', 0.04), ('meaning', 0.04), ('idiomatic', 0.038), ('sporleder', 0.038), ('schuler', 0.038), ('anomalies', 0.038), ('coecke', 0.038), ('semantic', 0.038), ('aravind', 0.038), ('lapata', 0.038), ('pointwise', 0.037), ('space', 0.037), ('im', 0.036), ('averaging', 0.036), ('yields', 0.036), ('biemann', 0.036), ('rj', 0.036), ('bannard', 0.036), ('constructing', 0.036), ('ratings', 0.036), ('phrasal', 0.036), ('socher', 0.035), ('zamparelli', 0.035), ('contribution', 0.034), ('constituents', 0.034), ('av', 0.033), ('pronouns', 0.033), ('katz', 0.032), ('bucket', 0.032), ('conversely', 0.031), ('baldwin', 0.03), ('weighting', 0.029), ('eva', 0.028), ('unsupervised', 0.027), ('frequency', 0.027), ('maria', 0.027), ('treatment', 0.027), ('verbs', 0.027), ('svms', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="60-tfidf-1" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>Author: Douwe Kiela ; Stephen Clark</p><p>Abstract: We present a novel unsupervised approach to detecting the compositionality of multi-word expressions. We compute the compositionality of a phrase through substituting the constituent words with their “neighbours” in a semantic vector space and averaging over the distance between the original phrase and the substituted neighbour phrases. Several methods of obtaining neighbours are presented. The results are compared to existing supervised results and achieve state-of-the-art performance on a verb-object dataset of human compositionality ratings.</p><p>2 0.18232693 <a title="60-tfidf-2" href="./emnlp-2013-Identifying_Phrasal_Verbs_Using_Many_Bilingual_Corpora.html">96 emnlp-2013-Identifying Phrasal Verbs Using Many Bilingual Corpora</a></p>
<p>Author: Karl Pichotta ; John DeNero</p><p>Abstract: We address the problem of identifying multiword expressions in a language, focusing on English phrasal verbs. Our polyglot ranking approach integrates frequency statistics from translated corpora in 50 different languages. Our experimental evaluation demonstrates that combining statistical evidence from many parallel corpora using a novel ranking-oriented boosting algorithm produces a comprehensive set ofEnglish phrasal verbs, achieving performance comparable to a human-curated set.</p><p>3 0.13805017 <a title="60-tfidf-3" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>Author: Masashi Tsubaki ; Kevin Duh ; Masashi Shimbo ; Yuji Matsumoto</p><p>Abstract: We present a novel vector space model for semantic co-compositionality. Inspired by Generative Lexicon Theory (Pustejovsky, 1995), our goal is a compositional model where both predicate and argument are allowed to modify each others’ meaning representations while generating the overall semantics. This readily addresses some major challenges with current vector space models, notably the polysemy issue and the use of one representation per word type. We implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations. We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality. The model achieves the best result to date (ρ = 0.47) on the semantic similarity task of transitive verbs (Grefenstette and Sadrzadeh, 2011).</p><p>4 0.12625077 <a title="60-tfidf-4" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>Author: Richard Socher ; Alex Perelygin ; Jean Wu ; Jason Chuang ; Christopher D. Manning ; Andrew Ng ; Christopher Potts</p><p>Abstract: Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.</p><p>5 0.11923391 <a title="60-tfidf-5" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>Author: Angeliki Lazaridou ; Eva Maria Vecchi ; Marco Baroni</p><p>Abstract: In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the correct parsing of NPs (e.g., (live fish) transporter rather than live (fish transporter)). We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features.</p><p>6 0.11081044 <a title="60-tfidf-6" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>7 0.097614728 <a title="60-tfidf-7" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>8 0.092876434 <a title="60-tfidf-8" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>9 0.078789122 <a title="60-tfidf-9" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>10 0.073268712 <a title="60-tfidf-10" href="./emnlp-2013-Automatic_Idiom_Identification_in_Wiktionary.html">32 emnlp-2013-Automatic Idiom Identification in Wiktionary</a></p>
<p>11 0.072245769 <a title="60-tfidf-11" href="./emnlp-2013-Using_Paraphrases_and_Lexical_Semantics_to_Improve_the_Accuracy_and_the_Robustness_of_Supervised_Models_in_Situated_Dialogue_Systems.html">197 emnlp-2013-Using Paraphrases and Lexical Semantics to Improve the Accuracy and the Robustness of Supervised Models in Situated Dialogue Systems</a></p>
<p>12 0.067042284 <a title="60-tfidf-12" href="./emnlp-2013-Learning_to_Rank_Lexical_Substitutions.html">123 emnlp-2013-Learning to Rank Lexical Substitutions</a></p>
<p>13 0.065489121 <a title="60-tfidf-13" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>14 0.060704827 <a title="60-tfidf-14" href="./emnlp-2013-An_Efficient_Language_Model_Using_Double-Array_Structures.html">20 emnlp-2013-An Efficient Language Model Using Double-Array Structures</a></p>
<p>15 0.058077764 <a title="60-tfidf-15" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>16 0.052216545 <a title="60-tfidf-16" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>17 0.051783387 <a title="60-tfidf-17" href="./emnlp-2013-Learning_Distributions_over_Logical_Forms_for_Referring_Expression_Generation.html">119 emnlp-2013-Learning Distributions over Logical Forms for Referring Expression Generation</a></p>
<p>18 0.045583926 <a title="60-tfidf-18" href="./emnlp-2013-Scaling_to_Large3_Data%3A_An_Efficient_and_Effective_Method_to_Compute_Distributional_Thesauri.html">165 emnlp-2013-Scaling to Large3 Data: An Efficient and Effective Method to Compute Distributional Thesauri</a></p>
<p>19 0.045501761 <a title="60-tfidf-19" href="./emnlp-2013-Deriving_Adjectival_Scales_from_Continuous_Space_Word_Representations.html">59 emnlp-2013-Deriving Adjectival Scales from Continuous Space Word Representations</a></p>
<p>20 0.043797083 <a title="60-tfidf-20" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.155), (1, 0.003), (2, -0.055), (3, -0.041), (4, 0.011), (5, 0.201), (6, -0.049), (7, -0.119), (8, -0.186), (9, -0.076), (10, 0.067), (11, 0.159), (12, -0.011), (13, 0.134), (14, 0.004), (15, -0.076), (16, 0.035), (17, -0.128), (18, -0.027), (19, -0.129), (20, 0.014), (21, -0.005), (22, -0.122), (23, 0.019), (24, 0.096), (25, -0.001), (26, 0.072), (27, -0.01), (28, -0.158), (29, -0.023), (30, 0.027), (31, -0.099), (32, 0.024), (33, -0.083), (34, 0.008), (35, -0.048), (36, -0.108), (37, -0.006), (38, -0.06), (39, 0.024), (40, -0.004), (41, -0.046), (42, -0.012), (43, 0.089), (44, -0.052), (45, 0.069), (46, -0.025), (47, 0.033), (48, -0.107), (49, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94933373 <a title="60-lsi-1" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>Author: Douwe Kiela ; Stephen Clark</p><p>Abstract: We present a novel unsupervised approach to detecting the compositionality of multi-word expressions. We compute the compositionality of a phrase through substituting the constituent words with their “neighbours” in a semantic vector space and averaging over the distance between the original phrase and the substituted neighbour phrases. Several methods of obtaining neighbours are presented. The results are compared to existing supervised results and achieve state-of-the-art performance on a verb-object dataset of human compositionality ratings.</p><p>2 0.64978838 <a title="60-lsi-2" href="./emnlp-2013-Identifying_Phrasal_Verbs_Using_Many_Bilingual_Corpora.html">96 emnlp-2013-Identifying Phrasal Verbs Using Many Bilingual Corpora</a></p>
<p>Author: Karl Pichotta ; John DeNero</p><p>Abstract: We address the problem of identifying multiword expressions in a language, focusing on English phrasal verbs. Our polyglot ranking approach integrates frequency statistics from translated corpora in 50 different languages. Our experimental evaluation demonstrates that combining statistical evidence from many parallel corpora using a novel ranking-oriented boosting algorithm produces a comprehensive set ofEnglish phrasal verbs, achieving performance comparable to a human-curated set.</p><p>3 0.62377131 <a title="60-lsi-3" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>Author: Eva Maria Vecchi ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: In this study, we use compositional distributional semantic methods to investigate restrictions in adjective ordering. Specifically, we focus on properties distinguishing AdjectiveAdjective-Noun phrases in which there is flexibility in the adjective ordering from those bound to a rigid order. We explore a number of measures extracted from the distributional representation of AAN phrases which may indicate a word order restriction. We find that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modification of the adjectives. Our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics.</p><p>4 0.6091873 <a title="60-lsi-4" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>Author: Masashi Tsubaki ; Kevin Duh ; Masashi Shimbo ; Yuji Matsumoto</p><p>Abstract: We present a novel vector space model for semantic co-compositionality. Inspired by Generative Lexicon Theory (Pustejovsky, 1995), our goal is a compositional model where both predicate and argument are allowed to modify each others’ meaning representations while generating the overall semantics. This readily addresses some major challenges with current vector space models, notably the polysemy issue and the use of one representation per word type. We implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations. We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality. The model achieves the best result to date (ρ = 0.47) on the semantic similarity task of transitive verbs (Grefenstette and Sadrzadeh, 2011).</p><p>5 0.60628331 <a title="60-lsi-5" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>Author: Angeliki Lazaridou ; Eva Maria Vecchi ; Marco Baroni</p><p>Abstract: In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the correct parsing of NPs (e.g., (live fish) transporter rather than live (fish transporter)). We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features.</p><p>6 0.56864434 <a title="60-lsi-6" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>7 0.55372369 <a title="60-lsi-7" href="./emnlp-2013-Automatic_Idiom_Identification_in_Wiktionary.html">32 emnlp-2013-Automatic Idiom Identification in Wiktionary</a></p>
<p>8 0.55126089 <a title="60-lsi-8" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>9 0.45901385 <a title="60-lsi-9" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>10 0.44518712 <a title="60-lsi-10" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>11 0.44161874 <a title="60-lsi-11" href="./emnlp-2013-Learning_to_Rank_Lexical_Substitutions.html">123 emnlp-2013-Learning to Rank Lexical Substitutions</a></p>
<p>12 0.35272995 <a title="60-lsi-12" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>13 0.34662139 <a title="60-lsi-13" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<p>14 0.33639902 <a title="60-lsi-14" href="./emnlp-2013-Animacy_Detection_with_Voting_Models.html">23 emnlp-2013-Animacy Detection with Voting Models</a></p>
<p>15 0.32382658 <a title="60-lsi-15" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>16 0.31241828 <a title="60-lsi-16" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>17 0.31026348 <a title="60-lsi-17" href="./emnlp-2013-Scaling_to_Large3_Data%3A_An_Efficient_and_Effective_Method_to_Compute_Distributional_Thesauri.html">165 emnlp-2013-Scaling to Large3 Data: An Efficient and Effective Method to Compute Distributional Thesauri</a></p>
<p>18 0.30283856 <a title="60-lsi-18" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>19 0.2958402 <a title="60-lsi-19" href="./emnlp-2013-The_VerbCorner_Project%3A_Toward_an_Empirically-Based_Semantic_Decomposition_of_Verbs.html">183 emnlp-2013-The VerbCorner Project: Toward an Empirically-Based Semantic Decomposition of Verbs</a></p>
<p>20 0.29534289 <a title="60-lsi-20" href="./emnlp-2013-Using_Paraphrases_and_Lexical_Semantics_to_Improve_the_Accuracy_and_the_Robustness_of_Supervised_Models_in_Situated_Dialogue_Systems.html">197 emnlp-2013-Using Paraphrases and Lexical Semantics to Improve the Accuracy and the Robustness of Supervised Models in Situated Dialogue Systems</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.026), (6, 0.052), (18, 0.02), (22, 0.032), (30, 0.058), (45, 0.01), (50, 0.023), (51, 0.236), (54, 0.3), (66, 0.043), (71, 0.041), (75, 0.021), (77, 0.016), (90, 0.014), (96, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84840745 <a title="60-lda-1" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>Author: Micha Elsner ; Sharon Goldwater ; Naomi Feldman ; Frank Wood</p><p>Abstract: We present a cognitive model of early lexical acquisition which jointly performs word segmentation and learns an explicit model of phonetic variation. We define the model as a Bayesian noisy channel; we sample segmentations and word forms simultaneously from the posterior, using beam sampling to control the size of the search space. Compared to a pipelined approach in which segmentation is performed first, our model is qualitatively more similar to human learners. On data with vari- able pronunciations, the pipelined approach learns to treat syllables or morphemes as words. In contrast, our joint model, like infant learners, tends to learn multiword collocations. We also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors, and relate these to developmental evidence.</p><p>same-paper 2 0.81056207 <a title="60-lda-2" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>Author: Douwe Kiela ; Stephen Clark</p><p>Abstract: We present a novel unsupervised approach to detecting the compositionality of multi-word expressions. We compute the compositionality of a phrase through substituting the constituent words with their “neighbours” in a semantic vector space and averaging over the distance between the original phrase and the substituted neighbour phrases. Several methods of obtaining neighbours are presented. The results are compared to existing supervised results and achieve state-of-the-art performance on a verb-object dataset of human compositionality ratings.</p><p>3 0.63425136 <a title="60-lda-3" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>Author: Richard Socher ; Alex Perelygin ; Jean Wu ; Jason Chuang ; Christopher D. Manning ; Andrew Ng ; Christopher Potts</p><p>Abstract: Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.</p><p>4 0.63240916 <a title="60-lda-4" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<p>Author: Yiping Jin ; Min-Yen Kan ; Jun-Ping Ng ; Xiangnan He</p><p>Abstract: This paper presents DefMiner, a supervised sequence labeling system that identifies scientific terms and their accompanying definitions. DefMiner achieves 85% F1 on a Wikipedia benchmark corpus, significantly improving the previous state-of-the-art by 8%. We exploit DefMiner to process the ACL Anthology Reference Corpus (ARC) – a large, real-world digital library of scientific articles in computational linguistics. The resulting automatically-acquired glossary represents the terminology defined over several thousand individual research articles. We highlight several interesting observations: more definitions are introduced for conference and workshop papers over the years and that multiword terms account for slightly less than half of all terms. Obtaining a list of popular , defined terms in a corpus ofcomputational linguistics papers, we find that concepts can often be categorized into one of three categories: resources, methodologies and evaluation metrics.</p><p>5 0.63150984 <a title="60-lda-5" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>Author: Kuzman Ganchev ; Dipanjan Das</p><p>Abstract: We present a framework for cross-lingual transfer of sequence information from a resource-rich source language to a resourceimpoverished target language that incorporates soft constraints via posterior regularization. To this end, we use automatically word aligned bitext between the source and target language pair, and learn a discriminative conditional random field model on the target side. Our posterior regularization constraints are derived from simple intuitions about the task at hand and from cross-lingual alignment information. We show improvements over strong baselines for two tasks: part-of-speech tagging and namedentity segmentation.</p><p>6 0.62915868 <a title="60-lda-6" href="./emnlp-2013-Automatically_Detecting_and_Attributing_Indirect_Quotations.html">35 emnlp-2013-Automatically Detecting and Attributing Indirect Quotations</a></p>
<p>7 0.62901592 <a title="60-lda-7" href="./emnlp-2013-The_Effects_of_Syntactic_Features_in_Automatic_Prediction_of_Morphology.html">181 emnlp-2013-The Effects of Syntactic Features in Automatic Prediction of Morphology</a></p>
<p>8 0.62899953 <a title="60-lda-8" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>9 0.62688786 <a title="60-lda-9" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>10 0.62681031 <a title="60-lda-10" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>11 0.6253022 <a title="60-lda-11" href="./emnlp-2013-Semantic_Parsing_on_Freebase_from_Question-Answer_Pairs.html">166 emnlp-2013-Semantic Parsing on Freebase from Question-Answer Pairs</a></p>
<p>12 0.6252085 <a title="60-lda-12" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>13 0.62476522 <a title="60-lda-13" href="./emnlp-2013-Identifying_Phrasal_Verbs_Using_Many_Bilingual_Corpora.html">96 emnlp-2013-Identifying Phrasal Verbs Using Many Bilingual Corpora</a></p>
<p>14 0.62456959 <a title="60-lda-14" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>15 0.62441981 <a title="60-lda-15" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>16 0.62435222 <a title="60-lda-16" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>17 0.62408048 <a title="60-lda-17" href="./emnlp-2013-Success_with_Style%3A_Using_Writing_Style_to_Predict_the_Success_of_Novels.html">178 emnlp-2013-Success with Style: Using Writing Style to Predict the Success of Novels</a></p>
<p>18 0.6238358 <a title="60-lda-18" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>19 0.6237607 <a title="60-lda-19" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<p>20 0.62358797 <a title="60-lda-20" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
