<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-107" href="#">emnlp2013-107</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</h1>
<br/><p>Source: <a title="emnlp-2013-107-pdf" href="http://aclweb.org/anthology//D/D13/D13-1025.pdf">pdf</a></p><p>Author: Jesus Gonzalez-Rubio ; Daniel Ortiz-Martinez ; Jose-Miguel Benedi ; Francisco Casacuberta</p><p>Abstract: Current automatic machine translation systems are not able to generate error-free translations and human intervention is often required to correct their output. Alternatively, an interactive framework that integrates the human knowledge into the translation process has been presented in previous works. Here, we describe a new interactive machine translation approach that is able to work with phrase-based and hierarchical translation models, and integrates error-correction all in a unified statistical framework. In our experiments, our approach outperforms previous interactive translation systems, and achieves estimated effort reductions of as much as 48% relative over a traditional post-edition system.</p><p>Reference: <a title="emnlp-2013-107-reference" href="../emnlp2013_reference/emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Alternatively, an interactive framework that integrates the human knowledge into the translation process has been presented in previous works. [sent-3, score-0.248]
</p><p>2 Here, we describe a new interactive machine translation approach that is able to work with phrase-based and hierarchical translation models, and integrates error-correction all in a unified statistical framework. [sent-4, score-0.525]
</p><p>3 In our experiments, our approach outperforms previous interactive translation systems, and achieves estimated effort reductions of as much as 48% relative over a traditional post-edition system. [sent-5, score-0.34]
</p><p>4 1 Introduction  ,  Research in the field of machine translation (MT) aims to develop computer systems which are able to translate between languages automatically, without human intervention. [sent-6, score-0.192]
</p><p>5 e s c  244  translation (CAT) or interactive framework (Isabelle and Church, 1998). [sent-13, score-0.248]
</p><p>6 These research projects proposed to embed an MT system within an interactive translation environment. [sent-17, score-0.248]
</p><p>7 Particularly interesting is the interactive machine translation (IMT) approach proposed in (Barrachina et al. [sent-19, score-0.248]
</p><p>8 In this scenario, a statistical MT (SMT) system uses the source sentence and a previously validated part (prefix1) of its translation to propose a suitable continuation. [sent-21, score-0.268]
</p><p>9 Then the user finds and corrects the next system error, thereby providing a longer prefix which the system uses to suggests a new, hopefully better continuation. [sent-22, score-0.296]
</p><p>10 The first problem arises when the system cannot generate the prefix validated by the user. [sent-27, score-0.172]
</p><p>11 Particularly, the models used by the system were either mono1We use the terms prefix and suffix to denote any sub-string at the beginning and end respectively of a string of characters (including spaces and punctuation). [sent-30, score-0.247]
</p><p>12 On the one hand, we adopt the statistical formalization of the IMT framework described in (Ortiz-Mart ı´nez, 2011), which includes a stochastic error-correction model in its formalization to address prefix coverage problems. [sent-37, score-0.543]
</p><p>13 Moreover, we refine this formalization proposing an alternative error-correction formalization for the IMT framework (Section 2). [sent-38, score-0.391]
</p><p>14 We evaluate the proposed IMT approach on two different translation task. [sent-44, score-0.17]
</p><p>15 , (2009) and a conventional post-edition approach show that our IMT formalization for hierarchical SMT models indeed outperform other approaches (Sections 5 and 6). [sent-46, score-0.287]
</p><p>16 1 Statistical Machine Translation Assuming that we are given a sentence s in a source  language, the translation problem can be stated as finding its translation t in a target language of maximum probability (Brown et al. [sent-49, score-0.39]
</p><p>17 The desired translation is the translation the human user wants to obtain. [sent-51, score-0.467]
</p><p>18 At IT-1, the user moves the mouse to accept the first eight characters “To view ” and presses the akey (k), then the system suggests completing the sentence with “list of resources” (a new ts). [sent-53, score-0.175]
</p><p>19 Under this paradigm, translation is considered as an iterative left-to-right process where the human and the computer collaborate to generate the final translation. [sent-63, score-0.17]
</p><p>20 Initially, with no user feedback, the system suggests a complete translation ts =”To view the resources list”. [sent-66, score-0.404]
</p><p>21 From this translation, the user marks a prefix p =”To view” as correct and begins to type the rest of the target sentence. [sent-67, score-0.265]
</p><p>22 Depending on the system or the user’s preferences, the user might type the full next word, or only some letters of it (in our example, the user types the single next charac-  ter “a”). [sent-68, score-0.254]
</p><p>23 Then, the MT system suggests a new suffix ts =“list of resources” that completes the validated prefix and the input the user has just typed (p =”To view a”). [sent-69, score-0.54]
</p><p>24 The interaction continues with a new prefix validation followed, if necessary, by new input from the user, and so on, until the user considers the translation to be complete and satisfactory. [sent-70, score-0.435]
</p><p>25 Again decision theory tells us to maximize the probability of the suffix given the available information. [sent-72, score-0.134]
</p><p>26 Formally, the best suffix of a given length will be:  ˆts= argtmsaxPr(ts| s,p)  (4)  which can be straightforwardly rewritten as:  ˆts= argtmsaxPr(p,ts| s) = argtmsaxPr(p,ts) · Pr(s | p,ts)  (5) (6)  Note that, since p ts = t, this equation is very similar to Equation (2). [sent-73, score-0.267]
</p><p>27 3 IMT with Stochastic Error-Correction A common problem in IMT arises when the user sets a prefix which cannot be explained by the statistical models. [sent-80, score-0.304]
</p><p>28 As an alternative to this heuristic approach, Ortiz-Mart ı´nez (201 1) proposed a formalization of the IMT framework that does include stochastic error-correction models in its statistical formalization. [sent-83, score-0.247]
</p><p>29 Note that the translation result, given by Equation (9) may not contain p as prefix because every translation is compatible with p with a certain probability. [sent-85, score-0.478]
</p><p>30 Thus, despite being close, Equation (9) is not equivalent to the IMT formalization in Equation (6). [sent-86, score-0.183]
</p><p>31 To solve this problem, we define an alignment, a, between the user-defined prefix p and the hy-  tˆ,  pothesized translation t, so that the unaligned words of t, in an appropriate order, constitute the suffix searched in IMT. [sent-87, score-0.417]
</p><p>32 This allows us to rewrite the error correction probability as follows: Pr(p | t) =  XPr(p,a  | t)  (10)  Xa  To simplify things, we assume that p is monotonically aligned to t, leaving the potential wordreordering to the language and translation models. [sent-88, score-0.247]
</p><p>33 Under this assumption, a determines an alignment for t, such that t = tpts, where tp is fully-aligned to p and ts remains unaligned. [sent-89, score-0.139]
</p><p>34 Taking all these things into consideration, and following a maximum approximation, we finally arrive to the expression:  (tˆ, aˆ) = argtm,aaxPr(t)·Pr(s | t)·Pr(p,a | t) (11) where the suffix required in IMT is obtained as the portion of that is not aligned with the user prefix. [sent-90, score-0.268]
</p><p>35 4  Alternative Formalization for IMT with  Stochastic Error-Correction Alternatively to Equation (11), we can operate from Equation (9) and reach a different formalization for IMT with error-correction. [sent-93, score-0.183]
</p><p>36 The main difference between the two alternative  IMT formalization (Equations (11) and (17)) is that in the latter the suffix to be returned is conditioned by the user-validated prefix p. [sent-99, score-0.455]
</p><p>37 Thus, in the following we will refer to Equation (11) as independent suffix formalization while we will denote Equation (17) by conditioned suffix formalization. [sent-100, score-0.401]
</p><p>38 Specifically, the proposed approach models the edit distance as a Bernoulli process where each character of the candidate string has a probability pe of being erroneous. [sent-110, score-0.146]
</p><p>39 Finally, we compute the error-correction probability between two strings from the total number of  edits required to transform the candidate translation into the reference translation. [sent-118, score-0.227]
</p><p>40 pke(1 − pe)|p|−k  (19)  where k = Lev(p, ta) is the character-level Levenshtein distance between the user defined prefix p and the prefix ta of the hypothesized translation t defined by alignment a. [sent-122, score-0.594]
</p><p>41 2  Statistical Machine Translation Models  Next sections briefly describe the statistical translation models used to estimate the conditional probability distribution Pr(s | t). [sent-127, score-0.234]
</p><p>42 1  Phrase-Based Translation Models  Phrase-based translation models (Koehn et al. [sent-132, score-0.17]
</p><p>43 The usual phrase-based implementation of the translation probability takes a log-linear form: 248 Pr(s | t) ≈ λ1 · |t | + λ2 · K+  s,  XK  X ? [sent-135, score-0.195]
</p><p>44 (20)  kX= X1  where P(˜ s | t˜) is the translation probability between source phrase and target phrase and d(j) is a function (distortion model) that returns the score of translating the k-th source phrase given that it is separated j words from the (k−1)-th phrase. [sent-137, score-0.245]
</p><p>45 Hierarchical translation models provide a solution to this challenge by allowing gaps in the phrases (Chiang, 2005): yu X1 you X2 → have X2 with X1 where subscripts denote placeholders for subphrases. [sent-144, score-0.17]
</p><p>46 Additionally, two glue rules are also defined: S → < S1X2 , S1X2 > S → < X1 , X1> These give the model the option to build only partial translations using hierarchical phrases, and then combine them serially as in a phrase-based model. [sent-149, score-0.139]
</p><p>47 The typical implementation of the hierarchical translation model also takes the form of a log-linear model. [sent-150, score-0.238]
</p><p>48 4  Search  In offline MT, the generation of the best translation for a given source sentence is carried out by incrementally generating the target sentence2. [sent-160, score-0.195]
</p><p>49 Due to the demanding temporal constraints inherent to any interactive environment, performing a full search each time the user validates a new prefix is unfeasible. [sent-165, score-0.367]
</p><p>50 The computational cost of this approach is much lower, as the whole search for the translation must be carried out only once, and the generated representation can be reused for further completion requests. [sent-167, score-0.194]
</p><p>51 Then, we describe the algorithms implemented to search for suffix completions in them (Section 4. [sent-170, score-0.133]
</p><p>52 eTchtse anu hmeabder h yopfe tranilo dneod aensd i sa called the arity of the hyperedge and the arity of a hypergraph is the maximum arity of its hyperedges. [sent-176, score-0.156]
</p><p>53 Each hypernode represents a partial translation generated during the decoding process. [sent-178, score-0.218]
</p><p>54 Thus, we can achieve a compact representation of the translation space that allows us to derive efficient search algorithms. [sent-184, score-0.194]
</p><p>55 , 2002), which are used to represent the search space for phrase-based models, are a special case of hypergraphs in which the maximum arity is one. [sent-186, score-0.145]
</p><p>56 Thus, hypergraphs allow us to represent both phrase-based  and hierarchical systems in a unified framework. [sent-187, score-0.156]
</p><p>57 2 Suffix Search on Hypergraphs Now, we describe a unified search process to obtain the suffix ts that completes a prefix p given by the user according to the two IMT formulations (Equation (11) and Equation (17)) described in Section 2. [sent-189, score-0.508]
</p><p>58 Specifically, only those hypernodes that generate a prefix of a potential translation are to be taken into account3. [sent-191, score-0.392]
</p><p>59 The probability of the solution defined by each hypernode has two components, namely the probability of the SMT model (given by the language and translation models) and the probability of the error-correction model. [sent-192, score-0.293]
</p><p>60 On the one hand, the SMT model probability is given by the translation of maximum probability through the hypernode. [sent-193, score-0.22]
</p><p>61 On the other hand, the error-correction probability is computed between p and the partial translation of maximum probability actually covered by the hypernode. [sent-194, score-0.22]
</p><p>62 Once the best-scoring hypernode is identified, the  rest of the translation not covered by it is returned as the suffix completion required in IMT. [sent-196, score-0.359]
</p><p>63 1 EU and TED corpora We tested the proposed methods in two different translation tasks each one involving a different language pair: Spanish-to-English (Es–En) for the EU (Bulletin of the European Union) task, and Chineseto-English (Zh–En) for the TED (TED4 talks) task. [sent-205, score-0.17]
</p><p>64 The Zh–En corpus used in the experiments was part of the MT track in the 2011evaluation campaign of the workshop on spoken language translation (Federico et al. [sent-210, score-0.17]
</p><p>65 Then, the optimized models were used to generate the word-graphs and hypergraphs with the translations of the development and test partitions. [sent-222, score-0.159]
</p><p>66 , 2010), we used the references in the corpora to simulate the translations that a human user would want to obtain. [sent-226, score-0.198]
</p><p>67 Each time the system suggested a new translation, it was compared to the reference and the longest common prefix (LCP) was obtained. [sent-227, score-0.138]
</p><p>68 Finally, we used this user simulation to optimize the value for the probability of edition pe in the error-correction model (Section 3. [sent-230, score-0.24]
</p><p>69 In this case, these values were chosen so that they minimize the estimated user effort required to interactively translate the development partitions. [sent-232, score-0.273]
</p><p>70 On the one hand, different IMT systems can be compared according to the effort needed by a human user to generate the desired translations. [sent-235, score-0.196]
</p><p>71 This effort is usually estimated as the number of actions performed by the user while interacting with the system. [sent-236, score-0.219]
</p><p>72 In the user simulation described above these actions are: looking for the next error and moving the mouse pointer to that position (LCP computation), and correcting errors with some key strokes. [sent-237, score-0.199]
</p><p>73 For such systems, character-  level user effort is usually measured by the Character Error Rate (CER). [sent-241, score-0.196]
</p><p>74 Here, when the user enters a character to correct some incorrect word, the system automatically completes the word with the most probable word in the task vocabulary. [sent-244, score-0.188]
</p><p>75 To evaluate the effort of a user using such a system, we implement the following measure proposed in (Romero et al. [sent-245, score-0.217]
</p><p>76 , 2010): Post-editing key stroke ratio (PKSR): using a post-edition system with word-autocompleting, number of user key strokes divided by the total number of reference characters. [sent-246, score-0.163]
</p><p>77 4  Table 3: IMT results (KSMR [%]) for the EU and TED tasks using the independent suffix formalization (ISF) and the conditioned suffix formalization (CSF). [sent-296, score-0.584]
</p><p>78 PB stands for phrase-based model and HT stands for hierarchical translation model. [sent-297, score-0.238]
</p><p>79 6  Results  We start by reporting conventional MT quality results to test if the generated word-graphs and hypergraphs encode translations of similar quality. [sent-299, score-0.195]
</p><p>80 Additionally, the similar aver-  age BLEU results obtained for the 1000-best translations indicate that word-graphs and hypergraphs encode translations of similar quality. [sent-303, score-0.23]
</p><p>81 We report KSMR (as a percentage) for the independent suffix formalization (ISF) and the conditioned suffix formalization (CSF) using both phase-based (PB) and hierarchical (HT) translation models. [sent-307, score-0.822]
</p><p>82 2%)  Table 4: Estimation of the effort required to translate the test partition of the EU and TED tasks using postediting with word-completion (PE) and IMT under the independent suffix formalization (IMT). [sent-316, score-0.415]
</p><p>83 Regarding the CSF, its better results are due to the better suffixes that can be obtained by taking into account the actual prefix validated by the user. [sent-324, score-0.172]
</p><p>84 Finally, we compared the estimated human effort required to translate the test partitions of the EU and TED corpora with the best IMT configuration (independent suffix formalization with hierarchical translation model) and a conventional post-editing (PE)  CAT system with word-completion. [sent-325, score-0.712]
</p><p>85 That is, when the user corrects a character, the PE system automatically proposes a different word that begins with the given word prefix but, obviously, the rest of the sentence is not changed. [sent-326, score-0.296]
</p><p>86 According to the results, the estimated human effort to generate the error-free translations was significantly reduced with respect to using the conventional PE approach. [sent-327, score-0.199]
</p><p>87 7  Summary and Future Work  We have proposed a new IMT approach that uses hierarchical SMT models as its underlying translation technology. [sent-329, score-0.238]
</p><p>88 This approach is based on a statistical formalization previously described in the literature that includes stochastic error correction. [sent-330, score-0.243]
</p><p>89 Additionally, we have proposed a refined formalization that improves the quality of the IMT suffixes by taking into account the prefix validated by the user. [sent-331, score-0.355]
</p><p>90 Moreover, since word-graphs constitute a particular case of hypergraphs, we are able to manage both phrasebased and hierarchical translation models in a unified IMT framework. [sent-332, score-0.238]
</p><p>91 Simulated results on two different translation tasks showed that hierarchical translation models outperform phrase-based models in our IMT framework. [sent-333, score-0.408]
</p><p>92 Additionally, the proposed alternative IMT formalization also allows to improve the results of the IMT formalization previously described in the literature. [sent-334, score-0.391]
</p><p>93 Finally, the proposed IMT system with hierarchical SMT models largely reduces the estimated user effort required to generate correct translations in comparison to that of a conventional postedition system. [sent-335, score-0.426]
</p><p>94 In such scenarios, the user will be allowed to correct errors at any position in the translation while the IMT system will be required to derive translations compatible with these isolated corrections. [sent-339, score-0.4]
</p><p>95 Adaptive translation engines that take advan-  tage potifv vthee t ruasnesrl’ast cioonrree nctgioinness to improve iadtsv satan-tistical models. [sent-340, score-0.17]
</p><p>96 As the translator works and corrects the proposed translations, the translation engine will be able to make better predictions. [sent-341, score-0.253]
</p><p>97 t Specifically, measures mthaatte ee tsthiemate the cognitive load involve in reading, understanding and detecting an error in a translation (Foster et al. [sent-347, score-0.191]
</p><p>98 Balancing user effort and translation error in interactive machine translation via confidence measures. [sent-395, score-0.635]
</p><p>99 Adaptive language and translation models for interactive machine translation. [sent-427, score-0.248]
</p><p>100 Measuring confidence intervals for the machine translation evaluation metrics. [sent-478, score-0.17]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('imt', 0.776), ('formalization', 0.183), ('pr', 0.183), ('translation', 0.17), ('barrachina', 0.155), ('prefix', 0.138), ('user', 0.127), ('suffix', 0.109), ('eu', 0.105), ('ted', 0.1), ('hypergraphs', 0.088), ('ts', 0.085), ('csf', 0.084), ('hypernodes', 0.084), ('interactive', 0.078), ('equation', 0.073), ('ksmr', 0.072), ('translations', 0.071), ('effort', 0.069), ('hierarchical', 0.068), ('mt', 0.066), ('nez', 0.066), ('pe', 0.063), ('pksr', 0.06), ('ht', 0.059), ('isf', 0.058), ('tp', 0.054), ('pb', 0.053), ('translator', 0.052), ('argtmaxpr', 0.048), ('hypernode', 0.048), ('ksr', 0.048), ('smt', 0.04), ('statistical', 0.039), ('langlais', 0.038), ('argtmsaxpr', 0.036), ('decoupled', 0.036), ('isabelle', 0.036), ('strokes', 0.036), ('conventional', 0.036), ('character', 0.036), ('validated', 0.034), ('arity', 0.033), ('bleu', 0.033), ('required', 0.032), ('hypergraph', 0.032), ('levenshtein', 0.032), ('corrects', 0.031), ('telescope', 0.031), ('wg', 0.031), ('xpr', 0.031), ('correction', 0.031), ('francisco', 0.03), ('och', 0.029), ('philippe', 0.028), ('zollmann', 0.028), ('guy', 0.028), ('reordering', 0.027), ('mouse', 0.026), ('jes', 0.026), ('gonz', 0.026), ('foster', 0.026), ('saw', 0.026), ('spanish', 0.026), ('probability', 0.025), ('source', 0.025), ('alternative', 0.025), ('simulation', 0.025), ('completes', 0.025), ('hyperedge', 0.025), ('hg', 0.025), ('search', 0.024), ('additionally', 0.024), ('casacuberta', 0.024), ('errorcorrection', 0.024), ('formalizations', 0.024), ('garc', 0.024), ('ismael', 0.024), ('lapalme', 0.024), ('lcp', 0.024), ('lista', 0.024), ('nepveu', 0.024), ('recursos', 0.024), ('transtype', 0.024), ('estimated', 0.023), ('listing', 0.023), ('edit', 0.022), ('translate', 0.022), ('view', 0.022), ('pierre', 0.022), ('cat', 0.022), ('franz', 0.022), ('error', 0.021), ('hypothesized', 0.021), ('implement', 0.021), ('romero', 0.021), ('tseng', 0.021), ('aho', 0.021), ('ecnica', 0.021), ('para', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="107-tfidf-1" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>Author: Jesus Gonzalez-Rubio ; Daniel Ortiz-Martinez ; Jose-Miguel Benedi ; Francisco Casacuberta</p><p>Abstract: Current automatic machine translation systems are not able to generate error-free translations and human intervention is often required to correct their output. Alternatively, an interactive framework that integrates the human knowledge into the translation process has been presented in previous works. Here, we describe a new interactive machine translation approach that is able to work with phrase-based and hierarchical translation models, and integrates error-correction all in a unified statistical framework. In our experiments, our approach outperforms previous interactive translation systems, and achieves estimated effort reductions of as much as 48% relative over a traditional post-edition system.</p><p>2 0.1492521 <a title="107-tfidf-2" href="./emnlp-2013-What_is_Hidden_among_Translation_Rules.html">201 emnlp-2013-What is Hidden among Translation Rules</a></p>
<p>Author: Libin Shen ; Bowen Zhou</p><p>Abstract: Most of the machine translation systems rely on a large set of translation rules. These rules are treated as discrete and independent events. In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. We present a preliminary generative model to test this idea. Experimental results show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation.</p><p>3 0.13187458 <a title="107-tfidf-3" href="./emnlp-2013-Factored_Soft_Source_Syntactic_Constraints_for_Hierarchical_Machine_Translation.html">84 emnlp-2013-Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation</a></p>
<p>Author: Zhongqiang Huang ; Jacob Devlin ; Rabih Zbib</p><p>Abstract: Translation Jacob Devlin Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA j devl in@bbn . com Rabih Zbib Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA r zbib@bbn . com have tried to introduce grammaticality to the transThis paper describes a factored approach to incorporating soft source syntactic constraints into a hierarchical phrase-based translation system. In contrast to traditional approaches that directly introduce syntactic constraints to translation rules by explicitly decorating them with syntactic annotations, which often exacerbate the data sparsity problem and cause other problems, our approach keeps translation rules intact and factorizes the use of syntactic constraints through two separate models: 1) a syntax mismatch model that associates each nonterminal of a translation rule with a distribution of tags that is used to measure the degree of syntactic compatibility of the translation rule on source spans; 2) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language. The features produced by both models are used as soft constraints to guide the translation process. Experiments on Chinese-English translation show that the proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets.</p><p>4 0.12417103 <a title="107-tfidf-4" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>Author: Xinyan Xiao ; Deyi Xiong</p><p>Abstract: Traditional synchronous grammar induction estimates parameters by maximizing likelihood, which only has a loose relation to translation quality. Alternatively, we propose a max-margin estimation approach to discriminatively inducing synchronous grammars for machine translation, which directly optimizes translation quality measured by BLEU. In the max-margin estimation of parameters, we only need to calculate Viterbi translations. This further facilitates the incorporation of various non-local features that are defined on the target side. We test the effectiveness of our max-margin estimation framework on a competitive hierarchical phrase-based system. Experiments show that our max-margin method significantly outperforms the traditional twostep pipeline for synchronous rule extraction by 1.3 BLEU points and is also better than previous max-likelihood estimation method.</p><p>5 0.10527653 <a title="107-tfidf-5" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>Author: Joern Wuebker ; Stephan Peitz ; Felix Rietig ; Hermann Ney</p><p>Abstract: Automatically clustering words from a monolingual or bilingual training corpus into classes is a widely used technique in statistical natural language processing. We present a very simple and easy to implement method for using these word classes to improve translation quality. It can be applied across different machine translation paradigms and with arbitrary types of models. We show its efficacy on a small German→English and a larger F ornenc ah s→mGalelrm Gaenrm mtarann→slEatniognli tsahsk a nwdit ha lbaortghe rst Farnednacrhd→ phrase-based salandti nhie traaskrch wiciathl phrase-based translation systems for a common set of models. Our results show that with word class models, the baseline can be improved by up to 1.4% BLEU and 1.0% TER on the French→German task and 0.3% BLEU aonnd t h1e .1 F%re nTcEhR→ on tehrem German→English Btask.</p><p>6 0.1018079 <a title="107-tfidf-6" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>7 0.07831277 <a title="107-tfidf-7" href="./emnlp-2013-Flexible_and_Efficient_Hypergraph_Interactions_for_Joint_Hierarchical_and_Forest-to-String_Decoding.html">88 emnlp-2013-Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest-to-String Decoding</a></p>
<p>8 0.077899091 <a title="107-tfidf-8" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>9 0.075859368 <a title="107-tfidf-9" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>10 0.075376943 <a title="107-tfidf-10" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>11 0.075055428 <a title="107-tfidf-11" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>12 0.074655913 <a title="107-tfidf-12" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>13 0.068103142 <a title="107-tfidf-13" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>14 0.066940933 <a title="107-tfidf-14" href="./emnlp-2013-Efficient_Left-to-Right_Hierarchical_Phrase-Based_Translation_with_Improved_Reordering.html">71 emnlp-2013-Efficient Left-to-Right Hierarchical Phrase-Based Translation with Improved Reordering</a></p>
<p>15 0.064193197 <a title="107-tfidf-15" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>16 0.062499467 <a title="107-tfidf-16" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>17 0.059191205 <a title="107-tfidf-17" href="./emnlp-2013-Max-Violation_Perceptron_and_Forced_Decoding_for_Scalable_MT_Training.html">128 emnlp-2013-Max-Violation Perceptron and Forced Decoding for Scalable MT Training</a></p>
<p>18 0.058717389 <a title="107-tfidf-18" href="./emnlp-2013-Optimal_Beam_Search_for_Machine_Translation.html">145 emnlp-2013-Optimal Beam Search for Machine Translation</a></p>
<p>19 0.056916364 <a title="107-tfidf-19" href="./emnlp-2013-Translating_into_Morphologically_Rich_Languages_with_Synthetic_Phrases.html">186 emnlp-2013-Translating into Morphologically Rich Languages with Synthetic Phrases</a></p>
<p>20 0.05306945 <a title="107-tfidf-20" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.175), (1, -0.185), (2, 0.048), (3, 0.031), (4, 0.053), (5, -0.057), (6, -0.014), (7, 0.035), (8, 0.021), (9, -0.031), (10, -0.046), (11, 0.025), (12, 0.039), (13, -0.054), (14, -0.003), (15, 0.008), (16, 0.046), (17, 0.009), (18, 0.062), (19, 0.04), (20, -0.043), (21, 0.024), (22, -0.017), (23, -0.018), (24, 0.009), (25, -0.048), (26, -0.037), (27, -0.067), (28, -0.036), (29, 0.056), (30, -0.046), (31, -0.183), (32, -0.006), (33, -0.026), (34, 0.137), (35, -0.085), (36, 0.04), (37, 0.091), (38, -0.102), (39, 0.106), (40, 0.115), (41, 0.0), (42, -0.021), (43, 0.024), (44, -0.056), (45, 0.017), (46, -0.096), (47, 0.003), (48, 0.106), (49, -0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92325068 <a title="107-lsi-1" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>Author: Jesus Gonzalez-Rubio ; Daniel Ortiz-Martinez ; Jose-Miguel Benedi ; Francisco Casacuberta</p><p>Abstract: Current automatic machine translation systems are not able to generate error-free translations and human intervention is often required to correct their output. Alternatively, an interactive framework that integrates the human knowledge into the translation process has been presented in previous works. Here, we describe a new interactive machine translation approach that is able to work with phrase-based and hierarchical translation models, and integrates error-correction all in a unified statistical framework. In our experiments, our approach outperforms previous interactive translation systems, and achieves estimated effort reductions of as much as 48% relative over a traditional post-edition system.</p><p>2 0.76803088 <a title="107-lsi-2" href="./emnlp-2013-What_is_Hidden_among_Translation_Rules.html">201 emnlp-2013-What is Hidden among Translation Rules</a></p>
<p>Author: Libin Shen ; Bowen Zhou</p><p>Abstract: Most of the machine translation systems rely on a large set of translation rules. These rules are treated as discrete and independent events. In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. We present a preliminary generative model to test this idea. Experimental results show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation.</p><p>3 0.63766539 <a title="107-lsi-3" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>Author: Xiaoning Zhu ; Zhongjun He ; Hua Wu ; Haifeng Wang ; Conghui Zhu ; Tiejun Zhao</p><p>Abstract: This paper proposes a novel approach that utilizes a machine learning method to improve pivot-based statistical machine translation (SMT). For language pairs with few bilingual data, a possible solution in pivot-based SMT using another language as a</p><p>4 0.6310001 <a title="107-lsi-4" href="./emnlp-2013-Flexible_and_Efficient_Hypergraph_Interactions_for_Joint_Hierarchical_and_Forest-to-String_Decoding.html">88 emnlp-2013-Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest-to-String Decoding</a></p>
<p>Author: Martin Cmejrek ; Haitao Mi ; Bowen Zhou</p><p>Abstract: Machine translation benefits from system combination. We propose flexible interaction of hypergraphs as a novel technique combining different translation models within one decoder. We introduce features controlling the interactions between the two systems and explore three interaction schemes of hiero and forest-to-string models—specification, generalization, and interchange. The experiments are carried out on large training data with strong baselines utilizing rich sets of dense and sparse features. All three schemes significantly improve results of any single system on four testsets. We find that specification—a more constrained scheme that almost entirely uses forest-to-string rules, but optionally uses hiero rules for shorter spans—comes out as the strongest, yielding improvement up to 0.9 (T -B )/2 points. We also provide a detailed experimental and qualitative analysis of the results.</p><p>5 0.58914864 <a title="107-lsi-5" href="./emnlp-2013-Factored_Soft_Source_Syntactic_Constraints_for_Hierarchical_Machine_Translation.html">84 emnlp-2013-Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation</a></p>
<p>Author: Zhongqiang Huang ; Jacob Devlin ; Rabih Zbib</p><p>Abstract: Translation Jacob Devlin Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA j devl in@bbn . com Rabih Zbib Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA r zbib@bbn . com have tried to introduce grammaticality to the transThis paper describes a factored approach to incorporating soft source syntactic constraints into a hierarchical phrase-based translation system. In contrast to traditional approaches that directly introduce syntactic constraints to translation rules by explicitly decorating them with syntactic annotations, which often exacerbate the data sparsity problem and cause other problems, our approach keeps translation rules intact and factorizes the use of syntactic constraints through two separate models: 1) a syntax mismatch model that associates each nonterminal of a translation rule with a distribution of tags that is used to measure the degree of syntactic compatibility of the translation rule on source spans; 2) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language. The features produced by both models are used as soft constraints to guide the translation process. Experiments on Chinese-English translation show that the proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets.</p><p>6 0.57664317 <a title="107-lsi-6" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>7 0.56155461 <a title="107-lsi-7" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>8 0.54874986 <a title="107-lsi-8" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>9 0.5290879 <a title="107-lsi-9" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>10 0.50227678 <a title="107-lsi-10" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>11 0.49716654 <a title="107-lsi-11" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>12 0.44912902 <a title="107-lsi-12" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>13 0.44740099 <a title="107-lsi-13" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>14 0.4342159 <a title="107-lsi-14" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>15 0.41565248 <a title="107-lsi-15" href="./emnlp-2013-An_Efficient_Language_Model_Using_Double-Array_Structures.html">20 emnlp-2013-An Efficient Language Model Using Double-Array Structures</a></p>
<p>16 0.405404 <a title="107-lsi-16" href="./emnlp-2013-Structured_Penalties_for_Log-Linear_Language_Models.html">176 emnlp-2013-Structured Penalties for Log-Linear Language Models</a></p>
<p>17 0.40399352 <a title="107-lsi-17" href="./emnlp-2013-Converting_Continuous-Space_Language_Models_into_N-Gram_Language_Models_for_Statistical_Machine_Translation.html">52 emnlp-2013-Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation</a></p>
<p>18 0.40278581 <a title="107-lsi-18" href="./emnlp-2013-Recurrent_Continuous_Translation_Models.html">156 emnlp-2013-Recurrent Continuous Translation Models</a></p>
<p>19 0.40146983 <a title="107-lsi-19" href="./emnlp-2013-A_Convex_Alternative_to_IBM_Model_2.html">2 emnlp-2013-A Convex Alternative to IBM Model 2</a></p>
<p>20 0.39566618 <a title="107-lsi-20" href="./emnlp-2013-Boosting_Cross-Language_Retrieval_by_Learning_Bilingual_Phrase_Associations_from_Relevance_Rankings.html">39 emnlp-2013-Boosting Cross-Language Retrieval by Learning Bilingual Phrase Associations from Relevance Rankings</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.04), (10, 0.015), (18, 0.034), (20, 0.256), (22, 0.048), (26, 0.018), (30, 0.096), (45, 0.027), (50, 0.026), (51, 0.135), (66, 0.056), (71, 0.021), (75, 0.03), (77, 0.059), (96, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75982773 <a title="107-lda-1" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>Author: Jesus Gonzalez-Rubio ; Daniel Ortiz-Martinez ; Jose-Miguel Benedi ; Francisco Casacuberta</p><p>Abstract: Current automatic machine translation systems are not able to generate error-free translations and human intervention is often required to correct their output. Alternatively, an interactive framework that integrates the human knowledge into the translation process has been presented in previous works. Here, we describe a new interactive machine translation approach that is able to work with phrase-based and hierarchical translation models, and integrates error-correction all in a unified statistical framework. In our experiments, our approach outperforms previous interactive translation systems, and achieves estimated effort reductions of as much as 48% relative over a traditional post-edition system.</p><p>2 0.69957215 <a title="107-lda-2" href="./emnlp-2013-Simulating_Early-Termination_Search_for_Verbose_Spoken_Queries.html">173 emnlp-2013-Simulating Early-Termination Search for Verbose Spoken Queries</a></p>
<p>Author: Jerome White ; Douglas W. Oard ; Nitendra Rajput ; Marion Zalk</p><p>Abstract: Building search engines that can respond to spoken queries with spoken content requires that the system not just be able to find useful responses, but also that it know when it has heard enough about what the user wants to be able to do so. This paper describes a simulation study with queries spoken by non-native speakers that suggests that indicates that finding relevant content is often possible within a half minute, and that combining features based on automatically recognized words with features designed for automated prediction of query difficulty can serve as a useful basis for predicting when that useful content has been found.</p><p>3 0.66552132 <a title="107-lda-3" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>Author: Zhongqing Wang ; Shoushan LI ; Fang Kong ; Guodong Zhou</p><p>Abstract: Personal profile information on social media like LinkedIn.com and Facebook.com is at the core of many interesting applications, such as talent recommendation and contextual advertising. However, personal profiles usually lack organization confronted with the large amount of available information. Therefore, it is always a challenge for people to find desired information from them. In this paper, we address the task of personal profile summarization by leveraging both personal profile textual information and social networks. Here, using social networks is motivated by the intuition that, people with similar academic, business or social connections (e.g. co-major, co-university, and cocorporation) tend to have similar experience and summaries. To achieve the learning process, we propose a collective factor graph (CoFG) model to incorporate all these resources of knowledge to summarize personal profiles with local textual attribute functions and social connection factors. Extensive evaluation on a large-scale dataset from LinkedIn.com demonstrates the effectiveness of the proposed approach. 1</p><p>4 0.58993942 <a title="107-lda-4" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>Author: Xiaoqing Zheng ; Hanyang Chen ; Tianyu Xu</p><p>Abstract: This study explores the feasibility of performing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representation of Chinese characters, and use these improved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-theart performance with minimal computational cost. We also describe a perceptron-style algorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented.</p><p>5 0.58860731 <a title="107-lda-5" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>Author: Fandong Meng ; Jun Xie ; Linfeng Song ; Yajuan Lu ; Qun Liu</p><p>Abstract: We present a novel translation model, which simultaneously exploits the constituency and dependency trees on the source side, to combine the advantages of two types of trees. We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. Our rules hold the property of long distance reorderings and the compatibility with phrases. Large-scale experimental results show that our model achieves significantly improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets.</p><p>6 0.58831447 <a title="107-lda-6" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>7 0.58594406 <a title="107-lda-7" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>8 0.58453691 <a title="107-lda-8" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>9 0.58314216 <a title="107-lda-9" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>10 0.58142972 <a title="107-lda-10" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>11 0.58122963 <a title="107-lda-11" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>12 0.57963866 <a title="107-lda-12" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>13 0.57631898 <a title="107-lda-13" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>14 0.57517385 <a title="107-lda-14" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>15 0.57492232 <a title="107-lda-15" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>16 0.57419473 <a title="107-lda-16" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>17 0.5716669 <a title="107-lda-17" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>18 0.57132757 <a title="107-lda-18" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>19 0.57104069 <a title="107-lda-19" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>20 0.56979978 <a title="107-lda-20" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
