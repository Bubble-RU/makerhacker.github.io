<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-169" href="#">emnlp2013-169</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</h1>
<br/><p>Source: <a title="emnlp-2013-169-pdf" href="http://aclweb.org/anthology//D/D13/D13-1153.pdf">pdf</a></p><p>Author: Min Xiao ; Yuhong Guo</p><p>Abstract: Cross-lingual adaptation aims to learn a prediction model in a label-scarce target language by exploiting labeled data from a labelrich source language. An effective crosslingual adaptation system can substantially reduce the manual annotation effort required in many natural language processing tasks. In this paper, we propose a new cross-lingual adaptation approach for document classification based on learning cross-lingual discriminative distributed representations of words. Specifically, we propose to maximize the loglikelihood of the documents from both language domains under a cross-lingual logbilinear document model, while minimizing the prediction log-losses of labeled documents. We conduct extensive experiments on cross-lingual sentiment classification tasks of Amazon product reviews. Our experimental results demonstrate the efficacy of the pro- posed cross-lingual adaptation approach.</p><p>Reference: <a title="emnlp-2013-169-reference" href="../emnlp2013_reference/emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Cross-lingual adaptation aims to learn a prediction model in a label-scarce target language by exploiting labeled data from a labelrich source language. [sent-2, score-0.61]
</p><p>2 An effective crosslingual adaptation system can substantially reduce the manual annotation effort required in many natural language processing tasks. [sent-3, score-0.25]
</p><p>3 In this paper, we propose a new cross-lingual adaptation approach for document classification based on learning cross-lingual discriminative distributed representations of words. [sent-4, score-0.586]
</p><p>4 Specifically, we propose to maximize the loglikelihood of the documents from both language domains under a cross-lingual logbilinear document model, while minimizing the prediction log-losses of labeled documents. [sent-5, score-0.571]
</p><p>5 We conduct extensive experiments on cross-lingual sentiment classification tasks of Amazon product reviews. [sent-6, score-0.41]
</p><p>6 Recently, cross-lingual adaptation methods have been studied to exploit labeled information from an existing source language domain where labeled training data is abundant for use in a target language domain where annotated training data is scarce (Prettenhofer and Stein, 2010). [sent-11, score-1.024]
</p><p>7 Previous work has shown that cross-lingual adaptation 1465 can greatly reduce labeling effort for a variety of cross language NLP tasks such as document categorization (Bel et al. [sent-12, score-0.39]
</p><p>8 , 2009), genre classification (Petrenz and Webber, 2012), and sentiment classification (Shanahan et al. [sent-14, score-0.454]
</p><p>9 The fundamental challenge of cross-lingual adaptation stems from a lack of overlap between the fea-  ture space of the source language data and that of the target language data. [sent-16, score-0.392]
</p><p>10 They first translate all the text data from one language domain into the other and then apply techniques such as domain adaptation (Wan et al. [sent-18, score-0.454]
</p><p>11 As an economic alternative solution, cross-lingual representation learning has recently been used in the literature to learn language-independent representations of the data for cross language text classification (Prettenhofer and Stein, 2010; Petrenz and Webber, 2012). [sent-24, score-0.554]
</p><p>12 In this paper, we propose to tackle cross language text classification by inducing cross-lingual predictive data representations with both labeled and unlabeled documents from the two language domains. [sent-25, score-0.806]
</p><p>13 Specifically, we propose a cross-lingual log-bilinear  document model to learn distributed representations of words, which can capture both the semantic simProce Sdeiantgtlse o,f W thaesh 2i0n1gt3o nC,o UnSfeAre,n 1c8e- o2n1 E Omctpoibriecra 2l0 M13et. [sent-26, score-0.319]
</p><p>14 oc d2s0 i1n3 N Aastusorcaila Ltiaon g fuoarg Ceo Pmrpoucetastsi on ga,l p Laignegsu 1is4t6ic5s–1475, ilarities of words across languages and the predictive information with respect to the target classification task. [sent-28, score-0.378]
</p><p>15 We conduct the representation learning by maximizing the log-likelihood of all documents from both language domains under the crosslingual log-bilinear document model and minimizing the prediction log-losses of labeled documents. [sent-29, score-0.832]
</p><p>16 To evaluate the effectiveness of the proposed approach, we conduct experiments on the task of cross language sentiment classification of Amazon product reviews. [sent-31, score-0.532]
</p><p>17 Basically, they first employ machine translation tools to translate documents from one language domain to the other one and then induce low dimensional latent representations as interlingual representations (Littman et al. [sent-34, score-0.969]
</p><p>18 (1998) proposed a cross-language latent semantic indexing method to induce interlingual representations by performing latent semantic indexing over a dual-language document-term matrix, where each dual-language document contains its original words and the corresponding translation text. [sent-40, score-0.671]
</p><p>19 (201 1) proposed a bi-view non-negative matrix tri-factorization method for cross-lingual sentiment classification on the parallel training and test data. [sent-46, score-0.432]
</p><p>20 Guo and Xiao (2012a) developed a transductive subspace representation learning method for crosslingual text classification based on non-negative matrix factorization. [sent-47, score-0.485]
</p><p>21 Some other works exploited parallel data by using multilingual topic models to extract cross-language latent topics as interlingual representations (Mimno et al. [sent-48, score-0.485]
</p><p>22 , 2011) and using neural probabilistic language modes to learn word embeddings as cross-lingual distributed representations (Klementiev et al. [sent-52, score-0.275]
</p><p>23 , 2000) to induce cross-lingual word distributed representations on a set of wordlevel aligned parallel sentences. [sent-63, score-0.279]
</p><p>24 Another group of works propose to use bilingual dictionaries to learn interlingual representations (Gliozzo, 2006; Prettenhofer and Stein, 2010). [sent-67, score-0.426]
</p><p>25 Then they conducted latent semantic analysis (LSA) over the document-term matrix with concatenated vocabularies to obtain interlingual representations. [sent-69, score-0.294]
</p><p>26 Some other bilingual resources, such as multilingual WordNet (Fellbaum, 1998) and universal partof-speech (POS) tags (Petrov et al. [sent-74, score-0.241]
</p><p>27 (2012), which transformed words from different languages to WordNet synset identifiers as interlingual sense-based representations. [sent-80, score-0.213]
</p><p>28 Recently, Petrenz and Webber (2012) used language-specific POS taggers to tag each word and then mapped those language-specific POS tags to twelve universal POS tags as interlingual features for cross language fine-grained genre classification. [sent-82, score-0.28]
</p><p>29 3  Semi-Supervised Representation Learning for Cross-Lingual Text Classification  In this section, we introduce a semi-supervised cross-lingual representation learning method and then use it for cross language text classification. [sent-84, score-0.256]
</p><p>30 Assume we have ℓs labeled and us unlabeled doc-  uments in the source language domain S and ℓt labuemleedn asn idn ut eun solaubrceleed la ndgocuuamgee ndotsm iani tnhe S target lan1467 guage domain T . [sent-85, score-0.78]
</p><p>31 We assume all the documents are independent ann Td identically dei asltlri tbhuete ddo cinu meaecnhts sl aanreguage domain, and each document xi is represented as a bag of words, xi = {wi1, wi2 , . [sent-86, score-0.425]
</p><p>32 a Wnde its label, and consider exploiting the labeled documents in the source domain S for learning classifiers imn ethntes target d soomurcaien d Tom . [sent-91, score-0.753]
</p><p>33 s between the two language domains, we first construct a set of critical bilingual word pairs M = {(wis, wjt)}im=1, where wis is a critiwcaolr dw poardir si nM Mth =e source la)ng}uage domain, wjt is its translation in the target language domain, and m is the number of word pairs. [sent-93, score-0.492]
</p><p>34 First we select a subset ofwords from the source language domain, which have the highest mutual infor-  mation with the class labels in labeled source documents. [sent-96, score-0.382]
</p><p>35 The mutual information is computed based on the empirical distributions of words and labels in the labeled source documents. [sent-97, score-0.277]
</p><p>36 Then we translate the selected words into the target language using a translation tool to produce word pairs. [sent-98, score-0.266]
</p><p>37 Finally we produce the M set by eliminating any candidate pair (ws, wt), if either occurs less than a predefined threshold value φ in all source language documents or wt occurs less than φ in all target language documents. [sent-99, score-0.437]
</p><p>38 Given the constructed bilingual word pair set M, the words appearing in the source language documents but not in M can be put together to form a source specific vocabulary set Vs = {ws1, . [sent-100, score-0.58]
</p><p>39 Similarly, the words appearing in th=e target language documents but not in M can be put together to form a target specific vocabulary set Vt = {wt1, . [sent-104, score-0.584]
</p><p>40 ∪T Mhis, cross-lingual vocabulary set covers all words appearing in both domains, while mapping each bilingual pair in M into the same entry. [sent-110, score-0.207]
</p><p>41 To tackle cross language text classification, we  ws  wvss  then propose a cross-lingual log-bilinear document model to learn a predictive cross-lingual representation of words, which maps each entry in the vocabulary set V to one row vector in a word embedding matrix R ∈ Rv×k. [sent-111, score-0.677]
</p><p>42 Moreover, we explicitly incorporate the label information into our proposed approach, rendering the induced word embeddings more discriminative to the target prediction task. [sent-115, score-0.367]
</p><p>43 1 Cross-Lingual Word Embeddings As mentioned above, we assume a unified embedding matrix R which contains the distributed vector representations of words in the two language domains. [sent-117, score-0.422]
</p><p>44 However, even in a unified representation space, the distribution of words in the two domains will be different. [sent-118, score-0.231]
</p><p>45 To capture the distribution divergence of the two domains and facilitate cross-  lingual learning, we split the word embedding matrix into three parts: source language specific part Rs ∈ Rv×ks , common part Rc ∈ Rv×kc and target language specific part Rt ∈ ∈R vR×kt , such that k = ks + kc + kt. [sent-119, score-0.786]
</p><p>46 Intuitively, we assume that source language words contain no target language specific representations and target language words contain no source language specific representations. [sent-120, score-0.697]
</p><p>47 Thus for words in the two language domains, we retrieve their distributed vector representations from the embedding matrix R using two mapping functions, ΦS and ΦT, one for each language domain. [sent-121, score-0.422]
</p><p>48 To encode  more information into the common part of representation for better knowledge transfer from the source 1468 language domain to the target language domain, we assume kc ≥ ks and kc ≥ kt. [sent-124, score-0.846]
</p><p>49 The form of three part feature representations h≥as k been exploited in previous work of domain adaptation with heterogeneous feature spaces (Duan et al. [sent-125, score-0.411]
</p><p>50 However, their approach simply duplicates the original features as language-specific representations, while we will automatically learn those three part latent representations in our approach. [sent-127, score-0.215]
</p><p>51 The first part of the objective function captures the likelihood of the documents being generated with the learned representation R. [sent-130, score-0.292]
</p><p>52 The second part of the objective function in (3) takes the label information into account and aims to render the latent word representations more taskpredictive. [sent-133, score-0.215]
</p><p>53 6) where w, q are model parameters of the logistic regression model, ΨL (xi) is the k-dimensional vector representation of the document xi in the language domain L. [sent-139, score-0.491]
</p><p>54 The distributed vector representation of any given document can then be computed using Eq. [sent-152, score-0.342]
</p><p>55 4  Experiments  We empirically evaluate the proposed approach using the cross language sentiment classification tasks of Amazon product reviews in four languages. [sent-157, score-0.672]
</p><p>56 1 Dataset We used the multilingual sentiment classification dataset1 provided by Prettenhofer and Stein (2010), which contains Amazon product reviews in four different languages, English (E), French (F), German  (G) and Japanese (J). [sent-160, score-0.553]
</p><p>57 The English product reviews were sampled from previous cross-domain sentiment classification datasets (Blitzer et al. [sent-161, score-0.436]
</p><p>58 Following the work (Prettenhofer and Stein, 2010), we used the original English reviews as the source language while treating the other three languages as target languages. [sent-166, score-0.417]
</p><p>59 Thus, we construct nine cross language sentiment classification tasks (GB, GD, GM, FB, FD, FM, JB, JD, JM), one for each target language-category pair. [sent-167, score-0.815]
</p><p>60 2  Approaches  We compare our proposed semi-supervised crosslingual representation learning (CL-RL) approach to the following approaches for cross-lingual document classification. [sent-170, score-0.401]
</p><p>61 de/research/corpora/ Table 1: Average classification accuracies and standard deviations for the 9 cross-lingual sentiment classification tasks. [sent-173, score-0.628]
</p><p>62 •  •  TB: This is a target baseline method, which tTrBai:ns T a supervised monolingual cthlaosds,if wierh on the labeled training data from the target language domain without representation learning. [sent-176, score-0.783]
</p><p>63 1470 In all experiments, we used a linear support vector machine (SVM) for sentiment classification. [sent-181, score-0.199]
</p><p>64 For the CL-SCL method, we used the same parameter setting as suggested in the paper (Prettenhofer and Stein, 2010): the number of pivot features is set as 450, the threshold value for selecting pivot features is 30, and the reduced dimensionality after singular value decomposition is 100. [sent-184, score-0.231]
</p><p>65 For the CLD-LSA method, we set the dimensionality of latent representation as 1000. [sent-185, score-0.284]
</p><p>66 The values of α, β, γ and η are selected using the first cross language classification task GB. [sent-188, score-0.276]
</p><p>67 3  Classification Accuracy  For each of the nine cross language sentiment classification tasks with different target language-category pairs, we used the training set in the source language domain (English) as labeled data while treating the test set in the source language domain as unlabeled. [sent-195, score-1.485]
</p><p>68 For target language domain, we used the test set as test data while randomly selecting 100 documents from the training set as labeled data and treating the rest as unlabeled data. [sent-197, score-0.55]
</p><p>69 Thus, for each task, we have 2000 labeled documents and 2000 unlabeled documents from the source language domain, and 100 labeled and 1900 unlabeled documents from the tar-  get language domain for training. [sent-198, score-1.174]
</p><p>70 We have 2000 test documents from the target language domain as testing data. [sent-199, score-0.476]
</p><p>71 We repeated each experiment 1471 10 times with different random selections of 100 labeled training documents from the target language domain. [sent-201, score-0.561]
</p><p>72 The average classification accuracies and standard deviations are reported in Table 1. [sent-202, score-0.323]
</p><p>73 From Table 1, we can see that the proposed semisupervised cross-lingual representation learning approach, CL-RL, clearly outperforms all other comparison methods on eight out of the nine tasks. [sent-203, score-0.338]
</p><p>74 By exploiting the large amount of labeled training data from the source language domain, even the simple cross-lingual adaptation approach, CL-Dict, produces effective improvements over TB. [sent-206, score-0.395]
</p><p>75 With a better designed representation learning, CLD-LSA outperforms CL-Dict on all the nine tasks, but the improvements are very small on some tasks (e. [sent-210, score-0.343]
</p><p>76 4  Classification Accuracy vs the Number of Labeled Target Documents Next, we investigated the performance of the six approaches by varying the number of labeled training documents from the target language domain. [sent-223, score-0.504]
</p><p>77 In each experiment, for a given value ℓt, we randomly selected ℓt documents from the training set of the target language domain as labeled data and used the rest as unlabeled data. [sent-225, score-0.694]
</p><p>78 We still performed prediction on the same 2000 test documents in the target language domain. [sent-226, score-0.378]
</p><p>79 We repeated each experiment 10 times based on different random selections of the labeled training data from the target  language domain. [sent-227, score-0.398]
</p><p>80 The average classification accuracies and standard deviations across different ℓt values for all comparison methods on all the nine tasks are plotted in Figure 1. [sent-228, score-0.537]
</p><p>81 We can see when the number of labeled target documents is small, TB performs poorly, especially for the first six tasks (GB, GD, GM, FB, FD, FM). [sent-229, score-0.561]
</p><p>82 By increasing the size of labeled target training data, TB can greatly increase its prediction accuracies and even outperform the CL-Dict method. [sent-230, score-0.479]
</p><p>83 Its performance is better than TB when the labeled training data in the target language domain is very limited and is poor than TB when the labeled target data reaches 300 for the six tasks using German and French as target languages. [sent-232, score-1.052]
</p><p>84 Moreover, when adapting a system from English to a much more different target language (Japanese), CL-Dict produces much lower accuracies for all the three tasks comparing with TB. [sent-233, score-0.318]
</p><p>85 These results show that CL-Dict has very limited capacity on transferring labeled information from a related source language domain. [sent-234, score-0.277]
</p><p>86 By using more translation resources, the MT method outperforms TB, CL-Dict, CLD-LSA, CL-SCL in all the nine tasks across almost all scenarios. [sent-238, score-0.263]
</p><p>87 Moreover, it is especially important to notice that CL-RL achieves high test accuracies even when the number of labeled target instances is small. [sent-240, score-0.476]
</p><p>88 This is important for transferring knowledge from a source language to reduce the labeling effort in the target language. [sent-241, score-0.274]
</p><p>89 5 Sensitivity Analysis We also investigated the sensitivity of the proposed approach over the dimensionality of the induced cross-lingual representations. [sent-243, score-0.197]
</p><p>90 e W ke, repeated each experiment for 10 times based on dif-  ferent random selections of labeled target training data and plotted the average prediction accuracies and standard deviations in Figure 2 for all the nine cross-lingual sentiment classification tasks. [sent-249, score-1.08]
</p><p>91 This suggests the proposed approach is not very sensitive to the dimensionality ofthe cross-lingual embedding features within the considered range of values, and with a small dimensionality of 100, the induced representation can already perform very well. [sent-251, score-0.488]
</p><p>92 Given an English word as seed word, we find its five closest neighboring English words and German words according to the Euclidean distances calculated in the induced crosslingual representation space. [sent-254, score-0.317]
</p><p>93 From Table 2, we can see that the retrieved words in both language domains are seman-  tically close to the seed words, which indicates that our proposed method can capture semantic similarities of words not only in a monolingual setting but also in a multilingual setting. [sent-256, score-0.271]
</p><p>94 5  Conclusion  In this paper, we proposed a semi-supervised crosslingual representation learning approach to address cross-lingual text classification. [sent-257, score-0.313]
</p><p>95 The distributed word representation induced by the proposed approach can capture semantic similarities of words across languages while maintaining predictive information with respect to the target classification tasks. [sent-258, score-0.697]
</p><p>96 To evaluate the proposed approach, we conducted experiments on nine cross language sentiment classification tasks constructed from the Amazon product reviews in four languages, comparing to a number of comparison methods. [sent-259, score-0.829]
</p><p>97 Crosslingual sentiment analysis for indian languages using linked wordnets. [sent-273, score-0.216]
</p><p>98 Cross-lingual Table 2: Examples  of source seed words together with five closest English words and five closest German words  estimated using the Euclidean distance in the cross-lingual representation space on the task GB. [sent-279, score-0.234]
</p><p>99 Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classification. [sent-298, score-0.274]
</p><p>100 Cross lingual text classification by mining multilingual topics from wikipedia. [sent-395, score-0.344]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('prettenhofer', 0.247), ('wij', 0.185), ('stein', 0.181), ('labeled', 0.172), ('target', 0.169), ('documents', 0.163), ('nine', 0.157), ('sentiment', 0.156), ('interlingual', 0.153), ('tb', 0.152), ('classification', 0.149), ('representations', 0.149), ('domain', 0.144), ('crosslingual', 0.132), ('representation', 0.129), ('cross', 0.127), ('bilingual', 0.124), ('adaptation', 0.118), ('multilingual', 0.117), ('kc', 0.115), ('german', 0.108), ('source', 0.105), ('petrenz', 0.104), ('domains', 0.102), ('accuracies', 0.092), ('gliozzo', 0.09), ('dimensionality', 0.089), ('document', 0.088), ('xi', 0.087), ('reviews', 0.083), ('deviations', 0.082), ('gb', 0.082), ('distributed', 0.082), ('amini', 0.078), ('jb', 0.078), ('lingual', 0.078), ('vinokourov', 0.078), ('platt', 0.076), ('rc', 0.076), ('matrix', 0.075), ('embedding', 0.073), ('pivot', 0.071), ('ks', 0.069), ('smet', 0.068), ('bel', 0.068), ('latent', 0.066), ('fd', 0.066), ('guo', 0.064), ('gm', 0.062), ('shanahan', 0.062), ('pan', 0.061), ('languages', 0.06), ('japanese', 0.059), ('gd', 0.057), ('plsa', 0.057), ('selections', 0.057), ('tasks', 0.057), ('induced', 0.056), ('books', 0.055), ('kt', 0.054), ('rv', 0.054), ('klementiev', 0.054), ('fb', 0.054), ('mt', 0.053), ('amazon', 0.052), ('proposed', 0.052), ('bwij', 0.052), ('diamantaras', 0.052), ('logpl', 0.052), ('rigutini', 0.052), ('mimno', 0.052), ('row', 0.05), ('ws', 0.05), ('littman', 0.049), ('translation', 0.049), ('translate', 0.048), ('induce', 0.048), ('english', 0.048), ('product', 0.048), ('rs', 0.048), ('webber', 0.047), ('xiao', 0.047), ('prediction', 0.046), ('pl', 0.046), ('unlabeled', 0.046), ('rt', 0.045), ('wjt', 0.045), ('maas', 0.045), ('pakdd', 0.045), ('embeddings', 0.044), ('correspondence', 0.043), ('instances', 0.043), ('yi', 0.043), ('vector', 0.043), ('nips', 0.043), ('vocabulary', 0.042), ('jd', 0.041), ('temple', 0.041), ('lel', 0.041), ('appearing', 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="169-tfidf-1" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>Author: Min Xiao ; Yuhong Guo</p><p>Abstract: Cross-lingual adaptation aims to learn a prediction model in a label-scarce target language by exploiting labeled data from a labelrich source language. An effective crosslingual adaptation system can substantially reduce the manual annotation effort required in many natural language processing tasks. In this paper, we propose a new cross-lingual adaptation approach for document classification based on learning cross-lingual discriminative distributed representations of words. Specifically, we propose to maximize the loglikelihood of the documents from both language domains under a cross-lingual logbilinear document model, while minimizing the prediction log-losses of labeled documents. We conduct extensive experiments on cross-lingual sentiment classification tasks of Amazon product reviews. Our experimental results demonstrate the efficacy of the pro- posed cross-lingual adaptation approach.</p><p>2 0.39339671 <a title="169-tfidf-2" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>Author: Min Xiao ; Feipeng Zhao ; Yuhong Guo</p><p>Abstract: Domain adaptation has been popularly studied on exploiting labeled information from a source domain to learn a prediction model in a target domain. In this paper, we develop a novel representation learning approach to address domain adaptation for text classification with automatically induced discriminative latent features, which are generalizable across domains while informative to the prediction task. Specifically, we propose a hierarchical multinomial Naive Bayes model with latent variables to conduct supervised word clustering on labeled documents from both source and target domains, and then use the produced cluster distribution of each word as its latent feature representation for domain adaptation. We train this latent graphical model us- ing a simple expectation-maximization (EM) algorithm. We empirically evaluate the proposed method with both cross-domain document categorization tasks on Reuters-21578 dataset and cross-domain sentiment classification tasks on Amazon product review dataset. The experimental results demonstrate that our proposed approach achieves superior performance compared with alternative methods.</p><p>3 0.15698604 <a title="169-tfidf-3" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>Author: Lei Cui ; Xilun Chen ; Dongdong Zhang ; Shujie Liu ; Mu Li ; Ming Zhou</p><p>Abstract: Domain adaptation for SMT usually adapts models to an individual specific domain. However, it often lacks some correlation among different domains where common knowledge could be shared to improve the overall translation quality. In this paper, we propose a novel multi-domain adaptation approach for SMT using Multi-Task Learning (MTL), with in-domain models tailored for each specific domain and a general-domain model shared by different domains. The parameters of these models are tuned jointly via MTL so that they can learn general knowledge more accurately and exploit domain knowledge better. Our experiments on a largescale English-to-Chinese translation task validate that the MTL-based adaptation approach significantly and consistently improves the translation quality compared to a non-adapted baseline. Furthermore, it also outperforms the individual adaptation of each specific domain.</p><p>4 0.15463327 <a title="169-tfidf-4" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>Author: Will Y. Zou ; Richard Socher ; Daniel Cer ; Christopher D. Manning</p><p>Abstract: We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence. The new embeddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task.</p><p>5 0.14689292 <a title="169-tfidf-5" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>Author: Margaret Mitchell ; Jacqui Aguilar ; Theresa Wilson ; Benjamin Van Durme</p><p>Abstract: We propose a novel approach to sentiment analysis for a low resource setting. The intuition behind this work is that sentiment expressed towards an entity, targeted sentiment, may be viewed as a span of sentiment expressed across the entity. This representation allows us to model sentiment detection as a sequence tagging problem, jointly discovering people and organizations along with whether there is sentiment directed towards them. We compare performance in both Spanish and English on microblog data, using only a sentiment lexicon as an external resource. By leveraging linguisticallyinformed features within conditional random fields (CRFs) trained to minimize empirical risk, our best models in Spanish significantly outperform a strong baseline, and reach around 90% accuracy on the combined task of named entity recognition and sentiment prediction. Our models in English, trained on a much smaller dataset, are not yet statistically significant against their baselines.</p><p>6 0.14189649 <a title="169-tfidf-6" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>7 0.13236761 <a title="169-tfidf-7" href="./emnlp-2013-Building_Specialized_Bilingual_Lexicons_Using_Large_Scale_Background_Knowledge.html">42 emnlp-2013-Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge</a></p>
<p>8 0.13194849 <a title="169-tfidf-8" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>9 0.12968758 <a title="169-tfidf-9" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<p>10 0.12749229 <a title="169-tfidf-10" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>11 0.11762263 <a title="169-tfidf-11" href="./emnlp-2013-Automatic_Domain_Partitioning_for_Multi-Domain_Learning.html">29 emnlp-2013-Automatic Domain Partitioning for Multi-Domain Learning</a></p>
<p>12 0.11318479 <a title="169-tfidf-12" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>13 0.11135665 <a title="169-tfidf-13" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>14 0.10686928 <a title="169-tfidf-14" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>15 0.10493808 <a title="169-tfidf-15" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>16 0.10042401 <a title="169-tfidf-16" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>17 0.098639324 <a title="169-tfidf-17" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>18 0.094876781 <a title="169-tfidf-18" href="./emnlp-2013-Unsupervised_Induction_of_Cross-Lingual_Semantic_Relations.html">193 emnlp-2013-Unsupervised Induction of Cross-Lingual Semantic Relations</a></p>
<p>19 0.09180674 <a title="169-tfidf-19" href="./emnlp-2013-Word_Level_Language_Identification_in_Online_Multilingual_Communication.html">204 emnlp-2013-Word Level Language Identification in Online Multilingual Communication</a></p>
<p>20 0.084556274 <a title="169-tfidf-20" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.326), (1, -0.055), (2, -0.157), (3, -0.142), (4, 0.17), (5, 0.001), (6, 0.031), (7, 0.01), (8, -0.09), (9, -0.209), (10, 0.106), (11, -0.275), (12, 0.033), (13, -0.144), (14, 0.076), (15, -0.003), (16, -0.191), (17, 0.041), (18, -0.07), (19, -0.045), (20, 0.086), (21, -0.16), (22, 0.123), (23, -0.151), (24, 0.057), (25, 0.045), (26, 0.059), (27, -0.024), (28, -0.019), (29, 0.035), (30, -0.01), (31, 0.084), (32, 0.04), (33, 0.042), (34, 0.008), (35, -0.01), (36, -0.021), (37, 0.079), (38, 0.002), (39, 0.09), (40, -0.024), (41, -0.004), (42, 0.006), (43, -0.005), (44, -0.033), (45, 0.072), (46, 0.01), (47, 0.035), (48, 0.032), (49, -0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9763267 <a title="169-lsi-1" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>Author: Min Xiao ; Yuhong Guo</p><p>Abstract: Cross-lingual adaptation aims to learn a prediction model in a label-scarce target language by exploiting labeled data from a labelrich source language. An effective crosslingual adaptation system can substantially reduce the manual annotation effort required in many natural language processing tasks. In this paper, we propose a new cross-lingual adaptation approach for document classification based on learning cross-lingual discriminative distributed representations of words. Specifically, we propose to maximize the loglikelihood of the documents from both language domains under a cross-lingual logbilinear document model, while minimizing the prediction log-losses of labeled documents. We conduct extensive experiments on cross-lingual sentiment classification tasks of Amazon product reviews. Our experimental results demonstrate the efficacy of the pro- posed cross-lingual adaptation approach.</p><p>2 0.94026339 <a title="169-lsi-2" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>Author: Min Xiao ; Feipeng Zhao ; Yuhong Guo</p><p>Abstract: Domain adaptation has been popularly studied on exploiting labeled information from a source domain to learn a prediction model in a target domain. In this paper, we develop a novel representation learning approach to address domain adaptation for text classification with automatically induced discriminative latent features, which are generalizable across domains while informative to the prediction task. Specifically, we propose a hierarchical multinomial Naive Bayes model with latent variables to conduct supervised word clustering on labeled documents from both source and target domains, and then use the produced cluster distribution of each word as its latent feature representation for domain adaptation. We train this latent graphical model us- ing a simple expectation-maximization (EM) algorithm. We empirically evaluate the proposed method with both cross-domain document categorization tasks on Reuters-21578 dataset and cross-domain sentiment classification tasks on Amazon product review dataset. The experimental results demonstrate that our proposed approach achieves superior performance compared with alternative methods.</p><p>3 0.75043613 <a title="169-lsi-3" href="./emnlp-2013-Automatic_Domain_Partitioning_for_Multi-Domain_Learning.html">29 emnlp-2013-Automatic Domain Partitioning for Multi-Domain Learning</a></p>
<p>Author: Di Wang ; Chenyan Xiong ; William Yang Wang</p><p>Abstract: Chenyan Xiong School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA cx@ c s . cmu .edu William Yang Wang School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA ww@ cmu .edu might not be generalizable to other domains (BenDavid et al., 2006; Ben-David et al., 2010). Multi-Domain learning (MDL) assumes that the domain labels in the dataset are known. However, when there are multiple metadata at- tributes available, it is not always straightforward to select a single best attribute for domain partition, and it is possible that combining more than one metadata attributes (including continuous attributes) can lead to better MDL performance. In this work, we propose an automatic domain partitioning approach that aims at providing better domain identities for MDL. We use a supervised clustering approach that learns the domain distance between data instances , and then cluster the data into better domains for MDL. Our experiment on real multi-domain datasets shows that using our automatically generated domain partition improves over popular MDL methods.</p><p>4 0.68439108 <a title="169-lsi-4" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>Author: Lei Cui ; Xilun Chen ; Dongdong Zhang ; Shujie Liu ; Mu Li ; Ming Zhou</p><p>Abstract: Domain adaptation for SMT usually adapts models to an individual specific domain. However, it often lacks some correlation among different domains where common knowledge could be shared to improve the overall translation quality. In this paper, we propose a novel multi-domain adaptation approach for SMT using Multi-Task Learning (MTL), with in-domain models tailored for each specific domain and a general-domain model shared by different domains. The parameters of these models are tuned jointly via MTL so that they can learn general knowledge more accurately and exploit domain knowledge better. Our experiments on a largescale English-to-Chinese translation task validate that the MTL-based adaptation approach significantly and consistently improves the translation quality compared to a non-adapted baseline. Furthermore, it also outperforms the individual adaptation of each specific domain.</p><p>5 0.57326323 <a title="169-lsi-5" href="./emnlp-2013-Building_Specialized_Bilingual_Lexicons_Using_Large_Scale_Background_Knowledge.html">42 emnlp-2013-Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge</a></p>
<p>Author: Dhouha Bouamor ; Adrian Popescu ; Nasredine Semmar ; Pierre Zweigenbaum</p><p>Abstract: Bilingual lexicons are central components of machine translation and cross-lingual information retrieval systems. Their manual construction requires strong expertise in both languages involved and is a costly process. Several automatic methods were proposed as an alternative but they often rely on resources available in a limited number of languages and their performances are still far behind the quality of manual translations. We introduce a novel approach to the creation of specific domain bilingual lexicon that relies on Wikipedia. This massively multilingual encyclopedia makes it possible to create lexicons for a large number of language pairs. Wikipedia is used to extract domains in each language, to link domains between languages and to create generic translation dictionaries. The approach is tested on four specialized domains and is compared to three state of the art approaches using two language pairs: FrenchEnglish and Romanian-English. The newly introduced method compares favorably to existing methods in all configurations tested.</p><p>6 0.54683477 <a title="169-lsi-6" href="./emnlp-2013-Using_Crowdsourcing_to_get_Representations_based_on_Regular_Expressions.html">196 emnlp-2013-Using Crowdsourcing to get Representations based on Regular Expressions</a></p>
<p>7 0.53705561 <a title="169-lsi-7" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>8 0.50634986 <a title="169-lsi-8" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>9 0.50254053 <a title="169-lsi-9" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>10 0.48090297 <a title="169-lsi-10" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<p>11 0.4574877 <a title="169-lsi-11" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>12 0.45007661 <a title="169-lsi-12" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>13 0.4332808 <a title="169-lsi-13" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>14 0.40086246 <a title="169-lsi-14" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>15 0.39290097 <a title="169-lsi-15" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>16 0.39157712 <a title="169-lsi-16" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>17 0.39023325 <a title="169-lsi-17" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>18 0.38776717 <a title="169-lsi-18" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>19 0.38570559 <a title="169-lsi-19" href="./emnlp-2013-Recurrent_Continuous_Translation_Models.html">156 emnlp-2013-Recurrent Continuous Translation Models</a></p>
<p>20 0.38475493 <a title="169-lsi-20" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.018), (18, 0.477), (22, 0.054), (30, 0.069), (35, 0.01), (50, 0.013), (51, 0.16), (66, 0.032), (71, 0.029), (75, 0.018), (77, 0.035), (90, 0.014), (96, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92666304 <a title="169-lda-1" href="./emnlp-2013-Combining_Generative_and_Discriminative_Model_Scores_for_Distant_Supervision.html">49 emnlp-2013-Combining Generative and Discriminative Model Scores for Distant Supervision</a></p>
<p>Author: Benjamin Roth ; Dietrich Klakow</p><p>Abstract: Distant supervision is a scheme to generate noisy training data for relation extraction by aligning entities of a knowledge base with text. In this work we combine the output of a discriminative at-least-one learner with that of a generative hierarchical topic model to reduce the noise in distant supervision data. The combination significantly increases the ranking quality of extracted facts and achieves state-of-the-art extraction performance in an end-to-end setting. A simple linear interpolation of the model scores performs better than a parameter-free scheme based on nondominated sorting.</p><p>same-paper 2 0.91531634 <a title="169-lda-2" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>Author: Min Xiao ; Yuhong Guo</p><p>Abstract: Cross-lingual adaptation aims to learn a prediction model in a label-scarce target language by exploiting labeled data from a labelrich source language. An effective crosslingual adaptation system can substantially reduce the manual annotation effort required in many natural language processing tasks. In this paper, we propose a new cross-lingual adaptation approach for document classification based on learning cross-lingual discriminative distributed representations of words. Specifically, we propose to maximize the loglikelihood of the documents from both language domains under a cross-lingual logbilinear document model, while minimizing the prediction log-losses of labeled documents. We conduct extensive experiments on cross-lingual sentiment classification tasks of Amazon product reviews. Our experimental results demonstrate the efficacy of the pro- posed cross-lingual adaptation approach.</p><p>3 0.90347356 <a title="169-lda-3" href="./emnlp-2013-Efficient_Left-to-Right_Hierarchical_Phrase-Based_Translation_with_Improved_Reordering.html">71 emnlp-2013-Efficient Left-to-Right Hierarchical Phrase-Based Translation with Improved Reordering</a></p>
<p>Author: Maryam Siahbani ; Baskaran Sankaran ; Anoop Sarkar</p><p>Abstract: Left-to-right (LR) decoding (Watanabe et al., 2006b) is a promising decoding algorithm for hierarchical phrase-based translation (Hiero). It generates the target sentence by extending the hypotheses only on the right edge. LR decoding has complexity O(n2b) for input of n words and beam size b, compared to O(n3) for the CKY algorithm. It requires a single language model (LM) history for each target hypothesis rather than two LM histories per hypothesis as in CKY. In this paper we present an augmented LR decoding algorithm that builds on the original algorithm in (Watanabe et al., 2006b). Unlike that algorithm, using experiments over multiple language pairs we show two new results: our LR decoding algorithm provides demonstrably more efficient decoding than CKY Hiero, four times faster; and by introducing new distortion and reordering features for LR decoding, it maintains the same translation quality (as in BLEU scores) ob- tained phrase-based and CKY Hiero with the same translation model.</p><p>4 0.74049199 <a title="169-lda-4" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>Author: Min Xiao ; Feipeng Zhao ; Yuhong Guo</p><p>Abstract: Domain adaptation has been popularly studied on exploiting labeled information from a source domain to learn a prediction model in a target domain. In this paper, we develop a novel representation learning approach to address domain adaptation for text classification with automatically induced discriminative latent features, which are generalizable across domains while informative to the prediction task. Specifically, we propose a hierarchical multinomial Naive Bayes model with latent variables to conduct supervised word clustering on labeled documents from both source and target domains, and then use the produced cluster distribution of each word as its latent feature representation for domain adaptation. We train this latent graphical model us- ing a simple expectation-maximization (EM) algorithm. We empirically evaluate the proposed method with both cross-domain document categorization tasks on Reuters-21578 dataset and cross-domain sentiment classification tasks on Amazon product review dataset. The experimental results demonstrate that our proposed approach achieves superior performance compared with alternative methods.</p><p>5 0.66692966 <a title="169-lda-5" href="./emnlp-2013-Scaling_Semantic_Parsers_with_On-the-Fly_Ontology_Matching.html">164 emnlp-2013-Scaling Semantic Parsers with On-the-Fly Ontology Matching</a></p>
<p>Author: Tom Kwiatkowski ; Eunsol Choi ; Yoav Artzi ; Luke Zettlemoyer</p><p>Abstract: We consider the challenge of learning semantic parsers that scale to large, open-domain problems, such as question answering with Freebase. In such settings, the sentences cover a wide variety of topics and include many phrases whose meaning is difficult to represent in a fixed target ontology. For example, even simple phrases such as ‘daughter’ and ‘number of people living in’ cannot be directly represented in Freebase, whose ontology instead encodes facts about gender, parenthood, and population. In this paper, we introduce a new semantic parsing approach that learns to resolve such ontological mismatches. The parser is learned from question-answer pairs, uses a probabilistic CCG to build linguistically motivated logicalform meaning representations, and includes an ontology matching model that adapts the output logical forms for each target ontology. Experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus.</p><p>6 0.57175672 <a title="169-lda-6" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>7 0.56188142 <a title="169-lda-7" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>8 0.55863351 <a title="169-lda-8" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>9 0.53509271 <a title="169-lda-9" href="./emnlp-2013-Noise-Aware_Character_Alignment_for_Bootstrapping_Statistical_Machine_Transliteration_from_Bilingual_Corpora.html">139 emnlp-2013-Noise-Aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora</a></p>
<p>10 0.53445977 <a title="169-lda-10" href="./emnlp-2013-Two-Stage_Method_for_Large-Scale_Acquisition_of_Contradiction_Pattern_Pairs_using_Entailment.html">189 emnlp-2013-Two-Stage Method for Large-Scale Acquisition of Contradiction Pattern Pairs using Entailment</a></p>
<p>11 0.53411978 <a title="169-lda-11" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>12 0.53337574 <a title="169-lda-12" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>13 0.53054631 <a title="169-lda-13" href="./emnlp-2013-Flexible_and_Efficient_Hypergraph_Interactions_for_Joint_Hierarchical_and_Forest-to-String_Decoding.html">88 emnlp-2013-Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest-to-String Decoding</a></p>
<p>14 0.52906042 <a title="169-lda-14" href="./emnlp-2013-Word_Level_Language_Identification_in_Online_Multilingual_Communication.html">204 emnlp-2013-Word Level Language Identification in Online Multilingual Communication</a></p>
<p>15 0.52035862 <a title="169-lda-15" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>16 0.51804662 <a title="169-lda-16" href="./emnlp-2013-A_Synchronous_Context_Free_Grammar_for_Time_Normalization.html">14 emnlp-2013-A Synchronous Context Free Grammar for Time Normalization</a></p>
<p>17 0.51697564 <a title="169-lda-17" href="./emnlp-2013-Converting_Continuous-Space_Language_Models_into_N-Gram_Language_Models_for_Statistical_Machine_Translation.html">52 emnlp-2013-Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation</a></p>
<p>18 0.51626015 <a title="169-lda-18" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>19 0.51565033 <a title="169-lda-19" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>20 0.51435685 <a title="169-lda-20" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
