<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-140" href="#">emnlp2013-140</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</h1>
<br/><p>Source: <a title="emnlp-2013-140-pdf" href="http://aclweb.org/anthology//D/D13/D13-1202.pdf">pdf</a></p><p>Author: Andrew J. Anderson ; Elia Bruni ; Ulisse Bordignon ; Massimo Poesio ; Marco Baroni</p><p>Abstract: Traditional distributional semantic models extract word meaning representations from cooccurrence patterns of words in text corpora. Recently, the distributional approach has been extended to models that record the cooccurrence of words with visual features in image collections. These image-based models should be complementary to text-based ones, providing a more cognitively plausible view of meaning grounded in visual perception. In this study, we test whether image-based models capture the semantic patterns that emerge from fMRI recordings of the neural signal. Our results indicate that, indeed, there is a significant correlation between image-based and brain-based semantic similarities, and that image-based models complement text-based ones, so that the best correlations are achieved when the two modalities are combined. Despite some unsatisfactory, but explained out- comes (in particular, failure to detect differential association of models with brain areas), the results show, on the one hand, that imagebased distributional semantic models can be a precious new tool to explore semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility.</p><p>Reference: <a title="emnlp-2013-140-reference" href="../emnlp2013_reference/emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Of words, eyes and brains: Correlating image-based distributional semantic models with neural representations of concepts Andrew J. [sent-1, score-0.305]
</p><p>2 it ast  Abstract Traditional distributional semantic models extract word meaning representations from cooccurrence patterns of words in text corpora. [sent-5, score-0.255]
</p><p>3 Recently, the distributional approach has been extended to models that record the cooccurrence of words with visual features in image collections. [sent-6, score-0.544]
</p><p>4 These image-based models should be complementary to text-based ones, providing a more cognitively plausible view of meaning grounded in visual perception. [sent-7, score-0.347]
</p><p>5 Our results indicate that, indeed, there is a significant correlation between image-based and brain-based semantic similarities, and that image-based models complement text-based ones, so that the best correlations are achieved when the two modalities are combined. [sent-9, score-0.293]
</p><p>6 , using corpus-based representations to reconstruct the likely neural signal associated to words we have no direct brain data for. [sent-19, score-0.381]
</p><p>7 (2012), who showed that purely corpus-based distributional models are at least as good at brain signal prediction tasks as earlier models that made use of manually-generated or controlled knowledge sources (Chang et al. [sent-25, score-0.47]
</p><p>8 , 2011), and we evaluate a very recent type of distributional model, namely one that is not extracted from textual data but from image collections through automated visual feature extraction techniques. [sent-28, score-0.514]
</p><p>9 Finally, we will check for differences between anatomical regions in the degree to which text and/or image models are effective, as one might expect given the well-known functional specializations of different anatomical regions. [sent-34, score-0.613]
</p><p>10 The data were normalized to the MNI template brain image, spatially normalized into MNI space and resampled to 3 3 6 mm3 vox-  ienlts. [sent-58, score-0.308]
</p><p>11 Anatomical parcellation Analysis was conducted on the whole brain, and to address the question of whether there are differences in models’ effectiveness between anatomical regions, brains were further partitioned into frontal, parietal, temporal and occipital lobes. [sent-63, score-0.59]
</p><p>12 The occipital lobe houses the primary visual processing system and consequently it is reasonable to expect some bias toward image-based semantic models. [sent-67, score-1.004]
</p><p>13 Furthermore, given that experimental stimuli incorporated line drawings of the object,and the visual cortex has a well-established role in processing low-level visual statistics including edge detection (Bruce et al. [sent-68, score-0.594]
</p><p>14 Here the fusiform gyrus (shared with the occipital  lobe) plays a general role in object categorisation (e. [sent-72, score-0.465]
</p><p>15 Of wider relevance to semantic processing, the medial temporal gyrus, inferior temporal gyrus and ventral temporal lobe have generally been implicated to have roles in supramodal integration 1962 and concept retrieval (Binder et al. [sent-78, score-0.917]
</p><p>16 The visual action stream leads from the occipital lobe to the parietal lobe to support spatial cognition tasks and action control (Sack, 2009). [sent-81, score-1.74]
</p><p>17 As the parietal lobe also contains the angular gyrus, thought to be involved in complex, supra-modal information integration and knowledge retrieval (Binder et al. [sent-83, score-0.656]
</p><p>18 The frontal lobe, is traditionally associated with high-level processing and manipulation of abstract knowledge and rules and controlled behaviour (Miller et al. [sent-85, score-0.252]
</p><p>19 , uncued speech production), the ventromedial prefrontal cortex in motivation and emotional processing, the inferior frontal gyrus in phonological and syntactic processing, (Binder et al. [sent-89, score-0.429]
</p><p>20 The four lobes were identified and partitioned using Tzourio-Mazoyer et al. [sent-92, score-0.366]
</p><p>21 Voxel selection The set of 500 most stable voxels, both within the whole brain and from within each region of interest were identified for analysis. [sent-94, score-0.351]
</p><p>22 (2008), for each voxel, the set of 5 1 words from each unique pair of scanning sessions were correlated using Pearson’s correlation (6 sessions and therefore 15 unique pairs), and the mean of the 15 resulting correlation coefficients was taken as the measure of stability. [sent-97, score-0.233]
</p><p>23 3  Distributional models  Distributional semantic models approximate word meaning by keeping track of word co-occurrence statistics from large textual input, relying on the distributional hypothesis: The meaning of a word can be induced by the context in which it occurs (Turney and Pantel, 2010). [sent-99, score-0.279]
</p><p>24 Thanks to recent developments in computer vision, it is nowadays possible to take the visual perceptual channel into account, and build new computational models of semantics enhanced with visual information (Feng and Lapata, 2010; Bruni et al. [sent-101, score-0.617]
</p><p>25 Given a set of target concepts and a collection of images depicting those concepts, it is indeed possible to first encode the image content into low-level features, and subsequently convert it into a higher-level representation based on the bag-of-visual-words method (Grauman and Leibe, 2011). [sent-104, score-0.306]
</p><p>26 (2012) a window-based model very similar to ours was not significantly worse than their best model for brain decoding. [sent-122, score-0.308]
</p><p>27 Given that our focus here is on visual information, we only report results for Window2 and its combination with visual models. [sent-126, score-0.518]
</p><p>28 2 Visual models Our visual models are inspired by Bruni et al. [sent-128, score-0.319]
</p><p>29 They found that extracting visual features separately from the object and its surrounding context leads to better performance than not using localization, and using only object- and, more surprisingly, contextextracted features also results in performant models 2http : / /wacky . [sent-130, score-0.401]
</p><p>30 it/ (especially when evaluating inter-object similarity,  the context in which an object is located can significantly contribute to semantic representation, in certain cases carrying even more information than the depicted object itself). [sent-133, score-0.275]
</p><p>31 More in detail, with localization the visual features (visual words) can be extracted from the object bounding box (in our experiments, the Object model) or from only outside the object box (the Context model). [sent-134, score-0.513]
</p><p>32 Visual model construction pipeline To extract visual co-occurrence statistics, we use images from ImageNet (Deng et al. [sent-136, score-0.337]
</p><p>33 To build visual distributional models, we utilize  the bag-of-visual-words (BoVW) representation of images (Sivic and Zisserman, 2003; Csurka et al. [sent-142, score-0.465]
</p><p>34 Inspired by NLP, BoVW discretizes the image content in terms of a histogram of visual word counts. [sent-144, score-0.412]
</p><p>35 Differently from NLP, in vision there is not a natural notion of visual words, hence a visual vocabulary has to be built from scratch. [sent-145, score-0.543]
</p><p>36 Given then a new image, each of the low-level feature vectors extracted from the patches that compose it is mapped to the nearest visual word (e. [sent-149, score-0.259]
</p><p>37 , in terms of Euclidean distance from the cluster centroid) such that the image can be represented with a histogram counting the instances of each visual word in the image. [sent-151, score-0.412]
</p><p>38 To construct the visual vocabulary, we cluster the SIFT features into 25K different clusters. [sent-156, score-0.259]
</p><p>39 We use in total 8 different regions, obtaining a final vector of 200K dimensions (25K visual words 8 regions). [sent-159, score-0.259]
</p><p>40 Since edaimche concept 5inK our duaalta wseotr diss represented by minucletiple images, we pool the visual word occurrences across images by summing them up into a single vector. [sent-160, score-0.364]
</p><p>41 To perform the entire visual pipeline we use VSEM, an open library for visual semantics (Bruni et al. [sent-161, score-0.545]
</p><p>42 3 Model transformations and combination Once both the textual and the visual models are built, we perform two different transformations on the raw co-occurrence counts. [sent-164, score-0.343]
</p><p>43 it /vsem/ c 4 Experiments  A question is posed over how to evaluate the relationship between the different distributional models and brain data. [sent-180, score-0.44]
</p><p>44 (2012)) is one possibility: they used multiple regression to relate distributional codes to individual voxel activations, thus allowing brain states to be estimated from previously unseen distributional codes. [sent-183, score-0.55]
</p><p>45 Regression models were trained on 58/60 words and in testing the regression models estimated the brain state associated with the 2 unseen distributional codes. [sent-184, score-0.47]
</p><p>46 The predicted brain states were compared with the actual fMRI data, and the process repeated for each permutation of left-out words, to build a metric of prediction accuracy. [sent-185, score-0.308]
</p><p>47 For our purposes, a fair comparison of models using this strategy is complicated by differences in dimensionality between both semantic models and lobes (which we compare to other lobes) in association with the comparatively small number of words in the fMRI data set. [sent-186, score-0.539]
</p><p>48 Specifically, for each model/participant’s fMRI data/anatomical region, the similarity structure was evaluated by taking the pairwise correlation (Pearson’s correlation coefficient) between all unique category or word combinations. [sent-194, score-0.226]
</p><p>49 This produced a list of 55 category pair correlations and 121 word pair correlations for each data source. [sent-195, score-0.226]
</p><p>50 For all brain data, correlation lists were averaged across the nine participants to produce a single list of mean word pair correlations and a single list of mean category pair correlations for each anatomical region and the whole brain. [sent-196, score-0.889]
</p><p>51 Then to provide a measure of 1965 similarity between models and brain data, the cor-  relation lists for respective data sources were themselves correlated using Spearman’s rank correlation. [sent-197, score-0.366]
</p><p>52 1 Category-level analyses Do image models correlate with brain data? [sent-201, score-0.491]
</p><p>53 Table 2 displays results of Spearman’s correlations between the per-category similarity structure of distributional models and brain data. [sent-202, score-0.566]
</p><p>54 There is a significant correlation between every purely imagebased model and the occipital, parietal and temporal lobes, and also the whole brain (. [sent-203, score-0.718]
</p><p>55 Otherwise Window2 significantly correlates with the whole brain and all anatomical regions except for the frontal lobe where ρ=. [sent-216, score-1.243]
</p><p>56 In contrast Verb (the original, partially hand-crafted model used by Mitchell and colleagues) captures inter-relationships poorly and neither correlates with the whole brain or any lobe. [sent-219, score-0.308]
</p><p>57 2-way ANOVA without replication was used to test for differences in correlation coefficients between the five pure-modality models (Verb, Window2, Object, Context and Object&Context;), and the four brain lobes. [sent-221, score-0.492]
</p><p>58 There was a clear difference even when Verb (mean±sd over lobes = . [sent-226, score-0.366]
</p><p>59 However there was a highly significant difference between lobes F(3,12)=13. [sent-238, score-0.366]
</p><p>60 Post-hoc 2tailed t-tests comparing lobe pairs found that the frontal lobe yielded significantly different correlations (lower) than each other lobe. [sent-241, score-1.258]
</p><p>61 Category-level inter-correlations between lobes were all relatively strong and highly significant. [sent-251, score-0.366]
</p><p>62 The occipital lobe was found to be the most distinct, being similar to the temporal lobe (ρ=. [sent-252, score-1.247]
</p><p>63 001), but less so to the parietal and frontal lobes (ρ=. [sent-254, score-0.82]
</p><p>64 The 1966 temporal lobe shows roughly similar levels of correlation to each other lobe (all . [sent-259, score-1.091]
</p><p>65 001), to a slightly lesser extent to the temporal lobe (in both cases ρ=. [sent-265, score-0.553]
</p><p>66 To appraise this assertion in the context of the previously detected difference between the frontal lobe and all other lobes, we examine the raw category pair similarity matrices derived from the occipital lobe and the frontal lobe (Figure 1). [sent-269, score-2.164]
</p><p>67 All of these relationships are maintained in the occipital lobe, and many are visible in the frontal lobe (including the similarity between insects and clothes), however there are exceptions that are difficult to explain e. [sent-277, score-1.004]
</p><p>68 , within the frontal lobe, building parts are not similar to furniture, kitchen utensils are closer to clothing than to tools and vehicles are more similar to clothing than anything else. [sent-279, score-0.407]
</p><p>69 As such we conclude that category-level representations were similar across lobes with differ-  ences likely due to variation in signal quality between lobes. [sent-280, score-0.405]
</p><p>70 The Window2 model showed a stronger correlation than the Window2&Object; model for the frontal and parietal lobes, but was weaker than Window2&Object;&Context; and Window2&Context; in all tests and was also weaker than any joint model in whole-brain comparisons. [sent-283, score-0.627]
</p><p>71 The mean±sd correlatiino nwsh foolre -ablrla purely image-based e re msuelatns pooled over lobes (3 models * 4 lobes) was . [sent-284, score-0.396]
</p><p>72 O rebl-ject&Context; on the four different lobes is preserved between image-based and joint models: correlating the 12 combinations using Spearman’s correlation gives ρ=. [sent-294, score-0.45]
</p><p>73 Differences can be statistically quantified by pooling all image related correlation coefficients for each anatomical region (3  models * 4 regions), as for the respective joint models, and comparing with a 2-tailed Wilcoxon signed rank test. [sent-297, score-0.507]
</p><p>74 This evidence accumulates to sug1967  Figure 1: Similarity (Pearson correlation) between each category pair in (top) occipital and (bottom) frontal lobes. [sent-300, score-0.522]
</p><p>75 Per-word results generally corroborate the relationships observed in the previous section in the sense that Spearman’s correlation between perword and per-category results for the 40 combinations of models and lobes was ρ=. [sent-304, score-0.48]
</p><p>76 Subsets of per-word image-based models correlated with three lobes and the whole brain. [sent-308, score-0.396]
</p><p>77 05 were observed in the temporal and parietal lobes, for Context, Object&Context; and Window2 whereas Object was correlated with the occipital and temporal lobes (p <. [sent-310, score-1.006]
</p><p>78 There was again a significant difference between lobes (F(3,12)=7. [sent-322, score-0.366]
</p><p>79 01), with the frontal lobe showing the weak-  est correlations. [sent-324, score-0.706]
</p><p>80 Post-hoc 2-tailed t-tests comparing lobe-pairs found that the frontal lobe differed significantly (correlations were weaker) from the parietal (t =-9, p <. [sent-325, score-0.941]
</p><p>81 Word-level inter-correlations between lobes were all significant and the pattern of differences in correlation strength largely resembled that of the categorylevel analyses. [sent-333, score-0.487]
</p><p>82 The occipital lobe was again most similar to the temporal lobe (ρ=. [sent-334, score-1.247]
</p><p>83 001), but less so to the parietal and frontal lobes (ρ=. [sent-336, score-0.82]
</p><p>84 The temporal lobe this time showed stronger correlation to the parietal (ρ=. [sent-341, score-0.87]
</p><p>85 The frontal and parietal lobes were again strongly related to one another (ρ=. [sent-346, score-0.82]
</p><p>86 These results echo the category-level findings, that word-level brain activity is also organised in a similar way across lobes. [sent-349, score-0.308]
</p><p>87 Consequently this diminishes our chances of uncovering neat interactions between models and brain ar-  eas (where for instance the Window2 model correlates with the frontal lobe and Object model matches the occipital lobe). [sent-350, score-1.284]
</p><p>88 In particular the Context model better matches the parietal lobe than the 1968 Object model, which in turn better captures the occipital and temporal lobes (Observations are qualitative). [sent-352, score-1.361]
</p><p>89 Also as we see next, adding text information boosts performance in both parietal and temporal lobes (see Section 2 on our expectations about information encoded in the lobes). [sent-353, score-0.667]
</p><p>90 There was one exception: adding text to the Object model weakened correlation with the occipital lobe. [sent-356, score-0.324]
</p><p>91 Joint models were exclusively stronger than Window2 for the temporal and occipital lobes, and were stronger in 1/3 of cases for the frontal and parietal lobes. [sent-357, score-0.885]
</p><p>92 In an anal-  ogous comparison to the per-category analysis, a Wilcoxon signed rank test was used to examine the difference made by adding text information to image models (pooling 3 models over 4 anatomical areas for both image and joint models). [sent-358, score-0.554]
</p><p>93 6  Conclusion  This study brought together, for the first time, two recent research lines: The exploration of “semantic spaces” in the brain using distributional semantic models extracted from corpora, and the extension of the latter to image-based features. [sent-369, score-0.491]
</p><p>94 The lack of finding organisational differences between anatomical regions differentially described by the various models is perhaps disappointing, but not uncontroversial, given that the dataset was not originally designed to tease apart visual information from linguistic context. [sent-376, score-0.555]
</p><p>95 Collecting our own fMRI data will also allow us to move beyond exploratory analysis, to test sharper predictions about distributional models and their brain area correlates. [sent-379, score-0.44]
</p><p>96 There are also many opportunities for focusing analyses on different subsets of brain regions, with the semantic system identified by Binder et al. [sent-380, score-0.359]
</p><p>97 The visual word form area: expertise for reading in the fusiform gyrus. [sent-513, score-0.309]
</p><p>98 Two visual streams for perception and action: Current trends. [sent-517, score-0.291]
</p><p>99 Predicting human brain activity associated with the meanings of nouns. [sent-526, score-0.308]
</p><p>100 Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain. [sent-573, score-0.353]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lobe', 0.454), ('lobes', 0.366), ('brain', 0.308), ('visual', 0.259), ('frontal', 0.252), ('occipital', 0.24), ('parietal', 0.202), ('bruni', 0.165), ('anatomical', 0.164), ('image', 0.153), ('fmri', 0.121), ('object', 0.112), ('distributional', 0.102), ('temporal', 0.099), ('correlations', 0.098), ('correlation', 0.084), ('images', 0.078), ('cortex', 0.076), ('regions', 0.065), ('binder', 0.063), ('gyrus', 0.063), ('elia', 0.055), ('mitchell', 0.053), ('semantic', 0.051), ('clothing', 0.05), ('fusiform', 0.05), ('concepts', 0.049), ('objects', 0.045), ('spearman', 0.044), ('imagenet', 0.044), ('region', 0.043), ('df', 0.043), ('perceptual', 0.042), ('spatial', 0.042), ('sd', 0.04), ('furniture', 0.04), ('representations', 0.039), ('haxby', 0.038), ('mni', 0.038), ('prefrontal', 0.038), ('voxel', 0.038), ('voxels', 0.038), ('differences', 0.037), ('neural', 0.034), ('meaning', 0.033), ('bovw', 0.033), ('differed', 0.033), ('neuroscience', 0.033), ('sift', 0.033), ('coefficients', 0.033), ('murphy', 0.032), ('mean', 0.032), ('perception', 0.032), ('stronger', 0.031), ('stream', 0.031), ('category', 0.03), ('modalities', 0.03), ('representational', 0.03), ('localization', 0.03), ('insects', 0.03), ('kitchen', 0.03), ('models', 0.03), ('action', 0.029), ('weaker', 0.029), ('leong', 0.028), ('stimulus', 0.028), ('similarity', 0.028), ('concept', 0.027), ('transformations', 0.027), ('semantics', 0.027), ('animals', 0.026), ('representation', 0.026), ('cognitive', 0.026), ('vision', 0.025), ('bordignon', 0.025), ('brains', 0.025), ('closet', 0.025), ('csurka', 0.025), ('egocentric', 0.025), ('goodale', 0.025), ('huth', 0.025), ('imagebased', 0.025), ('kanwisher', 0.025), ('kriegeskorte', 0.025), ('lazebnik', 0.025), ('marcel', 0.025), ('mccandliss', 0.025), ('palatucci', 0.025), ('parcellation', 0.025), ('peelen', 0.025), ('selectivity', 0.025), ('sivic', 0.025), ('ulisse', 0.025), ('utensils', 0.025), ('ventral', 0.025), ('vsem', 0.025), ('complementary', 0.025), ('dimensionality', 0.025), ('sciences', 0.024), ('areas', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="140-tfidf-1" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>Author: Andrew J. Anderson ; Elia Bruni ; Ulisse Bordignon ; Massimo Poesio ; Marco Baroni</p><p>Abstract: Traditional distributional semantic models extract word meaning representations from cooccurrence patterns of words in text corpora. Recently, the distributional approach has been extended to models that record the cooccurrence of words with visual features in image collections. These image-based models should be complementary to text-based ones, providing a more cognitively plausible view of meaning grounded in visual perception. In this study, we test whether image-based models capture the semantic patterns that emerge from fMRI recordings of the neural signal. Our results indicate that, indeed, there is a significant correlation between image-based and brain-based semantic similarities, and that image-based models complement text-based ones, so that the best correlations are achieved when the two modalities are combined. Despite some unsatisfactory, but explained out- comes (in particular, failure to detect differential association of models with brain areas), the results show, on the one hand, that imagebased distributional semantic models can be a precious new tool to explore semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility.</p><p>2 0.22531928 <a title="140-tfidf-2" href="./emnlp-2013-Image_Description_using_Visual_Dependency_Representations.html">98 emnlp-2013-Image Description using Visual Dependency Representations</a></p>
<p>Author: Desmond Elliott ; Frank Keller</p><p>Abstract: Describing the main event of an image involves identifying the objects depicted and predicting the relationships between them. Previous approaches have represented images as unstructured bags of regions, which makes it difficult to accurately predict meaningful relationships between regions. In this paper, we introduce visual dependency representations to capture the relationships between the objects in an image, and hypothesize that this representation can improve image description. We test this hypothesis using a new data set of region-annotated images, associated with visual dependency representations and gold-standard descriptions. We describe two template-based description generation models that operate over visual dependency representations. In an image descrip- tion task, we find that these models outperform approaches that rely on object proximity or corpus information to generate descriptions on both automatic measures and on human judgements.</p><p>3 0.2039987 <a title="140-tfidf-3" href="./emnlp-2013-Exploiting_Language_Models_for_Visual_Recognition.html">78 emnlp-2013-Exploiting Language Models for Visual Recognition</a></p>
<p>Author: Dieu-Thu Le ; Jasper Uijlings ; Raffaella Bernardi</p><p>Abstract: The problem of learning language models from large text corpora has been widely studied within the computational linguistic community. However, little is known about the performance of these language models when applied to the computer vision domain. In this work, we compare representative models: a window-based model, a topic model, a distributional memory and a commonsense knowledge database, ConceptNet, in two visual recognition scenarios: human action recognition and object prediction. We examine whether the knowledge extracted from texts through these models are compatible to the knowledge represented in images. We determine the usefulness of different language models in aiding the two visual recognition tasks. The study shows that the language models built from general text corpora can be used instead of expensive annotated images and even outperform the image model when testing on a big general dataset.</p><p>4 0.18554831 <a title="140-tfidf-4" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>Author: Stephen Roller ; Sabine Schulte im Walde</p><p>Abstract: Recent investigations into grounded models of language have shown that holistic views of language and perception can provide higher performance than independent views. In this work, we improve a two-dimensional multimodal version of Latent Dirichlet Allocation (Andrews et al., 2009) in various ways. (1) We outperform text-only models in two different evaluations, and demonstrate that low-level visual features are directly compatible with the existing model. (2) We present a novel way to integrate visual features into the LDA model using unsupervised clusters of images. The clusters are directly interpretable and improve on our evaluation tasks. (3) We provide two novel ways to extend the bimodal mod- els to support three or more modalities. We find that the three-, four-, and five-dimensional models significantly outperform models using only one or two modalities, and that nontextual modalities each provide separate, disjoint knowledge that cannot be forced into a shared, latent structure.</p><p>5 0.10333935 <a title="140-tfidf-5" href="./emnlp-2013-Towards_Situated_Dialogue%3A_Revisiting_Referring_Expression_Generation.html">185 emnlp-2013-Towards Situated Dialogue: Revisiting Referring Expression Generation</a></p>
<p>Author: Rui Fang ; Changsong Liu ; Lanbo She ; Joyce Y. Chai</p><p>Abstract: In situated dialogue, humans and agents have mismatched capabilities of perceiving the shared environment. Their representations of the shared world are misaligned. Thus referring expression generation (REG) will need to take this discrepancy into consideration. To address this issue, we developed a hypergraph-based approach to account for group-based spatial relations and uncertainties in perceiving the environment. Our empirical results have shown that this approach outperforms a previous graph-based approach with an absolute gain of 9%. However, while these graph-based approaches perform effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), they perform rather poorly when the agent has imperfect perception of the environment (e.g., 45%). This big performance gap calls for new solutions to REG that can mediate a shared perceptual basis in situated dialogue.</p><p>6 0.089474067 <a title="140-tfidf-6" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>7 0.062275212 <a title="140-tfidf-7" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>8 0.05834214 <a title="140-tfidf-8" href="./emnlp-2013-Learning_Distributions_over_Logical_Forms_for_Referring_Expression_Generation.html">119 emnlp-2013-Learning Distributions over Logical Forms for Referring Expression Generation</a></p>
<p>9 0.056188595 <a title="140-tfidf-9" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>10 0.055171035 <a title="140-tfidf-10" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>11 0.05505684 <a title="140-tfidf-11" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>12 0.049332183 <a title="140-tfidf-12" href="./emnlp-2013-Centering_Similarity_Measures_to_Reduce_Hubs.html">44 emnlp-2013-Centering Similarity Measures to Reduce Hubs</a></p>
<p>13 0.048562765 <a title="140-tfidf-13" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>14 0.047897711 <a title="140-tfidf-14" href="./emnlp-2013-Predicting_the_Resolution_of_Referring_Expressions_from_User_Behavior.html">153 emnlp-2013-Predicting the Resolution of Referring Expressions from User Behavior</a></p>
<p>15 0.046850301 <a title="140-tfidf-15" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>16 0.045620583 <a title="140-tfidf-16" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>17 0.044121943 <a title="140-tfidf-17" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>18 0.043797083 <a title="140-tfidf-18" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>19 0.042212866 <a title="140-tfidf-19" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>20 0.041643396 <a title="140-tfidf-20" href="./emnlp-2013-Building_Event_Threads_out_of_Multiple_News_Articles.html">41 emnlp-2013-Building Event Threads out of Multiple News Articles</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.139), (1, 0.06), (2, -0.069), (3, 0.072), (4, -0.038), (5, 0.191), (6, -0.042), (7, -0.098), (8, -0.169), (9, -0.077), (10, -0.306), (11, 0.005), (12, 0.065), (13, -0.07), (14, -0.191), (15, -0.048), (16, -0.065), (17, 0.074), (18, 0.117), (19, 0.046), (20, -0.06), (21, 0.011), (22, -0.05), (23, -0.082), (24, 0.023), (25, 0.065), (26, 0.01), (27, 0.037), (28, 0.015), (29, -0.04), (30, -0.037), (31, 0.017), (32, -0.015), (33, 0.076), (34, -0.07), (35, 0.029), (36, 0.096), (37, 0.027), (38, 0.017), (39, 0.021), (40, 0.001), (41, -0.059), (42, -0.101), (43, -0.006), (44, -0.023), (45, 0.087), (46, -0.024), (47, -0.008), (48, -0.018), (49, 0.07)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93688685 <a title="140-lsi-1" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>Author: Andrew J. Anderson ; Elia Bruni ; Ulisse Bordignon ; Massimo Poesio ; Marco Baroni</p><p>Abstract: Traditional distributional semantic models extract word meaning representations from cooccurrence patterns of words in text corpora. Recently, the distributional approach has been extended to models that record the cooccurrence of words with visual features in image collections. These image-based models should be complementary to text-based ones, providing a more cognitively plausible view of meaning grounded in visual perception. In this study, we test whether image-based models capture the semantic patterns that emerge from fMRI recordings of the neural signal. Our results indicate that, indeed, there is a significant correlation between image-based and brain-based semantic similarities, and that image-based models complement text-based ones, so that the best correlations are achieved when the two modalities are combined. Despite some unsatisfactory, but explained out- comes (in particular, failure to detect differential association of models with brain areas), the results show, on the one hand, that imagebased distributional semantic models can be a precious new tool to explore semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility.</p><p>2 0.88856566 <a title="140-lsi-2" href="./emnlp-2013-Image_Description_using_Visual_Dependency_Representations.html">98 emnlp-2013-Image Description using Visual Dependency Representations</a></p>
<p>Author: Desmond Elliott ; Frank Keller</p><p>Abstract: Describing the main event of an image involves identifying the objects depicted and predicting the relationships between them. Previous approaches have represented images as unstructured bags of regions, which makes it difficult to accurately predict meaningful relationships between regions. In this paper, we introduce visual dependency representations to capture the relationships between the objects in an image, and hypothesize that this representation can improve image description. We test this hypothesis using a new data set of region-annotated images, associated with visual dependency representations and gold-standard descriptions. We describe two template-based description generation models that operate over visual dependency representations. In an image descrip- tion task, we find that these models outperform approaches that rely on object proximity or corpus information to generate descriptions on both automatic measures and on human judgements.</p><p>3 0.86210591 <a title="140-lsi-3" href="./emnlp-2013-Exploiting_Language_Models_for_Visual_Recognition.html">78 emnlp-2013-Exploiting Language Models for Visual Recognition</a></p>
<p>Author: Dieu-Thu Le ; Jasper Uijlings ; Raffaella Bernardi</p><p>Abstract: The problem of learning language models from large text corpora has been widely studied within the computational linguistic community. However, little is known about the performance of these language models when applied to the computer vision domain. In this work, we compare representative models: a window-based model, a topic model, a distributional memory and a commonsense knowledge database, ConceptNet, in two visual recognition scenarios: human action recognition and object prediction. We examine whether the knowledge extracted from texts through these models are compatible to the knowledge represented in images. We determine the usefulness of different language models in aiding the two visual recognition tasks. The study shows that the language models built from general text corpora can be used instead of expensive annotated images and even outperform the image model when testing on a big general dataset.</p><p>4 0.73981106 <a title="140-lsi-4" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>Author: Stephen Roller ; Sabine Schulte im Walde</p><p>Abstract: Recent investigations into grounded models of language have shown that holistic views of language and perception can provide higher performance than independent views. In this work, we improve a two-dimensional multimodal version of Latent Dirichlet Allocation (Andrews et al., 2009) in various ways. (1) We outperform text-only models in two different evaluations, and demonstrate that low-level visual features are directly compatible with the existing model. (2) We present a novel way to integrate visual features into the LDA model using unsupervised clusters of images. The clusters are directly interpretable and improve on our evaluation tasks. (3) We provide two novel ways to extend the bimodal mod- els to support three or more modalities. We find that the three-, four-, and five-dimensional models significantly outperform models using only one or two modalities, and that nontextual modalities each provide separate, disjoint knowledge that cannot be forced into a shared, latent structure.</p><p>5 0.44453377 <a title="140-lsi-5" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>Author: Shize Xu ; Shanshan Wang ; Yan Zhang</p><p>Abstract: The rapid development of Web2.0 leads to significant information redundancy. Especially for a complex news event, it is difficult to understand its general idea within a single coherent picture. A complex event often contains branches, intertwining narratives and side news which are all called storylines. In this paper, we propose a novel solution to tackle the challenging problem of storylines extraction and reconstruction. Specifically, we first investigate two requisite properties of an ideal storyline. Then a unified algorithm is devised to extract all effective storylines by optimizing these properties at the same time. Finally, we reconstruct all extracted lines and generate the high-quality story map. Experiments on real-world datasets show that our method is quite efficient and highly competitive, which can bring about quicker, clearer and deeper comprehension to readers.</p><p>6 0.38150814 <a title="140-lsi-6" href="./emnlp-2013-Towards_Situated_Dialogue%3A_Revisiting_Referring_Expression_Generation.html">185 emnlp-2013-Towards Situated Dialogue: Revisiting Referring Expression Generation</a></p>
<p>7 0.2954137 <a title="140-lsi-7" href="./emnlp-2013-Centering_Similarity_Measures_to_Reduce_Hubs.html">44 emnlp-2013-Centering Similarity Measures to Reduce Hubs</a></p>
<p>8 0.28835118 <a title="140-lsi-8" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>9 0.27162591 <a title="140-lsi-9" href="./emnlp-2013-Predicting_the_Resolution_of_Referring_Expressions_from_User_Behavior.html">153 emnlp-2013-Predicting the Resolution of Referring Expressions from User Behavior</a></p>
<p>10 0.27032369 <a title="140-lsi-10" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>11 0.2580066 <a title="140-lsi-11" href="./emnlp-2013-This_Text_Has_the_Scent_of_Starbucks%3A_A_Laplacian_Structured_Sparsity_Model_for_Computational_Branding_Analytics.html">184 emnlp-2013-This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</a></p>
<p>12 0.2579138 <a title="140-lsi-12" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>13 0.25162786 <a title="140-lsi-13" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>14 0.23328856 <a title="140-lsi-14" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>15 0.23305923 <a title="140-lsi-15" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>16 0.22428578 <a title="140-lsi-16" href="./emnlp-2013-Using_Topic_Modeling_to_Improve_Prediction_of_Neuroticism_and_Depression_in_College_Students.html">199 emnlp-2013-Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students</a></p>
<p>17 0.22191417 <a title="140-lsi-17" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>18 0.22006127 <a title="140-lsi-18" href="./emnlp-2013-Scaling_to_Large3_Data%3A_An_Efficient_and_Effective_Method_to_Compute_Distributional_Thesauri.html">165 emnlp-2013-Scaling to Large3 Data: An Efficient and Effective Method to Compute Distributional Thesauri</a></p>
<p>19 0.21733585 <a title="140-lsi-19" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>20 0.20933077 <a title="140-lsi-20" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.049), (6, 0.013), (18, 0.028), (22, 0.045), (23, 0.116), (24, 0.139), (30, 0.047), (47, 0.01), (50, 0.076), (51, 0.12), (53, 0.013), (66, 0.034), (71, 0.031), (75, 0.025), (77, 0.019), (88, 0.036), (90, 0.019), (95, 0.012), (96, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78391904 <a title="140-lda-1" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>Author: Andrew J. Anderson ; Elia Bruni ; Ulisse Bordignon ; Massimo Poesio ; Marco Baroni</p><p>Abstract: Traditional distributional semantic models extract word meaning representations from cooccurrence patterns of words in text corpora. Recently, the distributional approach has been extended to models that record the cooccurrence of words with visual features in image collections. These image-based models should be complementary to text-based ones, providing a more cognitively plausible view of meaning grounded in visual perception. In this study, we test whether image-based models capture the semantic patterns that emerge from fMRI recordings of the neural signal. Our results indicate that, indeed, there is a significant correlation between image-based and brain-based semantic similarities, and that image-based models complement text-based ones, so that the best correlations are achieved when the two modalities are combined. Despite some unsatisfactory, but explained out- comes (in particular, failure to detect differential association of models with brain areas), the results show, on the one hand, that imagebased distributional semantic models can be a precious new tool to explore semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility.</p><p>2 0.74013299 <a title="140-lda-2" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: Many statistical learning problems in NLP call for local model search methods. But accuracy tends to suffer with current techniques, which often explore either too narrowly or too broadly: hill-climbers can get stuck in local optima, whereas samplers may be inefficient. We propose to arrange individual local optimizers into organized networks. Our building blocks are operators of two types: (i) transform, which suggests new places to search, via non-random restarts from already-found local optima; and (ii) join, which merges candidate solutions to find better optima. Experiments on grammar induction show that pursuing different transforms (e.g., discarding parts of a learned model or ignoring portions of training data) results in improvements. Groups of locally-optimal solutions can be further perturbed jointly, by constructing mixtures. Using these tools, we designed several modular dependency grammar induction networks of increasing complexity. Our complete sys- tem achieves 48.6% accuracy (directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data) more than 5% higher than the previous state-of-the-art. —</p><p>3 0.66468298 <a title="140-lda-3" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>Author: Gary Patterson ; Andrew Kehler</p><p>Abstract: We present a classification model that predicts the presence or omission of a lexical connective between two clauses, based upon linguistic features of the clauses and the type of discourse relation holding between them. The model is trained on a set of high frequency relations extracted from the Penn Discourse Treebank and achieves an accuracy of 86.6%. Analysis of the results reveals that the most informative features relate to the discourse dependencies between sequences of coherence relations in the text. We also present results of an experiment that provides insight into the nature and difficulty of the task.</p><p>4 0.64835829 <a title="140-lda-4" href="./emnlp-2013-Adaptor_Grammars_for_Learning_Non-Concatenative_Morphology.html">19 emnlp-2013-Adaptor Grammars for Learning Non-Concatenative Morphology</a></p>
<p>Author: Jan A. Botha ; Phil Blunsom</p><p>Abstract: This paper contributes an approach for expressing non-concatenative morphological phenomena, such as stem derivation in Semitic languages, in terms of a mildly context-sensitive grammar formalism. This offers a convenient level of modelling abstraction while remaining computationally tractable. The nonparametric Bayesian framework of adaptor grammars is extended to this richer grammar formalism to propose a probabilistic model that can learn word segmentation and morpheme lexicons, including ones with discontiguous strings as elements, from unannotated data. Our experiments on Hebrew and three variants of Arabic data find that the additional expressiveness to capture roots and templates as atomic units improves the quality of concatenative segmentation and stem identification. We obtain 74% accuracy in identifying triliteral Hebrew roots, while performing morphological segmentation with an F1-score of 78. 1.</p><p>5 0.62952799 <a title="140-lda-5" href="./emnlp-2013-Image_Description_using_Visual_Dependency_Representations.html">98 emnlp-2013-Image Description using Visual Dependency Representations</a></p>
<p>Author: Desmond Elliott ; Frank Keller</p><p>Abstract: Describing the main event of an image involves identifying the objects depicted and predicting the relationships between them. Previous approaches have represented images as unstructured bags of regions, which makes it difficult to accurately predict meaningful relationships between regions. In this paper, we introduce visual dependency representations to capture the relationships between the objects in an image, and hypothesize that this representation can improve image description. We test this hypothesis using a new data set of region-annotated images, associated with visual dependency representations and gold-standard descriptions. We describe two template-based description generation models that operate over visual dependency representations. In an image descrip- tion task, we find that these models outperform approaches that rely on object proximity or corpus information to generate descriptions on both automatic measures and on human judgements.</p><p>6 0.62799257 <a title="140-lda-6" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>7 0.61204225 <a title="140-lda-7" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>8 0.60911423 <a title="140-lda-8" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>9 0.60767657 <a title="140-lda-9" href="./emnlp-2013-Exploiting_Language_Models_for_Visual_Recognition.html">78 emnlp-2013-Exploiting Language Models for Visual Recognition</a></p>
<p>10 0.60750258 <a title="140-lda-10" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>11 0.60621762 <a title="140-lda-11" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>12 0.60464358 <a title="140-lda-12" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>13 0.59935284 <a title="140-lda-13" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>14 0.59845108 <a title="140-lda-14" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>15 0.59719163 <a title="140-lda-15" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>16 0.59708154 <a title="140-lda-16" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>17 0.59623325 <a title="140-lda-17" href="./emnlp-2013-Regularized_Minimum_Error_Rate_Training.html">159 emnlp-2013-Regularized Minimum Error Rate Training</a></p>
<p>18 0.59621978 <a title="140-lda-18" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>19 0.59552473 <a title="140-lda-19" href="./emnlp-2013-Document_Summarization_via_Guided_Sentence_Compression.html">65 emnlp-2013-Document Summarization via Guided Sentence Compression</a></p>
<p>20 0.5955109 <a title="140-lda-20" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
