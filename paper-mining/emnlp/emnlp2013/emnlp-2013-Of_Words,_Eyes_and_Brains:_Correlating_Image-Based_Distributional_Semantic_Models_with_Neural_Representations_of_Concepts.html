<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-140" href="#">emnlp2013-140</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</h1>
<br/><p>Source: <a title="emnlp-2013-140-pdf" href="http://aclweb.org/anthology//D/D13/D13-1202.pdf">pdf</a></p><p>Author: Andrew J. Anderson ; Elia Bruni ; Ulisse Bordignon ; Massimo Poesio ; Marco Baroni</p><p>Abstract: Traditional distributional semantic models extract word meaning representations from cooccurrence patterns of words in text corpora. Recently, the distributional approach has been extended to models that record the cooccurrence of words with visual features in image collections. These image-based models should be complementary to text-based ones, providing a more cognitively plausible view of meaning grounded in visual perception. In this study, we test whether image-based models capture the semantic patterns that emerge from fMRI recordings of the neural signal. Our results indicate that, indeed, there is a significant correlation between image-based and brain-based semantic similarities, and that image-based models complement text-based ones, so that the best correlations are achieved when the two modalities are combined. Despite some unsatisfactory, but explained out- comes (in particular, failure to detect differential association of models with brain areas), the results show, on the one hand, that imagebased distributional semantic models can be a precious new tool to explore semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility.</p><p>Reference: <a title="emnlp-2013-140-reference" href="../emnlp2013_reference/emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lob', 0.755), ('brain', 0.301), ('occipit', 0.221), ('pariet', 0.186), ('vis', 0.16), ('front', 0.155), ('brun', 0.152), ('correl', 0.111), ('fmri', 0.111), ('im', 0.109), ('anatom', 0.1), ('temp', 0.085), ('cortex', 0.07), ('voxel', 0.07), ('reg', 0.061), ('cloth', 0.061), ('gyr', 0.058), ('bind', 0.055), ('perceiv', 0.055), ('object', 0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="140-tfidf-1" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>Author: Andrew J. Anderson ; Elia Bruni ; Ulisse Bordignon ; Massimo Poesio ; Marco Baroni</p><p>Abstract: Traditional distributional semantic models extract word meaning representations from cooccurrence patterns of words in text corpora. Recently, the distributional approach has been extended to models that record the cooccurrence of words with visual features in image collections. These image-based models should be complementary to text-based ones, providing a more cognitively plausible view of meaning grounded in visual perception. In this study, we test whether image-based models capture the semantic patterns that emerge from fMRI recordings of the neural signal. Our results indicate that, indeed, there is a significant correlation between image-based and brain-based semantic similarities, and that image-based models complement text-based ones, so that the best correlations are achieved when the two modalities are combined. Despite some unsatisfactory, but explained out- comes (in particular, failure to detect differential association of models with brain areas), the results show, on the one hand, that imagebased distributional semantic models can be a precious new tool to explore semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility.</p><p>2 0.14252926 <a title="140-tfidf-2" href="./emnlp-2013-Exploiting_Language_Models_for_Visual_Recognition.html">78 emnlp-2013-Exploiting Language Models for Visual Recognition</a></p>
<p>Author: Dieu-Thu Le ; Jasper Uijlings ; Raffaella Bernardi</p><p>Abstract: The problem of learning language models from large text corpora has been widely studied within the computational linguistic community. However, little is known about the performance of these language models when applied to the computer vision domain. In this work, we compare representative models: a window-based model, a topic model, a distributional memory and a commonsense knowledge database, ConceptNet, in two visual recognition scenarios: human action recognition and object prediction. We examine whether the knowledge extracted from texts through these models are compatible to the knowledge represented in images. We determine the usefulness of different language models in aiding the two visual recognition tasks. The study shows that the language models built from general text corpora can be used instead of expensive annotated images and even outperform the image model when testing on a big general dataset.</p><p>3 0.14220187 <a title="140-tfidf-3" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>Author: Stephen Roller ; Sabine Schulte im Walde</p><p>Abstract: Recent investigations into grounded models of language have shown that holistic views of language and perception can provide higher performance than independent views. In this work, we improve a two-dimensional multimodal version of Latent Dirichlet Allocation (Andrews et al., 2009) in various ways. (1) We outperform text-only models in two different evaluations, and demonstrate that low-level visual features are directly compatible with the existing model. (2) We present a novel way to integrate visual features into the LDA model using unsupervised clusters of images. The clusters are directly interpretable and improve on our evaluation tasks. (3) We provide two novel ways to extend the bimodal mod- els to support three or more modalities. We find that the three-, four-, and five-dimensional models significantly outperform models using only one or two modalities, and that nontextual modalities each provide separate, disjoint knowledge that cannot be forced into a shared, latent structure.</p><p>4 0.13927293 <a title="140-tfidf-4" href="./emnlp-2013-Image_Description_using_Visual_Dependency_Representations.html">98 emnlp-2013-Image Description using Visual Dependency Representations</a></p>
<p>Author: Desmond Elliott ; Frank Keller</p><p>Abstract: Describing the main event of an image involves identifying the objects depicted and predicting the relationships between them. Previous approaches have represented images as unstructured bags of regions, which makes it difficult to accurately predict meaningful relationships between regions. In this paper, we introduce visual dependency representations to capture the relationships between the objects in an image, and hypothesize that this representation can improve image description. We test this hypothesis using a new data set of region-annotated images, associated with visual dependency representations and gold-standard descriptions. We describe two template-based description generation models that operate over visual dependency representations. In an image descrip- tion task, we find that these models outperform approaches that rely on object proximity or corpus information to generate descriptions on both automatic measures and on human judgements.</p><p>5 0.088488355 <a title="140-tfidf-5" href="./emnlp-2013-Towards_Situated_Dialogue%3A_Revisiting_Referring_Expression_Generation.html">185 emnlp-2013-Towards Situated Dialogue: Revisiting Referring Expression Generation</a></p>
<p>Author: Rui Fang ; Changsong Liu ; Lanbo She ; Joyce Y. Chai</p><p>Abstract: In situated dialogue, humans and agents have mismatched capabilities of perceiving the shared environment. Their representations of the shared world are misaligned. Thus referring expression generation (REG) will need to take this discrepancy into consideration. To address this issue, we developed a hypergraph-based approach to account for group-based spatial relations and uncertainties in perceiving the environment. Our empirical results have shown that this approach outperforms a previous graph-based approach with an absolute gain of 9%. However, while these graph-based approaches perform effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), they perform rather poorly when the agent has imperfect perception of the environment (e.g., 45%). This big performance gap calls for new solutions to REG that can mediate a shared perceptual basis in situated dialogue.</p><p>6 0.069799177 <a title="140-tfidf-6" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>7 0.05378997 <a title="140-tfidf-7" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>8 0.052865721 <a title="140-tfidf-8" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>9 0.051014762 <a title="140-tfidf-9" href="./emnlp-2013-Learning_Distributions_over_Logical_Forms_for_Referring_Expression_Generation.html">119 emnlp-2013-Learning Distributions over Logical Forms for Referring Expression Generation</a></p>
<p>10 0.045135807 <a title="140-tfidf-10" href="./emnlp-2013-Building_Event_Threads_out_of_Multiple_News_Articles.html">41 emnlp-2013-Building Event Threads out of Multiple News Articles</a></p>
<p>11 0.040143173 <a title="140-tfidf-11" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>12 0.039788336 <a title="140-tfidf-12" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>13 0.038803123 <a title="140-tfidf-13" href="./emnlp-2013-Event-Based_Time_Label_Propagation_for_Automatic_Dating_of_News_Articles.html">74 emnlp-2013-Event-Based Time Label Propagation for Automatic Dating of News Articles</a></p>
<p>14 0.038690507 <a title="140-tfidf-14" href="./emnlp-2013-Learning_Biological_Processes_with_Global_Constraints.html">118 emnlp-2013-Learning Biological Processes with Global Constraints</a></p>
<p>15 0.035781965 <a title="140-tfidf-15" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>16 0.035293017 <a title="140-tfidf-16" href="./emnlp-2013-Predicting_the_Resolution_of_Referring_Expressions_from_User_Behavior.html">153 emnlp-2013-Predicting the Resolution of Referring Expressions from User Behavior</a></p>
<p>17 0.03462819 <a title="140-tfidf-17" href="./emnlp-2013-The_VerbCorner_Project%3A_Toward_an_Empirically-Based_Semantic_Decomposition_of_Verbs.html">183 emnlp-2013-The VerbCorner Project: Toward an Empirically-Based Semantic Decomposition of Verbs</a></p>
<p>18 0.033899322 <a title="140-tfidf-18" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>19 0.033240654 <a title="140-tfidf-19" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>20 0.03321163 <a title="140-tfidf-20" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.114), (1, -0.049), (2, 0.024), (3, -0.042), (4, -0.086), (5, -0.079), (6, -0.035), (7, 0.036), (8, -0.015), (9, 0.124), (10, -0.167), (11, 0.063), (12, 0.036), (13, 0.099), (14, -0.105), (15, -0.082), (16, -0.021), (17, -0.113), (18, 0.044), (19, -0.057), (20, -0.071), (21, -0.147), (22, -0.018), (23, -0.064), (24, 0.075), (25, 0.057), (26, 0.002), (27, -0.028), (28, 0.036), (29, -0.037), (30, 0.021), (31, 0.022), (32, 0.07), (33, -0.044), (34, -0.002), (35, 0.116), (36, -0.091), (37, 0.038), (38, 0.04), (39, -0.085), (40, -0.028), (41, -0.05), (42, -0.058), (43, 0.003), (44, -0.006), (45, -0.052), (46, 0.094), (47, -0.064), (48, 0.005), (49, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90966511 <a title="140-lsi-1" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>Author: Andrew J. Anderson ; Elia Bruni ; Ulisse Bordignon ; Massimo Poesio ; Marco Baroni</p><p>Abstract: Traditional distributional semantic models extract word meaning representations from cooccurrence patterns of words in text corpora. Recently, the distributional approach has been extended to models that record the cooccurrence of words with visual features in image collections. These image-based models should be complementary to text-based ones, providing a more cognitively plausible view of meaning grounded in visual perception. In this study, we test whether image-based models capture the semantic patterns that emerge from fMRI recordings of the neural signal. Our results indicate that, indeed, there is a significant correlation between image-based and brain-based semantic similarities, and that image-based models complement text-based ones, so that the best correlations are achieved when the two modalities are combined. Despite some unsatisfactory, but explained out- comes (in particular, failure to detect differential association of models with brain areas), the results show, on the one hand, that imagebased distributional semantic models can be a precious new tool to explore semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility.</p><p>2 0.81492692 <a title="140-lsi-2" href="./emnlp-2013-Exploiting_Language_Models_for_Visual_Recognition.html">78 emnlp-2013-Exploiting Language Models for Visual Recognition</a></p>
<p>Author: Dieu-Thu Le ; Jasper Uijlings ; Raffaella Bernardi</p><p>Abstract: The problem of learning language models from large text corpora has been widely studied within the computational linguistic community. However, little is known about the performance of these language models when applied to the computer vision domain. In this work, we compare representative models: a window-based model, a topic model, a distributional memory and a commonsense knowledge database, ConceptNet, in two visual recognition scenarios: human action recognition and object prediction. We examine whether the knowledge extracted from texts through these models are compatible to the knowledge represented in images. We determine the usefulness of different language models in aiding the two visual recognition tasks. The study shows that the language models built from general text corpora can be used instead of expensive annotated images and even outperform the image model when testing on a big general dataset.</p><p>3 0.78965533 <a title="140-lsi-3" href="./emnlp-2013-Image_Description_using_Visual_Dependency_Representations.html">98 emnlp-2013-Image Description using Visual Dependency Representations</a></p>
<p>Author: Desmond Elliott ; Frank Keller</p><p>Abstract: Describing the main event of an image involves identifying the objects depicted and predicting the relationships between them. Previous approaches have represented images as unstructured bags of regions, which makes it difficult to accurately predict meaningful relationships between regions. In this paper, we introduce visual dependency representations to capture the relationships between the objects in an image, and hypothesize that this representation can improve image description. We test this hypothesis using a new data set of region-annotated images, associated with visual dependency representations and gold-standard descriptions. We describe two template-based description generation models that operate over visual dependency representations. In an image descrip- tion task, we find that these models outperform approaches that rely on object proximity or corpus information to generate descriptions on both automatic measures and on human judgements.</p><p>4 0.7098791 <a title="140-lsi-4" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>Author: Stephen Roller ; Sabine Schulte im Walde</p><p>Abstract: Recent investigations into grounded models of language have shown that holistic views of language and perception can provide higher performance than independent views. In this work, we improve a two-dimensional multimodal version of Latent Dirichlet Allocation (Andrews et al., 2009) in various ways. (1) We outperform text-only models in two different evaluations, and demonstrate that low-level visual features are directly compatible with the existing model. (2) We present a novel way to integrate visual features into the LDA model using unsupervised clusters of images. The clusters are directly interpretable and improve on our evaluation tasks. (3) We provide two novel ways to extend the bimodal mod- els to support three or more modalities. We find that the three-, four-, and five-dimensional models significantly outperform models using only one or two modalities, and that nontextual modalities each provide separate, disjoint knowledge that cannot be forced into a shared, latent structure.</p><p>5 0.61635232 <a title="140-lsi-5" href="./emnlp-2013-Towards_Situated_Dialogue%3A_Revisiting_Referring_Expression_Generation.html">185 emnlp-2013-Towards Situated Dialogue: Revisiting Referring Expression Generation</a></p>
<p>Author: Rui Fang ; Changsong Liu ; Lanbo She ; Joyce Y. Chai</p><p>Abstract: In situated dialogue, humans and agents have mismatched capabilities of perceiving the shared environment. Their representations of the shared world are misaligned. Thus referring expression generation (REG) will need to take this discrepancy into consideration. To address this issue, we developed a hypergraph-based approach to account for group-based spatial relations and uncertainties in perceiving the environment. Our empirical results have shown that this approach outperforms a previous graph-based approach with an absolute gain of 9%. However, while these graph-based approaches perform effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), they perform rather poorly when the agent has imperfect perception of the environment (e.g., 45%). This big performance gap calls for new solutions to REG that can mediate a shared perceptual basis in situated dialogue.</p><p>6 0.44230065 <a title="140-lsi-6" href="./emnlp-2013-Predicting_the_Resolution_of_Referring_Expressions_from_User_Behavior.html">153 emnlp-2013-Predicting the Resolution of Referring Expressions from User Behavior</a></p>
<p>7 0.36469635 <a title="140-lsi-7" href="./emnlp-2013-Centering_Similarity_Measures_to_Reduce_Hubs.html">44 emnlp-2013-Centering Similarity Measures to Reduce Hubs</a></p>
<p>8 0.35973483 <a title="140-lsi-8" href="./emnlp-2013-Learning_Distributions_over_Logical_Forms_for_Referring_Expression_Generation.html">119 emnlp-2013-Learning Distributions over Logical Forms for Referring Expression Generation</a></p>
<p>9 0.33008325 <a title="140-lsi-9" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>10 0.32827827 <a title="140-lsi-10" href="./emnlp-2013-Learning_Biological_Processes_with_Global_Constraints.html">118 emnlp-2013-Learning Biological Processes with Global Constraints</a></p>
<p>11 0.31982014 <a title="140-lsi-11" href="./emnlp-2013-Animacy_Detection_with_Voting_Models.html">23 emnlp-2013-Animacy Detection with Voting Models</a></p>
<p>12 0.31947064 <a title="140-lsi-12" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>13 0.3145639 <a title="140-lsi-13" href="./emnlp-2013-This_Text_Has_the_Scent_of_Starbucks%3A_A_Laplacian_Structured_Sparsity_Model_for_Computational_Branding_Analytics.html">184 emnlp-2013-This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</a></p>
<p>14 0.30719131 <a title="140-lsi-14" href="./emnlp-2013-Building_Event_Threads_out_of_Multiple_News_Articles.html">41 emnlp-2013-Building Event Threads out of Multiple News Articles</a></p>
<p>15 0.30254632 <a title="140-lsi-15" href="./emnlp-2013-The_VerbCorner_Project%3A_Toward_an_Empirically-Based_Semantic_Decomposition_of_Verbs.html">183 emnlp-2013-The VerbCorner Project: Toward an Empirically-Based Semantic Decomposition of Verbs</a></p>
<p>16 0.28572682 <a title="140-lsi-16" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>17 0.27805811 <a title="140-lsi-17" href="./emnlp-2013-With_Blinkers_on%3A_Robust_Prediction_of_Eye_Movements_across_Readers.html">203 emnlp-2013-With Blinkers on: Robust Prediction of Eye Movements across Readers</a></p>
<p>18 0.27000949 <a title="140-lsi-18" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>19 0.26639521 <a title="140-lsi-19" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>20 0.25971624 <a title="140-lsi-20" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(12, 0.012), (23, 0.028), (24, 0.015), (25, 0.046), (28, 0.015), (29, 0.026), (45, 0.01), (47, 0.08), (51, 0.034), (52, 0.337), (65, 0.123), (69, 0.029), (72, 0.012), (73, 0.084), (79, 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.70692718 <a title="140-lda-1" href="./emnlp-2013-Animacy_Detection_with_Voting_Models.html">23 emnlp-2013-Animacy Detection with Voting Models</a></p>
<p>Author: Joshua Moore ; Christopher J.C. Burges ; Erin Renshaw ; Wen-tau Yih</p><p>Abstract: Animacy detection is a problem whose solution has been shown to be beneficial for a number of syntactic and semantic tasks. We present a state-of-the-art system for this task which uses a number of simple classifiers with heterogeneous data sources in a voting scheme. We show how this framework can give us direct insight into the behavior of the system, allowing us to more easily diagnose sources of error.</p><p>same-paper 2 0.67210066 <a title="140-lda-2" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>Author: Andrew J. Anderson ; Elia Bruni ; Ulisse Bordignon ; Massimo Poesio ; Marco Baroni</p><p>Abstract: Traditional distributional semantic models extract word meaning representations from cooccurrence patterns of words in text corpora. Recently, the distributional approach has been extended to models that record the cooccurrence of words with visual features in image collections. These image-based models should be complementary to text-based ones, providing a more cognitively plausible view of meaning grounded in visual perception. In this study, we test whether image-based models capture the semantic patterns that emerge from fMRI recordings of the neural signal. Our results indicate that, indeed, there is a significant correlation between image-based and brain-based semantic similarities, and that image-based models complement text-based ones, so that the best correlations are achieved when the two modalities are combined. Despite some unsatisfactory, but explained out- comes (in particular, failure to detect differential association of models with brain areas), the results show, on the one hand, that imagebased distributional semantic models can be a precious new tool to explore semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility.</p><p>3 0.47215387 <a title="140-lda-3" href="./emnlp-2013-Easy_Victories_and_Uphill_Battles_in_Coreference_Resolution.html">67 emnlp-2013-Easy Victories and Uphill Battles in Coreference Resolution</a></p>
<p>Author: Greg Durrett ; Dan Klein</p><p>Abstract: Classical coreference systems encode various syntactic, discourse, and semantic phenomena explicitly, using heterogenous features computed from hand-crafted heuristics. In contrast, we present a state-of-the-art coreference system that captures such phenomena implicitly, with a small number of homogeneous feature templates examining shallow properties of mentions. Surprisingly, our features are actually more effective than the corresponding hand-engineered ones at modeling these key linguistic phenomena, allowing us to win “easy victories” without crafted heuristics. These features are successful on syntax and discourse; however, they do not model semantic compatibility well, nor do we see gains from experiments with shallow semantic features from the literature, suggesting that this approach to semantics is an “uphill battle.” Nonetheless, our final system1 outperforms the Stanford system (Lee et al. (201 1), the winner of the CoNLL 2011 shared task) by 3.5% absolute on the CoNLL metric and outperforms the IMS system (Bj o¨rkelund and Farkas (2012), the best publicly available English coreference system) by 1.9% absolute.</p><p>4 0.47158909 <a title="140-lda-4" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>Author: Stephen Roller ; Sabine Schulte im Walde</p><p>Abstract: Recent investigations into grounded models of language have shown that holistic views of language and perception can provide higher performance than independent views. In this work, we improve a two-dimensional multimodal version of Latent Dirichlet Allocation (Andrews et al., 2009) in various ways. (1) We outperform text-only models in two different evaluations, and demonstrate that low-level visual features are directly compatible with the existing model. (2) We present a novel way to integrate visual features into the LDA model using unsupervised clusters of images. The clusters are directly interpretable and improve on our evaluation tasks. (3) We provide two novel ways to extend the bimodal mod- els to support three or more modalities. We find that the three-, four-, and five-dimensional models significantly outperform models using only one or two modalities, and that nontextual modalities each provide separate, disjoint knowledge that cannot be forced into a shared, latent structure.</p><p>5 0.47005337 <a title="140-lda-5" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>Author: Baichuan Li ; Jing Liu ; Chin-Yew Lin ; Irwin King ; Michael R. Lyu</p><p>Abstract: Social media like forums and microblogs have accumulated a huge amount of user generated content (UGC) containing human knowledge. Currently, most of UGC is listed as a whole or in pre-defined categories. This “list-based” approach is simple, but hinders users from browsing and learning knowledge of certain topics effectively. To address this problem, we propose a hierarchical entity-based approach for structuralizing UGC in social media. By using a large-scale entity repository, we design a three-step framework to organize UGC in a novel hierarchical structure called “cluster entity tree (CET)”. With Yahoo! Answers as a test case, we conduct experiments and the results show the effectiveness of our framework in constructing CET. We further evaluate the performance of CET on UGC organization in both user and system aspects. From a user aspect, our user study demonstrates that, with CET-based structure, users perform significantly better in knowledge learning than using traditional list-based approach. From a system aspect, CET substantially boosts the performance of two information retrieval models (i.e., vector space model and query likelihood language model).</p><p>6 0.46905026 <a title="140-lda-6" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>7 0.46748823 <a title="140-lda-7" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>8 0.46553916 <a title="140-lda-8" href="./emnlp-2013-Image_Description_using_Visual_Dependency_Representations.html">98 emnlp-2013-Image Description using Visual Dependency Representations</a></p>
<p>9 0.46541852 <a title="140-lda-9" href="./emnlp-2013-Exploiting_Language_Models_for_Visual_Recognition.html">78 emnlp-2013-Exploiting Language Models for Visual Recognition</a></p>
<p>10 0.46288484 <a title="140-lda-10" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>11 0.46202466 <a title="140-lda-11" href="./emnlp-2013-A_Dataset_for_Research_on_Short-Text_Conversations.html">4 emnlp-2013-A Dataset for Research on Short-Text Conversations</a></p>
<p>12 0.46174708 <a title="140-lda-12" href="./emnlp-2013-Scaling_Semantic_Parsers_with_On-the-Fly_Ontology_Matching.html">164 emnlp-2013-Scaling Semantic Parsers with On-the-Fly Ontology Matching</a></p>
<p>13 0.46025118 <a title="140-lda-13" href="./emnlp-2013-Chinese_Zero_Pronoun_Resolution%3A_Some_Recent_Advances.html">45 emnlp-2013-Chinese Zero Pronoun Resolution: Some Recent Advances</a></p>
<p>14 0.46009806 <a title="140-lda-14" href="./emnlp-2013-Latent_Anaphora_Resolution_for_Cross-Lingual_Pronoun_Prediction.html">117 emnlp-2013-Latent Anaphora Resolution for Cross-Lingual Pronoun Prediction</a></p>
<p>15 0.45977834 <a title="140-lda-15" href="./emnlp-2013-The_Answer_is_at_your_Fingertips%3A_Improving_Passage_Retrieval_for_Web_Question_Answering_with_Search_Behavior_Data.html">180 emnlp-2013-The Answer is at your Fingertips: Improving Passage Retrieval for Web Question Answering with Search Behavior Data</a></p>
<p>16 0.45919734 <a title="140-lda-16" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>17 0.45755357 <a title="140-lda-17" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>18 0.45699969 <a title="140-lda-18" href="./emnlp-2013-Discourse_Level_Explanatory_Relation_Extraction_from_Product_Reviews_Using_First-Order_Logic.html">63 emnlp-2013-Discourse Level Explanatory Relation Extraction from Product Reviews Using First-Order Logic</a></p>
<p>19 0.45692772 <a title="140-lda-19" href="./emnlp-2013-Learning_Biological_Processes_with_Global_Constraints.html">118 emnlp-2013-Learning Biological Processes with Global Constraints</a></p>
<p>20 0.45640141 <a title="140-lda-20" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
