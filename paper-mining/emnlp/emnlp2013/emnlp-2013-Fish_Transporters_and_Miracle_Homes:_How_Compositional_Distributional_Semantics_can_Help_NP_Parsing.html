<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-87" href="#">emnlp2013-87</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</h1>
<br/><p>Source: <a title="emnlp-2013-87-pdf" href="http://aclweb.org/anthology//D/D13/D13-1196.pdf">pdf</a></p><p>Author: Angeliki Lazaridou ; Eva Maria Vecchi ; Marco Baroni</p><p>Abstract: In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the correct parsing of NPs (e.g., (live fish) transporter rather than live (fish transporter)). We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features.</p><p>Reference: <a title="emnlp-2013-87-reference" href="../emnlp2013_reference/emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Fish transporters and miracle homes: How compositional distributional semantics can help NP parsing Angeliki Lazaridou, Eva Maria Vecchi and Marco Baroni Center for Mind/Brain Sciences University of Trento, Italy first . [sent-1, score-0.479]
</p><p>2 it  Abstract In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. [sent-3, score-0.611]
</p><p>3 We exploit this idea to choose the correct parsing of NPs (e. [sent-4, score-0.058]
</p><p>4 We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features. [sent-7, score-0.281]
</p><p>5 1 Introduction Live fish transporter: A transporter of live fish or rather a fish transporter that is not dead? [sent-8, score-0.694]
</p><p>6 While our intuition, based on the meaning of this phrase, prefers the former interpretation, the Stanford parser, which lacks semantic features, incor-  rectly predicts the latter as the correct parse. [sent-9, score-0.124]
</p><p>7 1 The correct syntactic parsing of sentences is clearly steered by semantic information (as formal syntacticians have pointed out at least since Fillmore (1968)), and consequently the semantic plausibility of alternative parses can provide crucial evidence about their validity. [sent-10, score-0.538]
</p><p>8 An emerging line of parsing research capitalizes on the advances of compositional distributional semantics (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Socher et al. [sent-11, score-0.343]
</p><p>9 Information related to compositionallyderived distributional representations of phrases is 1http : / / nlp . [sent-13, score-0.221]
</p><p>10 j sp  er  /  1908 integrated at various stages of the parsing process to improve overall performance. [sent-16, score-0.058]
</p><p>11 2 We are aware of two very recent studies exploiting the semantic information provided by distributional models to resolve syntactic ambiguity: Socher et al. [sent-17, score-0.218]
</p><p>12 (2013) present a recursive neural net-  work architecture which jointly learns semantic representations and syntactic categories of phrases. [sent-21, score-0.295]
</p><p>13 By annotating syntactic categories with their distributional representation, the method emulates lexicalized approaches (Collins, 2003) and captures similarity more flexibly than solutions based on hard clustering (Klein and Manning, 2003; Petrov et al. [sent-22, score-0.149]
</p><p>14 Thus, their approach mainly aims at improving parsing by capturing a richer, data-driven categorial structure. [sent-24, score-0.085]
</p><p>15 Their hypothesis is that parses that lead to less semantically plausible interpretations will be penalized by a reranker that looks at the composed semantic representation of the parse. [sent-27, score-0.234]
</p><p>16 However, as the authors also remark, because of their experimental setup, they cannot conclude that the improvement is truly due to the semantic composition component, a crucial issue that is deferred to further investigation. [sent-30, score-0.359]
</p><p>17 This work aims at corroborating the hypothesis that the semantic plausibility of a phrase can indeed determine its correct parsing. [sent-31, score-0.444]
</p><p>18 We develop a  system based on simple and intuitive measures, ex2Distributional representations approximate word and phrase meaning by vectors that record the contexts in which they are likely to appear in corpora; for a review see, e. [sent-32, score-0.16]
</p><p>19 We develop a controlled experimental setup, focusing on a single syntactic category, that is, noun phrases (NP), where our task can be formalized as (left or right) bracketing. [sent-39, score-0.12]
</p><p>20 Unlike previous work, we compare our compositional semantic component against features based on n-gram statistics, which can arguably also capture some semantic informa-  tion in terms of frequent occurrences of meaningful phrases. [sent-40, score-0.266]
</p><p>21 Inspired by previous literature demonstrating the power of metrics based on Pointwise Mutual Information (PMI) in NP bracketing (Nakov and Hearst, 2005; Pitler et al. [sent-41, score-0.09]
</p><p>22 , 2010; Vadas and Curran, 2011), we test an approach exploiting PMI features, and show that plausibility features relying on composed representations can significantly boost accuracy over PMI. [sent-42, score-0.392]
</p><p>23 2  Setup  Noun phrase dataset To construct our dataset, we used the Penn TreeBank (Marcus et al. [sent-43, score-0.068]
</p><p>24 We focus on NPs formed by three elements, where the first can be an adjective (A) or a noun (N), the other two are nouns. [sent-47, score-0.076]
</p><p>25 3 Distributional semantic space  As our source cor-  pus we use the concatenation of ukWaC, the English Wikipedia (2009 dump) and the BNC, with a total of 3The dataset is available from: unitn . [sent-49, score-0.133]
</p><p>26 4 We collect co-occurrence statistics for the top 8K Ns and 4K As, plus any other word from our NP dataset that was below this rank. [sent-54, score-0.028]
</p><p>27 Our context elements are composed of the top 10K content words (adjectives, adverbs, nouns and verbs). [sent-55, score-0.045]
</p><p>28 We apply (non-negative) Pointwise Mutual Information as weighting scheme and dimensionality reduction using Non-negative Matrix Factorization, setting the number of reduced-space dimensions to 300. [sent-57, score-0.033]
</p><p>29 7 For all methods, vectors were normalized before composing, both in training and in generation. [sent-61, score-0.024]
</p><p>30 Table 2 presents a summary description of the composition methods we used. [sent-62, score-0.29]
</p><p>31 Following previous literature (Mitchell and Lapata, 2010), and the general intuition that adjectival modification is quite a different process from noun combination (Gagn ´e and Spalding, 2009; McNally, 2013), we learn different parameters for noun-noun (NN) and adjective-noun (AN) phrases. [sent-63, score-0.088]
</p><p>32 As an example of the learned parameters, for the wadd model the ratio of parameters w1 and w2 is 1:2 for ANs, whereas for NNs it is almost 1:1, confirming the intuition that a non-head noun plays a stronger role in composition than an adjective modifier. [sent-64, score-0.52]
</p><p>33 bruni /MEN c ia 6We do not consider the popular multiplicative model, as it produces identical representations for NPs irrespective of their internal structure. [sent-78, score-0.066]
</p><p>34 it / compose s / c t oolkit / Model Composition function Parameters waddw1~ u + w2~ vw1, w2 dil fulladd lexfunc  || ~u||22 v~ + (λ − 1)h ~u, vi~ u |W|~ u1|| u~ +v + +W (2λv~ − Au~ v  λ W1, W2 ∈ Rm×m Au ∈ Rm∈× Rm  Table 2: Composition functions of inputs (u, v). [sent-82, score-0.31]
</p><p>35 Recursive composition In this study we also experiment with recursive composition; to the best of our knowledge, this is the first time that these composition functions have been explicitly used in this manner. [sent-83, score-0.739]
</p><p>36 For example, given the left bracketed NP (blood pressure) medicine, we want to obtain its compositional semantic representation, − − − − − − − − − − − − − − − − − − →  −bl −o −o −d −  p −r −e −s −s−u −r −e  −m −e −d −i −c −i →ne. [sent-84, score-0.237]
</p><p>37 First, basic composition  b−−l o − →od  −p −r −e −ss −u − →re  is applied, in which and are combined with one of the composition functions. [sent-85, score-0.607]
</p><p>38 Fol-  lowing that, we apply recursive composition; the − − − − − − − − − − →  output of basic composition, i. [sent-86, score-0.156]
</p><p>39 , −bl −o −o −d − − p −r −e −ss − −u →re, is fed to the function again to be composed with the representation of −m −e −d −i −c −i →ne. [sent-88, score-0.045]
</p><p>40 The latter step is straightforward for all composition functions except lexfunc applied to leftbracketed NPs, where the first step should return a matrix representing the left constituent (blood pressure in the running example). [sent-89, score-0.952]
</p><p>41 To cope with this nuisance, we apply the lexfunc method to basic composition only, while recursive representations are derived by summing (e. [sent-90, score-0.63]
</p><p>42 , −bl −o −o −d − − p −r −e −ss − −u →re is obtained by multiplying the blood matrix by the pressure vector, and it is then summed to −m −e −d −i −c −i →ne). [sent-92, score-0.761]
</p><p>43 − − − − − − →  − − − − − − − − − − →  − − − − − − →  3  Experiments  Semantic plausibility measures We use measures of semantic plausibility computed on composed semantic representations introduced by Vecchi et al. [sent-93, score-0.897]
</p><p>44 The rationale is that the correct (wrong) bracketing will lead to semantically more (less) plausible phrases. [sent-95, score-0.18]
</p><p>45 Thus, a measure able to discriminate semantically plausible from implausible phrases should also indicate the most likely parse. [sent-96, score-0.151]
</p><p>46 Considering, for example, the alternative parses of miracle home run, we observe that home run is a more semantically plausible phrase than miracle home. [sent-97, score-0.601]
</p><p>47 Furthermore, we might often refer to a baseball player’s miracle home run, but we doubt that 1910 even a miracle home can run! [sent-98, score-0.416]
</p><p>48 Given the composed representation of an AN (or NN), Vecchi et al. [sent-99, score-0.045]
</p><p>49 (201 1) define the following measures: • Density, quantified as the average cosine of a phrase yw,i qtuh aintst (top 10) nheea arevsetr neighbors, captures the intuition that a deviant phrase should be isolated in the semantic space. [sent-100, score-0.185]
</p><p>50 • Cosine of phrase and head N aims to capture tChoes finacet othf apth trhaes meaning odf N a adiemvisan tot AcaNpt (or NN) will tend to diverge from the meaning of the head noun. [sent-101, score-0.127]
</p><p>51 The intuition is that meaningless vectors, whose dimensions contain mostly noise, should have high entropy. [sent-105, score-0.069]
</p><p>52 NP Parsing as Classification Parsing NPs consisting of three elements can be treated as binary classification; given blood pressure medicine, we predict whether it is left- ((blood pressure) medicine) or right-bracketed (blood (pressure medicine)). [sent-106, score-0.761]
</p><p>53 8 Our dataset is split into 10 folds in which the ratio between the two classes is kept constant. [sent-108, score-0.06]
</p><p>54 We tune the SVM complexity parameter C on the first fold and we report accuracy results on the remaining nine folds after cross-validation. [sent-109, score-0.032]
</p><p>55 , 3 measures for each bracketing, evaluated on blood pressure and pressure medicine respectively, for a total of 6 features. [sent-112, score-1.447]
</p><p>56 frec contains 6 features computed on the vectfors resulting from the recursive compositions  8http : / / s cikit  -learn  . [sent-113, score-0.183]
</p><p>57 • pmi contains the PMI scores extracted from  our corpus nfosr t belo PoMd pressure xatnrdac pressure medicine. [sent-120, score-1.089]
</p><p>58 Baseline Model Given the skewed bracketing distribution in our dataset, we implement the following majority baselines: a) right classifies all phrases as right-bracketed; b) pos classifies NNN as leftbracketed (Lauer, 1995), ANN as right-bracketed. [sent-122, score-0.237]
</p><p>59 4  Results and Discussion  Table 3 omits results for dil and fulladd since they were outperformed by the right baseline. [sent-123, score-0.162]
</p><p>60 For both models, using both basic and recursive features leads to a boost in performance over basic features alone. [sent-125, score-0.183]
</p><p>61 Note that recursive features (frec) achieve at least equal or better performance than basic ones (fbasic). [sent-126, score-0.156]
</p><p>62 We expect indeed that in  many cases the asymmetry in plausibility will be 9Several approaches to computing PMI for these purposes have been proposed in the literature including the dependency model (Lauer, 1995) and the adjacency model (Marcus, 1980). [sent-127, score-0.308]
</p><p>63 We implement the latter since it has been shown to perform better (Vadas and Curran, 2007b) on NPs extracted from Penn TreeBank. [sent-128, score-0.025]
</p><p>64 1911 sharper when considering the whole NP rather than its sub-parts; a pressure medicine is still a conceivable concept, but blood (pressure medicine) makes no sense whatsoever. [sent-129, score-0.969]
</p><p>65 Finally, wadd outperforms both the more informative baseline pos and lexfunc. [sent-130, score-0.118]
</p><p>66 The difference between wadd and lexfunc is significant (p < 0. [sent-131, score-0.236]
</p><p>67 05)10 only when they are trained with recursive composition features, probably due to our suboptimal adaptation of the latter to recursive composition (see Section 2). [sent-132, score-0.863]
</p><p>68 The pmi approach outperforms the best plausibility-based feature set waddplausibility. [sent-133, score-0.219]
</p><p>69 However, the two make only a small proportion of common errors (29% of the total waddplausibility errors, 32% for pmi), suggesting that they are complementary. [sent-134, score-0.081]
</p><p>70 Indeed the pmi + waddplausibility combination significantly outperforms pmi alone  (p < 0. [sent-135, score-0.519]
</p><p>71 001), indicating that plausibility features can improve NP bracketing on top of the powerful PMI-based approach. [sent-136, score-0.371]
</p><p>72 The same effect can also be observed in the combination of pmi + lexfuncplausibility, which again significantly outperforms pmi alone (p < 0. [sent-137, score-0.438]
</p><p>73 This behaviour further suggests that the different types of errors are not a result of the parameters or type of composition applied, but rather highlights fundamental differences in the kind of information that PMI and composition models are able to capture. [sent-139, score-0.616]
</p><p>74 5  Conclusion  Our pilot study showed that semantic plausibility, as measured on compositional distributional representations, can improve syntactic parsing of NPs. [sent-142, score-0.404]
</p><p>75 Our results further suggest that state-of-the-art PMI features and the ones extracted from compositional representations are complementary, and thus, when combined, can lead to significantly better results. [sent-143, score-0.194]
</p><p>76 Besides paving the way to a more general integration 10Significance values are based on t-tests. [sent-144, score-0.027]
</p><p>77 of compositional distributional semantics in syntactic parsing, the proposed methodology provides a new way to evaluate composition functions. [sent-145, score-0.606]
</p><p>78 The relatively simple-minded wadd approach outperformed more complex models such as lexfunc. [sent-146, score-0.118]
</p><p>79 We plan to experiment next with more linguistically motivated ways to adapt the latter to recursive composition, including hybrid methods where ANs and NNs are treated differently. [sent-147, score-0.154]
</p><p>80 We would also like to consider more sophisticated semantic plausibility measures (e. [sent-148, score-0.393]
</p><p>81 , supervised ones), and apply them to other ambiguous syntactic constructions. [sent-150, score-0.031]
</p><p>82 Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. [sent-156, score-0.069]
</p><p>83 Constituent integration during the processing of compound words: Does it involve the use of relational structures? [sent-173, score-0.068]
</p><p>84 A regression model of adjective-noun compositionality in distributional semantics. [sent-177, score-0.174]
</p><p>85 Corpus statistics meet the noun compound: Some empirical results. [sent-186, score-0.052]
</p><p>86 Learning from errors: Using vector-based compositional semantics for parse reranking. [sent-190, score-0.167]
</p><p>87 Search engine statistics beyond the n-gram: Application to noun compound bracketing. [sent-212, score-0.093]
</p><p>88 Using web-scale n-grams to improve base NP parsing performance. [sent-220, score-0.058]
</p><p>89 (Linear) maps of the impossible: Capturing semantic anomalies in distributional space. [sent-253, score-0.187]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pressure', 0.435), ('blood', 0.326), ('composition', 0.29), ('plausibility', 0.281), ('pmi', 0.219), ('medicine', 0.208), ('vadas', 0.142), ('miracle', 0.136), ('transporter', 0.136), ('vecchi', 0.129), ('recursive', 0.129), ('compositional', 0.128), ('nps', 0.123), ('fish', 0.12), ('lexfunc', 0.118), ('wadd', 0.118), ('distributional', 0.118), ('np', 0.095), ('bracketing', 0.09), ('dil', 0.081), ('fbasic', 0.081), ('fulladd', 0.081), ('waddplausibility', 0.081), ('home', 0.072), ('semantic', 0.069), ('baroni', 0.068), ('representations', 0.066), ('live', 0.062), ('socher', 0.061), ('parsing', 0.058), ('compositionality', 0.056), ('plausible', 0.055), ('cime', 0.054), ('fplausibility', 0.054), ('frec', 0.054), ('gagn', 0.054), ('lauer', 0.054), ('leftbracketed', 0.054), ('penn', 0.054), ('mitchell', 0.052), ('noun', 0.052), ('dinu', 0.05), ('marco', 0.047), ('bl', 0.046), ('rm', 0.046), ('composed', 0.045), ('curran', 0.043), ('nns', 0.043), ('dissect', 0.043), ('cli', 0.043), ('concatenates', 0.043), ('measures', 0.043), ('maria', 0.042), ('compound', 0.041), ('nakov', 0.04), ('bracketed', 0.04), ('guevara', 0.04), ('phrase', 0.04), ('semantics', 0.039), ('au', 0.038), ('phrases', 0.037), ('behaviour', 0.036), ('unitn', 0.036), ('zamparelli', 0.036), ('pitler', 0.036), ('intuition', 0.036), ('semantically', 0.035), ('ss', 0.035), ('georgiana', 0.034), ('marcus', 0.034), ('sofia', 0.033), ('nn', 0.033), ('ans', 0.033), ('dimensions', 0.033), ('folds', 0.032), ('syntactic', 0.031), ('meaning', 0.03), ('parses', 0.03), ('functions', 0.03), ('lapata', 0.029), ('eva', 0.029), ('cambridge', 0.028), ('dataset', 0.028), ('additive', 0.028), ('roberto', 0.028), ('classifies', 0.028), ('indeed', 0.027), ('aims', 0.027), ('basic', 0.027), ('integration', 0.027), ('turney', 0.027), ('pointwise', 0.026), ('latter', 0.025), ('run', 0.025), ('petrov', 0.025), ('vectors', 0.024), ('adjective', 0.024), ('implausible', 0.024), ('bracketings', 0.024), ('pacling', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="87-tfidf-1" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>Author: Angeliki Lazaridou ; Eva Maria Vecchi ; Marco Baroni</p><p>Abstract: In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the correct parsing of NPs (e.g., (live fish) transporter rather than live (fish transporter)). We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features.</p><p>2 0.15840623 <a title="87-tfidf-2" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>Author: Eva Maria Vecchi ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: In this study, we use compositional distributional semantic methods to investigate restrictions in adjective ordering. Specifically, we focus on properties distinguishing AdjectiveAdjective-Noun phrases in which there is flexibility in the adjective ordering from those bound to a rigid order. We explore a number of measures extracted from the distributional representation of AAN phrases which may indicate a word order restriction. We find that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modification of the adjectives. Our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics.</p><p>3 0.14275771 <a title="87-tfidf-3" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<p>Author: Om P. Damani ; Shweta Ghonge</p><p>Abstract: Two recent measures incorporate the notion of statistical significance in basic PMI formulation. In some tasks, we find that the new measures perform worse than the PMI. Our analysis shows that while the basic ideas in incorporating statistical significance in PMI are reasonable, they have been applied slightly inappropriately. By fixing this, we get new measures that improve performance over not just PMI but on other popular co-occurrence measures as well. In fact, the revised measures perform reasonably well compared with more resource intensive non co-occurrence based methods also.</p><p>4 0.12568721 <a title="87-tfidf-4" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>Author: Masashi Tsubaki ; Kevin Duh ; Masashi Shimbo ; Yuji Matsumoto</p><p>Abstract: We present a novel vector space model for semantic co-compositionality. Inspired by Generative Lexicon Theory (Pustejovsky, 1995), our goal is a compositional model where both predicate and argument are allowed to modify each others’ meaning representations while generating the overall semantics. This readily addresses some major challenges with current vector space models, notably the polysemy issue and the use of one representation per word type. We implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations. We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality. The model achieves the best result to date (ρ = 0.47) on the semantic similarity task of transitive verbs (Grefenstette and Sadrzadeh, 2011).</p><p>5 0.12326594 <a title="87-tfidf-5" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>Author: Richard Socher ; Alex Perelygin ; Jean Wu ; Jason Chuang ; Christopher D. Manning ; Andrew Ng ; Christopher Potts</p><p>Abstract: Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.</p><p>6 0.11923391 <a title="87-tfidf-6" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>7 0.10597286 <a title="87-tfidf-7" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>8 0.10249446 <a title="87-tfidf-8" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>9 0.09647613 <a title="87-tfidf-9" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>10 0.076352812 <a title="87-tfidf-10" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>11 0.072885871 <a title="87-tfidf-11" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>12 0.053351238 <a title="87-tfidf-12" href="./emnlp-2013-Scaling_to_Large3_Data%3A_An_Efficient_and_Effective_Method_to_Compute_Distributional_Thesauri.html">165 emnlp-2013-Scaling to Large3 Data: An Efficient and Effective Method to Compute Distributional Thesauri</a></p>
<p>13 0.048562765 <a title="87-tfidf-13" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>14 0.046732426 <a title="87-tfidf-14" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>15 0.044456102 <a title="87-tfidf-15" href="./emnlp-2013-Semantic_Parsing_on_Freebase_from_Question-Answer_Pairs.html">166 emnlp-2013-Semantic Parsing on Freebase from Question-Answer Pairs</a></p>
<p>16 0.043387096 <a title="87-tfidf-16" href="./emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations.html">17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</a></p>
<p>17 0.042151358 <a title="87-tfidf-17" href="./emnlp-2013-Unsupervised_Induction_of_Contingent_Event_Pairs_from_Film_Scenes.html">192 emnlp-2013-Unsupervised Induction of Contingent Event Pairs from Film Scenes</a></p>
<p>18 0.041479502 <a title="87-tfidf-18" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>19 0.04104504 <a title="87-tfidf-19" href="./emnlp-2013-Deriving_Adjectival_Scales_from_Continuous_Space_Word_Representations.html">59 emnlp-2013-Deriving Adjectival Scales from Continuous Space Word Representations</a></p>
<p>20 0.04088553 <a title="87-tfidf-20" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.156), (1, 0.012), (2, -0.046), (3, -0.053), (4, 0.002), (5, 0.204), (6, -0.048), (7, -0.136), (8, -0.185), (9, -0.001), (10, 0.096), (11, 0.114), (12, -0.097), (13, 0.053), (14, 0.02), (15, -0.093), (16, 0.06), (17, -0.15), (18, -0.15), (19, -0.005), (20, -0.055), (21, 0.026), (22, -0.099), (23, 0.007), (24, 0.085), (25, -0.048), (26, -0.078), (27, 0.05), (28, -0.106), (29, -0.159), (30, -0.078), (31, -0.1), (32, 0.088), (33, 0.184), (34, -0.011), (35, -0.08), (36, 0.115), (37, -0.132), (38, 0.046), (39, 0.021), (40, 0.11), (41, 0.002), (42, 0.089), (43, 0.017), (44, -0.021), (45, 0.02), (46, 0.045), (47, 0.004), (48, 0.018), (49, -0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95038199 <a title="87-lsi-1" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>Author: Angeliki Lazaridou ; Eva Maria Vecchi ; Marco Baroni</p><p>Abstract: In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the correct parsing of NPs (e.g., (live fish) transporter rather than live (fish transporter)). We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features.</p><p>2 0.89581978 <a title="87-lsi-2" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>Author: Eva Maria Vecchi ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: In this study, we use compositional distributional semantic methods to investigate restrictions in adjective ordering. Specifically, we focus on properties distinguishing AdjectiveAdjective-Noun phrases in which there is flexibility in the adjective ordering from those bound to a rigid order. We explore a number of measures extracted from the distributional representation of AAN phrases which may indicate a word order restriction. We find that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modification of the adjectives. Our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics.</p><p>3 0.78855366 <a title="87-lsi-3" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>Author: Polina Kuznetsova ; Jianfu Chen ; Yejin Choi</p><p>Abstract: Why do certain combinations of words such as “disadvantageous peace ” or “metal to the petal” appeal to our minds as interesting expressions with a sense of creativity, while other phrases such as “quiet teenager”, or “geometrical base ” not as much? We present statistical explorations to understand the characteristics of lexical compositions that give rise to the perception of being original, interesting, and at times even artistic. We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learning that give us further insights on how different aspects of lexical composition collectively contribute to the perceived creativity.</p><p>4 0.74363023 <a title="87-lsi-4" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<p>Author: Om P. Damani ; Shweta Ghonge</p><p>Abstract: Two recent measures incorporate the notion of statistical significance in basic PMI formulation. In some tasks, we find that the new measures perform worse than the PMI. Our analysis shows that while the basic ideas in incorporating statistical significance in PMI are reasonable, they have been applied slightly inappropriately. By fixing this, we get new measures that improve performance over not just PMI but on other popular co-occurrence measures as well. In fact, the revised measures perform reasonably well compared with more resource intensive non co-occurrence based methods also.</p><p>5 0.61863565 <a title="87-lsi-5" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>Author: Douwe Kiela ; Stephen Clark</p><p>Abstract: We present a novel unsupervised approach to detecting the compositionality of multi-word expressions. We compute the compositionality of a phrase through substituting the constituent words with their “neighbours” in a semantic vector space and averaging over the distance between the original phrase and the substituted neighbour phrases. Several methods of obtaining neighbours are presented. The results are compared to existing supervised results and achieve state-of-the-art performance on a verb-object dataset of human compositionality ratings.</p><p>6 0.59877199 <a title="87-lsi-6" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>7 0.49374112 <a title="87-lsi-7" href="./emnlp-2013-Scaling_to_Large3_Data%3A_An_Efficient_and_Effective_Method_to_Compute_Distributional_Thesauri.html">165 emnlp-2013-Scaling to Large3 Data: An Efficient and Effective Method to Compute Distributional Thesauri</a></p>
<p>8 0.49023929 <a title="87-lsi-8" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>9 0.41999117 <a title="87-lsi-9" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>10 0.39513314 <a title="87-lsi-10" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>11 0.33389652 <a title="87-lsi-11" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>12 0.30956692 <a title="87-lsi-12" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>13 0.30201596 <a title="87-lsi-13" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>14 0.27632675 <a title="87-lsi-14" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>15 0.2598792 <a title="87-lsi-15" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>16 0.25050735 <a title="87-lsi-16" href="./emnlp-2013-Unsupervised_Induction_of_Contingent_Event_Pairs_from_Film_Scenes.html">192 emnlp-2013-Unsupervised Induction of Contingent Event Pairs from Film Scenes</a></p>
<p>17 0.24864495 <a title="87-lsi-17" href="./emnlp-2013-Learning_to_Rank_Lexical_Substitutions.html">123 emnlp-2013-Learning to Rank Lexical Substitutions</a></p>
<p>18 0.24805997 <a title="87-lsi-18" href="./emnlp-2013-Deriving_Adjectival_Scales_from_Continuous_Space_Word_Representations.html">59 emnlp-2013-Deriving Adjectival Scales from Continuous Space Word Representations</a></p>
<p>19 0.24510168 <a title="87-lsi-19" href="./emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM.html">99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</a></p>
<p>20 0.21986945 <a title="87-lsi-20" href="./emnlp-2013-Semantic_Parsing_on_Freebase_from_Question-Answer_Pairs.html">166 emnlp-2013-Semantic Parsing on Freebase from Question-Answer Pairs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.042), (6, 0.069), (18, 0.025), (22, 0.05), (30, 0.047), (45, 0.021), (50, 0.018), (51, 0.167), (64, 0.315), (66, 0.034), (71, 0.023), (75, 0.03), (77, 0.019), (90, 0.026), (96, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76906288 <a title="87-lda-1" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>Author: Angeliki Lazaridou ; Eva Maria Vecchi ; Marco Baroni</p><p>Abstract: In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the correct parsing of NPs (e.g., (live fish) transporter rather than live (fish transporter)). We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features.</p><p>2 0.65632671 <a title="87-lda-2" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>Author: Xiaoning Zhu ; Zhongjun He ; Hua Wu ; Haifeng Wang ; Conghui Zhu ; Tiejun Zhao</p><p>Abstract: This paper proposes a novel approach that utilizes a machine learning method to improve pivot-based statistical machine translation (SMT). For language pairs with few bilingual data, a possible solution in pivot-based SMT using another language as a</p><p>3 0.62511134 <a title="87-lda-3" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>Author: Uri Lerner ; Slav Petrov</p><p>Abstract: We present a simple and novel classifier-based preordering approach. Unlike existing preordering models, we train feature-rich discriminative classifiers that directly predict the target-side word order. Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discriminative model with a rich set of features, including lexical features. We present extensive experiments on 22 language pairs, including preordering into English from 7 other languages. We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task. For languages from different families the improvements often exceed 2 BLEU. Many of these gains are also significant in human evaluations.</p><p>4 0.53694522 <a title="87-lda-4" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>Author: Richard Socher ; Alex Perelygin ; Jean Wu ; Jason Chuang ; Christopher D. Manning ; Andrew Ng ; Christopher Potts</p><p>Abstract: Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.</p><p>5 0.53630763 <a title="87-lda-5" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>Author: Masashi Tsubaki ; Kevin Duh ; Masashi Shimbo ; Yuji Matsumoto</p><p>Abstract: We present a novel vector space model for semantic co-compositionality. Inspired by Generative Lexicon Theory (Pustejovsky, 1995), our goal is a compositional model where both predicate and argument are allowed to modify each others’ meaning representations while generating the overall semantics. This readily addresses some major challenges with current vector space models, notably the polysemy issue and the use of one representation per word type. We implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations. We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality. The model achieves the best result to date (ρ = 0.47) on the semantic similarity task of transitive verbs (Grefenstette and Sadrzadeh, 2011).</p><p>6 0.51471698 <a title="87-lda-6" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>7 0.51415664 <a title="87-lda-7" href="./emnlp-2013-Combining_PCFG-LA_Models_with_Dual_Decomposition%3A_A_Case_Study_with_Function_Labels_and_Binarization.html">50 emnlp-2013-Combining PCFG-LA Models with Dual Decomposition: A Case Study with Function Labels and Binarization</a></p>
<p>8 0.50988865 <a title="87-lda-8" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>9 0.50895053 <a title="87-lda-9" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>10 0.50867337 <a title="87-lda-10" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>11 0.50817478 <a title="87-lda-11" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>12 0.50619829 <a title="87-lda-12" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>13 0.50616497 <a title="87-lda-13" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>14 0.50595766 <a title="87-lda-14" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>15 0.5054028 <a title="87-lda-15" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>16 0.50487626 <a title="87-lda-16" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<p>17 0.50465202 <a title="87-lda-17" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>18 0.50461769 <a title="87-lda-18" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>19 0.50371265 <a title="87-lda-19" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>20 0.50304061 <a title="87-lda-20" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
