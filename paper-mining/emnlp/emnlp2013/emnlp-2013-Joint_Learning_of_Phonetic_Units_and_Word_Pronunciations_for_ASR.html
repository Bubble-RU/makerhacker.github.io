<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>115 emnlp-2013-Joint Learning of Phonetic Units and Word Pronunciations for ASR</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-115" href="#">emnlp2013-115</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>115 emnlp-2013-Joint Learning of Phonetic Units and Word Pronunciations for ASR</h1>
<br/><p>Source: <a title="emnlp-2013-115-pdf" href="http://aclweb.org/anthology//D/D13/D13-1019.pdf">pdf</a></p><p>Author: Chia-ying Lee ; Yu Zhang ; James Glass</p><p>Abstract: The creation of a pronunciation lexicon remains the most inefficient process in developing an Automatic Speech Recognizer (ASR). In this paper, we propose an unsupervised alternative requiring no language-specific knowledge to the conventional manual approach for creating pronunciation dictionaries. We present a hierarchical Bayesian model, which jointly discovers the phonetic inventory and the Letter-to-Sound (L2S) mapping rules in a language using only transcribed data. When tested on a corpus of spontaneous queries, the results demonstrate the superiority of the proposed joint learning scheme over its sequential counterpart, in which the latent phonetic inventory and L2S mappings are learned separately. Furthermore, the recognizers built with the automatically induced lexicon consistently outperform grapheme-based recognizers and even approach the performance of recognition systems trained using – – conventional supervised procedures.</p><p>Reference: <a title="emnlp-2013-115-reference" href="../emnlp2013_reference/emnlp-2013-Joint_Learning_of_Phonetic_Units_and_Word_Pronunciations_for_ASR_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we propose an unsupervised alternative requiring no language-specific knowledge to the conventional manual approach for creating pronunciation dictionaries. [sent-2, score-0.146]
</p><p>2 We present a hierarchical Bayesian model, which jointly discovers the phonetic inventory and the Letter-to-Sound (L2S) mapping rules in a language using only transcribed data. [sent-3, score-0.706]
</p><p>3 When tested on a corpus of spontaneous queries, the results demonstrate the superiority of the proposed joint learning scheme over its sequential counterpart, in which the latent phonetic inventory and L2S mappings are learned separately. [sent-4, score-0.634]
</p><p>4 Furthermore, the recognizers built with the automatically induced lexicon consistently outperform grapheme-based recognizers and even approach the performance of recognition systems trained using –  –  conventional supervised procedures. [sent-5, score-0.373]
</p><p>5 When creating a speech recognizer for a new language the usual requirements are: first, a large speech corpus with word-level annotations; second, a pronunciation dictionary that essentially defines a phonetic inventory  . [sent-7, score-1.052]
</p><p>6 One of the interesting aspects of this formulation is the inherent dependence on the dictionary, which defines both the phonetic inventory of a language, and the pronunciations of all the words in the vocabulary. [sent-14, score-0.772]
</p><p>7 The dictionary is arguably the cornerstone of a speech recognizer as it provides the essential transduction from sounds to words. [sent-15, score-0.32]
</p><p>8 Unfortunately, the dependency on this resource is a significant impediment to the creation of speech recognizers for new languages, since they are typically created by experts, whereas annotated corpora can be relatively more easily created by native speakers of a language. [sent-16, score-0.285]
</p><p>9 The existence of an expert-derived dictionary in the midst of stochastic speech recognition models is  somewhat ironic, and it is natural to ask why it continues to receive special status after all these years. [sent-17, score-0.146]
</p><p>10 Why can we not learn the inventory of sounds of a language and associated word pronunciations automatically, much as we learn our acoustic model parameters? [sent-18, score-0.716]
</p><p>11 hc o2d0s1 i3n A Nsastoucria lti Loan fgoura Cgoem Ppruotcaetsiosin agl, L piang eusis 1t8ic2s–192, limits us from having speech recognizers for all languages of the world, instead of the less than 2% that currently exist. [sent-21, score-0.307]
</p><p>12 In this paper, we investigate the problem of inferring a pronunciation lexicon from an annotated corpus without exploiting any language-specific knowledge. [sent-22, score-0.177]
</p><p>13 We formulate our approach as a hierarchical Bayesian model, which jointly discovers the acoustic inventory and the latent encoding scheme between the letters and the sounds of a language. [sent-23, score-0.674]
</p><p>14 We evaluate the quality of the induced lexicon and acoustic model through a series of speech recognition experiments on a conversational weather query corpus (Zue et al. [sent-24, score-0.427]
</p><p>15 The results demonstrate that our model consistently generates close perfor-  mance to recognizers that are trained with expertdefined phonetic inventory and lexicon. [sent-26, score-0.782]
</p><p>16 Finally, the joint learning framework proposed in this paper is proven to be much more effective than modeling the acoustic units and the letter-to-sound mappings separately, as shown in a 45% WER deduction our model achieves compared to a sequential approach. [sent-29, score-0.478]
</p><p>17 2  Related Work  Various algorithms for learning sub-word based pronunciations were proposed in (Lee et al. [sent-30, score-0.194]
</p><p>18 In these previous approaches, spoken samples of a word are gathered, and usually only one single pronunciation for the word is derived based on the acoustic evidence observed in the spoken samples. [sent-33, score-0.529]
</p><p>19 The major difference between our work and these previous works is that our model learns word pronunciations in the context of letter sequences. [sent-34, score-0.434]
</p><p>20 More specifically, our model learns letter pronunciations first and then concatenates the pro-  nunciation of each letter in a word to form the word pronunciation. [sent-35, score-0.674]
</p><p>21 The advantage of our approach is that pronunciation knowledge learned for a particular letter in some arbitrary word can subsequently be used to help learn the letter’s pronunciation in other words. [sent-36, score-0.532]
</p><p>22 This property allows our model to potentially learn better pronunciations for less frequent words. [sent-37, score-0.194]
</p><p>23 (2013) has made extensive use of self-organizing units for keyword spotting and other tasks for languages with limited linguistic resources. [sent-39, score-0.191]
</p><p>24 The latter work introduced a nonparametric Bayesian inference procedure for automatically learning acoustic units that is most similar to our current work except that our model also infers word pronunciations simultaneously. [sent-42, score-0.645]
</p><p>25 The concept of creating a speech recognizer for a language with only orthographically annotated speech data has also been explored previously by  means of graphemes. [sent-43, score-0.296]
</p><p>26 This approach has been shown to be effective for alphabetic languages with relatively straightforward grapheme to phoneme transformations and does not require any unsupervised learning of units or pronunciations (Killer et al. [sent-44, score-0.385]
</p><p>27 As we explain in later sections, grapheme-based systems can actually be regarded as a special case ofour model; therefore, we expect our model to have greater flexibilities for capturing pronunciation rules of graphemes. [sent-46, score-0.146]
</p><p>28 3  Model  The goal of our model is to induce a word pronunciation lexicon from spoken utterances and their corresponding word transcriptions. [sent-47, score-0.229]
</p><p>29 No other languagespecific knowledge is assumed to be available, including the phonetic inventory of the language. [sent-48, score-0.578]
</p><p>30 To achieve the goal, our model needs to solve the following two tasks: •  •  Discover the phonetic inventory. [sent-49, score-0.444]
</p><p>31 Reveal the latent mapping between the letters aRnedv etahel dthiesc loavteernetd m phonetic uentwitse. [sent-50, score-0.567]
</p><p>32 e  We propose a hierarchical Bayesian model for jointly discovering the two latent structures from an annotated speech corpus. [sent-51, score-0.176]
</p><p>33 Before presenting our model, we first describe the key latent and observed variables of the problem. [sent-52, score-0.106]
</p><p>34 Letter (lmi) We use lim to denote the ith letter observed in the word transcription of the mth training sample. [sent-53, score-0.505]
</p><p>35 To be sure, a training sample involves a speech utterance and its corresponding text transcription. [sent-54, score-0.21]
</p><p>36 The letter sequence composed of lim and its context, namely  l~mi,κ. [sent-55, score-0.31]
</p><p>37 lim−κ, ··· ,lmi−1,lim,lim+1, ··· ,lmi+κ, is denoted as Although lim is referred,· t·o· as a letter in this paper, it can represent any character observed in the text data, including space and symbols indicating sentence boundaries. [sent-56, score-0.353]
</p><p>38 For notation simplicity, we use Lκ to denote the set of letter sequences yof, length 2 Lκ + 1that appear in the dataset and use to denote the elements in Lκ. [sent-58, score-0.368]
</p><p>39 Number of Mapped Acoustic Units (nim) Each letter lim in the transcriptions is assumed to be mapped to a certain number of phonetic units. [sent-60, score-0.826]
</p><p>40 For example, the letter x in the word fox is mapped to 2 phonetic units /k/ and /s/, while the letter e in the word lake is mapped to 0 phonetic units. [sent-61, score-1.681]
</p><p>41 We denote this number as nim and limit its value to be 0, 1 or 2 in our model. [sent-62, score-0.162]
</p><p>42 The value of nim is always unobserved and needs to be inferred by the our model. [sent-63, score-0.098]
</p><p>43 Identity of the Acoustic Unit (cim,p) For each phonetic unit that lim maps to, we use cim,p, for 1 ≤ p ≤ nim, to denote the identity of the phonetic ru 1nit ≤. [sent-64, score-1.097]
</p><p>44 N po t≤e that the phonetic inventory that describes the data set is unknown to our model, and the identities of the phonetic units are associated with the acoustic units discovered automatically by our model. [sent-65, score-1.698]
</p><p>45 Speech Feature xtm The observed speech data in our problem are converted to a series of 25 ms 13dimensional MFCCs (Davis and Mermelstein, 1980) and their first- and second-order time derivatives at a 10 ms analysis rate. [sent-66, score-0.331]
</p><p>46 We use xtm ∈ R39 to denote the tth feature frame of the mth utte∈ran Rce. [sent-67, score-0.217]
</p><p>47 1 Generative Process  We present the generative process for a single training sample (i. [sent-69, score-0.082]
</p><p>48 , a speech utterance and its corresponding text transcription); to keep notation simple, we discard the index variable m in this section. [sent-71, score-0.237]
</p><p>49 For each li in the transcription, the model generates ni, given l~i,κ, from the 3-dimensional categorical distribution φl~i,κ (ni). [sent-72, score-0.204]
</p><p>50 Note that for every unique  l~i,κ  letter sequence, there is an associated φl~i,κ(ni) 184  Figure 1: The graphical representation of the proposed hierarchical Bayesian model. [sent-73, score-0.273]
</p><p>51 The shaded circle denotes the observed text and speech data, and the squares denote the hyperparameters of the priors in our model. [sent-74, score-0.306]
</p><p>52 distribution, which captures the fact that the number  of phonetic units a letter maps to may depend on its context. [sent-77, score-0.88]
</p><p>53 In our model, we impose a Dirichlet distribution prior Dir(η) on φl~i,κ (ni). [sent-78, score-0.063]
</p><p>54 1 shows that for each combination of ni and p, there is an unique categorical distribution. [sent-81, score-0.281]
</p><p>55 An important property of these categorical distributions is that they are coupled together such that their outcomes point to a consistent set of phonetic units. [sent-82, score-0.525]
</p><p>56 4, we envision that the observed speech data are generated by a Kcomponent mixture model, ofwhich the components correspond to the phonetic units in the language. [sent-86, score-0.849]
</p><p>57 2 can be viewed as the mixture weight over the components, which indicates how likely we are to observe each acoustic unit in the data overall. [sent-88, score-0.409]
</p><p>58 By adopting this point of view, we can also regard the mapping between li and the phonetic units as a mixture model, and πli,ni,p1 represents how probable li is mapped to each phonetic unit given ni and p. [sent-89, score-1.69]
</p><p>59 We apply a Dirichlet distribution prior parametrized by α0β to πli,ni,p as shown in Eq. [sent-90, score-0.063]
</p><p>60 With this parameterization, the mean of πli,ni,p is the global mixture weight β, and α0 controls how similar πli,ni,p is to the mean. [sent-92, score-0.079]
</p><p>61 4 shows that the prior for πl~i,κ,ni,p is seeded by pseudo-counts that are proportional to the mapping weights over the phonetic units of li in a shorter context. [sent-101, score-0.792]
</p><p>62 In other  words, the mapping distribution of li in a shorter context can be thought of as a back-off distribution of li’s mapping weights in a longer context. [sent-102, score-0.26]
</p><p>63 Each component of the K-dimensional mixture model is linked to a 3-state Hidden Markov Model (HMM). [sent-103, score-0.079]
</p><p>64 These K HMMs are used to model the phonetic units in the language (Jelinek, 1976). [sent-104, score-0.613]
</p><p>65 We use θc to represent the set of parameters that define the cth HMM, which includes the state transition probability and the GMM parameters of each state emission distribution. [sent-106, score-0.083]
</p><p>66 1; M is the total number of transcribed utterances in the corpus, and Lm is the number of letters in utterance m. [sent-111, score-0.171]
</p><p>67 The shaded circles denote the observed data, and the squares denote the hyperparameters of the priors used in our model. [sent-112, score-0.285]
</p><p>68 Lastly, the unshaded circles denote the latent variables of our model, for which we derive inference algorithms in the next section. [sent-113, score-0.156]
</p><p>69 , 2004) to approximate the posterior distribution of the latent variables in our model. [sent-115, score-0.137]
</p><p>70 In the following sections, we first present a message-passing algorithm for blocksampling ni and ci,p, and then describe how we leverage acoustic cues to accelerate the computation of the message-passing algorithm. [sent-116, score-0.561]
</p><p>71 Note that the block-sampling algorithm for ni and ci,p can be parallelized across utterances. [sent-117, score-0.23]
</p><p>72 1 Block-sampling  ni  and  ci,p  To understand the message-passing algorithm in this study, it is helpful to think of our model as a simplified Hidden Semi-Markov Model (HSMM), in which the letters represent the states and the speech features are the observations. [sent-120, score-0.402]
</p><p>73 However, unlike in a regular HSMM, where the state sequence is hidden, in our case, the state sequence is fixed to be the given letter sequence. [sent-121, score-0.296]
</p><p>74 With this point of view, we can modify the message-passing algorithms of Murphy (2002) and Johnson and Willsky (2013) to compute the posterior information required for blocksampling ni and ci,p. [sent-122, score-0.317]
</p><p>75 Let L(xt) be a function that returns the index of the letter from which xt is generated; also, let Ft = 1be a tag indicating that a new phone segment starts at t + 1. [sent-123, score-0.596]
</p><p>76 Given the constraint that 0 ≤ ni ≤ 2, fsotarr t0s ≤ ti ≤ Lm aennd t h0e ≤ nt ≤ Tm, tahte 0 b ≤ac nkw≤ard 2s, messages Bt(i) anadn Bt∗ (i) fo tr ≤ ≤the T mth training sample can be defined and computed as in Eq. [sent-124, score-0.308]
</p><p>77 Our inference algorithm only allows up ,to· ·U· ,lxetters to emit 0 acoustic units in a row. [sent-129, score-0.451]
</p><p>78 Bt∗ (i) contains the probability of all the alignments between xt+1:T and li+1:L that map xt+1 to li particularly. [sent-132, score-0.11]
</p><p>79 This alignment constraint between xt+1 and li is explicitly shown in the first term of Eq. [sent-133, score-0.084]
</p><p>80 6, which represents how likely the speech segment xt+1:t+d is generated by li given li’s context. [sent-134, score-0.227]
</p><p>81 This likelihood is simply the marginal probability of p(xt+1:t+d, ni, with ni and ci,p integrated out, which can be| expanded and computed as shown in the last three rows of Eq. [sent-135, score-0.23]
</p><p>82 The index v specifies where the phone boundary is between the two acoustic units that li is aligned with when ni = 2. [sent-137, score-0.92]
</p><p>83 9 specifies that at most U letters at the end of an sentence can be left unaligned with any speech features, while Eq. [sent-143, score-0.195]
</p><p>84 10 indicates that all of the speech features in an utterance must be assigned to a letter. [sent-144, score-0.177]
</p><p>85 The SampleFromBt(i) function in line 4 returns a random sample from the relative probability distribution composed by entries of the summation in Eq. [sent-147, score-0.069]
</p><p>86 Line 5 to line 9 check whether li (and maybe li+1) is mapped to zero phonetic units. [sent-149, score-0.6]
</p><p>87 nexti points to the letter that needs to be aligned with 1 or 2 phone segments starting from xt. [sent-150, score-0.469]
</p><p>88 The number of phonetic units that lnexti maps to and the identities of the units are sampled in SampleFromBt∗ (i). [sent-151, score-0.843]
</p><p>89 This subroutine generates a tuple of d, ni, hci,pi as well as v (if ni = 2) from all the entries o, fh cthei isau ms wmealtlio ans shown in Eq. [sent-152, score-0.312]
</p><p>90 3We use hci,pi to denote that hci,pi may consist of two numbers,W ci,1 sane dhc ci,2, twoh deenn ni = a2t. [sent-154, score-0.294]
</p><p>91 7 enumerate through every frame index in a sentence, treating each feature frame as a potential boundary between acoustic units. [sent-157, score-0.431]
</p><p>92 However, it is possible to exploit acoustic cues to avoid checking feature frames that are unlikely to be phonetic boundaries. [sent-158, score-0.753]
</p><p>93 Another heuristic applied to our algorithm to reduce the search space for d and v is based on the observation that the average duration of phonetic units is usually no longer than 300 ms. [sent-160, score-0.645]
</p><p>94 Therefore, when computing Bt∗ (i), we only consider speech segments that are shorter than 300 ms to avoid aligning letters to speech segments that are too long to be phonetic units. [sent-161, score-0.867]
</p><p>95 3  Sampling φl~κ,  Sampling φl~κ  πl~κ,ni,p,  β and θc  To compute the posterior distribu-  l~κ  tion of φl~κ, we count how many times is mapped to 0, 1 and 2 phonetic units from nim. [sent-163, score-0.723]
</p><p>96 More specifically, we define N~lκ (j) for 0 ≤ j ≤ 2 as follows:  N~lκ(j) =XMXLmδ(nim,j)δ(l~im,κ,l~κ) mX=1 Xi= X1  where we use δ(·) to denote the discrete Kronecker wdehletar. [sent-164, score-0.064]
</p><p>97 e W weith u N~lκ , we can simply sample a new evacklueer fdoerl φl~κ Wfritohm N the following distribution: φl~κ ∼ Dir(η + Nl~κ) Sampling πl~κ,n,p and β The posterior distributions of πl~κ,n,p and β are constructed recursively due to the hierarchical structure imposed on πl~κ,n,p and β. [sent-165, score-0.104]
</p><p>98 , πl~2,n,p given that κ is set to 2 in our model implementation, and then sample pseudo-counts for the π variables at higher hierarchies as well as β. [sent-168, score-0.067]
</p><p>99 More specifically, we define Cl~2,n,p(k) to be the  l~2  number of times that is mapped to n units and the unit in position p is the kth phonetic unit. [sent-170, score-0.733]
</p><p>100 Cl~2,n,p(k) =mXM=1XiL=m1δ(l~i,2,l~2)δ(nim,n)δ(cim,p,k) To derive the posterior distribution of πl~1,n,p analytically, we need to sample pseudo-counts Cl~1,n,p, awlyhitcicha il sy ,dwe fiene nde as ftool sloawmsp. [sent-172, score-0.129]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('phonetic', 0.444), ('bt', 0.382), ('acoustic', 0.282), ('letter', 0.24), ('xt', 0.234), ('ni', 0.23), ('pronunciations', 0.194), ('recognizers', 0.171), ('units', 0.169), ('nexti', 0.146), ('pronunciation', 0.146), ('inventory', 0.134), ('speech', 0.114), ('sounds', 0.106), ('nim', 0.098), ('samplefrombt', 0.098), ('li', 0.084), ('mixture', 0.079), ('mapped', 0.072), ('lim', 0.07), ('dir', 0.069), ('recognizer', 0.068), ('denote', 0.064), ('utterance', 0.063), ('glass', 0.058), ('letters', 0.058), ('phone', 0.055), ('hmm', 0.053), ('categorical', 0.051), ('generative', 0.049), ('ms', 0.049), ('blocksampling', 0.049), ('mermelstein', 0.049), ('mfccs', 0.049), ('xtm', 0.049), ('asr', 0.049), ('unit', 0.048), ('mth', 0.045), ('transcription', 0.043), ('observed', 0.043), ('gmm', 0.042), ('hmms', 0.042), ('hsmm', 0.042), ('yj', 0.042), ('boundary', 0.039), ('xd', 0.039), ('ky', 0.039), ('dirichlet', 0.039), ('posterior', 0.038), ('index', 0.038), ('xk', 0.037), ('bayesian', 0.037), ('davis', 0.036), ('frame', 0.036), ('distribution', 0.036), ('mapping', 0.036), ('jx', 0.034), ('identities', 0.034), ('variables', 0.034), ('generates', 0.033), ('hierarchical', 0.033), ('sample', 0.033), ('duration', 0.032), ('discovers', 0.032), ('shaded', 0.032), ('shorter', 0.032), ('dictionary', 0.032), ('squares', 0.031), ('lexicon', 0.031), ('outcomes', 0.03), ('cl', 0.029), ('segment', 0.029), ('spoken', 0.029), ('latent', 0.029), ('circles', 0.029), ('segments', 0.028), ('nk', 0.028), ('state', 0.028), ('maps', 0.027), ('prior', 0.027), ('derivatives', 0.027), ('emission', 0.027), ('transcribed', 0.027), ('mappings', 0.027), ('frames', 0.027), ('sampling', 0.027), ('alignments', 0.026), ('min', 0.024), ('changed', 0.023), ('tth', 0.023), ('specifies', 0.023), ('signal', 0.023), ('utterances', 0.023), ('lm', 0.023), ('languages', 0.022), ('discard', 0.022), ('priors', 0.022), ('il', 0.022), ('discovered', 0.022), ('gaussian', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999845 <a title="115-tfidf-1" href="./emnlp-2013-Joint_Learning_of_Phonetic_Units_and_Word_Pronunciations_for_ASR.html">115 emnlp-2013-Joint Learning of Phonetic Units and Word Pronunciations for ASR</a></p>
<p>Author: Chia-ying Lee ; Yu Zhang ; James Glass</p><p>Abstract: The creation of a pronunciation lexicon remains the most inefficient process in developing an Automatic Speech Recognizer (ASR). In this paper, we propose an unsupervised alternative requiring no language-specific knowledge to the conventional manual approach for creating pronunciation dictionaries. We present a hierarchical Bayesian model, which jointly discovers the phonetic inventory and the Letter-to-Sound (L2S) mapping rules in a language using only transcribed data. When tested on a corpus of spontaneous queries, the results demonstrate the superiority of the proposed joint learning scheme over its sequential counterpart, in which the latent phonetic inventory and L2S mappings are learned separately. Furthermore, the recognizers built with the automatically induced lexicon consistently outperform grapheme-based recognizers and even approach the performance of recognition systems trained using – – conventional supervised procedures.</p><p>2 0.24022447 <a title="115-tfidf-2" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>Author: Micha Elsner ; Sharon Goldwater ; Naomi Feldman ; Frank Wood</p><p>Abstract: We present a cognitive model of early lexical acquisition which jointly performs word segmentation and learns an explicit model of phonetic variation. We define the model as a Bayesian noisy channel; we sample segmentations and word forms simultaneously from the posterior, using beam sampling to control the size of the search space. Compared to a pipelined approach in which segmentation is performed first, our model is qualitatively more similar to human learners. On data with vari- able pronunciations, the pipelined approach learns to treat syllables or morphemes as words. In contrast, our joint model, like infant learners, tends to learn multiword collocations. We also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors, and relate these to developmental evidence.</p><p>3 0.18275538 <a title="115-tfidf-3" href="./emnlp-2013-Pair_Language_Models_for_Deriving_Alternative_Pronunciations_and_Spellings_from_Pronunciation_Dictionaries.html">150 emnlp-2013-Pair Language Models for Deriving Alternative Pronunciations and Spellings from Pronunciation Dictionaries</a></p>
<p>Author: Russell Beckley ; Brian Roark</p><p>Abstract: Pronunciation dictionaries provide a readily available parallel corpus for learning to transduce between character strings and phoneme strings or vice versa. Translation models can be used to derive character-level paraphrases on either side of this transduction, allowing for the automatic derivation of alternative pronunciations or spellings. We examine finitestate and SMT-based methods for these related tasks, and demonstrate that the tasks have different characteristics finding alternative spellings is harder than alternative pronunciations and benefits from round-trip algorithms when the other does not. We also show that we can increase accuracy by modeling syllable stress. –</p><p>4 0.060237568 <a title="115-tfidf-4" href="./emnlp-2013-A_Generative_Joint%2C_Additive%2C_Sequential_Model_of_Topics_and_Speech_Acts_in_Patient-Doctor_Communication.html">6 emnlp-2013-A Generative Joint, Additive, Sequential Model of Topics and Speech Acts in Patient-Doctor Communication</a></p>
<p>Author: Byron C. Wallace ; Thomas A Trikalinos ; M. Barton Laws ; Ira B. Wilson ; Eugene Charniak</p><p>Abstract: We develop a novel generative model of conversation that jointly captures both the topical content and the speech act type associated with each utterance. Our model expresses both token emission and state transition probabilities as log-linear functions of separate components corresponding to topics and speech acts (and their interactions). We apply this model to a dataset comprising annotated patient-physician visits and show that the proposed joint approach outperforms a baseline univariate model.</p><p>5 0.059342168 <a title="115-tfidf-5" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>Author: Stella Frank ; Frank Keller ; Sharon Goldwater</p><p>Abstract: Frank Keller keller@ inf .ed .ac .uk Sharon Goldwater sgwater@ inf .ed .ac .uk ILCC, School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK interactions are often (but not necessarily) synergisChildren learn various levels of linguistic structure concurrently, yet most existing models of language acquisition deal with only a single level of structure, implicitly assuming a sequential learning process. Developing models that learn multiple levels simultaneously can provide important insights into how these levels might interact synergistically dur- ing learning. Here, we present a model that jointly induces syntactic categories and morphological segmentations by combining two well-known models for the individual tasks. We test on child-directed utterances in English and Spanish and compare to single-task baselines. In the morphologically poorer language (English), the model improves morphological segmentation, while in the morphologically richer language (Spanish), it leads to better syntactic categorization. These results provide further evidence that joint learning is useful, but also suggest that the benefits may be different for typologically different languages.</p><p>6 0.052364487 <a title="115-tfidf-6" href="./emnlp-2013-Improvements_to_the_Bayesian_Topic_N-Gram_Models.html">100 emnlp-2013-Improvements to the Bayesian Topic N-Gram Models</a></p>
<p>7 0.048038129 <a title="115-tfidf-7" href="./emnlp-2013-Russian_Stress_Prediction_using_Maximum_Entropy_Ranking.html">162 emnlp-2013-Russian Stress Prediction using Maximum Entropy Ranking</a></p>
<p>8 0.045513362 <a title="115-tfidf-8" href="./emnlp-2013-Automatic_Knowledge_Acquisition_for_Case_Alternation_between_the_Passive_and_Active_Voices_in_Japanese.html">33 emnlp-2013-Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in Japanese</a></p>
<p>9 0.041808505 <a title="115-tfidf-9" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>10 0.041485343 <a title="115-tfidf-10" href="./emnlp-2013-Leveraging_Lexical_Cohesion_and_Disruption_for_Topic_Segmentation.html">124 emnlp-2013-Leveraging Lexical Cohesion and Disruption for Topic Segmentation</a></p>
<p>11 0.040575512 <a title="115-tfidf-11" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>12 0.039880645 <a title="115-tfidf-12" href="./emnlp-2013-A_Log-Linear_Model_for_Unsupervised_Text_Normalization.html">9 emnlp-2013-A Log-Linear Model for Unsupervised Text Normalization</a></p>
<p>13 0.03963602 <a title="115-tfidf-13" href="./emnlp-2013-Automatic_Extraction_of_Morphological_Lexicons_from_Morphologically_Annotated_Corpora.html">30 emnlp-2013-Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora</a></p>
<p>14 0.037134744 <a title="115-tfidf-14" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>15 0.036920562 <a title="115-tfidf-15" href="./emnlp-2013-Simulating_Early-Termination_Search_for_Verbose_Spoken_Queries.html">173 emnlp-2013-Simulating Early-Termination Search for Verbose Spoken Queries</a></p>
<p>16 0.035702109 <a title="115-tfidf-16" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>17 0.03360242 <a title="115-tfidf-17" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>18 0.032315828 <a title="115-tfidf-18" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>19 0.031341117 <a title="115-tfidf-19" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>20 0.03103352 <a title="115-tfidf-20" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.128), (1, -0.027), (2, -0.031), (3, -0.027), (4, -0.092), (5, -0.022), (6, 0.025), (7, 0.09), (8, -0.047), (9, -0.018), (10, -0.051), (11, -0.043), (12, 0.026), (13, 0.16), (14, 0.073), (15, 0.022), (16, 0.253), (17, -0.164), (18, 0.136), (19, 0.218), (20, 0.013), (21, -0.069), (22, 0.241), (23, -0.092), (24, 0.133), (25, 0.104), (26, 0.005), (27, 0.105), (28, -0.074), (29, -0.008), (30, 0.02), (31, 0.074), (32, -0.025), (33, -0.063), (34, -0.08), (35, -0.004), (36, 0.016), (37, 0.078), (38, -0.084), (39, 0.058), (40, -0.045), (41, 0.151), (42, 0.133), (43, 0.112), (44, -0.05), (45, 0.021), (46, -0.027), (47, 0.052), (48, -0.057), (49, 0.102)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96736002 <a title="115-lsi-1" href="./emnlp-2013-Joint_Learning_of_Phonetic_Units_and_Word_Pronunciations_for_ASR.html">115 emnlp-2013-Joint Learning of Phonetic Units and Word Pronunciations for ASR</a></p>
<p>Author: Chia-ying Lee ; Yu Zhang ; James Glass</p><p>Abstract: The creation of a pronunciation lexicon remains the most inefficient process in developing an Automatic Speech Recognizer (ASR). In this paper, we propose an unsupervised alternative requiring no language-specific knowledge to the conventional manual approach for creating pronunciation dictionaries. We present a hierarchical Bayesian model, which jointly discovers the phonetic inventory and the Letter-to-Sound (L2S) mapping rules in a language using only transcribed data. When tested on a corpus of spontaneous queries, the results demonstrate the superiority of the proposed joint learning scheme over its sequential counterpart, in which the latent phonetic inventory and L2S mappings are learned separately. Furthermore, the recognizers built with the automatically induced lexicon consistently outperform grapheme-based recognizers and even approach the performance of recognition systems trained using – – conventional supervised procedures.</p><p>2 0.73169041 <a title="115-lsi-2" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>Author: Micha Elsner ; Sharon Goldwater ; Naomi Feldman ; Frank Wood</p><p>Abstract: We present a cognitive model of early lexical acquisition which jointly performs word segmentation and learns an explicit model of phonetic variation. We define the model as a Bayesian noisy channel; we sample segmentations and word forms simultaneously from the posterior, using beam sampling to control the size of the search space. Compared to a pipelined approach in which segmentation is performed first, our model is qualitatively more similar to human learners. On data with vari- able pronunciations, the pipelined approach learns to treat syllables or morphemes as words. In contrast, our joint model, like infant learners, tends to learn multiword collocations. We also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors, and relate these to developmental evidence.</p><p>3 0.67955101 <a title="115-lsi-3" href="./emnlp-2013-Pair_Language_Models_for_Deriving_Alternative_Pronunciations_and_Spellings_from_Pronunciation_Dictionaries.html">150 emnlp-2013-Pair Language Models for Deriving Alternative Pronunciations and Spellings from Pronunciation Dictionaries</a></p>
<p>Author: Russell Beckley ; Brian Roark</p><p>Abstract: Pronunciation dictionaries provide a readily available parallel corpus for learning to transduce between character strings and phoneme strings or vice versa. Translation models can be used to derive character-level paraphrases on either side of this transduction, allowing for the automatic derivation of alternative pronunciations or spellings. We examine finitestate and SMT-based methods for these related tasks, and demonstrate that the tasks have different characteristics finding alternative spellings is harder than alternative pronunciations and benefits from round-trip algorithms when the other does not. We also show that we can increase accuracy by modeling syllable stress. –</p><p>4 0.46710137 <a title="115-lsi-4" href="./emnlp-2013-Russian_Stress_Prediction_using_Maximum_Entropy_Ranking.html">162 emnlp-2013-Russian Stress Prediction using Maximum Entropy Ranking</a></p>
<p>Author: Keith Hall ; Richard Sproat</p><p>Abstract: We explore a model of stress prediction in Russian using a combination of local contextual features and linguisticallymotivated features associated with the word’s stem and suffix. We frame this as a ranking problem, where the objective is to rank the pronunciation with the correct stress above those with incorrect stress. We train our models using a simple Maximum Entropy ranking framework allowing for efficient prediction. An empirical evaluation shows that a model combining the local contextual features and the linguistically-motivated non-local features performs best in identifying both primary and secondary stress. 1</p><p>5 0.38486552 <a title="115-lsi-5" href="./emnlp-2013-Automatic_Knowledge_Acquisition_for_Case_Alternation_between_the_Passive_and_Active_Voices_in_Japanese.html">33 emnlp-2013-Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in Japanese</a></p>
<p>Author: Ryohei Sasano ; Daisuke Kawahara ; Sadao Kurohashi ; Manabu Okumura</p><p>Abstract: We present a method for automatically acquiring knowledge for case alternation between the passive and active voices in Japanese. By leveraging several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, our method aligns a case frame in the passive voice to a corresponding case frame in the active voice and finds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness.</p><p>6 0.31261477 <a title="115-lsi-6" href="./emnlp-2013-A_Generative_Joint%2C_Additive%2C_Sequential_Model_of_Topics_and_Speech_Acts_in_Patient-Doctor_Communication.html">6 emnlp-2013-A Generative Joint, Additive, Sequential Model of Topics and Speech Acts in Patient-Doctor Communication</a></p>
<p>7 0.2766448 <a title="115-lsi-7" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>8 0.24868938 <a title="115-lsi-8" href="./emnlp-2013-A_Log-Linear_Model_for_Unsupervised_Text_Normalization.html">9 emnlp-2013-A Log-Linear Model for Unsupervised Text Normalization</a></p>
<p>9 0.23936366 <a title="115-lsi-9" href="./emnlp-2013-Elephant%3A_Sequence_Labeling_for_Word_and_Sentence_Segmentation.html">72 emnlp-2013-Elephant: Sequence Labeling for Word and Sentence Segmentation</a></p>
<p>10 0.23326847 <a title="115-lsi-10" href="./emnlp-2013-Improvements_to_the_Bayesian_Topic_N-Gram_Models.html">100 emnlp-2013-Improvements to the Bayesian Topic N-Gram Models</a></p>
<p>11 0.23019664 <a title="115-lsi-11" href="./emnlp-2013-Leveraging_Lexical_Cohesion_and_Disruption_for_Topic_Segmentation.html">124 emnlp-2013-Leveraging Lexical Cohesion and Disruption for Topic Segmentation</a></p>
<p>12 0.22348067 <a title="115-lsi-12" href="./emnlp-2013-Unsupervised_Spectral_Learning_of_WCFG_as_Low-rank_Matrix_Completion.html">195 emnlp-2013-Unsupervised Spectral Learning of WCFG as Low-rank Matrix Completion</a></p>
<p>13 0.22328192 <a title="115-lsi-13" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>14 0.2126015 <a title="115-lsi-14" href="./emnlp-2013-This_Text_Has_the_Scent_of_Starbucks%3A_A_Laplacian_Structured_Sparsity_Model_for_Computational_Branding_Analytics.html">184 emnlp-2013-This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</a></p>
<p>15 0.20190799 <a title="115-lsi-15" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>16 0.19828489 <a title="115-lsi-16" href="./emnlp-2013-Simulating_Early-Termination_Search_for_Verbose_Spoken_Queries.html">173 emnlp-2013-Simulating Early-Termination Search for Verbose Spoken Queries</a></p>
<p>17 0.19162196 <a title="115-lsi-17" href="./emnlp-2013-Predicting_the_Resolution_of_Referring_Expressions_from_User_Behavior.html">153 emnlp-2013-Predicting the Resolution of Referring Expressions from User Behavior</a></p>
<p>18 0.19112007 <a title="115-lsi-18" href="./emnlp-2013-Centering_Similarity_Measures_to_Reduce_Hubs.html">44 emnlp-2013-Centering Similarity Measures to Reduce Hubs</a></p>
<p>19 0.18677151 <a title="115-lsi-19" href="./emnlp-2013-With_Blinkers_on%3A_Robust_Prediction_of_Eye_Movements_across_Readers.html">203 emnlp-2013-With Blinkers on: Robust Prediction of Eye Movements across Readers</a></p>
<p>20 0.18181138 <a title="115-lsi-20" href="./emnlp-2013-The_VerbCorner_Project%3A_Toward_an_Empirically-Based_Semantic_Decomposition_of_Verbs.html">183 emnlp-2013-The VerbCorner Project: Toward an Empirically-Based Semantic Decomposition of Verbs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.034), (9, 0.023), (18, 0.032), (22, 0.04), (30, 0.086), (50, 0.018), (51, 0.129), (61, 0.372), (66, 0.043), (71, 0.022), (75, 0.027), (77, 0.038), (90, 0.019), (96, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77835411 <a title="115-lda-1" href="./emnlp-2013-Joint_Learning_of_Phonetic_Units_and_Word_Pronunciations_for_ASR.html">115 emnlp-2013-Joint Learning of Phonetic Units and Word Pronunciations for ASR</a></p>
<p>Author: Chia-ying Lee ; Yu Zhang ; James Glass</p><p>Abstract: The creation of a pronunciation lexicon remains the most inefficient process in developing an Automatic Speech Recognizer (ASR). In this paper, we propose an unsupervised alternative requiring no language-specific knowledge to the conventional manual approach for creating pronunciation dictionaries. We present a hierarchical Bayesian model, which jointly discovers the phonetic inventory and the Letter-to-Sound (L2S) mapping rules in a language using only transcribed data. When tested on a corpus of spontaneous queries, the results demonstrate the superiority of the proposed joint learning scheme over its sequential counterpart, in which the latent phonetic inventory and L2S mappings are learned separately. Furthermore, the recognizers built with the automatically induced lexicon consistently outperform grapheme-based recognizers and even approach the performance of recognition systems trained using – – conventional supervised procedures.</p><p>2 0.6941883 <a title="115-lda-2" href="./emnlp-2013-Leveraging_Lexical_Cohesion_and_Disruption_for_Topic_Segmentation.html">124 emnlp-2013-Leveraging Lexical Cohesion and Disruption for Topic Segmentation</a></p>
<p>Author: Anca-Roxana Simon ; Guillaume Gravier ; Pascale Sebillot</p><p>Abstract: Topic segmentation classically relies on one of two criteria, either finding areas with coherent vocabulary use or detecting discontinuities. In this paper, we propose a segmentation criterion combining both lexical cohesion and disruption, enabling a trade-off between the two. We provide the mathematical formulation of the criterion and an efficient graph based decoding algorithm for topic segmentation. Experimental results on standard textual data sets and on a more challenging corpus of automatically transcribed broadcast news shows demonstrate the benefit of such a combination. Gains were observed in all conditions, with segments of either regular or varying length and abrupt or smooth topic shifts. Long segments benefit more than short segments. However the algorithm has proven robust on automatic transcripts with short segments and limited vocabulary reoccurrences.</p><p>3 0.58833849 <a title="115-lda-3" href="./emnlp-2013-Pair_Language_Models_for_Deriving_Alternative_Pronunciations_and_Spellings_from_Pronunciation_Dictionaries.html">150 emnlp-2013-Pair Language Models for Deriving Alternative Pronunciations and Spellings from Pronunciation Dictionaries</a></p>
<p>Author: Russell Beckley ; Brian Roark</p><p>Abstract: Pronunciation dictionaries provide a readily available parallel corpus for learning to transduce between character strings and phoneme strings or vice versa. Translation models can be used to derive character-level paraphrases on either side of this transduction, allowing for the automatic derivation of alternative pronunciations or spellings. We examine finitestate and SMT-based methods for these related tasks, and demonstrate that the tasks have different characteristics finding alternative spellings is harder than alternative pronunciations and benefits from round-trip algorithms when the other does not. We also show that we can increase accuracy by modeling syllable stress. –</p><p>4 0.42892474 <a title="115-lda-4" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>Author: Micha Elsner ; Sharon Goldwater ; Naomi Feldman ; Frank Wood</p><p>Abstract: We present a cognitive model of early lexical acquisition which jointly performs word segmentation and learns an explicit model of phonetic variation. We define the model as a Bayesian noisy channel; we sample segmentations and word forms simultaneously from the posterior, using beam sampling to control the size of the search space. Compared to a pipelined approach in which segmentation is performed first, our model is qualitatively more similar to human learners. On data with vari- able pronunciations, the pipelined approach learns to treat syllables or morphemes as words. In contrast, our joint model, like infant learners, tends to learn multiword collocations. We also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors, and relate these to developmental evidence.</p><p>5 0.42675796 <a title="115-lda-5" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>Author: Xiaoqing Zheng ; Hanyang Chen ; Tianyu Xu</p><p>Abstract: This study explores the feasibility of performing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representation of Chinese characters, and use these improved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-theart performance with minimal computational cost. We also describe a perceptron-style algorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented.</p><p>6 0.42270941 <a title="115-lda-6" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>7 0.42098868 <a title="115-lda-7" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>8 0.42057917 <a title="115-lda-8" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>9 0.42010897 <a title="115-lda-9" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>10 0.41961166 <a title="115-lda-10" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>11 0.4191736 <a title="115-lda-11" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>12 0.41750604 <a title="115-lda-12" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>13 0.41670737 <a title="115-lda-13" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>14 0.41654417 <a title="115-lda-14" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>15 0.41594905 <a title="115-lda-15" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>16 0.41561356 <a title="115-lda-16" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>17 0.41552487 <a title="115-lda-17" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>18 0.41525981 <a title="115-lda-18" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>19 0.41493014 <a title="115-lda-19" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>20 0.41353449 <a title="115-lda-20" href="./emnlp-2013-Dynamic_Feature_Selection_for_Dependency_Parsing.html">66 emnlp-2013-Dynamic Feature Selection for Dependency Parsing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
