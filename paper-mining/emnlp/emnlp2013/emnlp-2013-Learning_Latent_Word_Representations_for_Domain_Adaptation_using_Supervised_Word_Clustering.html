<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-120" href="#">emnlp2013-120</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</h1>
<br/><p>Source: <a title="emnlp-2013-120-pdf" href="http://aclweb.org/anthology//D/D13/D13-1016.pdf">pdf</a></p><p>Author: Min Xiao ; Feipeng Zhao ; Yuhong Guo</p><p>Abstract: Domain adaptation has been popularly studied on exploiting labeled information from a source domain to learn a prediction model in a target domain. In this paper, we develop a novel representation learning approach to address domain adaptation for text classification with automatically induced discriminative latent features, which are generalizable across domains while informative to the prediction task. Specifically, we propose a hierarchical multinomial Naive Bayes model with latent variables to conduct supervised word clustering on labeled documents from both source and target domains, and then use the produced cluster distribution of each word as its latent feature representation for domain adaptation. We train this latent graphical model us- ing a simple expectation-maximization (EM) algorithm. We empirically evaluate the proposed method with both cross-domain document categorization tasks on Reuters-21578 dataset and cross-domain sentiment classification tasks on Amazon product review dataset. The experimental results demonstrate that our proposed approach achieves superior performance compared with alternative methods.</p><p>Reference: <a title="emnlp-2013-120-reference" href="../emnlp2013_reference/emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract Domain adaptation has been popularly studied on exploiting labeled information from a source domain to learn a prediction model in a target domain. [sent-2, score-0.918]
</p><p>2 In this paper, we develop a novel representation learning approach to address domain adaptation for text classification with automatically induced discriminative latent features, which are generalizable across domains while informative to the prediction task. [sent-3, score-1.157]
</p><p>3 We empirically evaluate the proposed method with both cross-domain document categorization tasks on Reuters-21578 dataset and cross-domain sentiment classification tasks on Amazon product review dataset. [sent-6, score-0.729]
</p><p>4 edu  the prediction model training in the target domain (Ben-David et al. [sent-9, score-0.353]
</p><p>5 As an effective tool to reduce annotation effort, domain adaptation has achieved success in various crossdomain natural language processing (NLP) systems such as document categorization (Dai et al. [sent-14, score-0.778]
</p><p>6 One primary challenge of domain adaptation lies in the distribution divergence of the two domains in the original feature representation space. [sent-20, score-0.77]
</p><p>7 For example, documents about books may contain very different high-frequency words and discriminative words from documents about kitchen. [sent-21, score-0.399]
</p><p>8 A good crossdomain feature representation thus has been viewed as critical for bridging the domain divergence gap and facilitating domain adaptation in the NLP area (Ben-David et al. [sent-22, score-0.928]
</p><p>9 Many domain adaptation works have been proposed to learn new cross-domain feature representations (Blitzer et al. [sent-24, score-0.604]
</p><p>10 Though demonstrated good performance on certain problems, these works mostly induce new feature representations in an unsupervised Supervised prediction models typically require a large amount of labeled data for training. [sent-26, score-0.216]
</p><p>11 However, manually collecting data annotations is expensive in many real-world applications such as document categorization or sentiment classification. [sent-27, score-0.383]
</p><p>12 Recently, domain adaptation has been proposed to exploit existing labeled data in a related source domain to assist  way, without taking the valuable label information into account. [sent-28, score-1.083]
</p><p>13 In this work, we present a novel supervised representation learning approach to discover a latent representation of words which is not only generalizable across domains but also informative to the classification task. [sent-29, score-0.794]
</p><p>14 hc o2d0s1 i3n A Nsastoucriaalti Loann fgoura Cgoem Ppruotcaetsiosinnagl, L pianggeusis 1t5ic2s–162, archical multinomial Naive Bayes model with latent word cluster variables to perform supervised word clustering on labeled documents from both domains. [sent-32, score-0.909]
</p><p>15 Our model directly models the relationships between the observed document label variables and  the latent word cluster variables. [sent-33, score-0.412]
</p><p>16 The induced cluster representation of each word thus will be informative for the classification labels, and hence discriminative for the target classification task. [sent-34, score-0.568]
</p><p>17 We train this directed graphical model using an expectationmaximization (EM) algorithm, which maximizes the log-likelihood of the observations of labeled documents. [sent-35, score-0.279]
</p><p>18 The induced cluster distribution of each word can then be used as its generalizable representation to construct new cluster-based representation of each document. [sent-36, score-0.482]
</p><p>19 For domain adaptation, we train a supervised learning system with labeled data from both domains in the new representation space and apply it to categorize test documents in the target domain. [sent-37, score-1.032]
</p><p>20 In order to evaluate the proposed technique, we conduct extensive experiments on the Reuters21578 dataset for cross-domain document categorization and on Amazon product review dataset for cross-domain sentiment classification. [sent-38, score-0.527]
</p><p>21 The experimental results show the proposed approach can produce more effective representations than the comparison domain adaptation methods. [sent-39, score-0.604]
</p><p>22 2  Related Work  Domain adaptation has recently been popularly studied in natural language processing and a variety of domain adaptation approaches have been developed, including instance weighting adaptation methods and feature representation learning methods. [sent-40, score-1.201]
</p><p>23 Instance weighting adaptation methods improve the transferability of a prediction model by training an instance weighted learning system. [sent-41, score-0.286]
</p><p>24 Jiang and Zhai (2007) applied instance weighting algorithms to tackle cross-domain NLP tasks and proposed to remove misleading source training data and assign less weights to labeled data from the source domain than labeled data from the target domain. [sent-45, score-0.985]
</p><p>25 (2007) proposed to increase the weights of mistakenly predicted instances from the target domain and decrease the weights of incor153 rectly predicted instances from the source domain during an iterative training process. [sent-47, score-0.883]
</p><p>26 Representation learning methods bridge domain divergence either by differentiating domain-  invariant features from domain-specific features (Daum ´e III, 2007; Daum e´ III et al. [sent-48, score-0.285]
</p><p>27 , 2011; Finkel and Manning, 2009) or seeking generalizable latent features across domains (Blitzer et al. [sent-50, score-0.434]
</p><p>28 (2010) proposed a simple heuristic feature replication method to represent common, source specific and target specific features. [sent-53, score-0.219]
</p><p>29 It uses the correlation to induce latent domain-invariant features as augmenting features for supervised learning. [sent-59, score-0.237]
</p><p>30 However, unlike the unsupervised representation learning methods reviewed above, our proposed approach learns generalizable feature representations of words by exploiting data labels from the two domains. [sent-63, score-0.323]
</p><p>31 3  Learning Latent Word Representations using Supervised Word Clustering  In this paper, we address domain adaptation for text classification. [sent-64, score-0.5]
</p><p>32 Given a source domain DS with plenty ossfi fliacbaetileodn documents, arcned a target domain DT with a very few labeled documents, the task is tDo learn a classifier from the labeled documents in both domains, and use it to classify the unlabeled documents in the target domain. [sent-65, score-1.413]
</p><p>33 The documents in the two domains share the same universal vocabulary V = {w1, w2, · · · , wn}, but the word distrilbaurytioVn s i=n t {hwe two d,·o·m·a ,iwns are typically drdiff deirsetnrti. [sent-66, score-0.367]
</p><p>34 Therefore, training the classification model directly from the original word feature space V may not genferroalmize th we oelril giinn tahle w target datoumrea sinp. [sent-67, score-0.229]
</p><p>35 We propose to address this problem by first learn-  ing a supervised mapping function φ : V −→ Z firnogm a th suep plaerbveisleedd d mocaupmpienngts f uinn bctoitohn domains, w→hic Zh maps the input word features in the large vocabulary set V into a low dimensional latent feature space Zlar. [sent-68, score-0.294]
</p><p>36 By filtering olouwt unimportant ldaetteanitls f aantudr noises, we expect rthineg lo ouwt udinmimepnosirotnaanlt mapping can capture the intrinsic structure of the input data that is discriminative for the classification task and generalizable across domains. [sent-69, score-0.322]
</p><p>37 In particular, we learn such a mapping function by conducting supervised word clustering on the labeled documents using a hierarchical multinomial Naive Bayes model. [sent-70, score-0.649]
</p><p>38 Below, we will first introduce this supervised word clustering model and then use the mapping function produced to transform documents in different domains into the same low-dimensional space for training cross domain text classification systems. [sent-71, score-0.987]
</p><p>39 The latent variable Ct,i denotes the cluster membership of the word Wt,i, and all the cluster variables, share the same set of conditional dis-  {Ct,i}tT=,N1,ti=1,  tributions {θC|y}yK=1 across documents and words. [sent-75, score-0.553]
</p><p>40 ical models, we can see that given the cluster variable values, the document label variables will be completely independent of the word variables. [sent-80, score-0.309]
</p><p>41 By learning this latent directed graphical model, we thus expect the important classification information expressed in the input observation words can be effectively summarized into the latent cluster variables. [sent-81, score-0.506]
</p><p>42 This latent model is much simpler than the supervised topic models (Blei and McAuliffe, 2007), but we will show later that it can suitably produce a generalizable feature mapping function for domain adaptation. [sent-82, score-0.626]
</p><p>43 To train the latent graphical model in Fig-  ure 1 on labeled documents D, we use a standard expectation-maximization (EM) algorithm (Dempster et al. [sent-83, score-0.498]
</p><p>44 2 Induced Word Representation After training the supervised clustering model using EM algorithm, a set of local optimal model parameters θ∗ will be returned, which define a joint distribution over the three groups of variables in the directed graphical model. [sent-88, score-0.352]
</p><p>45 can be viewed as a soft word clustering matrix, and Πi,j denotes the probability of word wi belongs to  Π  Π ΠΠ  the j-th cluster. [sent-94, score-0.27]
</p><p>46 Given the original document-word frequency matrix Xtr ∈ RT×n for the labeled training documents from th∈e Rtwo domains, we can construct its representations Ztr ∈ RT×m in the predictive latent clustering space by performing the following transformation: Ztr = XtrΠ. [sent-95, score-0.597]
</p><p>47 We then train a classification model on the labeled data Ztr and apply it to classify the test data Zts. [sent-97, score-0.303]
</p><p>48 4  Experiments  We evaluate the proposed approach with experiments on cross domain document categorization of Reuters data and cross domain sentiment classification of Amazon product reviews, comparing to a number of baseline and existing domain adaptation methods. [sent-98, score-1.615]
</p><p>49 1 Approaches We compared our proposed supervised word clustering approach (SWC) with the following five comparison methods for domain adaptation: (1) BOW: This is a bag-of-word baseline method, which trains a SVM classifier with labeled data from both domains using the original bag-ofword features. [sent-101, score-0.886]
</p><p>50 (3) FDLDA: This is an alternative supervised word clustering method we built by training the Fast-Discriminative Latent Dirichlet Allocation model (Shan et al. [sent-103, score-0.243]
</p><p>51 After training the model, we used the learned topic distribution p(z) and the conditional word distributions p(w|z) to compute tihtieo ncaolnd witoiorndal d idsitsrtirbiubutitoionns over topics  p(z|w) for each word as the soft clustering ofthe pw(ozr|dw. [sent-105, score-0.24]
</p><p>52 (4) SCL: This is the structural correspondence learning based domain adaptation method (Blitzer et al. [sent-107, score-0.569]
</p><p>53 It first induces generalizable features with all data from both domains by modeling the correlations between pivot features and non-pivot features, and then uses the produced generalizable features as augmenting features to train SVM classifiers. [sent-109, score-0.583]
</p><p>54 (5) CPSP: This is coupled subspace learning based domain adaptation method (Blitzer et al. [sent-110, score-0.567]
</p><p>55 It first learns two domain projectors using all data from the two domains by approximating multi-view dimensionality reduction, and then projects the labeled data to low dimensional latent feature space to train SVM Classifiers. [sent-112, score-0.76]
</p><p>56 deviation) for three cross-domain  document  categorization tasks on  PO eorgpslev TsvasPk eloacpel s 7613. [sent-115, score-0.282]
</p><p>57 , 2007), which contains three crossdomain document categorization tasks, Orgs vs People, Orgs vs Places, People vs Places. [sent-129, score-0.656]
</p><p>58 The source and target domains of each task contain documents sampled from different non-overlapping subcategories. [sent-130, score-0.468]
</p><p>59 From example, the task of Orgs vs People assigns a document into one of the two top categories (Orgs, People), and the source domain documents and the target domain documents are sampled from different subcategories of Orgs and People. [sent-131, score-1.174]
</p><p>60 There are 1237 source documents and 1208 target documents for the task of Orgs vs People, 1016 source documents and 1043 target documents for the task of Orgs vs Places, and 1077 source documents and 1077 target documents for the task ofPeople vs Places. [sent-132, score-1.764]
</p><p>61 For each task, we built a unigram vocabulary based on all the documents from the two domains and represented each document as a feature vector containing term frequency values. [sent-133, score-0.413]
</p><p>62 , the cluster number in our proposed approach, from {5, 10, 20, 50, 100} according to the average c flraomssif {ic5a,ti1o0n, 2 r0e,su5l0ts, over }3 a runs on gth teo ta thske of Orgs vs People. [sent-140, score-0.301]
</p><p>63 We then repeated each experiment 10 times on each task with different random selections ofthe 100 labeled target documents to compare the six comparison approaches. [sent-142, score-0.49]
</p><p>64 We can see that by simply combining labeled documents from the two domains without adaptation, the BOW method performs poorly across the three tasks. [sent-144, score-0.518]
</p><p>65 The supervised word clustering method FDLDA, though performing slightly better than the unsupervised clustering method PLSA, produces poor performance comparing to the proposed SWC method. [sent-146, score-0.429]
</p><p>66 One possible reason is that the FDLDA model is not specialized for supervised word clustering, and it uses a logistic regression model to predict the labels from the word topics, while the final soft word clustering is computed from the learned distribution p(z) and p(w|z). [sent-147, score-0.356]
</p><p>67 e Tnhceat tt ihse, word clusterings indirectly and hence its influence can be much smaller than the influence of labels as direct parent variables of the word cluster variables in the SWC model. [sent-149, score-0.404]
</p><p>68 The two domain adaptation approaches, SCL and CPSP, both produce significant improvements over BOW, PLSA and FDLDA on the two tasks of Orgs vs People and Orgs vs Places,  while the CPSP method produces slightly inferior performance than PLSA and FDLDA on the task of People vs Places. [sent-150, score-0.941]
</p><p>69 the number of clusters for the three cross-domain document categorization tasks on Reuters-21578 dataset. [sent-155, score-0.363]
</p><p>70 2  Document Categorization Accuracy vs Label Complexity in Target Domain We next conducted experiments to compare the six approaches by varying the amount of the labeled data from the target domain. [sent-165, score-0.423]
</p><p>71 For each different s value, we repeated the experiments 10 times by randomly selecting s labeled documents from the target domain using the same experimental setting as before. [sent-167, score-0.712]
</p><p>72 We can see that in general the performance of  each method improves with the increase of the number of labeled documents from the target domain. [sent-169, score-0.419]
</p><p>73 The proposed method SWC and the domain adaptation method SCL clearly outperform the other four methods. [sent-170, score-0.559]
</p><p>74 Moreover, the proposed method SWC not 157 only maintains consistent and significant advantages over all other methods across the range of different s values, its performance with 300 labeled target instances is even superior to the other methods with 500 labeled target instances. [sent-171, score-0.739]
</p><p>75 3 Experiments on Amazon Product Reviews We conducted cross-domain sentiment classification on the widely used Amazon product reviews (Blitzer et al. [sent-174, score-0.442]
</p><p>76 We constructed 12 cross-domain sentiment classification tasks, one for each source-target domain pair, B2D,  B2E, B2K, D2B, D2E, D2K, E2B, E2D, E2K, K2B, K2D, K2E. [sent-177, score-0.524]
</p><p>77 For example, the task B2D means that we use the Books reviews as the source domain and the DVD reviews as the target domain. [sent-178, score-0.592]
</p><p>78 For each pair of domains, we built a vocabulary with both unigram and bigram features extracted from all the documents of the two domains, and then represented each review document as a feature vector with term frequency values. [sent-179, score-0.256]
</p><p>79 For the baseline method BOW, we used binary indicator values as features, which has been shown to work better than the term-frequency features for sentiment classification tasks (Pang et al. [sent-183, score-0.331]
</p><p>80 Orgs vs People  Orgs vs Places  #Labeled instances  #Labeled instances  People vs Places  #Labeled instances Figure 3: Average classification results for three cross-domain document categorization tasks on Reuters-21578 dataset by varying the amount of labeled training data from the target domain. [sent-191, score-1.26]
</p><p>81 Table 2: Average results (accuracy±standard deviation) for twelve cross-domain sentiment classification tasks on ATambalezo 2n: product ere rveiseuwltss. [sent-192, score-0.514]
</p><p>82 We then repeated each experiment 10 times based on different random selections of 100 labeled reviews from the target domain to compare the six methods on the twelve tasks. [sent-193, score-0.809]
</p><p>83 We also conducted sensitivity analysis over the 158 proposed approach regarding the number of clusters on the twelve cross-domain sentiment classification tasks, by testing a set of cluster number values m = {5, 10, 20, 50, 100}. [sent-198, score-0.73]
</p><p>84 For example, one can see that it is much easier to conduct domain adaptation from Kitchen to Electronics (with an accuracy around 84%) than from Kitchen to Books (with an accuracy around 75%), as Kitchen is more closely related to Electronics than Books. [sent-207, score-0.528]
</p><p>85 2  Sentiment Classification Accuracy vs Label Complexity in Target Domain Similar as before, we tested the proposed approach using a set of different values s ∈ {100, 200, 300, 400, 500} as the number of labele∈d {re1v0ie0w,2s0 f0r,o3m00 t,h4e0 target }d aomsta hine. [sent-210, score-0.282]
</p><p>86 We can see that the performance of each approach in general improves with the increase of the number of labeled reviews from the target domain. [sent-213, score-0.356]
</p><p>87 The proposed approach maintains a clear advantage over all the other methods on all the twelve tasks across different label complexities. [sent-214, score-0.35]
</p><p>88 All those empirical results demonstrate the effectiveness of the proposed approach for cross-domain sentiment classification. [sent-215, score-0.223]
</p><p>89 3 Illustration of the Word Clusters Finally, we would also like to demonstrate the hard word clusters produced by the proposed supervised word clustering method. [sent-218, score-0.443]
</p><p>90 We assign a word into the cluster it most likely belongs to according to its soft clustering representation, such as c∗ = arg maxc P(c|w, θ∗). [sent-219, score-0.358]
</p><p>91 We can see that the first three clusters (C1, C2, and C3) contain words with positive sentiment polarity in different degrees. [sent-223, score-0.245]
</p><p>92 The last cluster (C10) contains words of negative sentiment polarity. [sent-226, score-0.28]
</p><p>93 These results demonstrate that the proposed supervised word clustering can produce task meaningful word clusters and hence label-informative latent features, which justifies its effectiveness. [sent-227, score-0.514]
</p><p>94 5  Conclusion  In this paper, we proposed a novel supervised representation learning method to tackle domain adaptation by inducing predictive latent features based on supervised word clustering. [sent-228, score-0.95]
</p><p>95 With the soft word clustering produced, we can transform all documents from the two domains into a unified lowdimensional feature space for effective training of cross-domain NLP prediction system. [sent-229, score-0.52]
</p><p>96 We conducted extensive experiments on cross-domain document categorization tasks on Reuters-21578 dataset and cross-domain sentiment classification tasks on Amazon product reviews. [sent-230, score-0.699]
</p><p>97 Analysis of representations for domain adaptaTable 3: Clustering illustration for the task of B2K on Amazon product reviews. [sent-237, score-0.358]
</p><p>98 Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. [sent-265, score-0.408]
</p><p>99 Crossing media streams with sentiment: Domain adaptation in blogs, reviews and twitter. [sent-344, score-0.332]
</p><p>100 Effectiveness of simple linguistic processing in automatic sentiment classification of product reviews. [sent-353, score-0.325]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('blitzer', 0.3), ('domain', 0.256), ('adaptation', 0.244), ('orgs', 0.236), ('scl', 0.204), ('swc', 0.204), ('cpsp', 0.181), ('fdlda', 0.181), ('labeled', 0.171), ('plsa', 0.168), ('sentiment', 0.164), ('domains', 0.157), ('documents', 0.151), ('categorization', 0.145), ('generalizable', 0.135), ('clustering', 0.127), ('twelve', 0.126), ('vs', 0.126), ('kitchen', 0.126), ('bow', 0.117), ('cluster', 0.116), ('classification', 0.104), ('latent', 0.103), ('target', 0.097), ('books', 0.097), ('daum', 0.091), ('reviews', 0.088), ('supervised', 0.088), ('representation', 0.084), ('clusters', 0.081), ('instances', 0.076), ('document', 0.074), ('clusterings', 0.072), ('electronics', 0.072), ('ztr', 0.068), ('dai', 0.067), ('source', 0.063), ('tasks', 0.063), ('amazon', 0.061), ('crossdomain', 0.059), ('popularly', 0.059), ('dvd', 0.059), ('proposed', 0.059), ('places', 0.059), ('product', 0.057), ('variables', 0.057), ('iii', 0.057), ('soft', 0.057), ('yt', 0.055), ('prettenhofer', 0.054), ('sensitivity', 0.051), ('people', 0.051), ('pivot', 0.05), ('em', 0.049), ('reuters', 0.047), ('nips', 0.046), ('augmenting', 0.046), ('tt', 0.046), ('mejova', 0.045), ('projectors', 0.045), ('shan', 0.045), ('sugiyama', 0.045), ('xtr', 0.045), ('representations', 0.045), ('graphical', 0.045), ('mapping', 0.044), ('weighting', 0.042), ('multinomial', 0.04), ('across', 0.039), ('dimension', 0.039), ('svm', 0.039), ('correspondence', 0.038), ('bayes', 0.038), ('repeated', 0.037), ('temple', 0.036), ('induced', 0.035), ('naive', 0.035), ('directed', 0.035), ('label', 0.034), ('selections', 0.034), ('subspace', 0.034), ('deviation', 0.033), ('coupled', 0.033), ('produced', 0.032), ('vocabulary', 0.031), ('finkel', 0.031), ('structural', 0.031), ('dempster', 0.03), ('belongs', 0.03), ('divergence', 0.029), ('stein', 0.029), ('maintains', 0.029), ('conducted', 0.029), ('studied', 0.028), ('train', 0.028), ('word', 0.028), ('conduct', 0.028), ('plotted', 0.028), ('weinberger', 0.028), ('pw', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999857 <a title="120-tfidf-1" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>Author: Min Xiao ; Feipeng Zhao ; Yuhong Guo</p><p>Abstract: Domain adaptation has been popularly studied on exploiting labeled information from a source domain to learn a prediction model in a target domain. In this paper, we develop a novel representation learning approach to address domain adaptation for text classification with automatically induced discriminative latent features, which are generalizable across domains while informative to the prediction task. Specifically, we propose a hierarchical multinomial Naive Bayes model with latent variables to conduct supervised word clustering on labeled documents from both source and target domains, and then use the produced cluster distribution of each word as its latent feature representation for domain adaptation. We train this latent graphical model us- ing a simple expectation-maximization (EM) algorithm. We empirically evaluate the proposed method with both cross-domain document categorization tasks on Reuters-21578 dataset and cross-domain sentiment classification tasks on Amazon product review dataset. The experimental results demonstrate that our proposed approach achieves superior performance compared with alternative methods.</p><p>2 0.39339671 <a title="120-tfidf-2" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>Author: Min Xiao ; Yuhong Guo</p><p>Abstract: Cross-lingual adaptation aims to learn a prediction model in a label-scarce target language by exploiting labeled data from a labelrich source language. An effective crosslingual adaptation system can substantially reduce the manual annotation effort required in many natural language processing tasks. In this paper, we propose a new cross-lingual adaptation approach for document classification based on learning cross-lingual discriminative distributed representations of words. Specifically, we propose to maximize the loglikelihood of the documents from both language domains under a cross-lingual logbilinear document model, while minimizing the prediction log-losses of labeled documents. We conduct extensive experiments on cross-lingual sentiment classification tasks of Amazon product reviews. Our experimental results demonstrate the efficacy of the pro- posed cross-lingual adaptation approach.</p><p>3 0.20818302 <a title="120-tfidf-3" href="./emnlp-2013-Automatic_Domain_Partitioning_for_Multi-Domain_Learning.html">29 emnlp-2013-Automatic Domain Partitioning for Multi-Domain Learning</a></p>
<p>Author: Di Wang ; Chenyan Xiong ; William Yang Wang</p><p>Abstract: Chenyan Xiong School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA cx@ c s . cmu .edu William Yang Wang School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA ww@ cmu .edu might not be generalizable to other domains (BenDavid et al., 2006; Ben-David et al., 2010). Multi-Domain learning (MDL) assumes that the domain labels in the dataset are known. However, when there are multiple metadata at- tributes available, it is not always straightforward to select a single best attribute for domain partition, and it is possible that combining more than one metadata attributes (including continuous attributes) can lead to better MDL performance. In this work, we propose an automatic domain partitioning approach that aims at providing better domain identities for MDL. We use a supervised clustering approach that learns the domain distance between data instances , and then cluster the data into better domains for MDL. Our experiment on real multi-domain datasets shows that using our automatically generated domain partition improves over popular MDL methods.</p><p>4 0.18124981 <a title="120-tfidf-4" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>Author: Lei Cui ; Xilun Chen ; Dongdong Zhang ; Shujie Liu ; Mu Li ; Ming Zhou</p><p>Abstract: Domain adaptation for SMT usually adapts models to an individual specific domain. However, it often lacks some correlation among different domains where common knowledge could be shared to improve the overall translation quality. In this paper, we propose a novel multi-domain adaptation approach for SMT using Multi-Task Learning (MTL), with in-domain models tailored for each specific domain and a general-domain model shared by different domains. The parameters of these models are tuned jointly via MTL so that they can learn general knowledge more accurately and exploit domain knowledge better. Our experiments on a largescale English-to-Chinese translation task validate that the MTL-based adaptation approach significantly and consistently improves the translation quality compared to a non-adapted baseline. Furthermore, it also outperforms the individual adaptation of each specific domain.</p><p>5 0.15463582 <a title="120-tfidf-5" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>Author: Margaret Mitchell ; Jacqui Aguilar ; Theresa Wilson ; Benjamin Van Durme</p><p>Abstract: We propose a novel approach to sentiment analysis for a low resource setting. The intuition behind this work is that sentiment expressed towards an entity, targeted sentiment, may be viewed as a span of sentiment expressed across the entity. This representation allows us to model sentiment detection as a sequence tagging problem, jointly discovering people and organizations along with whether there is sentiment directed towards them. We compare performance in both Spanish and English on microblog data, using only a sentiment lexicon as an external resource. By leveraging linguisticallyinformed features within conditional random fields (CRFs) trained to minimize empirical risk, our best models in Spanish significantly outperform a strong baseline, and reach around 90% accuracy on the combined task of named entity recognition and sentiment prediction. Our models in English, trained on a much smaller dataset, are not yet statistically significant against their baselines.</p><p>6 0.10995425 <a title="120-tfidf-6" href="./emnlp-2013-Using_Crowdsourcing_to_get_Representations_based_on_Regular_Expressions.html">196 emnlp-2013-Using Crowdsourcing to get Representations based on Regular Expressions</a></p>
<p>7 0.10084693 <a title="120-tfidf-7" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>8 0.097968273 <a title="120-tfidf-8" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>9 0.0973529 <a title="120-tfidf-9" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>10 0.09538649 <a title="120-tfidf-10" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>11 0.095122665 <a title="120-tfidf-11" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>12 0.093223177 <a title="120-tfidf-12" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>13 0.084059574 <a title="120-tfidf-13" href="./emnlp-2013-Building_Specialized_Bilingual_Lexicons_Using_Large_Scale_Background_Knowledge.html">42 emnlp-2013-Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge</a></p>
<p>14 0.081773773 <a title="120-tfidf-14" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>15 0.073941119 <a title="120-tfidf-15" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<p>16 0.071000412 <a title="120-tfidf-16" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>17 0.070795141 <a title="120-tfidf-17" href="./emnlp-2013-Success_with_Style%3A_Using_Writing_Style_to_Predict_the_Success_of_Novels.html">178 emnlp-2013-Success with Style: Using Writing Style to Predict the Success of Novels</a></p>
<p>18 0.069979422 <a title="120-tfidf-18" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>19 0.068563968 <a title="120-tfidf-19" href="./emnlp-2013-Improvements_to_the_Bayesian_Topic_N-Gram_Models.html">100 emnlp-2013-Improvements to the Bayesian Topic N-Gram Models</a></p>
<p>20 0.065525472 <a title="120-tfidf-20" href="./emnlp-2013-Learning_Topics_and_Positions_from_Debatepedia.html">121 emnlp-2013-Learning Topics and Positions from Debatepedia</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.248), (1, 0.019), (2, -0.16), (3, -0.131), (4, 0.166), (5, -0.006), (6, 0.053), (7, 0.006), (8, -0.027), (9, -0.158), (10, 0.061), (11, -0.338), (12, -0.032), (13, -0.123), (14, 0.111), (15, 0.0), (16, -0.196), (17, 0.015), (18, -0.06), (19, -0.082), (20, 0.108), (21, -0.114), (22, 0.185), (23, -0.212), (24, 0.119), (25, 0.07), (26, 0.075), (27, -0.077), (28, -0.067), (29, -0.01), (30, 0.016), (31, 0.053), (32, 0.145), (33, 0.041), (34, 0.074), (35, 0.041), (36, 0.012), (37, 0.014), (38, -0.011), (39, 0.084), (40, -0.09), (41, -0.03), (42, 0.046), (43, 0.026), (44, -0.0), (45, 0.144), (46, -0.026), (47, 0.062), (48, 0.026), (49, -0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97920823 <a title="120-lsi-1" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>Author: Min Xiao ; Feipeng Zhao ; Yuhong Guo</p><p>Abstract: Domain adaptation has been popularly studied on exploiting labeled information from a source domain to learn a prediction model in a target domain. In this paper, we develop a novel representation learning approach to address domain adaptation for text classification with automatically induced discriminative latent features, which are generalizable across domains while informative to the prediction task. Specifically, we propose a hierarchical multinomial Naive Bayes model with latent variables to conduct supervised word clustering on labeled documents from both source and target domains, and then use the produced cluster distribution of each word as its latent feature representation for domain adaptation. We train this latent graphical model us- ing a simple expectation-maximization (EM) algorithm. We empirically evaluate the proposed method with both cross-domain document categorization tasks on Reuters-21578 dataset and cross-domain sentiment classification tasks on Amazon product review dataset. The experimental results demonstrate that our proposed approach achieves superior performance compared with alternative methods.</p><p>2 0.88263017 <a title="120-lsi-2" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>Author: Min Xiao ; Yuhong Guo</p><p>Abstract: Cross-lingual adaptation aims to learn a prediction model in a label-scarce target language by exploiting labeled data from a labelrich source language. An effective crosslingual adaptation system can substantially reduce the manual annotation effort required in many natural language processing tasks. In this paper, we propose a new cross-lingual adaptation approach for document classification based on learning cross-lingual discriminative distributed representations of words. Specifically, we propose to maximize the loglikelihood of the documents from both language domains under a cross-lingual logbilinear document model, while minimizing the prediction log-losses of labeled documents. We conduct extensive experiments on cross-lingual sentiment classification tasks of Amazon product reviews. Our experimental results demonstrate the efficacy of the pro- posed cross-lingual adaptation approach.</p><p>3 0.84645021 <a title="120-lsi-3" href="./emnlp-2013-Automatic_Domain_Partitioning_for_Multi-Domain_Learning.html">29 emnlp-2013-Automatic Domain Partitioning for Multi-Domain Learning</a></p>
<p>Author: Di Wang ; Chenyan Xiong ; William Yang Wang</p><p>Abstract: Chenyan Xiong School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA cx@ c s . cmu .edu William Yang Wang School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA ww@ cmu .edu might not be generalizable to other domains (BenDavid et al., 2006; Ben-David et al., 2010). Multi-Domain learning (MDL) assumes that the domain labels in the dataset are known. However, when there are multiple metadata at- tributes available, it is not always straightforward to select a single best attribute for domain partition, and it is possible that combining more than one metadata attributes (including continuous attributes) can lead to better MDL performance. In this work, we propose an automatic domain partitioning approach that aims at providing better domain identities for MDL. We use a supervised clustering approach that learns the domain distance between data instances , and then cluster the data into better domains for MDL. Our experiment on real multi-domain datasets shows that using our automatically generated domain partition improves over popular MDL methods.</p><p>4 0.6099695 <a title="120-lsi-4" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>Author: Lei Cui ; Xilun Chen ; Dongdong Zhang ; Shujie Liu ; Mu Li ; Ming Zhou</p><p>Abstract: Domain adaptation for SMT usually adapts models to an individual specific domain. However, it often lacks some correlation among different domains where common knowledge could be shared to improve the overall translation quality. In this paper, we propose a novel multi-domain adaptation approach for SMT using Multi-Task Learning (MTL), with in-domain models tailored for each specific domain and a general-domain model shared by different domains. The parameters of these models are tuned jointly via MTL so that they can learn general knowledge more accurately and exploit domain knowledge better. Our experiments on a largescale English-to-Chinese translation task validate that the MTL-based adaptation approach significantly and consistently improves the translation quality compared to a non-adapted baseline. Furthermore, it also outperforms the individual adaptation of each specific domain.</p><p>5 0.54352891 <a title="120-lsi-5" href="./emnlp-2013-Using_Crowdsourcing_to_get_Representations_based_on_Regular_Expressions.html">196 emnlp-2013-Using Crowdsourcing to get Representations based on Regular Expressions</a></p>
<p>Author: Anders Sgaard ; Hector Martinez ; Jakob Elming ; Anders Johannsen</p><p>Abstract: Often the bottleneck in document classification is finding good representations that zoom in on the most important aspects of the documents. Most research uses n-gram representations, but relevant features often occur discontinuously, e.g., not. . . good in sentiment analysis. In this paper we present experiments getting experts to provide regular expressions, as well as crowdsourced annotation tasks from which regular expressions can be derived. Somewhat surprisingly, it turns out that these crowdsourced feature combinations outperform automatic feature combination methods, as well as expert features, by a very large margin and reduce error by 24-41% over n-gram representations.</p><p>6 0.47678259 <a title="120-lsi-6" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>7 0.42121735 <a title="120-lsi-7" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>8 0.40521246 <a title="120-lsi-8" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>9 0.3685272 <a title="120-lsi-9" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>10 0.36662439 <a title="120-lsi-10" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>11 0.36030358 <a title="120-lsi-11" href="./emnlp-2013-Success_with_Style%3A_Using_Writing_Style_to_Predict_the_Success_of_Novels.html">178 emnlp-2013-Success with Style: Using Writing Style to Predict the Success of Novels</a></p>
<p>12 0.35364112 <a title="120-lsi-12" href="./emnlp-2013-Growing_Multi-Domain_Glossaries_from_a_Few_Seeds_using_Probabilistic_Topic_Models.html">92 emnlp-2013-Growing Multi-Domain Glossaries from a Few Seeds using Probabilistic Topic Models</a></p>
<p>13 0.34923205 <a title="120-lsi-13" href="./emnlp-2013-Building_Specialized_Bilingual_Lexicons_Using_Large_Scale_Background_Knowledge.html">42 emnlp-2013-Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge</a></p>
<p>14 0.3413212 <a title="120-lsi-14" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>15 0.32043374 <a title="120-lsi-15" href="./emnlp-2013-Learning_to_Rank_Lexical_Substitutions.html">123 emnlp-2013-Learning to Rank Lexical Substitutions</a></p>
<p>16 0.31911102 <a title="120-lsi-16" href="./emnlp-2013-Identifying_Multiple_Userids_of_the_Same_Author.html">95 emnlp-2013-Identifying Multiple Userids of the Same Author</a></p>
<p>17 0.3110083 <a title="120-lsi-17" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<p>18 0.30249441 <a title="120-lsi-18" href="./emnlp-2013-Opinion_Mining_in_Newspaper_Articles_by_Entropy-Based_Word_Connections.html">144 emnlp-2013-Opinion Mining in Newspaper Articles by Entropy-Based Word Connections</a></p>
<p>19 0.30161303 <a title="120-lsi-19" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>20 0.30149803 <a title="120-lsi-20" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.026), (18, 0.148), (22, 0.085), (30, 0.065), (35, 0.249), (50, 0.01), (51, 0.143), (66, 0.041), (71, 0.04), (75, 0.037), (77, 0.038), (90, 0.014), (96, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80037987 <a title="120-lda-1" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>Author: Min Xiao ; Feipeng Zhao ; Yuhong Guo</p><p>Abstract: Domain adaptation has been popularly studied on exploiting labeled information from a source domain to learn a prediction model in a target domain. In this paper, we develop a novel representation learning approach to address domain adaptation for text classification with automatically induced discriminative latent features, which are generalizable across domains while informative to the prediction task. Specifically, we propose a hierarchical multinomial Naive Bayes model with latent variables to conduct supervised word clustering on labeled documents from both source and target domains, and then use the produced cluster distribution of each word as its latent feature representation for domain adaptation. We train this latent graphical model us- ing a simple expectation-maximization (EM) algorithm. We empirically evaluate the proposed method with both cross-domain document categorization tasks on Reuters-21578 dataset and cross-domain sentiment classification tasks on Amazon product review dataset. The experimental results demonstrate that our proposed approach achieves superior performance compared with alternative methods.</p><p>2 0.71943378 <a title="120-lda-2" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>Author: Do Kook Choe ; Eugene Charniak</p><p>Abstract: We introduce an extended naive Bayes model for word sense induction (WSI) and apply it to a WSI task. The extended model incorporates the idea the words closer to the target word are more relevant in predicting its sense. The proposed model is very simple yet effective when evaluated on SemEval-2010 WSI data. 1</p><p>3 0.67000645 <a title="120-lda-3" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>Author: Min Xiao ; Yuhong Guo</p><p>Abstract: Cross-lingual adaptation aims to learn a prediction model in a label-scarce target language by exploiting labeled data from a labelrich source language. An effective crosslingual adaptation system can substantially reduce the manual annotation effort required in many natural language processing tasks. In this paper, we propose a new cross-lingual adaptation approach for document classification based on learning cross-lingual discriminative distributed representations of words. Specifically, we propose to maximize the loglikelihood of the documents from both language domains under a cross-lingual logbilinear document model, while minimizing the prediction log-losses of labeled documents. We conduct extensive experiments on cross-lingual sentiment classification tasks of Amazon product reviews. Our experimental results demonstrate the efficacy of the pro- posed cross-lingual adaptation approach.</p><p>4 0.66306871 <a title="120-lda-4" href="./emnlp-2013-Efficient_Left-to-Right_Hierarchical_Phrase-Based_Translation_with_Improved_Reordering.html">71 emnlp-2013-Efficient Left-to-Right Hierarchical Phrase-Based Translation with Improved Reordering</a></p>
<p>Author: Maryam Siahbani ; Baskaran Sankaran ; Anoop Sarkar</p><p>Abstract: Left-to-right (LR) decoding (Watanabe et al., 2006b) is a promising decoding algorithm for hierarchical phrase-based translation (Hiero). It generates the target sentence by extending the hypotheses only on the right edge. LR decoding has complexity O(n2b) for input of n words and beam size b, compared to O(n3) for the CKY algorithm. It requires a single language model (LM) history for each target hypothesis rather than two LM histories per hypothesis as in CKY. In this paper we present an augmented LR decoding algorithm that builds on the original algorithm in (Watanabe et al., 2006b). Unlike that algorithm, using experiments over multiple language pairs we show two new results: our LR decoding algorithm provides demonstrably more efficient decoding than CKY Hiero, four times faster; and by introducing new distortion and reordering features for LR decoding, it maintains the same translation quality (as in BLEU scores) ob- tained phrase-based and CKY Hiero with the same translation model.</p><p>5 0.66005278 <a title="120-lda-5" href="./emnlp-2013-Combining_Generative_and_Discriminative_Model_Scores_for_Distant_Supervision.html">49 emnlp-2013-Combining Generative and Discriminative Model Scores for Distant Supervision</a></p>
<p>Author: Benjamin Roth ; Dietrich Klakow</p><p>Abstract: Distant supervision is a scheme to generate noisy training data for relation extraction by aligning entities of a knowledge base with text. In this work we combine the output of a discriminative at-least-one learner with that of a generative hierarchical topic model to reduce the noise in distant supervision data. The combination significantly increases the ranking quality of extracted facts and achieves state-of-the-art extraction performance in an end-to-end setting. A simple linear interpolation of the model scores performs better than a parameter-free scheme based on nondominated sorting.</p><p>6 0.62654924 <a title="120-lda-6" href="./emnlp-2013-Scaling_Semantic_Parsers_with_On-the-Fly_Ontology_Matching.html">164 emnlp-2013-Scaling Semantic Parsers with On-the-Fly Ontology Matching</a></p>
<p>7 0.61048877 <a title="120-lda-7" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>8 0.59883392 <a title="120-lda-8" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>9 0.59781474 <a title="120-lda-9" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>10 0.59357721 <a title="120-lda-10" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>11 0.59355956 <a title="120-lda-11" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>12 0.59113801 <a title="120-lda-12" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>13 0.58985609 <a title="120-lda-13" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>14 0.58750147 <a title="120-lda-14" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>15 0.58678055 <a title="120-lda-15" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>16 0.5843181 <a title="120-lda-16" href="./emnlp-2013-Two-Stage_Method_for_Large-Scale_Acquisition_of_Contradiction_Pattern_Pairs_using_Entailment.html">189 emnlp-2013-Two-Stage Method for Large-Scale Acquisition of Contradiction Pattern Pairs using Entailment</a></p>
<p>17 0.58428776 <a title="120-lda-17" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>18 0.58264363 <a title="120-lda-18" href="./emnlp-2013-Flexible_and_Efficient_Hypergraph_Interactions_for_Joint_Hierarchical_and_Forest-to-String_Decoding.html">88 emnlp-2013-Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest-to-String Decoding</a></p>
<p>19 0.58252835 <a title="120-lda-19" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>20 0.58245283 <a title="120-lda-20" href="./emnlp-2013-Converting_Continuous-Space_Language_Models_into_N-Gram_Language_Models_for_Statistical_Machine_Translation.html">52 emnlp-2013-Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
