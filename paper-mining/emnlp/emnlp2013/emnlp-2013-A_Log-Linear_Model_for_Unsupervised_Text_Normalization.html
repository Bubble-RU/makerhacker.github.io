<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>9 emnlp-2013-A Log-Linear Model for Unsupervised Text Normalization</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-9" href="#">emnlp2013-9</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>9 emnlp-2013-A Log-Linear Model for Unsupervised Text Normalization</h1>
<br/><p>Source: <a title="emnlp-2013-9-pdf" href="http://aclweb.org/anthology//D/D13/D13-1007.pdf">pdf</a></p><p>Author: Yi Yang ; Jacob Eisenstein</p><p>Abstract: We present a unified unsupervised statistical model for text normalization. The relationship between standard and non-standard tokens is characterized by a log-linear model, permitting arbitrary features. The weights of these features are trained in a maximumlikelihood framework, employing a novel sequential Monte Carlo training algorithm to overcome the large label space, which would be impractical for traditional dynamic programming solutions. This model is implemented in a normalization system called UNLOL, which achieves the best known results on two normalization datasets, outperforming more complex systems. We use the output of UNLOL to automatically normalize a large corpus of social media text, revealing a set of coherent orthographic styles that underlie online language variation.</p><p>Reference: <a title="emnlp-2013-9-reference" href="../emnlp2013_reference/emnlp-2013-A_Log-Linear_Model_for_Unsupervised_Text_Normalization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The weights of these features are trained in a maximumlikelihood framework, employing a novel sequential Monte Carlo training algorithm to overcome the large label space, which would be impractical for traditional dynamic programming solutions. [sent-4, score-0.209]
</p><p>2 This model is implemented in a normalization system called UNLOL, which achieves the best known results on two normalization datasets, outperforming more complex systems. [sent-5, score-0.652]
</p><p>3 We use the output of UNLOL to automatically normalize a large corpus of social media text, revealing a  set of coherent orthographic styles that underlie online language variation. [sent-6, score-0.442]
</p><p>4 Many ofthe attempts to characterize and overcome this variation have focused on normalization: transforming social media language into text that better matches standard datasets (Sproat et al. [sent-8, score-0.208]
</p><p>5 Because there is little available training data, and because social media language changes rapidly (Eisenstein, 2013b), fully supervised training is generally not considered appropriate for this task. [sent-11, score-0.174]
</p><p>6 We propose a different approach, performing normalization in a maximum-likelihood framework. [sent-17, score-0.326]
</p><p>7 There are two main sources of information to be exploited: local context, and surface similarity between the observed strings and normalization candidates. [sent-18, score-0.371]
</p><p>8 Because labeled examples of normalized text are not available, this model cannot be trained in the standard supervised fashion. [sent-20, score-0.129]
</p><p>9 , 2010), as their complexity is quadratic in the size of label space; in  normalization, the label space is the vocabulary itself, with at least 104 elements. [sent-22, score-0.111]
</p><p>10 This training method may be applicable in other unsupervised learning problems with a large label space. [sent-24, score-0.087]
</p><p>11 This model is implemented in a normalization system called UNLOL (unsupervised normalization in a LOg-Linear model). [sent-25, score-0.652]
</p><p>12 Our evaluations show that UNLOL outperforms the state-of-the-art on standard normalization datasets. [sent-29, score-0.326]
</p><p>13 In addition, we demonstrate the linguistic insights that can be obtained from normalization, using UNLOL to identify classes of orthographic transformations that form coherent linguistic styles. [sent-30, score-0.105]
</p><p>14 2  Background  The text normalization task was introduced by Sproat et al. [sent-31, score-0.326]
</p><p>15 It has become still more salient in the era of widespread social media, particularly Twitter. [sent-34, score-0.096]
</p><p>16 Han and Baldwin (201 1) formally define a normalization task for Twitter, focusing on normalizations between single tokens, and excluding multi-word tokens like lol (laugh out loud). [sent-35, score-0.455]
</p><p>17 The normalization task has been criticized by Eisenstein (2013b), who argues that it strips away important social meanings. [sent-36, score-0.422]
</p><p>18 In recent work, normalization has been shown to yield improvements for part-of-speech tagging (Han et al. [sent-37, score-0.326]
</p><p>19 As we will show in Section 7, accurate automated normalization can also improve our understanding of the nature of social media language. [sent-40, score-0.5]
</p><p>20 Supervised methods Early work on normalization focused on labeled SMS datasets, using approaches such as noisy-channel modeling (Choudhury et al. [sent-41, score-0.361]
</p><p>21 The scalar parameters are then estimated using expectation maximization. [sent-50, score-0.084]
</p><p>22 (2010) use string edit distance to identify closely-related candidate orthographic forms and then decode the message using a language model. [sent-53, score-0.261]
</p><p>23 (2010), we apply string edit distance, and like Gouws et al. [sent-57, score-0.119]
</p><p>24 At present, labeled data for Twitter normalization is available only in small quantities. [sent-72, score-0.361]
</p><p>25 Moreover, as social media language is undergoing rapid change (Eisenstein, 2013b), labeled datasets may become stale and increasingly ill-suited to new spellings and words. [sent-73, score-0.243]
</p><p>26 Resources that characterize the current state of internet language risk becoming outdated; in this paper we investigate whether high-quality normalization is possible without any such resources. [sent-77, score-0.326]
</p><p>27 These features may include simple string edit distance metrics, as well as lexical features that memorize specific pairs of standard and nonstandard words. [sent-87, score-0.217]
</p><p>28 For example, in the phrase give me sutt in t o be l ieve in, even a reader who has never before seen the word sutt in may recognize it as a phonetic transcription of something. [sent-91, score-0.164]
</p><p>29 The relatively high string edit distance is overcome by the strong  contextual preference for the word something over orthographically closer alternatives such as button or suiting. [sent-92, score-0.119]
</p><p>30 We can apply an arbitrary target language model, leveraging large amounts of unlabeled data and catering to the desired linguistic characteristics of the normalized content. [sent-93, score-0.154]
</p><p>31 —  —  63 None of these tokens are standard (except 2, which appears in a nonstandard sense here), so without joint inference, it would not be possible to use context to help normalize sutt in. [sent-98, score-0.302]
</p><p>32 However, unlike the supervised case, here both terms are expectations: the outer expectation is over all target sequences (given the observed source sequence), and the nested expectation is over all source sequences, given the target sequence. [sent-131, score-0.369]
</p><p>33 As the space of possible target sequences t grows exponentially in the length of the source sequence, it will not be practical to compute this expectation directly. [sent-132, score-0.144]
</p><p>34 First, while the forward-  backward algorithm would enable us to compute Et|s, it would not give us the nested expectation Et|s [Es0|t] ; this is the classic challenge in training globally-normalized log-linear models without labeled data (Smith and Eisner, 2005). [sent-135, score-0.162]
</p><p>35 SMC algorithms maintain a set of weighted hypotheses;  the weights correspond to probabilities, and in our case, the hypotheses correspond to target language word sequences. [sent-142, score-0.16]
</p><p>36 Specifically, we approximate the conditional probability, XK  P(t1:n|s1:n) ≈Xk=1ωnkδt1k:n(t1:n), where ωnk is the normalized weight of sample k at word n ( ω˜nkis the unnormalized weight), and δt1k:n is a delta function centered at t1k:n. [sent-143, score-0.208]
</p><p>37 At each step, and for each hypothesis k, a new target word is sampled from a proposal distribution, and the weight of the hypothesis is then updated. [sent-144, score-0.338]
</p><p>38 We maintain feature counts for each hypothesis, and approximate the expectation by taking a weighted average using the hypothesis weights. [sent-145, score-0.162]
</p><p>39 The proposal distribution will be described in detail later. [sent-146, score-0.242]
</p><p>40 With these assumptionsQ, we can view normalization as a finite state-space model in which the target language model defines the prior distribution ofthe process and Equation 3 defines the likelihood function. [sent-158, score-0.468]
</p><p>41 We are able to compute the the posterior probability P(t |s) using sequential importance sampling, a imtyem Pb(etr|s o)f u uthsien gSM seCq family. [sent-159, score-0.158]
</p><p>42 The crucial idea in sequential importance sampling is to update the hypotheses t1k:n and their weights ωnk so that they approximate the posterior distribution at the next time step, P(t1:n+1 |s1:n+1). [sent-160, score-0.374]
</p><p>43 We further assume the proposal distribution Q can be factored as: Q(t1:n|s1:n) =Q(tn|t1:n−1, s1:n)Q(t1:n−1 |s1:n−1) =Q(tn|tn−1, sn)Q(t1:n−1|s1:n−1). [sent-162, score-0.242]
</p><p>44 (10) where we sample tnk and update ωnk while moving from left-to-right, and sample s‘n,k at each n. [sent-165, score-0.329]
</p><p>45 Note that although the sequential importance sampler moves left-to-right like a filter, we use only the final weights ωN to compute the expectation. [sent-166, score-0.213]
</p><p>46 Thus, the resulting expectation is based on the distribution P(s1:N |t1:N), so that no backwards “smoothing” pass (Godsill et al. [sent-167, score-0.128]
</p><p>47 Other applications of sequential Monte Carlo make use of resampling (Cappe et al. [sent-169, score-0.119]
</p><p>48 2 Proposal distribution The major computational challenge for dynamic programming approaches to normalization is the large label space, equal to the size of the target vocabulary. [sent-172, score-0.465]
</p><p>49 It may appear that all we have gained by applying sequential Monte Carlo is to convert  a computational problem into a statistical one: a naive sampling approach will have little hope of finding the small high-probability region ofthe highdimensional label space. [sent-173, score-0.188]
</p><p>50 However, sequential importance sampling allows us to address this issue through the proposal distribution, from which we sample the candidate words tn. [sent-174, score-0.39]
</p><p>51 Careful design ofthe proposal distribution can guide sampling towards the high-probability space. [sent-175, score-0.276]
</p><p>52 In the asymptotic limit of an infinite number of samples, any non-pathological proposal distribution will ultimately arrive at the desired estimate, but a good proposal distribution can greatly reduce the number of samples needed. [sent-176, score-0.524]
</p><p>53 In low-dimensional settings, a convenient solution is to set the proposal distribution equal to the transition distribution, Q(tnk |sn, tnk−1) = P(tnk|tnk−1, . [sent-180, score-0.242]
</p><p>54 We strike a middle ground between efficiency and accuracy, using a proposal distribution that is closely related to the overall likelihood, yet is tractable to sample and compute:  ω(k)  Q(tnk|sn,tk) d=ef P(sn|tkn)Z(tnk)P(tnk|tk)  Pt0P(sn|t0)Z(t0)P(t0|tk)  (12)  exp ? [sent-188, score-0.242]
</p><p>55 To update the unnormalized hypothesis weights ω˜kn, we have  ω˜kn=ωkn−1Pt0exp? [sent-197, score-0.171]
</p><p>56 3 Decoding Given an input source sentence s, the decoding problem is to find a target sentence t that maximizes P(t|s) ∝ P(s|t)P(t) = P(sn|tn)P(tn|tn−1). [sent-201, score-0.105]
</p><p>57 This must be multiplied by the cost of computing the normalized probability P(sn |tn), resulting in a prohibitive time complexity of O(#|t|νS|#|νT|2N). [sent-203, score-0.129]
</p><p>58 The first is to simply apply the proposal distribution, with linear complexity in the size of the two vocabularies. [sent-205, score-0.198]
</p><p>59 Alternatively, we can apply t(hte) proposal distribution for selecting target word candidates, then apply the Viterbi algorithm only within these candidates. [sent-207, score-0.302]
</p><p>60 (2010), which has proven effective for normalization in prior work. [sent-218, score-0.364]
</p><p>61 We bin this similarity to create binary features indicating whether a string s is in the top-N most similar strings to t; this binning yields substantial speed improvements without negatively impacting accuracy. [sent-219, score-0.106]
</p><p>62 5  Implementation and data  The model and inference described in the previous section are implemented in a software system for normalizing text on twitter, called UNLOL: unsupervised normalization in a LOgLinear model. [sent-220, score-0.427]
</p><p>63 1 Normalization candidates Most tokens in tweets do not require normalization. [sent-224, score-0.117]
</p><p>64 The question of how to identify which words are to be normalized is still an open problem. [sent-225, score-0.094]
</p><p>65 Following Han and Baldwin (201 1), we build a dictionary of words which are permissible in the target domain, and make no attempt to normalize source strings that match these words. [sent-226, score-0.18]
</p><p>66 As with other comparable approaches, we are therefore unable to normalize strings like i l l into I’ll. [sent-227, score-0.12]
</p><p>67 For all in-vocabulary words, we define P(sn |tn) = δ(sn, tn), taking the value of zero when sn tn. [sent-235, score-0.146]
</p><p>68 In addition to words that are in the target vocabulary, there are many other strings that should not be normalized, such as names and multiword shortenings (e. [sent-237, score-0.146]
</p><p>69 1 We follow prior work and assume that the set of normalization candidates is known in advance during test set decoding (Han et al. [sent-240, score-0.409]
</p><p>70 Thus, during training we attempt to normalize all tokens that (1) are not in our lexicon of IV words, and (2) are composed of letters, numbers and the apostrophe. [sent-243, score-0.122]
</p><p>71 This set includes contractions like "gonna" and "gotta", which would not appear in the test set, but are nonetheless normalized —  =  1Whether multiword shortenings should be normalized is arguable, but they are outside the scope of current normalization datasets (Han and Baldwin, 2011). [sent-244, score-0.589]
</p><p>72 For each OOV token, we conduct a pre-normalization step by reducing any repetitions of more than two letters in the nonstandard words to exactly two letters (e. [sent-246, score-0.098]
</p><p>73 3 Parameters The Monte Carlo approximations require two parameters: the number of samples for sequential Monte Carlo (K), and the number of samples for the non-sequential sampler of the nested expectation (L,  from Equation 10). [sent-255, score-0.326]
</p><p>74 , words that are not in the target vocabulary) and their normalized forms. [sent-262, score-0.154]
</p><p>75 As this corpus does not provide linguistic context, its decoding must use a unigram target language model. [sent-264, score-0.105]
</p><p>76 1 by its authors Han and Baldwin (201 1) contains 549 complete tweets with  —  —  —  —  1,184 nonstandard tokens (558 unique word types). [sent-266, score-0.215]
</p><p>77 1 revealed some inconsistencies in annotation (for example, y ’ al l and 2 are sometimes normalized to you and to, but are left unnormalized in other cases). [sent-299, score-0.17]
</p><p>78 For example, smh is normalized to somehow in LexNorm1 . [sent-301, score-0.135]
</p><p>79 2 in the hope that it will become standard in future work on normalization in English. [sent-317, score-0.326]
</p><p>80 68 Metrics Prior work on these datasets has assumed perfect detection of words requiring normalization,  and has focused on finding the correct normalization for these words (Han and Baldwin, 2011; Han et al. [sent-328, score-0.36]
</p><p>81 Recall has been defined as the proportion of words requiring normalization which are normalized correctly; precision is defined as the proportion of normalizations which are correct. [sent-330, score-0.502]
</p><p>82 In the normalization task that we consider, the tokens to be normalized are specified in advance. [sent-337, score-0.467]
</p><p>83 Regularization One potential concern is that the number of non-zero feature weights will continually increase until the memory cost becomes overwhelming. [sent-343, score-0.09]
</p><p>84 7  Analysis  We apply our normalization system to investigate the orthographic processes underlying language variation in social media. [sent-361, score-0.527]
</p><p>85 We then treat these normalizations as labeled training data, and examine the Levenshtein alignment between the source and target tokens. [sent-363, score-0.177]
</p><p>86 ” We apply non-negative matrix factorization (Lee and Seung,  2001), which characterizes each author by a vector of k style loadings, and simultaneously constructs k style dictionaries, which each put weight on different orthographic rules. [sent-367, score-0.266]
</p><p>87 Because the loadings are constrained to be non-negative, the factorization can be seen as sparsely assigning varying amounts of each style to each author. [sent-368, score-0.141]
</p><p>88 The resulting styles are shown in Table 3, for k = 10; other values of k give similar overall results with more or less detail. [sent-372, score-0.088]
</p><p>89 The styles incorporate a number of linguistic phenomena, including: expressive lengthening (styles 7-9; see Brody and Diakopoulos, 2011); g- and t-dropping (style 5, see Eisenstein 2013a) ; th-stopping (style 6); and the dropping of several word-final vowels (styles 1-3). [sent-373, score-0.134]
</p><p>90 Some of these styles, such as t-dropping and th-stopping, have direct analogues in spoken language varieties (Tagliamonte and Temple, 2005; Green, 2002), while others, like expressive lengthening, seem more unique to social media. [sent-374, score-0.132]
</p><p>91 The relationships between these orthographic styles and  social variables such as geography and demograph2We tried adding these rules as features and retraining the normalization system, but this hurt performance. [sent-375, score-0.615]
</p><p>92 The tokens ima, outt a, and needa all refer to multi-word expressions in standard English, and are thus outside the scope of the normalization task as defined by Han et al. [sent-392, score-0.455]
</p><p>93 But while these normalizations are  wrong, the resulting style nonetheless captures a coherent orthographic phenomenon. [sent-395, score-0.248]
</p><p>94 8  Conclusion  We have presented a unified, unsupervised statistical model for normalizing social media text, attaining the best reported performance on the two standard normalization datasets. [sent-396, score-0.601]
</p><p>95 The power of our approach comes from flexible modeling of word-to-word relationships through features, while exploiting contextual regularity to train the corresponding feature 70  weights without labeled data. [sent-397, score-0.09]
</p><p>96 The primary technical challenge was overcoming the large label space of the normalization task; we accomplish this using sequential Monte Carlo. [sent-398, score-0.48]
</p><p>97 Future work may consider whether sequential Monte Carlo can offer similar advantages in other unsupervised NLP tasks. [sent-399, score-0.171]
</p><p>98 A hybrid rule/model-based finite-state framework for normalizing sms messages. [sent-420, score-0.121]
</p><p>99 An overview of existing methods and recent advances in sequential monte carlo. [sent-453, score-0.337]
</p><p>100 Joint inference of named entity recognition and normalization for tweets. [sent-576, score-0.326]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tnk', 0.329), ('normalization', 0.326), ('unlol', 0.247), ('tn', 0.225), ('monte', 0.218), ('proposal', 0.198), ('han', 0.169), ('carlo', 0.153), ('sn', 0.146), ('baldwin', 0.14), ('sequential', 0.119), ('orthographic', 0.105), ('eisenstein', 0.104), ('lexnorm', 0.103), ('nonstandard', 0.098), ('social', 0.096), ('normalized', 0.094), ('twitter', 0.089), ('styles', 0.088), ('oov', 0.086), ('expectation', 0.084), ('doucet', 0.082), ('sutt', 0.082), ('nk', 0.082), ('normalizations', 0.082), ('media', 0.078), ('unnormalized', 0.076), ('normalize', 0.075), ('sms', 0.072), ('tweets', 0.07), ('liu', 0.069), ('viterbi', 0.069), ('contractor', 0.065), ('choudhury', 0.065), ('cappe', 0.062), ('godsill', 0.062), ('gouws', 0.062), ('smc', 0.062), ('tkn', 0.062), ('style', 0.061), ('string', 0.061), ('target', 0.06), ('jacob', 0.059), ('edit', 0.058), ('cook', 0.057), ('kn', 0.056), ('weights', 0.055), ('unsupervised', 0.052), ('tf', 0.052), ('normalizing', 0.049), ('petrovi', 0.049), ('tokens', 0.047), ('tk', 0.047), ('lengthening', 0.046), ('sproat', 0.046), ('decoding', 0.045), ('strings', 0.045), ('expectations', 0.045), ('hypotheses', 0.045), ('distribution', 0.044), ('nested', 0.043), ('quadratic', 0.041), ('xk', 0.041), ('beaufort', 0.041), ('finna', 0.041), ('gud', 0.041), ('ima', 0.041), ('kobus', 0.041), ('loadings', 0.041), ('needa', 0.041), ('nimfa', 0.041), ('outt', 0.041), ('pkk', 0.041), ('saraf', 0.041), ('shortenings', 0.041), ('smh', 0.041), ('texting', 0.041), ('tpn', 0.041), ('xtp', 0.041), ('hypothesis', 0.04), ('samples', 0.04), ('factorization', 0.039), ('importance', 0.039), ('prior', 0.038), ('outer', 0.038), ('approximate', 0.038), ('message', 0.037), ('ech', 0.036), ('gat', 0.036), ('menezes', 0.036), ('nnk', 0.036), ('tagliamonte', 0.036), ('varieties', 0.036), ('hassan', 0.035), ('labeled', 0.035), ('label', 0.035), ('smith', 0.035), ('cost', 0.035), ('datasets', 0.034), ('sampling', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="9-tfidf-1" href="./emnlp-2013-A_Log-Linear_Model_for_Unsupervised_Text_Normalization.html">9 emnlp-2013-A Log-Linear Model for Unsupervised Text Normalization</a></p>
<p>Author: Yi Yang ; Jacob Eisenstein</p><p>Abstract: We present a unified unsupervised statistical model for text normalization. The relationship between standard and non-standard tokens is characterized by a log-linear model, permitting arbitrary features. The weights of these features are trained in a maximumlikelihood framework, employing a novel sequential Monte Carlo training algorithm to overcome the large label space, which would be impractical for traditional dynamic programming solutions. This model is implemented in a normalization system called UNLOL, which achieves the best known results on two normalization datasets, outperforming more complex systems. We use the output of UNLOL to automatically normalize a large corpus of social media text, revealing a set of coherent orthographic styles that underlie online language variation.</p><p>2 0.22701722 <a title="9-tfidf-2" href="./emnlp-2013-Paraphrasing_4_Microblog_Normalization.html">151 emnlp-2013-Paraphrasing 4 Microblog Normalization</a></p>
<p>Author: Wang Ling ; Chris Dyer ; Alan W Black ; Isabel Trancoso</p><p>Abstract: Compared to the edited genres that have played a central role in NLP research, microblog texts use a more informal register with nonstandard lexical items, abbreviations, and free orthographic variation. When confronted with such input, conventional text analysis tools often perform poorly. Normalization replacing orthographically or lexically idiosyncratic forms with more standard variants can improve performance. We propose a method for learning normalization rules from machine translations of a parallel corpus of microblog messages. To validate the utility of our approach, we evaluate extrinsically, showing that normalizing English tweets and then translating improves translation quality (compared to translating unnormalized text) using three standard web translation services as well as a phrase-based translation system trained — — on parallel microblog data.</p><p>3 0.091793582 <a title="9-tfidf-3" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<p>Author: John Philip McCrae ; Philipp Cimiano ; Roman Klinger</p><p>Abstract: Cross-lingual topic modelling has applications in machine translation, word sense disambiguation and terminology alignment. Multilingual extensions of approaches based on latent (LSI), generative (LDA, PLSI) as well as explicit (ESA) topic modelling can induce an interlingual topic space allowing documents in different languages to be mapped into the same space and thus to be compared across languages. In this paper, we present a novel approach that combines latent and explicit topic modelling approaches in the sense that it builds on a set of explicitly defined topics, but then computes latent relations between these. Thus, the method combines the benefits of both explicit and latent topic modelling approaches. We show that on a crosslingual mate retrieval task, our model significantly outperforms LDA, LSI, and ESA, as well as a baseline that translates every word in a document into the target language.</p><p>4 0.088664994 <a title="9-tfidf-4" href="./emnlp-2013-Pair_Language_Models_for_Deriving_Alternative_Pronunciations_and_Spellings_from_Pronunciation_Dictionaries.html">150 emnlp-2013-Pair Language Models for Deriving Alternative Pronunciations and Spellings from Pronunciation Dictionaries</a></p>
<p>Author: Russell Beckley ; Brian Roark</p><p>Abstract: Pronunciation dictionaries provide a readily available parallel corpus for learning to transduce between character strings and phoneme strings or vice versa. Translation models can be used to derive character-level paraphrases on either side of this transduction, allowing for the automatic derivation of alternative pronunciations or spellings. We examine finitestate and SMT-based methods for these related tasks, and demonstrate that the tasks have different characteristics finding alternative spellings is harder than alternative pronunciations and benefits from round-trip algorithms when the other does not. We also show that we can increase accuracy by modeling syllable stress. –</p><p>5 0.087568164 <a title="9-tfidf-5" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>Author: Shi Feng ; Le Zhang ; Binyang Li ; Daling Wang ; Ge Yu ; Kam-Fai Wong</p><p>Abstract: Extensive experiments have validated the effectiveness of the corpus-based method for classifying the word’s sentiment polarity. However, no work is done for comparing different corpora in the polarity classification task. Nowadays, Twitter has aggregated huge amount of data that are full of people’s sentiments. In this paper, we empirically evaluate the performance of different corpora in sentiment similarity measurement, which is the fundamental task for word polarity classification. Experiment results show that the Twitter data can achieve a much better performance than the Google, Web1T and Wikipedia based methods.</p><p>6 0.085947908 <a title="9-tfidf-6" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>7 0.078131422 <a title="9-tfidf-7" href="./emnlp-2013-Gender_Inference_of_Twitter_Users_in_Non-English_Contexts.html">89 emnlp-2013-Gender Inference of Twitter Users in Non-English Contexts</a></p>
<p>8 0.0776015 <a title="9-tfidf-8" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>9 0.077494003 <a title="9-tfidf-9" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>10 0.075546175 <a title="9-tfidf-10" href="./emnlp-2013-Authorship_Attribution_of_Micro-Messages.html">27 emnlp-2013-Authorship Attribution of Micro-Messages</a></p>
<p>11 0.06944108 <a title="9-tfidf-11" href="./emnlp-2013-A_Unified_Model_for_Topics%2C_Events_and_Users_on_Twitter.html">16 emnlp-2013-A Unified Model for Topics, Events and Users on Twitter</a></p>
<p>12 0.067970231 <a title="9-tfidf-12" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>13 0.066786185 <a title="9-tfidf-13" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>14 0.063156679 <a title="9-tfidf-14" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>15 0.06249008 <a title="9-tfidf-15" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>16 0.06132691 <a title="9-tfidf-16" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>17 0.060003746 <a title="9-tfidf-17" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>18 0.059336312 <a title="9-tfidf-18" href="./emnlp-2013-Semi-Markov_Phrase-Based_Monolingual_Alignment.html">167 emnlp-2013-Semi-Markov Phrase-Based Monolingual Alignment</a></p>
<p>19 0.058996003 <a title="9-tfidf-19" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>20 0.057613157 <a title="9-tfidf-20" href="./emnlp-2013-Efficient_Left-to-Right_Hierarchical_Phrase-Based_Translation_with_Improved_Reordering.html">71 emnlp-2013-Efficient Left-to-Right Hierarchical Phrase-Based Translation with Improved Reordering</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.226), (1, -0.028), (2, -0.065), (3, -0.066), (4, -0.005), (5, -0.061), (6, 0.044), (7, 0.05), (8, 0.021), (9, -0.019), (10, -0.037), (11, 0.059), (12, 0.071), (13, 0.2), (14, 0.035), (15, -0.081), (16, 0.134), (17, -0.008), (18, -0.022), (19, -0.01), (20, -0.066), (21, -0.034), (22, 0.131), (23, 0.006), (24, -0.059), (25, 0.005), (26, -0.042), (27, 0.026), (28, 0.088), (29, 0.061), (30, 0.044), (31, 0.037), (32, 0.132), (33, 0.076), (34, 0.031), (35, 0.004), (36, -0.029), (37, -0.022), (38, 0.134), (39, 0.063), (40, 0.044), (41, -0.027), (42, -0.002), (43, -0.277), (44, -0.113), (45, 0.043), (46, -0.089), (47, 0.206), (48, -0.102), (49, -0.11)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92753822 <a title="9-lsi-1" href="./emnlp-2013-A_Log-Linear_Model_for_Unsupervised_Text_Normalization.html">9 emnlp-2013-A Log-Linear Model for Unsupervised Text Normalization</a></p>
<p>Author: Yi Yang ; Jacob Eisenstein</p><p>Abstract: We present a unified unsupervised statistical model for text normalization. The relationship between standard and non-standard tokens is characterized by a log-linear model, permitting arbitrary features. The weights of these features are trained in a maximumlikelihood framework, employing a novel sequential Monte Carlo training algorithm to overcome the large label space, which would be impractical for traditional dynamic programming solutions. This model is implemented in a normalization system called UNLOL, which achieves the best known results on two normalization datasets, outperforming more complex systems. We use the output of UNLOL to automatically normalize a large corpus of social media text, revealing a set of coherent orthographic styles that underlie online language variation.</p><p>2 0.79396677 <a title="9-lsi-2" href="./emnlp-2013-Paraphrasing_4_Microblog_Normalization.html">151 emnlp-2013-Paraphrasing 4 Microblog Normalization</a></p>
<p>Author: Wang Ling ; Chris Dyer ; Alan W Black ; Isabel Trancoso</p><p>Abstract: Compared to the edited genres that have played a central role in NLP research, microblog texts use a more informal register with nonstandard lexical items, abbreviations, and free orthographic variation. When confronted with such input, conventional text analysis tools often perform poorly. Normalization replacing orthographically or lexically idiosyncratic forms with more standard variants can improve performance. We propose a method for learning normalization rules from machine translations of a parallel corpus of microblog messages. To validate the utility of our approach, we evaluate extrinsically, showing that normalizing English tweets and then translating improves translation quality (compared to translating unnormalized text) using three standard web translation services as well as a phrase-based translation system trained — — on parallel microblog data.</p><p>3 0.56320214 <a title="9-lsi-3" href="./emnlp-2013-A_Synchronous_Context_Free_Grammar_for_Time_Normalization.html">14 emnlp-2013-A Synchronous Context Free Grammar for Time Normalization</a></p>
<p>Author: Steven Bethard</p><p>Abstract: We present an approach to time normalization (e.g. the day before yesterday⇒20 13-04- 12) based on a synchronous contex⇒t free grammar. Synchronous rules map the source language to formally defined operators for manipulating times (FINDENCLOSED, STARTATENDOF, etc.). Time expressions are then parsed using an extended CYK+ algorithm, and converted to a normalized form by applying the operators recursively. For evaluation, a small set of synchronous rules for English time expressions were developed. Our model outperforms HeidelTime, the best time normalization system in TempEval 2013, on four different time normalization corpora.</p><p>4 0.43792558 <a title="9-lsi-4" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<p>Author: John Philip McCrae ; Philipp Cimiano ; Roman Klinger</p><p>Abstract: Cross-lingual topic modelling has applications in machine translation, word sense disambiguation and terminology alignment. Multilingual extensions of approaches based on latent (LSI), generative (LDA, PLSI) as well as explicit (ESA) topic modelling can induce an interlingual topic space allowing documents in different languages to be mapped into the same space and thus to be compared across languages. In this paper, we present a novel approach that combines latent and explicit topic modelling approaches in the sense that it builds on a set of explicitly defined topics, but then computes latent relations between these. Thus, the method combines the benefits of both explicit and latent topic modelling approaches. We show that on a crosslingual mate retrieval task, our model significantly outperforms LDA, LSI, and ESA, as well as a baseline that translates every word in a document into the target language.</p><p>5 0.40273428 <a title="9-lsi-5" href="./emnlp-2013-Pair_Language_Models_for_Deriving_Alternative_Pronunciations_and_Spellings_from_Pronunciation_Dictionaries.html">150 emnlp-2013-Pair Language Models for Deriving Alternative Pronunciations and Spellings from Pronunciation Dictionaries</a></p>
<p>Author: Russell Beckley ; Brian Roark</p><p>Abstract: Pronunciation dictionaries provide a readily available parallel corpus for learning to transduce between character strings and phoneme strings or vice versa. Translation models can be used to derive character-level paraphrases on either side of this transduction, allowing for the automatic derivation of alternative pronunciations or spellings. We examine finitestate and SMT-based methods for these related tasks, and demonstrate that the tasks have different characteristics finding alternative spellings is harder than alternative pronunciations and benefits from round-trip algorithms when the other does not. We also show that we can increase accuracy by modeling syllable stress. –</p><p>6 0.34978157 <a title="9-lsi-6" href="./emnlp-2013-This_Text_Has_the_Scent_of_Starbucks%3A_A_Laplacian_Structured_Sparsity_Model_for_Computational_Branding_Analytics.html">184 emnlp-2013-This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</a></p>
<p>7 0.34686685 <a title="9-lsi-7" href="./emnlp-2013-Mining_New_Business_Opportunities%3A_Identifying_Trend_related_Products_by_Leveraging_Commercial_Intents_from_Microblogs.html">131 emnlp-2013-Mining New Business Opportunities: Identifying Trend related Products by Leveraging Commercial Intents from Microblogs</a></p>
<p>8 0.34603468 <a title="9-lsi-8" href="./emnlp-2013-Gender_Inference_of_Twitter_Users_in_Non-English_Contexts.html">89 emnlp-2013-Gender Inference of Twitter Users in Non-English Contexts</a></p>
<p>9 0.34286615 <a title="9-lsi-9" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>10 0.3376939 <a title="9-lsi-10" href="./emnlp-2013-With_Blinkers_on%3A_Robust_Prediction_of_Eye_Movements_across_Readers.html">203 emnlp-2013-With Blinkers on: Robust Prediction of Eye Movements across Readers</a></p>
<p>11 0.33144325 <a title="9-lsi-11" href="./emnlp-2013-Success_with_Style%3A_Using_Writing_Style_to_Predict_the_Success_of_Novels.html">178 emnlp-2013-Success with Style: Using Writing Style to Predict the Success of Novels</a></p>
<p>12 0.31056687 <a title="9-lsi-12" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>13 0.30747652 <a title="9-lsi-13" href="./emnlp-2013-Semi-Markov_Phrase-Based_Monolingual_Alignment.html">167 emnlp-2013-Semi-Markov Phrase-Based Monolingual Alignment</a></p>
<p>14 0.29673243 <a title="9-lsi-14" href="./emnlp-2013-Using_Topic_Modeling_to_Improve_Prediction_of_Neuroticism_and_Depression_in_College_Students.html">199 emnlp-2013-Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students</a></p>
<p>15 0.29131526 <a title="9-lsi-15" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>16 0.28779632 <a title="9-lsi-16" href="./emnlp-2013-Authorship_Attribution_of_Micro-Messages.html">27 emnlp-2013-Authorship Attribution of Micro-Messages</a></p>
<p>17 0.28742126 <a title="9-lsi-17" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>18 0.28350464 <a title="9-lsi-18" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>19 0.27800706 <a title="9-lsi-19" href="./emnlp-2013-Feature_Noising_for_Log-Linear_Structured_Prediction.html">86 emnlp-2013-Feature Noising for Log-Linear Structured Prediction</a></p>
<p>20 0.27477342 <a title="9-lsi-20" href="./emnlp-2013-Joint_Learning_of_Phonetic_Units_and_Word_Pronunciations_for_ASR.html">115 emnlp-2013-Joint Learning of Phonetic Units and Word Pronunciations for ASR</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.025), (18, 0.046), (22, 0.03), (30, 0.089), (45, 0.011), (47, 0.334), (50, 0.02), (51, 0.174), (66, 0.044), (71, 0.055), (75, 0.018), (77, 0.023), (96, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81431711 <a title="9-lda-1" href="./emnlp-2013-A_Log-Linear_Model_for_Unsupervised_Text_Normalization.html">9 emnlp-2013-A Log-Linear Model for Unsupervised Text Normalization</a></p>
<p>Author: Yi Yang ; Jacob Eisenstein</p><p>Abstract: We present a unified unsupervised statistical model for text normalization. The relationship between standard and non-standard tokens is characterized by a log-linear model, permitting arbitrary features. The weights of these features are trained in a maximumlikelihood framework, employing a novel sequential Monte Carlo training algorithm to overcome the large label space, which would be impractical for traditional dynamic programming solutions. This model is implemented in a normalization system called UNLOL, which achieves the best known results on two normalization datasets, outperforming more complex systems. We use the output of UNLOL to automatically normalize a large corpus of social media text, revealing a set of coherent orthographic styles that underlie online language variation.</p><p>2 0.75970161 <a title="9-lda-2" href="./emnlp-2013-Word_Level_Language_Identification_in_Online_Multilingual_Communication.html">204 emnlp-2013-Word Level Language Identification in Online Multilingual Communication</a></p>
<p>Author: Dong Nguyen ; A. Seza Dogruoz</p><p>Abstract: Multilingual speakers switch between languages in online and spoken communication. Analyses of large scale multilingual data require automatic language identification at the word level. For our experiments with multilingual online discussions, we first tag the language of individual words using language models and dictionaries. Secondly, we incorporate context to improve the performance. We achieve an accuracy of 98%. Besides word level accuracy, we use two new metrics to evaluate this task.</p><p>3 0.75229657 <a title="9-lda-3" href="./emnlp-2013-Learning_Biological_Processes_with_Global_Constraints.html">118 emnlp-2013-Learning Biological Processes with Global Constraints</a></p>
<p>Author: Aju Thalappillil Scaria ; Jonathan Berant ; Mengqiu Wang ; Peter Clark ; Justin Lewis ; Brittany Harding ; Christopher D. Manning</p><p>Abstract: Biological processes are complex phenomena involving a series of events that are related to one another through various relationships. Systems that can understand and reason over biological processes would dramatically improve the performance of semantic applications involving inference such as question answering (QA) – specifically “How? ” and “Why? ” questions. In this paper, we present the task of process extraction, in which events within a process and the relations between the events are automatically extracted from text. We represent processes by graphs whose edges describe a set oftemporal, causal and co-reference event-event relations, and characterize the structural properties of these graphs (e.g., the graphs are connected). Then, we present a method for extracting relations between the events, which exploits these structural properties by performing joint in- ference over the set of extracted relations. On a novel dataset containing 148 descriptions of biological processes (released with this paper), we show significant improvement comparing to baselines that disregard process structure.</p><p>4 0.55803961 <a title="9-lda-4" href="./emnlp-2013-Paraphrasing_4_Microblog_Normalization.html">151 emnlp-2013-Paraphrasing 4 Microblog Normalization</a></p>
<p>Author: Wang Ling ; Chris Dyer ; Alan W Black ; Isabel Trancoso</p><p>Abstract: Compared to the edited genres that have played a central role in NLP research, microblog texts use a more informal register with nonstandard lexical items, abbreviations, and free orthographic variation. When confronted with such input, conventional text analysis tools often perform poorly. Normalization replacing orthographically or lexically idiosyncratic forms with more standard variants can improve performance. We propose a method for learning normalization rules from machine translations of a parallel corpus of microblog messages. To validate the utility of our approach, we evaluate extrinsically, showing that normalizing English tweets and then translating improves translation quality (compared to translating unnormalized text) using three standard web translation services as well as a phrase-based translation system trained — — on parallel microblog data.</p><p>5 0.53447497 <a title="9-lda-5" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>Author: Margaret Mitchell ; Jacqui Aguilar ; Theresa Wilson ; Benjamin Van Durme</p><p>Abstract: We propose a novel approach to sentiment analysis for a low resource setting. The intuition behind this work is that sentiment expressed towards an entity, targeted sentiment, may be viewed as a span of sentiment expressed across the entity. This representation allows us to model sentiment detection as a sequence tagging problem, jointly discovering people and organizations along with whether there is sentiment directed towards them. We compare performance in both Spanish and English on microblog data, using only a sentiment lexicon as an external resource. By leveraging linguisticallyinformed features within conditional random fields (CRFs) trained to minimize empirical risk, our best models in Spanish significantly outperform a strong baseline, and reach around 90% accuracy on the combined task of named entity recognition and sentiment prediction. Our models in English, trained on a much smaller dataset, are not yet statistically significant against their baselines.</p><p>6 0.53238356 <a title="9-lda-6" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>7 0.53154618 <a title="9-lda-7" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>8 0.53026676 <a title="9-lda-8" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>9 0.52827376 <a title="9-lda-9" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>10 0.52744335 <a title="9-lda-10" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>11 0.52653962 <a title="9-lda-11" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>12 0.5263865 <a title="9-lda-12" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>13 0.52601266 <a title="9-lda-13" href="./emnlp-2013-Learning_to_Rank_Lexical_Substitutions.html">123 emnlp-2013-Learning to Rank Lexical Substitutions</a></p>
<p>14 0.52539837 <a title="9-lda-14" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>15 0.525316 <a title="9-lda-15" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>16 0.5249297 <a title="9-lda-16" href="./emnlp-2013-Classifying_Message_Board_Posts_with_an_Extracted_Lexicon_of_Patient_Attributes.html">46 emnlp-2013-Classifying Message Board Posts with an Extracted Lexicon of Patient Attributes</a></p>
<p>17 0.52473634 <a title="9-lda-17" href="./emnlp-2013-Leveraging_Lexical_Cohesion_and_Disruption_for_Topic_Segmentation.html">124 emnlp-2013-Leveraging Lexical Cohesion and Disruption for Topic Segmentation</a></p>
<p>18 0.52441502 <a title="9-lda-18" href="./emnlp-2013-Semi-Markov_Phrase-Based_Monolingual_Alignment.html">167 emnlp-2013-Semi-Markov Phrase-Based Monolingual Alignment</a></p>
<p>19 0.52384901 <a title="9-lda-19" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>20 0.52325773 <a title="9-lda-20" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
