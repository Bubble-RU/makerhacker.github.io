<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-134" href="#">emnlp2013-134</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</h1>
<br/><p>Source: <a title="emnlp-2013-134-pdf" href="http://aclweb.org/anthology//D/D13/D13-1014.pdf">pdf</a></p><p>Author: Masashi Tsubaki ; Kevin Duh ; Masashi Shimbo ; Yuji Matsumoto</p><p>Abstract: We present a novel vector space model for semantic co-compositionality. Inspired by Generative Lexicon Theory (Pustejovsky, 1995), our goal is a compositional model where both predicate and argument are allowed to modify each others’ meaning representations while generating the overall semantics. This readily addresses some major challenges with current vector space models, notably the polysemy issue and the use of one representation per word type. We implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations. We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality. The model achieves the best result to date (ρ = 0.47) on the semantic similarity task of transitive verbs (Grefenstette and Sadrzadeh, 2011).</p><p>Reference: <a title="emnlp-2013-134-reference" href="../emnlp2013_reference/emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Inspired by Generative Lexicon Theory (Pustejovsky, 1995), our goal is a compositional model where both predicate and argument are allowed to modify each others’ meaning representations while generating the overall semantics. [sent-4, score-0.427]
</p><p>2 This readily addresses some major challenges with current vector space models, notably the polysemy issue and the use of one representation per word type. [sent-5, score-0.313]
</p><p>3 We implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations. [sent-6, score-0.823]
</p><p>4 We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality. [sent-7, score-0.308]
</p><p>5 Much research has addressed the question of how to construct individual word representations, for example distributional models (Mitchell and Lapata, 2010) and neural models (Collobert and Weston, 2008). [sent-11, score-0.206]
</p><p>6 Recently, modeling of semantic compositionality (Frege, 1892) in vector space has emerged as another important line of research (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zam-  parelli, 2010; Socher et al. [sent-16, score-0.274]
</p><p>7 We propose such a model here, using what we call prototype projections. [sent-30, score-0.637]
</p><p>8 For each predicate, we transform its vector representation by projecting it into a latent space that is prototypical of its argument. [sent-31, score-0.235]
</p><p>9 This projection is performed analogously for each argument as well, and the final meaning is computed by composition of these transformed vectors (Figure 1). [sent-32, score-0.354]
</p><p>10 In addition, the model is cast as a neural network where word representations could be re-trained or fine-tuned. [sent-33, score-0.308]
</p><p>11 This model, based on prototype projections, is easy to implement  and achieves state-of-the-art performance in the sentence similarity dataset developed by Grefenstette and Sadrzadeh (201 1). [sent-36, score-0.703]
</p><p>12 We operate within the vector space model of distributional semantics, so these ideas are implemented with matrix algebra, which is a natural fit with neural networks. [sent-40, score-0.343]
</p><p>13 We further propose an unsupervised neural network training algorithm that jointly fine-tunes the word representations within the co-composition model, resulting in even better performance on the sentence similarity task. [sent-42, score-0.345]
</p><p>14 Semantics research is divided in two  strands, one focusing on learning word representations without consideration for compositionality, and the other focusing on compositional semantics using the representations only as an input. [sent-44, score-0.518]
</p><p>15 Our neural network model bridges these two strands of research by modeling cocompositionality and learning word representations simultaneously. [sent-46, score-0.385]
</p><p>16 1  Simple Distributional Semantic space (SDS) word vectors Word meaning is often represented in a high dimensional space, where each element corresponds to some contextual element in which the word is found. [sent-54, score-0.273]
</p><p>17 2  Neural Language Model (NLM) word embeddings Another popular way to learn word representations is based on the Neural Language Model (NLM) (Bengio et al. [sent-59, score-0.235]
</p><p>18 These dense feature vectors are usually called word embeddings, and it has been shown that such vectors can capture interesting linear relationships, such as king man + woman ≈ queen (Mikolov ceth al. [sent-64, score-0.215]
</p><p>19 The idea is to construct a neural network based on word sequences, where one outputs high scores for n-grams that occur in a large unlabeled corpus and low scores for nonsense n-grams where one word is replaced by a random word. [sent-69, score-0.234]
</p><p>20 ; d(wm)], where wi is ith word in the sequence, m is the window size, d(w) is the vector representation of word w (an n-dimensional column vector) and [d(w1) ; d(w2) ; . [sent-77, score-0.238]
</p><p>21 ; d(wm)] is the concatenation of word vectors as an input of neural network. [sent-80, score-0.214]
</p><p>22 Our model handles both cases using the concept of projection to latent prototype space. [sent-97, score-0.77]
</p><p>23 The fundamental idea is that for each word w and a syntactic/semantic (binary) relation R (such as verb-object relation), w has a set of prototype words with which it frequently occurs in relation R. [sent-98, score-0.75]
</p><p>24 For example, if w is a word company, and R is the object-verb relation, prototype words should include start, build, and buy (Figure 1). [sent-99, score-0.678]
</p><p>25 For each word-relation pair, we pre-compute the latent semantic subspace spanned by these prototype words. [sent-100, score-0.743]
</p><p>26 Later, when we encounter a phrase expressing a relation R between two words w1 and w2, each word is first projected onto a latent subspace determined by the other word and relation R. [sent-101, score-0.275]
</p><p>27 The projection operation shifts the meaning of individual words in accordance with context, and through this operation we realize coercion/co-composition. [sent-102, score-0.211]
</p><p>28 First, we collect from a corpus a set of prototype words that occur frequently in relation R with target word w0. [sent-105, score-0.714]
</p><p>29 Original verb and landmark verb similarity, prototype projected verb and landmark verb similarity, as measure by cosine using Collobert and Weston’s word embeddings. [sent-111, score-1.307]
</p><p>30 Meet has a abstract meaning itself, but after prototype projection with matrix constructed by word vectors of W( VerbOf, criterion), meet  is more close to meaning of satisfy. [sent-112, score-1.126]
</p><p>31 ×  Now let W(R, w0) = {w1, w2, · · · , wm} be the m prototype words we collected, ·a·n·d , wlet d(w) denote the n-dimensional (column) vector representation of word w (either by SDS or NLM representation). [sent-117, score-0.834]
</p><p>32 We make an m n matrix C(R,w0) by stacking tthioen prototype we aonrd m vectors, i. [sent-118, score-0.689]
</p><p>33 We call this matrix the prototype space o·f , wword} w0 wei ctha respect taot rreixla tthioen p Rro. [sent-125, score-0.731]
</p><p>34 133 Note that the matrix of orthogonal projection onto this prototype space is given by P(R,w0) = (ΣkVkT)T(ΣkVkT). [sent-126, score-0.864]
</p><p>35 Hence, when we observe a relation R(w0, w), the projected representation of word w in this context is computed by prpj(R,w0) (w) defined as follows:  prpj(R,w0)(w) = P(R,w0)d(w). [sent-127, score-0.208]
</p><p>36 (6)  Table 1 shows several examples of how meanings change after prototype projection using word embeddings of Collobert and Weston (2008). [sent-128, score-0.849]
</p><p>37 2  Co-Compositionality  In order to model co-compositionality, we apply prototype projection to both the verb and the object. [sent-130, score-0.857]
</p><p>38 In particular, suppose verb is wv and object is wo, C(VerbOf,wo) is used to project wv and C(ObjOf,wv) is used to project wo. [sent-131, score-0.434]
</p><p>39 The vector that represents the overall meaning of verb-object with prototype projection is computed by: cocomp(wv, wo) =  f(prpj(VerbOf,wo)(wv), prpj(ObjOf,wv)(wo)) (7) Function f can be a compositional computation like simple addition or element-wise multiplication of two vectors. [sent-132, score-1.139]
</p><p>40 4  Unsupervised Learning of Co-Compositionality  In this section, we propose a new neural language model that learns word representations while jointly accounting for compositional semantics. [sent-134, score-0.465]
</p><p>41 One central assumption of our work (and many other works in compositional semantics) is that a single vector 2ronan . [sent-135, score-0.291]
</p><p>42 We will show that such an assumption is quite reasonable under our model, since the prototype projections successfully tease out the proper semantics from these aggregate representations. [sent-140, score-0.743]
</p><p>43 However, it is natural to wonder whether one can do better if one incorporates the compositional model into the training of the word representations  in the first place. [sent-141, score-0.363]
</p><p>44 1 Compositional Neural Language Model (C-NLM) Compositional Neural Language Model (C-NLM) is a combination of a word representation learning method and compositional rule. [sent-149, score-0.32]
</p><p>45 In contrast to other compositional models based on machine learning, our model has no complex parameters for modeling composition. [sent-150, score-0.207]
</p><p>46 134  Figure 4: Co-Compositional Neural Language Model (CoC-NLM) is C-NLM with prototype projection. [sent-156, score-0.637]
</p><p>47 First, given some initial word representations and raw sentences, we compute the compositional vector with function f (in this section, we will assume that we will be using the  addition operator). [sent-158, score-0.447]
</p><p>48 Second, in order to obtain the score of compositional vector, we compute the dot product with vector s ∈ Rn (n is the dimension of tphreo dwuoctrd w vector space): v eRrb vector v = d(wv) and object vector o = d(wo). [sent-159, score-0.625]
</p><p>49 The new verb vector vnew trained within additive compositionality is just vnew = znew − o. [sent-167, score-0.426]
</p><p>50 2  Co-Compositional Neural Language Model (CoC-NLM) We now add prototype projection into C-NLM,  making our final model: Co-Compositional Neural Language Model (CoC-NLM). [sent-170, score-0.77]
</p><p>51 We define the score function as dot product of s and additional vector of prototype projected vectors (Figure 4). [sent-171, score-0.882]
</p><p>52 Second, the projected verb vector is updated as xnew = znew − y. [sent-177, score-0.33]
</p><p>53 1 Dataset In order to evaluate the performance of our new co-compositional model with prototype projection  and word representation learning algorithm, we make use of the disambiguation task of transitive sentences developed by Grefenstette and Sadrzadeh (201 1). [sent-180, score-0.932]
</p><p>54 The dataset consists of similarity judgments between a landmark verb and a triple consisting of a transitive target verb, subject and object extracted from the BNC corpus. [sent-182, score-0.368]
</p><p>55 For example, Table 2 shows some examples from the data: we see that the verb meet with subject system and object criterion is judged similar to the landmark verb satisfy but not visit. [sent-184, score-0.405]
</p><p>56 4 The task is to have the model produce a score for each pair of landmark verb and verb-subject-object triple. [sent-186, score-0.198]
</p><p>57 Verb meet with subject system and object criterion is judged similar to the landmark verb satisfy but not visit. [sent-195, score-0.318]
</p><p>58 This model computes the outer product of the subject and object vector, the outer product of the verb vector with itself, and then the element-wise product of both results. [sent-202, score-0.348]
</p><p>59 We use ukWaC corpus to collect W(VerbOf, wo) and W(ObjOf, wv) for prototype projections. [sent-226, score-0.637]
</p><p>60 We set the number of prototype vectors to be m = 20, where W( VerbOf, wo) is filtered with high frequency words and W(ObjOf, wv) is filtered with both high frequency and high similarity words. [sent-231, score-0.745]
</p><p>61 This implies that the prototype projection is robust to the underlying word representation, which is a desired characteristic of compositional models. [sent-250, score-1.045]
</p><p>62 The number of prototype words m = 20 in all our models. [sent-257, score-0.637]
</p><p>63 It is simply implementing the co-compositionality idea using prototype projections (Section 3. [sent-270, score-0.703]
</p><p>64 We focus on the additive model of Compositional NLM, both basic and prototype projection. [sent-276, score-0.672]
</p><p>65 In contrast to other previous compositional models, our model does not require estimating a large number of parameters for computation of compositional vectors and word representation itself is more suitable for  it. [sent-288, score-0.598]
</p><p>66 3 The number of prototype words The number of prototype words (m in Figure 1) we use to generate the prototype space is one hyperparameter that our model has. [sent-292, score-1.953]
</p><p>67 Figure 5 shows the relation of m and the performance of co-compositional model with prototype projections using either SDS or NLM representations. [sent-294, score-0.739]
</p><p>68 4 Variations in model configuration We have presented a compositional model of the form d(ws) + cocomp(wv, wo), where prototype  projections are performed on both wv and wo and ws is composed as is without projection. [sent-300, score-1.199]
</p><p>69 Here in Table 5 we show the results of 137  Figure 5: The relation between the number of prototype words and correlation of SDS or NLM. [sent-302, score-0.705]
</p><p>70 43 n187almode, SubjVerbObjNLM ρSDS ρ  based on how subject, verb, and object vector representations are included. [sent-306, score-0.25]
</p><p>71 + indicates that the vector is added without projection first. [sent-308, score-0.217]
</p><p>72 Blank indicates that the vector is not used in the final compositional score. [sent-309, score-0.291]
</p><p>73 7  Related work  In recent years, several sophisticated vector space models have been proposed for computing compositional semantics. [sent-314, score-0.333]
</p><p>74 However, it may not be sufficiently expressive to represent the various factors involved in compositional semantics, such as syntax and context. [sent-319, score-0.207]
</p><p>75 To this end, Baroni and Zamparelli (2010) present a compositional model for adjectives and nouns. [sent-320, score-0.207]
</p><p>76 The vector captures the actual meaning of the word itself, while the matrix is modeled as a operator that modify the meaning of neighboring words and phrases. [sent-327, score-0.367]
</p><p>77 This model captures semantic change phenomenon like not bad is similar to good due to a composition of the bad vector with a meaning-flipping not matrix. [sent-328, score-0.2]
</p><p>78 Our prototype projection model is similar to these models as a matrix-vector operation, except that the matrix is not learned and computed from prototype words. [sent-330, score-1.459]
</p><p>79 In future work, we can imagine integrating the two models, using these prototype projection matrices as initial values for MV-RNN training (Socher et al. [sent-331, score-0.81]
</p><p>80 In their mathematical framework unifying categorical logic and vector space models, the sentence vector is modeled as a function of the Kronecker product of its word vectors. [sent-335, score-0.282]
</p><p>81 They also developed a new semantic similarity task based on transitive 138  Composition OperatorParameter  bine two word vector representations, u, v ∈ Rn and tbhineeir learning parameters. [sent-338, score-0.255]
</p><p>82 sOeunrt mtioondse,l only ∈nee Rds two hyper-parameters: the number of prototype words m and dimensional reduction k in SVD  ×  verbs, which is the dataset we used here. [sent-339, score-0.637]
</p><p>83 Blacoe and Lapata (2012) highlight the importance of jointly examining word representations and compositionality operators. [sent-345, score-0.26]
</p><p>84 (2013) describe the relation between word vector and compositionality in more detail with free parameters. [sent-348, score-0.265]
</p><p>85 Table 6 summarizes some ways to compose the meaning of two word vectors (u, v), following (Dinu et al. [sent-349, score-0.19]
</p><p>86 Both methods are characterized by the use of selectional preference information for subjects, verbs, and objects in context; our prototype word vectors are essentially equivalent to this idea. [sent-361, score-0.749]
</p><p>87 The main difference is in how we modify the target word representation v using this information: whereas we project v onto a latent subspace formed by collection of prototype vectors, Erk and Pad o´ (2008; 2009) and Thater et al. [sent-362, score-0.812]
</p><p>88 (2010; 2011) use the prototype vectors to directly modify the elements of v, i. [sent-363, score-0.708]
</p><p>89 Intuitively, both our method and theirs essentially delete part of a word vector representation to adapt the meaning in context. [sent-366, score-0.275]
</p><p>90 We believe the projection is more robust to the underlying word representation (and this is shown in the results for SDS vs. [sent-367, score-0.273]
</p><p>91 NLM  representations), but we note that we may be able to borrow some of more sophisticated ways to find prototype vectors from Erk and Pad o´ (2008; 2009) and Thater et al. [sent-368, score-0.708]
</p><p>92 8  Conclusion and Future Work  We began this work by asking how it is possible to handle polysemy issues in compositional semantics, especially when adopting distributional semantics methods that construct only one representation per word type. [sent-370, score-0.497]
</p><p>93 , the meaning of an ambiguous verb is generated by the properties the object it takes, and vice versa. [sent-374, score-0.216]
</p><p>94 We implement this idea in a novel neural network model using prototype projections. [sent-375, score-0.818]
</p><p>95 Frege in space: A program for compositional distributional semantics. [sent-395, score-0.27]
</p><p>96 Mathematical foundations for a compositional distributional model of meaning. [sent-407, score-0.27]
</p><p>97 General estimation and evaluation of compositional distributional semantic models. [sent-420, score-0.314]
</p><p>98 A structured vector space model for word meaning in context. [sent-424, score-0.245]
</p><p>99 Experimental support for a categorical compositional distributional model of meaning. [sent-440, score-0.27]
</p><p>100 Improving word representations via global context and multiple word prototypes. [sent-448, score-0.197]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('prototype', 0.637), ('nlm', 0.285), ('sds', 0.212), ('compositional', 0.207), ('wv', 0.148), ('projection', 0.133), ('representations', 0.115), ('erk', 0.112), ('landmark', 0.111), ('compositionality', 0.104), ('neural', 0.102), ('wo', 0.101), ('sadrzadeh', 0.096), ('svd', 0.089), ('verb', 0.087), ('pad', 0.085), ('grefenstette', 0.085), ('vector', 0.084), ('prpj', 0.084), ('verbof', 0.084), ('meaning', 0.078), ('wm', 0.074), ('polysemy', 0.074), ('representation', 0.072), ('collobert', 0.072), ('composition', 0.072), ('vectors', 0.071), ('cocomp', 0.067), ('pobjv', 0.067), ('thater', 0.067), ('projections', 0.066), ('lapata', 0.066), ('distributional', 0.063), ('subspace', 0.062), ('projected', 0.059), ('baroni', 0.059), ('mitchell', 0.054), ('blacoe', 0.053), ('cruys', 0.053), ('matrix', 0.052), ('object', 0.051), ('cocompositionality', 0.05), ('kvkt', 0.05), ('xnew', 0.05), ('znew', 0.05), ('network', 0.05), ('transitive', 0.049), ('weston', 0.048), ('pustejovsky', 0.047), ('socher', 0.045), ('xc', 0.044), ('semantic', 0.044), ('company', 0.043), ('space', 0.042), ('word', 0.041), ('dinu', 0.041), ('ws', 0.04), ('matrices', 0.04), ('semantics', 0.04), ('embeddings', 0.038), ('prototypical', 0.037), ('coecke', 0.037), ('similarity', 0.037), ('relation', 0.036), ('optimize', 0.036), ('meet', 0.036), ('additive', 0.035), ('generative', 0.034), ('operator', 0.034), ('cruy', 0.033), ('objof', 0.033), ('pverbo', 0.033), ('vc', 0.033), ('vnew', 0.033), ('wverb', 0.033), ('tanh', 0.033), ('turian', 0.033), ('subject', 0.033), ('van', 0.033), ('correlation', 0.032), ('dense', 0.032), ('ukwac', 0.032), ('katrin', 0.032), ('product', 0.031), ('verbs', 0.03), ('marco', 0.029), ('implement', 0.029), ('jsps', 0.029), ('kakenhi', 0.029), ('masashi', 0.029), ('corrupted', 0.029), ('frege', 0.029), ('run', 0.027), ('predicate', 0.027), ('underlying', 0.027), ('rh', 0.027), ('ought', 0.027), ('geometrical', 0.027), ('strands', 0.027), ('reisinger', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="134-tfidf-1" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>Author: Masashi Tsubaki ; Kevin Duh ; Masashi Shimbo ; Yuji Matsumoto</p><p>Abstract: We present a novel vector space model for semantic co-compositionality. Inspired by Generative Lexicon Theory (Pustejovsky, 1995), our goal is a compositional model where both predicate and argument are allowed to modify each others’ meaning representations while generating the overall semantics. This readily addresses some major challenges with current vector space models, notably the polysemy issue and the use of one representation per word type. We implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations. We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality. The model achieves the best result to date (ρ = 0.47) on the semantic similarity task of transitive verbs (Grefenstette and Sadrzadeh, 2011).</p><p>2 0.16995446 <a title="134-tfidf-2" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>Author: Dimitri Kartsaklis ; Mehrnoosh Sadrzadeh</p><p>Abstract: Recent work has shown that compositionaldistributional models using element-wise operations on contextual word vectors benefit from the introduction of a prior disambiguation step. The purpose of this paper is to generalise these ideas to tensor-based models, where relational words such as verbs and adjectives are represented by linear maps (higher order tensors) acting on a number of arguments (vectors). We propose disambiguation algorithms for a number of tensor-based models, which we then test on a variety of tasks. The results show that disambiguation can provide better compositional representation even for the case of tensor-based models. Further- more, we confirm previous findings regarding the positive effect of disambiguation on vector mixture models, and we compare the effectiveness of the two approaches.</p><p>3 0.13936873 <a title="134-tfidf-3" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>Author: Richard Socher ; Alex Perelygin ; Jean Wu ; Jason Chuang ; Christopher D. Manning ; Andrew Ng ; Christopher Potts</p><p>Abstract: Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.</p><p>4 0.13805017 <a title="134-tfidf-4" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>Author: Douwe Kiela ; Stephen Clark</p><p>Abstract: We present a novel unsupervised approach to detecting the compositionality of multi-word expressions. We compute the compositionality of a phrase through substituting the constituent words with their “neighbours” in a semantic vector space and averaging over the distance between the original phrase and the substituted neighbour phrases. Several methods of obtaining neighbours are presented. The results are compared to existing supervised results and achieve state-of-the-art performance on a verb-object dataset of human compositionality ratings.</p><p>5 0.12568721 <a title="134-tfidf-5" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>Author: Angeliki Lazaridou ; Eva Maria Vecchi ; Marco Baroni</p><p>Abstract: In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the correct parsing of NPs (e.g., (live fish) transporter rather than live (fish transporter)). We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features.</p><p>6 0.10288118 <a title="134-tfidf-6" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>7 0.091018341 <a title="134-tfidf-7" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>8 0.088990889 <a title="134-tfidf-8" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>9 0.08898031 <a title="134-tfidf-9" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>10 0.084476903 <a title="134-tfidf-10" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>11 0.084136792 <a title="134-tfidf-11" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>12 0.077531077 <a title="134-tfidf-12" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>13 0.077369921 <a title="134-tfidf-13" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>14 0.075476326 <a title="134-tfidf-14" href="./emnlp-2013-Deriving_Adjectival_Scales_from_Continuous_Space_Word_Representations.html">59 emnlp-2013-Deriving Adjectival Scales from Continuous Space Word Representations</a></p>
<p>15 0.061964203 <a title="134-tfidf-15" href="./emnlp-2013-Identifying_Phrasal_Verbs_Using_Many_Bilingual_Corpora.html">96 emnlp-2013-Identifying Phrasal Verbs Using Many Bilingual Corpora</a></p>
<p>16 0.061092895 <a title="134-tfidf-16" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>17 0.056188595 <a title="134-tfidf-17" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>18 0.054508045 <a title="134-tfidf-18" href="./emnlp-2013-Learning_to_Rank_Lexical_Substitutions.html">123 emnlp-2013-Learning to Rank Lexical Substitutions</a></p>
<p>19 0.054255515 <a title="134-tfidf-19" href="./emnlp-2013-Multi-Relational_Latent_Semantic_Analysis.html">137 emnlp-2013-Multi-Relational Latent Semantic Analysis</a></p>
<p>20 0.052941561 <a title="134-tfidf-20" href="./emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations.html">17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.18), (1, 0.008), (2, -0.066), (3, -0.048), (4, 0.009), (5, 0.232), (6, -0.03), (7, -0.103), (8, -0.236), (9, -0.021), (10, 0.143), (11, 0.048), (12, -0.083), (13, 0.002), (14, -0.018), (15, -0.114), (16, 0.075), (17, -0.112), (18, -0.074), (19, -0.013), (20, 0.0), (21, 0.012), (22, -0.033), (23, -0.008), (24, -0.027), (25, 0.029), (26, -0.013), (27, -0.025), (28, 0.002), (29, -0.006), (30, -0.072), (31, -0.05), (32, -0.019), (33, -0.005), (34, -0.018), (35, -0.022), (36, -0.059), (37, 0.077), (38, 0.012), (39, 0.011), (40, -0.034), (41, 0.03), (42, 0.046), (43, 0.013), (44, -0.066), (45, 0.022), (46, 0.064), (47, -0.014), (48, 0.077), (49, -0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94250631 <a title="134-lsi-1" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>Author: Masashi Tsubaki ; Kevin Duh ; Masashi Shimbo ; Yuji Matsumoto</p><p>Abstract: We present a novel vector space model for semantic co-compositionality. Inspired by Generative Lexicon Theory (Pustejovsky, 1995), our goal is a compositional model where both predicate and argument are allowed to modify each others’ meaning representations while generating the overall semantics. This readily addresses some major challenges with current vector space models, notably the polysemy issue and the use of one representation per word type. We implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations. We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality. The model achieves the best result to date (ρ = 0.47) on the semantic similarity task of transitive verbs (Grefenstette and Sadrzadeh, 2011).</p><p>2 0.80569649 <a title="134-lsi-2" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>Author: Dimitri Kartsaklis ; Mehrnoosh Sadrzadeh</p><p>Abstract: Recent work has shown that compositionaldistributional models using element-wise operations on contextual word vectors benefit from the introduction of a prior disambiguation step. The purpose of this paper is to generalise these ideas to tensor-based models, where relational words such as verbs and adjectives are represented by linear maps (higher order tensors) acting on a number of arguments (vectors). We propose disambiguation algorithms for a number of tensor-based models, which we then test on a variety of tasks. The results show that disambiguation can provide better compositional representation even for the case of tensor-based models. Further- more, we confirm previous findings regarding the positive effect of disambiguation on vector mixture models, and we compare the effectiveness of the two approaches.</p><p>3 0.73186207 <a title="134-lsi-3" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>Author: Eva Maria Vecchi ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: In this study, we use compositional distributional semantic methods to investigate restrictions in adjective ordering. Specifically, we focus on properties distinguishing AdjectiveAdjective-Noun phrases in which there is flexibility in the adjective ordering from those bound to a rigid order. We explore a number of measures extracted from the distributional representation of AAN phrases which may indicate a word order restriction. We find that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modification of the adjectives. Our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics.</p><p>4 0.73174417 <a title="134-lsi-4" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>Author: Kazuma Hashimoto ; Makoto Miwa ; Yoshimasa Tsuruoka ; Takashi Chikayama</p><p>Abstract: In this paper, we present a recursive neural network (RNN) model that works on a syntactic tree. Our model differs from previous RNN models in that the model allows for an explicit weighting of important phrases for the target task. We also propose to average parameters in training. Our experimental results on semantic relation classification show that both phrase categories and task-specific weighting significantly improve the prediction accuracy of the model. We also show that averaging the model parameters is effective in stabilizing the learning and improves generalization capacity. The proposed model marks scores competitive with state-of-the-art RNN-based models.</p><p>5 0.72610551 <a title="134-lsi-5" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>Author: Douwe Kiela ; Stephen Clark</p><p>Abstract: We present a novel unsupervised approach to detecting the compositionality of multi-word expressions. We compute the compositionality of a phrase through substituting the constituent words with their “neighbours” in a semantic vector space and averaging over the distance between the original phrase and the substituted neighbour phrases. Several methods of obtaining neighbours are presented. The results are compared to existing supervised results and achieve state-of-the-art performance on a verb-object dataset of human compositionality ratings.</p><p>6 0.71348375 <a title="134-lsi-6" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>7 0.64837956 <a title="134-lsi-7" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>8 0.64401877 <a title="134-lsi-8" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>9 0.54820764 <a title="134-lsi-9" href="./emnlp-2013-Multi-Relational_Latent_Semantic_Analysis.html">137 emnlp-2013-Multi-Relational Latent Semantic Analysis</a></p>
<p>10 0.5366658 <a title="134-lsi-10" href="./emnlp-2013-Deriving_Adjectival_Scales_from_Continuous_Space_Word_Representations.html">59 emnlp-2013-Deriving Adjectival Scales from Continuous Space Word Representations</a></p>
<p>11 0.45176497 <a title="134-lsi-11" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>12 0.40711284 <a title="134-lsi-12" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>13 0.40251854 <a title="134-lsi-13" href="./emnlp-2013-Decoding_with_Large-Scale_Neural_Language_Models_Improves_Translation.html">55 emnlp-2013-Decoding with Large-Scale Neural Language Models Improves Translation</a></p>
<p>14 0.39774838 <a title="134-lsi-14" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>15 0.39522645 <a title="134-lsi-15" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>16 0.37032577 <a title="134-lsi-16" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>17 0.36242381 <a title="134-lsi-17" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>18 0.35597271 <a title="134-lsi-18" href="./emnlp-2013-The_VerbCorner_Project%3A_Toward_an_Empirically-Based_Semantic_Decomposition_of_Verbs.html">183 emnlp-2013-The VerbCorner Project: Toward an Empirically-Based Semantic Decomposition of Verbs</a></p>
<p>19 0.35442978 <a title="134-lsi-19" href="./emnlp-2013-Unsupervised_Spectral_Learning_of_WCFG_as_Low-rank_Matrix_Completion.html">195 emnlp-2013-Unsupervised Spectral Learning of WCFG as Low-rank Matrix Completion</a></p>
<p>20 0.33596846 <a title="134-lsi-20" href="./emnlp-2013-Recurrent_Continuous_Translation_Models.html">156 emnlp-2013-Recurrent Continuous Translation Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.03), (6, 0.354), (9, 0.012), (18, 0.054), (22, 0.035), (30, 0.072), (45, 0.01), (50, 0.02), (51, 0.133), (66, 0.034), (71, 0.04), (75, 0.034), (77, 0.023), (90, 0.02), (96, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74753463 <a title="134-lda-1" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>Author: Masashi Tsubaki ; Kevin Duh ; Masashi Shimbo ; Yuji Matsumoto</p><p>Abstract: We present a novel vector space model for semantic co-compositionality. Inspired by Generative Lexicon Theory (Pustejovsky, 1995), our goal is a compositional model where both predicate and argument are allowed to modify each others’ meaning representations while generating the overall semantics. This readily addresses some major challenges with current vector space models, notably the polysemy issue and the use of one representation per word type. We implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations. We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality. The model achieves the best result to date (ρ = 0.47) on the semantic similarity task of transitive verbs (Grefenstette and Sadrzadeh, 2011).</p><p>2 0.69428152 <a title="134-lda-2" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>Author: Richard Socher ; Alex Perelygin ; Jean Wu ; Jason Chuang ; Christopher D. Manning ; Andrew Ng ; Christopher Potts</p><p>Abstract: Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.</p><p>3 0.52337575 <a title="134-lda-3" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>Author: Angeliki Lazaridou ; Eva Maria Vecchi ; Marco Baroni</p><p>Abstract: In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the correct parsing of NPs (e.g., (live fish) transporter rather than live (fish transporter)). We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features.</p><p>4 0.50537324 <a title="134-lda-4" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>Author: Kazuma Hashimoto ; Makoto Miwa ; Yoshimasa Tsuruoka ; Takashi Chikayama</p><p>Abstract: In this paper, we present a recursive neural network (RNN) model that works on a syntactic tree. Our model differs from previous RNN models in that the model allows for an explicit weighting of important phrases for the target task. We also propose to average parameters in training. Our experimental results on semantic relation classification show that both phrase categories and task-specific weighting significantly improve the prediction accuracy of the model. We also show that averaging the model parameters is effective in stabilizing the learning and improves generalization capacity. The proposed model marks scores competitive with state-of-the-art RNN-based models.</p><p>5 0.50267047 <a title="134-lda-5" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>Author: Peng Li ; Yang Liu ; Maosong Sun</p><p>Abstract: While inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (i.e., straight and inverted) dependent on actual blocks being merged remains a challenge. Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points.</p><p>6 0.49643248 <a title="134-lda-6" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>7 0.49133506 <a title="134-lda-7" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>8 0.47774845 <a title="134-lda-8" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>9 0.47481313 <a title="134-lda-9" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>10 0.46128115 <a title="134-lda-10" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>11 0.4493261 <a title="134-lda-11" href="./emnlp-2013-Recurrent_Continuous_Translation_Models.html">156 emnlp-2013-Recurrent Continuous Translation Models</a></p>
<p>12 0.44116223 <a title="134-lda-12" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>13 0.43948674 <a title="134-lda-13" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>14 0.43920034 <a title="134-lda-14" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>15 0.437677 <a title="134-lda-15" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>16 0.43452108 <a title="134-lda-16" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>17 0.43426847 <a title="134-lda-17" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>18 0.43408737 <a title="134-lda-18" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>19 0.43386543 <a title="134-lda-19" href="./emnlp-2013-Scaling_Semantic_Parsers_with_On-the-Fly_Ontology_Matching.html">164 emnlp-2013-Scaling Semantic Parsers with On-the-Fly Ontology Matching</a></p>
<p>20 0.43023512 <a title="134-lda-20" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
