<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>92 emnlp-2013-Growing Multi-Domain Glossaries from a Few Seeds using Probabilistic Topic Models</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-92" href="#">emnlp2013-92</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>92 emnlp-2013-Growing Multi-Domain Glossaries from a Few Seeds using Probabilistic Topic Models</h1>
<br/><p>Source: <a title="emnlp-2013-92-pdf" href="http://aclweb.org/anthology//D/D13/D13-1018.pdf">pdf</a></p><p>Author: Stefano Faralli ; Roberto Navigli</p><p>Abstract: In this paper we present a minimallysupervised approach to the multi-domain acquisition ofwide-coverage glossaries. We start from a small number of hypernymy relation seeds and bootstrap glossaries from the Web for dozens of domains using Probabilistic Topic Models. Our experiments show that we are able to extract high-precision glossaries comprising thousands of terms and definitions.</p><p>Reference: <a title="emnlp-2013-92-reference" href="../emnlp2013_reference/emnlp-2013-Growing_Multi-Domain_Glossaries_from_a_Few_Seeds_using_Probabilistic_Topic_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Growing Multi-Domain Glossaries from a Few Seeds using Probabilistic Topic Models Stefano Faralli and Roberto Navigli Dipartimento di Informatica Sapienza Universit a` di Roma {faral l ,navigl i}@ di . [sent-1, score-0.153]
</p><p>2 it i  Abstract In this paper we present a minimallysupervised approach to the multi-domain acquisition ofwide-coverage glossaries. [sent-3, score-0.094]
</p><p>3 We start from a small number of hypernymy relation seeds and bootstrap glossaries from the Web for dozens of domains using Probabilistic Topic Models. [sent-4, score-1.065]
</p><p>4 Our experiments show that we are able to extract high-precision glossaries comprising thousands of terms and definitions. [sent-5, score-0.529]
</p><p>5 1 Introduction Dictionaries, thesauri and glossaries are useful sources of information for students, scholars and everyday readers, who use them to look up words of which they either do not know, or have forgotten, the meaning. [sent-6, score-0.61]
</p><p>6 With the advent of the Web an increasing number of dictionaries and technical glossaries has been made available online, thereby speeding  up the definition search process. [sent-7, score-0.636]
</p><p>7 However, finding definitions is not always immediate, especially if the target term pertains to a specialized domain. [sent-8, score-0.277]
</p><p>8 Indeed, not even well-known services such as Google Define are able to provide definitions for scientific or technical terms such as taxonomy learning or distant supervision in AI or figure-four leglock and suspended surfboard in wrestling. [sent-9, score-0.394]
</p><p>9 , 2007), Word Sense Disambiguation (Duan and Yates, 2010; Faralli and 170 Navigli, 2012) and ontology learning (Velardi et al. [sent-13, score-0.028]
</p><p>10 Unfortunately, most of the Web dictionaries and glossaries available online comprise just a few hundred definitions, and they therefore provide only a partial view of a domain. [sent-15, score-0.622]
</p><p>11 This is also the case with manually compiled glossaries created by means of collaborative efforts, such as Wikipedia. [sent-16, score-0.529]
</p><p>12 1 The coverage issue is addressed by online aggrega-  tion services such as Google Define, which bring together definitions from several online dictionaries. [sent-17, score-0.324]
</p><p>13 However, these services do not classify textual definitions by domain: they just present the collected definitions for all the possible meanings of a given term. [sent-18, score-0.5]
</p><p>14 In order to automatically obtain large domain glossaries, in recent years computational approaches have been developed which extract textual definitions from corpora (Navigli and Velardi, 2010; Reiplinger et al. [sent-19, score-0.383]
</p><p>15 The methods involving corpora start from a given set of terms (possibly automatically extracted from a domain corpus) and then harvest textual definitions for these terms from the input corpus using a supervised system. [sent-22, score-0.59]
</p><p>16 Web-based methods, instead, extract text snippets from Web pages which match pre-defined lexical patterns, such as “X is a Y”, along the lines of Hearst (1992). [sent-23, score-0.112]
</p><p>17 These approaches typically perform with high precision and low recall, because they fall short of detecting the high variability of the syntactic structure of textual definitions. [sent-24, score-0.094]
</p><p>18 To address the low-recall issue, recurring cue terms occurring  1See http : / / en . [sent-25, score-0.06]
</p><p>19 hc o2d0s1 i3n A Nsastoucria lti Loan fgoura Cgoem Ppruotcaetsiosin agl, L piang eusis 1t7ic0s–181, within dictionary and encyclopedic resources can be automatically extracted and incorporated into lexical patterns (Saggion, 2004). [sent-29, score-0.03]
</p><p>20 The goal of the new approach outlined in this paper is to enable the automatic harvesting of largescale, full-fledged domain glossaries for dozens of domains, an outcome which should be very useful for both human activities and automatic tasks. [sent-31, score-0.802]
</p><p>21 ProToDog is able to harvest definitions from the Web and thus drop the requirement of large corpora for each domain. [sent-36, score-0.271]
</p><p>22 It is thus applicable to virtually any language of interest. [sent-38, score-0.034]
</p><p>23 , dn}, for each dGoivmeanin a d s ∈ Df d PormoaTionDso DG =har {vdests a do}m,a fionr glossary Gd containing pairs Gof h tahrev eksitnsd a (t, g) winh egrleos tis a domain term and g is its textual definition, i. [sent-42, score-0.618]
</p><p>24 We show the pseudocode of ProToDoG in Algorithm 1. [sent-45, score-0.036]
</p><p>25 Initial seed selection:  Algorithm 1 takes  as input a set of domains D and, for each domain d ∈ D, a small set of hypernymy relation seeds Sd = {(t1, h1) , . [sent-47, score-0.671]
</p><p>26 ,max+1 Gdj  20: ← 21: end for← 22: return GS return  S=  {(Gd, d) : d  G  =  ∈  D}  {(G,d)  :  d  ∈  D}  pair (tj, hj) contains a term tj and its generalization hj (e. [sent-53, score-0.565]
</p><p>27 This is the only human input to the entire glossary acquisition process. [sent-56, score-0.389]
</p><p>28 The selection of the input seeds plays a key role in the bootstrapping process, in that the pattern and gloss extraction process will be driven by them. [sent-57, score-0.362]
</p><p>29 The chosen hypernymy relations thus have to be as topical and representative as possible for the domain of interest (e. [sent-58, score-0.328]
</p><p>30 , (compiler, computer program) is an appropriate pair for computer science, while (byte, unit of measurement) is not, as it might cause the extraction of out-of-domain glossaries of units and measures). [sent-60, score-0.501]
</p><p>31 The algorithm first sets the iteration counter k to 1 (line 1) and starts the first iteration of the glos-  sary bootstrapping process (lines 2-17), each involving steps 2-4, described below. [sent-61, score-0.352]
</p><p>32 After each iteration k, for each domain d we keep track of the set of glosses Gdk acquired during that iteration. [sent-62, score-0.439]
</p><p>33 After the last iteration, we perform step (5) of gloss recovery (lines 18-21). [sent-63, score-0.133]
</p><p>34 Web search and glossary extraction (lines 3-9): For each domain d, we first initialize the domain glossary for iteration k: Gdk := ∅ (line 4). [sent-65, score-0.961]
</p><p>35 Then, for each seed pair (tj , hj) ∈ Sd, we isnuebm 4i). [sent-66, score-0.061]
</p><p>36 t the following query to a Web s)ea ∈rch S engine: “tj” “hj” glos sary and collect the top-ranking results for each query (line 6). [sent-67, score-0.143]
</p><p>37 2 Each resulting page is a candidate glossary for the domain d. [sent-68, score-0.444]
</p><p>38 We then call the extractGlossary function (line 7) which extracts terms and glosses from the retrieved pages as follows. [sent-69, score-0.259]
</p><p>39 From each candidate page, we harvest all the text snippets s starting with tj and ending with hj (e. [sent-70, score-0.595]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('glossaries', 0.501), ('glossary', 0.309), ('hj', 0.27), ('protodog', 0.243), ('glosses', 0.231), ('hypernymy', 0.193), ('definitions', 0.182), ('seeds', 0.181), ('tj', 0.181), ('sd', 0.155), ('gdk', 0.146), ('domain', 0.135), ('velardi', 0.127), ('gd', 0.108), ('faralli', 0.097), ('gloss', 0.097), ('roma', 0.097), ('navigli', 0.097), ('harvest', 0.089), ('sary', 0.085), ('dozens', 0.077), ('linux', 0.077), ('iteration', 0.073), ('domains', 0.073), ('services', 0.07), ('textual', 0.066), ('seed', 0.061), ('lines', 0.057), ('bootstrapping', 0.056), ('snippets', 0.055), ('web', 0.055), ('dictionaries', 0.054), ('acquisition', 0.052), ('di', 0.051), ('growing', 0.047), ('supervision', 0.044), ('compiler', 0.042), ('novelties', 0.042), ('minimallysupervised', 0.042), ('har', 0.042), ('suspended', 0.042), ('advent', 0.042), ('byte', 0.042), ('cont', 0.042), ('eda', 0.042), ('fionr', 0.042), ('harvests', 0.042), ('reiplinger', 0.042), ('saggion', 0.042), ('websearch', 0.042), ('topic', 0.041), ('bootstrap', 0.04), ('line', 0.039), ('speeding', 0.039), ('everyday', 0.039), ('recovery', 0.036), ('scholars', 0.036), ('sde', 0.036), ('dse', 0.036), ('dg', 0.036), ('pseudocode', 0.036), ('winh', 0.036), ('pertains', 0.036), ('yates', 0.036), ('online', 0.036), ('fujii', 0.034), ('thesauri', 0.034), ('virtually', 0.034), ('involving', 0.034), ('recurring', 0.032), ('simultaneous', 0.032), ('ach', 0.032), ('leveraged', 0.031), ('cui', 0.031), ('hundred', 0.031), ('counter', 0.031), ('yw', 0.031), ('harvesting', 0.031), ('othne', 0.031), ('term', 0.03), ('duan', 0.03), ('stefano', 0.03), ('encyclopedic', 0.03), ('activities', 0.03), ('query', 0.029), ('gs', 0.029), ('dn', 0.029), ('specialized', 0.029), ('probabilistic', 0.028), ('terms', 0.028), ('input', 0.028), ('return', 0.028), ('outlined', 0.028), ('variability', 0.028), ('compiled', 0.028), ('universit', 0.028), ('taxonomy', 0.028), ('df', 0.028), ('ontology', 0.028), ('end', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="92-tfidf-1" href="./emnlp-2013-Growing_Multi-Domain_Glossaries_from_a_Few_Seeds_using_Probabilistic_Topic_Models.html">92 emnlp-2013-Growing Multi-Domain Glossaries from a Few Seeds using Probabilistic Topic Models</a></p>
<p>Author: Stefano Faralli ; Roberto Navigli</p><p>Abstract: In this paper we present a minimallysupervised approach to the multi-domain acquisition ofwide-coverage glossaries. We start from a small number of hypernymy relation seeds and bootstrap glossaries from the Web for dozens of domains using Probabilistic Topic Models. Our experiments show that we are able to extract high-precision glossaries comprising thousands of terms and definitions.</p><p>2 0.16315299 <a title="92-tfidf-2" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<p>Author: Yiping Jin ; Min-Yen Kan ; Jun-Ping Ng ; Xiangnan He</p><p>Abstract: This paper presents DefMiner, a supervised sequence labeling system that identifies scientific terms and their accompanying definitions. DefMiner achieves 85% F1 on a Wikipedia benchmark corpus, significantly improving the previous state-of-the-art by 8%. We exploit DefMiner to process the ACL Anthology Reference Corpus (ARC) – a large, real-world digital library of scientific articles in computational linguistics. The resulting automatically-acquired glossary represents the terminology defined over several thousand individual research articles. We highlight several interesting observations: more definitions are introduced for conference and workshop papers over the years and that multiword terms account for slightly less than half of all terms. Obtaining a list of popular , defined terms in a corpus ofcomputational linguistics papers, we find that concepts can often be categorized into one of three categories: resources, methodologies and evaluation metrics.</p><p>3 0.074669689 <a title="92-tfidf-3" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>Author: Ivan Vulic ; Marie-Francine Moens</p><p>Abstract: We present a new language pair agnostic approach to inducing bilingual vector spaces from non-parallel data without any other resource in a bootstrapping fashion. The paper systematically introduces and describes all key elements of the bootstrapping procedure: (1) starting point or seed lexicon, (2) the confidence estimation and selection of new dimensions of the space, and (3) convergence. We test the quality of the induced bilingual vector spaces, and analyze the influence of the different components of the bootstrapping approach in the task of bilingual lexicon extraction (BLE) for two language pairs. Results reveal that, contrary to conclusions from prior work, the seeding of the bootstrapping process has a heavy impact on the quality of the learned lexicons. We also show that our approach outperforms the best performing fully corpus-based BLE methods on these test sets.</p><p>4 0.065604754 <a title="92-tfidf-4" href="./emnlp-2013-Building_Specialized_Bilingual_Lexicons_Using_Large_Scale_Background_Knowledge.html">42 emnlp-2013-Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge</a></p>
<p>Author: Dhouha Bouamor ; Adrian Popescu ; Nasredine Semmar ; Pierre Zweigenbaum</p><p>Abstract: Bilingual lexicons are central components of machine translation and cross-lingual information retrieval systems. Their manual construction requires strong expertise in both languages involved and is a costly process. Several automatic methods were proposed as an alternative but they often rely on resources available in a limited number of languages and their performances are still far behind the quality of manual translations. We introduce a novel approach to the creation of specific domain bilingual lexicon that relies on Wikipedia. This massively multilingual encyclopedia makes it possible to create lexicons for a large number of language pairs. Wikipedia is used to extract domains in each language, to link domains between languages and to create generic translation dictionaries. The approach is tested on four specialized domains and is compared to three state of the art approaches using two language pairs: FrenchEnglish and Romanian-English. The newly introduced method compares favorably to existing methods in all configurations tested.</p><p>5 0.065476388 <a title="92-tfidf-5" href="./emnlp-2013-Automatic_Domain_Partitioning_for_Multi-Domain_Learning.html">29 emnlp-2013-Automatic Domain Partitioning for Multi-Domain Learning</a></p>
<p>Author: Di Wang ; Chenyan Xiong ; William Yang Wang</p><p>Abstract: Chenyan Xiong School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA cx@ c s . cmu .edu William Yang Wang School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA ww@ cmu .edu might not be generalizable to other domains (BenDavid et al., 2006; Ben-David et al., 2010). Multi-Domain learning (MDL) assumes that the domain labels in the dataset are known. However, when there are multiple metadata at- tributes available, it is not always straightforward to select a single best attribute for domain partition, and it is possible that combining more than one metadata attributes (including continuous attributes) can lead to better MDL performance. In this work, we propose an automatic domain partitioning approach that aims at providing better domain identities for MDL. We use a supervised clustering approach that learns the domain distance between data instances , and then cluster the data into better domains for MDL. Our experiment on real multi-domain datasets shows that using our automatically generated domain partition improves over popular MDL methods.</p><p>6 0.054424804 <a title="92-tfidf-6" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>7 0.054001678 <a title="92-tfidf-7" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>8 0.049587063 <a title="92-tfidf-8" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>9 0.044547744 <a title="92-tfidf-9" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>10 0.039130893 <a title="92-tfidf-10" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>11 0.038956467 <a title="92-tfidf-11" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>12 0.035597336 <a title="92-tfidf-12" href="./emnlp-2013-Combining_Generative_and_Discriminative_Model_Scores_for_Distant_Supervision.html">49 emnlp-2013-Combining Generative and Discriminative Model Scores for Distant Supervision</a></p>
<p>13 0.034096539 <a title="92-tfidf-13" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>14 0.033796463 <a title="92-tfidf-14" href="./emnlp-2013-Identifying_Web_Search_Query_Reformulation_using_Concept_based_Matching.html">97 emnlp-2013-Identifying Web Search Query Reformulation using Concept based Matching</a></p>
<p>15 0.032670047 <a title="92-tfidf-15" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>16 0.030804332 <a title="92-tfidf-16" href="./emnlp-2013-Optimal_Incremental_Parsing_via_Best-First_Dynamic_Programming.html">146 emnlp-2013-Optimal Incremental Parsing via Best-First Dynamic Programming</a></p>
<p>17 0.030131249 <a title="92-tfidf-17" href="./emnlp-2013-Learning_Biological_Processes_with_Global_Constraints.html">118 emnlp-2013-Learning Biological Processes with Global Constraints</a></p>
<p>18 0.029694831 <a title="92-tfidf-18" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>19 0.028377414 <a title="92-tfidf-19" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>20 0.027862305 <a title="92-tfidf-20" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.105), (1, 0.018), (2, -0.039), (3, 0.001), (4, 0.025), (5, 0.021), (6, 0.015), (7, 0.059), (8, 0.032), (9, -0.084), (10, -0.004), (11, -0.065), (12, -0.023), (13, 0.023), (14, 0.076), (15, 0.004), (16, -0.103), (17, 0.024), (18, 0.025), (19, 0.039), (20, -0.002), (21, 0.004), (22, -0.0), (23, 0.033), (24, 0.073), (25, -0.101), (26, 0.16), (27, 0.025), (28, -0.065), (29, -0.074), (30, -0.262), (31, 0.062), (32, 0.023), (33, -0.136), (34, -0.005), (35, 0.119), (36, 0.17), (37, -0.131), (38, 0.037), (39, 0.086), (40, -0.111), (41, 0.077), (42, -0.23), (43, -0.184), (44, 0.154), (45, -0.106), (46, 0.044), (47, 0.199), (48, 0.064), (49, 0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96604812 <a title="92-lsi-1" href="./emnlp-2013-Growing_Multi-Domain_Glossaries_from_a_Few_Seeds_using_Probabilistic_Topic_Models.html">92 emnlp-2013-Growing Multi-Domain Glossaries from a Few Seeds using Probabilistic Topic Models</a></p>
<p>Author: Stefano Faralli ; Roberto Navigli</p><p>Abstract: In this paper we present a minimallysupervised approach to the multi-domain acquisition ofwide-coverage glossaries. We start from a small number of hypernymy relation seeds and bootstrap glossaries from the Web for dozens of domains using Probabilistic Topic Models. Our experiments show that we are able to extract high-precision glossaries comprising thousands of terms and definitions.</p><p>2 0.7484107 <a title="92-lsi-2" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<p>Author: Yiping Jin ; Min-Yen Kan ; Jun-Ping Ng ; Xiangnan He</p><p>Abstract: This paper presents DefMiner, a supervised sequence labeling system that identifies scientific terms and their accompanying definitions. DefMiner achieves 85% F1 on a Wikipedia benchmark corpus, significantly improving the previous state-of-the-art by 8%. We exploit DefMiner to process the ACL Anthology Reference Corpus (ARC) – a large, real-world digital library of scientific articles in computational linguistics. The resulting automatically-acquired glossary represents the terminology defined over several thousand individual research articles. We highlight several interesting observations: more definitions are introduced for conference and workshop papers over the years and that multiword terms account for slightly less than half of all terms. Obtaining a list of popular , defined terms in a corpus ofcomputational linguistics papers, we find that concepts can often be categorized into one of three categories: resources, methodologies and evaluation metrics.</p><p>3 0.37790835 <a title="92-lsi-3" href="./emnlp-2013-Automatic_Domain_Partitioning_for_Multi-Domain_Learning.html">29 emnlp-2013-Automatic Domain Partitioning for Multi-Domain Learning</a></p>
<p>Author: Di Wang ; Chenyan Xiong ; William Yang Wang</p><p>Abstract: Chenyan Xiong School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA cx@ c s . cmu .edu William Yang Wang School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA ww@ cmu .edu might not be generalizable to other domains (BenDavid et al., 2006; Ben-David et al., 2010). Multi-Domain learning (MDL) assumes that the domain labels in the dataset are known. However, when there are multiple metadata at- tributes available, it is not always straightforward to select a single best attribute for domain partition, and it is possible that combining more than one metadata attributes (including continuous attributes) can lead to better MDL performance. In this work, we propose an automatic domain partitioning approach that aims at providing better domain identities for MDL. We use a supervised clustering approach that learns the domain distance between data instances , and then cluster the data into better domains for MDL. Our experiment on real multi-domain datasets shows that using our automatically generated domain partition improves over popular MDL methods.</p><p>4 0.36680868 <a title="92-lsi-4" href="./emnlp-2013-Building_Specialized_Bilingual_Lexicons_Using_Large_Scale_Background_Knowledge.html">42 emnlp-2013-Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge</a></p>
<p>Author: Dhouha Bouamor ; Adrian Popescu ; Nasredine Semmar ; Pierre Zweigenbaum</p><p>Abstract: Bilingual lexicons are central components of machine translation and cross-lingual information retrieval systems. Their manual construction requires strong expertise in both languages involved and is a costly process. Several automatic methods were proposed as an alternative but they often rely on resources available in a limited number of languages and their performances are still far behind the quality of manual translations. We introduce a novel approach to the creation of specific domain bilingual lexicon that relies on Wikipedia. This massively multilingual encyclopedia makes it possible to create lexicons for a large number of language pairs. Wikipedia is used to extract domains in each language, to link domains between languages and to create generic translation dictionaries. The approach is tested on four specialized domains and is compared to three state of the art approaches using two language pairs: FrenchEnglish and Romanian-English. The newly introduced method compares favorably to existing methods in all configurations tested.</p><p>5 0.3133817 <a title="92-lsi-5" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>Author: Ivan Vulic ; Marie-Francine Moens</p><p>Abstract: We present a new language pair agnostic approach to inducing bilingual vector spaces from non-parallel data without any other resource in a bootstrapping fashion. The paper systematically introduces and describes all key elements of the bootstrapping procedure: (1) starting point or seed lexicon, (2) the confidence estimation and selection of new dimensions of the space, and (3) convergence. We test the quality of the induced bilingual vector spaces, and analyze the influence of the different components of the bootstrapping approach in the task of bilingual lexicon extraction (BLE) for two language pairs. Results reveal that, contrary to conclusions from prior work, the seeding of the bootstrapping process has a heavy impact on the quality of the learned lexicons. We also show that our approach outperforms the best performing fully corpus-based BLE methods on these test sets.</p><p>6 0.31207719 <a title="92-lsi-6" href="./emnlp-2013-Animacy_Detection_with_Voting_Models.html">23 emnlp-2013-Animacy Detection with Voting Models</a></p>
<p>7 0.29829469 <a title="92-lsi-7" href="./emnlp-2013-Automatic_Idiom_Identification_in_Wiktionary.html">32 emnlp-2013-Automatic Idiom Identification in Wiktionary</a></p>
<p>8 0.2635166 <a title="92-lsi-8" href="./emnlp-2013-Rule-Based_Information_Extraction_is_Dead%21_Long_Live_Rule-Based_Information_Extraction_Systems%21.html">161 emnlp-2013-Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!</a></p>
<p>9 0.25825948 <a title="92-lsi-9" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>10 0.23418616 <a title="92-lsi-10" href="./emnlp-2013-Tree_Kernel-based_Negation_and_Speculation_Scope_Detection_with_Structured_Syntactic_Parse_Features.html">188 emnlp-2013-Tree Kernel-based Negation and Speculation Scope Detection with Structured Syntactic Parse Features</a></p>
<p>11 0.23070617 <a title="92-lsi-11" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>12 0.23036775 <a title="92-lsi-12" href="./emnlp-2013-Two-Stage_Method_for_Large-Scale_Acquisition_of_Contradiction_Pattern_Pairs_using_Entailment.html">189 emnlp-2013-Two-Stage Method for Large-Scale Acquisition of Contradiction Pattern Pairs using Entailment</a></p>
<p>13 0.21695958 <a title="92-lsi-13" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>14 0.21291383 <a title="92-lsi-14" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<p>15 0.20455955 <a title="92-lsi-15" href="./emnlp-2013-With_Blinkers_on%3A_Robust_Prediction_of_Eye_Movements_across_Readers.html">203 emnlp-2013-With Blinkers on: Robust Prediction of Eye Movements across Readers</a></p>
<p>16 0.20324397 <a title="92-lsi-16" href="./emnlp-2013-Measuring_Ideological_Proportions_in_Political_Speeches.html">129 emnlp-2013-Measuring Ideological Proportions in Political Speeches</a></p>
<p>17 0.19596587 <a title="92-lsi-17" href="./emnlp-2013-Structured_Penalties_for_Log-Linear_Language_Models.html">176 emnlp-2013-Structured Penalties for Log-Linear Language Models</a></p>
<p>18 0.19514158 <a title="92-lsi-18" href="./emnlp-2013-Scaling_to_Large3_Data%3A_An_Efficient_and_Effective_Method_to_Compute_Distributional_Thesauri.html">165 emnlp-2013-Scaling to Large3 Data: An Efficient and Effective Method to Compute Distributional Thesauri</a></p>
<p>19 0.19387449 <a title="92-lsi-19" href="./emnlp-2013-Regularized_Minimum_Error_Rate_Training.html">159 emnlp-2013-Regularized Minimum Error Rate Training</a></p>
<p>20 0.1873747 <a title="92-lsi-20" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.019), (18, 0.022), (22, 0.031), (30, 0.675), (51, 0.085), (66, 0.032), (75, 0.014), (96, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99065977 <a title="92-lda-1" href="./emnlp-2013-An_Efficient_Language_Model_Using_Double-Array_Structures.html">20 emnlp-2013-An Efficient Language Model Using Double-Array Structures</a></p>
<p>Author: Makoto Yasuhara ; Toru Tanaka ; Jun-ya Norimatsu ; Mikio Yamamoto</p><p>Abstract: Ngram language models tend to increase in size with inflating the corpus size, and consume considerable resources. In this paper, we propose an efficient method for implementing ngram models based on doublearray structures. First, we propose a method for representing backwards suffix trees using double-array structures and demonstrate its efficiency. Next, we propose two optimization methods for improving the efficiency of data representation in the double-array structures. Embedding probabilities into unused spaces in double-array structures reduces the model size. Moreover, tuning the word IDs in the language model makes the model smaller and faster. We also show that our method can be used for building large language models using the division method. Lastly, we show that our method outperforms methods based on recent related works from the viewpoints of model size and query speed when both optimization methods are used.</p><p>same-paper 2 0.95278913 <a title="92-lda-2" href="./emnlp-2013-Growing_Multi-Domain_Glossaries_from_a_Few_Seeds_using_Probabilistic_Topic_Models.html">92 emnlp-2013-Growing Multi-Domain Glossaries from a Few Seeds using Probabilistic Topic Models</a></p>
<p>Author: Stefano Faralli ; Roberto Navigli</p><p>Abstract: In this paper we present a minimallysupervised approach to the multi-domain acquisition ofwide-coverage glossaries. We start from a small number of hypernymy relation seeds and bootstrap glossaries from the Web for dozens of domains using Probabilistic Topic Models. Our experiments show that we are able to extract high-precision glossaries comprising thousands of terms and definitions.</p><p>3 0.93924725 <a title="92-lda-3" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>Author: Xinyan Xiao ; Deyi Xiong</p><p>Abstract: Traditional synchronous grammar induction estimates parameters by maximizing likelihood, which only has a loose relation to translation quality. Alternatively, we propose a max-margin estimation approach to discriminatively inducing synchronous grammars for machine translation, which directly optimizes translation quality measured by BLEU. In the max-margin estimation of parameters, we only need to calculate Viterbi translations. This further facilitates the incorporation of various non-local features that are defined on the target side. We test the effectiveness of our max-margin estimation framework on a competitive hierarchical phrase-based system. Experiments show that our max-margin method significantly outperforms the traditional twostep pipeline for synchronous rule extraction by 1.3 BLEU points and is also better than previous max-likelihood estimation method.</p><p>4 0.93704158 <a title="92-lda-4" href="./emnlp-2013-Structured_Penalties_for_Log-Linear_Language_Models.html">176 emnlp-2013-Structured Penalties for Log-Linear Language Models</a></p>
<p>Author: Anil Kumar Nelakanti ; Cedric Archambeau ; Julien Mairal ; Francis Bach ; Guillaume Bouchard</p><p>Abstract: Language models can be formalized as loglinear regression models where the input features represent previously observed contexts up to a certain length m. The complexity of existing algorithms to learn the parameters by maximum likelihood scale linearly in nd, where n is the length of the training corpus and d is the number of observed features. We present a model that grows logarithmically in d, making it possible to efficiently leverage longer contexts. We account for the sequential structure of natural language using treestructured penalized objectives to avoid overfitting and achieve better generalization.</p><p>5 0.8646853 <a title="92-lda-5" href="./emnlp-2013-A_Dataset_for_Research_on_Short-Text_Conversations.html">4 emnlp-2013-A Dataset for Research on Short-Text Conversations</a></p>
<p>Author: Hao Wang ; Zhengdong Lu ; Hang Li ; Enhong Chen</p><p>Abstract: Natural language conversation is widely regarded as a highly difficult problem, which is usually attacked with either rule-based or learning-based models. In this paper we propose a retrieval-based automatic response model for short-text conversation, to exploit the vast amount of short conversation instances available on social media. For this purpose we introduce a dataset of short-text conversation based on the real-world instances from Sina Weibo (a popular Chinese microblog service), which will be soon released to public. This dataset provides rich collection of instances for the research on finding natural and relevant short responses to a given short text, and useful for both training and testing of conversation models. This dataset consists of both naturally formed conversations, manually labeled data, and a large repository of candidate responses. Our preliminary experiments demonstrate that the simple retrieval-based conversation model performs reasonably well when combined with the rich instances in our dataset.</p><p>6 0.8556639 <a title="92-lda-6" href="./emnlp-2013-Sarcasm_as_Contrast_between_a_Positive_Sentiment_and_Negative_Situation.html">163 emnlp-2013-Sarcasm as Contrast between a Positive Sentiment and Negative Situation</a></p>
<p>7 0.67496037 <a title="92-lda-7" href="./emnlp-2013-Joint_Language_and_Translation_Modeling_with_Recurrent_Neural_Networks.html">113 emnlp-2013-Joint Language and Translation Modeling with Recurrent Neural Networks</a></p>
<p>8 0.63754421 <a title="92-lda-8" href="./emnlp-2013-A_Corpus_Level_MIRA_Tuning_Strategy_for_Machine_Translation.html">3 emnlp-2013-A Corpus Level MIRA Tuning Strategy for Machine Translation</a></p>
<p>9 0.62704104 <a title="92-lda-9" href="./emnlp-2013-Optimal_Incremental_Parsing_via_Best-First_Dynamic_Programming.html">146 emnlp-2013-Optimal Incremental Parsing via Best-First Dynamic Programming</a></p>
<p>10 0.6153695 <a title="92-lda-10" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>11 0.61191314 <a title="92-lda-11" href="./emnlp-2013-Recurrent_Continuous_Translation_Models.html">156 emnlp-2013-Recurrent Continuous Translation Models</a></p>
<p>12 0.60435748 <a title="92-lda-12" href="./emnlp-2013-A_Convex_Alternative_to_IBM_Model_2.html">2 emnlp-2013-A Convex Alternative to IBM Model 2</a></p>
<p>13 0.60054189 <a title="92-lda-13" href="./emnlp-2013-Dynamic_Feature_Selection_for_Dependency_Parsing.html">66 emnlp-2013-Dynamic Feature Selection for Dependency Parsing</a></p>
<p>14 0.5991559 <a title="92-lda-14" href="./emnlp-2013-A_Synchronous_Context_Free_Grammar_for_Time_Normalization.html">14 emnlp-2013-A Synchronous Context Free Grammar for Time Normalization</a></p>
<p>15 0.59521341 <a title="92-lda-15" href="./emnlp-2013-Improving_Web_Search_Ranking_by_Incorporating_Structured_Annotation_of_Queries.html">105 emnlp-2013-Improving Web Search Ranking by Incorporating Structured Annotation of Queries</a></p>
<p>16 0.58036041 <a title="92-lda-16" href="./emnlp-2013-Learning_to_Freestyle%3A_Hip_Hop_Challenge-Response_Induction_via_Transduction_Rule_Segmentation.html">122 emnlp-2013-Learning to Freestyle: Hip Hop Challenge-Response Induction via Transduction Rule Segmentation</a></p>
<p>17 0.57995069 <a title="92-lda-17" href="./emnlp-2013-Max-Violation_Perceptron_and_Forced_Decoding_for_Scalable_MT_Training.html">128 emnlp-2013-Max-Violation Perceptron and Forced Decoding for Scalable MT Training</a></p>
<p>18 0.57721245 <a title="92-lda-18" href="./emnlp-2013-Converting_Continuous-Space_Language_Models_into_N-Gram_Language_Models_for_Statistical_Machine_Translation.html">52 emnlp-2013-Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation</a></p>
<p>19 0.57031822 <a title="92-lda-19" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>20 0.56950837 <a title="92-lda-20" href="./emnlp-2013-Flexible_and_Efficient_Hypergraph_Interactions_for_Joint_Hierarchical_and_Forest-to-String_Decoding.html">88 emnlp-2013-Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest-to-String Decoding</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
