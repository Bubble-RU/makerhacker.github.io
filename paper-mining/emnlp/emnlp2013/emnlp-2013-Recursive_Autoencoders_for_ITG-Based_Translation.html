<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-157" href="#">emnlp2013-157</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</h1>
<br/><p>Source: <a title="emnlp-2013-157-pdf" href="http://aclweb.org/anthology//D/D13/D13-1054.pdf">pdf</a></p><p>Author: Peng Li ; Yang Liu ; Maosong Sun</p><p>Abstract: While inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (i.e., straight and inverted) dependent on actual blocks being merged remains a challenge. Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points.</p><p>Reference: <a title="emnlp-2013-157-reference" href="../emnlp2013_reference/emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn 1  ,  Abstract While inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (i. [sent-4, score-0.586]
</p><p>2 , straight and inverted) dependent on actual blocks being merged remains a challenge. [sent-6, score-0.438]
</p><p>3 Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively. [sent-7, score-0.792]
</p><p>4 The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. [sent-8, score-0.744]
</p><p>5 Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e. [sent-17, score-0.536]
</p><p>6 Among them, reordering models based on inversion transduction grammar (ITG) (Wu, 1997) are one of the important ongoing research directions. [sent-26, score-0.586]
</p><p>7 As a result, a number of authors have incorporated ITG into leftto-right decoding to constrain the reordering space and reported significant improvements (e. [sent-28, score-0.536]
</p><p>8 (2006) propose a maximum entropy (MaxEnt) reordering model based on ITG. [sent-34, score-0.468]
</p><p>9 They use the CKY algorithm to recursively merge two blocks (i. [sent-35, score-0.285]
</p><p>10 , a pair of source and target strings) into larger blocks, either in a straight or an inverted order. [sent-37, score-0.37]
</p><p>11 Unlike lexicalized reordering models (Tillman, 2004; Koehn et al. [sent-38, score-0.468]
</p><p>12 , 2007; Galley and Manning, 2008) that are defined on individual bilingual phrases, the MaxEnt ITG reordering model is a two-category classifier (i. [sent-39, score-0.571]
</p><p>13 , straight or inverted) for two arbitrary bilingual phrases of which the source phrases are adja-  cent. [sent-41, score-0.438]
</p><p>14 hc o2d0s1 i3n A Nsastoucria lti Loan fgoura Cgoem Ppruotcaetsiosin agl, L piang eusis 5t6ic7s–57 , problem since there are usually a large number of reordering training examples available (Xiong et al. [sent-44, score-0.468]
</p><p>15 Despite these successful efforts, the ITG reordering classifiers still face a major challenge: how to extract features from training examples (i. [sent-49, score-0.468]
</p><p>16 More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al. [sent-72, score-0.65]
</p><p>17 , 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from a neural language modeling point of view. [sent-73, score-0.769]
</p><p>18 In this work, we propose an ITG reordering classifier based on recursive autoencoders. [sent-74, score-0.731]
</p><p>19 , the first source phrase, the first target phrase, the second source phrase, and the second target phrase) and a softmax layer. [sent-77, score-0.212]
</p><p>20 The recursive autoencoders, which are trained on reordering examples extracted from word-aligned bilingual corpus, are capable of producing vector space representations for arbitrary multi-word strings in decoding. [sent-78, score-1.031]
</p><p>21 Therefore, our model takes the whole phrases rather than only boundary words into consideration when predict-  ing phrase permutations. [sent-79, score-0.212]
</p><p>22 X1 and X2 are two neighboring blocks  of which the two source phrases are adjacent. [sent-86, score-0.383]
</p><p>23 While rule (1) merges two target phrases in a straight order, rule (2) merges in an inverted order. [sent-87, score-0.394]
</p><p>24 Besides these two reordering rules, rule (3) is a lexical rule that translates a source phrase f into a target phrase e. [sent-88, score-0.694]
</p><p>25 atomic blocks: blocks generated by applying lexical rules, 2. [sent-93, score-0.352]
</p><p>26 Our neural ITG reordering model first assigns vector space representations to single wo=rd hsf and tih teon r produces ave bclotcorks. [sent-99, score-0.791]
</p><p>27 for phrases using recursive autoencoders, which form atomic blocks. [sent-100, score-0.389]
</p><p>28 The atomic blocks are recursively merged into composed blocks, the vector space representations of which are produced by recursive autoencoders simultaneously. [sent-101, score-1.065]
</p><p>29 The neural classifier makes decisions at each node using the vectors of all its descendants. [sent-102, score-0.24]
</p><p>30 More  hfij  formally, a block Xi,j,k,l = , elki is a pair of a source phrase = fi+1 . [sent-108, score-0.274]
</p><p>31 Obviously, these atomic blocks are generated by lexical rules. [sent-115, score-0.352]
</p><p>32 Two blocks of which the source phrases are adjacent can be merged into a larger one in two ways: concatenating the target phrases in a straight order using rule (1) or in an inverted order using rule (2). [sent-116, score-0.818]
</p><p>33 For example, atomic blocks X3,5,5,6 and X5,8,6,8 are merged into a composed block X3,8,5,8 in a straight order, which is further merged with an atomic block X8,10,3,5 into another composed block X3,10,3,8 in an inverted order. [sent-117, score-1.181]
</p><p>34 The major challenge of applying ITG to machine translation is to decide when to merge two blocks in a straight order and when in an inverted order. [sent-119, score-0.562]
</p><p>35 Therefore, the ITG reordering model can be seen as a two-category classifier P(o|X1 , X2), where o ∈ {straight, inverted}. [sent-120, score-0.521]
</p><p>36 tAr aniagivhet way eisr t eod assign fixed probabilities to two reordering rules, which is referred to as flat model by Xiong et al. [sent-121, score-0.468]
</p><p>37 1p − p o = =i s ntrvaeirgtehdt  (4)  The drawback of the flat model is ignoring the actual blocks being merged. [sent-123, score-0.251]
</p><p>38 Intuitively, different blocks should have different preferences between the two orders. [sent-124, score-0.251]
</p><p>39 Actually, 570  Figure 2: A recursive autoencoder for multi-word strings. [sent-132, score-0.306]
</p><p>40 it is hard to decide which internal words composed blocks are representative and tive. [sent-136, score-0.317]
</p><p>41 (2008) find that the MaxEnt classifier with boundary words as features is prone to make wrong predictions for long composed blocks. [sent-140, score-0.218]
</p><p>42 As a result, they have to impose a hard constraint to always prefer merging long composed blocks in a monotonic way. [sent-141, score-0.403]
</p><p>43 Therefore, it is important to consider more than boundary words to make more accurate reordering predictions. [sent-142, score-0.534]
</p><p>44 1 Vector Space Representations for Words In neural networks, a natural language word is represented as a real-valued vector (Bengio et al. [sent-147, score-0.209]
</p><p>45 4]T to represent “female” and 1Strictly speaking, the ITG reordering model is not a phrase reordering model since phrase pairs are only the atomic blocks. [sent-152, score-1.173]
</p><p>46 Instead, it is defined to work on arbitrarily long strings because composed blocks become larger and larger until the entire sentence pair is generated. [sent-153, score-0.441]
</p><p>47 The binary classifier makes decisions based on the vector space representa-  tions of the source and target sides of merging blocks. [sent-155, score-0.324]
</p><p>48 Such vector space representations enable natural language words to be fed to neural networks as input. [sent-160, score-0.362]
</p><p>49 aGtriivxen L a sentence ,t whahte eirse an o |rd iser theed lviostof m words, each word has an associated vocabulary index k into the word embedding matrix L that we use to retrieve the word’s vector space representation. [sent-163, score-0.218]
</p><p>50 2  Vector Space Representations for Multi-Word Strings To apply neural networks to ITG-based translation, it is important to generate vector space representations for atomic and composed blocks. [sent-168, score-0.529]
</p><p>51 The same neural network can be recursively applied to two strings until the vector of the entire sentence is generated. [sent-188, score-0.367]
</p><p>52 As ITG derivation builds a binary parse tree, the neural network can be naturally integrated into CKY parsing. [sent-189, score-0.23]
</p><p>53 These neural networks are called recursive autoencoders (Socher et al. [sent-192, score-0.572]
</p><p>54 Figure 2 illustrates an application of a recursive autoencoder to a  f(2)  b(2)  binary tree. [sent-194, score-0.334]
</p><p>55 The binary tree is composed of a set of triplets in the form of (p → c1 c2), where p is a parent vector and c1 aonfd ( c2 are children vectors of p. [sent-197, score-0.266]
</p><p>56 In→ Figure 1, we use recursive autoencoders to generate vector space representations for Chinese and English phrases, which form the atomic blocks for further block merging. [sent-200, score-1.002]
</p><p>57 3 A Neural ITG Reordering Model Once the vectors for blocks are generated, it is straightforward to introduce a neural ITG reordering model. [sent-203, score-0.906]
</p><p>58 As shown in Figure 3, the neural network consists of an input layer and a softmax layer. [sent-204, score-0.254]
</p><p>59 The input layer is composed of the vectors of the first source phrase, the first target phrase, the second source phrase, and the second target phrase. [sent-205, score-0.338]
</p><p>60 Note that all phrases in the same language use the the same recursive autoencoder. [sent-206, score-0.288]
</p><p>61 3  Training  There are three sets of parameters in our recursive autoencoders: 1. [sent-208, score-0.21]
</p><p>62 θrec: recursive autoencoder parameter matrices and bias terms for both source and target languages (Section 2. [sent-212, score-0.454]
</p><p>63 θreo: neural ITG reordering model parameter matrix Wo and bias term bo (Section 2. [sent-215, score-0.68]
</p><p>64 This works well in a supervised scenario, in which a neural network updates the matrix in order to optimize some task-specific objectives (Collobert et al. [sent-222, score-0.216]
</p><p>65 In the second setting, the word embedding matrix is pre-trained using an unsupervised neural language model (Bengio et al. [sent-225, score-0.26]
</p><p>66 In this work, we prefer to the first setting because the word embedding matrices can be trained to minimize errors with respect to reordering modeling. [sent-227, score-0.574]
</p><p>67 reconstruction error: how well the learned vector space representations represent the corresponding strings? [sent-229, score-0.299]
</p><p>68 reordering error: how well the classifier predicts the merging order? [sent-231, score-0.574]
</p><p>69 2, the input vector c1 and c2 of a recursive autoencoder can be reconstructed using Eq. [sent-234, score-0.446]
</p><p>70 We use Euclidean distance between the input and the reconstructed vectors to measure the reconstruction error:  Erec([c1;c2];θ) = 21 ? [sent-236, score-0.236]
</p><p>71 Suppose Erec( [x1; x2] ; θ) is the smallest, the algorithm will replace x1 and x2 with their vector representation y1 produced by the recursive autoencoder. [sent-250, score-0.277]
</p><p>72 Given a training example set S = {ti = (oi, Xi1, Xi2)}, the average reconstruction error on the source si)d}e, on eth aev training set istsr duectfiinoend e as  Erec,s(S;θ) =N1sXip∈TXRθ(ti,s)Erec([p. [sent-254, score-0.21]
</p><p>73 reordering error aisi dgehftin,iendv as  As a result, the  Ereo(S;θ) =|S1|XiEc(ti;θ). [sent-264, score-0.506]
</p><p>74 (15)  Therefore, the joint training objective function is J = αErec(S; θ)  + (1−α)Ereo(S;  θ)  + R(θ)  (16)  where α is a parameter used to balance the preference between reconstruction error and reordering error, R(θ) is the regularizer and defined as 2  R(θ) =λ2LkθLk2+λ2reckθreck2+λr2eokθreok2. [sent-265, score-0.624]
</p><p>75 (201 1c) stated, a naive way for lowering the reconstruction error is to make the magnitude of the hidden layer very small, which is 2The bias terms b(1), b(2) and bo are not regularized. [sent-267, score-0.232]
</p><p>76 Because of the expensive computational cost for training our neural ITG reordering model, only the reordering examples extracted from about 1/5 of the entire parallel training corpus were used to train our neural ITG reordering model. [sent-289, score-1.719]
</p><p>77 For the neural ITG reordering model, we set the dimension of the word embedding vectors to 25 empirically, which is a trade-offbetween computational cost and expressive power. [sent-290, score-0.732]
</p><p>78 We randomly select 400,000 reordering examples as training set, 500 as development set, and another 500 as test set. [sent-294, score-0.468]
</p><p>79 The numbers of straight and inverted reordering examples in the development/test set are set to be equal  to avoid biases. [sent-295, score-0.748]
</p><p>80 “maxent” denotes the baseline maximum entropy system and “neural” denotes our recursive autoencoder system. [sent-304, score-0.306]
</p><p>81 Our system is different from the baseline by replacing the MaxEnt reordering model with a neural model. [sent-317, score-0.61]
</p><p>82 574  Figure 4: Comparison of reordering classification accuracies between the MaxEnt and neural classifiers over varying phrase lengths. [sent-333, score-0.678]
</p><p>83 “Length” denotes the sum of the lengths of two source phrases in a reordering example. [sent-334, score-0.6]
</p><p>84 8743285908  Table 3: The effect of reordering training data size on BLEU scores. [sent-338, score-0.468]
</p><p>85 Due to the computational cost, we only used 1/5 of the entire bilingual corpus to train our neural reordering model. [sent-340, score-0.691]
</p><p>86 “Length” denotes the sum of the lengths of two source phrases in a reordering example. [sent-343, score-0.6]
</p><p>87 For each length, we randomly select 200 unseen reordering examples to calculate the classification accuracy. [sent-344, score-0.468]
</p><p>88 (2008) find that the performance of the baseline system can be improved by forbidding  inverted reordering if the phrase length exceeds a pre-defined distortion limit. [sent-347, score-0.707]
</p><p>89 The above results suggest that our system does go beyond using boundary words and make a better use of the merging blocks by using vector space representations. [sent-356, score-0.47]
</p><p>90 As the training process is very time-consuming, only the reordering examples extracted from 1/5 of the entire parallel training cor-  pus are used in our experiments to train our model. [sent-359, score-0.499]
</p><p>91 Obviously, with more efficient training algorithms, making full use of all the reordering examples extracted from the entire corpus will result in better results. [sent-360, score-0.499]
</p><p>92 We find that the words and phrases in the same cluster have sim-  575 ilar behaviors from a reordering point of view rather than relatedness. [sent-367, score-0.585]
</p><p>93 This indicates that the vector representations produced by the recursive autoencoders are helpful for capturing reordering regularities. [sent-368, score-1.007]
</p><p>94 5  Conclusion  We have presented an ITG reordering classifier based on recursive autoencoders. [sent-369, score-0.731]
</p><p>95 As recursive autoencoders are capable of producing vector space representations for arbitrary multi-word strings in decoding, our neural ITG system achieves an absolute improvement of 1. [sent-370, score-0.836]
</p><p>96 (2013) to combine  linguistically-motivated labels with recursive neural networks. [sent-377, score-0.352]
</p><p>97 A  simple and effective hierarchical phrase reordering model. [sent-435, score-0.536]
</p><p>98 Dynamic pooling and unfolding recursive autoencoders  for paraphrase detection. [sent-496, score-0.391]
</p><p>99 Parsing natural scenes and natural language with recursive neural networks. [sent-503, score-0.352]
</p><p>100 Maximum entropy based phrase reordering model for statistical machine translation. [sent-539, score-0.536]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reordering', 0.468), ('itg', 0.379), ('blocks', 0.251), ('recursive', 0.21), ('erec', 0.201), ('autoencoders', 0.181), ('xiong', 0.172), ('socher', 0.172), ('maxent', 0.166), ('straight', 0.146), ('neural', 0.142), ('inverted', 0.134), ('reconstruction', 0.118), ('atomic', 0.101), ('autoencoder', 0.096), ('bleu', 0.088), ('representations', 0.081), ('phrases', 0.078), ('block', 0.078), ('embedding', 0.077), ('friend', 0.073), ('reconstructed', 0.073), ('bordes', 0.073), ('nist', 0.07), ('collobert', 0.07), ('phrase', 0.068), ('vector', 0.067), ('boundary', 0.066), ('composed', 0.066), ('tsinghua', 0.064), ('transduction', 0.062), ('strings', 0.06), ('female', 0.059), ('reo', 0.058), ('inversion', 0.056), ('source', 0.054), ('classifier', 0.053), ('weston', 0.053), ('merging', 0.053), ('bilingual', 0.05), ('yoshua', 0.049), ('layer', 0.047), ('rec', 0.046), ('vectors', 0.045), ('oi', 0.045), ('glorot', 0.044), ('matrix', 0.041), ('merged', 0.041), ('antoine', 0.041), ('bengio', 0.04), ('cluster', 0.039), ('networks', 0.039), ('deyi', 0.038), ('error', 0.038), ('distortion', 0.037), ('wo', 0.037), ('bergstra', 0.037), ('bisazza', 0.037), ('elki', 0.037), ('ereo', 0.037), ('hfij', 0.037), ('pages', 0.036), ('target', 0.036), ('decoding', 0.035), ('zens', 0.035), ('euclidean', 0.035), ('recursively', 0.034), ('space', 0.033), ('long', 0.033), ('galley', 0.033), ('children', 0.033), ('greedy', 0.033), ('network', 0.033), ('arbitrary', 0.032), ('richard', 0.032), ('softmax', 0.032), ('tillman', 0.032), ('translation', 0.031), ('och', 0.031), ('entire', 0.031), ('capable', 0.03), ('koehn', 0.029), ('shouxun', 0.029), ('goller', 0.029), ('bias', 0.029), ('hermann', 0.029), ('matrices', 0.029), ('qun', 0.028), ('binary', 0.028), ('ti', 0.027), ('aug', 0.027), ('cong', 0.027), ('aiti', 0.027), ('triplets', 0.027), ('riezler', 0.027), ('derivation', 0.027), ('child', 0.026), ('proceedings', 0.026), ('christopher', 0.026), ('activation', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="157-tfidf-1" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>Author: Peng Li ; Yang Liu ; Maosong Sun</p><p>Abstract: While inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (i.e., straight and inverted) dependent on actual blocks being merged remains a challenge. Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points.</p><p>2 0.30545819 <a title="157-tfidf-2" href="./emnlp-2013-Factored_Soft_Source_Syntactic_Constraints_for_Hierarchical_Machine_Translation.html">84 emnlp-2013-Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation</a></p>
<p>Author: Zhongqiang Huang ; Jacob Devlin ; Rabih Zbib</p><p>Abstract: Translation Jacob Devlin Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA j devl in@bbn . com Rabih Zbib Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA r zbib@bbn . com have tried to introduce grammaticality to the transThis paper describes a factored approach to incorporating soft source syntactic constraints into a hierarchical phrase-based translation system. In contrast to traditional approaches that directly introduce syntactic constraints to translation rules by explicitly decorating them with syntactic annotations, which often exacerbate the data sparsity problem and cause other problems, our approach keeps translation rules intact and factorizes the use of syntactic constraints through two separate models: 1) a syntax mismatch model that associates each nonterminal of a translation rule with a distribution of tags that is used to measure the degree of syntactic compatibility of the translation rule on source spans; 2) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language. The features produced by both models are used as soft constraints to guide the translation process. Experiments on Chinese-English translation show that the proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets.</p><p>3 0.24011338 <a title="157-tfidf-3" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>Author: Uri Lerner ; Slav Petrov</p><p>Abstract: We present a simple and novel classifier-based preordering approach. Unlike existing preordering models, we train feature-rich discriminative classifiers that directly predict the target-side word order. Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discriminative model with a rich set of features, including lexical features. We present extensive experiments on 22 language pairs, including preordering into English from 7 other languages. We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task. For languages from different families the improvements often exceed 2 BLEU. Many of these gains are also significant in human evaluations.</p><p>4 0.19173989 <a title="157-tfidf-4" href="./emnlp-2013-Shift-Reduce_Word_Reordering_for_Machine_Translation.html">171 emnlp-2013-Shift-Reduce Word Reordering for Machine Translation</a></p>
<p>Author: Katsuhiko Hayashi ; Katsuhito Sudoh ; Hajime Tsukada ; Jun Suzuki ; Masaaki Nagata</p><p>Abstract: This paper presents a novel word reordering model that employs a shift-reduce parser for inversion transduction grammars. Our model uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system.</p><p>5 0.18936223 <a title="157-tfidf-5" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>Author: Hendra Setiawan ; Bowen Zhou ; Bing Xiang</p><p>Abstract: Reordering poses one of the greatest challenges in Statistical Machine Translation research as the key contextual information may well be beyond the confine oftranslation units. We present the “Anchor Graph” (AG) model where we use a graph structure to model global contextual information that is crucial for reordering. The key ingredient of our AG model is the edges that capture the relationship between the reordering around a set of selected translation units, which we refer to as anchors. As the edges link anchors that may span multiple translation units at decoding time, our AG model effectively encodes global contextual information that is previously absent. We integrate our proposed model into a state-of-the-art translation system and demonstrate the efficacy of our proposal in a largescale Chinese-to-English translation task.</p><p>6 0.16717863 <a title="157-tfidf-6" href="./emnlp-2013-Efficient_Left-to-Right_Hierarchical_Phrase-Based_Translation_with_Improved_Reordering.html">71 emnlp-2013-Efficient Left-to-Right Hierarchical Phrase-Based Translation with Improved Reordering</a></p>
<p>7 0.16022897 <a title="157-tfidf-7" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>8 0.15854031 <a title="157-tfidf-8" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>9 0.12927659 <a title="157-tfidf-9" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>10 0.1236731 <a title="157-tfidf-10" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>11 0.12195998 <a title="157-tfidf-11" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>12 0.10894369 <a title="157-tfidf-12" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>13 0.08952029 <a title="157-tfidf-13" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>14 0.088990889 <a title="157-tfidf-14" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>15 0.083540425 <a title="157-tfidf-15" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>16 0.083088011 <a title="157-tfidf-16" href="./emnlp-2013-Joint_Language_and_Translation_Modeling_with_Recurrent_Neural_Networks.html">113 emnlp-2013-Joint Language and Translation Modeling with Recurrent Neural Networks</a></p>
<p>17 0.076352812 <a title="157-tfidf-17" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>18 0.075312681 <a title="157-tfidf-18" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>19 0.071838483 <a title="157-tfidf-19" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>20 0.071837828 <a title="157-tfidf-20" href="./emnlp-2013-Decoding_with_Large-Scale_Neural_Language_Models_Improves_Translation.html">55 emnlp-2013-Decoding with Large-Scale Neural Language Models Improves Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.259), (1, -0.254), (2, 0.058), (3, 0.006), (4, 0.135), (5, 0.033), (6, -0.03), (7, -0.117), (8, -0.137), (9, 0.092), (10, 0.069), (11, 0.117), (12, -0.146), (13, -0.154), (14, -0.2), (15, 0.182), (16, 0.059), (17, -0.197), (18, -0.116), (19, -0.011), (20, -0.023), (21, -0.073), (22, 0.103), (23, 0.077), (24, 0.093), (25, 0.069), (26, -0.007), (27, -0.051), (28, -0.023), (29, 0.051), (30, -0.013), (31, 0.115), (32, -0.065), (33, 0.056), (34, -0.083), (35, 0.057), (36, -0.001), (37, -0.029), (38, 0.057), (39, -0.007), (40, -0.028), (41, -0.028), (42, -0.04), (43, -0.003), (44, 0.055), (45, -0.088), (46, -0.02), (47, 0.072), (48, -0.007), (49, 0.06)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94302076 <a title="157-lsi-1" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>Author: Peng Li ; Yang Liu ; Maosong Sun</p><p>Abstract: While inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (i.e., straight and inverted) dependent on actual blocks being merged remains a challenge. Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points.</p><p>2 0.73698574 <a title="157-lsi-2" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>Author: Uri Lerner ; Slav Petrov</p><p>Abstract: We present a simple and novel classifier-based preordering approach. Unlike existing preordering models, we train feature-rich discriminative classifiers that directly predict the target-side word order. Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discriminative model with a rich set of features, including lexical features. We present extensive experiments on 22 language pairs, including preordering into English from 7 other languages. We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task. For languages from different families the improvements often exceed 2 BLEU. Many of these gains are also significant in human evaluations.</p><p>3 0.72516745 <a title="157-lsi-3" href="./emnlp-2013-Shift-Reduce_Word_Reordering_for_Machine_Translation.html">171 emnlp-2013-Shift-Reduce Word Reordering for Machine Translation</a></p>
<p>Author: Katsuhiko Hayashi ; Katsuhito Sudoh ; Hajime Tsukada ; Jun Suzuki ; Masaaki Nagata</p><p>Abstract: This paper presents a novel word reordering model that employs a shift-reduce parser for inversion transduction grammars. Our model uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system.</p><p>4 0.64526546 <a title="157-lsi-4" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>Author: Hendra Setiawan ; Bowen Zhou ; Bing Xiang</p><p>Abstract: Reordering poses one of the greatest challenges in Statistical Machine Translation research as the key contextual information may well be beyond the confine oftranslation units. We present the “Anchor Graph” (AG) model where we use a graph structure to model global contextual information that is crucial for reordering. The key ingredient of our AG model is the edges that capture the relationship between the reordering around a set of selected translation units, which we refer to as anchors. As the edges link anchors that may span multiple translation units at decoding time, our AG model effectively encodes global contextual information that is previously absent. We integrate our proposed model into a state-of-the-art translation system and demonstrate the efficacy of our proposal in a largescale Chinese-to-English translation task.</p><p>5 0.63577157 <a title="157-lsi-5" href="./emnlp-2013-Factored_Soft_Source_Syntactic_Constraints_for_Hierarchical_Machine_Translation.html">84 emnlp-2013-Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation</a></p>
<p>Author: Zhongqiang Huang ; Jacob Devlin ; Rabih Zbib</p><p>Abstract: Translation Jacob Devlin Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA j devl in@bbn . com Rabih Zbib Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA r zbib@bbn . com have tried to introduce grammaticality to the transThis paper describes a factored approach to incorporating soft source syntactic constraints into a hierarchical phrase-based translation system. In contrast to traditional approaches that directly introduce syntactic constraints to translation rules by explicitly decorating them with syntactic annotations, which often exacerbate the data sparsity problem and cause other problems, our approach keeps translation rules intact and factorizes the use of syntactic constraints through two separate models: 1) a syntax mismatch model that associates each nonterminal of a translation rule with a distribution of tags that is used to measure the degree of syntactic compatibility of the translation rule on source spans; 2) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language. The features produced by both models are used as soft constraints to guide the translation process. Experiments on Chinese-English translation show that the proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets.</p><p>6 0.53222448 <a title="157-lsi-6" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>7 0.51149344 <a title="157-lsi-7" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>8 0.51121074 <a title="157-lsi-8" href="./emnlp-2013-Efficient_Left-to-Right_Hierarchical_Phrase-Based_Translation_with_Improved_Reordering.html">71 emnlp-2013-Efficient Left-to-Right Hierarchical Phrase-Based Translation with Improved Reordering</a></p>
<p>9 0.43460727 <a title="157-lsi-9" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>10 0.38722664 <a title="157-lsi-10" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>11 0.38605449 <a title="157-lsi-11" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>12 0.38589713 <a title="157-lsi-12" href="./emnlp-2013-Recurrent_Continuous_Translation_Models.html">156 emnlp-2013-Recurrent Continuous Translation Models</a></p>
<p>13 0.36332157 <a title="157-lsi-13" href="./emnlp-2013-Decoding_with_Large-Scale_Neural_Language_Models_Improves_Translation.html">55 emnlp-2013-Decoding with Large-Scale Neural Language Models Improves Translation</a></p>
<p>14 0.35531977 <a title="157-lsi-14" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>15 0.3537789 <a title="157-lsi-15" href="./emnlp-2013-Deriving_Adjectival_Scales_from_Continuous_Space_Word_Representations.html">59 emnlp-2013-Deriving Adjectival Scales from Continuous Space Word Representations</a></p>
<p>16 0.33871314 <a title="157-lsi-16" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>17 0.33595365 <a title="157-lsi-17" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>18 0.33053461 <a title="157-lsi-18" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>19 0.32902503 <a title="157-lsi-19" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>20 0.31663111 <a title="157-lsi-20" href="./emnlp-2013-Joint_Language_and_Translation_Modeling_with_Recurrent_Neural_Networks.html">113 emnlp-2013-Joint Language and Translation Modeling with Recurrent Neural Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.026), (6, 0.048), (10, 0.015), (18, 0.04), (22, 0.065), (30, 0.115), (44, 0.174), (45, 0.018), (50, 0.021), (51, 0.132), (66, 0.04), (71, 0.023), (75, 0.034), (77, 0.087), (96, 0.025), (97, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86411345 <a title="157-lda-1" href="./emnlp-2013-Joint_Parsing_and_Disfluency_Detection_in_Linear_Time.html">116 emnlp-2013-Joint Parsing and Disfluency Detection in Linear Time</a></p>
<p>Author: Mohammad Sadegh Rasooli ; Joel Tetreault</p><p>Abstract: We introduce a novel method to jointly parse and detect disfluencies in spoken utterances. Our model can use arbitrary features for parsing sentences and adapt itself with out-ofdomain data. We show that our method, based on transition-based parsing, performs at a high level of accuracy for both the parsing and disfluency detection tasks. Additionally, our method is the fastest for the joint task, running in linear time.</p><p>same-paper 2 0.83624822 <a title="157-lda-2" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>Author: Peng Li ; Yang Liu ; Maosong Sun</p><p>Abstract: While inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (i.e., straight and inverted) dependent on actual blocks being merged remains a challenge. Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points.</p><p>3 0.72981989 <a title="157-lda-3" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>Author: Fandong Meng ; Jun Xie ; Linfeng Song ; Yajuan Lu ; Qun Liu</p><p>Abstract: We present a novel translation model, which simultaneously exploits the constituency and dependency trees on the source side, to combine the advantages of two types of trees. We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. Our rules hold the property of long distance reorderings and the compatibility with phrases. Large-scale experimental results show that our model achieves significantly improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets.</p><p>4 0.72360629 <a title="157-lda-4" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>Author: Richard Socher ; Alex Perelygin ; Jean Wu ; Jason Chuang ; Christopher D. Manning ; Andrew Ng ; Christopher Potts</p><p>Abstract: Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.</p><p>5 0.7212407 <a title="157-lda-5" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>Author: Uri Lerner ; Slav Petrov</p><p>Abstract: We present a simple and novel classifier-based preordering approach. Unlike existing preordering models, we train feature-rich discriminative classifiers that directly predict the target-side word order. Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discriminative model with a rich set of features, including lexical features. We present extensive experiments on 22 language pairs, including preordering into English from 7 other languages. We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task. For languages from different families the improvements often exceed 2 BLEU. Many of these gains are also significant in human evaluations.</p><p>6 0.71869385 <a title="157-lda-6" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>7 0.71783042 <a title="157-lda-7" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>8 0.71558398 <a title="157-lda-8" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>9 0.7150656 <a title="157-lda-9" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>10 0.71478796 <a title="157-lda-10" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>11 0.71415585 <a title="157-lda-11" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>12 0.7136898 <a title="157-lda-12" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>13 0.70801973 <a title="157-lda-13" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>14 0.70777136 <a title="157-lda-14" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>15 0.70631808 <a title="157-lda-15" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>16 0.69802582 <a title="157-lda-16" href="./emnlp-2013-Joint_Language_and_Translation_Modeling_with_Recurrent_Neural_Networks.html">113 emnlp-2013-Joint Language and Translation Modeling with Recurrent Neural Networks</a></p>
<p>17 0.69735587 <a title="157-lda-17" href="./emnlp-2013-Flexible_and_Efficient_Hypergraph_Interactions_for_Joint_Hierarchical_and_Forest-to-String_Decoding.html">88 emnlp-2013-Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest-to-String Decoding</a></p>
<p>18 0.69662601 <a title="157-lda-18" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>19 0.69529843 <a title="157-lda-19" href="./emnlp-2013-Converting_Continuous-Space_Language_Models_into_N-Gram_Language_Models_for_Statistical_Machine_Translation.html">52 emnlp-2013-Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation</a></p>
<p>20 0.69426757 <a title="157-lda-20" href="./emnlp-2013-Recurrent_Continuous_Translation_Models.html">156 emnlp-2013-Recurrent Continuous Translation Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
