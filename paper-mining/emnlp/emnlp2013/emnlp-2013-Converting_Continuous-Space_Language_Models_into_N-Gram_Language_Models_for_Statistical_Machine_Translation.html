<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>52 emnlp-2013-Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-52" href="#">emnlp2013-52</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>52 emnlp-2013-Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation</h1>
<br/><p>Source: <a title="emnlp-2013-52-pdf" href="http://aclweb.org/anthology//D/D13/D13-1082.pdf">pdf</a></p><p>Author: Rui Wang ; Masao Utiyama ; Isao Goto ; Eiichro Sumita ; Hai Zhao ; Bao-Liang Lu</p><p>Abstract: Neural network language models, or continuous-space language models (CSLMs), have been shown to improve the performance of statistical machine translation (SMT) when they are used for reranking n-best translations. However, CSLMs have not been used in the first pass decoding of SMT, because using CSLMs in decoding takes a lot of time. In contrast, we propose a method for converting CSLMs into back-off n-gram language models (BNLMs) so that we can use converted CSLMs in decoding. We show that they outperform the original BNLMs and are comparable with the traditional use of CSLMs in reranking.</p><p>Reference: <a title="emnlp-2013-52-reference" href="../emnlp2013_reference/emnlp-2013-Converting_Continuous-Space_Language_Models_into_N-Gram_Language_Models_for_Statistical_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn  Abstract Neural network language models, or continuous-space language models (CSLMs), have been shown to improve the performance of statistical machine translation (SMT) when they are used for reranking n-best translations. [sent-12, score-0.185]
</p><p>2 However, CSLMs have not been used in the first pass decoding of SMT, because using CSLMs in decoding takes a lot of time. [sent-13, score-0.125]
</p><p>3 In contrast, we propose a method for converting CSLMs into back-off n-gram language models (BNLMs) so that we can use converted CSLMs in decoding. [sent-14, score-0.064]
</p><p>4 1 Introduction Language models are important in natural language  processing tasks such as speech recognition and statistical machine translation. [sent-16, score-0.046]
</p><p>5 Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al. [sent-18, score-0.081]
</p><p>6 , 2011) are being used in statistical machine translation (SMT) (Schwenk et al. [sent-20, score-0.067]
</p><p>7 , 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language 845 modeling are the same size. [sent-26, score-0.02]
</p><p>8 One reason is that the computational costs of training and using CSLMs are very high. [sent-28, score-0.028]
</p><p>9 Since the using costs of CSLMs are very high, it is difficult to use CSLMs in decoding directly. [sent-34, score-0.061]
</p><p>10 A common approach in SMT using CSLMs is the two pass approach, or n-best reranking. [sent-35, score-0.059]
</p><p>11 In this approach, the first pass uses a BNLM in decoding to produce an n-best list. [sent-36, score-0.092]
</p><p>12 Then, a CSLM is used to rerank those n-best translations in the second pass. [sent-37, score-0.027]
</p><p>13 , 2012) Another approach is using restricted Boltzmann machines (RBMs) (Niehues and Waibel, 2012) instead of using multi-layer neural networks (Bengio et al. [sent-42, score-0.04]
</p><p>14 Since probability in a RBM can be calculated very efficiently (Niehues and Waibel, 2012), they can use the RBM language model in SMT decoding. [sent-45, score-0.055]
</p><p>15 However, the RBM was just used in an adaptation of SMT, not in a large SMT task, because the training costs of RBMs are very high. [sent-46, score-0.028]
</p><p>16 , 2011) used a recurrent neural network language model (RNNLM) to generate a large  amount of text, which was generated by sampling words from the probability distributions calculated by the RNNLM. [sent-51, score-0.136]
</p><p>17 Then, they trained the BNLM ProceSe datintlges, o Wfa tsh ein 2g01to3n, C UoSnfAe,re 1n8c-e2 o1n O Ecmtopbier ic 2a0l1 M3. [sent-52, score-0.023]
</p><p>18 hc o2d0s1 i3n A Nsastoucria lti Loan fgoura Cgoem Ppruotcaetsiosin agl, L piang eusis 8t4ic5s–850, from the text using the interpolated Kneser-Ney smoothing method. [sent-54, score-0.063]
</p><p>19 , 2013) converted neural network language models of increasing order to pruned back-off language models, using lowerorder models to constrain the n-grams allowed in higher-order models. [sent-56, score-0.099]
</p><p>20 Both of these methods were used in decoding for speech recognition. [sent-57, score-0.059]
</p><p>21 In contrast, our method is applied to SMT and can be used to improve a BNLM created from 746 M words by using a CSLM trained from 42 M words. [sent-60, score-0.023]
</p><p>22 Because BNLMs can be trained from much larger corpora than those that can be used for training CSLMs, improving a BNLM by using a CSLM trained from a smaller corpus is very important. [sent-61, score-0.046]
</p><p>23 Actually, a CSLM trained from a smaller corpus  can improve the BLEU scores of SMT if it is used in the n-best reranking (Schwenk, 2010; Huang et al. [sent-62, score-0.12]
</p><p>24 In contrast, we will demonstrate that a BNLM simulating a CSLM can improve the BLEU scores of SMT in the first pass decoding. [sent-64, score-0.079]
</p><p>25 (3) Finally, we rewrite the probability of each n-gram of the BNLM with that probability calculated from the CSLM. [sent-67, score-0.118]
</p><p>26 We also re-normalize the probabilities of the BNLM, then use the re-written BNLM in SMT decoding. [sent-68, score-0.046]
</p><p>27 In Section 3, we describe the method of converting a CSLM into a BNLM. [sent-70, score-0.064]
</p><p>28 2  Language Models  In this section, we will introduce the standard BNLM and CSLM structure and probability calculation. [sent-72, score-0.027]
</p><p>29 1 Standard back-off ngram language model A BNLM predicts the probability of a word wi given its preceding n − 1 words hi = wii−−n1+1 . [sent-74, score-0.236]
</p><p>30 In the case of the modified Kneser-Ney smoothing (Chen and Goodman, 1998), the probability of wi given hi under a BNLM, Pb(wi |hi), is: Pb(wi|hi) = Pˆb(wi|hi) + γ(hi)Pb(wi|wii−−1n+2) (1) where |hi) is a discounted probability and γ(hi) is the ba|hck-off weight. [sent-77, score-0.285]
</p><p>31 The CSLM calculates the probabilities of all words in the vocabulary of the corpus given the context at once. [sent-82, score-0.077]
</p><p>32 However, because the computational complexity of calculating the probabilities of all words is quite high, the CSLM is only used to calculate the probabilities of a subset of the whole vocabulary. [sent-83, score-0.108]
</p><p>33 The CSLM also calculates the sum of the probabilities of all words not in the short-list by assigning a neuron for that purpose. [sent-85, score-0.077]
</p><p>34 The probabilities of other words not in the short-list are obtained from a BNLM (Schwenk, 2007; Schwenk, 2010). [sent-86, score-0.046]
</p><p>35 (3) v∈sh∑ort-list It can be considered that the CSLM redistributes the probability mass of all words in the short-list. [sent-90, score-0.043]
</p><p>36 This probability mass is calculated by using the BNLM. [sent-91, score-0.071]
</p><p>37 Then, we rewrite the probability of each ngram in the BNLM with the probability calculated from the CSLM. [sent-94, score-0.133]
</p><p>38 First, we use the probabilities of 1-grams in the BNLM as they are. [sent-95, score-0.046]
</p><p>39 Next, we rewrite the probabilities of n-grams (n=2,3,4,5) in the BNLM with the probabilities calculated by using the n-gram CSLM, respectively. [sent-96, score-0.156]
</p><p>40 Note amlseoa tsha tth we only gntehed o fto it sre hwisrtioter yth ies probabilities  of n-grams ending with a word in the short-list. [sent-98, score-0.046]
</p><p>41 Finally, we re-normalize the probabilities of the BNLM using the SRILM’s ‘-renorm’ option. [sent-99, score-0.046]
</p><p>42 When we rewrite a BNLM trained from a larger corpus, the ngrams in the BNLM often contain unknown words for the CSLM. [sent-100, score-0.059]
</p><p>43 In that case, we use the probabilities in the BNLM as they are. [sent-101, score-0.046]
</p><p>44 1 Common settings We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al. [sent-103, score-0.346]
</p><p>45 The parallel training, development, and test data consisted of 1 M, 2,000, and 2,000 sentences, respectively. [sent-105, score-0.029]
</p><p>46 We followed the settings of the NTCIR-9 Chinese to English translation baseline system (Goto et al. [sent-106, score-0.047]
</p><p>47 The translation performance was measured by the case-insensitive BLEU scores on the tokenized  test data. [sent-110, score-0.067]
</p><p>48 mig/te st s /mt / 2 0 0 9 / 847 We used the 14 standard SMT features: five translation model scores, one word penalty score, seven distortion scores and one language model score. [sent-116, score-0.067]
</p><p>49 As the baseline BNLM, we trained a 5-gram BNLM with modified Kneser-Ney smoothing using the English side of the 1 M sentences training data, which consisted of 42 M words. [sent-118, score-0.089]
</p><p>50 A 5-gram CSLM was trained on the same 1 M training sentences using the CSLM toolkit (Schwenk, 2010). [sent-122, score-0.023]
</p><p>51 The settings for the CSLM were: projection layer of dimension 256 for each word, hidden layer of dimension 384 and output layer (short-list) of dimension 8192, which were  recommended in the CSLM toolkit. [sent-123, score-0.252]
</p><p>52 We also trained a larger 5-gram BNLM with modified Kneser-Ney smoothing by adding sentences from the 2005 US patent data distributed in the NTCIR-8 patent translation task (Fujii et al. [sent-126, score-0.275]
</p><p>53 The interpolation weight was determined by the grid search. [sent-133, score-0.066]
</p><p>54 That is, we changed the interpolation weight to 0. [sent-134, score-0.082]
</p><p>55 Then we used that BNLM in the SMT system to tune the weight parameters on the first half of the development data. [sent-140, score-0.023]
</p><p>56 Next, we selected the interpolation weight that obtained the highest BLEU score on the second half of the development data. [sent-141, score-0.066]
</p><p>57 After we selected the interpolation weight, we applied MERT again to the 2,000 sentence  development data to tune the weight parameters. [sent-142, score-0.066]
</p><p>58 We also obtained CONV746 by re-writing BNLM746 with CSLM42 2We aware that the interpolation weight might be determined by minimizing the perplexity on the development data. [sent-144, score-0.085]
</p><p>59 2 Experimental results Table 1 shows the percent BLEU scores on the test data. [sent-149, score-0.02]
</p><p>60 The figures in the “1st pass” column show the BLEU scores in the first pass decoding when we changed the language model. [sent-150, score-0.128]
</p><p>61 The figures in the “reranking” column show the BLEU scores when we applied CSLM42 to rerank the 100-best lists for the different language models. [sent-151, score-0.047]
</p><p>62 The weight parameters were tuned by using Z-MERT (Zaidan, 2009). [sent-153, score-0.023]
</p><p>63 9435648  LMs1st passrerank  Table 1: Comparison of BLEU scores  We also performed the paired bootstrap resampling test (Koehn, 2004). [sent-156, score-0.02]
</p><p>64 Table 2 shows the results of a statistical significance test, in which the “1st” is short for the “1st pass”. [sent-158, score-0.04]
</p><p>65 t0h5e) reranking by applying CSLM42 increased the BLEU scores for all language models. [sent-164, score-0.097]
</p><p>66 44)  were not better than those of the first pass of BNLM746 (32. [sent-168, score-0.059]
</p><p>67 This indicates that if the underlying BNLM is made from a small corpus, the reranking using CSLM can not compensate for it. [sent-170, score-0.077]
</p><p>68 This indicated that our conversion method improved the BNLMs, even if the underlying BNLM was trained on a larger corpus than that used for training the CSLM. [sent-178, score-0.065]
</p><p>69 As described in the introduction, this is very important because BNLMs can be trained from much larger corpora than those that can be used for training CSLMs. [sent-179, score-0.023]
</p><p>70 22) were comparable with those of the reranking results of BNLM42 and BNLM746 (32. [sent-183, score-0.077]
</p><p>71 This indicates that our conversion method preserves the performance of the reranking using CSLM. [sent-187, score-0.119]
</p><p>72 5  Conclusion  We have proposed a method for converting CSLMs into BNLMs. [sent-188, score-0.064]
</p><p>73 The method can be used to improve a BNLM by using a CSLM trained from a smaller corpus than that used for training the BNLM. [sent-189, score-0.023]
</p><p>74 We have also shown that BNLMs created by our method  performs as good as the reranking using CSLMs. [sent-190, score-0.077]
</p><p>75 Our future work is to compare our conversion method with that of (Arsoy et al. [sent-191, score-0.042]
</p><p>76 However, the experiments were conducted on a speech recognition task and the scale of the experiment was not so large. [sent-195, score-0.026]
</p><p>77 Converting neural network language models into back-off language models for efficient decoding in automatic speech recognition. [sent-207, score-0.14]
</p><p>78 An empirical study of smoothing techniques for language modeling. [sent-220, score-0.037]
</p><p>79 In Proceedings of the 34th annual meeting on Association for Computational Linguistics, ACL ’96, pages 3 10–318, Santa Cruz, California, June. [sent-221, score-0.021]
</p><p>80 An empirical study of smoothing techniques for language modeling. [sent-226, score-0.037]
</p><p>81 In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 5532– 5535, Prague, Czech Republic, May. [sent-235, score-0.021]
</p><p>82 Overview of the patent translation task at the ntcir-8 workshop. [sent-239, score-0.131]
</p><p>83 In In Proceedings of the 8th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, QuestionAnswering and Crosslingual Information Access, pages 293–302, Tokyo, Japan, June. [sent-240, score-0.021]
</p><p>84 Overview of the patent machine translation task at the NTCIR-9 workshop. [sent-244, score-0.131]
</p><p>85 In Proceedings of NTCIR-9 Workshop Meeting, pages 559–578, Tokyo, Japan, December. [sent-245, score-0.021]
</p><p>86 In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 48–54, Edmonton, Canada. [sent-253, score-0.021]
</p><p>87 In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 388–395, Barcelona, Spain, July. [sent-258, score-0.021]
</p><p>88 In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 5524–5527, Prague, Czech Republic, May. [sent-267, score-0.021]
</p><p>89 Strategies for training large scale neural network language models. [sent-271, score-0.081]
</p><p>90 In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 196– 201, Prague, Czech Republic, May. [sent-272, score-0.021]
</p><p>91 In Proceedings of the International Workshop for Spoken Language Translation, IWSLT 2012, pages 3 11–3 18, Hong Kong. [sent-277, score-0.021]
</p><p>92 In  Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan, July. [sent-285, score-0.021]
</p><p>93 In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 3 11– 3 18, Philadelphia, Pennsylvania, June. [sent-290, score-0.021]
</p><p>94 In Proceedings of the COLING/ACL on Main conference poster sessions, COLING-ACL ’06, pages 723–730, Sydney, Australia, July. [sent-295, score-0.021]
</p><p>95 Large, pruned or continuous space language models on a gpu for statistical machine translation. [sent-299, score-0.075]
</p><p>96 On the Future ofLanguage ModelingforHLT, WLM ’ 12, pages 11–19, Montreal, Canada, June. [sent-301, score-0.021]
</p><p>97 In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’ 10, pages 778–788, Cambridge, Massachusetts, October. [sent-314, score-0.021]
</p><p>98 In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’ 12, pages 39–48, Montreal, Canada, June. [sent-319, score-0.021]
</p><p>99 In Proceedings International Conference on Spoken Language Processing, pages 257–286, November. [sent-324, score-0.021]
</p><p>100 Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems. [sent-328, score-0.047]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bnlm', 0.654), ('cslm', 0.499), ('cslms', 0.258), ('schwenk', 0.204), ('bnlms', 0.155), ('hi', 0.137), ('smt', 0.108), ('arsoy', 0.103), ('patent', 0.084), ('reranking', 0.077), ('bleu', 0.077), ('son', 0.073), ('deoras', 0.069), ('layer', 0.066), ('converting', 0.064), ('shanghai', 0.06), ('niehues', 0.06), ('pass', 0.059), ('wi', 0.057), ('rbm', 0.052), ('waibel', 0.052), ('holger', 0.051), ('translation', 0.047), ('probabilities', 0.046), ('pb', 0.044), ('interpolation', 0.043), ('acoustics', 0.042), ('conversion', 0.042), ('goto', 0.041), ('network', 0.041), ('icassp', 0.04), ('neural', 0.04), ('japan', 0.039), ('smoothing', 0.037), ('continuous', 0.037), ('rewrite', 0.036), ('rbms', 0.034), ('unviersity', 0.034), ('stanley', 0.034), ('allauzen', 0.034), ('decoding', 0.033), ('prague', 0.033), ('calculates', 0.031), ('goodman', 0.031), ('tokyo', 0.031), ('hai', 0.03), ('wii', 0.03), ('consisted', 0.029), ('mikolov', 0.029), ('calculated', 0.028), ('costs', 0.028), ('rerank', 0.027), ('isao', 0.027), ('jiao', 0.027), ('probability', 0.027), ('interpolated', 0.026), ('speech', 0.026), ('boltzmann', 0.025), ('fujii', 0.024), ('masao', 0.024), ('och', 0.024), ('trained', 0.023), ('chen', 0.023), ('weight', 0.023), ('tong', 0.023), ('china', 0.023), ('canada', 0.023), ('bengio', 0.023), ('bulletin', 0.022), ('bc', 0.022), ('le', 0.022), ('signal', 0.022), ('pages', 0.021), ('republic', 0.02), ('cois', 0.02), ('significance', 0.02), ('josef', 0.02), ('scores', 0.02), ('statistical', 0.02), ('montreal', 0.02), ('czech', 0.019), ('aware', 0.019), ('franz', 0.019), ('ps', 0.018), ('ieee', 0.018), ('joshua', 0.018), ('fran', 0.018), ('pruned', 0.018), ('dimension', 0.018), ('alexandre', 0.017), ('koehn', 0.017), ('mert', 0.016), ('changed', 0.016), ('mass', 0.016), ('calculating', 0.016), ('mathematical', 0.016), ('papineni', 0.015), ('ngram', 0.015), ('jan', 0.015), ('wisniewski', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="52-tfidf-1" href="./emnlp-2013-Converting_Continuous-Space_Language_Models_into_N-Gram_Language_Models_for_Statistical_Machine_Translation.html">52 emnlp-2013-Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation</a></p>
<p>Author: Rui Wang ; Masao Utiyama ; Isao Goto ; Eiichro Sumita ; Hai Zhao ; Bao-Liang Lu</p><p>Abstract: Neural network language models, or continuous-space language models (CSLMs), have been shown to improve the performance of statistical machine translation (SMT) when they are used for reranking n-best translations. However, CSLMs have not been used in the first pass decoding of SMT, because using CSLMs in decoding takes a lot of time. In contrast, we propose a method for converting CSLMs into back-off n-gram language models (BNLMs) so that we can use converted CSLMs in decoding. We show that they outperform the original BNLMs and are comparable with the traditional use of CSLMs in reranking.</p><p>2 0.092410401 <a title="52-tfidf-2" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>Author: Lei Cui ; Xilun Chen ; Dongdong Zhang ; Shujie Liu ; Mu Li ; Ming Zhou</p><p>Abstract: Domain adaptation for SMT usually adapts models to an individual specific domain. However, it often lacks some correlation among different domains where common knowledge could be shared to improve the overall translation quality. In this paper, we propose a novel multi-domain adaptation approach for SMT using Multi-Task Learning (MTL), with in-domain models tailored for each specific domain and a general-domain model shared by different domains. The parameters of these models are tuned jointly via MTL so that they can learn general knowledge more accurately and exploit domain knowledge better. Our experiments on a largescale English-to-Chinese translation task validate that the MTL-based adaptation approach significantly and consistently improves the translation quality compared to a non-adapted baseline. Furthermore, it also outperforms the individual adaptation of each specific domain.</p><p>3 0.083190329 <a title="52-tfidf-3" href="./emnlp-2013-Joint_Language_and_Translation_Modeling_with_Recurrent_Neural_Networks.html">113 emnlp-2013-Joint Language and Translation Modeling with Recurrent Neural Networks</a></p>
<p>Author: Michael Auli ; Michel Galley ; Chris Quirk ; Geoffrey Zweig</p><p>Abstract: We present a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words. The weaker independence assumptions of this model result in a vastly larger search space compared to related feedforward-based language or translation models. We tackle this issue with a new lattice rescoring algorithm and demonstrate its effectiveness empirically. Our joint model builds on a well known recurrent neural network language model (Mikolov, 2012) augmented by a layer of additional inputs from the source language. We show competitive accuracy compared to the traditional channel model features. Our best results improve the output of a system trained on WMT 2012 French-English data by up to 1.5 BLEU, and by 1.1BLEU on average across several test sets.</p><p>4 0.076317988 <a title="52-tfidf-4" href="./emnlp-2013-Boosting_Cross-Language_Retrieval_by_Learning_Bilingual_Phrase_Associations_from_Relevance_Rankings.html">39 emnlp-2013-Boosting Cross-Language Retrieval by Learning Bilingual Phrase Associations from Relevance Rankings</a></p>
<p>Author: Artem Sokokov ; Laura Jehl ; Felix Hieber ; Stefan Riezler</p><p>Abstract: We present an approach to learning bilingual n-gram correspondences from relevance rankings of English documents for Japanese queries. We show that directly optimizing cross-lingual rankings rivals and complements machine translation-based cross-language information retrieval (CLIR). We propose an efficient boosting algorithm that deals with very large cross-product spaces of word correspondences. We show in an experimental evaluation on patent prior art search that our approach, and in particular a consensus-based combination of boosting and translation-based approaches, yields substantial improvements in CLIR performance. Our training and test data are made publicly available.</p><p>5 0.075742692 <a title="52-tfidf-5" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>Author: Joern Wuebker ; Stephan Peitz ; Felix Rietig ; Hermann Ney</p><p>Abstract: Automatically clustering words from a monolingual or bilingual training corpus into classes is a widely used technique in statistical natural language processing. We present a very simple and easy to implement method for using these word classes to improve translation quality. It can be applied across different machine translation paradigms and with arbitrary types of models. We show its efficacy on a small German→English and a larger F ornenc ah s→mGalelrm Gaenrm mtarann→slEatniognli tsahsk a nwdit ha lbaortghe rst Farnednacrhd→ phrase-based salandti nhie traaskrch wiciathl phrase-based translation systems for a common set of models. Our results show that with word class models, the baseline can be improved by up to 1.4% BLEU and 1.0% TER on the French→German task and 0.3% BLEU aonnd t h1e .1 F%re nTcEhR→ on tehrem German→English Btask.</p><p>6 0.065450035 <a title="52-tfidf-6" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>7 0.059093621 <a title="52-tfidf-7" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>8 0.049708895 <a title="52-tfidf-8" href="./emnlp-2013-Decoding_with_Large-Scale_Neural_Language_Models_Improves_Translation.html">55 emnlp-2013-Decoding with Large-Scale Neural Language Models Improves Translation</a></p>
<p>9 0.04742299 <a title="52-tfidf-9" href="./emnlp-2013-A_Corpus_Level_MIRA_Tuning_Strategy_for_Machine_Translation.html">3 emnlp-2013-A Corpus Level MIRA Tuning Strategy for Machine Translation</a></p>
<p>10 0.045707263 <a title="52-tfidf-10" href="./emnlp-2013-Recurrent_Continuous_Translation_Models.html">156 emnlp-2013-Recurrent Continuous Translation Models</a></p>
<p>11 0.044597365 <a title="52-tfidf-11" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>12 0.041647248 <a title="52-tfidf-12" href="./emnlp-2013-Dependency_Language_Models_for_Sentence_Completion.html">58 emnlp-2013-Dependency Language Models for Sentence Completion</a></p>
<p>13 0.040155593 <a title="52-tfidf-13" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>14 0.040149156 <a title="52-tfidf-14" href="./emnlp-2013-Deriving_Adjectival_Scales_from_Continuous_Space_Word_Representations.html">59 emnlp-2013-Deriving Adjectival Scales from Continuous Space Word Representations</a></p>
<p>15 0.038447812 <a title="52-tfidf-15" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>16 0.037209354 <a title="52-tfidf-16" href="./emnlp-2013-Efficient_Left-to-Right_Hierarchical_Phrase-Based_Translation_with_Improved_Reordering.html">71 emnlp-2013-Efficient Left-to-Right Hierarchical Phrase-Based Translation with Improved Reordering</a></p>
<p>17 0.036479339 <a title="52-tfidf-17" href="./emnlp-2013-Latent_Anaphora_Resolution_for_Cross-Lingual_Pronoun_Prediction.html">117 emnlp-2013-Latent Anaphora Resolution for Cross-Lingual Pronoun Prediction</a></p>
<p>18 0.034720283 <a title="52-tfidf-18" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>19 0.033754151 <a title="52-tfidf-19" href="./emnlp-2013-Shift-Reduce_Word_Reordering_for_Machine_Translation.html">171 emnlp-2013-Shift-Reduce Word Reordering for Machine Translation</a></p>
<p>20 0.033554085 <a title="52-tfidf-20" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.098), (1, -0.103), (2, 0.019), (3, 0.001), (4, 0.043), (5, -0.009), (6, 0.025), (7, 0.031), (8, -0.043), (9, -0.019), (10, 0.006), (11, -0.004), (12, 0.001), (13, -0.122), (14, 0.029), (15, -0.019), (16, 0.083), (17, 0.023), (18, 0.022), (19, 0.097), (20, 0.063), (21, 0.061), (22, -0.046), (23, 0.084), (24, -0.1), (25, 0.1), (26, 0.082), (27, -0.006), (28, 0.075), (29, 0.014), (30, 0.038), (31, -0.099), (32, 0.073), (33, -0.109), (34, 0.01), (35, 0.109), (36, 0.046), (37, -0.177), (38, -0.07), (39, -0.065), (40, 0.064), (41, -0.025), (42, 0.078), (43, 0.042), (44, -0.027), (45, 0.067), (46, 0.061), (47, -0.024), (48, 0.036), (49, -0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91396022 <a title="52-lsi-1" href="./emnlp-2013-Converting_Continuous-Space_Language_Models_into_N-Gram_Language_Models_for_Statistical_Machine_Translation.html">52 emnlp-2013-Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation</a></p>
<p>Author: Rui Wang ; Masao Utiyama ; Isao Goto ; Eiichro Sumita ; Hai Zhao ; Bao-Liang Lu</p><p>Abstract: Neural network language models, or continuous-space language models (CSLMs), have been shown to improve the performance of statistical machine translation (SMT) when they are used for reranking n-best translations. However, CSLMs have not been used in the first pass decoding of SMT, because using CSLMs in decoding takes a lot of time. In contrast, we propose a method for converting CSLMs into back-off n-gram language models (BNLMs) so that we can use converted CSLMs in decoding. We show that they outperform the original BNLMs and are comparable with the traditional use of CSLMs in reranking.</p><p>2 0.70717949 <a title="52-lsi-2" href="./emnlp-2013-Decoding_with_Large-Scale_Neural_Language_Models_Improves_Translation.html">55 emnlp-2013-Decoding with Large-Scale Neural Language Models Improves Translation</a></p>
<p>Author: Ashish Vaswani ; Yinggong Zhao ; Victoria Fossum ; David Chiang</p><p>Abstract: We explore the application of neural language models to machine translation. We develop a new model that combines the neural probabilistic language model of Bengio et al., rectified linear units, and noise-contrastive estimation, and we incorporate it into a machine translation system both by reranking k-best lists and by direct integration into the decoder. Our large-scale, large-vocabulary experiments across four language pairs show that our neural language model improves translation quality by up to 1. 1B .</p><p>3 0.69446999 <a title="52-lsi-3" href="./emnlp-2013-Joint_Language_and_Translation_Modeling_with_Recurrent_Neural_Networks.html">113 emnlp-2013-Joint Language and Translation Modeling with Recurrent Neural Networks</a></p>
<p>Author: Michael Auli ; Michel Galley ; Chris Quirk ; Geoffrey Zweig</p><p>Abstract: We present a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words. The weaker independence assumptions of this model result in a vastly larger search space compared to related feedforward-based language or translation models. We tackle this issue with a new lattice rescoring algorithm and demonstrate its effectiveness empirically. Our joint model builds on a well known recurrent neural network language model (Mikolov, 2012) augmented by a layer of additional inputs from the source language. We show competitive accuracy compared to the traditional channel model features. Our best results improve the output of a system trained on WMT 2012 French-English data by up to 1.5 BLEU, and by 1.1BLEU on average across several test sets.</p><p>4 0.54288596 <a title="52-lsi-4" href="./emnlp-2013-Recurrent_Continuous_Translation_Models.html">156 emnlp-2013-Recurrent Continuous Translation Models</a></p>
<p>Author: Nal Kalchbrenner ; Phil Blunsom</p><p>Abstract: We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.</p><p>5 0.45559242 <a title="52-lsi-5" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>Author: Lei Cui ; Xilun Chen ; Dongdong Zhang ; Shujie Liu ; Mu Li ; Ming Zhou</p><p>Abstract: Domain adaptation for SMT usually adapts models to an individual specific domain. However, it often lacks some correlation among different domains where common knowledge could be shared to improve the overall translation quality. In this paper, we propose a novel multi-domain adaptation approach for SMT using Multi-Task Learning (MTL), with in-domain models tailored for each specific domain and a general-domain model shared by different domains. The parameters of these models are tuned jointly via MTL so that they can learn general knowledge more accurately and exploit domain knowledge better. Our experiments on a largescale English-to-Chinese translation task validate that the MTL-based adaptation approach significantly and consistently improves the translation quality compared to a non-adapted baseline. Furthermore, it also outperforms the individual adaptation of each specific domain.</p><p>6 0.45240265 <a title="52-lsi-6" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>7 0.43785709 <a title="52-lsi-7" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>8 0.4199051 <a title="52-lsi-8" href="./emnlp-2013-Boosting_Cross-Language_Retrieval_by_Learning_Bilingual_Phrase_Associations_from_Relevance_Rankings.html">39 emnlp-2013-Boosting Cross-Language Retrieval by Learning Bilingual Phrase Associations from Relevance Rankings</a></p>
<p>9 0.37767857 <a title="52-lsi-9" href="./emnlp-2013-Dependency_Language_Models_for_Sentence_Completion.html">58 emnlp-2013-Dependency Language Models for Sentence Completion</a></p>
<p>10 0.37412375 <a title="52-lsi-10" href="./emnlp-2013-A_Corpus_Level_MIRA_Tuning_Strategy_for_Machine_Translation.html">3 emnlp-2013-A Corpus Level MIRA Tuning Strategy for Machine Translation</a></p>
<p>11 0.32741821 <a title="52-lsi-11" href="./emnlp-2013-Deriving_Adjectival_Scales_from_Continuous_Space_Word_Representations.html">59 emnlp-2013-Deriving Adjectival Scales from Continuous Space Word Representations</a></p>
<p>12 0.31188306 <a title="52-lsi-12" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>13 0.31009978 <a title="52-lsi-13" href="./emnlp-2013-Improving_Alignment_of_System_Combination_by_Using_Multi-objective_Optimization.html">101 emnlp-2013-Improving Alignment of System Combination by Using Multi-objective Optimization</a></p>
<p>14 0.30561617 <a title="52-lsi-14" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>15 0.30196238 <a title="52-lsi-15" href="./emnlp-2013-An_Efficient_Language_Model_Using_Double-Array_Structures.html">20 emnlp-2013-An Efficient Language Model Using Double-Array Structures</a></p>
<p>16 0.30123517 <a title="52-lsi-16" href="./emnlp-2013-Elephant%3A_Sequence_Labeling_for_Word_and_Sentence_Segmentation.html">72 emnlp-2013-Elephant: Sequence Labeling for Word and Sentence Segmentation</a></p>
<p>17 0.29990238 <a title="52-lsi-17" href="./emnlp-2013-Learning_to_Rank_Lexical_Substitutions.html">123 emnlp-2013-Learning to Rank Lexical Substitutions</a></p>
<p>18 0.29952347 <a title="52-lsi-18" href="./emnlp-2013-Noise-Aware_Character_Alignment_for_Bootstrapping_Statistical_Machine_Transliteration_from_Bilingual_Corpora.html">139 emnlp-2013-Noise-Aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora</a></p>
<p>19 0.28391477 <a title="52-lsi-19" href="./emnlp-2013-Latent_Anaphora_Resolution_for_Cross-Lingual_Pronoun_Prediction.html">117 emnlp-2013-Latent Anaphora Resolution for Cross-Lingual Pronoun Prediction</a></p>
<p>20 0.2758444 <a title="52-lsi-20" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.013), (10, 0.032), (18, 0.042), (22, 0.074), (29, 0.018), (30, 0.106), (34, 0.277), (43, 0.018), (45, 0.027), (50, 0.013), (51, 0.099), (66, 0.046), (71, 0.014), (75, 0.027), (77, 0.039), (85, 0.01), (90, 0.013), (96, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.71155548 <a title="52-lda-1" href="./emnlp-2013-Converting_Continuous-Space_Language_Models_into_N-Gram_Language_Models_for_Statistical_Machine_Translation.html">52 emnlp-2013-Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation</a></p>
<p>Author: Rui Wang ; Masao Utiyama ; Isao Goto ; Eiichro Sumita ; Hai Zhao ; Bao-Liang Lu</p><p>Abstract: Neural network language models, or continuous-space language models (CSLMs), have been shown to improve the performance of statistical machine translation (SMT) when they are used for reranking n-best translations. However, CSLMs have not been used in the first pass decoding of SMT, because using CSLMs in decoding takes a lot of time. In contrast, we propose a method for converting CSLMs into back-off n-gram language models (BNLMs) so that we can use converted CSLMs in decoding. We show that they outperform the original BNLMs and are comparable with the traditional use of CSLMs in reranking.</p><p>2 0.55490971 <a title="52-lda-2" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>Author: Kuzman Ganchev ; Dipanjan Das</p><p>Abstract: We present a framework for cross-lingual transfer of sequence information from a resource-rich source language to a resourceimpoverished target language that incorporates soft constraints via posterior regularization. To this end, we use automatically word aligned bitext between the source and target language pair, and learn a discriminative conditional random field model on the target side. Our posterior regularization constraints are derived from simple intuitions about the task at hand and from cross-lingual alignment information. We show improvements over strong baselines for two tasks: part-of-speech tagging and namedentity segmentation.</p><p>3 0.51562381 <a title="52-lda-3" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>Author: Joern Wuebker ; Stephan Peitz ; Felix Rietig ; Hermann Ney</p><p>Abstract: Automatically clustering words from a monolingual or bilingual training corpus into classes is a widely used technique in statistical natural language processing. We present a very simple and easy to implement method for using these word classes to improve translation quality. It can be applied across different machine translation paradigms and with arbitrary types of models. We show its efficacy on a small German→English and a larger F ornenc ah s→mGalelrm Gaenrm mtarann→slEatniognli tsahsk a nwdit ha lbaortghe rst Farnednacrhd→ phrase-based salandti nhie traaskrch wiciathl phrase-based translation systems for a common set of models. Our results show that with word class models, the baseline can be improved by up to 1.4% BLEU and 1.0% TER on the French→German task and 0.3% BLEU aonnd t h1e .1 F%re nTcEhR→ on tehrem German→English Btask.</p><p>4 0.50957435 <a title="52-lda-4" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>Author: Fandong Meng ; Jun Xie ; Linfeng Song ; Yajuan Lu ; Qun Liu</p><p>Abstract: We present a novel translation model, which simultaneously exploits the constituency and dependency trees on the source side, to combine the advantages of two types of trees. We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. Our rules hold the property of long distance reorderings and the compatibility with phrases. Large-scale experimental results show that our model achieves significantly improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets.</p><p>5 0.50359625 <a title="52-lda-5" href="./emnlp-2013-A_Corpus_Level_MIRA_Tuning_Strategy_for_Machine_Translation.html">3 emnlp-2013-A Corpus Level MIRA Tuning Strategy for Machine Translation</a></p>
<p>Author: Ming Tan ; Tian Xia ; Shaojun Wang ; Bowen Zhou</p><p>Abstract: MIRA based tuning methods have been widely used in statistical machine translation (SMT) system with a large number of features. Since the corpus-level BLEU is not decomposable, these MIRA approaches usually define a variety of heuristic-driven sentencelevel BLEUs in their model losses. Instead, we present a new MIRA method, which employs an exact corpus-level BLEU to compute the model loss. Our method is simpler in implementation. Experiments on Chinese-toEnglish translation show its effectiveness over two state-of-the-art MIRA implementations.</p><p>6 0.50204003 <a title="52-lda-6" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>7 0.50085598 <a title="52-lda-7" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>8 0.50076079 <a title="52-lda-8" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>9 0.5006997 <a title="52-lda-9" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>10 0.49670386 <a title="52-lda-10" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>11 0.4963508 <a title="52-lda-11" href="./emnlp-2013-Flexible_and_Efficient_Hypergraph_Interactions_for_Joint_Hierarchical_and_Forest-to-String_Decoding.html">88 emnlp-2013-Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest-to-String Decoding</a></p>
<p>12 0.49507204 <a title="52-lda-12" href="./emnlp-2013-Joint_Language_and_Translation_Modeling_with_Recurrent_Neural_Networks.html">113 emnlp-2013-Joint Language and Translation Modeling with Recurrent Neural Networks</a></p>
<p>13 0.49204564 <a title="52-lda-13" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>14 0.49110526 <a title="52-lda-14" href="./emnlp-2013-A_Dataset_for_Research_on_Short-Text_Conversations.html">4 emnlp-2013-A Dataset for Research on Short-Text Conversations</a></p>
<p>15 0.48996049 <a title="52-lda-15" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>16 0.48836547 <a title="52-lda-16" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>17 0.488345 <a title="52-lda-17" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>18 0.48806217 <a title="52-lda-18" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>19 0.48793697 <a title="52-lda-19" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>20 0.48788056 <a title="52-lda-20" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
