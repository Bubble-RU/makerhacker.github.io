<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>33 emnlp-2013-Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in Japanese</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-33" href="#">emnlp2013-33</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>33 emnlp-2013-Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in Japanese</h1>
<br/><p>Source: <a title="emnlp-2013-33-pdf" href="http://aclweb.org/anthology//D/D13/D13-1121.pdf">pdf</a></p><p>Author: Ryohei Sasano ; Daisuke Kawahara ; Sadao Kurohashi ; Manabu Okumura</p><p>Abstract: We present a method for automatically acquiring knowledge for case alternation between the passive and active voices in Japanese. By leveraging several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, our method aligns a case frame in the passive voice to a corresponding case frame in the active voice and finds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness.</p><p>Reference: <a title="emnlp-2013-33-reference" href="../emnlp2013_reference/emnlp-2013-Automatic_Knowledge_Acquisition_for_Case_Alternation_between_the_Passive_and_Active_Voices_in_Japanese_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ac Abstract  We present a method for automatically acquiring knowledge for case alternation between the passive and active voices in Japanese. [sent-3, score-1.371]
</p><p>2 By leveraging several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, our method aligns a case frame in the passive voice to a corresponding case frame in the active voice and finds an alignment between their cases. [sent-4, score-2.604]
</p><p>3 We then apply the acquired knowledge to a case alternation task and prove its usefulness. [sent-5, score-0.56]
</p><p>4 However, since case particles vary depending on the voices, we have to take case alternation into account to represent predicate-argument structure. [sent-10, score-0.688]
</p><p>5 Examples (1) and (2) describe the same event in the passive and active voices, respectively. [sent-22, score-0.739]
</p><p>6 2 On the other hand, when we use the normalized-cases for the base form, the case of “女 (woman)” is wo2 and the case of “男 (man)” is ga, which are the same as the surface cases in the active voice as in Example (2). [sent-24, score-0.797]
</p><p>7 In particular, we focus on the transformation of the passive voice into the active voice in this paper. [sent-38, score-1.311]
</p><p>8 For example, while the case particle ni in Example (1) is alternated with ga in the active voice, the case particle ni in Example (3) is not alternated in the active voice as in Example (4) even though both their predicates are “突 き と さ れ た (be pushed  落  down). [sent-41, score-1.971]
</p><p>9 )  In example (5), although the ni-case argument “男 (man)” is the same as in Example (1), the case  particle ni indicates recipient and is not alternated in the active voice. [sent-49, score-0.698]
</p><p>10 1214 alternation between the passive and active voices in Japanese depends on not only predicates but also arguments, and we have to consider their combinations. [sent-53, score-1.233]
</p><p>11 Since it is impractical to manually describe the case alternation rules for all combinations of predicates and arguments, we have to acquire such knowledge automatically. [sent-54, score-0.518]
</p><p>12 Thus, in this paper, we present a method for acquiring the knowledge for case alternation between the passive and active voices in Japanese. [sent-55, score-1.371]
</p><p>13 Our method leverages several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. [sent-56, score-0.992]
</p><p>14 They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. [sent-67, score-0.68]
</p><p>15 (2001) dealt with case alternation between the passive and active voices as a subtask of paraphrasing a simple sentence. [sent-69, score-1.342]
</p><p>16 They manually introduced case alternation rules on the basis of verb types and case patterns and transformed passive sentences into  active sentences. [sent-70, score-1.397]
</p><p>17 These case frames are constructed for each predicate like PropBank frames (Palmer et al. [sent-81, score-0.558]
</p><p>18 Each case frame describes surface cases that each predicate has and instances that can fill a case slot, which is fully lexicalized like the subcategorization lexicon VALEX (Korhonen et al. [sent-85, score-0.588]
</p><p>19 We list some Japanese postpositional case particles with their typical grammatical functions in Table 1 and show examples of case frames in Table 2. [sent-87, score-0.526]
</p><p>20 4 Ideally, one case frame is constructed for each meaning and voice of the target predicate. [sent-88, score-0.586]
</p><p>21 For example, 59 and eight case frames were respectively constructed for the predicate in the passive voice “突き落 と さ れる (be pushed down)” and in the active voice “突き落 と す (push down)” from 6. [sent-94, score-1.787]
</p><p>22 Table 2 shows the 4th and 5th case frames for “突き 落 と さ れる (be pushed down)” and the 2nd and 4th case frames for “突き落 と す (push down). [sent-96, score-0.711]
</p><p>23 This is because, as we will show in the next section, this phrase can be represented as ga-case in the passive voice. [sent-100, score-0.527]
</p><p>24 a”e 4  Passive-Active Transformation in Japanese  Morphologically speaking, the passive voice in Japanese is expressed by using the auxiliary verbs “れ る (reru)” and “ ら れ る (rareru),” whose past forms are “れた (reta)” and “ ら れた (rareta),” respectively. [sent-102, score-0.814]
</p><p>25 For example, the verb in the base form  と  “突 き 落 す (tsukiotosu, push down)” is transformed into the past passive form “突 き 落 さ れ た (tsukiotosa-reta, was pushed down). [sent-103, score-0.706]
</p><p>26 One is the case represented as ga in the passive voice, and the other is the case represented as ga in the active voice. [sent-106, score-1.543]
</p><p>27 Japanese passive sentences can be classified into three types in accordance with what is represented as ga-case in the passive voice: direct passive, indirect passive, and possessor passive. [sent-107, score-1.115]
</p><p>28 In direct passive sentence, the object of the predicate in the active voice is represented as ga-case. [sent-108, score-1.094]
</p><p>29 Examples (1), (3), and (5) are all direct passive sentences. [sent-109, score-0.505]
</p><p>30 The case that is represented as ga in the active voice is usually represented as ni, niyotte, kara, or de in the passive sentence. [sent-110, score-1.43]
</p><p>31 In the first sentence of Examples (6) and (7),5 ga-cases in the active voice are represented as niyotte and kara, respectively. [sent-111, score-0.615]
</p><p>32 On the other hand, ga-case in the passive sentence is alternated with wo or ni as shown with broken lines in the second sentence of Examples (6) and (7). [sent-112, score-0.853]
</p><p>33 )  5“P” denotes a passive sentence and “A” denotes the sponding active sentence in these examples. [sent-143, score-0.775]
</p><p>34 Indirect passive is also called adversative passive, in which an indirectly influenced agent is represented with ga. [sent-165, score-0.549]
</p><p>35 For example, “私 (I),” the argument represented with ga in the first sentence of Example (8), does not appear in the active voice, i. [sent-166, score-0.576]
</p><p>36 In the case of in-  direct passive, ga-case in the active sentence is always alternated with ni-case in the passive sentence as shown with solid lines in Examples (8). [sent-169, score-1.001]
</p><p>37 )  Possessor passive is similar to indirect passive in that the argument represented with ga-case does not appear as an argument of the predicate in the active voice. [sent-183, score-1.456]
</p><p>38 Therefore, possessor passive is sometimes treated as a kind of indirect passive. [sent-184, score-0.588]
</p><p>39 However, in the case of possessor passive, the argument appears in the active sentence as a possessor of the direct object. [sent-185, score-0.506]
</p><p>40 (A man hit the head  In conclusion, the number of case alternation patterns accompanying passive-active transformation in Japanese is limited. [sent-209, score-0.629]
</p><p>41 Ga-case in the passive voice can be alternated only with either wo, ni, or no, or does not appear in the active voice. [sent-210, score-1.159]
</p><p>42 Ga-case in the active voice can be represented only by ni, niyotte, kara, or de in the passive voice. [sent-211, score-1.028]
</p><p>43 1 Task Definition Our objective is to acquire knowledge for case alternation between the passive and active voices in Japanese. [sent-214, score-1.359]
</p><p>44 We leverage lexical case frames obtained from a large Web corpus by using Kawahara and Kurohashi (2002)’s method and align cases of a case  frame in the passive voice and cases of a case frame in the active voice. [sent-215, score-1.968]
</p><p>45 As described in Section 2, several case frames are constructed for each voice of each predicate. [sent-216, score-0.58]
</p><p>46 Identify a corresponding case frame in the active voice. [sent-218, score-0.528]
</p><p>47 Find an alignment between cases of case frames in the passive and active voice. [sent-220, score-1.113]
</p><p>48 If a case frame in the passive voice is input, we identify a corresponding case frame in the active voice, and find an alignment between cases by using the algorithm described in Section 5. [sent-222, score-1.68]
</p><p>49 2 Clues for Knowledge Acquisition We exploit three clues for corresponding case frame  identification and case alignment as follows: 1. [sent-226, score-0.446]
</p><p>50 For example, the instances of the ga-case of the case frame “突 き 落 さ れ る -5 (be pushed down-5)” and the wo-case of the case frame “突き 落 す-4 (push down-4),” which are considered to be aligned and represent patient, are similar. [sent-343, score-0.798]
</p><p>51 Then we define semantic similarity of a case alignment A between case frames CF1 and CF2. [sent-350, score-0.472]
</p><p>52 iN=1sims(C1,i,C2,a(i) , where N denotes the number of case slots of CF1, C1,i denotes a set of instances of the i-th case slot of CF1, and C2,a(i) denotes the set of the aligned case instances of CF2. [sent-352, score-0.513]
</p><p>53 ,  }  Case distribution similarity Although arguments are often omitted in Japanese, arguments that are usually mentioned explicitly in the passive voice will be also explicitly mentioned in the active voice. [sent-357, score-1.094]
</p><p>54 As an example, consider the alignment between a passive case6 “選ばれる -1 (be selected-1)” and the corresponding active case frame “選ぶ-13 (select13)” in Table 4. [sent-366, score-1.076]
</p><p>55 Preference of alternation patterns Some alternation patterns often appear, and others do not. [sent-374, score-0.824]
</p><p>56 47% of ga-case is alternated with wocase in passive-active transformation in Japanese, 6This case frame should not have wo-case. [sent-377, score-0.485]
</p><p>57 However, since we constructed case frames automatically, some case frames have improper cases. [sent-378, score-0.625]
</p><p>58 7  When we can use  7Since fPP (A) is defined with a set of weights of case alternation patterns, fPP (A) contains these weights implicitly, and thus there is only a single explicit weight in equation (ii). [sent-388, score-0.501]
</p><p>59 Since some combinations of cga to and cto ga never occur, our algorithm filters them out in line 5 of the algorithm. [sent-397, score-0.481]
</p><p>60 Of course, these case frames contain improper ones, that is, several frames mix several meanings or usages of the predicates. [sent-401, score-0.491]
</p><p>61 Instead, we evaluate the usefulness of the acquired knowledge on a case alternation task between the passive and active voices. [sent-403, score-1.299]
</p><p>62 As mentioned in Section 2, they extracted 3,576 case particles in passive sentences from the Kyoto University Text Corpus, and tagged their cases in the active voice. [sent-407, score-0.991]
</p><p>63 Since they treated possessor passive as a kind of indirect passive, they did not adopt the case alternation between ga and no. [sent-408, score-1.36]
</p><p>64 1219 were changed to the case alternation between ga and no. [sent-417, score-0.772]
</p><p>65 Experiments without either development or training data In the first setting, we aligned the input passive case frame to one of the active case frames of the same predicate only by using simSEM and simDIST with the parameter α = 1. [sent-429, score-1.461]
</p><p>66 If a passive sentence is input, perform syntactic and surface case structure analysis by using Kawahara and Kurohashi (2006)’s model. [sent-432, score-0.649]
</p><p>67 9 Their model identified a proper case frame for each predicate, and assigned arguments in the input sentence to case slots of the case frame. [sent-433, score-0.578]
</p><p>68 By using the acquired knowledge for case alternation, alternate input surface cases with cases in the active voice. [sent-435, score-0.543]
</p><p>69 For example, if Example (10) is input, the ga-case argument is assigned to the ga-case of the case frame “突き落 さ れる -5 (be pushed down-5). [sent-437, score-0.478]
</p><p>70 ” Since this case is aligned to the wo-case of the case frame “突き落 す-4 (push down-4)” as shown in Figure 1, this ga-case is alternated with wo-case. [sent-438, score-0.61]
</p><p>71 0) 2: acc = faccuracy (x) ,pre acc = 0 3: while acc > pre acc 4: pre acc = acc 5: for i∈ {0, . [sent-454, score-0.609]
</p><p>72 ,  −1  10: end for 11: end while Experiments with development data In the second setting, we aligned the input passive case frame to one of the active case frames of the same predicate by using simSEM , simDIST , and fPP with α tuned on the development data. [sent-474, score-1.48]
</p><p>73 Algorithm 2 shows the hill-climbing algorithm for  tuning the parameter vector x, where faccuracy(x) is a function that returns the case alternation accuracy on the development data with parameter x. [sent-480, score-0.501]
</p><p>74 After acquiring knowledge for case alternation with the tuned parameter, we applied the same method for case alternation as the first setting. [sent-483, score-1.05]
</p><p>75 883 (3,159/3,576) Table 5: Experimental results of case alternation without  training data. [sent-503, score-0.501]
</p><p>76 Specifically, we first tuned the parameter vector x on the training data and acquired the knowledge for case alternation with the tuned parameter. [sent-507, score-0.598]
</p><p>77 Table 7 shows an example of case alternation between the passive and active voices. [sent-544, score-1.24]
</p><p>78 When the passive sentence was input, the argument “松樹 さ ん . [sent-545, score-0.554]
</p><p>79 ” Since this case was aligned to no-case of the case frame “殴 -2 (hit-2),” the input ga-case was alternated with no-case. [sent-571, score-0.63]
</p><p>80 On the other hand, the cases of the other arguments “バ バ ッ ト で (bat-de)” and “頭を (head-wo)” were output as they were in the passive sentence. [sent-572, score-0.576]
</p><p>81 We now list three error causes observed in our experiments of the case alternation task: 1) The passive voice in Japanese is expressed by using the auxiliary verbs “れ (reru)” and “ ら れ (rareru). [sent-573, score-1.315]
</p><p>82 }-nidyeot  Corresponding active case frame and case alignment: Case alignment: {niyotte→ga, ga →no, wo→wo, de→de}  TCab{sle頭男拳自F7分ra(:fhmiAes(atm)nd:y3“殴51esx64laるf,2)平拳m4:3-p6手2顔(0lefi,hs私tpof)a-:23lc·m()0Ie”a,s·:誰23e15かa7l,t·e(拳sronm}(·af-ei. [sent-594, score-0.908]
</p><p>83 Since Kawahara and Kurohashi (2002)’s method does not distinguish between these meanings, our case frames sometimes contain improper cases such as wo-case in case frame “選ばれ る -1 (be selected-1)” in Table 4. [sent-598, score-0.649]
</p><p>84 2) In some passive sentences, there are two surface ni-cases as in Example (11). [sent-599, score-0.54]
</p><p>85 )  3) Agent of a predicate can be represented by using several types of case particles in the passive voice. [sent-602, score-0.78]
</p><p>86 For example, “会社 (company)” in Example (12) is the agent of “雇用 し た (employed),” which can be represented by either of ni, niyotte, and kara in the passive voice. [sent-603, score-0.62]
</p><p>87 However, since our method enforces a one-to-one alignments, only one of these cases is properly aligned to the corresponding case in the active voice. [sent-605, score-0.44]
</p><p>88 3  Application to Alternation between the Causative and Active Voices  To confirm the applicability of our framework to other types of alternation than the active-passive alternation, we applied our framework to case alternation between the causative and active voices. [sent-608, score-1.188]
</p><p>89 The  causative voice in Japanese is a grammatical voice and is expressed by using the auxiliary verbs “せる (seru)” and “さ せ る (saseru). [sent-609, score-0.637]
</p><p>90 ” We basically used the same algorithm as Algorithm 1 for acquiring the knowledge for case alternation, but used different constraints on case alternation patterns because possible case alternation patterns are different from those of active-passive alternation. [sent-610, score-1.201]
</p><p>91 Specifically, we replaced the third and fourth lines of Algorithm 1 with “for each cto ga ∈ {NIL, ni}” and “for each cga to ∈ {wo, ni},” respectively, n bi}as”ed a on linguistic analysis {ofw oac,ti nvi}e-,c”a ruessapteicveti vaeltley,rn baatisoend oinn Japanese. [sent-611, score-0.481]
</p><p>92 Their data consists of 4,671 case particles in passive or causative sentences from the Kyoto University Text Corpus with their cases in the active voice. [sent-613, score-1.03]
</p><p>93 7  Conclusions and Future Directions  We have presented a method for automatically acquiring knowledge for case alternation between the passive and active voices in Japanese. [sent-622, score-1.371]
</p><p>94 872 (457/524) Table 8: Experimental results of case alternation between the causative and active voices. [sent-628, score-0.796]
</p><p>95 aligned an input case frame in the passive voice to a corresponding case frame in the active voice and found an alignment between their cases. [sent-629, score-1.978]
</p><p>96 We then  applied the acquired knowledge to a case alternation task and proved its usefulness. [sent-630, score-0.56]
</p><p>97 The knowledge we have to manually construct is only the knowledge of linguistic constraints on case alternation patterns. [sent-631, score-0.501]
</p><p>98 Thus, although this paper focused on the activepassive alternation in Japanese, our framework is applicable to the other types of case alternation and to other languages, especially similar languages such as Korean. [sent-633, score-0.913]
</p><p>99 We plan to apply our framework to other types of case alternation such as case alternation between intransitive and transitive verbs. [sent-634, score-1.002]
</p><p>100 Machine-learning-based transformation of passive Japanese sentences into active by separating training data into each input particle. [sent-724, score-0.797]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('passive', 0.505), ('alternation', 0.392), ('ga', 0.271), ('voice', 0.267), ('active', 0.234), ('frame', 0.185), ('frames', 0.179), ('japanese', 0.172), ('alternated', 0.153), ('murata', 0.136), ('pushed', 0.135), ('ni', 0.118), ('cga', 0.112), ('case', 0.109), ('voices', 0.102), ('cto', 0.098), ('acc', 0.093), ('niyotte', 0.092), ('simdist', 0.092), ('woman', 0.081), ('kawahara', 0.081), ('fpp', 0.08), ('kurohashi', 0.08), ('particles', 0.078), ('wo', 0.077), ('kara', 0.071), ('simsem', 0.071), ('predicate', 0.066), ('causative', 0.061), ('acquired', 0.059), ('possessor', 0.057), ('aligned', 0.054), ('nil', 0.052), ('faccuracy', 0.051), ('postpositional', 0.051), ('sadao', 0.051), ('argument', 0.049), ('sima', 0.044), ('alignment', 0.043), ('cases', 0.043), ('alternations', 0.043), ('cfactive', 0.041), ('push', 0.038), ('transformation', 0.038), ('hit', 0.037), ('kyoto', 0.036), ('slot', 0.036), ('talked', 0.035), ('particle', 0.035), ('surface', 0.035), ('man', 0.033), ('daisuke', 0.032), ('similarity', 0.032), ('kondo', 0.031), ('modelaccuracy', 0.031), ('ryohei', 0.031), ('sasano', 0.031), ('mcnemar', 0.03), ('acquiring', 0.029), ('arguments', 0.028), ('verb', 0.028), ('indirect', 0.026), ('constructed', 0.025), ('improper', 0.024), ('confirmed', 0.024), ('represented', 0.022), ('agent', 0.022), ('tagged', 0.022), ('korhonen', 0.021), ('dative', 0.021), ('basically', 0.021), ('verbs', 0.021), ('instances', 0.021), ('auxiliary', 0.021), ('activepassive', 0.02), ('bunrui', 0.02), ('cfsactive', 0.02), ('diathesis', 0.02), ('goi', 0.02), ('hyo', 0.02), ('rareru', 0.02), ('reru', 0.02), ('subcategorization', 0.02), ('taira', 0.02), ('tcab', 0.02), ('theijssen', 0.02), ('input', 0.02), ('patterns', 0.02), ('tuned', 0.019), ('identified', 0.018), ('denotes', 0.018), ('cos', 0.018), ('masaki', 0.018), ('cried', 0.018), ('iida', 0.018), ('ipa', 0.018), ('joanis', 0.018), ('metal', 0.018), ('nagao', 0.018), ('acquire', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="33-tfidf-1" href="./emnlp-2013-Automatic_Knowledge_Acquisition_for_Case_Alternation_between_the_Passive_and_Active_Voices_in_Japanese.html">33 emnlp-2013-Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in Japanese</a></p>
<p>Author: Ryohei Sasano ; Daisuke Kawahara ; Sadao Kurohashi ; Manabu Okumura</p><p>Abstract: We present a method for automatically acquiring knowledge for case alternation between the passive and active voices in Japanese. By leveraging several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, our method aligns a case frame in the passive voice to a corresponding case frame in the active voice and finds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness.</p><p>2 0.057182848 <a title="33-tfidf-2" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>Author: Wiltrud Kessler ; Jonas Kuhn</p><p>Abstract: This short paper presents a pilot study investigating the training of a standard Semantic Role Labeling (SRL) system on product reviews for the new task of detecting comparisons. An (opinionated) comparison consists of a comparative “predicate” and up to three “arguments”: the entity evaluated positively, the entity evaluated negatively, and the aspect under which the comparison is made. In user-generated product reviews, the “predicate” and “arguments” are expressed in highly heterogeneous ways; but since the elements are textually annotated in existing datasets, SRL is technically applicable. We address the interesting question how well training an outof-the-box SRL model works for English data. We observe that even without any feature engineering or other major adaptions to our task, the system outperforms a reasonable heuristic baseline in all steps (predicate identification, argument identification and argument classification) and in three different datasets.</p><p>3 0.056303021 <a title="33-tfidf-3" href="./emnlp-2013-The_VerbCorner_Project%3A_Toward_an_Empirically-Based_Semantic_Decomposition_of_Verbs.html">183 emnlp-2013-The VerbCorner Project: Toward an Empirically-Based Semantic Decomposition of Verbs</a></p>
<p>Author: Joshua K. Hartshorne ; Claire Bonial ; Martha Palmer</p><p>Abstract: This research describes efforts to use crowdsourcing to improve the validity of the semantic predicates in VerbNet, a lexicon of about 6300 English verbs. The current semantic predicates can be thought of semantic primitives, into which the concepts denoted by a verb can be decomposed. For example, the verb spray (of the Spray class), involves the predicates MOTION, NOT, and LOCATION, where the event can be decomposed into an AGENT causing a THEME that was originally not in a particular location to now be in that location. Although VerbNet’s predicates are theoretically well-motivated, systematic empirical data is scarce. This paper describes a recently-launched attempt to address this issue with a series of human judgment tasks, posed to subjects in the form of games.</p><p>4 0.0542344 <a title="33-tfidf-4" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>Author: Eduardo Blanco ; Dan Moldovan</p><p>Abstract: This paper presents a novel approach to determine textual similarity. A layered methodology to transform text into logic forms is proposed, and semantic features are derived from a logic prover. Experimental results show that incorporating the semantic structure of sentences is beneficial. When training data is unavailable, scores obtained from the logic prover in an unsupervised manner outperform supervised methods.</p><p>5 0.047234602 <a title="33-tfidf-5" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>Author: Uri Lerner ; Slav Petrov</p><p>Abstract: We present a simple and novel classifier-based preordering approach. Unlike existing preordering models, we train feature-rich discriminative classifiers that directly predict the target-side word order. Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discriminative model with a rich set of features, including lexical features. We present extensive experiments on 22 language pairs, including preordering into English from 7 other languages. We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task. For languages from different families the improvements often exceed 2 BLEU. Many of these gains are also significant in human evaluations.</p><p>6 0.045513362 <a title="33-tfidf-6" href="./emnlp-2013-Joint_Learning_of_Phonetic_Units_and_Word_Pronunciations_for_ASR.html">115 emnlp-2013-Joint Learning of Phonetic Units and Word Pronunciations for ASR</a></p>
<p>7 0.042981666 <a title="33-tfidf-7" href="./emnlp-2013-Gender_Inference_of_Twitter_Users_in_Non-English_Contexts.html">89 emnlp-2013-Gender Inference of Twitter Users in Non-English Contexts</a></p>
<p>8 0.039113816 <a title="33-tfidf-8" href="./emnlp-2013-Noise-Aware_Character_Alignment_for_Bootstrapping_Statistical_Machine_Transliteration_from_Bilingual_Corpora.html">139 emnlp-2013-Noise-Aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora</a></p>
<p>9 0.037388626 <a title="33-tfidf-9" href="./emnlp-2013-Two-Stage_Method_for_Large-Scale_Acquisition_of_Contradiction_Pattern_Pairs_using_Entailment.html">189 emnlp-2013-Two-Stage Method for Large-Scale Acquisition of Contradiction Pattern Pairs using Entailment</a></p>
<p>10 0.037125513 <a title="33-tfidf-10" href="./emnlp-2013-Semi-Markov_Phrase-Based_Monolingual_Alignment.html">167 emnlp-2013-Semi-Markov Phrase-Based Monolingual Alignment</a></p>
<p>11 0.034142721 <a title="33-tfidf-11" href="./emnlp-2013-Effectiveness_and_Efficiency_of_Open_Relation_Extraction.html">68 emnlp-2013-Effectiveness and Efficiency of Open Relation Extraction</a></p>
<p>12 0.033447485 <a title="33-tfidf-12" href="./emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model.html">75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</a></p>
<p>13 0.032680325 <a title="33-tfidf-13" href="./emnlp-2013-Identifying_Phrasal_Verbs_Using_Many_Bilingual_Corpora.html">96 emnlp-2013-Identifying Phrasal Verbs Using Many Bilingual Corpora</a></p>
<p>14 0.03221697 <a title="33-tfidf-14" href="./emnlp-2013-Shift-Reduce_Word_Reordering_for_Machine_Translation.html">171 emnlp-2013-Shift-Reduce Word Reordering for Machine Translation</a></p>
<p>15 0.03210786 <a title="33-tfidf-15" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>16 0.030933864 <a title="33-tfidf-16" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>17 0.029533006 <a title="33-tfidf-17" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>18 0.028100429 <a title="33-tfidf-18" href="./emnlp-2013-Semantic_Parsing_on_Freebase_from_Question-Answer_Pairs.html">166 emnlp-2013-Semantic Parsing on Freebase from Question-Answer Pairs</a></p>
<p>19 0.02710171 <a title="33-tfidf-19" href="./emnlp-2013-Combining_Generative_and_Discriminative_Model_Scores_for_Distant_Supervision.html">49 emnlp-2013-Combining Generative and Discriminative Model Scores for Distant Supervision</a></p>
<p>20 0.027091086 <a title="33-tfidf-20" href="./emnlp-2013-Boosting_Cross-Language_Retrieval_by_Learning_Bilingual_Phrase_Associations_from_Relevance_Rankings.html">39 emnlp-2013-Boosting Cross-Language Retrieval by Learning Bilingual Phrase Associations from Relevance Rankings</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.092), (1, 0.012), (2, -0.006), (3, 0.006), (4, -0.016), (5, 0.042), (6, -0.03), (7, -0.001), (8, -0.002), (9, -0.016), (10, 0.023), (11, 0.033), (12, 0.053), (13, 0.05), (14, 0.015), (15, 0.048), (16, -0.015), (17, -0.07), (18, 0.032), (19, 0.007), (20, 0.014), (21, 0.074), (22, 0.037), (23, 0.024), (24, -0.008), (25, 0.118), (26, -0.034), (27, 0.083), (28, 0.092), (29, 0.037), (30, -0.077), (31, 0.159), (32, 0.045), (33, -0.043), (34, -0.079), (35, 0.1), (36, -0.059), (37, -0.004), (38, -0.128), (39, 0.037), (40, -0.042), (41, 0.177), (42, 0.08), (43, 0.021), (44, -0.136), (45, 0.096), (46, 0.113), (47, -0.083), (48, 0.059), (49, 0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96185189 <a title="33-lsi-1" href="./emnlp-2013-Automatic_Knowledge_Acquisition_for_Case_Alternation_between_the_Passive_and_Active_Voices_in_Japanese.html">33 emnlp-2013-Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in Japanese</a></p>
<p>Author: Ryohei Sasano ; Daisuke Kawahara ; Sadao Kurohashi ; Manabu Okumura</p><p>Abstract: We present a method for automatically acquiring knowledge for case alternation between the passive and active voices in Japanese. By leveraging several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, our method aligns a case frame in the passive voice to a corresponding case frame in the active voice and finds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness.</p><p>2 0.49188361 <a title="33-lsi-2" href="./emnlp-2013-Noise-Aware_Character_Alignment_for_Bootstrapping_Statistical_Machine_Transliteration_from_Bilingual_Corpora.html">139 emnlp-2013-Noise-Aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora</a></p>
<p>Author: Katsuhito Sudoh ; Shinsuke Mori ; Masaaki Nagata</p><p>Abstract: This paper proposes a novel noise-aware character alignment method for bootstrapping statistical machine transliteration from automatically extracted phrase pairs. The model is an extension of a Bayesian many-to-many alignment method for distinguishing nontransliteration (noise) parts in phrase pairs. It worked effectively in the experiments of bootstrapping Japanese-to-English statistical machine transliteration in patent domain using patent bilingual corpora.</p><p>3 0.48765141 <a title="33-lsi-3" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>Author: Wiltrud Kessler ; Jonas Kuhn</p><p>Abstract: This short paper presents a pilot study investigating the training of a standard Semantic Role Labeling (SRL) system on product reviews for the new task of detecting comparisons. An (opinionated) comparison consists of a comparative “predicate” and up to three “arguments”: the entity evaluated positively, the entity evaluated negatively, and the aspect under which the comparison is made. In user-generated product reviews, the “predicate” and “arguments” are expressed in highly heterogeneous ways; but since the elements are textually annotated in existing datasets, SRL is technically applicable. We address the interesting question how well training an outof-the-box SRL model works for English data. We observe that even without any feature engineering or other major adaptions to our task, the system outperforms a reasonable heuristic baseline in all steps (predicate identification, argument identification and argument classification) and in three different datasets.</p><p>4 0.44022059 <a title="33-lsi-4" href="./emnlp-2013-The_VerbCorner_Project%3A_Toward_an_Empirically-Based_Semantic_Decomposition_of_Verbs.html">183 emnlp-2013-The VerbCorner Project: Toward an Empirically-Based Semantic Decomposition of Verbs</a></p>
<p>Author: Joshua K. Hartshorne ; Claire Bonial ; Martha Palmer</p><p>Abstract: This research describes efforts to use crowdsourcing to improve the validity of the semantic predicates in VerbNet, a lexicon of about 6300 English verbs. The current semantic predicates can be thought of semantic primitives, into which the concepts denoted by a verb can be decomposed. For example, the verb spray (of the Spray class), involves the predicates MOTION, NOT, and LOCATION, where the event can be decomposed into an AGENT causing a THEME that was originally not in a particular location to now be in that location. Although VerbNet’s predicates are theoretically well-motivated, systematic empirical data is scarce. This paper describes a recently-launched attempt to address this issue with a series of human judgment tasks, posed to subjects in the form of games.</p><p>5 0.39006862 <a title="33-lsi-5" href="./emnlp-2013-Joint_Learning_of_Phonetic_Units_and_Word_Pronunciations_for_ASR.html">115 emnlp-2013-Joint Learning of Phonetic Units and Word Pronunciations for ASR</a></p>
<p>Author: Chia-ying Lee ; Yu Zhang ; James Glass</p><p>Abstract: The creation of a pronunciation lexicon remains the most inefficient process in developing an Automatic Speech Recognizer (ASR). In this paper, we propose an unsupervised alternative requiring no language-specific knowledge to the conventional manual approach for creating pronunciation dictionaries. We present a hierarchical Bayesian model, which jointly discovers the phonetic inventory and the Letter-to-Sound (L2S) mapping rules in a language using only transcribed data. When tested on a corpus of spontaneous queries, the results demonstrate the superiority of the proposed joint learning scheme over its sequential counterpart, in which the latent phonetic inventory and L2S mappings are learned separately. Furthermore, the recognizers built with the automatically induced lexicon consistently outperform grapheme-based recognizers and even approach the performance of recognition systems trained using – – conventional supervised procedures.</p><p>6 0.37367967 <a title="33-lsi-6" href="./emnlp-2013-Two-Stage_Method_for_Large-Scale_Acquisition_of_Contradiction_Pattern_Pairs_using_Entailment.html">189 emnlp-2013-Two-Stage Method for Large-Scale Acquisition of Contradiction Pattern Pairs using Entailment</a></p>
<p>7 0.34979388 <a title="33-lsi-7" href="./emnlp-2013-Joint_Parsing_and_Disfluency_Detection_in_Linear_Time.html">116 emnlp-2013-Joint Parsing and Disfluency Detection in Linear Time</a></p>
<p>8 0.34227854 <a title="33-lsi-8" href="./emnlp-2013-Improving_Alignment_of_System_Combination_by_Using_Multi-objective_Optimization.html">101 emnlp-2013-Improving Alignment of System Combination by Using Multi-objective Optimization</a></p>
<p>9 0.32322907 <a title="33-lsi-9" href="./emnlp-2013-Semi-Markov_Phrase-Based_Monolingual_Alignment.html">167 emnlp-2013-Semi-Markov Phrase-Based Monolingual Alignment</a></p>
<p>10 0.31665891 <a title="33-lsi-10" href="./emnlp-2013-Automatically_Detecting_and_Attributing_Indirect_Quotations.html">35 emnlp-2013-Automatically Detecting and Attributing Indirect Quotations</a></p>
<p>11 0.3028664 <a title="33-lsi-11" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>12 0.30151099 <a title="33-lsi-12" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>13 0.30106872 <a title="33-lsi-13" href="./emnlp-2013-Ubertagging%3A_Joint_Segmentation_and_Supertagging_for_English.html">190 emnlp-2013-Ubertagging: Joint Segmentation and Supertagging for English</a></p>
<p>14 0.28999859 <a title="33-lsi-14" href="./emnlp-2013-Effectiveness_and_Efficiency_of_Open_Relation_Extraction.html">68 emnlp-2013-Effectiveness and Efficiency of Open Relation Extraction</a></p>
<p>15 0.26941246 <a title="33-lsi-15" href="./emnlp-2013-Mining_New_Business_Opportunities%3A_Identifying_Trend_related_Products_by_Leveraging_Commercial_Intents_from_Microblogs.html">131 emnlp-2013-Mining New Business Opportunities: Identifying Trend related Products by Leveraging Commercial Intents from Microblogs</a></p>
<p>16 0.26652193 <a title="33-lsi-16" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>17 0.24580535 <a title="33-lsi-17" href="./emnlp-2013-Combining_PCFG-LA_Models_with_Dual_Decomposition%3A_A_Case_Study_with_Function_Labels_and_Binarization.html">50 emnlp-2013-Combining PCFG-LA Models with Dual Decomposition: A Case Study with Function Labels and Binarization</a></p>
<p>18 0.24051484 <a title="33-lsi-18" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>19 0.23461488 <a title="33-lsi-19" href="./emnlp-2013-This_Text_Has_the_Scent_of_Starbucks%3A_A_Laplacian_Structured_Sparsity_Model_for_Computational_Branding_Analytics.html">184 emnlp-2013-This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</a></p>
<p>20 0.23180538 <a title="33-lsi-20" href="./emnlp-2013-Gender_Inference_of_Twitter_Users_in_Non-English_Contexts.html">89 emnlp-2013-Gender Inference of Twitter Users in Non-English Contexts</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.036), (18, 0.041), (22, 0.029), (30, 0.038), (50, 0.012), (51, 0.14), (66, 0.023), (71, 0.02), (75, 0.019), (90, 0.516), (96, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80486667 <a title="33-lda-1" href="./emnlp-2013-Question_Difficulty_Estimation_in_Community_Question_Answering_Services.html">155 emnlp-2013-Question Difficulty Estimation in Community Question Answering Services</a></p>
<p>Author: Jing Liu ; Quan Wang ; Chin-Yew Lin ; Hsiao-Wuen Hon</p><p>Abstract: In this paper, we address the problem of estimating question difficulty in community question answering services. We propose a competition-based model for estimating question difficulty by leveraging pairwise comparisons between questions and users. Our experimental results show that our model significantly outperforms a PageRank-based approach. Most importantly, our analysis shows that the text of question descriptions reflects the question difficulty. This implies the possibility of predicting question difficulty from the text of question descriptions.</p><p>same-paper 2 0.77774024 <a title="33-lda-2" href="./emnlp-2013-Automatic_Knowledge_Acquisition_for_Case_Alternation_between_the_Passive_and_Active_Voices_in_Japanese.html">33 emnlp-2013-Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in Japanese</a></p>
<p>Author: Ryohei Sasano ; Daisuke Kawahara ; Sadao Kurohashi ; Manabu Okumura</p><p>Abstract: We present a method for automatically acquiring knowledge for case alternation between the passive and active voices in Japanese. By leveraging several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, our method aligns a case frame in the passive voice to a corresponding case frame in the active voice and finds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness.</p><p>3 0.72902381 <a title="33-lda-3" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>Author: Eva Maria Vecchi ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: In this study, we use compositional distributional semantic methods to investigate restrictions in adjective ordering. Specifically, we focus on properties distinguishing AdjectiveAdjective-Noun phrases in which there is flexibility in the adjective ordering from those bound to a rigid order. We explore a number of measures extracted from the distributional representation of AAN phrases which may indicate a word order restriction. We find that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modification of the adjectives. Our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics.</p><p>4 0.64500928 <a title="33-lda-4" href="./emnlp-2013-Automatic_Domain_Partitioning_for_Multi-Domain_Learning.html">29 emnlp-2013-Automatic Domain Partitioning for Multi-Domain Learning</a></p>
<p>Author: Di Wang ; Chenyan Xiong ; William Yang Wang</p><p>Abstract: Chenyan Xiong School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA cx@ c s . cmu .edu William Yang Wang School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA ww@ cmu .edu might not be generalizable to other domains (BenDavid et al., 2006; Ben-David et al., 2010). Multi-Domain learning (MDL) assumes that the domain labels in the dataset are known. However, when there are multiple metadata at- tributes available, it is not always straightforward to select a single best attribute for domain partition, and it is possible that combining more than one metadata attributes (including continuous attributes) can lead to better MDL performance. In this work, we propose an automatic domain partitioning approach that aims at providing better domain identities for MDL. We use a supervised clustering approach that learns the domain distance between data instances , and then cluster the data into better domains for MDL. Our experiment on real multi-domain datasets shows that using our automatically generated domain partition improves over popular MDL methods.</p><p>5 0.59570992 <a title="33-lda-5" href="./emnlp-2013-Unsupervised_Induction_of_Contingent_Event_Pairs_from_Film_Scenes.html">192 emnlp-2013-Unsupervised Induction of Contingent Event Pairs from Film Scenes</a></p>
<p>Author: Zhichao Hu ; Elahe Rahimtoroghi ; Larissa Munishkina ; Reid Swanson ; Marilyn A. Walker</p><p>Abstract: Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning. Researchers in NLP have tackled modeling such expectations from a range of perspectives, including treating it as the inference of the CONTINGENT discourse relation, or as a type of common-sense causal reasoning. Our approach is to model likelihood between events by drawing on several of these lines of previous work. We implement and evaluate different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another. We refine event pairs that we learn from a corpus of film scene descriptions utilizing web search counts, and evaluate our results by collecting human judgments ofcontingency. Our results indicate that the use of web search counts increases the av- , erage accuracy of our best method to 85.64% over a baseline of 50%, as compared to an average accuracy of 75. 15% without web search.</p><p>6 0.33742386 <a title="33-lda-6" href="./emnlp-2013-Gender_Inference_of_Twitter_Users_in_Non-English_Contexts.html">89 emnlp-2013-Gender Inference of Twitter Users in Non-English Contexts</a></p>
<p>7 0.33701503 <a title="33-lda-7" href="./emnlp-2013-Classifying_Message_Board_Posts_with_an_Extracted_Lexicon_of_Patient_Attributes.html">46 emnlp-2013-Classifying Message Board Posts with an Extracted Lexicon of Patient Attributes</a></p>
<p>8 0.33658108 <a title="33-lda-8" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>9 0.32919258 <a title="33-lda-9" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>10 0.32766244 <a title="33-lda-10" href="./emnlp-2013-Image_Description_using_Visual_Dependency_Representations.html">98 emnlp-2013-Image Description using Visual Dependency Representations</a></p>
<p>11 0.32725772 <a title="33-lda-11" href="./emnlp-2013-Two-Stage_Method_for_Large-Scale_Acquisition_of_Contradiction_Pattern_Pairs_using_Entailment.html">189 emnlp-2013-Two-Stage Method for Large-Scale Acquisition of Contradiction Pattern Pairs using Entailment</a></p>
<p>12 0.32439804 <a title="33-lda-12" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>13 0.32331601 <a title="33-lda-13" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>14 0.3217133 <a title="33-lda-14" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>15 0.32108682 <a title="33-lda-15" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>16 0.31973726 <a title="33-lda-16" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>17 0.31766239 <a title="33-lda-17" href="./emnlp-2013-MCTest%3A_A_Challenge_Dataset_for_the_Open-Domain_Machine_Comprehension_of_Text.html">126 emnlp-2013-MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text</a></p>
<p>18 0.31530365 <a title="33-lda-18" href="./emnlp-2013-The_VerbCorner_Project%3A_Toward_an_Empirically-Based_Semantic_Decomposition_of_Verbs.html">183 emnlp-2013-The VerbCorner Project: Toward an Empirically-Based Semantic Decomposition of Verbs</a></p>
<p>19 0.31522408 <a title="33-lda-19" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>20 0.31396061 <a title="33-lda-20" href="./emnlp-2013-Using_Crowdsourcing_to_get_Representations_based_on_Regular_Expressions.html">196 emnlp-2013-Using Crowdsourcing to get Representations based on Regular Expressions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
