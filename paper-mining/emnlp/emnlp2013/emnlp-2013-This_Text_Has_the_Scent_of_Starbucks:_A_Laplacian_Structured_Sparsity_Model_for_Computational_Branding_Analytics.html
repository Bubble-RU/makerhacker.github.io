<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>184 emnlp-2013-This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-184" href="#">emnlp2013-184</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>184 emnlp-2013-This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</h1>
<br/><p>Source: <a title="emnlp-2013-184-pdf" href="http://aclweb.org/anthology//D/D13/D13-1131.pdf">pdf</a></p><p>Author: William Yang Wang ; Edward Lin ; John Kominek</p><p>Abstract: We propose a Laplacian structured sparsity model to study computational branding analytics. To do this, we collected customer reviews from Starbucks, Dunkin’ Donuts, and other coffee shops across 38 major cities in the Midwest and Northeastern regions of USA. We study the brand related language use through these reviews, with focuses on the brand satisfaction and gender factors. In particular, we perform three tasks: automatic brand identification from raw text, joint brand-satisfaction prediction, and joint brandgender-satisfaction prediction. This work extends previous studies in text classification by incorporating the dependency and interaction among local features in the form of structured sparsity in a log-linear model. Our quantitative evaluation shows that our approach which combines the advantages of graphical modeling and sparsity modeling techniques significantly outperforms various standard and stateof-the-art text classification algorithms. In addition, qualitative analysis of our model reveals important features of the language uses associated with the specific brands.</p><p>Reference: <a title="emnlp-2013-184-reference" href="../emnlp2013_reference/emnlp-2013-This_Text_Has_the_Scent_of_Starbucks%3A_A_Laplacian_Structured_Sparsity_Model_for_Computational_Branding_Analytics_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract We propose a Laplacian structured sparsity model to study computational branding analytics. [sent-2, score-0.455]
</p><p>2 To do this, we collected customer reviews from Starbucks, Dunkin’ Donuts, and other coffee shops across 38 major cities in the Midwest and Northeastern regions of USA. [sent-3, score-0.327]
</p><p>3 We study the brand related language use through these reviews, with focuses on the brand satisfaction and gender factors. [sent-4, score-0.889]
</p><p>4 In particular, we perform three tasks: automatic brand identification from raw text, joint brand-satisfaction prediction, and joint brandgender-satisfaction prediction. [sent-5, score-0.483]
</p><p>5 This work extends previous studies in text classification by  incorporating the dependency and interaction among local features in the form of structured sparsity in a log-linear model. [sent-6, score-0.293]
</p><p>6 Our quantitative evaluation shows that our approach which combines the advantages of graphical modeling and sparsity modeling techniques significantly outperforms various standard and stateof-the-art text classification algorithms. [sent-7, score-0.216]
</p><p>7 1 Introduction In marketing science, branding is a modern marketing strategy of creating a unique image for a product in the customers’ mind. [sent-9, score-0.383]
</p><p>8 Establishing the brand in the broad social context is just as important as building a good product (Makens, 1965; Lederer and Hill, 2001 ; Kim et al. [sent-10, score-0.382]
</p><p>9 In fact, blind taste test experiments have frequently shown how branding directly leads to the success of products 1325 Edward Lin and John Kominek  Voci Technologies, Inc. [sent-12, score-0.237]
</p><p>10 Most notably is a continued study sponsored by Pepsi, known as the Pepsi Challenge1 , where Pepsi demonstrates how even though people preferred the taste of Pepsi, Coca-Cola’s branding has made it more popular. [sent-17, score-0.237]
</p><p>11 Even now, Microsoft uses similar blind taste tests2 to compare search engines, Bing and Google, showing that although participants prefer Bing’s results, Google’s brand might have strengthened over the years. [sent-18, score-0.423]
</p><p>12 These studies all suggest that brand and its associations play important roles in the customers’ perceptions and decisions. [sent-19, score-0.382]
</p><p>13 To accommodate the market change, companies frequently adjust branding strategies by analyzing how their customers receive and respond to branding messages. [sent-20, score-0.496]
</p><p>14 Recently, with the advance of machine learning techniques, researchers from the chemistry and vision communities started to pay attention to the problem of automatic brand identification from smell (Luo et al. [sent-22, score-0.451]
</p><p>15 In contrast, even though textual data that contains hidden branding information is abundantly available in many forms over the Web, automatic discovery and computational analysis on such data are not well studied in the past. [sent-25, score-0.196]
</p><p>16 Computational branding analytics (CBA) seeks to extract information, trends, and demographics about a brand on the basis of free-form text, e. [sent-26, score-0.623]
</p><p>17 oc d2s0 i1n3 N Aastusorcaila Ltiaon g fuoarg Ceo Pmrpoucetastsi on ga,l p Laignegsu 1is3t2ic5s–13 6, set of online Yelp reviews that discuss coffee shops. [sent-35, score-0.212]
</p><p>18 •  How well can the brand being discussed be iHdeonwtif wieedl by atnhe t raw treaxnt? [sent-38, score-0.382]
</p><p>19 d  •  •  How well can the joint value of brand and rating w be w predicted? [sent-39, score-0.411]
</p><p>20 This can be done, for example, to contrast the preferences of males and females with respect to evaluating the qualities of a coffee shop. [sent-44, score-0.201]
</p><p>21 In this paper, we propose a Laplacian structured sparsity model for computational branding analytics. [sent-47, score-0.455]
</p><p>22 In addition, our Laplacian augmented L1ball projection experiment shows that the advantage of Laplacian structured sparsity is robust across different parameter settings in a L1-constrained problem. [sent-50, score-0.332]
</p><p>23 Secondly, the qualitative analysis of our machine learning model shows the interesting features 1326 and language use that relate to brand and its associated pragmatics. [sent-51, score-0.382]
</p><p>24 The Laplacian structured sparsity model is introduced in Section 4. [sent-54, score-0.259]
</p><p>25 2  Related Work  Early work on statistical brand analysis in the marketing community dates back to the work of Kuehn (1962), where he first hypothesizes that brand choice could be described as a learning process. [sent-57, score-0.836]
</p><p>26 Outside the marketing community, statistical brand analysis is rarely seen. [sent-59, score-0.454]
</p><p>27 In image processing, researchers have studied the problem of brand identification from image using histogram comparison (Pelisson et al. [sent-62, score-0.511]
</p><p>28 However, to the best of our knowledge, even though textual data is vastly available, the problems of automatic brand identification from raw text and computational branding analytics, are new. [sent-64, score-0.621]
</p><p>29 Over the years, logistic regression and linear kernel SVM have shown to be very successful in various regression and classification tasks in NLP (Chahuneau et al. [sent-66, score-0.202]
</p><p>30 (201 1b) use the L1,∞ sparsity model to discover sociolinguistic patterns. [sent-71, score-0.182]
</p><p>31 (201 1) investigate the tree-structured overlapping group lasso for structured prediction problems. [sent-75, score-0.202]
</p><p>32 (2013) study the use of element-wise, group-wise, and hierarchical sparsity models for dialogue act classifcation. [sent-77, score-0.182]
</p><p>33 (2012) propose a histogram intersection based kNN method to construct a Laplacian matrix for a least-square sparse coding problem in image processing. [sent-88, score-0.223]
</p><p>34 3  Datasets  We collected Yelp reviews from 1,860 Starbucks, Dunkin’ Donuts3, and other coffee shops all over the Midwest and Northeast regions in the period of  2009. [sent-90, score-0.28]
</p><p>35 For each region, we divide the coffee shops into 60% training, 20% development, and 20% test, and there are no overlaps of coffee shops among these subsets. [sent-93, score-0.476]
</p><p>36 There are three values for the brand label: Starbucks, Dunkin’ Donuts, and all other coffee shop brands. [sent-94, score-0.578]
</p><p>37 The ma3We chose these two brands because they are reported as the leading coffee shops by WSJ (Ovide, 2011) and Forbes (DiCarlo, 2004). [sent-95, score-0.374]
</p><p>38 jority class is “all other coffee shop brands”, and the majority baseline is shown in Table 2. [sent-106, score-0.196]
</p><p>39 1 Problem Formulation and Predictive Tasks The automatic brand identification problem could be considered as a traditional multiclass classifica6http://www. [sent-117, score-0.455]
</p><p>40 In this paper, we investigate three multiclass classification tasks: first, we perform a 3-way classification task for automatic brand identification. [sent-132, score-0.48]
</p><p>41 In the task of brand-satisfaction prediction, we model the brand and the satisfaction label at the same time (Chahuneau et al. [sent-133, score-0.451]
</p><p>42 However, since the L1 penalty can introduce discontinuities to the original convex function, we can also consider an alternative non-sparse ridge estimator (Le Cessie and Van Houwelingen, 1992) with log loss and L2 norm, and has the convex property: min  ? [sent-158, score-0.172]
</p><p>43 sparsity and smoothness would be the elastic net model (Zou and Hastie, 2005) that uses the composite penalty: min  ? [sent-162, score-0.368]
</p><p>44 In our new objective function, it is desirable to introduce a new component that structurally penalize these cases where features that are very similar to each other, but have learned completely different weights, probably due to the noise or the data sparsity issue in the training data. [sent-169, score-0.182]
</p><p>45 In the spectral graph theory, this affinity matrix can be viewed as a weighted undirected graph G = (V, E), where each node Vp denotes a feature p, and each edge E(p,q) indicates the closeness among the features p and q. [sent-171, score-0.23]
</p><p>46 (12) (13)  where α is the regularization parameter for the Laplacian structured sparsity term. [sent-177, score-0.305]
</p><p>47 Or, another view of this new model could be seen as a Laplacian augmented elastic net model where structured sparsity and feature interaction are considered. [sent-179, score-0.45]
</p><p>48 We propose the following three steps to learn the Laplacian matrix: 1329  Figure 1: An example of the graph G, the corresponding affinity matrix A, and the corresponding Laplacian matrix L. [sent-181, score-0.247]
</p><p>49 As discussed earlier, we sum up the symmetric affinity matrix by row, and obtain a diagonal degree matrix D, and we further define a Laplacian matrix L = D A. [sent-194, score-0.304]
</p><p>50 An intuitive example ofthe graph G, its associated affinity matrix A, and Laplacian matrix L, is shown in Figure 1. [sent-197, score-0.247]
</p><p>51 Parameter Estimation: Regarding the optimization of objective function in (12-13), a notable problem is that the sparsity inducing L1 term is nondifferentiable, whereas this is not the case for the L2 norm and the Laplacian structured sparsity term. [sent-198, score-0.474]
</p><p>52 XW~p Xp  To test the robustness of Laplacian structured sparsity term in the setup of a L1-constrained problem, we can incorporate the Laplacian penalty term into the above formula, and derive:  min? [sent-207, score-0.31]
</p><p>53 5  Experiments  We first compare our model to various baselines in the 3-way automatic brand identification task. [sent-220, score-0.425]
</p><p>54 Besides the logistic regression, lasso, ridge and elastic net model that we introduced in Section 4. [sent-221, score-0.313]
</p><p>55 To test the robustness of our model, we vary the levels of sparsity of our Laplacian augmented method in a L1-constrained problem. [sent-226, score-0.217]
</p><p>56 We tune the regularization parameters of log-linear models and  Method Majority class Logistic regression Linear SVM PCA Lasso Ridge Elastic net Laplacian structured sparsity  Dev. [sent-229, score-0.408]
</p><p>57 44*  Table 2: The automatic brand identification (3-way) performances. [sent-246, score-0.425]
</p><p>58 blogs, tweets, news, or forum posts), the first task for CBA is to identify which brand this text is related to. [sent-257, score-0.382]
</p><p>59 Our customer review data set is useful for this task, because the ground truth of the brand label is attached to each review. [sent-258, score-0.456]
</p><p>60 Table 2 shows the result of our model in this automatic brand identification task. [sent-259, score-0.425]
</p><p>61 In this 3-way classification task, the overall results indicate that it is relatively easy to identify the related brand from customer reviews. [sent-260, score-0.463]
</p><p>62 When evaluating our Laplacian structured sparsity model, our proposed model obtains the best performances of 93. [sent-261, score-0.259]
</p><p>63 2 Joint Brand-Satisfaction Prediction In our training data set, we observe a subtle correlation between the brand and satisfaction labels (r = 0. [sent-266, score-0.451]
</p><p>64 56% accuracy on the  1331  Method Majority class Logistic regression Linear SVM PCA Lasso Ridge Elastic net Laplacian structured sparsity  Dev. [sent-271, score-0.362]
</p><p>65 44  Lasso Ridge Elastic net Laplacian structured sparsity  40. [sent-301, score-0.311]
</p><p>66 Table 4 shows that our proposed Laplacian structured sparsity model obtains a test accuracy of 40. [sent-319, score-0.259]
</p><p>67 Figure 2: Automatic brand identification test performance varying the level of sparsity τ in a L1 constrained problem. [sent-322, score-0.607]
</p><p>68 Figure 3: Joint brand-satisfaction prediction test performance varying the level of sparsity τ in a L1 constrained problem. [sent-323, score-0.228]
</p><p>69 4  Varying the Level of Sparsity in a L1-Constrained Problem To test the robustness of the Laplacian structured sparsity component, we exponentially increase the sum of weights τ to vary the level of sparsity in a L1-constrained setup. [sent-325, score-0.441]
</p><p>70 Figures 2 and 3 show that the Laplacian augmented L1-ball projection statistically outperform the L1-ball projection baseline in all levels of sparsity (p < . [sent-327, score-0.293]
</p><p>71 5 Exploratory Data Analysis We outline the top 15 keywords from the Laplacian structured sparsity model that are associated with the Starbucks and Dunkin’ Donuts brands in the automatic brand identification task in the Table 5. [sent-332, score-0.852]
</p><p>72 1332  Figure 4: Joint brand-gender-satisfaction prediction test performance varying the level of sparsity τ in a L1 constrained problem. [sent-334, score-0.228]
</p><p>73 Also, the results imply that Starbucks’ unique cup size branding strategy, “venti”, “grande”, “tall”, has resonated with their customers as the words prominently show up as top features in reviews. [sent-335, score-0.3]
</p><p>74 To understand the preferences of different gender subgroups towards the two brands, we contrast in Table 6 and Table 7 the top features that identify the satisfied female and male customers in the joint brand-gender-satisfaction prediction task. [sent-341, score-0.316]
</p><p>75 Not surprisingly, the cue words that the male customers identify the Starbucks brand do not always agree with those of the females. [sent-345, score-0.518]
</p><p>76 For example, instead of “fireplace”,  they prefer staying at the “patio”, and drink the coffee from the “clover” brewing system. [sent-346, score-0.199]
</p><p>77 Interestingly,  Starbucks  weight  Dunkin’  weight  starbucks sbux venti corporate store particular tall restroom tourists public lines drink bathroom spacious location grande  1. [sent-347, score-0.543]
</p><p>78 4563  dd dunkin donuts dunks dds donut dunkins glazed robbins baskin sugar d ice stale game tv  2. [sent-363, score-0.683]
</p><p>79 The word “name” is a prominent indicator for the female customers of Starbucks: at first we were puzzled, but after we digged into the database, we found reviews such as: • “. [sent-383, score-0.195]
</p><p>80 This place was probably one of the better starbucks ive been to. [sent-397, score-0.326]
</p><p>81 And that  as a keyword spotter one can use it to extract surrounding context and feed that through to the next 1333  Starbucks  weight  Dunkin’  weight  starbucks chain winter fireplace studying particular super name know because  0. [sent-407, score-0.365]
</p><p>82 2263  dd dds dunkin donuts donut dunks morning quick how munchkins  0. [sent-417, score-0.683]
</p><p>83 2758  Table 6: Top features that jointly identify the satisfied female customers and the Starbucks and Dunkin’ Donuts brands from the best model. [sent-427, score-0.289]
</p><p>84 Starbucks  weight  Dunkin’  weight  starbucks  0. [sent-428, score-0.326]
</p><p>85 2153  dunkin dds donuts donut dunks morning rush fast moving glazed  0. [sent-440, score-0.652]
</p><p>86 2326  Table 7: Top features that jointly identify the satisfied male customers and the Starbucks and Dunkin’ Donuts brands from the best model. [sent-450, score-0.272]
</p><p>87 Regarding the alternative problem setups, our preliminary experiments showed that instead of using one-vs-all binary classifiers, a direct 9-way multiclass classification of joint brand-satisfaction labels using logistic regression only resulted an accuracy of 62%. [sent-458, score-0.21]
</p><p>88 We also observed that the accuracy of the automatic brand identification task was high, indicating the promising future of CBA for hidden brand information from other genres of text over the Web. [sent-468, score-0.807]
</p><p>89 However, since the focus of this paper is a proof of concept for Laplacian structured sparsity models and computational branding analytics, we have not yet explored various multiview representations to augment our model. [sent-471, score-0.455]
</p><p>90 Why does Laplacian structured sparsity model work better in these classication tasks? [sent-472, score-0.259]
</p><p>91 Thirdly, by embedding the structure in the regularization term, our model is more flexible: one can now control the structured penalty by tuning the regularization parameter on the devel-  opment set. [sent-476, score-0.22]
</p><p>92 7  Conclusions  We introduce a Laplacian structured sparsity model for computational branding analytics (CBA). [sent-477, score-0.5]
</p><p>93 In the automatic brand identification, our model achieves the best result, dominating many competitive baselines. [sent-478, score-0.382]
</p><p>94 We also introduce the tasks of joint brandsatisfaction and brand-gender-satisfaction predictions, and show that the Laplacian structured sparsity do well in these tasks. [sent-479, score-0.288]
</p><p>95 A closer evaluation that varying the levels of sparsity in a L1 constrained problem also indicates that the Laplacian augmented L1-ball projection model can provide state-of-theart results. [sent-480, score-0.255]
</p><p>96 By examining the weights of the derived Laplacian structured sparsity model, interesting indicators of brands and theirs gender-specific customer satisfaction associations are also discovered. [sent-481, score-0.511]
</p><p>97 Laplacian sparse coding, hypergraph laplacian sparse coding, and applications. [sent-613, score-0.608]
</p><p>98 A logit model of brand choice calibrated on scanner data. [sent-622, score-0.382]
</p><p>99 Application of ann with extracted parameters from an electronic nose in cigarette brand identification. [sent-671, score-0.382]
</p><p>100 Effect of brand preference upon consumers perceived taste of turkey meat. [sent-677, score-0.423]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('laplacian', 0.492), ('brand', 0.382), ('starbucks', 0.326), ('dunkin', 0.3), ('branding', 0.196), ('sparsity', 0.182), ('coffee', 0.17), ('donuts', 0.17), ('brands', 0.136), ('elastic', 0.104), ('customers', 0.104), ('ridge', 0.091), ('matrix', 0.082), ('lasso', 0.079), ('cba', 0.078), ('structured', 0.077), ('marketing', 0.072), ('satisfaction', 0.069), ('shops', 0.068), ('lw', 0.068), ('yj', 0.067), ('logistic', 0.066), ('affinity', 0.058), ('sparse', 0.058), ('gender', 0.056), ('dds', 0.052), ('donut', 0.052), ('pepsi', 0.052), ('yyjj', 0.052), ('net', 0.052), ('regression', 0.051), ('penalty', 0.051), ('female', 0.049), ('customer', 0.047), ('regularization', 0.046), ('prediction', 0.046), ('analytics', 0.045), ('midwest', 0.045), ('moon', 0.045), ('schmidt', 0.045), ('identification', 0.043), ('image', 0.043), ('reviews', 0.042), ('pca', 0.041), ('taste', 0.041), ('coding', 0.04), ('spectral', 0.04), ('eisenstein', 0.039), ('dunks', 0.039), ('fireplace', 0.039), ('glazed', 0.039), ('munchkins', 0.039), ('pelisson', 0.039), ('quelch', 0.039), ('sbux', 0.039), ('knn', 0.039), ('projection', 0.038), ('store', 0.037), ('augmented', 0.035), ('gao', 0.035), ('belkin', 0.034), ('grande', 0.034), ('classification', 0.034), ('norm', 0.033), ('keywords', 0.032), ('male', 0.032), ('males', 0.031), ('yelp', 0.031), ('chahuneau', 0.031), ('dd', 0.031), ('min', 0.03), ('sigdial', 0.03), ('multiclass', 0.03), ('joint', 0.029), ('drink', 0.029), ('classifiers', 0.028), ('derivative', 0.028), ('review', 0.027), ('beyer', 0.026), ('biadsy', 0.026), ('census', 0.026), ('cessie', 0.026), ('clover', 0.026), ('guadagni', 0.026), ('laplacians', 0.026), ('lederer', 0.026), ('niyogi', 0.026), ('northeast', 0.026), ('oconnell', 0.026), ('patio', 0.026), ('restroom', 0.026), ('rifkin', 0.026), ('shop', 0.026), ('smell', 0.026), ('spacious', 0.026), ('venti', 0.026), ('svm', 0.026), ('luo', 0.026), ('graph', 0.025), ('derive', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="184-tfidf-1" href="./emnlp-2013-This_Text_Has_the_Scent_of_Starbucks%3A_A_Laplacian_Structured_Sparsity_Model_for_Computational_Branding_Analytics.html">184 emnlp-2013-This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</a></p>
<p>Author: William Yang Wang ; Edward Lin ; John Kominek</p><p>Abstract: We propose a Laplacian structured sparsity model to study computational branding analytics. To do this, we collected customer reviews from Starbucks, Dunkin’ Donuts, and other coffee shops across 38 major cities in the Midwest and Northeastern regions of USA. We study the brand related language use through these reviews, with focuses on the brand satisfaction and gender factors. In particular, we perform three tasks: automatic brand identification from raw text, joint brand-satisfaction prediction, and joint brandgender-satisfaction prediction. This work extends previous studies in text classification by incorporating the dependency and interaction among local features in the form of structured sparsity in a log-linear model. Our quantitative evaluation shows that our approach which combines the advantages of graphical modeling and sparsity modeling techniques significantly outperforms various standard and stateof-the-art text classification algorithms. In addition, qualitative analysis of our model reveals important features of the language uses associated with the specific brands.</p><p>2 0.090018921 <a title="184-tfidf-2" href="./emnlp-2013-Centering_Similarity_Measures_to_Reduce_Hubs.html">44 emnlp-2013-Centering Similarity Measures to Reduce Hubs</a></p>
<p>Author: Ikumi Suzuki ; Kazuo Hara ; Masashi Shimbo ; Marco Saerens ; Kenji Fukumizu</p><p>Abstract: The performance of nearest neighbor methods is degraded by the presence of hubs, i.e., objects in the dataset that are similar to many other objects. In this paper, we show that the classical method of centering, the transformation that shifts the origin of the space to the data centroid, provides an effective way to reduce hubs. We show analytically why hubs emerge and why they are suppressed by centering, under a simple probabilistic model of data. To further reduce hubs, we also move the origin more aggressively towards hubs, through weighted centering. Our experimental results show that (weighted) centering is effective for natural language data; it improves the performance of the k-nearest neighbor classi- fiers considerably in word sense disambiguation and document classification tasks.</p><p>3 0.066363551 <a title="184-tfidf-3" href="./emnlp-2013-Gender_Inference_of_Twitter_Users_in_Non-English_Contexts.html">89 emnlp-2013-Gender Inference of Twitter Users in Non-English Contexts</a></p>
<p>Author: Morgane Ciot ; Morgan Sonderegger ; Derek Ruths</p><p>Abstract: While much work has considered the problem of latent attribute inference for users of social media such as Twitter, little has been done on non-English-based content and users. Here, we conduct the first assessment of latent attribute inference in languages beyond English, focusing on gender inference. We find that the gender inference problem in quite diverse languages can be addressed using existing machinery. Further, accuracy gains can be made by taking language-specific features into account. We identify languages with complex orthography, such as Japanese, as difficult for existing methods, suggesting a valuable direction for future research.</p><p>4 0.050731909 <a title="184-tfidf-4" href="./emnlp-2013-Image_Description_using_Visual_Dependency_Representations.html">98 emnlp-2013-Image Description using Visual Dependency Representations</a></p>
<p>Author: Desmond Elliott ; Frank Keller</p><p>Abstract: Describing the main event of an image involves identifying the objects depicted and predicting the relationships between them. Previous approaches have represented images as unstructured bags of regions, which makes it difficult to accurately predict meaningful relationships between regions. In this paper, we introduce visual dependency representations to capture the relationships between the objects in an image, and hypothesize that this representation can improve image description. We test this hypothesis using a new data set of region-annotated images, associated with visual dependency representations and gold-standard descriptions. We describe two template-based description generation models that operate over visual dependency representations. In an image descrip- tion task, we find that these models outperform approaches that rely on object proximity or corpus information to generate descriptions on both automatic measures and on human judgements.</p><p>5 0.048142347 <a title="184-tfidf-5" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>Author: Svitlana Volkova ; Theresa Wilson ; David Yarowsky</p><p>Abstract: Theresa Wilson Human Language Technology Center of Excellence Johns Hopkins University Baltimore, MD t aw@ j hu .edu differences may Different demographics, e.g., gender or age, can demonstrate substantial variation in their language use, particularly in informal contexts such as social media. In this paper we focus on learning gender differences in the use of subjective language in English, Spanish, and Russian Twitter data, and explore cross-cultural differences in emoticon and hashtag use for male and female users. We show that gender differences in subjective language can effectively be used to improve sentiment analysis, and in particular, polarity classification for Spanish and Russian. Our results show statistically significant relative F-measure improvement over the gender-independent baseline 1.5% and 1% for Russian, 2% and 0.5% for Spanish, and 2.5% and 5% for English for polarity and subjectivity classification.</p><p>6 0.047882352 <a title="184-tfidf-6" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>7 0.045413151 <a title="184-tfidf-7" href="./emnlp-2013-Where_Not_to_Eat%3F_Improving_Public_Policy_by_Predicting_Hygiene_Inspections_Using_Online_Reviews.html">202 emnlp-2013-Where Not to Eat? Improving Public Policy by Predicting Hygiene Inspections Using Online Reviews</a></p>
<p>8 0.043821391 <a title="184-tfidf-8" href="./emnlp-2013-Sentiment_Analysis%3A_How_to_Derive_Prior_Polarities_from_SentiWordNet.html">170 emnlp-2013-Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet</a></p>
<p>9 0.042656578 <a title="184-tfidf-9" href="./emnlp-2013-Identifying_Manipulated_Offerings_on_Review_Portals.html">94 emnlp-2013-Identifying Manipulated Offerings on Review Portals</a></p>
<p>10 0.041865852 <a title="184-tfidf-10" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>11 0.041509129 <a title="184-tfidf-11" href="./emnlp-2013-A_Generative_Joint%2C_Additive%2C_Sequential_Model_of_Topics_and_Speech_Acts_in_Patient-Doctor_Communication.html">6 emnlp-2013-A Generative Joint, Additive, Sequential Model of Topics and Speech Acts in Patient-Doctor Communication</a></p>
<p>12 0.040752273 <a title="184-tfidf-12" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>13 0.040504254 <a title="184-tfidf-13" href="./emnlp-2013-Feature_Noising_for_Log-Linear_Structured_Prediction.html">86 emnlp-2013-Feature Noising for Log-Linear Structured Prediction</a></p>
<p>14 0.039246298 <a title="184-tfidf-14" href="./emnlp-2013-Improvements_to_the_Bayesian_Topic_N-Gram_Models.html">100 emnlp-2013-Improvements to the Bayesian Topic N-Gram Models</a></p>
<p>15 0.038842838 <a title="184-tfidf-15" href="./emnlp-2013-Structured_Penalties_for_Log-Linear_Language_Models.html">176 emnlp-2013-Structured Penalties for Log-Linear Language Models</a></p>
<p>16 0.03821535 <a title="184-tfidf-16" href="./emnlp-2013-Regularized_Minimum_Error_Rate_Training.html">159 emnlp-2013-Regularized Minimum Error Rate Training</a></p>
<p>17 0.037997048 <a title="184-tfidf-17" href="./emnlp-2013-A_Unified_Model_for_Topics%2C_Events_and_Users_on_Twitter.html">16 emnlp-2013-A Unified Model for Topics, Events and Users on Twitter</a></p>
<p>18 0.037950482 <a title="184-tfidf-18" href="./emnlp-2013-Rule-Based_Information_Extraction_is_Dead%21_Long_Live_Rule-Based_Information_Extraction_Systems%21.html">161 emnlp-2013-Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!</a></p>
<p>19 0.035928346 <a title="184-tfidf-19" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>20 0.035599805 <a title="184-tfidf-20" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.134), (1, 0.011), (2, -0.051), (3, -0.016), (4, 0.004), (5, 0.014), (6, 0.034), (7, -0.008), (8, -0.023), (9, -0.023), (10, -0.102), (11, -0.017), (12, -0.045), (13, 0.005), (14, 0.025), (15, -0.031), (16, 0.004), (17, -0.002), (18, -0.008), (19, -0.008), (20, 0.022), (21, 0.024), (22, 0.054), (23, 0.082), (24, -0.102), (25, 0.093), (26, -0.106), (27, -0.055), (28, -0.068), (29, 0.012), (30, -0.02), (31, 0.136), (32, -0.005), (33, -0.076), (34, -0.001), (35, 0.025), (36, 0.042), (37, 0.142), (38, -0.022), (39, -0.016), (40, 0.049), (41, 0.099), (42, -0.052), (43, -0.151), (44, 0.003), (45, -0.034), (46, -0.047), (47, -0.007), (48, -0.038), (49, 0.091)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89712131 <a title="184-lsi-1" href="./emnlp-2013-This_Text_Has_the_Scent_of_Starbucks%3A_A_Laplacian_Structured_Sparsity_Model_for_Computational_Branding_Analytics.html">184 emnlp-2013-This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</a></p>
<p>Author: William Yang Wang ; Edward Lin ; John Kominek</p><p>Abstract: We propose a Laplacian structured sparsity model to study computational branding analytics. To do this, we collected customer reviews from Starbucks, Dunkin’ Donuts, and other coffee shops across 38 major cities in the Midwest and Northeastern regions of USA. We study the brand related language use through these reviews, with focuses on the brand satisfaction and gender factors. In particular, we perform three tasks: automatic brand identification from raw text, joint brand-satisfaction prediction, and joint brandgender-satisfaction prediction. This work extends previous studies in text classification by incorporating the dependency and interaction among local features in the form of structured sparsity in a log-linear model. Our quantitative evaluation shows that our approach which combines the advantages of graphical modeling and sparsity modeling techniques significantly outperforms various standard and stateof-the-art text classification algorithms. In addition, qualitative analysis of our model reveals important features of the language uses associated with the specific brands.</p><p>2 0.59444582 <a title="184-lsi-2" href="./emnlp-2013-Centering_Similarity_Measures_to_Reduce_Hubs.html">44 emnlp-2013-Centering Similarity Measures to Reduce Hubs</a></p>
<p>Author: Ikumi Suzuki ; Kazuo Hara ; Masashi Shimbo ; Marco Saerens ; Kenji Fukumizu</p><p>Abstract: The performance of nearest neighbor methods is degraded by the presence of hubs, i.e., objects in the dataset that are similar to many other objects. In this paper, we show that the classical method of centering, the transformation that shifts the origin of the space to the data centroid, provides an effective way to reduce hubs. We show analytically why hubs emerge and why they are suppressed by centering, under a simple probabilistic model of data. To further reduce hubs, we also move the origin more aggressively towards hubs, through weighted centering. Our experimental results show that (weighted) centering is effective for natural language data; it improves the performance of the k-nearest neighbor classi- fiers considerably in word sense disambiguation and document classification tasks.</p><p>3 0.56632876 <a title="184-lsi-3" href="./emnlp-2013-Mining_New_Business_Opportunities%3A_Identifying_Trend_related_Products_by_Leveraging_Commercial_Intents_from_Microblogs.html">131 emnlp-2013-Mining New Business Opportunities: Identifying Trend related Products by Leveraging Commercial Intents from Microblogs</a></p>
<p>Author: Jinpeng Wang ; Wayne Xin Zhao ; Haitian Wei ; Hongfei Yan ; Xiaoming Li</p><p>Abstract: Hot trends are likely to bring new business opportunities. For example, “Air Pollution” might lead to a significant increase of the sales of related products, e.g., mouth mask. For ecommerce companies, it is very important to make rapid and correct response to these hot trends in order to improve product sales. In this paper, we take the initiative to study the task of how to identify trend related products. The major novelty of our work is that we automatically learn commercial intents revealed from microblogs. We carefully construct a data collection for this task and present quite a few insightful findings. In order to solve this problem, we further propose a graph based method, which jointly models relevance and associativity. We perform extensive experiments and the results showed that our methods are very effective.</p><p>4 0.47806442 <a title="184-lsi-4" href="./emnlp-2013-Feature_Noising_for_Log-Linear_Structured_Prediction.html">86 emnlp-2013-Feature Noising for Log-Linear Structured Prediction</a></p>
<p>Author: Sida Wang ; Mengqiu Wang ; Stefan Wager ; Percy Liang ; Christopher D. Manning</p><p>Abstract: NLP models have many and sparse features, and regularization is key for balancing model overfitting versus underfitting. A recently repopularized form of regularization is to generate fake training data by repeatedly adding noise to real data. We reinterpret this noising as an explicit regularizer, and approximate it with a second-order formula that can be used during training without actually generating fake data. We show how to apply this method to structured prediction using multinomial logistic regression and linear-chain CRFs. We tackle the key challenge of developing a dynamic program to compute the gradient of the regularizer efficiently. The regularizer is a sum over inputs, so we can estimate it more accurately via a semi-supervised or transductive extension. Applied to text classification and NER, our method provides a > 1% absolute performance gain over use of standard L2 regularization.</p><p>5 0.44441256 <a title="184-lsi-5" href="./emnlp-2013-Where_Not_to_Eat%3F_Improving_Public_Policy_by_Predicting_Hygiene_Inspections_Using_Online_Reviews.html">202 emnlp-2013-Where Not to Eat? Improving Public Policy by Predicting Hygiene Inspections Using Online Reviews</a></p>
<p>Author: Jun Seok Kang ; Polina Kuznetsova ; Michael Luca ; Yejin Choi</p><p>Abstract: This paper offers an approach for governments to harness the information contained in social media in order to make public inspections and disclosure more efficient. As a case study, we turn to restaurant hygiene inspections which are done for restaurants throughout the United States and in most of the world and are a frequently cited example of public inspections and disclosure. We present the first empirical study that shows the viability of statistical models that learn the mapping between textual signals in restaurant reviews and the hygiene inspection records from the Department of Public Health. The learned model achieves over 82% accuracy in discriminating severe – offenders from places with no violation, and provides insights into salient cues in reviews that are indicative of the restaurant’s sanitary conditions. Our study suggests that public disclosure policy can be improved by mining public opinions from social media to target inspections and to provide alternative forms of disclosure to customers.</p><p>6 0.43686947 <a title="184-lsi-6" href="./emnlp-2013-Identifying_Manipulated_Offerings_on_Review_Portals.html">94 emnlp-2013-Identifying Manipulated Offerings on Review Portals</a></p>
<p>7 0.38192657 <a title="184-lsi-7" href="./emnlp-2013-A_temporal_model_of_text_periodicities_using_Gaussian_Processes.html">18 emnlp-2013-A temporal model of text periodicities using Gaussian Processes</a></p>
<p>8 0.37899408 <a title="184-lsi-8" href="./emnlp-2013-Well-Argued_Recommendation%3A_Adaptive_Models_Based_on_Words_in_Recommender_Systems.html">200 emnlp-2013-Well-Argued Recommendation: Adaptive Models Based on Words in Recommender Systems</a></p>
<p>9 0.37440196 <a title="184-lsi-9" href="./emnlp-2013-Regularized_Minimum_Error_Rate_Training.html">159 emnlp-2013-Regularized Minimum Error Rate Training</a></p>
<p>10 0.36440173 <a title="184-lsi-10" href="./emnlp-2013-Gender_Inference_of_Twitter_Users_in_Non-English_Contexts.html">89 emnlp-2013-Gender Inference of Twitter Users in Non-English Contexts</a></p>
<p>11 0.35675043 <a title="184-lsi-11" href="./emnlp-2013-Open-Domain_Fine-Grained_Class_Extraction_from_Web_Search_Queries.html">142 emnlp-2013-Open-Domain Fine-Grained Class Extraction from Web Search Queries</a></p>
<p>12 0.35386074 <a title="184-lsi-12" href="./emnlp-2013-Rule-Based_Information_Extraction_is_Dead%21_Long_Live_Rule-Based_Information_Extraction_Systems%21.html">161 emnlp-2013-Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!</a></p>
<p>13 0.34814262 <a title="184-lsi-13" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>14 0.34339041 <a title="184-lsi-14" href="./emnlp-2013-Structured_Penalties_for_Log-Linear_Language_Models.html">176 emnlp-2013-Structured Penalties for Log-Linear Language Models</a></p>
<p>15 0.33827135 <a title="184-lsi-15" href="./emnlp-2013-A_Log-Linear_Model_for_Unsupervised_Text_Normalization.html">9 emnlp-2013-A Log-Linear Model for Unsupervised Text Normalization</a></p>
<p>16 0.33811584 <a title="184-lsi-16" href="./emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations.html">17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</a></p>
<p>17 0.31405914 <a title="184-lsi-17" href="./emnlp-2013-Using_Topic_Modeling_to_Improve_Prediction_of_Neuroticism_and_Depression_in_College_Students.html">199 emnlp-2013-Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students</a></p>
<p>18 0.31088534 <a title="184-lsi-18" href="./emnlp-2013-Automated_Essay_Scoring_by_Maximizing_Human-Machine_Agreement.html">28 emnlp-2013-Automated Essay Scoring by Maximizing Human-Machine Agreement</a></p>
<p>19 0.30785325 <a title="184-lsi-19" href="./emnlp-2013-Unsupervised_Spectral_Learning_of_WCFG_as_Low-rank_Matrix_Completion.html">195 emnlp-2013-Unsupervised Spectral Learning of WCFG as Low-rank Matrix Completion</a></p>
<p>20 0.30604818 <a title="184-lsi-20" href="./emnlp-2013-Improving_Learning_and_Inference_in_a_Large_Knowledge-Base_using_Latent_Syntactic_Cues.html">102 emnlp-2013-Improving Learning and Inference in a Large Knowledge-Base using Latent Syntactic Cues</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.036), (9, 0.02), (10, 0.011), (16, 0.344), (18, 0.035), (22, 0.035), (30, 0.07), (50, 0.03), (51, 0.127), (66, 0.03), (71, 0.039), (75, 0.039), (77, 0.021), (90, 0.014), (96, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70184827 <a title="184-lda-1" href="./emnlp-2013-This_Text_Has_the_Scent_of_Starbucks%3A_A_Laplacian_Structured_Sparsity_Model_for_Computational_Branding_Analytics.html">184 emnlp-2013-This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</a></p>
<p>Author: William Yang Wang ; Edward Lin ; John Kominek</p><p>Abstract: We propose a Laplacian structured sparsity model to study computational branding analytics. To do this, we collected customer reviews from Starbucks, Dunkin’ Donuts, and other coffee shops across 38 major cities in the Midwest and Northeastern regions of USA. We study the brand related language use through these reviews, with focuses on the brand satisfaction and gender factors. In particular, we perform three tasks: automatic brand identification from raw text, joint brand-satisfaction prediction, and joint brandgender-satisfaction prediction. This work extends previous studies in text classification by incorporating the dependency and interaction among local features in the form of structured sparsity in a log-linear model. Our quantitative evaluation shows that our approach which combines the advantages of graphical modeling and sparsity modeling techniques significantly outperforms various standard and stateof-the-art text classification algorithms. In addition, qualitative analysis of our model reveals important features of the language uses associated with the specific brands.</p><p>2 0.66685855 <a title="184-lda-2" href="./emnlp-2013-Using_Soft_Constraints_in_Joint_Inference_for_Clinical_Concept_Recognition.html">198 emnlp-2013-Using Soft Constraints in Joint Inference for Clinical Concept Recognition</a></p>
<p>Author: Prateek Jindal ; Dan Roth</p><p>Abstract: This paper introduces IQPs (Integer Quadratic Programs) as a way to model joint inference for the task of concept recognition in clinical domain. IQPs make it possible to easily incorporate soft constraints in the optimization framework and still support exact global inference. We show that soft constraints give statistically significant performance improvements when compared to hard constraints.</p><p>3 0.43603289 <a title="184-lda-3" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>Author: Xiaoqing Zheng ; Hanyang Chen ; Tianyu Xu</p><p>Abstract: This study explores the feasibility of performing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representation of Chinese characters, and use these improved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-theart performance with minimal computational cost. We also describe a perceptron-style algorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented.</p><p>4 0.43301469 <a title="184-lda-4" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>Author: Zhongqing Wang ; Shoushan LI ; Fang Kong ; Guodong Zhou</p><p>Abstract: Personal profile information on social media like LinkedIn.com and Facebook.com is at the core of many interesting applications, such as talent recommendation and contextual advertising. However, personal profiles usually lack organization confronted with the large amount of available information. Therefore, it is always a challenge for people to find desired information from them. In this paper, we address the task of personal profile summarization by leveraging both personal profile textual information and social networks. Here, using social networks is motivated by the intuition that, people with similar academic, business or social connections (e.g. co-major, co-university, and cocorporation) tend to have similar experience and summaries. To achieve the learning process, we propose a collective factor graph (CoFG) model to incorporate all these resources of knowledge to summarize personal profiles with local textual attribute functions and social connection factors. Extensive evaluation on a large-scale dataset from LinkedIn.com demonstrates the effectiveness of the proposed approach. 1</p><p>5 0.43064836 <a title="184-lda-5" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>Author: Oier Lopez de Lacalle ; Mirella Lapata</p><p>Abstract: In this paper we present an unsupervised approach to relational information extraction. Our model partitions tuples representing an observed syntactic relationship between two named entities (e.g., “X was born in Y” and “X is from Y”) into clusters corresponding to underlying semantic relation types (e.g., BornIn, Located). Our approach incorporates general domain knowledge which we encode as First Order Logic rules and automatically combine with a topic model developed specifically for the relation extraction task. Evaluation results on the ACE 2007 English Relation Detection and Categorization (RDC) task show that our model outperforms competitive unsupervised approaches by a wide margin and is able to produce clusters shaped by both the data and the rules.</p><p>6 0.42881122 <a title="184-lda-6" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>7 0.42743987 <a title="184-lda-7" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>8 0.42685208 <a title="184-lda-8" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>9 0.42674661 <a title="184-lda-9" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>10 0.42435166 <a title="184-lda-10" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>11 0.42410347 <a title="184-lda-11" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>12 0.42394513 <a title="184-lda-12" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>13 0.42355248 <a title="184-lda-13" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>14 0.42337674 <a title="184-lda-14" href="./emnlp-2013-Automatically_Determining_a_Proper_Length_for_Multi-Document_Summarization%3A_A_Bayesian_Nonparametric_Approach.html">36 emnlp-2013-Automatically Determining a Proper Length for Multi-Document Summarization: A Bayesian Nonparametric Approach</a></p>
<p>15 0.42217019 <a title="184-lda-15" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>16 0.42170751 <a title="184-lda-16" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>17 0.42127645 <a title="184-lda-17" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>18 0.42120311 <a title="184-lda-18" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>19 0.42110252 <a title="184-lda-19" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>20 0.42060506 <a title="184-lda-20" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
