<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>69 emnlp-2013-Efficient Collective Entity Linking with Stacking</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-69" href="#">emnlp2013-69</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>69 emnlp-2013-Efficient Collective Entity Linking with Stacking</h1>
<br/><p>Source: <a title="emnlp-2013-69-pdf" href="http://aclweb.org/anthology//D/D13/D13-1041.pdf">pdf</a></p><p>Author: Zhengyan He ; Shujie Liu ; Yang Song ; Mu Li ; Ming Zhou ; Houfeng Wang</p><p>Abstract: Entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base. Recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes. We propose a fast collective disambiguation approach based on stacking. First, we train a local predictor g0 with learning to rank as base learner, to generate initial ranking list of candidates. Second, top k candidates of related instances are searched for constructing expressive global coherence features. A global predictor g1 is trained in the augmented feature space and stacking is employed to tackle the train/test mismatch problem. The proposed method is fast and easy to implement. Experiments show its effectiveness over various algorithms on several public datasets. By learning a rich semantic relatedness measure be- . tween entity categories and context document, performance is further improved.</p><p>Reference: <a title="emnlp-2013-69-reference" href="../emnlp2013_reference/emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 gcio ocm m@ g {msahiu ulj Abstract Entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base. [sent-6, score-0.484]
</p><p>2 Recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes. [sent-7, score-0.449]
</p><p>3 We propose a fast collective disambiguation approach based on stacking. [sent-8, score-0.346]
</p><p>4 First, we train a local predictor g0 with learning to rank as base learner, to generate initial ranking list of candidates. [sent-9, score-0.534]
</p><p>5 Second, top k candidates of related instances are searched for constructing expressive global coherence features. [sent-10, score-0.407]
</p><p>6 A global predictor g1 is trained in the augmented feature space and stacking is employed to tackle the train/test mismatch problem. [sent-11, score-0.866]
</p><p>7 tween entity categories and context document, performance is further improved. [sent-15, score-0.272]
</p><p>8 1 Introduction When extracting knowledge from natural language text into a machine readable format, ambiguous names must be resolved in order to tell which realworld entity the name refers to. [sent-16, score-0.336]
</p><p>9 The task of linking names to knowledge base is known as entity linking or disambiguation (Ji et al. [sent-17, score-0.642]
</p><p>10 Learning to rank (L2R) approaches use hand-crafted features f(d, e) to describe the similarity or dissimilarity between contextual document d and entity definition e. [sent-23, score-0.378]
</p><p>11 Collective approaches utilize dependencies between different decisions and resolve all ambiguous mentions within the same context simultaneously (Han et al. [sent-30, score-0.247]
</p><p>12 They often utilize semantic relations across different mentions, and is why they are called global approaches, while L2R methods fall into local approaches (Ratinov et al. [sent-36, score-0.286]
</p><p>13 However, collective inference processes are often expensive and involve an exponential search space. [sent-38, score-0.27]
</p><p>14 We propose a collective entity linking method based on stacking. [sent-39, score-0.628]
</p><p>15 The predictions of the first learner are taken as augmented features for the second learner. [sent-41, score-0.208]
</p><p>16 The nice property of stacking is that  it does not restrict the form of the base learner. [sent-42, score-0.328]
</p><p>17 hc o2d0s1 i3n A Nsastoucria lti Loan fgoura Cgoem Ppruotcaetsiosin agl, L piang eusis 4t2ic6s–435, At the next level, we search for semantic coherent entities from the top k candidates of neighboring mentions. [sent-46, score-0.384]
</p><p>18 The second learner is trained on the augmented feature space to enforce semantic coherence. [sent-47, score-0.257]
</p><p>19 Compared with existing collective methods, the inference process of our method is much faster because of the simple form of its base learner. [sent-49, score-0.345]
</p><p>20 Wikipedians annotate each entity with categories which provide another source of valuable semantic information. [sent-50, score-0.273]
</p><p>21 Our contributions are as follows: (1) We propose a fast and accurate stacking-based collective entity linking method, which combines the benefits ofboth coherence modeling ofcollective approaches and expressivity of L2R methods. [sent-54, score-0.731]
</p><p>22 We show an effective usage of ranking list as global features, which is a key improvement for the global predictor. [sent-55, score-0.281]
</p><p>23 (2) To overcome problems of scalability and shallow word-level comparison, we learn the categorycontext correlation with recent advances of representation learning, and show that this extra semantic information indeed helps improve entity linking performance. [sent-56, score-0.464]
</p><p>24 2  Related Work  Most popular entity linking systems use the L2R framework (Bunescu and Pasca, 2006; Li et al. [sent-57, score-0.358]
</p><p>25 It can include any features that describe the similarity or dissimilarity of context d and candidate entity e. [sent-62, score-0.391]
</p><p>26 This category falls into the local  approach as the decision processes for each mention are made independently (Ratinov et al. [sent-64, score-0.274]
</p><p>27 (Cucerzan, 2007) first suggests to optimize an objective function that is similar to the collective ap427 proach. [sent-66, score-0.27]
</p><p>28 Our method without stacking resembles the method of (Ratinov et al. [sent-72, score-0.253]
</p><p>29 , 2011) in that they use the predictions of a local ranker to generate features for global ranker. [sent-73, score-0.283]
</p><p>30 The differences are that we use stacking to train the local ranker to handle the train/test mismatch problem and top k candidates to generate features for the global ranker. [sent-74, score-0.797]
</p><p>31 Various applications of stacking in NLP have been proposed, such as collective document classification (Kou and Cohen, 2007), stacked dependency parsing (Martins et al. [sent-77, score-0.753]
</p><p>32 (Kou and Cohen, 2007) propose stacked graphical learning which captures dependencies between data with relational template. [sent-79, score-0.306]
</p><p>33 We search related entity candidates in a large semantic relatedness graph, based on the assumption that true candidates are often semantically correlated while false ones scattered around. [sent-82, score-0.626]
</p><p>34 1 Base learner and local predictor g0 Entity linking is formalized as follows: given an ambiguous name mention m with its contextual document d, a list of candidate entities e1, e2, . [sent-94, score-1.047]
</p><p>35 , en(m) ∈ C(m) is generated for m, our predictor g will generate a ranking score g(ei) ofuorr each candidate ei. [sent-97, score-0.388]
</p><p>36 c Worree wcta entity ohpatsim a higher score over negative ones. [sent-104, score-0.225]
</p><p>37 cosine similarity of TF-IDF score between context and entire Wikipedia page of candidate 2. [sent-129, score-0.197]
</p><p>38 jaccard distance between context and introduction of Wikipedia page Popularity or prominence feature: percentage of Wikipedia hyperlinks pointing to e given mention m, i. [sent-132, score-0.267]
</p><p>39 useful features like string matching and entity popularity cannot be easily expressed by collective approaches like (Hoffart et al. [sent-138, score-0.532]
</p><p>40 The features for level 0 predictor g0 are described in Table 1. [sent-141, score-0.319]
</p><p>41 2 Stacking training for global predictor g1 Stacked generalization (Wolpert, 1992) is a meta learning algorithm that stacks two “levels” of predictors. [sent-147, score-0.478]
</p><p>42 rTahien eledv oenl 1 predictor : Rd+K → R is trained in the augmented (d+K)-dimensional Rfea istu trreai space, i tnh we ahuicghpredictions at level 0 are taken as extra features in  h(1)  h(1). [sent-152, score-0.392]
</p><p>43 (Kou and Cohen, 2007) proposed stacked graphical learning for learning and inference on relational data. [sent-153, score-0.306]
</p><p>44 In stacked graphical learning, dependencies among data are captured by relational template, with which one searches for related instances of the current instance. [sent-154, score-0.306]
</p><p>45 For instance, in collective document classification (Kou and Cohen, 2007) employ relational template to extract documents that link to this document, then apply a COUNT aggregator over each category on neighboring documents as level 1 features. [sent-157, score-0.605]
</p><p>46 In our entity linking task, we use a single predictor g0 trained with local features at level 0. [sent-158, score-0.764]
</p><p>47 At level 1, for  each document-candidate entity pair, we use the relational template N(x) to find related entities for entity x, alntedm construct global nfedartuerleatse wdietnht some ofurencn-tion G({g0 (n) |n ∈ N(x)}) (details in Sec. [sent-160, score-0.847]
</p><p>48 tTiohen global predictor g1 xre)}ce)iv (edse as input tehce. [sent-163, score-0.394]
</p><p>49 2 Predict all related instances in Di with this predictor g0 2. [sent-176, score-0.276]
</p><p>50 Train level 0 predictor g0 on entire D, for expanding feature space for toesnt edanttair 4. [sent-178, score-0.399]
</p><p>51 Train level 1 predictor g1 on entire D, in the augmented feature space. [sent-179, score-0.436]
</p><p>52 3 Enforcing coherence with global features G If one wants to identify the correct entity for an ambiguous name, he would possibly look for related entities in its surrounding context. [sent-182, score-0.615]
</p><p>53 In ideal cases, most true candidates are inter-connected with semantic links while negative candidates are scattered around (Fig. [sent-184, score-0.377]
</p><p>54 Thus, we ask the following question: Is there any highly relevant entity to this candidate in context? [sent-186, score-0.292]
</p><p>55 Or, is there any mention with highly relevant entity to this candidate in the top k ranking list of this mention? [sent-187, score-0.51]
</p><p>56 g0 may not perfectly rank related entity at the first place, e. [sent-190, score-0.276]
</p><p>57 For each mention mi ∈ M, we rank each entity ei,j ∈ C(mi) by its score g0 (ei,j). [sent-194, score-0.426]
</p><p>58 For each entity e in the candidate set E = {ei,j |∀ei,j ∈ C(mi) , ∀mi ∈ M}, we search rEela =ted { einst|a∀neces f∈or C e as f)o,l∀lomws: 1. [sent-196, score-0.292]
</p><p>59 search in E for entities with semantic relatedness above a threshold ({0. [sent-197, score-0.216]
</p><p>60 map entities in step (2) to unique set of mentions U, excluding current m, i. [sent-204, score-0.214]
</p><p>61 He was re-elected president in November  Party (United States)]]  links for collective entity linking. [sent-232, score-0.531]
</p><p>62 Finally, the semantic relatedness measure of two entities ei,ej is defined as the common in-links of ei and ej in Wikipedia (Milne and Witten, 2008; Han et al. [sent-234, score-0.283]
</p><p>63 , 2011):  SR(ei,ej) = 1−loglo(mg(|aWx(||A) −|,| lBog|)()m −in l(o|gA(||,A|B ∩|) B)|) )(5)) where A and B are the set of in-links for entity ei and ej respectively, and W is the set of all Wikipedia pages. [sent-235, score-0.292]
</p><p>64 Our method is a trade-off between exact collective inference and approximating related instance  with top ranked entities produced by g0. [sent-236, score-0.436]
</p><p>65 Most collective approaches take all ambiguous mentions into consideration and disambiguate them simultaneously, resulting in difficulty when inference in large search space (Kulkarni et al. [sent-237, score-0.473]
</p><p>66 This most resembles our approach, except we use stacking to tackle the train/test mismatch problem, and construct different set of features from top k candidates predicted by g0. [sent-244, score-0.549]
</p><p>67 4  Learning category-context coherence model cat Entities in Wikipedia are annotated with rich semantic structures. [sent-247, score-0.248]
</p><p>68 Category network provides us with another valuable information for entity linking. [sent-248, score-0.225]
</p><p>69 “Barack Obama” with category “Democratic Party presidential nominees” shares the category “United States presidential candidates by party” with “Mitt Romney” when travelling two levels up the network. [sent-253, score-0.407]
</p><p>70 Thus, we use context as query q and the categories of this candidate entity as d. [sent-273, score-0.381]
</p><p>71 We also treat the definition page of an entity as its context, and first train the model with definition pages, because definition pages exhibit more focused topic. [sent-274, score-0.264]
</p><p>72 To  reduce noise, We input the categories directly connected with one entity as a word vector. [sent-276, score-0.225]
</p><p>73 TAC-KBP has several years of data for evaluating entity linking system, but is not well suited for evaluating collective approaches. [sent-281, score-0.628]
</p><p>74 , 2011) annotated a clean and much larger dataset AIDA 1 for collective approaches evaluation based on CoNLL 2003 NER dataset. [sent-283, score-0.27]
</p><p>75 We use annotation from Wikipedia to build a name dictionary from mention string m to entity e for candidate generation, including redirects, disambiguation pages and hyperlinks, follows the approach of (Cucerzan, 2007). [sent-294, score-0.533]
</p><p>76 For candidate generation, we keep the top 30 candidates by popularity (Tbl. [sent-295, score-0.29]
</p><p>77 , 2011) evaluate on “solvable” mentions and we have no way to recover those mentions, we re-implement their global features and the final scores are not directly comparable to theirs. [sent-300, score-0.224]
</p><p>78 2 Methods under comparison We compare our algorithm with several state-of-theart collective entity disambiguation systems. [sent-302, score-0.571]
</p><p>79 Dataset  ndocs  nonNIL  identified  solvable  AIDA dev 216 4791 4791 4707 AIDA test 231 4485 4485 4411 ACE36257238209(185) AQUAINT Wikipedia  50 40  727 928  697 918  668(588) 854(843)  Table 2: Number of mentions in each dataset. [sent-320, score-0.229]
</p><p>80 “identified” means the mention exists in our name dictionary and “solvable” means the true entity are among the top 30 candidates by popularity. [sent-321, score-0.576]
</p><p>81 The difference is that their method use top 1 candidate generated by local learner for global feature generation , while we search the top k candidates. [sent-330, score-0.488]
</p><p>82 Moreover, stacking is used to tackle the train/test mismatch problem in our model. [sent-331, score-0.363]
</p><p>83 , 2011) and use our local predictor g0 for level 0 predictor. [sent-333, score-0.406]
</p><p>84 Note that we only implement their global features concerning common inlinks and inter-connection (totally 9 features) for fair comparison because all other models don’t use common outgoing links for global coherence. [sent-334, score-0.272]
</p><p>85 The number of fold for stacking is set to {1,5,10} (see Table 4, default is 10; 1 means no stacking, i}. [sent-349, score-0.293]
</p><p>86 The number k for searching neighboring entities with relational template is set to {1,3,5,7} (e. [sent-352, score-0.34]
</p><p>87 86031247  Table 3: Top k recall for local predictor  g0. [sent-367, score-0.363]
</p><p>88 top k means up to k candidates are used for searching related instances with relational template. [sent-379, score-0.328]
</p><p>89 Group 5 in Table 4 shows cat information generally boosts performance for both predictor g0 and g1. [sent-381, score-0.373]
</p><p>90 Effect of stacking: Group 3 in Table 4 shows the results with different fold in stacking training. [sent-382, score-0.293]
</p><p>91 We deduce it is possible the way we fire global features with top k candidates that alleviates the problem of train/test mismatch when extending feature space for g1. [sent-385, score-0.45]
</p><p>92 Despite the ranking of true entity can be lower in testset than in training  set, the semantic coherence information can still be captured with searching over top k candidates. [sent-386, score-0.597]
</p><p>93 Table 3 shows the top k recall of local predictor g0. [sent-389, score-0.421]
</p><p>94 While their method is a trade-off between expensive exact search over all mentions and greedy assigning all mentions with local predictor, we show this idea can be further extended, somewhat like increasing the beam search size without additional computational overhead. [sent-393, score-0.299]
</p><p>95 Finally, as we analyze the development set of AIDA, we discover that some location entities rely on more distant information across the context, as we increase the context to the entire contextual document, we can gain extra performance boost. [sent-459, score-0.199]
</p><p>96 5  Conclusions  We propose a stacking based collective entity linking method, which stacks a global predictor on top of a local predictor to collect coherence information from neighboring decisions. [sent-473, score-1.882]
</p><p>97 Our method trades off between inefficient exact search and greedily assigning mention with local predictor. [sent-475, score-0.202]
</p><p>98 It can be seen as searching related  entities with relational template in stacked graphical learning, with beam size k. [sent-476, score-0.524]
</p><p>99 Combining these two techniques, our model consistently outperforms all existing more sophisticated collective approaches in our experiments. [sent-479, score-0.27]
</p><p>100 Collective entity linking in web text: a graph-based method. [sent-510, score-0.358]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('predictor', 0.276), ('collective', 0.27), ('stacking', 0.253), ('aida', 0.235), ('entity', 0.225), ('ratinov', 0.219), ('hoffart', 0.186), ('stacked', 0.18), ('kou', 0.141), ('wikipedia', 0.14), ('linking', 0.133), ('candidates', 0.128), ('solvable', 0.123), ('romney', 0.122), ('global', 0.118), ('mention', 0.115), ('mismatch', 0.11), ('entities', 0.108), ('mentions', 0.106), ('coherence', 0.103), ('learner', 0.1), ('cat', 0.097), ('local', 0.087), ('bunescu', 0.086), ('mitt', 0.082), ('relational', 0.08), ('party', 0.08), ('disambiguation', 0.076), ('base', 0.075), ('han', 0.075), ('augmented', 0.073), ('category', 0.072), ('kulkarni', 0.072), ('wolpert', 0.071), ('pasca', 0.069), ('candidate', 0.067), ('tac', 0.067), ('lehmann', 0.066), ('zheng', 0.063), ('searching', 0.062), ('ambiguous', 0.061), ('democratic', 0.061), ('relatedness', 0.06), ('pm', 0.06), ('correlation', 0.058), ('top', 0.058), ('substring', 0.057), ('bai', 0.056), ('republican', 0.056), ('testset', 0.056), ('mrr', 0.055), ('dissimilarity', 0.052), ('rank', 0.051), ('name', 0.05), ('document', 0.05), ('di', 0.049), ('semantic', 0.048), ('template', 0.048), ('kataria', 0.047), ('mrank', 0.047), ('nominee', 0.047), ('wikipedians', 0.047), ('zhengyan', 0.047), ('context', 0.047), ('presidential', 0.047), ('graphical', 0.046), ('france', 0.046), ('ranking', 0.045), ('cucerzan', 0.045), ('cohen', 0.045), ('entire', 0.044), ('level', 0.043), ('ranker', 0.043), ('meta', 0.043), ('neighboring', 0.042), ('query', 0.042), ('stacks', 0.041), ('travelling', 0.041), ('fold', 0.04), ('china', 0.039), ('page', 0.039), ('aquaint', 0.037), ('aanndd', 0.037), ('scattered', 0.037), ('popularity', 0.037), ('scales', 0.037), ('links', 0.036), ('space', 0.036), ('predictions', 0.035), ('mi', 0.035), ('xml', 0.035), ('macro', 0.035), ('cutting', 0.035), ('ei', 0.034), ('datasets', 0.034), ('utilize', 0.033), ('ej', 0.033), ('hyperlinks', 0.033), ('pointing', 0.033), ('campaign', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="69-tfidf-1" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>Author: Zhengyan He ; Shujie Liu ; Yang Song ; Mu Li ; Ming Zhou ; Houfeng Wang</p><p>Abstract: Entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base. Recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes. We propose a fast collective disambiguation approach based on stacking. First, we train a local predictor g0 with learning to rank as base learner, to generate initial ranking list of candidates. Second, top k candidates of related instances are searched for constructing expressive global coherence features. A global predictor g1 is trained in the augmented feature space and stacking is employed to tackle the train/test mismatch problem. The proposed method is fast and easy to implement. Experiments show its effectiveness over various algorithms on several public datasets. By learning a rich semantic relatedness measure be- . tween entity categories and context document, performance is further improved.</p><p>2 0.2676549 <a title="69-tfidf-2" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>Author: Xiao Cheng ; Dan Roth</p><p>Abstract: Wikification, commonly referred to as Disambiguation to Wikipedia (D2W), is the task of identifying concepts and entities in text and disambiguating them into the most specific corresponding Wikipedia pages. Previous approaches to D2W focused on the use of local and global statistics over the given text, Wikipedia articles and its link structures, to evaluate context compatibility among a list of probable candidates. However, these methods fail (often, embarrassingly), when some level of text understanding is needed to support Wikification. In this paper we introduce a novel approach to Wikification by incorporating, along with statistical methods, richer relational analysis of the text. We provide an extensible, efficient and modular Integer Linear Programming (ILP) formulation of Wikification that incorporates the entity-relation inference problem, and show that the ability to identify relations in text helps both candi- date generation and ranking Wikipedia titles considerably. Our results show significant improvements in both Wikification and the TAC Entity Linking task.</p><p>3 0.17315724 <a title="69-tfidf-3" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>Author: Hrushikesh Mohapatra ; Siddhanth Jain ; Soumen Chakrabarti</p><p>Abstract: Web search can be enhanced in powerful ways if token spans in Web text are annotated with disambiguated entities from large catalogs like Freebase. Entity annotators need to be trained on sample mention snippets. Wikipedia entities and annotated pages offer high-quality labeled data for training and evaluation. Unfortunately, Wikipedia features only one-ninth the number of entities as Freebase, and these are a highly biased sample of well-connected, frequently mentioned “head” entities. To bring hope to “tail” entities, we broaden our goal to a second task: assigning types to entities in Freebase but not Wikipedia. The two tasks are synergistic: knowing the types of unfamiliar entities helps disambiguate mentions, and words in mention contexts help assign types to entities. We present TMI, a bipartite graphical model for joint type-mention inference. TMI attempts no schema integration or entity resolution, but exploits the above-mentioned synergy. In experiments involving 780,000 people in Wikipedia, 2.3 million people in Freebase, 700 million Web pages, and over 20 professional editors, TMI shows considerable annotation accuracy improvement (e.g., 70%) compared to baselines (e.g., 46%), especially for “tail” and emerging entities. We also compare with Google’s recent annotations of the same corpus with Freebase entities, and report considerable improvements within the people domain.</p><p>4 0.16692632 <a title="69-tfidf-4" href="./emnlp-2013-Microblog_Entity_Linking_by_Leveraging_Extra_Posts.html">130 emnlp-2013-Microblog Entity Linking by Leveraging Extra Posts</a></p>
<p>Author: Yuhang Guo ; Bing Qin ; Ting Liu ; Sheng Li</p><p>Abstract: Linking name mentions in microblog posts to a knowledge base, namely microblog entity linking, is useful for text mining tasks on microblog. Entity linking in long text has been well studied in previous works. However few work has focused on short text such as microblog post. Microblog posts are short and noisy. Previous method can extract few features from the post context. In this paper we propose to use extra posts for the microblog entity linking task. Experimental results show that our proposed method significantly improves the linking accuracy over traditional methods by 8.3% and 7.5% respectively.</p><p>5 0.15832879 <a title="69-tfidf-5" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>Author: Baichuan Li ; Jing Liu ; Chin-Yew Lin ; Irwin King ; Michael R. Lyu</p><p>Abstract: Social media like forums and microblogs have accumulated a huge amount of user generated content (UGC) containing human knowledge. Currently, most of UGC is listed as a whole or in pre-defined categories. This “list-based” approach is simple, but hinders users from browsing and learning knowledge of certain topics effectively. To address this problem, we propose a hierarchical entity-based approach for structuralizing UGC in social media. By using a large-scale entity repository, we design a three-step framework to organize UGC in a novel hierarchical structure called “cluster entity tree (CET)”. With Yahoo! Answers as a test case, we conduct experiments and the results show the effectiveness of our framework in constructing CET. We further evaluate the performance of CET on UGC organization in both user and system aspects. From a user aspect, our user study demonstrates that, with CET-based structure, users perform significantly better in knowledge learning than using traditional list-based approach. From a system aspect, CET substantially boosts the performance of two information retrieval models (i.e., vector space model and query likelihood language model).</p><p>6 0.14431491 <a title="69-tfidf-6" href="./emnlp-2013-Joint_Coreference_Resolution_and_Named-Entity_Linking_with_Multi-Pass_Sieves.html">112 emnlp-2013-Joint Coreference Resolution and Named-Entity Linking with Multi-Pass Sieves</a></p>
<p>7 0.13534212 <a title="69-tfidf-7" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>8 0.11431967 <a title="69-tfidf-8" href="./emnlp-2013-Easy_Victories_and_Uphill_Battles_in_Coreference_Resolution.html">67 emnlp-2013-Easy Victories and Uphill Battles in Coreference Resolution</a></p>
<p>9 0.1137825 <a title="69-tfidf-9" href="./emnlp-2013-Combining_Generative_and_Discriminative_Model_Scores_for_Distant_Supervision.html">49 emnlp-2013-Combining Generative and Discriminative Model Scores for Distant Supervision</a></p>
<p>10 0.10894901 <a title="69-tfidf-10" href="./emnlp-2013-A_Constrained_Latent_Variable_Model_for_Coreference_Resolution.html">1 emnlp-2013-A Constrained Latent Variable Model for Coreference Resolution</a></p>
<p>11 0.10664255 <a title="69-tfidf-11" href="./emnlp-2013-Application_of_Localized_Similarity_for_Web_Documents.html">24 emnlp-2013-Application of Localized Similarity for Web Documents</a></p>
<p>12 0.10061602 <a title="69-tfidf-12" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>13 0.096163161 <a title="69-tfidf-13" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>14 0.08961235 <a title="69-tfidf-14" href="./emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model.html">75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</a></p>
<p>15 0.083077632 <a title="69-tfidf-15" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>16 0.077270731 <a title="69-tfidf-16" href="./emnlp-2013-Automatic_Feature_Engineering_for_Answer_Selection_and_Extraction.html">31 emnlp-2013-Automatic Feature Engineering for Answer Selection and Extraction</a></p>
<p>17 0.070563838 <a title="69-tfidf-17" href="./emnlp-2013-Semi-Supervised_Feature_Transformation_for_Dependency_Parsing.html">168 emnlp-2013-Semi-Supervised Feature Transformation for Dependency Parsing</a></p>
<p>18 0.068756662 <a title="69-tfidf-18" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>19 0.067064673 <a title="69-tfidf-19" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>20 0.066272661 <a title="69-tfidf-20" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.268), (1, 0.166), (2, 0.133), (3, -0.033), (4, 0.01), (5, 0.062), (6, 0.091), (7, 0.168), (8, 0.148), (9, -0.06), (10, 0.071), (11, 0.022), (12, -0.115), (13, -0.002), (14, -0.223), (15, -0.127), (16, 0.022), (17, 0.021), (18, -0.013), (19, 0.021), (20, -0.109), (21, 0.025), (22, -0.055), (23, 0.033), (24, 0.081), (25, -0.002), (26, 0.079), (27, -0.005), (28, 0.012), (29, -0.019), (30, 0.101), (31, 0.002), (32, 0.123), (33, -0.0), (34, 0.112), (35, 0.084), (36, -0.018), (37, -0.032), (38, -0.055), (39, -0.085), (40, 0.046), (41, 0.108), (42, -0.049), (43, 0.037), (44, -0.06), (45, -0.003), (46, 0.016), (47, 0.028), (48, -0.02), (49, 0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96100342 <a title="69-lsi-1" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>Author: Zhengyan He ; Shujie Liu ; Yang Song ; Mu Li ; Ming Zhou ; Houfeng Wang</p><p>Abstract: Entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base. Recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes. We propose a fast collective disambiguation approach based on stacking. First, we train a local predictor g0 with learning to rank as base learner, to generate initial ranking list of candidates. Second, top k candidates of related instances are searched for constructing expressive global coherence features. A global predictor g1 is trained in the augmented feature space and stacking is employed to tackle the train/test mismatch problem. The proposed method is fast and easy to implement. Experiments show its effectiveness over various algorithms on several public datasets. By learning a rich semantic relatedness measure be- . tween entity categories and context document, performance is further improved.</p><p>2 0.81818396 <a title="69-lsi-2" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>Author: Hrushikesh Mohapatra ; Siddhanth Jain ; Soumen Chakrabarti</p><p>Abstract: Web search can be enhanced in powerful ways if token spans in Web text are annotated with disambiguated entities from large catalogs like Freebase. Entity annotators need to be trained on sample mention snippets. Wikipedia entities and annotated pages offer high-quality labeled data for training and evaluation. Unfortunately, Wikipedia features only one-ninth the number of entities as Freebase, and these are a highly biased sample of well-connected, frequently mentioned “head” entities. To bring hope to “tail” entities, we broaden our goal to a second task: assigning types to entities in Freebase but not Wikipedia. The two tasks are synergistic: knowing the types of unfamiliar entities helps disambiguate mentions, and words in mention contexts help assign types to entities. We present TMI, a bipartite graphical model for joint type-mention inference. TMI attempts no schema integration or entity resolution, but exploits the above-mentioned synergy. In experiments involving 780,000 people in Wikipedia, 2.3 million people in Freebase, 700 million Web pages, and over 20 professional editors, TMI shows considerable annotation accuracy improvement (e.g., 70%) compared to baselines (e.g., 46%), especially for “tail” and emerging entities. We also compare with Google’s recent annotations of the same corpus with Freebase entities, and report considerable improvements within the people domain.</p><p>3 0.8017382 <a title="69-lsi-3" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>Author: Ruiji Fu ; Bing Qin ; Ting Liu</p><p>Abstract: Hypernym discovery aims to extract such noun pairs that one noun is a hypernym of the other. Most previous methods are based on lexical patterns but perform badly on opendomain data. Other work extracts hypernym relations from encyclopedias but has limited coverage. This paper proposes a simple yet effective distant supervision framework for Chinese open-domain hypernym discovery. Given an entity name, we try to discover its hypernyms by leveraging knowledge from multiple sources, i.e., search engine results, encyclopedias, and morphology of the entity name. First, we extract candidate hypernyms from the above sources. Then, we apply a statistical ranking model to select correct hypernyms. A set of novel features is proposed for the rank- ing model. We also present a heuristic strategy to build a large-scale noisy training data for the model without human annotation. Experimental results demonstrate that our approach outperforms the state-of-the-art methods on a manually labeled test dataset.</p><p>4 0.80097055 <a title="69-lsi-4" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>Author: Xiao Cheng ; Dan Roth</p><p>Abstract: Wikification, commonly referred to as Disambiguation to Wikipedia (D2W), is the task of identifying concepts and entities in text and disambiguating them into the most specific corresponding Wikipedia pages. Previous approaches to D2W focused on the use of local and global statistics over the given text, Wikipedia articles and its link structures, to evaluate context compatibility among a list of probable candidates. However, these methods fail (often, embarrassingly), when some level of text understanding is needed to support Wikification. In this paper we introduce a novel approach to Wikification by incorporating, along with statistical methods, richer relational analysis of the text. We provide an extensible, efficient and modular Integer Linear Programming (ILP) formulation of Wikification that incorporates the entity-relation inference problem, and show that the ability to identify relations in text helps both candi- date generation and ranking Wikipedia titles considerably. Our results show significant improvements in both Wikification and the TAC Entity Linking task.</p><p>5 0.72606707 <a title="69-lsi-5" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>Author: Baichuan Li ; Jing Liu ; Chin-Yew Lin ; Irwin King ; Michael R. Lyu</p><p>Abstract: Social media like forums and microblogs have accumulated a huge amount of user generated content (UGC) containing human knowledge. Currently, most of UGC is listed as a whole or in pre-defined categories. This “list-based” approach is simple, but hinders users from browsing and learning knowledge of certain topics effectively. To address this problem, we propose a hierarchical entity-based approach for structuralizing UGC in social media. By using a large-scale entity repository, we design a three-step framework to organize UGC in a novel hierarchical structure called “cluster entity tree (CET)”. With Yahoo! Answers as a test case, we conduct experiments and the results show the effectiveness of our framework in constructing CET. We further evaluate the performance of CET on UGC organization in both user and system aspects. From a user aspect, our user study demonstrates that, with CET-based structure, users perform significantly better in knowledge learning than using traditional list-based approach. From a system aspect, CET substantially boosts the performance of two information retrieval models (i.e., vector space model and query likelihood language model).</p><p>6 0.60999775 <a title="69-lsi-6" href="./emnlp-2013-Microblog_Entity_Linking_by_Leveraging_Extra_Posts.html">130 emnlp-2013-Microblog Entity Linking by Leveraging Extra Posts</a></p>
<p>7 0.60150915 <a title="69-lsi-7" href="./emnlp-2013-Combining_Generative_and_Discriminative_Model_Scores_for_Distant_Supervision.html">49 emnlp-2013-Combining Generative and Discriminative Model Scores for Distant Supervision</a></p>
<p>8 0.59076554 <a title="69-lsi-8" href="./emnlp-2013-Joint_Coreference_Resolution_and_Named-Entity_Linking_with_Multi-Pass_Sieves.html">112 emnlp-2013-Joint Coreference Resolution and Named-Entity Linking with Multi-Pass Sieves</a></p>
<p>9 0.54580897 <a title="69-lsi-9" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>10 0.49616501 <a title="69-lsi-10" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>11 0.47460452 <a title="69-lsi-11" href="./emnlp-2013-Application_of_Localized_Similarity_for_Web_Documents.html">24 emnlp-2013-Application of Localized Similarity for Web Documents</a></p>
<p>12 0.46245328 <a title="69-lsi-12" href="./emnlp-2013-Two-Stage_Method_for_Large-Scale_Acquisition_of_Contradiction_Pattern_Pairs_using_Entailment.html">189 emnlp-2013-Two-Stage Method for Large-Scale Acquisition of Contradiction Pattern Pairs using Entailment</a></p>
<p>13 0.41718021 <a title="69-lsi-13" href="./emnlp-2013-Rule-Based_Information_Extraction_is_Dead%21_Long_Live_Rule-Based_Information_Extraction_Systems%21.html">161 emnlp-2013-Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!</a></p>
<p>14 0.40997657 <a title="69-lsi-14" href="./emnlp-2013-Detecting_Promotional_Content_in_Wikipedia.html">61 emnlp-2013-Detecting Promotional Content in Wikipedia</a></p>
<p>15 0.40387893 <a title="69-lsi-15" href="./emnlp-2013-Automatically_Classifying_Edit_Categories_in_Wikipedia_Revisions.html">34 emnlp-2013-Automatically Classifying Edit Categories in Wikipedia Revisions</a></p>
<p>16 0.40309784 <a title="69-lsi-16" href="./emnlp-2013-Mining_New_Business_Opportunities%3A_Identifying_Trend_related_Products_by_Leveraging_Commercial_Intents_from_Microblogs.html">131 emnlp-2013-Mining New Business Opportunities: Identifying Trend related Products by Leveraging Commercial Intents from Microblogs</a></p>
<p>17 0.40244383 <a title="69-lsi-17" href="./emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model.html">75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</a></p>
<p>18 0.38243783 <a title="69-lsi-18" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>19 0.37396976 <a title="69-lsi-19" href="./emnlp-2013-Effectiveness_and_Efficiency_of_Open_Relation_Extraction.html">68 emnlp-2013-Effectiveness and Efficiency of Open Relation Extraction</a></p>
<p>20 0.37253326 <a title="69-lsi-20" href="./emnlp-2013-Learning_to_Rank_Lexical_Substitutions.html">123 emnlp-2013-Learning to Rank Lexical Substitutions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.051), (18, 0.042), (22, 0.057), (29, 0.012), (30, 0.069), (36, 0.18), (47, 0.012), (50, 0.022), (51, 0.205), (66, 0.026), (71, 0.034), (75, 0.051), (77, 0.021), (90, 0.01), (95, 0.02), (96, 0.065), (98, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91717339 <a title="69-lda-1" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>Author: Xiao Cheng ; Dan Roth</p><p>Abstract: Wikification, commonly referred to as Disambiguation to Wikipedia (D2W), is the task of identifying concepts and entities in text and disambiguating them into the most specific corresponding Wikipedia pages. Previous approaches to D2W focused on the use of local and global statistics over the given text, Wikipedia articles and its link structures, to evaluate context compatibility among a list of probable candidates. However, these methods fail (often, embarrassingly), when some level of text understanding is needed to support Wikification. In this paper we introduce a novel approach to Wikification by incorporating, along with statistical methods, richer relational analysis of the text. We provide an extensible, efficient and modular Integer Linear Programming (ILP) formulation of Wikification that incorporates the entity-relation inference problem, and show that the ability to identify relations in text helps both candi- date generation and ranking Wikipedia titles considerably. Our results show significant improvements in both Wikification and the TAC Entity Linking task.</p><p>2 0.89893365 <a title="69-lda-2" href="./emnlp-2013-Automatically_Identifying_Pseudepigraphic_Texts.html">37 emnlp-2013-Automatically Identifying Pseudepigraphic Texts</a></p>
<p>Author: Moshe Koppel ; Shachar Seidman</p><p>Abstract: The identification of pseudepigraphic texts texts not written by the authors to which they are attributed – has important historical, forensic and commercial applications. We introduce an unsupervised technique for identifying pseudepigrapha. The idea is to identify textual outliers in a corpus based on the pairwise similarities of all documents in the corpus. The crucial point is that document similarity not be measured in any of the standard ways but rather be based on the output of a recently introduced algorithm for authorship verification. The proposed method strongly outperforms existing techniques in systematic experiments on a blog corpus. 1</p><p>same-paper 3 0.88400853 <a title="69-lda-3" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>Author: Zhengyan He ; Shujie Liu ; Yang Song ; Mu Li ; Ming Zhou ; Houfeng Wang</p><p>Abstract: Entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base. Recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes. We propose a fast collective disambiguation approach based on stacking. First, we train a local predictor g0 with learning to rank as base learner, to generate initial ranking list of candidates. Second, top k candidates of related instances are searched for constructing expressive global coherence features. A global predictor g1 is trained in the augmented feature space and stacking is employed to tackle the train/test mismatch problem. The proposed method is fast and easy to implement. Experiments show its effectiveness over various algorithms on several public datasets. By learning a rich semantic relatedness measure be- . tween entity categories and context document, performance is further improved.</p><p>4 0.79181671 <a title="69-lda-4" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>Author: Zhongqing Wang ; Shoushan LI ; Fang Kong ; Guodong Zhou</p><p>Abstract: Personal profile information on social media like LinkedIn.com and Facebook.com is at the core of many interesting applications, such as talent recommendation and contextual advertising. However, personal profiles usually lack organization confronted with the large amount of available information. Therefore, it is always a challenge for people to find desired information from them. In this paper, we address the task of personal profile summarization by leveraging both personal profile textual information and social networks. Here, using social networks is motivated by the intuition that, people with similar academic, business or social connections (e.g. co-major, co-university, and cocorporation) tend to have similar experience and summaries. To achieve the learning process, we propose a collective factor graph (CoFG) model to incorporate all these resources of knowledge to summarize personal profiles with local textual attribute functions and social connection factors. Extensive evaluation on a large-scale dataset from LinkedIn.com demonstrates the effectiveness of the proposed approach. 1</p><p>5 0.78792137 <a title="69-lda-5" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>Author: Hrushikesh Mohapatra ; Siddhanth Jain ; Soumen Chakrabarti</p><p>Abstract: Web search can be enhanced in powerful ways if token spans in Web text are annotated with disambiguated entities from large catalogs like Freebase. Entity annotators need to be trained on sample mention snippets. Wikipedia entities and annotated pages offer high-quality labeled data for training and evaluation. Unfortunately, Wikipedia features only one-ninth the number of entities as Freebase, and these are a highly biased sample of well-connected, frequently mentioned “head” entities. To bring hope to “tail” entities, we broaden our goal to a second task: assigning types to entities in Freebase but not Wikipedia. The two tasks are synergistic: knowing the types of unfamiliar entities helps disambiguate mentions, and words in mention contexts help assign types to entities. We present TMI, a bipartite graphical model for joint type-mention inference. TMI attempts no schema integration or entity resolution, but exploits the above-mentioned synergy. In experiments involving 780,000 people in Wikipedia, 2.3 million people in Freebase, 700 million Web pages, and over 20 professional editors, TMI shows considerable annotation accuracy improvement (e.g., 70%) compared to baselines (e.g., 46%), especially for “tail” and emerging entities. We also compare with Google’s recent annotations of the same corpus with Freebase entities, and report considerable improvements within the people domain.</p><p>6 0.78129601 <a title="69-lda-6" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>7 0.77664369 <a title="69-lda-7" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>8 0.77652317 <a title="69-lda-8" href="./emnlp-2013-Exploiting_Zero_Pronouns_to_Improve_Chinese_Coreference_Resolution.html">80 emnlp-2013-Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</a></p>
<p>9 0.77565664 <a title="69-lda-9" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>10 0.77471912 <a title="69-lda-10" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>11 0.77337432 <a title="69-lda-11" href="./emnlp-2013-Unsupervised_Induction_of_Cross-Lingual_Semantic_Relations.html">193 emnlp-2013-Unsupervised Induction of Cross-Lingual Semantic Relations</a></p>
<p>12 0.77253246 <a title="69-lda-12" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>13 0.77164137 <a title="69-lda-13" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>14 0.76946151 <a title="69-lda-14" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>15 0.76797491 <a title="69-lda-15" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>16 0.76794201 <a title="69-lda-16" href="./emnlp-2013-Microblog_Entity_Linking_by_Leveraging_Extra_Posts.html">130 emnlp-2013-Microblog Entity Linking by Leveraging Extra Posts</a></p>
<p>17 0.76744014 <a title="69-lda-17" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>18 0.76729059 <a title="69-lda-18" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>19 0.76720738 <a title="69-lda-19" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>20 0.7669673 <a title="69-lda-20" href="./emnlp-2013-Document_Summarization_via_Guided_Sentence_Compression.html">65 emnlp-2013-Document Summarization via Guided Sentence Compression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
