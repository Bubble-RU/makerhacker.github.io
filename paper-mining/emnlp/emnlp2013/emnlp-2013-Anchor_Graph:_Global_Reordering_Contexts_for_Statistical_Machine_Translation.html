<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-22" href="#">emnlp2013-22</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</h1>
<br/><p>Source: <a title="emnlp-2013-22-pdf" href="http://aclweb.org/anthology//D/D13/D13-1048.pdf">pdf</a></p><p>Author: Hendra Setiawan ; Bowen Zhou ; Bing Xiang</p><p>Abstract: Reordering poses one of the greatest challenges in Statistical Machine Translation research as the key contextual information may well be beyond the confine oftranslation units. We present the “Anchor Graph” (AG) model where we use a graph structure to model global contextual information that is crucial for reordering. The key ingredient of our AG model is the edges that capture the relationship between the reordering around a set of selected translation units, which we refer to as anchors. As the edges link anchors that may span multiple translation units at decoding time, our AG model effectively encodes global contextual information that is previously absent. We integrate our proposed model into a state-of-the-art translation system and demonstrate the efficacy of our proposal in a largescale Chinese-to-English translation task.</p><p>Reference: <a title="emnlp-2013-22-reference" href="../emnlp2013_reference/emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We present the “Anchor Graph” (AG) model where we use a graph structure to model global contextual information that is crucial for reordering. [sent-2, score-0.16]
</p><p>2 The key ingredient of our AG model is the edges that capture the relationship between the reordering around a set of  selected translation units, which we refer to as anchors. [sent-3, score-0.558]
</p><p>3 As the edges link anchors that may span multiple translation units at decoding time, our AG model effectively encodes global contextual information that is previously absent. [sent-4, score-1.106]
</p><p>4 We integrate our proposed model into a state-of-the-art translation system and demonstrate the efficacy of our proposal in a largescale Chinese-to-English translation task. [sent-5, score-0.481]
</p><p>5 For example, state-of-the-art translation models, such as Hiero (Chiang, 2005) or Moses (Koehn et al. [sent-7, score-0.208]
</p><p>6 , 2007), are good at capturing local reordering within the confine of a translation unit, but their formulation is approximately a simple unigram model  units. [sent-8, score-0.539]
</p><p>7 1We define translation units as phrases in phrase-based SMT or as translation rules in syntax-based SMT. [sent-10, score-0.543]
</p><p>8 501 Bowen Zhou Bing Xiang ∗ IBM Research Thomson Reuters 1101 Kitchawan Road 3 Times Square NY 10598, USA NY 10036, USA over derivation (a sequence of the application of translation units) with some aid from target language models. [sent-11, score-0.286]
</p><p>9 In this paper, we develop “Anchor Graph” (AG) where we use a graph structure to capture global contexts that are crucial for translation. [sent-13, score-0.113]
</p><p>10 To circumvent the sparsity issue, we design our model to rely only on contexts from a set of selected translation units, particularly those that appear frequently with important reordering patterns. [sent-14, score-0.489]
</p><p>11 We refer to the units in this special set as anchors where they act as vertices in the graph. [sent-15, score-0.666]
</p><p>12 To address the spurious ambiguity issue, we insist on computing the model score for  every anchors in the derivation, including those that appear inside larger translation units, as such our AG model gives the same score to the derivations that share the same reordering pattern. [sent-16, score-1.154]
</p><p>13 In AG model, the actual reordering is modeled by the edges, or more specifically, by the edges’ labels where different reordering around the anchors would correspond to a different label. [sent-17, score-1.099]
</p><p>14 As detailed later, we consider two distinct set of labels, namely dominance and precedence, reflecting the two dominant views about reordering in literature, i. [sent-18, score-0.616]
</p><p>15 More concretely, the dominance looks at the anchors’ relative positions in the translated sentence, while the precedence looks at the anchors’ relative positions in a latent structure, induced via a novel synchronous grammar: Anchorcentric, Lexicalized Synchronous Grammar. [sent-23, score-0.728]
</p><p>16 From these two sets of labels, we develop two probabilistic models, namely the dominance and the orientation models. [sent-24, score-0.433]
</p><p>17 As the edges of AG link pairs of anchors that may appear in multiple translation units, our AG models are able to capture high order contextual information that is previously absent. [sent-25, score-0.895]
</p><p>18 More importantly, our experimental results demonstrate the efficacy ofour proposed AGbased models, which we integrate into a state-of-theart syntax-based translation system, in a large scale Chinese-to-English translation task. [sent-27, score-0.481]
</p><p>19 We would like to emphasize that although we use a syntax-based translation system in our experiments, in principle, our approach is applicable to other translation models as it is agnostic to the translation units. [sent-28, score-0.624]
</p><p>20 2  Anchor Graph Model  Formally, an AG consists of {A, L} where A is a sFeotr mofa vllyer,ti acnes A tGhat c correspond Ato, anchors, ew Ahile is L a sise a set oerft ilcaebsel tehda edges tshpoatn dlin tko a pair rosf, a wnhchileors L. [sent-29, score-0.069]
</p><p>21 In principle, our AG model is part of a translation model that focuses on the reordering within the  source sentence F and its translation E. [sent-30, score-0.707]
</p><p>22 Thus, we start by first introducing A into a translation model (either word-based, phrase-based or syntax-based model) followed by L. [sent-31, score-0.208]
</p><p>23 Let P(E, ∼ |F) be a translation model where ∼ corresponds ∼to Fthe) alignments tbioetnw meeond euln witsh irne F∼ and E. [sent-34, score-0.208]
</p><p>24 We introduce A into a translation model,  2  2Alignment (∼) represents an existing latent variable. [sent-35, score-0.208]
</p><p>25 Given two anchors am, an where m < n, we define the 4We represent a constituent as a source and target phrase pair (fjj12/eii12 ) where the subscript and the superscript indicate the starting and the ending indices as such fjj21 denotes a source phrase that spans from j1to j2. [sent-48, score-0.655]
</p><p>26 dominance relation between am and an via MR(am) and ML(an). [sent-49, score-0.294]
</p><p>27 To estimate them, we train a discriminative classifier for each model and use the normalized posteriors at decoding time as additional feature scores in SMT’s log-linear framework. [sent-56, score-0.041]
</p><p>28 Additionally, we augment the feature set with compound features, e. [sent-58, score-0.037]
</p><p>29 a conjunction of the source word of the left anchor and the source word of the right anchor. [sent-60, score-0.235]
</p><p>30 • non-local: (the previous anchor’s source word) , (the next a (ntcheho prr’esv source word), (’s rPcOeS w tag), (’s POS tag). [sent-64, score-0.084]
</p><p>31 There is a separate set of elementary features for am and an and we come up with manual combination to construct compound features. [sent-65, score-0.074]
</p><p>32 507 In training the models, we manually come up with around 30-50 types of features, which consists of a combination of elementary and compound features. [sent-66, score-0.106]
</p><p>33 6  Decoding  As mentioned earlier, we wish to avoid the spurious ambiguity issue where different derivations have radically different scores although they lead to the same reordering. [sent-72, score-0.232]
</p><p>34 This section describes our decoding algorithm that avoids spurious ambiguity issue by incrementally constructing MLs and MRs thus allowing the computation of the models over partial hypotheses. [sent-73, score-0.197]
</p><p>35 In our experiments, we integrate our dominance model as well as our orientation model into a syntaxbased SMT system that uses SCFG formalism. [sent-74, score-0.479]
</p><p>36 Integrating the models into syntax-based SMT systems is non-trivial, especially since the anchors often reside within translation rules and the model  doesn’t always decompose naturally with the hypothesis structure. [sent-75, score-0.747]
</p><p>37 To facilitate that, we need to first induce the necessary alignment for all translation units in the hypothesis. [sent-76, score-0.335]
</p><p>38 2 with the following set of hierarchical phrases:  hAozhou1shi2 X1, Australia1 is2X1i Xb→ hyu3Beihan4 X1, X1with3 North4 Koreai Xc→ hyou5bangjiao6,have5dipl. [sent-78, score-0.045]
</p><p>39 i Xd→ hX1 de7shaoshu8 guojia9 zhi10 yi11, one11of10the few8 countries9 that7X1i  Xa→  As a case in point, let us consider D = Xa ≺ Xb ≺ Xd ≺ Xc, which will lead to the correct English Target string (w/ source index)Symbol(s) readOp. [sent-80, score-0.042]
</p><p>40 , S, R and A refer to shift, reduce and accept operation  respectively. [sent-84, score-0.047]
</p><p>41 Note that the translation rules contain internal word alignment, which we assume to have been previously inferred. [sent-87, score-0.208]
</p><p>42 The algorithm bears a close resemblance to the shift-reduce algorithm found in phrase-based decoding (Galley and Manning, 2008; Feng et al. [sent-88, score-0.041]
</p><p>43 A stack is used to accumulate (partial) information about a, ML and MR for each a ∈ A in the derivation. [sent-91, score-0.066]
</p><p>44 i Tthheirs tahleg shift or atkhee sr aednu inceoperations starting from the beginning until the end of the stream. [sent-93, score-0.053]
</p><p>45 The shift operation advances the input stream by one symbol and push the symbol into the stack; while the reduce operation applies some rule to the top-most elements of the stack. [sent-94, score-0.338]
</p><p>46 The algorithm terminates at the end of the input stream where the resulting stack will be propagated to the parent for the later stage of decoding. [sent-95, score-0.146]
</p><p>47 In our case, the input stream is the target string of the rule and the symbol is the corresponding source index of the elements of the target string. [sent-96, score-0.279]
</p><p>48 The reduction rule looks at two indices and merge them if they are adjacent (i. [sent-97, score-0.168]
</p><p>49 Table 2 shows the execution trace of the algorithm for the derivation described earlier. [sent-101, score-0.046]
</p><p>50 For conciseness, we assume that there is only one anchor and that is de7/that7. [sent-102, score-0.151]
</p><p>51 It then projects the source index to the corresponding target word and then enumerates the target string in a left to right fashion. [sent-104, score-0.145]
</p><p>52 If it finds a target word with a source index, it applies the shift oper508 ation, pushing the index to the stack. [sent-105, score-0.166]
</p><p>53 Unless the symbol corresponds to an anchor, it tries to apply the reduce operation. [sent-106, score-0.057]
</p><p>54 If the symbol being read is a nonterminal, then we push the entire stack that corresponds to that nonterminal. [sent-108, score-0.123]
</p><p>55 For example, when the algorithm reads Xd at line (6), it pushes the entire stack from line (5). [sent-109, score-0.146]
</p><p>56 The training corpora include a mixed genre of newswire, weblog, broadcast news, broadcast conversation, discussion forums and comes from various sources such as LDC, HK Law, HK Hansard and UN data. [sent-115, score-0.141]
</p><p>57 In total, our baseline model employs more than 50 features, including from our proposed dominance and orientation models. [sent-116, score-0.406]
</p><p>58 Lines 2-7 shows the results of the dominance model with O = 1− 6. [sent-118, score-0.294]
</p><p>59 The best BLEU, TER and Comb on each genre of the first set are in italic while those of the second set are in bold. [sent-123, score-0.071]
</p><p>60 features such as translation probabilities, we incorporate features that are found useful for developing a state-of-the-art baseline, such as the provenance features (Chiang et al. [sent-125, score-0.208]
</p><p>61 As for our string-todependency system, we train 3-gram models for left and right dependencies and unigram for head using the target side of the parallel corpus. [sent-129, score-0.1]
</p><p>62 As for the blind test set, we report the performance on the NIST MT08 evaluation set, which consists of 691 sentences from newswire and 666 sentences from weblog. [sent-133, score-0.047]
</p><p>63 The first set looks at the contribution of the dominance model with varying values of o. [sent-136, score-0.43]
</p><p>64 The second one looks at the combination of the dominance model and the orientation model. [sent-137, score-0.542]
</p><p>65 We report the results on newswire genre in columns a-c, those on weblog genre in column d-f, and those on mixed genre in column g-i. [sent-139, score-0.346]
</p><p>66 The performance of our baseline string-to-dependency syntaxbased SMT is shown in the first line. [sent-140, score-0.037]
</p><p>67 Lines 2-7 in Table 3 show the results of our first set of experiments, starting from the result of dom1, which looks at only at pairs of adjacent anchors, to the result of dom6, which looks at pairs of anchors that are at most 5 anchors away. [sent-141, score-1.35]
</p><p>68 As shown in line 2, our dominance model provides a nice improvement of around 0. [sent-142, score-0.366]
</p><p>69 5 point over the baseline even if it only looks at restricted context. [sent-143, score-0.136]
</p><p>70 Increasing the order of our dominance model provides an additional gain. [sent-144, score-0.294]
</p><p>71 However, the gain is more pronounced in the weblog genre (up to around 1 BLEU point) than in the newswire genre. [sent-145, score-0.236]
</p><p>72 We conjecture that this may be the artifact of our tune set, which comes from the weblog genre. [sent-146, score-0.086]
</p><p>73 Line 8 shows the result of adding the orientation model (ori) to the baseline system. [sent-149, score-0.112]
</p><p>74 We see a very encouraging result as adding  the dominance model increases the performance further, consistently over different value of o. [sent-152, score-0.294]
</p><p>75 This suggests that the dominance model is complementary to the orientation model. [sent-153, score-0.406]
</p><p>76 We see this result as confirming our intuition that the global contextual information provided by our AG model can significantly improve the performance of SMT even in a state-of-the-art system. [sent-155, score-0.122]
</p><p>77 In this section, we mainly focus on work related to introducing higher-order contextual information to reordering model. [sent-157, score-0.328]
</p><p>78 In providing global contextual information, our work is related to a large amount of literature. [sent-158, score-0.122]
</p><p>79 To name a few, Zens and Ney (2006) improves the lexicalized reordering model of Tillman (2004) by incorporating part-of-speech information. [sent-159, score-0.249]
</p><p>80 (201 1) introduces rule markov models for a forest-to-string model in which the number of possible derivations is restricted. [sent-166, score-0.096]
</p><p>81 Similar to these models, our proposed model also provide context dependencies to the application of translation rules, however, as they focus on minimal translation units (MTU) where we focus on a selected set oftranslation units. [sent-170, score-0.586]
</p><p>82 , 2005) introduces a bigram model for monotone phrasebased system, but their definition of translation units 510 is suitable only for language pairs with limited reordering, such as translating Spanish to English. [sent-172, score-0.335]
</p><p>83 In equating anchors with the function word class, our work is closely related to the function wordcentered model of Setiawan et al. [sent-173, score-0.539]
</p><p>84 Our dominance model is closely related to the reordering model of Setiawan et al. [sent-175, score-0.543]
</p><p>85 (2009), except that they only look at pair of adjacent anchors, forming a chain structure instead of  a graph like in our dominance model. [sent-176, score-0.332]
</p><p>86 9  Conclusion  We propose the “Anchor Graph” (AG) model to encode global contextual information. [sent-181, score-0.122]
</p><p>87 A selected set of translation units, which we call anchors, serves as the vertices of AG. [sent-182, score-0.208]
</p><p>88 As the models look at the pairs of anchors that go beyond multiple translation units, our AG model provides global contextual information. [sent-184, score-0.869]
</p><p>89 Our AG model embodies (admittedly crudely) some basic principles of sentence organization, namely categorization (in categorizing units into anchors and non-anchors), linear order (in modeling the precedence of anchors) and constituency structure (in modeling the dominance between anchors). [sent-185, score-1.073]
</p><p>90 We are encouraged by the facts that we learn these principles in an unsupervised way and that we can achieve a significant improvement over a strong baseline in a large-scale Chinese-to-English translation task. [sent-186, score-0.208]
</p><p>91 In the future, we hope to continue this line of research, perhaps by learning to identify anchors automatically from training data or by using our models to induce derivations directly from unaligned sentence pair. [sent-187, score-0.643]
</p><p>92 The views, opinions, and/or findings contained in this article/presentation are those of the author/presenter and should not be interpreted as representing the official views or policies, either expressed or implied, of the DARPA. [sent-189, score-0.046]
</p><p>93 Source-side dependency tree reordering models with subtree movements and constraints. [sent-192, score-0.249]
</p><p>94 Statistical machine translation of Euparl data by using bilingual n-grams. [sent-200, score-0.208]
</p><p>95 On hierarchical re-ordering and permutation parsing for phrase-based decoding. [sent-217, score-0.045]
</p><p>96 Model with minimal translation units, but decode with phrases. [sent-232, score-0.239]
</p><p>97 An efficient shift-reduce decoding algorithm for phrasedbased machine translation. [sent-241, score-0.041]
</p><p>98 Moses: Open source toolkit for statistical machine translation, June. [sent-258, score-0.042]
</p><p>99 Topological ordering of function words in hierarchical phrase-based translation. [sent-266, score-0.045]
</p><p>100 A new string-to-dependency machine translation algorithm with a target dependency language model. [sent-276, score-0.24]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('anchors', 0.539), ('dominance', 0.294), ('ag', 0.275), ('reordering', 0.249), ('translation', 0.208), ('anchor', 0.151), ('looks', 0.136), ('setiawan', 0.128), ('units', 0.127), ('ldom', 0.124), ('rdom', 0.124), ('orientation', 0.112), ('dom', 0.104), ('xd', 0.098), ('precedence', 0.086), ('weblog', 0.086), ('contextual', 0.079), ('oril', 0.074), ('hendra', 0.073), ('smt', 0.073), ('genre', 0.071), ('ml', 0.07), ('mr', 0.07), ('edges', 0.069), ('stack', 0.066), ('xa', 0.066), ('derivations', 0.064), ('spurious', 0.058), ('symbol', 0.057), ('shift', 0.053), ('xb', 0.052), ('banchs', 0.05), ('comb', 0.05), ('confine', 0.05), ('durrani', 0.05), ('mrs', 0.05), ('xc', 0.049), ('newswire', 0.047), ('haizhou', 0.047), ('operation', 0.047), ('views', 0.046), ('derivation', 0.046), ('stream', 0.045), ('hierarchical', 0.045), ('mls', 0.043), ('oftranslation', 0.043), ('global', 0.043), ('association', 0.043), ('source', 0.042), ('decoding', 0.041), ('line', 0.04), ('kitchawan', 0.039), ('radically', 0.039), ('index', 0.039), ('positions', 0.038), ('graph', 0.038), ('compound', 0.037), ('elementary', 0.037), ('vaswani', 0.037), ('bowen', 0.037), ('bach', 0.037), ('ori', 0.037), ('syntaxbased', 0.037), ('integrate', 0.036), ('ambiguity', 0.036), ('parallel', 0.036), ('zens', 0.036), ('parent', 0.035), ('issue', 0.035), ('broadcast', 0.035), ('greatest', 0.035), ('ter', 0.034), ('tag', 0.034), ('pages', 0.033), ('boulder', 0.033), ('hk', 0.033), ('michigan', 0.033), ('rule', 0.032), ('around', 0.032), ('contexts', 0.032), ('unigram', 0.032), ('target', 0.032), ('usa', 0.032), ('decode', 0.031), ('colorado', 0.031), ('actual', 0.03), ('arbor', 0.03), ('haitao', 0.03), ('liblinear', 0.03), ('ra', 0.029), ('libin', 0.029), ('xiong', 0.029), ('efficacy', 0.029), ('xiang', 0.028), ('road', 0.028), ('chiang', 0.028), ('shen', 0.028), ('ny', 0.027), ('namely', 0.027), ('partial', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="22-tfidf-1" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>Author: Hendra Setiawan ; Bowen Zhou ; Bing Xiang</p><p>Abstract: Reordering poses one of the greatest challenges in Statistical Machine Translation research as the key contextual information may well be beyond the confine oftranslation units. We present the “Anchor Graph” (AG) model where we use a graph structure to model global contextual information that is crucial for reordering. The key ingredient of our AG model is the edges that capture the relationship between the reordering around a set of selected translation units, which we refer to as anchors. As the edges link anchors that may span multiple translation units at decoding time, our AG model effectively encodes global contextual information that is previously absent. We integrate our proposed model into a state-of-the-art translation system and demonstrate the efficacy of our proposal in a largescale Chinese-to-English translation task.</p><p>2 0.26688519 <a title="22-tfidf-2" href="./emnlp-2013-Factored_Soft_Source_Syntactic_Constraints_for_Hierarchical_Machine_Translation.html">84 emnlp-2013-Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation</a></p>
<p>Author: Zhongqiang Huang ; Jacob Devlin ; Rabih Zbib</p><p>Abstract: Translation Jacob Devlin Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA j devl in@bbn . com Rabih Zbib Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA r zbib@bbn . com have tried to introduce grammaticality to the transThis paper describes a factored approach to incorporating soft source syntactic constraints into a hierarchical phrase-based translation system. In contrast to traditional approaches that directly introduce syntactic constraints to translation rules by explicitly decorating them with syntactic annotations, which often exacerbate the data sparsity problem and cause other problems, our approach keeps translation rules intact and factorizes the use of syntactic constraints through two separate models: 1) a syntax mismatch model that associates each nonterminal of a translation rule with a distribution of tags that is used to measure the degree of syntactic compatibility of the translation rule on source spans; 2) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language. The features produced by both models are used as soft constraints to guide the translation process. Experiments on Chinese-English translation show that the proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets.</p><p>3 0.18936223 <a title="22-tfidf-3" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>Author: Peng Li ; Yang Liu ; Maosong Sun</p><p>Abstract: While inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (i.e., straight and inverted) dependent on actual blocks being merged remains a challenge. Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points.</p><p>4 0.17961869 <a title="22-tfidf-4" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>Author: Xinyan Xiao ; Deyi Xiong</p><p>Abstract: Traditional synchronous grammar induction estimates parameters by maximizing likelihood, which only has a loose relation to translation quality. Alternatively, we propose a max-margin estimation approach to discriminatively inducing synchronous grammars for machine translation, which directly optimizes translation quality measured by BLEU. In the max-margin estimation of parameters, we only need to calculate Viterbi translations. This further facilitates the incorporation of various non-local features that are defined on the target side. We test the effectiveness of our max-margin estimation framework on a competitive hierarchical phrase-based system. Experiments show that our max-margin method significantly outperforms the traditional twostep pipeline for synchronous rule extraction by 1.3 BLEU points and is also better than previous max-likelihood estimation method.</p><p>5 0.16362655 <a title="22-tfidf-5" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>Author: Joern Wuebker ; Stephan Peitz ; Felix Rietig ; Hermann Ney</p><p>Abstract: Automatically clustering words from a monolingual or bilingual training corpus into classes is a widely used technique in statistical natural language processing. We present a very simple and easy to implement method for using these word classes to improve translation quality. It can be applied across different machine translation paradigms and with arbitrary types of models. We show its efficacy on a small German→English and a larger F ornenc ah s→mGalelrm Gaenrm mtarann→slEatniognli tsahsk a nwdit ha lbaortghe rst Farnednacrhd→ phrase-based salandti nhie traaskrch wiciathl phrase-based translation systems for a common set of models. Our results show that with word class models, the baseline can be improved by up to 1.4% BLEU and 1.0% TER on the French→German task and 0.3% BLEU aonnd t h1e .1 F%re nTcEhR→ on tehrem German→English Btask.</p><p>6 0.16147508 <a title="22-tfidf-6" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>7 0.15784347 <a title="22-tfidf-7" href="./emnlp-2013-Application_of_Localized_Similarity_for_Web_Documents.html">24 emnlp-2013-Application of Localized Similarity for Web Documents</a></p>
<p>8 0.14566773 <a title="22-tfidf-8" href="./emnlp-2013-Efficient_Left-to-Right_Hierarchical_Phrase-Based_Translation_with_Improved_Reordering.html">71 emnlp-2013-Efficient Left-to-Right Hierarchical Phrase-Based Translation with Improved Reordering</a></p>
<p>9 0.11317422 <a title="22-tfidf-9" href="./emnlp-2013-Shift-Reduce_Word_Reordering_for_Machine_Translation.html">171 emnlp-2013-Shift-Reduce Word Reordering for Machine Translation</a></p>
<p>10 0.11017966 <a title="22-tfidf-10" href="./emnlp-2013-What_is_Hidden_among_Translation_Rules.html">201 emnlp-2013-What is Hidden among Translation Rules</a></p>
<p>11 0.1008593 <a title="22-tfidf-11" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>12 0.098773874 <a title="22-tfidf-12" href="./emnlp-2013-Flexible_and_Efficient_Hypergraph_Interactions_for_Joint_Hierarchical_and_Forest-to-String_Decoding.html">88 emnlp-2013-Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest-to-String Decoding</a></p>
<p>13 0.094249859 <a title="22-tfidf-13" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>14 0.091787592 <a title="22-tfidf-14" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>15 0.091718905 <a title="22-tfidf-15" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>16 0.087958805 <a title="22-tfidf-16" href="./emnlp-2013-Max-Violation_Perceptron_and_Forced_Decoding_for_Scalable_MT_Training.html">128 emnlp-2013-Max-Violation Perceptron and Forced Decoding for Scalable MT Training</a></p>
<p>17 0.086003453 <a title="22-tfidf-17" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>18 0.076063417 <a title="22-tfidf-18" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>19 0.075859368 <a title="22-tfidf-19" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>20 0.073719203 <a title="22-tfidf-20" href="./emnlp-2013-Optimal_Beam_Search_for_Machine_Translation.html">145 emnlp-2013-Optimal Beam Search for Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.241), (1, -0.268), (2, 0.094), (3, 0.076), (4, 0.12), (5, -0.051), (6, -0.041), (7, -0.032), (8, 0.026), (9, 0.041), (10, -0.026), (11, 0.035), (12, -0.07), (13, -0.034), (14, -0.134), (15, 0.179), (16, 0.0), (17, -0.123), (18, -0.075), (19, 0.025), (20, -0.093), (21, -0.032), (22, 0.021), (23, -0.018), (24, 0.06), (25, -0.065), (26, 0.007), (27, 0.036), (28, 0.013), (29, -0.006), (30, 0.025), (31, 0.071), (32, -0.1), (33, 0.006), (34, -0.001), (35, 0.089), (36, 0.031), (37, 0.038), (38, -0.081), (39, -0.054), (40, -0.005), (41, -0.013), (42, 0.068), (43, -0.042), (44, 0.048), (45, 0.028), (46, -0.108), (47, 0.057), (48, 0.003), (49, -0.11)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90877616 <a title="22-lsi-1" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>Author: Hendra Setiawan ; Bowen Zhou ; Bing Xiang</p><p>Abstract: Reordering poses one of the greatest challenges in Statistical Machine Translation research as the key contextual information may well be beyond the confine oftranslation units. We present the “Anchor Graph” (AG) model where we use a graph structure to model global contextual information that is crucial for reordering. The key ingredient of our AG model is the edges that capture the relationship between the reordering around a set of selected translation units, which we refer to as anchors. As the edges link anchors that may span multiple translation units at decoding time, our AG model effectively encodes global contextual information that is previously absent. We integrate our proposed model into a state-of-the-art translation system and demonstrate the efficacy of our proposal in a largescale Chinese-to-English translation task.</p><p>2 0.76991469 <a title="22-lsi-2" href="./emnlp-2013-Factored_Soft_Source_Syntactic_Constraints_for_Hierarchical_Machine_Translation.html">84 emnlp-2013-Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation</a></p>
<p>Author: Zhongqiang Huang ; Jacob Devlin ; Rabih Zbib</p><p>Abstract: Translation Jacob Devlin Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA j devl in@bbn . com Rabih Zbib Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA r zbib@bbn . com have tried to introduce grammaticality to the transThis paper describes a factored approach to incorporating soft source syntactic constraints into a hierarchical phrase-based translation system. In contrast to traditional approaches that directly introduce syntactic constraints to translation rules by explicitly decorating them with syntactic annotations, which often exacerbate the data sparsity problem and cause other problems, our approach keeps translation rules intact and factorizes the use of syntactic constraints through two separate models: 1) a syntax mismatch model that associates each nonterminal of a translation rule with a distribution of tags that is used to measure the degree of syntactic compatibility of the translation rule on source spans; 2) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language. The features produced by both models are used as soft constraints to guide the translation process. Experiments on Chinese-English translation show that the proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets.</p><p>3 0.73659253 <a title="22-lsi-3" href="./emnlp-2013-Efficient_Left-to-Right_Hierarchical_Phrase-Based_Translation_with_Improved_Reordering.html">71 emnlp-2013-Efficient Left-to-Right Hierarchical Phrase-Based Translation with Improved Reordering</a></p>
<p>Author: Maryam Siahbani ; Baskaran Sankaran ; Anoop Sarkar</p><p>Abstract: Left-to-right (LR) decoding (Watanabe et al., 2006b) is a promising decoding algorithm for hierarchical phrase-based translation (Hiero). It generates the target sentence by extending the hypotheses only on the right edge. LR decoding has complexity O(n2b) for input of n words and beam size b, compared to O(n3) for the CKY algorithm. It requires a single language model (LM) history for each target hypothesis rather than two LM histories per hypothesis as in CKY. In this paper we present an augmented LR decoding algorithm that builds on the original algorithm in (Watanabe et al., 2006b). Unlike that algorithm, using experiments over multiple language pairs we show two new results: our LR decoding algorithm provides demonstrably more efficient decoding than CKY Hiero, four times faster; and by introducing new distortion and reordering features for LR decoding, it maintains the same translation quality (as in BLEU scores) ob- tained phrase-based and CKY Hiero with the same translation model.</p><p>4 0.70925331 <a title="22-lsi-4" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>Author: Uri Lerner ; Slav Petrov</p><p>Abstract: We present a simple and novel classifier-based preordering approach. Unlike existing preordering models, we train feature-rich discriminative classifiers that directly predict the target-side word order. Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discriminative model with a rich set of features, including lexical features. We present extensive experiments on 22 language pairs, including preordering into English from 7 other languages. We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task. For languages from different families the improvements often exceed 2 BLEU. Many of these gains are also significant in human evaluations.</p><p>5 0.69433659 <a title="22-lsi-5" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>Author: Peng Li ; Yang Liu ; Maosong Sun</p><p>Abstract: While inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (i.e., straight and inverted) dependent on actual blocks being merged remains a challenge. Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points.</p><p>6 0.67717063 <a title="22-lsi-6" href="./emnlp-2013-Shift-Reduce_Word_Reordering_for_Machine_Translation.html">171 emnlp-2013-Shift-Reduce Word Reordering for Machine Translation</a></p>
<p>7 0.64666229 <a title="22-lsi-7" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>8 0.60525429 <a title="22-lsi-8" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>9 0.54115081 <a title="22-lsi-9" href="./emnlp-2013-Flexible_and_Efficient_Hypergraph_Interactions_for_Joint_Hierarchical_and_Forest-to-String_Decoding.html">88 emnlp-2013-Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest-to-String Decoding</a></p>
<p>10 0.54014963 <a title="22-lsi-10" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>11 0.53683901 <a title="22-lsi-11" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>12 0.4889012 <a title="22-lsi-12" href="./emnlp-2013-What_is_Hidden_among_Translation_Rules.html">201 emnlp-2013-What is Hidden among Translation Rules</a></p>
<p>13 0.428931 <a title="22-lsi-13" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>14 0.42179948 <a title="22-lsi-14" href="./emnlp-2013-Application_of_Localized_Similarity_for_Web_Documents.html">24 emnlp-2013-Application of Localized Similarity for Web Documents</a></p>
<p>15 0.41568881 <a title="22-lsi-15" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>16 0.41239539 <a title="22-lsi-16" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>17 0.41079304 <a title="22-lsi-17" href="./emnlp-2013-Optimal_Beam_Search_for_Machine_Translation.html">145 emnlp-2013-Optimal Beam Search for Machine Translation</a></p>
<p>18 0.37505293 <a title="22-lsi-18" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>19 0.356511 <a title="22-lsi-19" href="./emnlp-2013-Boosting_Cross-Language_Retrieval_by_Learning_Bilingual_Phrase_Associations_from_Relevance_Rankings.html">39 emnlp-2013-Boosting Cross-Language Retrieval by Learning Bilingual Phrase Associations from Relevance Rankings</a></p>
<p>20 0.35121861 <a title="22-lsi-20" href="./emnlp-2013-A_Corpus_Level_MIRA_Tuning_Strategy_for_Machine_Translation.html">3 emnlp-2013-A Corpus Level MIRA Tuning Strategy for Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.029), (10, 0.012), (18, 0.046), (22, 0.051), (30, 0.103), (45, 0.013), (50, 0.014), (51, 0.157), (55, 0.28), (66, 0.057), (71, 0.031), (75, 0.022), (77, 0.087), (96, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83816701 <a title="22-lda-1" href="./emnlp-2013-Recurrent_Continuous_Translation_Models.html">156 emnlp-2013-Recurrent Continuous Translation Models</a></p>
<p>Author: Nal Kalchbrenner ; Phil Blunsom</p><p>Abstract: We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.</p><p>2 0.82598954 <a title="22-lda-2" href="./emnlp-2013-Open-Domain_Fine-Grained_Class_Extraction_from_Web_Search_Queries.html">142 emnlp-2013-Open-Domain Fine-Grained Class Extraction from Web Search Queries</a></p>
<p>Author: Marius Pasca</p><p>Abstract: This paper introduces a method for extracting fine-grained class labels ( “countries with double taxation agreements with india ”) from Web search queries. The class labels are more numerous and more diverse than those produced by current extraction methods. Also extracted are representative sets of instances (singapore, united kingdom) for the class labels.</p><p>same-paper 3 0.7835176 <a title="22-lda-3" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>Author: Hendra Setiawan ; Bowen Zhou ; Bing Xiang</p><p>Abstract: Reordering poses one of the greatest challenges in Statistical Machine Translation research as the key contextual information may well be beyond the confine oftranslation units. We present the “Anchor Graph” (AG) model where we use a graph structure to model global contextual information that is crucial for reordering. The key ingredient of our AG model is the edges that capture the relationship between the reordering around a set of selected translation units, which we refer to as anchors. As the edges link anchors that may span multiple translation units at decoding time, our AG model effectively encodes global contextual information that is previously absent. We integrate our proposed model into a state-of-the-art translation system and demonstrate the efficacy of our proposal in a largescale Chinese-to-English translation task.</p><p>4 0.60804194 <a title="22-lda-4" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>Author: Ann Irvine ; Chris Quirk ; Hal Daume III</p><p>Abstract: When using a machine translation (MT) model trained on OLD-domain parallel data to translate NEW-domain text, one major challenge is the large number of out-of-vocabulary (OOV) and new-translation-sense words. We present a method to identify new translations of both known and unknown source language words that uses NEW-domain comparable document pairs. Starting with a joint distribution of source-target word pairs derived from the OLD-domain parallel corpus, our method recovers a new joint distribution that matches the marginal distributions of the NEW-domain comparable document pairs, while minimizing the divergence from the OLD-domain distribution. Adding learned translations to our French-English MT model results in gains of about 2 BLEU points over strong baselines.</p><p>5 0.60787725 <a title="22-lda-5" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>Author: Fandong Meng ; Jun Xie ; Linfeng Song ; Yajuan Lu ; Qun Liu</p><p>Abstract: We present a novel translation model, which simultaneously exploits the constituency and dependency trees on the source side, to combine the advantages of two types of trees. We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. Our rules hold the property of long distance reorderings and the compatibility with phrases. Large-scale experimental results show that our model achieves significantly improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets.</p><p>6 0.6051228 <a title="22-lda-6" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>7 0.60369509 <a title="22-lda-7" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>8 0.60304779 <a title="22-lda-8" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>9 0.60139567 <a title="22-lda-9" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>10 0.6010778 <a title="22-lda-10" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>11 0.59889752 <a title="22-lda-11" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>12 0.59850633 <a title="22-lda-12" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>13 0.59598994 <a title="22-lda-13" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>14 0.59258187 <a title="22-lda-14" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>15 0.59014732 <a title="22-lda-15" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>16 0.58925301 <a title="22-lda-16" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>17 0.58539701 <a title="22-lda-17" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>18 0.58444017 <a title="22-lda-18" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>19 0.58391017 <a title="22-lda-19" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>20 0.58261698 <a title="22-lda-20" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
