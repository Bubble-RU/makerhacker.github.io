<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>199 emnlp-2013-Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-199" href="#">emnlp2013-199</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>199 emnlp-2013-Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students</h1>
<br/><p>Source: <a title="emnlp-2013-199-pdf" href="http://aclweb.org/anthology//D/D13/D13-1133.pdf">pdf</a></p><p>Author: Philip Resnik ; Anderson Garron ; Rebecca Resnik</p><p>Abstract: in College Students Anderson Garron University of Maryland College Park, MD 20742 agarron@cs.umd.edu Rebecca Resnik Mindwell Psychology Bethesda 5602 Shields Drive Bethesda, MD 20817 drrebeccaresnik@gmail.com out adequate insurance or in rural areas – cannot ac- We investigate the value-add of topic modeling in text analysis for depression, and for neuroticism as a strongly associated personality measure. Using Pennebaker’s Linguistic Inquiry and Word Count (LIWC) lexicon to provide baseline features, we show that straightforward topic modeling using Latent Dirichlet Allocation (LDA) yields interpretable, psychologically relevant “themes” that add value in prediction of clinical assessments.</p><p>Reference: <a title="emnlp-2013-199-reference" href="../emnlp2013_reference/emnlp-2013-Using_Topic_Modeling_to_Improve_Prediction_of_Neuroticism_and_Depression_in_College_Students_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  out adequate insurance or in rural areas  –  cannot ac-  We investigate the value-add of topic modeling in text analysis for depression, and for neuroticism as a strongly associated personality  measure. [sent-5, score-0.642]
</p><p>2 Using Pennebaker’s Linguistic Inquiry and Word Count (LIWC) lexicon to provide baseline features, we show that straightforward topic modeling using Latent Dirichlet Allocation (LDA) yields interpretable, psychologically relevant “themes” that add value in prediction of clinical assessments. [sent-6, score-0.313]
</p><p>3 1 Introduction In the United States, where 25 million adults per year suffer a major depressive episode (NAMI, 2013), identifying people with mental health problems is a key challenge. [sent-7, score-0.186]
</p><p>4 For clinical psychologists, language plays a central role in diagnosis: many clinical instruments fundamentally rely on what is, in effect, manual coding of patient language. [sent-8, score-0.466]
</p><p>5 Automating language assessment in this domain potentially has enormous impact, for two reasons. [sent-9, score-0.076]
</p><p>6 those with–  1As evidenced by the fact that assessments such as the MMPI-2RF include validity scales to detect, e. [sent-18, score-0.078]
</p><p>7 , defensiveness, atyp1348 cess a clinician qualified to perform a psychological evaluation (Sibelius, 2013; APA, 2013). [sent-20, score-0.057]
</p><p>8 There is enormous value in inexpensive screening measures that could be administered in primary care and by social workers and other providers. [sent-21, score-0.137]
</p><p>9 We take as a starting point the well known lexicon-driven methods of Pennebaker and colleagues (LIWC, Pennebaker and King (1999)), which relate language use to psychological variables, and improve on them straightforwardly using topic modeling (LDA, Blei et al. [sent-22, score-0.121]
</p><p>10 First, we show that taking automatically derived topics into account improves prediction of neuroticism (emotional instability, John and Srivastava (1999)), as measured by correlation with widely used clinical instruments, when compared with lexically-based prediction alone. [sent-24, score-0.563]
</p><p>11 2 Second, we show a similar correlation improvement result for prediction of depression, adding improvement ical responses, and overly positive self-portrayals (Tellegen et al. [sent-26, score-0.043]
</p><p>12 2The Diagnostic and Statistical Manual of Mental Disorders is a widely used organization of mental health conditions; it served as the standard for diagnosis from 1994 until the release of the (quite controversial) DSM-5 in May, 2013. [sent-28, score-0.206]
</p><p>13 Axis Iin the DSM-IV includes all the major diagnostic categories, excluding mental retardation and personality disorders. [sent-29, score-0.373]
</p><p>14 oc d2s0 i1n3 N Aastusorcaila Ltiaonng fuoarg Ceo Pmrpoucetastsi onnga,l p Laignegsu 1is3t4ic8s–1353, on precision (with no decrease in recall), as well as comparison with human performance by clinical psychologists. [sent-32, score-0.168]
</p><p>15 Third, we show that LDA has identified meaningful, population-specific themes that go beyond the pre-defined LIWC categories and are psychologically relevant. [sent-33, score-0.221]
</p><p>16 We utilize a collection of 6,459 stream-ofconsciousness essays collected from college students by Pennebaker and King (1999) between 1997 and 2008, averaging approximately 780 words each. [sent-36, score-0.191]
</p><p>17 Students were asked to think about their thoughts, sensations, and feelings in the moment and “write your thoughts as they come to you”. [sent-37, score-0.046]
</p><p>18 Each essay is accompanied by metadata for its author, which includes scores for the Big-5 personality traits (John and Srivastava, 1999): agreeableness, conscientiousness, extraversion, neuroticism, and openness. [sent-38, score-0.346]
</p><p>19 Because Big-5 assessment can be done using a variety of different survey instruments (John et al. [sent-39, score-0.131]
</p><p>20 , 2008), and different instruments were used from year to year, we treat the data from each year as an independent dataset. [sent-40, score-0.18]
</p><p>21 This resulted in a dataset containing 4,777 essays with associated Big-5 metadata. [sent-44, score-0.075]
</p><p>22 For instance, the anger category contains 190 word patterns specifying, for example, words descriptive of contexts involving anger (e. [sent-56, score-0.149]
</p><p>23 We also explored including essays’ average sentence length and total wordcount, as an initial proxy for language complexity, which often figures into psychological assessments. [sent-61, score-0.057]
</p><p>24 We use vanilla LDA as implemented in the Mallet toolkit (McCallum, 2002), developing a k-topic model onjust training documents,  and using the posterior topic distribution for each training and test document as a set of k features. [sent-64, score-0.064]
</p><p>25 We utilize linear regression in the WEKA toolkit (Hall et al. [sent-70, score-0.033]
</p><p>26 , 2009), estimated on training documents, to predict the neuroticism score associated with the author of each test document. [sent-71, score-0.343]
</p><p>27 2 Results Table 1 shows the quality of prediction via linear regression, averaged over the eleven datasets, 1997 through 2008, using Pearson correlation (r) as the evaluation metric. [sent-73, score-0.043]
</p><p>28 5 A first thing to observe is that the multiple regressions using all LIWC categories produce much  stronger correlations with neuroticism than the individual category correlations reported by Pennebaker and  King. [sent-76, score-0.395]
</p><p>29 6  There the strongest  individual corre-  lations with neuroticism for any LIWC categories are . [sent-77, score-0.352]
</p><p>30 13 (positive emotion words), though it should be noted that their goal was to validate their categories as a meaningful 3Using richer measures of complexity, e. [sent-79, score-0.137]
</p><p>31 4In previous work we have found that multiple linear regression is competitive with more complicated techniques such as SVM regression, though we plan to explore the latter in future work. [sent-83, score-0.033]
</p><p>32 5Full year-by-year data appears in supplemental materials at http : / /umiac s . [sent-84, score-0.067]
</p><p>33 02  way to explore personality  differences, not predic-  tion. [sent-100, score-0.231]
</p><p>34 05) establish that, in comparing the cross-year averages, augmenting LIWC with topic features improves average correlation significantly  over using  LIWC features alone for 30, 40, and 50 topics. [sent-102, score-0.095]
</p><p>35 LDA features alone do not improve significantly over LIWC. [sent-103, score-0.031]
</p><p>36 1; in this case, students were (a2s0ke0d4 )t soi m“dielasrclyrib toe your deepest thoughts eanntds wfeeerleings about being in college”. [sent-108, score-0.091]
</p><p>37 Each essay is accompanied by the author’s Beck Depression Inventory score (BDI). [sent-109, score-0.062]
</p><p>38 , 1961) is a widely used 21-question instrument that correlates strongly with ratings of depression by psychiatrists. [sent-111, score-0.338]
</p><p>39 We created a set of expert human results for comparison by asking three practicing clinical psychologists to review the test documents and rate whether or not the author is suffer7Each question contributes a score value from 0 to 3, so BDI scores range from 0 to 63. [sent-125, score-0.361]
</p><p>40 1 plus the BDI training items, using the posterior topic dluisstr thibeut BioDnIs t as fienagtu irteems as isnin §2. [sent-132, score-0.064]
</p><p>41 2  Results  Regression on LIWC features alone achieved r = . [sent-140, score-0.031]
</p><p>42 288, and adding topic features improved this substantially to r = . [sent-141, score-0.064]
</p><p>43 tDhIat ≥ an a4ut ahsor th hise depressed), Table 2 shows that adding topic features improves precision without harming recall. [sent-146, score-0.064]
</p><p>44 Automatic prediction is more conservative than human ratings, trading recall for precision to achieve comparable F-measure on this test set. [sent-147, score-0.043]
</p><p>45 9 8These psychologists all have doctoral degrees, are licensed, and spend significant time primarily in assessment and diagnosis of psychological disorders. [sent-148, score-0.281]
</p><p>46 These are challenging questions, and the ability to trade off precision versus recall more flexibly is a topic we are interested in investigating in future work. [sent-152, score-0.064]
</p><p>47 4  Qualitative themes  In order to explore uncovered  the relevance  of the themes  by LDA, the third author, a practicing  clinical psychologist,  reviewed the 50 LDA cate-  gories created in §3. [sent-169, score-0.492]
</p><p>48 words, was given a  Then, for each category, she  was asked: “If you were conducting a clinical interview, would observing these themes in a patient’s responses make you more (less) likely to feel that the patient merited further evaluation for depression? [sent-172, score-0.35]
</p><p>49 10 These capture population-specific properties in ways that LIWC cannot for example, although LIWC does have a body category, it does not have a category that corresponds to somatic complaints, which often co-occur with depression. [sent-174, score-0.043]
</p><p>50 tired, would be captured in LIWC’s body, bio, and/or health category, but the LDA theme corresponding to low energy or lack of sleep, another potential depression cue, contains words that make sense there only in context (e. [sent-177, score-0.395]
</p><p>51 —  5  Related Work  The application of NLP to psychological variables has seen a recent uptick in community activity. [sent-181, score-0.057]
</p><p>52 One  recent shared task brings together research on the Big-5 personality traits (Celli et al. [sent-182, score-0.284]
</p><p>53 , 2013), and another involved research on identification of emotion in suicide notes (Pestian et al. [sent-184, score-0.132]
</p><p>54 Other examples include NLP research on 10All 50 can be found in the supplemental materials at http : / /umiacs . [sent-186, score-0.067]
</p><p>55 3 1351 autistic spectrum disorders (Van Santen et al. [sent-189, score-0.096]
</p><p>56 (2012) develop a corpus-based “depression lexicon” and produce promising screening results, and De Choudhury et al. [sent-197, score-0.058]
</p><p>57 (2013) predict social network behavior changes related to post-partum depression. [sent-198, score-0.046]
</p><p>58 Neither, however, evaluates using formal instruments for clinical assessment. [sent-199, score-0.256]
</p><p>59 (2012), who use LIWC to provide priors for corpus-specific emotion categories; Stark et al. [sent-201, score-0.094]
</p><p>60 (2012), who combine LIWC and LDA-based  features in classification of social relationships; and Schwartz et al. [sent-202, score-0.046]
</p><p>61 6  Conclusions  In this paper, we have aimed for a small, focused contribution, investigating the value-add of topic modeling in text analysis for depression, and for neuroticism as a strongly associated personality measure. [sent-204, score-0.604]
</p><p>62 (2012) employ topic-based features for prediction in a supervised setting. [sent-207, score-0.043]
</p><p>63 Rather, our contribution here has been to show that topic models can get us beyond the LIWC categories to relevant, population-specific themes related to neuroticism and depression, and to support that claim using evaluation against formal clinical assessments. [sent-208, score-0.724]
</p><p>64 (2004), to the three psychologists who kindly took the time to provide human ratings, and to our reviewers for helpful comments. [sent-217, score-0.115]
</p><p>65 Predicting postpartum changes in emotion and behavior via social media. [sent-283, score-0.14]
</p><p>66 The big five trait taxonomy: History, measurement, and theoretical perspectives. [sent-293, score-0.044]
</p><p>67 Paradigm shift to the integrative big five trait taxonomy: History, measurement, and conceptual issues. [sent-304, score-0.044]
</p><p>68 Private traits and attributes are predictable from digital records of human behavior. [sent-309, score-0.053]
</p><p>69 Fully automated neuropsychological assess-  ment for detecting mild cognitive impairment. [sent-314, score-0.049]
</p><p>70 Proactive screening for depression through metaphorical and automatic 1352  text analysis. [sent-339, score-0.366]
</p><p>71 Computerized assessment of syntactic complexity in alzheimers disease: a case study of iris murdochs writ-  ing. [sent-352, score-0.043]
</p><p>72 Spoken language derived measures for detecting mild cognitive impairment. [sent-372, score-0.049]
</p><p>73 lda: a flexible large scale topic modeling package using variational inference in mapreduce. [sent-419, score-0.064]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('liwc', 0.486), ('neuroticism', 0.309), ('depression', 0.308), ('personality', 0.231), ('bdi', 0.177), ('pennebaker', 0.175), ('clinical', 0.168), ('lda', 0.146), ('themes', 0.14), ('rude', 0.133), ('lehr', 0.115), ('psychologists', 0.115), ('disorders', 0.096), ('emotion', 0.094), ('kosinski', 0.088), ('pakhomov', 0.088), ('prudhommeaux', 0.088), ('instruments', 0.088), ('mental', 0.084), ('essays', 0.075), ('college', 0.071), ('stark', 0.07), ('artstein', 0.066), ('batista', 0.066), ('celli', 0.066), ('diagnosis', 0.066), ('neuman', 0.066), ('pestian', 0.066), ('psychiatric', 0.066), ('santen', 0.066), ('tellegen', 0.066), ('topic', 0.064), ('beck', 0.059), ('diagnostic', 0.058), ('screening', 0.058), ('izhak', 0.058), ('shafran', 0.058), ('mallet', 0.058), ('psychological', 0.057), ('health', 0.056), ('blei', 0.054), ('choudhury', 0.053), ('traits', 0.053), ('rater', 0.053), ('anger', 0.053), ('emily', 0.052), ('roark', 0.05), ('mild', 0.049), ('king', 0.049), ('zhai', 0.049), ('year', 0.046), ('thoughts', 0.046), ('social', 0.046), ('students', 0.045), ('brian', 0.045), ('assessments', 0.044), ('bethesda', 0.044), ('depressed', 0.044), ('practicing', 0.044), ('trait', 0.044), ('margaret', 0.044), ('schwartz', 0.044), ('assessment', 0.043), ('categories', 0.043), ('category', 0.043), ('prediction', 0.043), ('patient', 0.042), ('inventory', 0.04), ('lois', 0.038), ('suicide', 0.038), ('psychologically', 0.038), ('supplemental', 0.038), ('stillwell', 0.038), ('hommeaux', 0.038), ('maider', 0.038), ('prud', 0.038), ('mock', 0.038), ('rural', 0.038), ('minnesota', 0.038), ('nguyen', 0.038), ('bird', 0.038), ('park', 0.036), ('md', 0.035), ('edition', 0.035), ('scales', 0.034), ('resnik', 0.034), ('author', 0.034), ('regression', 0.033), ('john', 0.033), ('axis', 0.033), ('accompanied', 0.033), ('fabio', 0.033), ('enormous', 0.033), ('michal', 0.031), ('energy', 0.031), ('alone', 0.031), ('ratings', 0.03), ('essay', 0.029), ('materials', 0.029), ('weka', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="199-tfidf-1" href="./emnlp-2013-Using_Topic_Modeling_to_Improve_Prediction_of_Neuroticism_and_Depression_in_College_Students.html">199 emnlp-2013-Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students</a></p>
<p>Author: Philip Resnik ; Anderson Garron ; Rebecca Resnik</p><p>Abstract: in College Students Anderson Garron University of Maryland College Park, MD 20742 agarron@cs.umd.edu Rebecca Resnik Mindwell Psychology Bethesda 5602 Shields Drive Bethesda, MD 20817 drrebeccaresnik@gmail.com out adequate insurance or in rural areas – cannot ac- We investigate the value-add of topic modeling in text analysis for depression, and for neuroticism as a strongly associated personality measure. Using Pennebaker’s Linguistic Inquiry and Word Count (LIWC) lexicon to provide baseline features, we show that straightforward topic modeling using Latent Dirichlet Allocation (LDA) yields interpretable, psychologically relevant “themes” that add value in prediction of clinical assessments.</p><p>2 0.073619939 <a title="199-tfidf-2" href="./emnlp-2013-Using_Soft_Constraints_in_Joint_Inference_for_Clinical_Concept_Recognition.html">198 emnlp-2013-Using Soft Constraints in Joint Inference for Clinical Concept Recognition</a></p>
<p>Author: Prateek Jindal ; Dan Roth</p><p>Abstract: This paper introduces IQPs (Integer Quadratic Programs) as a way to model joint inference for the task of concept recognition in clinical domain. IQPs make it possible to easily incorporate soft constraints in the optimization framework and still support exact global inference. We show that soft constraints give statistically significant performance improvements when compared to hard constraints.</p><p>3 0.068646133 <a title="199-tfidf-3" href="./emnlp-2013-Automated_Essay_Scoring_by_Maximizing_Human-Machine_Agreement.html">28 emnlp-2013-Automated Essay Scoring by Maximizing Human-Machine Agreement</a></p>
<p>Author: Hongbo Chen ; Ben He</p><p>Abstract: Previous approaches for automated essay scoring (AES) learn a rating model by minimizing either the classification, regression, or pairwise classification loss, depending on the learning algorithm used. In this paper, we argue that the current AES systems can be further improved by taking into account the agreement between human and machine raters. To this end, we propose a rankbased approach that utilizes listwise learning to rank algorithms for learning a rating model, where the agreement between the human and machine raters is directly incorporated into the loss function. Various linguistic and statistical features are utilized to facilitate the learning algorithms. Experiments on the publicly available English essay dataset, Automated Student Assessment Prize (ASAP), show that our proposed approach outperforms the state-of-the-art algorithms, and achieves performance comparable to professional human raters, which suggests the effectiveness of our proposed method for automated essay scoring.</p><p>4 0.063744359 <a title="199-tfidf-4" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>Author: Zhiyuan Chen ; Arjun Mukherjee ; Bing Liu ; Meichun Hsu ; Malu Castellanos ; Riddhiman Ghosh</p><p>Abstract: Aspect extraction is one of the key tasks in sentiment analysis. In recent years, statistical models have been used for the task. However, such models without any domain knowledge often produce aspects that are not interpretable in applications. To tackle the issue, some knowledge-based topic models have been proposed, which allow the user to input some prior domain knowledge to generate coherent aspects. However, existing knowledge-based topic models have several major shortcomings, e.g., little work has been done to incorporate the cannot-link type of knowledge or to automatically adjust the number of topics based on domain knowledge. This paper proposes a more advanced topic model, called MC-LDA (LDA with m-set and c-set), to address these problems, which is based on an Extended generalized Pólya urn (E-GPU) model (which is also proposed in this paper). Experiments on real-life product reviews from a variety of domains show that MCLDA outperforms the existing state-of-the-art models markedly.</p><p>5 0.046789665 <a title="199-tfidf-5" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>Author: Oier Lopez de Lacalle ; Mirella Lapata</p><p>Abstract: In this paper we present an unsupervised approach to relational information extraction. Our model partitions tuples representing an observed syntactic relationship between two named entities (e.g., “X was born in Y” and “X is from Y”) into clusters corresponding to underlying semantic relation types (e.g., BornIn, Located). Our approach incorporates general domain knowledge which we encode as First Order Logic rules and automatically combine with a topic model developed specifically for the relation extraction task. Evaluation results on the ACE 2007 English Relation Detection and Categorization (RDC) task show that our model outperforms competitive unsupervised approaches by a wide margin and is able to produce clusters shaped by both the data and the rules.</p><p>6 0.045701031 <a title="199-tfidf-6" href="./emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM.html">99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</a></p>
<p>7 0.045339685 <a title="199-tfidf-7" href="./emnlp-2013-Improvements_to_the_Bayesian_Topic_N-Gram_Models.html">100 emnlp-2013-Improvements to the Bayesian Topic N-Gram Models</a></p>
<p>8 0.045068558 <a title="199-tfidf-8" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<p>9 0.040155619 <a title="199-tfidf-9" href="./emnlp-2013-Learning_Topics_and_Positions_from_Debatepedia.html">121 emnlp-2013-Learning Topics and Positions from Debatepedia</a></p>
<p>10 0.039867554 <a title="199-tfidf-10" href="./emnlp-2013-Classifying_Message_Board_Posts_with_an_Extracted_Lexicon_of_Patient_Attributes.html">46 emnlp-2013-Classifying Message Board Posts with an Extracted Lexicon of Patient Attributes</a></p>
<p>11 0.039674927 <a title="199-tfidf-11" href="./emnlp-2013-A_Generative_Joint%2C_Additive%2C_Sequential_Model_of_Topics_and_Speech_Acts_in_Patient-Doctor_Communication.html">6 emnlp-2013-A Generative Joint, Additive, Sequential Model of Topics and Speech Acts in Patient-Doctor Communication</a></p>
<p>12 0.038864583 <a title="199-tfidf-12" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>13 0.036031362 <a title="199-tfidf-13" href="./emnlp-2013-Modeling_Scientific_Impact_with_Topical_Influence_Regression.html">133 emnlp-2013-Modeling Scientific Impact with Topical Influence Regression</a></p>
<p>14 0.033904858 <a title="199-tfidf-14" href="./emnlp-2013-A_Unified_Model_for_Topics%2C_Events_and_Users_on_Twitter.html">16 emnlp-2013-A Unified Model for Topics, Events and Users on Twitter</a></p>
<p>15 0.030151047 <a title="199-tfidf-15" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>16 0.02960518 <a title="199-tfidf-16" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>17 0.028941322 <a title="199-tfidf-17" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>18 0.028715501 <a title="199-tfidf-18" href="./emnlp-2013-Application_of_Localized_Similarity_for_Web_Documents.html">24 emnlp-2013-Application of Localized Similarity for Web Documents</a></p>
<p>19 0.028645795 <a title="199-tfidf-19" href="./emnlp-2013-A_Log-Linear_Model_for_Unsupervised_Text_Normalization.html">9 emnlp-2013-A Log-Linear Model for Unsupervised Text Normalization</a></p>
<p>20 0.028407766 <a title="199-tfidf-20" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.105), (1, 0.037), (2, -0.054), (3, -0.016), (4, -0.002), (5, 0.005), (6, 0.03), (7, 0.014), (8, -0.006), (9, -0.04), (10, -0.065), (11, -0.05), (12, -0.052), (13, 0.053), (14, 0.055), (15, 0.041), (16, 0.069), (17, -0.002), (18, -0.027), (19, -0.016), (20, -0.008), (21, 0.104), (22, 0.025), (23, 0.048), (24, -0.002), (25, 0.035), (26, -0.004), (27, -0.03), (28, -0.033), (29, 0.015), (30, -0.006), (31, -0.033), (32, -0.087), (33, 0.013), (34, -0.016), (35, 0.013), (36, 0.035), (37, 0.052), (38, 0.111), (39, 0.01), (40, 0.128), (41, -0.211), (42, -0.022), (43, 0.007), (44, -0.143), (45, 0.015), (46, -0.01), (47, -0.027), (48, -0.016), (49, 0.092)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91687626 <a title="199-lsi-1" href="./emnlp-2013-Using_Topic_Modeling_to_Improve_Prediction_of_Neuroticism_and_Depression_in_College_Students.html">199 emnlp-2013-Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students</a></p>
<p>Author: Philip Resnik ; Anderson Garron ; Rebecca Resnik</p><p>Abstract: in College Students Anderson Garron University of Maryland College Park, MD 20742 agarron@cs.umd.edu Rebecca Resnik Mindwell Psychology Bethesda 5602 Shields Drive Bethesda, MD 20817 drrebeccaresnik@gmail.com out adequate insurance or in rural areas – cannot ac- We investigate the value-add of topic modeling in text analysis for depression, and for neuroticism as a strongly associated personality measure. Using Pennebaker’s Linguistic Inquiry and Word Count (LIWC) lexicon to provide baseline features, we show that straightforward topic modeling using Latent Dirichlet Allocation (LDA) yields interpretable, psychologically relevant “themes” that add value in prediction of clinical assessments.</p><p>2 0.65240419 <a title="199-lsi-2" href="./emnlp-2013-Automated_Essay_Scoring_by_Maximizing_Human-Machine_Agreement.html">28 emnlp-2013-Automated Essay Scoring by Maximizing Human-Machine Agreement</a></p>
<p>Author: Hongbo Chen ; Ben He</p><p>Abstract: Previous approaches for automated essay scoring (AES) learn a rating model by minimizing either the classification, regression, or pairwise classification loss, depending on the learning algorithm used. In this paper, we argue that the current AES systems can be further improved by taking into account the agreement between human and machine raters. To this end, we propose a rankbased approach that utilizes listwise learning to rank algorithms for learning a rating model, where the agreement between the human and machine raters is directly incorporated into the loss function. Various linguistic and statistical features are utilized to facilitate the learning algorithms. Experiments on the publicly available English essay dataset, Automated Student Assessment Prize (ASAP), show that our proposed approach outperforms the state-of-the-art algorithms, and achieves performance comparable to professional human raters, which suggests the effectiveness of our proposed method for automated essay scoring.</p><p>3 0.48859963 <a title="199-lsi-3" href="./emnlp-2013-Using_Soft_Constraints_in_Joint_Inference_for_Clinical_Concept_Recognition.html">198 emnlp-2013-Using Soft Constraints in Joint Inference for Clinical Concept Recognition</a></p>
<p>Author: Prateek Jindal ; Dan Roth</p><p>Abstract: This paper introduces IQPs (Integer Quadratic Programs) as a way to model joint inference for the task of concept recognition in clinical domain. IQPs make it possible to easily incorporate soft constraints in the optimization framework and still support exact global inference. We show that soft constraints give statistically significant performance improvements when compared to hard constraints.</p><p>4 0.46816084 <a title="199-lsi-4" href="./emnlp-2013-Modeling_Scientific_Impact_with_Topical_Influence_Regression.html">133 emnlp-2013-Modeling Scientific Impact with Topical Influence Regression</a></p>
<p>Author: James Foulds ; Padhraic Smyth</p><p>Abstract: When reviewing scientific literature, it would be useful to have automatic tools that identify the most influential scientific articles as well as how ideas propagate between articles. In this context, this paper introduces topical influence, a quantitative measure of the extent to which an article tends to spread its topics to the articles that cite it. Given the text of the articles and their citation graph, we show how to learn a probabilistic model to recover both the degree of topical influence of each article and the influence relationships between articles. Experimental results on corpora from two well-known computer science conferences are used to illustrate and validate the proposed approach.</p><p>5 0.4470025 <a title="199-lsi-5" href="./emnlp-2013-Well-Argued_Recommendation%3A_Adaptive_Models_Based_on_Words_in_Recommender_Systems.html">200 emnlp-2013-Well-Argued Recommendation: Adaptive Models Based on Words in Recommender Systems</a></p>
<p>Author: Julien Gaillard ; Marc El-Beze ; Eitan Altman ; Emmanuel Ethis</p><p>Abstract: Recommendation systems (RS) take advantage ofproducts and users information in order to propose items to consumers. Collaborative, content-based and a few hybrid RS have been developed in the past. In contrast, we propose a new domain-independent semantic RS. By providing textually well-argued recommendations, we aim to give more responsibility to the end user in his decision. The system includes a new similarity measure keeping up both the accuracy of rating predictions and coverage. We propose an innovative way to apply a fast adaptation scheme at a semantic level, providing recommendations and arguments in phase with the very recent past. We have performed several experiments on films data, providing textually well-argued recommendations.</p><p>6 0.42427012 <a title="199-lsi-6" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>7 0.42406848 <a title="199-lsi-7" href="./emnlp-2013-Improvements_to_the_Bayesian_Topic_N-Gram_Models.html">100 emnlp-2013-Improvements to the Bayesian Topic N-Gram Models</a></p>
<p>8 0.40926233 <a title="199-lsi-8" href="./emnlp-2013-A_Generative_Joint%2C_Additive%2C_Sequential_Model_of_Topics_and_Speech_Acts_in_Patient-Doctor_Communication.html">6 emnlp-2013-A Generative Joint, Additive, Sequential Model of Topics and Speech Acts in Patient-Doctor Communication</a></p>
<p>9 0.40328759 <a title="199-lsi-9" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>10 0.38006851 <a title="199-lsi-10" href="./emnlp-2013-Learning_Topics_and_Positions_from_Debatepedia.html">121 emnlp-2013-Learning Topics and Positions from Debatepedia</a></p>
<p>11 0.3580189 <a title="199-lsi-11" href="./emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM.html">99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</a></p>
<p>12 0.35562357 <a title="199-lsi-12" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>13 0.35163119 <a title="199-lsi-13" href="./emnlp-2013-Feature_Noising_for_Log-Linear_Structured_Prediction.html">86 emnlp-2013-Feature Noising for Log-Linear Structured Prediction</a></p>
<p>14 0.35131764 <a title="199-lsi-14" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<p>15 0.34078798 <a title="199-lsi-15" href="./emnlp-2013-This_Text_Has_the_Scent_of_Starbucks%3A_A_Laplacian_Structured_Sparsity_Model_for_Computational_Branding_Analytics.html">184 emnlp-2013-This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</a></p>
<p>16 0.33452669 <a title="199-lsi-16" href="./emnlp-2013-Automatically_Classifying_Edit_Categories_in_Wikipedia_Revisions.html">34 emnlp-2013-Automatically Classifying Edit Categories in Wikipedia Revisions</a></p>
<p>17 0.33411658 <a title="199-lsi-17" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>18 0.33116388 <a title="199-lsi-18" href="./emnlp-2013-Detecting_Promotional_Content_in_Wikipedia.html">61 emnlp-2013-Detecting Promotional Content in Wikipedia</a></p>
<p>19 0.32522249 <a title="199-lsi-19" href="./emnlp-2013-Learning_to_Rank_Lexical_Substitutions.html">123 emnlp-2013-Learning to Rank Lexical Substitutions</a></p>
<p>20 0.31615999 <a title="199-lsi-20" href="./emnlp-2013-Classifying_Message_Board_Posts_with_an_Extracted_Lexicon_of_Patient_Attributes.html">46 emnlp-2013-Classifying Message Board Posts with an Extracted Lexicon of Patient Attributes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.058), (18, 0.021), (22, 0.033), (30, 0.037), (43, 0.015), (45, 0.012), (50, 0.01), (51, 0.144), (52, 0.018), (66, 0.041), (71, 0.027), (75, 0.018), (77, 0.011), (86, 0.424), (90, 0.014), (96, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75971866 <a title="199-lda-1" href="./emnlp-2013-Using_Topic_Modeling_to_Improve_Prediction_of_Neuroticism_and_Depression_in_College_Students.html">199 emnlp-2013-Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students</a></p>
<p>Author: Philip Resnik ; Anderson Garron ; Rebecca Resnik</p><p>Abstract: in College Students Anderson Garron University of Maryland College Park, MD 20742 agarron@cs.umd.edu Rebecca Resnik Mindwell Psychology Bethesda 5602 Shields Drive Bethesda, MD 20817 drrebeccaresnik@gmail.com out adequate insurance or in rural areas – cannot ac- We investigate the value-add of topic modeling in text analysis for depression, and for neuroticism as a strongly associated personality measure. Using Pennebaker’s Linguistic Inquiry and Word Count (LIWC) lexicon to provide baseline features, we show that straightforward topic modeling using Latent Dirichlet Allocation (LDA) yields interpretable, psychologically relevant “themes” that add value in prediction of clinical assessments.</p><p>2 0.36285529 <a title="199-lda-2" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>Author: Eduardo Blanco ; Dan Moldovan</p><p>Abstract: This paper presents a novel approach to determine textual similarity. A layered methodology to transform text into logic forms is proposed, and semantic features are derived from a logic prover. Experimental results show that incorporating the semantic structure of sentences is beneficial. When training data is unavailable, scores obtained from the logic prover in an unsupervised manner outperform supervised methods.</p><p>3 0.36015534 <a title="199-lda-3" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>Author: Andrew J. Anderson ; Elia Bruni ; Ulisse Bordignon ; Massimo Poesio ; Marco Baroni</p><p>Abstract: Traditional distributional semantic models extract word meaning representations from cooccurrence patterns of words in text corpora. Recently, the distributional approach has been extended to models that record the cooccurrence of words with visual features in image collections. These image-based models should be complementary to text-based ones, providing a more cognitively plausible view of meaning grounded in visual perception. In this study, we test whether image-based models capture the semantic patterns that emerge from fMRI recordings of the neural signal. Our results indicate that, indeed, there is a significant correlation between image-based and brain-based semantic similarities, and that image-based models complement text-based ones, so that the best correlations are achieved when the two modalities are combined. Despite some unsatisfactory, but explained out- comes (in particular, failure to detect differential association of models with brain areas), the results show, on the one hand, that imagebased distributional semantic models can be a precious new tool to explore semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility.</p><p>4 0.35972136 <a title="199-lda-4" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>Author: Gary Patterson ; Andrew Kehler</p><p>Abstract: We present a classification model that predicts the presence or omission of a lexical connective between two clauses, based upon linguistic features of the clauses and the type of discourse relation holding between them. The model is trained on a set of high frequency relations extracted from the Penn Discourse Treebank and achieves an accuracy of 86.6%. Analysis of the results reveals that the most informative features relate to the discourse dependencies between sequences of coherence relations in the text. We also present results of an experiment that provides insight into the nature and difficulty of the task.</p><p>5 0.35950902 <a title="199-lda-5" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<p>Author: Yiping Jin ; Min-Yen Kan ; Jun-Ping Ng ; Xiangnan He</p><p>Abstract: This paper presents DefMiner, a supervised sequence labeling system that identifies scientific terms and their accompanying definitions. DefMiner achieves 85% F1 on a Wikipedia benchmark corpus, significantly improving the previous state-of-the-art by 8%. We exploit DefMiner to process the ACL Anthology Reference Corpus (ARC) – a large, real-world digital library of scientific articles in computational linguistics. The resulting automatically-acquired glossary represents the terminology defined over several thousand individual research articles. We highlight several interesting observations: more definitions are introduced for conference and workshop papers over the years and that multiword terms account for slightly less than half of all terms. Obtaining a list of popular , defined terms in a corpus ofcomputational linguistics papers, we find that concepts can often be categorized into one of three categories: resources, methodologies and evaluation metrics.</p><p>6 0.35914215 <a title="199-lda-6" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>7 0.35892165 <a title="199-lda-7" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>8 0.35869977 <a title="199-lda-8" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>9 0.35868272 <a title="199-lda-9" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>10 0.35849568 <a title="199-lda-10" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>11 0.35780466 <a title="199-lda-11" href="./emnlp-2013-The_Effects_of_Syntactic_Features_in_Automatic_Prediction_of_Morphology.html">181 emnlp-2013-The Effects of Syntactic Features in Automatic Prediction of Morphology</a></p>
<p>12 0.35770184 <a title="199-lda-12" href="./emnlp-2013-Building_Specialized_Bilingual_Lexicons_Using_Large_Scale_Background_Knowledge.html">42 emnlp-2013-Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge</a></p>
<p>13 0.35725617 <a title="199-lda-13" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>14 0.35724935 <a title="199-lda-14" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>15 0.35682592 <a title="199-lda-15" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>16 0.3564598 <a title="199-lda-16" href="./emnlp-2013-Gender_Inference_of_Twitter_Users_in_Non-English_Contexts.html">89 emnlp-2013-Gender Inference of Twitter Users in Non-English Contexts</a></p>
<p>17 0.35643384 <a title="199-lda-17" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>18 0.35639942 <a title="199-lda-18" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>19 0.35590148 <a title="199-lda-19" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>20 0.35579517 <a title="199-lda-20" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
