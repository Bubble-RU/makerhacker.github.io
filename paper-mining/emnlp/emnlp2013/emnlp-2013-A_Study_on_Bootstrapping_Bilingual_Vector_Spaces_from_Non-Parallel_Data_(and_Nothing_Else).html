<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-13" href="#">emnlp2013-13</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</h1>
<br/><p>Source: <a title="emnlp-2013-13-pdf" href="http://aclweb.org/anthology//D/D13/D13-1168.pdf">pdf</a></p><p>Author: Ivan Vulic ; Marie-Francine Moens</p><p>Abstract: We present a new language pair agnostic approach to inducing bilingual vector spaces from non-parallel data without any other resource in a bootstrapping fashion. The paper systematically introduces and describes all key elements of the bootstrapping procedure: (1) starting point or seed lexicon, (2) the confidence estimation and selection of new dimensions of the space, and (3) convergence. We test the quality of the induced bilingual vector spaces, and analyze the influence of the different components of the bootstrapping approach in the task of bilingual lexicon extraction (BLE) for two language pairs. Results reveal that, contrary to conclusions from prior work, the seeding of the bootstrapping process has a heavy impact on the quality of the learned lexicons. We also show that our approach outperforms the best performing fully corpus-based BLE methods on these test sets.</p><p>Reference: <a title="emnlp-2013-13-reference" href="../emnlp2013_reference/emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 be c  Abstract We present a new language pair agnostic approach to inducing bilingual vector spaces from non-parallel data without any other resource in a bootstrapping fashion. [sent-5, score-0.931]
</p><p>2 The paper systematically introduces and describes all key elements of the bootstrapping procedure: (1) starting point or seed lexicon, (2) the confidence estimation and selection of new dimensions of the space, and (3) convergence. [sent-6, score-0.99]
</p><p>3 We test the quality of the induced bilingual vector spaces, and analyze the influence of the different components of the bootstrapping approach in the task of bilingual lexicon extraction (BLE) for two language pairs. [sent-7, score-1.415]
</p><p>4 Results reveal that, contrary to conclusions from prior work, the seeding of the bootstrapping process has a heavy impact on the quality of the learned lexicons. [sent-8, score-0.493]
</p><p>5 The utility of the transfer or annotation projection by means of bilingual lexicons has already been proven in various tasks such as semantic role labeling (Pad o´ and Lapata, 2009; van der Plas et al. [sent-14, score-0.574]
</p><p>6 Techniques for automatic bilingual lexicon extraction (BLE) from parallel corpora on the basis of word alignment models are well established (Och and Ney, 2003). [sent-20, score-0.555]
</p><p>7 oc d2s0 i1n3 N Aastusorcaila Ltiaon g fuoarg Ceo Pmrpoucetastsi on ga,l p Laignegsu 1is6t1ic3s–1624, bilingual vector space. [sent-42, score-0.467]
</p><p>8 The standard way of building a bilingual vector space is to use bilingual lexicon entries (Rapp, 1999; Fung and Cheung, 2004; Gaussier et al. [sent-43, score-1.048]
</p><p>9 However, there seems to be an apparent flaw in logic, since the methods assume that there exist readily available bilingual lexicons that are then used to induce bilingual lexicons! [sent-45, score-0.921]
</p><p>10 Therefore, the focus of the researchers has turned to designing BLE methods that do not rely on any external translation resources such as machine-readable bilingual lexicons and parallel corpora (Haghighi et al. [sent-46, score-0.684]
</p><p>11 In order to circumvent this issue, one line of recent work aims to bootstrap high-quality bilingual vector spaces from a small initial seed lexicon. [sent-49, score-0.924]
</p><p>12 The seed lexicon is constructed by harvesting identical or similarly spelled words across languages (Koehn  and Knight, 2002; Peirsman and Pad o´, 2010), and it spans the initial bilingual vector space. [sent-50, score-0.978]
</p><p>13 The bootstrapping process has already proven its validity in inducing bilingual lexicons for closely similar languages such as Spanish-Portuguese or Croatian-Slovene (Fi sˇer and Ljube ˇsi c´, 2011), but it still lacks further generalization to more distant language pairs. [sent-52, score-0.881]
</p><p>14 We aim to answer the following key questions: •  •  How to seed bilingual vector spaces besides using only orthographically tsoirm siplaarc weso brdessi? [sent-54, score-0.866]
</p><p>15 d Is it better to seed bilingual spaces with translIsat iiotn b pairs/dimensions gthuaatl are frequent itrna tnhsecorpus, and does the frequency matter at all? [sent-55, score-0.854]
</p><p>16 Does the size of the initial seed lexicon matter? [sent-56, score-0.482]
</p><p>17 •  How to enrich bilingual vector spaces with only highly reenlriaibclheb dili mngeunasilovnecs oinr sopradecre stow prevent semantic drift? [sent-57, score-0.601]
</p><p>18 With respect to these questions, the main contributions of this article are:  •  •  •  1614 We present a complete overview of the framewWoer kp oesfe bootstrapping bilingual vector spaces from non-parallel data without any additional resources. [sent-58, score-0.931]
</p><p>19 We introduce a new way of seeding the bootstrapping procedure t whaaty d oofes s nedoti rely on any orthographic clues and that yields bilingual vector spaces of higher quality. [sent-60, score-1.099]
</p><p>20 We analyze the impact of different seed lexicons on the quality of induced bilingual vector spaces. [sent-61, score-0.95]
</p><p>21 We show that in the setting without any extWerena slh torwan tshlaattio inn resources, our bootstrapping approach yields lexicons that outperform the best performing corpus-based BLE methods on standard test datasets for 2 language pairs. [sent-62, score-0.485]
</p><p>22 The goal is to bwuoirldd a bilingual vector space using only corpus C. [sent-68, score-0.519]
</p><p>23 Dimensions of the bilingual vector space are one-to-one word translation pairs. [sent-70, score-0.652]
</p><p>24 Computing cross-lingual word similarity in a bilingual vector space. [sent-75, score-0.539]
</p><p>25 Now, assume that our bilingual vector space consists of N one-to-one word translation pairs ck = (cSk, ckT), k = 1, . [sent-76, score-0.77]
</p><p>26 For each word wiS ∈ VS, we compute the similarity of that word with each word wjT ∈ VT by computing the similarity between their context vectors cv(wiS) and cv(wjT), which are actually their representations in the N-dimensional bilingual vector space. [sent-80, score-0.611]
</p><p>27 , 2004): (1) For each source word wiS ∈ VS, build its Ndimensional context vector cv(w∈Si V) that consists of association scores scSk(cSk), that is, we compute the strength of association with the “source” part ofeach dimension ck that constitutes the N-dimensional bilingual space. [sent-82, score-0.508]
</p><p>28 (3) Since ckS and ckT address the same dimension ck in the bilingual vector space for each k = 1, . [sent-86, score-0.56]
</p><p>29 The key idea of the bootstrapping approach relies on an insight that highly reliable translation pairs (wS1, w2T) that are encountered using the N-dimensional bilingual vector space might be added as new dimensions of the space. [sent-96, score-1.285]
</p><p>30 By adding 1615 these new dimensions, it might be possible to extract more highly reliable translation pairs that were previously not used as dimensions of the space, and the iterative procedure repeats until no new dimensions are found. [sent-97, score-0.64]
</p><p>31 The induced bilingual vector space may then be observed as a bilingual lexicon per se, but it may also be used to find translation equivalents for other words which are not used to span the space. [sent-98, score-1.214]
</p><p>32 Algorithm  1:  Algorithm 1: Bootstrapping a bilingual vector space Input : Bilingual corpus C = CS∪ CT IInniptiuatliz :e B: (1) gOubatla cion a ounse C-to =-o Cne ∪see Cd lexicon. [sent-99, score-0.519]
</p><p>33 The intuition here is that we expect for the quality of the space to increase at each stage of the bootstrapping process, and newer translation pairs should be more confident than the older ones. [sent-107, score-0.685]
</p><p>34 For instance, if 2 out of N dimensions of a Spanish-English bilingual space are pairs (piedra,wall) and (tapia,stone), but then if during the bootstrapping process we extract a new candidate pair (piedra,stone), we will delete the former two dimensions and add the latter. [sent-108, score-1.219]
</p><p>35 2 Initializing Bilingual Vector Spaces Seeding or initializing a bootstrapping procedure is often a critical step regardless of the actual task (McIntosh and Curran, 2009; Kozareva and Hovy, 2010), and it decides whether the complete process will end as a success or a failure. [sent-110, score-0.489]
</p><p>36 However, Peirsman and Pad o´ (201 1) argue that the initialization step is not crucial when dealing with bootstrapping bilingual vector spaces. [sent-111, score-0.823]
</p><p>37 Here, we present two different strategies of initializing the bilingual vector space. [sent-112, score-0.502]
</p><p>38 Another problem with using only identical words and cognates as seeds lies in the fact that many of them might be infrequent in the corpus, and as a consequence the expressiveness of a bilingual vector space might be limited. [sent-117, score-0.617]
</p><p>39 , 2010), that does not require a bilingual lexicon, it operates with nonparallel data, and is able to produce highly confident translation pairs for high-frequency words (Mimno et al. [sent-121, score-0.643]
</p><p>40 2 Therefore, we can construct the initial seed lexicon as follows: (1) Train a multilingual topic model on the corpus. [sent-123, score-0.511]
</p><p>41 This step ensures that only highly confident pairs are used  as seed translation pairs. [sent-129, score-0.538]
</p><p>42 3 Estimating Confidence of New Dimensions Another crucial step in the bootstrapping procedure is the estimation of confidence in a translation pair/candidate dimension. [sent-134, score-0.679]
</p><p>43 We therefore impose the constraint which requires translation pairs to be symmetric in order to qualify as potential new dimensions of the space. [sent-136, score-0.462]
</p><p>44 In each iteration of the bootstrapping process, we may add all symmetric pairs from the pool of candidates as new dimensions, or we could impose additional selection criteria that quantify the degree of confidence in translation pairs. [sent-139, score-0.828]
</p><p>45 We are then able to rank the symmetric candidate translation pairs in the pool of candidates according to their confidence scores (step 3 of alg. [sent-140, score-0.432]
</p><p>46 , “poisoning” the bootstrapping process that might happen if we include incorrect translation pairs as dimensions of the space. [sent-144, score-0.735]
</p><p>47 The goal of the BLE task is to extract a bilingual lexicon of one-to-one translations. [sent-161, score-0.529]
</p><p>48 In order to test the quality of bilingual vector spaces induced by our bootstrapping approach, we evaluate it on standard 1000 ground truth one-to-one translation pairs built for the Wiki-ES-EN and Wiki-ITEN datasets (Vuli ´c and Moens, 2013). [sent-162, score-1.256]
</p><p>49 Note that we do not explicitly test the bilingual vector space as a bilingual lexicon, but rather its ability to find semantically similar words and translations also for words that are not used as dimensions of the space (see sect. [sent-163, score-1.159]
</p><p>50 It denotes the number of source  words wiS from ground truth translation pairs whose list RLM(wiS) contains the correct translation according to our ground truth over the total number of ground truth translation pairs (=1000) (Gaussier et al. [sent-168, score-0.733]
</p><p>51 To produce the lists of one-to-one translation pairs that are used as seeds for the bootstrapping approach (see sect. [sent-182, score-0.664]
</p><p>52 , 2011) using 5We can build a one-to-one bilingual lexicon by harvesting one-to-one translation pairs (wSi , TC(wiS)), and the quality of that lexicon is best reflected in the Acc1 score. [sent-187, score-0.931]
</p><p>53 We use these models of similarity as a black box to acquire seeds for the bootstrapping approach, but we encourage the interested reader to find more details about the methods in the relevant literature. [sent-193, score-0.556]
</p><p>54 These two models also serve as our baseline models, and our goal is to test whether we are able to obtain bilingual lexicons of higher quality using bootstrapping that starts from the output of these models. [sent-194, score-0.911]
</p><p>55 In recent work, Peirsman and Pad o´ (2010; 2011) report that “the size and quality of the (seed) lex1618 icon are not of primary importance given that the bootstrapping procedure effectively helped filter out incorrect translation pairs and added more newly identified mutual nearest neighbors. [sent-208, score-0.657]
</p><p>56 Additionally, they do not provide any insight whether the frequency of seeds in the corpus influences the quality of induced bilingual vector spaces. [sent-210, score-0.66]
</p><p>57 All experiments conducted in this section do not rely on any extra confidence estimation except for  the symmetry constraint, that is, in each step we enrich the bilingual vector space with all new symmetric translation pairs (see alg. [sent-212, score-0.956]
</p><p>58 The goal of this experiment is to test whether the quality of seeds plays an important role in the bootstrapping approach. [sent-218, score-0.484]
</p><p>59 This procedure results in 459 seed translation pairs for ES-EN, and 43 1 pairs for IT-EN (SEED-ID), (2) We obtain symmetric translation pairs using the TopicBC method (see sect. [sent-220, score-0.905]
</p><p>60 1(a) and 1(b) display the progress ofthe same bootstrapping procedure using the 3 different seed  lexicons. [sent-224, score-0.708]
</p><p>61 We derive several interesting conclusions: (i) Regardless of the actual choice of the seeding method, the bootstrapping process proves its validity and utility since we observe that the quality of induced bilingual vector spaces increases over time for all 3 seeding methods. [sent-225, score-1.208]
</p><p>62 (a) Spanish to English  (b) Italian to English Figure 2: Results on the BLE task with SEED-RB when using seed translation pairs of different frequency:  (i) high-  frequency (HF-SEED), (ii) medium-frequency (MF-SEED), (iii) low-frequency (LF-SEED). [sent-230, score-0.533]
</p><p>63 A bootstrapping approach that starts with a better seed lexicon is able to extract bilingual lexicons of higher quality as reflected in Acc1 scores. [sent-234, score-1.364]
</p><p>64 Starting with SEED-ID, the approach is able to recover noisy dimensions from an initial bilingual vector space, but it is still unable to match the results that are obtained when starting from a better initial space (e. [sent-236, score-0.849]
</p><p>65 In the next experiment, we test whether the frequency of seeds in the corpus plays an important role in the bootstrapping process. [sent-252, score-0.486]
</p><p>66 The intuition is that by using highly frequent and highly confident translation pairs the bootstrapping method has more reliable clues that help extract new dimensions in subsequent iterations. [sent-253, score-0.803]
</p><p>67 Following that, we split the list in 3 parts of equal size: (i) the top third comprises translation pairs with the highest frequency in the corpus (HF-SEED), (ii) the middle third comprises pairs of “medium” frequency (MF-SEED), (iii) the bottom third are low-frequency pairs (LFSEED). [sent-257, score-0.452]
</p><p>68 We then use these 3 different seed lexicons  of equal size to seed the bootstrapping approach. [sent-258, score-1.067]
</p><p>69 2(a) and 2(b) show the progress of the bootstrapping process using these 3 seed lexicons. [sent-260, score-0.647]
</p><p>70 We again observe several interesting phenomena: (i) High-frequency seed translation pairs are better seeds, and that finding is in line with our hypothesis. [sent-261, score-0.501]
</p><p>71 Although the bootstrapping approach again displays a positive trend regardless of the actual choice of seeds (we observe an increase even when using LFSEED), high-frequency seeds lead to better overall results in the BLE task. [sent-262, score-0.589]
</p><p>72 Besides its high presence in contexts of other words, another advantage of highfrequency seed pairs is the fact that an initial similarity method will typically acquire more reliable translation candidates for such words (Pekar et al. [sent-263, score-0.724]
</p><p>73 The following experiment investigates whether bilingual vector spaces may be effectively bootstrapped from small highquality seed lexicons, and if larger seed lexicons 1620 necessarily lead to bilingual vector spaces of higher quality as reflected in BLE results. [sent-273, score-1.92]
</p><p>74 Following that, we build seed lexicons of various sizes by retaining only the first N pairs from the list, where we vary N from 200 to 1400 in steps of 200. [sent-276, score-0.497]
</p><p>75 We also use the entire sorted list as a seed lexicon (All), and compare the results on the BLE task with the results obtained by applying the ResponseBC and TopicBC methods directly (Vuli ´c and Moens, 2013). [sent-277, score-0.448]
</p><p>76 We observe the following: (i) If we provide a seed lexicon with sufficient entries, the bootstrapping procedure provides comparable results regardless of the seed lexicon size, although results tend to be higher for larger seed lex-  icons (e. [sent-279, score-1.626]
</p><p>77 When starting with the size of 600, the bootstrapping approach is able to find dimensions that were already in the seed lexicon of size 1200. [sent-282, score-0.994]
</p><p>78 The consequence is that, although bootstrapping with a smaller seed lexicon displays a slower start (see the difference in results at iteration 0), the performances level after convergence. [sent-283, score-0.82]
</p><p>79 (ii) Regardless of the seed lexicon size, the bootstrapping approach is valuable. [sent-284, score-0.78]
</p><p>80 It consistently improves the quality of the induced bilingual vector space, and consequently, the quality of bilingual lexicons extracted using that vector space. [sent-285, score-1.156]
</p><p>81 , we observe an increase of 78% for ES-EN when starting with only 200 seed pairs, compared to an increase of 15% when starting with 800 seed pairs, and 10% when starting with 1400 seed pairs. [sent-288, score-1.008]
</p><p>82 (iii) The bootstrapping approach outperforms ResponseBC and TopicBC in terms of Acc1 and MRR scores for both language pairs when the seed lexicon provides a sufficient number of entries. [sent-289, score-0.857]
</p><p>83 Both TopicBC and ResponseBC are MuPTM-based methods that, due to MuPTM properties, model the similarity of two words at the level of documents as contexts, while the bootstrapping approach is a window-based approach that narrows down the context to a local neighborhood of a word. [sent-291, score-0.428]
</p><p>84 The number in the parentheses denotes the number of dimensions in the bilingual space after the bootstrapping procedure converges. [sent-395, score-1.034]
</p><p>85 The number in the parentheses denotes the number of dimensions in the bilingual space after the bootstrapping procedure converges. [sent-500, score-1.034]
</p><p>86 According to the results from tables 1 and 2, regardless of the seed lexicon size, the bootstrapping approach does not suffer from semantic drift, i. [sent-511, score-0.843]
</p><p>87 , if we seed the process with high-quality symmetric translation pairs, it is able to recover more pairs and  add them as new dimensions of the bilingual vector space. [sent-513, score-1.193]
</p><p>88 The symmetry constraint alone seems to be sufficient to prevent semantic drift, but it might also be a too strong and a too conservative assumption, since only a small portion of all possible translation pairs is used to span the bilingual vector space (for instance,  A1c0. [sent-518, score-0.824]
</p><p>89 when starting with 600 entries for ES-EN, the final bilingual vector space consists of only 1554 pairs, while the total number of ES nouns is 9439). [sent-522, score-0.564]
</p><p>90 One line of future work will address the construction of bootstrapping algorithms that also enable the usage of highly reliable asymmetric pairs as dimensions, and the confidence estimation functions might have a more important role in that setting. [sent-523, score-0.593]
</p><p>91 5  Conclusion  We have presented a new bootstrapping approach to inducing bilingual vector spaces from non-parallel data, and have shown the utility of the induced space in the BLE task. [sent-524, score-1.016]
</p><p>92 Results reveal that, contrary to conclusions from prior  work, the initialization of the bilingual vector space is especially important. [sent-527, score-0.519]
</p><p>93 We have presented a novel approach to initializing the bootstrapping procedure, and have shown that better results in the BLE task are obtained by starting from seed lexicons that comprise highly reliable high-frequent translation pairs. [sent-528, score-1.02]
</p><p>94 , 2008)) that could be used as preliminary models for constructing an initial bilingual vector space. [sent-532, score-0.525]
</p><p>95 Furthermore, we plan 1622 to study other confidence functions and explore if asymmetric translation candidates could also contribute to the bootstrapping method. [sent-533, score-0.621]
</p><p>96 Mining verynon-parallel corpora: Parallel sentence and lexicon extraction via bootstrapping and EM. [sent-585, score-0.489]
</p><p>97 A geometric  view on bilingual lexicon extraction from comparable corpora. [sent-593, score-0.562]
</p><p>98 Crosslingual induction of selectional preferences with bilingual vector spaces. [sent-683, score-0.467]
</p><p>99 A corpus-based bootstrapping algorithm for semi-automated semantic lexicon construction. [sent-703, score-0.515]
</p><p>100 A bootstrapping method for learning semantic lexicons using extraction pattern contexts. [sent-736, score-0.511]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bilingual', 0.396), ('bootstrapping', 0.356), ('wis', 0.33), ('seed', 0.291), ('responsebc', 0.21), ('vuli', 0.197), ('wjt', 0.171), ('topicbc', 0.171), ('dimensions', 0.169), ('ble', 0.165), ('moens', 0.135), ('lexicon', 0.133), ('translation', 0.133), ('lexicons', 0.129), ('spaces', 0.108), ('seeding', 0.107), ('confidence', 0.1), ('seeds', 0.098), ('cv', 0.092), ('pairs', 0.077), ('mrr', 0.077), ('similarity', 0.072), ('vector', 0.071), ('pad', 0.067), ('tc', 0.067), ('procedure', 0.061), ('drift', 0.058), ('initial', 0.058), ('symmetric', 0.056), ('ljube', 0.053), ('fung', 0.052), ('space', 0.052), ('peirsman', 0.048), ('vt', 0.048), ('gaussier', 0.046), ('smet', 0.046), ('starting', 0.045), ('symmetry', 0.042), ('ackstr', 0.042), ('bullinaria', 0.042), ('mcintosh', 0.042), ('ck', 0.041), ('iteration', 0.04), ('accm', 0.039), ('leuven', 0.039), ('muptm', 0.039), ('tamura', 0.039), ('iii', 0.038), ('confident', 0.037), ('regardless', 0.037), ('pages', 0.036), ('sim', 0.035), ('initializing', 0.035), ('mimno', 0.035), ('rlm', 0.034), ('ser', 0.034), ('pool', 0.034), ('pantel', 0.034), ('comparable', 0.033), ('induced', 0.033), ('turney', 0.032), ('frequency', 0.032), ('candidates', 0.032), ('reliable', 0.031), ('quality', 0.03), ('haghighi', 0.03), ('ii', 0.03), ('acquire', 0.03), ('ivan', 0.03), ('multilingual', 0.029), ('estimation', 0.029), ('rl', 0.029), ('rapp', 0.029), ('spelled', 0.029), ('reflected', 0.029), ('ranked', 0.028), ('matter', 0.027), ('vs', 0.027), ('constraint', 0.027), ('truth', 0.026), ('ground', 0.026), ('cks', 0.026), ('ckt', 0.026), ('csk', 0.026), ('lfseed', 0.026), ('niwa', 0.026), ('pekar', 0.026), ('plas', 0.026), ('seedrb', 0.026), ('shezaf', 0.026), ('simm', 0.026), ('thelen', 0.026), ('pascale', 0.026), ('corpora', 0.026), ('proceedings', 0.026), ('semantic', 0.026), ('cf', 0.025), ('list', 0.024), ('translations', 0.023), ('transfer', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="13-tfidf-1" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>Author: Ivan Vulic ; Marie-Francine Moens</p><p>Abstract: We present a new language pair agnostic approach to inducing bilingual vector spaces from non-parallel data without any other resource in a bootstrapping fashion. The paper systematically introduces and describes all key elements of the bootstrapping procedure: (1) starting point or seed lexicon, (2) the confidence estimation and selection of new dimensions of the space, and (3) convergence. We test the quality of the induced bilingual vector spaces, and analyze the influence of the different components of the bootstrapping approach in the task of bilingual lexicon extraction (BLE) for two language pairs. Results reveal that, contrary to conclusions from prior work, the seeding of the bootstrapping process has a heavy impact on the quality of the learned lexicons. We also show that our approach outperforms the best performing fully corpus-based BLE methods on these test sets.</p><p>2 0.18386456 <a title="13-tfidf-2" href="./emnlp-2013-Building_Specialized_Bilingual_Lexicons_Using_Large_Scale_Background_Knowledge.html">42 emnlp-2013-Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge</a></p>
<p>Author: Dhouha Bouamor ; Adrian Popescu ; Nasredine Semmar ; Pierre Zweigenbaum</p><p>Abstract: Bilingual lexicons are central components of machine translation and cross-lingual information retrieval systems. Their manual construction requires strong expertise in both languages involved and is a costly process. Several automatic methods were proposed as an alternative but they often rely on resources available in a limited number of languages and their performances are still far behind the quality of manual translations. We introduce a novel approach to the creation of specific domain bilingual lexicon that relies on Wikipedia. This massively multilingual encyclopedia makes it possible to create lexicons for a large number of language pairs. Wikipedia is used to extract domains in each language, to link domains between languages and to create generic translation dictionaries. The approach is tested on four specialized domains and is compared to three state of the art approaches using two language pairs: FrenchEnglish and Romanian-English. The newly introduced method compares favorably to existing methods in all configurations tested.</p><p>3 0.17568505 <a title="13-tfidf-3" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>Author: Will Y. Zou ; Richard Socher ; Daniel Cer ; Christopher D. Manning</p><p>Abstract: We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence. The new embeddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task.</p><p>4 0.14189649 <a title="13-tfidf-4" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>Author: Min Xiao ; Yuhong Guo</p><p>Abstract: Cross-lingual adaptation aims to learn a prediction model in a label-scarce target language by exploiting labeled data from a labelrich source language. An effective crosslingual adaptation system can substantially reduce the manual annotation effort required in many natural language processing tasks. In this paper, we propose a new cross-lingual adaptation approach for document classification based on learning cross-lingual discriminative distributed representations of words. Specifically, we propose to maximize the loglikelihood of the documents from both language domains under a cross-lingual logbilinear document model, while minimizing the prediction log-losses of labeled documents. We conduct extensive experiments on cross-lingual sentiment classification tasks of Amazon product reviews. Our experimental results demonstrate the efficacy of the pro- posed cross-lingual adaptation approach.</p><p>5 0.11921445 <a title="13-tfidf-5" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>Author: Shi Feng ; Le Zhang ; Binyang Li ; Daling Wang ; Ge Yu ; Kam-Fai Wong</p><p>Abstract: Extensive experiments have validated the effectiveness of the corpus-based method for classifying the word’s sentiment polarity. However, no work is done for comparing different corpora in the polarity classification task. Nowadays, Twitter has aggregated huge amount of data that are full of people’s sentiments. In this paper, we empirically evaluate the performance of different corpora in sentiment similarity measurement, which is the fundamental task for word polarity classification. Experiment results show that the Twitter data can achieve a much better performance than the Google, Web1T and Wikipedia based methods.</p><p>6 0.11145 <a title="13-tfidf-6" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>7 0.10117711 <a title="13-tfidf-7" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>8 0.095523693 <a title="13-tfidf-8" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>9 0.093551964 <a title="13-tfidf-9" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>10 0.092521518 <a title="13-tfidf-10" href="./emnlp-2013-Identifying_Phrasal_Verbs_Using_Many_Bilingual_Corpora.html">96 emnlp-2013-Identifying Phrasal Verbs Using Many Bilingual Corpora</a></p>
<p>11 0.08877901 <a title="13-tfidf-11" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>12 0.075671732 <a title="13-tfidf-12" href="./emnlp-2013-Noise-Aware_Character_Alignment_for_Bootstrapping_Statistical_Machine_Transliteration_from_Bilingual_Corpora.html">139 emnlp-2013-Noise-Aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora</a></p>
<p>13 0.075209364 <a title="13-tfidf-13" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>14 0.074669689 <a title="13-tfidf-14" href="./emnlp-2013-Growing_Multi-Domain_Glossaries_from_a_Few_Seeds_using_Probabilistic_Topic_Models.html">92 emnlp-2013-Growing Multi-Domain Glossaries from a Few Seeds using Probabilistic Topic Models</a></p>
<p>15 0.070619158 <a title="13-tfidf-15" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>16 0.065126911 <a title="13-tfidf-16" href="./emnlp-2013-Word_Level_Language_Identification_in_Online_Multilingual_Communication.html">204 emnlp-2013-Word Level Language Identification in Online Multilingual Communication</a></p>
<p>17 0.063272677 <a title="13-tfidf-17" href="./emnlp-2013-Factored_Soft_Source_Syntactic_Constraints_for_Hierarchical_Machine_Translation.html">84 emnlp-2013-Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation</a></p>
<p>18 0.062588803 <a title="13-tfidf-18" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>19 0.061209302 <a title="13-tfidf-19" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>20 0.059514701 <a title="13-tfidf-20" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.214), (1, -0.1), (2, -0.042), (3, -0.053), (4, 0.118), (5, 0.0), (6, -0.027), (7, 0.014), (8, -0.025), (9, -0.19), (10, 0.08), (11, -0.056), (12, 0.133), (13, -0.015), (14, 0.027), (15, -0.084), (16, -0.029), (17, 0.083), (18, -0.001), (19, 0.005), (20, -0.057), (21, -0.08), (22, -0.011), (23, 0.069), (24, -0.006), (25, -0.204), (26, 0.066), (27, 0.096), (28, -0.014), (29, -0.03), (30, -0.213), (31, 0.018), (32, -0.09), (33, 0.009), (34, -0.146), (35, -0.009), (36, 0.005), (37, 0.041), (38, -0.134), (39, -0.034), (40, 0.02), (41, 0.047), (42, -0.112), (43, 0.066), (44, -0.032), (45, -0.149), (46, 0.149), (47, -0.046), (48, -0.029), (49, -0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95922852 <a title="13-lsi-1" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>Author: Ivan Vulic ; Marie-Francine Moens</p><p>Abstract: We present a new language pair agnostic approach to inducing bilingual vector spaces from non-parallel data without any other resource in a bootstrapping fashion. The paper systematically introduces and describes all key elements of the bootstrapping procedure: (1) starting point or seed lexicon, (2) the confidence estimation and selection of new dimensions of the space, and (3) convergence. We test the quality of the induced bilingual vector spaces, and analyze the influence of the different components of the bootstrapping approach in the task of bilingual lexicon extraction (BLE) for two language pairs. Results reveal that, contrary to conclusions from prior work, the seeding of the bootstrapping process has a heavy impact on the quality of the learned lexicons. We also show that our approach outperforms the best performing fully corpus-based BLE methods on these test sets.</p><p>2 0.79179585 <a title="13-lsi-2" href="./emnlp-2013-Building_Specialized_Bilingual_Lexicons_Using_Large_Scale_Background_Knowledge.html">42 emnlp-2013-Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge</a></p>
<p>Author: Dhouha Bouamor ; Adrian Popescu ; Nasredine Semmar ; Pierre Zweigenbaum</p><p>Abstract: Bilingual lexicons are central components of machine translation and cross-lingual information retrieval systems. Their manual construction requires strong expertise in both languages involved and is a costly process. Several automatic methods were proposed as an alternative but they often rely on resources available in a limited number of languages and their performances are still far behind the quality of manual translations. We introduce a novel approach to the creation of specific domain bilingual lexicon that relies on Wikipedia. This massively multilingual encyclopedia makes it possible to create lexicons for a large number of language pairs. Wikipedia is used to extract domains in each language, to link domains between languages and to create generic translation dictionaries. The approach is tested on four specialized domains and is compared to three state of the art approaches using two language pairs: FrenchEnglish and Romanian-English. The newly introduced method compares favorably to existing methods in all configurations tested.</p><p>3 0.66558933 <a title="13-lsi-3" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>Author: Will Y. Zou ; Richard Socher ; Daniel Cer ; Christopher D. Manning</p><p>Abstract: We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence. The new embeddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task.</p><p>4 0.51764596 <a title="13-lsi-4" href="./emnlp-2013-Noise-Aware_Character_Alignment_for_Bootstrapping_Statistical_Machine_Transliteration_from_Bilingual_Corpora.html">139 emnlp-2013-Noise-Aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora</a></p>
<p>Author: Katsuhito Sudoh ; Shinsuke Mori ; Masaaki Nagata</p><p>Abstract: This paper proposes a novel noise-aware character alignment method for bootstrapping statistical machine transliteration from automatically extracted phrase pairs. The model is an extension of a Bayesian many-to-many alignment method for distinguishing nontransliteration (noise) parts in phrase pairs. It worked effectively in the experiments of bootstrapping Japanese-to-English statistical machine transliteration in patent domain using patent bilingual corpora.</p><p>5 0.50110257 <a title="13-lsi-5" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>Author: Ann Irvine ; Chris Quirk ; Hal Daume III</p><p>Abstract: When using a machine translation (MT) model trained on OLD-domain parallel data to translate NEW-domain text, one major challenge is the large number of out-of-vocabulary (OOV) and new-translation-sense words. We present a method to identify new translations of both known and unknown source language words that uses NEW-domain comparable document pairs. Starting with a joint distribution of source-target word pairs derived from the OLD-domain parallel corpus, our method recovers a new joint distribution that matches the marginal distributions of the NEW-domain comparable document pairs, while minimizing the divergence from the OLD-domain distribution. Adding learned translations to our French-English MT model results in gains of about 2 BLEU points over strong baselines.</p><p>6 0.42958173 <a title="13-lsi-6" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>7 0.41806531 <a title="13-lsi-7" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>8 0.39393201 <a title="13-lsi-8" href="./emnlp-2013-Growing_Multi-Domain_Glossaries_from_a_Few_Seeds_using_Probabilistic_Topic_Models.html">92 emnlp-2013-Growing Multi-Domain Glossaries from a Few Seeds using Probabilistic Topic Models</a></p>
<p>9 0.38983589 <a title="13-lsi-9" href="./emnlp-2013-Word_Level_Language_Identification_in_Online_Multilingual_Communication.html">204 emnlp-2013-Word Level Language Identification in Online Multilingual Communication</a></p>
<p>10 0.38839406 <a title="13-lsi-10" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>11 0.37618005 <a title="13-lsi-11" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<p>12 0.36739266 <a title="13-lsi-12" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>13 0.36259001 <a title="13-lsi-13" href="./emnlp-2013-Identifying_Phrasal_Verbs_Using_Many_Bilingual_Corpora.html">96 emnlp-2013-Identifying Phrasal Verbs Using Many Bilingual Corpora</a></p>
<p>14 0.35754707 <a title="13-lsi-14" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>15 0.35157451 <a title="13-lsi-15" href="./emnlp-2013-Boosting_Cross-Language_Retrieval_by_Learning_Bilingual_Phrase_Associations_from_Relevance_Rankings.html">39 emnlp-2013-Boosting Cross-Language Retrieval by Learning Bilingual Phrase Associations from Relevance Rankings</a></p>
<p>16 0.31411064 <a title="13-lsi-16" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>17 0.31301299 <a title="13-lsi-17" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>18 0.29613549 <a title="13-lsi-18" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<p>19 0.28874803 <a title="13-lsi-19" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>20 0.28853124 <a title="13-lsi-20" href="./emnlp-2013-The_Topology_of_Semantic_Knowledge.html">182 emnlp-2013-The Topology of Semantic Knowledge</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.022), (9, 0.011), (18, 0.042), (22, 0.061), (30, 0.128), (43, 0.017), (50, 0.013), (51, 0.18), (66, 0.049), (71, 0.022), (75, 0.03), (77, 0.037), (83, 0.231), (90, 0.011), (96, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82110214 <a title="13-lda-1" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>Author: Ivan Vulic ; Marie-Francine Moens</p><p>Abstract: We present a new language pair agnostic approach to inducing bilingual vector spaces from non-parallel data without any other resource in a bootstrapping fashion. The paper systematically introduces and describes all key elements of the bootstrapping procedure: (1) starting point or seed lexicon, (2) the confidence estimation and selection of new dimensions of the space, and (3) convergence. We test the quality of the induced bilingual vector spaces, and analyze the influence of the different components of the bootstrapping approach in the task of bilingual lexicon extraction (BLE) for two language pairs. Results reveal that, contrary to conclusions from prior work, the seeding of the bootstrapping process has a heavy impact on the quality of the learned lexicons. We also show that our approach outperforms the best performing fully corpus-based BLE methods on these test sets.</p><p>2 0.76454419 <a title="13-lda-2" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<p>Author: Yiping Jin ; Min-Yen Kan ; Jun-Ping Ng ; Xiangnan He</p><p>Abstract: This paper presents DefMiner, a supervised sequence labeling system that identifies scientific terms and their accompanying definitions. DefMiner achieves 85% F1 on a Wikipedia benchmark corpus, significantly improving the previous state-of-the-art by 8%. We exploit DefMiner to process the ACL Anthology Reference Corpus (ARC) – a large, real-world digital library of scientific articles in computational linguistics. The resulting automatically-acquired glossary represents the terminology defined over several thousand individual research articles. We highlight several interesting observations: more definitions are introduced for conference and workshop papers over the years and that multiword terms account for slightly less than half of all terms. Obtaining a list of popular , defined terms in a corpus ofcomputational linguistics papers, we find that concepts can often be categorized into one of three categories: resources, methodologies and evaluation metrics.</p><p>3 0.71199697 <a title="13-lda-3" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>Author: Xiaoqing Zheng ; Hanyang Chen ; Tianyu Xu</p><p>Abstract: This study explores the feasibility of performing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representation of Chinese characters, and use these improved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-theart performance with minimal computational cost. We also describe a perceptron-style algorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented.</p><p>4 0.70800102 <a title="13-lda-4" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>Author: Uri Lerner ; Slav Petrov</p><p>Abstract: We present a simple and novel classifier-based preordering approach. Unlike existing preordering models, we train feature-rich discriminative classifiers that directly predict the target-side word order. Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discriminative model with a rich set of features, including lexical features. We present extensive experiments on 22 language pairs, including preordering into English from 7 other languages. We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task. For languages from different families the improvements often exceed 2 BLEU. Many of these gains are also significant in human evaluations.</p><p>5 0.70675093 <a title="13-lda-5" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>Author: Will Y. Zou ; Richard Socher ; Daniel Cer ; Christopher D. Manning</p><p>Abstract: We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence. The new embeddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task.</p><p>6 0.70623994 <a title="13-lda-6" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>7 0.70517576 <a title="13-lda-7" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>8 0.70476353 <a title="13-lda-8" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>9 0.70439631 <a title="13-lda-9" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>10 0.70233434 <a title="13-lda-10" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>11 0.70022929 <a title="13-lda-11" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>12 0.69946957 <a title="13-lda-12" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>13 0.69720101 <a title="13-lda-13" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>14 0.69584167 <a title="13-lda-14" href="./emnlp-2013-Converting_Continuous-Space_Language_Models_into_N-Gram_Language_Models_for_Statistical_Machine_Translation.html">52 emnlp-2013-Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation</a></p>
<p>15 0.69553894 <a title="13-lda-15" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>16 0.6948055 <a title="13-lda-16" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>17 0.69478768 <a title="13-lda-17" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>18 0.6932748 <a title="13-lda-18" href="./emnlp-2013-Semi-Markov_Phrase-Based_Monolingual_Alignment.html">167 emnlp-2013-Semi-Markov Phrase-Based Monolingual Alignment</a></p>
<p>19 0.69181317 <a title="13-lda-19" href="./emnlp-2013-Dynamic_Feature_Selection_for_Dependency_Parsing.html">66 emnlp-2013-Dynamic Feature Selection for Dependency Parsing</a></p>
<p>20 0.69175828 <a title="13-lda-20" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
