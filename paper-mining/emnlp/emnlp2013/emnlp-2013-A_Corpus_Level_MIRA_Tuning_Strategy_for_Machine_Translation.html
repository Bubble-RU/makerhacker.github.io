<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>3 emnlp-2013-A Corpus Level MIRA Tuning Strategy for Machine Translation</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-3" href="#">emnlp2013-3</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>3 emnlp-2013-A Corpus Level MIRA Tuning Strategy for Machine Translation</h1>
<br/><p>Source: <a title="emnlp-2013-3-pdf" href="http://aclweb.org/anthology//D/D13/D13-1083.pdf">pdf</a></p><p>Author: Ming Tan ; Tian Xia ; Shaojun Wang ; Bowen Zhou</p><p>Abstract: MIRA based tuning methods have been widely used in statistical machine translation (SMT) system with a large number of features. Since the corpus-level BLEU is not decomposable, these MIRA approaches usually define a variety of heuristic-driven sentencelevel BLEUs in their model losses. Instead, we present a new MIRA method, which employs an exact corpus-level BLEU to compute the model loss. Our method is simpler in implementation. Experiments on Chinese-toEnglish translation show its effectiveness over two state-of-the-art MIRA implementations.</p><p>Reference: <a title="emnlp-2013-3-reference" href="../emnlp2013_reference/emnlp-2013-A_Corpus_Level_MIRA_Tuning_Strategy_for_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A Corpus Level MIRA Tuning Strategy for Machine Translation  Ming Tan, Tian Xia, Shaojun Wang Wright State University 3640 Colonel Glenn Hwy, Dayton, OH 45435 USA {t an 6 xia 7 shao j un wang}  . [sent-1, score-0.036]
</p><p>2 Abstract MIRA based tuning methods have been widely used in statistical machine translation (SMT) system with a large number of features. [sent-6, score-0.247]
</p><p>3 Since the corpus-level BLEU is not decomposable, these MIRA approaches usually define a variety of heuristic-driven sentencelevel BLEUs in their model losses. [sent-7, score-0.048]
</p><p>4 Experiments on Chinese-toEnglish translation show its effectiveness over two state-of-the-art MIRA implementations. [sent-10, score-0.052]
</p><p>5 1 Introduction  Margin infused relaxed algorithm (MIRA) has been widely adopted for the parameter optimization in SMT with a large feature size (Watanabe et al. [sent-11, score-0.131]
</p><p>6 Since BLEU is defined on the corpus, and not decomposed into sentences, most MIRA approaches consider a variety of sentence-level BLEUs for the model losses, many of which are heuristic-driven (Watanabe et al. [sent-15, score-0.054]
</p><p>7 The sentence-level BLEU appearing in the objective is generally based on a pseudo-document, which may not precisely reflect the corpus-level BLEU. [sent-19, score-0.03]
</p><p>8 We believe that this mismatch could potentially harm the performance. [sent-20, score-0.054]
</p><p>9 To avoid the sentence BLEU, the work in (Haddow et al. [sent-21, score-0.031]
</p><p>10 , 2009) technique to search the hope and fear hypotheses, and they did not compare with MIRA. [sent-29, score-0.351]
</p><p>11 Watanabe (2012) also tuned the parameters with small batches of sentences and optimized a hinge loss not explicitly related to BLEU using stochastic gradient descent. [sent-30, score-0.192]
</p><p>12 In contrast, we propose a remarkably simple but efficient batch MIRA approach which exploits the exact corpus-level BLEU to compute model losses. [sent-32, score-0.193]
</p><p>13 We search for a hope and a fear hypotheses for the corpus with a straightforward approach and minimize the structured hinge loss defined on them. [sent-33, score-0.607]
</p><p>14 The experiments show that our method consistently outperforms two state-of-the-art MIRAs in Chinese-toEnglish translation tasks with a moderate margin. [sent-34, score-0.052]
</p><p>15 2  Margin Infused Relaxed Algorithm  We optimize the model parameters based on N-best  lists. [sent-35, score-0.028]
</p><p>16 Our development (dev) set is a set of triples {(fi, ei , ri)}iM=1, where fi is a source-language sentence, corresponded by a list of target-language hy-  {eij}jN=(1fi),  potheses ei = with a number of references ri. [sent-36, score-0.301]
</p><p>17 Generally, most decoders return a top-1 candidate as the translation result, such that ei (w) = arg maxj w · h( eij ), where w are the model parameters. [sent-38, score-0.526]
</p><p>18 MIRA is an instance of online learning which assumes an overlap of the decoding procedure and the parameter optimization procedure. [sent-41, score-0.132]
</p><p>19 hc o2d0s1 i3n A Nsastoucria lti Loan fgoura Cgoem Ppruotcaetsiosin agl, L piang eusis 8t5ic1s–856, is performed after an input sentence are decoded, and the next sentence is decoded with the updated parameters. [sent-46, score-0.107]
</p><p>20 The objective for each sentence iis,  21||w − w0||2 + C · li(w) li(w) = meiajx{b(ei∗) − b(eij) mwin  −w ·  [h(ei∗) − h(  eij  )] }  (1)  (2)  where ei∗ ∈ ei is a hope candidate, w0 is the parameter vec∈tor e from the last sentence. [sent-47, score-0.687]
</p><p>21 Since MIRA defines its objective only based on the current sentence, b(·) is a sentence-level BLEU. [sent-48, score-0.03]
</p><p>22 Most MIRA algorithms need a deliberate definition of b(·), since BLEU cannot be decomposed into stieonnte onfc be(s·. [sent-49, score-0.054]
</p><p>23 ,T shine types EofU t chea nsnenotte bnece d eBcoLmEUpo sceadlcu inlatotion includes: (a) a smoothed version of BLEU for eij (Liang et al. [sent-50, score-0.37]
</p><p>24 , 2006), (b) fit eij into a pseudodocument considering the history (Chiang et al. [sent-51, score-0.338]
</p><p>25 , 2008; Chiang, 2012), (c) use eij to replace the corresponding hypothesis in the oracles (Watanabe et al. [sent-52, score-0.415]
</p><p>26 The sentence-level BLEU sometimes perplexes the algorithms and results in a mismatch with the corpus-level BLEU. [sent-54, score-0.054]
</p><p>27 1 Algorithm We propose a batch tuning strategy, corpus-level MIRA (c-MIRA), in which an objective is not built upon a hinge loss of a single sentence, but upon that of the entire corpus. [sent-56, score-0.524]
</p><p>28 eE,iis the hypothesis of the source senteXnce fi covered by E. [sent-63, score-0.084]
</p><p>29 E is corresponded to a corpus-level BcoLvEeUred, w byhic Eh. [sent-64, score-0.066]
</p><p>30 wEe i su cltiomrreatseplyon dweadnt t oto ao cpotirmpuizs-el. [sent-65, score-0.026]
</p><p>31 , 852 2008), c-MIRA repeatedly optimizes, mwin lcorpus (w) =  12||w − w0||2+ C · lcorpus(w) (3) max{B(E∗) − B(E) −w ·  [H(E∗) − H(E)]}  (4)  where B(·) is a corpus-level BLEU. [sent-68, score-0.144]
</p><p>32 E is ∈ L co, rwpuhesr-ele vLe ils B BtLheE hypothesis space  hofy tphoeth heenstiisr. [sent-70, score-0.077]
</p><p>33 {(fi,ei,ri)}iM=1, w0, C · T d)o} ·,ET0 d=o {} Initialize the hope and fear 1 , E· · ·= =M { do 1=· argmeaijx[wt−1 · h(eij) + b0(eij)]  11:  E∗ ← E∗ + {eE∗ ,i} EE0 ←← E E0 ++ {eE0,i} endE Efor← 4B = B(E∗) B(E0) 44H == BH((hEE0)) −− B H(E(E∗) α = minhC,4B+||w4tH−||12·4Hi  12:  wt  13:  −  =  . [sent-73, score-0.401]
</p><p>34 = argmeaijx[wt−1 · h(eij) − b0(eij)]  wt−h1  − α ·  Build the hope Build the fear  the BLEU difference ithe feature difference  4H  w¯t=t +1 1tX=t0wt  14: end for 15: return w¯ t with the optimal BLEU on the dev set. [sent-77, score-0.494]
</p><p>35 n Ct rwamithone hope E∗ and one fear E0 admits a closed-form oupndea hteo paned E pearnfodrm ons ew feelalr. [sent-83, score-0.377]
</p><p>36 W Ee denote one execution of the outer loop as an epoch. [sent-84, score-0.037]
</p><p>37 , 2008), the hope and fear hypotheses are defined as following, E∗  =  max[w  H(E) + B(E)]  (5)  E0  =  max[w · H(E) − B(E)]  (6)  ·  Eq. [sent-87, score-0.444]
</p><p>38 5 and 6 find the hypotheses with the best and worse BLEU that the decoder can easily achieve. [sent-88, score-0.093]
</p><p>39 It is unnecessary to search the entire space of L for precise cseoslusatiryon t oE∗ saenadrc Eh0, t bee ecanutisree sMpaIRceA only faot-r tempts to separate the hope from the fear by a margin proportional to their BLEU differentials (Cherry and Foster, 2012). [sent-89, score-0.382]
</p><p>40 A smoothed BLEU is good enough to pick up a “satisfying” pair of hope and fear. [sent-91, score-0.166]
</p><p>41 However, the updating step (Line 11) uses the corpus-level BLEU. [sent-92, score-0.028]
</p><p>42 2  Justification  c-MIRA treats a corpus as one sentence for decoding, while conventional decoders process sentences one by one. [sent-94, score-0.066]
</p><p>43 We search a hypothesis on corpus E = { e1,k1 , e2,k2 , . [sent-97, score-0.051]
</p><p>44 7 shows that tchoen fsetaantutr we ivtehcPrt eors poefc Etiv ies tdoet eEr. [sent-106, score-0.032]
</p><p>45 Also, hthee mumod oelf score can be decomposed into each sentence in Eq. [sent-108, score-0.085]
</p><p>46 8, which shows that decoding all sentences together equals to decoding one by one. [sent-109, score-0.084]
</p><p>47 We also show that if the metric is decomposable, the loss in c-MIRA is actually the sum of the hinge loss li(w) in structural SVM (Tsochantaridis et al. [sent-110, score-0.292]
</p><p>48 We assume B(eij) to be the metric of a sentence hypothesis, then the 853 loss of c-MIRA in Eq. [sent-112, score-0.109]
</p><p>49 , 2004), we optimize the same loss with a MIRA pattern in a simpler way. [sent-114, score-0.106]
</p><p>50 However, since BLEU is not decomposable, the structural SVM (Cherry and Foster, 2012) uses an interpolated sentence BLEU (Liang et al. [sent-115, score-0.082]
</p><p>51 Although Algorithm 1 has an outlook similar to the batch-MIRA algorithm in (Cherry and Foster, 2012), their loss definitions differ fundamentally. [sent-117, score-0.078]
</p><p>52 Batch MIRA basically uses a sentence-level loss, and they also follow the sentence-by-sentence tuning pattern. [sent-118, score-0.138]
</p><p>53 In the future work, we will compare structural SVM and c-MIRA under decomposable metrics like WER or SSER (Och and Ney, 2002). [sent-119, score-0.148]
</p><p>54 4  Experiments and Analysis  We first evaluate c-MIRA in a iterative batch tuning procedure in a Chinese-to-English machine translation system with 228 features. [sent-120, score-0.411]
</p><p>55 Here, we roughly choose C with the best BLEU on dev set, from {0. [sent-126, score-0.117]
</p><p>56 We conduct experiments in a server of 8-cores with 2. [sent-143, score-0.037]
</p><p>57 We set the maximum number of epochs as we generally do not observe an obvious increase on the dev set BLEU. [sent-145, score-0.175]
</p><p>58 05 level) are compared with MIRA2 The epoch size for MIRA1 and MIRA2 is 40, while the one for c-MIRA is 400. [sent-166, score-0.068]
</p><p>59 Also, we increase the epoch sizes of MIRA1 and MIRA2 to 400, and find there is no improvement on their performance. [sent-169, score-0.068]
</p><p>60 1 Iterative Batch Training In this experiment, we conduct the batch tuning procedure shown in section 3. [sent-171, score-0.368]
</p><p>61 We align the FBIS data  including about 230K sentence pairs with GIZA++ for extracting grammar, and train a 4-gram language model on the Xinhua portion of Gigaword corpus. [sent-172, score-0.031]
</p><p>62 A hierarchical phrase-based model (Chiang, 2007) is tuned on NIST MT 2002, which has 878 sentences, and tested on MT 2004, 2005, 2006, and 2008. [sent-173, score-0.028]
</p><p>63 length on each side of a hierarchical grammar is lim-  ×  ited to 10. [sent-182, score-0.071]
</p><p>64 We select the iteration with the best BLEU of the dev set for testing. [sent-188, score-0.117]
</p><p>65 In the first case, due to the small feature size, MERT can get a better BLEU of the dev set, and all MIRA algorithms fails to generally beat MERT on the test set. [sent-190, score-0.117]
</p><p>66 However, as the feature size increase to 228, MERT degrades on the dev-set BLEU, and also become worse on test sets, while MIRA algorithms improve on the dev set expectedly. [sent-191, score-0.117]
</p><p>67 Only MIRA2 is fairly faster than c-MIRA because of more epochs in c-MIRA. [sent-203, score-0.058]
</p><p>68 2  Re-ranking Experiments  The baseline system is a state-of-the-art hierarchical phrase-based system, and trained on six million parallel sentences corpora available to the DARPA BOLT Chinese-English task. [sent-205, score-0.028]
</p><p>69 This system includes 51 dense features (including translation probabilities, provenance features, etc. [sent-206, score-0.078]
</p><p>70 The language model is a six-gram model trained on a 10 billion words monolingual corpus, including the English side of our parallel corpora plus other corpora such as Gigaword (LDC201 1T07) and Google News. [sent-208, score-0.043]
</p><p>71 We use 1275 sentences for tuning and 1239 sentences for testing from the LDC2010E30 corpus respectively. [sent-209, score-0.165]
</p><p>72 There are four reference translations for each input sentence in both tuning and testing datasets. [sent-210, score-0.196]
</p><p>73 MIRA1MIRA2c-MIRA about 1,966,72035,120400 Table 4: Times of updating model parameters. [sent-224, score-0.028]
</p><p>74 put of the baseline system optimized on TER-BLEU instead of BLEU. [sent-225, score-0.029]
</p><p>75 Before the re-ranking task, the initial BLEUs of the top-1 hypotheses on the tuning and testing set are 31. [sent-226, score-0.258]
</p><p>76 The average numbers of hypotheses per sentence are about 200 and 500, respectively for the tuning and testing sets. [sent-229, score-0.289]
</p><p>77 Again, we use the best epoch on the tuning set for testing. [sent-230, score-0.206]
</p><p>78 The BLEUs on dev and test sets are reported in Table 3. [sent-231, score-0.117]
</p><p>79 1), we use two hope/fear building strategies to get gE. [sent-235, score-0.038]
</p><p>80 ∗ 1 a)n, dw Ee 0u : (1) simply connect edainchg ei∗ aatnedg ei0 i tno L geinte E 4∼a5n do fE Algorithm 1, (2) conduct a slow beamin s Leianrceh 4 among Athlgeo rNit-bhemst 1 ,li (st2s) coof nadllu fcotr a-  ×  eign sentences from e1 to eM and use Eq. [sent-236, score-0.089]
</p><p>81 We observe that there is no significant difference between the two strategies on the BLEU of the dev set. [sent-239, score-0.155]
</p><p>82 But the second strategy is about 10 times slower. [sent-240, score-0.038]
</p><p>83 By beam search, we obtain one corpus-level oracle and 29 other hypotheses similar to (Chiang et al. [sent-243, score-0.093]
</p><p>84 As shown in Table 4, in one execution, our method updates the parameters by only 400 times; MIRA2 updates by 40 878 = 35120 times; and MIRA1 updates m byuc 4h0 more (about 1,966,720 times) due to the SMO procedure. [sent-246, score-0.105]
</p><p>85 Regarding simplicity, MIRA1 uses a strongly-  heuristic definition of a sentence BLEU, and MIRA2 needs a pseudo-document with a decay rate of γ = 0. [sent-249, score-0.031]
</p><p>86 In comparison, c-MIRA avoids both the sentence level BLEU and the pseudo-document, thus needs fewer variables. [sent-251, score-0.031]
</p><p>87 5  Conclusion  We present a simple and effective MIRA batch tuning algorithm without the heuristic-driven calculation of sentence-level BLEU, due to the indecomposability of a corpus-level BLEU. [sent-252, score-0.331]
</p><p>88 Our optimization objective is directly defined on the corpus-level hypotheses. [sent-253, score-0.081]
</p><p>89 This work simplifies the tuning process, and avoid the mismatch between the sentencelevel BLEU and the corpus-level BLEU. [sent-254, score-0.24]
</p><p>90 This strategy can be potentially applied to other optimization paradigms, such as the structural SVM (Cherry and Foster, 2012), SGD and AROW (Chiang, 2012), and other forms of samples, such as forests (Chiang, 2012) and lattice (Cherry and Foster, 2012). [sent-255, score-0.14]
</p><p>91 Hope and fear for discriminative training of statistical translation models. [sent-279, score-0.298]
</p><p>92 Online largemargin training of syntactic and structural translation features. [sent-293, score-0.103]
</p><p>93 Optimization strategies for online large-margin learning in machine translation. [sent-313, score-0.105]
</p><p>94 Orange: a method for evaluating automatic evaluation metrics for machine translation. [sent-353, score-0.028]
</p><p>95 Discriminative training and maximum entropy models for statistical machine translation. [sent-366, score-0.057]
</p><p>96 BLEU: a method for automatic evaluation of machine translation. [sent-374, score-0.028]
</p><p>97 Support vector machine learning for interdependent and structured output spaces. [sent-389, score-0.028]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mira', 0.429), ('bleu', 0.355), ('eij', 0.338), ('chiang', 0.239), ('fear', 0.217), ('batch', 0.193), ('foster', 0.167), ('cherry', 0.16), ('bleus', 0.152), ('tuning', 0.138), ('hope', 0.134), ('dev', 0.117), ('ei', 0.101), ('decomposable', 0.097), ('hypotheses', 0.093), ('lcorpus', 0.091), ('miras', 0.091), ('hinge', 0.085), ('watanabe', 0.084), ('haddow', 0.079), ('tsochantaridis', 0.079), ('loss', 0.078), ('mert', 0.073), ('epoch', 0.068), ('arun', 0.064), ('argmeaijx', 0.061), ('cmira', 0.061), ('efor', 0.061), ('ee', 0.059), ('epochs', 0.058), ('im', 0.057), ('ix', 0.056), ('decomposed', 0.054), ('mismatch', 0.054), ('mwin', 0.053), ('smo', 0.053), ('translation', 0.052), ('och', 0.052), ('structural', 0.051), ('hypothesis', 0.051), ('optimization', 0.051), ('wt', 0.05), ('infused', 0.048), ('sentencelevel', 0.048), ('crammer', 0.047), ('xm', 0.047), ('decoded', 0.045), ('feat', 0.045), ('side', 0.043), ('decoding', 0.042), ('corresponded', 0.04), ('ob', 0.04), ('reversed', 0.04), ('watson', 0.04), ('online', 0.039), ('ibm', 0.038), ('strategy', 0.038), ('strategies', 0.038), ('conduct', 0.037), ('execution', 0.037), ('iis', 0.036), ('xia', 0.036), ('updates', 0.035), ('svm', 0.035), ('decoders', 0.035), ('jmlr', 0.035), ('fi', 0.033), ('ies', 0.032), ('smoothed', 0.032), ('relaxed', 0.032), ('margin', 0.031), ('sentence', 0.031), ('objective', 0.03), ('optimized', 0.029), ('zi', 0.029), ('statistical', 0.029), ('updating', 0.028), ('liang', 0.028), ('optimize', 0.028), ('gigaword', 0.028), ('machine', 0.028), ('hierarchical', 0.028), ('max', 0.028), ('papineni', 0.027), ('testing', 0.027), ('bh', 0.026), ('burch', 0.026), ('byhic', 0.026), ('fbis', 0.026), ('fcotr', 0.026), ('hofy', 0.026), ('hteo', 0.026), ('hwy', 0.026), ('ithe', 0.026), ('lsin', 0.026), ('nadllu', 0.026), ('oracles', 0.026), ('oto', 0.026), ('potheses', 0.026), ('provenance', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="3-tfidf-1" href="./emnlp-2013-A_Corpus_Level_MIRA_Tuning_Strategy_for_Machine_Translation.html">3 emnlp-2013-A Corpus Level MIRA Tuning Strategy for Machine Translation</a></p>
<p>Author: Ming Tan ; Tian Xia ; Shaojun Wang ; Bowen Zhou</p><p>Abstract: MIRA based tuning methods have been widely used in statistical machine translation (SMT) system with a large number of features. Since the corpus-level BLEU is not decomposable, these MIRA approaches usually define a variety of heuristic-driven sentencelevel BLEUs in their model losses. Instead, we present a new MIRA method, which employs an exact corpus-level BLEU to compute the model loss. Our method is simpler in implementation. Experiments on Chinese-toEnglish translation show its effectiveness over two state-of-the-art MIRA implementations.</p><p>2 0.19192672 <a title="3-tfidf-2" href="./emnlp-2013-Max-Violation_Perceptron_and_Forced_Decoding_for_Scalable_MT_Training.html">128 emnlp-2013-Max-Violation Perceptron and Forced Decoding for Scalable MT Training</a></p>
<p>Author: Heng Yu ; Liang Huang ; Haitao Mi ; Kai Zhao</p><p>Abstract: While large-scale discriminative training has triumphed in many NLP problems, its definite success on machine translation has been largely elusive. Most recent efforts along this line are not scalable (training on the small dev set with features from top ∼100 most frequent wt woridths) f eaantdu overly complicated. oWste f iren-stead present a very simple yet theoretically motivated approach by extending the recent framework of “violation-fixing perceptron”, using forced decoding to compute the target derivations. Extensive phrase-based translation experiments on both Chinese-to-English and Spanish-to-English tasks show substantial gains in BLEU by up to +2.3/+2.0 on dev/test over MERT, thanks to 20M+ sparse features. This is the first successful effort of large-scale online discriminative training for MT. 1Introduction Large-scale discriminative training has witnessed great success in many NLP problems such as parsing (McDonald et al., 2005) and tagging (Collins, 2002), but not yet for machine translation (MT) despite numerous recent efforts. Due to scalability issues, most of these recent methods can only train on a small dev set of about a thousand sentences rather than on the full training set, and only with 2,000–10,000 rather “dense-like” features (either unlexicalized or only considering highest-frequency words), as in MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang, 2012), PRO (Hopkins and May, 2011), and RAMP (Gimpel and Smith, 2012). However, it is well-known that the most important features for NLP are lexicalized, most of which can not ∗ Work done while visiting City University of New York. Corresponding author. † 1112 be seen on a small dataset. Furthermore, these methods often involve complicated loss functions and intricate choices of the “target” derivations to update towards or against (e.g. k-best/forest oracles, or hope/fear derivations), and are thus hard to replicate. As a result, the classical method of MERT (Och, 2003) remains the default training algorithm for MT even though it can only tune a handful of dense features. See also Section 6 for other related work. As a notable exception, Liang et al. (2006) do train a structured perceptron model on the training data with sparse features, but fail to outperform MERT. We argue this is because structured perceptron, like many structured learning algorithms such as CRF and MIRA, assumes exact search, and search errors inevitably break theoretical properties such as convergence (Huang et al., 2012). Empirically, it is now well accepted that standard perceptron performs poorly when search error is severe (Collins and Roark, 2004; Zhang et al., 2013). To address the search error problem we propose a very simple approach based on the recent framework of “violation-fixing perceptron” (Huang et al., 2012) which is designed specifically for inexact search, with a theoretical convergence guarantee and excellent empirical performance on beam search parsing and tagging. The basic idea is to update when search error happens, rather than at the end of the search. To adapt it to MT, we extend this framework to handle latent variables corresponding to the hidden derivations. We update towards “gold-standard” derivations computed by forced decoding so that each derivation leads to the exact reference translation. Forced decoding is also used as a way of data selection, since those reachable sentence pairs are generally more literal and of higher quality, which the training should focus on. When the reachable subset is small for some language pairs, we augment Proce Sdeiantgtlse o,f W thaesh 2i0n1gt3o nC,o UnSfeAre,n 1c8e- o2n1 E Omctpoibriecra 2l0 M13et.h ?oc d2s0 i1n3 N Aastusorcaila Ltiaon g fuoarg Ceo Pmrpoucetastsi on ga,l p Laignegsu 1is1t1ic2s–1 23, it by including reachable prefix-pairs when the full sentence pair is not. We make the following contributions: 1. Our work is the first successful effort to scale online structured learning to a large portion of the training data (as opposed to the dev set). 2. Our work is the first to use a principled learning method customized for inexact search which updates on partial derivations rather than full ones in order to fix search errors. We adapt it to MT using latent variables for derivations. 3. Contrary to the common wisdom, we show that simply updating towards the exact reference translation is helpful, which is much simpler than k-best/forest oracles or loss-augmented (e.g. hope/fear) derivations, avoiding sentencelevel BLEU scores or other loss functions. 4. We present a convincing analysis that it is the search errors and standard perceptron’s inability to deal with them that prevent previous work, esp. Liang et al. (2006), from succeeding. 5. Scaling to the training data enables us to engineer a very rich feature set of sparse, lexicalized, and non-local features, and we propose various ways to alleviate overfitting. For simplicity and efficiency reasons, in this paper we use phrase-based translation, but our method has the potential to be applicable to other translation paradigms. Extensive experiments on both Chineseto-English and Spanish-to-English tasks show statistically significant gains in BLEU by up to +2.3/+2.0 on dev/test over MERT, and up to +1.5/+1.5 over PRO, thanks to 20M+ sparse features. 2 Phrase-Based MT and Forced Decoding We first review the basic phrase-based decoding algorithm (Koehn, 2004), which will be adapted for forced decoding. 2.1 Background: Phrase-based Decoding We will use the following running example from Chinese to English from Mi et al. (2008): 0123456 Figure 1: Standard beam-search phrase-based decoding. B `ush´ ı y uˇ Sh¯ al´ ong j ˇux ´ıng le hu` ıt´ an Bush with Sharon hold -ed meeting ‘Bush held a meeting with Sharon’ Phrase-based decoders generate partial targetlanguage outputs in left-to-right order in the form of hypotheses (or states) (Koehn, 2004). Each hypothesis has a coverage vector capturing the sourcelanguage words translated so far, and can be extended into a longer hypothesis by a phrase-pair translating an uncovered segment. For example, the following is one possible derivation: (• 3(• •() • :1( •s063),:“(Bs)u2s:,h)“(hBs:e1ul(d,s0“ht,aB“hleuk”ls) hdw”t)ailhkrsS1”h)aro2n”)r3 where a • in the coverage vector indicates the source wwoherdre a at •th i ns position aisg e“ vcoecvteorred in”d iacnadte ws thheer seo euarcche si is the score of each state, each adding the rule score and the distortion cost (dc) to the score of the previous state. To compute the distortion cost we also need to maintain the ending position of the last phrase (e.g., the 3 and 6 in the coverage vectors). In phrase-based translation there is also a distortionlimit which prohibits long-distance reorderings. The above states are called −LM states since they do Tnhoet ainbovovleve st language mlleodd −el LcMos tsst.a eTso iandcde a beiygram model, we split each −LM state into a series ogrfa +mL mMo states; ee sapchli t+ eaLcMh −staLtMe h satsa ttehe in ftoor ma (v,a) where a is the last word of the hypothesis. Thus a +LM version of the above derivation might be: (• 3(• ,(•Sh1a•(r6o0,nta)l:ks,()Bsu:03sh,(s“<)s02</p><p>3 0.17529313 <a title="3-tfidf-3" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Dhruv Batra ; Chris Dyer ; Gregory Shakhnarovich</p><p>Abstract: This paper addresses the problem of producing a diverse set of plausible translations. We present a simple procedure that can be used with any statistical machine translation (MT) system. We explore three ways of using diverse translations: (1) system combination, (2) discriminative reranking with rich features, and (3) a novel post-editing scenario in which multiple translations are presented to users. We find that diversity can improve performance on these tasks, especially for sentences that are difficult for MT.</p><p>4 0.16301337 <a title="3-tfidf-4" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>Author: Joern Wuebker ; Stephan Peitz ; Felix Rietig ; Hermann Ney</p><p>Abstract: Automatically clustering words from a monolingual or bilingual training corpus into classes is a widely used technique in statistical natural language processing. We present a very simple and easy to implement method for using these word classes to improve translation quality. It can be applied across different machine translation paradigms and with arbitrary types of models. We show its efficacy on a small German→English and a larger F ornenc ah s→mGalelrm Gaenrm mtarann→slEatniognli tsahsk a nwdit ha lbaortghe rst Farnednacrhd→ phrase-based salandti nhie traaskrch wiciathl phrase-based translation systems for a common set of models. Our results show that with word class models, the baseline can be improved by up to 1.4% BLEU and 1.0% TER on the French→German task and 0.3% BLEU aonnd t h1e .1 F%re nTcEhR→ on tehrem German→English Btask.</p><p>5 0.15822506 <a title="3-tfidf-5" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>Author: Xinyan Xiao ; Deyi Xiong</p><p>Abstract: Traditional synchronous grammar induction estimates parameters by maximizing likelihood, which only has a loose relation to translation quality. Alternatively, we propose a max-margin estimation approach to discriminatively inducing synchronous grammars for machine translation, which directly optimizes translation quality measured by BLEU. In the max-margin estimation of parameters, we only need to calculate Viterbi translations. This further facilitates the incorporation of various non-local features that are defined on the target side. We test the effectiveness of our max-margin estimation framework on a competitive hierarchical phrase-based system. Experiments show that our max-margin method significantly outperforms the traditional twostep pipeline for synchronous rule extraction by 1.3 BLEU points and is also better than previous max-likelihood estimation method.</p><p>6 0.14929166 <a title="3-tfidf-6" href="./emnlp-2013-Regularized_Minimum_Error_Rate_Training.html">159 emnlp-2013-Regularized Minimum Error Rate Training</a></p>
<p>7 0.1386753 <a title="3-tfidf-7" href="./emnlp-2013-Efficient_Left-to-Right_Hierarchical_Phrase-Based_Translation_with_Improved_Reordering.html">71 emnlp-2013-Efficient Left-to-Right Hierarchical Phrase-Based Translation with Improved Reordering</a></p>
<p>8 0.12982298 <a title="3-tfidf-8" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>9 0.10547532 <a title="3-tfidf-9" href="./emnlp-2013-What_is_Hidden_among_Translation_Rules.html">201 emnlp-2013-What is Hidden among Translation Rules</a></p>
<p>10 0.098804049 <a title="3-tfidf-10" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>11 0.091431014 <a title="3-tfidf-11" href="./emnlp-2013-A_Convex_Alternative_to_IBM_Model_2.html">2 emnlp-2013-A Convex Alternative to IBM Model 2</a></p>
<p>12 0.086317509 <a title="3-tfidf-12" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>13 0.076598614 <a title="3-tfidf-13" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>14 0.073505618 <a title="3-tfidf-14" href="./emnlp-2013-Optimal_Beam_Search_for_Machine_Translation.html">145 emnlp-2013-Optimal Beam Search for Machine Translation</a></p>
<p>15 0.070531808 <a title="3-tfidf-15" href="./emnlp-2013-Factored_Soft_Source_Syntactic_Constraints_for_Hierarchical_Machine_Translation.html">84 emnlp-2013-Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation</a></p>
<p>16 0.069197647 <a title="3-tfidf-16" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>17 0.065369681 <a title="3-tfidf-17" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>18 0.06502676 <a title="3-tfidf-18" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>19 0.063021317 <a title="3-tfidf-19" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>20 0.061986662 <a title="3-tfidf-20" href="./emnlp-2013-Improving_Alignment_of_System_Combination_by_Using_Multi-objective_Optimization.html">101 emnlp-2013-Improving Alignment of System Combination by Using Multi-objective Optimization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.184), (1, -0.227), (2, 0.086), (3, 0.072), (4, 0.11), (5, -0.047), (6, 0.02), (7, -0.001), (8, 0.028), (9, 0.022), (10, -0.069), (11, -0.038), (12, 0.007), (13, -0.092), (14, 0.065), (15, -0.18), (16, 0.024), (17, 0.058), (18, -0.023), (19, 0.107), (20, -0.008), (21, 0.185), (22, -0.126), (23, -0.025), (24, 0.026), (25, 0.099), (26, -0.069), (27, -0.058), (28, -0.049), (29, -0.151), (30, 0.149), (31, 0.013), (32, -0.071), (33, -0.032), (34, -0.016), (35, 0.07), (36, -0.153), (37, -0.141), (38, 0.114), (39, 0.026), (40, -0.07), (41, 0.031), (42, 0.064), (43, -0.049), (44, 0.058), (45, -0.025), (46, -0.022), (47, -0.02), (48, -0.028), (49, 0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95939624 <a title="3-lsi-1" href="./emnlp-2013-A_Corpus_Level_MIRA_Tuning_Strategy_for_Machine_Translation.html">3 emnlp-2013-A Corpus Level MIRA Tuning Strategy for Machine Translation</a></p>
<p>Author: Ming Tan ; Tian Xia ; Shaojun Wang ; Bowen Zhou</p><p>Abstract: MIRA based tuning methods have been widely used in statistical machine translation (SMT) system with a large number of features. Since the corpus-level BLEU is not decomposable, these MIRA approaches usually define a variety of heuristic-driven sentencelevel BLEUs in their model losses. Instead, we present a new MIRA method, which employs an exact corpus-level BLEU to compute the model loss. Our method is simpler in implementation. Experiments on Chinese-toEnglish translation show its effectiveness over two state-of-the-art MIRA implementations.</p><p>2 0.82321191 <a title="3-lsi-2" href="./emnlp-2013-Regularized_Minimum_Error_Rate_Training.html">159 emnlp-2013-Regularized Minimum Error Rate Training</a></p>
<p>Author: Michel Galley ; Chris Quirk ; Colin Cherry ; Kristina Toutanova</p><p>Abstract: Minimum Error Rate Training (MERT) remains one of the preferred methods for tuning linear parameters in machine translation systems, yet it faces significant issues. First, MERT is an unregularized learner and is therefore prone to overfitting. Second, it is commonly used on a noisy, non-convex loss function that becomes more difficult to optimize as the number of parameters increases. To address these issues, we study the addition of a regularization term to the MERT objective function. Since standard regularizers such as ‘2 are inapplicable to MERT due to the scale invariance of its objective function, we turn to two regularizers—‘0 and a modification of‘2— and present methods for efficiently integrating them during search. To improve search in large parameter spaces, we also present a new direction finding algorithm that uses the gradient of expected BLEU to orient MERT’s exact line searches. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO, a learner often used with large feature sets.</p><p>3 0.78387898 <a title="3-lsi-3" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Dhruv Batra ; Chris Dyer ; Gregory Shakhnarovich</p><p>Abstract: This paper addresses the problem of producing a diverse set of plausible translations. We present a simple procedure that can be used with any statistical machine translation (MT) system. We explore three ways of using diverse translations: (1) system combination, (2) discriminative reranking with rich features, and (3) a novel post-editing scenario in which multiple translations are presented to users. We find that diversity can improve performance on these tasks, especially for sentences that are difficult for MT.</p><p>4 0.66872209 <a title="3-lsi-4" href="./emnlp-2013-Max-Violation_Perceptron_and_Forced_Decoding_for_Scalable_MT_Training.html">128 emnlp-2013-Max-Violation Perceptron and Forced Decoding for Scalable MT Training</a></p>
<p>Author: Heng Yu ; Liang Huang ; Haitao Mi ; Kai Zhao</p><p>Abstract: While large-scale discriminative training has triumphed in many NLP problems, its definite success on machine translation has been largely elusive. Most recent efforts along this line are not scalable (training on the small dev set with features from top ∼100 most frequent wt woridths) f eaantdu overly complicated. oWste f iren-stead present a very simple yet theoretically motivated approach by extending the recent framework of “violation-fixing perceptron”, using forced decoding to compute the target derivations. Extensive phrase-based translation experiments on both Chinese-to-English and Spanish-to-English tasks show substantial gains in BLEU by up to +2.3/+2.0 on dev/test over MERT, thanks to 20M+ sparse features. This is the first successful effort of large-scale online discriminative training for MT. 1Introduction Large-scale discriminative training has witnessed great success in many NLP problems such as parsing (McDonald et al., 2005) and tagging (Collins, 2002), but not yet for machine translation (MT) despite numerous recent efforts. Due to scalability issues, most of these recent methods can only train on a small dev set of about a thousand sentences rather than on the full training set, and only with 2,000–10,000 rather “dense-like” features (either unlexicalized or only considering highest-frequency words), as in MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang, 2012), PRO (Hopkins and May, 2011), and RAMP (Gimpel and Smith, 2012). However, it is well-known that the most important features for NLP are lexicalized, most of which can not ∗ Work done while visiting City University of New York. Corresponding author. † 1112 be seen on a small dataset. Furthermore, these methods often involve complicated loss functions and intricate choices of the “target” derivations to update towards or against (e.g. k-best/forest oracles, or hope/fear derivations), and are thus hard to replicate. As a result, the classical method of MERT (Och, 2003) remains the default training algorithm for MT even though it can only tune a handful of dense features. See also Section 6 for other related work. As a notable exception, Liang et al. (2006) do train a structured perceptron model on the training data with sparse features, but fail to outperform MERT. We argue this is because structured perceptron, like many structured learning algorithms such as CRF and MIRA, assumes exact search, and search errors inevitably break theoretical properties such as convergence (Huang et al., 2012). Empirically, it is now well accepted that standard perceptron performs poorly when search error is severe (Collins and Roark, 2004; Zhang et al., 2013). To address the search error problem we propose a very simple approach based on the recent framework of “violation-fixing perceptron” (Huang et al., 2012) which is designed specifically for inexact search, with a theoretical convergence guarantee and excellent empirical performance on beam search parsing and tagging. The basic idea is to update when search error happens, rather than at the end of the search. To adapt it to MT, we extend this framework to handle latent variables corresponding to the hidden derivations. We update towards “gold-standard” derivations computed by forced decoding so that each derivation leads to the exact reference translation. Forced decoding is also used as a way of data selection, since those reachable sentence pairs are generally more literal and of higher quality, which the training should focus on. When the reachable subset is small for some language pairs, we augment Proce Sdeiantgtlse o,f W thaesh 2i0n1gt3o nC,o UnSfeAre,n 1c8e- o2n1 E Omctpoibriecra 2l0 M13et.h ?oc d2s0 i1n3 N Aastusorcaila Ltiaon g fuoarg Ceo Pmrpoucetastsi on ga,l p Laignegsu 1is1t1ic2s–1 23, it by including reachable prefix-pairs when the full sentence pair is not. We make the following contributions: 1. Our work is the first successful effort to scale online structured learning to a large portion of the training data (as opposed to the dev set). 2. Our work is the first to use a principled learning method customized for inexact search which updates on partial derivations rather than full ones in order to fix search errors. We adapt it to MT using latent variables for derivations. 3. Contrary to the common wisdom, we show that simply updating towards the exact reference translation is helpful, which is much simpler than k-best/forest oracles or loss-augmented (e.g. hope/fear) derivations, avoiding sentencelevel BLEU scores or other loss functions. 4. We present a convincing analysis that it is the search errors and standard perceptron’s inability to deal with them that prevent previous work, esp. Liang et al. (2006), from succeeding. 5. Scaling to the training data enables us to engineer a very rich feature set of sparse, lexicalized, and non-local features, and we propose various ways to alleviate overfitting. For simplicity and efficiency reasons, in this paper we use phrase-based translation, but our method has the potential to be applicable to other translation paradigms. Extensive experiments on both Chineseto-English and Spanish-to-English tasks show statistically significant gains in BLEU by up to +2.3/+2.0 on dev/test over MERT, and up to +1.5/+1.5 over PRO, thanks to 20M+ sparse features. 2 Phrase-Based MT and Forced Decoding We first review the basic phrase-based decoding algorithm (Koehn, 2004), which will be adapted for forced decoding. 2.1 Background: Phrase-based Decoding We will use the following running example from Chinese to English from Mi et al. (2008): 0123456 Figure 1: Standard beam-search phrase-based decoding. B `ush´ ı y uˇ Sh¯ al´ ong j ˇux ´ıng le hu` ıt´ an Bush with Sharon hold -ed meeting ‘Bush held a meeting with Sharon’ Phrase-based decoders generate partial targetlanguage outputs in left-to-right order in the form of hypotheses (or states) (Koehn, 2004). Each hypothesis has a coverage vector capturing the sourcelanguage words translated so far, and can be extended into a longer hypothesis by a phrase-pair translating an uncovered segment. For example, the following is one possible derivation: (• 3(• •() • :1( •s063),:“(Bs)u2s:,h)“(hBs:e1ul(d,s0“ht,aB“hleuk”ls) hdw”t)ailhkrsS1”h)aro2n”)r3 where a • in the coverage vector indicates the source wwoherdre a at •th i ns position aisg e“ vcoecvteorred in”d iacnadte ws thheer seo euarcche si is the score of each state, each adding the rule score and the distortion cost (dc) to the score of the previous state. To compute the distortion cost we also need to maintain the ending position of the last phrase (e.g., the 3 and 6 in the coverage vectors). In phrase-based translation there is also a distortionlimit which prohibits long-distance reorderings. The above states are called −LM states since they do Tnhoet ainbovovleve st language mlleodd −el LcMos tsst.a eTso iandcde a beiygram model, we split each −LM state into a series ogrfa +mL mMo states; ee sapchli t+ eaLcMh −staLtMe h satsa ttehe in ftoor ma (v,a) where a is the last word of the hypothesis. Thus a +LM version of the above derivation might be: (• 3(• ,(•Sh1a•(r6o0,nta)l:ks,()Bsu:03sh,(s“<)s02</p><p>5 0.56611282 <a title="3-lsi-5" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>Author: Joern Wuebker ; Stephan Peitz ; Felix Rietig ; Hermann Ney</p><p>Abstract: Automatically clustering words from a monolingual or bilingual training corpus into classes is a widely used technique in statistical natural language processing. We present a very simple and easy to implement method for using these word classes to improve translation quality. It can be applied across different machine translation paradigms and with arbitrary types of models. We show its efficacy on a small German→English and a larger F ornenc ah s→mGalelrm Gaenrm mtarann→slEatniognli tsahsk a nwdit ha lbaortghe rst Farnednacrhd→ phrase-based salandti nhie traaskrch wiciathl phrase-based translation systems for a common set of models. Our results show that with word class models, the baseline can be improved by up to 1.4% BLEU and 1.0% TER on the French→German task and 0.3% BLEU aonnd t h1e .1 F%re nTcEhR→ on tehrem German→English Btask.</p><p>6 0.48690656 <a title="3-lsi-6" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>7 0.48031586 <a title="3-lsi-7" href="./emnlp-2013-A_Convex_Alternative_to_IBM_Model_2.html">2 emnlp-2013-A Convex Alternative to IBM Model 2</a></p>
<p>8 0.45806715 <a title="3-lsi-8" href="./emnlp-2013-Efficient_Left-to-Right_Hierarchical_Phrase-Based_Translation_with_Improved_Reordering.html">71 emnlp-2013-Efficient Left-to-Right Hierarchical Phrase-Based Translation with Improved Reordering</a></p>
<p>9 0.44572866 <a title="3-lsi-9" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>10 0.4383505 <a title="3-lsi-10" href="./emnlp-2013-Converting_Continuous-Space_Language_Models_into_N-Gram_Language_Models_for_Statistical_Machine_Translation.html">52 emnlp-2013-Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation</a></p>
<p>11 0.42912367 <a title="3-lsi-11" href="./emnlp-2013-Decoding_with_Large-Scale_Neural_Language_Models_Improves_Translation.html">55 emnlp-2013-Decoding with Large-Scale Neural Language Models Improves Translation</a></p>
<p>12 0.36457294 <a title="3-lsi-12" href="./emnlp-2013-Optimal_Beam_Search_for_Machine_Translation.html">145 emnlp-2013-Optimal Beam Search for Machine Translation</a></p>
<p>13 0.34584725 <a title="3-lsi-13" href="./emnlp-2013-Improving_Alignment_of_System_Combination_by_Using_Multi-objective_Optimization.html">101 emnlp-2013-Improving Alignment of System Combination by Using Multi-objective Optimization</a></p>
<p>14 0.34541962 <a title="3-lsi-14" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>15 0.33908296 <a title="3-lsi-15" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>16 0.32449394 <a title="3-lsi-16" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>17 0.30731484 <a title="3-lsi-17" href="./emnlp-2013-What_is_Hidden_among_Translation_Rules.html">201 emnlp-2013-What is Hidden among Translation Rules</a></p>
<p>18 0.30549926 <a title="3-lsi-18" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>19 0.30055997 <a title="3-lsi-19" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>20 0.29191723 <a title="3-lsi-20" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.015), (3, 0.015), (18, 0.056), (22, 0.083), (30, 0.131), (43, 0.317), (45, 0.016), (50, 0.024), (51, 0.089), (66, 0.038), (71, 0.016), (75, 0.021), (77, 0.056), (95, 0.018), (96, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.81054956 <a title="3-lda-1" href="./emnlp-2013-Deriving_Adjectival_Scales_from_Continuous_Space_Word_Representations.html">59 emnlp-2013-Deriving Adjectival Scales from Continuous Space Word Representations</a></p>
<p>Author: Joo-Kyung Kim ; Marie-Catherine de Marneffe</p><p>Abstract: Continuous space word representations extracted from neural network language models have been used effectively for natural language processing, but until recently it was not clear whether the spatial relationships of such representations were interpretable. Mikolov et al. (2013) show that these representations do capture syntactic and semantic regularities. Here, we push the interpretation of continuous space word representations further by demonstrating that vector offsets can be used to derive adjectival scales (e.g., okay < good < excellent). We evaluate the scales on the indirect answers to yes/no questions corpus (de Marneffe et al., 2010). We obtain 72.8% accuracy, which outperforms previous results (∼60%) on tichihs corpus aornmd highlights sth rees quality o6f0% the) scales extracted, providing further support that the continuous space word representations are meaningful.</p><p>same-paper 2 0.76621133 <a title="3-lda-2" href="./emnlp-2013-A_Corpus_Level_MIRA_Tuning_Strategy_for_Machine_Translation.html">3 emnlp-2013-A Corpus Level MIRA Tuning Strategy for Machine Translation</a></p>
<p>Author: Ming Tan ; Tian Xia ; Shaojun Wang ; Bowen Zhou</p><p>Abstract: MIRA based tuning methods have been widely used in statistical machine translation (SMT) system with a large number of features. Since the corpus-level BLEU is not decomposable, these MIRA approaches usually define a variety of heuristic-driven sentencelevel BLEUs in their model losses. Instead, we present a new MIRA method, which employs an exact corpus-level BLEU to compute the model loss. Our method is simpler in implementation. Experiments on Chinese-toEnglish translation show its effectiveness over two state-of-the-art MIRA implementations.</p><p>3 0.65821201 <a title="3-lda-3" href="./emnlp-2013-Building_Specialized_Bilingual_Lexicons_Using_Large_Scale_Background_Knowledge.html">42 emnlp-2013-Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge</a></p>
<p>Author: Dhouha Bouamor ; Adrian Popescu ; Nasredine Semmar ; Pierre Zweigenbaum</p><p>Abstract: Bilingual lexicons are central components of machine translation and cross-lingual information retrieval systems. Their manual construction requires strong expertise in both languages involved and is a costly process. Several automatic methods were proposed as an alternative but they often rely on resources available in a limited number of languages and their performances are still far behind the quality of manual translations. We introduce a novel approach to the creation of specific domain bilingual lexicon that relies on Wikipedia. This massively multilingual encyclopedia makes it possible to create lexicons for a large number of language pairs. Wikipedia is used to extract domains in each language, to link domains between languages and to create generic translation dictionaries. The approach is tested on four specialized domains and is compared to three state of the art approaches using two language pairs: FrenchEnglish and Romanian-English. The newly introduced method compares favorably to existing methods in all configurations tested.</p><p>4 0.50487292 <a title="3-lda-4" href="./emnlp-2013-Recurrent_Continuous_Translation_Models.html">156 emnlp-2013-Recurrent Continuous Translation Models</a></p>
<p>Author: Nal Kalchbrenner ; Phil Blunsom</p><p>Abstract: We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.</p><p>5 0.49033651 <a title="3-lda-5" href="./emnlp-2013-Converting_Continuous-Space_Language_Models_into_N-Gram_Language_Models_for_Statistical_Machine_Translation.html">52 emnlp-2013-Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation</a></p>
<p>Author: Rui Wang ; Masao Utiyama ; Isao Goto ; Eiichro Sumita ; Hai Zhao ; Bao-Liang Lu</p><p>Abstract: Neural network language models, or continuous-space language models (CSLMs), have been shown to improve the performance of statistical machine translation (SMT) when they are used for reranking n-best translations. However, CSLMs have not been used in the first pass decoding of SMT, because using CSLMs in decoding takes a lot of time. In contrast, we propose a method for converting CSLMs into back-off n-gram language models (BNLMs) so that we can use converted CSLMs in decoding. We show that they outperform the original BNLMs and are comparable with the traditional use of CSLMs in reranking.</p><p>6 0.48531002 <a title="3-lda-6" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>7 0.4817422 <a title="3-lda-7" href="./emnlp-2013-A_Dataset_for_Research_on_Short-Text_Conversations.html">4 emnlp-2013-A Dataset for Research on Short-Text Conversations</a></p>
<p>8 0.47653764 <a title="3-lda-8" href="./emnlp-2013-Joint_Language_and_Translation_Modeling_with_Recurrent_Neural_Networks.html">113 emnlp-2013-Joint Language and Translation Modeling with Recurrent Neural Networks</a></p>
<p>9 0.4756619 <a title="3-lda-9" href="./emnlp-2013-Flexible_and_Efficient_Hypergraph_Interactions_for_Joint_Hierarchical_and_Forest-to-String_Decoding.html">88 emnlp-2013-Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest-to-String Decoding</a></p>
<p>10 0.47397456 <a title="3-lda-10" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>11 0.47053677 <a title="3-lda-11" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>12 0.46928135 <a title="3-lda-12" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>13 0.469237 <a title="3-lda-13" href="./emnlp-2013-Structured_Penalties_for_Log-Linear_Language_Models.html">176 emnlp-2013-Structured Penalties for Log-Linear Language Models</a></p>
<p>14 0.46705115 <a title="3-lda-14" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>15 0.46628213 <a title="3-lda-15" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>16 0.46603283 <a title="3-lda-16" href="./emnlp-2013-Sarcasm_as_Contrast_between_a_Positive_Sentiment_and_Negative_Situation.html">163 emnlp-2013-Sarcasm as Contrast between a Positive Sentiment and Negative Situation</a></p>
<p>17 0.46440575 <a title="3-lda-17" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>18 0.46277469 <a title="3-lda-18" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>19 0.46103621 <a title="3-lda-19" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>20 0.45841989 <a title="3-lda-20" href="./emnlp-2013-Max-Violation_Perceptron_and_Forced_Decoding_for_Scalable_MT_Training.html">128 emnlp-2013-Max-Violation Perceptron and Forced Decoding for Scalable MT Training</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
