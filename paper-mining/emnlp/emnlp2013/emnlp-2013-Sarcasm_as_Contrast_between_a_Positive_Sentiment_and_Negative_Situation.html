<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>163 emnlp-2013-Sarcasm as Contrast between a Positive Sentiment and Negative Situation</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-163" href="#">emnlp2013-163</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>163 emnlp-2013-Sarcasm as Contrast between a Positive Sentiment and Negative Situation</h1>
<br/><p>Source: <a title="emnlp-2013-163-pdf" href="http://aclweb.org/anthology//D/D13/D13-1066.pdf">pdf</a></p><p>Author: Ellen Riloff ; Ashequl Qadir ; Prafulla Surve ; Lalindra De Silva ; Nathan Gilbert ; Ruihong Huang</p><p>Abstract: A common form of sarcasm on Twitter consists of a positive sentiment contrasted with a negative situation. For example, many sarcastic tweets include a positive sentiment, such as “love” or “enjoy”, followed by an expression that describes an undesirable activity or state (e.g., “taking exams” or “being ignored”). We have developed a sarcasm recognizer to identify this type of sarcasm in tweets. We present a novel bootstrapping algorithm that automatically learns lists of positive sentiment phrases and negative situation phrases from sarcastic tweets. We show that identifying contrasting contexts using the phrases learned through bootstrapping yields improved recall for sarcasm recognition.</p><p>Reference: <a title="emnlp-2013-163-reference" href="../emnlp2013_reference/emnlp-2013-Sarcasm_as_Contrast_between_a_Positive_Sentiment_and_Negative_Situation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract A common form of sarcasm on Twitter consists of a positive sentiment contrasted with a negative situation. [sent-5, score-1.133]
</p><p>2 For example, many sarcastic tweets include a positive sentiment, such as “love” or “enjoy”, followed by an expression that describes an undesirable activity or state (e. [sent-6, score-1.0]
</p><p>3 We have developed a sarcasm recognizer to identify this type of sarcasm in tweets. [sent-9, score-1.132]
</p><p>4 We present a novel bootstrapping algorithm that automatically learns lists of positive sentiment phrases and negative situation phrases from sarcastic tweets. [sent-10, score-1.585]
</p><p>5 We show that identifying contrasting contexts using the phrases learned through bootstrapping yields improved recall for sarcasm recognition. [sent-11, score-0.826]
</p><p>6 Sarcasm can be manifested in many different ways, but recognizing sarcasm is important for natural language processing to avoid misinterpreting sarcastic statements as literal. [sent-13, score-1.118]
</p><p>7 704 In the realm of Twitter, we observed that many sarcastic tweets have a common structure that creates a positive/negative contrast between a sentiment and a situation. [sent-20, score-1.004]
</p><p>8 Specifically, sarcastic tweets often express a positive sentiment in reference to a negative activity or state. [sent-21, score-1.322]
</p><p>9 For example, consider the tweets below, where the positive sentiment terms  are underlined and the negative activity/state terms are italicized. [sent-22, score-0.762]
</p><p>10 The goal of our research is to identify sarcasm that arises from the contrast between a positive sentiment referring to a negative situation. [sent-28, score-1.163]
</p><p>11 We present a bootstrapping algorithm that automatically learns phrases corresponding to positive sentiments and phrases corresponding to negative situations. [sent-37, score-0.706]
</p><p>12 We use tweets that contain a sarcasm hashtag as positive instances for the learning process. [sent-38, score-1.01]
</p><p>13 First, we learn negative situation phrases that follow a positive sentiment (initially, the seed word “love”). [sent-40, score-0.871]
</p><p>14 Second, we learn positive sentiment phrases that occur near a negative situation phrase. [sent-41, score-0.868]
</p><p>15 The bootstrapping process iterates, alternately learning new negative situations and new positive sentiment phrases. [sent-42, score-0.709]
</p><p>16 Finally, we use the learned lists of sentiment and situation phrases to recognize sarcasm in new tweets by identifying contexts that contain a positive sentiment in close proximity to a negative situation phrase. [sent-43, score-2.185]
</p><p>17 Lukin and Walker (2013) explored the potential of a bootstrapping method for sarcasm classification in social dialogue to learn lexical N-gram cues associated with sarcasm (e. [sent-51, score-1.24]
</p><p>18 Filatova (2012) presented a detailed description of sarcasm corpus creation with sarcasm annotations of Amazon product reviews. [sent-58, score-1.132]
</p><p>19 (2010) used sarcastic tweets and sarcastic Amazon product reviews to train a sarcasm classifier with syntactic and pattern-based features. [sent-66, score-1.846]
</p><p>20 They examined whether tweets with a sarcasm hashtag are reliable enough indicators of sarcasm to be used as a gold standard for evaluation, but found that sarcasm hashtags are noisy and possibly biased to-  wards the hardest form of sarcasm (where even humans have difficulty). [sent-67, score-2.582]
</p><p>21 The novel contributions of our work include explicitly recognizing contexts that contrast a positive sentiment with a negative activity or state, as well as a bootstrapped learning framework to automatically acquire positive sentiment and negative situation phrases. [sent-91, score-1.422]
</p><p>22 Our goal is to create a sarcasm classifier for  tweets that explicitly recognizes contexts that contain a positive sentiment contrasted with a negative situation. [sent-98, score-1.374]
</p><p>23 Our approach learns rich phrasal lexicons of positive sentiments and negative situations using only the seed word “love” and a collection of sarcastic tweets as input. [sent-99, score-1.266]
</p><p>24 A key factor that makes the algorithm work is the presumption that if you find a positive sentiment or a negative situation in a sarcastic tweet, then you have found the source of the sarcasm. [sent-100, score-1.261]
</p><p>25 We further assume that the sarcasm probably arises from positive/negative contrast and we exploit syntactic structure to extract phrases that are likely to have contrasting polarity. [sent-101, score-0.788]
</p><p>26 1 Overview of the Learning Process Our bootstrapping algorithm operates on the assumption that many sarcastic tweets contain both a positive sentiment and a negative situation in close proximity, which is the source of the sarcasm. [sent-106, score-1.597]
</p><p>27 2 Al-  though sentiments and situations can be expressed 2Sarcasm can arise from a negative sentiment contrasted with a positive situation too, but our observation is that this is much less common, at least on Twitter. [sent-107, score-0.911]
</p><p>28 The learning process relies on an assumption that a positive sentiment verb phrase usually appears to the left of a negative situation phrase and in close proximity (usually, but not always, adjacent). [sent-111, score-1.004]
</p><p>29 Pictorially, we assume that many sarcastic tweets contain this structure: [+ VERB PHRASE] [– SITUATION PHRASE] This structural assumption drives our bootstrapping algorithm, which is illustrated in Figure 1. [sent-112, score-0.846]
</p><p>30 The bootstrapping process begins with a single seed word, “love”, which seems to be the most common positive sentiment term in sarcastic tweets. [sent-113, score-0.987]
</p><p>31 Given a sarcastic tweet containing the word “love”, our structural assumption infers that “love” is probably followed by an expression that refers to a negative situation. [sent-114, score-0.93]
</p><p>32 Given a sarcastic tweet that contains a negative situation phrase, we infer that the negative situation phrase is preceded by a positive sentiment. [sent-118, score-1.64]
</p><p>33 We harvest the n-grams that precede the negative situation phrases as positive sentiment candidates, score and select the best candidates, and add them to a list of positive sentiment phrases. [sent-119, score-1.252]
</p><p>34 The bootstrapping process then iterates, alternately learning more positive sentiment phrases and more negative situation phrases. [sent-120, score-0.953]
</p><p>35 Therefore we also include a step in the learning process to harvest predicative phrases that occur in close proximity to a negative situation phrase. [sent-129, score-0.742]
</p><p>36 We removed the tweets that contain a sarcasm hashtag, and considered the rest to be negative instances of sarcasm. [sent-135, score-0.987]
</p><p>37 Ofcourse, there will be some sarcastic tweets that do not have a sarcasm hashtag, so the negative instances will contain some noise. [sent-136, score-1.515]
</p><p>38 There will also be noise in the positive instances because a sarcasm hashtag does not guarantee that there is sarcasm in the body of the tweet (e. [sent-138, score-1.497]
</p><p>39 Our tweet collection therefore contains a total of 175,000 tweets: 20% are labeled as sarcastic and 80% are labeled as not sarcastic. [sent-142, score-0.73]
</p><p>40 We chose this seed because it seems to be the most common positive sentiment word in sarcastic tweets. [sent-147, score-0.91]
</p><p>41 To collect candidate phrases for negative situations, we extract n-grams that follow a positive sentiment phrase in a sarcastic tweet. [sent-151, score-1.276]
</p><p>42 For negative situation phrases, our goal is to learn possible verb phrase (VP) complements that are themselves verb phrases because they should represent activities and states. [sent-154, score-0.77]
</p><p>43 First, we collect phrases that potentially convey a positive sentiment by extracting n-grams that precede a negative situation phrase in a sarcastic tweet. [sent-186, score-1.462]
</p><p>44 To learn positive sentiment verb phrases, we extract every 1-gram and 2-gram that occurs immediately before (on the left-hand side of) a negative situation phrase. [sent-187, score-0.841]
</p><p>45 Finally, we score each candidate sentiment verb phrase by estimating the probability that a tweet is sarcastic given that it contains the candidate phrase preceding a negative situation phrase:  |prece|dperse(c+ecdaensd(i+dcaatnedViPd,a–tseiVtuPa,t–isoitnu)a &tio; sna)rc|astic| 3. [sent-190, score-1.564]
</p><p>46 6 Learning Positive Predicative Phrases We also use the negative situation phrases to harvest predicative expressions (predicate adjective or predicate nominal structures) that occur nearby. [sent-191, score-0.812]
</p><p>47 Based on the same assumption that sarcasm often arises from the contrast between a positive sentiment and a negative situation, we identify tweets that contain a negative situation and a predicative expression in close proximity. [sent-192, score-2.0]
</p><p>48 We extract positive sentiment candidates by extracting 1-grams, 2-grams, and 3-grams that appear immediately after a copular verb and occur within 5 words of the negative situation phrase, on either side. [sent-195, score-0.861]
</p><p>49 As a result, we sort the candidates by their probability and conservatively add only the top 5 positive verb phrases and top 5 positive predicative expressions in each bootstrapping iteration. [sent-212, score-0.778]
</p><p>50 7  The Learned Phrase Lists  The bootstrapping process alternately learns positive sentiments and negative situations until no more phrases can be learned. [sent-216, score-0.686]
</p><p>51 In our experiments, we learned 26 positive sentiment verb phrases, 20 predicative expressions and 239 negative situation phrases. [sent-217, score-1.026]
</p><p>52 Table 1 shows the first 15 positive verb phrases, the first 15 positive predicative expressions, and the first 40 negative situation phrases learned by the bootstrapping algorithm. [sent-218, score-1.097]
</p><p>53 Some of the negative situation phrases are not complete expressions, but it  is clear that they will often match negative activities and states. [sent-219, score-0.715]
</p><p>54 For example, “getting yelled” was generated from sarcastic comments such as “I love getting yelled at”, “being home” occurred in tweets about “being home alone”, and “being told” is often being told what to do. [sent-220, score-0.85]
</p><p>55 Even for people, it is not always easy to identify sarcasm in tweets because sarcasm often depends on conversational context that spans more than a single tweet. [sent-225, score-1.391]
</p><p>56 We focus on identifying sarcasm that is selfcontained in one tweet and does not depend on prior conversational context. [sent-227, score-0.763]
</p><p>57 We defined annotation guidelines that instructed human annotators to read isolated tweets and label a tweet as sarcastic if it contains comments judged to be sarcastic based solely on the content of that tweet. [sent-228, score-1.482]
</p><p>58 ” should be labeled as not sarcastic because the sarcastic content was (presumably) in a previous tweet. [sent-231, score-1.076]
</p><p>59 The guidelines did not contain any instructions that required positive/negative contrast to be present in the tweet, so all forms of sarcasm were considered to be positive examples. [sent-232, score-0.751]
</p><p>60 To ensure that our evaluation data had a healthy mix of both sarcastic and non-sarcastic tweets, we collected 1,600 tweets with a sarcasm hashtag (#sarcasm or #sarcastic), and 1,600 tweets without these sarcasm hashtags from Twitter’s random streaming API. [sent-233, score-2.202]
</p><p>61 When presenting the tweets to the annotators, the sarcasm hashtags were removed so the annotators had to judge whether a tweet was sarcastic or not without seeing those hashtags. [sent-234, score-1.526]
</p><p>62 To ensure that we had high-quality annotations, three annotators were asked to annotate the same set of 200 tweets (100 sarcastic + 100 not sarcastic). [sent-235, score-0.77]
</p><p>63 Only 713 of the 1,600 tweets with sarcasm hashtags (45%) were judged to be sarcastic based on our annotation guidelines. [sent-244, score-1.368]
</p><p>64 There are several reasons why a tweet with a sarcasm hashtag might not have been judged to be sarcastic. [sent-245, score-0.816]
</p><p>65 , multiple tweets), or the sarcastic content may be in a URL and not the tweet itself, or the tweet’s content may not obviously be sarcastic without seeing the sarcasm hashtag (e. [sent-248, score-1.85]
</p><p>66 Of the 1,600 tweets in our data set that were obtained from the random stream and did not have a sarcasm hashtag, 29 (1. [sent-251, score-0.79]
</p><p>67 2 Baselines Overall, 693 of the 3,000 tweets in our Test Set were annotated as sarcastic, so a system that classifies every tweet as sarcastic will have 23% precision. [sent-254, score-0.914]
</p><p>68 To assess the difficulty of recognizing the sarcastic  tweets in our data set, we evaluated a variety of baseline systems. [sent-255, score-0.776]
</p><p>69 We considered all words with negative values to have negative polarity (1598 words), and all words with positive values to have positive polarity (879 words). [sent-272, score-0.69]
</p><p>70 We performed four sets of experiments with each resource to see how beneficial existing sentiment  lexicons could be for sarcasm recognition in tweets. [sent-273, score-0.811]
</p><p>71 Since our hypothesis is that sarcasm often arises from the contrast between something positive and something negative, we systematically evaluated the positive and negative phrases individually, jointly, and jointly in a specific order (a positive phrase followed by a negative phrase). [sent-274, score-1.601]
</p><p>72 First, we labeled a tweet as sarcastic if it contains any positive term in each resource. [sent-275, score-0.847]
</p><p>73 Second, we labeled a tweet as sarcastic if it contains any negative term from each resource. [sent-277, score-0.89]
</p><p>74 Third, we labeled a tweet as sarcastic if it contains both a positive sentiment term and a negative sentiment term, in any order. [sent-280, score-1.469]
</p><p>75 The Positive and Negative Sentiment, Unordered section of Table 2 shows that this approach yields low recall, indicating that relatively few sarcastic tweets contain both positive and negative sentiments, and low precision as well. [sent-281, score-1.108]
</p><p>76 This criteria reflects our observation that positive sentiments often closely precede negative situations in sarcastic tweets, so we wanted  to see if the same ordering tendency holds for negative sentiments. [sent-283, score-1.198]
</p><p>77 3 Evaluation of Bootstrapped Phrase Lists The next set of experiments evaluates the effectiveness of the positive sentiment and negative situation phrases learned by our bootstrapping algorithm. [sent-290, score-0.924]
</p><p>78 For the sake of comparison with other sentiment resources, we first evaluated our positive sentiment verb phrases and negative situation phrases independently. [sent-292, score-1.268]
</p><p>79 Our positive verb phrases achieved much lower recall than the positive sentiment phrases in the other resources, but they had higher precision (45%). [sent-293, score-0.851]
</p><p>80 Despite its relatively small size, our list of negative situation phrases achieved 29% recall, which is comparable to the negative sentiments, but higher precision (38%). [sent-295, score-0.691]
</p><p>81 Next, we classified a tweet as sarcastic if it contains both a positive verb phrase and a negative situation phrase from our bootstrapped lists, in any order. [sent-296, score-1.478]
</p><p>82 Finally, we enforced an ordering constraint so a tweet is labeled as sarcastic only if it contains a positive verb phrase that precedes a negative situation in close proximity (no more than 5 words apart). [sent-298, score-1.43]
</p><p>83 Note that the same ordering constraint applied to a positive verb phrase followed by a negative sentiment produced much lower precision (at  best 40% precision using the Liu05 lexicon). [sent-301, score-0.759]
</p><p>84 Contrasting a positive sentiment with a negative situation seems to be a key element of sarcasm. [sent-302, score-0.733]
</p><p>85 712 In the last experiment, we added the positive predicative expressions and also labeled a tweet as sarcastic if a positive predicative appeared in close proximity to (within 5 words of) a negative situation. [sent-303, score-1.601]
</p><p>86 4 A Hybrid Approach Thus far, we have used the bootstrapped lexicons to recognize sarcasm by looking for phrases in our lists. [sent-306, score-0.805]
</p><p>87 We will refer to our approach as the Contrast method, which labels a tweet as sarcastic if it contains a positive sentiment phrase in close proximity to a negative situation phrase. [sent-307, score-1.545]
</p><p>88 Since neither approach has high  recall, we decided to see whether they are complementary and the Contrast method is finding sarcastic tweets that the SVM classifier overlooks. [sent-310, score-0.752]
</p><p>89 In this hybrid approach, a tweet is labeled as sarcastic if either the SVM classifier or the Contrast method identifies it as sarcastic. [sent-311, score-0.71]
</p><p>90 This result shows that our bootstrapped phrase lists are recognizing sarcastic tweets that the SVM classifier misses. [sent-315, score-0.922]
</p><p>91 5 Analysis To get a better sense of the strength and limitations of our approach, we manually inspected some of the tweets that were labeled as sarcastic using our bootstrapped phrase lists. [sent-324, score-0.899]
</p><p>92 Table 3 shows some of the sarcastic tweets found by the Contrast method but not by the SVM classifier. [sent-325, score-0.752]
</p><p>93 arCeopnlytas method but not the SVM These tweets are good examples of a positive sentiment (love, enjoy, awesome, can’t wait) contrasting with a negative situation. [sent-328, score-0.811]
</p><p>94 For example, “working” was learned as a negative situation phrase because it is often negative when it follows a positive sentiment (“I love  working. [sent-330, score-1.055]
</p><p>95 We also examined tweets that were incorrectly labeled as sarcastic by the Contrast method. [sent-335, score-0.772]
</p><p>96 Our work identifies just one type of sarcasm that is common in tweets: contrast between a positive sentiment and negative situation. [sent-344, score-1.135]
</p><p>97 We presented a bootstrapped learning method to acquire lists of positive sentiment phrases and negative activities and states, and show that these lists can be used to recognize sarcastic tweets. [sent-345, score-1.365]
</p><p>98 For example, sarcasm often arises from a description of a negative event followed by a positive emotion but in a separate clause or sentence, such as: “Going to the dentist for a root canal this afternoon. [sent-350, score-0.968]
</p><p>99 The perfect solution for detecting sarcasm in tweets #not. [sent-420, score-0.79]
</p><p>100 apparently bootstrapping improves the performance of sarcasm and nastiness classifiers for online dialogue. [sent-430, score-0.643]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sarcasm', 0.566), ('sarcastic', 0.528), ('tweets', 0.224), ('sentiment', 0.221), ('situation', 0.195), ('negative', 0.18), ('predicative', 0.171), ('tweet', 0.162), ('positive', 0.137), ('phrases', 0.114), ('verb', 0.086), ('sentiments', 0.084), ('love', 0.079), ('bootstrapping', 0.077), ('waiting', 0.074), ('stereotypically', 0.067), ('hashtag', 0.066), ('situations', 0.065), ('bootstrapped', 0.064), ('phrase', 0.063), ('contrasting', 0.049), ('forever', 0.048), ('adjective', 0.047), ('activities', 0.046), ('predicate', 0.046), ('proximity', 0.041), ('cheang', 0.038), ('yay', 0.038), ('twitter', 0.038), ('recognize', 0.037), ('expressions', 0.036), ('conversational', 0.035), ('candidate', 0.033), ('activity', 0.032), ('expression', 0.032), ('contrast', 0.031), ('cues', 0.031), ('svm', 0.03), ('alternately', 0.029), ('contrasted', 0.029), ('dentist', 0.029), ('kreuz', 0.029), ('followed', 0.028), ('arises', 0.028), ('hashtags', 0.028), ('adj', 0.028), ('polarity', 0.028), ('adv', 0.027), ('enjoy', 0.025), ('gina', 0.025), ('irony', 0.025), ('lexicons', 0.024), ('precede', 0.024), ('recognizing', 0.024), ('seed', 0.024), ('harvest', 0.023), ('subjectivity', 0.023), ('adjectives', 0.023), ('precision', 0.022), ('judged', 0.022), ('wilson', 0.022), ('immediately', 0.022), ('near', 0.021), ('ignored', 0.021), ('labeled', 0.02), ('candidates', 0.02), ('pos', 0.02), ('nominals', 0.02), ('infinitive', 0.02), ('recall', 0.02), ('lists', 0.019), ('lexicon', 0.019), ('amplitude', 0.019), ('arup', 0.019), ('candidatepred', 0.019), ('caucci', 0.019), ('finn', 0.019), ('liebrecht', 0.019), ('negativity', 0.019), ('nielsen', 0.019), ('ofcontrast', 0.019), ('paralinguistic', 0.019), ('pell', 0.019), ('rankin', 0.019), ('shoveling', 0.019), ('subsumed', 0.019), ('tepperman', 0.019), ('thrilled', 0.019), ('unenjoyable', 0.019), ('yelled', 0.019), ('tsur', 0.019), ('davidov', 0.019), ('undesirable', 0.019), ('close', 0.018), ('annotators', 0.018), ('adverb', 0.017), ('contain', 0.017), ('carvalho', 0.017), ('vacation', 0.017), ('iphone', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="163-tfidf-1" href="./emnlp-2013-Sarcasm_as_Contrast_between_a_Positive_Sentiment_and_Negative_Situation.html">163 emnlp-2013-Sarcasm as Contrast between a Positive Sentiment and Negative Situation</a></p>
<p>Author: Ellen Riloff ; Ashequl Qadir ; Prafulla Surve ; Lalindra De Silva ; Nathan Gilbert ; Ruihong Huang</p><p>Abstract: A common form of sarcasm on Twitter consists of a positive sentiment contrasted with a negative situation. For example, many sarcastic tweets include a positive sentiment, such as “love” or “enjoy”, followed by an expression that describes an undesirable activity or state (e.g., “taking exams” or “being ignored”). We have developed a sarcasm recognizer to identify this type of sarcasm in tweets. We present a novel bootstrapping algorithm that automatically learns lists of positive sentiment phrases and negative situation phrases from sarcastic tweets. We show that identifying contrasting contexts using the phrases learned through bootstrapping yields improved recall for sarcasm recognition.</p><p>2 0.21297292 <a title="163-tfidf-2" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>Author: Margaret Mitchell ; Jacqui Aguilar ; Theresa Wilson ; Benjamin Van Durme</p><p>Abstract: We propose a novel approach to sentiment analysis for a low resource setting. The intuition behind this work is that sentiment expressed towards an entity, targeted sentiment, may be viewed as a span of sentiment expressed across the entity. This representation allows us to model sentiment detection as a sequence tagging problem, jointly discovering people and organizations along with whether there is sentiment directed towards them. We compare performance in both Spanish and English on microblog data, using only a sentiment lexicon as an external resource. By leveraging linguisticallyinformed features within conditional random fields (CRFs) trained to minimize empirical risk, our best models in Spanish significantly outperform a strong baseline, and reach around 90% accuracy on the combined task of named entity recognition and sentiment prediction. Our models in English, trained on a much smaller dataset, are not yet statistically significant against their baselines.</p><p>3 0.1912328 <a title="163-tfidf-3" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>Author: Shi Feng ; Le Zhang ; Binyang Li ; Daling Wang ; Ge Yu ; Kam-Fai Wong</p><p>Abstract: Extensive experiments have validated the effectiveness of the corpus-based method for classifying the word’s sentiment polarity. However, no work is done for comparing different corpora in the polarity classification task. Nowadays, Twitter has aggregated huge amount of data that are full of people’s sentiments. In this paper, we empirically evaluate the performance of different corpora in sentiment similarity measurement, which is the fundamental task for word polarity classification. Experiment results show that the Twitter data can achieve a much better performance than the Google, Web1T and Wikipedia based methods.</p><p>4 0.16099118 <a title="163-tfidf-4" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>Author: Svitlana Volkova ; Theresa Wilson ; David Yarowsky</p><p>Abstract: Theresa Wilson Human Language Technology Center of Excellence Johns Hopkins University Baltimore, MD t aw@ j hu .edu differences may Different demographics, e.g., gender or age, can demonstrate substantial variation in their language use, particularly in informal contexts such as social media. In this paper we focus on learning gender differences in the use of subjective language in English, Spanish, and Russian Twitter data, and explore cross-cultural differences in emoticon and hashtag use for male and female users. We show that gender differences in subjective language can effectively be used to improve sentiment analysis, and in particular, polarity classification for Spanish and Russian. Our results show statistically significant relative F-measure improvement over the gender-independent baseline 1.5% and 1% for Russian, 2% and 0.5% for Spanish, and 2.5% and 5% for English for polarity and subjectivity classification.</p><p>5 0.14541683 <a title="163-tfidf-5" href="./emnlp-2013-Authorship_Attribution_of_Micro-Messages.html">27 emnlp-2013-Authorship Attribution of Micro-Messages</a></p>
<p>Author: Roy Schwartz ; Oren Tsur ; Ari Rappoport ; Moshe Koppel</p><p>Abstract: Work on authorship attribution has traditionally focused on long texts. In this work, we tackle the question of whether the author of a very short text can be successfully identified. We use Twitter as an experimental testbed. We introduce the concept of an author’s unique “signature”, and show that such signatures are typical of many authors when writing very short texts. We also present a new authorship attribution feature (“flexible patterns”) and demonstrate a significant improvement over our baselines. Our results show that the author of a single tweet can be identified with good accuracy in an array of flavors of the authorship attribution task.</p><p>6 0.13227081 <a title="163-tfidf-6" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>7 0.11982355 <a title="163-tfidf-7" href="./emnlp-2013-A_Unified_Model_for_Topics%2C_Events_and_Users_on_Twitter.html">16 emnlp-2013-A Unified Model for Topics, Events and Users on Twitter</a></p>
<p>8 0.093045726 <a title="163-tfidf-8" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>9 0.085621715 <a title="163-tfidf-9" href="./emnlp-2013-Gender_Inference_of_Twitter_Users_in_Non-English_Contexts.html">89 emnlp-2013-Gender Inference of Twitter Users in Non-English Contexts</a></p>
<p>10 0.073941119 <a title="163-tfidf-10" href="./emnlp-2013-Opinion_Mining_in_Newspaper_Articles_by_Entropy-Based_Word_Connections.html">144 emnlp-2013-Opinion Mining in Newspaper Articles by Entropy-Based Word Connections</a></p>
<p>11 0.071794972 <a title="163-tfidf-11" href="./emnlp-2013-Sentiment_Analysis%3A_How_to_Derive_Prior_Polarities_from_SentiWordNet.html">170 emnlp-2013-Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet</a></p>
<p>12 0.06139515 <a title="163-tfidf-12" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>13 0.059414912 <a title="163-tfidf-13" href="./emnlp-2013-A_temporal_model_of_text_periodicities_using_Gaussian_Processes.html">18 emnlp-2013-A temporal model of text periodicities using Gaussian Processes</a></p>
<p>14 0.056068931 <a title="163-tfidf-14" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>15 0.05396628 <a title="163-tfidf-15" href="./emnlp-2013-Discourse_Level_Explanatory_Relation_Extraction_from_Product_Reviews_Using_First-Order_Logic.html">63 emnlp-2013-Discourse Level Explanatory Relation Extraction from Product Reviews Using First-Order Logic</a></p>
<p>16 0.053630989 <a title="163-tfidf-16" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>17 0.053428739 <a title="163-tfidf-17" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>18 0.05219971 <a title="163-tfidf-18" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>19 0.048689976 <a title="163-tfidf-19" href="./emnlp-2013-Identifying_Phrasal_Verbs_Using_Many_Bilingual_Corpora.html">96 emnlp-2013-Identifying Phrasal Verbs Using Many Bilingual Corpora</a></p>
<p>20 0.046735045 <a title="163-tfidf-20" href="./emnlp-2013-Learning_Topics_and_Positions_from_Debatepedia.html">121 emnlp-2013-Learning Topics and Positions from Debatepedia</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.151), (1, 0.08), (2, -0.194), (3, -0.193), (4, 0.137), (5, -0.1), (6, -0.07), (7, -0.119), (8, 0.073), (9, 0.113), (10, -0.035), (11, 0.087), (12, 0.068), (13, 0.034), (14, -0.005), (15, -0.067), (16, -0.01), (17, -0.07), (18, 0.047), (19, -0.022), (20, -0.045), (21, 0.0), (22, 0.029), (23, 0.032), (24, -0.029), (25, -0.026), (26, 0.045), (27, -0.045), (28, -0.007), (29, 0.017), (30, 0.046), (31, -0.006), (32, -0.014), (33, -0.016), (34, 0.03), (35, 0.022), (36, 0.008), (37, -0.017), (38, -0.041), (39, 0.099), (40, -0.115), (41, 0.055), (42, -0.036), (43, 0.068), (44, -0.097), (45, -0.037), (46, -0.042), (47, 0.023), (48, 0.007), (49, -0.099)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96650064 <a title="163-lsi-1" href="./emnlp-2013-Sarcasm_as_Contrast_between_a_Positive_Sentiment_and_Negative_Situation.html">163 emnlp-2013-Sarcasm as Contrast between a Positive Sentiment and Negative Situation</a></p>
<p>Author: Ellen Riloff ; Ashequl Qadir ; Prafulla Surve ; Lalindra De Silva ; Nathan Gilbert ; Ruihong Huang</p><p>Abstract: A common form of sarcasm on Twitter consists of a positive sentiment contrasted with a negative situation. For example, many sarcastic tweets include a positive sentiment, such as “love” or “enjoy”, followed by an expression that describes an undesirable activity or state (e.g., “taking exams” or “being ignored”). We have developed a sarcasm recognizer to identify this type of sarcasm in tweets. We present a novel bootstrapping algorithm that automatically learns lists of positive sentiment phrases and negative situation phrases from sarcastic tweets. We show that identifying contrasting contexts using the phrases learned through bootstrapping yields improved recall for sarcasm recognition.</p><p>2 0.76602215 <a title="163-lsi-2" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>Author: Shi Feng ; Le Zhang ; Binyang Li ; Daling Wang ; Ge Yu ; Kam-Fai Wong</p><p>Abstract: Extensive experiments have validated the effectiveness of the corpus-based method for classifying the word’s sentiment polarity. However, no work is done for comparing different corpora in the polarity classification task. Nowadays, Twitter has aggregated huge amount of data that are full of people’s sentiments. In this paper, we empirically evaluate the performance of different corpora in sentiment similarity measurement, which is the fundamental task for word polarity classification. Experiment results show that the Twitter data can achieve a much better performance than the Google, Web1T and Wikipedia based methods.</p><p>3 0.73633003 <a title="163-lsi-3" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>Author: Margaret Mitchell ; Jacqui Aguilar ; Theresa Wilson ; Benjamin Van Durme</p><p>Abstract: We propose a novel approach to sentiment analysis for a low resource setting. The intuition behind this work is that sentiment expressed towards an entity, targeted sentiment, may be viewed as a span of sentiment expressed across the entity. This representation allows us to model sentiment detection as a sequence tagging problem, jointly discovering people and organizations along with whether there is sentiment directed towards them. We compare performance in both Spanish and English on microblog data, using only a sentiment lexicon as an external resource. By leveraging linguisticallyinformed features within conditional random fields (CRFs) trained to minimize empirical risk, our best models in Spanish significantly outperform a strong baseline, and reach around 90% accuracy on the combined task of named entity recognition and sentiment prediction. Our models in English, trained on a much smaller dataset, are not yet statistically significant against their baselines.</p><p>4 0.68461841 <a title="163-lsi-4" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>Author: Svitlana Volkova ; Theresa Wilson ; David Yarowsky</p><p>Abstract: Theresa Wilson Human Language Technology Center of Excellence Johns Hopkins University Baltimore, MD t aw@ j hu .edu differences may Different demographics, e.g., gender or age, can demonstrate substantial variation in their language use, particularly in informal contexts such as social media. In this paper we focus on learning gender differences in the use of subjective language in English, Spanish, and Russian Twitter data, and explore cross-cultural differences in emoticon and hashtag use for male and female users. We show that gender differences in subjective language can effectively be used to improve sentiment analysis, and in particular, polarity classification for Spanish and Russian. Our results show statistically significant relative F-measure improvement over the gender-independent baseline 1.5% and 1% for Russian, 2% and 0.5% for Spanish, and 2.5% and 5% for English for polarity and subjectivity classification.</p><p>5 0.5956918 <a title="163-lsi-5" href="./emnlp-2013-Sentiment_Analysis%3A_How_to_Derive_Prior_Polarities_from_SentiWordNet.html">170 emnlp-2013-Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet</a></p>
<p>Author: Marco Guerini ; Lorenzo Gatti ; Marco Turchi</p><p>Abstract: Assigning a positive or negative score to a word out of context (i.e. a word’s prior polarity) is a challenging task for sentiment analysis. In the literature, various approaches based on SentiWordNet have been proposed. In this paper, we compare the most often used techniques together with newly proposed ones and incorporate all of them in a learning framework to see whether blending them can further improve the estimation of prior polarity scores. Using two different versions of SentiWordNet and testing regression and classification models across tasks and datasets, our learning approach consistently outperforms the single metrics, providing a new state-ofthe-art approach in computing words’ prior polarity for sentiment analysis. We conclude our investigation showing interesting biases in calculated prior polarity scores when word Part of Speech and annotator gender are considered.</p><p>6 0.52666801 <a title="163-lsi-6" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>7 0.50679344 <a title="163-lsi-7" href="./emnlp-2013-Authorship_Attribution_of_Micro-Messages.html">27 emnlp-2013-Authorship Attribution of Micro-Messages</a></p>
<p>8 0.48734498 <a title="163-lsi-8" href="./emnlp-2013-Opinion_Mining_in_Newspaper_Articles_by_Entropy-Based_Word_Connections.html">144 emnlp-2013-Opinion Mining in Newspaper Articles by Entropy-Based Word Connections</a></p>
<p>9 0.41756371 <a title="163-lsi-9" href="./emnlp-2013-Gender_Inference_of_Twitter_Users_in_Non-English_Contexts.html">89 emnlp-2013-Gender Inference of Twitter Users in Non-English Contexts</a></p>
<p>10 0.41252956 <a title="163-lsi-10" href="./emnlp-2013-Using_Crowdsourcing_to_get_Representations_based_on_Regular_Expressions.html">196 emnlp-2013-Using Crowdsourcing to get Representations based on Regular Expressions</a></p>
<p>11 0.39214575 <a title="163-lsi-11" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>12 0.37385619 <a title="163-lsi-12" href="./emnlp-2013-A_temporal_model_of_text_periodicities_using_Gaussian_Processes.html">18 emnlp-2013-A temporal model of text periodicities using Gaussian Processes</a></p>
<p>13 0.36881515 <a title="163-lsi-13" href="./emnlp-2013-A_Unified_Model_for_Topics%2C_Events_and_Users_on_Twitter.html">16 emnlp-2013-A Unified Model for Topics, Events and Users on Twitter</a></p>
<p>14 0.36239544 <a title="163-lsi-14" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>15 0.34513858 <a title="163-lsi-15" href="./emnlp-2013-Success_with_Style%3A_Using_Writing_Style_to_Predict_the_Success_of_Novels.html">178 emnlp-2013-Success with Style: Using Writing Style to Predict the Success of Novels</a></p>
<p>16 0.30366009 <a title="163-lsi-16" href="./emnlp-2013-A_Log-Linear_Model_for_Unsupervised_Text_Normalization.html">9 emnlp-2013-A Log-Linear Model for Unsupervised Text Normalization</a></p>
<p>17 0.27589253 <a title="163-lsi-17" href="./emnlp-2013-Two-Stage_Method_for_Large-Scale_Acquisition_of_Contradiction_Pattern_Pairs_using_Entailment.html">189 emnlp-2013-Two-Stage Method for Large-Scale Acquisition of Contradiction Pattern Pairs using Entailment</a></p>
<p>18 0.27043894 <a title="163-lsi-18" href="./emnlp-2013-Detecting_Promotional_Content_in_Wikipedia.html">61 emnlp-2013-Detecting Promotional Content in Wikipedia</a></p>
<p>19 0.26575387 <a title="163-lsi-19" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>20 0.25719464 <a title="163-lsi-20" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.034), (18, 0.022), (22, 0.024), (30, 0.511), (51, 0.147), (66, 0.032), (71, 0.031), (75, 0.019), (77, 0.015), (90, 0.019), (96, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99205548 <a title="163-lda-1" href="./emnlp-2013-An_Efficient_Language_Model_Using_Double-Array_Structures.html">20 emnlp-2013-An Efficient Language Model Using Double-Array Structures</a></p>
<p>Author: Makoto Yasuhara ; Toru Tanaka ; Jun-ya Norimatsu ; Mikio Yamamoto</p><p>Abstract: Ngram language models tend to increase in size with inflating the corpus size, and consume considerable resources. In this paper, we propose an efficient method for implementing ngram models based on doublearray structures. First, we propose a method for representing backwards suffix trees using double-array structures and demonstrate its efficiency. Next, we propose two optimization methods for improving the efficiency of data representation in the double-array structures. Embedding probabilities into unused spaces in double-array structures reduces the model size. Moreover, tuning the word IDs in the language model makes the model smaller and faster. We also show that our method can be used for building large language models using the division method. Lastly, we show that our method outperforms methods based on recent related works from the viewpoints of model size and query speed when both optimization methods are used.</p><p>2 0.98500037 <a title="163-lda-2" href="./emnlp-2013-Growing_Multi-Domain_Glossaries_from_a_Few_Seeds_using_Probabilistic_Topic_Models.html">92 emnlp-2013-Growing Multi-Domain Glossaries from a Few Seeds using Probabilistic Topic Models</a></p>
<p>Author: Stefano Faralli ; Roberto Navigli</p><p>Abstract: In this paper we present a minimallysupervised approach to the multi-domain acquisition ofwide-coverage glossaries. We start from a small number of hypernymy relation seeds and bootstrap glossaries from the Web for dozens of domains using Probabilistic Topic Models. Our experiments show that we are able to extract high-precision glossaries comprising thousands of terms and definitions.</p><p>3 0.97669548 <a title="163-lda-3" href="./emnlp-2013-Structured_Penalties_for_Log-Linear_Language_Models.html">176 emnlp-2013-Structured Penalties for Log-Linear Language Models</a></p>
<p>Author: Anil Kumar Nelakanti ; Cedric Archambeau ; Julien Mairal ; Francis Bach ; Guillaume Bouchard</p><p>Abstract: Language models can be formalized as loglinear regression models where the input features represent previously observed contexts up to a certain length m. The complexity of existing algorithms to learn the parameters by maximum likelihood scale linearly in nd, where n is the length of the training corpus and d is the number of observed features. We present a model that grows logarithmically in d, making it possible to efficiently leverage longer contexts. We account for the sequential structure of natural language using treestructured penalized objectives to avoid overfitting and achieve better generalization.</p><p>4 0.97640121 <a title="163-lda-4" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>Author: Xinyan Xiao ; Deyi Xiong</p><p>Abstract: Traditional synchronous grammar induction estimates parameters by maximizing likelihood, which only has a loose relation to translation quality. Alternatively, we propose a max-margin estimation approach to discriminatively inducing synchronous grammars for machine translation, which directly optimizes translation quality measured by BLEU. In the max-margin estimation of parameters, we only need to calculate Viterbi translations. This further facilitates the incorporation of various non-local features that are defined on the target side. We test the effectiveness of our max-margin estimation framework on a competitive hierarchical phrase-based system. Experiments show that our max-margin method significantly outperforms the traditional twostep pipeline for synchronous rule extraction by 1.3 BLEU points and is also better than previous max-likelihood estimation method.</p><p>5 0.93135136 <a title="163-lda-5" href="./emnlp-2013-A_Dataset_for_Research_on_Short-Text_Conversations.html">4 emnlp-2013-A Dataset for Research on Short-Text Conversations</a></p>
<p>Author: Hao Wang ; Zhengdong Lu ; Hang Li ; Enhong Chen</p><p>Abstract: Natural language conversation is widely regarded as a highly difficult problem, which is usually attacked with either rule-based or learning-based models. In this paper we propose a retrieval-based automatic response model for short-text conversation, to exploit the vast amount of short conversation instances available on social media. For this purpose we introduce a dataset of short-text conversation based on the real-world instances from Sina Weibo (a popular Chinese microblog service), which will be soon released to public. This dataset provides rich collection of instances for the research on finding natural and relevant short responses to a given short text, and useful for both training and testing of conversation models. This dataset consists of both naturally formed conversations, manually labeled data, and a large repository of candidate responses. Our preliminary experiments demonstrate that the simple retrieval-based conversation model performs reasonably well when combined with the rich instances in our dataset.</p><p>same-paper 6 0.92728388 <a title="163-lda-6" href="./emnlp-2013-Sarcasm_as_Contrast_between_a_Positive_Sentiment_and_Negative_Situation.html">163 emnlp-2013-Sarcasm as Contrast between a Positive Sentiment and Negative Situation</a></p>
<p>7 0.75956511 <a title="163-lda-7" href="./emnlp-2013-Joint_Language_and_Translation_Modeling_with_Recurrent_Neural_Networks.html">113 emnlp-2013-Joint Language and Translation Modeling with Recurrent Neural Networks</a></p>
<p>8 0.73040938 <a title="163-lda-8" href="./emnlp-2013-A_Corpus_Level_MIRA_Tuning_Strategy_for_Machine_Translation.html">3 emnlp-2013-A Corpus Level MIRA Tuning Strategy for Machine Translation</a></p>
<p>9 0.72020835 <a title="163-lda-9" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>10 0.71831787 <a title="163-lda-10" href="./emnlp-2013-Optimal_Incremental_Parsing_via_Best-First_Dynamic_Programming.html">146 emnlp-2013-Optimal Incremental Parsing via Best-First Dynamic Programming</a></p>
<p>11 0.7143833 <a title="163-lda-11" href="./emnlp-2013-A_Convex_Alternative_to_IBM_Model_2.html">2 emnlp-2013-A Convex Alternative to IBM Model 2</a></p>
<p>12 0.71170759 <a title="163-lda-12" href="./emnlp-2013-Dynamic_Feature_Selection_for_Dependency_Parsing.html">66 emnlp-2013-Dynamic Feature Selection for Dependency Parsing</a></p>
<p>13 0.71096647 <a title="163-lda-13" href="./emnlp-2013-Recurrent_Continuous_Translation_Models.html">156 emnlp-2013-Recurrent Continuous Translation Models</a></p>
<p>14 0.70301282 <a title="163-lda-14" href="./emnlp-2013-Improving_Web_Search_Ranking_by_Incorporating_Structured_Annotation_of_Queries.html">105 emnlp-2013-Improving Web Search Ranking by Incorporating Structured Annotation of Queries</a></p>
<p>15 0.70225781 <a title="163-lda-15" href="./emnlp-2013-A_Synchronous_Context_Free_Grammar_for_Time_Normalization.html">14 emnlp-2013-A Synchronous Context Free Grammar for Time Normalization</a></p>
<p>16 0.69224405 <a title="163-lda-16" href="./emnlp-2013-Max-Violation_Perceptron_and_Forced_Decoding_for_Scalable_MT_Training.html">128 emnlp-2013-Max-Violation Perceptron and Forced Decoding for Scalable MT Training</a></p>
<p>17 0.68500286 <a title="163-lda-17" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>18 0.68425125 <a title="163-lda-18" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>19 0.68399286 <a title="163-lda-19" href="./emnlp-2013-Converting_Continuous-Space_Language_Models_into_N-Gram_Language_Models_for_Statistical_Machine_Translation.html">52 emnlp-2013-Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation</a></p>
<p>20 0.68358505 <a title="163-lda-20" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
