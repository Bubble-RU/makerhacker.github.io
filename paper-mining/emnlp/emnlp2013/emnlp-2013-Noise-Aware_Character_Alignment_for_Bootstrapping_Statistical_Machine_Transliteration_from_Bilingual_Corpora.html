<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>139 emnlp-2013-Noise-Aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-139" href="#">emnlp2013-139</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>139 emnlp-2013-Noise-Aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora</h1>
<br/><p>Source: <a title="emnlp-2013-139-pdf" href="http://aclweb.org/anthology//D/D13/D13-1021.pdf">pdf</a></p><p>Author: Katsuhito Sudoh ; Shinsuke Mori ; Masaaki Nagata</p><p>Abstract: This paper proposes a novel noise-aware character alignment method for bootstrapping statistical machine transliteration from automatically extracted phrase pairs. The model is an extension of a Bayesian many-to-many alignment method for distinguishing nontransliteration (noise) parts in phrase pairs. It worked effectively in the experiments of bootstrapping Japanese-to-English statistical machine transliteration in patent domain using patent bilingual corpora.</p><p>Reference: <a title="emnlp-2013-139-reference" href="../emnlp2013_reference/emnlp-2013-Noise-Aware_Character_Alignment_for_Bootstrapping_Statistical_Machine_Transliteration_from_Bilingual_Corpora_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp  Abstract This paper proposes a novel noise-aware character alignment method for bootstrapping statistical machine transliteration from automatically extracted phrase pairs. [sent-5, score-1.162]
</p><p>2 The model is an extension of a Bayesian many-to-many alignment method for distinguishing nontransliteration (noise) parts in phrase pairs. [sent-6, score-0.368]
</p><p>3 It worked effectively in the experiments of bootstrapping Japanese-to-English statistical machine transliteration in patent domain using patent bilingual corpora. [sent-7, score-1.154]
</p><p>4 1 Introduction Transliteration is used for providing translations for source language words that have no appropriate counterparts in target language, such as some technical terms and named entities. [sent-8, score-0.03]
</p><p>5 Statistical machine transliteration (Knight and Graehl, 1998) is a tech-  nology to solve it in a statistical manner. [sent-9, score-0.748]
</p><p>6 Bilingual dictionaries can be used to train its model, but many of their entries are actually translation but not transliteration. [sent-10, score-0.071]
</p><p>7 Such non-transliteration pairs hurt the transliteration model and should be eliminated beforehand. [sent-11, score-0.851]
</p><p>8 (2012) proposed a method to identify such non-transliteration pairs, and applied it successfully to noisy word pairs obtained from automatic word alignment on bilingual corpora. [sent-13, score-0.421]
</p><p>9 It enables the statistical machine transliteration to be bootstrapped from bilingual corpora. [sent-14, score-0.837]
</p><p>10 This approach is beneficial because it does not require carefullydeveloped bilingual transliteration dictionaries and it can learn domain-specific transliteration patterns 204 from bilingual corpora in the target domain. [sent-15, score-1.612]
</p><p>11 However, their transliteration mining approach is samplewise; that is, it makes a decision whether a bilingual phrase pair is transliteration or not. [sent-16, score-1.59]
</p><p>12 Suppose that a compound word in a language A is transliterated into two words in another language B. [sent-17, score-0.073]
</p><p>13 Their correspondence may not be fully identified by automatic  word alignment and a wrong alignment between the compound word in A and only one component word in B is found. [sent-18, score-0.494]
</p><p>14 The sample-wise mining cannot make a correct decision of partial transliteration on the aligned candidate, and may introduces noise to the statistical transliteration model. [sent-19, score-2.015]
</p><p>15 This paper proposes a novel transliteration mining method for such partial transliterations. [sent-20, score-0.927]
</p><p>16 The method uses a noise-aware character alignment model that distinguish non-transliteration (noise) parts from transliteration (signal) parts. [sent-21, score-1.078]
</p><p>17 The model is an extension of a Bayesian alignment model (Finch and Sumita, 2010) and can be trained by a sampling algorithm extended for a constraint on noise. [sent-22, score-0.28]
</p><p>18 Our experiments of Japanese-to-English transliteration achieved 16% relative error reduction in transliteration accuracy from the sample-wise method. [sent-23, score-1.459]
</p><p>19 hc o2d0s1 i3n A Nsastoucria lti Loan fgoura Cgoem Ppruotcaetsiosin agl, L piang eusis 2t0ic4s–209, 2  Bayesian many-to-many alignment  We briefly review a Bayesian many-to-many character alignment proposed by Finch and Sumita (2010) on which our model is based. [sent-27, score-0.555]
</p><p>20 The model is based on a generative process of bilingual substring pairs ⟨ s¯, t¯⟩ by the following Dirichlet process (DP): G|α,G0  ∼  DP(α, G0)  ⟨ s¯, t¯⟩ |G  ∼  G,  where G is a probability distribution over substring pairs according to a DP prior with base measure G0 and hyperparameter α. [sent-28, score-0.518]
</p><p>21 G0 is modeled as a joint spelling model as follows:  G0(⟨ s¯,t¯⟩) =λ| s¯|ss|¯! [sent-29, score-0.035]
</p><p>22 (1)  This is a simple joint probability of the spelling  models, in which each alphabet appears based on a uniform distribution over the vocabulary (of size vs and vt) and each string length follows a Poisson distribution (with the average length λs and λt). [sent-32, score-0.035]
</p><p>23 The model handles infinite number of substring pairs according to the Chinese Restaurant Process (CRP). [sent-33, score-0.228]
</p><p>24 The probability of a substring pair ⟨ s¯ k, t¯k⟩ (isC Rb aPs)e. [sent-34, score-0.158]
</p><p>25 d T ohne th preo cboaubnitlsit yof o afll a o stuhbers sruinbgstr painigr p⟨ s¯ airs as⟩ follows: p (⟨ ¯sk,t¯k⟩| {⟨¯ s,¯t⟩})  =N (⟨s¯ )∑k,it¯Nk⟩ ()⟨ + ¯si, αt¯Gi⟩0)( +⟨ s¯k α,t¯k⟩). [sent-35, score-0.04]
</p><p>26 (2)  Here {⟨ s¯, t¯⟩ }−k means a set of substring pairs excluding ⟨ s¯ k, t¯k−⟩,k and N (⟨¯ sk, t¯k⟩) is the number of ⟨ s¯ k, t¯k⟩ ⟨i sn¯ the current sample space. [sent-36, score-0.201]
</p><p>27 eT nhuims alignment m⟩o idnel t hise su cuitrarbelnet f soarm representing very sparse distribution over arbitrary substring pairs, thanks to reasonable CRP-based smoothing for unseen pairs based on the spelling model. [sent-37, score-0.465]
</p><p>28 3  Proposed method  We propose an extended many-to-many alignment  model that can handle partial noise. [sent-38, score-0.417]
</p><p>29 We extend the model in the previous section by introducing a noise symbol and state-based probability calculation. [sent-39, score-0.344]
</p><p>30 c カo バv eー r (a) no noise  (b) noise  (c) partial noise: English side should be “give up”  (d) partial noise: Japanese side should be “ リ カ バー”  Figure 1: Three types of noise in transliteration data. [sent-44, score-2.04]
</p><p>31 1 Partial noise in transliteration data Figure 1 shows transliteration examples with “no noise,” “noise,” and “partial noise. [sent-47, score-1.751]
</p><p>32 ” Solid lines in the figure show correct many-to-many alignment links. [sent-48, score-0.257]
</p><p>33 We aim to do alignment as in the examples (c) and (d) by distinguishing its non-transliteration (noise) part, which cannot be handled by the existing methods. [sent-51, score-0.253]
</p><p>34 2 Noise-aware alignment model We introduce a noise symbol to handle partial noise in the many-to-many alignment model. [sent-53, score-1.283]
</p><p>35 (2012) extended the many-to-many alignment for the sample-wise transliteration mining, but its noise model only handles the sample-wise noise and cannot distinguish partial noise. [sent-55, score-1.791]
</p><p>36 We model partial noise in the CRP-based joint substring model. [sent-56, score-0.639]
</p><p>37 Partial noise in transliteration data typically appears in compound words as mentioned earlier, because their counterparts consisting of two or more words may not be fully covered in automatically extracted words and phrases as shown in Figure 1(c). [sent-57, score-1.141]
</p><p>38 Another type of partial noise is derived from morphological differences due to inflection, which usually appear in the sub-word level as prefixes and suffixes as shown in Figure 1(d). [sent-58, score-0.481]
</p><p>39 According to this intuition, we assume that partial noise appears in the beginning and/or end of transliteration data (in  case of sample-wise noise, we assume the noise is in the beginning). [sent-59, score-1.555]
</p><p>40 This assumption derives a constraint between signal and noise parts that helps to avoid a welter of transliteration and non-transliteration parts. [sent-60, score-1.113]
</p><p>41 s Figure 2: Example of many-to-many alignment with partial noise in the beginning and end. [sent-64, score-0.75]
</p><p>42 “noise” stands for the noise symbol and “sp” stands for a white space. [sent-65, score-0.404]
</p><p>43 not appropriate for noise in the middle, but handling arbitrary number of noise parts increases computational complexity and sparseness. [sent-66, score-0.661]
</p><p>44 Figure 2 shows a partial noise example in both the beginning and end. [sent-68, score-0.521]
</p><p>45 This example is actually correct translation but includes noise in a sense of transliteration; an article “the” is wrongly included in the phrase pair (no articles are used in Japanese) and a plural noun “masks” is transliterated into  “マス ク ”(mask). [sent-69, score-0.446]
</p><p>46 These non-transliteration parts are aligned to noise symbols in the proposed model. [sent-70, score-0.446]
</p><p>47 The noise symbols are treated as zero-length substrings in the model, same as other substrings. [sent-71, score-0.392]
</p><p>48 3  Constrained Gibbs sampling  Finch and Sumita (2010) used a blocked Gibbs sampling algorithm with forward-filtering backwardsampling (FFBS) (Mochihashi et al. [sent-73, score-0.064]
</p><p>49 We extend their algorithm for our noise-aware model using a state-based calculation over the three states: non-transliteration part in the beginning (noiseB), transliteration part (signal), non-transliteration part in the end (noiseE). [sent-75, score-0.777]
</p><p>50 At first in the forward filtering, we begin with transition to noiseB and signal. [sent-77, score-0.041]
</p><p>51 The calculation of forward probabilities itself is almost the same as Finch and Sumita (2010) except for state transition constraints: from noiseB to signal, from signal to noiseE. [sent-78, score-0.113]
</p><p>52 The backward-sampling traverses a path by probabilitybased sampling with true posteriors, starting from the choice of the ending state among noiseB (means full noise), signal, and noiseE. [sent-79, score-0.032]
</p><p>53 206  Figure 3: State-based FFBS for the proposed model. [sent-81, score-0.036]
</p><p>54 4  Experiments  We conducted experiments comparing the proposed method with the conventional sample-wise method for the use in bootstrapping statistical  machine transliteration using Japanese-to-English patent translation dataset (Goto et al. [sent-82, score-1.046]
</p><p>55 1 Training data setup First, we trained a phrase table on the 3. [sent-85, score-0.045]
</p><p>56 We obtained 591,840 phrase table entries whose Japanese side was written in katakana (Japanese phonogram) only2. [sent-87, score-0.234]
</p><p>57 Then, we iteratively ran the method of Sajjad et al. [sent-88, score-0.024]
</p><p>58 (2012) on these entries and eliminate non-transliteration pairs, until the number of pairs converged. [sent-89, score-0.067]
</p><p>59 Finally we obtain 104,563 katakana-English pairs after 10 iterations; they were our baseline training set mined by sample-wise method. [sent-90, score-0.065]
</p><p>60 ’s method as preprocessing for filtering sample-wise noise while the proposed method could also do that, because the proposed method took much more training time for all phrase table entries. [sent-92, score-0.532]
</p><p>61 2 Transliteration experiments The transliteration experiment used a translationbased implementation with Moses, using a  1http://code. [sent-94, score-0.737]
</p><p>62 com/p/mecab/ 2This katakana-based filtering is a language dependent heuristic for choosing potential transliteration candidate, because transliterations in Japanese are usually written in katakana. [sent-96, score-0.762]
</p><p>63 character-based 7-gram language model trained on 300M English patent sentences. [sent-97, score-0.131]
</p><p>64 The test set was top-1000 unknown (in the Japanese-to-English translation model) katakana words appeared in 400M Japanese patent sentences. [sent-99, score-0.339]
</p><p>65 8% of all unknown words (excluding numbers); that is, more than a half of unknown words were katakana words. [sent-102, score-0.217]
</p><p>66 1 Sample-wise method (BASELINE) We used the baseline training set to train statistical machine transliteration model for our baseline. [sent-105, score-0.794]
</p><p>67 The training procedure was based on Moses: MGIZA++ word alignment, grow-diag-final-and alignment symmetrization and phrase extraction with the maximum phrase length of 7. [sent-106, score-0.319]
</p><p>68 2 Proposed method (PROPOSED) We applied the proposed method to the baseline  training set with 30 sampling iterations and eliminated partial noise. [sent-109, score-0.368]
</p><p>69 The transliteration model was trained in the same manner as BASELINE after eliminating noise. [sent-110, score-0.717]
</p><p>70 The hyperparameters, α, λs, and λt, were optimized using a held-out set of 2,000 katakanaEnglish pairs that were randomly chosen from a general-domain bilingual dictionary. [sent-111, score-0.132]
</p><p>71 The hyperparameter optimization was based on F-score values on the held-out set with varying α among 0. [sent-112, score-0.027]
</p><p>72 Note that we applied the proposed method to BASELINE data (the sample-wise method was already applied until convergence). [sent-119, score-0.084]
</p><p>73 The proposed method eliminated only two transliteration candidates in sample-wise but also eliminated 5,714 (0. [sent-120, score-0.909]
</p><p>74 PBRAOSPELOISNEED110 4 , 5 6 318 9 39, 3068601 , 3 17 2, 295963 phrases in statistical machine transliteration and improved transliteration performance (Finch and Sumita, 2010). [sent-129, score-1.485]
</p><p>75 We extracted them by: 1) generate many-to-many word alignment, in which all possible word alignment links in many-to-many correspondences (e. [sent-130, score-0.229]
</p><p>76 , 0-0 0-1 0-2 1-0 1-1 1-2 for ⟨ コ ン, c o m⟩), 2) run phrase e0-x1tra 0c-2tio 1n- 0an 1d-1 scoring same as a osta mn⟩d)a, 2rd) rMunos pehsr training. [sent-132, score-0.045]
</p><p>77 tTiohnis a procedure extracts longer phrases satisfying the many-to-many alignment constraints than the simple use of extracted joint substring pairs as phrases. [sent-133, score-0.45]
</p><p>78 PROPOSED achieved 63% in ACC (16% relative error reduction from BASELINE), and 94. [sent-141, score-0.025]
</p><p>79 These improvements clearly showed an advantage of the proposed method over the samplewise mining. [sent-143, score-0.107]
</p><p>80 The results suggest that the partial noise can hurt transliteration models. [sent-148, score-1.223]
</p><p>81 PROPOSED-JOINT showed similar performance as PROPOSED with a slight drop in BLEUc, although many-to-many substring alignment was expected to improve transliteration as reported by Finch and Sumita (2010). [sent-149, score-1.104]
</p><p>82 The difference may be due to the difference in coverage of the phrase tables; PROPOSED-JOINT retained relatively long substrings by the many-to-many alignment constraints in contrast to the less-constrained grow-  diag-final-and alignments in PROPOSED. [sent-150, score-0.355]
</p><p>83 Since the training data in our bootstrapping experiments conTable 2: Japanese-to-English transliteration results for top-1000 unknown katakana words. [sent-151, score-0.952]
</p><p>84 8 968748  tained many similar phrases unlike dictionary-based data in Finch and Sumita (2010), the phrase table of PROPOSED-JOINT may have a small coverage due to long and sparse substring pairs with large probabilities even if the many-to-many alignment was good. [sent-156, score-0.514]
</p><p>85 This sparseness problem is beyond the scope of this paper and worth further study. [sent-157, score-0.046]
</p><p>86 4 Alignment Examples Figure 4 shows examples of the alignment results in the training data. [sent-159, score-0.229]
</p><p>87 As expected, partial noise both in Japanese and English was identified correctly in (a), (b), and (c). [sent-160, score-0.481]
</p><p>88 There were some alignment errors in the signal part in (b), in which characters in boundary  positions were aligned incorrectly to adjacent substrings. [sent-161, score-0.348]
</p><p>89 These alignment errors did not directly degrade the partial noise identification but may cause a negative effect on overall alignment performance in the sampling-based optimization. [sent-162, score-0.939]
</p><p>90 (d) is a negative example in which partial noise was incorrectly aligned. [sent-163, score-0.501]
</p><p>91 (c) and (d) have similar partial noise in their English word endings, but it could not be identified in (d). [sent-164, score-0.481]
</p><p>92 One possible reason for that is the sparseness problem mentioned above, as shown in erroneous long character alignments in (d). [sent-165, score-0.112]
</p><p>93 5  Conclusion  This paper proposed a noise-aware many-to-many alignment model that can distinguish partial noise in transliteration pairs for bootstrapping statistical machine transliteration model from automatically extracted phrase pairs. [sent-166, score-2.374]
</p><p>94 The proposed method was proved to be effective in Japanese-to-English transliteration experiments in patent domain. [sent-168, score-0.908]
</p><p>95 Future work will investigate the proposed method  208  nao ise? [sent-169, score-0.06]
</p><p>96 (a) Correctly aligned  (b) Some alignment errors in transliteration part  (c) Correctly aligned  (d) Errors in partial noise  Figure 4: Examples of noise-aware many-to-many alignment in the training data. [sent-175, score-1.75]
</p><p>97 Dashed lines show incorrect alignments, and bold grey lines mean their corrections. [sent-177, score-0.056]
</p><p>98 The partial noise would appear in other language pairs, typically between agglutinative and non-agglutinative  languages. [sent-179, score-0.481]
</p><p>99 It is also worth extending the approach into word alignment in statistical machine translation. [sent-180, score-0.28]
</p><p>100 A statistical model for unsupervised and semisupervised transliteration mining. [sent-216, score-0.748]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('transliteration', 0.717), ('noise', 0.317), ('alignment', 0.229), ('finch', 0.205), ('sumita', 0.164), ('partial', 0.164), ('substring', 0.158), ('katakana', 0.143), ('patent', 0.131), ('sajjad', 0.118), ('bleuc', 0.094), ('noiseb', 0.094), ('bilingual', 0.089), ('japanese', 0.086), ('ffbs', 0.071), ('eliminated', 0.066), ('character', 0.061), ('acc', 0.057), ('substrings', 0.056), ('bootstrapping', 0.055), ('signal', 0.052), ('htun', 0.047), ('noi', 0.047), ('samplewise', 0.047), ('aligned', 0.047), ('phrase', 0.045), ('pairs', 0.043), ('eiichiro', 0.043), ('bayesian', 0.043), ('sp', 0.04), ('beginning', 0.04), ('transliterated', 0.037), ('goto', 0.037), ('unknown', 0.037), ('dp', 0.037), ('proposed', 0.036), ('compound', 0.036), ('mochihashi', 0.035), ('spelling', 0.035), ('kyoto', 0.033), ('ise', 0.033), ('sampling', 0.032), ('statistical', 0.031), ('sudoh', 0.031), ('moses', 0.03), ('stands', 0.03), ('counterparts', 0.03), ('translation', 0.028), ('lines', 0.028), ('parts', 0.027), ('symbol', 0.027), ('hyperparameter', 0.027), ('handles', 0.027), ('filtering', 0.026), ('sparseness', 0.026), ('hurt', 0.025), ('alignments', 0.025), ('reduction', 0.025), ('method', 0.024), ('entries', 0.024), ('distinguishing', 0.024), ('mining', 0.022), ('baseline', 0.022), ('side', 0.022), ('forward', 0.022), ('papineni', 0.021), ('se', 0.021), ('covered', 0.021), ('knight', 0.021), ('solid', 0.021), ('naonori', 0.02), ('yoshiki', 0.02), ('preo', 0.02), ('mgiza', 0.02), ('translationbased', 0.02), ('twhi', 0.02), ('isc', 0.02), ('yof', 0.02), ('mask', 0.02), ('ja', 0.02), ('kumaran', 0.02), ('laboratories', 0.02), ('phrases', 0.02), ('calculation', 0.02), ('worth', 0.02), ('incorrectly', 0.02), ('distinguish', 0.02), ('transition', 0.019), ('extension', 0.019), ('symbols', 0.019), ('actually', 0.019), ('shinsuke', 0.019), ('crp', 0.019), ('tained', 0.019), ('transliterations', 0.019), ('masks', 0.019), ('pna', 0.019), ('chow', 0.019), ('isao', 0.019), ('aps', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="139-tfidf-1" href="./emnlp-2013-Noise-Aware_Character_Alignment_for_Bootstrapping_Statistical_Machine_Transliteration_from_Bilingual_Corpora.html">139 emnlp-2013-Noise-Aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora</a></p>
<p>Author: Katsuhito Sudoh ; Shinsuke Mori ; Masaaki Nagata</p><p>Abstract: This paper proposes a novel noise-aware character alignment method for bootstrapping statistical machine transliteration from automatically extracted phrase pairs. The model is an extension of a Bayesian many-to-many alignment method for distinguishing nontransliteration (noise) parts in phrase pairs. It worked effectively in the experiments of bootstrapping Japanese-to-English statistical machine transliteration in patent domain using patent bilingual corpora.</p><p>2 0.1196112 <a title="139-tfidf-2" href="./emnlp-2013-Semi-Markov_Phrase-Based_Monolingual_Alignment.html">167 emnlp-2013-Semi-Markov Phrase-Based Monolingual Alignment</a></p>
<p>Author: Xuchen Yao ; Benjamin Van Durme ; Chris Callison-Burch ; Peter Clark</p><p>Abstract: We introduce a novel discriminative model for phrase-based monolingual alignment using a semi-Markov CRF. Our model achieves stateof-the-art alignment accuracy on two phrasebased alignment datasets (RTE and paraphrase), while doing significantly better than other strong baselines in both non-identical alignment and phrase-only alignment. Additional experiments highlight the potential benefit of our alignment model to RTE, paraphrase identification and question answering, where even a naive application of our model’s alignment score approaches the state ofthe art.</p><p>3 0.095657386 <a title="139-tfidf-3" href="./emnlp-2013-Boosting_Cross-Language_Retrieval_by_Learning_Bilingual_Phrase_Associations_from_Relevance_Rankings.html">39 emnlp-2013-Boosting Cross-Language Retrieval by Learning Bilingual Phrase Associations from Relevance Rankings</a></p>
<p>Author: Artem Sokokov ; Laura Jehl ; Felix Hieber ; Stefan Riezler</p><p>Abstract: We present an approach to learning bilingual n-gram correspondences from relevance rankings of English documents for Japanese queries. We show that directly optimizing cross-lingual rankings rivals and complements machine translation-based cross-language information retrieval (CLIR). We propose an efficient boosting algorithm that deals with very large cross-product spaces of word correspondences. We show in an experimental evaluation on patent prior art search that our approach, and in particular a consensus-based combination of boosting and translation-based approaches, yields substantial improvements in CLIR performance. Our training and test data are made publicly available.</p><p>4 0.082966626 <a title="139-tfidf-4" href="./emnlp-2013-Improving_Alignment_of_System_Combination_by_Using_Multi-objective_Optimization.html">101 emnlp-2013-Improving Alignment of System Combination by Using Multi-objective Optimization</a></p>
<p>Author: Tian Xia ; Zongcheng Ji ; Shaodan Zhai ; Yidong Chen ; Qun Liu ; Shaojun Wang</p><p>Abstract: This paper proposes a multi-objective optimization framework which supports heterogeneous information sources to improve alignment in machine translation system combination techniques. In this area, most of techniques usually utilize confusion networks (CN) as their central data structure to compact an exponential number of an potential hypotheses, and because better hypothesis alignment may benefit constructing better quality confusion networks, it is natural to add more useful information to improve alignment results. However, these information may be heterogeneous, so the widely-used Viterbi algorithm for searching the best alignment may not apply here. In the multi-objective optimization framework, each information source is viewed as an independent objective, and a new goal of improving all objectives can be searched by mature algorithms. The solutions from this framework, termed Pareto optimal solutions, are then combined to construct confusion networks. Experiments on two Chinese-to-English translation datasets show significant improvements, 0.97 and 1.06 BLEU points over a strong Indirected Hidden Markov Model-based (IHMM) system, and 4.75 and 3.53 points over the best single machine translation systems.</p><p>5 0.075671732 <a title="139-tfidf-5" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>Author: Ivan Vulic ; Marie-Francine Moens</p><p>Abstract: We present a new language pair agnostic approach to inducing bilingual vector spaces from non-parallel data without any other resource in a bootstrapping fashion. The paper systematically introduces and describes all key elements of the bootstrapping procedure: (1) starting point or seed lexicon, (2) the confidence estimation and selection of new dimensions of the space, and (3) convergence. We test the quality of the induced bilingual vector spaces, and analyze the influence of the different components of the bootstrapping approach in the task of bilingual lexicon extraction (BLE) for two language pairs. Results reveal that, contrary to conclusions from prior work, the seeding of the bootstrapping process has a heavy impact on the quality of the learned lexicons. We also show that our approach outperforms the best performing fully corpus-based BLE methods on these test sets.</p><p>6 0.067696199 <a title="139-tfidf-6" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>7 0.065748535 <a title="139-tfidf-7" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>8 0.060264945 <a title="139-tfidf-8" href="./emnlp-2013-Shift-Reduce_Word_Reordering_for_Machine_Translation.html">171 emnlp-2013-Shift-Reduce Word Reordering for Machine Translation</a></p>
<p>9 0.057405353 <a title="139-tfidf-9" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>10 0.056329262 <a title="139-tfidf-10" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>11 0.054113362 <a title="139-tfidf-11" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>12 0.052424647 <a title="139-tfidf-12" href="./emnlp-2013-Paraphrasing_4_Microblog_Normalization.html">151 emnlp-2013-Paraphrasing 4 Microblog Normalization</a></p>
<p>13 0.04836484 <a title="139-tfidf-13" href="./emnlp-2013-A_Convex_Alternative_to_IBM_Model_2.html">2 emnlp-2013-A Convex Alternative to IBM Model 2</a></p>
<p>14 0.047568709 <a title="139-tfidf-14" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>15 0.046386503 <a title="139-tfidf-15" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>16 0.042761832 <a title="139-tfidf-16" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>17 0.041741516 <a title="139-tfidf-17" href="./emnlp-2013-Identifying_Phrasal_Verbs_Using_Many_Bilingual_Corpora.html">96 emnlp-2013-Identifying Phrasal Verbs Using Many Bilingual Corpora</a></p>
<p>18 0.041118231 <a title="139-tfidf-18" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>19 0.039113816 <a title="139-tfidf-19" href="./emnlp-2013-Automatic_Knowledge_Acquisition_for_Case_Alternation_between_the_Passive_and_Active_Voices_in_Japanese.html">33 emnlp-2013-Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in Japanese</a></p>
<p>20 0.038992703 <a title="139-tfidf-20" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.131), (1, -0.093), (2, 0.008), (3, -0.005), (4, 0.003), (5, -0.015), (6, 0.005), (7, 0.077), (8, -0.031), (9, -0.049), (10, 0.025), (11, 0.03), (12, 0.12), (13, 0.073), (14, 0.03), (15, -0.011), (16, 0.025), (17, 0.017), (18, 0.083), (19, -0.041), (20, -0.02), (21, -0.061), (22, -0.014), (23, 0.119), (24, -0.002), (25, 0.058), (26, -0.026), (27, -0.009), (28, 0.085), (29, -0.027), (30, -0.154), (31, 0.11), (32, -0.031), (33, 0.153), (34, -0.008), (35, 0.121), (36, -0.063), (37, -0.147), (38, -0.154), (39, 0.009), (40, 0.082), (41, 0.075), (42, -0.075), (43, 0.12), (44, 0.0), (45, 0.043), (46, 0.185), (47, -0.151), (48, 0.055), (49, -0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9616096 <a title="139-lsi-1" href="./emnlp-2013-Noise-Aware_Character_Alignment_for_Bootstrapping_Statistical_Machine_Transliteration_from_Bilingual_Corpora.html">139 emnlp-2013-Noise-Aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora</a></p>
<p>Author: Katsuhito Sudoh ; Shinsuke Mori ; Masaaki Nagata</p><p>Abstract: This paper proposes a novel noise-aware character alignment method for bootstrapping statistical machine transliteration from automatically extracted phrase pairs. The model is an extension of a Bayesian many-to-many alignment method for distinguishing nontransliteration (noise) parts in phrase pairs. It worked effectively in the experiments of bootstrapping Japanese-to-English statistical machine transliteration in patent domain using patent bilingual corpora.</p><p>2 0.67576182 <a title="139-lsi-2" href="./emnlp-2013-Improving_Alignment_of_System_Combination_by_Using_Multi-objective_Optimization.html">101 emnlp-2013-Improving Alignment of System Combination by Using Multi-objective Optimization</a></p>
<p>Author: Tian Xia ; Zongcheng Ji ; Shaodan Zhai ; Yidong Chen ; Qun Liu ; Shaojun Wang</p><p>Abstract: This paper proposes a multi-objective optimization framework which supports heterogeneous information sources to improve alignment in machine translation system combination techniques. In this area, most of techniques usually utilize confusion networks (CN) as their central data structure to compact an exponential number of an potential hypotheses, and because better hypothesis alignment may benefit constructing better quality confusion networks, it is natural to add more useful information to improve alignment results. However, these information may be heterogeneous, so the widely-used Viterbi algorithm for searching the best alignment may not apply here. In the multi-objective optimization framework, each information source is viewed as an independent objective, and a new goal of improving all objectives can be searched by mature algorithms. The solutions from this framework, termed Pareto optimal solutions, are then combined to construct confusion networks. Experiments on two Chinese-to-English translation datasets show significant improvements, 0.97 and 1.06 BLEU points over a strong Indirected Hidden Markov Model-based (IHMM) system, and 4.75 and 3.53 points over the best single machine translation systems.</p><p>3 0.58634275 <a title="139-lsi-3" href="./emnlp-2013-Semi-Markov_Phrase-Based_Monolingual_Alignment.html">167 emnlp-2013-Semi-Markov Phrase-Based Monolingual Alignment</a></p>
<p>Author: Xuchen Yao ; Benjamin Van Durme ; Chris Callison-Burch ; Peter Clark</p><p>Abstract: We introduce a novel discriminative model for phrase-based monolingual alignment using a semi-Markov CRF. Our model achieves stateof-the-art alignment accuracy on two phrasebased alignment datasets (RTE and paraphrase), while doing significantly better than other strong baselines in both non-identical alignment and phrase-only alignment. Additional experiments highlight the potential benefit of our alignment model to RTE, paraphrase identification and question answering, where even a naive application of our model’s alignment score approaches the state ofthe art.</p><p>4 0.53022224 <a title="139-lsi-4" href="./emnlp-2013-Automatic_Knowledge_Acquisition_for_Case_Alternation_between_the_Passive_and_Active_Voices_in_Japanese.html">33 emnlp-2013-Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in Japanese</a></p>
<p>Author: Ryohei Sasano ; Daisuke Kawahara ; Sadao Kurohashi ; Manabu Okumura</p><p>Abstract: We present a method for automatically acquiring knowledge for case alternation between the passive and active voices in Japanese. By leveraging several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, our method aligns a case frame in the passive voice to a corresponding case frame in the active voice and finds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness.</p><p>5 0.47003862 <a title="139-lsi-5" href="./emnlp-2013-Two-Stage_Method_for_Large-Scale_Acquisition_of_Contradiction_Pattern_Pairs_using_Entailment.html">189 emnlp-2013-Two-Stage Method for Large-Scale Acquisition of Contradiction Pattern Pairs using Entailment</a></p>
<p>Author: Julien Kloetzer ; Stijn De Saeger ; Kentaro Torisawa ; Chikara Hashimoto ; Jong-Hoon Oh ; Motoki Sano ; Kiyonori Ohtake</p><p>Abstract: In this paper we propose a two-stage method to acquire contradiction relations between typed lexico-syntactic patterns such as Xdrug prevents Ydisease and Ydisease caused by Xdrug. In the first stage, we train an SVM classifier to detect contradiction pattern pairs in a large web archive by exploiting the excitation polarity (Hashimoto et al., 2012) of the patterns. In the second stage, we enlarge the first stage classifier’s training data with new contradiction pairs obtained by combining the output of the first stage’s classifier and that of an entailment classifier. We acquired this way 750,000 typed Japanese contradiction pattern pairs with an estimated precision of 80%. We plan to release this resource to the NLP community. 1</p><p>6 0.46504325 <a title="139-lsi-6" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>7 0.4394922 <a title="139-lsi-7" href="./emnlp-2013-A_Convex_Alternative_to_IBM_Model_2.html">2 emnlp-2013-A Convex Alternative to IBM Model 2</a></p>
<p>8 0.39378247 <a title="139-lsi-8" href="./emnlp-2013-Boosting_Cross-Language_Retrieval_by_Learning_Bilingual_Phrase_Associations_from_Relevance_Rankings.html">39 emnlp-2013-Boosting Cross-Language Retrieval by Learning Bilingual Phrase Associations from Relevance Rankings</a></p>
<p>9 0.37515518 <a title="139-lsi-9" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>10 0.32692012 <a title="139-lsi-10" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>11 0.29883114 <a title="139-lsi-11" href="./emnlp-2013-Converting_Continuous-Space_Language_Models_into_N-Gram_Language_Models_for_Statistical_Machine_Translation.html">52 emnlp-2013-Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation</a></p>
<p>12 0.29227117 <a title="139-lsi-12" href="./emnlp-2013-Shift-Reduce_Word_Reordering_for_Machine_Translation.html">171 emnlp-2013-Shift-Reduce Word Reordering for Machine Translation</a></p>
<p>13 0.27826551 <a title="139-lsi-13" href="./emnlp-2013-Building_Specialized_Bilingual_Lexicons_Using_Large_Scale_Background_Knowledge.html">42 emnlp-2013-Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge</a></p>
<p>14 0.26413351 <a title="139-lsi-14" href="./emnlp-2013-Combining_Generative_and_Discriminative_Model_Scores_for_Distant_Supervision.html">49 emnlp-2013-Combining Generative and Discriminative Model Scores for Distant Supervision</a></p>
<p>15 0.26383209 <a title="139-lsi-15" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>16 0.25641507 <a title="139-lsi-16" href="./emnlp-2013-Paraphrasing_4_Microblog_Normalization.html">151 emnlp-2013-Paraphrasing 4 Microblog Normalization</a></p>
<p>17 0.24793032 <a title="139-lsi-17" href="./emnlp-2013-Identifying_Phrasal_Verbs_Using_Many_Bilingual_Corpora.html">96 emnlp-2013-Identifying Phrasal Verbs Using Many Bilingual Corpora</a></p>
<p>18 0.24290213 <a title="139-lsi-18" href="./emnlp-2013-Ubertagging%3A_Joint_Segmentation_and_Supertagging_for_English.html">190 emnlp-2013-Ubertagging: Joint Segmentation and Supertagging for English</a></p>
<p>19 0.23983316 <a title="139-lsi-19" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>20 0.23785359 <a title="139-lsi-20" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.025), (9, 0.021), (10, 0.011), (18, 0.044), (22, 0.046), (30, 0.107), (45, 0.029), (50, 0.01), (51, 0.164), (52, 0.307), (66, 0.03), (71, 0.019), (75, 0.029), (77, 0.021), (90, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.77740479 <a title="139-lda-1" href="./emnlp-2013-Automated_Essay_Scoring_by_Maximizing_Human-Machine_Agreement.html">28 emnlp-2013-Automated Essay Scoring by Maximizing Human-Machine Agreement</a></p>
<p>Author: Hongbo Chen ; Ben He</p><p>Abstract: Previous approaches for automated essay scoring (AES) learn a rating model by minimizing either the classification, regression, or pairwise classification loss, depending on the learning algorithm used. In this paper, we argue that the current AES systems can be further improved by taking into account the agreement between human and machine raters. To this end, we propose a rankbased approach that utilizes listwise learning to rank algorithms for learning a rating model, where the agreement between the human and machine raters is directly incorporated into the loss function. Various linguistic and statistical features are utilized to facilitate the learning algorithms. Experiments on the publicly available English essay dataset, Automated Student Assessment Prize (ASAP), show that our proposed approach outperforms the state-of-the-art algorithms, and achieves performance comparable to professional human raters, which suggests the effectiveness of our proposed method for automated essay scoring.</p><p>same-paper 2 0.76687729 <a title="139-lda-2" href="./emnlp-2013-Noise-Aware_Character_Alignment_for_Bootstrapping_Statistical_Machine_Transliteration_from_Bilingual_Corpora.html">139 emnlp-2013-Noise-Aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora</a></p>
<p>Author: Katsuhito Sudoh ; Shinsuke Mori ; Masaaki Nagata</p><p>Abstract: This paper proposes a novel noise-aware character alignment method for bootstrapping statistical machine transliteration from automatically extracted phrase pairs. The model is an extension of a Bayesian many-to-many alignment method for distinguishing nontransliteration (noise) parts in phrase pairs. It worked effectively in the experiments of bootstrapping Japanese-to-English statistical machine transliteration in patent domain using patent bilingual corpora.</p><p>3 0.75861382 <a title="139-lda-3" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>Author: Stella Frank ; Frank Keller ; Sharon Goldwater</p><p>Abstract: Frank Keller keller@ inf .ed .ac .uk Sharon Goldwater sgwater@ inf .ed .ac .uk ILCC, School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK interactions are often (but not necessarily) synergisChildren learn various levels of linguistic structure concurrently, yet most existing models of language acquisition deal with only a single level of structure, implicitly assuming a sequential learning process. Developing models that learn multiple levels simultaneously can provide important insights into how these levels might interact synergistically dur- ing learning. Here, we present a model that jointly induces syntactic categories and morphological segmentations by combining two well-known models for the individual tasks. We test on child-directed utterances in English and Spanish and compare to single-task baselines. In the morphologically poorer language (English), the model improves morphological segmentation, while in the morphologically richer language (Spanish), it leads to better syntactic categorization. These results provide further evidence that joint learning is useful, but also suggest that the benefits may be different for typologically different languages.</p><p>4 0.5590499 <a title="139-lda-4" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>Author: Xiaoqing Zheng ; Hanyang Chen ; Tianyu Xu</p><p>Abstract: This study explores the feasibility of performing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representation of Chinese characters, and use these improved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-theart performance with minimal computational cost. We also describe a perceptron-style algorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented.</p><p>5 0.55624348 <a title="139-lda-5" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>Author: Uri Lerner ; Slav Petrov</p><p>Abstract: We present a simple and novel classifier-based preordering approach. Unlike existing preordering models, we train feature-rich discriminative classifiers that directly predict the target-side word order. Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discriminative model with a rich set of features, including lexical features. We present extensive experiments on 22 language pairs, including preordering into English from 7 other languages. We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task. For languages from different families the improvements often exceed 2 BLEU. Many of these gains are also significant in human evaluations.</p><p>6 0.5547787 <a title="139-lda-6" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>7 0.55382621 <a title="139-lda-7" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>8 0.55377889 <a title="139-lda-8" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>9 0.55339003 <a title="139-lda-9" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>10 0.55236596 <a title="139-lda-10" href="./emnlp-2013-Dynamic_Feature_Selection_for_Dependency_Parsing.html">66 emnlp-2013-Dynamic Feature Selection for Dependency Parsing</a></p>
<p>11 0.55090475 <a title="139-lda-11" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>12 0.55034584 <a title="139-lda-12" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>13 0.5492385 <a title="139-lda-13" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>14 0.54857701 <a title="139-lda-14" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>15 0.54852474 <a title="139-lda-15" href="./emnlp-2013-Semi-Markov_Phrase-Based_Monolingual_Alignment.html">167 emnlp-2013-Semi-Markov Phrase-Based Monolingual Alignment</a></p>
<p>16 0.54840994 <a title="139-lda-16" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>17 0.54774278 <a title="139-lda-17" href="./emnlp-2013-Converting_Continuous-Space_Language_Models_into_N-Gram_Language_Models_for_Statistical_Machine_Translation.html">52 emnlp-2013-Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation</a></p>
<p>18 0.54743296 <a title="139-lda-18" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>19 0.5466662 <a title="139-lda-19" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>20 0.54616034 <a title="139-lda-20" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
