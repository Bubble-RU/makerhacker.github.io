<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-17" href="#">emnlp2013-17</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</h1>
<br/><p>Source: <a title="emnlp-2013-17-pdf" href="http://aclweb.org/anthology//D/D13/D13-1144.pdf">pdf</a></p><p>Author: Shashank Srivastava ; Dirk Hovy ; Eduard Hovy</p><p>Abstract: In this paper, we propose a walk-based graph kernel that generalizes the notion of treekernels to continuous spaces. Our proposed approach subsumes a general framework for word-similarity, and in particular, provides a flexible way to incorporate distributed representations. Using vector representations, such an approach captures both distributional semantic similarities among words as well as the structural relations between them (encoded as the structure of the parse tree). We show an efficient formulation to compute this kernel using simple matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results.</p><p>Reference: <a title="emnlp-2013-17-reference" href="../emnlp2013_reference/emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract In this paper, we propose a walk-based graph kernel that generalizes the notion of treekernels to continuous spaces. [sent-2, score-0.482]
</p><p>2 Our proposed approach subsumes a general framework for word-similarity, and in particular, provides a flexible way to incorporate distributed representations. [sent-3, score-0.093]
</p><p>3 Using vector representations, such an approach captures both distributional semantic similarities among words as well as the structural relations between them (encoded as the structure of the parse tree). [sent-4, score-0.273]
</p><p>4 We show an efficient formulation to compute this kernel using simple matrix operations. [sent-5, score-0.564]
</p><p>5 1 Introduction Capturing semantic similarity between sentences is a fundamental issue in NLP, with applications in a wide range of tasks. [sent-7, score-0.178]
</p><p>6 Previously, tree kernels based  on common substructures have been used to model similarity between parse trees (Collins and Duffy, 2002; Moschitti, 2004; Moschitti, 2006b). [sent-8, score-0.976]
</p><p>7 These kernels encode a high number of latent syntactic features within a concise representation, and compute the similarity between two parse trees based on the matching of node-labels (words, POS tags, etc. [sent-9, score-0.913]
</p><p>8 While this is sufficient to capture syntactic similarity, it does not capture semantic similarity very well, even when using discrete semantic types as node labels. [sent-11, score-0.405]
</p><p>9 This constrains the utility of many traditional tree kernels in two ways: i) two sentences that are syntactically identical, but have no semantic similarity can receive a high matching score (see Table 1, top) while ii) two sentences with only local 1411 edu, mai l dirkhovy . [sent-12, score-0.897]
</p><p>10 com @ syntactic overlap, but high semantic similarity can receive low scores (see Table 1, bottom). [sent-13, score-0.254]
</p><p>11 ✓aticpshlougwresmantic similarity  In contrast, distributional vector representations of words have been successful in capturing finegrained semantics, but lack syntactic knowledge. [sent-16, score-0.311]
</p><p>12 Resources such as Wordnet, dictionaries and ontologies that encode different semantic perspectives can also provide additional knowledge infusion. [sent-17, score-0.052]
</p><p>13 In this paper, we describe a generic walk-based graph kernel for dependency parse trees that subsumes general notions of word-similarity, while focusing on vector representations of words to capture lexical semantics. [sent-18, score-0.826]
</p><p>14 Through a convolutional framework, our approach takes into account the distributional semantic similarities between words in a sentence as well as the structure of the parse  tree. [sent-19, score-0.27]
</p><p>15 We present a new graph kernel for NLP that extends to distributed word representations, and diverse word similarity measures. [sent-21, score-0.686]
</p><p>16 Our generic kernel shows state-of-the-art performance on three eclectic NLP tasks. [sent-25, score-0.376]
</p><p>17 oc d2s0 i1n3 N Aastusorcaila Ltiaon g fuoarg Ceo Pmrpoucetastsi on ga,l p Laignegsu 1is4t1ic1s–1416, 2  Related Work  Tree kernels in NLP Tree kernels have been extensively used to capture syntactic information about parse trees in tasks such as parsing (Collins and Duffy, 2002), NER (Wang et al. [sent-28, score-1.156]
</p><p>18 These kernels are based on the paradigm that parse trees are similar if they contain many common substructures, consisting ofnodes with identical labels (Vishwanathan and Smola, 2003; Collins and Duffy, 2002). [sent-32, score-0.656]
</p><p>19 Moschitti  (2006a) proposed a partial tree kernel that adds flexibility in matching tree substructures. [sent-33, score-0.695]
</p><p>20 (201 1) introduce a lexical semantic tree kernel that incorporates continuous similarity values between node labels, albeit with a different focus than ours and would not match words with different POS. [sent-35, score-0.786]
</p><p>21 This would miss the similarity of “feline friend” and “cat” in our examples, as it requires matching the adjective “feline” with “cat”, and verb “kissed” with “kiss”. [sent-36, score-0.165]
</p><p>22 Walk based kernels Kernels for structured data derive from the seminal Convolution Kernel formalism by Haussler (1999) for designing kernels for structured objects through local decompositions. [sent-37, score-1.005]
</p><p>23 Our proposed kernel for parse trees is most closely associated with the random walk-based kernels defined by Gartner et al. [sent-38, score-1.032]
</p><p>24 The walk-based graph kernels proposed by Gartner et al. [sent-41, score-0.526]
</p><p>25 (2003) count the common walks between two input graphs, using the adjacency matrix of the product graph. [sent-42, score-0.676]
</p><p>26 This work extends to graphs with a finite set of edge and node labels by appropriately modifying the adjacency matrix. [sent-43, score-0.353]
</p><p>27 Our kernel differs from these kernels in two significant ways: (i) Our method extends beyond label matching to con-  tinuous similarity metrics (this conforms with the very general formalism for graph kernels in Vishwanathan et al. [sent-44, score-1.63]
</p><p>28 (ii) Rather than using the adjacency matrix to model edge-strengths, we modify the product graph and the corresponding adjacency matrix to model node similarities. [sent-46, score-0.743]
</p><p>29 3  Vector Tree Kernels  In this section, we describe our kernel and an algorithm to compute it as a simple matrix multiplication formulation. [sent-47, score-0.512]
</p><p>30 1 Kernel description The similarity kernel K between two dependency trees can be defined as:  K(T1,T2) =  X  k(h1,h2)  h1⊆TX1,h2⊆T2 len(⊆hT1X X)=le⊆n(Th2) where the summation is over pairs of equal length walks h1 and h2 on the trees T1 and T2 respectively. [sent-49, score-1.24]
</p><p>31 The vector representation allows us several choices for the node kernel function κ. [sent-51, score-0.498]
</p><p>32 nonnegative values in [0, 1] (assuming word vector representations are normalized). [sent-62, score-0.085]
</p><p>33 Non-negativity is necessary, since we define the walk kernel to be the product of the individual kernels. [sent-63, score-0.588]
</p><p>34 As walk kernels are products of individual node-kernels, boundedness by 1 ensures that the kernel contribution does not grow arbitrarily for longer length walks. [sent-64, score-1.07]
</p><p>35 The kernel function K puts a high similarity  weight between parse trees if they contain common walks with semantically similar words in corresponding positions. [sent-65, score-1.055]
</p><p>36 Apart from the Gaussian kernel, the other two kernels are based on the dot-product of the word vector representations. [sent-66, score-0.484]
</p><p>37 We observe that the positive-linear kernel defined above is not a Mercer kernel, since the max operation makes it nonpositive semidefinite (PSD). [sent-67, score-0.376]
</p><p>38 However, this formulation has desirable properties, most significant being that all walks with one or more node-pair mismatches are strictly penalized and add no score to the tree-kernel. [sent-68, score-0.433]
</p><p>39 This is a more selective condition than the other two kernels, where mediocre walk combinations could also add small contributions to the score. [sent-69, score-0.141]
</p><p>40 The sigmoid kernel is also non-PSD, but is known to work well empirically (Boughorbel et al. [sent-70, score-0.412]
</p><p>41 We also observe while the summation in the kernel is over equal length walks, the formalism can allow comparisons over different length paths by including self-loops at nodes in the tree. [sent-72, score-0.66]
</p><p>42 With a notion of similarity between words that defines the local node kernels, we need computational machinery to enumerate all pairs of walks between two trees, and compute the summation  over products in the kernel K(T1, T2) efficiently. [sent-73, score-1.143]
</p><p>43 We now show a convenient way to compute this as a matrix geometric series. [sent-74, score-0.136]
</p><p>44 2 Matrix Formulation for Kernel Computation Walk-based kernels compute the number of common walks using the adjacency matrix of the product graph (Gartner et al. [sent-76, score-1.248]
</p><p>45 In our case, this computation is complicated by the fact that instead of counting common walks, we need to compute a product of node-similarities for each walk. [sent-78, score-0.117]
</p><p>46 Since we compute similarity scores over nodes, rather than edges, the product for a walk of length n involves n + 1factors. [sent-79, score-0.434]
</p><p>47 However, we can still compute the tree kernel K as a simple sum of matrix products. [sent-80, score-0.652]
</p><p>48 Given two trees T(V, E) and T0(V0, E0), we define a modified product graph G(Vp, Ep) with an additional ghost node u added to the vertex set. [sent-81, score-0.381]
</p><p>49 In our formulation, u now serves as a starting location for all random walks on G, and a k + 1length walk of G corresponds to a pair of k length walks on T and T0. [sent-83, score-0.893]
</p><p>50 We now define the weighted adjacency matrix W for G, which incorporates the local node kernels. [sent-84, score-0.375]
</p><p>51 1413  W(vi1,vj10),(vi2,vj20)=(0κ( :v(i(2v,iv1,jv2j10)0), :(v oi2th,vej2r0w))i ∈/seEp = κ(vi1, vj10)  Wu,(vi1,vj10) W(v,u) = 0 ∀ v ∈ Vp There is a straightforward bijective mapping from walks on G starting from u to pairs of walks on T and T0. [sent-85, score-0.702]
</p><p>52 Restricting ourselves to the case when the first node of a k + 1 length walk is u, the next k steps allow us to efficiently compute the products of the node similarities along the k nodes in the corresponding k length walks in T and T0. [sent-86, score-0.961]
</p><p>53 Given this ad-  jacency matrix for G, the sum of values of k length walk kernels is given by the uth row of the (k + 1)th exponent of the weighted adjacency matrix (denoted as Wk+1). [sent-87, score-1.016]
</p><p>54 This corresponds to k+ 1length walks on G starting from u and ending at any node. [sent-88, score-0.351]
</p><p>55 Specifically, Wu,(vi,vj0) corresponds to the sum of similarities of all common walks of length n in T and T0 that end in vi in T and vj0 in T0. [sent-89, score-0.449]
</p><p>56 The kernel K for walks upto length N can now be calculated as :  K(T,T0) =X|Vp|Su,i Xi  where S  =  W + W2 + . [sent-90, score-0.777]
</p><p>57 WN+1  We note that in out formulation, longer walks are naturally discounted, since they involve products of more factors (generally all less than unity). [sent-93, score-0.4]
</p><p>58 The above kernel provides a similarity measure between any two pairs of dependency parse-trees. [sent-94, score-0.539]
</p><p>59 Depending on whether we consider directional relations in the parse tree, the edge set Ep changes, while the procedure for the kernel computation remains the same. [sent-95, score-0.521]
</p><p>60 Finally, to avoid larger trees yield-  ing larger values for the kernel, we normalize the kernel by the number of edges in the product graph. [sent-96, score-0.589]
</p><p>61 We create dependency trees using the FANSE parser (Tratz and Hovy, 2011), and use distribution-based SENNA word embeddings by Collobert et al. [sent-98, score-0.203]
</p><p>62 These embeddings provide low-dimensional vector representations of words, while encoding distributional semantic characteristics. [sent-100, score-0.244]
</p><p>63 The task is to identify the polarity of a given sentence. [sent-104, score-0.065]
</p><p>64 “terribly entertaining” vs “terribly written”), so simple lexical approaches are not expected to work well here, while syntactic context could help disambiguation. [sent-107, score-0.046]
</p><p>65 Next, we try our approach on the MSR paraphrase corpus. [sent-108, score-0.099]
</p><p>66 Each instance consists of a pair of sentences, so the VTK cannot be directly used by a kernel machine for classification. [sent-110, score-0.376]
</p><p>67 Instead, we generate 16 kernel values based for each pair on different parameter settings of the kernel, and feed these as features to a linear SVM. [sent-111, score-0.376]
</p><p>68 We focus on target phrases by upweighting walks that pass through target nodes. [sent-116, score-0.351]
</p><p>69 This is done by simply multiplying the corresponding entries in the adjacency matrix by a constant factor. [sent-117, score-0.254]
</p><p>70 654a7193t4ase  On the polarity data set, Vector Tree Kernel  (VTK) significantly outperforms the state-of-the-art method by Carrillo de Albornoz et al. [sent-123, score-0.065]
</p><p>71 (2010), who use a hybrid model incorporating databases of affective lexicons, and also explicitly model the effect of negation and quantifiers (see Table 2). [sent-124, score-0.041]
</p><p>72 Lexical approaches using pairwise semantic similarity 1414 of SENNA embeddings (DSM), as well as Wordnet Affective Database-based (WNA) labels perform poorly (Carrillo de Albornoz et al. [sent-125, score-0.231]
</p><p>73 On the other hand, a syntactic tree kernel (SSTK) that ignores distributional semantic similarity between words, fails as expected. [sent-127, score-0.794]
</p><p>74 c76o5429rpus  On the MSR paraphrase corpus, VTK performs competitively against state-of-the-art-methods. [sent-133, score-0.126]
</p><p>75 We expected paraphrasing to be challenging to our method, since it can involve little syntactic overlap. [sent-134, score-0.046]
</p><p>76 However, data analysis reveals that the corpus generally contains sentence pairs with high syntactic similarity. [sent-135, score-0.046]
</p><p>77 Results for this task are encouraging since ours is a general approach, while other systems use multiple task-specific features like semantic role labels, active-passive voice conversion, and synonymy resolution. [sent-136, score-0.052]
</p><p>78 (2013), whose approach uses an conjunction of lexical and syntactic tree kernels (Moschitti, 2006b), and distributional vectors. [sent-144, score-0.694]
</p><p>79 VTK identified several templates of metaphor usage  such as “warm heart” and “cold shoulder”. [sent-145, score-0.137]
</p><p>80 We look towards approaches for automatedly mining such metaphor patterns from a corpus. [sent-146, score-0.137]
</p><p>81 6 Conclusion We present a general formalism for walk-based kernels to evaluate similarity of dependency trees. [sent-147, score-0.685]
</p><p>82 Our method generalizes tree kernels to take distributed representations of nodes as input, and capture both lexical semantics and syntactic structures of parse trees. [sent-148, score-0.899]
</p><p>83 Our approach has tunable parameters to look for larger or smaller syntactic constructs. [sent-149, score-0.046]
</p><p>84 The approach can generalize to any task involving structural and local similarity, and arbitrary node similarity measures. [sent-151, score-0.247]
</p><p>85 Conditionally positive definite kernels for svm based image recognition. [sent-154, score-0.454]
</p><p>86 A hybrid approach to emotional sentence polarity and intensity classification. [sent-158, score-0.092]
</p><p>87 In Proceedings of the Fourteenth Conference on Computational  Natural Language Learning, pages 153–161. [sent-159, score-0.033]
</p><p>88 New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. [sent-163, score-0.037]
</p><p>89 In Proceedings of the 40th annual meeting on association for computational linguistics, pages 263–270. [sent-164, score-0.033]
</p><p>90 Structured lexical similarity via convolution kernels on dependency trees. [sent-172, score-0.72]
</p><p>91 In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1034–1046. [sent-173, score-0.033]
</p><p>92 of the International Conference on Machine Learning, pages 107–1 14. [sent-179, score-0.033]
</p><p>93 Using machine translation evaluation techniques to determine sentence-level semantic equivalence. [sent-182, score-0.052]
</p><p>94 In Proceedings of the Annual Conference  on Computational Learning Theory, pages 129–143. [sent-187, score-0.033]
</p><p>95 In Proceedings of the Twentieth International Conference on Machine Learning, pages 321–328. [sent-200, score-0.033]
</p><p>96 Paraphrase recognition using machine learning to combine similarity measures. [sent-204, score-0.126]
</p><p>97 In Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, ACLstudent ’09, pages 27– 35, Stroudsburg, PA, USA. [sent-205, score-0.033]
</p><p>98 A study on convolution  kernels for shallow semantic parsing. [sent-213, score-0.609]
</p><p>99 Efficient convolution kernels for dependency and constituent syntactic trees. [sent-218, score-0.64]
</p><p>100 Exploiting constituent dependencies for tree kernel-based semantic relation extraction. [sent-231, score-0.192]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kernels', 0.454), ('kernel', 0.376), ('walks', 0.351), ('vtk', 0.217), ('adjacency', 0.164), ('walk', 0.141), ('tree', 0.14), ('metaphor', 0.137), ('moschitti', 0.132), ('similarity', 0.126), ('gartner', 0.124), ('vishwanathan', 0.124), ('trees', 0.113), ('convolution', 0.103), ('paraphrase', 0.099), ('albornoz', 0.093), ('carrillo', 0.093), ('node', 0.092), ('matrix', 0.09), ('parse', 0.089), ('alessandro', 0.088), ('duffy', 0.074), ('summation', 0.074), ('graph', 0.072), ('product', 0.071), ('formalism', 0.068), ('polarity', 0.065), ('boughorbel', 0.062), ('croce', 0.062), ('cumby', 0.062), ('feline', 0.062), ('senna', 0.062), ('terribly', 0.062), ('tratz', 0.062), ('msr', 0.062), ('vp', 0.055), ('representations', 0.055), ('dirk', 0.054), ('kashima', 0.054), ('shashank', 0.054), ('substructures', 0.054), ('subsumes', 0.054), ('distributional', 0.054), ('hovy', 0.053), ('embeddings', 0.053), ('formulation', 0.052), ('semantic', 0.052), ('ep', 0.051), ('length', 0.05), ('products', 0.049), ('similarities', 0.048), ('syntactic', 0.046), ('compute', 0.046), ('eduard', 0.044), ('nodes', 0.042), ('extends', 0.041), ('affective', 0.041), ('qian', 0.039), ('matching', 0.039), ('distributed', 0.039), ('discrete', 0.037), ('collins', 0.037), ('dependency', 0.037), ('sigmoid', 0.036), ('generalizes', 0.034), ('vertex', 0.033), ('pages', 0.033), ('diverse', 0.032), ('stroudsburg', 0.032), ('roberto', 0.032), ('nlp', 0.032), ('pang', 0.03), ('vector', 0.03), ('strictly', 0.03), ('collobert', 0.03), ('receive', 0.03), ('edge', 0.029), ('edges', 0.029), ('local', 0.029), ('cat', 0.028), ('gaussian', 0.028), ('graphs', 0.027), ('akihiro', 0.027), ('uth', 0.027), ('arr', 0.027), ('competitively', 0.027), ('constrains', 0.027), ('convolutional', 0.027), ('daniele', 0.027), ('denmark', 0.027), ('directional', 0.027), ('entertaining', 0.027), ('gerv', 0.027), ('hardness', 0.027), ('intensity', 0.027), ('kondor', 0.027), ('pighin', 0.027), ('prodromos', 0.027), ('psd', 0.027), ('risi', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="17-tfidf-1" href="./emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations.html">17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</a></p>
<p>Author: Shashank Srivastava ; Dirk Hovy ; Eduard Hovy</p><p>Abstract: In this paper, we propose a walk-based graph kernel that generalizes the notion of treekernels to continuous spaces. Our proposed approach subsumes a general framework for word-similarity, and in particular, provides a flexible way to incorporate distributed representations. Using vector representations, such an approach captures both distributional semantic similarities among words as well as the structural relations between them (encoded as the structure of the parse tree). We show an efficient formulation to compute this kernel using simple matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results.</p><p>2 0.18424928 <a title="17-tfidf-2" href="./emnlp-2013-Automatic_Feature_Engineering_for_Answer_Selection_and_Extraction.html">31 emnlp-2013-Automatic Feature Engineering for Answer Selection and Extraction</a></p>
<p>Author: Aliaksei Severyn ; Alessandro Moschitti</p><p>Abstract: This paper proposes a framework for automatically engineering features for two important tasks of question answering: answer sentence selection and answer extraction. We represent question and answer sentence pairs with linguistic structures enriched by semantic information, where the latter is produced by automatic classifiers, e.g., question classifier and Named Entity Recognizer. Tree kernels applied to such structures enable a simple way to generate highly discriminative structural features that combine syntactic and semantic information encoded in the input trees. We conduct experiments on a public benchmark from TREC to compare with previous systems for answer sentence selection and answer extraction. The results show that our models greatly improve on the state of the art, e.g., up to 22% on F1 (relative improvement) for answer extraction, while using no additional resources and no manual feature engineering.</p><p>3 0.15528256 <a title="17-tfidf-3" href="./emnlp-2013-A_temporal_model_of_text_periodicities_using_Gaussian_Processes.html">18 emnlp-2013-A temporal model of text periodicities using Gaussian Processes</a></p>
<p>Author: Daniel Preotiuc-Pietro ; Trevor Cohn</p><p>Abstract: Temporal variations of text are usually ignored in NLP applications. However, text use changes with time, which can affect many applications. In this paper we model periodic distributions of words over time. Focusing on hashtag frequency in Twitter, we first automatically identify the periodic patterns. We use this for regression in order to forecast the volume of a hashtag based on past data. We use Gaussian Processes, a state-ofthe-art bayesian non-parametric model, with a novel periodic kernel. We demonstrate this in a text classification setting, assigning the tweet hashtag based on the rest of its text. This method shows significant improvements over competitive baselines.</p><p>4 0.13299561 <a title="17-tfidf-4" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>Author: Yangfeng Ji ; Jacob Eisenstein</p><p>Abstract: Matrix and tensor factorization have been applied to a number of semantic relatedness tasks, including paraphrase identification. The key idea is that similarity in the latent space implies semantic relatedness. We describe three ways in which labeled data can improve the accuracy of these approaches on paraphrase classification. First, we design a new discriminative term-weighting metric called TF-KLD, which outperforms TF-IDF. Next, we show that using the latent representation from matrix factorization as features in a classification algorithm substantially improves accuracy. Finally, we combine latent features with fine-grained n-gram overlap features, yielding performance that is 3% more accurate than the prior state-of-the-art.</p><p>5 0.11723483 <a title="17-tfidf-5" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>Author: Xiaoning Zhu ; Zhongjun He ; Hua Wu ; Haifeng Wang ; Conghui Zhu ; Tiejun Zhao</p><p>Abstract: This paper proposes a novel approach that utilizes a machine learning method to improve pivot-based statistical machine translation (SMT). For language pairs with few bilingual data, a possible solution in pivot-based SMT using another language as a</p><p>6 0.082060866 <a title="17-tfidf-6" href="./emnlp-2013-A_Multi-Teraflop_Constituency_Parser_using_GPUs.html">10 emnlp-2013-A Multi-Teraflop Constituency Parser using GPUs</a></p>
<p>7 0.077178165 <a title="17-tfidf-7" href="./emnlp-2013-Centering_Similarity_Measures_to_Reduce_Hubs.html">44 emnlp-2013-Centering Similarity Measures to Reduce Hubs</a></p>
<p>8 0.07492166 <a title="17-tfidf-8" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>9 0.071795367 <a title="17-tfidf-9" href="./emnlp-2013-Tree_Kernel-based_Negation_and_Speculation_Scope_Detection_with_Structured_Syntactic_Parse_Features.html">188 emnlp-2013-Tree Kernel-based Negation and Speculation Scope Detection with Structured Syntactic Parse Features</a></p>
<p>10 0.071584947 <a title="17-tfidf-10" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>11 0.070844561 <a title="17-tfidf-11" href="./emnlp-2013-Sentiment_Analysis%3A_How_to_Derive_Prior_Polarities_from_SentiWordNet.html">170 emnlp-2013-Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet</a></p>
<p>12 0.067252062 <a title="17-tfidf-12" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>13 0.065622859 <a title="17-tfidf-13" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>14 0.06200783 <a title="17-tfidf-14" href="./emnlp-2013-Dependency_Language_Models_for_Sentence_Completion.html">58 emnlp-2013-Dependency Language Models for Sentence Completion</a></p>
<p>15 0.061554361 <a title="17-tfidf-15" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>16 0.06132295 <a title="17-tfidf-16" href="./emnlp-2013-The_Topology_of_Semantic_Knowledge.html">182 emnlp-2013-The Topology of Semantic Knowledge</a></p>
<p>17 0.060567584 <a title="17-tfidf-17" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>18 0.059120093 <a title="17-tfidf-18" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>19 0.058656141 <a title="17-tfidf-19" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>20 0.057863832 <a title="17-tfidf-20" href="./emnlp-2013-Dynamic_Feature_Selection_for_Dependency_Parsing.html">66 emnlp-2013-Dynamic Feature Selection for Dependency Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.195), (1, 0.009), (2, -0.05), (3, -0.007), (4, 0.032), (5, 0.105), (6, -0.004), (7, -0.081), (8, -0.064), (9, 0.022), (10, 0.056), (11, 0.101), (12, -0.121), (13, 0.055), (14, 0.034), (15, 0.022), (16, -0.104), (17, 0.232), (18, 0.014), (19, 0.028), (20, 0.021), (21, -0.188), (22, 0.073), (23, -0.018), (24, -0.232), (25, -0.072), (26, -0.215), (27, 0.07), (28, -0.074), (29, -0.033), (30, 0.167), (31, 0.133), (32, -0.14), (33, -0.095), (34, -0.011), (35, -0.13), (36, 0.202), (37, -0.147), (38, -0.003), (39, -0.043), (40, 0.028), (41, 0.135), (42, -0.063), (43, 0.021), (44, -0.172), (45, 0.034), (46, -0.001), (47, -0.083), (48, 0.006), (49, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96642107 <a title="17-lsi-1" href="./emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations.html">17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</a></p>
<p>Author: Shashank Srivastava ; Dirk Hovy ; Eduard Hovy</p><p>Abstract: In this paper, we propose a walk-based graph kernel that generalizes the notion of treekernels to continuous spaces. Our proposed approach subsumes a general framework for word-similarity, and in particular, provides a flexible way to incorporate distributed representations. Using vector representations, such an approach captures both distributional semantic similarities among words as well as the structural relations between them (encoded as the structure of the parse tree). We show an efficient formulation to compute this kernel using simple matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results.</p><p>2 0.74555933 <a title="17-lsi-2" href="./emnlp-2013-A_temporal_model_of_text_periodicities_using_Gaussian_Processes.html">18 emnlp-2013-A temporal model of text periodicities using Gaussian Processes</a></p>
<p>Author: Daniel Preotiuc-Pietro ; Trevor Cohn</p><p>Abstract: Temporal variations of text are usually ignored in NLP applications. However, text use changes with time, which can affect many applications. In this paper we model periodic distributions of words over time. Focusing on hashtag frequency in Twitter, we first automatically identify the periodic patterns. We use this for regression in order to forecast the volume of a hashtag based on past data. We use Gaussian Processes, a state-ofthe-art bayesian non-parametric model, with a novel periodic kernel. We demonstrate this in a text classification setting, assigning the tweet hashtag based on the rest of its text. This method shows significant improvements over competitive baselines.</p><p>3 0.58087748 <a title="17-lsi-3" href="./emnlp-2013-Tree_Kernel-based_Negation_and_Speculation_Scope_Detection_with_Structured_Syntactic_Parse_Features.html">188 emnlp-2013-Tree Kernel-based Negation and Speculation Scope Detection with Structured Syntactic Parse Features</a></p>
<p>Author: Bowei Zou ; Guodong Zhou ; Qiaoming Zhu</p><p>Abstract: Scope detection is a key task in information extraction. This paper proposes a new approach for tree kernel-based scope detection by using the structured syntactic parse information. In addition, we have explored the way of selecting compatible features for different part-of-speech cues. Experiments on the BioScope corpus show that both constituent and dependency structured syntactic parse features have the advantage in capturing the potential relationships between cues and their scopes. Compared with the state of the art scope detection systems, our system achieves substantial improvement.</p><p>4 0.48328096 <a title="17-lsi-4" href="./emnlp-2013-Automatic_Feature_Engineering_for_Answer_Selection_and_Extraction.html">31 emnlp-2013-Automatic Feature Engineering for Answer Selection and Extraction</a></p>
<p>Author: Aliaksei Severyn ; Alessandro Moschitti</p><p>Abstract: This paper proposes a framework for automatically engineering features for two important tasks of question answering: answer sentence selection and answer extraction. We represent question and answer sentence pairs with linguistic structures enriched by semantic information, where the latter is produced by automatic classifiers, e.g., question classifier and Named Entity Recognizer. Tree kernels applied to such structures enable a simple way to generate highly discriminative structural features that combine syntactic and semantic information encoded in the input trees. We conduct experiments on a public benchmark from TREC to compare with previous systems for answer sentence selection and answer extraction. The results show that our models greatly improve on the state of the art, e.g., up to 22% on F1 (relative improvement) for answer extraction, while using no additional resources and no manual feature engineering.</p><p>5 0.46471539 <a title="17-lsi-5" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>Author: Yangfeng Ji ; Jacob Eisenstein</p><p>Abstract: Matrix and tensor factorization have been applied to a number of semantic relatedness tasks, including paraphrase identification. The key idea is that similarity in the latent space implies semantic relatedness. We describe three ways in which labeled data can improve the accuracy of these approaches on paraphrase classification. First, we design a new discriminative term-weighting metric called TF-KLD, which outperforms TF-IDF. Next, we show that using the latent representation from matrix factorization as features in a classification algorithm substantially improves accuracy. Finally, we combine latent features with fine-grained n-gram overlap features, yielding performance that is 3% more accurate than the prior state-of-the-art.</p><p>6 0.4645305 <a title="17-lsi-6" href="./emnlp-2013-Centering_Similarity_Measures_to_Reduce_Hubs.html">44 emnlp-2013-Centering Similarity Measures to Reduce Hubs</a></p>
<p>7 0.42387554 <a title="17-lsi-7" href="./emnlp-2013-Sentiment_Analysis%3A_How_to_Derive_Prior_Polarities_from_SentiWordNet.html">170 emnlp-2013-Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet</a></p>
<p>8 0.36701313 <a title="17-lsi-8" href="./emnlp-2013-This_Text_Has_the_Scent_of_Starbucks%3A_A_Laplacian_Structured_Sparsity_Model_for_Computational_Branding_Analytics.html">184 emnlp-2013-This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</a></p>
<p>9 0.35653156 <a title="17-lsi-9" href="./emnlp-2013-Scaling_to_Large3_Data%3A_An_Efficient_and_Effective_Method_to_Compute_Distributional_Thesauri.html">165 emnlp-2013-Scaling to Large3 Data: An Efficient and Effective Method to Compute Distributional Thesauri</a></p>
<p>10 0.35318708 <a title="17-lsi-10" href="./emnlp-2013-Mining_New_Business_Opportunities%3A_Identifying_Trend_related_Products_by_Leveraging_Commercial_Intents_from_Microblogs.html">131 emnlp-2013-Mining New Business Opportunities: Identifying Trend related Products by Leveraging Commercial Intents from Microblogs</a></p>
<p>11 0.34720409 <a title="17-lsi-11" href="./emnlp-2013-A_Multi-Teraflop_Constituency_Parser_using_GPUs.html">10 emnlp-2013-A Multi-Teraflop Constituency Parser using GPUs</a></p>
<p>12 0.34691942 <a title="17-lsi-12" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>13 0.31822884 <a title="17-lsi-13" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>14 0.31547996 <a title="17-lsi-14" href="./emnlp-2013-Learning_to_Rank_Lexical_Substitutions.html">123 emnlp-2013-Learning to Rank Lexical Substitutions</a></p>
<p>15 0.30392006 <a title="17-lsi-15" href="./emnlp-2013-Using_Crowdsourcing_to_get_Representations_based_on_Regular_Expressions.html">196 emnlp-2013-Using Crowdsourcing to get Representations based on Regular Expressions</a></p>
<p>16 0.30325538 <a title="17-lsi-16" href="./emnlp-2013-Dependency_Language_Models_for_Sentence_Completion.html">58 emnlp-2013-Dependency Language Models for Sentence Completion</a></p>
<p>17 0.29489797 <a title="17-lsi-17" href="./emnlp-2013-Improving_Learning_and_Inference_in_a_Large_Knowledge-Base_using_Latent_Syntactic_Cues.html">102 emnlp-2013-Improving Learning and Inference in a Large Knowledge-Base using Latent Syntactic Cues</a></p>
<p>18 0.29119349 <a title="17-lsi-18" href="./emnlp-2013-Chinese_Zero_Pronoun_Resolution%3A_Some_Recent_Advances.html">45 emnlp-2013-Chinese Zero Pronoun Resolution: Some Recent Advances</a></p>
<p>19 0.28756687 <a title="17-lsi-19" href="./emnlp-2013-Dynamic_Feature_Selection_for_Dependency_Parsing.html">66 emnlp-2013-Dynamic Feature Selection for Dependency Parsing</a></p>
<p>20 0.27445012 <a title="17-lsi-20" href="./emnlp-2013-The_Topology_of_Semantic_Knowledge.html">182 emnlp-2013-The Topology of Semantic Knowledge</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.016), (9, 0.015), (18, 0.022), (22, 0.04), (30, 0.09), (50, 0.013), (51, 0.134), (66, 0.02), (71, 0.031), (75, 0.044), (77, 0.024), (90, 0.014), (96, 0.466)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9334814 <a title="17-lda-1" href="./emnlp-2013-Scaling_to_Large3_Data%3A_An_Efficient_and_Effective_Method_to_Compute_Distributional_Thesauri.html">165 emnlp-2013-Scaling to Large3 Data: An Efficient and Effective Method to Compute Distributional Thesauri</a></p>
<p>Author: Martin Riedl ; Chris Biemann</p><p>Abstract: We introduce a new highly scalable approach for computing Distributional Thesauri (DTs). By employing pruning techniques and a distributed framework, we make the computation for very large corpora feasible on comparably small computational resources. We demonstrate this by releasing a DT for the whole vocabulary of Google Books syntactic n-grams. Evaluating against lexical resources using two measures, we show that our approach produces higher quality DTs than previous approaches, and is thus preferable in terms of speed and quality for large corpora.</p><p>same-paper 2 0.81560934 <a title="17-lda-2" href="./emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations.html">17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</a></p>
<p>Author: Shashank Srivastava ; Dirk Hovy ; Eduard Hovy</p><p>Abstract: In this paper, we propose a walk-based graph kernel that generalizes the notion of treekernels to continuous spaces. Our proposed approach subsumes a general framework for word-similarity, and in particular, provides a flexible way to incorporate distributed representations. Using vector representations, such an approach captures both distributional semantic similarities among words as well as the structural relations between them (encoded as the structure of the parse tree). We show an efficient formulation to compute this kernel using simple matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results.</p><p>3 0.78827798 <a title="17-lda-3" href="./emnlp-2013-Joint_Coreference_Resolution_and_Named-Entity_Linking_with_Multi-Pass_Sieves.html">112 emnlp-2013-Joint Coreference Resolution and Named-Entity Linking with Multi-Pass Sieves</a></p>
<p>Author: Hannaneh Hajishirzi ; Leila Zilles ; Daniel S. Weld ; Luke Zettlemoyer</p><p>Abstract: Many errors in coreference resolution come from semantic mismatches due to inadequate world knowledge. Errors in named-entity linking (NEL), on the other hand, are often caused by superficial modeling of entity context. This paper demonstrates that these two tasks are complementary. We introduce NECO, a new model for named entity linking and coreference resolution, which solves both problems jointly, reducing the errors made on each. NECO extends the Stanford deterministic coreference system by automatically linking mentions to Wikipedia and introducing new NEL-informed mention-merging sieves. Linking improves mention-detection and enables new semantic attributes to be incorporated from Freebase, while coreference provides better context modeling by propagating named-entity links within mention clusters. Experiments show consistent improve- ments across a number of datasets and experimental conditions, including over 11% reduction in MUC coreference error and nearly 21% reduction in F1 NEL error on ACE 2004 newswire data.</p><p>4 0.75011337 <a title="17-lda-4" href="./emnlp-2013-A_Unified_Model_for_Topics%2C_Events_and_Users_on_Twitter.html">16 emnlp-2013-A Unified Model for Topics, Events and Users on Twitter</a></p>
<p>Author: Qiming Diao ; Jing Jiang</p><p>Abstract: With the rapid growth of social media, Twitter has become one of the most widely adopted platforms for people to post short and instant message. On the one hand, people tweets about their daily lives, and on the other hand, when major events happen, people also follow and tweet about them. Moreover, people’s posting behaviors on events are often closely tied to their personal interests. In this paper, we try to model topics, events and users on Twitter in a unified way. We propose a model which combines an LDA-like topic model and the Recurrent Chinese Restaurant Process to capture topics and events. We further propose a duration-based regularization component to find bursty events. We also propose to use event-topic affinity vectors to model the asso- . ciation between events and topics. Our experiments shows that our model can accurately identify meaningful events and the event-topic affinity vectors are effective for event recommendation and grouping events by topics.</p><p>5 0.74712276 <a title="17-lda-5" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>Author: Baichuan Li ; Jing Liu ; Chin-Yew Lin ; Irwin King ; Michael R. Lyu</p><p>Abstract: Social media like forums and microblogs have accumulated a huge amount of user generated content (UGC) containing human knowledge. Currently, most of UGC is listed as a whole or in pre-defined categories. This “list-based” approach is simple, but hinders users from browsing and learning knowledge of certain topics effectively. To address this problem, we propose a hierarchical entity-based approach for structuralizing UGC in social media. By using a large-scale entity repository, we design a three-step framework to organize UGC in a novel hierarchical structure called “cluster entity tree (CET)”. With Yahoo! Answers as a test case, we conduct experiments and the results show the effectiveness of our framework in constructing CET. We further evaluate the performance of CET on UGC organization in both user and system aspects. From a user aspect, our user study demonstrates that, with CET-based structure, users perform significantly better in knowledge learning than using traditional list-based approach. From a system aspect, CET substantially boosts the performance of two information retrieval models (i.e., vector space model and query likelihood language model).</p><p>6 0.53804111 <a title="17-lda-6" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>7 0.53031182 <a title="17-lda-7" href="./emnlp-2013-Microblog_Entity_Linking_by_Leveraging_Extra_Posts.html">130 emnlp-2013-Microblog Entity Linking by Leveraging Extra Posts</a></p>
<p>8 0.51324022 <a title="17-lda-8" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>9 0.5055787 <a title="17-lda-9" href="./emnlp-2013-Exploiting_Zero_Pronouns_to_Improve_Chinese_Coreference_Resolution.html">80 emnlp-2013-Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</a></p>
<p>10 0.49929166 <a title="17-lda-10" href="./emnlp-2013-A_Constrained_Latent_Variable_Model_for_Coreference_Resolution.html">1 emnlp-2013-A Constrained Latent Variable Model for Coreference Resolution</a></p>
<p>11 0.49552557 <a title="17-lda-11" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>12 0.49109966 <a title="17-lda-12" href="./emnlp-2013-Well-Argued_Recommendation%3A_Adaptive_Models_Based_on_Words_in_Recommender_Systems.html">200 emnlp-2013-Well-Argued Recommendation: Adaptive Models Based on Words in Recommender Systems</a></p>
<p>13 0.49058574 <a title="17-lda-13" href="./emnlp-2013-A_temporal_model_of_text_periodicities_using_Gaussian_Processes.html">18 emnlp-2013-A temporal model of text periodicities using Gaussian Processes</a></p>
<p>14 0.48668891 <a title="17-lda-14" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>15 0.46635795 <a title="17-lda-15" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>16 0.45937407 <a title="17-lda-16" href="./emnlp-2013-Easy_Victories_and_Uphill_Battles_in_Coreference_Resolution.html">67 emnlp-2013-Easy Victories and Uphill Battles in Coreference Resolution</a></p>
<p>17 0.45867145 <a title="17-lda-17" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>18 0.45248646 <a title="17-lda-18" href="./emnlp-2013-Gender_Inference_of_Twitter_Users_in_Non-English_Contexts.html">89 emnlp-2013-Gender Inference of Twitter Users in Non-English Contexts</a></p>
<p>19 0.45231593 <a title="17-lda-19" href="./emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model.html">75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</a></p>
<p>20 0.45026803 <a title="17-lda-20" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
