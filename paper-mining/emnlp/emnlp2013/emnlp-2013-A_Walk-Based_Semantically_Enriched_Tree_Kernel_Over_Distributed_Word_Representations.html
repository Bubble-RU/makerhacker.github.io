<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-17" href="#">emnlp2013-17</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</h1>
<br/><p>Source: <a title="emnlp-2013-17-pdf" href="http://aclweb.org/anthology//D/D13/D13-1144.pdf">pdf</a></p><p>Author: Shashank Srivastava ; Dirk Hovy ; Eduard Hovy</p><p>Abstract: In this paper, we propose a walk-based graph kernel that generalizes the notion of treekernels to continuous spaces. Our proposed approach subsumes a general framework for word-similarity, and in particular, provides a flexible way to incorporate distributed representations. Using vector representations, such an approach captures both distributional semantic similarities among words as well as the structural relations between them (encoded as the structure of the parse tree). We show an efficient formulation to compute this kernel using simple matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results.</p><p>Reference: <a title="emnlp-2013-17-reference" href="../emnlp2013_reference/emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kernel', 0.692), ('walk', 0.36), ('vtk', 0.224), ('metaph', 0.148), ('moschitt', 0.136), ('gartn', 0.128), ('vishwanath', 0.128), ('convolv', 0.127), ('paraphras', 0.107), ('nod', 0.103), ('albornoz', 0.096), ('carrillo', 0.096), ('matrix', 0.093), ('alessandro', 0.091), ('tre', 0.08), ('adjac', 0.078), ('duffy', 0.076), ('graph', 0.068), ('boughorbel', 0.064), ('croc', 0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="17-tfidf-1" href="./emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations.html">17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</a></p>
<p>Author: Shashank Srivastava ; Dirk Hovy ; Eduard Hovy</p><p>Abstract: In this paper, we propose a walk-based graph kernel that generalizes the notion of treekernels to continuous spaces. Our proposed approach subsumes a general framework for word-similarity, and in particular, provides a flexible way to incorporate distributed representations. Using vector representations, such an approach captures both distributional semantic similarities among words as well as the structural relations between them (encoded as the structure of the parse tree). We show an efficient formulation to compute this kernel using simple matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results.</p><p>2 0.23291132 <a title="17-tfidf-2" href="./emnlp-2013-A_temporal_model_of_text_periodicities_using_Gaussian_Processes.html">18 emnlp-2013-A temporal model of text periodicities using Gaussian Processes</a></p>
<p>Author: Daniel Preotiuc-Pietro ; Trevor Cohn</p><p>Abstract: Temporal variations of text are usually ignored in NLP applications. However, text use changes with time, which can affect many applications. In this paper we model periodic distributions of words over time. Focusing on hashtag frequency in Twitter, we first automatically identify the periodic patterns. We use this for regression in order to forecast the volume of a hashtag based on past data. We use Gaussian Processes, a state-ofthe-art bayesian non-parametric model, with a novel periodic kernel. We demonstrate this in a text classification setting, assigning the tweet hashtag based on the rest of its text. This method shows significant improvements over competitive baselines.</p><p>3 0.20912535 <a title="17-tfidf-3" href="./emnlp-2013-Automatic_Feature_Engineering_for_Answer_Selection_and_Extraction.html">31 emnlp-2013-Automatic Feature Engineering for Answer Selection and Extraction</a></p>
<p>Author: Aliaksei Severyn ; Alessandro Moschitti</p><p>Abstract: This paper proposes a framework for automatically engineering features for two important tasks of question answering: answer sentence selection and answer extraction. We represent question and answer sentence pairs with linguistic structures enriched by semantic information, where the latter is produced by automatic classifiers, e.g., question classifier and Named Entity Recognizer. Tree kernels applied to such structures enable a simple way to generate highly discriminative structural features that combine syntactic and semantic information encoded in the input trees. We conduct experiments on a public benchmark from TREC to compare with previous systems for answer sentence selection and answer extraction. The results show that our models greatly improve on the state of the art, e.g., up to 22% on F1 (relative improvement) for answer extraction, while using no additional resources and no manual feature engineering.</p><p>4 0.17997707 <a title="17-tfidf-4" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>Author: Xiaoning Zhu ; Zhongjun He ; Hua Wu ; Haifeng Wang ; Conghui Zhu ; Tiejun Zhao</p><p>Abstract: This paper proposes a novel approach that utilizes a machine learning method to improve pivot-based statistical machine translation (SMT). For language pairs with few bilingual data, a possible solution in pivot-based SMT using another language as a</p><p>5 0.12124078 <a title="17-tfidf-5" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>Author: Yangfeng Ji ; Jacob Eisenstein</p><p>Abstract: Matrix and tensor factorization have been applied to a number of semantic relatedness tasks, including paraphrase identification. The key idea is that similarity in the latent space implies semantic relatedness. We describe three ways in which labeled data can improve the accuracy of these approaches on paraphrase classification. First, we design a new discriminative term-weighting metric called TF-KLD, which outperforms TF-IDF. Next, we show that using the latent representation from matrix factorization as features in a classification algorithm substantially improves accuracy. Finally, we combine latent features with fine-grained n-gram overlap features, yielding performance that is 3% more accurate than the prior state-of-the-art.</p><p>6 0.11034603 <a title="17-tfidf-6" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>7 0.09180972 <a title="17-tfidf-7" href="./emnlp-2013-A_Multi-Teraflop_Constituency_Parser_using_GPUs.html">10 emnlp-2013-A Multi-Teraflop Constituency Parser using GPUs</a></p>
<p>8 0.086036064 <a title="17-tfidf-8" href="./emnlp-2013-Sentiment_Analysis%3A_How_to_Derive_Prior_Polarities_from_SentiWordNet.html">170 emnlp-2013-Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet</a></p>
<p>9 0.084363714 <a title="17-tfidf-9" href="./emnlp-2013-Tree_Kernel-based_Negation_and_Speculation_Scope_Detection_with_Structured_Syntactic_Parse_Features.html">188 emnlp-2013-Tree Kernel-based Negation and Speculation Scope Detection with Structured Syntactic Parse Features</a></p>
<p>10 0.081792749 <a title="17-tfidf-10" href="./emnlp-2013-Centering_Similarity_Measures_to_Reduce_Hubs.html">44 emnlp-2013-Centering Similarity Measures to Reduce Hubs</a></p>
<p>11 0.072734483 <a title="17-tfidf-11" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>12 0.072591044 <a title="17-tfidf-12" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>13 0.071662962 <a title="17-tfidf-13" href="./emnlp-2013-The_Topology_of_Semantic_Knowledge.html">182 emnlp-2013-The Topology of Semantic Knowledge</a></p>
<p>14 0.055417612 <a title="17-tfidf-14" href="./emnlp-2013-Dynamic_Feature_Selection_for_Dependency_Parsing.html">66 emnlp-2013-Dynamic Feature Selection for Dependency Parsing</a></p>
<p>15 0.054654837 <a title="17-tfidf-15" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>16 0.05424184 <a title="17-tfidf-16" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>17 0.053859767 <a title="17-tfidf-17" href="./emnlp-2013-Opinion_Mining_in_Newspaper_Articles_by_Entropy-Based_Word_Connections.html">144 emnlp-2013-Opinion Mining in Newspaper Articles by Entropy-Based Word Connections</a></p>
<p>18 0.053052377 <a title="17-tfidf-18" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>19 0.052073121 <a title="17-tfidf-19" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>20 0.051764682 <a title="17-tfidf-20" href="./emnlp-2013-Improving_Learning_and_Inference_in_a_Large_Knowledge-Base_using_Latent_Syntactic_Cues.html">102 emnlp-2013-Improving Learning and Inference in a Large Knowledge-Base using Latent Syntactic Cues</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.18), (1, -0.035), (2, 0.053), (3, 0.01), (4, -0.055), (5, -0.054), (6, -0.005), (7, 0.02), (8, 0.015), (9, -0.077), (10, -0.036), (11, -0.029), (12, -0.183), (13, -0.025), (14, 0.184), (15, -0.002), (16, 0.039), (17, 0.009), (18, -0.064), (19, -0.156), (20, 0.146), (21, -0.196), (22, -0.109), (23, -0.08), (24, -0.287), (25, 0.005), (26, 0.196), (27, -0.291), (28, 0.018), (29, 0.022), (30, 0.028), (31, -0.095), (32, 0.038), (33, -0.021), (34, -0.271), (35, 0.097), (36, 0.043), (37, 0.073), (38, -0.13), (39, 0.023), (40, 0.04), (41, 0.145), (42, -0.016), (43, -0.022), (44, 0.106), (45, -0.028), (46, 0.012), (47, 0.031), (48, 0.054), (49, -0.002)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9614073 <a title="17-lsi-1" href="./emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations.html">17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</a></p>
<p>Author: Shashank Srivastava ; Dirk Hovy ; Eduard Hovy</p><p>Abstract: In this paper, we propose a walk-based graph kernel that generalizes the notion of treekernels to continuous spaces. Our proposed approach subsumes a general framework for word-similarity, and in particular, provides a flexible way to incorporate distributed representations. Using vector representations, such an approach captures both distributional semantic similarities among words as well as the structural relations between them (encoded as the structure of the parse tree). We show an efficient formulation to compute this kernel using simple matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results.</p><p>2 0.76367128 <a title="17-lsi-2" href="./emnlp-2013-A_temporal_model_of_text_periodicities_using_Gaussian_Processes.html">18 emnlp-2013-A temporal model of text periodicities using Gaussian Processes</a></p>
<p>Author: Daniel Preotiuc-Pietro ; Trevor Cohn</p><p>Abstract: Temporal variations of text are usually ignored in NLP applications. However, text use changes with time, which can affect many applications. In this paper we model periodic distributions of words over time. Focusing on hashtag frequency in Twitter, we first automatically identify the periodic patterns. We use this for regression in order to forecast the volume of a hashtag based on past data. We use Gaussian Processes, a state-ofthe-art bayesian non-parametric model, with a novel periodic kernel. We demonstrate this in a text classification setting, assigning the tweet hashtag based on the rest of its text. This method shows significant improvements over competitive baselines.</p><p>3 0.56298393 <a title="17-lsi-3" href="./emnlp-2013-Tree_Kernel-based_Negation_and_Speculation_Scope_Detection_with_Structured_Syntactic_Parse_Features.html">188 emnlp-2013-Tree Kernel-based Negation and Speculation Scope Detection with Structured Syntactic Parse Features</a></p>
<p>Author: Bowei Zou ; Guodong Zhou ; Qiaoming Zhu</p><p>Abstract: Scope detection is a key task in information extraction. This paper proposes a new approach for tree kernel-based scope detection by using the structured syntactic parse information. In addition, we have explored the way of selecting compatible features for different part-of-speech cues. Experiments on the BioScope corpus show that both constituent and dependency structured syntactic parse features have the advantage in capturing the potential relationships between cues and their scopes. Compared with the state of the art scope detection systems, our system achieves substantial improvement.</p><p>4 0.48392642 <a title="17-lsi-4" href="./emnlp-2013-Centering_Similarity_Measures_to_Reduce_Hubs.html">44 emnlp-2013-Centering Similarity Measures to Reduce Hubs</a></p>
<p>Author: Ikumi Suzuki ; Kazuo Hara ; Masashi Shimbo ; Marco Saerens ; Kenji Fukumizu</p><p>Abstract: The performance of nearest neighbor methods is degraded by the presence of hubs, i.e., objects in the dataset that are similar to many other objects. In this paper, we show that the classical method of centering, the transformation that shifts the origin of the space to the data centroid, provides an effective way to reduce hubs. We show analytically why hubs emerge and why they are suppressed by centering, under a simple probabilistic model of data. To further reduce hubs, we also move the origin more aggressively towards hubs, through weighted centering. Our experimental results show that (weighted) centering is effective for natural language data; it improves the performance of the k-nearest neighbor classi- fiers considerably in word sense disambiguation and document classification tasks.</p><p>5 0.42764667 <a title="17-lsi-5" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>Author: Xiaoning Zhu ; Zhongjun He ; Hua Wu ; Haifeng Wang ; Conghui Zhu ; Tiejun Zhao</p><p>Abstract: This paper proposes a novel approach that utilizes a machine learning method to improve pivot-based statistical machine translation (SMT). For language pairs with few bilingual data, a possible solution in pivot-based SMT using another language as a</p><p>6 0.38342497 <a title="17-lsi-6" href="./emnlp-2013-Sentiment_Analysis%3A_How_to_Derive_Prior_Polarities_from_SentiWordNet.html">170 emnlp-2013-Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet</a></p>
<p>7 0.38293567 <a title="17-lsi-7" href="./emnlp-2013-Automatic_Feature_Engineering_for_Answer_Selection_and_Extraction.html">31 emnlp-2013-Automatic Feature Engineering for Answer Selection and Extraction</a></p>
<p>8 0.35429507 <a title="17-lsi-8" href="./emnlp-2013-This_Text_Has_the_Scent_of_Starbucks%3A_A_Laplacian_Structured_Sparsity_Model_for_Computational_Branding_Analytics.html">184 emnlp-2013-This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</a></p>
<p>9 0.35014656 <a title="17-lsi-9" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>10 0.34680989 <a title="17-lsi-10" href="./emnlp-2013-A_Multi-Teraflop_Constituency_Parser_using_GPUs.html">10 emnlp-2013-A Multi-Teraflop Constituency Parser using GPUs</a></p>
<p>11 0.29415616 <a title="17-lsi-11" href="./emnlp-2013-The_Topology_of_Semantic_Knowledge.html">182 emnlp-2013-The Topology of Semantic Knowledge</a></p>
<p>12 0.29328397 <a title="17-lsi-12" href="./emnlp-2013-Using_Crowdsourcing_to_get_Representations_based_on_Regular_Expressions.html">196 emnlp-2013-Using Crowdsourcing to get Representations based on Regular Expressions</a></p>
<p>13 0.28752372 <a title="17-lsi-13" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>14 0.252033 <a title="17-lsi-14" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>15 0.24894935 <a title="17-lsi-15" href="./emnlp-2013-Improving_Learning_and_Inference_in_a_Large_Knowledge-Base_using_Latent_Syntactic_Cues.html">102 emnlp-2013-Improving Learning and Inference in a Large Knowledge-Base using Latent Syntactic Cues</a></p>
<p>16 0.24681905 <a title="17-lsi-16" href="./emnlp-2013-Learning_to_Rank_Lexical_Substitutions.html">123 emnlp-2013-Learning to Rank Lexical Substitutions</a></p>
<p>17 0.24555595 <a title="17-lsi-17" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>18 0.23757508 <a title="17-lsi-18" href="./emnlp-2013-Recurrent_Continuous_Translation_Models.html">156 emnlp-2013-Recurrent Continuous Translation Models</a></p>
<p>19 0.2182986 <a title="17-lsi-19" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>20 0.21313156 <a title="17-lsi-20" href="./emnlp-2013-Dynamic_Feature_Selection_for_Dependency_Parsing.html">66 emnlp-2013-Dynamic Feature Selection for Dependency Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.016), (25, 0.102), (29, 0.042), (37, 0.418), (45, 0.013), (47, 0.056), (51, 0.03), (65, 0.11), (69, 0.016), (73, 0.083), (79, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.69061816 <a title="17-lda-1" href="./emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations.html">17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</a></p>
<p>Author: Shashank Srivastava ; Dirk Hovy ; Eduard Hovy</p><p>Abstract: In this paper, we propose a walk-based graph kernel that generalizes the notion of treekernels to continuous spaces. Our proposed approach subsumes a general framework for word-similarity, and in particular, provides a flexible way to incorporate distributed representations. Using vector representations, such an approach captures both distributional semantic similarities among words as well as the structural relations between them (encoded as the structure of the parse tree). We show an efficient formulation to compute this kernel using simple matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results.</p><p>2 0.61448115 <a title="17-lda-2" href="./emnlp-2013-Semantic_Parsing_on_Freebase_from_Question-Answer_Pairs.html">166 emnlp-2013-Semantic Parsing on Freebase from Question-Answer Pairs</a></p>
<p>Author: Jonathan Berant ; Andrew Chou ; Roy Frostig ; Percy Liang</p><p>Abstract: In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset ofCai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline.</p><p>3 0.59851116 <a title="17-lda-3" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>Author: Shi Feng ; Le Zhang ; Binyang Li ; Daling Wang ; Ge Yu ; Kam-Fai Wong</p><p>Abstract: Extensive experiments have validated the effectiveness of the corpus-based method for classifying the word’s sentiment polarity. However, no work is done for comparing different corpora in the polarity classification task. Nowadays, Twitter has aggregated huge amount of data that are full of people’s sentiments. In this paper, we empirically evaluate the performance of different corpora in sentiment similarity measurement, which is the fundamental task for word polarity classification. Experiment results show that the Twitter data can achieve a much better performance than the Google, Web1T and Wikipedia based methods.</p><p>4 0.4533948 <a title="17-lda-4" href="./emnlp-2013-Scaling_Semantic_Parsers_with_On-the-Fly_Ontology_Matching.html">164 emnlp-2013-Scaling Semantic Parsers with On-the-Fly Ontology Matching</a></p>
<p>Author: Tom Kwiatkowski ; Eunsol Choi ; Yoav Artzi ; Luke Zettlemoyer</p><p>Abstract: We consider the challenge of learning semantic parsers that scale to large, open-domain problems, such as question answering with Freebase. In such settings, the sentences cover a wide variety of topics and include many phrases whose meaning is difficult to represent in a fixed target ontology. For example, even simple phrases such as ‘daughter’ and ‘number of people living in’ cannot be directly represented in Freebase, whose ontology instead encodes facts about gender, parenthood, and population. In this paper, we introduce a new semantic parsing approach that learns to resolve such ontological mismatches. The parser is learned from question-answer pairs, uses a probabilistic CCG to build linguistically motivated logicalform meaning representations, and includes an ontology matching model that adapts the output logical forms for each target ontology. Experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus.</p><p>5 0.41286039 <a title="17-lda-5" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>Author: Jason Weston ; Antoine Bordes ; Oksana Yakhnenko ; Nicolas Usunier</p><p>Abstract: This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over methods that rely on text features alone.</p><p>6 0.40257043 <a title="17-lda-6" href="./emnlp-2013-Inducing_Document_Plans_for_Concept-to-Text_Generation.html">106 emnlp-2013-Inducing Document Plans for Concept-to-Text Generation</a></p>
<p>7 0.39727777 <a title="17-lda-7" href="./emnlp-2013-A_Unified_Model_for_Topics%2C_Events_and_Users_on_Twitter.html">16 emnlp-2013-A Unified Model for Topics, Events and Users on Twitter</a></p>
<p>8 0.3964974 <a title="17-lda-8" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>9 0.39574879 <a title="17-lda-9" href="./emnlp-2013-The_Effects_of_Syntactic_Features_in_Automatic_Prediction_of_Morphology.html">181 emnlp-2013-The Effects of Syntactic Features in Automatic Prediction of Morphology</a></p>
<p>10 0.39563462 <a title="17-lda-10" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>11 0.39431202 <a title="17-lda-11" href="./emnlp-2013-Chinese_Zero_Pronoun_Resolution%3A_Some_Recent_Advances.html">45 emnlp-2013-Chinese Zero Pronoun Resolution: Some Recent Advances</a></p>
<p>12 0.39296088 <a title="17-lda-12" href="./emnlp-2013-Gender_Inference_of_Twitter_Users_in_Non-English_Contexts.html">89 emnlp-2013-Gender Inference of Twitter Users in Non-English Contexts</a></p>
<p>13 0.39261866 <a title="17-lda-13" href="./emnlp-2013-Automatic_Feature_Engineering_for_Answer_Selection_and_Extraction.html">31 emnlp-2013-Automatic Feature Engineering for Answer Selection and Extraction</a></p>
<p>14 0.39238912 <a title="17-lda-14" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>15 0.39146712 <a title="17-lda-15" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>16 0.38928431 <a title="17-lda-16" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>17 0.38905203 <a title="17-lda-17" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>18 0.38851422 <a title="17-lda-18" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>19 0.38846749 <a title="17-lda-19" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>20 0.38690868 <a title="17-lda-20" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
