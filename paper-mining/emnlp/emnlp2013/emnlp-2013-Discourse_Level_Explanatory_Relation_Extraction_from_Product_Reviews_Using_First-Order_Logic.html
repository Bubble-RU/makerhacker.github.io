<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>63 emnlp-2013-Discourse Level Explanatory Relation Extraction from Product Reviews Using First-Order Logic</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-63" href="#">emnlp2013-63</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>63 emnlp-2013-Discourse Level Explanatory Relation Extraction from Product Reviews Using First-Order Logic</h1>
<br/><p>Source: <a title="emnlp-2013-63-pdf" href="http://aclweb.org/anthology//D/D13/D13-1097.pdf">pdf</a></p><p>Author: Qi Zhang ; Jin Qian ; Huan Chen ; Jihua Kang ; Xuanjing Huang</p><p>Abstract: Explanatory sentences are employed to clarify reasons, details, facts, and so on. High quality online product reviews usually include not only positive or negative opinions, but also a variety of explanations of why these opinions were given. These explanations can help readers get easily comprehensible information of the discussed products and aspects. Moreover, explanatory relations can also benefit sentiment analysis applications. In this work, we focus on the task of identifying subjective text segments and extracting their corresponding explanations from product reviews in discourse level. We propose a novel joint extraction method using firstorder logic to model rich linguistic features and long distance constraints. Experimental results demonstrate the effectiveness of the proposed method.</p><p>Reference: <a title="emnlp-2013-63-reference" href="../emnlp2013_reference/emnlp-2013-Discourse_Level_Explanatory_Relation_Extraction_from_Product_Reviews_Using_First-Order_Logic_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 High quality online product reviews usually include not only positive or negative opinions, but also a variety of explanations of why these opinions were given. [sent-4, score-0.238]
</p><p>2 Moreover, explanatory relations can also benefit sentiment analysis applications. [sent-6, score-0.613]
</p><p>3 In this work, we focus on the task of identifying subjective text segments and extracting their corresponding explanations from product reviews in discourse level. [sent-7, score-0.561]
</p><p>4 We propose a novel joint extraction method using firstorder logic to model rich linguistic features and long distance constraints. [sent-8, score-0.199]
</p><p>5 1 Introduction Through analyzing product reviews with high helpfulness ratings assigned by readers, we find that a large number of explanatory sentences are used to clarify the causes, details, or consequences of opinions. [sent-10, score-0.657]
</p><p>6 1% opinion expressions are further explained by other sentences. [sent-12, score-0.194]
</p><p>7 am The first sentence of example 1 expresses negative opinion about refresh rate, which is one of the most important attributes of TV. [sent-25, score-0.224]
</p><p>8 In example 2, detail descriptions are used to explain the reflection problem of the camera screen. [sent-27, score-0.243]
</p><p>9 Although, explanations provide valuable information, to the best of our knowledge, there is no existing work that deals with explanation extraction for opinions in discourse level. [sent-28, score-0.327]
</p><p>10 We think that if explanatory relations can be automatically identified from reviews, sentiment analysis applications may benefit from it. [sent-29, score-0.613]
</p><p>11 Existing opinion mining approaches mainly focus on subjective text. [sent-30, score-0.347]
</p><p>12 Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al. [sent-40, score-0.652]
</p><p>13 In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. [sent-47, score-0.561]
</p><p>14 We propose to use Markov Logic Networks (MLN) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. [sent-48, score-0.766]
</p><p>15 It can easily incorporate rich linguistic features and global constraints by designing various logic formulas, which can also be viewed as templates or rules. [sent-53, score-0.194]
</p><p>16 Logic formulas are combined in a probabilistic framework to model soft constraints. [sent-54, score-0.344]
</p><p>17 To evaluate the proposed method, we crawled a large number of product reviews and constructed a labeled corpus through Amazon’s Mechanical Turk. [sent-56, score-0.192]
</p><p>18 2  Problem Statement  Motivated by the argument structure of discourse relations used in Penn Discourse Treebank (Rashmi Prasad and Webber, 2008), in this work, we adopt the clause unit-based definition. [sent-64, score-0.408]
</p><p>19 It means that clauses are treated as the basic units of opinion expressions and explanations. [sent-65, score-0.341]
</p><p>20 =Di {rcected graph 947 G = (V, E) is used to represent the subjectivity of clauses and explanatory relationships between them. [sent-71, score-0.725]
</p><p>21 Directed edges describe the explanatory relationships between them, of which the heads are explanatory clauses. [sent-73, score-0.908]
</p><p>22 If clause ca describes a set of facts which clarify the causes, context, situation, or consequences of another clause cb, ca −→ cb is used to indicate that clause ca explains cb. [sent-74, score-0.776]
</p><p>23 In the graph, vertices whose color are black stand for subjective clauses. [sent-78, score-0.184]
</p><p>24 Edges describe the explanatory relationships between them, of which the heads are explanatory clauses. [sent-80, score-0.908]
</p><p>25 Although the explanatory relation extraction task has been studied from the view of linguistic and discourse representation by existing works (Carston, 1993; Lascarides and Asher, 1993), the automatic extraction task is still an open question. [sent-81, score-0.754]
</p><p>26 However, the second  sentence itself also expresses opinion on various opinion targets. [sent-85, score-0.326]
</p><p>27 In other words, both subjective and objective sentences can be used as explanations. [sent-86, score-0.184]
</p><p>28 Many sentences, which express explanatory relation, do not contain any connectives (e. [sent-141, score-0.454]
</p><p>29 (2009) generalized four challenges (include ambiguity, inference, context, and world knowledge) to automated implicit discourse relation recognition. [sent-145, score-0.214]
</p><p>30 From the these examples, we can observe that extracting explanatory relations from product reviews is a challenging task. [sent-147, score-0.672]
</p><p>31 3  The Proposed Approach  In this section, we present our method for jointly classifying the subjectivity of text segments and extracting explanatory relations. [sent-149, score-0.615]
</p><p>32 Then, we introduce the clause extraction method based on the definition described in the Section 2. [sent-151, score-0.265]
</p><p>33 Finally, we present the first-order logic formulas including local formulas and global formulas used for joint modeling in this work. [sent-152, score-1.268]
</p><p>34 1 Markov Logic Networks A MLN consists of a set of logic formulas that describe first-order knowledge base. [sent-154, score-0.5]
</p><p>35 Different with first-order logic, these hard logic formulas are softened and can be violated with some penalty (the weight of 948 formula) in MLN. [sent-156, score-0.5]
</p><p>36 Thes)e} weighted formulas define a probability distribution over sets of possible worlds. [sent-158, score-0.344]
</p><p>37 cpiend tofadepn cy Describing the attributes of the clause ci  Describing  the  attributes of relations  between clause ci and clause cj  scelanutesneDceiDstiasntcane(cie,(ji, mj,)n)D is t a n c e b e t w e e n c la u s e c ia n d c la u s e c jin scelanutesnecse is i ms. [sent-179, score-0.78]
</p><p>38 2 Clause Identification We model the clause boundary identification prob-  lem through sequence labeling and use Conditional Random Fields (CRFs) to identify clause boundaries. [sent-183, score-0.444]
</p><p>39 3 Formulas In this work, we propose to use predicate subj(i) to indicate that the ith clause is subjective and explain(i, j) to indicate that the jth clause explains the ith clause. [sent-190, score-0.692]
</p><p>40 Both subj and explain are hidden predicates and jointly modeled by MLN. [sent-191, score-0.462]
</p><p>41 We use local and global formulas to model rich linguistic features and long distance constraints. [sent-192, score-0.424]
</p><p>42 1 Local Formulas The local formulas relate one or more observed predicates to exactly one hidden predicate. [sent-195, score-0.526]
</p><p>43 In this work, we define a list of observed predicates to describe the properties of individual clauses and attributes of relations between two clauses. [sent-196, score-0.372]
</p><p>44 The observed predicates and descriptions are shown in  949 Table 1. [sent-197, score-0.17]
</p><p>45 The observed predicates can be categorized into 3 groups: words, clauses, and relations between clauses. [sent-198, score-0.196]
</p><p>46 Table 2 lists the local formulas used in this work. [sent-202, score-0.386]
</p><p>47 The “+” notation in the formulas indicates that each constant of the logic variable should be weighted separately. [sent-203, score-0.5]
</p><p>48 For subjective classification and relation extraction, we construct a number of formulas respectively. [sent-204, score-0.656]
</p><p>49 For subjective classification, the first two formulas model the influence of lexical and POS tag. [sent-205, score-0.528]
</p><p>50 Since words which provide positive or negative opinions may provide important information for subjectivity classification, we combine predicates of words and lexicon of opinion words. [sent-207, score-0.429]
</p><p>51 950  also construct  local formulas  based on predicates  extracted from dependency trees of clauses. [sent-211, score-0.5]
</p><p>52 For explanatory relation extraction, we firstly use formulas to capture lexical and syntactic information from both of the clauses. [sent-212, score-0.882]
</p><p>53 Although some connective words are ambiguous in terms of relation they mark (Pitler and Nenkova, 2009), they may still be useful for explanation relation extraction. [sent-215, score-0.243]
</p><p>54 Hence, we construct local formulas with relation lexicon and other predicates. [sent-216, score-0.47]
</p><p>55 2 Global Formulas Local formulas are designed to deal with subjective classification of a single clause or relation determination of a single pair of clauses. [sent-219, score-0.878]
</p><p>56 Global formulas are designed to handle global constraints of multiple clauses. [sent-220, score-0.382]
</p><p>57 From the definition of explanatory relation and corpus statistics, we observe the following properties: Property 1: One clause can only serve as the explanation of one subjective clause. [sent-221, score-1.022]
</p><p>58 Property 2: Explanatory clauses occur immediately before or after their corresponding subjective clauses. [sent-222, score-0.331]
</p><p>59 Property 3: The positions of explanatory clauses are consecutive. [sent-223, score-0.601]
</p><p>60 In other words, if clause ck and ck+2 explain clause cj, the clause ck+1 would also be explanatory clause of cj. [sent-224, score-1.554]
</p><p>61 For property 1, we use the following global formula to make sure that one clause only explains at most one another clause. [sent-225, score-0.352]
</p><p>62 explain(i, j) ⇒ ¬explain(k, j) ∀k  = i,j  (1)  Based on the property 2 and 3, explanatory clauses are consecutive and immediately before or after their corresponding subjective clauses. [sent-226, score-0.812]
</p><p>63 694 clauses are labeled subjective and 478 clauses explain other ones. [sent-233, score-0.665]
</p><p>64 1% opinion expressions are explained by their corresponding explanatory sentences. [sent-235, score-0.648]
</p><p>65 The two projects we deployed on Amazon’s Mechanical Turk are: 1) Determine whether a clause contains opinion expressions or not; 2) Determine whether a clause clarifies causes, reasons, or consequences of another given clause. [sent-236, score-0.674]
</p><p>66 And, those clause pairs in the second project were not considered as explanation relations. [sent-250, score-0.295]
</p><p>67 For resolving Markov logic network, we use the  toolkit thebeast 7. [sent-1289, score-0.188]
</p><p>68 For parameter learning, the weights for formulas are updated by an online learning algorithm with MIRA update rule. [sent-1291, score-0.344]
</p><p>69 Evaluation metrics used for subjectivity classification and relation extraction throughout the experiments include: Precision, Recall, and F1-score. [sent-1294, score-0.295]
</p><p>70 The feature sets are similar as the local formulas for MLN including words, POS tags, dependency relations, and opinion lexicon. [sent-1301, score-0.549]
</p><p>71 PDTB-Rel: For discourse relation extraction, we use “PDTB-Styled Ersend r-etloa-tEionnd eDxitrscacotuiornse, Parser” (Lin et al. [sent-1306, score-0.214]
</p><p>72 Since it is a general discourse relations identification algorithms, “Cause”, “Pragmatic Cause”, “Instantiation”, and “Restatement” relation types are treated as explanatory relation in this work. [sent-1308, score-0.808]
</p><p>73 3 Results Table 3 shows the comparisons of the proposed method with the state-of-the-art systems on subjectivity classification and explanatory relation extraction. [sent-1312, score-0.741]
</p><p>74 From the results, we can observe that recursive autoencoders based subjectivity classification method achieves slightly better performance than our method and conditional random fields based method. [sent-1313, score-0.199]
</p><p>75 We think that the main reason is that only lexical features are used in MLN models for subjective classification. [sent-1315, score-0.184]
</p><p>76 For all the subjective clauses identified by RAE, PDTB-Rel and SVM-Rel are used to extract corresponding explanatory clauses. [sent-1327, score-0.785]
</p><p>77 We subtract one observed predicate and its corresponding local formulas from the original sets at a time. [sent-1335, score-0.438]
</p><p>78 The results of both subjectivity classification and relation extraction are shown in Table 4. [sent-1336, score-0.295]
</p><p>79 The first row shows the result of the MLN based method with all observed predicates and local formulas. [sent-1337, score-0.182]
</p><p>80 From the results we can observe that the observed predicates which are not used in the local formulas for subjectivity classification also impact the performance of subjectivity classification. [sent-1338, score-0.849]
</p><p>81 We think that the performance is effected by the global formulas, which combine the procedure of subjectivity classification 953  and relation extraction. [sent-1339, score-0.29]
</p><p>82 Without word predicate, the F1 score of subjectivity classification and relation extraction significantly drop to 51. [sent-1341, score-0.295]
</p><p>83 For subjectivity classification, subjective lexicon contributes a lot for recall. [sent-1344, score-0.308]
</p><p>84 For relation extraction, the impacts of clause distance and sentence distance are not as significant as the other features. [sent-1345, score-0.306]
</p><p>85 5  Related Work  Our work relates to three research areas: sentiment analysis/opinion mining, discourse-level relation extraction, and Markov logic networks. [sent-1346, score-0.343]
</p><p>86 Along with the increasing requirement, subjectivity classification has recently received considerable attention from both the industry and researchers. [sent-1347, score-0.168]
</p><p>87 Another research direction tries to convert the sentiment analysis task into entity identification and relation extraction. [sent-1356, score-0.187]
</p><p>88 (2007) assumed that evaluative opinions could be structured as a frame which is composed by opinion holder, subject, aspect, and evaluation. [sent-1359, score-0.191]
</p><p>89 Since the cross sentences relations are considered in this work, the discourse-level relation extraction methods are also related to ours. [sent-1367, score-0.183]
</p><p>90 (2009) studied discourse segments containing opinion expressions from the perspective of linguistics. [sent-1372, score-0.361]
</p><p>91 In this paper, we used MLN framework to capture another different discourse-level relation, which exists between subject clauses or subject clause and objective clause. [sent-1382, score-0.369]
</p><p>92 Richardson and Domingos (2006) proposed Markov Logic Networks, which combines firstorder logic and probabilistic graphical models. [sent-1383, score-0.191]
</p><p>93 (2009) proposed to use Markov logic to incorporate both local features and global constraints that hold between temporal relations. [sent-1393, score-0.271]
</p><p>94 6  Conclusions  In this paper, we propose to use Markov logic networks to identify subjective text segments and extract their corresponding explanations in discourse level. [sent-1396, score-0.586]
</p><p>95 We use MLN to jointly model subjectivity classification and explanatory relation extraction. [sent-1397, score-0.706]
</p><p>96 Rich linguistic features and global constraints are incorporated by various logic formulas and global formulas. [sent-1398, score-0.576]
</p><p>97 To evaluate the proposed method, we collected a large number of product reviews and constructed a labeled corpus through Amazon’s Me-  chanical Turk. [sent-1399, score-0.166]
</p><p>98 0: An enhanced lexical resource for sentiment analysis and opinion mining. [sent-1415, score-0.266]
</p><p>99 Recognizing implicit discourse relations in the penn discourse treebank. [sent-1493, score-0.342]
</p><p>100 Walk and learn: a two-stage approach for opinion words and opinion targets coextraction. [sent-1605, score-0.326]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('explanatory', 0.454), ('formulas', 0.344), ('clause', 0.222), ('mln', 0.216), ('firstword', 0.194), ('explain', 0.187), ('subjective', 0.184), ('dep', 0.167), ('opinion', 0.163), ('subjlexicon', 0.162), ('subj', 0.161), ('logic', 0.156), ('clauses', 0.147), ('relationlexicon', 0.146), ('discourse', 0.13), ('sentencedistance', 0.13), ('subjectivity', 0.124), ('predicates', 0.114), ('clausedistance', 0.113), ('sentiment', 0.103), ('domingos', 0.092), ('reviews', 0.086), ('relation', 0.084), ('singla', 0.081), ('explanations', 0.079), ('pos', 0.062), ('markov', 0.061), ('relations', 0.056), ('andrzejewski', 0.048), ('explanation', 0.047), ('shanghai', 0.045), ('product', 0.045), ('richardson', 0.044), ('classification', 0.044), ('extraction', 0.043), ('yoshikawa', 0.043), ('local', 0.042), ('prasad', 0.041), ('polarity', 0.041), ('pang', 0.04), ('kobayashi', 0.039), ('rae', 0.039), ('explains', 0.038), ('global', 0.038), ('segments', 0.037), ('kang', 0.037), ('consequences', 0.036), ('clarify', 0.036), ('proposed', 0.035), ('rashmi', 0.034), ('asher', 0.034), ('janyce', 0.033), ('calzolari', 0.032), ('dragut', 0.032), ('refresh', 0.032), ('screenshots', 0.032), ('tapias', 0.032), ('thebeast', 0.032), ('zirn', 0.032), ('observe', 0.031), ('liu', 0.031), ('expressions', 0.031), ('descriptions', 0.03), ('proceedings', 0.029), ('attributes', 0.029), ('stroudsburg', 0.028), ('bente', 0.028), ('choukri', 0.028), ('khalid', 0.028), ('maegaard', 0.028), ('mariani', 0.028), ('nicoletta', 0.028), ('piperidis', 0.028), ('stelios', 0.028), ('connective', 0.028), ('takamura', 0.028), ('opinions', 0.028), ('poon', 0.028), ('amazon', 0.027), ('formula', 0.027), ('property', 0.027), ('korea', 0.026), ('predicate', 0.026), ('crawled', 0.026), ('holder', 0.026), ('narayanan', 0.026), ('tjong', 0.026), ('municipal', 0.026), ('reflection', 0.026), ('statistic', 0.026), ('project', 0.026), ('observed', 0.026), ('penn', 0.026), ('ck', 0.025), ('bing', 0.025), ('mechanical', 0.024), ('dasgupta', 0.024), ('lascarides', 0.024), ('ziheng', 0.024), ('island', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="63-tfidf-1" href="./emnlp-2013-Discourse_Level_Explanatory_Relation_Extraction_from_Product_Reviews_Using_First-Order_Logic.html">63 emnlp-2013-Discourse Level Explanatory Relation Extraction from Product Reviews Using First-Order Logic</a></p>
<p>Author: Qi Zhang ; Jin Qian ; Huan Chen ; Jihua Kang ; Xuanjing Huang</p><p>Abstract: Explanatory sentences are employed to clarify reasons, details, facts, and so on. High quality online product reviews usually include not only positive or negative opinions, but also a variety of explanations of why these opinions were given. These explanations can help readers get easily comprehensible information of the discussed products and aspects. Moreover, explanatory relations can also benefit sentiment analysis applications. In this work, we focus on the task of identifying subjective text segments and extracting their corresponding explanations from product reviews in discourse level. We propose a novel joint extraction method using firstorder logic to model rich linguistic features and long distance constraints. Experimental results demonstrate the effectiveness of the proposed method.</p><p>2 0.16814387 <a title="63-tfidf-2" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>Author: Svitlana Volkova ; Theresa Wilson ; David Yarowsky</p><p>Abstract: Theresa Wilson Human Language Technology Center of Excellence Johns Hopkins University Baltimore, MD t aw@ j hu .edu differences may Different demographics, e.g., gender or age, can demonstrate substantial variation in their language use, particularly in informal contexts such as social media. In this paper we focus on learning gender differences in the use of subjective language in English, Spanish, and Russian Twitter data, and explore cross-cultural differences in emoticon and hashtag use for male and female users. We show that gender differences in subjective language can effectively be used to improve sentiment analysis, and in particular, polarity classification for Spanish and Russian. Our results show statistically significant relative F-measure improvement over the gender-independent baseline 1.5% and 1% for Russian, 2% and 0.5% for Spanish, and 2.5% and 5% for English for polarity and subjectivity classification.</p><p>3 0.15379506 <a title="63-tfidf-3" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>Author: Xinjie Zhou ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style. In this paper, we study the problem of extracting opinion targets of Chinese microblog messages. Such fine-grained word-level task has not been well investigated in microblogs yet. We propose an unsupervised label propagation algorithm to address the problem. The opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets. Topics in microblogs are identified by hashtags or using clustering algorithms. Experimental results on Chinese microblogs show the effectiveness of our framework and algorithms.</p><p>4 0.1437086 <a title="63-tfidf-4" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>Author: Gary Patterson ; Andrew Kehler</p><p>Abstract: We present a classification model that predicts the presence or omission of a lexical connective between two clauses, based upon linguistic features of the clauses and the type of discourse relation holding between them. The model is trained on a set of high frequency relations extracted from the Penn Discourse Treebank and achieves an accuracy of 86.6%. Analysis of the results reveals that the most informative features relate to the discourse dependencies between sequences of coherence relations in the text. We also present results of an experiment that provides insight into the nature and difficulty of the task.</p><p>5 0.14238316 <a title="63-tfidf-5" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>Author: Margaret Mitchell ; Jacqui Aguilar ; Theresa Wilson ; Benjamin Van Durme</p><p>Abstract: We propose a novel approach to sentiment analysis for a low resource setting. The intuition behind this work is that sentiment expressed towards an entity, targeted sentiment, may be viewed as a span of sentiment expressed across the entity. This representation allows us to model sentiment detection as a sequence tagging problem, jointly discovering people and organizations along with whether there is sentiment directed towards them. We compare performance in both Spanish and English on microblog data, using only a sentiment lexicon as an external resource. By leveraging linguisticallyinformed features within conditional random fields (CRFs) trained to minimize empirical risk, our best models in Spanish significantly outperform a strong baseline, and reach around 90% accuracy on the combined task of named entity recognition and sentiment prediction. Our models in English, trained on a much smaller dataset, are not yet statistically significant against their baselines.</p><p>6 0.12117261 <a title="63-tfidf-6" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>7 0.11862718 <a title="63-tfidf-7" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>8 0.11380216 <a title="63-tfidf-8" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>9 0.11132129 <a title="63-tfidf-9" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>10 0.10948432 <a title="63-tfidf-10" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>11 0.1050544 <a title="63-tfidf-11" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>12 0.091407314 <a title="63-tfidf-12" href="./emnlp-2013-Opinion_Mining_in_Newspaper_Articles_by_Entropy-Based_Word_Connections.html">144 emnlp-2013-Opinion Mining in Newspaper Articles by Entropy-Based Word Connections</a></p>
<p>13 0.086135834 <a title="63-tfidf-13" href="./emnlp-2013-Unsupervised_Induction_of_Cross-Lingual_Semantic_Relations.html">193 emnlp-2013-Unsupervised Induction of Cross-Lingual Semantic Relations</a></p>
<p>14 0.074736878 <a title="63-tfidf-14" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>15 0.069437496 <a title="63-tfidf-15" href="./emnlp-2013-Semantic_Parsing_on_Freebase_from_Question-Answer_Pairs.html">166 emnlp-2013-Semantic Parsing on Freebase from Question-Answer Pairs</a></p>
<p>16 0.069153026 <a title="63-tfidf-16" href="./emnlp-2013-Learning_Topics_and_Positions_from_Debatepedia.html">121 emnlp-2013-Learning Topics and Positions from Debatepedia</a></p>
<p>17 0.065934353 <a title="63-tfidf-17" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>18 0.064218014 <a title="63-tfidf-18" href="./emnlp-2013-Exploiting_Zero_Pronouns_to_Improve_Chinese_Coreference_Resolution.html">80 emnlp-2013-Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</a></p>
<p>19 0.061155431 <a title="63-tfidf-19" href="./emnlp-2013-Sentiment_Analysis%3A_How_to_Derive_Prior_Polarities_from_SentiWordNet.html">170 emnlp-2013-Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet</a></p>
<p>20 0.058904432 <a title="63-tfidf-20" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.205), (1, 0.122), (2, -0.11), (3, -0.067), (4, 0.076), (5, -0.027), (6, -0.063), (7, -0.079), (8, 0.082), (9, 0.141), (10, 0.05), (11, -0.133), (12, 0.048), (13, 0.056), (14, -0.005), (15, 0.195), (16, -0.061), (17, 0.017), (18, 0.126), (19, 0.103), (20, 0.094), (21, 0.071), (22, -0.214), (23, -0.081), (24, -0.033), (25, 0.114), (26, -0.071), (27, 0.036), (28, 0.019), (29, -0.011), (30, -0.09), (31, -0.005), (32, 0.053), (33, -0.005), (34, -0.077), (35, -0.064), (36, -0.069), (37, 0.004), (38, 0.027), (39, -0.02), (40, 0.129), (41, 0.004), (42, 0.083), (43, 0.035), (44, 0.118), (45, -0.09), (46, -0.013), (47, 0.114), (48, -0.048), (49, 0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94083071 <a title="63-lsi-1" href="./emnlp-2013-Discourse_Level_Explanatory_Relation_Extraction_from_Product_Reviews_Using_First-Order_Logic.html">63 emnlp-2013-Discourse Level Explanatory Relation Extraction from Product Reviews Using First-Order Logic</a></p>
<p>Author: Qi Zhang ; Jin Qian ; Huan Chen ; Jihua Kang ; Xuanjing Huang</p><p>Abstract: Explanatory sentences are employed to clarify reasons, details, facts, and so on. High quality online product reviews usually include not only positive or negative opinions, but also a variety of explanations of why these opinions were given. These explanations can help readers get easily comprehensible information of the discussed products and aspects. Moreover, explanatory relations can also benefit sentiment analysis applications. In this work, we focus on the task of identifying subjective text segments and extracting their corresponding explanations from product reviews in discourse level. We propose a novel joint extraction method using firstorder logic to model rich linguistic features and long distance constraints. Experimental results demonstrate the effectiveness of the proposed method.</p><p>2 0.64839584 <a title="63-lsi-2" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>Author: Gary Patterson ; Andrew Kehler</p><p>Abstract: We present a classification model that predicts the presence or omission of a lexical connective between two clauses, based upon linguistic features of the clauses and the type of discourse relation holding between them. The model is trained on a set of high frequency relations extracted from the Penn Discourse Treebank and achieves an accuracy of 86.6%. Analysis of the results reveals that the most informative features relate to the discourse dependencies between sequences of coherence relations in the text. We also present results of an experiment that provides insight into the nature and difficulty of the task.</p><p>3 0.56390989 <a title="63-lsi-3" href="./emnlp-2013-Opinion_Mining_in_Newspaper_Articles_by_Entropy-Based_Word_Connections.html">144 emnlp-2013-Opinion Mining in Newspaper Articles by Entropy-Based Word Connections</a></p>
<p>Author: Thomas Scholz ; Stefan Conrad</p><p>Abstract: A very valuable piece of information in newspaper articles is the tonality of extracted statements. For the analysis of tonality of newspaper articles either a big human effort is needed, when it is carried out by media analysts, or an automated approach which has to be as accurate as possible for a Media Response Analysis (MRA). To this end, we will compare several state-of-the-art approaches for Opinion Mining in newspaper articles in this paper. Furthermore, we will introduce a new technique to extract entropy-based word connections which identifies the word combinations which create a tonality. In the evaluation, we use two different corpora consisting of news articles, by which we show that the new approach achieves better results than the four state-of-the-art methods.</p><p>4 0.50438267 <a title="63-lsi-4" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>Author: Xinjie Zhou ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style. In this paper, we study the problem of extracting opinion targets of Chinese microblog messages. Such fine-grained word-level task has not been well investigated in microblogs yet. We propose an unsupervised label propagation algorithm to address the problem. The opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets. Topics in microblogs are identified by hashtags or using clustering algorithms. Experimental results on Chinese microblogs show the effectiveness of our framework and algorithms.</p><p>5 0.48872986 <a title="63-lsi-5" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>Author: Wiltrud Kessler ; Jonas Kuhn</p><p>Abstract: This short paper presents a pilot study investigating the training of a standard Semantic Role Labeling (SRL) system on product reviews for the new task of detecting comparisons. An (opinionated) comparison consists of a comparative “predicate” and up to three “arguments”: the entity evaluated positively, the entity evaluated negatively, and the aspect under which the comparison is made. In user-generated product reviews, the “predicate” and “arguments” are expressed in highly heterogeneous ways; but since the elements are textually annotated in existing datasets, SRL is technically applicable. We address the interesting question how well training an outof-the-box SRL model works for English data. We observe that even without any feature engineering or other major adaptions to our task, the system outperforms a reasonable heuristic baseline in all steps (predicate identification, argument identification and argument classification) and in three different datasets.</p><p>6 0.48760393 <a title="63-lsi-6" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>7 0.47232175 <a title="63-lsi-7" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>8 0.45410931 <a title="63-lsi-8" href="./emnlp-2013-Effectiveness_and_Efficiency_of_Open_Relation_Extraction.html">68 emnlp-2013-Effectiveness and Efficiency of Open Relation Extraction</a></p>
<p>9 0.452912 <a title="63-lsi-9" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>10 0.43286356 <a title="63-lsi-10" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>11 0.41492715 <a title="63-lsi-11" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>12 0.39714512 <a title="63-lsi-12" href="./emnlp-2013-Sentiment_Analysis%3A_How_to_Derive_Prior_Polarities_from_SentiWordNet.html">170 emnlp-2013-Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet</a></p>
<p>13 0.39562336 <a title="63-lsi-13" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>14 0.39301875 <a title="63-lsi-14" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>15 0.37818974 <a title="63-lsi-15" href="./emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM.html">99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</a></p>
<p>16 0.36038172 <a title="63-lsi-16" href="./emnlp-2013-Unsupervised_Induction_of_Cross-Lingual_Semantic_Relations.html">193 emnlp-2013-Unsupervised Induction of Cross-Lingual Semantic Relations</a></p>
<p>17 0.34883571 <a title="63-lsi-17" href="./emnlp-2013-Single-Document_Summarization_as_a_Tree_Knapsack_Problem.html">174 emnlp-2013-Single-Document Summarization as a Tree Knapsack Problem</a></p>
<p>18 0.33641964 <a title="63-lsi-18" href="./emnlp-2013-Identifying_Manipulated_Offerings_on_Review_Portals.html">94 emnlp-2013-Identifying Manipulated Offerings on Review Portals</a></p>
<p>19 0.32848525 <a title="63-lsi-19" href="./emnlp-2013-Learning_Topics_and_Positions_from_Debatepedia.html">121 emnlp-2013-Learning Topics and Positions from Debatepedia</a></p>
<p>20 0.31778064 <a title="63-lsi-20" href="./emnlp-2013-Where_Not_to_Eat%3F_Improving_Public_Policy_by_Predicting_Hygiene_Inspections_Using_Online_Reviews.html">202 emnlp-2013-Where Not to Eat? Improving Public Policy by Predicting Hygiene Inspections Using Online Reviews</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.039), (18, 0.025), (22, 0.048), (30, 0.05), (51, 0.13), (66, 0.058), (71, 0.44), (75, 0.043), (77, 0.015), (90, 0.013), (96, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9619813 <a title="63-lda-1" href="./emnlp-2013-Rule-Based_Information_Extraction_is_Dead%21_Long_Live_Rule-Based_Information_Extraction_Systems%21.html">161 emnlp-2013-Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!</a></p>
<p>Author: Laura Chiticariu ; Yunyao Li ; Frederick R. Reiss</p><p>Abstract: The rise of “Big Data” analytics over unstructured text has led to renewed interest in information extraction (IE). We surveyed the landscape ofIE technologies and identified a major disconnect between industry and academia: while rule-based IE dominates the commercial world, it is widely regarded as dead-end technology by the academia. We believe the disconnect stems from the way in which the two communities measure the benefits and costs of IE, as well as academia’s perception that rulebased IE is devoid of research challenges. We make a case for the importance of rule-based IE to industry practitioners. We then lay out a research agenda in advancing the state-of-theart in rule-based IE systems which we believe has the potential to bridge the gap between academic research and industry practice.</p><p>same-paper 2 0.8264755 <a title="63-lda-2" href="./emnlp-2013-Discourse_Level_Explanatory_Relation_Extraction_from_Product_Reviews_Using_First-Order_Logic.html">63 emnlp-2013-Discourse Level Explanatory Relation Extraction from Product Reviews Using First-Order Logic</a></p>
<p>Author: Qi Zhang ; Jin Qian ; Huan Chen ; Jihua Kang ; Xuanjing Huang</p><p>Abstract: Explanatory sentences are employed to clarify reasons, details, facts, and so on. High quality online product reviews usually include not only positive or negative opinions, but also a variety of explanations of why these opinions were given. These explanations can help readers get easily comprehensible information of the discussed products and aspects. Moreover, explanatory relations can also benefit sentiment analysis applications. In this work, we focus on the task of identifying subjective text segments and extracting their corresponding explanations from product reviews in discourse level. We propose a novel joint extraction method using firstorder logic to model rich linguistic features and long distance constraints. Experimental results demonstrate the effectiveness of the proposed method.</p><p>3 0.77415377 <a title="63-lda-3" href="./emnlp-2013-Paraphrasing_4_Microblog_Normalization.html">151 emnlp-2013-Paraphrasing 4 Microblog Normalization</a></p>
<p>Author: Wang Ling ; Chris Dyer ; Alan W Black ; Isabel Trancoso</p><p>Abstract: Compared to the edited genres that have played a central role in NLP research, microblog texts use a more informal register with nonstandard lexical items, abbreviations, and free orthographic variation. When confronted with such input, conventional text analysis tools often perform poorly. Normalization replacing orthographically or lexically idiosyncratic forms with more standard variants can improve performance. We propose a method for learning normalization rules from machine translations of a parallel corpus of microblog messages. To validate the utility of our approach, we evaluate extrinsically, showing that normalizing English tweets and then translating improves translation quality (compared to translating unnormalized text) using three standard web translation services as well as a phrase-based translation system trained — — on parallel microblog data.</p><p>4 0.7204634 <a title="63-lda-4" href="./emnlp-2013-Learning_to_Rank_Lexical_Substitutions.html">123 emnlp-2013-Learning to Rank Lexical Substitutions</a></p>
<p>Author: Gyorgy Szarvas ; Robert Busa-Fekete ; Eyke Hullermeier</p><p>Abstract: The problem to replace a word with a synonym that fits well in its sentential context is known as the lexical substitution task. In this paper, we tackle this task as a supervised ranking problem. Given a dataset of target words, their sentential contexts and the potential substitutions for the target words, the goal is to train a model that accurately ranks the candidate substitutions based on their contextual fitness. As a key contribution, we customize and evaluate several learning-to-rank models to the lexical substitution task, including classification-based and regression-based approaches. On two datasets widely used for lexical substitution, our best models signifi- cantly advance the state-of-the-art.</p><p>5 0.51891047 <a title="63-lda-5" href="./emnlp-2013-Mining_New_Business_Opportunities%3A_Identifying_Trend_related_Products_by_Leveraging_Commercial_Intents_from_Microblogs.html">131 emnlp-2013-Mining New Business Opportunities: Identifying Trend related Products by Leveraging Commercial Intents from Microblogs</a></p>
<p>Author: Jinpeng Wang ; Wayne Xin Zhao ; Haitian Wei ; Hongfei Yan ; Xiaoming Li</p><p>Abstract: Hot trends are likely to bring new business opportunities. For example, “Air Pollution” might lead to a significant increase of the sales of related products, e.g., mouth mask. For ecommerce companies, it is very important to make rapid and correct response to these hot trends in order to improve product sales. In this paper, we take the initiative to study the task of how to identify trend related products. The major novelty of our work is that we automatically learn commercial intents revealed from microblogs. We carefully construct a data collection for this task and present quite a few insightful findings. In order to solve this problem, we further propose a graph based method, which jointly models relevance and associativity. We perform extensive experiments and the results showed that our methods are very effective.</p><p>6 0.50297129 <a title="63-lda-6" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>7 0.47911996 <a title="63-lda-7" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>8 0.47052875 <a title="63-lda-8" href="./emnlp-2013-Opinion_Mining_in_Newspaper_Articles_by_Entropy-Based_Word_Connections.html">144 emnlp-2013-Opinion Mining in Newspaper Articles by Entropy-Based Word Connections</a></p>
<p>9 0.46462864 <a title="63-lda-9" href="./emnlp-2013-Where_Not_to_Eat%3F_Improving_Public_Policy_by_Predicting_Hygiene_Inspections_Using_Online_Reviews.html">202 emnlp-2013-Where Not to Eat? Improving Public Policy by Predicting Hygiene Inspections Using Online Reviews</a></p>
<p>10 0.45294625 <a title="63-lda-10" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>11 0.44906166 <a title="63-lda-11" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>12 0.4481371 <a title="63-lda-12" href="./emnlp-2013-Using_Crowdsourcing_to_get_Representations_based_on_Regular_Expressions.html">196 emnlp-2013-Using Crowdsourcing to get Representations based on Regular Expressions</a></p>
<p>13 0.44636357 <a title="63-lda-13" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>14 0.43977839 <a title="63-lda-14" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>15 0.43957716 <a title="63-lda-15" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>16 0.43624404 <a title="63-lda-16" href="./emnlp-2013-Identifying_Manipulated_Offerings_on_Review_Portals.html">94 emnlp-2013-Identifying Manipulated Offerings on Review Portals</a></p>
<p>17 0.43566686 <a title="63-lda-17" href="./emnlp-2013-A_temporal_model_of_text_periodicities_using_Gaussian_Processes.html">18 emnlp-2013-A temporal model of text periodicities using Gaussian Processes</a></p>
<p>18 0.43521491 <a title="63-lda-18" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>19 0.43447155 <a title="63-lda-19" href="./emnlp-2013-A_Log-Linear_Model_for_Unsupervised_Text_Normalization.html">9 emnlp-2013-A Log-Linear Model for Unsupervised Text Normalization</a></p>
<p>20 0.4342944 <a title="63-lda-20" href="./emnlp-2013-Sentiment_Analysis%3A_How_to_Derive_Prior_Polarities_from_SentiWordNet.html">170 emnlp-2013-Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
