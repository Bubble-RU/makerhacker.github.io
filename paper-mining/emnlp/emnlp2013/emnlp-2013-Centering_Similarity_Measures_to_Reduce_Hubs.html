<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>44 emnlp-2013-Centering Similarity Measures to Reduce Hubs</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-44" href="#">emnlp2013-44</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>44 emnlp-2013-Centering Similarity Measures to Reduce Hubs</h1>
<br/><p>Source: <a title="emnlp-2013-44-pdf" href="http://aclweb.org/anthology//D/D13/D13-1058.pdf">pdf</a></p><p>Author: Ikumi Suzuki ; Kazuo Hara ; Masashi Shimbo ; Marco Saerens ; Kenji Fukumizu</p><p>Abstract: The performance of nearest neighbor methods is degraded by the presence of hubs, i.e., objects in the dataset that are similar to many other objects. In this paper, we show that the classical method of centering, the transformation that shifts the origin of the space to the data centroid, provides an effective way to reduce hubs. We show analytically why hubs emerge and why they are suppressed by centering, under a simple probabilistic model of data. To further reduce hubs, we also move the origin more aggressively towards hubs, through weighted centering. Our experimental results show that (weighted) centering is effective for natural language data; it improves the performance of the k-nearest neighbor classi- fiers considerably in word sense disambiguation and document classification tasks.</p><p>Reference: <a title="emnlp-2013-44-reference" href="../emnlp2013_reference/emnlp-2013-Centering_Similarity_Measures_to_Reduce_Hubs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 , objects in the dataset that are similar to many other objects. [sent-11, score-0.368]
</p><p>2 We show analytically why hubs emerge and why they are suppressed by centering, under a simple probabilistic model of data. [sent-13, score-0.417]
</p><p>3 Our experimental results show that (weighted) centering is effective for natural language data; it improves the performance of the k-nearest neighbor classi-  fiers considerably in word sense disambiguation and document classification tasks. [sent-15, score-0.69]
</p><p>4 jp  unknown class label of a test object is predicted by a majority vote of the classes of its k most similar objects in the labeled training set. [sent-24, score-0.532]
</p><p>5 , 2010b) have shown that if the feature space is high-dimensional, some objects in the dataset emerge as hubs; i. [sent-27, score-0.413]
</p><p>6 , these objects frequently appear in the k nearest neighbors of other objects. [sent-29, score-0.549]
</p><p>7 The emergence of hubs may deteriorate the performance of kNN classification and nearest neighbor search in general: • If hub objects exist in the training set, they have a strong cjehcatnsc eex to bine t a ek tNraNin ionfg many test oavbejects. [sent-30, score-1.121]
</p><p>8 •  In information retrieval, nearest neighbor Isenar cinhf ofirmndast objects einv tlh,e database that are most relevant, or similar, to user-provided queries. [sent-32, score-0.519]
</p><p>9 The distance between data objects is not changed by centering, but their inner product and cosine are affected; see Section 3 for detail. [sent-42, score-0.589]
</p><p>10 Specifically, we propose to measure the similarity of objects by the inner product (not distance or cosine) in the centered feature space. [sent-44, score-0.723]
</p><p>11 Our approach is motivated by the observation that the objects similar to the data centroid tend to be-  come hubs (Radovanovi c´ et al. [sent-45, score-0.877]
</p><p>12 This observation suggests that the number of hubs may be reduced if we can define a similarity measure that makes all objects in a dataset equally similar to the centroid (Suzuki et al. [sent-47, score-1.019]
</p><p>13 In Section 4, we analyze why hubs emerge under a simple probabilistic model of data, and also give an account of why they are suppressed by centering. [sent-50, score-0.417]
</p><p>14 Using both synthetic and real datasets, we show that objects similar to the centroid also emerge as hubs in multi-cluster data (Section 5), so the application of centering is wider than expected. [sent-51, score-1.477]
</p><p>15 To further reduce hubs, we also propose to move the origin of the space more aggressively towards hubs, through weighted centering (Section 6). [sent-52, score-0.792]
</p><p>16 In Section 7, we show that centering and weighted centering are effective for natural language data. [sent-53, score-1.112]
</p><p>17 For instance, centering forms a preprocessing step in principal component analysis and Fisher linear discriminant analysis. [sent-56, score-0.519]
</p><p>18 In NLP, however, centering is seldom used; the use of cosine and inner product similarities is quite common, but they are nearly always used uncentered. [sent-57, score-0.789]
</p><p>19 (2012) proposed the Mutual Proximity transformation that rescales  distance measures to decrease hubs in a dataset. [sent-66, score-0.397]
</p><p>20 In Section 7, we evaluate centering, Mutual Proximity, and Laplacian kernels in NLP tasks, and demonstrate that centering is equally or even more effective. [sent-70, score-0.565]
</p><p>21 Section 4 presents a theoretical justification for using centering to reduce hubs, but this kind of analysis is missing for the Laplacian kernels. [sent-71, score-0.544]
</p><p>22 Throughout this paper, we use the inner product hxi, xji as a measure pofa similarity ebe thtewe inenne xi raonddu xj. [sent-76, score-0.38]
</p><p>23 Centering is a transformation in which the origin of the feature space is shifted to the data centroid  ¯x =n1Xi=n1xi,  (1)  and object x is mapped to the centered feature vector  xcent  =x  −  x¯. [sent-82, score-0.668]
</p><p>24 (2)  The similarity between two objects x and x0 is now measured by hxcent, x0centi = hx − x¯, x0 − x¯i. [sent-83, score-0.412]
</p><p>25 After centering, the inner product between any object and the data centroid (which is a zero vector because x¯ cent = ¯ x − ¯ x = 0) is uniformly 0; in other words, all objects i −n tx¯ h =e d 0a)ta isset u nhiafoverm an equal soitmhei-r larity to the centroid. [sent-84, score-0.877]
</p><p>26 According to the observation  ×  that the objects similar to the centroid become hubs (Radovanovi c´ et al. [sent-85, score-0.877]
</p><p>27 , 2010a), we can expect hubs to be reduced after centering. [sent-86, score-0.372]
</p><p>28 Intuitively, centering reduces hubs because it makes the length of the feature vector short for (hub) objects x that lie close to the data centroid ¯x ; see Eq. [sent-87, score-1.396]
</p><p>29 And since we measure object similarity by inner product, shorter vectors tend to produce smaller similarity scores. [sent-89, score-0.518]
</p><p>30 Hence objects close to the data centroid become less similar to other objects after centering, and no longer be hubs. [sent-90, score-0.824]
</p><p>31 In Section 4, we analyze the effect of centering on hubness in more detail. [sent-91, score-0.641]
</p><p>32 mTahteri symmetric em aantri nxH = I−(1 /n)11T is called centering matrix, because tHhe = ce I−nt(e1r/end) 1da1ta matrix Xcent = [xc1ent, · · · ,xcnent] can be computed by Xcent = XH (Mardia et al. [sent-94, score-0.585]
</p><p>33 The Gram matrix Kcent of the centered feature vectors, whose (i, j) element holds the inner product hxicent, xcjenti, can be calculated from the original  Gucrtam hx matrix Ki by Kcent =  ? [sent-96, score-0.443]
</p><p>34 , centering can be applied even if data matrix X is not available but the similarity of objects can be measured by a kernel function in an implicit feature space. [sent-105, score-1.031]
</p><p>35 615 4  Theoretical analysis of the effect of centering on hubness  We now analyze why objects most similar to the centroid tend to be hubs in the dataset, and give an explanation as to why centering may suppress the emergence of hubs. [sent-106, score-2.105]
</p><p>36 1 Before centering Consider a dataset of m-dimensional feature vectors, with each vector x ∈ Rm generated independently  µ. [sent-108, score-0.568]
</p><p>37 fwroitmh a dcihst vreibcutotiron x w ∈it hR a finite mean vector In other words, objects x in this dataset are drawn from a distribution P(x), i. [sent-109, score-0.451]
</p><p>38 Let h and ‘ be two fixed objects in the dataset, such that the inner product to the true mean is higher for h than for ‘, i. [sent-121, score-0.592]
</p><p>39 (5)  We are interested in which of h and ‘ is more similar to x (in terms of inner product), or in other words, the difference of two inner products z = hh, xi h‘, xi = hh ‘, xi. [sent-124, score-0.537]
</p><p>40 Now, if we let h be the object in a given dataset with the highest similarity (inner product) to the mean and let ‘ be any other object in the set, then we see from the above discussion that h is likely to  µ,  µ  have higher similarity to x, a test sample drawn from distribution P(x). [sent-139, score-0.618]
</p><p>41 2 After centering Next let us investigate what happens if the dataset is centered. [sent-142, score-0.568]
</p><p>42 After centering, the similarity of x with each of the two fixed objects h and ‘ are evaluated by hh x¯, x x¯i and h‘ x¯, x x¯i, respectively. [sent-145, score-0.563]
</p><p>43 −  −  Their difference  zcent  = = = =  −  −  616  zcent is given by  hh x¯, x hh ‘, x hh ‘, xi z hh ‘, −  −  −  −  −  x¯i h‘ x¯, x x¯i hh ‘, x¯i x¯i. [sent-146, score-0.918]
</p><p>44 By definition we have z ∼ Q(z), and since hh ‘, x¯i is a constant, −  zcent  =z  −  hh  −  ‘, x¯ i  ∼  Q(z + hh  −  ‘, x¯ i). [sent-149, score-0.514]
</p><p>45 µ  In other words, the shape of the distribution does not change, but the mean is shifted to E[zcent] = E[z] − hh − ‘, x¯i = hh − ‘, µi− hh − ‘, x¯i = hh  −  ‘,  −  x¯i,  µ, µ,  where E[z] is given by Eq. [sent-150, score-0.714]
</p><p>46 5  Hubs in multi-cluster data  In this section, we discuss emergence of hubs when the data consists of multiple clusters. [sent-159, score-0.44]
</p><p>47 How-  ever, one might still argue that objects similar to the data centroid should hardly occur in that case. [sent-161, score-0.505]
</p><p>48 Using both synthetic and real datasets, we demonstrate below that even in multi-cluster data, objects that are only slightly more similar to the data mean (centroid) may emerge as hubs. [sent-162, score-0.451]
</p><p>49 (a), (d): scatter plot of the N10 value of objects and their similarity to centroid. [sent-167, score-0.475]
</p><p>50 (c), (f): the number of times (y-axis) an object (whose ID is on the x-axis) appears in the 10 nearest neighbors of objects of the same cluster (black bars), and those of different clusters (magenta). [sent-170, score-0.729]
</p><p>51 The von Mises-Fisher distribution is a distribution of unit vectors (it can roughly be thought of as a normal distribution on a unit hypersphere), so for objects (feature vectors) sampled from this distribution, inner product reduces to cosine similarity. [sent-172, score-0.826]
</p><p>52 We 100 objects from each of the ten distributions (clusters), and made a dataset of 1,000 objects in total. [sent-173, score-0.716]
</p><p>53 Note that even though all sampled objects reside on the surface of the unit hypersphere, the data centroid lies not on the surface but inside the hypersphere. [sent-189, score-0.529]
</p><p>54 , object similarity is measured by raw inner product, not by cosine. [sent-192, score-0.395]
</p><p>55 2 Correlation between hubness and centroid similarity The scatter plot in Figure 1(a) shows the correlation between the degree of hubness (N10) of an object and its inner product similarity to the data centroid. [sent-195, score-1.051]
</p><p>56 The N10 value of an object is defined as the number of times the object appears in the 10 nearest neighbors of other objects in the dataset. [sent-196, score-0.849]
</p><p>57 The similarity to the centroid is uniformly 0 as a result of centering, and no objects have an N10 value greater than 33. [sent-203, score-0.598]
</p><p>58 3  Influence of hubs on objects in different clusters The kNN matrix of Figure 1(b) depicts the kNN relations with k = 10 among objects before centering. [sent-206, score-1.076]
</p><p>59 If object x is in the 10 nearest neighbors of object y, a point is plotted at coordinates (x, y). [sent-208, score-0.53]
</p><p>60 In this matrix, object IDs are sorted by the cluster the objects belong to. [sent-211, score-0.499]
</p><p>61 Hence in the ideal case in which the k nearest neighbors of every object consist genuinely of objects from the same cluster, only the diagonal blocks would be colored, and off-diagonal areas would be left blank. [sent-212, score-0.699]
</p><p>62 The presence of many warm colored vertical lines suggests that many hub objects appear in the 10 nearest neighbors of other objects that are not in the same cluster as the hubs. [sent-214, score-1.059]
</p><p>63 Thus these hubs may have a strong influence on the kNN prediction of other objects. [sent-215, score-0.372]
</p><p>64 Re618 call that N10 is the number oftimes an object appears in the 10 nearest neighbors of other objects. [sent-219, score-0.38]
</p><p>65 The bar for each object is broken down by whether the object and its neighbors belong to the same cluster (black bar) or in different clusters (magenta bar). [sent-220, score-0.458]
</p><p>66 In terms of kNN classification, having a large number of nearest neighbors with the same class improves the classification performance, so longer black bars and shorter magenta bars are more desirable. [sent-221, score-0.55]
</p><p>67 Before centering (Figure 1(c)), hub objects with large N10 values are similar not only to objects belonging to the same cluster (as indicated by black bars), but also to objects belonging to different clusters (magenta bars). [sent-222, score-1.661]
</p><p>68 After centering (Figure 1(f)), the number of tall magenta bars decreases. [sent-223, score-0.661]
</p><p>69 7% of the 10 nearest neighbors of an object have the same class label as the object (as indicated by the ratio of the total height of black bars relative to that of all bars in Figure 1(c)). [sent-225, score-0.749]
</p><p>70 We can observe the same trends as we saw in Figure 1 for the synthetic data: positive correlation between hubness (N10) and inner product with the data centroid before centering; hubs appearing in the nearest neigh-  bors of many objects of different classes; and both are reduced after centering. [sent-238, score-1.385]
</p><p>71 6  Hubness weighted centering  Centering shifts the origin of the space to the data centroid, and objects similar to the centroid tend to become hubs. [sent-242, score-1.247]
</p><p>72 Thus in a sense, centering can be interpreted as an operation that shifts the origin towards hubs. [sent-243, score-0.668]
</p><p>73 In this section, we extrapolate this interpretation,  product similarity to the data centroid  product similarity to the data centroid Figure 2: Reuters Transcribed data. [sent-244, score-0.698]
</p><p>74 To this end, we consider weighted centering, a variation of centering in which each object is associated with a weight, and the origin is shifted to the weighted mean of the data. [sent-246, score-1.013]
</p><p>75 Specifically, we define the weight of an object as the sum of the similarities (inner products) between the object and all objects, regarding this sum as the index of how likely the object can be a hub. [sent-247, score-0.45]
</p><p>76 1 Weighted centering In weighted centering, we associate weight wi to each object iin the dataset, and move the origin to the weighted centroid  x¯weighted=Xi=n1wixi 619 where Pin=1 wi = 1 and 0 ≤ wi ≤ 1 for i = 1, . [sent-249, score-1.146]
</p><p>77 Notice that the original centering formula (2) is recovered by letting wi = 1/n for all i= 1, . [sent-254, score-0.519]
</p><p>78 Weighted centering can also be kernelized by using the weighted centering matrix H(w) = I 1wT − iinn place woefi gHh itend Eq. [sent-258, score-1.178]
</p><p>79 2 Similarity-dependent weighting To move the origin towards hubs more aggressively, we place more weights on objects that are more likely to become hubs. [sent-262, score-0.834]
</p><p>80 This likelihood is estimated by the similarity of individual objects to all objects in the data set. [sent-263, score-0.731]
</p><p>81 Let di be the sum of the similarity between object xi and all objects in the dataset. [sent-264, score-0.603]
</p><p>82 When γ > 0, weighted centering moves the origin closer to the objects with a large di than normal centering would. [sent-269, score-1.576]
</p><p>83 7  Experiments  We evaluated the effect of centering in two natural language tasks: word sense disambiguation (WSD) and document classification. [sent-270, score-0.578]
</p><p>84 We are interested in whether hubs are actually reduced after centering, and whether the performance of kNN classification is improved. [sent-271, score-0.412]
</p><p>85 , inner product of feature vectors  normalized to unit length; Kcent denotes the centered similarity matrix computed by Eq. [sent-274, score-0.524]
</p><p>86 Since the Laplacian kernels are defined for graph nodes, we computed them by taking the cosine similarity matrix K as the weighted adjacency (affinity) matrix of a graph. [sent-281, score-0.393]
</p><p>87 2 Compared methods We applied kNN classification using cosine similarity K, and its four transformed similarity measures: centered similarity Kcent, its weighted variant Kweighted, Mutual Proximity and graph Laplacian kernels. [sent-299, score-0.53]
</p><p>88 The sense of a test object was predicted by  voting from the k training objects most similar to the test object, as measured by the respective similarity measures. [sent-300, score-0.596]
</p><p>89 , either (unweighted) majority vote, or weighted vote in which votes from individual objects are weighted by their similarity score to the test objects. [sent-303, score-0.599]
</p><p>90 , 2010a) and counted Nk(x), the number of times object x occurs in the kNN lists of other objects in the dataset (we fix k = 10 below). [sent-315, score-0.518]
</p><p>91 The emergence of hubs in a dataset can then be quantified with skewness, defined as follows:  SNk=Eh? [sent-316, score-0.489]
</p><p>92 When hubs exist in a dataset, the distribution of Nk is expected to skew to the right, and yields a large SNk (Radovanovi c´ et al. [sent-320, score-0.404]
</p><p>93 After centering (Kcent and Kweighted) skewness became markedly smaller than that of the noncentered cosine K. [sent-338, score-0.738]
</p><p>94 In particular, weighted centering (Kweighted) slightly outperformed GAMBL, though the difference was small. [sent-340, score-0.593]
</p><p>95 kNN classification was done in a standard way: The class of object x is predicted by the majority vote from k = 10 objects most similar to x, measured by a specified similarity measure. [sent-364, score-0.665]
</p><p>96 Weighted centering (Kweighted) further improved F1 on the Mini Newsgroups data. [sent-372, score-0.519]
</p><p>97 8  Conclusion  We have shown that centering similarity matrices reduces the emergence of hubs in the data, and consequently improves the accuracy of nearest neighbor classification. [sent-373, score-1.252]
</p><p>98 We have theoretically analyzed why objects most similar to the mean tend to make hubs, and also proved that centering cancels the bias in the distribution of inner products, and thus is expected 622  to reduce hubs. [sent-374, score-1.098]
</p><p>99 Weighted centering shifts the origin towards hubs more aggressively, and further improved the classification performance in some cases. [sent-376, score-1.08]
</p><p>100 In future work, we plan to exploit the class distribution in the dataset to make more effective similarity measures; notice that the hubness weighted centering of Section 6 is an unsupervised method, in the sense that class information was not used for determining weights. [sent-377, score-0.971]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('centering', 0.519), ('hubs', 0.372), ('objects', 0.319), ('centroid', 0.186), ('knn', 0.181), ('skewness', 0.171), ('inner', 0.152), ('hh', 0.151), ('object', 0.15), ('nearest', 0.128), ('hub', 0.122), ('hubness', 0.122), ('kweighted', 0.122), ('laplacian', 0.118), ('origin', 0.118), ('kcent', 0.11), ('radovanovi', 0.11), ('neighbors', 0.102), ('similarity', 0.093), ('centered', 0.089), ('bars', 0.081), ('weighted', 0.074), ('gambl', 0.073), ('xcent', 0.073), ('neighbor', 0.072), ('product', 0.07), ('emergence', 0.068), ('matrix', 0.066), ('magenta', 0.061), ('mardia', 0.061), ('saerens', 0.061), ('zcent', 0.061), ('mean', 0.051), ('dataset', 0.049), ('nk', 0.049), ('cosine', 0.048), ('kernels', 0.046), ('emerge', 0.045), ('suzuki', 0.045), ('xi', 0.041), ('classification', 0.04), ('proximity', 0.039), ('vote', 0.039), ('colored', 0.039), ('wsd', 0.039), ('chebotarev', 0.037), ('hypersphere', 0.037), ('ikumi', 0.037), ('kazuo', 0.037), ('mini', 0.037), ('scatter', 0.037), ('schnitzer', 0.037), ('snk', 0.037), ('von', 0.036), ('synthetic', 0.036), ('kernel', 0.034), ('reuters', 0.034), ('sense', 0.034), ('els', 0.034), ('black', 0.033), ('shimbo', 0.032), ('distribution', 0.032), ('mutual', 0.031), ('shifts', 0.031), ('aggressively', 0.031), ('vectors', 0.03), ('cluster', 0.03), ('mihalcea', 0.029), ('fisher', 0.029), ('ten', 0.029), ('breakdown', 0.029), ('normal', 0.027), ('shifted', 0.027), ('transcribed', 0.027), ('bar', 0.026), ('plot', 0.026), ('transformation', 0.025), ('disambiguation', 0.025), ('reduce', 0.025), ('median', 0.025), ('move', 0.025), ('alexandros', 0.024), ('decadt', 0.024), ('eriksson', 0.024), ('fukumizu', 0.024), ('ivanovi', 0.024), ('lenz', 0.024), ('masand', 0.024), ('mirjana', 0.024), ('mishima', 0.024), ('nanopoulos', 0.024), ('qamar', 0.024), ('shamis', 0.024), ('shizuoka', 0.024), ('warmer', 0.024), ('xji', 0.024), ('navigli', 0.024), ('unit', 0.024), ('rada', 0.024), ('class', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="44-tfidf-1" href="./emnlp-2013-Centering_Similarity_Measures_to_Reduce_Hubs.html">44 emnlp-2013-Centering Similarity Measures to Reduce Hubs</a></p>
<p>Author: Ikumi Suzuki ; Kazuo Hara ; Masashi Shimbo ; Marco Saerens ; Kenji Fukumizu</p><p>Abstract: The performance of nearest neighbor methods is degraded by the presence of hubs, i.e., objects in the dataset that are similar to many other objects. In this paper, we show that the classical method of centering, the transformation that shifts the origin of the space to the data centroid, provides an effective way to reduce hubs. We show analytically why hubs emerge and why they are suppressed by centering, under a simple probabilistic model of data. To further reduce hubs, we also move the origin more aggressively towards hubs, through weighted centering. Our experimental results show that (weighted) centering is effective for natural language data; it improves the performance of the k-nearest neighbor classi- fiers considerably in word sense disambiguation and document classification tasks.</p><p>2 0.098652817 <a title="44-tfidf-2" href="./emnlp-2013-Exploiting_Language_Models_for_Visual_Recognition.html">78 emnlp-2013-Exploiting Language Models for Visual Recognition</a></p>
<p>Author: Dieu-Thu Le ; Jasper Uijlings ; Raffaella Bernardi</p><p>Abstract: The problem of learning language models from large text corpora has been widely studied within the computational linguistic community. However, little is known about the performance of these language models when applied to the computer vision domain. In this work, we compare representative models: a window-based model, a topic model, a distributional memory and a commonsense knowledge database, ConceptNet, in two visual recognition scenarios: human action recognition and object prediction. We examine whether the knowledge extracted from texts through these models are compatible to the knowledge represented in images. We determine the usefulness of different language models in aiding the two visual recognition tasks. The study shows that the language models built from general text corpora can be used instead of expensive annotated images and even outperform the image model when testing on a big general dataset.</p><p>3 0.094457947 <a title="44-tfidf-3" href="./emnlp-2013-Learning_Distributions_over_Logical_Forms_for_Referring_Expression_Generation.html">119 emnlp-2013-Learning Distributions over Logical Forms for Referring Expression Generation</a></p>
<p>Author: Nicholas FitzGerald ; Yoav Artzi ; Luke Zettlemoyer</p><p>Abstract: We present a new approach to referring expression generation, casting it as a density estimation problem where the goal is to learn distributions over logical expressions identifying sets of objects in the world. Despite an extremely large space of possible expressions, we demonstrate effective learning of a globally normalized log-linear distribution. This learning is enabled by a new, multi-stage approximate inference technique that uses a pruning model to construct only the most likely logical forms. We train and evaluate the approach on a new corpus of references to sets of visual objects. Experiments show the approach is able to learn accurate models, which generate over 87% of the expressions people used. Additionally, on the previously studied special case of single object reference, we show a 35% relative error reduction over previous state of the art.</p><p>4 0.090018921 <a title="44-tfidf-4" href="./emnlp-2013-This_Text_Has_the_Scent_of_Starbucks%3A_A_Laplacian_Structured_Sparsity_Model_for_Computational_Branding_Analytics.html">184 emnlp-2013-This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</a></p>
<p>Author: William Yang Wang ; Edward Lin ; John Kominek</p><p>Abstract: We propose a Laplacian structured sparsity model to study computational branding analytics. To do this, we collected customer reviews from Starbucks, Dunkin’ Donuts, and other coffee shops across 38 major cities in the Midwest and Northeastern regions of USA. We study the brand related language use through these reviews, with focuses on the brand satisfaction and gender factors. In particular, we perform three tasks: automatic brand identification from raw text, joint brand-satisfaction prediction, and joint brandgender-satisfaction prediction. This work extends previous studies in text classification by incorporating the dependency and interaction among local features in the form of structured sparsity in a log-linear model. Our quantitative evaluation shows that our approach which combines the advantages of graphical modeling and sparsity modeling techniques significantly outperforms various standard and stateof-the-art text classification algorithms. In addition, qualitative analysis of our model reveals important features of the language uses associated with the specific brands.</p><p>5 0.077502087 <a title="44-tfidf-5" href="./emnlp-2013-Towards_Situated_Dialogue%3A_Revisiting_Referring_Expression_Generation.html">185 emnlp-2013-Towards Situated Dialogue: Revisiting Referring Expression Generation</a></p>
<p>Author: Rui Fang ; Changsong Liu ; Lanbo She ; Joyce Y. Chai</p><p>Abstract: In situated dialogue, humans and agents have mismatched capabilities of perceiving the shared environment. Their representations of the shared world are misaligned. Thus referring expression generation (REG) will need to take this discrepancy into consideration. To address this issue, we developed a hypergraph-based approach to account for group-based spatial relations and uncertainties in perceiving the environment. Our empirical results have shown that this approach outperforms a previous graph-based approach with an absolute gain of 9%. However, while these graph-based approaches perform effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), they perform rather poorly when the agent has imperfect perception of the environment (e.g., 45%). This big performance gap calls for new solutions to REG that can mediate a shared perceptual basis in situated dialogue.</p><p>6 0.077178165 <a title="44-tfidf-6" href="./emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations.html">17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</a></p>
<p>7 0.064169616 <a title="44-tfidf-7" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>8 0.060957931 <a title="44-tfidf-8" href="./emnlp-2013-Automatically_Identifying_Pseudepigraphic_Texts.html">37 emnlp-2013-Automatically Identifying Pseudepigraphic Texts</a></p>
<p>9 0.050587967 <a title="44-tfidf-9" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>10 0.049332183 <a title="44-tfidf-10" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>11 0.04833208 <a title="44-tfidf-11" href="./emnlp-2013-Predicting_the_Resolution_of_Referring_Expressions_from_User_Behavior.html">153 emnlp-2013-Predicting the Resolution of Referring Expressions from User Behavior</a></p>
<p>12 0.046283297 <a title="44-tfidf-12" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>13 0.042655874 <a title="44-tfidf-13" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<p>14 0.040858895 <a title="44-tfidf-14" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>15 0.04066686 <a title="44-tfidf-15" href="./emnlp-2013-Image_Description_using_Visual_Dependency_Representations.html">98 emnlp-2013-Image Description using Visual Dependency Representations</a></p>
<p>16 0.038848471 <a title="44-tfidf-16" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>17 0.037837055 <a title="44-tfidf-17" href="./emnlp-2013-Easy_Victories_and_Uphill_Battles_in_Coreference_Resolution.html">67 emnlp-2013-Easy Victories and Uphill Battles in Coreference Resolution</a></p>
<p>18 0.037249643 <a title="44-tfidf-18" href="./emnlp-2013-Dynamic_Feature_Selection_for_Dependency_Parsing.html">66 emnlp-2013-Dynamic Feature Selection for Dependency Parsing</a></p>
<p>19 0.036492173 <a title="44-tfidf-19" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>20 0.036317814 <a title="44-tfidf-20" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.122), (1, 0.04), (2, -0.048), (3, 0.011), (4, -0.009), (5, 0.126), (6, -0.004), (7, -0.047), (8, -0.09), (9, -0.039), (10, -0.131), (11, -0.017), (12, -0.009), (13, -0.0), (14, -0.007), (15, -0.049), (16, -0.058), (17, -0.001), (18, -0.042), (19, 0.048), (20, -0.003), (21, -0.042), (22, 0.051), (23, -0.013), (24, -0.144), (25, -0.057), (26, -0.093), (27, -0.043), (28, 0.002), (29, 0.018), (30, 0.058), (31, 0.123), (32, -0.042), (33, -0.088), (34, 0.115), (35, 0.027), (36, -0.017), (37, 0.048), (38, -0.071), (39, -0.054), (40, 0.079), (41, 0.235), (42, 0.071), (43, -0.022), (44, 0.077), (45, -0.033), (46, -0.044), (47, 0.089), (48, 0.016), (49, 0.107)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9508338 <a title="44-lsi-1" href="./emnlp-2013-Centering_Similarity_Measures_to_Reduce_Hubs.html">44 emnlp-2013-Centering Similarity Measures to Reduce Hubs</a></p>
<p>Author: Ikumi Suzuki ; Kazuo Hara ; Masashi Shimbo ; Marco Saerens ; Kenji Fukumizu</p><p>Abstract: The performance of nearest neighbor methods is degraded by the presence of hubs, i.e., objects in the dataset that are similar to many other objects. In this paper, we show that the classical method of centering, the transformation that shifts the origin of the space to the data centroid, provides an effective way to reduce hubs. We show analytically why hubs emerge and why they are suppressed by centering, under a simple probabilistic model of data. To further reduce hubs, we also move the origin more aggressively towards hubs, through weighted centering. Our experimental results show that (weighted) centering is effective for natural language data; it improves the performance of the k-nearest neighbor classi- fiers considerably in word sense disambiguation and document classification tasks.</p><p>2 0.59035277 <a title="44-lsi-2" href="./emnlp-2013-Towards_Situated_Dialogue%3A_Revisiting_Referring_Expression_Generation.html">185 emnlp-2013-Towards Situated Dialogue: Revisiting Referring Expression Generation</a></p>
<p>Author: Rui Fang ; Changsong Liu ; Lanbo She ; Joyce Y. Chai</p><p>Abstract: In situated dialogue, humans and agents have mismatched capabilities of perceiving the shared environment. Their representations of the shared world are misaligned. Thus referring expression generation (REG) will need to take this discrepancy into consideration. To address this issue, we developed a hypergraph-based approach to account for group-based spatial relations and uncertainties in perceiving the environment. Our empirical results have shown that this approach outperforms a previous graph-based approach with an absolute gain of 9%. However, while these graph-based approaches perform effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), they perform rather poorly when the agent has imperfect perception of the environment (e.g., 45%). This big performance gap calls for new solutions to REG that can mediate a shared perceptual basis in situated dialogue.</p><p>3 0.56488132 <a title="44-lsi-3" href="./emnlp-2013-This_Text_Has_the_Scent_of_Starbucks%3A_A_Laplacian_Structured_Sparsity_Model_for_Computational_Branding_Analytics.html">184 emnlp-2013-This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</a></p>
<p>Author: William Yang Wang ; Edward Lin ; John Kominek</p><p>Abstract: We propose a Laplacian structured sparsity model to study computational branding analytics. To do this, we collected customer reviews from Starbucks, Dunkin’ Donuts, and other coffee shops across 38 major cities in the Midwest and Northeastern regions of USA. We study the brand related language use through these reviews, with focuses on the brand satisfaction and gender factors. In particular, we perform three tasks: automatic brand identification from raw text, joint brand-satisfaction prediction, and joint brandgender-satisfaction prediction. This work extends previous studies in text classification by incorporating the dependency and interaction among local features in the form of structured sparsity in a log-linear model. Our quantitative evaluation shows that our approach which combines the advantages of graphical modeling and sparsity modeling techniques significantly outperforms various standard and stateof-the-art text classification algorithms. In addition, qualitative analysis of our model reveals important features of the language uses associated with the specific brands.</p><p>4 0.47021899 <a title="44-lsi-4" href="./emnlp-2013-Predicting_the_Resolution_of_Referring_Expressions_from_User_Behavior.html">153 emnlp-2013-Predicting the Resolution of Referring Expressions from User Behavior</a></p>
<p>Author: Nikos Engonopoulos ; Martin Villalba ; Ivan Titov ; Alexander Koller</p><p>Abstract: We present a statistical model for predicting how the user of an interactive, situated NLP system resolved a referring expression. The model makes an initial prediction based on the meaning of the utterance, and revises it continuously based on the user’s behavior. The combined model outperforms its components in predicting reference resolution and when to give feedback.</p><p>5 0.4303017 <a title="44-lsi-5" href="./emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations.html">17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</a></p>
<p>Author: Shashank Srivastava ; Dirk Hovy ; Eduard Hovy</p><p>Abstract: In this paper, we propose a walk-based graph kernel that generalizes the notion of treekernels to continuous spaces. Our proposed approach subsumes a general framework for word-similarity, and in particular, provides a flexible way to incorporate distributed representations. Using vector representations, such an approach captures both distributional semantic similarities among words as well as the structural relations between them (encoded as the structure of the parse tree). We show an efficient formulation to compute this kernel using simple matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results.</p><p>6 0.42717254 <a title="44-lsi-6" href="./emnlp-2013-Learning_Distributions_over_Logical_Forms_for_Referring_Expression_Generation.html">119 emnlp-2013-Learning Distributions over Logical Forms for Referring Expression Generation</a></p>
<p>7 0.40933189 <a title="44-lsi-7" href="./emnlp-2013-Mining_New_Business_Opportunities%3A_Identifying_Trend_related_Products_by_Leveraging_Commercial_Intents_from_Microblogs.html">131 emnlp-2013-Mining New Business Opportunities: Identifying Trend related Products by Leveraging Commercial Intents from Microblogs</a></p>
<p>8 0.39963722 <a title="44-lsi-8" href="./emnlp-2013-Exploiting_Language_Models_for_Visual_Recognition.html">78 emnlp-2013-Exploiting Language Models for Visual Recognition</a></p>
<p>9 0.38615423 <a title="44-lsi-9" href="./emnlp-2013-A_temporal_model_of_text_periodicities_using_Gaussian_Processes.html">18 emnlp-2013-A temporal model of text periodicities using Gaussian Processes</a></p>
<p>10 0.38613078 <a title="44-lsi-10" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>11 0.38151437 <a title="44-lsi-11" href="./emnlp-2013-Multi-Relational_Latent_Semantic_Analysis.html">137 emnlp-2013-Multi-Relational Latent Semantic Analysis</a></p>
<p>12 0.366932 <a title="44-lsi-12" href="./emnlp-2013-Automatically_Identifying_Pseudepigraphic_Texts.html">37 emnlp-2013-Automatically Identifying Pseudepigraphic Texts</a></p>
<p>13 0.36524758 <a title="44-lsi-13" href="./emnlp-2013-Unsupervised_Spectral_Learning_of_WCFG_as_Low-rank_Matrix_Completion.html">195 emnlp-2013-Unsupervised Spectral Learning of WCFG as Low-rank Matrix Completion</a></p>
<p>14 0.32786429 <a title="44-lsi-14" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>15 0.31542927 <a title="44-lsi-15" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>16 0.29129454 <a title="44-lsi-16" href="./emnlp-2013-The_Topology_of_Semantic_Knowledge.html">182 emnlp-2013-The Topology of Semantic Knowledge</a></p>
<p>17 0.29010507 <a title="44-lsi-17" href="./emnlp-2013-Image_Description_using_Visual_Dependency_Representations.html">98 emnlp-2013-Image Description using Visual Dependency Representations</a></p>
<p>18 0.28802016 <a title="44-lsi-18" href="./emnlp-2013-Scaling_to_Large3_Data%3A_An_Efficient_and_Effective_Method_to_Compute_Distributional_Thesauri.html">165 emnlp-2013-Scaling to Large3 Data: An Efficient and Effective Method to Compute Distributional Thesauri</a></p>
<p>19 0.2838755 <a title="44-lsi-19" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>20 0.27461651 <a title="44-lsi-20" href="./emnlp-2013-Identifying_Multiple_Userids_of_the_Same_Author.html">95 emnlp-2013-Identifying Multiple Userids of the Same Author</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.031), (6, 0.01), (9, 0.01), (18, 0.029), (22, 0.034), (30, 0.057), (50, 0.027), (51, 0.126), (57, 0.405), (66, 0.03), (71, 0.04), (75, 0.031), (96, 0.044), (97, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.71715212 <a title="44-lda-1" href="./emnlp-2013-Centering_Similarity_Measures_to_Reduce_Hubs.html">44 emnlp-2013-Centering Similarity Measures to Reduce Hubs</a></p>
<p>Author: Ikumi Suzuki ; Kazuo Hara ; Masashi Shimbo ; Marco Saerens ; Kenji Fukumizu</p><p>Abstract: The performance of nearest neighbor methods is degraded by the presence of hubs, i.e., objects in the dataset that are similar to many other objects. In this paper, we show that the classical method of centering, the transformation that shifts the origin of the space to the data centroid, provides an effective way to reduce hubs. We show analytically why hubs emerge and why they are suppressed by centering, under a simple probabilistic model of data. To further reduce hubs, we also move the origin more aggressively towards hubs, through weighted centering. Our experimental results show that (weighted) centering is effective for natural language data; it improves the performance of the k-nearest neighbor classi- fiers considerably in word sense disambiguation and document classification tasks.</p><p>2 0.61338454 <a title="44-lda-2" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>Author: Polina Kuznetsova ; Jianfu Chen ; Yejin Choi</p><p>Abstract: Why do certain combinations of words such as “disadvantageous peace ” or “metal to the petal” appeal to our minds as interesting expressions with a sense of creativity, while other phrases such as “quiet teenager”, or “geometrical base ” not as much? We present statistical explorations to understand the characteristics of lexical compositions that give rise to the perception of being original, interesting, and at times even artistic. We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learning that give us further insights on how different aspects of lexical composition collectively contribute to the perceived creativity.</p><p>3 0.36648238 <a title="44-lda-3" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>Author: Zhongqing Wang ; Shoushan LI ; Fang Kong ; Guodong Zhou</p><p>Abstract: Personal profile information on social media like LinkedIn.com and Facebook.com is at the core of many interesting applications, such as talent recommendation and contextual advertising. However, personal profiles usually lack organization confronted with the large amount of available information. Therefore, it is always a challenge for people to find desired information from them. In this paper, we address the task of personal profile summarization by leveraging both personal profile textual information and social networks. Here, using social networks is motivated by the intuition that, people with similar academic, business or social connections (e.g. co-major, co-university, and cocorporation) tend to have similar experience and summaries. To achieve the learning process, we propose a collective factor graph (CoFG) model to incorporate all these resources of knowledge to summarize personal profiles with local textual attribute functions and social connection factors. Extensive evaluation on a large-scale dataset from LinkedIn.com demonstrates the effectiveness of the proposed approach. 1</p><p>4 0.35922024 <a title="44-lda-4" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>Author: Jason Weston ; Antoine Bordes ; Oksana Yakhnenko ; Nicolas Usunier</p><p>Abstract: This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over methods that rely on text features alone.</p><p>5 0.35911813 <a title="44-lda-5" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>Author: Xinjie Zhou ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style. In this paper, we study the problem of extracting opinion targets of Chinese microblog messages. Such fine-grained word-level task has not been well investigated in microblogs yet. We propose an unsupervised label propagation algorithm to address the problem. The opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets. Topics in microblogs are identified by hashtags or using clustering algorithms. Experimental results on Chinese microblogs show the effectiveness of our framework and algorithms.</p><p>6 0.35841691 <a title="44-lda-6" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>7 0.3580144 <a title="44-lda-7" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>8 0.35799077 <a title="44-lda-8" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>9 0.35770625 <a title="44-lda-9" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>10 0.35760865 <a title="44-lda-10" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>11 0.3571406 <a title="44-lda-11" href="./emnlp-2013-Exploiting_Zero_Pronouns_to_Improve_Chinese_Coreference_Resolution.html">80 emnlp-2013-Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</a></p>
<p>12 0.3569364 <a title="44-lda-12" href="./emnlp-2013-Microblog_Entity_Linking_by_Leveraging_Extra_Posts.html">130 emnlp-2013-Microblog Entity Linking by Leveraging Extra Posts</a></p>
<p>13 0.35637566 <a title="44-lda-13" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>14 0.35618705 <a title="44-lda-14" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>15 0.35512424 <a title="44-lda-15" href="./emnlp-2013-A_temporal_model_of_text_periodicities_using_Gaussian_Processes.html">18 emnlp-2013-A temporal model of text periodicities using Gaussian Processes</a></p>
<p>16 0.35501722 <a title="44-lda-16" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>17 0.3546426 <a title="44-lda-17" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>18 0.35455909 <a title="44-lda-18" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>19 0.35424629 <a title="44-lda-19" href="./emnlp-2013-A_Unified_Model_for_Topics%2C_Events_and_Users_on_Twitter.html">16 emnlp-2013-A Unified Model for Topics, Events and Users on Twitter</a></p>
<p>20 0.35324302 <a title="44-lda-20" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
