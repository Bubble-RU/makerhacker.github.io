<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-154" href="#">emnlp2013-154</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</h1>
<br/><p>Source: <a title="emnlp-2013-154-pdf" href="http://aclweb.org/anthology//D/D13/D13-1166.pdf">pdf</a></p><p>Author: Dimitri Kartsaklis ; Mehrnoosh Sadrzadeh</p><p>Abstract: Recent work has shown that compositionaldistributional models using element-wise operations on contextual word vectors benefit from the introduction of a prior disambiguation step. The purpose of this paper is to generalise these ideas to tensor-based models, where relational words such as verbs and adjectives are represented by linear maps (higher order tensors) acting on a number of arguments (vectors). We propose disambiguation algorithms for a number of tensor-based models, which we then test on a variety of tasks. The results show that disambiguation can provide better compositional representation even for the case of tensor-based models. Further- more, we confirm previous findings regarding the positive effect of disambiguation on vector mixture models, and we compare the effectiveness of the two approaches.</p><p>Reference: <a title="emnlp-2013-154-reference" href="../emnlp2013_reference/emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk s Abstract Recent work has shown that compositionaldistributional models using element-wise operations on contextual word vectors benefit from the introduction of a prior disambiguation step. [sent-5, score-0.363]
</p><p>2 The purpose of this paper is to generalise these ideas to tensor-based models, where relational words such as verbs and adjectives are represented by linear maps (higher order tensors) acting on a number of arguments (vectors). [sent-6, score-0.466]
</p><p>3 We propose disambiguation algorithms for a number of tensor-based models, which we then test on a variety of tasks. [sent-7, score-0.161]
</p><p>4 The results show that disambiguation can provide better compositional representation even for the case of tensor-based models. [sent-8, score-0.289]
</p><p>5 Further-  more, we confirm previous findings regarding the positive effect of disambiguation on vector mixture models, and we compare the effectiveness of the two approaches. [sent-9, score-0.413]
</p><p>6 This idea allows the words to be represented by vectors of statistics collected from a sufficiently large corpus of text; each element of the vector reflects how many times a word co-occurs in the same context with another word of the vocabulary. [sent-13, score-0.383]
</p><p>7 Following an influential work (Mitchell and Lapata, 2008), the models in the first category compute a sentence vector as a mixture of the original word vectors, using simple oper-  ations such as element-wise multiplication and addition; we refer to these models as vector mixtures. [sent-20, score-0.495]
</p><p>8 The main characteristic of these models is that they do not distinguish between the type-logical identities of the different words: an intransitive verb, for example, is of the same order as its subject (a noun), and both will contribute equally to the composite sentence vector. [sent-21, score-0.335]
</p><p>9 Words with special meanings, such as verbs and adjectives, are usually seen as functions acting on, hence modifying, a number of arguments rather than lexical units of the same order as them; an adjective, for example, is a function that returns a modified version of its input noun. [sent-23, score-0.178]
</p><p>10 The notion of a framework where relational words are entities living in vector spaces of higher order than nouns, which are simple vectors, has been formalized by Coecke et al. [sent-28, score-0.334]
</p><p>11 At least for the vector mixture approach, this practice has been proved suboptimal: Reddy et al. [sent-32, score-0.252]
</p><p>12 (2013) test a number of simple multiplicative and additive models using disambiguated vector representations on various tasks, showing that the introduction of a disambiguation step prior to actual composition can indeed increase the quality of the composite vectors. [sent-34, score-0.697]
</p><p>13 However, the fact that disambiguation can be beneficial for models based on vector mixtures is not very surprising. [sent-35, score-0.342]
</p><p>14 Both additive and multiplicative compositions are but a kind of average  of the vectors of the words in the sentence, hence can directly benefit from the provision of more accurate starting points. [sent-36, score-0.273]
</p><p>15 Perhaps a more interesting question, and one that the current paper aims to address, is to what extent disambiguation can also provide benefits for tensor-based approaches, which in general constitute more powerful models for natural language (see discussion in Section 2). [sent-37, score-0.161]
</p><p>16 Based on the generic procedure of Sch u¨tze (1998), we propose algorithms for a num1591 ber of tensor-based models, where the composition is modelled as the application of linear maps (tensor contractions). [sent-39, score-0.245]
</p><p>17 Following Mitchell and Lapata (2008) and many others, we test our models on two disambiguation tasks similar to that of Kintsch (2001), and on the phrase similarity task introduced in (Mitchell and Lapata, 2010). [sent-40, score-0.161]
</p><p>18 In almost every case, the results show that disambiguation can make  a great difference in the case oftensor-based models; they also reconfirm previous findings regarding the effectiveness of the method for simple vector mixture models. [sent-41, score-0.413]
</p><p>19 On the other hand, the elementwise multiplication of two vectors can be seen as the intersection of their features: a zero element in one of the input vectors will eliminate the corresponding feature in the output, no matter how high the other input component was. [sent-45, score-0.433]
</p><p>20 In addition to failing to identify the special roles of words in a sentence, vector mixture models disregard grammar in another way: the commutativity of operators make them a bag-  of-words approach, where the meaning of sentence ‘dog bites man’ is equated to that of ‘man bites dog’ . [sent-46, score-0.437]
</p><p>21 On the contrary to the above element-wise treatment, a compositional approach based on linear maps computes each element of the resulting vecVector mixture  Tensor-based  Figure1:V=ectormixtureandtensor-basedmo=delsfor  composition. [sent-47, score-0.385]
</p><p>22 In the latter approach, the ith element of the output vector is the linear combination of the input vector with the ith row of the matrix. [sent-48, score-0.364]
</p><p>23 tor via a linear combination of all the elements of the input vector (right part of Figure 1); in other words, possible interdependencies between different features are also taken into account, offering (in principle) more power. [sent-49, score-0.175]
</p><p>24 For example, one can consider build-  ing linear maps for prepositions and logical words, rather than treating them as noise and discard them, as commonly done in the vector mixture models. [sent-52, score-0.362]
</p><p>25 3  Disambiguation in vector mixtures  For a compositional model based on vector mixtures, polysemy of words can be a critical factor. [sent-53, score-0.408]
</p><p>26 This effect is demonstrated in Figure 2, which shows the composition of the ambiguous verb ‘run’ (with meanings moving fast and dissolving) with the subject ‘horse’ . [sent-56, score-0.471]
</p><p>27 The first three components of our toy vector space are related to the dissolving meaning, while the last three of them to the moving fast meaning. [sent-57, score-0.181]
</p><p>28 An ambiguous vector for ‘run’ will have non-zero values for every component. [sent-58, score-0.215]
</p><p>29 The left part of Figure 2 shows what happens when the ambiguous ‘run’ vector is used; the multiplication with the ‘horse’ vector will produce an impure result, half affected by composition and half by disambiguation. [sent-62, score-0.513]
</p><p>30 In order to  achieve this, we have to introduce a disambiguation step prior to composition (right part of Figure 2). [sent-64, score-0.299]
</p><p>31 What remains to be seen is if disambiguation can also provide benefits for the linguistically motivated setting of tensorbased models, the principles of which are shortly discussed in the next section. [sent-68, score-0.211]
</p><p>32 4  Tensors as multilinear maps  A tensor is a geometric object that can be seen as the generalization of the familiar notion of a vector in higher dimensions. [sent-69, score-0.597]
</p><p>33 The order of a tensor is the number of its dimensions; in other words, the number of indices we need to fully describe a random element of the tensor. [sent-70, score-0.45]
</p><p>34 Hence, a vector is a tensor of order 1, a matrix is a tensor of order 2, and so on. [sent-71, score-0.977]
</p><p>35 adjectives, intransitive verbs) is represented by a tensor of order 2 (a matrix), a binary function (e. [sent-83, score-0.479]
</p><p>36 a transitive verb) as an order 3 tensor, and so on. [sent-85, score-0.248]
</p><p>37 Due to the above isomorphism, function application (and hence our compositional operation) becomes a generalisation of matrix multiplication, formalised in terms of the inner product. [sent-86, score-0.229]
</p><p>38 In the case of a unary relational word, such as an adjective, this is nothing more than the usual notion of matrix multiplication between a matrix and a vector. [sent-87, score-0.356]
</p><p>39 The generalization of this process to tensors of higher order is known as tensor contraction. [sent-88, score-0.85]
</p><p>40 Given two tensors of orders n and m, the tensor contraction operation will always produce a tensor of order n + m − 2. [sent-89, score-1.2]
</p><p>41 +Le mt us see an example of how this works for a simple transitive sentence. [sent-90, score-0.206]
</p><p>42 n aV m ×aOtrix w)i1l ; a further interaction of this result with the subject will return a vector for the whole transitive sentence living in RJ. [sent-96, score-0.42]
</p><p>43 We should note that the order in which the verb is applied to its arguments is not important; so in general the meaning of a transitive sentence is given by: (V  O)T  S=  (VT  S)  O  (2)  where T denotes a transpose and makes indices match, since subject precedes the verb. [sent-97, score-0.598]
</p><p>44 5  Creating verb tensors  In this section we review a number of proposals regarding concrete methods ofconstructing tensors for relational words in the context of the frameworks of Coecke et al. [sent-98, score-1.29]
</p><p>45 2 Relational Following ideas from the set-theoretic view of formal semantics, Grefenstette and Sadrzadeh (201 1a) suggest that the meaning of a relational word should be represented as the sum of its arguments. [sent-100, score-0.205]
</p><p>46 The meaning of adjective ‘red’,  ×  for example, becomes the sum of the vectors of all the nouns that ‘red’ modifies in the corpus; so = Pi −n −ou − →ni, where iiterates through all the occurrenPces of ‘red’ . [sent-101, score-0.252]
</p><p>47 This can be generalised to relationaPl words of any arity, by summing the tensor product of their arguments. [sent-102, score-0.35]
</p><p>48 So for a transitive verb we have:  r− e→d  verb2 = X(s−u − →bji  ⊗  −o −b →ji)  (3)  Xi  1The symbol denotes tensor contraction. [sent-103, score-0.727]
</p><p>49 sitive verb as an example; however the descriptions apply to any relational word of any arity. [sent-105, score-0.332]
</p><p>50 A vector (an order-1 tensor) is denoted as →x ; tensors of order n > 1are shown as xn for clarity. [sent-106, score-0.631]
</p><p>51 1593 where iagain iterates over all occurrences ofthe specific verb in the corpus and the superscript denotes the order of the tensor. [sent-107, score-0.213]
</p><p>52 Recall from Section 4 that for the transitive  case this increases the order of the verb tensor to 4 (2 dimensions for the arguments plus another 2 for the result). [sent-109, score-0.871]
</p><p>53 The other two dimensions of the tensor remain empty (filled with zeros), a fact that simplifies the calculations but also considerably weakens the expressive power of the model. [sent-111, score-0.4]
</p><p>54 This simplification transforms Equation 2 to the following:  subj verb obj2  =  (−s −u →bj −o →bj) ? [sent-112, score-0.171]
</p><p>55 verb2 ⊗  (4)  where ⊗ denotes the tensor product and ? [sent-113, score-0.35]
</p><p>56 Frobenius The previous models bring the important limitation that only sentences of the same struc-  ×  ture can be meaningfully compared; it is not possible, for example, to compare an intransitive sentence (e. [sent-116, score-0.204]
</p><p>57 ‘kids play’) with a transitive one (‘children play football’), since the former is a vector and the latter a matrix. [sent-118, score-0.337]
</p><p>58 These models turn the matrix of Equation 3 to a tensor of order 3 (as required by the type-logical identities) by copying one of the existing dimensions. [sent-121, score-0.491]
</p><p>59 When the dimension of rows (corresponding to subjects) is copied, the calculation of a vector for a transitive sentence becomes: −s u− b− j− − ve −r −b − o − →bj  =  s− −u →bj  ? [sent-122, score-0.378]
</p><p>60 (7)  Linear regression None of the above models create tensors that are fully populated: one or more dimensions will always remain empty. [sent-126, score-0.561]
</p><p>61 (2013) use linear regression in order to learn full tensors of order 3 for transitive verbs. [sent-128, score-0.845]
</p><p>62 One can generalize this procedure to tensors of higher order by proceeding step-wise, as done by Grefenstette et al. [sent-133, score-0.539]
</p><p>63 t 6  Generic context-based disambiguation  In all of the models of Section 5, the training of a relational word tensor is based on the set of contexts where this word occurs. [sent-136, score-0.704]
</p><p>64 Hence, in these models the problem of creating disambiguated versions of tensors can be recasted to that of further breaking the set of contexts in a way that each subset reflects a different sense of the word in the corpus. [sent-137, score-0.752]
</p><p>65 Each one of these subsets can then be used to train a tensor for a specific sense of the target relational word. [sent-142, score-0.605]
</p><p>66 Towards this purpose we use a variation of the effective procedure of Sch u¨tze (1998): first, each context for a target word wt is represented by a context vector of the form (−w →1 . [sent-143, score-0.254]
</p><p>67 Next, we apply a clusteri6=ng w method on this set of vectors in order to discover the latent senses of wt. [sent-147, score-0.272]
</p><p>68 The above procedure will give us a number of clusters, each consisting of context vectors; we use the centroid of each cluster as a vectorial representation of the corresponding sense. [sent-149, score-0.178]
</p><p>69 So in our model each word w is initially represented by a tuple h −→w, Si, where →w is the lexical vector of the word as hcwrea,tSedi b wyh tehree usual distributional practice, and S is a set of sense vectors (centroids of context vector clusters) produced by the above procedure. [sent-150, score-0.593]
</p><p>70 The disambiguation of a new word w under a context C can now be accomplished as follows: we create a context vector →c for C as above, and we compare it with every sense vector of w; the word is assigned to the sense corresponding to the closest sense vector. [sent-151, score-0.789]
</p><p>71 Specifically, if Sw is the set of sense vectors for w, →c the context vector for C, and d(− →v , →u ) our vector distance measure, the preferred sense is given by:  s  s = avr−→sg∈mSwind(v −→s, −→c )  (9)  For the actual clustering step we follow the setting of Kartsaklis et al. [sent-152, score-0.644]
</p><p>72 The output of HAC is a dendrogram embedding all the possible partitionings of the 3Informal experimentation with more robust probabilistic techniques, such as Dirichlet process gaussian mixture models, revealed no significant benefits for our setting. [sent-158, score-0.171]
</p><p>73 7  Constructing unambiguous verb tensors  The procedure described in Section 6 provides us with n clusters of context vectors for a target word. [sent-163, score-0.908]
</p><p>74 Since in our case each context vector corresponds to a distinct sentence, the output of the clustering scheme can also be seen as n subsets of sentences, where the word appears under different senses. [sent-164, score-0.173]
</p><p>75 It is now quite straightforward to use this partitioning of the training corpus in order to learn unambiguous versions of verb tensors, as detailed below. [sent-165, score-0.312]
</p><p>76 Relational/Frobenius Both the Relational and the Frobenius models use the same way of creating an initial verb matrix (Equation 3) which then they expand to a higher order tensor. [sent-166, score-0.311]
</p><p>77 Then, the verb tensor for the ith sense is:  verbi2  =  X(−s −u −b →js  ⊗  −o −b →js)  (10)  sX∈Si where subjs and objs refer to the subject/object pair that occurred with the verb in sentence s. [sent-171, score-0.827]
</p><p>78 This can  be generalized to any arity n as follows: On  wordin = X Oa−r −g − k→,s  (11)  sX∈Si Ok=1  where argk,s denotes the kth argument of the target word in sentence s. [sent-172, score-0.161]
</p><p>79 Kronecker For a given verb v in a context C, let →vi be the sense vector of v given C corresponding to the sense ireturned by Equation 9. [sent-173, score-0.532]
</p><p>80 Then we have:  verbi2  = →vi ⊗ →vi  (12)  The generalized version to arity n is given by: On  wordin = O →vi  (13)  Ok=1  1595 Linear regression Creating unambiguous full tensors using linear regression is also quite straightforward. [sent-174, score-0.774]
</p><p>81 Let us assume again that the clustering step for a verb v returns n sets of sentences S1 . [sent-175, score-0.207]
</p><p>82 This will result in n verb tensors, which will correspond to the different senses of the verb. [sent-180, score-0.249]
</p><p>83 Generalization to higher arities is a straightforward extension of the step-wise process in Section 5 for transitive verbs. [sent-181, score-0.206]
</p><p>84 8  Experiments  In this section we will test the effect of disambiguation on the models of Section 5 in a variety of tasks. [sent-182, score-0.161]
</p><p>85 The vectors are disambiguated both syntactically and semantically: first, separate vectors have been created for different syntactic usages of the same word in the corpus; for ex-  ample, the word ‘book’ has two vectors, one for its noun sense and one for its verb sense. [sent-188, score-0.701]
</p><p>86 Models We compare the tensor-based models of Section 5 with the multiplicative and additive models of Mitchell and Lapata (2008), reporting results for both ambiguous and disambiguated versions. [sent-190, score-0.305]
</p><p>87 For all the disambiguated models, the best sense for each word in the sentence or phrase is first selected by applying the procedure of Section 6 and Equation 9. [sent-191, score-0.306]
</p><p>88 If the model is based on a vector mixture, the sense vectors corresponding to these senses are multiplied or added to form the composite representation for the sentence or phrase. [sent-192, score-0.616]
</p><p>89 For the tensor-based models, the composite meanings are calculated according to the equations of Section 5, using verb tensors created by the procedures of Section 7. [sent-193, score-0.795]
</p><p>90 1 Verb disambiguation task Perhaps surprisingly, one of the most popular tasks for testing compositionality in distributional models is based on disambiguation. [sent-201, score-0.204]
</p><p>91 This task, originally introduced by Kintsch (2001), has been adopted by Mitchell and Lapata (2008) and others for evaluating the quality of composition in vector spaces. [sent-202, score-0.227]
</p><p>92 Given an ambiguous verb such as ‘file’, the goal is to find out to what extent the presence of an appropriate context will disambiguate its intended meaning. [sent-203, score-0.297]
</p><p>93 a subject/object pair) is composed with two landmark verbs corresponding to the different senses (‘smooth’ and ‘register’) to create simple sentences. [sent-206, score-0.186]
</p><p>94 In this paper we test our models on two different  datasets of transitive sentences, that of Grefenstette and Sadrzadeh (201 1a) and Kartsaklis et al. [sent-208, score-0.206]
</p><p>95 ‘say’ with meanings state and allege or ‘write’ with meanings publish and spell), the landmarks in the second dataset correspond to clearly separated senses (e. [sent-215, score-0.284]
</p><p>96 ‘file’ with meanings register and smooth or ‘charge’ with meanings accuse and bill). [sent-217, score-0.246]
</p><p>97 The scores of the compositional models are the cosine distances (or the Frobenius inner products, in the case of matrices) between the composite representations of the sentences of each pair. [sent-223, score-0.259]
</p><p>98 5 The verbs-only model (BL) refers to a non-compositional evaluation, where the similarity between two sentences is solely based on the distance between the two verbs, without applying any compositional step with subject and object. [sent-227, score-0.174]
</p><p>99 The tensor-based models present much better performance than the vector mixture ones, with the disambiguated version of the copy-object model significantly higher than the relational model. [sent-228, score-0.545]
</p><p>100 In general, the disambiguation step improves the results of all the tensor-based models except Kronecker; the effect is reversed for the vector mixture models, where the disambiguated versions present much worse performance (these 5For all tables in this section, ? [sent-231, score-0.545]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tensors', 0.458), ('tensor', 0.35), ('transitive', 0.206), ('kartsaklis', 0.196), ('verb', 0.171), ('disambiguation', 0.161), ('relational', 0.161), ('vectors', 0.152), ('disambiguated', 0.132), ('bj', 0.132), ('vector', 0.131), ('mixture', 0.121), ('horse', 0.111), ('compositional', 0.096), ('composition', 0.096), ('sense', 0.094), ('frobenius', 0.093), ('composite', 0.088), ('kronecker', 0.087), ('intransitive', 0.087), ('ambiguous', 0.084), ('sadrzadeh', 0.079), ('senses', 0.078), ('meanings', 0.078), ('vrc', 0.075), ('multiplication', 0.071), ('grefenstette', 0.071), ('arity', 0.07), ('maps', 0.066), ('vectorial', 0.065), ('matrix', 0.062), ('exemplar', 0.06), ('element', 0.058), ('adjective', 0.056), ('landmark', 0.056), ('lapata', 0.054), ('partitioning', 0.053), ('regression', 0.053), ('arguments', 0.052), ('verbs', 0.052), ('dimensions', 0.05), ('bites', 0.05), ('bji', 0.05), ('cali', 0.05), ('compositionaldistributional', 0.05), ('dissolving', 0.05), ('landmarks', 0.05), ('multilinear', 0.05), ('partitionings', 0.05), ('tensorbased', 0.05), ('wordin', 0.05), ('register', 0.05), ('mixtures', 0.05), ('woman', 0.05), ('zamparelli', 0.05), ('multiplicative', 0.05), ('adjectives', 0.049), ('equation', 0.048), ('mitchell', 0.048), ('file', 0.048), ('unambiguous', 0.046), ('meaning', 0.044), ('vi', 0.044), ('linear', 0.044), ('isomorphism', 0.044), ('hac', 0.044), ('ok', 0.044), ('vk', 0.044), ('kintsch', 0.044), ('distributional', 0.043), ('subject', 0.042), ('context', 0.042), ('order', 0.042), ('red', 0.041), ('sentence', 0.041), ('smooth', 0.04), ('meaningfully', 0.04), ('pulman', 0.04), ('objects', 0.04), ('inner', 0.039), ('additive', 0.039), ('procedure', 0.039), ('baroni', 0.038), ('copying', 0.037), ('variances', 0.037), ('coecke', 0.037), ('perhaps', 0.037), ('sentences', 0.036), ('creating', 0.036), ('js', 0.035), ('python', 0.035), ('identities', 0.035), ('run', 0.035), ('reddy', 0.033), ('mehrnoosh', 0.033), ('creation', 0.033), ('subjects', 0.033), ('contexts', 0.032), ('representation', 0.032), ('hence', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="154-tfidf-1" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>Author: Dimitri Kartsaklis ; Mehrnoosh Sadrzadeh</p><p>Abstract: Recent work has shown that compositionaldistributional models using element-wise operations on contextual word vectors benefit from the introduction of a prior disambiguation step. The purpose of this paper is to generalise these ideas to tensor-based models, where relational words such as verbs and adjectives are represented by linear maps (higher order tensors) acting on a number of arguments (vectors). We propose disambiguation algorithms for a number of tensor-based models, which we then test on a variety of tasks. The results show that disambiguation can provide better compositional representation even for the case of tensor-based models. Further- more, we confirm previous findings regarding the positive effect of disambiguation on vector mixture models, and we compare the effectiveness of the two approaches.</p><p>2 0.21276839 <a title="154-tfidf-2" href="./emnlp-2013-Multi-Relational_Latent_Semantic_Analysis.html">137 emnlp-2013-Multi-Relational Latent Semantic Analysis</a></p>
<p>Author: Kai-Wei Chang ; Wen-tau Yih ; Christopher Meek</p><p>Abstract: We present Multi-Relational Latent Semantic Analysis (MRLSA) which generalizes Latent Semantic Analysis (LSA). MRLSA provides an elegant approach to combining multiple relations between words by constructing a 3-way tensor. Similar to LSA, a lowrank approximation of the tensor is derived using a tensor decomposition. Each word in the vocabulary is thus represented by a vector in the latent semantic space and each relation is captured by a latent square matrix. The degree of two words having a specific relation can then be measured through simple linear algebraic operations. We demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources, MRLSA achieves state- of-the-art performance on existing benchmark datasets for two relations, antonymy and is-a.</p><p>3 0.16995446 <a title="154-tfidf-3" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>Author: Masashi Tsubaki ; Kevin Duh ; Masashi Shimbo ; Yuji Matsumoto</p><p>Abstract: We present a novel vector space model for semantic co-compositionality. Inspired by Generative Lexicon Theory (Pustejovsky, 1995), our goal is a compositional model where both predicate and argument are allowed to modify each others’ meaning representations while generating the overall semantics. This readily addresses some major challenges with current vector space models, notably the polysemy issue and the use of one representation per word type. We implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations. We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality. The model achieves the best result to date (ρ = 0.47) on the semantic similarity task of transitive verbs (Grefenstette and Sadrzadeh, 2011).</p><p>4 0.16227175 <a title="154-tfidf-4" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>Author: Richard Socher ; Alex Perelygin ; Jean Wu ; Jason Chuang ; Christopher D. Manning ; Andrew Ng ; Christopher Potts</p><p>Abstract: Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.</p><p>5 0.11081044 <a title="154-tfidf-5" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>Author: Douwe Kiela ; Stephen Clark</p><p>Abstract: We present a novel unsupervised approach to detecting the compositionality of multi-word expressions. We compute the compositionality of a phrase through substituting the constituent words with their “neighbours” in a semantic vector space and averaging over the distance between the original phrase and the substituted neighbour phrases. Several methods of obtaining neighbours are presented. The results are compared to existing supervised results and achieve state-of-the-art performance on a verb-object dataset of human compositionality ratings.</p><p>6 0.10249446 <a title="154-tfidf-6" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>7 0.09825249 <a title="154-tfidf-7" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>8 0.081977308 <a title="154-tfidf-8" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>9 0.078827076 <a title="154-tfidf-9" href="./emnlp-2013-Effectiveness_and_Efficiency_of_Open_Relation_Extraction.html">68 emnlp-2013-Effectiveness and Efficiency of Open Relation Extraction</a></p>
<p>10 0.078513213 <a title="154-tfidf-10" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>11 0.077696905 <a title="154-tfidf-11" href="./emnlp-2013-Identifying_Phrasal_Verbs_Using_Many_Bilingual_Corpora.html">96 emnlp-2013-Identifying Phrasal Verbs Using Many Bilingual Corpora</a></p>
<p>12 0.069711998 <a title="154-tfidf-12" href="./emnlp-2013-Deriving_Adjectival_Scales_from_Continuous_Space_Word_Representations.html">59 emnlp-2013-Deriving Adjectival Scales from Continuous Space Word Representations</a></p>
<p>13 0.065445744 <a title="154-tfidf-13" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<p>14 0.064169616 <a title="154-tfidf-14" href="./emnlp-2013-Centering_Similarity_Measures_to_Reduce_Hubs.html">44 emnlp-2013-Centering Similarity Measures to Reduce Hubs</a></p>
<p>15 0.06412939 <a title="154-tfidf-15" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>16 0.063688509 <a title="154-tfidf-16" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>17 0.063091695 <a title="154-tfidf-17" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>18 0.058207624 <a title="154-tfidf-18" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>19 0.056352474 <a title="154-tfidf-19" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>20 0.055549212 <a title="154-tfidf-20" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.201), (1, 0.042), (2, -0.051), (3, -0.031), (4, 0.001), (5, 0.225), (6, -0.044), (7, -0.133), (8, -0.171), (9, -0.057), (10, 0.11), (11, 0.043), (12, -0.124), (13, 0.071), (14, -0.004), (15, -0.132), (16, 0.087), (17, -0.103), (18, -0.075), (19, 0.037), (20, -0.038), (21, 0.082), (22, -0.057), (23, -0.047), (24, -0.016), (25, -0.108), (26, -0.072), (27, -0.004), (28, 0.033), (29, 0.052), (30, -0.051), (31, -0.033), (32, -0.013), (33, -0.116), (34, 0.098), (35, 0.096), (36, -0.248), (37, 0.169), (38, 0.009), (39, -0.016), (40, -0.079), (41, 0.173), (42, -0.017), (43, -0.085), (44, 0.053), (45, 0.09), (46, 0.044), (47, -0.166), (48, -0.006), (49, -0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94928157 <a title="154-lsi-1" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>Author: Dimitri Kartsaklis ; Mehrnoosh Sadrzadeh</p><p>Abstract: Recent work has shown that compositionaldistributional models using element-wise operations on contextual word vectors benefit from the introduction of a prior disambiguation step. The purpose of this paper is to generalise these ideas to tensor-based models, where relational words such as verbs and adjectives are represented by linear maps (higher order tensors) acting on a number of arguments (vectors). We propose disambiguation algorithms for a number of tensor-based models, which we then test on a variety of tasks. The results show that disambiguation can provide better compositional representation even for the case of tensor-based models. Further- more, we confirm previous findings regarding the positive effect of disambiguation on vector mixture models, and we compare the effectiveness of the two approaches.</p><p>2 0.8003847 <a title="154-lsi-2" href="./emnlp-2013-Multi-Relational_Latent_Semantic_Analysis.html">137 emnlp-2013-Multi-Relational Latent Semantic Analysis</a></p>
<p>Author: Kai-Wei Chang ; Wen-tau Yih ; Christopher Meek</p><p>Abstract: We present Multi-Relational Latent Semantic Analysis (MRLSA) which generalizes Latent Semantic Analysis (LSA). MRLSA provides an elegant approach to combining multiple relations between words by constructing a 3-way tensor. Similar to LSA, a lowrank approximation of the tensor is derived using a tensor decomposition. Each word in the vocabulary is thus represented by a vector in the latent semantic space and each relation is captured by a latent square matrix. The degree of two words having a specific relation can then be measured through simple linear algebraic operations. We demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources, MRLSA achieves state- of-the-art performance on existing benchmark datasets for two relations, antonymy and is-a.</p><p>3 0.69768113 <a title="154-lsi-3" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>Author: Masashi Tsubaki ; Kevin Duh ; Masashi Shimbo ; Yuji Matsumoto</p><p>Abstract: We present a novel vector space model for semantic co-compositionality. Inspired by Generative Lexicon Theory (Pustejovsky, 1995), our goal is a compositional model where both predicate and argument are allowed to modify each others’ meaning representations while generating the overall semantics. This readily addresses some major challenges with current vector space models, notably the polysemy issue and the use of one representation per word type. We implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations. We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality. The model achieves the best result to date (ρ = 0.47) on the semantic similarity task of transitive verbs (Grefenstette and Sadrzadeh, 2011).</p><p>4 0.5524655 <a title="154-lsi-4" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>Author: Douwe Kiela ; Stephen Clark</p><p>Abstract: We present a novel unsupervised approach to detecting the compositionality of multi-word expressions. We compute the compositionality of a phrase through substituting the constituent words with their “neighbours” in a semantic vector space and averaging over the distance between the original phrase and the substituted neighbour phrases. Several methods of obtaining neighbours are presented. The results are compared to existing supervised results and achieve state-of-the-art performance on a verb-object dataset of human compositionality ratings.</p><p>5 0.50351083 <a title="154-lsi-5" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>Author: Eva Maria Vecchi ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: In this study, we use compositional distributional semantic methods to investigate restrictions in adjective ordering. Specifically, we focus on properties distinguishing AdjectiveAdjective-Noun phrases in which there is flexibility in the adjective ordering from those bound to a rigid order. We explore a number of measures extracted from the distributional representation of AAN phrases which may indicate a word order restriction. We find that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modification of the adjectives. Our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics.</p><p>6 0.45078722 <a title="154-lsi-6" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>7 0.41932908 <a title="154-lsi-7" href="./emnlp-2013-Centering_Similarity_Measures_to_Reduce_Hubs.html">44 emnlp-2013-Centering Similarity Measures to Reduce Hubs</a></p>
<p>8 0.40874591 <a title="154-lsi-8" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>9 0.40103066 <a title="154-lsi-9" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>10 0.38858384 <a title="154-lsi-10" href="./emnlp-2013-Deriving_Adjectival_Scales_from_Continuous_Space_Word_Representations.html">59 emnlp-2013-Deriving Adjectival Scales from Continuous Space Word Representations</a></p>
<p>11 0.38568532 <a title="154-lsi-11" href="./emnlp-2013-Effectiveness_and_Efficiency_of_Open_Relation_Extraction.html">68 emnlp-2013-Effectiveness and Efficiency of Open Relation Extraction</a></p>
<p>12 0.38493821 <a title="154-lsi-12" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>13 0.34734061 <a title="154-lsi-13" href="./emnlp-2013-Automatic_Knowledge_Acquisition_for_Case_Alternation_between_the_Passive_and_Active_Voices_in_Japanese.html">33 emnlp-2013-Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in Japanese</a></p>
<p>14 0.34695742 <a title="154-lsi-14" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>15 0.34036586 <a title="154-lsi-15" href="./emnlp-2013-The_VerbCorner_Project%3A_Toward_an_Empirically-Based_Semantic_Decomposition_of_Verbs.html">183 emnlp-2013-The VerbCorner Project: Toward an Empirically-Based Semantic Decomposition of Verbs</a></p>
<p>16 0.3398034 <a title="154-lsi-16" href="./emnlp-2013-Unsupervised_Spectral_Learning_of_WCFG_as_Low-rank_Matrix_Completion.html">195 emnlp-2013-Unsupervised Spectral Learning of WCFG as Low-rank Matrix Completion</a></p>
<p>17 0.33080027 <a title="154-lsi-17" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>18 0.29825169 <a title="154-lsi-18" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>19 0.2972365 <a title="154-lsi-19" href="./emnlp-2013-A_Multi-Teraflop_Constituency_Parser_using_GPUs.html">10 emnlp-2013-A Multi-Teraflop Constituency Parser using GPUs</a></p>
<p>20 0.29393753 <a title="154-lsi-20" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.033), (6, 0.046), (18, 0.043), (22, 0.053), (30, 0.065), (38, 0.237), (47, 0.01), (50, 0.032), (51, 0.224), (66, 0.025), (71, 0.029), (75, 0.038), (77, 0.015), (90, 0.031), (96, 0.031), (97, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88646895 <a title="154-lda-1" href="./emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model.html">75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</a></p>
<p>Author: Nathanael Chambers</p><p>Abstract: Event schema induction is the task of learning high-level representations of complex events (e.g., a bombing) and their entity roles (e.g., perpetrator and victim) from unlabeled text. Event schemas have important connections to early NLP research on frames and scripts, as well as modern applications like template extraction. Recent research suggests event schemas can be learned from raw text. Inspired by a pipelined learner based on named entity coreference, this paper presents the first generative model for schema induction that integrates coreference chains into learning. Our generative model is conceptually simpler than the pipelined approach and requires far less training data. It also provides an interesting contrast with a recent HMM-based model. We evaluate on a common dataset for template schema extraction. Our generative model matches the pipeline’s performance, and outperforms the HMM by 7 F1 points (20%).</p><p>same-paper 2 0.84108317 <a title="154-lda-2" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>Author: Dimitri Kartsaklis ; Mehrnoosh Sadrzadeh</p><p>Abstract: Recent work has shown that compositionaldistributional models using element-wise operations on contextual word vectors benefit from the introduction of a prior disambiguation step. The purpose of this paper is to generalise these ideas to tensor-based models, where relational words such as verbs and adjectives are represented by linear maps (higher order tensors) acting on a number of arguments (vectors). We propose disambiguation algorithms for a number of tensor-based models, which we then test on a variety of tasks. The results show that disambiguation can provide better compositional representation even for the case of tensor-based models. Further- more, we confirm previous findings regarding the positive effect of disambiguation on vector mixture models, and we compare the effectiveness of the two approaches.</p><p>3 0.71259964 <a title="154-lda-3" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>Author: Richard Socher ; Alex Perelygin ; Jean Wu ; Jason Chuang ; Christopher D. Manning ; Andrew Ng ; Christopher Potts</p><p>Abstract: Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.</p><p>4 0.71160179 <a title="154-lda-4" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>Author: Jason Weston ; Antoine Bordes ; Oksana Yakhnenko ; Nicolas Usunier</p><p>Abstract: This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over methods that rely on text features alone.</p><p>5 0.70713019 <a title="154-lda-5" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>Author: Kuzman Ganchev ; Dipanjan Das</p><p>Abstract: We present a framework for cross-lingual transfer of sequence information from a resource-rich source language to a resourceimpoverished target language that incorporates soft constraints via posterior regularization. To this end, we use automatically word aligned bitext between the source and target language pair, and learn a discriminative conditional random field model on the target side. Our posterior regularization constraints are derived from simple intuitions about the task at hand and from cross-lingual alignment information. We show improvements over strong baselines for two tasks: part-of-speech tagging and namedentity segmentation.</p><p>6 0.70707464 <a title="154-lda-6" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>7 0.70654494 <a title="154-lda-7" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>8 0.70645052 <a title="154-lda-8" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>9 0.705935 <a title="154-lda-9" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>10 0.7059207 <a title="154-lda-10" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<p>11 0.70522678 <a title="154-lda-11" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>12 0.70421028 <a title="154-lda-12" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>13 0.70359564 <a title="154-lda-13" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>14 0.70229888 <a title="154-lda-14" href="./emnlp-2013-Unsupervised_Induction_of_Cross-Lingual_Semantic_Relations.html">193 emnlp-2013-Unsupervised Induction of Cross-Lingual Semantic Relations</a></p>
<p>15 0.70227182 <a title="154-lda-15" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>16 0.7014572 <a title="154-lda-16" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>17 0.70131701 <a title="154-lda-17" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>18 0.70084006 <a title="154-lda-18" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>19 0.70005697 <a title="154-lda-19" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>20 0.69995463 <a title="154-lda-20" href="./emnlp-2013-Scaling_Semantic_Parsers_with_On-the-Fly_Ontology_Matching.html">164 emnlp-2013-Scaling Semantic Parsers with On-the-Fly Ontology Matching</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
