<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-40" href="#">emnlp2013-40</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</h1>
<br/><p>Source: <a title="emnlp-2013-40-pdf" href="http://aclweb.org/anthology//D/D13/D13-1204.pdf">pdf</a></p><p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: Many statistical learning problems in NLP call for local model search methods. But accuracy tends to suffer with current techniques, which often explore either too narrowly or too broadly: hill-climbers can get stuck in local optima, whereas samplers may be inefficient. We propose to arrange individual local optimizers into organized networks. Our building blocks are operators of two types: (i) transform, which suggests new places to search, via non-random restarts from already-found local optima; and (ii) join, which merges candidate solutions to find better optima. Experiments on grammar induction show that pursuing different transforms (e.g., discarding parts of a learned model or ignoring portions of training data) results in improvements. Groups of locally-optimal solutions can be further perturbed jointly, by constructing mixtures. Using these tools, we designed several modular dependency grammar induction networks of increasing complexity. Our complete sys- tem achieves 48.6% accuracy (directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data) more than 5% higher than the previous state-of-the-art. —</p><p>Reference: <a title="emnlp-2013-40-reference" href="../emnlp2013_reference/emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Many statistical learning problems in NLP call for local model search methods. [sent-6, score-0.126]
</p><p>2 But accuracy tends to suffer with current techniques, which often explore either too narrowly or too broadly: hill-climbers can get stuck in local optima, whereas samplers may be inefficient. [sent-7, score-0.082]
</p><p>3 We propose to arrange individual local optimizers into organized networks. [sent-8, score-0.143]
</p><p>4 Our building blocks are operators of two types: (i) transform, which suggests new places to search, via non-random restarts from already-found local optima; and (ii) join, which merges candidate solutions to find better optima. [sent-9, score-0.397]
</p><p>5 Experiments on grammar induction show that pursuing different transforms (e. [sent-10, score-0.407]
</p><p>6 Groups of locally-optimal solutions can be further perturbed jointly, by constructing mixtures. [sent-13, score-0.123]
</p><p>7 Using these tools, we designed several modular dependency grammar induction networks of increasing complexity. [sent-14, score-0.486]
</p><p>8 6% accuracy (directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data) more than 5% higher than the previous state-of-the-art. [sent-16, score-0.092]
</p><p>9 —  1 Introduction Statistical methods for grammar induction often boil down to solving non-convex optimization problems. [sent-17, score-0.384]
</p><p>10 Early work attempted to locally maximize the likelihood of a corpus, using EM to estimate probabilities of dependency arcs between word bigrams (Paskin 2001a; 2001b). [sent-18, score-0.148]
</p><p>11 But even the latest techniques can be quite error-prone and sensitive to initialization, because of approximate, local search. [sent-22, score-0.082]
</p><p>12 In theory, global optima can be found by enumerating all parse forests that derive a corpus, though this is usually prohibitively expensive in practice. [sent-23, score-0.177]
</p><p>13 A 1983 preferable brute force approach is sampling, as in Markov-chain Monte Carlo (MCMC) and random  restarts (Hu et al. [sent-24, score-0.14]
</p><p>14 At the other extreme, MCMC may cling to a neighborhood, rejecting most proposed moves that would escape a local attractor. [sent-27, score-0.118]
</p><p>15 Sampling methods thus take unbounded time to solve a problem (and can’t certify optimality) but are useful for finding approximate solutions to grammar induction (Cohn et al. [sent-28, score-0.448]
</p><p>16 We propose an alternative (deterministic) search heuristic that combines local optimization via EM with non-random restarts. [sent-30, score-0.185]
</p><p>17 Another is by merging promising solutions, since even simple interpolation (Jelinek and Mercer, 1980) of local optima may be superior to all of the originals. [sent-33, score-0.193]
</p><p>18 Informed restarts can make it possible to explore a combinatorial search space more rapidly and thoroughly than with traditional methods alone. [sent-34, score-0.184]
</p><p>19 2  Abstract Operators  Let C be a collection of counts the sufficient statistics from which a candidate solution to an optimization problem could be computed, e. [sent-35, score-0.175]
</p><p>20 The counts may be fractional and solutions could take the form of multinomial distributions. [sent-38, score-0.239]
</p><p>21 Given only C and LD, the single-node optimization network above would be the minimal search pattern worth considering. [sent-43, score-0.161]
</p><p>22 However, if we had another optimizer L′ or a fresh starting point C′ then more —  —  complicated networks could become useful. [sent-44, score-0.163]
</p><p>23 For instance, ifD consisted of several heterogeneous data sources, then the counts from some of them could be ignored: a classifier might be estimated from just news text. [sent-54, score-0.116]
</p><p>24 1 Models and Data We constrain all parse structures to be projective, via dependency-and-boundary grammars (Spitkovsky et al. [sent-70, score-0.134]
</p><p>25 Laplace) smoothing, 2We use the short-hand DBM to refer to DBM-3, which is equivalent to DBM-2 if D has no internally-punctuated sentences (D=Dsplit), and DBM-1 if all inputs also have trailing punctuation (D=Dsimp); DBM0 is our short-hand for DBM-0. [sent-92, score-0.124]
</p><p>26 This approach does not require tuning termination thresholds, allowing optimizers to run to numerical convergence if necessary, and handles only our shorter inputs (l ≤ 15), starting with soft EM (L = SL, for “soft lateen”). [sent-107, score-0.232]
</p><p>27 Alternating rEdM EMs Mwo uunltdil be expensive here, since updates take (at least) O(l3) time, and hard EM’s objective (L = H) is the one better suited to long inputs (Spitkovsky et al. [sent-110, score-0.099]
</p><p>28 We compute performance as directed dependency accuracies (DDA), fractions of correct unlabeled arcs in parsed output (an extrinsic For most WSJ experiments we include also sentence and parse tree cross-entropies (soft and hard EMs’ intrinsic metrics), in bits per token (bpt). [sent-124, score-0.306]
</p><p>29 4  Concrete Operators  We will now instantiate the operators sketched out in §2 specifically for the grammar induction task. [sent-128, score-0.377]
</p><p>30 1 Transform #1: A Simple Filter Given a model that was estimated from (and therefore parses) a data set D, the simple filter (F) attempts to extract a cleaner model, based on the simpler complete sentences of Dsimp. [sent-131, score-0.085]
</p><p>31 This punctuationsensitive heuristic may steer a learner towards easy but representative training text and, we showed, aids grammar induction (Spitkovsky et al. [sent-133, score-0.325]
</p><p>32 It blurs all details of induced parses in a data set D, except the number of times each (ordered) word pair participates in a dependency relation. [sent-138, score-0.092]
</p><p>33 Grammar inducers can sometimes make good progress in resolving undirected parse structures despite being wrong about the polarities of most arcs (Spitkovsky et al. [sent-140, score-0.214]
</p><p>34 For the desired effect, these aggregated counts are left unnormalized, while all other counts (of word fertilities and sentence roots) get discarded. [sent-153, score-0.156]
</p><p>35 3 Join: A Combiner The combiner must admit arbitrary inputs, including models not estimated from D, unlike the transforms. [sent-180, score-0.153]
</p><p>36 Consequently, as a preliminary step, we convert each input Ci into parse trees of D, with counts Ci′, via Viterbi-decoding with a smoothed, unlexicalized version of the corresponding incoming model. [sent-181, score-0.294]
</p><p>37 Actual combination is then performed in a more precise (unsmoothed) fashion: Ci∗ are the (lexicalized) solutions starting from Ci′; and C+∗ is initialized with their sum, Pi Ci∗. [sent-182, score-0.179]
</p><p>38 Counts of the lexicalized model with lowestP Pcross-entropy on D become the output:7  CC12LD  (9)  5  Basic Networks  We are ready to propose a non-trivial subnetwork for grammar induction, based on the transform and join operators, which we will reuse in larger networks. [sent-183, score-0.48]
</p><p>39 We first fork off two variations of the incoming model based on D0: (i) a filtered view, which focuses on cleaner, simpler data (transform #1); and (ii) a symmetrized view that backs off to word associations (transform #2). [sent-190, score-0.134]
</p><p>40 Finally, the two new induced sets of parse trees, for D, are merged (lexicalized join): —  —  CSFD0S L D D B M 0C 12C 2′1 HLD· BM(10)  The idea here is to prepare for two scenarios: an incoming grammar that is either good or bad for D. [sent-194, score-0.273]
</p><p>41 Since we can’t know ahead of time which is the true case, we pursue both optimization paths simultaneously and let a combiner later decide for us. [sent-197, score-0.212]
</p><p>42 This is because soft EM integrates previously unseen tokens into new grammars better than hard EM, as evidenced by our failed attempt to reproduce the “baby steps” strategy with Viterbi training (Spitkovsky et al. [sent-199, score-0.156]
</p><p>43 A combiner then executes hard EM, and since outputs of transforms are trees, the end-to-end process is a chain of  lateen alternations that starts and ends with hard EM. [sent-201, score-0.395]
</p><p>44 Smoothing 1987 causes initial parse trees to be chosen uniformly at random, as suggested by Cohen and Smith (2010): ∅ 15 (12) 5. [sent-203, score-0.122]
</p><p>45 This iterative approach to optimization is akin to deterministic annealing (Rose, 1998), and is patterned after “baby steps” (Spitkovsky et al. [sent-205, score-0.145]
</p><p>46 Unlike the basic FJ, where symmetrization was a no-op (since there were no counts in C = ∅), IFJ makes use of symmetrizers — e. [sent-208, score-0.13]
</p><p>47 Although it should be easy to learn words that go together from very short fragments, extracting correct polarities of their relations could be a challenge: to a large extent, outputs of early inductors may be artifacts of how our generative models factor (see §4. [sent-211, score-0.13]
</p><p>48 pWtietherefore expect symmetrization to be crucial in earlier stages but to weaken any high quality grammars, nearer the end; it will be up to combiners to handle such phase transitions correctly (or gracefully). [sent-214, score-0.129]
</p><p>49 3  Grounded Iterated Fork/Join (GIFJ)  So far, our networks have been either purely iterative (IFJ) or static (FJ). [sent-216, score-0.113]
</p><p>50 These two approaches can also be combined, by injecting FJ’s solutions into IFJ’s more dynamic stream. [sent-217, score-0.169]
</p><p>51 Its empty initializer corresponds to guessing (projective) parse trees uniformly at random, which has 21. [sent-222, score-0.122]
</p><p>52 FJ’s filter starts from parse trees ofWSJ1s4imp only, and trains up a full DBM. [sent-230, score-0.122]
</p><p>53 The join operator uses counts from A and B, C1 and C2, to obtain parse trees whose own counts C1′ and C2′ initialize lexicalized training. [sent-234, score-0.464]
</p><p>54 Their mixture C+ is a simple sum of counts in C1∗ and C2∗ : it is not expected to be an improvement but happens to be a good move, resulting in a grammar with higher accuracy (64. [sent-239, score-0.236]
</p><p>55 2%) and will be the local optimum returned by FJ’s join operator, because it attains the lowest cross-entropy (7. [sent-246, score-0.21]
</p><p>56 To test how much of this performance could be obtained by a simpler iterated network, we experimented with ablated systems that don’t fork or join, i. [sent-252, score-0.207]
</p><p>57 We conclude from these results that an ability to branch out into different promising regions of a solution space, and to merge solutions of varying quality into better models, are important properties of FJ subnetworks. [sent-256, score-0.123]
</p><p>58 This result shows that fresh perspectives from optimizers that start over can make search efforts more fruitful. [sent-261, score-0.105]
</p><p>59 1 An Iterative Combiner (IC) Our basic combiner introduced a third option, C+∗, into a pool of candidate solutions, {C∗1 , C2∗}. [sent-265, score-0.153]
</p><p>60 00bpt), and its advantages can be realized more fully in the larger networks (albeit without any end-to-end guarantees): upgrading all 15 combiners in IFJ would  improve performance (slightly) more than grounding (71. [sent-272, score-0.189]
</p><p>61 We shall now state our most general iterative combiner (IC) algorithm: Start with a solution pool Pp = {C∗i}in=1. [sent-280, score-0.197]
</p><p>62 Finally, ifp = p′, return the best of the solutions in p; otherwise, repeat from p := p′. [sent-282, score-0.123]
</p><p>63 2 A Grammar Transformer (GT) The levels of our systems’ performance at grammar induction thus far suggest that the space of possible networks (say, with up to k components) may itself  be worth exploring more thoroughly. [sent-290, score-0.394]
</p><p>64 1w)e,r bthutan n othwe original 45) instead of starting from ∅ directly: —  —  1989  ∅SDDsBlimMpl+1   (16)  l  The output of this subnetwork can then be refined, by reconciling it with a previous dynamic solution. [sent-296, score-0.117]
</p><p>65 To test the generalized IC algorithm, we merged our implementations of these three strong grammar induction pipelines into a combined system (CS). [sent-307, score-0.325]
</p><p>66 —  ( GIFTJ))###132HLD·D45BMCS  (19)  The quality of bracketings corresponding to (nontrivial) spans derived by heads of our dependency structures is competitive with the state-of-the-art in unsupervised constituent parsing. [sent-310, score-0.208]
</p><p>67 Table 2: Directed dependency accuracies (DDA) on Section 23 of WSJ (all sentences and up to length ten) for recent systems, our full networks (IFJ and GT), and threeway combination (CS) with the previous state-of-the-art. [sent-317, score-0.217]
</p><p>68 Even gold tags aren’t always helpful, as their number is rarely ideal for grammar induction (e. [sent-347, score-0.325]
</p><p>69 11  Related Work  The surest way to avoid local optima is to craft an objective that doesn’t have them. [sent-357, score-0.193]
</p><p>70 But for many NLP tasks, including grammar induction, the most relevant known objective functions are still riddled with local optima. [sent-363, score-0.24]
</p><p>71 Renewed efforts to find exact solutions (Eisner, 2012; Gormley and Eisner, 2013) may be a good fit for the smaller and simpler, earlier stages of our iterative networks. [sent-364, score-0.167]
</p><p>72 Moreover, random restarts via uniform probability measures can be optimal, in a worst-case-analysis sense, with parallel processing sometimes leading to exponential speed-ups (Hu et al. [sent-368, score-0.14]
</p><p>73 Iterated local search methods (Hoos and St¨ utzle, 2004; Johnson et al. [sent-374, score-0.126]
</p><p>74 , 1994), by introducing “uphill” moves via mixtures of random walks and greedy search strategies, does better than random noise alone or simulated annealing (Kirkpatrick et al. [sent-381, score-0.122]
</p><p>75 In NLP, Moore and Quirk’s (2008) random walks from previous local optima were faster than uniform sampling and also increased BLEU scores; Elsner and Schudy (2009) showed that local search can outperform greedy solutions for document clustering and —  chat disentanglement tasks; and Mei et al. [sent-383, score-0.442]
</p><p>76 (2001) incorporated tabu search (Glover, 1989; Glover and Laguna, 1993, Ch. [sent-384, score-0.105]
</p><p>77 Genetic algorithms are a fusion of what’s best in local search and multi-start methods (Houck et al. [sent-386, score-0.126]
</p><p>78 , 1996), exploiting a problem’s structure to combine valid parts of any partial solutions (Holland, 1975; Goldberg, 1989). [sent-387, score-0.123]
</p><p>79 Evolutionary heuristics proved useful in the induction of phonotactics (Belz, 1998), text planning (Mellish et al. [sent-388, score-0.167]
</p><p>80 , 1998), factored modeling of morphologically-rich languages (Duh and Kirchhoff, 2004) and plot induction for story generation (McIntyre and Lapata, 2010). [sent-389, score-0.167]
</p><p>81 Multi-objective genetic algorithms (Fonseca and Fleming, 1993) can handle problems with equally important but conflicting criteria (Stadler, 1988), using Pareto-optimal ensembles. [sent-390, score-0.104]
</p><p>82 This selection of text filters is a specialized case of more general “data perturbation” techniques even cycling over randomly chosen mini-batches that partition a data set helps avoid some local optima (Liang and Klein, 2009). [sent-396, score-0.193]
</p><p>83 This family of techniques has met with success in semisupervised named entity classification (Collins and Singer, 1999; Yarowsky, 1995),11 parts-of-speech induction (Clark, 2000; 2003), and language modeling (Krueger and Dayan, 2009; Bengio et al. [sent-401, score-0.167]
</p><p>84 —  12  Conclusion  We proposed several simple algorithms for combining grammars and showed their usefulness in merging the outputs of iterative and static grammar induction systems. [sent-405, score-0.437]
</p><p>85 We exploited these properties of the combiners to reconcile grammars induced by different views of data (Blum and Mitchell, 1998). [sent-410, score-0.145]
</p><p>86 The induced dependency trees can themselves also be viewed not only as directed structures but also as skeleton parses, facilitating the recovery of correct polarities for unlabeled dependency arcs. [sent-413, score-0.279]
</p><p>87 By reusing templates, as in dynamic Bayesian network (DBN) frameworks (Koller and Friedman, 11The so-called Yarowsky-cautious modification of the original algorithm for unsupervised word-sense disambiguation. [sent-414, score-0.132]
</p><p>88 —  —  Future parsing models, in grammar induction, may benefit by modeling head-dependent relations separately from direction. [sent-423, score-0.158]
</p><p>89 Unsupervised induction of tree substitution grammars for dependency parsing. [sent-483, score-0.327]
</p><p>90 Lexical heads, phrase structure and the induction of grammar. [sent-558, score-0.167]
</p><p>91 Improving unsupervised dependency parsing with richer contexts and smoothing. [sent-661, score-0.166]
</p><p>92 Comparison of genetic algorithms, random restart, and two-opt switching for solving large location-allocation problems. [sent-701, score-0.104]
</p><p>93 Corpus-based induction of syntactic structure: Models of dependency and constituency. [sent-795, score-0.259]
</p><p>94 Stop-probability estimates computed on a large corpus improve unsupervised dependency parsing. [sent-843, score-0.166]
</p><p>95 Gibbs sampling with treeness constraint in unsupervised dependency parsing. [sent-849, score-0.166]
</p><p>96 Random restarts in minimum error rate training for statistical machine translation. [sent-898, score-0.14]
</p><p>97 Simple unsupervised grammar induction from raw text with cascaded finite state models. [sent-956, score-0.399]
</p><p>98 Baby Steps: How “Less is More” in unsupervised dependency parsing. [sent-1012, score-0.166]
</p><p>99 Lateen EM: Unsupervised training with multiple objectives, applied to dependency grammar induction. [sent-1032, score-0.25]
</p><p>100 Bootstrapping dependency grammar inducers from incomplete sentence fragments via austere models. [sent-1058, score-0.412]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spitkovsky', 0.423), ('ifj', 0.214), ('induction', 0.167), ('grammar', 0.158), ('combiner', 0.153), ('kahneman', 0.141), ('restarts', 0.14), ('alshawi', 0.138), ('fj', 0.129), ('join', 0.128), ('solutions', 0.123), ('optima', 0.111), ('fragments', 0.109), ('em', 0.108), ('mare', 0.107), ('tversky', 0.106), ('genetic', 0.104), ('viterbi', 0.101), ('dependency', 0.092), ('abokrtsk', 0.088), ('dbm', 0.088), ('gifj', 0.088), ('lateen', 0.088), ('wsj', 0.084), ('iterated', 0.084), ('local', 0.082), ('transforms', 0.082), ('counts', 0.078), ('combiners', 0.077), ('mcmc', 0.077), ('transform', 0.075), ('unsupervised', 0.074), ('dcomp', 0.07), ('dda', 0.07), ('dsimp', 0.07), ('inductor', 0.07), ('ponvert', 0.07), ('selman', 0.07), ('symmetrizer', 0.07), ('gt', 0.07), ('networks', 0.069), ('grammars', 0.068), ('parse', 0.066), ('baby', 0.065), ('inputs', 0.063), ('punctuation', 0.061), ('optimizers', 0.061), ('subnetwork', 0.061), ('tabu', 0.061), ('optimization', 0.059), ('lexicalized', 0.058), ('network', 0.058), ('ci', 0.057), ('arcs', 0.056), ('starting', 0.056), ('trees', 0.056), ('accuracies', 0.056), ('dsplit', 0.053), ('glover', 0.053), ('inducers', 0.053), ('inductors', 0.053), ('saj', 0.053), ('subnetworks', 0.053), ('symmetrization', 0.052), ('operators', 0.052), ('soft', 0.052), ('incoming', 0.049), ('conll', 0.049), ('injecting', 0.046), ('cleaner', 0.046), ('fork', 0.046), ('ek', 0.046), ('hinton', 0.046), ('unlexicalized', 0.045), ('iterative', 0.044), ('search', 0.044), ('grounding', 0.043), ('heads', 0.042), ('annealing', 0.042), ('philology', 0.042), ('curriculum', 0.042), ('golland', 0.042), ('steps', 0.041), ('objectives', 0.04), ('polarities', 0.039), ('simpler', 0.039), ('smoothing', 0.038), ('could', 0.038), ('buchholz', 0.037), ('hard', 0.036), ('moves', 0.036), ('convex', 0.035), ('bhargava', 0.035), ('bpt', 0.035), ('cek', 0.035), ('corlett', 0.035), ('craik', 0.035), ('dbms', 0.035), ('diagrammed', 0.035), ('dslplit', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000017 <a title="40-tfidf-1" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: Many statistical learning problems in NLP call for local model search methods. But accuracy tends to suffer with current techniques, which often explore either too narrowly or too broadly: hill-climbers can get stuck in local optima, whereas samplers may be inefficient. We propose to arrange individual local optimizers into organized networks. Our building blocks are operators of two types: (i) transform, which suggests new places to search, via non-random restarts from already-found local optima; and (ii) join, which merges candidate solutions to find better optima. Experiments on grammar induction show that pursuing different transforms (e.g., discarding parts of a learned model or ignoring portions of training data) results in improvements. Groups of locally-optimal solutions can be further perturbed jointly, by constructing mixtures. Using these tools, we designed several modular dependency grammar induction networks of increasing complexity. Our complete sys- tem achieves 48.6% accuracy (directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data) more than 5% higher than the previous state-of-the-art. —</p><p>2 0.11851961 <a title="40-tfidf-2" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>Author: Xinyan Xiao ; Deyi Xiong</p><p>Abstract: Traditional synchronous grammar induction estimates parameters by maximizing likelihood, which only has a loose relation to translation quality. Alternatively, we propose a max-margin estimation approach to discriminatively inducing synchronous grammars for machine translation, which directly optimizes translation quality measured by BLEU. In the max-margin estimation of parameters, we only need to calculate Viterbi translations. This further facilitates the incorporation of various non-local features that are defined on the target side. We test the effectiveness of our max-margin estimation framework on a competitive hierarchical phrase-based system. Experiments show that our max-margin method significantly outperforms the traditional twostep pipeline for synchronous rule extraction by 1.3 BLEU points and is also better than previous max-likelihood estimation method.</p><p>3 0.11058 <a title="40-tfidf-3" href="./emnlp-2013-Decipherment_with_a_Million_Random_Restarts.html">54 emnlp-2013-Decipherment with a Million Random Restarts</a></p>
<p>Author: Taylor Berg-Kirkpatrick ; Dan Klein</p><p>Abstract: This paper investigates the utility and effect of running numerous random restarts when using EM to attack decipherment problems. We find that simple decipherment models are able to crack homophonic substitution ciphers with high accuracy if a large number of random restarts are used but almost completely fail with only a few random restarts. For particularly difficult homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340.</p><p>4 0.10238171 <a title="40-tfidf-4" href="./emnlp-2013-A_Convex_Alternative_to_IBM_Model_2.html">2 emnlp-2013-A Convex Alternative to IBM Model 2</a></p>
<p>Author: Andrei Simion ; Michael Collins ; Cliff Stein</p><p>Abstract: The IBM translation models have been hugely influential in statistical machine translation; they are the basis of the alignment models used in modern translation systems. Excluding IBM Model 1, the IBM translation models, and practically all variants proposed in the literature, have relied on the optimization of likelihood functions or similar functions that are non-convex, and hence have multiple local optima. In this paper we introduce a convex relaxation of IBM Model 2, and describe an optimization algorithm for the relaxation based on a subgradient method combined with exponentiated-gradient updates. Our approach gives the same level of alignment accuracy as IBM Model 2.</p><p>5 0.091913231 <a title="40-tfidf-5" href="./emnlp-2013-Adaptor_Grammars_for_Learning_Non-Concatenative_Morphology.html">19 emnlp-2013-Adaptor Grammars for Learning Non-Concatenative Morphology</a></p>
<p>Author: Jan A. Botha ; Phil Blunsom</p><p>Abstract: This paper contributes an approach for expressing non-concatenative morphological phenomena, such as stem derivation in Semitic languages, in terms of a mildly context-sensitive grammar formalism. This offers a convenient level of modelling abstraction while remaining computationally tractable. The nonparametric Bayesian framework of adaptor grammars is extended to this richer grammar formalism to propose a probabilistic model that can learn word segmentation and morpheme lexicons, including ones with discontiguous strings as elements, from unannotated data. Our experiments on Hebrew and three variants of Arabic data find that the additional expressiveness to capture roots and templates as atomic units improves the quality of concatenative segmentation and stem identification. We obtain 74% accuracy in identifying triliteral Hebrew roots, while performing morphological segmentation with an F1-score of 78. 1.</p><p>6 0.086791866 <a title="40-tfidf-6" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>7 0.079971477 <a title="40-tfidf-7" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>8 0.076214232 <a title="40-tfidf-8" href="./emnlp-2013-Dynamic_Feature_Selection_for_Dependency_Parsing.html">66 emnlp-2013-Dynamic Feature Selection for Dependency Parsing</a></p>
<p>9 0.07364969 <a title="40-tfidf-9" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>10 0.07325466 <a title="40-tfidf-10" href="./emnlp-2013-Dependency_Language_Models_for_Sentence_Completion.html">58 emnlp-2013-Dependency Language Models for Sentence Completion</a></p>
<p>11 0.073216893 <a title="40-tfidf-11" href="./emnlp-2013-Combining_PCFG-LA_Models_with_Dual_Decomposition%3A_A_Case_Study_with_Function_Labels_and_Binarization.html">50 emnlp-2013-Combining PCFG-LA Models with Dual Decomposition: A Case Study with Function Labels and Binarization</a></p>
<p>12 0.071259573 <a title="40-tfidf-12" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>13 0.066261984 <a title="40-tfidf-13" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>14 0.065627903 <a title="40-tfidf-14" href="./emnlp-2013-Improving_Alignment_of_System_Combination_by_Using_Multi-objective_Optimization.html">101 emnlp-2013-Improving Alignment of System Combination by Using Multi-objective Optimization</a></p>
<p>15 0.062864155 <a title="40-tfidf-15" href="./emnlp-2013-Inducing_Document_Plans_for_Concept-to-Text_Generation.html">106 emnlp-2013-Inducing Document Plans for Concept-to-Text Generation</a></p>
<p>16 0.060292166 <a title="40-tfidf-16" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>17 0.059700314 <a title="40-tfidf-17" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>18 0.059244748 <a title="40-tfidf-18" href="./emnlp-2013-Unsupervised_Spectral_Learning_of_WCFG_as_Low-rank_Matrix_Completion.html">195 emnlp-2013-Unsupervised Spectral Learning of WCFG as Low-rank Matrix Completion</a></p>
<p>19 0.058557145 <a title="40-tfidf-19" href="./emnlp-2013-A_Multi-Teraflop_Constituency_Parser_using_GPUs.html">10 emnlp-2013-A Multi-Teraflop Constituency Parser using GPUs</a></p>
<p>20 0.057651259 <a title="40-tfidf-20" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.225), (1, -0.053), (2, 0.02), (3, 0.01), (4, -0.108), (5, 0.014), (6, 0.036), (7, 0.018), (8, -0.026), (9, 0.087), (10, -0.024), (11, -0.042), (12, -0.049), (13, 0.059), (14, 0.064), (15, -0.074), (16, -0.01), (17, 0.058), (18, 0.05), (19, -0.024), (20, 0.037), (21, -0.007), (22, 0.093), (23, 0.055), (24, -0.003), (25, -0.01), (26, -0.134), (27, -0.068), (28, 0.189), (29, -0.228), (30, -0.111), (31, -0.087), (32, 0.064), (33, 0.057), (34, 0.033), (35, 0.018), (36, -0.005), (37, 0.055), (38, 0.005), (39, -0.191), (40, -0.185), (41, -0.06), (42, 0.015), (43, 0.097), (44, -0.02), (45, -0.024), (46, -0.032), (47, 0.104), (48, 0.086), (49, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93979764 <a title="40-lsi-1" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: Many statistical learning problems in NLP call for local model search methods. But accuracy tends to suffer with current techniques, which often explore either too narrowly or too broadly: hill-climbers can get stuck in local optima, whereas samplers may be inefficient. We propose to arrange individual local optimizers into organized networks. Our building blocks are operators of two types: (i) transform, which suggests new places to search, via non-random restarts from already-found local optima; and (ii) join, which merges candidate solutions to find better optima. Experiments on grammar induction show that pursuing different transforms (e.g., discarding parts of a learned model or ignoring portions of training data) results in improvements. Groups of locally-optimal solutions can be further perturbed jointly, by constructing mixtures. Using these tools, we designed several modular dependency grammar induction networks of increasing complexity. Our complete sys- tem achieves 48.6% accuracy (directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data) more than 5% higher than the previous state-of-the-art. —</p><p>2 0.76666528 <a title="40-lsi-2" href="./emnlp-2013-Decipherment_with_a_Million_Random_Restarts.html">54 emnlp-2013-Decipherment with a Million Random Restarts</a></p>
<p>Author: Taylor Berg-Kirkpatrick ; Dan Klein</p><p>Abstract: This paper investigates the utility and effect of running numerous random restarts when using EM to attack decipherment problems. We find that simple decipherment models are able to crack homophonic substitution ciphers with high accuracy if a large number of random restarts are used but almost completely fail with only a few random restarts. For particularly difficult homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340.</p><p>3 0.652839 <a title="40-lsi-3" href="./emnlp-2013-Unsupervised_Spectral_Learning_of_WCFG_as_Low-rank_Matrix_Completion.html">195 emnlp-2013-Unsupervised Spectral Learning of WCFG as Low-rank Matrix Completion</a></p>
<p>Author: Raphael Bailly ; Xavier Carreras ; Franco M. Luque ; Ariadna Quattoni</p><p>Abstract: We derive a spectral method for unsupervised learning of Weighted Context Free Grammars. We frame WCFG induction as finding a Hankel matrix that has low rank and is linearly constrained to represent a function computed by inside-outside recursions. The proposed algorithm picks the grammar that agrees with a sample and is the simplest with respect to the nuclear norm of the Hankel matrix.</p><p>4 0.56463277 <a title="40-lsi-4" href="./emnlp-2013-A_Convex_Alternative_to_IBM_Model_2.html">2 emnlp-2013-A Convex Alternative to IBM Model 2</a></p>
<p>Author: Andrei Simion ; Michael Collins ; Cliff Stein</p><p>Abstract: The IBM translation models have been hugely influential in statistical machine translation; they are the basis of the alignment models used in modern translation systems. Excluding IBM Model 1, the IBM translation models, and practically all variants proposed in the literature, have relied on the optimization of likelihood functions or similar functions that are non-convex, and hence have multiple local optima. In this paper we introduce a convex relaxation of IBM Model 2, and describe an optimization algorithm for the relaxation based on a subgradient method combined with exponentiated-gradient updates. Our approach gives the same level of alignment accuracy as IBM Model 2.</p><p>5 0.56129235 <a title="40-lsi-5" href="./emnlp-2013-Combining_PCFG-LA_Models_with_Dual_Decomposition%3A_A_Case_Study_with_Function_Labels_and_Binarization.html">50 emnlp-2013-Combining PCFG-LA Models with Dual Decomposition: A Case Study with Function Labels and Binarization</a></p>
<p>Author: Joseph Le Roux ; Antoine Rozenknop ; Jennifer Foster</p><p>Abstract: It has recently been shown that different NLP models can be effectively combined using dual decomposition. In this paper we demonstrate that PCFG-LA parsing models are suitable for combination in this way. We experiment with the different models which result from alternative methods of extracting a grammar from a treebank (retaining or discarding function labels, left binarization versus right binarization) and achieve a labeled Parseval F-score of 92.4 on Wall Street Journal Section 23 this represents an absolute improvement of 0.7 and an error reduction rate of 7% over a strong PCFG-LA product-model baseline. Although we experiment only with binarization and function labels in this study, there is much scope for applying this approach to – other grammar extraction strategies.</p><p>6 0.52080077 <a title="40-lsi-6" href="./emnlp-2013-Learning_to_Freestyle%3A_Hip_Hop_Challenge-Response_Induction_via_Transduction_Rule_Segmentation.html">122 emnlp-2013-Learning to Freestyle: Hip Hop Challenge-Response Induction via Transduction Rule Segmentation</a></p>
<p>7 0.50303739 <a title="40-lsi-7" href="./emnlp-2013-A_Multi-Teraflop_Constituency_Parser_using_GPUs.html">10 emnlp-2013-A Multi-Teraflop Constituency Parser using GPUs</a></p>
<p>8 0.48615283 <a title="40-lsi-8" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>9 0.46414447 <a title="40-lsi-9" href="./emnlp-2013-Inducing_Document_Plans_for_Concept-to-Text_Generation.html">106 emnlp-2013-Inducing Document Plans for Concept-to-Text Generation</a></p>
<p>10 0.45515051 <a title="40-lsi-10" href="./emnlp-2013-A_Synchronous_Context_Free_Grammar_for_Time_Normalization.html">14 emnlp-2013-A Synchronous Context Free Grammar for Time Normalization</a></p>
<p>11 0.43593445 <a title="40-lsi-11" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>12 0.4265371 <a title="40-lsi-12" href="./emnlp-2013-Adaptor_Grammars_for_Learning_Non-Concatenative_Morphology.html">19 emnlp-2013-Adaptor Grammars for Learning Non-Concatenative Morphology</a></p>
<p>13 0.40855098 <a title="40-lsi-13" href="./emnlp-2013-Joint_Parsing_and_Disfluency_Detection_in_Linear_Time.html">116 emnlp-2013-Joint Parsing and Disfluency Detection in Linear Time</a></p>
<p>14 0.40607533 <a title="40-lsi-14" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>15 0.39111194 <a title="40-lsi-15" href="./emnlp-2013-Dependency_Language_Models_for_Sentence_Completion.html">58 emnlp-2013-Dependency Language Models for Sentence Completion</a></p>
<p>16 0.37553746 <a title="40-lsi-16" href="./emnlp-2013-Dynamic_Feature_Selection_for_Dependency_Parsing.html">66 emnlp-2013-Dynamic Feature Selection for Dependency Parsing</a></p>
<p>17 0.3725186 <a title="40-lsi-17" href="./emnlp-2013-Ubertagging%3A_Joint_Segmentation_and_Supertagging_for_English.html">190 emnlp-2013-Ubertagging: Joint Segmentation and Supertagging for English</a></p>
<p>18 0.36479056 <a title="40-lsi-18" href="./emnlp-2013-With_Blinkers_on%3A_Robust_Prediction_of_Eye_Movements_across_Readers.html">203 emnlp-2013-With Blinkers on: Robust Prediction of Eye Movements across Readers</a></p>
<p>19 0.36411095 <a title="40-lsi-19" href="./emnlp-2013-Improving_Alignment_of_System_Combination_by_Using_Multi-objective_Optimization.html">101 emnlp-2013-Improving Alignment of System Combination by Using Multi-objective Optimization</a></p>
<p>20 0.36360431 <a title="40-lsi-20" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.028), (18, 0.035), (22, 0.04), (24, 0.289), (30, 0.106), (45, 0.013), (50, 0.029), (51, 0.134), (66, 0.046), (71, 0.029), (75, 0.032), (77, 0.033), (90, 0.011), (95, 0.013), (96, 0.024), (97, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77029002 <a title="40-lda-1" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: Many statistical learning problems in NLP call for local model search methods. But accuracy tends to suffer with current techniques, which often explore either too narrowly or too broadly: hill-climbers can get stuck in local optima, whereas samplers may be inefficient. We propose to arrange individual local optimizers into organized networks. Our building blocks are operators of two types: (i) transform, which suggests new places to search, via non-random restarts from already-found local optima; and (ii) join, which merges candidate solutions to find better optima. Experiments on grammar induction show that pursuing different transforms (e.g., discarding parts of a learned model or ignoring portions of training data) results in improvements. Groups of locally-optimal solutions can be further perturbed jointly, by constructing mixtures. Using these tools, we designed several modular dependency grammar induction networks of increasing complexity. Our complete sys- tem achieves 48.6% accuracy (directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data) more than 5% higher than the previous state-of-the-art. —</p><p>2 0.6530298 <a title="40-lda-2" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>Author: Andrew J. Anderson ; Elia Bruni ; Ulisse Bordignon ; Massimo Poesio ; Marco Baroni</p><p>Abstract: Traditional distributional semantic models extract word meaning representations from cooccurrence patterns of words in text corpora. Recently, the distributional approach has been extended to models that record the cooccurrence of words with visual features in image collections. These image-based models should be complementary to text-based ones, providing a more cognitively plausible view of meaning grounded in visual perception. In this study, we test whether image-based models capture the semantic patterns that emerge from fMRI recordings of the neural signal. Our results indicate that, indeed, there is a significant correlation between image-based and brain-based semantic similarities, and that image-based models complement text-based ones, so that the best correlations are achieved when the two modalities are combined. Despite some unsatisfactory, but explained out- comes (in particular, failure to detect differential association of models with brain areas), the results show, on the one hand, that imagebased distributional semantic models can be a precious new tool to explore semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility.</p><p>3 0.54550457 <a title="40-lda-3" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>Author: Xiaoqing Zheng ; Hanyang Chen ; Tianyu Xu</p><p>Abstract: This study explores the feasibility of performing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representation of Chinese characters, and use these improved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-theart performance with minimal computational cost. We also describe a perceptron-style algorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented.</p><p>4 0.53801262 <a title="40-lda-4" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>Author: Jesus Gonzalez-Rubio ; Daniel Ortiz-Martinez ; Jose-Miguel Benedi ; Francisco Casacuberta</p><p>Abstract: Current automatic machine translation systems are not able to generate error-free translations and human intervention is often required to correct their output. Alternatively, an interactive framework that integrates the human knowledge into the translation process has been presented in previous works. Here, we describe a new interactive machine translation approach that is able to work with phrase-based and hierarchical translation models, and integrates error-correction all in a unified statistical framework. In our experiments, our approach outperforms previous interactive translation systems, and achieves estimated effort reductions of as much as 48% relative over a traditional post-edition system.</p><p>5 0.53794676 <a title="40-lda-5" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>Author: Margaret Mitchell ; Jacqui Aguilar ; Theresa Wilson ; Benjamin Van Durme</p><p>Abstract: We propose a novel approach to sentiment analysis for a low resource setting. The intuition behind this work is that sentiment expressed towards an entity, targeted sentiment, may be viewed as a span of sentiment expressed across the entity. This representation allows us to model sentiment detection as a sequence tagging problem, jointly discovering people and organizations along with whether there is sentiment directed towards them. We compare performance in both Spanish and English on microblog data, using only a sentiment lexicon as an external resource. By leveraging linguisticallyinformed features within conditional random fields (CRFs) trained to minimize empirical risk, our best models in Spanish significantly outperform a strong baseline, and reach around 90% accuracy on the combined task of named entity recognition and sentiment prediction. Our models in English, trained on a much smaller dataset, are not yet statistically significant against their baselines.</p><p>6 0.53752142 <a title="40-lda-6" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>7 0.53716141 <a title="40-lda-7" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>8 0.53603888 <a title="40-lda-8" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>9 0.53552783 <a title="40-lda-9" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>10 0.53326368 <a title="40-lda-10" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>11 0.5314424 <a title="40-lda-11" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>12 0.53040707 <a title="40-lda-12" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>13 0.53038067 <a title="40-lda-13" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>14 0.53024083 <a title="40-lda-14" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>15 0.52929443 <a title="40-lda-15" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>16 0.52756637 <a title="40-lda-16" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>17 0.52568829 <a title="40-lda-17" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>18 0.52433074 <a title="40-lda-18" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>19 0.52388448 <a title="40-lda-19" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>20 0.52368599 <a title="40-lda-20" href="./emnlp-2013-Dynamic_Feature_Selection_for_Dependency_Parsing.html">66 emnlp-2013-Dynamic Feature Selection for Dependency Parsing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
