<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>50 emnlp-2013-Combining PCFG-LA Models with Dual Decomposition: A Case Study with Function Labels and Binarization</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-50" href="#">emnlp2013-50</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>50 emnlp-2013-Combining PCFG-LA Models with Dual Decomposition: A Case Study with Function Labels and Binarization</h1>
<br/><p>Source: <a title="emnlp-2013-50-pdf" href="http://aclweb.org/anthology//D/D13/D13-1116.pdf">pdf</a></p><p>Author: Joseph Le Roux ; Antoine Rozenknop ; Jennifer Foster</p><p>Abstract: It has recently been shown that different NLP models can be effectively combined using dual decomposition. In this paper we demonstrate that PCFG-LA parsing models are suitable for combination in this way. We experiment with the different models which result from alternative methods of extracting a grammar from a treebank (retaining or discarding function labels, left binarization versus right binarization) and achieve a labeled Parseval F-score of 92.4 on Wall Street Journal Section 23 this represents an absolute improvement of 0.7 and an error reduction rate of 7% over a strong PCFG-LA product-model baseline. Although we experiment only with binarization and function labels in this study, there is much scope for applying this approach to – other grammar extraction strategies.</p><p>Reference: <a title="emnlp-2013-50-reference" href="../emnlp2013_reference/emnlp-2013-Combining_PCFG-LA_Models_with_Dual_Decomposition%3A_A_Case_Study_with_Function_Labels_and_Binarization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ie  Abstract It has recently been shown that different NLP models can be effectively combined using dual decomposition. [sent-7, score-0.182]
</p><p>2 We experiment with the different models which result from alternative methods of extracting a grammar from a treebank (retaining or discarding function labels, left binarization versus right binarization) and achieve a labeled Parseval F-score of 92. [sent-9, score-0.625]
</p><p>3 Although we experiment only with binarization and function labels in this study, there is much scope for applying this approach to –  other grammar extraction strategies. [sent-12, score-0.47]
</p><p>4 1 Introduction  Because of the large amount of possibly contradictory information contained in a treebank, learning a phrase-structure-based parser implies making several choices regarding the prevalent annotations which have to be kept or discarded in order to guide the learning algorithm. [sent-13, score-0.136]
</p><p>5 These choices, which include whether to keep function labels and empty nodes, how to binarize the trees and whether to alter the granularity of the tagset, are often motivated empirically by parsing performance rather than by the different aspects of the language they may be able to capture. [sent-14, score-0.269]
</p><p>6 We want to mix different parsers trained on different versions of a treebank each of which makes some annotation choices in order to learn more specific or richer information. [sent-20, score-0.194]
</p><p>7 We will use state-of-the-art unlexicalized probabilistic contextfree grammars with latent annotations (PCFG-LA) in order to compare our approach with a strong baseline of high-quality parses. [sent-21, score-0.149]
</p><p>8 The systems being combined make different choices with regard  to i) function labels and ii) grammar binarization. [sent-23, score-0.213]
</p><p>9 Common sense would suggest that information in the form of function labels syntactic labels such as SBJ and PRD and semantic labels such as TMP and LOC might help in obtaining a fine-grained analysis. [sent-24, score-0.237]
</p><p>10 oc d2s0 i1n3 N Aastusorcaila Ltiaon g fuoarg Ceo Pmrpoucetastsi on ga,l p Laignegsu 1is1t5ic8s–1 69, sis on which CFGs rely and on which most popular parsers are based may be too strong to learn the dependencies between functions across the parse trees. [sent-27, score-0.256]
</p><p>11 Also, the number of parameters increases with the use of function labels and this can affect the learning process. [sent-28, score-0.117]
</p><p>12 At first glance, binarization need not be an issue, as CFGs admit a binarized form recognizing exactly the same language. [sent-29, score-0.37]
</p><p>13 But binarization can be associated with horizontal markovization and in this case the recognized language will differ. [sent-30, score-0.371]
</p><p>14 3 on Section 23 of the Wall Street Journal) are obtained by combining three grammars which encode different function label/binarization decisions. [sent-36, score-0.219]
</p><p>15 § o3r presents approximate P 2C rFevGi-eLwAs parsers as rlikn. [sent-39, score-0.133]
</p><p>16 ea §r models, ws ahpilepr § 4im sahotew PsC hFoGw- we can use adsu lainl decomposition to d §er 4ive sh an algorithm for combining these models. [sent-40, score-0.175]
</p><p>17 2  Related Work  Parser Model Combination It is well known that improved parsing performance can be achieved by 1159 leveraging the alternative perspectives provided by several parsing models rather than relying on just one. [sent-42, score-0.206]
</p><p>18 , 2010) or several nonprojective dependency parsing models (Koo et al. [sent-45, score-0.137]
</p><p>19 , 2011), and using expectation propagation, a related approach to dual decomposition, to combine lexicalized, unlexicalized and PCFG-LA models (Hall and Klein, 2012). [sent-47, score-0.182]
</p><p>20 In this  last example, the models must factor in the same way: in other words, the grammars must use the same binarization scheme. [sent-48, score-0.405]
</p><p>21 In our study, we employ PCFG-LA product models with dual decomposition, and we relax the constraints on factorization, as we require only a loose coupling of the models. [sent-49, score-0.268]
</p><p>22 Function Label Parsing Although function labels have been available in the Penn Treebank (PTB) for almost twenty years (Marcus et al. [sent-50, score-0.117]
</p><p>23 , 1994), they have been to a large extent overlooked in English parsing research most studies that report parsing results on Section 23 of the Wall Street Journal (WSJ) use parsing models that are trained on a version of the WSJ trees where the function labels have been removed. [sent-51, score-0.475]
</p><p>24 (2006) who each trained a parsing model on a version of the PTB with function labels intact. [sent-53, score-0.22]
</p><p>25 (2006) found that parsing accuracy was not affected by keeping the function labels. [sent-55, score-0.16]
</p><p>26 There have also been attempts to use machine learning to recover the function labels post-parsing (Blaheta and Charniak, 2000; Chrupala et al. [sent-56, score-0.117]
</p><p>27 We recover function labels as part of the parsing process, and use dual decomposition to —  combine parsing models with and without function labels. [sent-58, score-0.697]
</p><p>28 (2005) compare binarization strategies for PCFG-LA parsing, and conclude that the differences between them have a minor effect on parsing accuracy as the number of latent annotations increases beyond two. [sent-61, score-0.413]
</p><p>29 Hall and Klein (2012) are forced to use head binarization when combining their lexicalized and unlexicalized parsers. [sent-62, score-0.323]
</p><p>30 Dual decomposition allows us to combine models with different binarization schemes. [sent-63, score-0.418]
</p><p>31 3  Approximation of PCFG-LAs as Linear Models  In this section, we explain how we can use PCFGLAs to devise linear models suitable for the dual decomposition framework. [sent-64, score-0.317]
</p><p>32 With such a grammar G we can define probabilities over trees in the following way. [sent-75, score-0.119]
</p><p>33 We will consider two types of trees, annotated trees and skeletal  trees. [sent-76, score-0.235]
</p><p>34 An annotated tree is a sequence of rules from RH, while a skeletal tree is a sequence of skeletal rRules from R. [sent-77, score-0.526]
</p><p>35 1160 p(TH) =  Y  p(r)  (1)  r∈YTH We define a projection ρ from annotated trees to skeletal trees. [sent-80, score-0.235]
</p><p>36 The probability of a skeletal tree T is a sum of the probabilities of all annotated trees that admit T as their projection. [sent-82, score-0.324]
</p><p>37 More precisely, they find a PCFG that only recognizes the input sentence where the probabilities q(rs) of the rules are set according to their marginal  probabilities in the original PCFG-LA parse forest. [sent-90, score-0.118]
</p><p>38 The parameters rs are skeletal rules with span information. [sent-91, score-0.253]
</p><p>39 In both cases, the probability of a skeletal tree now becomes a simple product of parameters associated with anchored skeletal rules. [sent-122, score-0.611]
</p><p>40 The parsing problem becomes2:  T∗  = = =  argTmaxrsY∈Tq(rs) argTmaxrXs∈Tlogq(rs)  argTmaxrXs∈Fwrs· 1{rs∈ T} = argTmaxσ(T) Thus, from a PCFG-LA we are able to define a linear model whose parameters are the logprobabilities of the rules in distribution q. [sent-127, score-0.139]
</p><p>41 Fortunately, this can be partly overcome by combining grammars that only differ on the initial parameterization of the EM algorithm. [sent-131, score-0.162]
</p><p>42 The probability of a skeletal tree is the product of the probabilities assigned by each single grammar Gi. [sent-132, score-0.371]
</p><p>43 T∗  =  Yn  argTmaxiY=1qGi(T)  (4)  Since grammars only differ by their numerical parameters (i. [sent-133, score-0.122]
</p><p>44 skeletal rules are the same), inference can be efficiently implemented using dynamic programming (Petrov, 2010). [sent-135, score-0.222]
</p><p>45 Scoring with n such grammars now becomes:  T∗  =  Xn  argTmaxXi=1XlogqGi(r) XXn  (5)  =  argTmaxXXi=1logqGi(r)  (6)  The distributions qGi still have to be computed independently and possibly in parallel but the final decoding can be performed jointly. [sent-136, score-0.122]
</p><p>46 This is still a linear model for PCFG-LA parsing, but restricted to grammars that share the same skeletal rules. [sent-137, score-0.308]
</p><p>47 4 Dual Decomposition –  –  In this section, we show how we derive an algorithm to work out the best parse according to a set of n grammars that do not share the exact same skeletal rules. [sent-138, score-0.355]
</p><p>48 As such, the grammars’ product cannot be easily conducted inside the parser to produce and score a same and unique best tree, and we now consider a c(ompound)-parse as a tuple (T1 . [sent-139, score-0.139]
</p><p>49 Each grammar Gi is responsible for scoring tree Ti, and we seek to obtain the c-parse that maximizes the sum of the scores of its different trees. [sent-143, score-0.129]
</p><p>50 1 Compound Parse Consistency Let us suppose we have a set of phrase-structure parsers trained on different versions of the same treebank. [sent-146, score-0.133]
</p><p>51 Hence, some elements in the charts will either be the same or can be mapped to each other provided an equivalence relation and we define consensus between parsers on these elements. [sent-147, score-0.193]
</p><p>52 When the grammar is not functionally annotated, phrase-structure trees can be decomposed into a set of anchored (syntactical) categories Xs, asserting that a category X is in the tree at position3 s. [sent-148, score-0.327]
</p><p>53 Thus, such a tree T can be described by means of a boolean vector z(T) indexed by anchored labels Xs, where z(T)Xs = 1if Xs is in T and 0 otherwise. [sent-149, score-0.275]
</p><p>54 We will differentiate the set of natural nonterminals that occur in the treebanks from the set of artificial non-terminals that do not occur in the treebank and are the results of a binarization with markovization. [sent-150, score-0.318]
</p><p>55 As these artificial non-terminals disappear after reversing binarization in solution trees, they do not play any role in the consensus between parsers, and we only consider natural non-terminals in the set of anchored labels. [sent-151, score-0.433]
</p><p>56 When the grammar is functionally annotated, each label X¯ in a tree is a pair (X, F), where X  is a syntactical category and F is a function label. [sent-152, score-0.261]
</p><p>57 In case the grammar contains unary nonlexical rules, the anchor also discriminates the different positions in a sequence of unary rules. [sent-154, score-0.146]
</p><p>58 1162 non-functional grammars, we decompose such a tree into two sets: a set of anchored categories Xs and a set of anchored function labels Fs. [sent-155, score-0.424]
</p><p>59 Tn) is said to be consistent iff every tree shares the same set of anchored categories, i. [sent-160, score-0.183]
</p><p>60 We follow the presentation of the decomposition from (Martins et al. [sent-164, score-0.135]
</p><p>61 , 2011) to explain how we can combine several PCFG-LA parsers together. [sent-165, score-0.133]
</p><p>62 Fi (s) is the set of trees in grammar Gi w Fh(osse) yields are the input sentence s. [sent-175, score-0.119]
</p><p>63 This decomposition is a two-step process: Step 1(Relaxation): the coupling constraints (10) are removed by introducing a vector of Lagrange multipliers Λi = (λi,Xs )Xs for each parser i, indexed by anchored categories Xs, and writing the equivalent problem:  (RP) :  oRP  = u,mT1a. [sent-197, score-0.433]
</p><p>64 Step 2 (dualization): the dual problem (LP) is obtained by permuting max and min in (RP): 1163  (LP1) :  oLP  = mΛinum,Ta1. [sent-214, score-0.182]
</p><p>65 The dual problem becomes: Xn  (LP) :oLP= mΛinXi=1Tmi∈aFxi(σi(Ti) + z(Ti) · Λi) s. [sent-229, score-0.182]
</p><p>66 updates are zeros for a natural span if the parsers agree on it). [sent-237, score-0.159]
</p><p>67 4 All grammars are trained using 6 split/merge EM cycles. [sent-267, score-0.122]
</p><p>68 The basic settings are a combination of the two following parameters: left or right binarization: we conjecture that this affects the quality of the parsers by impacting the recognition of left and right constituent frontiers. [sent-275, score-0.493]
</p><p>69 We set vertical markovization to 1 (no parent annotation) and horizontal markovization to 0 (we drop all left/right annotations). [sent-276, score-0.15]
</p><p>70 We test the 4 different settings on the development set, using a single grammar or a product of n grammars. [sent-280, score-0.126]
</p><p>71 We can see that right binarization performs better than left binarization. [sent-282, score-0.463]
</p><p>72 (2006), function labels are detrimental for parsing performance for one grammar only. [sent-284, score-0.29]
</p><p>73 However, they do not penalize performance when using the product model with 8 grammars or more. [sent-285, score-0.178]
</p><p>74 Figure 3: F1 for products of n grammars on the dev. [sent-286, score-0.167]
</p><p>75 On the other hand, as differences between left and right binarization settings remain over all product sizes, right binarization seems more useful on its own. [sent-289, score-0.888]
</p><p>76 The first part of Table 1 gives F-score and Exact Match results of the product models with 16 grammars on the development set. [sent-290, score-0.178]
</p><p>77 Unsurprisingly, the best configuration is the one combining the two best product systems (with right binarization) but all combined systems perform better than their single components. [sent-296, score-0.182]
</p><p>78 We also combine 3 and 4 parsers to see if combining the above DD Right Bin setting with information that could improve the recognition of beginning of constituents can be helpful. [sent-318, score-0.173]
</p><p>79 We have 2 settings:  DD3 The 2 right-binarized parsers combined with the left binarized parser without functions, 1165 DD4 The 4 parsers together. [sent-319, score-0.5]
</p><p>80 In both cases the system returns the rightbinarized function annotated parse. [sent-320, score-0.117]
</p><p>81 We compare the results obtained directly from the parser output with results obtained with Funtag, a state-of-the-art functional tagger that is applied on parser output, using a gold model trained on sections 02 to 21 of the WSJ (Chrupala et al. [sent-335, score-0.166]
</p><p>82 Second, the quality of the Funtag prediction seems to be influenced by the fact that parser already handle functions and by the accuracy of the parser (Parseval F-score). [sent-358, score-0.197]
</p><p>83 On the other hand, this is not the case with parser predicted functions, where the best system is the right-binarized product model with functions, with very similar performance obtained by the combinations consisting of 2 function parsers, settings DD Func and DD4. [sent-360, score-0.196]
</p><p>84 This tends to indicate that the constraints we have set to define consistencies in c-parses, focusing on syntactical categories, do not help in retrieving better function labels. [sent-361, score-0.107]
</p><p>85 This suggests some possible further improvements where parsers with functional annotations should be forced  to agree on these too. [sent-362, score-0.186]
</p><p>86 5  Evaluation of Dependencies  Dependency-based evaluation of phrase structure parser output has been used in recent years to provide a more rounded view on parser performance and to compare with direct dependency parsers (Cer et al. [sent-364, score-0.333]
</p><p>87 We evaluate our various parsing models on their ability to recover three types of dependencies: basic  Stanford dependencies (de Marneffe and Manning, 2008)5, LTH dependencies (Johansson and Nugues, 5We used the latest version  at  the time of writing, i. [sent-369, score-0.193]
</p><p>88 The results, shown in Table 4, mirror the constituency evaluation results in that the dual decomposition results tend to outperform the basic product model results, and combining three or four grammars using dual decomposition yields the highest scores. [sent-375, score-0.852]
</p><p>89 The tool used to produce Stanford dependencies has been designed to work with phrase structure trees that do not contain function labels. [sent-377, score-0.151]
</p><p>90 By retaining function labels during parsing, we have shown that LTH dependencies can be recovered with a high level of accuracy without having to resort to a post-parsing function labeling step. [sent-380, score-0.219]
</p><p>91 We present parser accuracy results, measured using Parseval F-score and penn2malt UAS, and, for our systems, function label accuracy for labels produced during parsing and after parsing using Funtag. [sent-383, score-0.406]
</p><p>92 However, to facilitate comparison with other parsers and dependency schemes, we did not use it in our experiments. [sent-390, score-0.167]
</p><p>93 , 2009) (Petrov, 2010) (Huang, 2008) (Bohnet and Nivre, 2012)  score  trees and the TSG  parser  92. [sent-429, score-0.132]
</p><p>94 It is approximately  3 times slower than the  slowest product model (left binarization with function labels) and 7 slower than the fastest one (right binarization without function labels). [sent-458, score-0.736]
</p><p>95 , 2012) that can take into account larger tree fragments: this would indicate that by combining our parsers we extend the domain of locality, horizontally with binarization schemes and vertically with function labels. [sent-465, score-0.572]
</p><p>96 We also include for comparison state-of-the-art dependency parsing results (Bohnet and Nivre, 2012). [sent-468, score-0.137]
</p><p>97 more linguistically motivated binarization strategies or of particular interest to us annotation of empty elements. [sent-472, score-0.283]
</p><p>98 A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing. [sent-590, score-0.381]
</p><p>99 On dual decomposition and linear programming relaxations for natural language processing. [sent-594, score-0.317]
</p><p>100 Dependency parsing and domain adaptation with LR models and parser ensembles. [sent-599, score-0.186]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('func', 0.642), ('dd', 0.293), ('binarization', 0.283), ('skeletal', 0.186), ('dual', 0.182), ('decomposition', 0.135), ('parsers', 0.133), ('anchored', 0.124), ('grammars', 0.122), ('bin', 0.111), ('parsing', 0.103), ('left', 0.094), ('xs', 0.087), ('right', 0.086), ('petrov', 0.085), ('parser', 0.083), ('lth', 0.08), ('hsi', 0.079), ('rush', 0.072), ('parseval', 0.071), ('grammar', 0.07), ('ti', 0.068), ('markovization', 0.062), ('labels', 0.06), ('tree', 0.059), ('funtag', 0.057), ('gabbard', 0.057), ('olp', 0.057), ('binarized', 0.057), ('function', 0.057), ('product', 0.056), ('nivre', 0.05), ('syntactical', 0.05), ('trees', 0.049), ('parse', 0.047), ('foster', 0.047), ('dependencies', 0.045), ('products', 0.045), ('variational', 0.043), ('bcde', 0.043), ('komodakis', 0.043), ('slave', 0.043), ('martins', 0.04), ('combining', 0.04), ('lagrangian', 0.038), ('ipn', 0.037), ('chrupala', 0.037), ('roux', 0.037), ('shindo', 0.037), ('rules', 0.036), ('treebank', 0.035), ('rp', 0.035), ('recognizes', 0.035), ('joakim', 0.034), ('charts', 0.034), ('dependency', 0.034), ('op', 0.033), ('indexed', 0.032), ('rs', 0.031), ('functions', 0.031), ('returns', 0.031), ('koo', 0.03), ('rh', 0.03), ('wsj', 0.03), ('coupling', 0.03), ('admit', 0.03), ('ptb', 0.03), ('compound', 0.029), ('argtmaxrxs', 0.029), ('attia', 0.029), ('cruder', 0.029), ('deirdre', 0.029), ('lorg', 0.029), ('multipliers', 0.029), ('rightbinarized', 0.029), ('rrules', 0.029), ('matsuzaki', 0.028), ('bohnet', 0.028), ('penn', 0.028), ('pi', 0.028), ('slav', 0.027), ('lp', 0.027), ('gi', 0.027), ('sagae', 0.027), ('uas', 0.027), ('annotations', 0.027), ('relaxation', 0.026), ('consensus', 0.026), ('anchor', 0.026), ('horizontal', 0.026), ('choices', 0.026), ('agree', 0.026), ('stanford', 0.026), ('xn', 0.025), ('unary', 0.025), ('merlo', 0.025), ('cfgs', 0.025), ('functionally', 0.025), ('blaheta', 0.025), ('fto', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="50-tfidf-1" href="./emnlp-2013-Combining_PCFG-LA_Models_with_Dual_Decomposition%3A_A_Case_Study_with_Function_Labels_and_Binarization.html">50 emnlp-2013-Combining PCFG-LA Models with Dual Decomposition: A Case Study with Function Labels and Binarization</a></p>
<p>Author: Joseph Le Roux ; Antoine Rozenknop ; Jennifer Foster</p><p>Abstract: It has recently been shown that different NLP models can be effectively combined using dual decomposition. In this paper we demonstrate that PCFG-LA parsing models are suitable for combination in this way. We experiment with the different models which result from alternative methods of extracting a grammar from a treebank (retaining or discarding function labels, left binarization versus right binarization) and achieve a labeled Parseval F-score of 92.4 on Wall Street Journal Section 23 this represents an absolute improvement of 0.7 and an error reduction rate of 7% over a strong PCFG-LA product-model baseline. Although we experiment only with binarization and function labels in this study, there is much scope for applying this approach to – other grammar extraction strategies.</p><p>2 0.10030627 <a title="50-tfidf-2" href="./emnlp-2013-Optimal_Incremental_Parsing_via_Best-First_Dynamic_Programming.html">146 emnlp-2013-Optimal Incremental Parsing via Best-First Dynamic Programming</a></p>
<p>Author: Kai Zhao ; James Cross ; Liang Huang</p><p>Abstract: We present the first provably optimal polynomial time dynamic programming (DP) algorithm for best-first shift-reduce parsing, which applies the DP idea of Huang and Sagae (2010) to the best-first parser of Sagae and Lavie (2006) in a non-trivial way, reducing the complexity of the latter from exponential to polynomial. We prove the correctness of our algorithm rigorously. Experiments confirm that DP leads to a significant speedup on a probablistic best-first shift-reduce parser, and makes exact search under such a model tractable for the first time.</p><p>3 0.090283483 <a title="50-tfidf-3" href="./emnlp-2013-Semi-Supervised_Feature_Transformation_for_Dependency_Parsing.html">168 emnlp-2013-Semi-Supervised Feature Transformation for Dependency Parsing</a></p>
<p>Author: Wenliang Chen ; Min Zhang ; Yue Zhang</p><p>Abstract: In current dependency parsing models, conventional features (i.e. base features) defined over surface words and part-of-speech tags in a relatively high-dimensional feature space may suffer from the data sparseness problem and thus exhibit less discriminative power on unseen data. In this paper, we propose a novel semi-supervised approach to addressing the problem by transforming the base features into high-level features (i.e. meta features) with the help of a large amount of automatically parsed data. The meta features are used together with base features in our final parser. Our studies indicate that our proposed approach is very effective in processing unseen data and features. Experiments on Chinese and English data sets show that the final parser achieves the best-reported accuracy on the Chinese data and comparable accuracy with the best known parsers on the English data.</p><p>4 0.089113951 <a title="50-tfidf-4" href="./emnlp-2013-Dynamic_Feature_Selection_for_Dependency_Parsing.html">66 emnlp-2013-Dynamic Feature Selection for Dependency Parsing</a></p>
<p>Author: He He ; Hal Daume III ; Jason Eisner</p><p>Abstract: Feature computation and exhaustive search have significantly restricted the speed of graph-based dependency parsing. We propose a faster framework of dynamic feature selection, where features are added sequentially as needed, edges are pruned early, and decisions are made online for each sentence. We model this as a sequential decision-making problem and solve it by imitation learning techniques. We test our method on 7 languages. Our dynamic parser can achieve accuracies comparable or even superior to parsers using a full set of features, while computing fewer than 30% of the feature templates.</p><p>5 0.073794574 <a title="50-tfidf-5" href="./emnlp-2013-A_Multi-Teraflop_Constituency_Parser_using_GPUs.html">10 emnlp-2013-A Multi-Teraflop Constituency Parser using GPUs</a></p>
<p>Author: John Canny ; David Hall ; Dan Klein</p><p>Abstract: Constituency parsing with rich grammars remains a computational challenge. Graphics Processing Units (GPUs) have previously been used to accelerate CKY chart evaluation, but gains over CPU parsers were modest. In this paper, we describe a collection of new techniques that enable chart evaluation at close to the GPU’s practical maximum speed (a Teraflop), or around a half-trillion rule evaluations per second. Net parser performance on a 4-GPU system is over 1 thousand length30 sentences/second (1 trillion rules/sec), and 400 general sentences/second for the Berkeley Parser Grammar. The techniques we introduce include grammar compilation, recursive symbol blocking, and cache-sharing.</p><p>6 0.073216893 <a title="50-tfidf-6" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>7 0.072921291 <a title="50-tfidf-7" href="./emnlp-2013-The_Topology_of_Semantic_Knowledge.html">182 emnlp-2013-The Topology of Semantic Knowledge</a></p>
<p>8 0.067595892 <a title="50-tfidf-8" href="./emnlp-2013-Adaptor_Grammars_for_Learning_Non-Concatenative_Morphology.html">19 emnlp-2013-Adaptor Grammars for Learning Non-Concatenative Morphology</a></p>
<p>9 0.064310461 <a title="50-tfidf-9" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>10 0.064217009 <a title="50-tfidf-10" href="./emnlp-2013-Optimal_Beam_Search_for_Machine_Translation.html">145 emnlp-2013-Optimal Beam Search for Machine Translation</a></p>
<p>11 0.063309416 <a title="50-tfidf-11" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>12 0.06317462 <a title="50-tfidf-12" href="./emnlp-2013-Shift-Reduce_Word_Reordering_for_Machine_Translation.html">171 emnlp-2013-Shift-Reduce Word Reordering for Machine Translation</a></p>
<p>13 0.060778793 <a title="50-tfidf-13" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>14 0.057254378 <a title="50-tfidf-14" href="./emnlp-2013-Dependency_Language_Models_for_Sentence_Completion.html">58 emnlp-2013-Dependency Language Models for Sentence Completion</a></p>
<p>15 0.054409247 <a title="50-tfidf-15" href="./emnlp-2013-The_Effects_of_Syntactic_Features_in_Automatic_Prediction_of_Morphology.html">181 emnlp-2013-The Effects of Syntactic Features in Automatic Prediction of Morphology</a></p>
<p>16 0.052148066 <a title="50-tfidf-16" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>17 0.051353533 <a title="50-tfidf-17" href="./emnlp-2013-Fast_Joint_Compression_and_Summarization_via_Graph_Cuts.html">85 emnlp-2013-Fast Joint Compression and Summarization via Graph Cuts</a></p>
<p>18 0.051326547 <a title="50-tfidf-18" href="./emnlp-2013-Joint_Parsing_and_Disfluency_Detection_in_Linear_Time.html">116 emnlp-2013-Joint Parsing and Disfluency Detection in Linear Time</a></p>
<p>19 0.05035571 <a title="50-tfidf-19" href="./emnlp-2013-Ubertagging%3A_Joint_Segmentation_and_Supertagging_for_English.html">190 emnlp-2013-Ubertagging: Joint Segmentation and Supertagging for English</a></p>
<p>20 0.050305735 <a title="50-tfidf-20" href="./emnlp-2013-Inducing_Document_Plans_for_Concept-to-Text_Generation.html">106 emnlp-2013-Inducing Document Plans for Concept-to-Text Generation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.162), (1, -0.053), (2, 0.019), (3, 0.021), (4, -0.091), (5, 0.034), (6, 0.013), (7, -0.042), (8, 0.019), (9, 0.117), (10, -0.012), (11, -0.025), (12, -0.122), (13, 0.076), (14, 0.053), (15, -0.026), (16, -0.125), (17, 0.047), (18, -0.061), (19, -0.016), (20, 0.032), (21, -0.067), (22, 0.121), (23, 0.085), (24, 0.01), (25, -0.027), (26, -0.034), (27, 0.088), (28, 0.114), (29, -0.066), (30, -0.102), (31, 0.005), (32, 0.07), (33, -0.054), (34, -0.005), (35, 0.068), (36, -0.053), (37, 0.091), (38, 0.001), (39, 0.013), (40, 0.004), (41, -0.059), (42, 0.14), (43, 0.028), (44, -0.067), (45, -0.068), (46, -0.012), (47, -0.102), (48, -0.043), (49, -0.0)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93798792 <a title="50-lsi-1" href="./emnlp-2013-Combining_PCFG-LA_Models_with_Dual_Decomposition%3A_A_Case_Study_with_Function_Labels_and_Binarization.html">50 emnlp-2013-Combining PCFG-LA Models with Dual Decomposition: A Case Study with Function Labels and Binarization</a></p>
<p>Author: Joseph Le Roux ; Antoine Rozenknop ; Jennifer Foster</p><p>Abstract: It has recently been shown that different NLP models can be effectively combined using dual decomposition. In this paper we demonstrate that PCFG-LA parsing models are suitable for combination in this way. We experiment with the different models which result from alternative methods of extracting a grammar from a treebank (retaining or discarding function labels, left binarization versus right binarization) and achieve a labeled Parseval F-score of 92.4 on Wall Street Journal Section 23 this represents an absolute improvement of 0.7 and an error reduction rate of 7% over a strong PCFG-LA product-model baseline. Although we experiment only with binarization and function labels in this study, there is much scope for applying this approach to – other grammar extraction strategies.</p><p>2 0.72629523 <a title="50-lsi-2" href="./emnlp-2013-Dynamic_Feature_Selection_for_Dependency_Parsing.html">66 emnlp-2013-Dynamic Feature Selection for Dependency Parsing</a></p>
<p>Author: He He ; Hal Daume III ; Jason Eisner</p><p>Abstract: Feature computation and exhaustive search have significantly restricted the speed of graph-based dependency parsing. We propose a faster framework of dynamic feature selection, where features are added sequentially as needed, edges are pruned early, and decisions are made online for each sentence. We model this as a sequential decision-making problem and solve it by imitation learning techniques. We test our method on 7 languages. Our dynamic parser can achieve accuracies comparable or even superior to parsers using a full set of features, while computing fewer than 30% of the feature templates.</p><p>3 0.66281581 <a title="50-lsi-3" href="./emnlp-2013-Semi-Supervised_Feature_Transformation_for_Dependency_Parsing.html">168 emnlp-2013-Semi-Supervised Feature Transformation for Dependency Parsing</a></p>
<p>Author: Wenliang Chen ; Min Zhang ; Yue Zhang</p><p>Abstract: In current dependency parsing models, conventional features (i.e. base features) defined over surface words and part-of-speech tags in a relatively high-dimensional feature space may suffer from the data sparseness problem and thus exhibit less discriminative power on unseen data. In this paper, we propose a novel semi-supervised approach to addressing the problem by transforming the base features into high-level features (i.e. meta features) with the help of a large amount of automatically parsed data. The meta features are used together with base features in our final parser. Our studies indicate that our proposed approach is very effective in processing unseen data and features. Experiments on Chinese and English data sets show that the final parser achieves the best-reported accuracy on the Chinese data and comparable accuracy with the best known parsers on the English data.</p><p>4 0.64781672 <a title="50-lsi-4" href="./emnlp-2013-A_Multi-Teraflop_Constituency_Parser_using_GPUs.html">10 emnlp-2013-A Multi-Teraflop Constituency Parser using GPUs</a></p>
<p>Author: John Canny ; David Hall ; Dan Klein</p><p>Abstract: Constituency parsing with rich grammars remains a computational challenge. Graphics Processing Units (GPUs) have previously been used to accelerate CKY chart evaluation, but gains over CPU parsers were modest. In this paper, we describe a collection of new techniques that enable chart evaluation at close to the GPU’s practical maximum speed (a Teraflop), or around a half-trillion rule evaluations per second. Net parser performance on a 4-GPU system is over 1 thousand length30 sentences/second (1 trillion rules/sec), and 400 general sentences/second for the Berkeley Parser Grammar. The techniques we introduce include grammar compilation, recursive symbol blocking, and cache-sharing.</p><p>5 0.63932478 <a title="50-lsi-5" href="./emnlp-2013-Joint_Parsing_and_Disfluency_Detection_in_Linear_Time.html">116 emnlp-2013-Joint Parsing and Disfluency Detection in Linear Time</a></p>
<p>Author: Mohammad Sadegh Rasooli ; Joel Tetreault</p><p>Abstract: We introduce a novel method to jointly parse and detect disfluencies in spoken utterances. Our model can use arbitrary features for parsing sentences and adapt itself with out-ofdomain data. We show that our method, based on transition-based parsing, performs at a high level of accuracy for both the parsing and disfluency detection tasks. Additionally, our method is the fastest for the joint task, running in linear time.</p><p>6 0.62707835 <a title="50-lsi-6" href="./emnlp-2013-Optimal_Incremental_Parsing_via_Best-First_Dynamic_Programming.html">146 emnlp-2013-Optimal Incremental Parsing via Best-First Dynamic Programming</a></p>
<p>7 0.58408183 <a title="50-lsi-7" href="./emnlp-2013-Unsupervised_Spectral_Learning_of_WCFG_as_Low-rank_Matrix_Completion.html">195 emnlp-2013-Unsupervised Spectral Learning of WCFG as Low-rank Matrix Completion</a></p>
<p>8 0.57396144 <a title="50-lsi-8" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>9 0.5162102 <a title="50-lsi-9" href="./emnlp-2013-Ubertagging%3A_Joint_Segmentation_and_Supertagging_for_English.html">190 emnlp-2013-Ubertagging: Joint Segmentation and Supertagging for English</a></p>
<p>10 0.51597857 <a title="50-lsi-10" href="./emnlp-2013-Dependency_Language_Models_for_Sentence_Completion.html">58 emnlp-2013-Dependency Language Models for Sentence Completion</a></p>
<p>11 0.50990558 <a title="50-lsi-11" href="./emnlp-2013-The_Topology_of_Semantic_Knowledge.html">182 emnlp-2013-The Topology of Semantic Knowledge</a></p>
<p>12 0.43491521 <a title="50-lsi-12" href="./emnlp-2013-Shift-Reduce_Word_Reordering_for_Machine_Translation.html">171 emnlp-2013-Shift-Reduce Word Reordering for Machine Translation</a></p>
<p>13 0.35600153 <a title="50-lsi-13" href="./emnlp-2013-Optimal_Beam_Search_for_Machine_Translation.html">145 emnlp-2013-Optimal Beam Search for Machine Translation</a></p>
<p>14 0.35279357 <a title="50-lsi-14" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>15 0.35144216 <a title="50-lsi-15" href="./emnlp-2013-Learning_to_Freestyle%3A_Hip_Hop_Challenge-Response_Induction_via_Transduction_Rule_Segmentation.html">122 emnlp-2013-Learning to Freestyle: Hip Hop Challenge-Response Induction via Transduction Rule Segmentation</a></p>
<p>16 0.35105434 <a title="50-lsi-16" href="./emnlp-2013-Adaptor_Grammars_for_Learning_Non-Concatenative_Morphology.html">19 emnlp-2013-Adaptor Grammars for Learning Non-Concatenative Morphology</a></p>
<p>17 0.34612587 <a title="50-lsi-17" href="./emnlp-2013-A_Synchronous_Context_Free_Grammar_for_Time_Normalization.html">14 emnlp-2013-A Synchronous Context Free Grammar for Time Normalization</a></p>
<p>18 0.33380792 <a title="50-lsi-18" href="./emnlp-2013-Feature_Noising_for_Log-Linear_Structured_Prediction.html">86 emnlp-2013-Feature Noising for Log-Linear Structured Prediction</a></p>
<p>19 0.32937098 <a title="50-lsi-19" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>20 0.3260926 <a title="50-lsi-20" href="./emnlp-2013-Using_Soft_Constraints_in_Joint_Inference_for_Clinical_Concept_Recognition.html">198 emnlp-2013-Using Soft Constraints in Joint Inference for Clinical Concept Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.032), (18, 0.046), (22, 0.044), (26, 0.028), (30, 0.086), (45, 0.027), (48, 0.267), (50, 0.024), (51, 0.173), (64, 0.019), (66, 0.04), (71, 0.026), (75, 0.017), (77, 0.025), (95, 0.015), (96, 0.016), (97, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.85129213 <a title="50-lda-1" href="./emnlp-2013-Russian_Stress_Prediction_using_Maximum_Entropy_Ranking.html">162 emnlp-2013-Russian Stress Prediction using Maximum Entropy Ranking</a></p>
<p>Author: Keith Hall ; Richard Sproat</p><p>Abstract: We explore a model of stress prediction in Russian using a combination of local contextual features and linguisticallymotivated features associated with the word’s stem and suffix. We frame this as a ranking problem, where the objective is to rank the pronunciation with the correct stress above those with incorrect stress. We train our models using a simple Maximum Entropy ranking framework allowing for efficient prediction. An empirical evaluation shows that a model combining the local contextual features and the linguistically-motivated non-local features performs best in identifying both primary and secondary stress. 1</p><p>same-paper 2 0.78515708 <a title="50-lda-2" href="./emnlp-2013-Combining_PCFG-LA_Models_with_Dual_Decomposition%3A_A_Case_Study_with_Function_Labels_and_Binarization.html">50 emnlp-2013-Combining PCFG-LA Models with Dual Decomposition: A Case Study with Function Labels and Binarization</a></p>
<p>Author: Joseph Le Roux ; Antoine Rozenknop ; Jennifer Foster</p><p>Abstract: It has recently been shown that different NLP models can be effectively combined using dual decomposition. In this paper we demonstrate that PCFG-LA parsing models are suitable for combination in this way. We experiment with the different models which result from alternative methods of extracting a grammar from a treebank (retaining or discarding function labels, left binarization versus right binarization) and achieve a labeled Parseval F-score of 92.4 on Wall Street Journal Section 23 this represents an absolute improvement of 0.7 and an error reduction rate of 7% over a strong PCFG-LA product-model baseline. Although we experiment only with binarization and function labels in this study, there is much scope for applying this approach to – other grammar extraction strategies.</p><p>3 0.63468087 <a title="50-lda-3" href="./emnlp-2013-Pair_Language_Models_for_Deriving_Alternative_Pronunciations_and_Spellings_from_Pronunciation_Dictionaries.html">150 emnlp-2013-Pair Language Models for Deriving Alternative Pronunciations and Spellings from Pronunciation Dictionaries</a></p>
<p>Author: Russell Beckley ; Brian Roark</p><p>Abstract: Pronunciation dictionaries provide a readily available parallel corpus for learning to transduce between character strings and phoneme strings or vice versa. Translation models can be used to derive character-level paraphrases on either side of this transduction, allowing for the automatic derivation of alternative pronunciations or spellings. We examine finitestate and SMT-based methods for these related tasks, and demonstrate that the tasks have different characteristics finding alternative spellings is harder than alternative pronunciations and benefits from round-trip algorithms when the other does not. We also show that we can increase accuracy by modeling syllable stress. –</p><p>4 0.6133762 <a title="50-lda-4" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>Author: Kuzman Ganchev ; Dipanjan Das</p><p>Abstract: We present a framework for cross-lingual transfer of sequence information from a resource-rich source language to a resourceimpoverished target language that incorporates soft constraints via posterior regularization. To this end, we use automatically word aligned bitext between the source and target language pair, and learn a discriminative conditional random field model on the target side. Our posterior regularization constraints are derived from simple intuitions about the task at hand and from cross-lingual alignment information. We show improvements over strong baselines for two tasks: part-of-speech tagging and namedentity segmentation.</p><p>5 0.61177069 <a title="50-lda-5" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>Author: Uri Lerner ; Slav Petrov</p><p>Abstract: We present a simple and novel classifier-based preordering approach. Unlike existing preordering models, we train feature-rich discriminative classifiers that directly predict the target-side word order. Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discriminative model with a rich set of features, including lexical features. We present extensive experiments on 22 language pairs, including preordering into English from 7 other languages. We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task. For languages from different families the improvements often exceed 2 BLEU. Many of these gains are also significant in human evaluations.</p><p>6 0.60933393 <a title="50-lda-6" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>7 0.60867363 <a title="50-lda-7" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>8 0.60673112 <a title="50-lda-8" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>9 0.60509753 <a title="50-lda-9" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>10 0.60453564 <a title="50-lda-10" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>11 0.60428059 <a title="50-lda-11" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>12 0.60362476 <a title="50-lda-12" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>13 0.60247666 <a title="50-lda-13" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>14 0.6023013 <a title="50-lda-14" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>15 0.6020295 <a title="50-lda-15" href="./emnlp-2013-Semi-Supervised_Feature_Transformation_for_Dependency_Parsing.html">168 emnlp-2013-Semi-Supervised Feature Transformation for Dependency Parsing</a></p>
<p>16 0.60158432 <a title="50-lda-16" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>17 0.60137427 <a title="50-lda-17" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>18 0.60129285 <a title="50-lda-18" href="./emnlp-2013-Scaling_Semantic_Parsers_with_On-the-Fly_Ontology_Matching.html">164 emnlp-2013-Scaling Semantic Parsers with On-the-Fly Ontology Matching</a></p>
<p>19 0.60073143 <a title="50-lda-19" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>20 0.59999424 <a title="50-lda-20" href="./emnlp-2013-Semi-Markov_Phrase-Based_Monolingual_Alignment.html">167 emnlp-2013-Semi-Markov Phrase-Based Monolingual Alignment</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
