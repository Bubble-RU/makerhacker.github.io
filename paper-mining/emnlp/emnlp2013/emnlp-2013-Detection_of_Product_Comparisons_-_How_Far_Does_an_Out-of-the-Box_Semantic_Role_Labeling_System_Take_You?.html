<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-62" href="#">emnlp2013-62</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</h1>
<br/><p>Source: <a title="emnlp-2013-62-pdf" href="http://aclweb.org/anthology//D/D13/D13-1194.pdf">pdf</a></p><p>Author: Wiltrud Kessler ; Jonas Kuhn</p><p>Abstract: This short paper presents a pilot study investigating the training of a standard Semantic Role Labeling (SRL) system on product reviews for the new task of detecting comparisons. An (opinionated) comparison consists of a comparative “predicate” and up to three “arguments”: the entity evaluated positively, the entity evaluated negatively, and the aspect under which the comparison is made. In user-generated product reviews, the “predicate” and “arguments” are expressed in highly heterogeneous ways; but since the elements are textually annotated in existing datasets, SRL is technically applicable. We address the interesting question how well training an outof-the-box SRL model works for English data. We observe that even without any feature engineering or other major adaptions to our task, the system outperforms a reasonable heuristic baseline in all steps (predicate identification, argument identification and argument classification) and in three different datasets.</p><p>Reference: <a title="emnlp-2013-62-reference" href="../emnlp2013_reference/emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ke s s ler@ ims  Abstract This short paper presents a pilot study investigating the training of a standard Semantic Role Labeling (SRL) system on product reviews for the new task of detecting comparisons. [sent-3, score-0.152]
</p><p>2 An (opinionated) comparison consists of a comparative “predicate” and up to three “arguments”: the entity evaluated positively, the entity evaluated negatively, and the aspect under which the comparison is made. [sent-4, score-0.921]
</p><p>3 In user-generated product reviews, the “predicate” and “arguments” are expressed in highly heterogeneous ways; but since the elements are textually annotated in existing datasets, SRL is technically applicable. [sent-5, score-0.204]
</p><p>4 We observe that even without any feature engineering or other major adaptions to our task, the system outperforms a reasonable heuristic baseline in all steps (predicate identification,  argument identification and argument classification) and in three different datasets. [sent-7, score-0.492]
</p><p>5 1 Introduction Sentiment analysis deals with the task of determining the polarity of an opinionated document or a sentence, in product reviews typically with regard to some target product. [sent-8, score-0.167]
</p><p>6 A common way to express sentiment about some product is by comparing it to a different product. [sent-9, score-0.2]
</p><p>7 Here are some examples of comparison sentences from our corpus: (1) a. [sent-11, score-0.084]
</p><p>8 ” Note that our definition of comparisons is broader than the linguistic category of comparative sentences, which only includes sentences that contain a comparative adjective or adverb. [sent-24, score-1.095]
</p><p>9 For our work, we consider comparisons expressed by any Part of Speech (POS). [sent-25, score-0.225]
</p><p>10 A comparison contains several parts that must be identified in order to get meaningful information. [sent-26, score-0.051]
</p><p>11 We call the word or phrase that is used to express the comparison (“better”, “beats”, . [sent-27, score-0.1]
</p><p>12 A comparison involves two entities, one or both of them may be implicit. [sent-31, score-0.051]
</p><p>13 In our data, most of the entities are products, e. [sent-32, score-0.104]
</p><p>14 , the two cameras “D70” and “EOS 300D” in sentence 1b. [sent-34, score-0.188]
</p><p>15 In graded comparisons, entity+ (E+) is the entity that is being evaluated positively, entity- (E-) the entity evaluated negatively. [sent-35, score-0.376]
</p><p>16 In many sentences one attribute or part of a product is being compared, like “image sensor” in sentence 1d. [sent-36, score-0.132]
</p><p>17 The task we want to solve for a given comparison sentence is to detect the comparative predicate,  the entities that are involved and the aspect that is being compared. [sent-38, score-0.729]
</p><p>18 In SRL, events are expressed by predicates and participants of these events are expressed by arguments that fill different semantic roles. [sent-40, score-0.594]
</p><p>19 Adapted to the problem of detecting comparisons, the events we are interested in are comparative predicates and the arguments are the two entities and the aspect that is being compared. [sent-41, score-1.093]
</p><p>20 Moreoever, the existing labeled datasets are based on an annotation methodology which gave the annotators a lot of freedom in deciding on the linguistic anchoring of the “predicate” and “arguments”. [sent-45, score-0.241]
</p><p>21 , 2009) on product review data labeled with comparative predicates and arguments. [sent-48, score-0.793]
</p><p>22 This is an encouraging result for a linguistically grounded modeling approach to comparison detection. [sent-50, score-0.146]
</p><p>23 2  Related Work  The syntax and semantics of comparative sentences have been the topic of research in linguistics for a long time (Moltmann, 1992; Kennedy, 1999). [sent-51, score-0.407]
</p><p>24 However, our focus is on computational methods and we also treat comparisons that are not comparative sentences in a linguistic sense. [sent-52, score-0.66]
</p><p>25 In sentiment analysis, some studies have been presented to identify comparison sentences. [sent-53, score-0.133]
</p><p>26 To identify entities and aspect, Jindal and Liu (2006b) use an involved pattern mining process to mine label sequential rules from annotated English sentences. [sent-58, score-0.179]
</p><p>27 Both approaches consider only nouns and pronouns for entities and aspects, we use all POS and allow for multi-word arguments. [sent-61, score-0.132]
</p><p>28 Jindal and Liu (2006b) base the recognition of comparative predi1893 cates on a list of manually compiled keywords. [sent-62, score-0.374]
</p><p>29 All works label the entities according to their position with respect to the predicate. [sent-65, score-0.104]
</p><p>30 This requires the identification of the preferred entity in a non-equal comparison as an additional step. [sent-66, score-0.253]
</p><p>31 Ganapathibhotla and Liu (2008) use hand-crafted rules based on the polarity of the predicate for this task. [sent-67, score-0.316]
</p><p>32 As we label  the entities with their roles from the start, we solve both problems at the same time. [sent-68, score-0.104]
</p><p>33 They present an approach that uses conditional random fields to extract relations (better, worse, same and no comparison) between two entitites, an attribute and a predicate phrase. [sent-71, score-0.316]
</p><p>34 They use SRL with standard SRL features to extract comparative relations from Chinese sentences. [sent-73, score-0.374]
</p><p>35 The result of our processing are one or more comparative predicates and for each predicate three arguments: The two entities that are being compared, and the aspect they  are compared in. [sent-78, score-1.251]
</p><p>36 Entity+ is the entity that is being evaluated as better than entity-. [sent-80, score-0.169]
</p><p>37 Currently, we treat only single words as comparative predicates. [sent-82, score-0.401]
</p><p>38 We allow for multi-word arguments, but annotate only the head word of the phrase and treat it as a one word argument for evaluation. [sent-84, score-0.253]
</p><p>39 As a first step, the comparative predicate is identified. [sent-87, score-0.69]
</p><p>40 The next step in SRL would be predicate disambiguation to identify the different frames this predicate can express. [sent-88, score-0.632]
</p><p>41 As we do not have such frame information, predicate disambiguation is not performed in our pipeline. [sent-89, score-0.316]
</p><p>42 The identification step is a binary classification whether a word in the sentence is some argument of the identified predicate. [sent-91, score-0.332]
</p><p>43 As a final classification step, it is determined for each found argument whether this argument is  entity+, entity- or the aspect. [sent-92, score-0.428]
</p><p>44 Features use attributes of the predicate itself, its head or its dependents. [sent-95, score-0.356]
</p><p>45 Additionally, for argument identification and classification there are features that describe the relation of predicate and argument, the argument itself, its leftmost and rightmost dependent and left and right sibling. [sent-96, score-0.804]
</p><p>46 For the classification tasks of the pipeline, the SRL system uses regularized linear logistic regression from the LIBLINEAR package (Fan et al. [sent-97, score-0.056]
</p><p>47 We set the SRL system to train separate classifiers for predicates ofdifferent POS. [sent-99, score-0.35]
</p><p>48 We use the annotation class “Comparison” that has four annotation slots. [sent-107, score-0.09]
</p><p>49 We  convert the “more” slot to entity+, the “less” slot to entity- and the “dimension” slot to the aspect. [sent-108, score-0.252]
</p><p>50 For now, we ignore the “same” slot which indicates if the two mentions are ranked as equal. [sent-109, score-0.122]
</p><p>51 edu/ j dpacorpus / we ignore cars batch 009 where no arguments of comparative predicates are annotated. [sent-115, score-0.93]
</p><p>52 In this dataset (J&L;), entities are annotated as entity 1 or entity 2 depending on their position before or after the predicate. [sent-122, score-0.434]
</p><p>53 In the JDPA corpus, if an annotated argument is outside the current sentence, we follow the coreference chain to find a coreferent annotation in the same sentence. [sent-127, score-0.277]
</p><p>54 If this is not successful, the argument is ignored. [sent-128, score-0.186]
</p><p>55 We extract all sentences where we found at least one comparative predicate as our dataset. [sent-129, score-0.723]
</p><p>56 We report precision (P), recall (R), F1-measure (F1), and for argument classification macro averaged F1-measure  (F1m) over the three arguments. [sent-133, score-0.287]
</p><p>57 The simplest baseline, BL POS classifies all tokens with a comparative POS (’JJR’, ’JJS’, ’RBR’, ’RBS’) as predicates. [sent-138, score-0.374]
</p><p>58 P  R  F1  Table 2: Results predicate identification  asrc mJL&SB; R L 345P80913. [sent-145, score-0.376]
</p><p>59 m01893∗ Table 4: Results argument classification (gold predicates)  piled comparative keyphrases from (Jindal and Liu, 2006a) in addition to the POS tags. [sent-155, score-0.669]
</p><p>60 The generally low recall values are mainly a result of the wide variety of predicates that are used to express comparisons (see Discussion). [sent-158, score-0.597]
</p><p>61 To get results indepent of the errors introduced by the relatively low performance on predicate identification, we use annotated predicates (gold predicates) as a starting point for the argument experiments. [sent-160, score-0.898]
</p><p>62 All results drop about 10% when system predicates are used. [sent-161, score-0.35]
</p><p>63 As a baseline (BL) for argument identification and classification, we use some heuristics based on the characteristics of our data. [sent-162, score-0.246]
</p><p>64 Most entities are (pro)nouns and most predicates are positive, so we classify the first noun or pronoun before the predicate as entity+ (entity 1 for J&L;) and the first noun or pronoun after the predicate as a entity- (entity 2). [sent-163, score-1.171]
</p><p>65 If the predicate is a comparative adjective, we classify the predicate itself as aspect, because this type of annotation is very frequent in the JDPA data. [sent-164, score-1.08]
</p><p>66 For other predicates except nouns and verbs, we classify  the direct head of the predicate as aspect. [sent-165, score-0.763]
</p><p>67 Table 3 shows the results for argument identifica1895 tion, the results for argument classification can be seen in Table 4. [sent-166, score-0.428]
</p><p>68 The differences are significant except for the cameras dataset. [sent-168, score-0.158]
</p><p>69 There are many ways to express a comparison and the size of the available training data is relatively small. [sent-172, score-0.1]
</p><p>70 This strongly influences the recall of our system as many predicates and arguments occur only once. [sent-173, score-0.476]
</p><p>71 As we can see in Table 1, 60% of the predicates in the cameras dataset occur only once. [sent-174, score-0.508]
</p><p>72 In contrast, only 12 predicates occur ten times or more. [sent-175, score-0.35]
</p><p>73 This particularily affects verbs and nouns, where many colloquial expressions are used (“hammers”, “pwns”, “go head to head with”, “put X to  the sword”, . [sent-177, score-0.08]
</p><p>74 Argument identification and classification would benefit from generalizing over the many different product identifiers like “EOS 5D” or “D200”. [sent-181, score-0.185]
</p><p>75 We want to try to use a Named Entity Recognition system trained on this type of entities for this purpose. [sent-182, score-0.134]
</p><p>76 The following examples show a problem that is typical for sentiment analysis and responsible for many false positive predicates: (2) a. [sent-184, score-0.082]
</p><p>77 but [higher]A then [Sony]E+” Although “higher” often expresses a comparison like in sentence 2b, in sentence 2a it only describes a camera setting and should not be extracted as a comparative predicate. [sent-192, score-0.548]
</p><p>78 There has been considerable work in the areas of subjectivity classification (Wilson and Wiebe, 2003) and the related sentiment relevance (Scheible and Sch u¨tze, 2013) which we will try to use to detect such irrelevant, “descriptive” uses of comparative words. [sent-193, score-0.545]
</p><p>79 In contrast to SRL, the task of comparison detection in reviews is a relatively new task without universally recognized definitions and annotation schemes. [sent-195, score-0.149]
</p><p>80 The annotators of the cor-  pora had a lot of freedom in their choice of linguistic anchoring of the predicates and arguments. [sent-196, score-0.517]
</p><p>81 Consider these examples from the cameras dataset: (3) a. [sent-197, score-0.158]
</p><p>82 ” Sentences 3a and 3b show a situation where two words are used to express the same comparison and it is unclear which one to chose as a predicate. [sent-211, score-0.1]
</p><p>83 There is some variety ofannotations on arguments as well. [sent-213, score-0.126]
</p><p>84 In the JDPA data, a comparative adjective is often annotated as aspect, sometimes even when there is an alternative, e. [sent-214, score-0.474]
</p><p>85 Also, for a phrase like “its screen”, we find “screen” annotated as the aspect (sentence 1a) or an entity (sentence 3c) – and both have their merit. [sent-217, score-0.295]
</p><p>86 We want to further study how different linguistic anchorings of comparisons effect classification performance. [sent-218, score-0.372]
</p><p>87 In graded comparisons, the distinction is informative, but sentiment information would be needed for 1896 the correct assignment. [sent-221, score-0.147]
</p><p>88 In the JDPA corpus, entities still have to be annotated as either entity+ or entity- and the annotation guidelines allow the annotator to choose freely. [sent-229, score-0.195]
</p><p>89 As a result, the data is noisy, for the same predicate sometimes entity- is before the predicate, sometimes entity+. [sent-230, score-0.316]
</p><p>90 If we eliminate this noise by always assigning the entities in order of surface position, we see a gain in macro averaged F1-measure for all systems of about 2% (cameras) to 4% (cars). [sent-231, score-0.177]
</p><p>91 6  Conclusions  We presented a pilot experiment on using an SRL-  inspired approach to detect comparisons (comparative predicate, entity+, entity-, aspect) in user generated content. [sent-232, score-0.261]
</p><p>92 We re-trained an existing SRL system on data that is labeled with comparative predicates and arguments. [sent-233, score-0.753]
</p><p>93 This is an encouraging result for a linguistically grounded modeling approach to comparison detection. [sent-235, score-0.146]
</p><p>94 For future work, we plan to include features that have been tailored specifically to the task of detecting product comparisons. [sent-236, score-0.069]
</p><p>95 To address the inherent diversity ofexpressions typical for user generated content, we want to employ generalization techniques, e. [sent-237, score-0.06]
</p><p>96 We also want to further study the different possible linguistic anchorings of comparisons and their effect on classification performance. [sent-240, score-0.372]
</p><p>97 Studies of this kind may also inform future data annotation efforts in that certain ways of anchoring the elements of a comparison linguistically may be more helpful than others. [sent-241, score-0.223]
</p><p>98 We also believe that the explicit modeling of different types (equative, superlative, non-equal gradable) of comparisons will have a positive effect on performance. [sent-242, score-0.198]
</p><p>99 Mining comparative opinions from customer reviews for competitive intelligence. [sent-312, score-0.458]
</p><p>100 Extracting comparative sentences from Korean text documents using comparative lexical patterns and machine learning techniques. [sent-318, score-0.781]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('srl', 0.4), ('comparative', 0.374), ('predicates', 0.35), ('predicate', 0.316), ('comparisons', 0.198), ('argument', 0.186), ('jdpa', 0.181), ('cameras', 0.158), ('entity', 0.142), ('jindal', 0.14), ('eos', 0.131), ('arguments', 0.126), ('aspect', 0.107), ('entities', 0.104), ('anchoring', 0.091), ('equative', 0.091), ('slot', 0.084), ('sentiment', 0.082), ('product', 0.069), ('screen', 0.064), ('camera', 0.063), ('adaptions', 0.06), ('anchorings', 0.06), ('sensor', 0.06), ('seon', 0.06), ('youngjoong', 0.06), ('identification', 0.06), ('korean', 0.057), ('kessler', 0.057), ('classification', 0.056), ('adjective', 0.054), ('reviews', 0.053), ('hou', 0.053), ('ganapathibhotla', 0.053), ('keyphrases', 0.053), ('bl', 0.051), ('comparison', 0.051), ('express', 0.049), ('scheible', 0.048), ('freedom', 0.048), ('rkelund', 0.048), ('annotated', 0.046), ('annotation', 0.045), ('beats', 0.045), ('positively', 0.045), ('macro', 0.045), ('opinionated', 0.045), ('liu', 0.043), ('cars', 0.042), ('head', 0.04), ('graded', 0.038), ('ko', 0.038), ('ignore', 0.038), ('liblinear', 0.037), ('linguistically', 0.036), ('nitin', 0.035), ('bj', 0.035), ('bing', 0.035), ('broader', 0.034), ('sentences', 0.033), ('concerns', 0.033), ('heterogeneous', 0.033), ('detect', 0.033), ('events', 0.032), ('yang', 0.032), ('opinions', 0.031), ('diversity', 0.03), ('grounded', 0.03), ('pilot', 0.03), ('sentence', 0.03), ('keywords', 0.03), ('want', 0.03), ('classify', 0.029), ('existing', 0.029), ('encouraging', 0.029), ('mining', 0.029), ('pos', 0.029), ('sch', 0.028), ('image', 0.028), ('pronoun', 0.028), ('nouns', 0.028), ('noise', 0.028), ('wilson', 0.028), ('fan', 0.028), ('linguistic', 0.028), ('distinction', 0.027), ('treat', 0.027), ('expressed', 0.027), ('evaluated', 0.027), ('stutt', 0.026), ('sword', 0.026), ('dissertations', 0.026), ('yuxia', 0.026), ('superlative', 0.026), ('automotive', 0.026), ('flash', 0.026), ('jiexun', 0.026), ('jjs', 0.026), ('jonas', 0.026), ('kennedy', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="62-tfidf-1" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>Author: Wiltrud Kessler ; Jonas Kuhn</p><p>Abstract: This short paper presents a pilot study investigating the training of a standard Semantic Role Labeling (SRL) system on product reviews for the new task of detecting comparisons. An (opinionated) comparison consists of a comparative “predicate” and up to three “arguments”: the entity evaluated positively, the entity evaluated negatively, and the aspect under which the comparison is made. In user-generated product reviews, the “predicate” and “arguments” are expressed in highly heterogeneous ways; but since the elements are textually annotated in existing datasets, SRL is technically applicable. We address the interesting question how well training an outof-the-box SRL model works for English data. We observe that even without any feature engineering or other major adaptions to our task, the system outperforms a reasonable heuristic baseline in all steps (predicate identification, argument identification and argument classification) and in three different datasets.</p><p>2 0.18497062 <a title="62-tfidf-2" href="./emnlp-2013-Semantic_Parsing_on_Freebase_from_Question-Answer_Pairs.html">166 emnlp-2013-Semantic Parsing on Freebase from Question-Answer Pairs</a></p>
<p>Author: Jonathan Berant ; Andrew Chou ; Roy Frostig ; Percy Liang</p><p>Abstract: In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset ofCai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline.</p><p>3 0.1789142 <a title="62-tfidf-3" href="./emnlp-2013-Unsupervised_Induction_of_Cross-Lingual_Semantic_Relations.html">193 emnlp-2013-Unsupervised Induction of Cross-Lingual Semantic Relations</a></p>
<p>Author: Mike Lewis ; Mark Steedman</p><p>Abstract: Creating a language-independent meaning representation would benefit many crosslingual NLP tasks. We introduce the first unsupervised approach to this problem, learning clusters of semantically equivalent English and French relations between referring expressions, based on their named-entity arguments in large monolingual corpora. The clusters can be used as language-independent semantic relations, by mapping clustered expressions in different languages onto the same relation. Our approach needs no parallel text for training, but outperforms a baseline that uses machine translation on a cross-lingual question answering task. We also show how to use the semantics to improve the accuracy of machine translation, by using it in a simple reranker.</p><p>4 0.15486422 <a title="62-tfidf-4" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>Author: Eduardo Blanco ; Dan Moldovan</p><p>Abstract: This paper presents a novel approach to determine textual similarity. A layered methodology to transform text into logic forms is proposed, and semantic features are derived from a logic prover. Experimental results show that incorporating the semantic structure of sentences is beneficial. When training data is unavailable, scores obtained from the logic prover in an unsupervised manner outperform supervised methods.</p><p>5 0.11791176 <a title="62-tfidf-5" href="./emnlp-2013-Effectiveness_and_Efficiency_of_Open_Relation_Extraction.html">68 emnlp-2013-Effectiveness and Efficiency of Open Relation Extraction</a></p>
<p>Author: Filipe Mesquita ; Jordan Schmidek ; Denilson Barbosa</p><p>Abstract: A large number of Open Relation Extraction approaches have been proposed recently, covering a wide range of NLP machinery, from “shallow” (e.g., part-of-speech tagging) to “deep” (e.g., semantic role labeling–SRL). A natural question then is what is the tradeoff between NLP depth (and associated computational cost) versus effectiveness. This paper presents a fair and objective experimental comparison of 8 state-of-the-art approaches over 5 different datasets, and sheds some light on the issue. The paper also describes a novel method, EXEMPLAR, which adapts ideas from SRL to less costly NLP machinery, resulting in substantial gains both in efficiency and effectiveness, over binary and n-ary relation extraction tasks.</p><p>6 0.11380216 <a title="62-tfidf-6" href="./emnlp-2013-Discourse_Level_Explanatory_Relation_Extraction_from_Product_Reviews_Using_First-Order_Logic.html">63 emnlp-2013-Discourse Level Explanatory Relation Extraction from Product Reviews Using First-Order Logic</a></p>
<p>7 0.11116894 <a title="62-tfidf-7" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>8 0.11114489 <a title="62-tfidf-8" href="./emnlp-2013-The_VerbCorner_Project%3A_Toward_an_Empirically-Based_Semantic_Decomposition_of_Verbs.html">183 emnlp-2013-The VerbCorner Project: Toward an Empirically-Based Semantic Decomposition of Verbs</a></p>
<p>9 0.094305135 <a title="62-tfidf-9" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>10 0.093398362 <a title="62-tfidf-10" href="./emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model.html">75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</a></p>
<p>11 0.08381246 <a title="62-tfidf-11" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>12 0.080545448 <a title="62-tfidf-12" href="./emnlp-2013-Learning_Topics_and_Positions_from_Debatepedia.html">121 emnlp-2013-Learning Topics and Positions from Debatepedia</a></p>
<p>13 0.070143931 <a title="62-tfidf-13" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>14 0.069756046 <a title="62-tfidf-14" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>15 0.069598615 <a title="62-tfidf-15" href="./emnlp-2013-Learning_Biological_Processes_with_Global_Constraints.html">118 emnlp-2013-Learning Biological Processes with Global Constraints</a></p>
<p>16 0.068756662 <a title="62-tfidf-16" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>17 0.066860028 <a title="62-tfidf-17" href="./emnlp-2013-Learning_Distributions_over_Logical_Forms_for_Referring_Expression_Generation.html">119 emnlp-2013-Learning Distributions over Logical Forms for Referring Expression Generation</a></p>
<p>18 0.066619262 <a title="62-tfidf-18" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>19 0.065926954 <a title="62-tfidf-19" href="./emnlp-2013-Exploiting_Zero_Pronouns_to_Improve_Chinese_Coreference_Resolution.html">80 emnlp-2013-Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</a></p>
<p>20 0.063498333 <a title="62-tfidf-20" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.202), (1, 0.142), (2, -0.026), (3, -0.025), (4, 0.028), (5, 0.123), (6, -0.107), (7, 0.014), (8, 0.168), (9, 0.106), (10, 0.096), (11, -0.089), (12, 0.115), (13, -0.045), (14, -0.026), (15, 0.142), (16, -0.04), (17, -0.1), (18, 0.005), (19, -0.03), (20, 0.021), (21, 0.122), (22, -0.124), (23, -0.072), (24, 0.046), (25, 0.15), (26, -0.111), (27, 0.161), (28, 0.088), (29, -0.018), (30, -0.006), (31, 0.013), (32, 0.021), (33, -0.092), (34, -0.054), (35, -0.0), (36, 0.065), (37, 0.048), (38, -0.165), (39, 0.015), (40, -0.078), (41, 0.109), (42, -0.019), (43, -0.147), (44, -0.172), (45, -0.036), (46, 0.027), (47, 0.013), (48, 0.04), (49, 0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96038955 <a title="62-lsi-1" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>Author: Wiltrud Kessler ; Jonas Kuhn</p><p>Abstract: This short paper presents a pilot study investigating the training of a standard Semantic Role Labeling (SRL) system on product reviews for the new task of detecting comparisons. An (opinionated) comparison consists of a comparative “predicate” and up to three “arguments”: the entity evaluated positively, the entity evaluated negatively, and the aspect under which the comparison is made. In user-generated product reviews, the “predicate” and “arguments” are expressed in highly heterogeneous ways; but since the elements are textually annotated in existing datasets, SRL is technically applicable. We address the interesting question how well training an outof-the-box SRL model works for English data. We observe that even without any feature engineering or other major adaptions to our task, the system outperforms a reasonable heuristic baseline in all steps (predicate identification, argument identification and argument classification) and in three different datasets.</p><p>2 0.71775967 <a title="62-lsi-2" href="./emnlp-2013-The_VerbCorner_Project%3A_Toward_an_Empirically-Based_Semantic_Decomposition_of_Verbs.html">183 emnlp-2013-The VerbCorner Project: Toward an Empirically-Based Semantic Decomposition of Verbs</a></p>
<p>Author: Joshua K. Hartshorne ; Claire Bonial ; Martha Palmer</p><p>Abstract: This research describes efforts to use crowdsourcing to improve the validity of the semantic predicates in VerbNet, a lexicon of about 6300 English verbs. The current semantic predicates can be thought of semantic primitives, into which the concepts denoted by a verb can be decomposed. For example, the verb spray (of the Spray class), involves the predicates MOTION, NOT, and LOCATION, where the event can be decomposed into an AGENT causing a THEME that was originally not in a particular location to now be in that location. Although VerbNet’s predicates are theoretically well-motivated, systematic empirical data is scarce. This paper describes a recently-launched attempt to address this issue with a series of human judgment tasks, posed to subjects in the form of games.</p><p>3 0.66673821 <a title="62-lsi-3" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>Author: Eduardo Blanco ; Dan Moldovan</p><p>Abstract: This paper presents a novel approach to determine textual similarity. A layered methodology to transform text into logic forms is proposed, and semantic features are derived from a logic prover. Experimental results show that incorporating the semantic structure of sentences is beneficial. When training data is unavailable, scores obtained from the logic prover in an unsupervised manner outperform supervised methods.</p><p>4 0.56614703 <a title="62-lsi-4" href="./emnlp-2013-Unsupervised_Induction_of_Cross-Lingual_Semantic_Relations.html">193 emnlp-2013-Unsupervised Induction of Cross-Lingual Semantic Relations</a></p>
<p>Author: Mike Lewis ; Mark Steedman</p><p>Abstract: Creating a language-independent meaning representation would benefit many crosslingual NLP tasks. We introduce the first unsupervised approach to this problem, learning clusters of semantically equivalent English and French relations between referring expressions, based on their named-entity arguments in large monolingual corpora. The clusters can be used as language-independent semantic relations, by mapping clustered expressions in different languages onto the same relation. Our approach needs no parallel text for training, but outperforms a baseline that uses machine translation on a cross-lingual question answering task. We also show how to use the semantics to improve the accuracy of machine translation, by using it in a simple reranker.</p><p>5 0.54834569 <a title="62-lsi-5" href="./emnlp-2013-Automatic_Knowledge_Acquisition_for_Case_Alternation_between_the_Passive_and_Active_Voices_in_Japanese.html">33 emnlp-2013-Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in Japanese</a></p>
<p>Author: Ryohei Sasano ; Daisuke Kawahara ; Sadao Kurohashi ; Manabu Okumura</p><p>Abstract: We present a method for automatically acquiring knowledge for case alternation between the passive and active voices in Japanese. By leveraging several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, our method aligns a case frame in the passive voice to a corresponding case frame in the active voice and finds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness.</p><p>6 0.47426268 <a title="62-lsi-6" href="./emnlp-2013-Discourse_Level_Explanatory_Relation_Extraction_from_Product_Reviews_Using_First-Order_Logic.html">63 emnlp-2013-Discourse Level Explanatory Relation Extraction from Product Reviews Using First-Order Logic</a></p>
<p>7 0.46856335 <a title="62-lsi-7" href="./emnlp-2013-Semantic_Parsing_on_Freebase_from_Question-Answer_Pairs.html">166 emnlp-2013-Semantic Parsing on Freebase from Question-Answer Pairs</a></p>
<p>8 0.46463406 <a title="62-lsi-8" href="./emnlp-2013-Effectiveness_and_Efficiency_of_Open_Relation_Extraction.html">68 emnlp-2013-Effectiveness and Efficiency of Open Relation Extraction</a></p>
<p>9 0.40619817 <a title="62-lsi-9" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>10 0.39723772 <a title="62-lsi-10" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>11 0.35745588 <a title="62-lsi-11" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>12 0.33032992 <a title="62-lsi-12" href="./emnlp-2013-Learning_Topics_and_Positions_from_Debatepedia.html">121 emnlp-2013-Learning Topics and Positions from Debatepedia</a></p>
<p>13 0.32475519 <a title="62-lsi-13" href="./emnlp-2013-Automatically_Detecting_and_Attributing_Indirect_Quotations.html">35 emnlp-2013-Automatically Detecting and Attributing Indirect Quotations</a></p>
<p>14 0.31193197 <a title="62-lsi-14" href="./emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model.html">75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</a></p>
<p>15 0.31119192 <a title="62-lsi-15" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>16 0.30956808 <a title="62-lsi-16" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>17 0.30829927 <a title="62-lsi-17" href="./emnlp-2013-Rule-Based_Information_Extraction_is_Dead%21_Long_Live_Rule-Based_Information_Extraction_Systems%21.html">161 emnlp-2013-Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!</a></p>
<p>18 0.2969293 <a title="62-lsi-18" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>19 0.27169454 <a title="62-lsi-19" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>20 0.2676582 <a title="62-lsi-20" href="./emnlp-2013-Scaling_Semantic_Parsers_with_On-the-Fly_Ontology_Matching.html">164 emnlp-2013-Scaling Semantic Parsers with On-the-Fly Ontology Matching</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.039), (18, 0.05), (22, 0.05), (30, 0.048), (47, 0.011), (50, 0.017), (51, 0.218), (66, 0.034), (71, 0.046), (75, 0.039), (76, 0.281), (77, 0.012), (90, 0.023), (96, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80655211 <a title="62-lda-1" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>Author: Wiltrud Kessler ; Jonas Kuhn</p><p>Abstract: This short paper presents a pilot study investigating the training of a standard Semantic Role Labeling (SRL) system on product reviews for the new task of detecting comparisons. An (opinionated) comparison consists of a comparative “predicate” and up to three “arguments”: the entity evaluated positively, the entity evaluated negatively, and the aspect under which the comparison is made. In user-generated product reviews, the “predicate” and “arguments” are expressed in highly heterogeneous ways; but since the elements are textually annotated in existing datasets, SRL is technically applicable. We address the interesting question how well training an outof-the-box SRL model works for English data. We observe that even without any feature engineering or other major adaptions to our task, the system outperforms a reasonable heuristic baseline in all steps (predicate identification, argument identification and argument classification) and in three different datasets.</p><p>2 0.65105993 <a title="62-lda-2" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>Author: Jason Weston ; Antoine Bordes ; Oksana Yakhnenko ; Nicolas Usunier</p><p>Abstract: This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over methods that rely on text features alone.</p><p>3 0.64630616 <a title="62-lda-3" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>Author: Yangfeng Ji ; Jacob Eisenstein</p><p>Abstract: Matrix and tensor factorization have been applied to a number of semantic relatedness tasks, including paraphrase identification. The key idea is that similarity in the latent space implies semantic relatedness. We describe three ways in which labeled data can improve the accuracy of these approaches on paraphrase classification. First, we design a new discriminative term-weighting metric called TF-KLD, which outperforms TF-IDF. Next, we show that using the latent representation from matrix factorization as features in a classification algorithm substantially improves accuracy. Finally, we combine latent features with fine-grained n-gram overlap features, yielding performance that is 3% more accurate than the prior state-of-the-art.</p><p>4 0.64503574 <a title="62-lda-4" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>Author: Gary Patterson ; Andrew Kehler</p><p>Abstract: We present a classification model that predicts the presence or omission of a lexical connective between two clauses, based upon linguistic features of the clauses and the type of discourse relation holding between them. The model is trained on a set of high frequency relations extracted from the Penn Discourse Treebank and achieves an accuracy of 86.6%. Analysis of the results reveals that the most informative features relate to the discourse dependencies between sequences of coherence relations in the text. We also present results of an experiment that provides insight into the nature and difficulty of the task.</p><p>5 0.6449042 <a title="62-lda-5" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>Author: Zhengyan He ; Shujie Liu ; Yang Song ; Mu Li ; Ming Zhou ; Houfeng Wang</p><p>Abstract: Entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base. Recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes. We propose a fast collective disambiguation approach based on stacking. First, we train a local predictor g0 with learning to rank as base learner, to generate initial ranking list of candidates. Second, top k candidates of related instances are searched for constructing expressive global coherence features. A global predictor g1 is trained in the augmented feature space and stacking is employed to tackle the train/test mismatch problem. The proposed method is fast and easy to implement. Experiments show its effectiveness over various algorithms on several public datasets. By learning a rich semantic relatedness measure be- . tween entity categories and context document, performance is further improved.</p><p>6 0.64262909 <a title="62-lda-6" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>7 0.64197278 <a title="62-lda-7" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>8 0.64163733 <a title="62-lda-8" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>9 0.64098662 <a title="62-lda-9" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<p>10 0.640953 <a title="62-lda-10" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>11 0.64056355 <a title="62-lda-11" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>12 0.64018518 <a title="62-lda-12" href="./emnlp-2013-Unsupervised_Induction_of_Cross-Lingual_Semantic_Relations.html">193 emnlp-2013-Unsupervised Induction of Cross-Lingual Semantic Relations</a></p>
<p>13 0.63972425 <a title="62-lda-13" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>14 0.63788271 <a title="62-lda-14" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>15 0.63746506 <a title="62-lda-15" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>16 0.6373105 <a title="62-lda-16" href="./emnlp-2013-Scaling_Semantic_Parsers_with_On-the-Fly_Ontology_Matching.html">164 emnlp-2013-Scaling Semantic Parsers with On-the-Fly Ontology Matching</a></p>
<p>17 0.63337702 <a title="62-lda-17" href="./emnlp-2013-Exploiting_Zero_Pronouns_to_Improve_Chinese_Coreference_Resolution.html">80 emnlp-2013-Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</a></p>
<p>18 0.63319492 <a title="62-lda-18" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>19 0.63317031 <a title="62-lda-19" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>20 0.63314337 <a title="62-lda-20" href="./emnlp-2013-MCTest%3A_A_Challenge_Dataset_for_the_Open-Domain_Machine_Comprehension_of_Text.html">126 emnlp-2013-MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
