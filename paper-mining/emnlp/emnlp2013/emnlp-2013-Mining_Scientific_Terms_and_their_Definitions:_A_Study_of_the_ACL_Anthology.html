<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-132" href="#">emnlp2013-132</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</h1>
<br/><p>Source: <a title="emnlp-2013-132-pdf" href="http://aclweb.org/anthology//D/D13/D13-1073.pdf">pdf</a></p><p>Author: Yiping Jin ; Min-Yen Kan ; Jun-Ping Ng ; Xiangnan He</p><p>Abstract: This paper presents DefMiner, a supervised sequence labeling system that identifies scientific terms and their accompanying definitions. DefMiner achieves 85% F1 on a Wikipedia benchmark corpus, significantly improving the previous state-of-the-art by 8%. We exploit DefMiner to process the ACL Anthology Reference Corpus (ARC) – a large, real-world digital library of scientific articles in computational linguistics. The resulting automatically-acquired glossary represents the terminology defined over several thousand individual research articles. We highlight several interesting observations: more definitions are introduced for conference and workshop papers over the years and that multiword terms account for slightly less than half of all terms. Obtaining a list of popular , defined terms in a corpus ofcomputational linguistics papers, we find that concepts can often be categorized into one of three categories: resources, methodologies and evaluation metrics.</p><p>Reference: <a title="emnlp-2013-132-reference" href="../emnlp2013_reference/emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We exploit DefMiner to process the ACL Anthology Reference Corpus (ARC) – a large, real-world digital library of scientific articles in computational linguistics. [sent-3, score-0.228]
</p><p>2 We highlight several interesting observations: more definitions are introduced for conference and workshop papers over the years and that multiword terms account for slightly less than half of all terms. [sent-5, score-0.571]
</p><p>3 Another indirect  result of this leads to a second problem: lexical resources tend to be general, and may contain multiple definitions for a single term. [sent-16, score-0.431]
</p><p>4 For example, the term “CRF” connotes “Conditional Random Fields” in most modern computational linguistics literature; however, there are many definitions for this acronym in Wikipedia. [sent-17, score-0.673]
</p><p>5 Because only one correct sense applies, readers may need to expend effort to identify the appropriate meaning of a term in context. [sent-18, score-0.218]
</p><p>6 We address both issues in this work by automatically extracting terms and definitions directly from primary sources: scientific publications. [sent-19, score-0.595]
</p><p>7 Since most new technical terms are introduced in scientific publications, our extraction process addresses the bottleneck of staleness. [sent-20, score-0.241]
</p><p>8 Second, since science is organized into disciplines and sub-disciplines, we can exploit this inherent structure to gather contextual information about a term and its definition. [sent-21, score-0.248]
</p><p>9 hc o2d0s1 i3n A Nsastoucria lti Loan fgoura Cgoem Ppruotcaetsiosin agl, L piang eusis 7t8ic0s–790, 2  Related Work  The task of definition mining has attracted a fair amount of research interest. [sent-25, score-0.354]
</p><p>10 The output of such systems can be used to produce glossaries or answer definition questions. [sent-26, score-0.354]
</p><p>11 The primary model for this task in past work has been one of binary classification: does a sentence contain a definition or not? [sent-27, score-0.402]
</p><p>12 Muresan and Klavans (2002) developed a rule-based system to extract definitions from online medical articles. [sent-32, score-0.432]
</p><p>13 Definitions can be expressed in a variety of ways, making it difficult to develop an exhaustive set of rules to locate all definition sentences. [sent-39, score-0.354]
</p><p>14 Fahmi and Bouma (2006) made used of supervised machine learning to extract definitions from a corpus of Dutch Wikipedia pages in the medical domain. [sent-41, score-0.399]
</p><p>15 They showed that a baseline approach which simply classifies every first sentence as a definition works surprisingly well, achieving an accuracy of 75. [sent-42, score-0.402]
</p><p>16 (2009) implemented a fully automated system to extract and rank definitions based on genetic algorithms and genetic programming. [sent-57, score-0.496]
</p><p>17 They employed bootstrapping to extract glossary sentences from scientific articles in the ACL Anthology Reference Corpus (ACL ARC) (Bird et al. [sent-72, score-0.264]
</p><p>18 Their results show that bootstrapping is useful for definition extraction. [sent-74, score-0.354]
</p><p>19 They neither sufficiently exploit the intrinsic characteristics of the term and definition, nor invest effort to localize them within the sentence1 . [sent-77, score-0.248]
</p><p>20 Given the significant structure in definitions, we take a more fine-grained approach, isolating the term and its definition from sentences. [sent-78, score-0.572]
</p><p>21 Our task is thus to find pairs of terms and their associated definitions in input scholarly articles. [sent-81, score-0.568]
</p><p>22 The sentence-level task of deciding whether a sentence s is a definition sentence or not, is thus a simplification of our task. [sent-82, score-0.45]
</p><p>23 TW)ee post-process our Ol)abtheelre}r’s fo orre seualcths to achieve parity with the simplified definition sentence task: When we detect both a term’s and definition’s presence in a sentence, we deem the sentence a definition sentence. [sent-84, score-0.847]
</p><p>24 To be clear, this is a requirement; when we detect only either a term or a definition, we filter these out as false positives and do not include them as system output by definition in DefMiner, terms must appear within the same sentence as their definitions. [sent-85, score-0.72]
</p><p>25 –  –  To train our classifier, we need a corpus of definition sentences where all terms and definitions are 1While Navigli and Velardi (2010) tagged terms and definitions explicitly in their corpus, their evaluation restricts itself to the task of definition sentence identification. [sent-86, score-1.734]
</p><p>26 While Navigli and Velardi (2010) compiled the WCL definition corpus from the English Wikipedia pages, we note that Wikipedia has stylistic conventions that make detection of definitions much easier than in the general case (i. [sent-88, score-0.801]
</p><p>27 We took all the 2,512 sentences marked as definition sentences by at least one of our individual prototypes and proceeded to annotate all of them. [sent-99, score-0.476]
</p><p>28 In total, 865 of the total 2,512 sentences were real definition sentences. [sent-100, score-0.4]
</p><p>29 Therefore, a sentence that is not a definition sentence would have all its tokens marked as O. [sent-103, score-0.509]
</p><p>30 , 2001) to extract the term and definition from input. [sent-106, score-0.572]
</p><p>31 In contrast, definitions exhibit more flexible structure and hence are more difficult to distinguish from normal English text. [sent-123, score-0.399]
</p><p>32 , first utilizing the results from term classification, and then incorporating them into definition classification. [sent-127, score-0.572]
</p><p>33 During  that the syntactic  develop-  variation  of  the definition might benefit from features that identify long-distance dependencies. [sent-150, score-0.354]
</p><p>34 FC12) Typed dependency path (W): The dependency path from the current word to the root of the sentence (recording the dependency types instead of the words in the path). [sent-164, score-0.247]
</p><p>35 We not only benchmark DefMiner’s perWe  now  formance  assess  over our own W00 collection,  but also  DefMiner against previous published work on the definition sentence identification task on the  compare  WCL (English Wikipedia) corpus. [sent-177, score-0.402]
</p><p>36 We calculate both micro and macro- (category) averaged F1 scores for term and definition extraction. [sent-182, score-0.572]
</p><p>37 As definition tokens greatly outnumber term tokens in our corpus (roughly 6: 1), we feel that the macroaverage is a better indicator of the balance between term and definition identification. [sent-184, score-1.238]
</p><p>38 Our best single-stage system (System 9 in Table 3) boosts recall for term and definition classification by 7% and 5%, respectively, without sacrificing precision. [sent-189, score-0.639]
</p><p>39 2 Serial Term and Definition Classification We now investigate the two-stage, serial architecture where the system first performs term classification before definition classification (i. [sent-198, score-0.753]
</p><p>40 ier with three additional features from the first-stage term classification output: whether the current word (1) is a term, and (2) appears before or (3) after a term. [sent-204, score-0.252]
</p><p>41 Interestingly, there is a 10% increase in the precision of definition classification. [sent-206, score-0.354]
</p><p>42 The results verify our intuition that term classification does help in definition classification. [sent-210, score-0.606]
</p><p>43 To determine the upper bound performance that could result from proper term identification, we provided correct, oracular term labels from our ground truth annotations in our corpus to the second-stage definition classifier. [sent-215, score-0.827]
</p><p>44 This scenario effectively upperbounds the performance that perfect term knowledge has on definition classification. [sent-216, score-0.572]
</p><p>45 The results of this system in Row 12 indicates a strong positive influence on definition extraction, improving definition  extraction from 49% to 80%, a leap of 31%. [sent-217, score-0.786]
</p><p>46 motivates future work as how to improve the performance of the term classifier so as to reap the benefits possible with our two-stage classifier. [sent-222, score-0.255]
</p><p>47 (2012) is the only attempt to extract definitions from the ACL ARC corpus, which is a superset of our W00 corpus. [sent-226, score-0.399]
</p><p>48 To directly compare with the previous, more complex state-of-the-art system from (Navigli and Velardi, 2010), we evaluate DefMiner on the definition sentence detection task. [sent-228, score-0.435]
</p><p>49 For the sentence-level evaluation, we calculate the P/R/F1 score based on whether the sentence is a definition sentence. [sent-229, score-0.402]
</p><p>50 We randomized the definition and none-definition sentences in their corpus and applied 10-folds cross validation. [sent-231, score-0.4]
</p><p>51 Even using just the simple heuristic of only classifying sentences that have identified terms as well as definitions as definition sentences, DefMiner serves to competitively identify definition sentences. [sent-245, score-1.22]
</p><p>52 DefMiner identifies 703 and 1,217 sentences in W01 and W02 as definition sentences separately. [sent-248, score-0.446]
</p><p>53 8% of the extracted sentences are real definition sentences, while the remaining are false positives. [sent-250, score-0.4]
</p><p>54 As our two-stage classifier still lags behind the system with oracular term labels by 24% in F1 for definition detection (Section 4. [sent-255, score-0.679]
</p><p>55 We show three example misclassified sentences that represent the major types of errors we observed, where DefMiner’s output annotations follow tokens marked as part of terms or definitions. [sent-257, score-0.206]
</p><p>56 This first instance shows that DefMiner tends to mark the first several tokens as “TERM” while the real term appears somewhere else in the sentence. [sent-260, score-0.247]
</p><p>57 The actual term being defined is “closed features” instead of “PSS”. [sent-261, score-0.248]
</p><p>58 It may be useful to thus model the (usual) distance between the term and its definition in a feature in future work. [sent-264, score-0.572]
</p><p>59 DefMiner is occasionally confused when encountering recursive definition or multiple definitions in a single sentence. [sent-266, score-0.753]
</p><p>60 In sentence 3), if we just look at part of the sentence “a nonspecialist is a key advantage of textual summaries over graphs”, without trying to understand the meaning of the sentence, we may well conclude that it is a definition sentence because of the cue phrase “is a”. [sent-272, score-0.498]
</p><p>61 But clearly, the whole sentence is not a definition sentence. [sent-273, score-0.402]
</p><p>62 We trained a model using the whole of the W00 corpus and used the obtained classifier to identify a list of terms and definitions for each publication in the ACL ARC. [sent-280, score-0.541]
</p><p>63 1 Demographics From a term’s perspective we can introspect properties of the enclosing paper, the host sentence, the term itself and its definition. [sent-283, score-0.261]
</p><p>64 At the sentence level, we analyze the position of the sentences that are definition sentences. [sent-285, score-0.489]
</p><p>65 How many words or clauses do definition sentences consist of? [sent-292, score-0.4]
</p><p>66 Do we lose a lot of recall by restricting definitions to a single sentence? [sent-293, score-0.399]
</p><p>67 Are embedded definitions (definitions embedded with other definitions) common? [sent-294, score-0.399]
</p><p>68 We highlight some specific findings from our basic analyses here: Where do definitions occur? [sent-295, score-0.399]
</p><p>69 As terms are usually defined on first use, we expect the distribution of definition sentences to skew towards the beginning portions of a scientific document as input. [sent-296, score-0.626]
</p><p>70 We count the occurrences of definition sentences in each of ten equally-sized (by number of sentences) nonoverlapping partitions. [sent-297, score-0.429]
</p><p>71 The results are shown in Figure 1, aligning with our intuition: The first three quantiles contribute almost 40% of all detected definition sentences, while the last three quantiles contain only 17. [sent-298, score-0.512]
</p><p>72 uycre nqF26480 0 0 0 5819 6258 65734 6204 512 5026 4371 35983 2931 2130 =  Quan,le (1 f irst 10%; 10  = last 10%)  Figure 1: Occurrences of definitions within different segments of an article. [sent-300, score-0.399]
</p><p>73 Over 54% of the detected terms are single tokens, where majority of the remaining 45% of terms being multi-word terms of six words or less. [sent-303, score-0.273]
</p><p>74 Slightly over half of the definitions have a length of 5–16 words; 75% have lengths between 3 and 23 words. [sent-309, score-0.399]
</p><p>75 We notice that determiners and prepositions are absent from the term list but are common in definitions. [sent-325, score-0.218]
</p><p>76 We study if definitions appear as frequently in different types of scientific articles (e. [sent-329, score-0.566]
</p><p>77 We also want to investigate if there is a significant shift in the distribution of definitions across years. [sent-332, score-0.399]
</p><p>78 In Figure 3, we present the density of definitions (defined as the  nitions. [sent-333, score-0.399]
</p><p>79 percentage of sentences that are identified as definition sentences), for these three different categories of publications6. [sent-334, score-0.4]
</p><p>80 In Figure 3, the three data series overlap each other, so we cannot conclude definitions appear more often in one type of papers than another. [sent-335, score-0.465]
</p><p>81 However, as a side effect, we see that while the definition density forjournal papers remain relatively constant, for conference and workshop papers the number of  definitions extracted per sentence has increased noticeably over time. [sent-336, score-0.933]
</p><p>82 The average number of definitions presented in conference papers, for instance, increased more than 100% in the 40 years represented in the ACL ARC. [sent-337, score-0.438]
</p><p>83 The increasing number of definitions alone does not show that new knowledge is introduced at a faster rate, as definitions may be repeated. [sent-338, score-0.798]
</p><p>84 To control for this effect, we also need to know which definitions are new or defined in previous year(s). [sent-339, score-0.429]
</p><p>85 For journal papers, the number of definitions of previously introduced terms in each year against the number of new definitions. [sent-341, score-0.502]
</p><p>86 We say a definition is new when the detected term was not identified in any article (not limited to journals) in previous years. [sent-342, score-0.644]
</p><p>87 The area between the two lines denotes the definitions 6The ACL ARC is organized by the venue ofthe publication, which is associated to a category. [sent-345, score-0.45]
</p><p>88 788 where the same term has been multiply defined from the same or previous years as the current year under investigation. [sent-347, score-0.323]
</p><p>89 3 Trends We can use terms and definitions to also introspect how the computational linguistics literature has changed over time. [sent-349, score-0.509]
</p><p>90 –  –  Conference Papers  New Terms  Journals  Workshops  Previously Defined Terms  Figure 3: Occurrences of definitions across publication cat- Figure 4: Relative proportions of new and recurring defini-  egories. [sent-366, score-0.437]
</p><p>91 Figure 5: The occurrence of definitions for various sequence labeling methodologies over the years. [sent-367, score-0.555]
</p><p>92 6  Conclusions and Future Work  We study the task ofidentifying definition sentences, as a two-part entity containing a term and its accompanying definition. [sent-368, score-0.623]
</p><p>93 Unlike previous work, we propose the harder task of delimiting the component term and definitions, which admits sequence labeling methodologies as a compatible solution. [sent-369, score-0.374]
</p><p>94 Leveraging the current best practice of using conditional random fields, we contribute two additional ideas that lead to DefMiner, a state-of-the-art scholarly definition mining system. [sent-370, score-0.488]
</p><p>95 Second, viewing the problem as two correlated subproblems of term and definition extraction, we measure the tightness and dependency of the correlation. [sent-372, score-0.619]
</p><p>96 find that a two-stage sequential learning architecture  (term first, definition second) leads to best performance. [sent-374, score-0.387]
</p><p>97 Downstream systems may predict which term will become popular, or could alert an author if their definition of a term significantly differs from the original source. [sent-380, score-0.79]
</p><p>98 We hope to tackle the annotation bottleneck in future work on definition extraction, common in many data-driven learning fields. [sent-381, score-0.354]
</p><p>99 The expression of definitions in specialised texts: a corpus-based analysis. [sent-435, score-0.399]
</p><p>100 Extracting glossary sentences from scholarly articles: a comparative evaluation of pattern bootstrapping and deep analysis. [sent-439, score-0.232]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('defminer', 0.47), ('definitions', 0.399), ('definition', 0.354), ('term', 0.218), ('scientific', 0.129), ('wcl', 0.128), ('westerhout', 0.128), ('arc', 0.114), ('velardi', 0.112), ('anthology', 0.111), ('np', 0.104), ('scholarly', 0.102), ('navigli', 0.099), ('keyphrase', 0.093), ('reiplinger', 0.093), ('shallow', 0.088), ('fahmi', 0.086), ('fmacro', 0.086), ('methodologies', 0.085), ('detected', 0.072), ('terms', 0.067), ('papers', 0.066), ('bouma', 0.056), ('keyphrases', 0.056), ('acronym', 0.056), ('wikipedia', 0.055), ('terminology', 0.052), ('accompanying', 0.051), ('glossary', 0.051), ('journals', 0.051), ('venue', 0.051), ('patterns', 0.05), ('fields', 0.049), ('compiled', 0.048), ('sentence', 0.048), ('serial', 0.047), ('orthography', 0.047), ('crf', 0.047), ('dependency', 0.047), ('tag', 0.047), ('sentences', 0.046), ('extraction', 0.045), ('muresan', 0.045), ('idf', 0.044), ('borg', 0.043), ('deem', 0.043), ('eline', 0.043), ('introspect', 0.043), ('kea', 0.043), ('monachesi', 0.043), ('pss', 0.043), ('quantiles', 0.043), ('xiangnan', 0.043), ('yiping', 0.043), ('star', 0.043), ('acl', 0.042), ('position', 0.041), ('capitalized', 0.041), ('years', 0.039), ('labeling', 0.039), ('articles', 0.038), ('singapore', 0.038), ('publication', 0.038), ('hmm', 0.037), ('oracular', 0.037), ('wde', 0.037), ('memm', 0.037), ('classifier', 0.037), ('publications', 0.036), ('feel', 0.036), ('bird', 0.036), ('year', 0.036), ('reference', 0.035), ('prototype', 0.034), ('opennlp', 0.034), ('governor', 0.034), ('misclassified', 0.034), ('classification', 0.034), ('comparative', 0.033), ('system', 0.033), ('architecture', 0.033), ('sequence', 0.032), ('conditional', 0.032), ('resources', 0.032), ('klavans', 0.032), ('genetic', 0.032), ('inspection', 0.031), ('crfs', 0.031), ('digital', 0.031), ('exploit', 0.03), ('defined', 0.03), ('paola', 0.03), ('framenet', 0.03), ('propbank', 0.03), ('isa', 0.03), ('marked', 0.03), ('occurrences', 0.029), ('insights', 0.029), ('tokens', 0.029), ('path', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="132-tfidf-1" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<p>Author: Yiping Jin ; Min-Yen Kan ; Jun-Ping Ng ; Xiangnan He</p><p>Abstract: This paper presents DefMiner, a supervised sequence labeling system that identifies scientific terms and their accompanying definitions. DefMiner achieves 85% F1 on a Wikipedia benchmark corpus, significantly improving the previous state-of-the-art by 8%. We exploit DefMiner to process the ACL Anthology Reference Corpus (ARC) – a large, real-world digital library of scientific articles in computational linguistics. The resulting automatically-acquired glossary represents the terminology defined over several thousand individual research articles. We highlight several interesting observations: more definitions are introduced for conference and workshop papers over the years and that multiword terms account for slightly less than half of all terms. Obtaining a list of popular , defined terms in a corpus ofcomputational linguistics papers, we find that concepts can often be categorized into one of three categories: resources, methodologies and evaluation metrics.</p><p>2 0.16315299 <a title="132-tfidf-2" href="./emnlp-2013-Growing_Multi-Domain_Glossaries_from_a_Few_Seeds_using_Probabilistic_Topic_Models.html">92 emnlp-2013-Growing Multi-Domain Glossaries from a Few Seeds using Probabilistic Topic Models</a></p>
<p>Author: Stefano Faralli ; Roberto Navigli</p><p>Abstract: In this paper we present a minimallysupervised approach to the multi-domain acquisition ofwide-coverage glossaries. We start from a small number of hypernymy relation seeds and bootstrap glossaries from the Web for dozens of domains using Probabilistic Topic Models. Our experiments show that we are able to extract high-precision glossaries comprising thousands of terms and definitions.</p><p>3 0.073774107 <a title="132-tfidf-3" href="./emnlp-2013-Automatic_Idiom_Identification_in_Wiktionary.html">32 emnlp-2013-Automatic Idiom Identification in Wiktionary</a></p>
<p>Author: Grace Muzny ; Luke Zettlemoyer</p><p>Abstract: Online resources, such as Wiktionary, provide an accurate but incomplete source ofidiomatic phrases. In this paper, we study the problem of automatically identifying idiomatic dictionary entries with such resources. We train an idiom classifier on a newly gathered corpus of over 60,000 Wiktionary multi-word definitions, incorporating features that model whether phrase meanings are constructed compositionally. Experiments demonstrate that the learned classifier can provide high quality idiom labels, more than doubling the number of idiomatic entries from 7,764 to 18,155 at precision levels of over 65%. These gains also translate to idiom detection in sentences, by simply using known word sense disambiguation algorithms to match phrases to their definitions. In a set of Wiktionary definition example sentences, the more complete set of idioms boosts detection recall by over 28 percentage points.</p><p>4 0.064482436 <a title="132-tfidf-4" href="./emnlp-2013-Modeling_Scientific_Impact_with_Topical_Influence_Regression.html">133 emnlp-2013-Modeling Scientific Impact with Topical Influence Regression</a></p>
<p>Author: James Foulds ; Padhraic Smyth</p><p>Abstract: When reviewing scientific literature, it would be useful to have automatic tools that identify the most influential scientific articles as well as how ideas propagate between articles. In this context, this paper introduces topical influence, a quantitative measure of the extent to which an article tends to spread its topics to the articles that cite it. Given the text of the articles and their citation graph, we show how to learn a probabilistic model to recover both the degree of topical influence of each article and the influence relationships between articles. Experimental results on corpora from two well-known computer science conferences are used to illustrate and validate the proposed approach.</p><p>5 0.058278468 <a title="132-tfidf-5" href="./emnlp-2013-Automatic_Feature_Engineering_for_Answer_Selection_and_Extraction.html">31 emnlp-2013-Automatic Feature Engineering for Answer Selection and Extraction</a></p>
<p>Author: Aliaksei Severyn ; Alessandro Moschitti</p><p>Abstract: This paper proposes a framework for automatically engineering features for two important tasks of question answering: answer sentence selection and answer extraction. We represent question and answer sentence pairs with linguistic structures enriched by semantic information, where the latter is produced by automatic classifiers, e.g., question classifier and Named Entity Recognizer. Tree kernels applied to such structures enable a simple way to generate highly discriminative structural features that combine syntactic and semantic information encoded in the input trees. We conduct experiments on a public benchmark from TREC to compare with previous systems for answer sentence selection and answer extraction. The results show that our models greatly improve on the state of the art, e.g., up to 22% on F1 (relative improvement) for answer extraction, while using no additional resources and no manual feature engineering.</p><p>6 0.058211811 <a title="132-tfidf-6" href="./emnlp-2013-A_Discourse-Driven_Content_Model_for_Summarising_Scientific_Articles_Evaluated_in_a_Complex_Question_Answering_Task.html">5 emnlp-2013-A Discourse-Driven Content Model for Summarising Scientific Articles Evaluated in a Complex Question Answering Task</a></p>
<p>7 0.056481432 <a title="132-tfidf-7" href="./emnlp-2013-Application_of_Localized_Similarity_for_Web_Documents.html">24 emnlp-2013-Application of Localized Similarity for Web Documents</a></p>
<p>8 0.055512868 <a title="132-tfidf-8" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>9 0.055368476 <a title="132-tfidf-9" href="./emnlp-2013-Learning_Topics_and_Positions_from_Debatepedia.html">121 emnlp-2013-Learning Topics and Positions from Debatepedia</a></p>
<p>10 0.055366635 <a title="132-tfidf-10" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>11 0.054958034 <a title="132-tfidf-11" href="./emnlp-2013-Animacy_Detection_with_Voting_Models.html">23 emnlp-2013-Animacy Detection with Voting Models</a></p>
<p>12 0.053787 <a title="132-tfidf-12" href="./emnlp-2013-Word_Level_Language_Identification_in_Online_Multilingual_Communication.html">204 emnlp-2013-Word Level Language Identification in Online Multilingual Communication</a></p>
<p>13 0.05334869 <a title="132-tfidf-13" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>14 0.052505098 <a title="132-tfidf-14" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>15 0.052408885 <a title="132-tfidf-15" href="./emnlp-2013-Efficient_Higher-Order_CRFs_for_Morphological_Tagging.html">70 emnlp-2013-Efficient Higher-Order CRFs for Morphological Tagging</a></p>
<p>16 0.052028779 <a title="132-tfidf-16" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>17 0.05160946 <a title="132-tfidf-17" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>18 0.048932876 <a title="132-tfidf-18" href="./emnlp-2013-Detecting_Promotional_Content_in_Wikipedia.html">61 emnlp-2013-Detecting Promotional Content in Wikipedia</a></p>
<p>19 0.048159223 <a title="132-tfidf-19" href="./emnlp-2013-Ubertagging%3A_Joint_Segmentation_and_Supertagging_for_English.html">190 emnlp-2013-Ubertagging: Joint Segmentation and Supertagging for English</a></p>
<p>20 0.047253828 <a title="132-tfidf-20" href="./emnlp-2013-Effectiveness_and_Efficiency_of_Open_Relation_Extraction.html">68 emnlp-2013-Effectiveness and Efficiency of Open Relation Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.202), (1, 0.033), (2, -0.024), (3, -0.016), (4, -0.053), (5, 0.022), (6, 0.034), (7, 0.021), (8, 0.004), (9, -0.033), (10, -0.0), (11, 0.035), (12, -0.048), (13, 0.069), (14, 0.052), (15, 0.061), (16, -0.106), (17, 0.032), (18, 0.008), (19, 0.027), (20, -0.016), (21, 0.127), (22, -0.011), (23, 0.081), (24, 0.116), (25, -0.06), (26, 0.141), (27, -0.016), (28, -0.009), (29, -0.077), (30, -0.19), (31, 0.081), (32, -0.065), (33, -0.157), (34, 0.028), (35, 0.057), (36, 0.216), (37, -0.105), (38, 0.089), (39, -0.001), (40, -0.053), (41, 0.054), (42, -0.123), (43, -0.192), (44, 0.135), (45, -0.093), (46, -0.043), (47, 0.136), (48, 0.118), (49, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95236099 <a title="132-lsi-1" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<p>Author: Yiping Jin ; Min-Yen Kan ; Jun-Ping Ng ; Xiangnan He</p><p>Abstract: This paper presents DefMiner, a supervised sequence labeling system that identifies scientific terms and their accompanying definitions. DefMiner achieves 85% F1 on a Wikipedia benchmark corpus, significantly improving the previous state-of-the-art by 8%. We exploit DefMiner to process the ACL Anthology Reference Corpus (ARC) – a large, real-world digital library of scientific articles in computational linguistics. The resulting automatically-acquired glossary represents the terminology defined over several thousand individual research articles. We highlight several interesting observations: more definitions are introduced for conference and workshop papers over the years and that multiword terms account for slightly less than half of all terms. Obtaining a list of popular , defined terms in a corpus ofcomputational linguistics papers, we find that concepts can often be categorized into one of three categories: resources, methodologies and evaluation metrics.</p><p>2 0.84063941 <a title="132-lsi-2" href="./emnlp-2013-Growing_Multi-Domain_Glossaries_from_a_Few_Seeds_using_Probabilistic_Topic_Models.html">92 emnlp-2013-Growing Multi-Domain Glossaries from a Few Seeds using Probabilistic Topic Models</a></p>
<p>Author: Stefano Faralli ; Roberto Navigli</p><p>Abstract: In this paper we present a minimallysupervised approach to the multi-domain acquisition ofwide-coverage glossaries. We start from a small number of hypernymy relation seeds and bootstrap glossaries from the Web for dozens of domains using Probabilistic Topic Models. Our experiments show that we are able to extract high-precision glossaries comprising thousands of terms and definitions.</p><p>3 0.54976034 <a title="132-lsi-3" href="./emnlp-2013-Animacy_Detection_with_Voting_Models.html">23 emnlp-2013-Animacy Detection with Voting Models</a></p>
<p>Author: Joshua Moore ; Christopher J.C. Burges ; Erin Renshaw ; Wen-tau Yih</p><p>Abstract: Animacy detection is a problem whose solution has been shown to be beneficial for a number of syntactic and semantic tasks. We present a state-of-the-art system for this task which uses a number of simple classifiers with heterogeneous data sources in a voting scheme. We show how this framework can give us direct insight into the behavior of the system, allowing us to more easily diagnose sources of error.</p><p>4 0.51091927 <a title="132-lsi-4" href="./emnlp-2013-Automatic_Idiom_Identification_in_Wiktionary.html">32 emnlp-2013-Automatic Idiom Identification in Wiktionary</a></p>
<p>Author: Grace Muzny ; Luke Zettlemoyer</p><p>Abstract: Online resources, such as Wiktionary, provide an accurate but incomplete source ofidiomatic phrases. In this paper, we study the problem of automatically identifying idiomatic dictionary entries with such resources. We train an idiom classifier on a newly gathered corpus of over 60,000 Wiktionary multi-word definitions, incorporating features that model whether phrase meanings are constructed compositionally. Experiments demonstrate that the learned classifier can provide high quality idiom labels, more than doubling the number of idiomatic entries from 7,764 to 18,155 at precision levels of over 65%. These gains also translate to idiom detection in sentences, by simply using known word sense disambiguation algorithms to match phrases to their definitions. In a set of Wiktionary definition example sentences, the more complete set of idioms boosts detection recall by over 28 percentage points.</p><p>5 0.45062098 <a title="132-lsi-5" href="./emnlp-2013-Tree_Kernel-based_Negation_and_Speculation_Scope_Detection_with_Structured_Syntactic_Parse_Features.html">188 emnlp-2013-Tree Kernel-based Negation and Speculation Scope Detection with Structured Syntactic Parse Features</a></p>
<p>Author: Bowei Zou ; Guodong Zhou ; Qiaoming Zhu</p><p>Abstract: Scope detection is a key task in information extraction. This paper proposes a new approach for tree kernel-based scope detection by using the structured syntactic parse information. In addition, we have explored the way of selecting compatible features for different part-of-speech cues. Experiments on the BioScope corpus show that both constituent and dependency structured syntactic parse features have the advantage in capturing the potential relationships between cues and their scopes. Compared with the state of the art scope detection systems, our system achieves substantial improvement.</p><p>6 0.41957447 <a title="132-lsi-6" href="./emnlp-2013-Rule-Based_Information_Extraction_is_Dead%21_Long_Live_Rule-Based_Information_Extraction_Systems%21.html">161 emnlp-2013-Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!</a></p>
<p>7 0.37117729 <a title="132-lsi-7" href="./emnlp-2013-Modeling_Scientific_Impact_with_Topical_Influence_Regression.html">133 emnlp-2013-Modeling Scientific Impact with Topical Influence Regression</a></p>
<p>8 0.3622956 <a title="132-lsi-8" href="./emnlp-2013-Assembling_the_Kazakh_Language_Corpus.html">26 emnlp-2013-Assembling the Kazakh Language Corpus</a></p>
<p>9 0.36102781 <a title="132-lsi-9" href="./emnlp-2013-Measuring_Ideological_Proportions_in_Political_Speeches.html">129 emnlp-2013-Measuring Ideological Proportions in Political Speeches</a></p>
<p>10 0.33890435 <a title="132-lsi-10" href="./emnlp-2013-This_Text_Has_the_Scent_of_Starbucks%3A_A_Laplacian_Structured_Sparsity_Model_for_Computational_Branding_Analytics.html">184 emnlp-2013-This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</a></p>
<p>11 0.33502287 <a title="132-lsi-11" href="./emnlp-2013-A_Discourse-Driven_Content_Model_for_Summarising_Scientific_Articles_Evaluated_in_a_Complex_Question_Answering_Task.html">5 emnlp-2013-A Discourse-Driven Content Model for Summarising Scientific Articles Evaluated in a Complex Question Answering Task</a></p>
<p>12 0.33195025 <a title="132-lsi-12" href="./emnlp-2013-Two-Stage_Method_for_Large-Scale_Acquisition_of_Contradiction_Pattern_Pairs_using_Entailment.html">189 emnlp-2013-Two-Stage Method for Large-Scale Acquisition of Contradiction Pattern Pairs using Entailment</a></p>
<p>13 0.3282463 <a title="132-lsi-13" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>14 0.32599971 <a title="132-lsi-14" href="./emnlp-2013-Elephant%3A_Sequence_Labeling_for_Word_and_Sentence_Segmentation.html">72 emnlp-2013-Elephant: Sequence Labeling for Word and Sentence Segmentation</a></p>
<p>15 0.32062346 <a title="132-lsi-15" href="./emnlp-2013-Scaling_to_Large3_Data%3A_An_Efficient_and_Effective_Method_to_Compute_Distributional_Thesauri.html">165 emnlp-2013-Scaling to Large3 Data: An Efficient and Effective Method to Compute Distributional Thesauri</a></p>
<p>16 0.31958786 <a title="132-lsi-16" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<p>17 0.31782877 <a title="132-lsi-17" href="./emnlp-2013-Detecting_Promotional_Content_in_Wikipedia.html">61 emnlp-2013-Detecting Promotional Content in Wikipedia</a></p>
<p>18 0.31362653 <a title="132-lsi-18" href="./emnlp-2013-Automatically_Detecting_and_Attributing_Indirect_Quotations.html">35 emnlp-2013-Automatically Detecting and Attributing Indirect Quotations</a></p>
<p>19 0.30959904 <a title="132-lsi-19" href="./emnlp-2013-Using_Soft_Constraints_in_Joint_Inference_for_Clinical_Concept_Recognition.html">198 emnlp-2013-Using Soft Constraints in Joint Inference for Clinical Concept Recognition</a></p>
<p>20 0.30888838 <a title="132-lsi-20" href="./emnlp-2013-Building_Specialized_Bilingual_Lexicons_Using_Large_Scale_Background_Knowledge.html">42 emnlp-2013-Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.061), (18, 0.032), (22, 0.047), (30, 0.112), (45, 0.015), (47, 0.014), (50, 0.02), (51, 0.224), (66, 0.032), (71, 0.043), (75, 0.037), (77, 0.016), (83, 0.207), (96, 0.023), (97, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86666256 <a title="132-lda-1" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>Author: Ivan Vulic ; Marie-Francine Moens</p><p>Abstract: We present a new language pair agnostic approach to inducing bilingual vector spaces from non-parallel data without any other resource in a bootstrapping fashion. The paper systematically introduces and describes all key elements of the bootstrapping procedure: (1) starting point or seed lexicon, (2) the confidence estimation and selection of new dimensions of the space, and (3) convergence. We test the quality of the induced bilingual vector spaces, and analyze the influence of the different components of the bootstrapping approach in the task of bilingual lexicon extraction (BLE) for two language pairs. Results reveal that, contrary to conclusions from prior work, the seeding of the bootstrapping process has a heavy impact on the quality of the learned lexicons. We also show that our approach outperforms the best performing fully corpus-based BLE methods on these test sets.</p><p>same-paper 2 0.85045475 <a title="132-lda-2" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<p>Author: Yiping Jin ; Min-Yen Kan ; Jun-Ping Ng ; Xiangnan He</p><p>Abstract: This paper presents DefMiner, a supervised sequence labeling system that identifies scientific terms and their accompanying definitions. DefMiner achieves 85% F1 on a Wikipedia benchmark corpus, significantly improving the previous state-of-the-art by 8%. We exploit DefMiner to process the ACL Anthology Reference Corpus (ARC) – a large, real-world digital library of scientific articles in computational linguistics. The resulting automatically-acquired glossary represents the terminology defined over several thousand individual research articles. We highlight several interesting observations: more definitions are introduced for conference and workshop papers over the years and that multiword terms account for slightly less than half of all terms. Obtaining a list of popular , defined terms in a corpus ofcomputational linguistics papers, we find that concepts can often be categorized into one of three categories: resources, methodologies and evaluation metrics.</p><p>3 0.77721685 <a title="132-lda-3" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>Author: Xiaoqing Zheng ; Hanyang Chen ; Tianyu Xu</p><p>Abstract: This study explores the feasibility of performing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representation of Chinese characters, and use these improved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-theart performance with minimal computational cost. We also describe a perceptron-style algorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented.</p><p>4 0.77126026 <a title="132-lda-4" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>Author: Kuzman Ganchev ; Dipanjan Das</p><p>Abstract: We present a framework for cross-lingual transfer of sequence information from a resource-rich source language to a resourceimpoverished target language that incorporates soft constraints via posterior regularization. To this end, we use automatically word aligned bitext between the source and target language pair, and learn a discriminative conditional random field model on the target side. Our posterior regularization constraints are derived from simple intuitions about the task at hand and from cross-lingual alignment information. We show improvements over strong baselines for two tasks: part-of-speech tagging and namedentity segmentation.</p><p>5 0.76916027 <a title="132-lda-5" href="./emnlp-2013-Semi-Markov_Phrase-Based_Monolingual_Alignment.html">167 emnlp-2013-Semi-Markov Phrase-Based Monolingual Alignment</a></p>
<p>Author: Xuchen Yao ; Benjamin Van Durme ; Chris Callison-Burch ; Peter Clark</p><p>Abstract: We introduce a novel discriminative model for phrase-based monolingual alignment using a semi-Markov CRF. Our model achieves stateof-the-art alignment accuracy on two phrasebased alignment datasets (RTE and paraphrase), while doing significantly better than other strong baselines in both non-identical alignment and phrase-only alignment. Additional experiments highlight the potential benefit of our alignment model to RTE, paraphrase identification and question answering, where even a naive application of our model’s alignment score approaches the state ofthe art.</p><p>6 0.76847959 <a title="132-lda-6" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>7 0.76835239 <a title="132-lda-7" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>8 0.76785916 <a title="132-lda-8" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>9 0.76666611 <a title="132-lda-9" href="./emnlp-2013-Automatically_Determining_a_Proper_Length_for_Multi-Document_Summarization%3A_A_Bayesian_Nonparametric_Approach.html">36 emnlp-2013-Automatically Determining a Proper Length for Multi-Document Summarization: A Bayesian Nonparametric Approach</a></p>
<p>10 0.76539469 <a title="132-lda-10" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>11 0.76516771 <a title="132-lda-11" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>12 0.76497078 <a title="132-lda-12" href="./emnlp-2013-Inducing_Document_Plans_for_Concept-to-Text_Generation.html">106 emnlp-2013-Inducing Document Plans for Concept-to-Text Generation</a></p>
<p>13 0.76491463 <a title="132-lda-13" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>14 0.76463938 <a title="132-lda-14" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>15 0.7635321 <a title="132-lda-15" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>16 0.76260513 <a title="132-lda-16" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>17 0.76216668 <a title="132-lda-17" href="./emnlp-2013-Feature_Noising_for_Log-Linear_Structured_Prediction.html">86 emnlp-2013-Feature Noising for Log-Linear Structured Prediction</a></p>
<p>18 0.76193893 <a title="132-lda-18" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>19 0.76167578 <a title="132-lda-19" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>20 0.7616449 <a title="132-lda-20" href="./emnlp-2013-An_Empirical_Study_Of_Semi-Supervised_Chinese_Word_Segmentation_Using_Co-Training.html">21 emnlp-2013-An Empirical Study Of Semi-Supervised Chinese Word Segmentation Using Co-Training</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
