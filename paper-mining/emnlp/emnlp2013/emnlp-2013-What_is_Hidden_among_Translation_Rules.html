<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>201 emnlp-2013-What is Hidden among Translation Rules</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-201" href="#">emnlp2013-201</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>201 emnlp-2013-What is Hidden among Translation Rules</h1>
<br/><p>Source: <a title="emnlp-2013-201-pdf" href="http://aclweb.org/anthology//D/D13/D13-1081.pdf">pdf</a></p><p>Author: Libin Shen ; Bowen Zhou</p><p>Abstract: Most of the machine translation systems rely on a large set of translation rules. These rules are treated as discrete and independent events. In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. We present a preliminary generative model to test this idea. Experimental results show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation.</p><p>Reference: <a title="emnlp-2013-201-reference" href="../emnlp2013_reference/emnlp-2013-What_is_Hidden_among_Translation_Rules_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com Abstract  Most of the machine translation systems rely on a large set of translation rules. [sent-3, score-0.406]
</p><p>2 These rules are treated as discrete and independent events. [sent-4, score-0.098]
</p><p>3 In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. [sent-5, score-0.296]
</p><p>4 We present a preliminary generative model to test this idea. [sent-6, score-0.135]
</p><p>5 , 2008), employ a large rule set that may contain tens of millions of translation rules or even more. [sent-11, score-0.434]
</p><p>6 In these systems, each translation rule has about 20 dense features, which represent key statistics collected from the training data, such as word translation probability, phrase translation probability etc. [sent-12, score-0.82]
</p><p>7 Except for these common features, there is no connection among the translation rules. [sent-13, score-0.232]
</p><p>8 The use of sparse features as in (Arun and Koehn, 2007; Watanabe et al. [sent-15, score-0.08]
</p><p>9 In their work, there are as many as 10,000 features defined on the appearance of certain frequent words and Part of Speech (POS) tags in rules. [sent-18, score-0.032]
</p><p>10 However, these sparse features fire quite randomly 839 Bowen Zhou IBM T. [sent-20, score-0.08]
</p><p>11 Thus, there is still plenty of space to better model translation rules. [sent-25, score-0.203]
</p><p>12 In this paper, we will explore the relationship among translation rules. [sent-26, score-0.203]
</p><p>13 We no longer view rules as discrete or unrelated events. [sent-27, score-0.098]
</p><p>14 Instead, we view rules, which are observed from training data, as random variables generated by a hidden model. [sent-28, score-0.116]
</p><p>15 All possible generative processes can be represented with factorized structures such as weighted hypergraphs and finite state machines. [sent-30, score-0.157]
</p><p>16 This approach leads to a compact model that has better generalization capability and allows translation rules not explicitly observed in training date. [sent-31, score-0.417]
</p><p>17 This paper reports work-in-progress to exploit hidden relations among rules. [sent-32, score-0.082]
</p><p>18 2  Hidden Models  Let G = {(r, f)} be a grammar observed from parallLeel training rd,afta),} w beheareg f mism mtaher frequency oofm a p bairlianl-gual translation rule r. [sent-34, score-0.444]
</p><p>19 Let M be a hidden model that generates every traLnseltat Mion b ruel ea r. [sent-35, score-0.082]
</p><p>20 Fdeorn example, aMt g ceoneulrdat e bse meveodryterlaends wlatiithon a weighted hypergraph or f cioniuteld st baete m machine. [sent-36, score-0.113]
</p><p>21 For the sake of convenience, in this section we assume M is a meta-grammar M = {m}, where ewaecahs m represents a emtae-tga-rarmulme. [sent-37, score-0.062]
</p><p>22 Fro Mr ea =ch { tmra}n,sl wahtieorne r, there exists a hypergraph Hr that represents all possible derivations Dr = {d} that can generate rule r. [sent-38, score-0.315]
</p><p>23 Here, each derivation d= i s{ a hyperpath using me reutlaerules Md, where Md ⊆ M. [sent-39, score-0.063]
</p><p>24 Thus, we can use hypergraph Hr to charac⊆teriz Me r. [sent-40, score-0.085]
</p><p>25 TThrauns,sl wateio cna nr u ulesse ei nh yGProceSe datintlges, o Wfa tsh ein 2g01to3n, C UoSnfAe,re 1n8c-e2 o1n O Ecmtopbier ic 2a0l1 M3. [sent-41, score-0.334]
</p><p>26 hc o2d0s1 i3n A Nsastoucria lti Loan fgoura Cgoem Ppruotcaetsiosin agl, L piang eusis 8t3ic9s–84 , can share nodes and meta-rules in their hypergraphs, so that M is more compact model than G. [sent-43, score-0.082]
</p><p>27 uce three methods to quantify Hr as features of rule r. [sent-45, score-0.133]
</p><p>28 It should be noted that there are more ways to exploit the compact model of M than these three. [sent-46, score-0.125]
</p><p>29 1 Type 1: A Generative Model Let θ be the parameters of a statistical model Pr(m; θ) for meta-rules m in meta-grammar M estPimr(amted;θ f)r foomr mtheet o-brusleersve md tnra mnseltaat-igorna grammar Gs. [sent-48, score-0.159]
</p><p>30 tTimhea probability oef o a strearnvseldat itorann rslualeti r can m bem caarlc uG-. [sent-49, score-0.032]
</p><p>31 Pr(r; θ)  ∝  =  Pr(Hr; θ)  dX∈DrPr(d;θ)  (1)  By assuming separability, Pr(d;θ)  =  Y Pr(m;θ)  (2)  m∈YMd  we can further decompose rule probability Pr(r; θ) as below. [sent-51, score-0.2]
</p><p>32 Pr(r;θ)  =  X Y Pr(m;θ) d∈XDr m∈YMd  (3)  In practice, Pr(r; θ) in (3) can be calculated through bottom-up dynamic programming on hypergraph Hr. [sent-52, score-0.085]
</p><p>33 Hypergraphs of different rules can share nodes and meta-rules. [sent-53, score-0.098]
</p><p>34 This reveals the underlying  relationship among translation rules. [sent-54, score-0.263]
</p><p>35 As a by-product of this generative model, we use the log-likelihood of a translation rule, log Pr(r; θ), as a new dense feature. [sent-55, score-0.351]
</p><p>36 2 Type 2 : Meta-Rules as Sparse Features As given in (3), likelihood of a translation rule is a function over Pr(m; θ), in which θ is estimated from the training data with a generative model. [sent-58, score-0.438]
</p><p>37 Following this practice, we treat each meta-rule m as a sparse feature. [sent-61, score-0.08]
</p><p>38 Feature value f(m) = 1 if 840 and only if m is used in hypergraph Hr. [sent-62, score-0.085]
</p><p>39 3 Type 3 : Posterior as Feature Values A natural question on the binary sparse features de-  fined above is why all the active features have the same value of 1. [sent-67, score-0.08]
</p><p>40 We use these meta-rules to represent a translation rule in feature space. [sent-68, score-0.336]
</p><p>41 Intuitively, for meta-rules with closer connection to the translation rules, we hope to use relatively larger feature values to increase their effect. [sent-69, score-0.232]
</p><p>42 We formalize this intuition with the posterior probability that a meta-rule m is used to generate r, as below. [sent-70, score-0.102]
</p><p>43 f(m)  ≡  = =  Pr(m|r; θ) Pr(m, r; θ) Pr(r;θ)  (4)  Pd∈Dr,m∈MdPr(d;θ) Pr(r; θ)  The posterior in (4) could be too sharp. [sent-71, score-0.07]
</p><p>44 Following the common practice, we smooth the posterior features with a scaling factor α. [sent-72, score-0.07]
</p><p>45 f(m)  ≡  Pr(m|r)α  We use Type 3(α) to represent the posterior model with a scaling factor of α in experiments. [sent-73, score-0.07]
</p><p>46 With proper definition of the underlying model M, we can eesrtim deaftien tθi ownit ohf ft thhee etr uadnditeirolnyailn gEM m algorithm or Bayesian methods. [sent-77, score-0.107]
</p><p>47 In the next section, we will present an example of the hidden model. [sent-78, score-0.082]
</p><p>48 Here, translation rules and their frequencies in G are observed data, an rudl dse arinvdat tihoeni rd f rfeoqru eeancchi rsu ilen r i sa heid odbesne. [sent-80, score-0.335]
</p><p>49 r eAdt the Expectation step, we search all derivations d in Dr of each rule r and calculate their probabilities according to equation (2). [sent-81, score-0.199]
</p><p>50 At the Maximization step, we re-estimate θ on all derivations in proportion to their posterior probability. [sent-82, score-0.136]
</p><p>51 3  Case Study  In Section 2, we explored the use of meta-grammars as the underlying model M and developed three mase tthheods u ntdoe drleyfiinneg f meaotudreels . [sent-83, score-0.06]
</p><p>52 M MSiam nidlar d techniques can be applied to finite state machines and other underlying models. [sent-84, score-0.06]
</p><p>53 Now, we introduce a POS-based underlying model to illustrate the generic model proposed  in Section 2. [sent-85, score-0.06]
</p><p>54 1 Meta-rules on POS tags Let r ∈ G be a translation rule composed of a pair of source an Gd b target wnsolardti strings (Fw, Ew). [sent-88, score-0.463]
</p><p>55 oLfe ta Fp arn odf Ep be the POS tags for the source and target sides respectively. [sent-89, score-0.159]
</p><p>56 For the sake of simplicity as the first attempt, we treat non-terminal as a special word X with POS tag X. [sent-90, score-0.075]
</p><p>57 Suppose we have a Chinese-to-English translation rule as below. [sent-91, score-0.336]
</p><p>58 yuehan qu zhijiage  ⇒  john leaves for chicago  We call NR VV NR  ⇒  NNP VBZ IN NNP  (5)  a translation rule in POS tags. [sent-92, score-0.336]
</p><p>59 We will propose an underlying model M to generaWtee etr wainl sl aptrioopno sruel easn uinn dPeOrSly tags inodsteelad M Mof t otra gnesn-lation rules themselves. [sent-93, score-0.293]
</p><p>60 For the rest of this section, we take translation rules in POS tags as the target of our generative model. [sent-94, score-0.476]
</p><p>61 We define meta-rules on pairs of POS tag strings, e. [sent-95, score-0.044]
</p><p>62 ion rule in (5) into a product on meta-rule probabilities via various derivations, such as •  Pr(NR VV, NNP VBZ) Pr(NR, IVN, NNP), VaBndZ  Pr(NR, NNP) Pr( VV, VBZ IN) Pr(NR, NNP). [sent-100, score-0.133]
</p><p>63 2 The Underlying Model and Features Now, we introduce a generative model M for translation rules in POS tags. [sent-102, score-0.403]
</p><p>64 We still use the example in (5) as shown in Figure 1, where the top box represents the source side and the bottom box represents the target side. [sent-103, score-0.257]
</p><p>65 •  841  Figure 1: An example  ×  We first generate the number of source tokens of a translation rule with a uniform distribution for up to, for example, 7 tokens. [sent-105, score-0.39]
</p><p>66 Then we split the source side into chunks with a binomial distribution with a Bernoulli variable at the gap between each two continuous words, which splits the two words into two chunks with a probability of p. [sent-106, score-0.289]
</p><p>67 For example, the probability of obtaining two chunks NR VV and NR is (1 − p)p, as shown in Figure u1n. [sent-107, score-0.091]
</p><p>68 Suppose we split the target side into two parts, NNP VBZ and IN NNP, which respects the word alignments. [sent-108, score-0.079]
</p><p>69 TNNheP probability fRor ⇒ ⇒th IeN Nfi NrstN mPe,t aas- sruholew isn Pr(|E| = 2 | |F| = 2)  Pr(NR VV, NNP VBZ | |F| = 2, |E| = 2), where |F| represents the number of source tokens, awnhde |E| Fth|e r enpurmesbeenrt so tfh target tboekren osf. [sent-111, score-0.158]
</p><p>70 Similarly, tnhse, probability o nfu utmheb eserc oofnd ta one tios as nfso. [sent-112, score-0.06]
</p><p>71 As for sparse features, we will obtain 7 meta-rule features as below. [sent-117, score-0.08]
</p><p>72 3 Implementation Details Even though the size of all possible meta-rules is  much smaller than the space of translation rules, it is still too large to work with existing optimization methods for sparse features in MT, i. [sent-120, score-0.283]
</p><p>73 Specifically, we first divide all the meta-rules into 100 bins, (|F| , |E|), where |F| is the number of words on the source side, ahnedre |E| |th ise target side, o0f < |F| , |E| ≤ 1so0u. [sent-127, score-0.123]
</p><p>74 842  A shortcoming of this filtering method is that all these features are positive indicators, while lowfrequency negative indicators are discarded. [sent-129, score-0.06]
</p><p>75 Test-1 is from a similar source of the tune set, and it contains 1239 sentences. [sent-140, score-0.054]
</p><p>76 The baseline rule set contains about 17 million rules. [sent-143, score-0.133]
</p><p>77 It contains about 40 dense features, including a 6-gram LM. [sent-144, score-0.046]
</p><p>78 The sparse feature optimization algorithm is similar to the MIRA recipe described in (Chiang et al. [sent-145, score-0.08]
</p><p>79 It should be noted that, even though our metric of tuning is T-B, the baseline  system already provides a very competitive BLEU score on MT08-WB as compared the best system in the evaluation1 , thanks to comprehensive features in the baseline system and more data in training. [sent-151, score-0.043]
</p><p>80 When we use meta-rules as binary sparse features in Type 2, we obtain about one point improvement on T-B on both sets. [sent-156, score-0.08]
</p><p>81 This shows the advantage of tuning individual meta-rule weights over a generative model. [sent-157, score-0.102]
</p><p>82 5  Discussion  In the case study of Section 3, we use POS-based rules as hidden states. [sent-161, score-0.18]
</p><p>83 However, it should be noted that the hidden structures surely do not have to be POS tags. [sent-162, score-0.125]
</p><p>84 For example, an alternative could be  unsupervised NT splitting similar to (Huang et al. [sent-163, score-0.039]
</p><p>85 The meta-grammar based approach was also motivated by the insight acquired on mono-lingual linguistic grammar generation, especially in the TAG related research (Xia, 2001; Prolo, 2002). [sent-165, score-0.104]
</p><p>86 (DeNeefe and Knight, 2009) re-visited the use of adjoining operation in the context of Statistical MT, and reported encouraging results. [sent-169, score-0.095]
</p><p>87 gov/iad/mig/tests/mt/2008/ 843 hand, (Dras, 1999) showed how a meta-level grammar could help in modeling parallel operations in (Shieber and Schabes, 1990). [sent-173, score-0.074]
</p><p>88 Our work is another effort of statistical modeling of well-recognized linguistic insight in NLP and MT. [sent-174, score-0.064]
</p><p>89 6  Conclusions and Future Work  In this paper, we introduced a novel method to model translation rules as observed generation output of a compact hidden model. [sent-175, score-0.499]
</p><p>90 As a case study to capitalize this model, we presented three methods to enrich rule modeling with features defined on a hidden model. [sent-176, score-0.215]
</p><p>91 • To try other prior distributions to generate the nTuom trbyer o othfe source t doiksetnrisb. [sent-179, score-0.054]
</p><p>92 a To incorporate rich models into the generative process, e. [sent-181, score-0.102]
</p><p>93 To improve the posterior model with better paTraom iemteprr estimation, e. [sent-184, score-0.07]
</p><p>94 To replace the exhaustive translation rule set  Twoith r a compact emxhetaau grammar stlhatatio can clree asteet and parameterize new translation rules dynamically, which is the ultimate goal of this line of work. [sent-187, score-0.793]
</p><p>95 Online learning methods for discriminative training of phrase based statistical machine translation. [sent-195, score-0.034]
</p><p>96 A meta-level grammar: redefining synchronous tag for translation and paraphrase. [sent-214, score-0.321]
</p><p>97 Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions. [sent-218, score-0.203]
</p><p>98 SPMT: Statistical machine translation with syntactified target language phrases. [sent-241, score-0.272]
</p><p>99 A new string-to-dependency machine translation algorithm with a target dependency language model. [sent-262, score-0.244]
</p><p>100 A study of translation edit rate with targeted human annotation. [sent-272, score-0.203]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nnp', 0.426), ('pr', 0.387), ('nr', 0.334), ('vbz', 0.307), ('vv', 0.244), ('translation', 0.203), ('rule', 0.133), ('ymd', 0.128), ('generative', 0.102), ('rules', 0.098), ('shieber', 0.096), ('adjoining', 0.095), ('hypergraph', 0.085), ('chiang', 0.084), ('hidden', 0.082), ('compact', 0.082), ('sparse', 0.08), ('schabes', 0.076), ('grammar', 0.074), ('hr', 0.073), ('type', 0.072), ('posterior', 0.07), ('derivations', 0.066), ('cmejrek', 0.064), ('joshi', 0.061), ('underlying', 0.06), ('em', 0.059), ('chunks', 0.059), ('pos', 0.055), ('fm', 0.055), ('hypergraphs', 0.055), ('source', 0.054), ('md', 0.051), ('matsoukas', 0.051), ('deneefe', 0.051), ('snover', 0.051), ('shen', 0.048), ('bowen', 0.047), ('etr', 0.047), ('binomial', 0.047), ('mt', 0.047), ('dense', 0.046), ('synchronous', 0.046), ('arun', 0.045), ('fth', 0.045), ('tag', 0.044), ('noted', 0.043), ('target', 0.041), ('ibm', 0.04), ('splitting', 0.039), ('side', 0.038), ('libin', 0.038), ('mira', 0.038), ('dr', 0.038), ('yves', 0.036), ('derivation', 0.035), ('watanabe', 0.035), ('decompose', 0.035), ('observed', 0.034), ('statistical', 0.034), ('preliminary', 0.033), ('categorical', 0.033), ('och', 0.033), ('bleu', 0.033), ('probability', 0.032), ('sides', 0.032), ('indicators', 0.032), ('fs', 0.032), ('tags', 0.032), ('alignment', 0.032), ('practice', 0.032), ('aravind', 0.031), ('koehn', 0.031), ('marcu', 0.031), ('box', 0.031), ('sake', 0.031), ('represents', 0.031), ('insight', 0.03), ('connection', 0.029), ('papineni', 0.029), ('knight', 0.028), ('spmt', 0.028), ('syntactified', 0.028), ('nfu', 0.028), ('charac', 0.028), ('lowfrequency', 0.028), ('metagrammar', 0.028), ('ahnedre', 0.028), ('tnhse', 0.028), ('pioneer', 0.028), ('eoft', 0.028), ('redefining', 0.028), ('americas', 0.028), ('micciulla', 0.028), ('helsinki', 0.028), ('atht', 0.028), ('uinn', 0.028), ('otra', 0.028), ('hyperpath', 0.028), ('bse', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="201-tfidf-1" href="./emnlp-2013-What_is_Hidden_among_Translation_Rules.html">201 emnlp-2013-What is Hidden among Translation Rules</a></p>
<p>Author: Libin Shen ; Bowen Zhou</p><p>Abstract: Most of the machine translation systems rely on a large set of translation rules. These rules are treated as discrete and independent events. In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. We present a preliminary generative model to test this idea. Experimental results show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation.</p><p>2 0.22223173 <a title="201-tfidf-2" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>Author: Xinyan Xiao ; Deyi Xiong</p><p>Abstract: Traditional synchronous grammar induction estimates parameters by maximizing likelihood, which only has a loose relation to translation quality. Alternatively, we propose a max-margin estimation approach to discriminatively inducing synchronous grammars for machine translation, which directly optimizes translation quality measured by BLEU. In the max-margin estimation of parameters, we only need to calculate Viterbi translations. This further facilitates the incorporation of various non-local features that are defined on the target side. We test the effectiveness of our max-margin estimation framework on a competitive hierarchical phrase-based system. Experiments show that our max-margin method significantly outperforms the traditional twostep pipeline for synchronous rule extraction by 1.3 BLEU points and is also better than previous max-likelihood estimation method.</p><p>3 0.20308894 <a title="201-tfidf-3" href="./emnlp-2013-Flexible_and_Efficient_Hypergraph_Interactions_for_Joint_Hierarchical_and_Forest-to-String_Decoding.html">88 emnlp-2013-Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest-to-String Decoding</a></p>
<p>Author: Martin Cmejrek ; Haitao Mi ; Bowen Zhou</p><p>Abstract: Machine translation benefits from system combination. We propose flexible interaction of hypergraphs as a novel technique combining different translation models within one decoder. We introduce features controlling the interactions between the two systems and explore three interaction schemes of hiero and forest-to-string models—specification, generalization, and interchange. The experiments are carried out on large training data with strong baselines utilizing rich sets of dense and sparse features. All three schemes significantly improve results of any single system on four testsets. We find that specification—a more constrained scheme that almost entirely uses forest-to-string rules, but optionally uses hiero rules for shorter spans—comes out as the strongest, yielding improvement up to 0.9 (T -B )/2 points. We also provide a detailed experimental and qualitative analysis of the results.</p><p>4 0.1809127 <a title="201-tfidf-4" href="./emnlp-2013-Factored_Soft_Source_Syntactic_Constraints_for_Hierarchical_Machine_Translation.html">84 emnlp-2013-Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation</a></p>
<p>Author: Zhongqiang Huang ; Jacob Devlin ; Rabih Zbib</p><p>Abstract: Translation Jacob Devlin Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA j devl in@bbn . com Rabih Zbib Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA r zbib@bbn . com have tried to introduce grammaticality to the transThis paper describes a factored approach to incorporating soft source syntactic constraints into a hierarchical phrase-based translation system. In contrast to traditional approaches that directly introduce syntactic constraints to translation rules by explicitly decorating them with syntactic annotations, which often exacerbate the data sparsity problem and cause other problems, our approach keeps translation rules intact and factorizes the use of syntactic constraints through two separate models: 1) a syntax mismatch model that associates each nonterminal of a translation rule with a distribution of tags that is used to measure the degree of syntactic compatibility of the translation rule on source spans; 2) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language. The features produced by both models are used as soft constraints to guide the translation process. Experiments on Chinese-English translation show that the proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets.</p><p>5 0.16029817 <a title="201-tfidf-5" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>Author: Kuzman Ganchev ; Dipanjan Das</p><p>Abstract: We present a framework for cross-lingual transfer of sequence information from a resource-rich source language to a resourceimpoverished target language that incorporates soft constraints via posterior regularization. To this end, we use automatically word aligned bitext between the source and target language pair, and learn a discriminative conditional random field model on the target side. Our posterior regularization constraints are derived from simple intuitions about the task at hand and from cross-lingual alignment information. We show improvements over strong baselines for two tasks: part-of-speech tagging and namedentity segmentation.</p><p>6 0.14931226 <a title="201-tfidf-6" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>7 0.1492521 <a title="201-tfidf-7" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>8 0.12378244 <a title="201-tfidf-8" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>9 0.11017966 <a title="201-tfidf-9" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>10 0.10547532 <a title="201-tfidf-10" href="./emnlp-2013-A_Corpus_Level_MIRA_Tuning_Strategy_for_Machine_Translation.html">3 emnlp-2013-A Corpus Level MIRA Tuning Strategy for Machine Translation</a></p>
<p>11 0.1019611 <a title="201-tfidf-11" href="./emnlp-2013-Max-Violation_Perceptron_and_Forced_Decoding_for_Scalable_MT_Training.html">128 emnlp-2013-Max-Violation Perceptron and Forced Decoding for Scalable MT Training</a></p>
<p>12 0.10140709 <a title="201-tfidf-12" href="./emnlp-2013-Efficient_Left-to-Right_Hierarchical_Phrase-Based_Translation_with_Improved_Reordering.html">71 emnlp-2013-Efficient Left-to-Right Hierarchical Phrase-Based Translation with Improved Reordering</a></p>
<p>13 0.092606962 <a title="201-tfidf-13" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>14 0.082288906 <a title="201-tfidf-14" href="./emnlp-2013-A_Convex_Alternative_to_IBM_Model_2.html">2 emnlp-2013-A Convex Alternative to IBM Model 2</a></p>
<p>15 0.081985228 <a title="201-tfidf-15" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>16 0.07662005 <a title="201-tfidf-16" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>17 0.07547047 <a title="201-tfidf-17" href="./emnlp-2013-Optimal_Beam_Search_for_Machine_Translation.html">145 emnlp-2013-Optimal Beam Search for Machine Translation</a></p>
<p>18 0.075432539 <a title="201-tfidf-18" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>19 0.075022385 <a title="201-tfidf-19" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>20 0.073910482 <a title="201-tfidf-20" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.227), (1, -0.254), (2, 0.101), (3, 0.061), (4, 0.081), (5, -0.061), (6, -0.018), (7, 0.01), (8, 0.075), (9, 0.051), (10, -0.058), (11, -0.039), (12, -0.015), (13, 0.07), (14, -0.026), (15, 0.004), (16, 0.043), (17, 0.042), (18, 0.053), (19, -0.065), (20, -0.049), (21, 0.079), (22, -0.037), (23, -0.148), (24, -0.048), (25, -0.04), (26, -0.04), (27, -0.151), (28, -0.019), (29, -0.027), (30, -0.104), (31, -0.11), (32, -0.012), (33, -0.008), (34, 0.199), (35, -0.137), (36, 0.109), (37, 0.146), (38, -0.013), (39, 0.061), (40, 0.033), (41, 0.015), (42, 0.066), (43, 0.055), (44, -0.102), (45, -0.039), (46, -0.019), (47, -0.045), (48, 0.118), (49, 0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94900584 <a title="201-lsi-1" href="./emnlp-2013-What_is_Hidden_among_Translation_Rules.html">201 emnlp-2013-What is Hidden among Translation Rules</a></p>
<p>Author: Libin Shen ; Bowen Zhou</p><p>Abstract: Most of the machine translation systems rely on a large set of translation rules. These rules are treated as discrete and independent events. In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. We present a preliminary generative model to test this idea. Experimental results show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation.</p><p>2 0.83118874 <a title="201-lsi-2" href="./emnlp-2013-Flexible_and_Efficient_Hypergraph_Interactions_for_Joint_Hierarchical_and_Forest-to-String_Decoding.html">88 emnlp-2013-Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest-to-String Decoding</a></p>
<p>Author: Martin Cmejrek ; Haitao Mi ; Bowen Zhou</p><p>Abstract: Machine translation benefits from system combination. We propose flexible interaction of hypergraphs as a novel technique combining different translation models within one decoder. We introduce features controlling the interactions between the two systems and explore three interaction schemes of hiero and forest-to-string models—specification, generalization, and interchange. The experiments are carried out on large training data with strong baselines utilizing rich sets of dense and sparse features. All three schemes significantly improve results of any single system on four testsets. We find that specification—a more constrained scheme that almost entirely uses forest-to-string rules, but optionally uses hiero rules for shorter spans—comes out as the strongest, yielding improvement up to 0.9 (T -B )/2 points. We also provide a detailed experimental and qualitative analysis of the results.</p><p>3 0.77253407 <a title="201-lsi-3" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>Author: Jesus Gonzalez-Rubio ; Daniel Ortiz-Martinez ; Jose-Miguel Benedi ; Francisco Casacuberta</p><p>Abstract: Current automatic machine translation systems are not able to generate error-free translations and human intervention is often required to correct their output. Alternatively, an interactive framework that integrates the human knowledge into the translation process has been presented in previous works. Here, we describe a new interactive machine translation approach that is able to work with phrase-based and hierarchical translation models, and integrates error-correction all in a unified statistical framework. In our experiments, our approach outperforms previous interactive translation systems, and achieves estimated effort reductions of as much as 48% relative over a traditional post-edition system.</p><p>4 0.72665274 <a title="201-lsi-4" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>Author: Xinyan Xiao ; Deyi Xiong</p><p>Abstract: Traditional synchronous grammar induction estimates parameters by maximizing likelihood, which only has a loose relation to translation quality. Alternatively, we propose a max-margin estimation approach to discriminatively inducing synchronous grammars for machine translation, which directly optimizes translation quality measured by BLEU. In the max-margin estimation of parameters, we only need to calculate Viterbi translations. This further facilitates the incorporation of various non-local features that are defined on the target side. We test the effectiveness of our max-margin estimation framework on a competitive hierarchical phrase-based system. Experiments show that our max-margin method significantly outperforms the traditional twostep pipeline for synchronous rule extraction by 1.3 BLEU points and is also better than previous max-likelihood estimation method.</p><p>5 0.6634382 <a title="201-lsi-5" href="./emnlp-2013-Factored_Soft_Source_Syntactic_Constraints_for_Hierarchical_Machine_Translation.html">84 emnlp-2013-Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation</a></p>
<p>Author: Zhongqiang Huang ; Jacob Devlin ; Rabih Zbib</p><p>Abstract: Translation Jacob Devlin Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA j devl in@bbn . com Rabih Zbib Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA r zbib@bbn . com have tried to introduce grammaticality to the transThis paper describes a factored approach to incorporating soft source syntactic constraints into a hierarchical phrase-based translation system. In contrast to traditional approaches that directly introduce syntactic constraints to translation rules by explicitly decorating them with syntactic annotations, which often exacerbate the data sparsity problem and cause other problems, our approach keeps translation rules intact and factorizes the use of syntactic constraints through two separate models: 1) a syntax mismatch model that associates each nonterminal of a translation rule with a distribution of tags that is used to measure the degree of syntactic compatibility of the translation rule on source spans; 2) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language. The features produced by both models are used as soft constraints to guide the translation process. Experiments on Chinese-English translation show that the proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets.</p><p>6 0.62944919 <a title="201-lsi-6" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>7 0.55038726 <a title="201-lsi-7" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>8 0.54053801 <a title="201-lsi-8" href="./emnlp-2013-Efficient_Left-to-Right_Hierarchical_Phrase-Based_Translation_with_Improved_Reordering.html">71 emnlp-2013-Efficient Left-to-Right Hierarchical Phrase-Based Translation with Improved Reordering</a></p>
<p>9 0.52493733 <a title="201-lsi-9" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>10 0.49261621 <a title="201-lsi-10" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>11 0.47791323 <a title="201-lsi-11" href="./emnlp-2013-Optimal_Beam_Search_for_Machine_Translation.html">145 emnlp-2013-Optimal Beam Search for Machine Translation</a></p>
<p>12 0.47636709 <a title="201-lsi-12" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>13 0.45101327 <a title="201-lsi-13" href="./emnlp-2013-A_Convex_Alternative_to_IBM_Model_2.html">2 emnlp-2013-A Convex Alternative to IBM Model 2</a></p>
<p>14 0.4291935 <a title="201-lsi-14" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>15 0.42582422 <a title="201-lsi-15" href="./emnlp-2013-Rule-Based_Information_Extraction_is_Dead%21_Long_Live_Rule-Based_Information_Extraction_Systems%21.html">161 emnlp-2013-Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!</a></p>
<p>16 0.42274562 <a title="201-lsi-16" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>17 0.40303749 <a title="201-lsi-17" href="./emnlp-2013-Recurrent_Continuous_Translation_Models.html">156 emnlp-2013-Recurrent Continuous Translation Models</a></p>
<p>18 0.38792923 <a title="201-lsi-18" href="./emnlp-2013-Translating_into_Morphologically_Rich_Languages_with_Synthetic_Phrases.html">186 emnlp-2013-Translating into Morphologically Rich Languages with Synthetic Phrases</a></p>
<p>19 0.38482612 <a title="201-lsi-19" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>20 0.37694201 <a title="201-lsi-20" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.027), (18, 0.017), (22, 0.032), (26, 0.016), (30, 0.103), (50, 0.011), (51, 0.138), (66, 0.483), (71, 0.011), (75, 0.012), (77, 0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92623514 <a title="201-lda-1" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>Author: Shi Feng ; Le Zhang ; Binyang Li ; Daling Wang ; Ge Yu ; Kam-Fai Wong</p><p>Abstract: Extensive experiments have validated the effectiveness of the corpus-based method for classifying the word’s sentiment polarity. However, no work is done for comparing different corpora in the polarity classification task. Nowadays, Twitter has aggregated huge amount of data that are full of people’s sentiments. In this paper, we empirically evaluate the performance of different corpora in sentiment similarity measurement, which is the fundamental task for word polarity classification. Experiment results show that the Twitter data can achieve a much better performance than the Google, Web1T and Wikipedia based methods.</p><p>2 0.87099832 <a title="201-lda-2" href="./emnlp-2013-Translating_into_Morphologically_Rich_Languages_with_Synthetic_Phrases.html">186 emnlp-2013-Translating into Morphologically Rich Languages with Synthetic Phrases</a></p>
<p>Author: Victor Chahuneau ; Eva Schlinger ; Noah A. Smith ; Chris Dyer</p><p>Abstract: Translation into morphologically rich languages is an important but recalcitrant problem in MT. We present a simple and effective approach that deals with the problem in two phases. First, a discriminative model is learned to predict inflections of target words from rich source-side annotations. Then, this model is used to create additional sentencespecific word- and phrase-level translations that are added to a standard translation model as “synthetic” phrases. Our approach relies on morphological analysis of the target language, but we show that an unsupervised Bayesian model of morphology can successfully be used in place of a supervised analyzer. We report significant improvements in translation quality when translating from English to Russian, Hebrew and Swahili.</p><p>same-paper 3 0.85601729 <a title="201-lda-3" href="./emnlp-2013-What_is_Hidden_among_Translation_Rules.html">201 emnlp-2013-What is Hidden among Translation Rules</a></p>
<p>Author: Libin Shen ; Bowen Zhou</p><p>Abstract: Most of the machine translation systems rely on a large set of translation rules. These rules are treated as discrete and independent events. In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. We present a preliminary generative model to test this idea. Experimental results show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation.</p><p>4 0.6801939 <a title="201-lda-4" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>Author: Jun-Ping Ng ; Min-Yen Kan ; Ziheng Lin ; Wei Feng ; Bin Chen ; Jian Su ; Chew Lim Tan</p><p>Abstract: In this paper we classify the temporal relations between pairs of events on an article-wide basis. This is in contrast to much of the existing literature which focuses on just event pairs which are found within the same or adjacent sentences. To achieve this, we leverage on discourse analysis as we believe that it provides more useful semantic information than typical lexico-syntactic features. We propose the use of several discourse analysis frameworks, including 1) Rhetorical Structure Theory (RST), 2) PDTB-styled discourse relations, and 3) topical text segmentation. We explain how features derived from these frameworks can be effectively used with support vector machines (SVM) paired with convolution kernels. Experiments show that our proposal is effective in improving on the state-of-the-art significantly by as much as 16% in terms of F1, even if we only adopt less-than-perfect automatic discourse analyzers and parsers. Making use of more accurate discourse analysis can further boost gains to 35%.</p><p>5 0.56527191 <a title="201-lda-5" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>Author: Svitlana Volkova ; Theresa Wilson ; David Yarowsky</p><p>Abstract: Theresa Wilson Human Language Technology Center of Excellence Johns Hopkins University Baltimore, MD t aw@ j hu .edu differences may Different demographics, e.g., gender or age, can demonstrate substantial variation in their language use, particularly in informal contexts such as social media. In this paper we focus on learning gender differences in the use of subjective language in English, Spanish, and Russian Twitter data, and explore cross-cultural differences in emoticon and hashtag use for male and female users. We show that gender differences in subjective language can effectively be used to improve sentiment analysis, and in particular, polarity classification for Spanish and Russian. Our results show statistically significant relative F-measure improvement over the gender-independent baseline 1.5% and 1% for Russian, 2% and 0.5% for Spanish, and 2.5% and 5% for English for polarity and subjectivity classification.</p><p>6 0.5590266 <a title="201-lda-6" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>7 0.52334553 <a title="201-lda-7" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>8 0.5166328 <a title="201-lda-8" href="./emnlp-2013-Adaptor_Grammars_for_Learning_Non-Concatenative_Morphology.html">19 emnlp-2013-Adaptor Grammars for Learning Non-Concatenative Morphology</a></p>
<p>9 0.51607549 <a title="201-lda-9" href="./emnlp-2013-Automatic_Extraction_of_Morphological_Lexicons_from_Morphologically_Annotated_Corpora.html">30 emnlp-2013-Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora</a></p>
<p>10 0.50527704 <a title="201-lda-10" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>11 0.5032385 <a title="201-lda-11" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>12 0.50109243 <a title="201-lda-12" href="./emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM.html">99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</a></p>
<p>13 0.50103813 <a title="201-lda-13" href="./emnlp-2013-The_Effects_of_Syntactic_Features_in_Automatic_Prediction_of_Morphology.html">181 emnlp-2013-The Effects of Syntactic Features in Automatic Prediction of Morphology</a></p>
<p>14 0.49850249 <a title="201-lda-14" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>15 0.49772397 <a title="201-lda-15" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>16 0.49346745 <a title="201-lda-16" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>17 0.49102485 <a title="201-lda-17" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>18 0.4854086 <a title="201-lda-18" href="./emnlp-2013-Recurrent_Continuous_Translation_Models.html">156 emnlp-2013-Recurrent Continuous Translation Models</a></p>
<p>19 0.48368919 <a title="201-lda-19" href="./emnlp-2013-Joint_Language_and_Translation_Modeling_with_Recurrent_Neural_Networks.html">113 emnlp-2013-Joint Language and Translation Modeling with Recurrent Neural Networks</a></p>
<p>20 0.48317167 <a title="201-lda-20" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
