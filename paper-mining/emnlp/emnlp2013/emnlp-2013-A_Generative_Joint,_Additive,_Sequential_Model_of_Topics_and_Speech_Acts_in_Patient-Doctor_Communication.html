<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>6 emnlp-2013-A Generative Joint, Additive, Sequential Model of Topics and Speech Acts in Patient-Doctor Communication</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-6" href="#">emnlp2013-6</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>6 emnlp-2013-A Generative Joint, Additive, Sequential Model of Topics and Speech Acts in Patient-Doctor Communication</h1>
<br/><p>Source: <a title="emnlp-2013-6-pdf" href="http://aclweb.org/anthology//D/D13/D13-1182.pdf">pdf</a></p><p>Author: Byron C. Wallace ; Thomas A Trikalinos ; M. Barton Laws ; Ira B. Wilson ; Eugene Charniak</p><p>Abstract: We develop a novel generative model of conversation that jointly captures both the topical content and the speech act type associated with each utterance. Our model expresses both token emission and state transition probabilities as log-linear functions of separate components corresponding to topics and speech acts (and their interactions). We apply this model to a dataset comprising annotated patient-physician visits and show that the proposed joint approach outperforms a baseline univariate model.</p><p>Reference: <a title="emnlp-2013-6-reference" href="../emnlp2013_reference/emnlp-2013-A_Generative_Joint%2C_Additive%2C_Sequential_Model_of_Topics_and_Speech_Acts_in_Patient-Doctor_Communication_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ebadur  ,  ,  ,  Abstract We develop a novel generative model of conversation that jointly captures both the topical content and the speech act type associated with each utterance. [sent-8, score-0.751]
</p><p>2 Our model expresses both token emission and state transition probabilities as log-linear functions of separate components corresponding to topics and speech acts (and their interactions). [sent-9, score-0.837]
</p><p>3 1 Introduction Communication involves at least two aspects:  the  words one says and the acts one performs in saying them. [sent-11, score-0.366]
</p><p>4 These are re-  ferred to as speech acts under the sociolinguistic theory of Austin (1955), which was further developed by Searle (1969; 1985). [sent-13, score-0.611]
</p><p>5 Recognizing speech acts is crucial to understanding communication because a speaker’s meaning is only partially captured by the words they use; much of their intent is expressed implicitly via speech acts (Searle, 1969). [sent-14, score-1.431]
</p><p>6 On this view, conversational utterances can be assigned both a topic and a speech act. [sent-15, score-0.589]
</p><p>7 For example, the utterance “Obama won the election” is topically political and is an example of an information giving speech act. [sent-19, score-0.353]
</p><p>8 ”, meanwhile, belongs 1765 RoleUtteranceTopicSpeech act DLet me just write down someLogisticsCommissive of these issues here so Iget them straight in my mind. [sent-21, score-0.344]
</p><p>9 Table 1: An excerpt from a patient-doctor interaction, annotated with topic and speech act codes. [sent-36, score-0.718]
</p><p>10 Previous computational work on speech acts which we review in Section 6 has modeled them in isolation (Perrault and Allen, 1980; Stolcke et al. [sent-44, score-0.611]
</p><p>11 But a richer model would account for both speech acts and the contextualizing topic of each utterance. [sent-50, score-0.74]
</p><p>12 To this end, we develop a novel joint, generative model of topics and speech acts. [sent-51, score-0.406]
</p><p>13 We provide an excerpt of a conversation between a patient and their doctor annotated with topics and speech acts in Table 1. [sent-56, score-0.914]
</p><p>14 A concrete example of this is the use of topic and speech act codes to assess the efficacy of an intervention meant to influence physician-patient communication regarding adherence to antiretroviral (ARV) medication (Wilson et al. [sent-59, score-1.374]
</p><p>15 To measure the effect of the intervention, investigators performed a randomized control trial in which they quantified change in communication patterns by tallying the number of information giving speech acts that fell under the ARV adherence topic. [sent-61, score-1.092]
</p><p>16 Without assigning both topics and speech acts to utterances, this analysis would not have been possible. [sent-62, score-0.697]
</p><p>17 , 2013) to the case of supervised sequential tasks to capture the joint conditional influence of topics and speech acts, both with respect to token generation and state transitions. [sent-66, score-0.413]
</p><p>18 In contrast to previous work on speech acts, JAS provides a single, coherent generative model of conversations. [sent-68, score-0.32]
</p><p>19 We demonstrate that JAS outperforms a generative univariate baseline in topic/speech act prediction. [sent-70, score-0.472]
</p><p>20 2  The Markov-Multinomial Model  We begin by considering a baseline generative ap-  proach to modeling topics and speech acts independently. [sent-72, score-0.772]
</p><p>21 It accounts for only a single output at each time point yt ∈ Y, and hence here we model topics and speech acts independently. [sent-75, score-0.995]
</p><p>22 A straight-forward (albeit na¨ ıve) alternative would be to treat the Cartesian product of topics and speech acts as a single output space on which emissions and transitions are conditioned, but this space is too large and sparse for this approach to be practicable. [sent-76, score-0.749]
</p><p>23 To make both topic and speech act predictions, we simply induce models for each and make predictions independently. [sent-89, score-0.759]
</p><p>24 3 JAS: A Joint, Additive, Sequential Model An obvious shortcoming of the simple MM model outlined above is that it treats topics and speech acts as statistically independent. [sent-90, score-0.697]
</p><p>25 One would prefer a more expressive model that conditions topic and speech act transitions as well as the production of utterances jointly on both the current topic and the current speech act. [sent-93, score-1.294]
</p><p>26 More specifically, we would like a model that reflects the assumption that some latent intent gives rise to both the topic and the speech act associated with an utterance. [sent-94, score-0.786]
</p><p>27 This is consistent with Searle’s (1969) notion of perlocutionary effects; one performs speech acts with the aim of getting someone to do something. [sent-95, score-0.611]
</p><p>28 Intent gives rise to the current topic and speech act, and the current intent affects the next; this induces a correlation between adjacent topics and speech acts. [sent-96, score-0.773]
</p><p>29 In our application the topical content may be ARV adherence and the type of speech act would be selected  by the provider (presumably to maximize the likelihood of patient adherence). [sent-101, score-0.847]
</p><p>30 We refer to the topic set by Y, the speech act setW by rSe aenrd t oth teh vocabulary as W Y,. [sent-108, score-0.718]
</p><p>31 Further, we include the component ηwy,s to capture interaction effects between topics and speech acts. [sent-110, score-0.4]
</p><p>32 We assume that the conditional probability of word w belonging to an utterance ut with corresponding topic yt and speech act st is log-linear with respect to these components, i. [sent-111, score-1.246]
</p><p>33 To this end, we model topic and speech act transition probabilities as log-linear functions of the preceding topic  and speech act. [sent-116, score-1.196]
</p><p>34 We denote log of the background topic frequencies by πY, and components capturing the influence of transitioning to topic yt due to the preceding topic and speech act by σyt−1,yt and σst−1,yt respectively. [sent-117, score-1.39]
</p><p>35 We also include a component σ(yt−1,st−1),yt that corresponds to the interaction effect on topic transition probability due to the preceding topic/speech act pair. [sent-118, score-0.646]
</p><p>36 On the left we show our motivating conceptualization: a latent intent gives rise to both the topic and speech acts; these, in turn, jointly induce a distribution over words and transitions. [sent-120, score-0.442]
</p><p>37 have:  P(st|st−1, yt−1) = Z1sexp{πsSt+σst−1,st+σyt−1,st+σ(yt−1,st−1),st} (8) Where Zs is a normalizing constant for speech acts analogous to Equation 7. [sent-123, score-0.611]
</p><p>38 Putting things together: P(yt, st |st−1 , yt−1 , ut) = ·  P(ut|yt, st) P(yt|yt−1, st−1) · P(st|st−1, yt−1) (9) As implied by Figure 1, this model assumes that the topic and speech act at time t are conditionally independent given the preceding topic and speech act (yt−1 and st−1). [sent-124, score-1.535]
</p><p>39 This is intuitively agreeable because time intervenes as a blocking factor; conditioning the current topic on the current speech act  (or vice versa) would contradict the fact that these occur simultaneously. [sent-125, score-0.718]
</p><p>40 Instead, the correlation is induced by the preceding topic/speech act pair. [sent-126, score-0.385]
</p><p>41 (That said, this is still a simplifying assumption, as one may instead choose to model speech act selection as conditional on topic (Traum and Larsson, 2003). [sent-127, score-0.718]
</p><p>42 ) Predictions can again be made via Viterbi decoding (Rabiner and Juang, 1986) over a matrix of pairs of joint topic/speech act states. [sent-128, score-0.344]
</p><p>43 We fix the ‘background’ frequencies θ, πY, πS to the log of the 1768 corresponding observed proportions of words, topics and speech acts, respectively. [sent-131, score-0.331]
</p><p>44 The GMIAS has been used to: characterize interaction processes in physicianpatient communication about ARV adherence in the 1With the exception that we do not explicitly model the distribution over component variances. [sent-140, score-0.355]
</p><p>45 1769 context of an intervention trial (Wilson et al. [sent-141, score-0.203]
</p><p>46 , 2011b); and to describe providerpatient communication regarding ARV adherence compared with communication about other issues (Laws et al. [sent-144, score-0.427]
</p><p>47 An utterance is here defined as a single completed speech act. [sent-148, score-0.316]
</p><p>48 These definitions provide helpful guidance to coders, but many speech acts are poorly formed grammatically, and cannot be described as a “clause”. [sent-153, score-0.611]
</p><p>49 Further, some speech acts cannot be said to convey a “thought” (or sentence) at all, but rather are pre-syntactical (e. [sent-154, score-0.611]
</p><p>50 Each utterance is assigned a single topic code and a single speech act code. [sent-164, score-0.789]
</p><p>51 edu/m-barton-laws/home/gmias  scribe the topics and speech acts we consider in more detail; Table 2 enumerates all pairs of these and their respective counts in the corpus. [sent-173, score-0.697]
</p><p>52 We note  that GMIAS defines a hierarchy of both topic and speech act codes, but here we only attempt to capture the highest level codes in these hierarchies. [sent-174, score-0.775]
</p><p>53 Antiretroviral (ARV) adherence applies to utterances that address ARV medication usage. [sent-176, score-0.341]
</p><p>54 The missing/other topic covers a few cases, including utterances that are effectively outside of the GMIAS universe and inaudible utterances; however we note that missing/other is a topic explicitly assigned by human annotators. [sent-179, score-0.408]
</p><p>55 Finally, socializing refers to casual conversation unrelated to the business of the medical visit, and to social rituals such as greetings. [sent-181, score-0.193]
</p><p>56 There are 10 speech acts:3 ask question, commissive, continuation, conversation management, directive, empathy, give information, humor/levity, missing/other, and social-ritual. [sent-182, score-0.37]
</p><p>57 A continuation refers to the completion of a previously interrupted speech act (these are rare). [sent-185, score-0.589]
</p><p>58 This data originated as part of 3These are high-level speech acts; technically each constitutes a category of speech act types. [sent-196, score-0.834]
</p><p>59 5  Experimental Results  Figure 2: Mean F-scores across all topic/speech act pairs for the Markov-Multinomial (MM; left) and the proposed Joint Additive Sequential (JAS; right) models. [sent-200, score-0.344]
</p><p>60 4  Our evaluation includes two First, we perform standard cross-validation over the aforementioned 360 annotated interactions, evaluating Fmeasure for each topic/speech act pair. [sent-203, score-0.344]
</p><p>61 a randomized control trial that assessed the efficacy of an intervention meant to alter physician-patient communication. [sent-206, score-0.341]
</p><p>62 Indeed, perhaps the main strength of the ad-  ditive component based sequential model we have proposed here is that it will allow us to easily incorporate physician-specific parameters that capture deviations in provider speech act and/or topic transition patterns. [sent-212, score-0.897]
</p><p>63 1 Cross-fold Validation Our aim is to measure model performance in terms of correctly identifying both the topic and speech act corresponding to each utterance. [sent-219, score-0.718]
</p><p>64 We quantify this via the F-score calculated for each topic/speech act pair that is observed at least once. [sent-220, score-0.344]
</p><p>65 We first report macro-averages, that is, averages of the individual topic/speech act pair F-scores. [sent-226, score-0.344]
</p><p>66 For a more granular picture, Figure 3 displays av1771 erage F-score differences with respect to every individual topic and speech act pair for which this difference was non-zero. [sent-231, score-0.753]
</p><p>67 The relatively low F-scores for the metrics quantifying performance with respect to the cross of topic  and speech act codes belie relatively good overall (marginal) predictive performance. [sent-236, score-0.81]
</p><p>68 That is, we achieve much better performance with respect to metrics that measure topic and speech act predictions independently ofone another. [sent-237, score-0.794]
</p><p>69 9% decrease) with respect to marginal topic code prediction, but improves the marginal speech act F-score by . [sent-246, score-0.853]
</p><p>70 2 (Re-)Analysis of Randomized Control Trial We also evaluated performance by tallying model predictions over 116 held-out cases collected from a randomized, cross-over study of an intervention aimed at improving physicians knowledge of patients anti-retroviral (ARV) adherence (Wilson et al. [sent-250, score-0.376]
</p><p>71 The intervention was a report given to the physician before a routine office visit that contained  information regarding the patients ARV usage and their beliefs about ARV therapy. [sent-252, score-0.302]
</p><p>72 (2010) demonstrated that the intervention indeed increased adherence-related dialogue, and specifically the number of information giving speech acts performed by the physician un-  Figure 3: Average difference in F-scores corresponding to specific topic/speech act pairs, sorted by magnitude. [sent-255, score-1.2]
</p><p>73 c10on (4tro,l 28)Truei2n3te (r1v1e,n 3t9io)nc1o3n (5tr,o 3l )M i2n7te (1rv6e,n 4t io)n1c2on (5tr,o 2l8)JASi2n3te (1rv4e,n 4ti0o)n  Table 3: Utterance counts {Median (25th, 75th percentile)} foUr tthteerAanRcVe/i cnofuornmtsat {iMone giving topic ,a 7nd5 speech acectn pair. [sent-257, score-0.411]
</p><p>74 To this end, we trained MM and JAS models over the aforementioned 360 annotated visits and then used this model to generate topic and speech act code predictions for the utterances comprising the 116 held-out visits used for the analysis (these were not part of the training set). [sent-262, score-1.079]
</p><p>75 We then assessed the direction and magnitude of the change in the number of ARV adherence/information giving utterances in the paired control versus intervention cases. [sent-263, score-0.382]
</p><p>76 , 2010), we report the median number of 1772  ARV/information giving utterances and corresponding 25th and 75th percentiles over the 58 control and intervention visits, as counted using the true (human) annotations and using the codes predicted by the MM and JAS models. [sent-266, score-0.439]
</p><p>77 6  Related work  There is a relatively long history of research into modeling conversational speech acts in computational linguistics. [sent-269, score-0.676]
</p><p>78 Perrault and Allen (1980) conducted pioneering work on computationally formalizing speech acts, though their work pre-dates statistical NLP and is therefore not directly relevant to the present work. [sent-270, score-0.245]
</p><p>79 (2000; 1998) proposed a probabilistic approach to modeling conversational speech acts based on the Hidden Markov Model (HMM) (Rabiner and Juang, 1986). [sent-272, score-0.676]
</p><p>80 They were interested in modeling an unrestricted set of conversations, and  did not impose a hierarchy on the speech acts; they therefore enumerated many more speech acts (42) than we do in the present work (recall that we use 10 ‘high-level’ speech acts). [sent-273, score-1.101]
</p><p>81 also considered jointly performing speech recognition and speech act classification. [sent-276, score-0.834]
</p><p>82 Another thread ofresearch has investigated classifying speech acts in emails into one of a small set of “email speech acts”, e. [sent-284, score-0.856]
</p><p>83 A separate line of inquiry concerns classifying dialogue acts in chat. [sent-294, score-0.443]
</p><p>84 Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al. [sent-295, score-0.421]
</p><p>85 (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al. [sent-299, score-0.516]
</p><p>86 Additive component models of text have recently 5We note that only 8 of the 42 speech acts appeared with greater than 1% frequency in Stolcke et al. [sent-302, score-0.644]
</p><p>87 6 7  Conclusions and Future Directions  We have proposed a novel Joint, Additive, Sequential (JAS) model of conversational topics and speech acts. [sent-308, score-0.396]
</p><p>88 In contrast to previous approaches to modeling conversational exchanges, this model factors both the current topic and the current speech act into token emission and state transition probabilities. [sent-309, score-0.888]
</p><p>89 We demonstrated that this model consistently outperforms a univariate generative baseline that treats speech acts and topics independently. [sent-310, score-0.825]
</p><p>90 Furthermore, we showed JAS can automatically re-produce the analysis of a randomized control trial designed to assess the efficacy of an intervention to alter physician communication habits with high-fidelity. [sent-311, score-0.581]
</p><p>91 Ultimately, we would like to correlate patterns in physician communication (as gleaned from the model) with objective, measured health outcomes (e. [sent-319, score-0.24]
</p><p>92 Automatic dialog act segmentation and classification in multiparty meetings. [sent-328, score-0.418]
</p><p>93 Behind the article: Recognizing dialog acts in wikipedia talk pages. [sent-375, score-0.397]
</p><p>94 The association of visit length and measures of patient-centered communication in HIV care: A mixed methods study. [sent-425, score-0.192]
</p><p>95 Provider-patient adherence dialogue in HIV care: results of a multisite study. [sent-429, score-0.222]
</p><p>96 Classifying sentences as speech acts in message board posts. [sent-465, score-0.611]
</p><p>97 Expression and meaning: Studies in the theory of speech acts. [sent-482, score-0.245]
</p><p>98 Dialogue act modeling for automatic tagging and recognition of conversational speech. [sent-490, score-0.409]
</p><p>99 Provider-focused intervention increases adherence-related dialogue, but does  not improve antiretroviral therapy adherence in persons with HIV. [sent-511, score-0.338]
</p><p>100 Towards scalable speech act recognition in twitter: Tackling insufficient training data. [sent-515, score-0.589]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('acts', 0.366), ('act', 0.344), ('yt', 0.298), ('jas', 0.279), ('speech', 0.245), ('arv', 0.183), ('utterances', 0.15), ('intervention', 0.147), ('adherence', 0.145), ('communication', 0.141), ('gmias', 0.137), ('mm', 0.134), ('topic', 0.129), ('additive', 0.095), ('conversation', 0.087), ('topics', 0.086), ('laws', 0.085), ('visits', 0.085), ('patient', 0.077), ('dialogue', 0.077), ('barton', 0.076), ('hiv', 0.076), ('generative', 0.075), ('utterance', 0.071), ('intent', 0.068), ('ira', 0.066), ('ut', 0.066), ('conversational', 0.065), ('transition', 0.063), ('eisenstein', 0.062), ('stolcke', 0.061), ('physician', 0.061), ('rabiner', 0.061), ('socializing', 0.061), ('yoojin', 0.061), ('st', 0.058), ('paul', 0.057), ('codes', 0.057), ('trial', 0.056), ('biomedical', 0.056), ('randomized', 0.054), ('doctor', 0.053), ('univariate', 0.053), ('transitions', 0.052), ('conversations', 0.052), ('visit', 0.051), ('marginal', 0.05), ('wilson', 0.049), ('control', 0.048), ('sequential', 0.047), ('antiretroviral', 0.046), ('juang', 0.046), ('medication', 0.046), ('meteer', 0.046), ('perrault', 0.046), ('roter', 0.046), ('searle', 0.046), ('wy', 0.045), ('medical', 0.045), ('medicine', 0.043), ('patients', 0.043), ('emission', 0.042), ('predictions', 0.041), ('preceding', 0.041), ('care', 0.041), ('carol', 0.04), ('factorial', 0.04), ('multiparty', 0.04), ('transitioning', 0.04), ('email', 0.039), ('health', 0.038), ('assess', 0.038), ('ask', 0.038), ('giving', 0.037), ('angus', 0.036), ('marie', 0.036), ('provider', 0.036), ('interaction', 0.036), ('efficacy', 0.036), ('folds', 0.036), ('respect', 0.035), ('components', 0.035), ('segmentation', 0.034), ('component', 0.033), ('beach', 0.032), ('byron', 0.032), ('ford', 0.032), ('rogers', 0.032), ('talk', 0.031), ('abbreviates', 0.031), ('ang', 0.031), ('cavedon', 0.031), ('cindy', 0.031), ('coccaro', 0.031), ('counseling', 0.031), ('cretchley', 0.031), ('directive', 0.031), ('forman', 0.031), ('gallois', 0.031), ('hospitals', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="6-tfidf-1" href="./emnlp-2013-A_Generative_Joint%2C_Additive%2C_Sequential_Model_of_Topics_and_Speech_Acts_in_Patient-Doctor_Communication.html">6 emnlp-2013-A Generative Joint, Additive, Sequential Model of Topics and Speech Acts in Patient-Doctor Communication</a></p>
<p>Author: Byron C. Wallace ; Thomas A Trikalinos ; M. Barton Laws ; Ira B. Wilson ; Eugene Charniak</p><p>Abstract: We develop a novel generative model of conversation that jointly captures both the topical content and the speech act type associated with each utterance. Our model expresses both token emission and state transition probabilities as log-linear functions of separate components corresponding to topics and speech acts (and their interactions). We apply this model to a dataset comprising annotated patient-physician visits and show that the proposed joint approach outperforms a baseline univariate model.</p><p>2 0.12912703 <a title="6-tfidf-2" href="./emnlp-2013-Feature_Noising_for_Log-Linear_Structured_Prediction.html">86 emnlp-2013-Feature Noising for Log-Linear Structured Prediction</a></p>
<p>Author: Sida Wang ; Mengqiu Wang ; Stefan Wager ; Percy Liang ; Christopher D. Manning</p><p>Abstract: NLP models have many and sparse features, and regularization is key for balancing model overfitting versus underfitting. A recently repopularized form of regularization is to generate fake training data by repeatedly adding noise to real data. We reinterpret this noising as an explicit regularizer, and approximate it with a second-order formula that can be used during training without actually generating fake data. We show how to apply this method to structured prediction using multinomial logistic regression and linear-chain CRFs. We tackle the key challenge of developing a dynamic program to compute the gradient of the regularizer efficiently. The regularizer is a sum over inputs, so we can estimate it more accurately via a semi-supervised or transductive extension. Applied to text classification and NER, our method provides a > 1% absolute performance gain over use of standard L2 regularization.</p><p>3 0.11834854 <a title="6-tfidf-3" href="./emnlp-2013-Grounding_Strategic_Conversation%3A_Using_Negotiation_Dialogues_to_Predict_Trades_in_a_Win-Lose_Game.html">91 emnlp-2013-Grounding Strategic Conversation: Using Negotiation Dialogues to Predict Trades in a Win-Lose Game</a></p>
<p>Author: Anais Cadilhac ; Nicholas Asher ; Farah Benamara ; Alex Lascarides</p><p>Abstract: This paper describes a method that predicts which trades players execute during a winlose game. Our method uses data collected from chat negotiations of the game The Settlers of Catan and exploits the conversation to construct dynamically a partial model of each player’s preferences. This in turn yields equilibrium trading moves via principles from game theory. We compare our method against four baselines and show that tracking how preferences evolve through the dialogue and reasoning about equilibrium moves are both crucial to success.</p><p>4 0.082432307 <a title="6-tfidf-4" href="./emnlp-2013-Using_Paraphrases_and_Lexical_Semantics_to_Improve_the_Accuracy_and_the_Robustness_of_Supervised_Models_in_Situated_Dialogue_Systems.html">197 emnlp-2013-Using Paraphrases and Lexical Semantics to Improve the Accuracy and the Robustness of Supervised Models in Situated Dialogue Systems</a></p>
<p>Author: Claire Gardent ; Lina M. Rojas Barahona</p><p>Abstract: This paper explores to what extent lemmatisation, lexical resources, distributional semantics and paraphrases can increase the accuracy of supervised models for dialogue management. The results suggest that each of these factors can help improve performance but that the impact will vary depending on their combination and on the evaluation mode.</p><p>5 0.080108374 <a title="6-tfidf-5" href="./emnlp-2013-Improvements_to_the_Bayesian_Topic_N-Gram_Models.html">100 emnlp-2013-Improvements to the Bayesian Topic N-Gram Models</a></p>
<p>Author: Hiroshi Noji ; Daichi Mochihashi ; Yusuke Miyao</p><p>Abstract: One of the language phenomena that n-gram language model fails to capture is the topic information of a given situation. We advance the previous study of the Bayesian topic language model by Wallach (2006) in two directions: one, investigating new priors to alleviate the sparseness problem caused by dividing all ngrams into exclusive topics, and two, developing a novel Gibbs sampler that enables moving multiple n-grams across different documents to another topic. Our blocked sampler can efficiently search for higher probability space even with higher order n-grams. In terms of modeling assumption, we found it is effective to assign a topic to only some parts of a document.</p><p>6 0.073617242 <a title="6-tfidf-6" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>7 0.072775312 <a title="6-tfidf-7" href="./emnlp-2013-Leveraging_Lexical_Cohesion_and_Disruption_for_Topic_Segmentation.html">124 emnlp-2013-Leveraging Lexical Cohesion and Disruption for Topic Segmentation</a></p>
<p>8 0.070581995 <a title="6-tfidf-8" href="./emnlp-2013-Classifying_Message_Board_Posts_with_an_Extracted_Lexicon_of_Patient_Attributes.html">46 emnlp-2013-Classifying Message Board Posts with an Extracted Lexicon of Patient Attributes</a></p>
<p>9 0.067266338 <a title="6-tfidf-9" href="./emnlp-2013-Learning_Topics_and_Positions_from_Debatepedia.html">121 emnlp-2013-Learning Topics and Positions from Debatepedia</a></p>
<p>10 0.064951845 <a title="6-tfidf-10" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>11 0.060237568 <a title="6-tfidf-11" href="./emnlp-2013-Joint_Learning_of_Phonetic_Units_and_Word_Pronunciations_for_ASR.html">115 emnlp-2013-Joint Learning of Phonetic Units and Word Pronunciations for ASR</a></p>
<p>12 0.055542909 <a title="6-tfidf-12" href="./emnlp-2013-Modeling_Scientific_Impact_with_Topical_Influence_Regression.html">133 emnlp-2013-Modeling Scientific Impact with Topical Influence Regression</a></p>
<p>13 0.054935016 <a title="6-tfidf-13" href="./emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM.html">99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</a></p>
<p>14 0.054675631 <a title="6-tfidf-14" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<p>15 0.054098297 <a title="6-tfidf-15" href="./emnlp-2013-A_Dataset_for_Research_on_Short-Text_Conversations.html">4 emnlp-2013-A Dataset for Research on Short-Text Conversations</a></p>
<p>16 0.053066034 <a title="6-tfidf-16" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>17 0.052972768 <a title="6-tfidf-17" href="./emnlp-2013-A_Unified_Model_for_Topics%2C_Events_and_Users_on_Twitter.html">16 emnlp-2013-A Unified Model for Topics, Events and Users on Twitter</a></p>
<p>18 0.051991988 <a title="6-tfidf-18" href="./emnlp-2013-Predicting_the_Resolution_of_Referring_Expressions_from_User_Behavior.html">153 emnlp-2013-Predicting the Resolution of Referring Expressions from User Behavior</a></p>
<p>19 0.04624952 <a title="6-tfidf-19" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>20 0.046130314 <a title="6-tfidf-20" href="./emnlp-2013-A_Log-Linear_Model_for_Unsupervised_Text_Normalization.html">9 emnlp-2013-A Log-Linear Model for Unsupervised Text Normalization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.153), (1, 0.028), (2, -0.069), (3, -0.001), (4, -0.053), (5, -0.014), (6, 0.043), (7, 0.08), (8, -0.047), (9, -0.043), (10, -0.119), (11, -0.07), (12, -0.042), (13, 0.131), (14, 0.041), (15, 0.088), (16, 0.167), (17, -0.022), (18, -0.066), (19, 0.06), (20, 0.168), (21, 0.101), (22, 0.049), (23, 0.149), (24, 0.035), (25, 0.008), (26, 0.018), (27, -0.156), (28, -0.084), (29, 0.118), (30, 0.01), (31, -0.015), (32, 0.003), (33, 0.144), (34, -0.007), (35, -0.085), (36, 0.103), (37, -0.019), (38, -0.146), (39, 0.055), (40, -0.135), (41, -0.082), (42, -0.009), (43, -0.11), (44, -0.09), (45, -0.122), (46, 0.126), (47, -0.09), (48, -0.23), (49, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96615064 <a title="6-lsi-1" href="./emnlp-2013-A_Generative_Joint%2C_Additive%2C_Sequential_Model_of_Topics_and_Speech_Acts_in_Patient-Doctor_Communication.html">6 emnlp-2013-A Generative Joint, Additive, Sequential Model of Topics and Speech Acts in Patient-Doctor Communication</a></p>
<p>Author: Byron C. Wallace ; Thomas A Trikalinos ; M. Barton Laws ; Ira B. Wilson ; Eugene Charniak</p><p>Abstract: We develop a novel generative model of conversation that jointly captures both the topical content and the speech act type associated with each utterance. Our model expresses both token emission and state transition probabilities as log-linear functions of separate components corresponding to topics and speech acts (and their interactions). We apply this model to a dataset comprising annotated patient-physician visits and show that the proposed joint approach outperforms a baseline univariate model.</p><p>2 0.77236032 <a title="6-lsi-2" href="./emnlp-2013-Grounding_Strategic_Conversation%3A_Using_Negotiation_Dialogues_to_Predict_Trades_in_a_Win-Lose_Game.html">91 emnlp-2013-Grounding Strategic Conversation: Using Negotiation Dialogues to Predict Trades in a Win-Lose Game</a></p>
<p>Author: Anais Cadilhac ; Nicholas Asher ; Farah Benamara ; Alex Lascarides</p><p>Abstract: This paper describes a method that predicts which trades players execute during a winlose game. Our method uses data collected from chat negotiations of the game The Settlers of Catan and exploits the conversation to construct dynamically a partial model of each player’s preferences. This in turn yields equilibrium trading moves via principles from game theory. We compare our method against four baselines and show that tracking how preferences evolve through the dialogue and reasoning about equilibrium moves are both crucial to success.</p><p>3 0.5681262 <a title="6-lsi-3" href="./emnlp-2013-Feature_Noising_for_Log-Linear_Structured_Prediction.html">86 emnlp-2013-Feature Noising for Log-Linear Structured Prediction</a></p>
<p>Author: Sida Wang ; Mengqiu Wang ; Stefan Wager ; Percy Liang ; Christopher D. Manning</p><p>Abstract: NLP models have many and sparse features, and regularization is key for balancing model overfitting versus underfitting. A recently repopularized form of regularization is to generate fake training data by repeatedly adding noise to real data. We reinterpret this noising as an explicit regularizer, and approximate it with a second-order formula that can be used during training without actually generating fake data. We show how to apply this method to structured prediction using multinomial logistic regression and linear-chain CRFs. We tackle the key challenge of developing a dynamic program to compute the gradient of the regularizer efficiently. The regularizer is a sum over inputs, so we can estimate it more accurately via a semi-supervised or transductive extension. Applied to text classification and NER, our method provides a > 1% absolute performance gain over use of standard L2 regularization.</p><p>4 0.49027616 <a title="6-lsi-4" href="./emnlp-2013-Using_Paraphrases_and_Lexical_Semantics_to_Improve_the_Accuracy_and_the_Robustness_of_Supervised_Models_in_Situated_Dialogue_Systems.html">197 emnlp-2013-Using Paraphrases and Lexical Semantics to Improve the Accuracy and the Robustness of Supervised Models in Situated Dialogue Systems</a></p>
<p>Author: Claire Gardent ; Lina M. Rojas Barahona</p><p>Abstract: This paper explores to what extent lemmatisation, lexical resources, distributional semantics and paraphrases can increase the accuracy of supervised models for dialogue management. The results suggest that each of these factors can help improve performance but that the impact will vary depending on their combination and on the evaluation mode.</p><p>5 0.40992215 <a title="6-lsi-5" href="./emnlp-2013-Learning_Topics_and_Positions_from_Debatepedia.html">121 emnlp-2013-Learning Topics and Positions from Debatepedia</a></p>
<p>Author: Swapna Gottipati ; Minghui Qiu ; Yanchuan Sim ; Jing Jiang ; Noah A. Smith</p><p>Abstract: We explore Debatepedia, a communityauthored encyclopedia of sociopolitical debates, as evidence for inferring a lowdimensional, human-interpretable representation in the domain of issues and positions. We introduce a generative model positing latent topics and cross-cutting positions that gives special treatment to person mentions and opinion words. We evaluate the resulting representation’s usefulness in attaching opinionated documents to arguments and its consistency with human judgments about positions.</p><p>6 0.40321475 <a title="6-lsi-6" href="./emnlp-2013-Improvements_to_the_Bayesian_Topic_N-Gram_Models.html">100 emnlp-2013-Improvements to the Bayesian Topic N-Gram Models</a></p>
<p>7 0.40321133 <a title="6-lsi-7" href="./emnlp-2013-Using_Topic_Modeling_to_Improve_Prediction_of_Neuroticism_and_Depression_in_College_Students.html">199 emnlp-2013-Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students</a></p>
<p>8 0.39806572 <a title="6-lsi-8" href="./emnlp-2013-Automatically_Detecting_and_Attributing_Indirect_Quotations.html">35 emnlp-2013-Automatically Detecting and Attributing Indirect Quotations</a></p>
<p>9 0.37033808 <a title="6-lsi-9" href="./emnlp-2013-Measuring_Ideological_Proportions_in_Political_Speeches.html">129 emnlp-2013-Measuring Ideological Proportions in Political Speeches</a></p>
<p>10 0.35030532 <a title="6-lsi-10" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>11 0.33033195 <a title="6-lsi-11" href="./emnlp-2013-Joint_Learning_of_Phonetic_Units_and_Word_Pronunciations_for_ASR.html">115 emnlp-2013-Joint Learning of Phonetic Units and Word Pronunciations for ASR</a></p>
<p>12 0.3006984 <a title="6-lsi-12" href="./emnlp-2013-Predicting_the_Resolution_of_Referring_Expressions_from_User_Behavior.html">153 emnlp-2013-Predicting the Resolution of Referring Expressions from User Behavior</a></p>
<p>13 0.29506743 <a title="6-lsi-13" href="./emnlp-2013-This_Text_Has_the_Scent_of_Starbucks%3A_A_Laplacian_Structured_Sparsity_Model_for_Computational_Branding_Analytics.html">184 emnlp-2013-This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</a></p>
<p>14 0.28539714 <a title="6-lsi-14" href="./emnlp-2013-Classifying_Message_Board_Posts_with_an_Extracted_Lexicon_of_Patient_Attributes.html">46 emnlp-2013-Classifying Message Board Posts with an Extracted Lexicon of Patient Attributes</a></p>
<p>15 0.28242564 <a title="6-lsi-15" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>16 0.27519453 <a title="6-lsi-16" href="./emnlp-2013-Leveraging_Lexical_Cohesion_and_Disruption_for_Topic_Segmentation.html">124 emnlp-2013-Leveraging Lexical Cohesion and Disruption for Topic Segmentation</a></p>
<p>17 0.27267122 <a title="6-lsi-17" href="./emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM.html">99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</a></p>
<p>18 0.26156998 <a title="6-lsi-18" href="./emnlp-2013-Rule-Based_Information_Extraction_is_Dead%21_Long_Live_Rule-Based_Information_Extraction_Systems%21.html">161 emnlp-2013-Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!</a></p>
<p>19 0.25545749 <a title="6-lsi-19" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<p>20 0.24957827 <a title="6-lsi-20" href="./emnlp-2013-A_Log-Linear_Model_for_Unsupervised_Text_Normalization.html">9 emnlp-2013-A Log-Linear Model for Unsupervised Text Normalization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.534), (18, 0.025), (22, 0.029), (30, 0.052), (50, 0.014), (51, 0.121), (66, 0.029), (71, 0.022), (75, 0.021), (77, 0.02), (96, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90742493 <a title="6-lda-1" href="./emnlp-2013-A_Generative_Joint%2C_Additive%2C_Sequential_Model_of_Topics_and_Speech_Acts_in_Patient-Doctor_Communication.html">6 emnlp-2013-A Generative Joint, Additive, Sequential Model of Topics and Speech Acts in Patient-Doctor Communication</a></p>
<p>Author: Byron C. Wallace ; Thomas A Trikalinos ; M. Barton Laws ; Ira B. Wilson ; Eugene Charniak</p><p>Abstract: We develop a novel generative model of conversation that jointly captures both the topical content and the speech act type associated with each utterance. Our model expresses both token emission and state transition probabilities as log-linear functions of separate components corresponding to topics and speech acts (and their interactions). We apply this model to a dataset comprising annotated patient-physician visits and show that the proposed joint approach outperforms a baseline univariate model.</p><p>2 0.80092168 <a title="6-lda-2" href="./emnlp-2013-A_Discourse-Driven_Content_Model_for_Summarising_Scientific_Articles_Evaluated_in_a_Complex_Question_Answering_Task.html">5 emnlp-2013-A Discourse-Driven Content Model for Summarising Scientific Articles Evaluated in a Complex Question Answering Task</a></p>
<p>Author: Maria Liakata ; Simon Dobnik ; Shyamasree Saha ; Colin Batchelor ; Dietrich Rebholz-Schuhmann</p><p>Abstract: We present a method which exploits automatically generated scientific discourse annotations to create a content model for the summarisation of scientific articles. Full papers are first automatically annotated using the CoreSC scheme, which captures 11 contentbased concepts such as Hypothesis, Result, Conclusion etc at the sentence level. A content model which follows the sequence of CoreSC categories observed in abstracts is used to provide the skeleton of the summary, making a distinction between dependent and independent categories. Summary creation is also guided by the distribution of CoreSC categories found in the full articles, in order to adequately represent the article content. Fi- nally, we demonstrate the usefulness of the summaries by evaluating them in a complex question answering task. Results are very encouraging as summaries of papers from automatically obtained CoreSCs enable experts to answer 66% of complex content-related questions designed on the basis of paper abstracts. The questions were answered with a precision of 75%, where the upper bound for human summaries (abstracts) was 95%.</p><p>3 0.78868514 <a title="6-lda-3" href="./emnlp-2013-Detecting_Promotional_Content_in_Wikipedia.html">61 emnlp-2013-Detecting Promotional Content in Wikipedia</a></p>
<p>Author: Shruti Bhosale ; Heath Vinicombe ; Raymond Mooney</p><p>Abstract: This paper presents an approach for detecting promotional content in Wikipedia. By incorporating stylometric features, including features based on n-gram and PCFG language models, we demonstrate improved accuracy at identifying promotional articles, compared to using only lexical information and metafeatures.</p><p>4 0.75113136 <a title="6-lda-4" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>Author: Eduardo Blanco ; Dan Moldovan</p><p>Abstract: This paper presents a novel approach to determine textual similarity. A layered methodology to transform text into logic forms is proposed, and semantic features are derived from a logic prover. Experimental results show that incorporating the semantic structure of sentences is beneficial. When training data is unavailable, scores obtained from the logic prover in an unsupervised manner outperform supervised methods.</p><p>5 0.42649183 <a title="6-lda-5" href="./emnlp-2013-Automatically_Determining_a_Proper_Length_for_Multi-Document_Summarization%3A_A_Bayesian_Nonparametric_Approach.html">36 emnlp-2013-Automatically Determining a Proper Length for Multi-Document Summarization: A Bayesian Nonparametric Approach</a></p>
<p>Author: Tengfei Ma ; Hiroshi Nakagawa</p><p>Abstract: Document summarization is an important task in the area of natural language processing, which aims to extract the most important information from a single document or a cluster of documents. In various summarization tasks, the summary length is manually defined. However, how to find the proper summary length is quite a problem; and keeping all summaries restricted to the same length is not always a good choice. It is obviously improper to generate summaries with the same length for two clusters of documents which contain quite different quantity of information. In this paper, we propose a Bayesian nonparametric model for multidocument summarization in order to automatically determine the proper lengths of summaries. Assuming that an original document can be reconstructed from its summary, we describe the ”reconstruction” by a Bayesian framework which selects sentences to form a good summary. Experimental results on DUC2004 data sets and some expanded data demonstrate the good quality of our summaries and the rationality of the length determination.</p><p>6 0.42276621 <a title="6-lda-6" href="./emnlp-2013-Modeling_Scientific_Impact_with_Topical_Influence_Regression.html">133 emnlp-2013-Modeling Scientific Impact with Topical Influence Regression</a></p>
<p>7 0.4166846 <a title="6-lda-7" href="./emnlp-2013-Single-Document_Summarization_as_a_Tree_Knapsack_Problem.html">174 emnlp-2013-Single-Document Summarization as a Tree Knapsack Problem</a></p>
<p>8 0.41403762 <a title="6-lda-8" href="./emnlp-2013-Automatically_Classifying_Edit_Categories_in_Wikipedia_Revisions.html">34 emnlp-2013-Automatically Classifying Edit Categories in Wikipedia Revisions</a></p>
<p>9 0.4134368 <a title="6-lda-9" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>10 0.39702636 <a title="6-lda-10" href="./emnlp-2013-Predicting_the_Resolution_of_Referring_Expressions_from_User_Behavior.html">153 emnlp-2013-Predicting the Resolution of Referring Expressions from User Behavior</a></p>
<p>11 0.3937183 <a title="6-lda-11" href="./emnlp-2013-Elephant%3A_Sequence_Labeling_for_Word_and_Sentence_Segmentation.html">72 emnlp-2013-Elephant: Sequence Labeling for Word and Sentence Segmentation</a></p>
<p>12 0.39199358 <a title="6-lda-12" href="./emnlp-2013-Word_Level_Language_Identification_in_Online_Multilingual_Communication.html">204 emnlp-2013-Word Level Language Identification in Online Multilingual Communication</a></p>
<p>13 0.38136381 <a title="6-lda-13" href="./emnlp-2013-Feature_Noising_for_Log-Linear_Structured_Prediction.html">86 emnlp-2013-Feature Noising for Log-Linear Structured Prediction</a></p>
<p>14 0.37989625 <a title="6-lda-14" href="./emnlp-2013-Inducing_Document_Plans_for_Concept-to-Text_Generation.html">106 emnlp-2013-Inducing Document Plans for Concept-to-Text Generation</a></p>
<p>15 0.3779237 <a title="6-lda-15" href="./emnlp-2013-Opinion_Mining_in_Newspaper_Articles_by_Entropy-Based_Word_Connections.html">144 emnlp-2013-Opinion Mining in Newspaper Articles by Entropy-Based Word Connections</a></p>
<p>16 0.37737072 <a title="6-lda-16" href="./emnlp-2013-Using_Topic_Modeling_to_Improve_Prediction_of_Neuroticism_and_Depression_in_College_Students.html">199 emnlp-2013-Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students</a></p>
<p>17 0.36999765 <a title="6-lda-17" href="./emnlp-2013-Measuring_Ideological_Proportions_in_Political_Speeches.html">129 emnlp-2013-Measuring Ideological Proportions in Political Speeches</a></p>
<p>18 0.36945266 <a title="6-lda-18" href="./emnlp-2013-Using_Crowdsourcing_to_get_Representations_based_on_Regular_Expressions.html">196 emnlp-2013-Using Crowdsourcing to get Representations based on Regular Expressions</a></p>
<p>19 0.36920679 <a title="6-lda-19" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>20 0.36769071 <a title="6-lda-20" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
