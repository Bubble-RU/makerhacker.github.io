<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-25" href="#">emnlp2013-25</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</h1>
<br/><p>Source: <a title="emnlp-2013-25-pdf" href="http://aclweb.org/anthology//D/D13/D13-1017.pdf">pdf</a></p><p>Author: Om P. Damani ; Shweta Ghonge</p><p>Abstract: Two recent measures incorporate the notion of statistical significance in basic PMI formulation. In some tasks, we find that the new measures perform worse than the PMI. Our analysis shows that while the basic ideas in incorporating statistical significance in PMI are reasonable, they have been applied slightly inappropriately. By fixing this, we get new measures that improve performance over not just PMI but on other popular co-occurrence measures as well. In fact, the revised measures perform reasonably well compared with more resource intensive non co-occurrence based methods also.</p><p>Reference: <a title="emnlp-2013-25-reference" href="../emnlp2013_reference/emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 in  Abstract Two recent measures incorporate the notion of statistical significance in basic PMI formulation. [sent-5, score-0.436]
</p><p>2 In some tasks, we find that the new measures perform worse than the PMI. [sent-6, score-0.288]
</p><p>3 Our analysis shows that while the basic ideas in incorporating statistical significance in PMI are reasonable, they have been applied slightly inappropriately. [sent-7, score-0.181]
</p><p>4 By fixing this, we get new measures that improve performance over not just PMI but on other popular co-occurrence measures as well. [sent-8, score-0.593]
</p><p>5 In fact, the revised measures perform reasonably well compared with more resource intensive non co-occurrence based methods also. [sent-9, score-0.592]
</p><p>6 1 Introduction The notion of word association is used in many language processing and information retrieval applications and it is important to have low-cost, highquality association measures. [sent-10, score-0.103]
</p><p>7 Lexical co-occurrence  based word association measures are popular because they are computationally efficient and they can be applied to any language easily. [sent-11, score-0.339]
</p><p>8 One of the most popular co-occurrence measure is Pointwise Mutual Information (PMI) (Church and Hanks, 1989). [sent-12, score-0.082]
</p><p>9 To overcome this, recently two new measures have been proposed that incorporate the notion of statistical significance in basic PMI formulation. [sent-14, score-0.436]
</p><p>10 In (Washtell and Markert, 2009), statistical significance is introduced in PMIsig by multiplying PMI value with the square root of the evidence. [sent-15, score-0.212]
</p><p>11 In contrast, in (Damani, 2013), cPMId is 163 introduced by bounding the probability of observing a given deviation between a given word pair’s cooccurrence count and its expected value under a null model where with each word a global unigram generation probability is associated. [sent-16, score-0.224]
</p><p>12 In Table 1, we give the definitions of PMI, PMIsig, and cPMId. [sent-17, score-0.042]
</p><p>13 While these new measures perform better than PMI on some of the tasks, on many other tasks,  we find that the new measures perform worse than the PMI. [sent-18, score-0.576]
</p><p>14 In Table 3, we show how these measures perform compared to PMI on four different tasks. [sent-19, score-0.288]
</p><p>15 We find that PMIsig degrades performance in three out of these four tasks while cPMId degrades performance in two out of these four tasks. [sent-20, score-0.068]
</p><p>16 Our analysis shows that while the basic ideas in incorporating statistical significance are reasonable, they have been applied slightly inappropriately. [sent-23, score-0.181]
</p><p>17 By fixing this, we get new measures that improve performance over not just PMI, but also on other popular co-occurrence measures on most of these tasks. [sent-24, score-0.593]
</p><p>18 In fact, the revised measures perform reasonably well compared with more resource intensive non cooccurrence based methods also. [sent-25, score-0.667]
</p><p>19 2  Adapting PMI for Statistical Significance  In (Washtell and Markert, 2009), it is assumed that the statistical significance of a word pair association is proportional to the square root of the evidence. [sent-26, score-0.212]
</p><p>20 The question of what constitutes the evidence is answered by taking the lesser of the frequencies of the  two words in the word pair, since at most that many pairings are possible. [sent-27, score-0.052]
</p><p>21 The sub-parts in bold represent the changes between the original formulas and the revised formulas. [sent-33, score-0.069]
</p><p>22 f tT hite eth pisro way t om emphasize t(yhe) )tr ∗an msfionr(mda(txio),nd f(ryo)m) icnP MsPMIdI. [sent-35, score-0.04]
</p><p>23 In (Dpamani, 2013), statistical significance is introduced by bounding the probability of observing a given number of word-pair occurrences in the corpus, just by chance, under a null model of independent unigram occurrences. [sent-37, score-0.384]
</p><p>24 For this computation, one needs to decide what constitutes a random trial when looking for a word-pair occurrence. [sent-38, score-0.104]
</p><p>25 Is it the occurrence of the first word (say x) in the pair, or the second (say y). [sent-39, score-0.059]
</p><p>26 In (Damani, 2013), occurrences of x are arbitrarily chosen to represent the sites of the random trial. [sent-40, score-0.126]
</p><p>27 Using Hoeffdings Inequality:  +  P[f(x, y) ≥ f(x) ∗ f(y)/W f(x) ∗ t] ≤ exp(−2 ∗ f(x) ∗ t2)  δ  By setting t = plnδ/(−2 ∗ f(x)), we get as an upper bound on pprobability ∗of f observing more tsh aann f(x) ∗ f(y)/W + f(x) ∗ t bigram occurrences in the corpus, just by +chfan(cxe). [sent-41, score-0.131]
</p><p>28 Bta bsiegdr on othccisu Corpus i Lne vtheel Significant PMI(cPMI) is defined as:  cPMI(x,y) = logf(x) ∗ f(yf)(/xW,y) + f(x) ∗ t  = logf(x) ∗ f(y)/W +f(px,fy()x) ∗plnδ/(−2) In (Damani, 2013), severapl variantsp of cPMI are introduced that incorporate different notions of statistical significance. [sent-42, score-0.067]
</p><p>29 1 Choice of Random Trial While considering statistical significance, one has to decide what constitutes a random trial. [sent-45, score-0.084]
</p><p>30 When  looking for a word-pair (x, y)’s occurrences, y can potentially occur near each occurrence of x, or x can potentially occur near each occurrence of y. [sent-46, score-0.194]
</p><p>31 Which of these two set of occurrences should be considered the sites of random trial. [sent-47, score-0.126]
</p><p>32 We believe that the occurrences of the more frequent of x and y should be considered, since near each ofthese occurrences the other word could have occurred. [sent-48, score-0.248]
</p><p>33 Similarly, d(x) and d(y) in cPMId formula should be replaced with max(d(x) , d(y)) and min(d(x) , d(y)) respectively to give a new measure Significant PMI based on Documpent count(sPMId). [sent-50, score-0.069]
</p><p>34 Using the same logic, pmin(f(x),f(y)) ipn PMIsig formula should bpe replaced with pmax(f(x),f(y)) to give the formula for a new pmeasure PMI-significant(PMIs). [sent-51, score-0.072]
</p><p>35 The definitions of sPMId and PMIs are also given in Table 1. [sent-52, score-0.042]
</p><p>36 3  Related Work  There are three main types of word association measures: Knowledge based, Distributional Similarity based, and Lexical Co-occurrence based. [sent-53, score-0.035]
</p><p>37 co-occurrence measures for each data-set is shown in bold and underline respectively. [sent-55, score-0.255]
</p><p>38 Except GoogleDistance and LLR, all results for all co-occurrence measures are statistically significant at p = . [sent-56, score-0.255]
</p><p>39 For each task, the best known result for different non co-occurrence based methods is also shown. [sent-58, score-0.102]
</p><p>40 165  two words for distributional similarity (Agirre et al. [sent-59, score-0.126]
</p><p>41 Knowledge-based measures use knowledgesources like thesauri, semantic networks, or taxonomies (Milne and Witten, 2008; Hughes and Ramage, 2007; Gabrilovich and Markovitch, 2007; Yeh et al. [sent-67, score-0.305]
</p><p>42 Co-occurrence based measures (Pecina and Schlesinger, 2006) simply rely on unigram and bigram frequencies of the words in a pair. [sent-70, score-0.291]
</p><p>43 1 Co-occurrence Measures being Compared Co-occurrence based measures of association be-  tween two entities are used in several domains like ecology, psychology, medicine, language processing, etc. [sent-73, score-0.29]
</p><p>44 To compare the performance of our newly introduced measures with other co-occurrence measures, we have selected a number of popular co-occurrence measures like ChiSquare (χ2), Dice (Dice, 1945), GoogleDistance (L. [sent-74, score-0.594]
</p><p>45 In addition to these popular measures, we also experiment with other known variations of PMI like nPMI (Bouma, 2009), PMI2 (Daille, 1994), Ochiai (Janson and Vegelius, 1981), and SCI (Washtell and Markert, 2009). [sent-76, score-0.049]
</p><p>46 In Table 2, we present the definitions of these measures. [sent-78, score-0.042]
</p><p>47 1, we can assume that SCI is PMI adapted for statistical significance (multiplied by √f(y)), where the site of random trial is taken to be the occurrences of the second word y, instead of the less frequent  word, as in the case of PMIsig. [sent-80, score-0.287]
</p><p>48 The span of a word-pair’s occurrence is the direction-independent distance between the occurrences of the members of the pair. [sent-82, score-0.175]
</p><p>49 We consider only those co-occurrences where span is less than a given threshold. [sent-83, score-0.029]
</p><p>50 Therefore, span threshold is a parameter for all the co-occurrence measures being considered. [sent-84, score-0.284]
</p><p>51 4  Performance Evaluation  Having introduced the revised measures PMIs and sPMId, we need to evaluate the performance ofthese measures compared to PMI and the original measures introducing significance. [sent-85, score-0.905]
</p><p>52 In addition, we also wish to compare the performance of these measures with other co-occurrence measures. [sent-86, score-0.255]
</p><p>53 To compare the performance of these measures with more resource heavy non co-occurrence based measures, we have chosen those tasks and datasets on which published results exist for distributional similarity and knowl-  edge based word association measures. [sent-87, score-0.554]
</p><p>54 1 Task Details We evaluate these measures on three tasks: Sentence Similarity(65 sentence-pairs from (Li et al. [sent-89, score-0.255]
</p><p>55 For each of these tasks, gold standard human judgment results exist. [sent-92, score-0.032]
</p><p>56 , 2006), we evaluate a measure by the Pearsons correlation between the ranking produced by the measure and the human ranking. [sent-94, score-0.066]
</p><p>57 For synonym selection, we compute the percentage of correct answers, since there is a unique answer for each challenge word in the datasets. [sent-95, score-0.03]
</p><p>58 Semantic relatedness has been evaluated by Spearman’s rank correlation with human judgment instead of Pearsons correlation in literature and we follow the same practice to make results comparable. [sent-96, score-0.117]
</p><p>59 For sentence similarity detection, the algorithm used by us (Li et al. [sent-97, score-0.053]
</p><p>60 Hence we normalize the value produced by each measure using  gray-row, for all other questions, incorrect answers becomes correct on using PMIs instead of PMIsig, and vice-versa for the gray-row. [sent-99, score-0.15]
</p><p>61 The association values have been suitably scaled for readability. [sent-100, score-0.035]
</p><p>62 max-min normalization: 0  v0  v − min = max − min mvax − − m minin  where max and min are computed over all association scores for the entire task for a given measure. [sent-103, score-0.289]
</p><p>63 In Table 3, we present the performance of all the co-occurrence measures considered on all the tasks. [sent-109, score-0.255]
</p><p>64 Note that, except GoogleDistance and LLR, all re-  sults for all co-occurrence measures are statistically significant at p = . [sent-110, score-0.255]
</p><p>65 For completeness of comparison, we also include the best known results from literature for different non co-occurrence based word association measures on these tasks. [sent-112, score-0.392]
</p><p>66 3 Performance Analysis and Conclusions We find that on average, PMIsig and cPMId, the recently introduced measures that incorporate significance in PMI, do not perform better than PMI on the given datasets. [sent-114, score-0.439]
</p><p>67 Both of them perform worse than PMI on three out of four datasets. [sent-115, score-0.033]
</p><p>68 By appropriately incorporating significance, we get new measures PMIs and sPMId that perform better than PMI(also PMIsig and cPMId respectively) on most datasets. [sent-116, score-0.354]
</p><p>69 For example, on the ESL dataset, while the percentage of correct answers increases from 58 to 66 from  PMIsig to PMIs, it is not the case that on moving from PMIsig to PMIs, several correct answers become incorrect and an even larger number of incorrect answers become correct. [sent-119, score-0.337]
</p><p>70 As shown in Table 4, only one correct answers become incorrect while seven incorrect answers get corrected. [sent-120, score-0.234]
</p><p>71 The same trend holds for most parameters values, and for moving from cPMId to sPMId. [sent-121, score-0.029]
</p><p>72 PMIs and sPMId perform better than not just PMI, but they perform better than all popular cooccurrence measures on most of these tasks. [sent-124, score-0.445]
</p><p>73 When compared with any other co-occurrence measure, on three out of four datasets each, both PMIs and sPMId perform better than that measure. [sent-125, score-0.033]
</p><p>74 In fact, PMIs and sPMId perform reasonably well compared with more resource intensive non co-occurrence based methods as well. [sent-126, score-0.268]
</p><p>75 Note that different non cooccurrence based measures perform well on different tasks. [sent-127, score-0.465]
</p><p>76 We are comparing the performance of a single measure (say sPMId or PMIs) against the best measure for each task. [sent-128, score-0.066]
</p><p>77 A study on similarity and relatedness using distributional and wordnet-based approaches. [sent-132, score-0.211]
</p><p>78 Measuring semantic similarity between words using web search engines. [sent-136, score-0.103]
</p><p>79 Normalized (pointwise) mutual information in collocation extraction, from form to meaning: Processing texts automatically. [sent-140, score-0.094]
</p><p>80 Novel association measures using web search with double checking. [sent-144, score-0.29]
</p><p>81 Improving pointwise mutual information (pmi) by incorporating significant cooccurrence. [sent-160, score-0.139]
</p><p>82 Measures of the amount of ecological association between species. [sent-166, score-0.092]
</p><p>83 General estimation and evaluation of compositional distributional  semantic models. [sent-187, score-0.123]
</p><p>84 Experimental support for a categorical compositional distributional model of meaning. [sent-195, score-0.073]
</p><p>85 A solution to platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. [sent-217, score-0.05]
</p><p>86 Sentence similarity based on semantic nets and corpus statistics. [sent-230, score-0.103]
</p><p>87 An effective, lowcost measure of semantic relatedness obtained from wikipedia links. [sent-239, score-0.168]
</p><p>88 More data trumps smarter algorithms: Comparing pointwise mutual information with latent semantic analysis. [sent-254, score-0.156]
</p><p>89 Hsh: Estimating semantic similarity of words and short phrases with frequency normalized distance measures. [sent-280, score-0.103]
</p><p>90 A comparison of windowless and window-based computational association measures as predictors of syntagmatic human associations. [sent-284, score-0.29]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pmi', 0.399), ('pmis', 0.368), ('spmid', 0.339), ('pmisig', 0.283), ('measures', 0.255), ('cpmid', 0.254), ('damani', 0.141), ('washtell', 0.141), ('significance', 0.116), ('cpmi', 0.113), ('non', 0.102), ('occurrences', 0.087), ('googledistance', 0.085), ('relatedness', 0.085), ('markert', 0.084), ('cooccurrence', 0.075), ('answers', 0.074), ('sci', 0.074), ('llr', 0.074), ('finkelstein', 0.074), ('distributional', 0.073), ('revised', 0.069), ('gabrilovich', 0.059), ('occurrence', 0.059), ('ecological', 0.057), ('ecology', 0.057), ('firth', 0.057), ('janson', 0.057), ('logf', 0.057), ('ochiai', 0.057), ('pearsons', 0.057), ('pln', 0.057), ('pmin', 0.057), ('wandmacher', 0.057), ('min', 0.056), ('pointwise', 0.054), ('intensive', 0.054), ('similarity', 0.053), ('mutual', 0.052), ('trial', 0.052), ('constitutes', 0.052), ('semantic', 0.05), ('agirre', 0.05), ('popular', 0.049), ('kartsaklis', 0.049), ('pecina', 0.049), ('simpson', 0.049), ('dice', 0.045), ('esl', 0.045), ('hughes', 0.045), ('liberman', 0.045), ('observing', 0.044), ('incorrect', 0.043), ('reasonably', 0.043), ('max', 0.043), ('definitions', 0.042), ('collocation', 0.042), ('bollegala', 0.042), ('markovitch', 0.042), ('shaul', 0.042), ('yeh', 0.042), ('yf', 0.042), ('om', 0.04), ('sites', 0.039), ('ramage', 0.039), ('evgeniy', 0.039), ('near', 0.038), ('aitor', 0.038), ('eneko', 0.038), ('mehrnoosh', 0.038), ('unigram', 0.036), ('milne', 0.036), ('ofthese', 0.036), ('sadrzadeh', 0.036), ('resource', 0.036), ('formula', 0.036), ('association', 0.035), ('introduced', 0.035), ('degrades', 0.034), ('fixing', 0.034), ('bounding', 0.034), ('pavel', 0.034), ('dinu', 0.034), ('incorporating', 0.033), ('jaccard', 0.033), ('appropriately', 0.033), ('landauer', 0.033), ('measure', 0.033), ('perform', 0.033), ('notion', 0.033), ('statistical', 0.032), ('grefenstette', 0.032), ('judgment', 0.032), ('strube', 0.032), ('church', 0.03), ('synonym', 0.03), ('square', 0.029), ('psychological', 0.029), ('moving', 0.029), ('span', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="25-tfidf-1" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<p>Author: Om P. Damani ; Shweta Ghonge</p><p>Abstract: Two recent measures incorporate the notion of statistical significance in basic PMI formulation. In some tasks, we find that the new measures perform worse than the PMI. Our analysis shows that while the basic ideas in incorporating statistical significance in PMI are reasonable, they have been applied slightly inappropriately. By fixing this, we get new measures that improve performance over not just PMI but on other popular co-occurrence measures as well. In fact, the revised measures perform reasonably well compared with more resource intensive non co-occurrence based methods also.</p><p>2 0.14275771 <a title="25-tfidf-2" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>Author: Angeliki Lazaridou ; Eva Maria Vecchi ; Marco Baroni</p><p>Abstract: In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the correct parsing of NPs (e.g., (live fish) transporter rather than live (fish transporter)). We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features.</p><p>3 0.075965449 <a title="25-tfidf-3" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>Author: Eva Maria Vecchi ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: In this study, we use compositional distributional semantic methods to investigate restrictions in adjective ordering. Specifically, we focus on properties distinguishing AdjectiveAdjective-Noun phrases in which there is flexibility in the adjective ordering from those bound to a rigid order. We explore a number of measures extracted from the distributional representation of AAN phrases which may indicate a word order restriction. We find that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modification of the adjectives. Our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics.</p><p>4 0.074437425 <a title="25-tfidf-4" href="./emnlp-2013-Unsupervised_Induction_of_Contingent_Event_Pairs_from_Film_Scenes.html">192 emnlp-2013-Unsupervised Induction of Contingent Event Pairs from Film Scenes</a></p>
<p>Author: Zhichao Hu ; Elahe Rahimtoroghi ; Larissa Munishkina ; Reid Swanson ; Marilyn A. Walker</p><p>Abstract: Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning. Researchers in NLP have tackled modeling such expectations from a range of perspectives, including treating it as the inference of the CONTINGENT discourse relation, or as a type of common-sense causal reasoning. Our approach is to model likelihood between events by drawing on several of these lines of previous work. We implement and evaluate different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another. We refine event pairs that we learn from a corpus of film scene descriptions utilizing web search counts, and evaluate our results by collecting human judgments ofcontingency. Our results indicate that the use of web search counts increases the av- , erage accuracy of our best method to 85.64% over a baseline of 50%, as compared to an average accuracy of 75. 15% without web search.</p><p>5 0.073820084 <a title="25-tfidf-5" href="./emnlp-2013-Scaling_to_Large3_Data%3A_An_Efficient_and_Effective_Method_to_Compute_Distributional_Thesauri.html">165 emnlp-2013-Scaling to Large3 Data: An Efficient and Effective Method to Compute Distributional Thesauri</a></p>
<p>Author: Martin Riedl ; Chris Biemann</p><p>Abstract: We introduce a new highly scalable approach for computing Distributional Thesauri (DTs). By employing pruning techniques and a distributed framework, we make the computation for very large corpora feasible on comparably small computational resources. We demonstrate this by releasing a DT for the whole vocabulary of Google Books syntactic n-grams. Evaluating against lexical resources using two measures, we show that our approach produces higher quality DTs than previous approaches, and is thus preferable in terms of speed and quality for large corpora.</p><p>6 0.071428634 <a title="25-tfidf-6" href="./emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM.html">99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</a></p>
<p>7 0.070015408 <a title="25-tfidf-7" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>8 0.058529813 <a title="25-tfidf-8" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>9 0.052383039 <a title="25-tfidf-9" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>10 0.051536236 <a title="25-tfidf-10" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>11 0.045735158 <a title="25-tfidf-11" href="./emnlp-2013-Building_Specialized_Bilingual_Lexicons_Using_Large_Scale_Background_Knowledge.html">42 emnlp-2013-Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge</a></p>
<p>12 0.044823259 <a title="25-tfidf-12" href="./emnlp-2013-Automatically_Identifying_Pseudepigraphic_Texts.html">37 emnlp-2013-Automatically Identifying Pseudepigraphic Texts</a></p>
<p>13 0.042794958 <a title="25-tfidf-13" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>14 0.041497122 <a title="25-tfidf-14" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>15 0.041152496 <a title="25-tfidf-15" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<p>16 0.038953155 <a title="25-tfidf-16" href="./emnlp-2013-Application_of_Localized_Similarity_for_Web_Documents.html">24 emnlp-2013-Application of Localized Similarity for Web Documents</a></p>
<p>17 0.037049644 <a title="25-tfidf-17" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<p>18 0.035802308 <a title="25-tfidf-18" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>19 0.035552081 <a title="25-tfidf-19" href="./emnlp-2013-Using_Paraphrases_and_Lexical_Semantics_to_Improve_the_Accuracy_and_the_Robustness_of_Supervised_Models_in_Situated_Dialogue_Systems.html">197 emnlp-2013-Using Paraphrases and Lexical Semantics to Improve the Accuracy and the Robustness of Supervised Models in Situated Dialogue Systems</a></p>
<p>20 0.035322413 <a title="25-tfidf-20" href="./emnlp-2013-Learning_to_Rank_Lexical_Substitutions.html">123 emnlp-2013-Learning to Rank Lexical Substitutions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.124), (1, 0.033), (2, -0.045), (3, -0.005), (4, 0.013), (5, 0.091), (6, 0.001), (7, -0.021), (8, -0.071), (9, -0.069), (10, 0.033), (11, 0.044), (12, -0.057), (13, 0.051), (14, 0.03), (15, -0.047), (16, 0.007), (17, -0.029), (18, -0.07), (19, 0.009), (20, -0.095), (21, 0.059), (22, -0.086), (23, -0.026), (24, 0.078), (25, -0.172), (26, -0.103), (27, 0.129), (28, -0.115), (29, -0.135), (30, -0.086), (31, -0.047), (32, 0.1), (33, 0.122), (34, -0.019), (35, 0.014), (36, 0.171), (37, -0.163), (38, -0.037), (39, -0.017), (40, 0.142), (41, 0.001), (42, 0.129), (43, 0.038), (44, 0.074), (45, -0.022), (46, -0.064), (47, 0.084), (48, 0.026), (49, -0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94895607 <a title="25-lsi-1" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<p>Author: Om P. Damani ; Shweta Ghonge</p><p>Abstract: Two recent measures incorporate the notion of statistical significance in basic PMI formulation. In some tasks, we find that the new measures perform worse than the PMI. Our analysis shows that while the basic ideas in incorporating statistical significance in PMI are reasonable, they have been applied slightly inappropriately. By fixing this, we get new measures that improve performance over not just PMI but on other popular co-occurrence measures as well. In fact, the revised measures perform reasonably well compared with more resource intensive non co-occurrence based methods also.</p><p>2 0.72744286 <a title="25-lsi-2" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>Author: Angeliki Lazaridou ; Eva Maria Vecchi ; Marco Baroni</p><p>Abstract: In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the correct parsing of NPs (e.g., (live fish) transporter rather than live (fish transporter)). We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features.</p><p>3 0.70643204 <a title="25-lsi-3" href="./emnlp-2013-Scaling_to_Large3_Data%3A_An_Efficient_and_Effective_Method_to_Compute_Distributional_Thesauri.html">165 emnlp-2013-Scaling to Large3 Data: An Efficient and Effective Method to Compute Distributional Thesauri</a></p>
<p>Author: Martin Riedl ; Chris Biemann</p><p>Abstract: We introduce a new highly scalable approach for computing Distributional Thesauri (DTs). By employing pruning techniques and a distributed framework, we make the computation for very large corpora feasible on comparably small computational resources. We demonstrate this by releasing a DT for the whole vocabulary of Google Books syntactic n-grams. Evaluating against lexical resources using two measures, we show that our approach produces higher quality DTs than previous approaches, and is thus preferable in terms of speed and quality for large corpora.</p><p>4 0.68948227 <a title="25-lsi-4" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>Author: Eva Maria Vecchi ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: In this study, we use compositional distributional semantic methods to investigate restrictions in adjective ordering. Specifically, we focus on properties distinguishing AdjectiveAdjective-Noun phrases in which there is flexibility in the adjective ordering from those bound to a rigid order. We explore a number of measures extracted from the distributional representation of AAN phrases which may indicate a word order restriction. We find that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modification of the adjectives. Our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics.</p><p>5 0.6206218 <a title="25-lsi-5" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>Author: Polina Kuznetsova ; Jianfu Chen ; Yejin Choi</p><p>Abstract: Why do certain combinations of words such as “disadvantageous peace ” or “metal to the petal” appeal to our minds as interesting expressions with a sense of creativity, while other phrases such as “quiet teenager”, or “geometrical base ” not as much? We present statistical explorations to understand the characteristics of lexical compositions that give rise to the perception of being original, interesting, and at times even artistic. We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learning that give us further insights on how different aspects of lexical composition collectively contribute to the perceived creativity.</p><p>6 0.37235028 <a title="25-lsi-6" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>7 0.36044624 <a title="25-lsi-7" href="./emnlp-2013-The_Topology_of_Semantic_Knowledge.html">182 emnlp-2013-The Topology of Semantic Knowledge</a></p>
<p>8 0.35515347 <a title="25-lsi-8" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>9 0.33498517 <a title="25-lsi-9" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>10 0.32798463 <a title="25-lsi-10" href="./emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM.html">99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</a></p>
<p>11 0.3028172 <a title="25-lsi-11" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<p>12 0.29455492 <a title="25-lsi-12" href="./emnlp-2013-Unsupervised_Induction_of_Contingent_Event_Pairs_from_Film_Scenes.html">192 emnlp-2013-Unsupervised Induction of Contingent Event Pairs from Film Scenes</a></p>
<p>13 0.28675371 <a title="25-lsi-13" href="./emnlp-2013-Learning_to_Rank_Lexical_Substitutions.html">123 emnlp-2013-Learning to Rank Lexical Substitutions</a></p>
<p>14 0.28097919 <a title="25-lsi-14" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>15 0.27380499 <a title="25-lsi-15" href="./emnlp-2013-With_Blinkers_on%3A_Robust_Prediction_of_Eye_Movements_across_Readers.html">203 emnlp-2013-With Blinkers on: Robust Prediction of Eye Movements across Readers</a></p>
<p>16 0.2697458 <a title="25-lsi-16" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>17 0.26756436 <a title="25-lsi-17" href="./emnlp-2013-Application_of_Localized_Similarity_for_Web_Documents.html">24 emnlp-2013-Application of Localized Similarity for Web Documents</a></p>
<p>18 0.2611635 <a title="25-lsi-18" href="./emnlp-2013-Automatically_Identifying_Pseudepigraphic_Texts.html">37 emnlp-2013-Automatically Identifying Pseudepigraphic Texts</a></p>
<p>19 0.25760287 <a title="25-lsi-19" href="./emnlp-2013-Building_Specialized_Bilingual_Lexicons_Using_Large_Scale_Background_Knowledge.html">42 emnlp-2013-Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge</a></p>
<p>20 0.25742063 <a title="25-lsi-20" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.021), (6, 0.013), (18, 0.02), (22, 0.59), (30, 0.032), (50, 0.01), (51, 0.133), (66, 0.034), (71, 0.018), (75, 0.02), (96, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92215788 <a title="25-lda-1" href="./emnlp-2013-Event-Based_Time_Label_Propagation_for_Automatic_Dating_of_News_Articles.html">74 emnlp-2013-Event-Based Time Label Propagation for Automatic Dating of News Articles</a></p>
<p>Author: Tao Ge ; Baobao Chang ; Sujian Li ; Zhifang Sui</p><p>Abstract: Since many applications such as timeline summaries and temporal IR involving temporal analysis rely on document timestamps, the task of automatic dating of documents has been increasingly important. Instead of using feature-based methods as conventional models, our method attempts to date documents in a year level by exploiting relative temporal relations between documents and events, which are very effective for dating documents. Based on this intuition, we proposed an eventbased time label propagation model called confidence boosting in which time label information can be propagated between documents and events on a bipartite graph. The experiments show that our event-based propagation model can predict document timestamps in high accuracy and the model combined with a MaxEnt classifier outperforms the state-ofthe-art method for this task especially when the size of the training set is small.</p><p>same-paper 2 0.91406184 <a title="25-lda-2" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<p>Author: Om P. Damani ; Shweta Ghonge</p><p>Abstract: Two recent measures incorporate the notion of statistical significance in basic PMI formulation. In some tasks, we find that the new measures perform worse than the PMI. Our analysis shows that while the basic ideas in incorporating statistical significance in PMI are reasonable, they have been applied slightly inappropriately. By fixing this, we get new measures that improve performance over not just PMI but on other popular co-occurrence measures as well. In fact, the revised measures perform reasonably well compared with more resource intensive non co-occurrence based methods also.</p><p>3 0.86764467 <a title="25-lda-3" href="./emnlp-2013-Building_Event_Threads_out_of_Multiple_News_Articles.html">41 emnlp-2013-Building Event Threads out of Multiple News Articles</a></p>
<p>Author: Xavier Tannier ; Veronique Moriceau</p><p>Abstract: We present an approach for building multidocument event threads from a large corpus of newswire articles. An event thread is basically a succession of events belonging to the same story. It helps the reader to contextualize the information contained in a single article, by navigating backward or forward in the thread from this article. A specific effort is also made on the detection of reactions to a particular event. In order to build these event threads, we use a cascade of classifiers and other modules, taking advantage of the redundancy of information in the newswire corpus. We also share interesting comments concerning our manual annotation procedure for building a training and testing set1.</p><p>4 0.85282463 <a title="25-lda-4" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>Author: Lei Cui ; Xilun Chen ; Dongdong Zhang ; Shujie Liu ; Mu Li ; Ming Zhou</p><p>Abstract: Domain adaptation for SMT usually adapts models to an individual specific domain. However, it often lacks some correlation among different domains where common knowledge could be shared to improve the overall translation quality. In this paper, we propose a novel multi-domain adaptation approach for SMT using Multi-Task Learning (MTL), with in-domain models tailored for each specific domain and a general-domain model shared by different domains. The parameters of these models are tuned jointly via MTL so that they can learn general knowledge more accurately and exploit domain knowledge better. Our experiments on a largescale English-to-Chinese translation task validate that the MTL-based adaptation approach significantly and consistently improves the translation quality compared to a non-adapted baseline. Furthermore, it also outperforms the individual adaptation of each specific domain.</p><p>5 0.60443372 <a title="25-lda-5" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>Author: Zhiyuan Chen ; Arjun Mukherjee ; Bing Liu ; Meichun Hsu ; Malu Castellanos ; Riddhiman Ghosh</p><p>Abstract: Aspect extraction is one of the key tasks in sentiment analysis. In recent years, statistical models have been used for the task. However, such models without any domain knowledge often produce aspects that are not interpretable in applications. To tackle the issue, some knowledge-based topic models have been proposed, which allow the user to input some prior domain knowledge to generate coherent aspects. However, existing knowledge-based topic models have several major shortcomings, e.g., little work has been done to incorporate the cannot-link type of knowledge or to automatically adjust the number of topics based on domain knowledge. This paper proposes a more advanced topic model, called MC-LDA (LDA with m-set and c-set), to address these problems, which is based on an Extended generalized Pólya urn (E-GPU) model (which is also proposed in this paper). Experiments on real-life product reviews from a variety of domains show that MCLDA outperforms the existing state-of-the-art models markedly.</p><p>6 0.59325892 <a title="25-lda-6" href="./emnlp-2013-Automatic_Domain_Partitioning_for_Multi-Domain_Learning.html">29 emnlp-2013-Automatic Domain Partitioning for Multi-Domain Learning</a></p>
<p>7 0.58813882 <a title="25-lda-7" href="./emnlp-2013-Learning_Biological_Processes_with_Global_Constraints.html">118 emnlp-2013-Learning Biological Processes with Global Constraints</a></p>
<p>8 0.54307479 <a title="25-lda-8" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>9 0.53220594 <a title="25-lda-9" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>10 0.52371168 <a title="25-lda-10" href="./emnlp-2013-An_Empirical_Study_Of_Semi-Supervised_Chinese_Word_Segmentation_Using_Co-Training.html">21 emnlp-2013-An Empirical Study Of Semi-Supervised Chinese Word Segmentation Using Co-Training</a></p>
<p>11 0.52333248 <a title="25-lda-11" href="./emnlp-2013-Unsupervised_Induction_of_Contingent_Event_Pairs_from_Film_Scenes.html">192 emnlp-2013-Unsupervised Induction of Contingent Event Pairs from Film Scenes</a></p>
<p>12 0.52181864 <a title="25-lda-12" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>13 0.50483161 <a title="25-lda-13" href="./emnlp-2013-Flexible_and_Efficient_Hypergraph_Interactions_for_Joint_Hierarchical_and_Forest-to-String_Decoding.html">88 emnlp-2013-Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest-to-String Decoding</a></p>
<p>14 0.50170308 <a title="25-lda-14" href="./emnlp-2013-Lexical_Chain_Based_Cohesion_Models_for_Document-Level_Statistical_Machine_Translation.html">125 emnlp-2013-Lexical Chain Based Cohesion Models for Document-Level Statistical Machine Translation</a></p>
<p>15 0.49432239 <a title="25-lda-15" href="./emnlp-2013-Semi-Supervised_Feature_Transformation_for_Dependency_Parsing.html">168 emnlp-2013-Semi-Supervised Feature Transformation for Dependency Parsing</a></p>
<p>16 0.49154887 <a title="25-lda-16" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>17 0.49045312 <a title="25-lda-17" href="./emnlp-2013-Generating_Coherent_Event_Schemas_at_Scale.html">90 emnlp-2013-Generating Coherent Event Schemas at Scale</a></p>
<p>18 0.4892714 <a title="25-lda-18" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>19 0.48533744 <a title="25-lda-19" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>20 0.48091552 <a title="25-lda-20" href="./emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM.html">99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
