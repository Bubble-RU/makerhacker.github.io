<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-175" href="#">emnlp2013-175</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</h1>
<br/><p>Source: <a title="emnlp-2013-175-pdf" href="http://aclweb.org/anthology//D/D13/D13-1049.pdf">pdf</a></p><p>Author: Uri Lerner ; Slav Petrov</p><p>Abstract: We present a simple and novel classifier-based preordering approach. Unlike existing preordering models, we train feature-rich discriminative classifiers that directly predict the target-side word order. Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discriminative model with a rich set of features, including lexical features. We present extensive experiments on 22 language pairs, including preordering into English from 7 other languages. We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task. For languages from different families the improvements often exceed 2 BLEU. Many of these gains are also significant in human evaluations.</p><p>Reference: <a title="emnlp-2013-175-reference" href="../emnlp2013_reference/emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We present a simple and novel classifier-based preordering approach. [sent-3, score-0.611]
</p><p>2 Unlike existing preordering models, we train feature-rich discriminative classifiers that directly predict the target-side word order. [sent-4, score-0.777]
</p><p>3 Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discriminative model with a rich set of features, including lexical features. [sent-5, score-1.124]
</p><p>4 We present extensive experiments on 22 language pairs, including preordering into English from 7 other languages. [sent-6, score-0.611]
</p><p>5 For languages from different families the improvements often exceed 2 BLEU. [sent-9, score-0.192]
</p><p>6 Lexical reordering approaches (Tillmann, 2004; Zens and Ney, 2006) add a reordering component to standard phrase-based translation systems (Och and Ney, 2004). [sent-13, score-0.84]
</p><p>7 Because the reordering model is trained discriminatively, it can use a rich set of lexical features. [sent-14, score-0.408]
</p><p>8 However, it only has access to the local context which often times is insufficient to make the long-distance reordering decisions that are necessary for language pairs with significantly different word order. [sent-15, score-0.434]
</p><p>9 Because preordering is performed prior to word alignment, it can improve the alignment process and can then be combined with any subsequent translation model. [sent-20, score-0.76]
</p><p>10 Most preordering models use a source-side syntactic parser and perform a series of tree transformations. [sent-21, score-0.712]
</p><p>11 The reordering operation is then to sort the words according to their assigned values. [sent-37, score-0.371]
</p><p>12 In this work we present a simple classifier-based preordering model. [sent-44, score-0.611]
</p><p>13 Our model operates over dependency parse trees and is therefore able to perform long-distance reordering decisions, as is typical for preordering models. [sent-45, score-1.072]
</p><p>14 But instead of deterministic rules or ranking functions, we use discriminative classifiers to directly predict the final word  order, using rich (bi-)lexical and syntactic features. [sent-46, score-0.213]
</p><p>15 The first model uses a classifier to directly predict the permutation order in which a family of words (a head word and all its children) will appear on the target side. [sent-48, score-0.455]
</p><p>16 In the first step, for each child word a binary classifier decides whether it appears before or after its parent in the target language. [sent-54, score-0.228]
</p><p>17 We present experiments on 22 language pairs from different language families using our preordering approach in a phrase-based system (Och and Ney, 2004), as well as a forest-to-string system (Zhang et al. [sent-57, score-0.654]
</p><p>18 In a second set of experiments, we use automatically mined parallel data from the web and build translation systems for languages from various language families. [sent-63, score-0.218]
</p><p>19 Finally, we compare training the preordering classifiers on small amounts of manually aligned data to training on large quantities of automatically aligned data for English to Arabic, Hebrew, and Japanese. [sent-67, score-0.966]
</p><p>20 When evaluated on a pure reordering task, the models trained on manually aligned data perform slightly  better, but similar BLEU scores are obtained in both scenarios on an end-to-end translation task. [sent-68, score-0.616]
</p><p>21 For example, when translating the English sentence: The black cat climbed to the tree top. [sent-70, score-0.383]
</p><p>22 to Spanish, we would like to reorder it as: The cat black climbed to the top tree. [sent-71, score-0.274]
</p><p>23 When translating to Japanese, we would like to get: The black cat the tree top to climbed. [sent-72, score-0.235]
</p><p>24 For each head  word we determine the order of the head and its children (independently of other decisions) and continue the traversal recursively in that order. [sent-76, score-0.443]
</p><p>25 In the example, we first need to decide on the order of the head “climbed” and the children “cat”, “to ”, and “. [sent-77, score-0.296]
</p><p>26 1 Classification Model & Features The reordering decisions are made by multi-class classifiers where class labels correspond to permutation sequences. [sent-80, score-0.622]
</p><p>27 Crucially, we do not learn explicit tree transformations rules, but let the classifiers learn to trade off between a rich set of overlapping features. [sent-82, score-0.19]
</p><p>28 2 Training Data The training data for the classifiers is generated from the word aligned parallel text. [sent-101, score-0.243]
</p><p>29 For every family in the source dependency tree we generate a training instance if and only if the intersection defines a full order on the source words: • Every source word must be aligned to at least one target rwceor wd. [sent-104, score-0.294]
</p><p>30 o 515  d e ta m o dn s u b jROOTp r e p p o bd j e tn c  The black cat climbed to the tree top . [sent-105, score-0.336]
</p><p>31 • If a source word is aligned to multiple target words, uthrceen no target lwigornde din t tohi ms range can gbeet aligned to a different source word. [sent-110, score-0.292]
</p><p>32 In particular, we do not need to extract training instances for all words in a given sentence since the reordering decisions are made independently for every head word. [sent-112, score-0.581]
</p><p>33 Russian or Japanese) the determiner “the ” may either not be aligned to any word or get aligned to the foreign word for “boy”. [sent-117, score-0.253]
</p><p>34 First, there are instances where the En•  glish word “the ” gets aligned to something (perhaps a preposition), and second, since the word “the ” is omitted in the target language its location in the reordered sentence is not very important. [sent-121, score-0.239]
</p><p>35 The other direction is also true if we run preordering on the source side then the alignment task becomes easier and tends to produce better results. [sent-123, score-0.693]
</p><p>36 Therefore it can be useful to iterate between generating the alignment and learning a preordering model. [sent-124, score-0.662]
</p><p>37 , create the alignment, train a preordering model, use the preordering model to learn a new alignment, and then train the final preordering model. [sent-127, score-1.833]
</p><p>38 3 1-Step Classifier As a first approach we use a single classifier to directly predict the correct permutation of a given family. [sent-129, score-0.206]
</p><p>39 A possible outcome of the classifier can be the  permutation 0-2-1-3, representing the order “cat”, “to ”, “climbed”, and “. [sent-133, score-0.169]
</p><p>40 The number of permutations for the head and n children is of course (n + 1)! [sent-135, score-0.337]
</p><p>41 The decision depends on the adjective itself and sometimes the head noun, but does not depend on other children. [sent-142, score-0.197]
</p><p>42 Ideally, if for some adjective we have enough examples with 1 or 2 children we would like to make the same decision for a larger number of children, but these classifiers may not have enough relevant examples. [sent-143, score-0.297]
</p><p>43 4  2-Step Classifier  Our 2-step approach addresses the exponential blowup of the number of children by decomposing the prediction into two steps: 1. [sent-145, score-0.183]
</p><p>44 Determine the order of the children that appear before the head and the order of the children after the head. [sent-148, score-0.478]
</p><p>45 The two steps make the reordering of the modifiers before and after the head independent of each other, which is reminiscent of the lexicalized parse tree 516 generation approach of Collins (1997). [sent-149, score-0.651]
</p><p>46 In the running example, for the head “climbed” we might first make the following three binary decisions: the word “cat” should appear before the head and the words “to” and “. [sent-150, score-0.327]
</p><p>47 The first step is implemented using a binary classifier, called the pivot classifier (since the head functions like the pivot in quicksort). [sent-154, score-0.328]
</p><p>48 The sec-  ond step classifiers directly predict the correct permutation of the children before / after the head. [sent-155, score-0.374]
</p><p>49 = 24 outcomes (if all the children are on one side of the head); if we are lucky and the children split evenly, then we only need two binary decisions in the second step (for the two pairs before and after the head). [sent-160, score-0.392]
</p><p>50 In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006). [sent-166, score-0.405]
</p><p>51 We then add our 1-step and 2-step preordering classifiers as preprocessing steps at training and test time. [sent-171, score-0.709]
</p><p>52 We train the reordering classifiers on up to 15M training instances. [sent-172, score-0.469]
</p><p>53 In our implementation, in the 1-step approach we did not do any reordering for nodes with 7 or more children. [sent-174, score-0.371]
</p><p>54 In the 2-step approach we did not reorder the children on either side of the head if there were 7 or more of them. [sent-175, score-0.327]
</p><p>55 There were very few cases where children were not reordered because of these thresholds, many of them corresponded to bad parses, and they had very little impact on the final scores. [sent-177, score-0.213]
</p><p>56 Thus, for the 1-step ap–  proach we had 6 classifiers: 1 binary classifier for a head and a single child and 5 multi-class classifiers for 3–7 words. [sent-178, score-0.437]
</p><p>57 For a direct comparison to a strong preordering system, we compare to the system of Genzel (2010), which learns a set of unlexicalized reordering rules from automatically aligned data by minimizing the number of crossing alignments. [sent-180, score-1.139]
</p><p>58 There are no length-based reordering constraints in the forest-to-string system. [sent-197, score-0.371]
</p><p>59 2  Additional Languages  In our second set of experiments, we explore the impact of classifier preordering for a number of languages with different word orders. [sent-203, score-0.775]
</p><p>60 Some of the languages included in our study are verb-subject-object (VSO) languages (Arabic, Irish, Welsh), subjectobject-verb (SOV) languages (Japanese, Korean), and fairly free word order languages (Dutch, Hungarian). [sent-204, score-0.34]
</p><p>61 Lexical reordering is  included where it helps, but typically makes only a small difference. [sent-213, score-0.371]
</p><p>62 This potentially underestimates the improvements that can be obtained, but also eliminates MERT as a possible source of improvement, allowing us to trace back improvements in translation quality directly to changes in preordering of the input data. [sent-217, score-0.837]
</p><p>63 Lexical reordering (Zens and Ney, 2006) never hurts and is thus included in all systems. [sent-253, score-0.371]
</p><p>64 The 2-step classifier preordering approach provides statistically significant improvements over the lexical reordering baseline on three out of the eight language pairs: English-Spanish (en-es: 1. [sent-255, score-1.162]
</p><p>65 While the forest-to-string system is capable ofperforming long distance reordering in the decoder, it appears that an explicitly trained lexicalized preordering model can provide complementary  benefits. [sent-261, score-1.016]
</p><p>66 For the romance languages (Spanish and French), word ordering depends highly on lexical choice which is captured by the lexical features in our classifiers. [sent-263, score-0.196]
</p><p>67 The base system includes a distance distortion model; the lexical system adds lexical reordering; rule is the rule preordering system of Genzel (2010) plus lexical reordering; 1-step and 2-step are our classifier-based systems plus lexical reordering. [sent-265, score-0.831]
</p><p>68 Compared to a state-of-the-art preordering system, the automatic rule extraction system of Genzel (2010), we observe significant gains in several  cases and no losses at all. [sent-269, score-0.729]
</p><p>69 Comparing the different languages, Czech (cs) appears the most immune to improvements from preordering (and lexical reordering). [sent-271, score-0.712]
</p><p>70 It is therefore difficult to learn reordering changes from English to Czech. [sent-273, score-0.371]
</p><p>71 The SOV  languages Korean (ko) and Japanese (ja) benefit the 519 most from preordering and gain more than 7 BLEU relative to the phrase-based baseline and still more than 3 BLEU for the forest-to-string system. [sent-279, score-0.696]
</p><p>72 The benefits of our 2-step approach over the 1step approach become apparent on this set of languages where reordering is most important. [sent-289, score-0.456]
</p><p>73 Lexical reordering is not included in any of the systems. [sent-296, score-0.371]
</p><p>74 The gains relative to the rule reordering system of Genzel (2010) and the no-preordering baseline are even larger and therefore clearly also significant. [sent-301, score-0.489]
</p><p>75 In all cases but English-Hungarian we observe significant improvements over the no preordering baseline. [sent-303, score-0.675]
</p><p>76 It should be noted that the gains are not symmetric sometimes there are larger gains for translating out of English, while for Hungarian the gains are higher for translating into English. [sent-304, score-0.34]
</p><p>77 For Dutch-English, the forest-tostring system yields the best results, which was also the case for German-English, further supporting the observation that combining different types of syntactic reordering approaches can be beneficial. [sent-306, score-0.371]
</p><p>78 Lexical reordering is not used for any language pair. [sent-312, score-0.371]
</p><p>79 The BLEU scores in Table 5 show that training from small amounts of manually aligned data or large amounts of automatically aligned data results in models of similar quality. [sent-316, score-0.257]
</p><p>80 In absolute terms, the reordering accuracy is around 80% for Arabic and Japanese and close to 90% for Hebrew. [sent-319, score-0.371]
</p><p>81 We also examined the accuracy of the individual classifiers and found that the pivot classifier has an accuracy around 95%. [sent-321, score-0.228]
</p><p>82 It is therefore unlikely that a word is reordered to the wrong side of its head in the 2-step reordering approach. [sent-322, score-0.613]
</p><p>83 5  Analysis  In this section, we analyze an example whose translation is significantly improved by our preordering approach, demonstrating the usefulness of our lexicalized features. [sent-324, score-0.743]
</p><p>84 In our experiments the rule-based approach of (Genzel, 2010) reordered the source sentence into: It was a whirlwind real. [sent-340, score-0.175]
</p><p>85 The head “whirlwind” is a noun and the child “real” is an adjective; since adjectives typically appear after nouns in Spanish, their order is reversed. [sent-345, score-0.332]
</p><p>86 In Table 6 we consider the 3 strongest features in favor of the child “real” appearing after the head “whirlwind” and the three strongest features in favor of the child appearing before the head. [sent-347, score-0.373]
</p><p>87 Recall that the pivot is a binary classifier: positive features support one decision (in our case: the child should be after the head) and the negative features support the other decision (the child should be before the head). [sent-348, score-0.277]
</p><p>88 It is interesting to note that for this particular ordering decision the child word is much more informative than the head word and indeed, all the important features contain information about the child and none of them contains any information about the head. [sent-351, score-0.41]
</p><p>89 6  Conclusions & Future Work  We presented a simple and novel preordering approach that produces substantial improvements in translation accuracy on a large number of languages. [sent-352, score-0.773]
</p><p>90 We use a source-side syntactic parser and train discriminative classifiers to predict the order of a parent and its children in the target language, using features from the dependency tree as well as (bi-)lexical features. [sent-353, score-0.505]
</p><p>91 5 BLEU over a strong directly comparable preordering system that is based on learning unlexicalized reordering rules. [sent-359, score-0.982]
</p><p>92 , 2006; Dyer and Resnik, 2010) we note that both approaches use syntactic information for reordering decisions. [sent-365, score-0.371]
</p><p>93 cause preordering is performed before learning word alignments, it has the potential to improve the word alignments. [sent-371, score-0.611]
</p><p>94 Finally, preordering can be combined with syntaxbased translation models and our results confirm the complementary benefits that can be obtained. [sent-373, score-0.709]
</p><p>95 Compared to other preordering models, our approach has the obvious problem of having to make predictions over an exponential set of permutations. [sent-374, score-0.645]
</p><p>96 Compared to preordering systems that use ranking functions, our model has the advantage that it can encode information about the complete permutation. [sent-383, score-0.611]
</p><p>97 NoNextSibling and NoNextHeadSibling mean that the child and head do not have a sibling to the right. [sent-396, score-0.295]
</p><p>98 Promising directions for future work are joint parsing and reordering models, and measuring the influence of parsing accuracy on preordering and final translation quality. [sent-398, score-1.08]
</p><p>99 Automatically learning source-side reordering rules for large scale machine translation. [sent-511, score-0.418]
</p><p>100 A rankingbased approach to word reordering for statistical machine translation. [sent-752, score-0.401]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('preordering', 0.611), ('reordering', 0.371), ('children', 0.149), ('climbed', 0.148), ('head', 0.147), ('bleu', 0.139), ('prevchild', 0.129), ('wmt', 0.119), ('child', 0.113), ('whirlwind', 0.111), ('aligned', 0.11), ('translation', 0.098), ('classifiers', 0.098), ('genzel', 0.096), ('cat', 0.093), ('permutation', 0.09), ('languages', 0.085), ('japanese', 0.084), ('gains', 0.082), ('classifier', 0.079), ('hungarian', 0.077), ('korean', 0.07), ('reordered', 0.064), ('malay', 0.064), ('improvements', 0.064), ('decisions', 0.063), ('tree', 0.062), ('arabic', 0.057), ('hebrew', 0.056), ('sov', 0.055), ('welsh', 0.055), ('dependency', 0.053), ('treebank', 0.052), ('alignment', 0.051), ('pivot', 0.051), ('petrov', 0.051), ('adjective', 0.05), ('det', 0.049), ('nivre', 0.048), ('talbot', 0.048), ('irish', 0.048), ('mccord', 0.048), ('ja', 0.048), ('rewrite', 0.048), ('ney', 0.047), ('translating', 0.047), ('rules', 0.047), ('abeill', 0.044), ('xia', 0.043), ('families', 0.043), ('dutch', 0.043), ('indonesian', 0.041), ('permutations', 0.041), ('bolded', 0.041), ('portuguese', 0.041), ('iw', 0.041), ('spanish', 0.04), ('parser', 0.039), ('noun', 0.039), ('uszkoreit', 0.039), ('nn', 0.038), ('manually', 0.037), ('predict', 0.037), ('della', 0.037), ('pietra', 0.037), ('prevsibling', 0.037), ('vso', 0.037), ('lexical', 0.037), ('ordering', 0.037), ('parse', 0.037), ('target', 0.036), ('rule', 0.036), ('shared', 0.036), ('parallel', 0.035), ('zens', 0.035), ('sibling', 0.035), ('english', 0.034), ('exponential', 0.034), ('lexicalized', 0.034), ('czech', 0.034), ('treebanks', 0.034), ('black', 0.033), ('family', 0.033), ('appear', 0.033), ('determiner', 0.033), ('real', 0.032), ('heuristics', 0.032), ('raising', 0.032), ('boy', 0.032), ('uri', 0.032), ('och', 0.032), ('discriminative', 0.031), ('side', 0.031), ('zhang', 0.031), ('statistical', 0.03), ('transformations', 0.03), ('neubig', 0.029), ('tromble', 0.029), ('glish', 0.029), ('jj', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="175-tfidf-1" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>Author: Uri Lerner ; Slav Petrov</p><p>Abstract: We present a simple and novel classifier-based preordering approach. Unlike existing preordering models, we train feature-rich discriminative classifiers that directly predict the target-side word order. Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discriminative model with a rich set of features, including lexical features. We present extensive experiments on 22 language pairs, including preordering into English from 7 other languages. We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task. For languages from different families the improvements often exceed 2 BLEU. Many of these gains are also significant in human evaluations.</p><p>2 0.30315334 <a title="175-tfidf-2" href="./emnlp-2013-Factored_Soft_Source_Syntactic_Constraints_for_Hierarchical_Machine_Translation.html">84 emnlp-2013-Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation</a></p>
<p>Author: Zhongqiang Huang ; Jacob Devlin ; Rabih Zbib</p><p>Abstract: Translation Jacob Devlin Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA j devl in@bbn . com Rabih Zbib Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA r zbib@bbn . com have tried to introduce grammaticality to the transThis paper describes a factored approach to incorporating soft source syntactic constraints into a hierarchical phrase-based translation system. In contrast to traditional approaches that directly introduce syntactic constraints to translation rules by explicitly decorating them with syntactic annotations, which often exacerbate the data sparsity problem and cause other problems, our approach keeps translation rules intact and factorizes the use of syntactic constraints through two separate models: 1) a syntax mismatch model that associates each nonterminal of a translation rule with a distribution of tags that is used to measure the degree of syntactic compatibility of the translation rule on source spans; 2) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language. The features produced by both models are used as soft constraints to guide the translation process. Experiments on Chinese-English translation show that the proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets.</p><p>3 0.24011338 <a title="175-tfidf-3" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>Author: Peng Li ; Yang Liu ; Maosong Sun</p><p>Abstract: While inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (i.e., straight and inverted) dependent on actual blocks being merged remains a challenge. Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points.</p><p>4 0.19940871 <a title="175-tfidf-4" href="./emnlp-2013-Shift-Reduce_Word_Reordering_for_Machine_Translation.html">171 emnlp-2013-Shift-Reduce Word Reordering for Machine Translation</a></p>
<p>Author: Katsuhiko Hayashi ; Katsuhito Sudoh ; Hajime Tsukada ; Jun Suzuki ; Masaaki Nagata</p><p>Abstract: This paper presents a novel word reordering model that employs a shift-reduce parser for inversion transduction grammars. Our model uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system.</p><p>5 0.18251355 <a title="175-tfidf-5" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>Author: Joern Wuebker ; Stephan Peitz ; Felix Rietig ; Hermann Ney</p><p>Abstract: Automatically clustering words from a monolingual or bilingual training corpus into classes is a widely used technique in statistical natural language processing. We present a very simple and easy to implement method for using these word classes to improve translation quality. It can be applied across different machine translation paradigms and with arbitrary types of models. We show its efficacy on a small German→English and a larger F ornenc ah s→mGalelrm Gaenrm mtarann→slEatniognli tsahsk a nwdit ha lbaortghe rst Farnednacrhd→ phrase-based salandti nhie traaskrch wiciathl phrase-based translation systems for a common set of models. Our results show that with word class models, the baseline can be improved by up to 1.4% BLEU and 1.0% TER on the French→German task and 0.3% BLEU aonnd t h1e .1 F%re nTcEhR→ on tehrem German→English Btask.</p><p>6 0.16147508 <a title="175-tfidf-6" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>7 0.14231043 <a title="175-tfidf-7" href="./emnlp-2013-Efficient_Left-to-Right_Hierarchical_Phrase-Based_Translation_with_Improved_Reordering.html">71 emnlp-2013-Efficient Left-to-Right Hierarchical Phrase-Based Translation with Improved Reordering</a></p>
<p>8 0.12017861 <a title="175-tfidf-8" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>9 0.10928002 <a title="175-tfidf-9" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>10 0.1085435 <a title="175-tfidf-10" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>11 0.10743923 <a title="175-tfidf-11" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>12 0.10295743 <a title="175-tfidf-12" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>13 0.10083625 <a title="175-tfidf-13" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>14 0.10078106 <a title="175-tfidf-14" href="./emnlp-2013-The_Effects_of_Syntactic_Features_in_Automatic_Prediction_of_Morphology.html">181 emnlp-2013-The Effects of Syntactic Features in Automatic Prediction of Morphology</a></p>
<p>15 0.092465758 <a title="175-tfidf-15" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>16 0.086931318 <a title="175-tfidf-16" href="./emnlp-2013-Dynamic_Feature_Selection_for_Dependency_Parsing.html">66 emnlp-2013-Dynamic Feature Selection for Dependency Parsing</a></p>
<p>17 0.086532101 <a title="175-tfidf-17" href="./emnlp-2013-Efficient_Higher-Order_CRFs_for_Morphological_Tagging.html">70 emnlp-2013-Efficient Higher-Order CRFs for Morphological Tagging</a></p>
<p>18 0.086317509 <a title="175-tfidf-18" href="./emnlp-2013-A_Corpus_Level_MIRA_Tuning_Strategy_for_Machine_Translation.html">3 emnlp-2013-A Corpus Level MIRA Tuning Strategy for Machine Translation</a></p>
<p>19 0.076399453 <a title="175-tfidf-19" href="./emnlp-2013-Translating_into_Morphologically_Rich_Languages_with_Synthetic_Phrases.html">186 emnlp-2013-Translating into Morphologically Rich Languages with Synthetic Phrases</a></p>
<p>20 0.075022385 <a title="175-tfidf-20" href="./emnlp-2013-What_is_Hidden_among_Translation_Rules.html">201 emnlp-2013-What is Hidden among Translation Rules</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.269), (1, -0.28), (2, 0.106), (3, 0.021), (4, 0.039), (5, -0.06), (6, -0.078), (7, -0.107), (8, 0.027), (9, 0.039), (10, -0.009), (11, 0.092), (12, -0.053), (13, -0.082), (14, -0.157), (15, 0.21), (16, -0.077), (17, -0.109), (18, -0.061), (19, -0.05), (20, 0.01), (21, 0.013), (22, 0.098), (23, 0.144), (24, 0.131), (25, 0.022), (26, -0.125), (27, 0.027), (28, -0.038), (29, 0.057), (30, 0.016), (31, 0.131), (32, -0.004), (33, 0.02), (34, -0.059), (35, 0.063), (36, -0.06), (37, -0.021), (38, 0.058), (39, 0.054), (40, -0.003), (41, 0.047), (42, -0.046), (43, 0.041), (44, 0.086), (45, 0.014), (46, -0.045), (47, 0.047), (48, -0.041), (49, 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93082774 <a title="175-lsi-1" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>Author: Uri Lerner ; Slav Petrov</p><p>Abstract: We present a simple and novel classifier-based preordering approach. Unlike existing preordering models, we train feature-rich discriminative classifiers that directly predict the target-side word order. Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discriminative model with a rich set of features, including lexical features. We present extensive experiments on 22 language pairs, including preordering into English from 7 other languages. We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task. For languages from different families the improvements often exceed 2 BLEU. Many of these gains are also significant in human evaluations.</p><p>2 0.82969564 <a title="175-lsi-2" href="./emnlp-2013-Shift-Reduce_Word_Reordering_for_Machine_Translation.html">171 emnlp-2013-Shift-Reduce Word Reordering for Machine Translation</a></p>
<p>Author: Katsuhiko Hayashi ; Katsuhito Sudoh ; Hajime Tsukada ; Jun Suzuki ; Masaaki Nagata</p><p>Abstract: This paper presents a novel word reordering model that employs a shift-reduce parser for inversion transduction grammars. Our model uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system.</p><p>3 0.77220035 <a title="175-lsi-3" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>Author: Peng Li ; Yang Liu ; Maosong Sun</p><p>Abstract: While inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (i.e., straight and inverted) dependent on actual blocks being merged remains a challenge. Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points.</p><p>4 0.71942925 <a title="175-lsi-4" href="./emnlp-2013-Factored_Soft_Source_Syntactic_Constraints_for_Hierarchical_Machine_Translation.html">84 emnlp-2013-Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation</a></p>
<p>Author: Zhongqiang Huang ; Jacob Devlin ; Rabih Zbib</p><p>Abstract: Translation Jacob Devlin Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA j devl in@bbn . com Rabih Zbib Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA r zbib@bbn . com have tried to introduce grammaticality to the transThis paper describes a factored approach to incorporating soft source syntactic constraints into a hierarchical phrase-based translation system. In contrast to traditional approaches that directly introduce syntactic constraints to translation rules by explicitly decorating them with syntactic annotations, which often exacerbate the data sparsity problem and cause other problems, our approach keeps translation rules intact and factorizes the use of syntactic constraints through two separate models: 1) a syntax mismatch model that associates each nonterminal of a translation rule with a distribution of tags that is used to measure the degree of syntactic compatibility of the translation rule on source spans; 2) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language. The features produced by both models are used as soft constraints to guide the translation process. Experiments on Chinese-English translation show that the proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets.</p><p>5 0.69115639 <a title="175-lsi-5" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>Author: Hendra Setiawan ; Bowen Zhou ; Bing Xiang</p><p>Abstract: Reordering poses one of the greatest challenges in Statistical Machine Translation research as the key contextual information may well be beyond the confine oftranslation units. We present the “Anchor Graph” (AG) model where we use a graph structure to model global contextual information that is crucial for reordering. The key ingredient of our AG model is the edges that capture the relationship between the reordering around a set of selected translation units, which we refer to as anchors. As the edges link anchors that may span multiple translation units at decoding time, our AG model effectively encodes global contextual information that is previously absent. We integrate our proposed model into a state-of-the-art translation system and demonstrate the efficacy of our proposal in a largescale Chinese-to-English translation task.</p><p>6 0.62281972 <a title="175-lsi-6" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>7 0.54890347 <a title="175-lsi-7" href="./emnlp-2013-Efficient_Left-to-Right_Hierarchical_Phrase-Based_Translation_with_Improved_Reordering.html">71 emnlp-2013-Efficient Left-to-Right Hierarchical Phrase-Based Translation with Improved Reordering</a></p>
<p>8 0.47415179 <a title="175-lsi-8" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>9 0.42942128 <a title="175-lsi-9" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>10 0.42112041 <a title="175-lsi-10" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>11 0.4107556 <a title="175-lsi-11" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>12 0.40722746 <a title="175-lsi-12" href="./emnlp-2013-The_Effects_of_Syntactic_Features_in_Automatic_Prediction_of_Morphology.html">181 emnlp-2013-The Effects of Syntactic Features in Automatic Prediction of Morphology</a></p>
<p>13 0.4050734 <a title="175-lsi-13" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>14 0.40475136 <a title="175-lsi-14" href="./emnlp-2013-Joint_Parsing_and_Disfluency_Detection_in_Linear_Time.html">116 emnlp-2013-Joint Parsing and Disfluency Detection in Linear Time</a></p>
<p>15 0.390816 <a title="175-lsi-15" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>16 0.37470984 <a title="175-lsi-16" href="./emnlp-2013-With_Blinkers_on%3A_Robust_Prediction_of_Eye_Movements_across_Readers.html">203 emnlp-2013-With Blinkers on: Robust Prediction of Eye Movements across Readers</a></p>
<p>17 0.36870518 <a title="175-lsi-17" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>18 0.36734685 <a title="175-lsi-18" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>19 0.36412665 <a title="175-lsi-19" href="./emnlp-2013-Ubertagging%3A_Joint_Segmentation_and_Supertagging_for_English.html">190 emnlp-2013-Ubertagging: Joint Segmentation and Supertagging for English</a></p>
<p>20 0.36031693 <a title="175-lsi-20" href="./emnlp-2013-A_Corpus_Level_MIRA_Tuning_Strategy_for_Machine_Translation.html">3 emnlp-2013-A Corpus Level MIRA Tuning Strategy for Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.034), (10, 0.011), (18, 0.031), (22, 0.047), (26, 0.011), (30, 0.124), (45, 0.023), (50, 0.04), (51, 0.156), (64, 0.213), (66, 0.038), (71, 0.022), (75, 0.032), (77, 0.094), (90, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81234866 <a title="175-lda-1" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>Author: Uri Lerner ; Slav Petrov</p><p>Abstract: We present a simple and novel classifier-based preordering approach. Unlike existing preordering models, we train feature-rich discriminative classifiers that directly predict the target-side word order. Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discriminative model with a rich set of features, including lexical features. We present extensive experiments on 22 language pairs, including preordering into English from 7 other languages. We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task. For languages from different families the improvements often exceed 2 BLEU. Many of these gains are also significant in human evaluations.</p><p>2 0.80968714 <a title="175-lda-2" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>Author: Angeliki Lazaridou ; Eva Maria Vecchi ; Marco Baroni</p><p>Abstract: In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the correct parsing of NPs (e.g., (live fish) transporter rather than live (fish transporter)). We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features.</p><p>3 0.80162537 <a title="175-lda-3" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>Author: Xiaoning Zhu ; Zhongjun He ; Hua Wu ; Haifeng Wang ; Conghui Zhu ; Tiejun Zhao</p><p>Abstract: This paper proposes a novel approach that utilizes a machine learning method to improve pivot-based statistical machine translation (SMT). For language pairs with few bilingual data, a possible solution in pivot-based SMT using another language as a</p><p>4 0.70449412 <a title="175-lda-4" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>Author: Ann Irvine ; Chris Quirk ; Hal Daume III</p><p>Abstract: When using a machine translation (MT) model trained on OLD-domain parallel data to translate NEW-domain text, one major challenge is the large number of out-of-vocabulary (OOV) and new-translation-sense words. We present a method to identify new translations of both known and unknown source language words that uses NEW-domain comparable document pairs. Starting with a joint distribution of source-target word pairs derived from the OLD-domain parallel corpus, our method recovers a new joint distribution that matches the marginal distributions of the NEW-domain comparable document pairs, while minimizing the divergence from the OLD-domain distribution. Adding learned translations to our French-English MT model results in gains of about 2 BLEU points over strong baselines.</p><p>5 0.70075822 <a title="175-lda-5" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>Author: Fandong Meng ; Jun Xie ; Linfeng Song ; Yajuan Lu ; Qun Liu</p><p>Abstract: We present a novel translation model, which simultaneously exploits the constituency and dependency trees on the source side, to combine the advantages of two types of trees. We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. Our rules hold the property of long distance reorderings and the compatibility with phrases. Large-scale experimental results show that our model achieves significantly improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets.</p><p>6 0.70013767 <a title="175-lda-6" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>7 0.69990903 <a title="175-lda-7" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>8 0.69867545 <a title="175-lda-8" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>9 0.69786924 <a title="175-lda-9" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>10 0.69592696 <a title="175-lda-10" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>11 0.68964434 <a title="175-lda-11" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>12 0.68922853 <a title="175-lda-12" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>13 0.68637246 <a title="175-lda-13" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>14 0.68155968 <a title="175-lda-14" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>15 0.67324048 <a title="175-lda-15" href="./emnlp-2013-Dynamic_Feature_Selection_for_Dependency_Parsing.html">66 emnlp-2013-Dynamic Feature Selection for Dependency Parsing</a></p>
<p>16 0.67314714 <a title="175-lda-16" href="./emnlp-2013-Max-Violation_Perceptron_and_Forced_Decoding_for_Scalable_MT_Training.html">128 emnlp-2013-Max-Violation Perceptron and Forced Decoding for Scalable MT Training</a></p>
<p>17 0.67311811 <a title="175-lda-17" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>18 0.67284411 <a title="175-lda-18" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>19 0.67164338 <a title="175-lda-19" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>20 0.67015904 <a title="175-lda-20" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
