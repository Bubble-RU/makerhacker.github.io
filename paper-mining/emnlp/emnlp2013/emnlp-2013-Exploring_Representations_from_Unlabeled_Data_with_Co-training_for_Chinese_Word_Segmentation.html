<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-82" href="#">emnlp2013-82</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</h1>
<br/><p>Source: <a title="emnlp-2013-82-pdf" href="http://aclweb.org/anthology//D/D13/D13-1031.pdf">pdf</a></p><p>Author: Longkai Zhang ; Houfeng Wang ; Xu Sun ; Mairgup Mansur</p><p>Abstract: Nowadays supervised sequence labeling models can reach competitive performance on the task of Chinese word segmentation. However, the ability of these models is restricted by the availability of annotated data and the design of features. We propose a scalable semi-supervised feature engineering approach. In contrast to previous works using pre-defined taskspecific features with fixed values, we dynamically extract representations of label distributions from both an in-domain corpus and an out-of-domain corpus. We update the representation values with a semi-supervised approach. Experiments on the benchmark datasets show that our approach achieve good results and reach an f-score of 0.961. The feature engineering approach proposed here is a general iterative semi-supervised method and not limited to the word segmentation task.</p><p>Reference: <a title="emnlp-2013-82-reference" href="../emnlp2013_reference/emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn, Abstract Nowadays supervised sequence labeling models can reach competitive performance on the task of Chinese word segmentation. [sent-4, score-0.169]
</p><p>2 In contrast to previous works using pre-defined taskspecific features with fixed values, we dynamically extract representations of label distributions from both an in-domain corpus and an out-of-domain corpus. [sent-7, score-0.25]
</p><p>3 The feature engineering approach proposed here is a general  iterative semi-supervised method and not limited to the word segmentation task. [sent-11, score-0.211]
</p><p>4 Previous research shows that sequence labeling models trained on labeled data can reach competitive accuracy on the CWS task, and supervised models are more accurate than unsupervised models (Xue, 2003; Low et al. [sent-14, score-0.232]
</p><p>5 Traditional semi-supervised methods focus on adding new unlabeled instances to the training set by a given criterion. [sent-21, score-0.299]
</p><p>6 Previous works mainly assume that context features are helpful to decide the potential label of a character. [sent-24, score-0.221]
</p><p>7 hc o2d0s1 i3n A Nsastoucria lti Loan fgoura Cgoem Ppruotcaetsiosin agl, L piang eusis 3t1ic1s–321, is extracted from the unlabeled corpus as this representation to enhance the supervised model. [sent-33, score-0.33]
</p><p>8 We add “pseudo-labels” by tagging the unlabeled data with the trained model on the training corpus. [sent-34, score-0.299]
</p><p>9 To accurately calculate the precise label distribution, we use a framework similar to the cotraining algorithm to adjust the feature values iteratively. [sent-37, score-0.178]
</p><p>10 Generally speaking, unlabeled data  can be classified as in-domain data and out-ofdomain data. [sent-38, score-0.299]
</p><p>11 In previous works these two kinds of unlabeled data are used separately for different purposes. [sent-39, score-0.379]
</p><p>12 In our work, we use in-domain and out-of-domain data together to adjust the labels of the unlabeled corpus. [sent-42, score-0.346]
</p><p>13 The main contributions of our work are as follows: • We proposed a general method to utilize the label distribution given text contexts as representations in a semi-supervised framework. [sent-51, score-0.159]
</p><p>14 We let the co-training process adjust the representation values from label distribution instead of using manually pre312 defined feature templates. [sent-52, score-0.177]
</p><p>15 1 Sequence Labeling Nowadays the character-based sequence labeling approach is widely used for the Chinese word segmentation problem. [sent-60, score-0.34]
</p><p>16 It was first proposed in Xue (2003) , which assigns each character a label to indicate its position in the word. [sent-61, score-0.37]
</p><p>17 This tag set uses B, M, E and S to represent the Beginning, the Middle, the End of a word and a Single character forming a word respectively. [sent-63, score-0.282]
</p><p>18 In previous works, these two kinds of unlabeled data are used separately for different purposes. [sent-70, score-0.327]
</p><p>19 Our study shows that by correctly designing features and algorithms, both in-domain unlabeled data and outof-domain unlabeled data can work together to help enhancing the segmentation model. [sent-74, score-0.901]
</p><p>20 In our algorithm, the dynamic features learned from one corpus can be adjusted incrementally with the dynamic features learned from the other corpus. [sent-75, score-0.249]
</p><p>21 In our experiment, two different views of features on unlabeled data are considered: Static Statistical Features (SSFs) : These features capture statistical information of characters and character n-grams from the unlabeled corpus. [sent-79, score-1.04]
</p><p>22 The values of these features are fixed during the training process once the unlabeled corpus is given. [sent-80, score-0.405]
</p><p>23 Dynamic Statistical Features (DSFs) : These features capture label distribution information from the unlabeled corpus given fixed text contexts. [sent-81, score-0.535]
</p><p>24 As the training process proceeds, the value of these features will change, since the trained tagger at each training iteration may assign different labels to the unlabeled data. [sent-82, score-0.389]
</p><p>25 3 Framework Suppose we have labeled data L, two unlabeled corpora Ua and Ub (one is an in-domain corpus and the other is an out-of-domain corpus) . [sent-84, score-0.393]
</p><p>26 During each iteration, we tag the unlabeled corpus Ua using Tb to get pseudo-labels. [sent-86, score-0.367]
</p><p>27 We  use the label distribution information as dynamic features. [sent-88, score-0.198]
</p><p>28 The difference is that there are not different views of features, but different kinds of unlabeled data. [sent-93, score-0.327]
</p><p>29 To be convenient, for a character ci with context . [sent-101, score-0.609]
</p><p>30 2  Static statistical features  Statistical features are statistics that distilled from the large unlabeled corpus. [sent-111, score-0.422]
</p><p>31 They are proved useful in the Chinese word segmentation task. [sent-112, score-0.211]
</p><p>32 Previous works have already explored the functions of the three static statistics in the Chinese word segmentation task, e. [sent-115, score-0.398]
</p><p>33 In our study we find that, in Chinese, there remains large amount of imbalanced cases, like a string with length 1 followed by a string with length 2, and vice versa. [sent-126, score-0.17]
</p><p>34 The character after punctuations must be the first character of a word. [sent-129, score-0.573]
</p><p>35 The character before punctuations must be the last character of a word. [sent-130, score-0.573]
</p><p>36 The situation is similar when a string appears frequently preceding punctuations. [sent-132, score-0.158]
</p><p>37 For a string with length len and probability p in the corpus, we define the left punctuation rate LPRlen as the number of times the string appears after punctuations, di314  vided by p. [sent-135, score-0.351]
</p><p>38 Similarly, the right punctuation rate RPRlen is defines as the number of times it appears preceding punctuations divided by its probability p. [sent-136, score-0.267]
</p><p>39 If a string appears after or preceding many different characters, this may provide some information of the string itself. [sent-139, score-0.243]
</p><p>40 For a string s with length l, we define the left accessor variety (LAV) as the types of distinct characters preceding s in the corpus, and the right accessor variety (RAV) as the types of distinct characters after s in the corpus. [sent-143, score-0.664]
</p><p>41 3 Dynamic statistical features The unlabeled corpus lacks precise labels. [sent-147, score-0.412]
</p><p>42 We  can use the trained tagger to give the unlabeled data “pseudo-labels”. [sent-148, score-0.348]
</p><p>43 We define the whole character sequence with length n as X = (x1, x2 · · · xj · · · xn) . [sent-158, score-0.324]
</p><p>44 Given a text context Ci, where i ·is· xcu·r·re·nxt character position, the DSFs can be represented as a list, DSF(Ci) = (DSF(Ci)1, · · · , DSF(Ci)K) Each element in the list represents the proba-  bility of the corresponding label in the distribution. [sent-159, score-0.373]
</p><p>45 For convenience, we further define function ‘count(condition) ’ as the total number of times a ‘condition’ is true in the unlabeled corpus. [sent-160, score-0.363]
</p><p>46 For example, count (current=‘a’) represents the times the current character equals ‘a’, which is exactly the number of times character ‘a’ appears in the unlabeled corpus. [sent-161, score-0.958]
</p><p>47 , D(Ci)K) We define Basic DSF with current character position i, text context Ci and label l (the lth dimension in the list) as: D(Ci)l = P(y = l|Ci = xi)  =  count(Ci = xi ∧ y = l) count(Ci= xi)  In this equation, the numerator counts the number of times current character is xi with label l. [sent-166, score-1.166]
</p><p>48 The denominator counts the number of times current character is xi. [sent-167, score-0.39]
</p><p>49 We use the term “Basic” because this kind of DSFs only considers the character of position i as its context. [sent-168, score-0.278]
</p><p>50 The text context refers to the current character itself. [sent-169, score-0.313]
</p><p>51 This feature captures the label distribution information given the character itself. [sent-170, score-0.375]
</p><p>52 Although characters convey context information, characters themselves in Chinese is sometimes meaningless. [sent-174, score-0.184]
</p><p>53 , B(Ci)K)  315 We define Bigram DSF with current character position i, text context Ci and label l (the lth dimension in the list) as: B(Ci)l = P(y = l|Ci = xi−jxi−j+1) =  count(Ci= xi−jxi−j+1∧ y = l)  count(Ci = xi−jxi−j+1 ) j can take value 0 and 1. [sent-180, score-0.517]
</p><p>54 In this equation, the numerator counts the number of times current context is xi−jxi−j+1 with label l. [sent-181, score-0.276]
</p><p>55 The denominator counts the number of times current context is xi−jxi−j+1 . [sent-182, score-0.181]
</p><p>56 We call it Window DSF, which considers the surrounding context of a character and omits the character itself. [sent-185, score-0.526]
</p><p>57 The denominator counts the number of times current context is xi−1xi+1 . [sent-190, score-0.181]
</p><p>58 A more natural way is to use discrete values to incorporate them into the sequence labeling models . [sent-195, score-0.165]
</p><p>59 In our method, we process static and dynamic statistical features using dif-  ferent strategies. [sent-198, score-0.285]
</p><p>60 For static statistical value: For mutual information, we round the real value to their nearest integer. [sent-199, score-0.218]
</p><p>61 For punctuation rate and accessor variety, as the values tend to be large, we first get the log value of the feature and then use the nearest integer as the corresponding discrete value. [sent-200, score-0.259]
</p><p>62 For dynamic statistical value: Dynamic statistical features are distributions of a label. [sent-201, score-0.191]
</p><p>63 Using the labeled data alone can get a relatively good tagger and the unlabeled data contributes little to the performance. [sent-224, score-0.411]
</p><p>64 We also used two un-segmented corpora as unlabeled data. [sent-229, score-0.299]
</p><p>65 Details of unlabeled data can be found in table 5. [sent-236, score-0.299]
</p><p>66 2 Main Results Table 6 summarizes the segmentation results on test data with different feature combinations. [sent-243, score-0.211]
</p><p>67 o8t a×l 1 ch06aracter Table 4: Details of the PKU data  GCiograpwuorsd5C0h0a0r1a9c3ter used Baike5000147 Table 5: Details of the unlabeled data. [sent-261, score-0.299]
</p><p>68 The size of unlabeled data is fixed as 5 million characters. [sent-268, score-0.385]
</p><p>69 To compare the contribution of unlabeled data, we conduct experiments of using different sizes of unlabeled data. [sent-275, score-0.598]
</p><p>70 Note that the SSFs are still calculated using all the unlabeled data. [sent-276, score-0.299]
</p><p>71 However, each iteration in the algorithm uses unlabeled data with different sizes. [sent-277, score-0.299]
</p><p>72 Table 7 shows the results when changing the size of unlabeled data. [sent-278, score-0.299]
</p><p>73 We further experimented on unlabeled corpus  317 with larger size (up to 100 million characters) . [sent-289, score-0.382]
</p><p>74 Besides, because the number of features in our method is very large, using too large unlabeled corpus is intractable in real applications due to the limitation of memory. [sent-291, score-0.371]
</p><p>75 The size of unlabeled data is fixed as 5 million characters. [sent-303, score-0.385]
</p><p>76 We compared our approach with the method which uses only one unlabeled corpus. [sent-309, score-0.299]
</p><p>77 We can see that our methods works better, especially  318 when handling the out-of-vocabulary named entities; 4  Related work  Recent studies show that character sequence labeling is an effective method of Chinese word segmentation for machine learning (Xue, 2003; Low et al. [sent-333, score-0.665]
</p><p>78 , 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. [sent-340, score-0.241]
</p><p>79 However, we cannot simply equal Chinese character to English word because characters in Chinese carry much less information than words in English and the clustering results is less meaningful. [sent-351, score-0.347]
</p><p>80 Features extracted from large unlabeled corpus in previous works mainly focus on statistical information of characters. [sent-352, score-0.423]
</p><p>81 (2004) used the accessor variety criterion to extract word types. [sent-354, score-0.182]
</p><p>82 Li and Sun (2009) used punctuation information in Chinese word segmentation by introducing extra labels ’L’ and ’R’. [sent-355, score-0.29]
</p><p>83 Chang and Han (2010) , Sun and Xu (2011) used rich statistical information as discrete features in a sequence labeling framework. [sent-356, score-0.247]
</p><p>84 All these approaches can be viewed as using static statistics features in a supervised approach. [sent-357, score-0.176]
</p><p>85 For the static statistics features in our approach, we not only consider  richer string pairs with the different lengths, but also consider term frequency when processing  UOsuirnmgeontheocdorpus0 P. [sent-359, score-0.261]
</p><p>86 There are previous works using features extracted from label distribution of unlabeled corpus in NLP tasks. [sent-368, score-0.553]
</p><p>87 Besides, our approach uses two unlabeled corpora which can mutually enhancing to get better result. [sent-377, score-0.35]
</p><p>88 Two kinds of new features are used for the iterative modeling: static statistical features and dy319 namic statistical features. [sent-379, score-0.327]
</p><p>89 The dynamic statistical features use label distribution information for text contexts, and can be adjusted automatically during the co-training process. [sent-380, score-0.28]
</p><p>90 Experimental results show that the new features can  improve the performance on the Chinese word segmentation task. [sent-381, score-0.252]
</p><p>91 The proposed iterative semi-supervised method is not limited to the Chinese word segmentation task. [sent-383, score-0.211]
</p><p>92 Enhancing domain portability of chinese segmentation model using chi-square statistics and bootstrapping. [sent-401, score-0.526]
</p><p>93 Chinese named entity recognition and word segmentation based on character. [sent-455, score-0.239]
</p><p>94 Unsupervised segmentation of chinese text by use of branching entropy. [sent-460, score-0.526]
</p><p>95 Chinese word segmentation without using lexicon and hand-crafted training data. [sent-489, score-0.211]
</p><p>96 Fast online training with frequency-adaptive learning rates for chinese word segmentation and new  word detection. [sent-545, score-0.526]
</p><p>97 A discriminative latent variable chinese segmenter with hybrid word/character information. [sent-554, score-0.315]
</p><p>98 Subword-based tagging by conditional random fields for chinese word segmentation. [sent-577, score-0.354]
</p><p>99 An improved chinese word segmentation system with conditional random field. [sent-589, score-0.565]
</p><p>100 Effective tag set selection in chinese word segmentation via conditional random field modeling. [sent-597, score-0.602]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dsf', 0.408), ('ci', 0.328), ('chinese', 0.315), ('unlabeled', 0.299), ('character', 0.245), ('segmentation', 0.211), ('dsfs', 0.204), ('sun', 0.15), ('accessor', 0.144), ('static', 0.135), ('cws', 0.126), ('jxi', 0.113), ('label', 0.092), ('bmes', 0.091), ('ssf', 0.091), ('ssfs', 0.091), ('sighan', 0.09), ('string', 0.085), ('punctuations', 0.083), ('labeling', 0.082), ('xi', 0.082), ('pku', 0.08), ('punctuation', 0.079), ('characters', 0.074), ('xu', 0.069), ('dynamic', 0.068), ('labeled', 0.063), ('ck', 0.059), ('crf', 0.058), ('million', 0.052), ('works', 0.052), ('enhancing', 0.051), ('numerator', 0.05), ('ner', 0.05), ('tagger', 0.049), ('denominator', 0.047), ('lth', 0.047), ('adjust', 0.047), ('sequence', 0.047), ('hafer', 0.045), ('mairgup', 0.045), ('pinto', 0.045), ('xue', 0.044), ('feng', 0.044), ('benchmark', 0.043), ('av', 0.043), ('mutual', 0.042), ('peking', 0.041), ('han', 0.041), ('statistical', 0.041), ('features', 0.041), ('reach', 0.04), ('cotraining', 0.039), ('successor', 0.039), ('conditional', 0.039), ('distribution', 0.038), ('appears', 0.038), ('variety', 0.038), ('tag', 0.037), ('discrete', 0.036), ('ua', 0.036), ('blum', 0.036), ('maosong', 0.036), ('nese', 0.036), ('context', 0.036), ('preceding', 0.035), ('count', 0.035), ('change', 0.035), ('mccallum', 0.034), ('mi', 0.034), ('fixed', 0.034), ('zhao', 0.034), ('counts', 0.034), ('dasgupta', 0.034), ('harris', 0.034), ('strings', 0.033), ('position', 0.033), ('times', 0.032), ('current', 0.032), ('define', 0.032), ('nowadays', 0.032), ('corpus', 0.031), ('besides', 0.031), ('volume', 0.031), ('msr', 0.03), ('turian', 0.03), ('china', 0.03), ('solve', 0.03), ('contexts', 0.029), ('pages', 0.029), ('weiss', 0.029), ('schuurmans', 0.029), ('schapire', 0.029), ('encyclopedia', 0.029), ('named', 0.028), ('traditional', 0.028), ('zhang', 0.028), ('carry', 0.028), ('kinds', 0.028), ('goldwater', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="82-tfidf-1" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>Author: Longkai Zhang ; Houfeng Wang ; Xu Sun ; Mairgup Mansur</p><p>Abstract: Nowadays supervised sequence labeling models can reach competitive performance on the task of Chinese word segmentation. However, the ability of these models is restricted by the availability of annotated data and the design of features. We propose a scalable semi-supervised feature engineering approach. In contrast to previous works using pre-defined taskspecific features with fixed values, we dynamically extract representations of label distributions from both an in-domain corpus and an out-of-domain corpus. We update the representation values with a semi-supervised approach. Experiments on the benchmark datasets show that our approach achieve good results and reach an f-score of 0.961. The feature engineering approach proposed here is a general iterative semi-supervised method and not limited to the word segmentation task.</p><p>2 0.4011803 <a title="82-tfidf-2" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>Author: Xiaoqing Zheng ; Hanyang Chen ; Tianyu Xu</p><p>Abstract: This study explores the feasibility of performing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representation of Chinese characters, and use these improved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-theart performance with minimal computational cost. We also describe a perceptron-style algorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented.</p><p>3 0.25586253 <a title="82-tfidf-3" href="./emnlp-2013-An_Empirical_Study_Of_Semi-Supervised_Chinese_Word_Segmentation_Using_Co-Training.html">21 emnlp-2013-An Empirical Study Of Semi-Supervised Chinese Word Segmentation Using Co-Training</a></p>
<p>Author: Fan Yang ; Paul Vozila</p><p>Abstract: In this paper we report an empirical study on semi-supervised Chinese word segmentation using co-training. We utilize two segmenters: 1) a word-based segmenter leveraging a word-level language model, and 2) a character-based segmenter using characterlevel features within a CRF-based sequence labeler. These two segmenters are initially trained with a small amount of segmented data, and then iteratively improve each other using the large amount of unlabelled data. Our experimental results show that co-training captures 20% and 31% of the performance improvement achieved by supervised training with an order of magnitude more data for the SIGHAN Bakeoff 2005 PKU and CU corpora respectively.</p><p>4 0.16338292 <a title="82-tfidf-4" href="./emnlp-2013-Joint_Chinese_Word_Segmentation_and_POS_Tagging_on_Heterogeneous_Annotated_Corpora_with_Multiple_Task_Learning.html">111 emnlp-2013-Joint Chinese Word Segmentation and POS Tagging on Heterogeneous Annotated Corpora with Multiple Task Learning</a></p>
<p>Author: Xipeng Qiu ; Jiayi Zhao ; Xuanjing Huang</p><p>Abstract: Chinese word segmentation and part-ofspeech tagging (S&T;) are fundamental steps for more advanced Chinese language processing tasks. Recently, it has attracted more and more research interests to exploit heterogeneous annotation corpora for Chinese S&T.; In this paper, we propose a unified model for Chinese S&T; with heterogeneous annotation corpora. We first automatically construct a loose and uncertain mapping between two representative heterogeneous corpora, Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD) . Then we regard the Chinese S&T; with heterogeneous corpora as two “related” tasks and train our model on two heterogeneous corpora simultaneously. Experiments show that our method can boost the performances of both of the heterogeneous corpora by using the shared information, and achieves significant im- provements over the state-of-the-art methods.</p><p>5 0.12891084 <a title="82-tfidf-5" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>Author: Xinjie Zhou ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style. In this paper, we study the problem of extracting opinion targets of Chinese microblog messages. Such fine-grained word-level task has not been well investigated in microblogs yet. We propose an unsupervised label propagation algorithm to address the problem. The opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets. Topics in microblogs are identified by hashtags or using clustering algorithms. Experimental results on Chinese microblogs show the effectiveness of our framework and algorithms.</p><p>6 0.11950815 <a title="82-tfidf-6" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>7 0.11657235 <a title="82-tfidf-7" href="./emnlp-2013-Elephant%3A_Sequence_Labeling_for_Word_and_Sentence_Segmentation.html">72 emnlp-2013-Elephant: Sequence Labeling for Word and Sentence Segmentation</a></p>
<p>8 0.09456297 <a title="82-tfidf-8" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>9 0.082721181 <a title="82-tfidf-9" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>10 0.080463313 <a title="82-tfidf-10" href="./emnlp-2013-Semi-Supervised_Feature_Transformation_for_Dependency_Parsing.html">168 emnlp-2013-Semi-Supervised Feature Transformation for Dependency Parsing</a></p>
<p>11 0.078197323 <a title="82-tfidf-11" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>12 0.074984185 <a title="82-tfidf-12" href="./emnlp-2013-Exploiting_Zero_Pronouns_to_Improve_Chinese_Coreference_Resolution.html">80 emnlp-2013-Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</a></p>
<p>13 0.07364969 <a title="82-tfidf-13" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>14 0.066642299 <a title="82-tfidf-14" href="./emnlp-2013-Leveraging_Lexical_Cohesion_and_Disruption_for_Topic_Segmentation.html">124 emnlp-2013-Leveraging Lexical Cohesion and Disruption for Topic Segmentation</a></p>
<p>15 0.065502375 <a title="82-tfidf-15" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>16 0.06132691 <a title="82-tfidf-16" href="./emnlp-2013-A_Log-Linear_Model_for_Unsupervised_Text_Normalization.html">9 emnlp-2013-A Log-Linear Model for Unsupervised Text Normalization</a></p>
<p>17 0.060840338 <a title="82-tfidf-17" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>18 0.060533222 <a title="82-tfidf-18" href="./emnlp-2013-Authorship_Attribution_of_Micro-Messages.html">27 emnlp-2013-Authorship Attribution of Micro-Messages</a></p>
<p>19 0.06046154 <a title="82-tfidf-19" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>20 0.059557118 <a title="82-tfidf-20" href="./emnlp-2013-Adaptor_Grammars_for_Learning_Non-Concatenative_Morphology.html">19 emnlp-2013-Adaptor Grammars for Learning Non-Concatenative Morphology</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.225), (1, -0.011), (2, -0.023), (3, -0.14), (4, -0.161), (5, -0.047), (6, 0.152), (7, 0.331), (8, -0.267), (9, 0.271), (10, 0.057), (11, -0.067), (12, 0.125), (13, -0.108), (14, -0.062), (15, 0.053), (16, -0.034), (17, -0.011), (18, 0.016), (19, -0.089), (20, -0.087), (21, -0.033), (22, -0.033), (23, -0.094), (24, 0.032), (25, -0.086), (26, -0.066), (27, 0.021), (28, -0.027), (29, -0.014), (30, -0.024), (31, -0.032), (32, -0.049), (33, -0.031), (34, 0.085), (35, -0.003), (36, -0.048), (37, -0.067), (38, -0.038), (39, -0.028), (40, -0.003), (41, -0.064), (42, -0.067), (43, -0.014), (44, 0.034), (45, 0.037), (46, -0.013), (47, -0.019), (48, -0.025), (49, -0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96345133 <a title="82-lsi-1" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>Author: Longkai Zhang ; Houfeng Wang ; Xu Sun ; Mairgup Mansur</p><p>Abstract: Nowadays supervised sequence labeling models can reach competitive performance on the task of Chinese word segmentation. However, the ability of these models is restricted by the availability of annotated data and the design of features. We propose a scalable semi-supervised feature engineering approach. In contrast to previous works using pre-defined taskspecific features with fixed values, we dynamically extract representations of label distributions from both an in-domain corpus and an out-of-domain corpus. We update the representation values with a semi-supervised approach. Experiments on the benchmark datasets show that our approach achieve good results and reach an f-score of 0.961. The feature engineering approach proposed here is a general iterative semi-supervised method and not limited to the word segmentation task.</p><p>2 0.9404633 <a title="82-lsi-2" href="./emnlp-2013-An_Empirical_Study_Of_Semi-Supervised_Chinese_Word_Segmentation_Using_Co-Training.html">21 emnlp-2013-An Empirical Study Of Semi-Supervised Chinese Word Segmentation Using Co-Training</a></p>
<p>Author: Fan Yang ; Paul Vozila</p><p>Abstract: In this paper we report an empirical study on semi-supervised Chinese word segmentation using co-training. We utilize two segmenters: 1) a word-based segmenter leveraging a word-level language model, and 2) a character-based segmenter using characterlevel features within a CRF-based sequence labeler. These two segmenters are initially trained with a small amount of segmented data, and then iteratively improve each other using the large amount of unlabelled data. Our experimental results show that co-training captures 20% and 31% of the performance improvement achieved by supervised training with an order of magnitude more data for the SIGHAN Bakeoff 2005 PKU and CU corpora respectively.</p><p>3 0.91390222 <a title="82-lsi-3" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>Author: Xiaoqing Zheng ; Hanyang Chen ; Tianyu Xu</p><p>Abstract: This study explores the feasibility of performing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representation of Chinese characters, and use these improved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-theart performance with minimal computational cost. We also describe a perceptron-style algorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented.</p><p>4 0.87502211 <a title="82-lsi-4" href="./emnlp-2013-Joint_Chinese_Word_Segmentation_and_POS_Tagging_on_Heterogeneous_Annotated_Corpora_with_Multiple_Task_Learning.html">111 emnlp-2013-Joint Chinese Word Segmentation and POS Tagging on Heterogeneous Annotated Corpora with Multiple Task Learning</a></p>
<p>Author: Xipeng Qiu ; Jiayi Zhao ; Xuanjing Huang</p><p>Abstract: Chinese word segmentation and part-ofspeech tagging (S&T;) are fundamental steps for more advanced Chinese language processing tasks. Recently, it has attracted more and more research interests to exploit heterogeneous annotation corpora for Chinese S&T.; In this paper, we propose a unified model for Chinese S&T; with heterogeneous annotation corpora. We first automatically construct a loose and uncertain mapping between two representative heterogeneous corpora, Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD) . Then we regard the Chinese S&T; with heterogeneous corpora as two “related” tasks and train our model on two heterogeneous corpora simultaneously. Experiments show that our method can boost the performances of both of the heterogeneous corpora by using the shared information, and achieves significant im- provements over the state-of-the-art methods.</p><p>5 0.70767695 <a title="82-lsi-5" href="./emnlp-2013-Elephant%3A_Sequence_Labeling_for_Word_and_Sentence_Segmentation.html">72 emnlp-2013-Elephant: Sequence Labeling for Word and Sentence Segmentation</a></p>
<p>Author: Kilian Evang ; Valerio Basile ; Grzegorz Chrupala ; Johan Bos</p><p>Abstract: Tokenization is widely regarded as a solved problem due to the high accuracy that rulebased tokenizers achieve. But rule-based tokenizers are hard to maintain and their rules language specific. We show that highaccuracy word and sentence segmentation can be achieved by using supervised sequence labeling on the character level combined with unsupervised feature learning. We evaluated our method on three languages and obtained error rates of 0.27 ‰ (English), 0.35 ‰ (Dutch) and 0.76 ‰ (Italian) for our best models. 1 An Elephant in the Room Tokenization, the task of segmenting a text into words and sentences, is often regarded as a solved problem in natural language processing (Dridan and . Oepen, 2012), probably because many corpora are already in tokenized format. But like an elephant in the living room, it is a problem that is impossible to overlook whenever new raw datasets need to be processed or when tokenization conventions are reconsidered. It is moreover an important problem, because any errors occurring early in the NLP pipeline affect further analysis negatively. And even though current tokenizers reach high performance, there are three issues that we feel haven’t been addressed satisfactorily so far: • • Most tokenizers are rule-based and therefore hard to maintain and hard to adapt to new domains and new languages (Silla Jr. and Kaestner, 2004); Word and sentence segmentation are often seen as separate tasks, but they obviously inform each other and it could be advantageous to view them as a combined task; 1422 bo s }@ rug .nl † g .chrupal a @ uvt .nl • Most tokenization methods provide no align- ment between raw and tokenized text, which makes mapping the tokenized version back onto the actual source hard or impossible. In short, we believe that regarding tokenization, there is still room for improvement, in particular on the methodological side of the task. We are particularly interested in the following questions: Can we use supervised learning to avoid hand-crafting rules? Can we use unsupervised feature learning to reduce feature engineering effort and boost performance? Can we use the same method across languages? Can we combine word and sentence boundary detection into one task? 2 Related Work Usually the text segmentation task is split into word tokenization and sentence boundary detection. Rulebased systems for finding word and sentence boundaries often are variations on matching hand-coded regular expressions (Grefenstette, 1999; Silla Jr. and Kaestner, 2004; Jurafsky and Martin, 2008; Dridan and Oepen, 2012). Several unsupervised systems have been proposed for sentence boundary detection. Kiss and Strunk (2006) present a language-independent, unsupervised approach and note that abbreviations form a major source of ambiguity in sentence boundary detection and use collocation detection to build a high-accuracy abbreviation detector. The resulting system reaches high accuracy, rivalling handcrafted rule-based and supervised systems. A similar system was proposed earlier by Mikheev (2002). Existing supervised learning approaches for sentence boundary detection use as features tokens preceding and following potential sentence boundary, part of speech, capitalization information and lists of abbreviations. Learning methods employed in Proce Sdeiantgtlse o,f W thaesh 2i0n1gt3o nC,o UnSfeAre,n 1c8e- o2n1 E Omctpoibriecra 2l0 M13et.h ?oc d2s0 i1n3 N Aastusorcaila Ltiaon g fuoarg Ceo Pmrpoucetastsi on ga,l p Laignegsu 1is4t2ic2s–1426, these approaches include maximum entropy models (Reynar and Ratnaparkhi, 1997) decision trees (Riley, 1989), and neural networks (Palmer and Hearst, 1997). Closest to our work are approaches that present token and sentence splitters using conditional random fields (Tomanek et al., 2007; Fares et al., 2013). However, these previous approaches consider tokens (i.e. character sequences) as basic units for labeling, whereas we consider single characters. As a consequence, labeling is more resource-intensive, but it also gives us more expressive power. In fact, our approach kills two birds with one stone, as it allows us to integrate token and sentence boundaries detection into one task. 3 Method 3.1 IOB Tokenization IOB tagging is widely used in tasks identifying chunks of tokens. We use it to identify chunks of characters. Characters outside of tokens are labeled O, inside of tokens I. For characters at the beginning of tokens, we use S at sentence boundaries, otherwise T (for token). This scheme offers some nice features, like allowing for discontinuous tokens (e.g. hyphenated words at line breaks) and starting a new token in the middle of a typographic word if the tokenization scheme requires it, as e.g. in did|n ’t. An example ins given ien r Figure 1 i.t It didn ’ t matter i f the face s were male , S I I T I OT I I I IOT I OT I I OT I I I I OT I I I I OT I II I OT I TO female or tho se of chi ldren . Eighty T I I I I I I OT I I I I I I I OT OT I I OT I I I TOS I I I O III three percent o f people in the 3 0 -to-3 4 I I I I I I OT I I I I I I OT I I I I I I OT I I I OT I I OT OT I I I IO year old age range gave correct responses . T I I I OT I OT I I OT I I I I I OT I I I I T I OT I I II I OT I I I IIII Figure 1: Example of IOB-labeled characters 3.2 Datasets In our experiments we use three datasets to compare our method for different languages and for different domains: manually checked English newswire texts taken from the Groningen Meaning Bank, GMB (Basile et al., 2012), Dutch newswire texts, comprising two days from January 2000 extracted from the Twente News Corpus, TwNC (Ordelman et al., 1423 2007), and a random sample of Italian texts from the corpus (Borghetti et al., 2011). PAISA` Table 1: Datasets characteristics. NameLanguageDomainSentences Tokens TGNMCB EDnugtclihshNNeewwsswwiir ee492,,58387686 604,,644337 PAIItalianWeb/various42,674869,095 The data was converted into IOB format by inferring an alignment between the raw text and the segmented text. 3.3 Sequence labeling We apply the Wapiti implementation (Lavergne et al., 2010) of Conditional Random Fields (Lafferty et al., 2001), using as features the output label of each character, combined with 1) the character itself, 2) the output label on the previous character, 3) characters and/or their Unicode categories from context windows of varying sizes. For example, with a context size of 3, in Figure 1, features for the E in Eighty-three with the output label S would be E/S, O/S, /S, i/S, Space/S, Lowercase/S. The intuition is that the 3 1 existing Unicode categories can generalize across similar characters whereas character features can identify specific contexts such as abbreviations or contractions (e.g. didn ’t). The context window sizes we use are 0, 1, 3, 5, 7, 9, 11 and 13, centered around the focus character. 3.4 Deep learning of features Automatically learned word embeddings have been successfully used in NLP to reduce reliance on manual feature engineering and boost performance. We adapt this approach to the character level, and thus, in addition to hand-crafted features we use text representations induced in an unsupervised fashion from character strings. A complete discussion of our approach to learning text embeddings can be found in (Chrupała, 2013). Here we provide a brief overview. Our representations correspond to the activation of the hidden layer in a simple recurrent neural (SRN) network (Elman, 1990; Elman, 1991), implemented in a customized version of Mikolov (2010)’s RNNLM toolkit. The network is sequentially presented with a large amount of raw text and learns to predict the next character in the sequence. It uses the units in the hidden layer to store a generalized representation of the recent history. After training the network on large amounts on unlabeled text, we run it on the training and test data, and record the activation of the hidden layer at each position in the string as it tries to predict the next character. The vector of activations of the hidden layer provides additional features used to train and run the CRF. For each of the K = 10 most active units out of total J = 400 hidden units, we create features (f(1) . . . f(K)) defined as f(k) = 1if sj(k) > 0.5 and f(k) = 0 otherwise, where sj (k) returns the activation of the kth most active unit. For training the SRN only raw text is necessary. We trained on the entire GMB 2.0.0 (2.5M characters), the portion of TwNC corresponding to January 2000 (43M characters) and a sample of the PAISA` corpus (39M characters). 4 Results and Evaluation In order to evaluate the quality of the tokenization produced by our models we conducted several experiments with different combinations of features and context sizes. For these tests, the models are trained on an 80% portion of the data sets and tested on a 10% development set. Final results are obtained on a 10% test set. We report both absolute number of errors and error rates per thousand (‰). 4.1 Feature sets We experiment with two kinds of features at the character level, namely Unicode categories (31 dif- ferent ones), Unicode character codes, and a combination of them. Unicode categories are less sparse than the character codes (there are 88, 134, and 502 unique characters for English, Dutch and Italian, respectively), so the combination provide some generalization over just character codes. Table 2: Error rates obtained with different feature sets. Cat stands for Unicode category, Code for Unicode character code, and Cat-Code for a union of these features. Error rates per thousand (‰) Feature setEnglishDutchItalian C ao td-9eC-9ode-94568 ( 0 1. 241950) 1,7 4807243 ( 12 . 685078) 1,65 459872 ( 12 . 162470) 1424 From these results we see that categories alone perform worse than only codes. For English there is no gain from the combination over using only character codes. For Dutch and Italian there is an improvement, although it is only significant for Italian (p = 0.480 and p = 0.005 respectively, binomial exact test). We use this feature combination in the experiments that follow. Note that these models are trained using a symmetrical context of 9 characters (four left and four right of the current character). In the next section we show performance of models with different window sizes. 4.2 Context window We run an experiment to evaluate how the size of the context in the training phase impacts the classification. In Table 4.2 we show the results for symmetrical windows ranging in size from 1to 13. Table 3: Using different context window sizes. Feature setEngElisrhror rateDs puetrch thousandI (t‰al)ian C Ca t - C Co d e - 31957217830 ( 308 . 2635218) 4,39 2753742085(1 (017. 0956208 6) 92,1760 8516873 (1 (135. 31854617) CCaat - CCood e - 1 3198 ( 0 . 2 58) 7 561 ( 1 . 5 64) 6 9702 ( 1 . 1271) 4.3 SRN features We also tested the automatically learned features de- rived from the activation of the hidden layer of an SRN language model, as explained in Section 3. We combined these features with character code and Unicode category features in windows of different sizes. The results of this test are shown in Table 4. The first row shows the performance of SRN features on their own. The following rows show the combination of SRN features with the basic feature sets of varying window size. It can be seen that augmenting the feature sets with SRN features results in large reductions of error rates. The Cat-Code-1SRN setting has error rates comparable to Cat-Code9. The addition of SRN features to the two best previous models, Cat-Code-9 and Cat-Code-13, reduces the error rate by 83% resp. 81% for Dutch, and by 24% resp. 26% for Italian. All these differences are statistically significant according to the binomial test (p < 0.001). For English, there are too few errors to detect a statistically significant effect for Cat-Code-9 (p = 0.07), but for Cat-Code-13 we find p = 0.016. Table 4: Results obtained using different context window sizes and addition of SRN features. Error rates per thousand (‰) Feature setEnglishDutchItalian C SaRtN-C o d e -59173 S -R SN 27413( 0 . 2107635)12 7643251 (0 .42358697)45 90376489(01 .829631) In a final step, we selected the best models based on the development sets (Cat-Code-7-SRN for English and Dutch, Cat-Code-1 1-SRN for Italian), and checked their performance on the final test set. This resulted in 10 errors (0.27 ‰) for English (GMB corpus), 199 errors (0.35 ‰) for Dutch (TwNC corpus), and 454 errors (0.76 ‰) for Italian (PAISA` corpus). 5 Discussion It is interesting to examine what kind of errors the SRN features help avoid. In the English and Dutch datasets many errors are caused by failure to recognize personal titles and initials or misparsing of numbers. In the Italian data, a large fraction of errors is due to verbs with clitics, which are written as a single word, but treated as separate tokens. Table 5 shows examples of errors made by a simpler model that are fixed by adding SRN features. Table 6 shows the confusion matrices for the Cat-Code-7 and CatCode-7-SRN sets on the Dutch data. The mistake most improved by SRN features is T/I with 89% error reduction (see also Table 5). The is also the most common remaining mistake. A comparison with other approaches is hard because of the difference in datasets and task definition (combined word/sentence segmentation). Here we just compare our results for sentence segmentation (sentence F1 score) with Punkt, a state-of-the1425 Table 5: Positive impact of SRN features. Table 6: Confusion matrix for Dutch development set. GoTOSIld32P8r1e52d480iIc7te52d,3O0C4 at-32C So20d8e-47612T089P3r2e8d5ic43t1065Ied7,2C3Oa04 t-C3o1d2S0 e-78S1R0562TN038 art sentence boundary detection system (Kiss and Strunk, 2006). With its standard distributed models, Punkt achieves 98.51% on our English test set, 98.87% on Dutch and 98.34% on Italian, compared with 100%, 99.54% and 99.51% for our system. Our system benefits here from its ability to adapt to a new domain with relatively little (but annotated) training data. 6 What Elephant? Word and sentence segmentation can be recast as a combined tagging task. This way, tokenization is cast as a supervised learning task, causing a shift of labor from writing rules to manually correcting labels. Learning this task with CRF achieves high accuracy.1 Furthermore, our tagging method does not lose the connection between original text and tokens. In future work, we plan to broaden the scope of this work to other steps in document preparation, 1All software needed to replicate our experiments is available at http : / / gmb . let . rug . nl / e lephant / experiments . php such as normalization of punctuation, and their interaction with segmentation. We further plan to test our method on a wider range of datasets, allowing a more direct comparison with other approaches. Finally, we plan to explore the possibility of a statistical universal segmentation model for mutliple languages and domains. In a famous scene with a live elephant on stage, the comedian Jimmy Durante was asked about it by a policeman and surprisedly answered: “What elephant?” We feel we can say the same now as far as tokenization is concerned. References Valerio Basile, Johan Bos, Kilian Evang, and Noortje Venhuizen. 2012. Developing a large semantically annotated corpus. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC 2012), pages 3 196–3200, Istanbul, Turkey. Claudia Borghetti, Sara Castagnoli, and Marco Brunello. 2011. Itesti del web: una proposta di classificazione sulla base del corpus PAISA`. In M. Cerruti, E. Corino, and C. Onesti, editors, Formale e informale. La variazione di registro nella comunicazione elettronica, pages 147–170. Carocci, Roma. Grzegorz Chrupała. 2013. Text segmentation with character-level text embeddings. In ICML Workshop on Deep Learning for Audio, Speech and Language Processing, Atlanta, USA. Rebecca Dridan and Stephan Oepen. 2012. Tokenization: Returning to a long solved problem a survey, contrastive experiment, recommendations, and toolkit In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 378–382, Jeju Island, Korea. Association for Computational Linguistics. Jeffrey L. Elman. 1990. Finding structure in time. Cognitive science, 14(2): 179–21 1. Jeffrey L. Elman. 1991 . Distributed representations, simple recurrent networks, and grammatical structure. Machine learning, 7(2): 195–225. Murhaf Fares, Stephan Oepen, and Zhang Yi. 2013. Machine learning for high-quality tokenization - replicating variable tokenization schemes. In A. Gelbukh, editor, CICLING 2013, volume 7816 of Lecture Notes in Computer Science, pages 23 1–244, Berlin Heidelberg. Springer-Verlag. Gregory Grefenstette. 1999. Tokenization. In Hans van Halteren, editor, Syntactic Wordclass Tagging, pages 117–133. Kluwer Academic Publishers, Dordrecht. – –. 1426 Daniel Jurafsky and James H. Martin. 2008. Speech and Language Processing. An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice Hall, 2nd edition. Tibor Kiss and Jan Strunk. 2006. Unsupervised multilingual sentence boundary detection. Computational Linguistics, 32(4):485–525. John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML-01, pages 282–289. Thomas Lavergne, Olivier Capp e´, and Fran ¸cois Yvon. 2010. Practical very large scale CRFs. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 504–5 13, Uppsala, Sweden, July. Association for Computational Linguistics. Andrei Mikheev. 2002. Periods, capitalized words, etc. Computational Linguistics, 28(3):289–3 18. Tom a´ˇ s Mikolov, Martin Karafi´ at, Luk a´ˇ s Burget, Jan Cˇernock y´, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech. Roeland Ordelman, Franciska de Jong, Arjan van Hessen, and Hendri Hondorp. 2007. TwNC: a multifaceted Dutch news corpus. ELRA Newsleter, 12(3/4):4–7. David D. Palmer and Marti A. Hearst. 1997. Adaptive multilingual sentence boundary disambiguation. Computational Linguistics, 23(2):241–267. Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 16– 19, Washington, DC, USA. Association for Computational Linguistics. Michael D. Riley. 1989. Some applications of tree-based modelling to speech and language. In Proceedings of the workshop on Speech and Natural Language, HLT ’89, pages 339–352, Stroudsburg, PA, USA. Association for Computational Linguistics. Carlos N. Silla Jr. and Celso A. A. Kaestner. 2004. An analysis of sentence boundary detection systems for English and Portuguese documents. In Fifth International Conference on Intelligent Text Processing and Computational Linguistics, volume 2945 of Lecture Notes in Computer Science, pages 135–141. Springer. Katrin Tomanek, Joachim Wermter, and Udo Hahn. 2007. Sentence and token splitting based on conditional random fields. In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics, pages 49–57, Melbourne, Australia.</p><p>6 0.50034654 <a title="82-lsi-6" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>7 0.47548598 <a title="82-lsi-7" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>8 0.39866191 <a title="82-lsi-8" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>9 0.39323118 <a title="82-lsi-9" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>10 0.37574291 <a title="82-lsi-10" href="./emnlp-2013-Semi-Supervised_Feature_Transformation_for_Dependency_Parsing.html">168 emnlp-2013-Semi-Supervised Feature Transformation for Dependency Parsing</a></p>
<p>11 0.36914456 <a title="82-lsi-11" href="./emnlp-2013-Ubertagging%3A_Joint_Segmentation_and_Supertagging_for_English.html">190 emnlp-2013-Ubertagging: Joint Segmentation and Supertagging for English</a></p>
<p>12 0.34311578 <a title="82-lsi-12" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>13 0.30586618 <a title="82-lsi-13" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>14 0.30086216 <a title="82-lsi-14" href="./emnlp-2013-Leveraging_Lexical_Cohesion_and_Disruption_for_Topic_Segmentation.html">124 emnlp-2013-Leveraging Lexical Cohesion and Disruption for Topic Segmentation</a></p>
<p>15 0.29792768 <a title="82-lsi-15" href="./emnlp-2013-Efficient_Higher-Order_CRFs_for_Morphological_Tagging.html">70 emnlp-2013-Efficient Higher-Order CRFs for Morphological Tagging</a></p>
<p>16 0.27239391 <a title="82-lsi-16" href="./emnlp-2013-Feature_Noising_for_Log-Linear_Structured_Prediction.html">86 emnlp-2013-Feature Noising for Log-Linear Structured Prediction</a></p>
<p>17 0.27058855 <a title="82-lsi-17" href="./emnlp-2013-Detecting_Promotional_Content_in_Wikipedia.html">61 emnlp-2013-Detecting Promotional Content in Wikipedia</a></p>
<p>18 0.26954311 <a title="82-lsi-18" href="./emnlp-2013-Authorship_Attribution_of_Micro-Messages.html">27 emnlp-2013-Authorship Attribution of Micro-Messages</a></p>
<p>19 0.26384532 <a title="82-lsi-19" href="./emnlp-2013-Chinese_Zero_Pronoun_Resolution%3A_Some_Recent_Advances.html">45 emnlp-2013-Chinese Zero Pronoun Resolution: Some Recent Advances</a></p>
<p>20 0.26298878 <a title="82-lsi-20" href="./emnlp-2013-Adaptor_Grammars_for_Learning_Non-Concatenative_Morphology.html">19 emnlp-2013-Adaptor Grammars for Learning Non-Concatenative Morphology</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.036), (18, 0.054), (22, 0.058), (30, 0.094), (46, 0.241), (50, 0.032), (51, 0.181), (66, 0.046), (71, 0.04), (75, 0.028), (77, 0.03), (90, 0.017), (96, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88539213 <a title="82-lda-1" href="./emnlp-2013-Flexible_and_Efficient_Hypergraph_Interactions_for_Joint_Hierarchical_and_Forest-to-String_Decoding.html">88 emnlp-2013-Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest-to-String Decoding</a></p>
<p>Author: Martin Cmejrek ; Haitao Mi ; Bowen Zhou</p><p>Abstract: Machine translation benefits from system combination. We propose flexible interaction of hypergraphs as a novel technique combining different translation models within one decoder. We introduce features controlling the interactions between the two systems and explore three interaction schemes of hiero and forest-to-string models—specification, generalization, and interchange. The experiments are carried out on large training data with strong baselines utilizing rich sets of dense and sparse features. All three schemes significantly improve results of any single system on four testsets. We find that specification—a more constrained scheme that almost entirely uses forest-to-string rules, but optionally uses hiero rules for shorter spans—comes out as the strongest, yielding improvement up to 0.9 (T -B )/2 points. We also provide a detailed experimental and qualitative analysis of the results.</p><p>same-paper 2 0.76653177 <a title="82-lda-2" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>Author: Longkai Zhang ; Houfeng Wang ; Xu Sun ; Mairgup Mansur</p><p>Abstract: Nowadays supervised sequence labeling models can reach competitive performance on the task of Chinese word segmentation. However, the ability of these models is restricted by the availability of annotated data and the design of features. We propose a scalable semi-supervised feature engineering approach. In contrast to previous works using pre-defined taskspecific features with fixed values, we dynamically extract representations of label distributions from both an in-domain corpus and an out-of-domain corpus. We update the representation values with a semi-supervised approach. Experiments on the benchmark datasets show that our approach achieve good results and reach an f-score of 0.961. The feature engineering approach proposed here is a general iterative semi-supervised method and not limited to the word segmentation task.</p><p>3 0.6843062 <a title="82-lda-3" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>Author: Xiaoqing Zheng ; Hanyang Chen ; Tianyu Xu</p><p>Abstract: This study explores the feasibility of performing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representation of Chinese characters, and use these improved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-theart performance with minimal computational cost. We also describe a perceptron-style algorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented.</p><p>4 0.67782819 <a title="82-lda-4" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>Author: Xinjie Zhou ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style. In this paper, we study the problem of extracting opinion targets of Chinese microblog messages. Such fine-grained word-level task has not been well investigated in microblogs yet. We propose an unsupervised label propagation algorithm to address the problem. The opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets. Topics in microblogs are identified by hashtags or using clustering algorithms. Experimental results on Chinese microblogs show the effectiveness of our framework and algorithms.</p><p>5 0.67735493 <a title="82-lda-5" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>Author: Kuzman Ganchev ; Dipanjan Das</p><p>Abstract: We present a framework for cross-lingual transfer of sequence information from a resource-rich source language to a resourceimpoverished target language that incorporates soft constraints via posterior regularization. To this end, we use automatically word aligned bitext between the source and target language pair, and learn a discriminative conditional random field model on the target side. Our posterior regularization constraints are derived from simple intuitions about the task at hand and from cross-lingual alignment information. We show improvements over strong baselines for two tasks: part-of-speech tagging and namedentity segmentation.</p><p>6 0.67724407 <a title="82-lda-6" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>7 0.67634135 <a title="82-lda-7" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>8 0.67497891 <a title="82-lda-8" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>9 0.67459196 <a title="82-lda-9" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>10 0.67446214 <a title="82-lda-10" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>11 0.67362559 <a title="82-lda-11" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>12 0.67290431 <a title="82-lda-12" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>13 0.67240751 <a title="82-lda-13" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>14 0.67202145 <a title="82-lda-14" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>15 0.6719045 <a title="82-lda-15" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>16 0.67000568 <a title="82-lda-16" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>17 0.66970676 <a title="82-lda-17" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>18 0.66885608 <a title="82-lda-18" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>19 0.66768813 <a title="82-lda-19" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>20 0.66666937 <a title="82-lda-20" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
