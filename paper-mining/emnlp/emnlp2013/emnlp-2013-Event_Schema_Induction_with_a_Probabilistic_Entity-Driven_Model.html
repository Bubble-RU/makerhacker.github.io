<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-75" href="#">emnlp2013-75</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</h1>
<br/><p>Source: <a title="emnlp-2013-75-pdf" href="http://aclweb.org/anthology//D/D13/D13-1185.pdf">pdf</a></p><p>Author: Nathanael Chambers</p><p>Abstract: Event schema induction is the task of learning high-level representations of complex events (e.g., a bombing) and their entity roles (e.g., perpetrator and victim) from unlabeled text. Event schemas have important connections to early NLP research on frames and scripts, as well as modern applications like template extraction. Recent research suggests event schemas can be learned from raw text. Inspired by a pipelined learner based on named entity coreference, this paper presents the first generative model for schema induction that integrates coreference chains into learning. Our generative model is conceptually simpler than the pipelined approach and requires far less training data. It also provides an interesting contrast with a recent HMM-based model. We evaluate on a common dataset for template schema extraction. Our generative model matches the pipeline’s performance, and outperforms the HMM by 7 F1 points (20%).</p><p>Reference: <a title="emnlp-2013-75-reference" href="../emnlp2013_reference/emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Event schema induction is the task of learning high-level representations of complex events (e. [sent-2, score-0.597]
</p><p>2 Event schemas have important connections to early NLP research on frames and scripts, as well as modern applications like template extraction. [sent-7, score-0.651]
</p><p>3 Recent research suggests event schemas can be learned from raw text. [sent-8, score-0.643]
</p><p>4 Inspired by a pipelined learner based on named entity coreference, this paper presents the first generative model for schema induction that integrates coreference chains into learning. [sent-9, score-0.89]
</p><p>5 We evaluate on a common dataset for template schema extraction. [sent-12, score-0.702]
</p><p>6 Many proposals, such as frames and scripts, used rich event schemas to model the situations described in text. [sent-15, score-0.606]
</p><p>7 While the field has since focused on more shallow approaches, recent work on schema induction shows that event schemas might be learnable from raw text. [sent-16, score-1.126]
</p><p>8 This paper continues the trend, addressing the question, can event schemas be induced from raw text without prior knowledge? [sent-17, score-0.58]
</p><p>9 We present a new generative model for event schemas, 1797 and it produces state-of-the-art induction results, including a 7 F1 point gain over a different generative proposal developed in parallel with this work. [sent-18, score-0.492]
</p><p>10 Event schemas are unique from most work in information extraction (IE). [sent-19, score-0.444]
</p><p>11 Event schemas build relations into coherent event structures, often called templates in IE. [sent-23, score-0.657]
</p><p>12 For instance, an election template  jointly connects that obama won a presidential election with romney was the defeated, the election occurred in 2012, and the popular vote was 50-48. [sent-24, score-0.362]
</p><p>13 The learner receives no human input, and it first induces a schema before extracting instances of it. [sent-28, score-0.483]
</p><p>14 Our proposed model contributes to a growing line of research in schema induction. [sent-29, score-0.447]
</p><p>15 However, central to their algorithm is the use of coreferring entity mentions to knit events and entities together into an event schema. [sent-36, score-0.548]
</p><p>16 Other research conducted at the time of this paper also proposes a generative model for schema induction (Cheung et al. [sent-39, score-0.624]
</p><p>17 Algorithms that do focus on event schema extraction typically require both the schemas and labeled corpora, such as rule-based approaches (Chinchor et al. [sent-54, score-1.133]
</p><p>18 The approach requires many documents about the exact same event instance, and relations are binary (not schemas) over repeated named entities. [sent-72, score-0.327]
</p><p>19 Our model instead learns  schemas from documents with mixed topics that don’t describe the same event, so repeated proper nouns are less helpful. [sent-73, score-0.455]
</p><p>20 As with others, training data is pre-clustered by event type and there is no schema connection between relations. [sent-77, score-0.684]
</p><p>21 They learned event schemas with a three-stage clustering algorithm that included a requirement to retrieve extra training data. [sent-79, score-0.643]
</p><p>22 (2013) is most related as a generative formulation of schema induction. [sent-85, score-0.525]
</p><p>23 Latent schema variables generate the event vari-  ables (in the spirit of preliminary work by O’Connor (2012)). [sent-87, score-0.705]
</p><p>24 In summary, this paper extends most previous work on event schema induction by removing the supervision. [sent-92, score-0.756]
</p><p>25 It contains Latin American newswire about terrorism events, and it provides a set of hand-constructed event schemas that are traditionally called template schemas. [sent-96, score-0.864]
</p><p>26 It also maps labeled templates to the text, providing a dataset for template extraction evaluations. [sent-97, score-0.406]
</p><p>27 We too evaluate our model through extraction, but we also compare our learned schemas to the hand-created template schemas. [sent-99, score-0.688]
</p><p>28 The corpus is particularly challenging because template schemas are inter-mixed and entities can play multiple roles across instances. [sent-104, score-0.713]
</p><p>29 Each entity receives a schema role label, so it allows all mentions of the entity to inform that role choice. [sent-112, score-0.841]
</p><p>30 This important constraint links coreferring mentions to the same schema role, and distinguishes our approach from others (Cheung et al. [sent-113, score-0.575]
</p><p>31 An entity is a set of entity mentions clustered by coreference resolution. [sent-117, score-0.407]
</p><p>32 Each entity will be labeled with both a slot variable s and a template variable t (e. [sent-122, score-0.691]
</p><p>33 3 The Generative Models Similar to topics in LDA, each document d in our model has a corresponding multinomial over schema types θd, drawn from a Dirichlet. [sent-143, score-0.564]
</p><p>34 These t variables represent the high level schema types, such as bombing or kidnapping. [sent-145, score-0.622]
</p><p>35 Finally, the entity’s canonical head word is generated from all entity mentions’ typed dependencies from δs, and named  βs,  ×  entity types from  γs. [sent-148, score-0.347]
</p><p>36 ×  1800 The most important characteristic of this model is the separation of event words from the lexical properties of specific entity mentions. [sent-149, score-0.353]
</p><p>37 The schema type variables t only model the distribution of event words (bomb, plant, defuse), but the slot variables s model the syntax (subject-bomb, subject-plant, object-arrest) and entity words (suspect, terrorist, man). [sent-150, score-1.112]
</p><p>38 This allows the high-level schemas to first select predicates, and then forces predicate arguments to prefer slots that are in the parent schema type. [sent-151, score-1.251]
</p><p>39 Formally, a document d receives a labeling Zd where each entity e ∈ Ed is labeled Zd,e = (t, s) wwihtehr a escahcehm ean type et a∈nd E a slot s. [sent-152, score-0.474]
</p><p>40 We assume the following generative process for a document d: Generate θd from Dir(α) for each schema type t = 1. [sent-155, score-0.599]
</p><p>41 r|oFm| Multinomial(γs) The number of schema types m and the number  of slots per schema k are chosen based on training set performance. [sent-165, score-1.283]
</p><p>42 Figure 3: The full plate diagram for the event schema model. [sent-166, score-0.683]
</p><p>43 The Flat Relation Model We also experiment with a Flat Relation Model that removes the hidden t variables, ignoring schema types. [sent-168, score-0.474]
</p><p>44 Predicates are more informative at the higher level, but less so for slots where syntax is more important. [sent-171, score-0.389]
</p><p>45 This flat model now learns a large set of k slots S that aren’t connected by a high-level schema variable. [sent-173, score-0.902]
</p><p>46 tin Eoamcihasl sl (h, M, F) saims ailar to above: (1) a multinomial over the head mentions βs, (2) a multinomial over the grammatical relations of the entity mentions δs, and (3) a multinomial over the entity features γs. [sent-175, score-0.706]
</p><p>47 For each entity in a document, a hidden slot s ∈ S is first drawn from Θa ,d oancudm mthenent, t ah eh iodbdseenrv selodt entity (h, M, F) iws nd frraowmn according to the multinomials (βs, γs, δs). [sent-176, score-0.475]
</p><p>48 We later evaluate this flat model to show the benefit of added schema structure. [sent-177, score-0.513]
</p><p>49 4 Inference We use collapsed Gibbs sampling for inference,  sampling the latent variables te,d and  se,d  in se1801  Figure 4: Simplified plate diagrams comparing the flat relation model to the full template model. [sent-179, score-0.422]
</p><p>50 Initial parameter values are set by randomly setting t and s  variables from the uniform distribution over schema types and slots, then computing the other parameter values based on these initial settings. [sent-182, score-0.495]
</p><p>51 The subject and direct object of a verb should not both receive high probability mass under the same schema slot δs. [sent-185, score-0.665]
</p><p>52 5  Entity Extraction for Template Filling  Inducing event schemas is only one benefit of the model. [sent-194, score-0.58]
</p><p>53 The learned model can also extract specific instances of the learned schemas without additional complexity. [sent-195, score-0.496]
</p><p>54 To evaluate the effectiveness of the model, we apply the model to perform standard template extraction on MUC-4. [sent-196, score-0.329]
</p><p>55 Previous MUC-4 induction required an extraction algorithm separate from induction because induction created hard clusters (Chambers and Jurafsky, 2011). [sent-197, score-0.399]
</p><p>56 We run inference as described above and each entity receives a template label te,d and a template slot label se,d. [sent-200, score-0.878]
</p><p>57 They instead focus on four main slots, ignoring the parameterized slots that involve deeper reasoning (such as ‘stage of execution’ and ‘effect of incident’). [sent-216, score-0.389]
</p><p>58 The four slots and example entity fillers are shown here: Perpetrator: Shining Path members Victim: Sergio Horna Target: public facilities  Instrument:  explosives  1802 We also focus only on these four slots. [sent-217, score-0.574]
</p><p>59 We merged MUC’s two perpetrator slots (individuals and orgs) into one gold Perpetrator. [sent-218, score-0.541]
</p><p>60 This is also consistent with the most recent event schema induction in Chambers and Jurafsky (201 1) and Cheung et al. [sent-221, score-0.756]
</p><p>61 , all its slots are optional), and some required templates contain optional slots (i. [sent-225, score-0.912]
</p><p>62 We ignore both optional templates and specific optional slots when computing recall, as in previous work (Patwardhan and Riloff, 2007; Patwardhan and Riloff, 2009; Chambers and Jurafsky, 2011). [sent-228, score-0.556]
</p><p>63 Comparison between the extracted strings and the gold template strings uses head word scoring. [sent-229, score-0.344]
</p><p>64 2 Mapping Learned Slots Induced schemas need to map to gold schemas before evaluation. [sent-235, score-0.829]
</p><p>65 The first ignores the schema type variables t, and simply finds the best performing s variable for each gold template slot2. [sent-238, score-0.868]
</p><p>66 The second approach is to map each template variable t to the best gold template type g, and limit the slot mapping so that only the slots under t can map to slots under g. [sent-240, score-1.713]
</p><p>67 The slot-only mapping can result in higher scores since it is not constrained to preserve schema structure in the mapping. [sent-242, score-0.497]
</p><p>68 Chambers and Jurafsky (201 1) used template mapping in their evaluation. [sent-243, score-0.305]
</p><p>69 µ  1Personal communications with Patwardhan and Riloff 2bombing-victim is a template slot distinct from kidnapvictim. [sent-247, score-0.444]
</p><p>70 There are two structure variables for the model: the number of schema types and the number of slots under each type. [sent-254, score-0.884]
</p><p>71 1 Template Schema Induction The first evaluation compares the learned schemas to the gold schemas in MUC-4. [sent-259, score-0.858]
</p><p>72 Since most previous work assumes this knowledge ahead of time, we align our schemas with the  main MUC-4 template types to measure quality. [sent-260, score-0.625]
</p><p>73 We inspected the learned event schemas that mapped to MUC-4 schemas based on the template mapping extraction evaluation. [sent-261, score-1.392]
</p><p>74 The predicate distribution for each event schema is shown, as well as the top 5 head words and grammatical relations for each slot. [sent-263, score-0.768]
</p><p>75 The bombing and kidnap schemas learned all of the equivalent MUC-4 gold slots. [sent-265, score-0.615]
</p><p>76 Figure 6 lists the MUC-4 slots that we did and did not learn for the four most prevelant types. [sent-268, score-0.389]
</p><p>77 IPTDLVnoaeircstpgaeritum/TroimnaetorBX mbKiX d-napAtX xackArXx son  1803  Figure 6: The MUC-4 gold slots that were learned. [sent-273, score-0.444]
</p><p>78 A learned schema first maps to a gold MUC template. [sent-282, score-0.565]
</p><p>79 Learned slots can then only map to slots in that template. [sent-283, score-0.812]
</p><p>80 Although our learned schemas closely match gold schemas, extraction depends on how well the model can extract from diverse lexical contexts. [sent-286, score-0.562]
</p><p>81 We ran inference on the full training and test sets, and used the inferred labels as schema labels. [sent-287, score-0.447]
</p><p>82 Table 1 shows the template mapping evaluation with Chambers and Jurafsky (C&J;). [sent-291, score-0.305]
</p><p>83 For each MUC-4 type, such as bombing, any four learned slots can map to the four MUC4 bombing slots. [sent-296, score-0.613]
</p><p>84 There is no constraint that the  learned slots must come from the same schema type. [sent-297, score-0.927]
</p><p>85 The more strict template mapping (Table 1) ensures that entire schema types are mapped together, and it reduces our performance from . [sent-298, score-0.752]
</p><p>86 Left columns are head word distributions β, right columns are syntactic relation distributions δ, and entity types in parentheses are the learned γ. [sent-302, score-0.325]
</p><p>87 Any Evaluation: Slot-Only Mapping  learned slot is allowed to map to any gold slot. [sent-309, score-0.341]
</p><p>88 3  Model Ablation  Table 2 shows that the flat relation model (no latent type variables t) is inferior to the full schema model. [sent-334, score-0.615]
</p><p>89 F1 drops 20% without the explicit modeling of both schema types t and their entity slots s. [sent-335, score-0.979]
</p><p>90 However, it is extremely useful to learn slots with NER labels like Person or Location. [sent-338, score-0.389]
</p><p>91 Performance drops 5-10% depending on the number of schemas learned. [sent-341, score-0.37]
</p><p>92 Anecdotally, it merges too many schema slots that should be separate. [sent-342, score-0.836]
</p><p>93 We thus attempted to induce and extract event schemas from just the 200 test set documents, with no training or development data. [sent-348, score-0.58]
</p><p>94 We repeated this experiment 30 times and averaged the results, setting the number of templates t = 20 and slots s = 10 as in the main experiment. [sent-349, score-0.467]
</p><p>95 7  Discussion  Our model is one of the first generative formulations of schema induction. [sent-357, score-0.525]
</p><p>96 Here we are the first to show how it can be used for schema induction in a probabilistic model, connecting predicates across a document in a way that is otherwise difficult to represent. [sent-362, score-0.633]
</p><p>97 Our model’s inference procedure to learn schemas is the same one that labels text for extraction. [sent-365, score-0.37]
</p><p>98 also include a hidden event variable between the template and slot variables. [sent-376, score-0.69]
</p><p>99 There are many ways to map induced schemas to  gold answers, and this paper illustrates how extraction performance is significantly affected by the choice. [sent-381, score-0.533]
</p><p>100 There is ample room for improvement and future research in event schema induction. [sent-384, score-0.657]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('schema', 0.447), ('slots', 0.389), ('schemas', 0.37), ('template', 0.255), ('event', 0.21), ('cheung', 0.19), ('slot', 0.189), ('entity', 0.143), ('bombing', 0.127), ('induction', 0.099), ('perpetrator', 0.097), ('riloff', 0.096), ('patwardhan', 0.094), ('chambers', 0.094), ('incident', 0.083), ('generative', 0.078), ('pipelined', 0.074), ('extraction', 0.074), ('mentions', 0.072), ('jurafsky', 0.072), ('multinomial', 0.07), ('victim', 0.069), ('flat', 0.066), ('arson', 0.064), ('yed', 0.064), ('learned', 0.063), ('optional', 0.061), ('bomb', 0.059), ('gold', 0.055), ('instrument', 0.053), ('documents', 0.052), ('events', 0.051), ('mapping', 0.05), ('coreference', 0.049), ('inv', 0.048), ('kidnapping', 0.048), ('militia', 0.048), ('variables', 0.048), ('carlson', 0.047), ('document', 0.047), ('dir', 0.045), ('templates', 0.045), ('predicate', 0.045), ('entities', 0.044), ('person', 0.044), ('roles', 0.044), ('message', 0.043), ('prep', 0.042), ('fillers', 0.042), ('shining', 0.042), ('banko', 0.04), ('predicates', 0.04), ('fe', 0.04), ('ellen', 0.039), ('irrelevant', 0.039), ('truck', 0.038), ('sekine', 0.038), ('variable', 0.036), ('receives', 0.036), ('filatova', 0.035), ('pt', 0.035), ('map', 0.034), ('head', 0.034), ('repeated', 0.033), ('relations', 0.032), ('labeled', 0.032), ('chinchor', 0.032), ('eenacehra', 0.032), ('embassy', 0.032), ('entet', 0.032), ('explosive', 0.032), ('guerrilla', 0.032), ('hruschka', 0.032), ('maslennikov', 0.032), ('perp', 0.032), ('rau', 0.032), ('sudo', 0.032), ('yangarber', 0.032), ('attack', 0.032), ('satoshi', 0.032), ('object', 0.029), ('durme', 0.029), ('traditionally', 0.029), ('distributions', 0.029), ('constraint', 0.028), ('formal', 0.028), ('coreferring', 0.028), ('betteridge', 0.028), ('required', 0.028), ('type', 0.027), ('election', 0.027), ('proposal', 0.027), ('removes', 0.027), ('typed', 0.027), ('relation', 0.027), ('plate', 0.026), ('frames', 0.026), ('vote', 0.026), ('naval', 0.025), ('chieu', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="75-tfidf-1" href="./emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model.html">75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</a></p>
<p>Author: Nathanael Chambers</p><p>Abstract: Event schema induction is the task of learning high-level representations of complex events (e.g., a bombing) and their entity roles (e.g., perpetrator and victim) from unlabeled text. Event schemas have important connections to early NLP research on frames and scripts, as well as modern applications like template extraction. Recent research suggests event schemas can be learned from raw text. Inspired by a pipelined learner based on named entity coreference, this paper presents the first generative model for schema induction that integrates coreference chains into learning. Our generative model is conceptually simpler than the pipelined approach and requires far less training data. It also provides an interesting contrast with a recent HMM-based model. We evaluate on a common dataset for template schema extraction. Our generative model matches the pipeline’s performance, and outperforms the HMM by 7 F1 points (20%).</p><p>2 0.46133298 <a title="75-tfidf-2" href="./emnlp-2013-Generating_Coherent_Event_Schemas_at_Scale.html">90 emnlp-2013-Generating Coherent Event Schemas at Scale</a></p>
<p>Author: Niranjan Balasubramanian ; Stephen Soderland ; Mausam ; Oren Etzioni</p><p>Abstract: Chambers and Jurafsky (2009) demonstrated that event schemas can be automatically induced from text corpora. However, our analysis of their schemas identifies several weaknesses, e.g., some schemas lack a common topic and distinct roles are incorrectly mixed into a single actor. It is due in part to their pair-wise representation that treats subjectverb independently from verb-object. This often leads to subject-verb-object triples that are not meaningful in the real-world. We present a novel approach to inducing open-domain event schemas that overcomes these limitations. Our approach uses cooccurrence statistics of semantically typed relational triples, which we call Rel-grams (relational n-grams). In a human evaluation, our schemas outperform Chambers’s schemas by wide margins on several evaluation criteria. Both Rel-grams and event schemas are freely available to the research community.</p><p>3 0.14798017 <a title="75-tfidf-3" href="./emnlp-2013-Learning_Biological_Processes_with_Global_Constraints.html">118 emnlp-2013-Learning Biological Processes with Global Constraints</a></p>
<p>Author: Aju Thalappillil Scaria ; Jonathan Berant ; Mengqiu Wang ; Peter Clark ; Justin Lewis ; Brittany Harding ; Christopher D. Manning</p><p>Abstract: Biological processes are complex phenomena involving a series of events that are related to one another through various relationships. Systems that can understand and reason over biological processes would dramatically improve the performance of semantic applications involving inference such as question answering (QA) – specifically “How? ” and “Why? ” questions. In this paper, we present the task of process extraction, in which events within a process and the relations between the events are automatically extracted from text. We represent processes by graphs whose edges describe a set oftemporal, causal and co-reference event-event relations, and characterize the structural properties of these graphs (e.g., the graphs are connected). Then, we present a method for extracting relations between the events, which exploits these structural properties by performing joint in- ference over the set of extracted relations. On a novel dataset containing 148 descriptions of biological processes (released with this paper), we show significant improvement comparing to baselines that disregard process structure.</p><p>4 0.13685657 <a title="75-tfidf-4" href="./emnlp-2013-Unsupervised_Induction_of_Contingent_Event_Pairs_from_Film_Scenes.html">192 emnlp-2013-Unsupervised Induction of Contingent Event Pairs from Film Scenes</a></p>
<p>Author: Zhichao Hu ; Elahe Rahimtoroghi ; Larissa Munishkina ; Reid Swanson ; Marilyn A. Walker</p><p>Abstract: Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning. Researchers in NLP have tackled modeling such expectations from a range of perspectives, including treating it as the inference of the CONTINGENT discourse relation, or as a type of common-sense causal reasoning. Our approach is to model likelihood between events by drawing on several of these lines of previous work. We implement and evaluate different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another. We refine event pairs that we learn from a corpus of film scene descriptions utilizing web search counts, and evaluate our results by collecting human judgments ofcontingency. Our results indicate that the use of web search counts increases the av- , erage accuracy of our best method to 85.64% over a baseline of 50%, as compared to an average accuracy of 75. 15% without web search.</p><p>5 0.13193534 <a title="75-tfidf-5" href="./emnlp-2013-A_Unified_Model_for_Topics%2C_Events_and_Users_on_Twitter.html">16 emnlp-2013-A Unified Model for Topics, Events and Users on Twitter</a></p>
<p>Author: Qiming Diao ; Jing Jiang</p><p>Abstract: With the rapid growth of social media, Twitter has become one of the most widely adopted platforms for people to post short and instant message. On the one hand, people tweets about their daily lives, and on the other hand, when major events happen, people also follow and tweet about them. Moreover, people’s posting behaviors on events are often closely tied to their personal interests. In this paper, we try to model topics, events and users on Twitter in a unified way. We propose a model which combines an LDA-like topic model and the Recurrent Chinese Restaurant Process to capture topics and events. We further propose a duration-based regularization component to find bursty events. We also propose to use event-topic affinity vectors to model the asso- . ciation between events and topics. Our experiments shows that our model can accurately identify meaningful events and the event-topic affinity vectors are effective for event recommendation and grouping events by topics.</p><p>6 0.11637995 <a title="75-tfidf-6" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>7 0.11175046 <a title="75-tfidf-7" href="./emnlp-2013-Optimized_Event_Storyline_Generation_based_on_Mixture-Event-Aspect_Model.html">147 emnlp-2013-Optimized Event Storyline Generation based on Mixture-Event-Aspect Model</a></p>
<p>8 0.10184333 <a title="75-tfidf-8" href="./emnlp-2013-Easy_Victories_and_Uphill_Battles_in_Coreference_Resolution.html">67 emnlp-2013-Easy Victories and Uphill Battles in Coreference Resolution</a></p>
<p>9 0.10023175 <a title="75-tfidf-9" href="./emnlp-2013-Event-Based_Time_Label_Propagation_for_Automatic_Dating_of_News_Articles.html">74 emnlp-2013-Event-Based Time Label Propagation for Automatic Dating of News Articles</a></p>
<p>10 0.095343575 <a title="75-tfidf-10" href="./emnlp-2013-Building_Event_Threads_out_of_Multiple_News_Articles.html">41 emnlp-2013-Building Event Threads out of Multiple News Articles</a></p>
<p>11 0.093398362 <a title="75-tfidf-11" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>12 0.090030901 <a title="75-tfidf-12" href="./emnlp-2013-Improving_Web_Search_Ranking_by_Incorporating_Structured_Annotation_of_Queries.html">105 emnlp-2013-Improving Web Search Ranking by Incorporating Structured Annotation of Queries</a></p>
<p>13 0.08961235 <a title="75-tfidf-13" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>14 0.086412787 <a title="75-tfidf-14" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>15 0.083618283 <a title="75-tfidf-15" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>16 0.081459865 <a title="75-tfidf-16" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>17 0.071968853 <a title="75-tfidf-17" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>18 0.070403486 <a title="75-tfidf-18" href="./emnlp-2013-Joint_Coreference_Resolution_and_Named-Entity_Linking_with_Multi-Pass_Sieves.html">112 emnlp-2013-Joint Coreference Resolution and Named-Entity Linking with Multi-Pass Sieves</a></p>
<p>19 0.069488272 <a title="75-tfidf-19" href="./emnlp-2013-Combining_Generative_and_Discriminative_Model_Scores_for_Distant_Supervision.html">49 emnlp-2013-Combining Generative and Discriminative Model Scores for Distant Supervision</a></p>
<p>20 0.069316372 <a title="75-tfidf-20" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.209), (1, 0.203), (2, 0.056), (3, 0.185), (4, 0.002), (5, -0.106), (6, -0.189), (7, 0.109), (8, 0.036), (9, -0.005), (10, 0.088), (11, -0.136), (12, -0.05), (13, -0.075), (14, -0.094), (15, -0.163), (16, 0.074), (17, -0.208), (18, 0.13), (19, -0.296), (20, 0.197), (21, 0.175), (22, 0.221), (23, -0.053), (24, -0.048), (25, -0.163), (26, -0.017), (27, 0.029), (28, 0.06), (29, -0.143), (30, 0.111), (31, 0.102), (32, -0.021), (33, 0.009), (34, -0.103), (35, -0.043), (36, -0.002), (37, -0.027), (38, -0.049), (39, -0.051), (40, 0.106), (41, 0.027), (42, -0.08), (43, -0.042), (44, -0.047), (45, -0.001), (46, -0.006), (47, -0.022), (48, -0.019), (49, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9455424 <a title="75-lsi-1" href="./emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model.html">75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</a></p>
<p>Author: Nathanael Chambers</p><p>Abstract: Event schema induction is the task of learning high-level representations of complex events (e.g., a bombing) and their entity roles (e.g., perpetrator and victim) from unlabeled text. Event schemas have important connections to early NLP research on frames and scripts, as well as modern applications like template extraction. Recent research suggests event schemas can be learned from raw text. Inspired by a pipelined learner based on named entity coreference, this paper presents the first generative model for schema induction that integrates coreference chains into learning. Our generative model is conceptually simpler than the pipelined approach and requires far less training data. It also provides an interesting contrast with a recent HMM-based model. We evaluate on a common dataset for template schema extraction. Our generative model matches the pipeline’s performance, and outperforms the HMM by 7 F1 points (20%).</p><p>2 0.94194424 <a title="75-lsi-2" href="./emnlp-2013-Generating_Coherent_Event_Schemas_at_Scale.html">90 emnlp-2013-Generating Coherent Event Schemas at Scale</a></p>
<p>Author: Niranjan Balasubramanian ; Stephen Soderland ; Mausam ; Oren Etzioni</p><p>Abstract: Chambers and Jurafsky (2009) demonstrated that event schemas can be automatically induced from text corpora. However, our analysis of their schemas identifies several weaknesses, e.g., some schemas lack a common topic and distinct roles are incorrectly mixed into a single actor. It is due in part to their pair-wise representation that treats subjectverb independently from verb-object. This often leads to subject-verb-object triples that are not meaningful in the real-world. We present a novel approach to inducing open-domain event schemas that overcomes these limitations. Our approach uses cooccurrence statistics of semantically typed relational triples, which we call Rel-grams (relational n-grams). In a human evaluation, our schemas outperform Chambers’s schemas by wide margins on several evaluation criteria. Both Rel-grams and event schemas are freely available to the research community.</p><p>3 0.43875986 <a title="75-lsi-3" href="./emnlp-2013-Unsupervised_Induction_of_Contingent_Event_Pairs_from_Film_Scenes.html">192 emnlp-2013-Unsupervised Induction of Contingent Event Pairs from Film Scenes</a></p>
<p>Author: Zhichao Hu ; Elahe Rahimtoroghi ; Larissa Munishkina ; Reid Swanson ; Marilyn A. Walker</p><p>Abstract: Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning. Researchers in NLP have tackled modeling such expectations from a range of perspectives, including treating it as the inference of the CONTINGENT discourse relation, or as a type of common-sense causal reasoning. Our approach is to model likelihood between events by drawing on several of these lines of previous work. We implement and evaluate different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another. We refine event pairs that we learn from a corpus of film scene descriptions utilizing web search counts, and evaluate our results by collecting human judgments ofcontingency. Our results indicate that the use of web search counts increases the av- , erage accuracy of our best method to 85.64% over a baseline of 50%, as compared to an average accuracy of 75. 15% without web search.</p><p>4 0.36696672 <a title="75-lsi-4" href="./emnlp-2013-Learning_Biological_Processes_with_Global_Constraints.html">118 emnlp-2013-Learning Biological Processes with Global Constraints</a></p>
<p>Author: Aju Thalappillil Scaria ; Jonathan Berant ; Mengqiu Wang ; Peter Clark ; Justin Lewis ; Brittany Harding ; Christopher D. Manning</p><p>Abstract: Biological processes are complex phenomena involving a series of events that are related to one another through various relationships. Systems that can understand and reason over biological processes would dramatically improve the performance of semantic applications involving inference such as question answering (QA) – specifically “How? ” and “Why? ” questions. In this paper, we present the task of process extraction, in which events within a process and the relations between the events are automatically extracted from text. We represent processes by graphs whose edges describe a set oftemporal, causal and co-reference event-event relations, and characterize the structural properties of these graphs (e.g., the graphs are connected). Then, we present a method for extracting relations between the events, which exploits these structural properties by performing joint in- ference over the set of extracted relations. On a novel dataset containing 148 descriptions of biological processes (released with this paper), we show significant improvement comparing to baselines that disregard process structure.</p><p>5 0.36136112 <a title="75-lsi-5" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>Author: Oier Lopez de Lacalle ; Mirella Lapata</p><p>Abstract: In this paper we present an unsupervised approach to relational information extraction. Our model partitions tuples representing an observed syntactic relationship between two named entities (e.g., “X was born in Y” and “X is from Y”) into clusters corresponding to underlying semantic relation types (e.g., BornIn, Located). Our approach incorporates general domain knowledge which we encode as First Order Logic rules and automatically combine with a topic model developed specifically for the relation extraction task. Evaluation results on the ACE 2007 English Relation Detection and Categorization (RDC) task show that our model outperforms competitive unsupervised approaches by a wide margin and is able to produce clusters shaped by both the data and the rules.</p><p>6 0.34385478 <a title="75-lsi-6" href="./emnlp-2013-Rule-Based_Information_Extraction_is_Dead%21_Long_Live_Rule-Based_Information_Extraction_Systems%21.html">161 emnlp-2013-Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!</a></p>
<p>7 0.31622639 <a title="75-lsi-7" href="./emnlp-2013-A_Unified_Model_for_Topics%2C_Events_and_Users_on_Twitter.html">16 emnlp-2013-A Unified Model for Topics, Events and Users on Twitter</a></p>
<p>8 0.27972344 <a title="75-lsi-8" href="./emnlp-2013-Optimized_Event_Storyline_Generation_based_on_Mixture-Event-Aspect_Model.html">147 emnlp-2013-Optimized Event Storyline Generation based on Mixture-Event-Aspect Model</a></p>
<p>9 0.26719368 <a title="75-lsi-9" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>10 0.2659654 <a title="75-lsi-10" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>11 0.2584171 <a title="75-lsi-11" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>12 0.2583513 <a title="75-lsi-12" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>13 0.25307944 <a title="75-lsi-13" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>14 0.23575337 <a title="75-lsi-14" href="./emnlp-2013-Event-Based_Time_Label_Propagation_for_Automatic_Dating_of_News_Articles.html">74 emnlp-2013-Event-Based Time Label Propagation for Automatic Dating of News Articles</a></p>
<p>15 0.2308414 <a title="75-lsi-15" href="./emnlp-2013-Building_Event_Threads_out_of_Multiple_News_Articles.html">41 emnlp-2013-Building Event Threads out of Multiple News Articles</a></p>
<p>16 0.21990202 <a title="75-lsi-16" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>17 0.21778561 <a title="75-lsi-17" href="./emnlp-2013-Improving_Web_Search_Ranking_by_Incorporating_Structured_Annotation_of_Queries.html">105 emnlp-2013-Improving Web Search Ranking by Incorporating Structured Annotation of Queries</a></p>
<p>18 0.21558625 <a title="75-lsi-18" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>19 0.21233627 <a title="75-lsi-19" href="./emnlp-2013-Joint_Coreference_Resolution_and_Named-Entity_Linking_with_Multi-Pass_Sieves.html">112 emnlp-2013-Joint Coreference Resolution and Named-Entity Linking with Multi-Pass Sieves</a></p>
<p>20 0.20576338 <a title="75-lsi-20" href="./emnlp-2013-Inducing_Document_Plans_for_Concept-to-Text_Generation.html">106 emnlp-2013-Inducing Document Plans for Concept-to-Text Generation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.027), (9, 0.016), (18, 0.042), (22, 0.059), (30, 0.062), (38, 0.295), (50, 0.01), (51, 0.151), (66, 0.034), (71, 0.029), (75, 0.065), (77, 0.023), (96, 0.052), (97, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77281284 <a title="75-lda-1" href="./emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model.html">75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</a></p>
<p>Author: Nathanael Chambers</p><p>Abstract: Event schema induction is the task of learning high-level representations of complex events (e.g., a bombing) and their entity roles (e.g., perpetrator and victim) from unlabeled text. Event schemas have important connections to early NLP research on frames and scripts, as well as modern applications like template extraction. Recent research suggests event schemas can be learned from raw text. Inspired by a pipelined learner based on named entity coreference, this paper presents the first generative model for schema induction that integrates coreference chains into learning. Our generative model is conceptually simpler than the pipelined approach and requires far less training data. It also provides an interesting contrast with a recent HMM-based model. We evaluate on a common dataset for template schema extraction. Our generative model matches the pipeline’s performance, and outperforms the HMM by 7 F1 points (20%).</p><p>2 0.68021584 <a title="75-lda-2" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>Author: Dimitri Kartsaklis ; Mehrnoosh Sadrzadeh</p><p>Abstract: Recent work has shown that compositionaldistributional models using element-wise operations on contextual word vectors benefit from the introduction of a prior disambiguation step. The purpose of this paper is to generalise these ideas to tensor-based models, where relational words such as verbs and adjectives are represented by linear maps (higher order tensors) acting on a number of arguments (vectors). We propose disambiguation algorithms for a number of tensor-based models, which we then test on a variety of tasks. The results show that disambiguation can provide better compositional representation even for the case of tensor-based models. Further- more, we confirm previous findings regarding the positive effect of disambiguation on vector mixture models, and we compare the effectiveness of the two approaches.</p><p>3 0.5527876 <a title="75-lda-3" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>Author: Zhongqing Wang ; Shoushan LI ; Fang Kong ; Guodong Zhou</p><p>Abstract: Personal profile information on social media like LinkedIn.com and Facebook.com is at the core of many interesting applications, such as talent recommendation and contextual advertising. However, personal profiles usually lack organization confronted with the large amount of available information. Therefore, it is always a challenge for people to find desired information from them. In this paper, we address the task of personal profile summarization by leveraging both personal profile textual information and social networks. Here, using social networks is motivated by the intuition that, people with similar academic, business or social connections (e.g. co-major, co-university, and cocorporation) tend to have similar experience and summaries. To achieve the learning process, we propose a collective factor graph (CoFG) model to incorporate all these resources of knowledge to summarize personal profiles with local textual attribute functions and social connection factors. Extensive evaluation on a large-scale dataset from LinkedIn.com demonstrates the effectiveness of the proposed approach. 1</p><p>4 0.54451966 <a title="75-lda-4" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>Author: Jason Weston ; Antoine Bordes ; Oksana Yakhnenko ; Nicolas Usunier</p><p>Abstract: This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over methods that rely on text features alone.</p><p>5 0.54340476 <a title="75-lda-5" href="./emnlp-2013-Exploiting_Zero_Pronouns_to_Improve_Chinese_Coreference_Resolution.html">80 emnlp-2013-Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</a></p>
<p>Author: Fang Kong ; Hwee Tou Ng</p><p>Abstract: Coreference resolution plays a critical role in discourse analysis. This paper focuses on exploiting zero pronouns to improve Chinese coreference resolution. In particular, a simplified semantic role labeling framework is proposed to identify clauses and to detect zero pronouns effectively, and two effective methods (refining syntactic parser and refining learning example generation) are employed to exploit zero pronouns for Chinese coreference resolution. Evaluation on the CoNLL-2012 shared task data set shows that zero pronouns can significantly improve Chinese coreference resolution.</p><p>6 0.53982002 <a title="75-lda-6" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>7 0.53955191 <a title="75-lda-7" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>8 0.53799683 <a title="75-lda-8" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>9 0.53702915 <a title="75-lda-9" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>10 0.53661203 <a title="75-lda-10" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>11 0.53480244 <a title="75-lda-11" href="./emnlp-2013-Unsupervised_Induction_of_Cross-Lingual_Semantic_Relations.html">193 emnlp-2013-Unsupervised Induction of Cross-Lingual Semantic Relations</a></p>
<p>12 0.53334248 <a title="75-lda-12" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>13 0.53250861 <a title="75-lda-13" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>14 0.53207481 <a title="75-lda-14" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>15 0.53172112 <a title="75-lda-15" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>16 0.53133017 <a title="75-lda-16" href="./emnlp-2013-A_Unified_Model_for_Topics%2C_Events_and_Users_on_Twitter.html">16 emnlp-2013-A Unified Model for Topics, Events and Users on Twitter</a></p>
<p>17 0.53096008 <a title="75-lda-17" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>18 0.53024399 <a title="75-lda-18" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>19 0.52985632 <a title="75-lda-19" href="./emnlp-2013-Microblog_Entity_Linking_by_Leveraging_Extra_Posts.html">130 emnlp-2013-Microblog Entity Linking by Leveraging Extra Posts</a></p>
<p>20 0.52922481 <a title="75-lda-20" href="./emnlp-2013-Document_Summarization_via_Guided_Sentence_Compression.html">65 emnlp-2013-Document Summarization via Guided Sentence Compression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
