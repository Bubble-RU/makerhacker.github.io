<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>54 emnlp-2013-Decipherment with a Million Random Restarts</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-54" href="#">emnlp2013-54</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>54 emnlp-2013-Decipherment with a Million Random Restarts</h1>
<br/><p>Source: <a title="emnlp-2013-54-pdf" href="http://aclweb.org/anthology//D/D13/D13-1087.pdf">pdf</a></p><p>Author: Taylor Berg-Kirkpatrick ; Dan Klein</p><p>Abstract: This paper investigates the utility and effect of running numerous random restarts when using EM to attack decipherment problems. We find that simple decipherment models are able to crack homophonic substitution ciphers with high accuracy if a large number of random restarts are used but almost completely fail with only a few random restarts. For particularly difficult homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340.</p><p>Reference: <a title="emnlp-2013-54-reference" href="../emnlp2013_reference/emnlp-2013-Decipherment_with_a_Million_Random_Restarts_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  ,  Abstract This paper investigates the utility and effect of running numerous random restarts when using EM to attack decipherment problems. [sent-3, score-0.752]
</p><p>2 We find that simple decipherment models are able to crack homophonic substitution ciphers with high accuracy if a large number of random restarts are used but almost completely fail with only a few random restarts. [sent-4, score-1.337]
</p><p>3 For particularly difficult homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. [sent-5, score-0.392]
</p><p>4 We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340. [sent-6, score-0.729]
</p><p>5 1 Introduction What can a million restarts do for decipherment? [sent-7, score-0.474]
</p><p>6 EM frequently gets stuck in local optima, so running between ten and a hundred random restarts is common practice (Knight et al. [sent-8, score-0.641]
</p><p>7 But, how important are random restarts and how many random restarts does it take to saturate gains in accuracy? [sent-10, score-1.135]
</p><p>8 We look at both Zodiac 408, a famous homophonic substitution cipher, and a more difficult homophonic cipher constructed to match properties of the famously unsolved Zodiac 340. [sent-12, score-1.034]
</p><p>9 Gains in accuracy saturate after only a hundred random restarts for Zodiac 408, but for the constructed cipher we see large gains 874 in accuracy even as we scale the number of random restarts up into the hundred thousands. [sent-13, score-1.716]
</p><p>10 In both cases the difference between few and many random restarts is the difference between almost complete failure and successful decipherment. [sent-14, score-0.543]
</p><p>11 We also find that millions of random restarts can be helpful for performing exploratory analysis. [sent-15, score-0.575]
</p><p>12 We look at some empirical properties of decipherment problems, visualizing the distribution of local op-  tima encountered by EM both in a successful decipherment of a homophonic cipher and in an unsuccessful attempt to decipher Zodiac 340. [sent-16, score-1.073]
</p><p>13 Finally, we attack a series of ciphers generated to match properties of Zodiac 340 and use the results to argue that Zodiac 340 is likely not a homophonic cipher under the commonly assumed linearization order. [sent-17, score-0.942]
</p><p>14 2  Decipherment Model  Various types of ciphers have been tackled by the NLP community with great success (Knight et al. [sent-18, score-0.193]
</p><p>15 Many of these approaches learn an encryption key by maximizing the score of the decrypted message under a language model. [sent-21, score-0.092]
</p><p>16 We focus on homophonic substitution ciphers, where the encryption key is a 1-to-many mapping from a plaintext alphabet to a cipher alphabet. [sent-22, score-0.815]
</p><p>17 , 1977) is used to learn the emission parameters of an HMM that has a character trigram language model as a backbone and the ciphertext as the observed sequence of emissions. [sent-25, score-0.125]
</p><p>18 This means that we learn a multinomial over cipher symbols for  each plaintext character, but do not learn transition ProceSe datintlges, o Wfa tsh ein 2g01to3n, C UoSnfAe,re 1n8c-e2 o1n O Ecmtopbier ic 2a0l1 M3. [sent-26, score-0.558]
</p><p>19 We predict the deciphered text using posterior decoding in the learned HMM. [sent-29, score-0.026]
</p><p>20 1 Implementation Running multiple random restarts means running EM to convergence multiple times, which can be computationally intensive; luckily, restarts can be run in parallel. [sent-31, score-1.035]
</p><p>21 We implemented EM with parallel random restarts using the CUDA API (Nickolls et al. [sent-33, score-0.543]
</p><p>22 With a GPU workstation,1 we can complete a million random restarts roughly a thousand times more quickly than we can complete the same computation with a serial implementation on a CPU. [sent-35, score-0.577]
</p><p>23 3  Experiments  We ran experiments  on several homophonic  sub-  stitution ciphers: some produced by the infamous Zodiac killer and others that were automatically generated to be similar to the Zodiac ciphers. [sent-36, score-0.289]
</p><p>24 In each of these experiments, we ran numerous random restarts; and in all cases we chose the random restart that attained the highest model score in order to produce the final decode. [sent-37, score-0.296]
</p><p>25 1 Experimental Setup The specifics of how random restarts are produced is usually considered a detail; however, in this work it is important to describe the process precisely. [sent-39, score-0.543]
</p><p>26 In order to generate random restarts, we sampled emission parameters by drawing uniformly at random from the interval [0, 1] and then normalizing. [sent-40, score-0.223]
</p><p>27 The corresponding distribution on the multinomial emission parameters is mildly concentrated at the center of the simplex. [sent-41, score-0.073]
</p><p>28 2 For each random restart, we ran EM for 200 itera1We used a single workstation with three NVIDIA GTX 580 GPUs. [sent-42, score-0.102]
</p><p>29 2We also ran experiments where emission parameters were drawn from Dirichlet distributions with various concentration parameter settings. [sent-44, score-0.059]
</p><p>30 We noticed little effect so long as the distribution did not favor the corners of the simplex. [sent-45, score-0.044]
</p><p>31 If the distribu-  tion did favor the corners of the simplex, decipherment results deteriorated sharply. [sent-46, score-0.188]
</p><p>32 875  Number of random restarts Figure 1: Zodiac 408 cipher. [sent-47, score-0.543]
</p><p>33 Accuracy by best model score and best model score vs. [sent-48, score-0.04]
</p><p>34 2 An Easy Cipher: Zodiac 408 Zodiac 408 is a homophonic cipher that is 408 characters long and contains 54 different cipher symbols. [sent-58, score-1.147]
</p><p>35 Produced by the Zodiac killer, this cipher was solved, manually, by two amateur code-breakers a week after its release to the public in 1969. [sent-59, score-0.451]
</p><p>36 Ravi and Knight (201 1) were the first to crack Zodiac 408 using completely automatic methods. [sent-60, score-0.089]
</p><p>37 In our first experiment, we compare a decode of Zodiac 408 using one random restart to a decode using 100 random restarts. [sent-61, score-0.317]
</p><p>38 Random restarts have high 3While this does not guarantee convergence, in practice 200 iterations seems to be sufficient for the problems we looked at. [sent-62, score-0.487]
</p><p>39 4The interpolation between n-gram orders is uniform, and the interpolation between corpora favors the Zodiac corpus with weight 0. [sent-63, score-0.03]
</p><p>40 variance, so when we present the accuracy corresponding to a given number of restarts we present an average over many bootstrap samples, drawn from a set of one million random restarts. [sent-65, score-0.595]
</p><p>41 If we attack Zodiac 408 with a single random restart, on average we achieve an accuracy of 18%. [sent-66, score-0.16]
</p><p>42 If we instead use 100 random restarts we achieve a much better average accuracy of 90%. [sent-67, score-0.58]
</p><p>43 The accuracies for various numbers of random restarts are plotted in Figure 1. [sent-68, score-0.543]
</p><p>44 Based on these results, we expect accuracy to increase by about 72% when using 100 random restarts instead of a single random restart; however, using more than 100 random restarts for this particular cipher does not appear to be useful. [sent-69, score-1.658]
</p><p>45 Also in Figure 1, we plot a related graph, this time showing the effect that random restarts have on the achieved model score. [sent-70, score-0.569]
</p><p>46 By construction, the (maximum) model score must increase as we increase the number of random restarts. [sent-71, score-0.104]
</p><p>47 We see that it quickly saturates in the same way that accuracy did. [sent-72, score-0.056]
</p><p>48 This raises the question: have we actually  achieved the globally optimal model score or have we only saturated the usefulness of random restarts? [sent-73, score-0.134]
</p><p>49 We can’t prove that we have achieved the global optimum,5 but we can at least check that we have surpassed the model score achieved by EM when it is initialized with the gold encryption key. [sent-74, score-0.152]
</p><p>50 On Zodiac 408, if we initialize with the gold key, EM finds a local optimum with a model score of −1467. [sent-75, score-0.161]
</p><p>51 The accuracy after gold initialization was 92%, while the accuracy of the best local optimum was only 89%. [sent-81, score-0.229]
</p><p>52 This suggests that the global optimum may not be worth finding if we haven’t already found it. [sent-82, score-0.069]
</p><p>53 From Figure 1, it appears that large increases in likelihood are correlated with increases in accuracy, but small improvements to high likelihoods (e. [sent-83, score-0.084]
</p><p>54 the best local optimum versus the gold initialization) may not to be. [sent-85, score-0.141]
</p><p>55 876  Number of random restarts Figure 2: Synth 340 cipher. [sent-87, score-0.543]
</p><p>56 Accuracy by best model score and best model score vs. [sent-88, score-0.04]
</p><p>57 Zodiac 340 is the second cipher released by the Zodiac killer, and it remains unsolved to this day. [sent-93, score-0.486]
</p><p>58 However, it is unknown whether Zodiac 340 is actually a homophonic cipher. [sent-94, score-0.232]
</p><p>59 If it were a homophonic cipher we would certainly expect it to be harder than Zodiac 408 because Zodiac 340 is shorter (only 340 characters long) and at the same time has more cipher symbols: 63. [sent-95, score-1.164]
</p><p>60 We sample a random consecutive sequence of 340 characters from our small Zodiac corpus and use this as our message (and, of course, remove this sequence from our language model training data). [sent-97, score-0.117]
</p><p>61 We then generate an encryption key by assigning each of 63 cipher symbols to a single plain text character so that the number of cipher symbols mapped to each plaintext character is proportional to the frequency of that character in the message (this balancing makes the cipher more difficult). [sent-98, score-1.68]
</p><p>62 Finally, we generate the actual ciphertext by randomly sampling a cipher token for each plain text token uniformly at random from the cipher symbols allowed for that to-  ken under our generated key. [sent-99, score-1.076]
</p><p>63 For this cipher, there is an abso-  Log likelihood Figure 3: Synth 340 cipher. [sent-101, score-0.025]
</p><p>64 Histogram of the likelihoods of the local optima encountered by EM across 1M random restarts. [sent-102, score-0.332]
</p><p>65 Several peaks are labeled with their average accuracy and a snippet of a decode. [sent-103, score-0.144]
</p><p>66 ”  lute gain in accuracy of about 9% between 100 random restarts and 100K random restarts. [sent-105, score-0.677]
</p><p>67 This means that, even after tens of thousands of random restarts, EM is still finding new local optima with better likelihoods. [sent-107, score-0.237]
</p><p>68 It also appears that, even for a short cipher like Synth 340, likelihood and accuracy are reasonably coupled. [sent-108, score-0.513]
</p><p>69 We can visualize the distribution of local optima encountered by EM across 1M random restarts by plotting a histogram. [sent-109, score-0.757]
</p><p>70 Figure 3 shows, for each range of likelihood, the number of random restarts that led to a local optimum with a model score in that range. [sent-110, score-0.682]
</p><p>71 It is quickly visible that a few model scores are substantially more likely than all the rest. [sent-111, score-0.035]
</p><p>72 This kind of sparsity might be expected if there were a small number of local optima that EM was extremely likely to find. [sent-112, score-0.153]
</p><p>73 We can check whether the peaks of this histogram each correspond to a single local optimum or whether each is composed of multiple local optima that happen to have the same likelihood. [sent-113, score-0.428]
</p><p>74 For the histogram bucket corresponding to a particular peak, we compute the average relative difference between each multinomial parameter and its  mean. [sent-114, score-0.109]
</p><p>75 The average relative difference for the highest peak in Figure 3 is 0. [sent-115, score-0.055]
</p><p>76 These values are much smaller than 877  Log likelihood Figure 4: Zodiac 340 cipher. [sent-118, score-0.025]
</p><p>77 Histogram ofthe likelihoods ofthe local optima encountered by EM across 1M random restarts. [sent-119, score-0.332]
</p><p>78 the average relative difference between the means of these two peaks, 40%, indicating that the peaks do correspond to single local optima or collections of extremely similar local optima. [sent-120, score-0.269]
</p><p>79 There are several very small peaks that have the highest model scores (the peak with the highest model score has a frequency of 90 which is too small to be visible in Figure 3). [sent-121, score-0.173]
</p><p>80 The fact that these model scores are both high and rare is the reason we continue to see improvements to both accuracy and model score as we run numerous random restarts. [sent-122, score-0.16]
</p><p>81 The two tallest peaks and the peak with highest model score are labeled with their average accuracy and a small snippet of a decode in Figure 3. [sent-123, score-0.266]
</p><p>82 As mentioned, this cipher has never been cracked and may not be a homphonic cipher or even a valid cipher of any kind. [sent-127, score-1.353]
</p><p>83 The reading order of the cipher, which consists of a grid of symbols, is unknown. [sent-128, score-0.021]
</p><p>84 We make two arguments supporting the claim that Zodiac 340 is not a homophonic cipher  with row-major reading order: the first is statistical, based on the success rate of attempts to crack similar synthetic ciphers; the second is qualitative, comparing distributions of local optimum likelihoods. [sent-129, score-0.929]
</p><p>85 If Zodiac 340 is a homophonic cipher should we expect to crack it? [sent-130, score-0.772]
</p><p>86 In order to answer this question we generate 100 more ciphers in the same way we generated Synth 340. [sent-131, score-0.193]
</p><p>87 We use 10K random restarts to attack each cipher, and compute accuracies by best model score. [sent-132, score-0.582]
</p><p>88 The average accuracy across these 100 ciphers was 75% and the minimum accuracy was 36%. [sent-133, score-0.267]
</p><p>89 All but two of the ciphers were deciphered with more than 5 1% accuracy, which is usually sufficient for a human to identify a decode as partially correct. [sent-134, score-0.266]
</p><p>90 We attempted to crack Zodiac 340 using a rowmajor reading order and 1M random restarts, but the decode with best model score was nonsensical. [sent-135, score-0.261]
</p><p>91 This outcome would be unlikely if Zodiac 340 were like our synthetic ciphers, so Zodiac 340 is probably not a homophonic cipher with a row-major order. [sent-136, score-0.7]
</p><p>92 Of course, it could be a homophonic cipher with a different reading order. [sent-137, score-0.704]
</p><p>93 In Figure 4, we show the histogram of model scores for the attempt to crack Zodiac 340. [sent-139, score-0.179]
</p><p>94 We note that this histogram is strikingly different from the histogram for Synth 340. [sent-140, score-0.18]
</p><p>95 Zodiac 340’s histogram is not as sparse, and the range of model scores is much smaller. [sent-141, score-0.09]
</p><p>96 The sparsity of Synth 340’s histogram (but not Zodiac 340’s histogram) is typical of histograms corresponding to our set of 100 generated ciphers. [sent-142, score-0.09]
</p><p>97 In particular, we found that the initializations that lead to the local optima with highest likelihoods are sometimes very rare, but finding them can be worthwhile; for the problems we looked at, local optima with high likelihoods also achieved high accuracies. [sent-144, score-0.482]
</p><p>98 While the present experiments are on a very specific unsupervised learning problem, it is certainly reasonable to think that large-scale random restarts have potential more broadly. [sent-145, score-0.56]
</p><p>99 In addition to improving search, large-scale restarts can also provide a novel perspective when  performing exploratory analysis, here letting us argue in support for the hypothesis that Zodiac 340 is not a row-major homophonic cipher. [sent-146, score-0.721]
</p><p>100 Maximum likelihood from incomplete data via the EM algorithm. [sent-157, score-0.025]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('zodiac', 0.578), ('restarts', 0.459), ('cipher', 0.451), ('homophonic', 0.232), ('ciphers', 0.193), ('synth', 0.133), ('decipherment', 0.131), ('optima', 0.103), ('histogram', 0.09), ('crack', 0.089), ('random', 0.084), ('em', 0.077), ('optimum', 0.069), ('peaks', 0.066), ('ravi', 0.059), ('likelihoods', 0.059), ('restart', 0.055), ('knight', 0.052), ('encryption', 0.052), ('plaintext', 0.052), ('local', 0.05), ('decode', 0.047), ('snippet', 0.041), ('emission', 0.041), ('peak', 0.039), ('attack', 0.039), ('character', 0.039), ('killer', 0.039), ('accuracy', 0.037), ('encountered', 0.036), ('symbols', 0.036), ('unsolved', 0.035), ('corners', 0.03), ('nickolls', 0.03), ('saturate', 0.03), ('surpassed', 0.03), ('substitution', 0.028), ('hundred', 0.028), ('ciphertext', 0.026), ('deciphered', 0.026), ('famously', 0.026), ('objectives', 0.025), ('likelihood', 0.025), ('kevin', 0.024), ('snyder', 0.024), ('sujith', 0.024), ('gold', 0.022), ('reading', 0.021), ('score', 0.02), ('message', 0.02), ('dempster', 0.02), ('running', 0.02), ('multinomial', 0.019), ('trigram', 0.019), ('numerous', 0.019), ('gains', 0.019), ('quickly', 0.019), ('exploratory', 0.018), ('bootstrapped', 0.018), ('ran', 0.018), ('synthetic', 0.017), ('certainly', 0.017), ('visible', 0.016), ('globally', 0.016), ('highest', 0.016), ('look', 0.015), ('million', 0.015), ('interpolation', 0.015), ('properties', 0.015), ('brants', 0.015), ('initialization', 0.014), ('favor', 0.014), ('taylor', 0.014), ('plain', 0.014), ('looked', 0.014), ('millions', 0.014), ('achieved', 0.014), ('uniformly', 0.014), ('problems', 0.014), ('convergence', 0.013), ('characters', 0.013), ('attacking', 0.013), ('cuda', 0.013), ('nvidia', 0.013), ('simt', 0.013), ('mildly', 0.013), ('anish', 0.013), ('deteriorated', 0.013), ('haven', 0.013), ('lute', 0.013), ('nishit', 0.013), ('optimally', 0.013), ('plotting', 0.013), ('rathod', 0.013), ('plot', 0.012), ('argue', 0.012), ('decipher', 0.012), ('kle', 0.012), ('visualize', 0.012), ('buck', 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="54-tfidf-1" href="./emnlp-2013-Decipherment_with_a_Million_Random_Restarts.html">54 emnlp-2013-Decipherment with a Million Random Restarts</a></p>
<p>Author: Taylor Berg-Kirkpatrick ; Dan Klein</p><p>Abstract: This paper investigates the utility and effect of running numerous random restarts when using EM to attack decipherment problems. We find that simple decipherment models are able to crack homophonic substitution ciphers with high accuracy if a large number of random restarts are used but almost completely fail with only a few random restarts. For particularly difficult homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340.</p><p>2 0.14513485 <a title="54-tfidf-2" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>Author: Qing Dou ; Kevin Knight</p><p>Abstract: We introduce dependency relations into deciphering foreign languages and show that dependency relations help improve the state-ofthe-art deciphering accuracy by over 500%. We learn a translation lexicon from large amounts of genuinely non parallel data with decipherment to improve a phrase-based machine translation system trained with limited parallel data. In experiments, we observe BLEU gains of 1.2 to 1.8 across three different test sets.</p><p>3 0.11058 <a title="54-tfidf-3" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: Many statistical learning problems in NLP call for local model search methods. But accuracy tends to suffer with current techniques, which often explore either too narrowly or too broadly: hill-climbers can get stuck in local optima, whereas samplers may be inefficient. We propose to arrange individual local optimizers into organized networks. Our building blocks are operators of two types: (i) transform, which suggests new places to search, via non-random restarts from already-found local optima; and (ii) join, which merges candidate solutions to find better optima. Experiments on grammar induction show that pursuing different transforms (e.g., discarding parts of a learned model or ignoring portions of training data) results in improvements. Groups of locally-optimal solutions can be further perturbed jointly, by constructing mixtures. Using these tools, we designed several modular dependency grammar induction networks of increasing complexity. Our complete sys- tem achieves 48.6% accuracy (directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data) more than 5% higher than the previous state-of-the-art. —</p><p>4 0.025521893 <a title="54-tfidf-4" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>Author: Xiaoqing Zheng ; Hanyang Chen ; Tianyu Xu</p><p>Abstract: This study explores the feasibility of performing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representation of Chinese characters, and use these improved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-theart performance with minimal computational cost. We also describe a perceptron-style algorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented.</p><p>5 0.025178123 <a title="54-tfidf-5" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>Author: Hrushikesh Mohapatra ; Siddhanth Jain ; Soumen Chakrabarti</p><p>Abstract: Web search can be enhanced in powerful ways if token spans in Web text are annotated with disambiguated entities from large catalogs like Freebase. Entity annotators need to be trained on sample mention snippets. Wikipedia entities and annotated pages offer high-quality labeled data for training and evaluation. Unfortunately, Wikipedia features only one-ninth the number of entities as Freebase, and these are a highly biased sample of well-connected, frequently mentioned “head” entities. To bring hope to “tail” entities, we broaden our goal to a second task: assigning types to entities in Freebase but not Wikipedia. The two tasks are synergistic: knowing the types of unfamiliar entities helps disambiguate mentions, and words in mention contexts help assign types to entities. We present TMI, a bipartite graphical model for joint type-mention inference. TMI attempts no schema integration or entity resolution, but exploits the above-mentioned synergy. In experiments involving 780,000 people in Wikipedia, 2.3 million people in Freebase, 700 million Web pages, and over 20 professional editors, TMI shows considerable annotation accuracy improvement (e.g., 70%) compared to baselines (e.g., 46%), especially for “tail” and emerging entities. We also compare with Google’s recent annotations of the same corpus with Freebase entities, and report considerable improvements within the people domain.</p><p>6 0.024892636 <a title="54-tfidf-6" href="./emnlp-2013-A_Convex_Alternative_to_IBM_Model_2.html">2 emnlp-2013-A Convex Alternative to IBM Model 2</a></p>
<p>7 0.02245629 <a title="54-tfidf-7" href="./emnlp-2013-Regularized_Minimum_Error_Rate_Training.html">159 emnlp-2013-Regularized Minimum Error Rate Training</a></p>
<p>8 0.022345027 <a title="54-tfidf-8" href="./emnlp-2013-A_Multi-Teraflop_Constituency_Parser_using_GPUs.html">10 emnlp-2013-A Multi-Teraflop Constituency Parser using GPUs</a></p>
<p>9 0.02171284 <a title="54-tfidf-9" href="./emnlp-2013-Learning_to_Rank_Lexical_Substitutions.html">123 emnlp-2013-Learning to Rank Lexical Substitutions</a></p>
<p>10 0.020564664 <a title="54-tfidf-10" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>11 0.020500928 <a title="54-tfidf-11" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>12 0.018818395 <a title="54-tfidf-12" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>13 0.018151466 <a title="54-tfidf-13" href="./emnlp-2013-An_Empirical_Study_Of_Semi-Supervised_Chinese_Word_Segmentation_Using_Co-Training.html">21 emnlp-2013-An Empirical Study Of Semi-Supervised Chinese Word Segmentation Using Co-Training</a></p>
<p>14 0.016959282 <a title="54-tfidf-14" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>15 0.016702106 <a title="54-tfidf-15" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>16 0.01591613 <a title="54-tfidf-16" href="./emnlp-2013-Elephant%3A_Sequence_Labeling_for_Word_and_Sentence_Segmentation.html">72 emnlp-2013-Elephant: Sequence Labeling for Word and Sentence Segmentation</a></p>
<p>17 0.015785633 <a title="54-tfidf-17" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>18 0.015078743 <a title="54-tfidf-18" href="./emnlp-2013-A_Log-Linear_Model_for_Unsupervised_Text_Normalization.html">9 emnlp-2013-A Log-Linear Model for Unsupervised Text Normalization</a></p>
<p>19 0.014568635 <a title="54-tfidf-19" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>20 0.014305782 <a title="54-tfidf-20" href="./emnlp-2013-Efficient_Higher-Order_CRFs_for_Morphological_Tagging.html">70 emnlp-2013-Efficient Higher-Order CRFs for Morphological Tagging</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.06), (1, -0.022), (2, 0.001), (3, -0.005), (4, -0.024), (5, -0.01), (6, 0.006), (7, 0.023), (8, -0.006), (9, -0.007), (10, -0.006), (11, -0.01), (12, 0.021), (13, 0.019), (14, 0.019), (15, -0.032), (16, 0.005), (17, 0.016), (18, 0.042), (19, 0.027), (20, -0.006), (21, 0.049), (22, 0.079), (23, 0.117), (24, 0.037), (25, -0.041), (26, -0.153), (27, 0.043), (28, 0.114), (29, -0.157), (30, 0.023), (31, -0.174), (32, 0.057), (33, 0.056), (34, 0.014), (35, -0.142), (36, 0.042), (37, 0.122), (38, 0.052), (39, -0.308), (40, -0.263), (41, -0.01), (42, -0.107), (43, 0.097), (44, 0.092), (45, 0.074), (46, 0.021), (47, 0.094), (48, 0.001), (49, 0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97065443 <a title="54-lsi-1" href="./emnlp-2013-Decipherment_with_a_Million_Random_Restarts.html">54 emnlp-2013-Decipherment with a Million Random Restarts</a></p>
<p>Author: Taylor Berg-Kirkpatrick ; Dan Klein</p><p>Abstract: This paper investigates the utility and effect of running numerous random restarts when using EM to attack decipherment problems. We find that simple decipherment models are able to crack homophonic substitution ciphers with high accuracy if a large number of random restarts are used but almost completely fail with only a few random restarts. For particularly difficult homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340.</p><p>2 0.55748481 <a title="54-lsi-2" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: Many statistical learning problems in NLP call for local model search methods. But accuracy tends to suffer with current techniques, which often explore either too narrowly or too broadly: hill-climbers can get stuck in local optima, whereas samplers may be inefficient. We propose to arrange individual local optimizers into organized networks. Our building blocks are operators of two types: (i) transform, which suggests new places to search, via non-random restarts from already-found local optima; and (ii) join, which merges candidate solutions to find better optima. Experiments on grammar induction show that pursuing different transforms (e.g., discarding parts of a learned model or ignoring portions of training data) results in improvements. Groups of locally-optimal solutions can be further perturbed jointly, by constructing mixtures. Using these tools, we designed several modular dependency grammar induction networks of increasing complexity. Our complete sys- tem achieves 48.6% accuracy (directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data) more than 5% higher than the previous state-of-the-art. —</p><p>3 0.54743904 <a title="54-lsi-3" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>Author: Qing Dou ; Kevin Knight</p><p>Abstract: We introduce dependency relations into deciphering foreign languages and show that dependency relations help improve the state-ofthe-art deciphering accuracy by over 500%. We learn a translation lexicon from large amounts of genuinely non parallel data with decipherment to improve a phrase-based machine translation system trained with limited parallel data. In experiments, we observe BLEU gains of 1.2 to 1.8 across three different test sets.</p><p>4 0.25659376 <a title="54-lsi-4" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>Author: Ann Irvine ; Chris Quirk ; Hal Daume III</p><p>Abstract: When using a machine translation (MT) model trained on OLD-domain parallel data to translate NEW-domain text, one major challenge is the large number of out-of-vocabulary (OOV) and new-translation-sense words. We present a method to identify new translations of both known and unknown source language words that uses NEW-domain comparable document pairs. Starting with a joint distribution of source-target word pairs derived from the OLD-domain parallel corpus, our method recovers a new joint distribution that matches the marginal distributions of the NEW-domain comparable document pairs, while minimizing the divergence from the OLD-domain distribution. Adding learned translations to our French-English MT model results in gains of about 2 BLEU points over strong baselines.</p><p>5 0.23327473 <a title="54-lsi-5" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>Author: Do Kook Choe ; Eugene Charniak</p><p>Abstract: We introduce an extended naive Bayes model for word sense induction (WSI) and apply it to a WSI task. The extended model incorporates the idea the words closer to the target word are more relevant in predicting its sense. The proposed model is very simple yet effective when evaluated on SemEval-2010 WSI data. 1</p><p>6 0.22338215 <a title="54-lsi-6" href="./emnlp-2013-Measuring_Ideological_Proportions_in_Political_Speeches.html">129 emnlp-2013-Measuring Ideological Proportions in Political Speeches</a></p>
<p>7 0.21722433 <a title="54-lsi-7" href="./emnlp-2013-With_Blinkers_on%3A_Robust_Prediction_of_Eye_Movements_across_Readers.html">203 emnlp-2013-With Blinkers on: Robust Prediction of Eye Movements across Readers</a></p>
<p>8 0.20235306 <a title="54-lsi-8" href="./emnlp-2013-A_Convex_Alternative_to_IBM_Model_2.html">2 emnlp-2013-A Convex Alternative to IBM Model 2</a></p>
<p>9 0.18038891 <a title="54-lsi-9" href="./emnlp-2013-Simulating_Early-Termination_Search_for_Verbose_Spoken_Queries.html">173 emnlp-2013-Simulating Early-Termination Search for Verbose Spoken Queries</a></p>
<p>10 0.17704678 <a title="54-lsi-10" href="./emnlp-2013-Learning_to_Freestyle%3A_Hip_Hop_Challenge-Response_Induction_via_Transduction_Rule_Segmentation.html">122 emnlp-2013-Learning to Freestyle: Hip Hop Challenge-Response Induction via Transduction Rule Segmentation</a></p>
<p>11 0.17354994 <a title="54-lsi-11" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>12 0.17003822 <a title="54-lsi-12" href="./emnlp-2013-Unsupervised_Spectral_Learning_of_WCFG_as_Low-rank_Matrix_Completion.html">195 emnlp-2013-Unsupervised Spectral Learning of WCFG as Low-rank Matrix Completion</a></p>
<p>13 0.15867963 <a title="54-lsi-13" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>14 0.15110455 <a title="54-lsi-14" href="./emnlp-2013-Dependency_Language_Models_for_Sentence_Completion.html">58 emnlp-2013-Dependency Language Models for Sentence Completion</a></p>
<p>15 0.14958559 <a title="54-lsi-15" href="./emnlp-2013-Identifying_Multiple_Userids_of_the_Same_Author.html">95 emnlp-2013-Identifying Multiple Userids of the Same Author</a></p>
<p>16 0.14761549 <a title="54-lsi-16" href="./emnlp-2013-Tree_Kernel-based_Negation_and_Speculation_Scope_Detection_with_Structured_Syntactic_Parse_Features.html">188 emnlp-2013-Tree Kernel-based Negation and Speculation Scope Detection with Structured Syntactic Parse Features</a></p>
<p>17 0.14567272 <a title="54-lsi-17" href="./emnlp-2013-Animacy_Detection_with_Voting_Models.html">23 emnlp-2013-Animacy Detection with Voting Models</a></p>
<p>18 0.1434046 <a title="54-lsi-18" href="./emnlp-2013-A_Multi-Teraflop_Constituency_Parser_using_GPUs.html">10 emnlp-2013-A Multi-Teraflop Constituency Parser using GPUs</a></p>
<p>19 0.14142685 <a title="54-lsi-19" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>20 0.13943717 <a title="54-lsi-20" href="./emnlp-2013-Ubertagging%3A_Joint_Segmentation_and_Supertagging_for_English.html">190 emnlp-2013-Ubertagging: Joint Segmentation and Supertagging for English</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.017), (18, 0.025), (22, 0.031), (30, 0.067), (50, 0.022), (51, 0.118), (53, 0.41), (66, 0.037), (71, 0.019), (75, 0.024), (77, 0.047), (96, 0.021), (97, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.71394837 <a title="54-lda-1" href="./emnlp-2013-Decipherment_with_a_Million_Random_Restarts.html">54 emnlp-2013-Decipherment with a Million Random Restarts</a></p>
<p>Author: Taylor Berg-Kirkpatrick ; Dan Klein</p><p>Abstract: This paper investigates the utility and effect of running numerous random restarts when using EM to attack decipherment problems. We find that simple decipherment models are able to crack homophonic substitution ciphers with high accuracy if a large number of random restarts are used but almost completely fail with only a few random restarts. For particularly difficult homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340.</p><p>2 0.67071754 <a title="54-lda-2" href="./emnlp-2013-Measuring_Ideological_Proportions_in_Political_Speeches.html">129 emnlp-2013-Measuring Ideological Proportions in Political Speeches</a></p>
<p>Author: Yanchuan Sim ; Brice D. L. Acree ; Justin H. Gross ; Noah A. Smith</p><p>Abstract: We seek to measure political candidates’ ideological positioning from their speeches. To accomplish this, we infer ideological cues from a corpus of political writings annotated with known ideologies. We then represent the speeches of U.S. Presidential candidates as sequences of cues and lags (filler distinguished only by its length in words). We apply a domain-informed Bayesian HMM to infer the proportions of ideologies each candidate uses in each campaign. The results are validated against a set of preregistered, domain expertauthored hypotheses.</p><p>3 0.66748488 <a title="54-lda-3" href="./emnlp-2013-Towards_Situated_Dialogue%3A_Revisiting_Referring_Expression_Generation.html">185 emnlp-2013-Towards Situated Dialogue: Revisiting Referring Expression Generation</a></p>
<p>Author: Rui Fang ; Changsong Liu ; Lanbo She ; Joyce Y. Chai</p><p>Abstract: In situated dialogue, humans and agents have mismatched capabilities of perceiving the shared environment. Their representations of the shared world are misaligned. Thus referring expression generation (REG) will need to take this discrepancy into consideration. To address this issue, we developed a hypergraph-based approach to account for group-based spatial relations and uncertainties in perceiving the environment. Our empirical results have shown that this approach outperforms a previous graph-based approach with an absolute gain of 9%. However, while these graph-based approaches perform effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), they perform rather poorly when the agent has imperfect perception of the environment (e.g., 45%). This big performance gap calls for new solutions to REG that can mediate a shared perceptual basis in situated dialogue.</p><p>4 0.58618796 <a title="54-lda-4" href="./emnlp-2013-Assembling_the_Kazakh_Language_Corpus.html">26 emnlp-2013-Assembling the Kazakh Language Corpus</a></p>
<p>Author: Olzhas Makhambetov ; Aibek Makazhanov ; Zhandos Yessenbayev ; Bakhyt Matkarimov ; Islam Sabyrgaliyev ; Anuar Sharafudinov</p><p>Abstract: This paper presents the Kazakh Language Corpus (KLC), which is one of the first attempts made within a local research community to assemble a Kazakh corpus. KLC is designed to be a large scale corpus containing over 135 million words and conveying five stylistic genres: literary, publicistic, official, scientific and informal. Along with its primary part KLC comprises such parts as: (i) annotated sub-corpus, containing segmented documents encoded in the eXtensible Markup Language (XML) that marks complete morphological, syntactic, and structural characteristics of texts; (ii) as well as a sub-corpus with the annotated speech data. KLC has a web-based corpus management system that helps to navigate the data and retrieve necessary information. KLC is also open for contributors, who are willing to make suggestions, donate texts and help with annotation of existing materials.</p><p>5 0.36138961 <a title="54-lda-5" href="./emnlp-2013-Learning_Topics_and_Positions_from_Debatepedia.html">121 emnlp-2013-Learning Topics and Positions from Debatepedia</a></p>
<p>Author: Swapna Gottipati ; Minghui Qiu ; Yanchuan Sim ; Jing Jiang ; Noah A. Smith</p><p>Abstract: We explore Debatepedia, a communityauthored encyclopedia of sociopolitical debates, as evidence for inferring a lowdimensional, human-interpretable representation in the domain of issues and positions. We introduce a generative model positing latent topics and cross-cutting positions that gives special treatment to person mentions and opinion words. We evaluate the resulting representation’s usefulness in attaching opinionated documents to arguments and its consistency with human judgments about positions.</p><p>6 0.35197067 <a title="54-lda-6" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>7 0.35192397 <a title="54-lda-7" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>8 0.35011354 <a title="54-lda-8" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>9 0.34986317 <a title="54-lda-9" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>10 0.34954008 <a title="54-lda-10" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>11 0.34930891 <a title="54-lda-11" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>12 0.34783593 <a title="54-lda-12" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>13 0.34748712 <a title="54-lda-13" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>14 0.34731206 <a title="54-lda-14" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>15 0.34625182 <a title="54-lda-15" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>16 0.34496093 <a title="54-lda-16" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>17 0.34424004 <a title="54-lda-17" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>18 0.34297973 <a title="54-lda-18" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>19 0.34244648 <a title="54-lda-19" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>20 0.34214628 <a title="54-lda-20" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
