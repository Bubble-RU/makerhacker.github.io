<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>162 emnlp-2013-Russian Stress Prediction using Maximum Entropy Ranking</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-162" href="#">emnlp2013-162</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>162 emnlp-2013-Russian Stress Prediction using Maximum Entropy Ranking</h1>
<br/><p>Source: <a title="emnlp-2013-162-pdf" href="http://aclweb.org/anthology//D/D13/D13-1088.pdf">pdf</a></p><p>Author: Keith Hall ; Richard Sproat</p><p>Abstract: We explore a model of stress prediction in Russian using a combination of local contextual features and linguisticallymotivated features associated with the word’s stem and suffix. We frame this as a ranking problem, where the objective is to rank the pronunciation with the correct stress above those with incorrect stress. We train our models using a simple Maximum Entropy ranking framework allowing for efficient prediction. An empirical evaluation shows that a model combining the local contextual features and the linguistically-motivated non-local features performs best in identifying both primary and secondary stress. 1</p><p>Reference: <a title="emnlp-2013-162-reference" href="../emnlp2013_reference/emnlp-2013-Russian_Stress_Prediction_using_Maximum_Entropy_Ranking_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We explore a model of stress prediction in Russian using a combination of local contextual features and linguisticallymotivated features associated with the word’s stem and suffix. [sent-2, score-0.973]
</p><p>2 We frame this as a ranking problem, where the objective is to rank the pronunciation with the correct stress above those with incorrect stress. [sent-3, score-0.912]
</p><p>3 We train our models using a simple Maximum Entropy ranking framework allowing for efficient prediction. [sent-4, score-0.056]
</p><p>4 An empirical evaluation shows that a model combining the local contextual features and the linguistically-motivated non-local features performs best in identifying both primary and secondary stress. [sent-5, score-0.312]
</p><p>5 1  Introduction  In many languages,  one component  of accu-  rate word pronunciation prediction is predicting the placement of lexical stress. [sent-6, score-0.149]
</p><p>6 Spanish) the lexical stress system is relatively simple, in others (e. [sent-9, score-0.772]
</p><p>7 In this work, we present a machinelearned system for predicting Russian stress which incorporates both data-driven contextual features as well as linguistically-motivated word features. [sent-14, score-0.832]
</p><p>8 2  Previous Work on Stress Prediction  Pronunciation prediction, of which stress prediction is a part, is important for many speech applications including automatic speech recog-  nition, text-to-speech synthesis, and transliteration for, say, machine translation. [sent-15, score-0.857]
</p><p>9 While there is by now a sizable literature on pronunciation prediction from spelling (often termed “grapheme-to-phoneme” conversion) , work that specifically focuses on stress prediction is more limited. [sent-16, score-0.975]
</p><p>10 One of the best-known early pieces of work is (Church, 1985) , which uses morphological rules and stress pattern templates to predict stress in novel words. [sent-17, score-1.56]
</p><p>11 The work we present here is closer in spirit to data-driven approaches such as (Webster, 2004; Pearson et al. [sent-19, score-0.021]
</p><p>12 , 2009) , whose features we use in the work described below. [sent-21, score-0.028]
</p><p>13 3  Russian Stress Patterns  Russian stress preserves many features of IndoEuropean accenting patterns (Halle, 1997) . [sent-22, score-0.83]
</p><p>14 In order to know the stress of a morphologically complex word consisting of a stem plus a suffix, one needs to know if the stem has an accent,  and if so on what syllable; and similarly for the suffix. [sent-23, score-1.006]
</p><p>15 For words where the stem is accented, 879  ProceSe datintlges, o Wfa tsh ein 2g01to3n, C UoSnfAe,re 1n8c-e2 o1n O Ecmtopbier ic 2a0l1 M3. [sent-24, score-0.075]
</p><p>16 this accent overrides any accent that may occur on the suffix. [sent-27, score-0.219]
</p><p>17 With unaccented stems, if the suffix has an accent, then stress for the whole word will be on the suffix; if there is also no stress on the suffix, then a default rule places stress on the first syllable of the word. [sent-28, score-2.518]
</p><p>18 In addition to these patterns, there are also postaccented words, where accent is placed uni-  formly on the first syllable of the suffix — an innovation of East and South Slavic languages (Halle, 1997) . [sent-29, score-0.313]
</p><p>19 These latter cases can be handled by assigning an accent to the stem, indicating that it is associated with the syllable after the stem. [sent-30, score-0.173]
</p><p>20 Stress placement in Russian is important for speech applications since over and above the phonetic effects of stress itself (prominence, duration, etc. [sent-37, score-0.865]
</p><p>21 ) , the position of stress strongly influences vowel quality. [sent-38, score-0.867]
</p><p>22 To take an example of the lexically unaccented noun город gorod ‘city’, the genitive singular г'орода g’ ’oroda /g"Or@d@/ contrasts with the nominative plural город'а gorod’ ’a /g@r2d"a/. [sent-39, score-0.219]
</p><p>23 All non-stressed /a/ are reduced to schwa — or by most accounts if before the stressed syllable to /2/; see (Wade, 1992). [sent-40, score-0.15]
</p><p>24 The stress patterns of Russian suggest that useful features for predicting stress might include (string) prefix and suffix features of the word in order to capture properties of the stem, since some stems are (un)accented, or of the suffix, since some suffixes are accented. [sent-41, score-1.777]
</p><p>25 (2009) , we frame the stress prediction problem as a ranking problem. [sent-43, score-0.9]
</p><p>26 For each word, we identify stressable vowels and generate a set of alternatives, each representing a different primary stress placement. [sent-44, score-0.879]
</p><p>27 Some words also have secondary stress which, if it occurs, always occurs before the primary stressed syllable. [sent-45, score-1.089]
</p><p>28 For each primary stress alternative, we generate all possible secondary stressed alternatives, including an alternative that has no secondary stress. [sent-46, score-1.243]
</p><p>29 (In the experiments reported below we actually consider two conditions: one where we ignore secondary stress in training and evaluation; and one where we include it. [sent-47, score-0.926]
</p><p>30 ) Formally, we model the problem using a Maximum Entropy ranking framework similar to  that presented in Collins and Koo (2005) . [sent-48, score-0.056]
</p><p>31 For each example, xi, we generate the set of possible stress patterns Yi. [sent-49, score-0.802]
</p><p>32 Our goal is to rank the items isntr Yi psuacthte rthnsat Y all of the valid stress patterns Yi∗ are above all of the invalid stress patterns. [sent-50, score-1.574]
</p><p>33 In our case, we use a variety of stochastic gra-  dient descent (SGD) which can be parallelized for efficient training. [sent-52, score-0.017]
</p><p>34 During training, we provide all plausibly correct primary stress patterns as the positive set 880  Yi∗ . [sent-53, score-0.902]
</p><p>35 At prediction-time, we evaluate all possibYle stress predictions and pick the one with the highest score under the trained model Θ:  aryg0∈mYaixp(y0|Yi) = aryg0∈mYaix∑kθkfk(y0,x)  (5)  The primary motivation for using Maximum Entropy rather the ranking-SVM is for efficient training and inference. [sent-54, score-0.875]
</p><p>36 This makes inference (prediction) fast in comparison to the ranking SVM-based approach proposed in Dou et al. [sent-58, score-0.056]
</p><p>37 , 2009) are based on trigrams consisting of a vowel letter, the preceding consonant letter (if any) and the following consonant letter (if any) . [sent-65, score-0.237]
</p><p>38 Attached to each trigram is the stress level of the trigram’s vowel — 1 , 2 or 0 (for no stress) . [sent-66, score-0.884]
</p><p>39 For the English word overdo with the stress pattern 2-0-1, the basic features would be ov:2, ver:0, and do:1. [sent-67, score-0.816]
</p><p>40 Notating these pairs as si : ti, where si is the triple, ti is the stress pattern and iis the position in the word, the complete feature set is given in Table 2, where the stress pattern for the whole word is given in the last row as t1t2 . [sent-68, score-1.576]
</p><p>41 Dou and colleagues use an SVMbased ranking approach, so they generated features for all possible stress assignments for each word, assigning the highest rank to the correct  assignment. [sent-72, score-0.856]
</p><p>42 The ranker was then trained to associate feature combinations to the correct ranking of alternative stress possibilities. [sent-73, score-0.853]
</p><p>43 Note that etymologically, and in some ways phonologically, в v behaves like a semivowel in Russian. [sent-75, score-0.039]
</p><p>44 of the word, which might be expected to better capture some of the properties of Russian stress patterns discussed above, than the much more local features from (Dou et al. [sent-76, score-0.83]
</p><p>45 In this case for all stress variants of the word we collect prefixes of length 1 through the length of the word, and similarly for suffixes, except that for the stress symbol we treat that together with the vowel it marks as a single symbol. [sent-78, score-1.709]
</p><p>46 Thus for the word gorod’ ’a, all prefixes of the word would be g, go, gor, goro, gorod, gorod’ ’a. [sent-79, score-0.052]
</p><p>47 In addition, we include prefixes and suffixes of an “abstract” version of the word where most consonants and vowels have been replaced by  a phonetic class. [sent-80, score-0.158]
</p><p>48 Note that in Russian the vowel ё /jO/ is always stressed, but is rarely written in text: it is usually spelled as е, whose stressed pronuncation is /(j)E/. [sent-82, score-0.187]
</p><p>49 Since written е is in general ambiguous between е and ё, when we compute stress variants of a word for the purpose of ranking, we include both variants that have е and ё. [sent-83, score-0.808]
</p><p>50 6  Data  Our data were 2,004,044 fully inflected words with assigned stress expanded from Zaliznyak’s Grammatical Dictionary of the Russian Language (Zaliznyak, 1977) . [sent-84, score-0.772]
</p><p>51 The 100,000 test examples obviously contain no forms that were found in the training data, but most of them are word forms that derive from lemmata from which some training data forms are also derived. [sent-86, score-0.17]
</p><p>52 Given the fact that Russian stress is lex-  ically determined as outlined in Section 3, this is perfectly reasonable: in order to know how to stress a form, it is often necessary to have seen other words that share the same lemma. [sent-87, score-1.579]
</p><p>53 Nonetheless, it is also of interest to know how well the system works on words that do not share any lemmata with words in the training data. [sent-88, score-0.139]
</p><p>54 To that end, we collected a set of 248 forms that shared no lemmata with the training data. [sent-89, score-0.151]
</p><p>55 The two sets will be referred to in the next section as the “shared lemmata” and “no shared lemmata” sets. [sent-90, score-0.015]
</p><p>56 7  Results  Table 4 gives word accuracy results for the different feature combinations, as follows: Dou et al’s features (Dou et al. [sent-91, score-0.028]
</p><p>57 , 2009) ; our affix features; our affix features plus affix features based on the abstract phonetic class versions of words; Dou et al’s features plus our affix features; Dou et al’s features plus our affix features plus the abstract affix features. [sent-92, score-1.358]
</p><p>58 When we consider only primary stress (col-  umn 2 in Table 4, for the shared-lemmata test data, Dou et al’s features performed the worst at 97. [sent-93, score-0.885]
</p><p>59 2% accuracy, with all feature combinations that include the affix features performing at the same level, 98. [sent-94, score-0.219]
</p><p>60 For the no-sharedlemmata test data, using Dou et al’s features alone achieved an accuracy of 80. [sent-96, score-0.028]
</p><p>61 8%, presumably because it is harder for them to gener-  Table 4: Word accuracies for various feature combi-  nations for both shared lemmata and no-shared lemmata conditions. [sent-99, score-0.253]
</p><p>62 The second column reports results where we consider only primary stress, the third column results where we also predict secondary stress. [sent-100, score-0.275]
</p><p>63 alize to unseen cases, but using the abstract affix features increased the performance to 81. [sent-101, score-0.194]
</p><p>64 0%, better than that of using Dou et al’s features alone. [sent-102, score-0.028]
</p><p>65 As can be seen combining Dou et al’s features with various combinations of the affix features improved the performance further. [sent-103, score-0.247]
</p><p>66 For primary and secondary stress prediction (column 3 in the table) , the results are overall degraded for most conditions but otherwise very similar in terms of ranking of the features to what we find with primary stress alone. [sent-104, score-2.028]
</p><p>67 Note though that for the shared-lemmata condition the results with affix features are almost as good as for the primary-stress-only case, whereas there is a significant drop in performance for the Dou et al. [sent-105, score-0.242]
</p><p>68 ’s features fare rather better compared to the affix features. [sent-108, score-0.194]
</p><p>69 Note that in the no-shared-lemmata condition, there is only one word that is marked with a secondary stress, and that stress is actually correctly predicted by all methods. [sent-110, score-0.926]
</p><p>70 features and the affix condition can be accounted for by three cases involving the same root, which the affix condition misas882  signs secondary stress to. [sent-112, score-1.382]
</p><p>71 features and the all-features condition, systematic benefit for the all-features condition was found for secondary stress assignment for productive prefixes where secondary stress is typically found. [sent-115, score-2.021]
</p><p>72 For example, the prefix аэро (‘aero-’) as in а`эродина' мика (‘aerodynamics’) typically has secondary stress. [sent-116, score-0.179]
</p><p>73 Since the no-shared-lemmata data set is small, we tested significance using two permutation tests. [sent-119, score-0.024]
</p><p>74 The second randomly permuted the test data 248 times, after each random permutation, removing the first ten examples, and computing the score. [sent-121, score-0.017]
</p><p>75 Pairwise t-tests between all conditions for the primary-stress-only and for the primary plus secondary stress predictions, were highly significant in all cases. [sent-122, score-1.078]
</p><p>76 We also experimented with a postaccent feature to model the postaccented class of nouns described in Section 3. [sent-123, score-0.078]
</p><p>77 For each prefix of the word, we record whether the following vowel is stressed or unstressed. [sent-124, score-0.198]
</p><p>78 8  Discussion  In this paper we have presented a Maximum  Entropy ranking-based approach to Russian stress prediction. [sent-126, score-0.772]
</p><p>79 The approach is similar in spirit to the SVM-based ranking approach presented in (Dou et al. [sent-127, score-0.077]
</p><p>80 , 2009) , but incorporates additional affix-based features, which are motivated by linguistic analyses of the problem. [sent-128, score-0.015]
</p><p>81 We have shown that these additional features generalize better than the Dou et al. [sent-129, score-0.028]
</p><p>82 features in cases where we have seen a related form of the test word, and that combing the additional features with the Dou et al. [sent-130, score-0.071]
</p><p>83 Stress assignment in letter to sound rules for speech synthesis. [sent-134, score-0.097]
</p><p>84 A ranking approach to stress prediction for letter-to-phoneme conversion. [sent-142, score-0.881]
</p><p>85 Word stress assignment in a text-to-speech synthesis system for British English. [sent-168, score-0.837]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stress', 0.772), ('dou', 0.354), ('russian', 0.194), ('affix', 0.166), ('secondary', 0.154), ('lemmata', 0.119), ('gorod', 0.117), ('accent', 0.101), ('vowel', 0.095), ('primary', 0.085), ('accented', 0.078), ('halle', 0.078), ('postaccented', 0.078), ('stressed', 0.078), ('stem', 0.075), ('syllable', 0.072), ('unaccented', 0.068), ('pronunciation', 0.065), ('suffix', 0.062), ('al', 0.06), ('gor', 0.059), ('ranking', 0.056), ('prediction', 0.053), ('prefixes', 0.052), ('yi', 0.052), ('condition', 0.048), ('aff', 0.046), ('kfk', 0.046), ('phonetic', 0.046), ('plus', 0.044), ('entropy', 0.041), ('assignment', 0.041), ('letter', 0.04), ('korolj', 0.039), ('semivowel', 0.039), ('zaliznyak', 0.039), ('suffixes', 0.038), ('consonant', 0.031), ('placement', 0.031), ('patterns', 0.03), ('features', 0.028), ('combinations', 0.025), ('prefix', 0.025), ('sgd', 0.024), ('permutation', 0.024), ('synthesis', 0.024), ('maximum', 0.023), ('conditions', 0.023), ('vowels', 0.022), ('stems', 0.022), ('dat', 0.021), ('pearson', 0.021), ('spirit', 0.021), ('alternatives', 0.021), ('church', 0.021), ('keith', 0.021), ('know', 0.02), ('hall', 0.02), ('frame', 0.019), ('plural', 0.018), ('variants', 0.018), ('predictions', 0.018), ('column', 0.018), ('forms', 0.017), ('trigram', 0.017), ('phonologically', 0.017), ('overrides', 0.017), ('leaning', 0.017), ('rankers', 0.017), ('parallelized', 0.017), ('camps', 0.017), ('morris', 0.017), ('sizable', 0.017), ('permuted', 0.017), ('terence', 0.017), ('moscow', 0.017), ('notating', 0.017), ('unacc', 0.017), ('contextual', 0.017), ('singular', 0.016), ('speech', 0.016), ('pattern', 0.016), ('pea', 0.015), ('nition', 0.015), ('webster', 0.015), ('termed', 0.015), ('qing', 0.015), ('prominence', 0.015), ('slavic', 0.015), ('town', 0.015), ('indoeuropean', 0.015), ('plausibly', 0.015), ('ically', 0.015), ('combing', 0.015), ('zk', 0.015), ('ally', 0.015), ('shared', 0.015), ('incorporates', 0.015), ('sproat', 0.014), ('spelled', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="162-tfidf-1" href="./emnlp-2013-Russian_Stress_Prediction_using_Maximum_Entropy_Ranking.html">162 emnlp-2013-Russian Stress Prediction using Maximum Entropy Ranking</a></p>
<p>Author: Keith Hall ; Richard Sproat</p><p>Abstract: We explore a model of stress prediction in Russian using a combination of local contextual features and linguisticallymotivated features associated with the word’s stem and suffix. We frame this as a ranking problem, where the objective is to rank the pronunciation with the correct stress above those with incorrect stress. We train our models using a simple Maximum Entropy ranking framework allowing for efficient prediction. An empirical evaluation shows that a model combining the local contextual features and the linguistically-motivated non-local features performs best in identifying both primary and secondary stress. 1</p><p>2 0.23385035 <a title="162-tfidf-2" href="./emnlp-2013-Pair_Language_Models_for_Deriving_Alternative_Pronunciations_and_Spellings_from_Pronunciation_Dictionaries.html">150 emnlp-2013-Pair Language Models for Deriving Alternative Pronunciations and Spellings from Pronunciation Dictionaries</a></p>
<p>Author: Russell Beckley ; Brian Roark</p><p>Abstract: Pronunciation dictionaries provide a readily available parallel corpus for learning to transduce between character strings and phoneme strings or vice versa. Translation models can be used to derive character-level paraphrases on either side of this transduction, allowing for the automatic derivation of alternative pronunciations or spellings. We examine finitestate and SMT-based methods for these related tasks, and demonstrate that the tasks have different characteristics finding alternative spellings is harder than alternative pronunciations and benefits from round-trip algorithms when the other does not. We also show that we can increase accuracy by modeling syllable stress. –</p><p>3 0.11437751 <a title="162-tfidf-3" href="./emnlp-2013-Translating_into_Morphologically_Rich_Languages_with_Synthetic_Phrases.html">186 emnlp-2013-Translating into Morphologically Rich Languages with Synthetic Phrases</a></p>
<p>Author: Victor Chahuneau ; Eva Schlinger ; Noah A. Smith ; Chris Dyer</p><p>Abstract: Translation into morphologically rich languages is an important but recalcitrant problem in MT. We present a simple and effective approach that deals with the problem in two phases. First, a discriminative model is learned to predict inflections of target words from rich source-side annotations. Then, this model is used to create additional sentencespecific word- and phrase-level translations that are added to a standard translation model as “synthetic” phrases. Our approach relies on morphological analysis of the target language, but we show that an unsupervised Bayesian model of morphology can successfully be used in place of a supervised analyzer. We report significant improvements in translation quality when translating from English to Russian, Hebrew and Swahili.</p><p>4 0.070287414 <a title="162-tfidf-4" href="./emnlp-2013-Automatic_Extraction_of_Morphological_Lexicons_from_Morphologically_Annotated_Corpora.html">30 emnlp-2013-Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora</a></p>
<p>Author: Ramy Eskander ; Nizar Habash ; Owen Rambow</p><p>Abstract: We present a method for automatically learning inflectional classes and associated lemmas from morphologically annotated corpora. The method consists of a core languageindependent algorithm, which can be optimized for specific languages. The method is demonstrated on Egyptian Arabic and German, two morphologically rich languages. Our best method for Egyptian Arabic provides an error reduction of 55.6% over a simple baseline; our best method for German achieves a 66.7% error reduction.</p><p>5 0.068129376 <a title="162-tfidf-5" href="./emnlp-2013-Assembling_the_Kazakh_Language_Corpus.html">26 emnlp-2013-Assembling the Kazakh Language Corpus</a></p>
<p>Author: Olzhas Makhambetov ; Aibek Makazhanov ; Zhandos Yessenbayev ; Bakhyt Matkarimov ; Islam Sabyrgaliyev ; Anuar Sharafudinov</p><p>Abstract: This paper presents the Kazakh Language Corpus (KLC), which is one of the first attempts made within a local research community to assemble a Kazakh corpus. KLC is designed to be a large scale corpus containing over 135 million words and conveying five stylistic genres: literary, publicistic, official, scientific and informal. Along with its primary part KLC comprises such parts as: (i) annotated sub-corpus, containing segmented documents encoded in the eXtensible Markup Language (XML) that marks complete morphological, syntactic, and structural characteristics of texts; (ii) as well as a sub-corpus with the annotated speech data. KLC has a web-based corpus management system that helps to navigate the data and retrieve necessary information. KLC is also open for contributors, who are willing to make suggestions, donate texts and help with annotation of existing materials.</p><p>6 0.068035126 <a title="162-tfidf-6" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>7 0.060086474 <a title="162-tfidf-7" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>8 0.055032864 <a title="162-tfidf-8" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>9 0.052466169 <a title="162-tfidf-9" href="./emnlp-2013-Adaptor_Grammars_for_Learning_Non-Concatenative_Morphology.html">19 emnlp-2013-Adaptor Grammars for Learning Non-Concatenative Morphology</a></p>
<p>10 0.048038129 <a title="162-tfidf-10" href="./emnlp-2013-Joint_Learning_of_Phonetic_Units_and_Word_Pronunciations_for_ASR.html">115 emnlp-2013-Joint Learning of Phonetic Units and Word Pronunciations for ASR</a></p>
<p>11 0.042520817 <a title="162-tfidf-11" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>12 0.030778019 <a title="162-tfidf-12" href="./emnlp-2013-Structured_Penalties_for_Log-Linear_Language_Models.html">176 emnlp-2013-Structured Penalties for Log-Linear Language Models</a></p>
<p>13 0.026882835 <a title="162-tfidf-13" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>14 0.02246033 <a title="162-tfidf-14" href="./emnlp-2013-Tree_Kernel-based_Negation_and_Speculation_Scope_Detection_with_Structured_Syntactic_Parse_Features.html">188 emnlp-2013-Tree Kernel-based Negation and Speculation Scope Detection with Structured Syntactic Parse Features</a></p>
<p>15 0.022263834 <a title="162-tfidf-15" href="./emnlp-2013-Efficient_Higher-Order_CRFs_for_Morphological_Tagging.html">70 emnlp-2013-Efficient Higher-Order CRFs for Morphological Tagging</a></p>
<p>16 0.021527423 <a title="162-tfidf-16" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>17 0.019986063 <a title="162-tfidf-17" href="./emnlp-2013-The_Effects_of_Syntactic_Features_in_Automatic_Prediction_of_Morphology.html">181 emnlp-2013-The Effects of Syntactic Features in Automatic Prediction of Morphology</a></p>
<p>18 0.019866908 <a title="162-tfidf-18" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<p>19 0.019470287 <a title="162-tfidf-19" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>20 0.018658221 <a title="162-tfidf-20" href="./emnlp-2013-An_Efficient_Language_Model_Using_Double-Array_Structures.html">20 emnlp-2013-An Efficient Language Model Using Double-Array Structures</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.082), (1, -0.026), (2, -0.016), (3, -0.055), (4, -0.124), (5, -0.057), (6, -0.044), (7, 0.002), (8, 0.018), (9, -0.08), (10, -0.011), (11, 0.008), (12, 0.015), (13, 0.046), (14, 0.024), (15, -0.042), (16, 0.136), (17, -0.09), (18, 0.122), (19, 0.187), (20, -0.048), (21, -0.109), (22, 0.198), (23, -0.105), (24, 0.103), (25, 0.154), (26, 0.078), (27, 0.186), (28, -0.098), (29, -0.092), (30, 0.024), (31, -0.049), (32, -0.093), (33, -0.13), (34, 0.144), (35, -0.113), (36, -0.032), (37, 0.043), (38, 0.101), (39, -0.042), (40, 0.16), (41, -0.05), (42, -0.022), (43, -0.028), (44, 0.161), (45, -0.119), (46, 0.181), (47, -0.042), (48, 0.1), (49, -0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97317994 <a title="162-lsi-1" href="./emnlp-2013-Russian_Stress_Prediction_using_Maximum_Entropy_Ranking.html">162 emnlp-2013-Russian Stress Prediction using Maximum Entropy Ranking</a></p>
<p>Author: Keith Hall ; Richard Sproat</p><p>Abstract: We explore a model of stress prediction in Russian using a combination of local contextual features and linguisticallymotivated features associated with the word’s stem and suffix. We frame this as a ranking problem, where the objective is to rank the pronunciation with the correct stress above those with incorrect stress. We train our models using a simple Maximum Entropy ranking framework allowing for efficient prediction. An empirical evaluation shows that a model combining the local contextual features and the linguistically-motivated non-local features performs best in identifying both primary and secondary stress. 1</p><p>2 0.77960086 <a title="162-lsi-2" href="./emnlp-2013-Pair_Language_Models_for_Deriving_Alternative_Pronunciations_and_Spellings_from_Pronunciation_Dictionaries.html">150 emnlp-2013-Pair Language Models for Deriving Alternative Pronunciations and Spellings from Pronunciation Dictionaries</a></p>
<p>Author: Russell Beckley ; Brian Roark</p><p>Abstract: Pronunciation dictionaries provide a readily available parallel corpus for learning to transduce between character strings and phoneme strings or vice versa. Translation models can be used to derive character-level paraphrases on either side of this transduction, allowing for the automatic derivation of alternative pronunciations or spellings. We examine finitestate and SMT-based methods for these related tasks, and demonstrate that the tasks have different characteristics finding alternative spellings is harder than alternative pronunciations and benefits from round-trip algorithms when the other does not. We also show that we can increase accuracy by modeling syllable stress. –</p><p>3 0.41712332 <a title="162-lsi-3" href="./emnlp-2013-Assembling_the_Kazakh_Language_Corpus.html">26 emnlp-2013-Assembling the Kazakh Language Corpus</a></p>
<p>Author: Olzhas Makhambetov ; Aibek Makazhanov ; Zhandos Yessenbayev ; Bakhyt Matkarimov ; Islam Sabyrgaliyev ; Anuar Sharafudinov</p><p>Abstract: This paper presents the Kazakh Language Corpus (KLC), which is one of the first attempts made within a local research community to assemble a Kazakh corpus. KLC is designed to be a large scale corpus containing over 135 million words and conveying five stylistic genres: literary, publicistic, official, scientific and informal. Along with its primary part KLC comprises such parts as: (i) annotated sub-corpus, containing segmented documents encoded in the eXtensible Markup Language (XML) that marks complete morphological, syntactic, and structural characteristics of texts; (ii) as well as a sub-corpus with the annotated speech data. KLC has a web-based corpus management system that helps to navigate the data and retrieve necessary information. KLC is also open for contributors, who are willing to make suggestions, donate texts and help with annotation of existing materials.</p><p>4 0.40120482 <a title="162-lsi-4" href="./emnlp-2013-Joint_Learning_of_Phonetic_Units_and_Word_Pronunciations_for_ASR.html">115 emnlp-2013-Joint Learning of Phonetic Units and Word Pronunciations for ASR</a></p>
<p>Author: Chia-ying Lee ; Yu Zhang ; James Glass</p><p>Abstract: The creation of a pronunciation lexicon remains the most inefficient process in developing an Automatic Speech Recognizer (ASR). In this paper, we propose an unsupervised alternative requiring no language-specific knowledge to the conventional manual approach for creating pronunciation dictionaries. We present a hierarchical Bayesian model, which jointly discovers the phonetic inventory and the Letter-to-Sound (L2S) mapping rules in a language using only transcribed data. When tested on a corpus of spontaneous queries, the results demonstrate the superiority of the proposed joint learning scheme over its sequential counterpart, in which the latent phonetic inventory and L2S mappings are learned separately. Furthermore, the recognizers built with the automatically induced lexicon consistently outperform grapheme-based recognizers and even approach the performance of recognition systems trained using – – conventional supervised procedures.</p><p>5 0.2633208 <a title="162-lsi-5" href="./emnlp-2013-Translating_into_Morphologically_Rich_Languages_with_Synthetic_Phrases.html">186 emnlp-2013-Translating into Morphologically Rich Languages with Synthetic Phrases</a></p>
<p>Author: Victor Chahuneau ; Eva Schlinger ; Noah A. Smith ; Chris Dyer</p><p>Abstract: Translation into morphologically rich languages is an important but recalcitrant problem in MT. We present a simple and effective approach that deals with the problem in two phases. First, a discriminative model is learned to predict inflections of target words from rich source-side annotations. Then, this model is used to create additional sentencespecific word- and phrase-level translations that are added to a standard translation model as “synthetic” phrases. Our approach relies on morphological analysis of the target language, but we show that an unsupervised Bayesian model of morphology can successfully be used in place of a supervised analyzer. We report significant improvements in translation quality when translating from English to Russian, Hebrew and Swahili.</p><p>6 0.2239442 <a title="162-lsi-6" href="./emnlp-2013-Automatically_Classifying_Edit_Categories_in_Wikipedia_Revisions.html">34 emnlp-2013-Automatically Classifying Edit Categories in Wikipedia Revisions</a></p>
<p>7 0.22277312 <a title="162-lsi-7" href="./emnlp-2013-Learning_to_Freestyle%3A_Hip_Hop_Challenge-Response_Induction_via_Transduction_Rule_Segmentation.html">122 emnlp-2013-Learning to Freestyle: Hip Hop Challenge-Response Induction via Transduction Rule Segmentation</a></p>
<p>8 0.20761229 <a title="162-lsi-8" href="./emnlp-2013-Automatic_Extraction_of_Morphological_Lexicons_from_Morphologically_Annotated_Corpora.html">30 emnlp-2013-Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora</a></p>
<p>9 0.18815644 <a title="162-lsi-9" href="./emnlp-2013-Automated_Essay_Scoring_by_Maximizing_Human-Machine_Agreement.html">28 emnlp-2013-Automated Essay Scoring by Maximizing Human-Machine Agreement</a></p>
<p>10 0.18645857 <a title="162-lsi-10" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>11 0.17319421 <a title="162-lsi-11" href="./emnlp-2013-A_Joint_Learning_Model_of_Word_Segmentation%2C_Lexical_Acquisition%2C_and_Phonetic_Variability.html">8 emnlp-2013-A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability</a></p>
<p>12 0.16844091 <a title="162-lsi-12" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>13 0.15063143 <a title="162-lsi-13" href="./emnlp-2013-Tree_Kernel-based_Negation_and_Speculation_Scope_Detection_with_Structured_Syntactic_Parse_Features.html">188 emnlp-2013-Tree Kernel-based Negation and Speculation Scope Detection with Structured Syntactic Parse Features</a></p>
<p>14 0.14192416 <a title="162-lsi-14" href="./emnlp-2013-Detecting_Promotional_Content_in_Wikipedia.html">61 emnlp-2013-Detecting Promotional Content in Wikipedia</a></p>
<p>15 0.14176917 <a title="162-lsi-15" href="./emnlp-2013-Adaptor_Grammars_for_Learning_Non-Concatenative_Morphology.html">19 emnlp-2013-Adaptor Grammars for Learning Non-Concatenative Morphology</a></p>
<p>16 0.14075245 <a title="162-lsi-16" href="./emnlp-2013-Identifying_Manipulated_Offerings_on_Review_Portals.html">94 emnlp-2013-Identifying Manipulated Offerings on Review Portals</a></p>
<p>17 0.13752331 <a title="162-lsi-17" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>18 0.13512245 <a title="162-lsi-18" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>19 0.13508427 <a title="162-lsi-19" href="./emnlp-2013-Learning_to_Rank_Lexical_Substitutions.html">123 emnlp-2013-Learning to Rank Lexical Substitutions</a></p>
<p>20 0.12921917 <a title="162-lsi-20" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.025), (14, 0.01), (18, 0.021), (22, 0.047), (30, 0.065), (48, 0.365), (50, 0.019), (51, 0.143), (61, 0.023), (66, 0.064), (71, 0.015), (75, 0.028), (77, 0.014), (90, 0.01), (96, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74067634 <a title="162-lda-1" href="./emnlp-2013-Russian_Stress_Prediction_using_Maximum_Entropy_Ranking.html">162 emnlp-2013-Russian Stress Prediction using Maximum Entropy Ranking</a></p>
<p>Author: Keith Hall ; Richard Sproat</p><p>Abstract: We explore a model of stress prediction in Russian using a combination of local contextual features and linguisticallymotivated features associated with the word’s stem and suffix. We frame this as a ranking problem, where the objective is to rank the pronunciation with the correct stress above those with incorrect stress. We train our models using a simple Maximum Entropy ranking framework allowing for efficient prediction. An empirical evaluation shows that a model combining the local contextual features and the linguistically-motivated non-local features performs best in identifying both primary and secondary stress. 1</p><p>2 0.62221432 <a title="162-lda-2" href="./emnlp-2013-Combining_PCFG-LA_Models_with_Dual_Decomposition%3A_A_Case_Study_with_Function_Labels_and_Binarization.html">50 emnlp-2013-Combining PCFG-LA Models with Dual Decomposition: A Case Study with Function Labels and Binarization</a></p>
<p>Author: Joseph Le Roux ; Antoine Rozenknop ; Jennifer Foster</p><p>Abstract: It has recently been shown that different NLP models can be effectively combined using dual decomposition. In this paper we demonstrate that PCFG-LA parsing models are suitable for combination in this way. We experiment with the different models which result from alternative methods of extracting a grammar from a treebank (retaining or discarding function labels, left binarization versus right binarization) and achieve a labeled Parseval F-score of 92.4 on Wall Street Journal Section 23 this represents an absolute improvement of 0.7 and an error reduction rate of 7% over a strong PCFG-LA product-model baseline. Although we experiment only with binarization and function labels in this study, there is much scope for applying this approach to – other grammar extraction strategies.</p><p>3 0.47030386 <a title="162-lda-3" href="./emnlp-2013-Pair_Language_Models_for_Deriving_Alternative_Pronunciations_and_Spellings_from_Pronunciation_Dictionaries.html">150 emnlp-2013-Pair Language Models for Deriving Alternative Pronunciations and Spellings from Pronunciation Dictionaries</a></p>
<p>Author: Russell Beckley ; Brian Roark</p><p>Abstract: Pronunciation dictionaries provide a readily available parallel corpus for learning to transduce between character strings and phoneme strings or vice versa. Translation models can be used to derive character-level paraphrases on either side of this transduction, allowing for the automatic derivation of alternative pronunciations or spellings. We examine finitestate and SMT-based methods for these related tasks, and demonstrate that the tasks have different characteristics finding alternative spellings is harder than alternative pronunciations and benefits from round-trip algorithms when the other does not. We also show that we can increase accuracy by modeling syllable stress. –</p><p>4 0.43932185 <a title="162-lda-4" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>Author: Jun-Ping Ng ; Min-Yen Kan ; Ziheng Lin ; Wei Feng ; Bin Chen ; Jian Su ; Chew Lim Tan</p><p>Abstract: In this paper we classify the temporal relations between pairs of events on an article-wide basis. This is in contrast to much of the existing literature which focuses on just event pairs which are found within the same or adjacent sentences. To achieve this, we leverage on discourse analysis as we believe that it provides more useful semantic information than typical lexico-syntactic features. We propose the use of several discourse analysis frameworks, including 1) Rhetorical Structure Theory (RST), 2) PDTB-styled discourse relations, and 3) topical text segmentation. We explain how features derived from these frameworks can be effectively used with support vector machines (SVM) paired with convolution kernels. Experiments show that our proposal is effective in improving on the state-of-the-art significantly by as much as 16% in terms of F1, even if we only adopt less-than-perfect automatic discourse analyzers and parsers. Making use of more accurate discourse analysis can further boost gains to 35%.</p><p>5 0.43452418 <a title="162-lda-5" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>Author: Margaret Mitchell ; Jacqui Aguilar ; Theresa Wilson ; Benjamin Van Durme</p><p>Abstract: We propose a novel approach to sentiment analysis for a low resource setting. The intuition behind this work is that sentiment expressed towards an entity, targeted sentiment, may be viewed as a span of sentiment expressed across the entity. This representation allows us to model sentiment detection as a sequence tagging problem, jointly discovering people and organizations along with whether there is sentiment directed towards them. We compare performance in both Spanish and English on microblog data, using only a sentiment lexicon as an external resource. By leveraging linguisticallyinformed features within conditional random fields (CRFs) trained to minimize empirical risk, our best models in Spanish significantly outperform a strong baseline, and reach around 90% accuracy on the combined task of named entity recognition and sentiment prediction. Our models in English, trained on a much smaller dataset, are not yet statistically significant against their baselines.</p><p>6 0.43148765 <a title="162-lda-6" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>7 0.42964956 <a title="162-lda-7" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>8 0.42928976 <a title="162-lda-8" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>9 0.42893958 <a title="162-lda-9" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>10 0.42874175 <a title="162-lda-10" href="./emnlp-2013-Collective_Opinion_Target_Extraction_in_Chinese_Microblogs.html">47 emnlp-2013-Collective Opinion Target Extraction in Chinese Microblogs</a></p>
<p>11 0.42718959 <a title="162-lda-11" href="./emnlp-2013-Leveraging_Lexical_Cohesion_and_Disruption_for_Topic_Segmentation.html">124 emnlp-2013-Leveraging Lexical Cohesion and Disruption for Topic Segmentation</a></p>
<p>12 0.42690557 <a title="162-lda-12" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>13 0.42664176 <a title="162-lda-13" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>14 0.42663318 <a title="162-lda-14" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>15 0.42611933 <a title="162-lda-15" href="./emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM.html">99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</a></p>
<p>16 0.42584109 <a title="162-lda-16" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>17 0.42508116 <a title="162-lda-17" href="./emnlp-2013-The_Effects_of_Syntactic_Features_in_Automatic_Prediction_of_Morphology.html">181 emnlp-2013-The Effects of Syntactic Features in Automatic Prediction of Morphology</a></p>
<p>18 0.4250485 <a title="162-lda-18" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>19 0.42495173 <a title="162-lda-19" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>20 0.42438611 <a title="162-lda-20" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
