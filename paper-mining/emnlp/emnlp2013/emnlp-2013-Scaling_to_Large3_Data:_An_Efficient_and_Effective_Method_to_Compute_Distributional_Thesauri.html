<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>165 emnlp-2013-Scaling to Large3 Data: An Efficient and Effective Method to Compute Distributional Thesauri</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-165" href="#">emnlp2013-165</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>165 emnlp-2013-Scaling to Large3 Data: An Efficient and Effective Method to Compute Distributional Thesauri</h1>
<br/><p>Source: <a title="emnlp-2013-165-pdf" href="http://aclweb.org/anthology//D/D13/D13-1089.pdf">pdf</a></p><p>Author: Martin Riedl ; Chris Biemann</p><p>Abstract: We introduce a new highly scalable approach for computing Distributional Thesauri (DTs). By employing pruning techniques and a distributed framework, we make the computation for very large corpora feasible on comparably small computational resources. We demonstrate this by releasing a DT for the whole vocabulary of Google Books syntactic n-grams. Evaluating against lexical resources using two measures, we show that our approach produces higher quality DTs than previous approaches, and is thus preferable in terms of speed and quality for large corpora.</p><p>Reference: <a title="emnlp-2013-165-reference" href="../emnlp2013_reference/emnlp-2013-Scaling_to_Large3_Data%3A_An_Efficient_and_Effective_Method_to_Compute_Distributional_Thesauri_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 By employing pruning techniques and a distributed framework, we make the computation for very large corpora feasible on comparably small computational resources. [sent-2, score-0.351]
</p><p>2 We demonstrate this by releasing a DT for the whole vocabulary of Google Books syntactic n-grams. [sent-3, score-0.071]
</p><p>3 Evaluating against lexical resources using two measures, we show that our approach produces higher quality DTs than previous approaches, and is thus preferable in terms of speed and quality for large corpora. [sent-4, score-0.087]
</p><p>4 1 Introduction Using larger data to estimate models for machine learning applications as well as for applications of  Natural Language Processing (NLP) has repeatedly shown to be advantageous, see e. [sent-5, score-0.042]
</p><p>5 In this work, we tackle the influence of corpus size for building a distributional thesaurus (Lin, 1998). [sent-9, score-0.592]
</p><p>6 Especially, we shed light on the interaction of similarity measures and corpus size, as well as aspects of scalability. [sent-10, score-0.259]
</p><p>7 We shortly introduce the JoBimText framework for distributional semantics and show its scalability for large corpora. [sent-11, score-0.273]
</p><p>8 For the computation of the data we follow the MapReduce (Dean and Ghemawat, 2004) paradigm. [sent-12, score-0.109]
</p><p>9 The computation of similarities between terms becomes challenging on large corpora, as both the numbers of terms to be compared and the number of context features increases. [sent-13, score-0.304]
</p><p>10 This makes standard similarity calculations as proposed in (Lin, 1998; Curran, 2002; Lund and Burgess, 1996; Weeds et al. [sent-14, score-0.141]
</p><p>11 These approaches first calculate an information measure between each word and the according context and then calculate the similarity between all words, based on the information measure for all shared contexts. [sent-18, score-0.499]
</p><p>12 2  Related Work  A variety of approaches to compute DTs have been proposed to tackle issues regarding size and runtime. [sent-19, score-0.127]
</p><p>13 The reduction of the feature space seems to be one possibility, but still requires the computation of such reduction cf. [sent-20, score-0.225]
</p><p>14 Other approaches use randomised indexing for storing counts or hashing functions to approximate counts and measures (Gorman and Curran, 2006; Goyal et al. [sent-23, score-0.359]
</p><p>15 Another possibility is the usage of distributed processing like MapReduce. [sent-25, score-0.149]
</p><p>16 6 Terawords), an amount of hardware only available to commercial search engines. [sent-30, score-0.097]
</p><p>17 Whereas Agirre uses a χ2 test to measure the information between terms and context, Pantel uses the Pointwise Mutual Information (PMI). [sent-31, score-0.086]
</p><p>18 Then, both approaches use the cosine similarity to calculate the similarity between terms. [sent-32, score-0.379]
</p><p>19 Furthermore, Pantel describes an optimization for the calculation of the cosine similarity. [sent-33, score-0.084]
</p><p>20 Whereas Pantel and Lin (2002) de-  scribe a method for sense clustering, they also use a method to calculate similarities between terms. [sent-34, score-0.27]
</p><p>21 Here, they propose a pruning scheme similar to ours, but do not explicitly evaluate its effect. [sent-35, score-0.061]
</p><p>22 The evaluation of DTs has been performed in extrinsic and intrinsic manner. [sent-36, score-0.174]
</p><p>23 hc o2d0s1 i3n A Nsastoucria lti Loan fgoura Cgoem Ppruotcaetsiosin agl, L piang eusis 8t8ic4s–890, set expansion (Pantel et al. [sent-41, score-0.032]
</p><p>24 , 2009) or phrase polarity identification (Goyal and Daum e´, 2011). [sent-42, score-0.034]
</p><p>25 In this work we will concentrate on intrinsic evaluations: Lin (1997; 1998) introduced two measures using WordNet (Miller, 1995) and Roget’s Thesaurus. [sent-43, score-0.21]
</p><p>26 Using WordNet, he defines context features (synsets a word occurs in Wordnet or subsets when using Roget’s Thesaurus) and then builds a gold standard thesaurus using a similarity measure. [sent-44, score-0.59]
</p><p>27 Then he evaluates his generated Distributional Thesaurus (DT) with respect to the gold standard thesauri. [sent-45, score-0.108]
</p><p>28 (2004) evaluate various similarity measures based on 1000 frequent and 1000 infrequent words. [sent-47, score-0.209]
</p><p>29 Curran (2004) created a gold standard thesaurus by manually extracting entries from several English thesauri  for 70 words. [sent-48, score-0.576]
</p><p>30 His automatically generated DTs are evaluated against this gold standard thesaurus using several measures. [sent-49, score-0.405]
</p><p>31 We will report on his measure and additionally propose a measure based on WordNet paths. [sent-50, score-0.106]
</p><p>32 3  Building a Distributional Thesaurus  Here we present our scalable DT algorithm using the MapReduce paradigm, which is divided into two parts: The holing system and a computational method to calculate distributional similarities. [sent-51, score-0.594]
</p><p>33 1 Holing System The holing operation splits an observation (e. [sent-54, score-0.313]
</p><p>34 a dependency relation) into a pair of two parts: a term and a context feature. [sent-56, score-0.034]
</p><p>35 These pairs are subsequently used for the computation of the similarities between terms, leading to a second-order relation. [sent-58, score-0.245]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dts', 0.425), ('thesaurus', 0.335), ('mapreduce', 0.284), ('holing', 0.245), ('riedl', 0.213), ('pantel', 0.174), ('thesauri', 0.171), ('distributional', 0.165), ('roget', 0.163), ('dt', 0.16), ('darmstadt', 0.142), ('weeds', 0.142), ('curran', 0.13), ('calculate', 0.118), ('biemann', 0.114), ('goyal', 0.114), ('computation', 0.109), ('wordnet', 0.097), ('extrinsic', 0.096), ('agirre', 0.096), ('similarities', 0.095), ('similarity', 0.089), ('measures', 0.08), ('intrinsic', 0.078), ('releasing', 0.071), ('burgess', 0.071), ('hashing', 0.071), ('sahlgren', 0.071), ('lin', 0.07), ('gold', 0.07), ('scalable', 0.066), ('possibility', 0.066), ('advantageous', 0.065), ('lund', 0.065), ('pruning', 0.061), ('shortly', 0.06), ('hardware', 0.06), ('adt', 0.06), ('dean', 0.06), ('tackle', 0.06), ('evaluations', 0.058), ('reduction', 0.058), ('scribe', 0.057), ('storing', 0.057), ('brill', 0.057), ('shed', 0.054), ('firstorder', 0.054), ('cores', 0.054), ('preferable', 0.054), ('fg', 0.054), ('measure', 0.053), ('calculations', 0.052), ('synsets', 0.052), ('concentrate', 0.052), ('banko', 0.052), ('comparably', 0.052), ('distributed', 0.052), ('cosine', 0.049), ('scalability', 0.048), ('universit', 0.046), ('miller', 0.046), ('billion', 0.043), ('books', 0.043), ('formalized', 0.043), ('repeatedly', 0.042), ('subsequently', 0.041), ('daum', 0.041), ('pmi', 0.041), ('counts', 0.04), ('infrequent', 0.04), ('brants', 0.04), ('germany', 0.04), ('whereas', 0.039), ('employing', 0.039), ('pointwise', 0.039), ('paradigm', 0.038), ('feasible', 0.038), ('evaluates', 0.038), ('parts', 0.038), ('commercial', 0.037), ('indexing', 0.037), ('splits', 0.037), ('light', 0.036), ('calculation', 0.035), ('polarity', 0.034), ('approaches', 0.034), ('context', 0.034), ('computationally', 0.033), ('blei', 0.033), ('terms', 0.033), ('regarding', 0.033), ('subsets', 0.032), ('expansion', 0.032), ('building', 0.032), ('usage', 0.031), ('operation', 0.031), ('scaling', 0.031), ('martin', 0.03), ('mutual', 0.03), ('builds', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="165-tfidf-1" href="./emnlp-2013-Scaling_to_Large3_Data%3A_An_Efficient_and_Effective_Method_to_Compute_Distributional_Thesauri.html">165 emnlp-2013-Scaling to Large3 Data: An Efficient and Effective Method to Compute Distributional Thesauri</a></p>
<p>Author: Martin Riedl ; Chris Biemann</p><p>Abstract: We introduce a new highly scalable approach for computing Distributional Thesauri (DTs). By employing pruning techniques and a distributed framework, we make the computation for very large corpora feasible on comparably small computational resources. We demonstrate this by releasing a DT for the whole vocabulary of Google Books syntactic n-grams. Evaluating against lexical resources using two measures, we show that our approach produces higher quality DTs than previous approaches, and is thus preferable in terms of speed and quality for large corpora.</p><p>2 0.073820084 <a title="165-tfidf-2" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<p>Author: Om P. Damani ; Shweta Ghonge</p><p>Abstract: Two recent measures incorporate the notion of statistical significance in basic PMI formulation. In some tasks, we find that the new measures perform worse than the PMI. Our analysis shows that while the basic ideas in incorporating statistical significance in PMI are reasonable, they have been applied slightly inappropriately. By fixing this, we get new measures that improve performance over not just PMI but on other popular co-occurrence measures as well. In fact, the revised measures perform reasonably well compared with more resource intensive non co-occurrence based methods also.</p><p>3 0.069385014 <a title="165-tfidf-3" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>Author: Yangfeng Ji ; Jacob Eisenstein</p><p>Abstract: Matrix and tensor factorization have been applied to a number of semantic relatedness tasks, including paraphrase identification. The key idea is that similarity in the latent space implies semantic relatedness. We describe three ways in which labeled data can improve the accuracy of these approaches on paraphrase classification. First, we design a new discriminative term-weighting metric called TF-KLD, which outperforms TF-IDF. Next, we show that using the latent representation from matrix factorization as features in a classification algorithm substantially improves accuracy. Finally, we combine latent features with fine-grained n-gram overlap features, yielding performance that is 3% more accurate than the prior state-of-the-art.</p><p>4 0.056859318 <a title="165-tfidf-4" href="./emnlp-2013-Learning_to_Rank_Lexical_Substitutions.html">123 emnlp-2013-Learning to Rank Lexical Substitutions</a></p>
<p>Author: Gyorgy Szarvas ; Robert Busa-Fekete ; Eyke Hullermeier</p><p>Abstract: The problem to replace a word with a synonym that fits well in its sentential context is known as the lexical substitution task. In this paper, we tackle this task as a supervised ranking problem. Given a dataset of target words, their sentential contexts and the potential substitutions for the target words, the goal is to train a model that accurately ranks the candidate substitutions based on their contextual fitness. As a key contribution, we customize and evaluate several learning-to-rank models to the lexical substitution task, including classification-based and regression-based approaches. On two datasets widely used for lexical substitution, our best models signifi- cantly advance the state-of-the-art.</p><p>5 0.053351238 <a title="165-tfidf-5" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>Author: Angeliki Lazaridou ; Eva Maria Vecchi ; Marco Baroni</p><p>Abstract: In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the correct parsing of NPs (e.g., (live fish) transporter rather than live (fish transporter)). We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features.</p><p>6 0.04971274 <a title="165-tfidf-6" href="./emnlp-2013-Multi-Relational_Latent_Semantic_Analysis.html">137 emnlp-2013-Multi-Relational Latent Semantic Analysis</a></p>
<p>7 0.046236049 <a title="165-tfidf-7" href="./emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations.html">17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</a></p>
<p>8 0.045583926 <a title="165-tfidf-8" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>9 0.043593794 <a title="165-tfidf-9" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<p>10 0.04162468 <a title="165-tfidf-10" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>11 0.038808934 <a title="165-tfidf-11" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>12 0.038366091 <a title="165-tfidf-12" href="./emnlp-2013-Boosting_Cross-Language_Retrieval_by_Learning_Bilingual_Phrase_Associations_from_Relevance_Rankings.html">39 emnlp-2013-Boosting Cross-Language Retrieval by Learning Bilingual Phrase Associations from Relevance Rankings</a></p>
<p>13 0.038186383 <a title="165-tfidf-13" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>14 0.0375879 <a title="165-tfidf-14" href="./emnlp-2013-Of_Words%2C_Eyes_and_Brains%3A_Correlating_Image-Based_Distributional_Semantic_Models_with_Neural_Representations_of_Concepts.html">140 emnlp-2013-Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts</a></p>
<p>15 0.036961462 <a title="165-tfidf-15" href="./emnlp-2013-The_Topology_of_Semantic_Knowledge.html">182 emnlp-2013-The Topology of Semantic Knowledge</a></p>
<p>16 0.036690183 <a title="165-tfidf-16" href="./emnlp-2013-Automatically_Identifying_Pseudepigraphic_Texts.html">37 emnlp-2013-Automatically Identifying Pseudepigraphic Texts</a></p>
<p>17 0.034388758 <a title="165-tfidf-17" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>18 0.034158837 <a title="165-tfidf-18" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>19 0.030939694 <a title="165-tfidf-19" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>20 0.029689575 <a title="165-tfidf-20" href="./emnlp-2013-Unsupervised_Induction_of_Cross-Lingual_Semantic_Relations.html">193 emnlp-2013-Unsupervised Induction of Cross-Lingual Semantic Relations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.106), (1, 0.012), (2, -0.036), (3, -0.007), (4, 0.009), (5, 0.086), (6, -0.006), (7, -0.025), (8, -0.069), (9, -0.043), (10, 0.019), (11, 0.042), (12, -0.059), (13, 0.047), (14, 0.029), (15, -0.057), (16, -0.066), (17, 0.016), (18, -0.024), (19, 0.047), (20, -0.039), (21, 0.027), (22, -0.045), (23, -0.025), (24, -0.009), (25, -0.078), (26, -0.09), (27, 0.088), (28, -0.034), (29, -0.088), (30, 0.005), (31, 0.057), (32, 0.052), (33, -0.036), (34, 0.016), (35, -0.027), (36, 0.098), (37, -0.109), (38, -0.148), (39, -0.083), (40, 0.046), (41, -0.154), (42, 0.078), (43, -0.023), (44, 0.099), (45, -0.004), (46, -0.033), (47, 0.088), (48, 0.015), (49, -0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96229821 <a title="165-lsi-1" href="./emnlp-2013-Scaling_to_Large3_Data%3A_An_Efficient_and_Effective_Method_to_Compute_Distributional_Thesauri.html">165 emnlp-2013-Scaling to Large3 Data: An Efficient and Effective Method to Compute Distributional Thesauri</a></p>
<p>Author: Martin Riedl ; Chris Biemann</p><p>Abstract: We introduce a new highly scalable approach for computing Distributional Thesauri (DTs). By employing pruning techniques and a distributed framework, we make the computation for very large corpora feasible on comparably small computational resources. We demonstrate this by releasing a DT for the whole vocabulary of Google Books syntactic n-grams. Evaluating against lexical resources using two measures, we show that our approach produces higher quality DTs than previous approaches, and is thus preferable in terms of speed and quality for large corpora.</p><p>2 0.73859763 <a title="165-lsi-2" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<p>Author: Om P. Damani ; Shweta Ghonge</p><p>Abstract: Two recent measures incorporate the notion of statistical significance in basic PMI formulation. In some tasks, we find that the new measures perform worse than the PMI. Our analysis shows that while the basic ideas in incorporating statistical significance in PMI are reasonable, they have been applied slightly inappropriately. By fixing this, we get new measures that improve performance over not just PMI but on other popular co-occurrence measures as well. In fact, the revised measures perform reasonably well compared with more resource intensive non co-occurrence based methods also.</p><p>3 0.55965281 <a title="165-lsi-3" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>Author: Yangfeng Ji ; Jacob Eisenstein</p><p>Abstract: Matrix and tensor factorization have been applied to a number of semantic relatedness tasks, including paraphrase identification. The key idea is that similarity in the latent space implies semantic relatedness. We describe three ways in which labeled data can improve the accuracy of these approaches on paraphrase classification. First, we design a new discriminative term-weighting metric called TF-KLD, which outperforms TF-IDF. Next, we show that using the latent representation from matrix factorization as features in a classification algorithm substantially improves accuracy. Finally, we combine latent features with fine-grained n-gram overlap features, yielding performance that is 3% more accurate than the prior state-of-the-art.</p><p>4 0.53819591 <a title="165-lsi-4" href="./emnlp-2013-Learning_to_Rank_Lexical_Substitutions.html">123 emnlp-2013-Learning to Rank Lexical Substitutions</a></p>
<p>Author: Gyorgy Szarvas ; Robert Busa-Fekete ; Eyke Hullermeier</p><p>Abstract: The problem to replace a word with a synonym that fits well in its sentential context is known as the lexical substitution task. In this paper, we tackle this task as a supervised ranking problem. Given a dataset of target words, their sentential contexts and the potential substitutions for the target words, the goal is to train a model that accurately ranks the candidate substitutions based on their contextual fitness. As a key contribution, we customize and evaluate several learning-to-rank models to the lexical substitution task, including classification-based and regression-based approaches. On two datasets widely used for lexical substitution, our best models signifi- cantly advance the state-of-the-art.</p><p>5 0.50804526 <a title="165-lsi-5" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>Author: Angeliki Lazaridou ; Eva Maria Vecchi ; Marco Baroni</p><p>Abstract: In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the correct parsing of NPs (e.g., (live fish) transporter rather than live (fish transporter)). We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features.</p><p>6 0.50201315 <a title="165-lsi-6" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>7 0.45518222 <a title="165-lsi-7" href="./emnlp-2013-Using_Paraphrases_and_Lexical_Semantics_to_Improve_the_Accuracy_and_the_Robustness_of_Supervised_Models_in_Situated_Dialogue_Systems.html">197 emnlp-2013-Using Paraphrases and Lexical Semantics to Improve the Accuracy and the Robustness of Supervised Models in Situated Dialogue Systems</a></p>
<p>8 0.44447854 <a title="165-lsi-8" href="./emnlp-2013-The_Topology_of_Semantic_Knowledge.html">182 emnlp-2013-The Topology of Semantic Knowledge</a></p>
<p>9 0.40664476 <a title="165-lsi-9" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>10 0.39864859 <a title="165-lsi-10" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>11 0.36672324 <a title="165-lsi-11" href="./emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations.html">17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</a></p>
<p>12 0.35912567 <a title="165-lsi-12" href="./emnlp-2013-Automatically_Identifying_Pseudepigraphic_Texts.html">37 emnlp-2013-Automatically Identifying Pseudepigraphic Texts</a></p>
<p>13 0.33808336 <a title="165-lsi-13" href="./emnlp-2013-Application_of_Localized_Similarity_for_Web_Documents.html">24 emnlp-2013-Application of Localized Similarity for Web Documents</a></p>
<p>14 0.33701232 <a title="165-lsi-14" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>15 0.33663258 <a title="165-lsi-15" href="./emnlp-2013-Well-Argued_Recommendation%3A_Adaptive_Models_Based_on_Words_in_Recommender_Systems.html">200 emnlp-2013-Well-Argued Recommendation: Adaptive Models Based on Words in Recommender Systems</a></p>
<p>16 0.33404598 <a title="165-lsi-16" href="./emnlp-2013-Centering_Similarity_Measures_to_Reduce_Hubs.html">44 emnlp-2013-Centering Similarity Measures to Reduce Hubs</a></p>
<p>17 0.33226576 <a title="165-lsi-17" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>18 0.32154077 <a title="165-lsi-18" href="./emnlp-2013-Mining_Scientific_Terms_and_their_Definitions%3A_A_Study_of_the_ACL_Anthology.html">132 emnlp-2013-Mining Scientific Terms and their Definitions: A Study of the ACL Anthology</a></p>
<p>19 0.3214559 <a title="165-lsi-19" href="./emnlp-2013-Animacy_Detection_with_Voting_Models.html">23 emnlp-2013-Animacy Detection with Voting Models</a></p>
<p>20 0.30152753 <a title="165-lsi-20" href="./emnlp-2013-Is_Twitter_A_Better_Corpus_for_Measuring_Sentiment_Similarity%3F.html">109 emnlp-2013-Is Twitter A Better Corpus for Measuring Sentiment Similarity?</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.015), (18, 0.024), (22, 0.038), (30, 0.066), (51, 0.106), (66, 0.015), (71, 0.014), (77, 0.019), (96, 0.605)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88267893 <a title="165-lda-1" href="./emnlp-2013-Scaling_to_Large3_Data%3A_An_Efficient_and_Effective_Method_to_Compute_Distributional_Thesauri.html">165 emnlp-2013-Scaling to Large3 Data: An Efficient and Effective Method to Compute Distributional Thesauri</a></p>
<p>Author: Martin Riedl ; Chris Biemann</p><p>Abstract: We introduce a new highly scalable approach for computing Distributional Thesauri (DTs). By employing pruning techniques and a distributed framework, we make the computation for very large corpora feasible on comparably small computational resources. We demonstrate this by releasing a DT for the whole vocabulary of Google Books syntactic n-grams. Evaluating against lexical resources using two measures, we show that our approach produces higher quality DTs than previous approaches, and is thus preferable in terms of speed and quality for large corpora.</p><p>2 0.7289229 <a title="165-lda-2" href="./emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations.html">17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</a></p>
<p>Author: Shashank Srivastava ; Dirk Hovy ; Eduard Hovy</p><p>Abstract: In this paper, we propose a walk-based graph kernel that generalizes the notion of treekernels to continuous spaces. Our proposed approach subsumes a general framework for word-similarity, and in particular, provides a flexible way to incorporate distributed representations. Using vector representations, such an approach captures both distributional semantic similarities among words as well as the structural relations between them (encoded as the structure of the parse tree). We show an efficient formulation to compute this kernel using simple matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results.</p><p>3 0.70325559 <a title="165-lda-3" href="./emnlp-2013-Joint_Coreference_Resolution_and_Named-Entity_Linking_with_Multi-Pass_Sieves.html">112 emnlp-2013-Joint Coreference Resolution and Named-Entity Linking with Multi-Pass Sieves</a></p>
<p>Author: Hannaneh Hajishirzi ; Leila Zilles ; Daniel S. Weld ; Luke Zettlemoyer</p><p>Abstract: Many errors in coreference resolution come from semantic mismatches due to inadequate world knowledge. Errors in named-entity linking (NEL), on the other hand, are often caused by superficial modeling of entity context. This paper demonstrates that these two tasks are complementary. We introduce NECO, a new model for named entity linking and coreference resolution, which solves both problems jointly, reducing the errors made on each. NECO extends the Stanford deterministic coreference system by automatically linking mentions to Wikipedia and introducing new NEL-informed mention-merging sieves. Linking improves mention-detection and enables new semantic attributes to be incorporated from Freebase, while coreference provides better context modeling by propagating named-entity links within mention clusters. Experiments show consistent improve- ments across a number of datasets and experimental conditions, including over 11% reduction in MUC coreference error and nearly 21% reduction in F1 NEL error on ACE 2004 newswire data.</p><p>4 0.65405691 <a title="165-lda-4" href="./emnlp-2013-A_Unified_Model_for_Topics%2C_Events_and_Users_on_Twitter.html">16 emnlp-2013-A Unified Model for Topics, Events and Users on Twitter</a></p>
<p>Author: Qiming Diao ; Jing Jiang</p><p>Abstract: With the rapid growth of social media, Twitter has become one of the most widely adopted platforms for people to post short and instant message. On the one hand, people tweets about their daily lives, and on the other hand, when major events happen, people also follow and tweet about them. Moreover, people’s posting behaviors on events are often closely tied to their personal interests. In this paper, we try to model topics, events and users on Twitter in a unified way. We propose a model which combines an LDA-like topic model and the Recurrent Chinese Restaurant Process to capture topics and events. We further propose a duration-based regularization component to find bursty events. We also propose to use event-topic affinity vectors to model the asso- . ciation between events and topics. Our experiments shows that our model can accurately identify meaningful events and the event-topic affinity vectors are effective for event recommendation and grouping events by topics.</p><p>5 0.65094686 <a title="165-lda-5" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>Author: Baichuan Li ; Jing Liu ; Chin-Yew Lin ; Irwin King ; Michael R. Lyu</p><p>Abstract: Social media like forums and microblogs have accumulated a huge amount of user generated content (UGC) containing human knowledge. Currently, most of UGC is listed as a whole or in pre-defined categories. This “list-based” approach is simple, but hinders users from browsing and learning knowledge of certain topics effectively. To address this problem, we propose a hierarchical entity-based approach for structuralizing UGC in social media. By using a large-scale entity repository, we design a three-step framework to organize UGC in a novel hierarchical structure called “cluster entity tree (CET)”. With Yahoo! Answers as a test case, we conduct experiments and the results show the effectiveness of our framework in constructing CET. We further evaluate the performance of CET on UGC organization in both user and system aspects. From a user aspect, our user study demonstrates that, with CET-based structure, users perform significantly better in knowledge learning than using traditional list-based approach. From a system aspect, CET substantially boosts the performance of two information retrieval models (i.e., vector space model and query likelihood language model).</p><p>6 0.41426495 <a title="165-lda-6" href="./emnlp-2013-Microblog_Entity_Linking_by_Leveraging_Extra_Posts.html">130 emnlp-2013-Microblog Entity Linking by Leveraging Extra Posts</a></p>
<p>7 0.41021907 <a title="165-lda-7" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>8 0.40223017 <a title="165-lda-8" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>9 0.38619095 <a title="165-lda-9" href="./emnlp-2013-A_Constrained_Latent_Variable_Model_for_Coreference_Resolution.html">1 emnlp-2013-A Constrained Latent Variable Model for Coreference Resolution</a></p>
<p>10 0.38495195 <a title="165-lda-10" href="./emnlp-2013-Well-Argued_Recommendation%3A_Adaptive_Models_Based_on_Words_in_Recommender_Systems.html">200 emnlp-2013-Well-Argued Recommendation: Adaptive Models Based on Words in Recommender Systems</a></p>
<p>11 0.37631989 <a title="165-lda-11" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>12 0.37554672 <a title="165-lda-12" href="./emnlp-2013-Exploiting_Zero_Pronouns_to_Improve_Chinese_Coreference_Resolution.html">80 emnlp-2013-Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</a></p>
<p>13 0.36814117 <a title="165-lda-13" href="./emnlp-2013-A_temporal_model_of_text_periodicities_using_Gaussian_Processes.html">18 emnlp-2013-A temporal model of text periodicities using Gaussian Processes</a></p>
<p>14 0.36474636 <a title="165-lda-14" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>15 0.3420155 <a title="165-lda-15" href="./emnlp-2013-Easy_Victories_and_Uphill_Battles_in_Coreference_Resolution.html">67 emnlp-2013-Easy Victories and Uphill Battles in Coreference Resolution</a></p>
<p>16 0.34104371 <a title="165-lda-16" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>17 0.33434469 <a title="165-lda-17" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>18 0.33377162 <a title="165-lda-18" href="./emnlp-2013-Gender_Inference_of_Twitter_Users_in_Non-English_Contexts.html">89 emnlp-2013-Gender Inference of Twitter Users in Non-English Contexts</a></p>
<p>19 0.3302719 <a title="165-lda-19" href="./emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model.html">75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</a></p>
<p>20 0.3251912 <a title="165-lda-20" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
