<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>88 emnlp-2013-Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest-to-String Decoding</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-88" href="#">emnlp2013-88</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>88 emnlp-2013-Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest-to-String Decoding</h1>
<br/><p>Source: <a title="emnlp-2013-88-pdf" href="http://aclweb.org/anthology//D/D13/D13-1052.pdf">pdf</a></p><p>Author: Martin Cmejrek ; Haitao Mi ; Bowen Zhou</p><p>Abstract: Machine translation benefits from system combination. We propose flexible interaction of hypergraphs as a novel technique combining different translation models within one decoder. We introduce features controlling the interactions between the two systems and explore three interaction schemes of hiero and forest-to-string models—specification, generalization, and interchange. The experiments are carried out on large training data with strong baselines utilizing rich sets of dense and sparse features. All three schemes significantly improve results of any single system on four testsets. We find that specification—a more constrained scheme that almost entirely uses forest-to-string rules, but optionally uses hiero rules for shorter spans—comes out as the strongest, yielding improvement up to 0.9 (T -B )/2 points. We also provide a detailed experimental and qualitative analysis of the results.</p><p>Reference: <a title="emnlp-2013-88-reference" href="../emnlp2013_reference/emnlp-2013-Flexible_and_Efficient_Hypergraph_Interactions_for_Joint_Hierarchical_and_Forest-to-String_Decoding_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest-to-String Decoding∗  Cˇmejrek†‡  Martin †IBM Prague mReejseraerkc†h‡ Lab V P rPagarukeu R R2e2s9e4a/r4c Prague, Czech Republic, 148 00 martin cmej rek@us ibm. [sent-1, score-0.033]
</p><p>2 We propose flexible interaction of hypergraphs as a novel technique combining different translation models within one decoder. [sent-6, score-0.487]
</p><p>3 We introduce features controlling the interactions between the two systems and explore three interaction schemes of hiero and forest-to-string models—specification, generalization, and interchange. [sent-7, score-0.697]
</p><p>4 All three schemes significantly improve results of any single system on four testsets. [sent-9, score-0.15]
</p><p>5 We find that specification—a more constrained scheme that almost entirely uses forest-to-string rules, but optionally uses  hiero rules for shorter spans—comes out as the strongest, yielding improvement up to 0. [sent-10, score-0.419]
</p><p>6 1 Introduction Recent years have witnessed the success of various statistical machine translation (SMT) models using different levels of linguistic knowledge– phrase (Koehn et al. [sent-13, score-0.24]
</p><p>7 , 2003), hiero (Chiang, 2005), and syntax-based (Liu et al. [sent-14, score-0.312]
</p><p>8 System combination became a promising way of building up synergy from different SMT systems and their specific merits. [sent-17, score-0.135]
</p><p>9 , 2010) aims at producing consensus translations from the outputs of multiple individual systems. [sent-32, score-0.198]
</p><p>10 Those outputs usually contain k-best lists of translations, which only explore a small portion of the entire search space of each system. [sent-33, score-0.056]
</p><p>11 This issue is well addressed in joint decoding (Liu et al. [sent-34, score-0.283]
</p><p>12 , 2009), or online system combination, showing comparable improvements to the offline combination methods. [sent-35, score-0.099]
</p><p>13 Rather than finding consensus translations from the outputs of individual systems, joint decoding works with different grammars at the decoding time. [sent-36, score-0.738]
</p><p>14 Although limited to individual systems sharing the same search paradigm (e. [sent-37, score-0.099]
</p><p>15 left-to-right or bottom-up), joint decoding offers many potential advatages: search through a larger space, better efficiency, features designed once for all subsystems, potential cross-system features, online sharing of partial hypotheses, and many others. [sent-39, score-0.334]
</p><p>16 Different approaches have different strengths in general–hiero rules are believed to provide reliable  lexical coverage, while tree-to-string rules are good at non-local reorderings. [sent-40, score-0.24]
</p><p>17 In this work, we study different schemes of interaction between translation models, reflecting their specific strengths at different (syntactic) contexts. [sent-42, score-0.531]
</p><p>18 We make five new contributions: First, we propose a framework for joint decoding by means of flexible combination of translation hypergraphs, allowing for detailed conProceSe datintlges, o Wfa tsh ein 2g01to3n, C UoSnfAe,re 1n8c-e2 o1n O Ecmtopbier ic 2a0l1 M3. [sent-43, score-0.578]
</p><p>19 hc o2d0s1 i3n A Nsastoucria lti Loan fgoura Cgoem Ppruotcaetsiosin agl, L piang eusis 5t4ic5s–5 5, trol of interactions between the different systems using soft constraints (Section 3). [sent-45, score-0.105]
</p><p>20 Second, we study three interaction schemes– special cases of joint decoding: generalization, specification, and interchange (Section 3. [sent-46, score-0.255]
</p><p>21 Third, instead of using a tree-to-string system, we use a much stronger forest-to-string system with fuzzy match ofnonterminal categories (Section 2. [sent-48, score-0.062]
</p><p>22 Fifth, we conduct a comprehensive experimental analysis, and find that joint decoding actually prefers tree-to-string rules in both shorter and longer spans. [sent-54, score-0.39]
</p><p>23 2  Individual Models  Our individual models are two state-of-the-art systems: a hiero model (Chiang, 2005), and a forest-tostring model (Mi et al. [sent-58, score-0.36]
</p><p>24 We will use the following example from Chinese to English to explain both individual and joint decoding algorithms throughout this paper. [sent-60, score-0.331]
</p><p>25 t ˇaol` un h u`i zˇ enmey a`ng discussion/NN will/VV how/VV discuss/VV meeting/NN There are several possible meanings based on the different POS tagging sequences: 1: NN VV VV: How is the discussion going? [sent-61, score-0.286]
</p><p>26 IP :NP  VP  x1  i  VV  x2:VP →  i  x2 is x1 going  h u`i Figure 1: Tree-to-string rule  r7. [sent-67, score-0.198]
</p><p>27 Table 1 shows translation rules that can generate all four translations. [sent-68, score-0.275]
</p><p>28 We will use those rules in the following sections. [sent-69, score-0.068]
</p><p>29 , 2008) is a linguistic syntax-based system, which significantly improves the translation quality of the tree-to-string  model (Liu et al. [sent-72, score-0.207]
</p><p>30 , 2006) by using a packed parse forest as the input instead of a single parse tree. [sent-74, score-0.37]
</p><p>31 Figure 1 shows a tree-to-string translation rule (Huang et al. [sent-75, score-0.295]
</p><p>32 }; rhs(r) hi us` t”h)e o target-side string expressed in targetlanguage words (like “going”) and variables; and ψ(r) is a mapping from X to nonterminals. [sent-79, score-0.144]
</p><p>33 Each IP0, 3  (a)V 0,1NP0,1NP1,2IP1,3V 1,2VP1 ,3P2,3⇒Rt t ˇaol` un  h u`i  zˇ enmey a`ng  ⇓ Rh  X0,3 e11 (b0)  X0,2  X1,3V  ⇒  e14  X2,3 t ˇaol` un  h u`i  zˇ enmey a`ng  t ˇaol` un  h u`i ⇓  zˇ enmey a`ng  Figure 2: Parse and translation hypergraphs. [sent-80, score-1.065]
</p><p>34 (b) The corresponding translation forest Ft after applying the tree-to-string translation rule set Rt. [sent-83, score-0.721]
</p><p>35 e7) has the same index as the corresponding rule (r7). [sent-87, score-0.088]
</p><p>36 VP1,3) became inaccessible due to the insufficient rule coverage. [sent-90, score-0.189]
</p><p>37 (b0) The translation forest Fh after applying the hierarchical rule set Rh to the input sentence. [sent-91, score-0.555]
</p><p>38 (c) The combined translation forest Hm obtained by superimposing b and b0. [sent-92, score-0.426]
</p><p>39 The nodes within each solid box share the same span. [sent-93, score-0.109]
</p><p>40 The forest-to-string system can produce the translation 1 (dashed derivation: r2, r4 and r7) and 2 (solid derivation: r1, r3, r40, r5, and r6). [sent-95, score-0.207]
</p><p>41 Hierarchical rules generate the translation 3 (r11 and r13). [sent-96, score-0.275]
</p><p>42 The translation 4 is available by using joint decoding at X1, 3 → IP1, 3 with the derivation: r1, r6, r12, and r14. [sent-97, score-0.49]
</p><p>43 sTa ekxae cthtley rounlece r7 nin lh Figure 1d for example, we have: lhs(r7) = IP(x1 :NP VP(VV(h u`i) x2:VP)), rhs(r7) = x2 is x1 going,  ψ(r7) = {x1  →  NP,  x2 →  VP}. [sent-99, score-0.035]
</p><p>44 Typically, a forest-to-string system performs translation in two steps (shown in Figure 2): parsing and decoding. [sent-100, score-0.207]
</p><p>45 In the parsing step, we convert the source language input into a parse forest (a). [sent-101, score-0.316]
</p><p>46 In the decoding step, we first convert the parse forest into a translation forest Ft in (b) by using the fast pattern547 matching technique (Zhang et al. [sent-102, score-0.964]
</p><p>47 For example, we pattern-match the rule r7 rooted at IP0, 3, in such a way that x1 spans NP0, 1 and x2 spans VP2, 3, and add a translation hyperedge e7 in (b). [sent-104, score-0.514]
</p><p>48 Then the decoder searches for the best derivation on the translation forest and outputs the target string. [sent-105, score-0.564]
</p><p>49 2  Hiero  Hiero (hierarchical phrase-based) model (Chiang, 2005) acquires rules of synchronous context-free grammars (SCFGs) from word-aligned parallel data, and uses plain sequences of words as the input, without any syntactic information. [sent-107, score-0.136]
</p><p>50 I P 01 , 3 X 01 , 3 schemeinteraction edges in supernode IP01,3X01,3 Generalization IP1,3 X1,3 IP01,3X01,3 Specification IP1,3 X1,3 IP01,3X01,3 Interchange IP1,3  X1,3  Figure 3: Three interaction schemes for joint decoding. [sent-108, score-0.405]
</p><p>51 Details of the interaction supernode for span (1, 3) shown in Figure 2 (c). [sent-109, score-0.194]
</p><p>52 SCFG can be formalized as a set of tuples hlhs(r), rhs(r), φ(r)i, where lhs(r) is the source-side holnhes-(lre)v,rehl CFG, wr)ih,os weh root hiss( rX) or S, saonudr we-hsoidsee  frontier nodes are labeled by source-language words (like “h` ui”) or variables from a set X = {x1, x2, . [sent-111, score-0.231]
</p><p>53 }; rhs(r) hi us` t”h)e o target-side string expressed in targetlanguage words (like “going”) and variables; and φ(r) is a mapping from X to nonterminals. [sent-114, score-0.144]
</p><p>54 Table 1 sφh(row) iss examples ogf f rhoiemro X Xrul teos n r11–r15. [sent-115, score-0.035]
</p><p>55 Although different on source side, hiero decoding can be formalized equally as forest-to-string decoding: First, pattern-match the input sentence into a translation forest Fh. [sent-116, score-1.041]
</p><p>56 For example, since the rule r11 matches “z ˇenmey a`ng” such that x1 spans the first two words, add a hyperedge e11 in Figure 2 (b0). [sent-117, score-0.244]
</p><p>57 Then search for the best derivation over the translation forest. [sent-118, score-0.289]
</p><p>58 3  Joint Decoding  The goal of joint decoding is to let different MT models collaborate within the framework of a single decoder. [sent-119, score-0.318]
</p><p>59 This can be done by combining translation hypergraphs of the different models at the decoding time, so that online sharing of partial hypotheses overcomes weaknesses and boosts strengths of the systems combined. [sent-120, score-0.787]
</p><p>60 548 As both forest-to-string and hiero produce translation forests that share the same hypergraph structure, we first formalize the hypergraph, then we introduce an algorithm to combine different hypergraphs, and finally we describe three joint decoding schemes over the merged hypergraph. [sent-121, score-1.047]
</p><p>61 1 Hypergraphs More formally, a hypergraph H is a pair hV, Ei, wMhoerere fVo rism tahlley set o hfy nodes, pahnd H HE tihse a set oifr hyperedges. [sent-123, score-0.134]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('vv', 0.341), ('hiero', 0.312), ('rhs', 0.233), ('enmey', 0.223), ('decoding', 0.222), ('vp', 0.219), ('forest', 0.219), ('translation', 0.207), ('aol', 0.179), ('nmey', 0.179), ('ip', 0.163), ('schemes', 0.15), ('lhs', 0.132), ('hypergraphs', 0.127), ('specification', 0.119), ('going', 0.11), ('np', 0.109), ('interaction', 0.105), ('hypergraph', 0.095), ('hyperedge', 0.093), ('hlhs', 0.089), ('interchange', 0.089), ('supernode', 0.089), ('rule', 0.088), ('ng', 0.082), ('derivation', 0.082), ('ol', 0.079), ('nn', 0.073), ('targetlanguage', 0.071), ('strengths', 0.069), ('rules', 0.068), ('interactions', 0.068), ('un', 0.063), ('spans', 0.063), ('became', 0.062), ('controlling', 0.062), ('fuzzy', 0.062), ('joint', 0.061), ('offline', 0.059), ('frontier', 0.059), ('parse', 0.059), ('solid', 0.059), ('outputs', 0.056), ('consensus', 0.054), ('mi', 0.054), ('ui', 0.052), ('sharing', 0.051), ('chiang', 0.05), ('nodes', 0.05), ('flexible', 0.048), ('individual', 0.048), ('rh', 0.048), ('formalized', 0.048), ('generalization', 0.047), ('variables', 0.041), ('hierarchical', 0.041), ('combination', 0.04), ('translations', 0.04), ('hypotheses', 0.039), ('shorter', 0.039), ('rek', 0.039), ('hz', 0.039), ('vin', 0.039), ('hm', 0.039), ('ihne', 0.039), ('inaccessible', 0.039), ('oifr', 0.039), ('overcomes', 0.039), ('scfgs', 0.039), ('convert', 0.038), ('discuss', 0.038), ('smt', 0.037), ('hi', 0.037), ('soft', 0.037), ('ft', 0.037), ('string', 0.036), ('yorktown', 0.035), ('lh', 0.035), ('hmi', 0.035), ('tyh', 0.035), ('rx', 0.035), ('ogf', 0.035), ('believed', 0.035), ('collaborate', 0.035), ('grammars', 0.035), ('prague', 0.034), ('equally', 0.033), ('martin', 0.033), ('bowen', 0.033), ('heights', 0.033), ('synergy', 0.033), ('nal', 0.033), ('packed', 0.033), ('rosti', 0.033), ('weh', 0.033), ('iw', 0.033), ('acquires', 0.033), ('weaknesses', 0.033), ('witnessed', 0.033), ('liu', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="88-tfidf-1" href="./emnlp-2013-Flexible_and_Efficient_Hypergraph_Interactions_for_Joint_Hierarchical_and_Forest-to-String_Decoding.html">88 emnlp-2013-Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest-to-String Decoding</a></p>
<p>Author: Martin Cmejrek ; Haitao Mi ; Bowen Zhou</p><p>Abstract: Machine translation benefits from system combination. We propose flexible interaction of hypergraphs as a novel technique combining different translation models within one decoder. We introduce features controlling the interactions between the two systems and explore three interaction schemes of hiero and forest-to-string models—specification, generalization, and interchange. The experiments are carried out on large training data with strong baselines utilizing rich sets of dense and sparse features. All three schemes significantly improve results of any single system on four testsets. We find that specification—a more constrained scheme that almost entirely uses forest-to-string rules, but optionally uses hiero rules for shorter spans—comes out as the strongest, yielding improvement up to 0.9 (T -B )/2 points. We also provide a detailed experimental and qualitative analysis of the results.</p><p>2 0.25889593 <a title="88-tfidf-2" href="./emnlp-2013-Efficient_Left-to-Right_Hierarchical_Phrase-Based_Translation_with_Improved_Reordering.html">71 emnlp-2013-Efficient Left-to-Right Hierarchical Phrase-Based Translation with Improved Reordering</a></p>
<p>Author: Maryam Siahbani ; Baskaran Sankaran ; Anoop Sarkar</p><p>Abstract: Left-to-right (LR) decoding (Watanabe et al., 2006b) is a promising decoding algorithm for hierarchical phrase-based translation (Hiero). It generates the target sentence by extending the hypotheses only on the right edge. LR decoding has complexity O(n2b) for input of n words and beam size b, compared to O(n3) for the CKY algorithm. It requires a single language model (LM) history for each target hypothesis rather than two LM histories per hypothesis as in CKY. In this paper we present an augmented LR decoding algorithm that builds on the original algorithm in (Watanabe et al., 2006b). Unlike that algorithm, using experiments over multiple language pairs we show two new results: our LR decoding algorithm provides demonstrably more efficient decoding than CKY Hiero, four times faster; and by introducing new distortion and reordering features for LR decoding, it maintains the same translation quality (as in BLEU scores) ob- tained phrase-based and CKY Hiero with the same translation model.</p><p>3 0.20308894 <a title="88-tfidf-3" href="./emnlp-2013-What_is_Hidden_among_Translation_Rules.html">201 emnlp-2013-What is Hidden among Translation Rules</a></p>
<p>Author: Libin Shen ; Bowen Zhou</p><p>Abstract: Most of the machine translation systems rely on a large set of translation rules. These rules are treated as discrete and independent events. In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. We present a preliminary generative model to test this idea. Experimental results show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation.</p><p>4 0.19321239 <a title="88-tfidf-4" href="./emnlp-2013-Factored_Soft_Source_Syntactic_Constraints_for_Hierarchical_Machine_Translation.html">84 emnlp-2013-Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation</a></p>
<p>Author: Zhongqiang Huang ; Jacob Devlin ; Rabih Zbib</p><p>Abstract: Translation Jacob Devlin Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA j devl in@bbn . com Rabih Zbib Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA r zbib@bbn . com have tried to introduce grammaticality to the transThis paper describes a factored approach to incorporating soft source syntactic constraints into a hierarchical phrase-based translation system. In contrast to traditional approaches that directly introduce syntactic constraints to translation rules by explicitly decorating them with syntactic annotations, which often exacerbate the data sparsity problem and cause other problems, our approach keeps translation rules intact and factorizes the use of syntactic constraints through two separate models: 1) a syntax mismatch model that associates each nonterminal of a translation rule with a distribution of tags that is used to measure the degree of syntactic compatibility of the translation rule on source spans; 2) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language. The features produced by both models are used as soft constraints to guide the translation process. Experiments on Chinese-English translation show that the proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets.</p><p>5 0.18487945 <a title="88-tfidf-5" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>Author: Xinyan Xiao ; Deyi Xiong</p><p>Abstract: Traditional synchronous grammar induction estimates parameters by maximizing likelihood, which only has a loose relation to translation quality. Alternatively, we propose a max-margin estimation approach to discriminatively inducing synchronous grammars for machine translation, which directly optimizes translation quality measured by BLEU. In the max-margin estimation of parameters, we only need to calculate Viterbi translations. This further facilitates the incorporation of various non-local features that are defined on the target side. We test the effectiveness of our max-margin estimation framework on a competitive hierarchical phrase-based system. Experiments show that our max-margin method significantly outperforms the traditional twostep pipeline for synchronous rule extraction by 1.3 BLEU points and is also better than previous max-likelihood estimation method.</p><p>6 0.1470295 <a title="88-tfidf-6" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>7 0.1173916 <a title="88-tfidf-7" href="./emnlp-2013-Max-Violation_Perceptron_and_Forced_Decoding_for_Scalable_MT_Training.html">128 emnlp-2013-Max-Violation Perceptron and Forced Decoding for Scalable MT Training</a></p>
<p>8 0.11542978 <a title="88-tfidf-8" href="./emnlp-2013-Optimal_Beam_Search_for_Machine_Translation.html">145 emnlp-2013-Optimal Beam Search for Machine Translation</a></p>
<p>9 0.098773874 <a title="88-tfidf-9" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>10 0.095722243 <a title="88-tfidf-10" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>11 0.086560488 <a title="88-tfidf-11" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>12 0.082394622 <a title="88-tfidf-12" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>13 0.079718761 <a title="88-tfidf-13" href="./emnlp-2013-Online_Learning_for_Inexact_Hypergraph_Search.html">141 emnlp-2013-Online Learning for Inexact Hypergraph Search</a></p>
<p>14 0.07831277 <a title="88-tfidf-14" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>15 0.070857309 <a title="88-tfidf-15" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>16 0.063996762 <a title="88-tfidf-16" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>17 0.060631171 <a title="88-tfidf-17" href="./emnlp-2013-Shift-Reduce_Word_Reordering_for_Machine_Translation.html">171 emnlp-2013-Shift-Reduce Word Reordering for Machine Translation</a></p>
<p>18 0.059977829 <a title="88-tfidf-18" href="./emnlp-2013-Exploiting_Zero_Pronouns_to_Improve_Chinese_Coreference_Resolution.html">80 emnlp-2013-Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</a></p>
<p>19 0.059693012 <a title="88-tfidf-19" href="./emnlp-2013-Dependency-Based_Decipherment_for_Resource-Limited_Machine_Translation.html">57 emnlp-2013-Dependency-Based Decipherment for Resource-Limited Machine Translation</a></p>
<p>20 0.05932131 <a title="88-tfidf-20" href="./emnlp-2013-Monolingual_Marginal_Matching_for_Translation_Model_Adaptation.html">135 emnlp-2013-Monolingual Marginal Matching for Translation Model Adaptation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.199), (1, -0.251), (2, 0.123), (3, 0.062), (4, 0.098), (5, -0.055), (6, -0.022), (7, -0.029), (8, 0.084), (9, 0.132), (10, -0.09), (11, 0.0), (12, -0.087), (13, 0.082), (14, 0.001), (15, 0.007), (16, 0.033), (17, 0.005), (18, 0.001), (19, -0.062), (20, -0.04), (21, 0.013), (22, -0.076), (23, -0.209), (24, -0.122), (25, -0.038), (26, 0.067), (27, -0.042), (28, -0.089), (29, 0.052), (30, -0.107), (31, -0.081), (32, -0.036), (33, -0.055), (34, 0.115), (35, -0.087), (36, 0.155), (37, 0.084), (38, -0.049), (39, -0.019), (40, 0.009), (41, -0.014), (42, 0.026), (43, 0.002), (44, -0.02), (45, 0.006), (46, 0.053), (47, -0.105), (48, -0.003), (49, 0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96173048 <a title="88-lsi-1" href="./emnlp-2013-Flexible_and_Efficient_Hypergraph_Interactions_for_Joint_Hierarchical_and_Forest-to-String_Decoding.html">88 emnlp-2013-Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest-to-String Decoding</a></p>
<p>Author: Martin Cmejrek ; Haitao Mi ; Bowen Zhou</p><p>Abstract: Machine translation benefits from system combination. We propose flexible interaction of hypergraphs as a novel technique combining different translation models within one decoder. We introduce features controlling the interactions between the two systems and explore three interaction schemes of hiero and forest-to-string models—specification, generalization, and interchange. The experiments are carried out on large training data with strong baselines utilizing rich sets of dense and sparse features. All three schemes significantly improve results of any single system on four testsets. We find that specification—a more constrained scheme that almost entirely uses forest-to-string rules, but optionally uses hiero rules for shorter spans—comes out as the strongest, yielding improvement up to 0.9 (T -B )/2 points. We also provide a detailed experimental and qualitative analysis of the results.</p><p>2 0.79649127 <a title="88-lsi-2" href="./emnlp-2013-What_is_Hidden_among_Translation_Rules.html">201 emnlp-2013-What is Hidden among Translation Rules</a></p>
<p>Author: Libin Shen ; Bowen Zhou</p><p>Abstract: Most of the machine translation systems rely on a large set of translation rules. These rules are treated as discrete and independent events. In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. We present a preliminary generative model to test this idea. Experimental results show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation.</p><p>3 0.75208688 <a title="88-lsi-3" href="./emnlp-2013-Efficient_Left-to-Right_Hierarchical_Phrase-Based_Translation_with_Improved_Reordering.html">71 emnlp-2013-Efficient Left-to-Right Hierarchical Phrase-Based Translation with Improved Reordering</a></p>
<p>Author: Maryam Siahbani ; Baskaran Sankaran ; Anoop Sarkar</p><p>Abstract: Left-to-right (LR) decoding (Watanabe et al., 2006b) is a promising decoding algorithm for hierarchical phrase-based translation (Hiero). It generates the target sentence by extending the hypotheses only on the right edge. LR decoding has complexity O(n2b) for input of n words and beam size b, compared to O(n3) for the CKY algorithm. It requires a single language model (LM) history for each target hypothesis rather than two LM histories per hypothesis as in CKY. In this paper we present an augmented LR decoding algorithm that builds on the original algorithm in (Watanabe et al., 2006b). Unlike that algorithm, using experiments over multiple language pairs we show two new results: our LR decoding algorithm provides demonstrably more efficient decoding than CKY Hiero, four times faster; and by introducing new distortion and reordering features for LR decoding, it maintains the same translation quality (as in BLEU scores) ob- tained phrase-based and CKY Hiero with the same translation model.</p><p>4 0.71590269 <a title="88-lsi-4" href="./emnlp-2013-Max-Margin_Synchronous_Grammar_Induction_for_Machine_Translation.html">127 emnlp-2013-Max-Margin Synchronous Grammar Induction for Machine Translation</a></p>
<p>Author: Xinyan Xiao ; Deyi Xiong</p><p>Abstract: Traditional synchronous grammar induction estimates parameters by maximizing likelihood, which only has a loose relation to translation quality. Alternatively, we propose a max-margin estimation approach to discriminatively inducing synchronous grammars for machine translation, which directly optimizes translation quality measured by BLEU. In the max-margin estimation of parameters, we only need to calculate Viterbi translations. This further facilitates the incorporation of various non-local features that are defined on the target side. We test the effectiveness of our max-margin estimation framework on a competitive hierarchical phrase-based system. Experiments show that our max-margin method significantly outperforms the traditional twostep pipeline for synchronous rule extraction by 1.3 BLEU points and is also better than previous max-likelihood estimation method.</p><p>5 0.66877371 <a title="88-lsi-5" href="./emnlp-2013-Factored_Soft_Source_Syntactic_Constraints_for_Hierarchical_Machine_Translation.html">84 emnlp-2013-Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation</a></p>
<p>Author: Zhongqiang Huang ; Jacob Devlin ; Rabih Zbib</p><p>Abstract: Translation Jacob Devlin Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA j devl in@bbn . com Rabih Zbib Raytheon BBN Technologies 50 Moulton St Cambridge, MA, USA r zbib@bbn . com have tried to introduce grammaticality to the transThis paper describes a factored approach to incorporating soft source syntactic constraints into a hierarchical phrase-based translation system. In contrast to traditional approaches that directly introduce syntactic constraints to translation rules by explicitly decorating them with syntactic annotations, which often exacerbate the data sparsity problem and cause other problems, our approach keeps translation rules intact and factorizes the use of syntactic constraints through two separate models: 1) a syntax mismatch model that associates each nonterminal of a translation rule with a distribution of tags that is used to measure the degree of syntactic compatibility of the translation rule on source spans; 2) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language. The features produced by both models are used as soft constraints to guide the translation process. Experiments on Chinese-English translation show that the proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets.</p><p>6 0.64062488 <a title="88-lsi-6" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>7 0.63382912 <a title="88-lsi-7" href="./emnlp-2013-Optimal_Beam_Search_for_Machine_Translation.html">145 emnlp-2013-Optimal Beam Search for Machine Translation</a></p>
<p>8 0.62188613 <a title="88-lsi-8" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>9 0.54926461 <a title="88-lsi-9" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>10 0.50478685 <a title="88-lsi-10" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>11 0.42846125 <a title="88-lsi-11" href="./emnlp-2013-Max-Violation_Perceptron_and_Forced_Decoding_for_Scalable_MT_Training.html">128 emnlp-2013-Max-Violation Perceptron and Forced Decoding for Scalable MT Training</a></p>
<p>12 0.41518658 <a title="88-lsi-12" href="./emnlp-2013-Recurrent_Continuous_Translation_Models.html">156 emnlp-2013-Recurrent Continuous Translation Models</a></p>
<p>13 0.41029596 <a title="88-lsi-13" href="./emnlp-2013-Improving_Statistical_Machine_Translation_with_Word_Class_Models.html">104 emnlp-2013-Improving Statistical Machine Translation with Word Class Models</a></p>
<p>14 0.37077984 <a title="88-lsi-14" href="./emnlp-2013-Tree_Kernel-based_Negation_and_Speculation_Scope_Detection_with_Structured_Syntactic_Parse_Features.html">188 emnlp-2013-Tree Kernel-based Negation and Speculation Scope Detection with Structured Syntactic Parse Features</a></p>
<p>15 0.36107272 <a title="88-lsi-15" href="./emnlp-2013-Rule-Based_Information_Extraction_is_Dead%21_Long_Live_Rule-Based_Information_Extraction_Systems%21.html">161 emnlp-2013-Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!</a></p>
<p>16 0.35081092 <a title="88-lsi-16" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>17 0.34616569 <a title="88-lsi-17" href="./emnlp-2013-Translating_into_Morphologically_Rich_Languages_with_Synthetic_Phrases.html">186 emnlp-2013-Translating into Morphologically Rich Languages with Synthetic Phrases</a></p>
<p>18 0.33186066 <a title="88-lsi-18" href="./emnlp-2013-Optimal_Incremental_Parsing_via_Best-First_Dynamic_Programming.html">146 emnlp-2013-Optimal Incremental Parsing via Best-First Dynamic Programming</a></p>
<p>19 0.33183163 <a title="88-lsi-19" href="./emnlp-2013-A_Multi-Teraflop_Constituency_Parser_using_GPUs.html">10 emnlp-2013-A Multi-Teraflop Constituency Parser using GPUs</a></p>
<p>20 0.32287917 <a title="88-lsi-20" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.014), (3, 0.024), (18, 0.066), (22, 0.051), (26, 0.032), (30, 0.094), (46, 0.353), (51, 0.105), (66, 0.05), (71, 0.014), (75, 0.019), (77, 0.055), (90, 0.014), (96, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74121422 <a title="88-lda-1" href="./emnlp-2013-Flexible_and_Efficient_Hypergraph_Interactions_for_Joint_Hierarchical_and_Forest-to-String_Decoding.html">88 emnlp-2013-Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest-to-String Decoding</a></p>
<p>Author: Martin Cmejrek ; Haitao Mi ; Bowen Zhou</p><p>Abstract: Machine translation benefits from system combination. We propose flexible interaction of hypergraphs as a novel technique combining different translation models within one decoder. We introduce features controlling the interactions between the two systems and explore three interaction schemes of hiero and forest-to-string models—specification, generalization, and interchange. The experiments are carried out on large training data with strong baselines utilizing rich sets of dense and sparse features. All three schemes significantly improve results of any single system on four testsets. We find that specification—a more constrained scheme that almost entirely uses forest-to-string rules, but optionally uses hiero rules for shorter spans—comes out as the strongest, yielding improvement up to 0.9 (T -B )/2 points. We also provide a detailed experimental and qualitative analysis of the results.</p><p>2 0.5096007 <a title="88-lda-2" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<p>Author: Longkai Zhang ; Houfeng Wang ; Xu Sun ; Mairgup Mansur</p><p>Abstract: Nowadays supervised sequence labeling models can reach competitive performance on the task of Chinese word segmentation. However, the ability of these models is restricted by the availability of annotated data and the design of features. We propose a scalable semi-supervised feature engineering approach. In contrast to previous works using pre-defined taskspecific features with fixed values, we dynamically extract representations of label distributions from both an in-domain corpus and an out-of-domain corpus. We update the representation values with a semi-supervised approach. Experiments on the benchmark datasets show that our approach achieve good results and reach an f-score of 0.961. The feature engineering approach proposed here is a general iterative semi-supervised method and not limited to the word segmentation task.</p><p>3 0.42724091 <a title="88-lda-3" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>Author: Fandong Meng ; Jun Xie ; Linfeng Song ; Yajuan Lu ; Qun Liu</p><p>Abstract: We present a novel translation model, which simultaneously exploits the constituency and dependency trees on the source side, to combine the advantages of two types of trees. We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. Our rules hold the property of long distance reorderings and the compatibility with phrases. Large-scale experimental results show that our model achieves significantly improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets.</p><p>4 0.42376971 <a title="88-lda-4" href="./emnlp-2013-Max-Violation_Perceptron_and_Forced_Decoding_for_Scalable_MT_Training.html">128 emnlp-2013-Max-Violation Perceptron and Forced Decoding for Scalable MT Training</a></p>
<p>Author: Heng Yu ; Liang Huang ; Haitao Mi ; Kai Zhao</p><p>Abstract: While large-scale discriminative training has triumphed in many NLP problems, its definite success on machine translation has been largely elusive. Most recent efforts along this line are not scalable (training on the small dev set with features from top ∼100 most frequent wt woridths) f eaantdu overly complicated. oWste f iren-stead present a very simple yet theoretically motivated approach by extending the recent framework of “violation-fixing perceptron”, using forced decoding to compute the target derivations. Extensive phrase-based translation experiments on both Chinese-to-English and Spanish-to-English tasks show substantial gains in BLEU by up to +2.3/+2.0 on dev/test over MERT, thanks to 20M+ sparse features. This is the first successful effort of large-scale online discriminative training for MT. 1Introduction Large-scale discriminative training has witnessed great success in many NLP problems such as parsing (McDonald et al., 2005) and tagging (Collins, 2002), but not yet for machine translation (MT) despite numerous recent efforts. Due to scalability issues, most of these recent methods can only train on a small dev set of about a thousand sentences rather than on the full training set, and only with 2,000–10,000 rather “dense-like” features (either unlexicalized or only considering highest-frequency words), as in MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang, 2012), PRO (Hopkins and May, 2011), and RAMP (Gimpel and Smith, 2012). However, it is well-known that the most important features for NLP are lexicalized, most of which can not ∗ Work done while visiting City University of New York. Corresponding author. † 1112 be seen on a small dataset. Furthermore, these methods often involve complicated loss functions and intricate choices of the “target” derivations to update towards or against (e.g. k-best/forest oracles, or hope/fear derivations), and are thus hard to replicate. As a result, the classical method of MERT (Och, 2003) remains the default training algorithm for MT even though it can only tune a handful of dense features. See also Section 6 for other related work. As a notable exception, Liang et al. (2006) do train a structured perceptron model on the training data with sparse features, but fail to outperform MERT. We argue this is because structured perceptron, like many structured learning algorithms such as CRF and MIRA, assumes exact search, and search errors inevitably break theoretical properties such as convergence (Huang et al., 2012). Empirically, it is now well accepted that standard perceptron performs poorly when search error is severe (Collins and Roark, 2004; Zhang et al., 2013). To address the search error problem we propose a very simple approach based on the recent framework of “violation-fixing perceptron” (Huang et al., 2012) which is designed specifically for inexact search, with a theoretical convergence guarantee and excellent empirical performance on beam search parsing and tagging. The basic idea is to update when search error happens, rather than at the end of the search. To adapt it to MT, we extend this framework to handle latent variables corresponding to the hidden derivations. We update towards “gold-standard” derivations computed by forced decoding so that each derivation leads to the exact reference translation. Forced decoding is also used as a way of data selection, since those reachable sentence pairs are generally more literal and of higher quality, which the training should focus on. When the reachable subset is small for some language pairs, we augment Proce Sdeiantgtlse o,f W thaesh 2i0n1gt3o nC,o UnSfeAre,n 1c8e- o2n1 E Omctpoibriecra 2l0 M13et.h ?oc d2s0 i1n3 N Aastusorcaila Ltiaon g fuoarg Ceo Pmrpoucetastsi on ga,l p Laignegsu 1is1t1ic2s–1 23, it by including reachable prefix-pairs when the full sentence pair is not. We make the following contributions: 1. Our work is the first successful effort to scale online structured learning to a large portion of the training data (as opposed to the dev set). 2. Our work is the first to use a principled learning method customized for inexact search which updates on partial derivations rather than full ones in order to fix search errors. We adapt it to MT using latent variables for derivations. 3. Contrary to the common wisdom, we show that simply updating towards the exact reference translation is helpful, which is much simpler than k-best/forest oracles or loss-augmented (e.g. hope/fear) derivations, avoiding sentencelevel BLEU scores or other loss functions. 4. We present a convincing analysis that it is the search errors and standard perceptron’s inability to deal with them that prevent previous work, esp. Liang et al. (2006), from succeeding. 5. Scaling to the training data enables us to engineer a very rich feature set of sparse, lexicalized, and non-local features, and we propose various ways to alleviate overfitting. For simplicity and efficiency reasons, in this paper we use phrase-based translation, but our method has the potential to be applicable to other translation paradigms. Extensive experiments on both Chineseto-English and Spanish-to-English tasks show statistically significant gains in BLEU by up to +2.3/+2.0 on dev/test over MERT, and up to +1.5/+1.5 over PRO, thanks to 20M+ sparse features. 2 Phrase-Based MT and Forced Decoding We first review the basic phrase-based decoding algorithm (Koehn, 2004), which will be adapted for forced decoding. 2.1 Background: Phrase-based Decoding We will use the following running example from Chinese to English from Mi et al. (2008): 0123456 Figure 1: Standard beam-search phrase-based decoding. B `ush´ ı y uˇ Sh¯ al´ ong j ˇux ´ıng le hu` ıt´ an Bush with Sharon hold -ed meeting ‘Bush held a meeting with Sharon’ Phrase-based decoders generate partial targetlanguage outputs in left-to-right order in the form of hypotheses (or states) (Koehn, 2004). Each hypothesis has a coverage vector capturing the sourcelanguage words translated so far, and can be extended into a longer hypothesis by a phrase-pair translating an uncovered segment. For example, the following is one possible derivation: (• 3(• •() • :1( •s063),:“(Bs)u2s:,h)“(hBs:e1ul(d,s0“ht,aB“hleuk”ls) hdw”t)ailhkrsS1”h)aro2n”)r3 where a • in the coverage vector indicates the source wwoherdre a at •th i ns position aisg e“ vcoecvteorred in”d iacnadte ws thheer seo euarcche si is the score of each state, each adding the rule score and the distortion cost (dc) to the score of the previous state. To compute the distortion cost we also need to maintain the ending position of the last phrase (e.g., the 3 and 6 in the coverage vectors). In phrase-based translation there is also a distortionlimit which prohibits long-distance reorderings. The above states are called −LM states since they do Tnhoet ainbovovleve st language mlleodd −el LcMos tsst.a eTso iandcde a beiygram model, we split each −LM state into a series ogrfa +mL mMo states; ee sapchli t+ eaLcMh −staLtMe h satsa ttehe in ftoor ma (v,a) where a is the last word of the hypothesis. Thus a +LM version of the above derivation might be: (• 3(• ,(•Sh1a•(r6o0,nta)l:ks,()Bsu:03sh,(s“<)s02</p><p>5 0.42188692 <a title="88-lda-5" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>Author: Peng Li ; Yang Liu ; Maosong Sun</p><p>Abstract: While inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (i.e., straight and inverted) dependent on actual blocks being merged remains a challenge. Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points.</p><p>6 0.42050323 <a title="88-lda-6" href="./emnlp-2013-Learning_to_Freestyle%3A_Hip_Hop_Challenge-Response_Induction_via_Transduction_Rule_Segmentation.html">122 emnlp-2013-Learning to Freestyle: Hip Hop Challenge-Response Induction via Transduction Rule Segmentation</a></p>
<p>7 0.4171342 <a title="88-lda-7" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>8 0.4160668 <a title="88-lda-8" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>9 0.41565943 <a title="88-lda-9" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>10 0.41554117 <a title="88-lda-10" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>11 0.41552293 <a title="88-lda-11" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>12 0.41181582 <a title="88-lda-12" href="./emnlp-2013-Anchor_Graph%3A_Global_Reordering_Contexts_for_Statistical_Machine_Translation.html">22 emnlp-2013-Anchor Graph: Global Reordering Contexts for Statistical Machine Translation</a></p>
<p>13 0.41074026 <a title="88-lda-13" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>14 0.41070414 <a title="88-lda-14" href="./emnlp-2013-Converting_Continuous-Space_Language_Models_into_N-Gram_Language_Models_for_Statistical_Machine_Translation.html">52 emnlp-2013-Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation</a></p>
<p>15 0.41050461 <a title="88-lda-15" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>16 0.40952462 <a title="88-lda-16" href="./emnlp-2013-Joint_Language_and_Translation_Modeling_with_Recurrent_Neural_Networks.html">113 emnlp-2013-Joint Language and Translation Modeling with Recurrent Neural Networks</a></p>
<p>17 0.40904802 <a title="88-lda-17" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>18 0.40864784 <a title="88-lda-18" href="./emnlp-2013-Efficient_Left-to-Right_Hierarchical_Phrase-Based_Translation_with_Improved_Reordering.html">71 emnlp-2013-Efficient Left-to-Right Hierarchical Phrase-Based Translation with Improved Reordering</a></p>
<p>19 0.40800849 <a title="88-lda-19" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>20 0.40772095 <a title="88-lda-20" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
