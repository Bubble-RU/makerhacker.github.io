<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>138 emnlp-2013-Naive Bayes Word Sense Induction</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-138" href="#">emnlp2013-138</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>138 emnlp-2013-Naive Bayes Word Sense Induction</h1>
<br/><p>Source: <a title="emnlp-2013-138-pdf" href="http://aclweb.org/anthology//D/D13/D13-1148.pdf">pdf</a></p><p>Author: Do Kook Choe ; Eugene Charniak</p><p>Abstract: We introduce an extended naive Bayes model for word sense induction (WSI) and apply it to a WSI task. The extended model incorporates the idea the words closer to the target word are more relevant in predicting its sense. The proposed model is very simple yet effective when evaluated on SemEval-2010 WSI data. 1</p><p>Reference: <a title="emnlp-2013-138-reference" href="../emnlp2013_reference/emnlp-2013-Naive_Bayes_Word_Sense_Induction_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wsi', 0.478), ('sens', 0.243), ('hermit', 0.216), ('nmflib', 0.216), ('uoy', 0.216), ('nb', 0.191), ('bay', 0.191), ('chip', 0.173), ('clust', 0.167), ('mfs', 0.15), ('naiv', 0.148), ('induc', 0.142), ('wsd', 0.137), ('attain', 0.134), ('allnounsverb', 0.129), ('manandh', 0.12), ('vmeas', 0.113), ('superv', 0.11), ('lau', 0.096), ('em', 0.095)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="138-tfidf-1" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>Author: Do Kook Choe ; Eugene Charniak</p><p>Abstract: We introduce an extended naive Bayes model for word sense induction (WSI) and apply it to a WSI task. The extended model incorporates the idea the words closer to the target word are more relevant in predicting its sense. The proposed model is very simple yet effective when evaluated on SemEval-2010 WSI data. 1</p><p>2 0.13074942 <a title="138-tfidf-2" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>Author: Min Xiao ; Feipeng Zhao ; Yuhong Guo</p><p>Abstract: Domain adaptation has been popularly studied on exploiting labeled information from a source domain to learn a prediction model in a target domain. In this paper, we develop a novel representation learning approach to address domain adaptation for text classification with automatically induced discriminative latent features, which are generalizable across domains while informative to the prediction task. Specifically, we propose a hierarchical multinomial Naive Bayes model with latent variables to conduct supervised word clustering on labeled documents from both source and target domains, and then use the produced cluster distribution of each word as its latent feature representation for domain adaptation. We train this latent graphical model us- ing a simple expectation-maximization (EM) algorithm. We empirically evaluate the proposed method with both cross-domain document categorization tasks on Reuters-21578 dataset and cross-domain sentiment classification tasks on Amazon product review dataset. The experimental results demonstrate that our proposed approach achieves superior performance compared with alternative methods.</p><p>3 0.096852623 <a title="138-tfidf-3" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>Author: Dimitri Kartsaklis ; Mehrnoosh Sadrzadeh</p><p>Abstract: Recent work has shown that compositionaldistributional models using element-wise operations on contextual word vectors benefit from the introduction of a prior disambiguation step. The purpose of this paper is to generalise these ideas to tensor-based models, where relational words such as verbs and adjectives are represented by linear maps (higher order tensors) acting on a number of arguments (vectors). We propose disambiguation algorithms for a number of tensor-based models, which we then test on a variety of tasks. The results show that disambiguation can provide better compositional representation even for the case of tensor-based models. Further- more, we confirm previous findings regarding the positive effect of disambiguation on vector mixture models, and we compare the effectiveness of the two approaches.</p><p>4 0.087095678 <a title="138-tfidf-4" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>Author: Oier Lopez de Lacalle ; Mirella Lapata</p><p>Abstract: In this paper we present an unsupervised approach to relational information extraction. Our model partitions tuples representing an observed syntactic relationship between two named entities (e.g., “X was born in Y” and “X is from Y”) into clusters corresponding to underlying semantic relation types (e.g., BornIn, Located). Our approach incorporates general domain knowledge which we encode as First Order Logic rules and automatically combine with a topic model developed specifically for the relation extraction task. Evaluation results on the ACE 2007 English Relation Detection and Categorization (RDC) task show that our model outperforms competitive unsupervised approaches by a wide margin and is able to produce clusters shaped by both the data and the rules.</p><p>5 0.082211763 <a title="138-tfidf-5" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: Many statistical learning problems in NLP call for local model search methods. But accuracy tends to suffer with current techniques, which often explore either too narrowly or too broadly: hill-climbers can get stuck in local optima, whereas samplers may be inefficient. We propose to arrange individual local optimizers into organized networks. Our building blocks are operators of two types: (i) transform, which suggests new places to search, via non-random restarts from already-found local optima; and (ii) join, which merges candidate solutions to find better optima. Experiments on grammar induction show that pursuing different transforms (e.g., discarding parts of a learned model or ignoring portions of training data) results in improvements. Groups of locally-optimal solutions can be further perturbed jointly, by constructing mixtures. Using these tools, we designed several modular dependency grammar induction networks of increasing complexity. Our complete sys- tem achieves 48.6% accuracy (directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data) more than 5% higher than the previous state-of-the-art. —</p><p>6 0.07607428 <a title="138-tfidf-6" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>7 0.075720616 <a title="138-tfidf-7" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>8 0.074397311 <a title="138-tfidf-8" href="./emnlp-2013-Building_Specialized_Bilingual_Lexicons_Using_Large_Scale_Background_Knowledge.html">42 emnlp-2013-Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge</a></p>
<p>9 0.068290047 <a title="138-tfidf-9" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>10 0.067105778 <a title="138-tfidf-10" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>11 0.061408669 <a title="138-tfidf-11" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<p>12 0.058618933 <a title="138-tfidf-12" href="./emnlp-2013-Exploring_the_Utility_of_Joint_Morphological_and_Syntactic_Learning_from_Child-directed_Speech.html">83 emnlp-2013-Exploring the Utility of Joint Morphological and Syntactic Learning from Child-directed Speech</a></p>
<p>13 0.057719804 <a title="138-tfidf-13" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>14 0.05668107 <a title="138-tfidf-14" href="./emnlp-2013-A_Log-Linear_Model_for_Unsupervised_Text_Normalization.html">9 emnlp-2013-A Log-Linear Model for Unsupervised Text Normalization</a></p>
<p>15 0.056671992 <a title="138-tfidf-15" href="./emnlp-2013-The_Topology_of_Semantic_Knowledge.html">182 emnlp-2013-The Topology of Semantic Knowledge</a></p>
<p>16 0.056662403 <a title="138-tfidf-16" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>17 0.055599876 <a title="138-tfidf-17" href="./emnlp-2013-Learning_to_Rank_Lexical_Substitutions.html">123 emnlp-2013-Learning to Rank Lexical Substitutions</a></p>
<p>18 0.055343628 <a title="138-tfidf-18" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>19 0.054589082 <a title="138-tfidf-19" href="./emnlp-2013-Improvements_to_the_Bayesian_Topic_N-Gram_Models.html">100 emnlp-2013-Improvements to the Bayesian Topic N-Gram Models</a></p>
<p>20 0.054505333 <a title="138-tfidf-20" href="./emnlp-2013-Sentiment_Analysis%3A_How_to_Derive_Prior_Polarities_from_SentiWordNet.html">170 emnlp-2013-Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.182), (1, -0.03), (2, 0.027), (3, -0.039), (4, -0.015), (5, -0.018), (6, -0.014), (7, 0.019), (8, -0.128), (9, 0.123), (10, 0.048), (11, 0.004), (12, 0.027), (13, 0.057), (14, 0.065), (15, -0.027), (16, -0.049), (17, 0.071), (18, 0.011), (19, -0.025), (20, 0.057), (21, 0.104), (22, -0.05), (23, -0.003), (24, -0.021), (25, -0.028), (26, -0.066), (27, -0.038), (28, 0.062), (29, -0.039), (30, -0.023), (31, 0.015), (32, 0.018), (33, -0.075), (34, 0.135), (35, 0.021), (36, -0.125), (37, 0.009), (38, -0.021), (39, 0.107), (40, 0.01), (41, -0.034), (42, -0.021), (43, -0.175), (44, 0.055), (45, -0.02), (46, 0.076), (47, -0.094), (48, 0.061), (49, -0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92244464 <a title="138-lsi-1" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>Author: Do Kook Choe ; Eugene Charniak</p><p>Abstract: We introduce an extended naive Bayes model for word sense induction (WSI) and apply it to a WSI task. The extended model incorporates the idea the words closer to the target word are more relevant in predicting its sense. The proposed model is very simple yet effective when evaluated on SemEval-2010 WSI data. 1</p><p>2 0.60115469 <a title="138-lsi-2" href="./emnlp-2013-Unsupervised_Spectral_Learning_of_WCFG_as_Low-rank_Matrix_Completion.html">195 emnlp-2013-Unsupervised Spectral Learning of WCFG as Low-rank Matrix Completion</a></p>
<p>Author: Raphael Bailly ; Xavier Carreras ; Franco M. Luque ; Ariadna Quattoni</p><p>Abstract: We derive a spectral method for unsupervised learning of Weighted Context Free Grammars. We frame WCFG induction as finding a Hankel matrix that has low rank and is linearly constrained to represent a function computed by inside-outside recursions. The proposed algorithm picks the grammar that agrees with a sample and is the simplest with respect to the nuclear norm of the Hankel matrix.</p><p>3 0.59369159 <a title="138-lsi-3" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>Author: Min Xiao ; Feipeng Zhao ; Yuhong Guo</p><p>Abstract: Domain adaptation has been popularly studied on exploiting labeled information from a source domain to learn a prediction model in a target domain. In this paper, we develop a novel representation learning approach to address domain adaptation for text classification with automatically induced discriminative latent features, which are generalizable across domains while informative to the prediction task. Specifically, we propose a hierarchical multinomial Naive Bayes model with latent variables to conduct supervised word clustering on labeled documents from both source and target domains, and then use the produced cluster distribution of each word as its latent feature representation for domain adaptation. We train this latent graphical model us- ing a simple expectation-maximization (EM) algorithm. We empirically evaluate the proposed method with both cross-domain document categorization tasks on Reuters-21578 dataset and cross-domain sentiment classification tasks on Amazon product review dataset. The experimental results demonstrate that our proposed approach achieves superior performance compared with alternative methods.</p><p>4 0.57537979 <a title="138-lsi-4" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>Author: Oier Lopez de Lacalle ; Mirella Lapata</p><p>Abstract: In this paper we present an unsupervised approach to relational information extraction. Our model partitions tuples representing an observed syntactic relationship between two named entities (e.g., “X was born in Y” and “X is from Y”) into clusters corresponding to underlying semantic relation types (e.g., BornIn, Located). Our approach incorporates general domain knowledge which we encode as First Order Logic rules and automatically combine with a topic model developed specifically for the relation extraction task. Evaluation results on the ACE 2007 English Relation Detection and Categorization (RDC) task show that our model outperforms competitive unsupervised approaches by a wide margin and is able to produce clusters shaped by both the data and the rules.</p><p>5 0.52946275 <a title="138-lsi-5" href="./emnlp-2013-Centering_Similarity_Measures_to_Reduce_Hubs.html">44 emnlp-2013-Centering Similarity Measures to Reduce Hubs</a></p>
<p>Author: Ikumi Suzuki ; Kazuo Hara ; Masashi Shimbo ; Marco Saerens ; Kenji Fukumizu</p><p>Abstract: The performance of nearest neighbor methods is degraded by the presence of hubs, i.e., objects in the dataset that are similar to many other objects. In this paper, we show that the classical method of centering, the transformation that shifts the origin of the space to the data centroid, provides an effective way to reduce hubs. We show analytically why hubs emerge and why they are suppressed by centering, under a simple probabilistic model of data. To further reduce hubs, we also move the origin more aggressively towards hubs, through weighted centering. Our experimental results show that (weighted) centering is effective for natural language data; it improves the performance of the k-nearest neighbor classi- fiers considerably in word sense disambiguation and document classification tasks.</p><p>6 0.52912098 <a title="138-lsi-6" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>7 0.48936823 <a title="138-lsi-7" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>8 0.48489562 <a title="138-lsi-8" href="./emnlp-2013-Orthonormal_Explicit_Topic_Analysis_for_Cross-Lingual_Document_Matching.html">148 emnlp-2013-Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</a></p>
<p>9 0.48223701 <a title="138-lsi-9" href="./emnlp-2013-Improvements_to_the_Bayesian_Topic_N-Gram_Models.html">100 emnlp-2013-Improvements to the Bayesian Topic N-Gram Models</a></p>
<p>10 0.47971159 <a title="138-lsi-10" href="./emnlp-2013-A_Log-Linear_Model_for_Unsupervised_Text_Normalization.html">9 emnlp-2013-A Log-Linear Model for Unsupervised Text Normalization</a></p>
<p>11 0.4796266 <a title="138-lsi-11" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>12 0.47191337 <a title="138-lsi-12" href="./emnlp-2013-Using_Soft_Constraints_in_Joint_Inference_for_Clinical_Concept_Recognition.html">198 emnlp-2013-Using Soft Constraints in Joint Inference for Clinical Concept Recognition</a></p>
<p>13 0.47151968 <a title="138-lsi-13" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>14 0.46702811 <a title="138-lsi-14" href="./emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</a></p>
<p>15 0.46529493 <a title="138-lsi-15" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>16 0.46238592 <a title="138-lsi-16" href="./emnlp-2013-Automatic_Domain_Partitioning_for_Multi-Domain_Learning.html">29 emnlp-2013-Automatic Domain Partitioning for Multi-Domain Learning</a></p>
<p>17 0.45852479 <a title="138-lsi-17" href="./emnlp-2013-This_Text_Has_the_Scent_of_Starbucks%3A_A_Laplacian_Structured_Sparsity_Model_for_Computational_Branding_Analytics.html">184 emnlp-2013-This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</a></p>
<p>18 0.44088462 <a title="138-lsi-18" href="./emnlp-2013-Automatically_Determining_a_Proper_Length_for_Multi-Document_Summarization%3A_A_Bayesian_Nonparametric_Approach.html">36 emnlp-2013-Automatically Determining a Proper Length for Multi-Document Summarization: A Bayesian Nonparametric Approach</a></p>
<p>19 0.43491411 <a title="138-lsi-19" href="./emnlp-2013-Learning_to_Rank_Lexical_Substitutions.html">123 emnlp-2013-Learning to Rank Lexical Substitutions</a></p>
<p>20 0.43144605 <a title="138-lsi-20" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.616), (29, 0.015), (47, 0.04), (51, 0.04), (65, 0.066), (69, 0.013), (73, 0.101), (79, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89518362 <a title="138-lda-1" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>Author: Kazuma Hashimoto ; Makoto Miwa ; Yoshimasa Tsuruoka ; Takashi Chikayama</p><p>Abstract: In this paper, we present a recursive neural network (RNN) model that works on a syntactic tree. Our model differs from previous RNN models in that the model allows for an explicit weighting of important phrases for the target task. We also propose to average parameters in training. Our experimental results on semantic relation classification show that both phrase categories and task-specific weighting significantly improve the prediction accuracy of the model. We also show that averaging the model parameters is effective in stabilizing the learning and improves generalization capacity. The proposed model marks scores competitive with state-of-the-art RNN-based models.</p><p>2 0.86739117 <a title="138-lda-2" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>Author: Angeliki Lazaridou ; Eva Maria Vecchi ; Marco Baroni</p><p>Abstract: In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the correct parsing of NPs (e.g., (live fish) transporter rather than live (fish transporter)). We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features.</p><p>same-paper 3 0.86686224 <a title="138-lda-3" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<p>Author: Do Kook Choe ; Eugene Charniak</p><p>Abstract: We introduce an extended naive Bayes model for word sense induction (WSI) and apply it to a WSI task. The extended model incorporates the idea the words closer to the target word are more relevant in predicting its sense. The proposed model is very simple yet effective when evaluated on SemEval-2010 WSI data. 1</p><p>4 0.77577454 <a title="138-lda-4" href="./emnlp-2013-The_Effects_of_Syntactic_Features_in_Automatic_Prediction_of_Morphology.html">181 emnlp-2013-The Effects of Syntactic Features in Automatic Prediction of Morphology</a></p>
<p>Author: Wolfgang Seeker ; Jonas Kuhn</p><p>Abstract: Morphology and syntax interact considerably in many languages and language processing should pay attention to these interdependencies. We analyze the effect of syntactic features when used in automatic morphology prediction on four typologically different languages. We show that predicting morphology for languages with highly ambiguous word forms profits from taking the syntactic context of words into account and results in state-ofthe-art models.</p><p>5 0.6768837 <a title="138-lda-5" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>Author: Douwe Kiela ; Stephen Clark</p><p>Abstract: We present a novel unsupervised approach to detecting the compositionality of multi-word expressions. We compute the compositionality of a phrase through substituting the constituent words with their “neighbours” in a semantic vector space and averaging over the distance between the original phrase and the substituted neighbour phrases. Several methods of obtaining neighbours are presented. The results are compared to existing supervised results and achieve state-of-the-art performance on a verb-object dataset of human compositionality ratings.</p><p>6 0.58973736 <a title="138-lda-6" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>7 0.55447531 <a title="138-lda-7" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>8 0.55284965 <a title="138-lda-8" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>9 0.54982829 <a title="138-lda-9" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>10 0.54702282 <a title="138-lda-10" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>11 0.52560937 <a title="138-lda-11" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>12 0.52495909 <a title="138-lda-12" href="./emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations.html">17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</a></p>
<p>13 0.49867779 <a title="138-lda-13" href="./emnlp-2013-Identifying_Phrasal_Verbs_Using_Many_Bilingual_Corpora.html">96 emnlp-2013-Identifying Phrasal Verbs Using Many Bilingual Corpora</a></p>
<p>14 0.48861212 <a title="138-lda-14" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>15 0.48537162 <a title="138-lda-15" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>16 0.48228776 <a title="138-lda-16" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>17 0.46666914 <a title="138-lda-17" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<p>18 0.46532178 <a title="138-lda-18" href="./emnlp-2013-The_Topology_of_Semantic_Knowledge.html">182 emnlp-2013-The Topology of Semantic Knowledge</a></p>
<p>19 0.46424159 <a title="138-lda-19" href="./emnlp-2013-A_Multimodal_LDA_Model_integrating_Textual%2C_Cognitive_and_Visual_Modalities.html">11 emnlp-2013-A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities</a></p>
<p>20 0.4626368 <a title="138-lda-20" href="./emnlp-2013-Automatic_Extraction_of_Morphological_Lexicons_from_Morphologically_Annotated_Corpora.html">30 emnlp-2013-Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
