<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-172" href="#">emnlp2013-172</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</h1>
<br/><p>Source: <a title="emnlp-2013-172-pdf" href="http://aclweb.org/anthology//D/D13/D13-1137.pdf">pdf</a></p><p>Author: Kazuma Hashimoto ; Makoto Miwa ; Yoshimasa Tsuruoka ; Takashi Chikayama</p><p>Abstract: In this paper, we present a recursive neural network (RNN) model that works on a syntactic tree. Our model differs from previous RNN models in that the model allows for an explicit weighting of important phrases for the target task. We also propose to average parameters in training. Our experimental results on semantic relation classification show that both phrase categories and task-specific weighting significantly improve the prediction accuracy of the model. We also show that averaging the model parameters is effective in stabilizing the learning and improves generalization capacity. The proposed model marks scores competitive with state-of-the-art RNN-based models.</p><p>Reference: <a title="emnlp-2013-172-reference" href="../emnlp2013_reference/emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uchke  ,  ,  Abstract In this paper, we present a recursive neural network (RNN) model that works on a syntactic tree. [sent-12, score-0.197]
</p><p>2 Our model differs from previous RNN models in that the model allows for an explicit weighting of important phrases for the target task. [sent-13, score-0.112]
</p><p>3 Our experimental results on semantic relation classification show that both phrase categories and task-specific weighting significantly improve the prediction accuracy of the model. [sent-15, score-0.373]
</p><p>4 We also show that averaging the model parameters is effective in stabilizing the learning and improves generalization capacity. [sent-16, score-0.358]
</p><p>5 The proposed model marks scores competitive with state-of-the-art RNN-based models. [sent-17, score-0.035]
</p><p>6 RNN models can represent phrases of arbitrary length in a vector space of a fixed dimension. [sent-21, score-0.142]
</p><p>7 Most of them use minimal syntactic information (Socher et al. [sent-22, score-0.108]
</p><p>8 Recently, Hermann and Blunsom (2013) proposed a method for leveraging syntactic information, namely CCG combinatory operators, to guide composition of phrases in RNN models. [sent-24, score-0.177]
</p><p>9 While their models were successfully applied to binary sentiment classification and compound similarity tasks, 1372 there are questions yet to be answered, e. [sent-25, score-0.092]
</p><p>10 , whether such enhancement is beneficial in other NLP tasks as well, and whether a similar improvement can be achieved by using syntactic information of more commonly available types such as phrase categories and syntactic heads. [sent-27, score-0.365]
</p><p>11 In this paper, we present a supervised RNN model for a semantic relation classification task. [sent-28, score-0.152]
</p><p>12 Our model  is different from existing RNN models in that important phrases can be explicitly weighted for the task. [sent-29, score-0.07]
</p><p>13 Syntactic information used in our model includes part-of-speech (POS) tags, phrase categories and syntactic heads. [sent-30, score-0.256]
</p><p>14 POS tags are used to assign vector representations to word-POS pairs. [sent-31, score-0.191]
</p><p>15 Phrase categories are used to determine which weight matrices are chosen to combine phrases. [sent-32, score-0.225]
</p><p>16 Syntactic heads are used to determine which phrase is weighted during combining phrases. [sent-33, score-0.188]
</p><p>17 To incorporate task-specific information, phrases on the path between entity pairs are further weighted. [sent-34, score-0.176]
</p><p>18 The second contribution of our work is the introduction of parameter averaging into RNN models. [sent-35, score-0.227]
</p><p>19 This fluctuation not only leads to unstable performance of the resulting models, but also makes it difficult to fine-tune the hyperparameters ofthe model. [sent-37, score-0.064]
</p><p>20 (2010), we propose to average the model parameters in the course of training. [sent-39, score-0.059]
</p><p>21 A recent technique for deep learning models of similar vein is dropout (Hinton et al. [sent-40, score-0.078]
</p><p>22 oc d2s0 i1n3 N Aastusorcaila Ltiaonng fuoarg Ceo Pmrpoucetastsi onnga,l p Laignegsu 1is3t7ic2s–1376,  Figure 1: A recursive representations of a phrase “a word vector” with POS tags of the words (DT, NN and NN respectively). [sent-44, score-0.307]
</p><p>23 For example, the two word-POS pairs “word NN” and “vector NN” with a syntactic category N are combined to represent the phrase “word vector”. [sent-45, score-0.176]
</p><p>24 By averaging the model parameters, our model achieves performance competitive with the MV-RNN model in Socher et al. [sent-47, score-0.262]
</p><p>25 2  An Averaged RNN Model with Syntax  Our model is a supervised RNN that works on a binary syntactic tree. [sent-49, score-0.077]
</p><p>26 As our first step to leverage information available in the tree, we distinguish words with the same spelling but POS tags in the vector space. [sent-50, score-0.147]
</p><p>27 Our model also uses different weight matrices dependent on the phrase categories of child nodes (phrases or words) in combining phrases. [sent-51, score-0.428]
</p><p>28 Our model further weights those nodes that appear to be important. [sent-52, score-0.094]
</p><p>29 1 Word-POS Vector Representations Our model simply assigns vector representations to word-POS pairs. [sent-56, score-0.146]
</p><p>30 The vectors are represented as column vectors in a matrix We ∈ where d is the dimension of a vector and V∈ iRs a set of all wordPOS pairs we consider. [sent-58, score-0.206]
</p><p>31 2 Compositional Functions with Syntax In construction of parse trees, we associate each of the tree node with its d-dimensional vector representation computed from vector representations of its  subtrees. [sent-60, score-0.264]
</p><p>32 For leaf nodes, we look up word-POS vec1373 tor representations in V. [sent-61, score-0.074]
</p><p>33 Figure 1 shows an example of such recursive representations. [sent-62, score-0.089]
</p><p>34 A parent vector p ∈ Rd×1 is computed from its direct child vectors cl ∈and R cr∈ Rd×1:  tanh(αlWlTcl,Tcrcl+αrWrTcl,Tcrcr+bTcl,Tcr), where WlTcl,Tcr and WrTcl,Tcr ∈ Rd×d are weight  p=  matrices that depend on the phrase categories of cl and cr. [sent-63, score-0.607]
</p><p>35 Here, cl and cr have phrase categories Tcl and Tcr respectively (such as N, V, etc. [sent-64, score-0.265]
</p><p>36 To incorporate the importance of phrases into the model, two subtrees of a node may have different weights αl ∈ [0, 1] and αr (= 1 − αl), taking phrase importance i,n1to] accou(n=t. [sent-67, score-0.259]
</p><p>37 1T −he value of αl is manually specified and automatically applied to all nodes based on prior knowledge about the task. [sent-68, score-0.05]
</p><p>38 In this way, we can compute vector representations for phrases of arbitrary length. [sent-69, score-0.216]
</p><p>39 We denote a set of such matrices as Wlr and bias vectors as b. [sent-70, score-0.221]
</p><p>40 3 Objective Function and Learning As with other RNN models, we add on the top of a node x a softmax classifier. [sent-72, score-0.134]
</p><p>41 The classifier is used to predict a K-class distribution d(x) ∈ RK×1 over a specific ata Ksk- ctloa tsrsa idni our mutioodne dl: d(x) = where Wlabel  softmax(Wlabelx  +  blabel),  (1)  ∈ RK×d is a weight matrix and  blabel ∈ RK×1 i∈s a R bias vector. [sent-73, score-0.31]
</p><p>42 We denote t(x) ∈ RK×1 as the target diiasstr viebcuttioorn. [sent-74, score-0.03]
</p><p>43 We then compute the cross entropy error between d(x) and t(x) : ∑K  E(x) = −∑tk(x)logdk(x), k∑= ∑1  and define an objective function as the sum of E(x) over all training data:  J(θ) =∑xE(x) +λ2∥θ∥2, where θ = (We, Wlr, b, Wlabel, blabel) is the set of  our model parameters that should be learned. [sent-77, score-0.059]
</p><p>44 To compute d(x), we can directly leverage any other nodes’ feature vectors in the same tree. [sent-79, score-0.097]
</p><p>45 We denote such additional feature vectors as xi′ ∈ Rd×1, and extend Eq. [sent-80, score-0.097]
</p><p>46 (1): d(x)  = softmax(Wlabelx+∑Wiaddxi′+blabel), ∑i  where Wiadd ∈ RK×d are weight matrices for additional features∈. [sent-81, score-0.145]
</p><p>47 The gradient of J(θ)  ∂J∂θ(θ)=∑x∂E∂(θx)+ λθ is efficiently computed via backpropagation through structure (Goller and K ¨uchler, 1996). [sent-84, score-0.047]
</p><p>48 4 Averaging We use averaged model parameters  θ =T1 + 1t∑=T0θt at test time, where θt is the vector of model parameters after t iterations of the L-BFGS optimization. [sent-88, score-0.234]
</p><p>49 Our preliminary experimental results suggest that averaging θ except We works well. [sent-89, score-0.227]
</p><p>50 3  Experimental Settings  We used the Enju parser (Miyao and Tsujii, 2008) for syntactic parsing. [sent-90, score-0.077]
</p><p>51 1 Task: Semantic Relation Classification We evaluated our model on a semantic relation classification task: SemEval 2010 Task 8 (Hendrickx et al. [sent-93, score-0.152]
</p><p>52 (2012), we regarded the task as a 19-class classification problem. [sent-96, score-0.054]
</p><p>53 1374  Figure 2: Classifying the relation between two entities. [sent-100, score-0.05]
</p><p>54 To predict a class label, we first find the minimal phrase that covers the target entities and then use the vector representation of the phrase (Figure 2). [sent-103, score-0.335]
</p><p>55 3, we can directly connect features on any other nodes to the softmax classifier. [sent-105, score-0.138]
</p><p>56 In this work, we used three such internal features: two vector representations of target entities and one averaged vector representation of words between the entities2. [sent-106, score-0.296]
</p><p>57 2 Weights on Phrases We tuned the weight αl (or αr) introduced in Sec-  tion 2. [sent-108, score-0.056]
</p><p>58 There are two factors: syntactic heads and syntactic path between target entities. [sent-110, score-0.319]
</p><p>59 5r reloanti ohne dcla pshsirafisceasti,o ann tasks, syntactic paths . [sent-114, score-0.129]
</p><p>60 be Ftowre reentarget entities are important (Zhang et al. [sent-115, score-0.034]
</p><p>61 , 2006), so our model also puts another weight γ ∈ [0. [sent-116, score-0.102]
</p><p>62 5, 1] on phrases on tahlseo path, aanndo 1h − γ on htht eγ o ∈th e[r0s. [sent-117, score-0.07]
</p><p>63 5 ,W1]h oenn bpohrtha cehsi lodn n thodee psa are on t 1h e− path or nee oitthheerr o. [sent-119, score-0.076]
</p><p>64 The two weight factors are summed up and divided by 2 to be the final weights αl and αr to combine the phrases. [sent-122, score-0.1]
</p><p>65 For example, we set αl = and αr = when the right child node is the head and the left child node is on the path. [sent-123, score-0.2]
</p><p>66 3  Initialization of Model Parameters and Tuning of Hyperparameters We initialized We with 50-dimensional word vectors3 trained with the model of Collobert et 2Socher et al. [sent-125, score-0.036]
</p><p>67 (2012) used richer features including words around entity pairs in their implementation. [sent-126, score-0.03]
</p><p>68 3The word vectors are provided at http : / / ronan . [sent-127, score-0.111]
</p><p>69 We used the vectors without any modifications such as normalization. [sent-130, score-0.067]
</p><p>70 1 Table 2: Contributions of syntactic and task-specific information and averaging. [sent-145, score-0.077]
</p><p>71 The initialization of Wlr is the same as that of Socher et al. [sent-150, score-0.036]
</p><p>72 We tuned hyperparameters in our model using the validation set for each experimental setting. [sent-153, score-0.064]
</p><p>73 The hyperparameters include the regularization parameters for We, Wlr, Wlabel and Wadd, and the weights β and γ. [sent-154, score-0.167]
</p><p>74 tably, the score of our model is competitive with that of the MV-RNN model (79. [sent-166, score-0.035]
</p><p>75 Our model outperforms the RNN model with one lexical and two semantic external features: POS tags, WordNet hypernyms and named entity tags (NER) of target word pairs (external features). [sent-171, score-0.199]
</p><p>76 The MV-RNN model with external features shows better performance than our model. [sent-172, score-0.076]
</p><p>77 An SVM with rich lexical and semantic features (Rink and Harabagiu, 2010) also outperforms ours. [sent-173, score-0.048]
</p><p>78 Note, however, that this is not a fair comparison because those models use rich external resources such as WordNet and named entity tags. [sent-174, score-0.106]
</p><p>79 1 Contributions of Proposed Methods We conducted additional experiments to quantify the contributions of phrase categories, heads, paths and averaging to our classification score. [sent-176, score-0.432]
</p><p>80 As shown in Table 2, our model without phrase categories, heads or paths still outperforms the RNN model with external features. [sent-177, score-0.316]
</p><p>81 On the other hand, our model without averaging yields a lower score than the RNN  model with external features, though it is still better than the RNN model alone. [sent-178, score-0.303]
</p><p>82 These results indicate that syntactic and task-specific information and averaging contribute to the performance improvement. [sent-181, score-0.304]
</p><p>83 The improvement is achieved by a simple modification of compositional functions in RNN models. [sent-182, score-0.08]
</p><p>84 2 Effects of Averaging in Training Figure 3 shows the training curves in terms of F1 scores. [sent-184, score-0.031]
</p><p>85 These curves clearly demonstrate that parameter averaging helps to stabilize the learning and improve generalization capacity. [sent-185, score-0.33]
</p><p>86 5  Conclusion  We have presented an averaged RNN model for semantic relation classification. [sent-186, score-0.142]
</p><p>87 Our experimental results show that syntactic information such as phrase categories and heads improves the performance, and the task-specific weighting is also beneficial. [sent-187, score-0.387]
</p><p>88 The results also demonstrate that averaging the model parameters not only stabilizes the learning but also improves the generalization capacity of the model. [sent-188, score-0.326]
</p><p>89 As future work, we plan to combine deep learning  models with richer information such as predicateargument structures. [sent-189, score-0.076]
</p><p>90 Improving neural networks by preventing coadaptation of feature detectors. [sent-211, score-0.063]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rnn', 0.652), ('averaging', 0.227), ('wlr', 0.224), ('blabel', 0.187), ('socher', 0.183), ('wlabel', 0.149), ('rk', 0.106), ('phrase', 0.099), ('wadd', 0.097), ('rink', 0.097), ('heads', 0.089), ('recursive', 0.089), ('matrices', 0.089), ('softmax', 0.088), ('rd', 0.087), ('categories', 0.08), ('compositional', 0.08), ('semeval', 0.077), ('syntactic', 0.077), ('external', 0.076), ('path', 0.076), ('swersky', 0.075), ('wiadd', 0.075), ('wlabelx', 0.075), ('representations', 0.074), ('hermann', 0.074), ('blunsom', 0.073), ('vector', 0.072), ('phrases', 0.07), ('vectors', 0.067), ('hyperparameters', 0.064), ('nn', 0.061), ('goller', 0.059), ('parameters', 0.059), ('weight', 0.056), ('hendrickx', 0.055), ('child', 0.054), ('classification', 0.054), ('collobert', 0.054), ('paths', 0.052), ('nodes', 0.05), ('relation', 0.05), ('pos', 0.05), ('semantic', 0.048), ('harabagiu', 0.047), ('hw', 0.047), ('backpropagation', 0.047), ('node', 0.046), ('deep', 0.046), ('miyao', 0.046), ('puts', 0.046), ('pw', 0.046), ('cl', 0.045), ('tags', 0.045), ('wordnet', 0.044), ('ronan', 0.044), ('averaged', 0.044), ('weights', 0.044), ('caused', 0.043), ('svm', 0.042), ('weighting', 0.042), ('cr', 0.041), ('generalization', 0.04), ('hinton', 0.039), ('compound', 0.038), ('pc', 0.037), ('initialization', 0.036), ('initialized', 0.036), ('bias', 0.035), ('competitive', 0.035), ('entities', 0.034), ('syntax', 0.034), ('vein', 0.032), ('vbn', 0.032), ('coadaptation', 0.032), ('stabilizing', 0.032), ('enhancement', 0.032), ('nitish', 0.032), ('idni', 0.032), ('utd', 0.032), ('uchler', 0.032), ('stabilize', 0.032), ('classication', 0.032), ('hashimoto', 0.032), ('hhea', 0.032), ('neural', 0.031), ('computationally', 0.031), ('minimal', 0.031), ('curves', 0.031), ('entity', 0.03), ('leverage', 0.03), ('denote', 0.03), ('diarmuid', 0.03), ('lorenza', 0.03), ('romano', 0.03), ('takashi', 0.03), ('toa', 0.03), ('yoshimasa', 0.03), ('predicateargument', 0.03), ('combinatory', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="172-tfidf-1" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>Author: Kazuma Hashimoto ; Makoto Miwa ; Yoshimasa Tsuruoka ; Takashi Chikayama</p><p>Abstract: In this paper, we present a recursive neural network (RNN) model that works on a syntactic tree. Our model differs from previous RNN models in that the model allows for an explicit weighting of important phrases for the target task. We also propose to average parameters in training. Our experimental results on semantic relation classification show that both phrase categories and task-specific weighting significantly improve the prediction accuracy of the model. We also show that averaging the model parameters is effective in stabilizing the learning and improves generalization capacity. The proposed model marks scores competitive with state-of-the-art RNN-based models.</p><p>2 0.25808856 <a title="172-tfidf-2" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>Author: Richard Socher ; Alex Perelygin ; Jean Wu ; Jason Chuang ; Christopher D. Manning ; Andrew Ng ; Christopher Potts</p><p>Abstract: Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.</p><p>3 0.12927659 <a title="172-tfidf-3" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>Author: Peng Li ; Yang Liu ; Maosong Sun</p><p>Abstract: While inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (i.e., straight and inverted) dependent on actual blocks being merged remains a challenge. Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points.</p><p>4 0.11014932 <a title="172-tfidf-4" href="./emnlp-2013-Deriving_Adjectival_Scales_from_Continuous_Space_Word_Representations.html">59 emnlp-2013-Deriving Adjectival Scales from Continuous Space Word Representations</a></p>
<p>Author: Joo-Kyung Kim ; Marie-Catherine de Marneffe</p><p>Abstract: Continuous space word representations extracted from neural network language models have been used effectively for natural language processing, but until recently it was not clear whether the spatial relationships of such representations were interpretable. Mikolov et al. (2013) show that these representations do capture syntactic and semantic regularities. Here, we push the interpretation of continuous space word representations further by demonstrating that vector offsets can be used to derive adjectival scales (e.g., okay < good < excellent). We evaluate the scales on the indirect answers to yes/no questions corpus (de Marneffe et al., 2010). We obtain 72.8% accuracy, which outperforms previous results (∼60%) on tichihs corpus aornmd highlights sth rees quality o6f0% the) scales extracted, providing further support that the continuous space word representations are meaningful.</p><p>5 0.09647613 <a title="172-tfidf-5" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>Author: Angeliki Lazaridou ; Eva Maria Vecchi ; Marco Baroni</p><p>Abstract: In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the correct parsing of NPs (e.g., (live fish) transporter rather than live (fish transporter)). We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features.</p><p>6 0.08898031 <a title="172-tfidf-6" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>7 0.082978167 <a title="172-tfidf-7" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>8 0.078789122 <a title="172-tfidf-8" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>9 0.061347678 <a title="172-tfidf-9" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>10 0.05635722 <a title="172-tfidf-10" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>11 0.055549212 <a title="172-tfidf-11" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>12 0.055306118 <a title="172-tfidf-12" href="./emnlp-2013-A_Walk-Based_Semantically_Enriched_Tree_Kernel_Over_Distributed_Word_Representations.html">17 emnlp-2013-A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</a></p>
<p>13 0.05489355 <a title="172-tfidf-13" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>14 0.050710201 <a title="172-tfidf-14" href="./emnlp-2013-A_Semantically_Enhanced_Approach_to_Determine_Textual_Similarity.html">12 emnlp-2013-A Semantically Enhanced Approach to Determine Textual Similarity</a></p>
<p>15 0.049525239 <a title="172-tfidf-15" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>16 0.048972398 <a title="172-tfidf-16" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>17 0.047913536 <a title="172-tfidf-17" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>18 0.047516145 <a title="172-tfidf-18" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>19 0.045657657 <a title="172-tfidf-19" href="./emnlp-2013-Recurrent_Continuous_Translation_Models.html">156 emnlp-2013-Recurrent Continuous Translation Models</a></p>
<p>20 0.045122523 <a title="172-tfidf-20" href="./emnlp-2013-Feature_Noising_for_Log-Linear_Structured_Prediction.html">86 emnlp-2013-Feature Noising for Log-Linear Structured Prediction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.176), (1, -0.014), (2, -0.044), (3, -0.068), (4, 0.031), (5, 0.151), (6, -0.037), (7, -0.11), (8, -0.167), (9, 0.074), (10, 0.182), (11, 0.078), (12, -0.143), (13, -0.065), (14, -0.062), (15, -0.052), (16, 0.067), (17, -0.073), (18, -0.035), (19, 0.006), (20, 0.089), (21, -0.016), (22, -0.027), (23, -0.013), (24, 0.013), (25, 0.072), (26, 0.005), (27, -0.069), (28, 0.053), (29, -0.021), (30, 0.007), (31, -0.01), (32, -0.04), (33, 0.039), (34, 0.026), (35, -0.099), (36, 0.018), (37, 0.103), (38, 0.048), (39, 0.095), (40, 0.029), (41, -0.058), (42, -0.088), (43, -0.024), (44, -0.036), (45, -0.072), (46, 0.073), (47, 0.035), (48, 0.123), (49, 0.19)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93721145 <a title="172-lsi-1" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>Author: Kazuma Hashimoto ; Makoto Miwa ; Yoshimasa Tsuruoka ; Takashi Chikayama</p><p>Abstract: In this paper, we present a recursive neural network (RNN) model that works on a syntactic tree. Our model differs from previous RNN models in that the model allows for an explicit weighting of important phrases for the target task. We also propose to average parameters in training. Our experimental results on semantic relation classification show that both phrase categories and task-specific weighting significantly improve the prediction accuracy of the model. We also show that averaging the model parameters is effective in stabilizing the learning and improves generalization capacity. The proposed model marks scores competitive with state-of-the-art RNN-based models.</p><p>2 0.73168969 <a title="172-lsi-2" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>Author: Richard Socher ; Alex Perelygin ; Jean Wu ; Jason Chuang ; Christopher D. Manning ; Andrew Ng ; Christopher Potts</p><p>Abstract: Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.</p><p>3 0.66300726 <a title="172-lsi-3" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>Author: Masashi Tsubaki ; Kevin Duh ; Masashi Shimbo ; Yuji Matsumoto</p><p>Abstract: We present a novel vector space model for semantic co-compositionality. Inspired by Generative Lexicon Theory (Pustejovsky, 1995), our goal is a compositional model where both predicate and argument are allowed to modify each others’ meaning representations while generating the overall semantics. This readily addresses some major challenges with current vector space models, notably the polysemy issue and the use of one representation per word type. We implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations. We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality. The model achieves the best result to date (ρ = 0.47) on the semantic similarity task of transitive verbs (Grefenstette and Sadrzadeh, 2011).</p><p>4 0.55289179 <a title="172-lsi-4" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>Author: Peng Li ; Yang Liu ; Maosong Sun</p><p>Abstract: While inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (i.e., straight and inverted) dependent on actual blocks being merged remains a challenge. Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points.</p><p>5 0.53307921 <a title="172-lsi-5" href="./emnlp-2013-Deriving_Adjectival_Scales_from_Continuous_Space_Word_Representations.html">59 emnlp-2013-Deriving Adjectival Scales from Continuous Space Word Representations</a></p>
<p>Author: Joo-Kyung Kim ; Marie-Catherine de Marneffe</p><p>Abstract: Continuous space word representations extracted from neural network language models have been used effectively for natural language processing, but until recently it was not clear whether the spatial relationships of such representations were interpretable. Mikolov et al. (2013) show that these representations do capture syntactic and semantic regularities. Here, we push the interpretation of continuous space word representations further by demonstrating that vector offsets can be used to derive adjectival scales (e.g., okay < good < excellent). We evaluate the scales on the indirect answers to yes/no questions corpus (de Marneffe et al., 2010). We obtain 72.8% accuracy, which outperforms previous results (∼60%) on tichihs corpus aornmd highlights sth rees quality o6f0% the) scales extracted, providing further support that the continuous space word representations are meaningful.</p><p>6 0.52335298 <a title="172-lsi-6" href="./emnlp-2013-Fish_Transporters_and_Miracle_Homes%3A_How_Compositional_Distributional_Semantics_can_Help_NP_Parsing.html">87 emnlp-2013-Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help NP Parsing</a></p>
<p>7 0.4749206 <a title="172-lsi-7" href="./emnlp-2013-Detecting_Compositionality_of_Multi-Word_Expressions_using_Nearest_Neighbours_in_Vector_Space_Models.html">60 emnlp-2013-Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</a></p>
<p>8 0.45047036 <a title="172-lsi-8" href="./emnlp-2013-Studying_the_Recursive_Behaviour_of_Adjectival_Modification_with_Compositional_Distributional_Semantics.html">177 emnlp-2013-Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics</a></p>
<p>9 0.44381723 <a title="172-lsi-9" href="./emnlp-2013-Understanding_and_Quantifying_Creativity_in_Lexical_Composition.html">191 emnlp-2013-Understanding and Quantifying Creativity in Lexical Composition</a></p>
<p>10 0.43073943 <a title="172-lsi-10" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>11 0.39617601 <a title="172-lsi-11" href="./emnlp-2013-Feature_Noising_for_Log-Linear_Structured_Prediction.html">86 emnlp-2013-Feature Noising for Log-Linear Structured Prediction</a></p>
<p>12 0.38077655 <a title="172-lsi-12" href="./emnlp-2013-Recurrent_Continuous_Translation_Models.html">156 emnlp-2013-Recurrent Continuous Translation Models</a></p>
<p>13 0.33481151 <a title="172-lsi-13" href="./emnlp-2013-Multi-Relational_Latent_Semantic_Analysis.html">137 emnlp-2013-Multi-Relational Latent Semantic Analysis</a></p>
<p>14 0.33058903 <a title="172-lsi-14" href="./emnlp-2013-Structured_Penalties_for_Log-Linear_Language_Models.html">176 emnlp-2013-Structured Penalties for Log-Linear Language Models</a></p>
<p>15 0.32490468 <a title="172-lsi-15" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>16 0.30160055 <a title="172-lsi-16" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>17 0.29779238 <a title="172-lsi-17" href="./emnlp-2013-Decoding_with_Large-Scale_Neural_Language_Models_Improves_Translation.html">55 emnlp-2013-Decoding with Large-Scale Neural Language Models Improves Translation</a></p>
<p>18 0.29410452 <a title="172-lsi-18" href="./emnlp-2013-Tree_Kernel-based_Negation_and_Speculation_Scope_Detection_with_Structured_Syntactic_Parse_Features.html">188 emnlp-2013-Tree Kernel-based Negation and Speculation Scope Detection with Structured Syntactic Parse Features</a></p>
<p>19 0.29269463 <a title="172-lsi-19" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>20 0.28974417 <a title="172-lsi-20" href="./emnlp-2013-Joint_Language_and_Translation_Modeling_with_Recurrent_Neural_Networks.html">113 emnlp-2013-Joint Language and Translation Modeling with Recurrent Neural Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.031), (6, 0.054), (9, 0.014), (13, 0.307), (18, 0.047), (22, 0.039), (30, 0.092), (45, 0.023), (50, 0.026), (51, 0.14), (64, 0.011), (66, 0.026), (71, 0.014), (75, 0.027), (77, 0.034), (96, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.72340328 <a title="172-lda-1" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>Author: Kazuma Hashimoto ; Makoto Miwa ; Yoshimasa Tsuruoka ; Takashi Chikayama</p><p>Abstract: In this paper, we present a recursive neural network (RNN) model that works on a syntactic tree. Our model differs from previous RNN models in that the model allows for an explicit weighting of important phrases for the target task. We also propose to average parameters in training. Our experimental results on semantic relation classification show that both phrase categories and task-specific weighting significantly improve the prediction accuracy of the model. We also show that averaging the model parameters is effective in stabilizing the learning and improves generalization capacity. The proposed model marks scores competitive with state-of-the-art RNN-based models.</p><p>2 0.71244383 <a title="172-lda-2" href="./emnlp-2013-Using_Paraphrases_and_Lexical_Semantics_to_Improve_the_Accuracy_and_the_Robustness_of_Supervised_Models_in_Situated_Dialogue_Systems.html">197 emnlp-2013-Using Paraphrases and Lexical Semantics to Improve the Accuracy and the Robustness of Supervised Models in Situated Dialogue Systems</a></p>
<p>Author: Claire Gardent ; Lina M. Rojas Barahona</p><p>Abstract: This paper explores to what extent lemmatisation, lexical resources, distributional semantics and paraphrases can increase the accuracy of supervised models for dialogue management. The results suggest that each of these factors can help improve performance but that the impact will vary depending on their combination and on the evaluation mode.</p><p>3 0.53209084 <a title="172-lda-3" href="./emnlp-2013-Recursive_Deep_Models_for_Semantic_Compositionality_Over_a_Sentiment_Treebank.html">158 emnlp-2013-Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></p>
<p>Author: Richard Socher ; Alex Perelygin ; Jean Wu ; Jason Chuang ; Christopher D. Manning ; Andrew Ng ; Christopher Potts</p><p>Abstract: Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.</p><p>4 0.52730846 <a title="172-lda-4" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>Author: Masashi Tsubaki ; Kevin Duh ; Masashi Shimbo ; Yuji Matsumoto</p><p>Abstract: We present a novel vector space model for semantic co-compositionality. Inspired by Generative Lexicon Theory (Pustejovsky, 1995), our goal is a compositional model where both predicate and argument are allowed to modify each others’ meaning representations while generating the overall semantics. This readily addresses some major challenges with current vector space models, notably the polysemy issue and the use of one representation per word type. We implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations. We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality. The model achieves the best result to date (ρ = 0.47) on the semantic similarity task of transitive verbs (Grefenstette and Sadrzadeh, 2011).</p><p>5 0.51753634 <a title="172-lda-5" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>Author: Peng Li ; Yang Liu ; Maosong Sun</p><p>Abstract: While inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (i.e., straight and inverted) dependent on actual blocks being merged remains a challenge. Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points.</p><p>6 0.50986725 <a title="172-lda-6" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>7 0.50872624 <a title="172-lda-7" href="./emnlp-2013-Bilingual_Word_Embeddings_for_Phrase-Based_Machine_Translation.html">38 emnlp-2013-Bilingual Word Embeddings for Phrase-Based Machine Translation</a></p>
<p>8 0.50805664 <a title="172-lda-8" href="./emnlp-2013-Source-Side_Classifier_Preordering_for_Machine_Translation.html">175 emnlp-2013-Source-Side Classifier Preordering for Machine Translation</a></p>
<p>9 0.50486451 <a title="172-lda-9" href="./emnlp-2013-Interactive_Machine_Translation_using_Hierarchical_Translation_Models.html">107 emnlp-2013-Interactive Machine Translation using Hierarchical Translation Models</a></p>
<p>10 0.50472033 <a title="172-lda-10" href="./emnlp-2013-A_Systematic_Exploration_of_Diversity_in_Machine_Translation.html">15 emnlp-2013-A Systematic Exploration of Diversity in Machine Translation</a></p>
<p>11 0.50423372 <a title="172-lda-11" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>12 0.50050747 <a title="172-lda-12" href="./emnlp-2013-Prior_Disambiguation_of_Word_Tensors_for_Constructing_Sentence_Vectors.html">154 emnlp-2013-Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</a></p>
<p>13 0.50039881 <a title="172-lda-13" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>14 0.50009924 <a title="172-lda-14" href="./emnlp-2013-A_Study_on_Bootstrapping_Bilingual_Vector_Spaces_from_Non-Parallel_Data_%28and_Nothing_Else%29.html">13 emnlp-2013-A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)</a></p>
<p>15 0.49932396 <a title="172-lda-15" href="./emnlp-2013-Open_Domain_Targeted_Sentiment.html">143 emnlp-2013-Open Domain Targeted Sentiment</a></p>
<p>16 0.49919835 <a title="172-lda-16" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>17 0.49857068 <a title="172-lda-17" href="./emnlp-2013-Translation_with_Source_Constituency_and_Dependency_Trees.html">187 emnlp-2013-Translation with Source Constituency and Dependency Trees</a></p>
<p>18 0.49825817 <a title="172-lda-18" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>19 0.4974232 <a title="172-lda-19" href="./emnlp-2013-Improving_Pivot-Based_Statistical_Machine_Translation_Using_Random_Walk.html">103 emnlp-2013-Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></p>
<p>20 0.49639323 <a title="172-lda-20" href="./emnlp-2013-Dynamic_Feature_Selection_for_Dependency_Parsing.html">66 emnlp-2013-Dynamic Feature Selection for Dependency Parsing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
