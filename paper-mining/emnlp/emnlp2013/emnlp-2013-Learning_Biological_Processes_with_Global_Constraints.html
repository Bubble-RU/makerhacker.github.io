<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>118 emnlp-2013-Learning Biological Processes with Global Constraints</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-118" href="#">emnlp2013-118</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>118 emnlp-2013-Learning Biological Processes with Global Constraints</h1>
<br/><p>Source: <a title="emnlp-2013-118-pdf" href="http://aclweb.org/anthology//D/D13/D13-1177.pdf">pdf</a></p><p>Author: Aju Thalappillil Scaria ; Jonathan Berant ; Mengqiu Wang ; Peter Clark ; Justin Lewis ; Brittany Harding ; Christopher D. Manning</p><p>Abstract: Biological processes are complex phenomena involving a series of events that are related to one another through various relationships. Systems that can understand and reason over biological processes would dramatically improve the performance of semantic applications involving inference such as question answering (QA) – specifically “How? ” and “Why? ” questions. In this paper, we present the task of process extraction, in which events within a process and the relations between the events are automatically extracted from text. We represent processes by graphs whose edges describe a set oftemporal, causal and co-reference event-event relations, and characterize the structural properties of these graphs (e.g., the graphs are connected). Then, we present a method for extracting relations between the events, which exploits these structural properties by performing joint in- ference over the set of extracted relations. On a novel dataset containing 148 descriptions of biological processes (released with this paper), we show significant improvement comparing to baselines that disregard process structure.</p><p>Reference: <a title="emnlp-2013-118-reference" href="../emnlp2013_reference/emnlp-2013-Learning_Biological_Processes_with_Global_Constraints_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Manning Stanford University, Stanford  Justin Lewis and Brittany Harding University of Washington, Seattle Abstract Biological processes are complex phenomena involving a series of events that are related to one another through various relationships. [sent-2, score-0.568]
</p><p>2 Systems that can understand and reason over biological processes would dramatically improve the performance of semantic applications involving inference such as question answering (QA) – specifically “How? [sent-3, score-0.411]
</p><p>3 In this paper, we present the task of process extraction, in which events within a process and the relations between the events are automatically extracted from text. [sent-6, score-1.024]
</p><p>4 We represent processes by graphs whose edges describe a set oftemporal, causal and co-reference event-event relations, and characterize the structural properties of these graphs (e. [sent-7, score-0.688]
</p><p>5 Then, we present a method for extracting relations between the events, which exploits these structural properties by performing joint in-  ference over the set of extracted relations. [sent-10, score-0.339]
</p><p>6 On a novel dataset containing 148 descriptions of biological processes (released with this paper), we show significant improvement comparing to baselines that disregard process structure. [sent-11, score-0.61]
</p><p>7 1 Introduction A process is defined as a series of inter-related events that involve multiple entities and lead to an end result. [sent-12, score-0.466]
</p><p>8 Product manufacturing, economical developments, and various phenomena in life and social sciences can all be viewed as types of processes. [sent-13, score-0.036]
</p><p>9 Processes are complicated objects; consider for example the biological process of ATP synthesis described in Figure 1. [sent-14, score-0.472]
</p><p>10 Additionally, it describes relations between events and entities, and the relationship between events (e. [sent-16, score-0.762]
</p><p>11 , the second occurrence of the event ‘enter’, causes the event ‘changing ’). [sent-18, score-0.983]
</p><p>12 ∗ Both authors equally contributed to the paper 1710 Peter Clark Allen Institute for Artificial Intelligence, Seattle  Automatically extracting the structure of processes from text is crucial for applications that require reasoning, such as non-factoid QA. [sent-19, score-0.218]
</p><p>13 For instance, answering a question on ATP synthesis, such as “How do H+ ions contribute to the production of ATP? [sent-20, score-0.058]
</p><p>14 ” requires a structure that links H+ ions (Figure 1, sentence 1) to ATP (Figure 1, sentence 4) through a sequence of intermediate events. [sent-21, score-0.058]
</p><p>15 , 2011), which further supports the importance of process extraction. [sent-24, score-0.152]
</p><p>16 Process extraction is related to two recent lines of work in Information Extraction event extraction and timeline construction. [sent-25, score-0.669]
</p><p>17 Traditional event extraction focuses on identifying a closed set of events within a single sentence. [sent-26, score-0.801]
</p><p>18 In practice, events are currently almost always extracted from a single sentence. [sent-30, score-0.28]
</p><p>19 Process extraction, on the other hand, is centered around discovering relations between events that span multiple sentences. [sent-31, score-0.482]
</p><p>20 The set of possible event types in process –  extraction is also much larger. [sent-32, score-0.673]
</p><p>21 Timeline construction involves identifying temporal relations between events (Do et al. [sent-33, score-0.627]
</p><p>22 , 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and is thus related to process extraction as both focus on event-event relations spanning multiple sentences. [sent-34, score-0.405]
</p><p>23 However, events in processes are tightly coupled in ways that go beyond simple temporal ordering, and these dependencies are central for the process extraction task. [sent-35, score-0.962]
</p><p>24 Hence, capturing process structure requires modeling a larger set of relations that includes temporal, causal and co-reference relations. [sent-36, score-0.399]
</p><p>25 In this paper, we formally define the task of process extraction and present automatic extraction methods. [sent-37, score-0.338]
</p><p>26 Our approach handles an open set of event types and works over multiple sentences, extracting a rich set of event-event relations. [sent-38, score-0.428]
</p><p>27 oc d2s0 i1n3 N Aastusorcaila Ltiaon g fuoarg Ceo Pmrpoucetastsi on ga,l p Laignegsu 1is7t1ic0s–1720,  Figure 1: Partial annotation of the ATP synthesis process. [sent-41, score-0.127]
</p><p>28 Most of the semantic roles have been removed for simplicity. [sent-42, score-0.057]
</p><p>29 we characterize a set of global properties of process structure that can be utilized during process extraction. [sent-43, score-0.482]
</p><p>30 For example, all events in a process are somehow connected to one another. [sent-44, score-0.432]
</p><p>31 Also, processes usually exhibit a “chain-like” structure reflecting process progression over time. [sent-45, score-0.407]
</p><p>32 We show that incorporating such global properties into our model and performing joint inference over the extracted relations significantly improves the quality of process structures predicted. [sent-46, score-0.435]
</p><p>33 We conduct experiments on a novel dataset of process descriptions from the textbook “Biology” (Campbell and Reece, 2005) that were annotated by trained biologists. [sent-47, score-0.232]
</p><p>34 We define process extraction and characterize processes’ structural properties. [sent-50, score-0.412]
</p><p>35 We model global structural properties in processes and demonstrate significant improvement in extraction accuracy. [sent-52, score-0.546]
</p><p>36 We publicly release a novel data set of 148 fully annotated biological process descript. [sent-54, score-0.345]
</p><p>37 2  Process Definition and Dataset  We define a process description as a paragraph or sequence of tokens x = {x1, . [sent-61, score-0.193]
</p><p>38 x|x| } that describes 1711 a series of events related by temporal and/or causal relations. [sent-64, score-0.63]
</p><p>39 For example, in ATP synthesis (Figure 1), the event of rotor spinning causes the event where an internal rod spins. [sent-65, score-1.262]
</p><p>40 We model the events within a process and their relations by a directed graph P = (V, E), where trhelea tnioodness b Vy = {1, . [sent-66, score-0.64]
</p><p>41 , |V | } represent ,eEve)n,t w menttihoens n oandeds sla Vbel =ed edges ,E| correspond ntot eevveenntt- emveenntrelations. [sent-69, score-0.108]
</p><p>42 An event mention v ∈ V is defined by a trigger tv, wAhni cehv eisn a span oofn w vo ∈rds V xi, xi+1 , . [sent-70, score-0.645]
</p><p>43 , xj ; and by a set of argument mentions Av, where each argument mention av ∈ Av is also a span of words labeled by a semantic r∈ole A ltaken from a set L. [sent-73, score-0.421]
</p><p>44 For example, yin a th seem mlaasntt event em le tnatkioenn ofrfo AmTP a synthesis, tv = produce, and one of the argument mentions is  av = (ATP, RESULT). [sent-74, score-0.781]
</p><p>45 A labeled edge (u, v, r) in the graph describes a relation r ∈ R between the event gmreanpthio dness u iabneds v. [sent-75, score-0.66]
</p><p>46 eTlaheti otans kr o∈f process eexntr tahceti eovne nist to extract the graph P from the text x. [sent-76, score-0.2]
</p><p>47 1 A natural way to break down process extraction into sub-parts is to first perform semantic role labeling (SRL), that is, identify triggers and predict argument mentions with their semantic role, and then extract event-event relations between pairs of event mentions. [sent-77, score-1.078]
</p><p>48 In this paper, we focus on the second step, where given a set of event triggers T , we find astlle pe,v ewnhte-revee gnivt relations, fw ehveernet a trigger represents the entire event. [sent-78, score-0.613]
</p><p>49 For completeness, we now describe the semantic roles L used in our dataset, and then 1Argument mentions are also related by coreference relations, but we neglect that since it is not central in this paper. [sent-79, score-0.277]
</p><p>50 eTsehen tse tht eL s ecton otfai enves sntt-anevdeanrdt rseelmataionntsic Rro. [sent-81, score-0.033]
</p><p>51 Two additional semantic roles were employed that are relevant for biological text: RESULT corresponds to an entity that is the result of an event, and RAW-MATERIAL describes an entity that is used or consumed during an event. [sent-83, score-0.292]
</p><p>52 For example, the last  event ‘produce ’ in Figure 1, has ‘ATP’ as the RESULT, and ‘ADP’ as the RAW-MATERIAL. [sent-84, score-0.428]
</p><p>53 The event-event relation set R contains the following (assuming a rlealbaetileodn edge (u, v, r)): 1. [sent-85, score-0.142]
</p><p>54 PREV denotes that u is an event immediately before v. [sent-86, score-0.483]
</p><p>55 Thus, the edges (u, v, PREV) and (v, w, PREV), preclude the edge (u, w, PREV). [sent-87, score-0.154]
</p><p>56 ”, there is no edge (strikes, reaches, PREV) due to the intervening event ‘passed’. [sent-97, score-0.507]
</p><p>57 COTEMP denotes that events u and v overlap in time (e. [sent-99, score-0.335]
</p><p>58 , the first two event mentions flowing and enter in Figure 1). [sent-101, score-0.617]
</p><p>59 For instance, in “During DNA replication, DNA polymerases proofread each nucleotide. [sent-104, score-0.083]
</p><p>60 ” there is an edge (DNA replication, proofread, SUPER). [sent-107, score-0.079]
</p><p>61 , the relation between changing and spins in sentence 2 of Figure 1). [sent-111, score-0.187]
</p><p>62 ENABLES denotes that event u creates preconditions that allow event v to take place. [sent-113, score-0.911]
</p><p>63 , allowing them to spread into nearby tissues” has the edge (lose, spread, ENABLES). [sent-120, score-0.13]
</p><p>64 An intuitive way to think about the difference between Causes and Enables is the following: if u causes v this means that if u happens, then v happens. [sent-121, score-0.127]
</p><p>65 If u enables v, then if u does not happen, then v does not happen. [sent-122, score-0.073]
</p><p>66 SAME denotes that u and v both refer to the same event (spins and Spinning in Figure 1). [sent-124, score-0.483]
</p><p>67 Early work on temporal logic (Allen, 1983) contained more temporal relations than are used in our 1712  Tabl#e1of:nProc-Ne#sOoNsf#tEasotrefisntle aovctnkseiocn ots ver16358A4. [sent-125, score-0.567]
</p><p>68 We chose a relation set R that captruelreasti othne s eests Ren. [sent-130, score-0.063]
</p><p>69 ti Wal aspects ao fr temporal tre Rlat thioants c bape-tween events in a process, while keeping the annotation as simple as possible. [sent-131, score-0.529]
</p><p>70 For instance, we include the SUPER relation that appears in temporal annotations such as the Timebank corpus (Pustejovsky et al. [sent-132, score-0.25]
</p><p>71 , 2003) and Allen’s work, but in practice was not considered by many temporal ordering systems (Chambers and Jurafsky, 2008; Yoshikawa et al. [sent-133, score-0.228]
</p><p>72 Importantly, our relation set also includes the relations CAUSES and ENABLES, which are fundamental to modeling processes and go beyond simple temporal ordering. [sent-136, score-0.628]
</p><p>73 in a temporal ordering task to modify probabilities provided by pairwise classifiers prior to joint inference. [sent-140, score-0.291]
</p><p>74 In this paper, we simply treat SAME as another event-event relation, which allows us to easily perform joint inference and employ structural constraints that combine both coreference and temporal relations simultaneously. [sent-141, score-0.524]
</p><p>75 3) We annotated 148 process descriptions based on the aforementioned definitions. [sent-143, score-0.199]
</p><p>76 Structural properties of processes Coherent processes exhibit many structural properties. [sent-145, score-0.652]
</p><p>77 For example, two argument mentions related to the same event cannot overlap a constraint that has been used in the past in SRL (Toutanova et al. [sent-146, score-0.586]
</p><p>78 In this paper we focus on three main structural properties of the graph P. [sent-148, score-0.227]
</p><p>79 First, in a coherent process, sa lol fev tehnets g mrapenhtio Pn. [sent-149, score-0.034]
</p><p>80 ed are related to one another, and hence the graph P must be connected. [sent-150, score-0.048]
</p><p>81 Second, processes tgernadp hto Pha mveu a t“ bcheai cno-nlinkeec”t sdt. [sent-151, score-0.251]
</p><p>82 ructure where one event follows another, and thus we expect –  Table2:No10D23d≥e4gde. [sent-152, score-0.428]
</p><p>83 Indeed, 90% of event ’m deengtrieoens t oh gavene degree ≤ 2, as ddeeemdo,n 9s0tr%ate odf by tnhte mGeonldti ocnoslu hmanve eo fd eTgarbelee 2≤. [sent-156, score-0.517]
</p><p>84 Last, i fd we ocnosntsraidteedr relations between all possible triples of events in a process, clearly some configurations are impossible, while others are common (illustrated in Figure 2). [sent-157, score-0.493]
</p><p>85 3, we show that modeling these properties using a joint inference framework improves the quality of process extraction significantly. [sent-159, score-0.312]
</p><p>86 3  Joint Model for Process Extraction  Given a paragraph x and a trigger set T , we wish Gtoi evexntra act p aarlla event-event dre ala ttriiogngse Er s. [sent-160, score-0.211]
</p><p>87 (2012), our model consists of a local pairwise classifier and global constraints. [sent-162, score-0.159]
</p><p>88 We first introduce a classifier that is based on features from previous work. [sent-163, score-0.04]
</p><p>89 Next, we describe novel features specific for process extraction. [sent-164, score-0.152]
</p><p>90 Last, we incorporate global constraints into our model using an ILP formulation. [sent-165, score-0.056]
</p><p>91 1 Local pairwise classifier The local pairwise classifier predicts relations between all event mention pairs. [sent-167, score-0.838]
</p><p>92 After adding NONE to indicate no relation, and including the undirected relations COTEMP and SAME, R contains 11relations. [sent-169, score-0.16]
</p><p>93 The classifier is hence a fun, cRtio cno f : nTs Trel → Rns. [sent-170, score-0.073]
</p><p>94 aLnet e n a bme pthlee, number of triggers in a process, and ti be the i-th trigger in its description. [sent-174, score-0.283]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('event', 0.428), ('atp', 0.333), ('events', 0.28), ('processes', 0.218), ('prev', 0.208), ('biological', 0.193), ('temporal', 0.187), ('relations', 0.16), ('process', 0.152), ('causes', 0.127), ('synthesis', 0.127), ('dna', 0.125), ('structural', 0.112), ('av', 0.106), ('trigger', 0.098), ('extraction', 0.093), ('tj', 0.089), ('triggers', 0.087), ('causal', 0.087), ('mentions', 0.087), ('cotemp', 0.083), ('proofread', 0.083), ('spinning', 0.083), ('spins', 0.083), ('strikes', 0.083), ('allen', 0.083), ('super', 0.083), ('edge', 0.079), ('enables', 0.073), ('argument', 0.071), ('properties', 0.067), ('enter', 0.066), ('replication', 0.066), ('coreference', 0.065), ('pairwise', 0.063), ('relation', 0.063), ('ti', 0.062), ('srl', 0.058), ('ions', 0.058), ('roles', 0.057), ('global', 0.056), ('timeline', 0.055), ('denotes', 0.055), ('characterize', 0.055), ('graphs', 0.055), ('fd', 0.053), ('tv', 0.053), ('spread', 0.051), ('passed', 0.051), ('lose', 0.049), ('graph', 0.048), ('descriptions', 0.047), ('seattle', 0.046), ('mention', 0.044), ('describes', 0.042), ('span', 0.042), ('ordering', 0.041), ('paragraph', 0.041), ('changing', 0.041), ('reaches', 0.041), ('classifier', 0.04), ('edges', 0.039), ('exhibit', 0.037), ('dre', 0.036), ('bionlp', 0.036), ('wal', 0.036), ('timebank', 0.036), ('rotor', 0.036), ('ala', 0.036), ('oftemporal', 0.036), ('ofrfo', 0.036), ('neglect', 0.036), ('preclude', 0.036), ('infor', 0.036), ('campbell', 0.036), ('attachments', 0.036), ('bme', 0.036), ('flowing', 0.036), ('harding', 0.036), ('mcclosky', 0.036), ('ntot', 0.036), ('ole', 0.036), ('pha', 0.036), ('sti', 0.036), ('tnhte', 0.036), ('phenomena', 0.036), ('coherent', 0.034), ('series', 0.034), ('eisn', 0.033), ('rod', 0.033), ('hto', 0.033), ('rds', 0.033), ('tse', 0.033), ('cno', 0.033), ('ots', 0.033), ('sla', 0.033), ('textbook', 0.033), ('wre', 0.033), ('central', 0.032), ('kim', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="118-tfidf-1" href="./emnlp-2013-Learning_Biological_Processes_with_Global_Constraints.html">118 emnlp-2013-Learning Biological Processes with Global Constraints</a></p>
<p>Author: Aju Thalappillil Scaria ; Jonathan Berant ; Mengqiu Wang ; Peter Clark ; Justin Lewis ; Brittany Harding ; Christopher D. Manning</p><p>Abstract: Biological processes are complex phenomena involving a series of events that are related to one another through various relationships. Systems that can understand and reason over biological processes would dramatically improve the performance of semantic applications involving inference such as question answering (QA) – specifically “How? ” and “Why? ” questions. In this paper, we present the task of process extraction, in which events within a process and the relations between the events are automatically extracted from text. We represent processes by graphs whose edges describe a set oftemporal, causal and co-reference event-event relations, and characterize the structural properties of these graphs (e.g., the graphs are connected). Then, we present a method for extracting relations between the events, which exploits these structural properties by performing joint in- ference over the set of extracted relations. On a novel dataset containing 148 descriptions of biological processes (released with this paper), we show significant improvement comparing to baselines that disregard process structure.</p><p>2 0.31957272 <a title="118-tfidf-2" href="./emnlp-2013-A_Unified_Model_for_Topics%2C_Events_and_Users_on_Twitter.html">16 emnlp-2013-A Unified Model for Topics, Events and Users on Twitter</a></p>
<p>Author: Qiming Diao ; Jing Jiang</p><p>Abstract: With the rapid growth of social media, Twitter has become one of the most widely adopted platforms for people to post short and instant message. On the one hand, people tweets about their daily lives, and on the other hand, when major events happen, people also follow and tweet about them. Moreover, people’s posting behaviors on events are often closely tied to their personal interests. In this paper, we try to model topics, events and users on Twitter in a unified way. We propose a model which combines an LDA-like topic model and the Recurrent Chinese Restaurant Process to capture topics and events. We further propose a duration-based regularization component to find bursty events. We also propose to use event-topic affinity vectors to model the asso- . ciation between events and topics. Our experiments shows that our model can accurately identify meaningful events and the event-topic affinity vectors are effective for event recommendation and grouping events by topics.</p><p>3 0.27631807 <a title="118-tfidf-3" href="./emnlp-2013-Unsupervised_Induction_of_Contingent_Event_Pairs_from_Film_Scenes.html">192 emnlp-2013-Unsupervised Induction of Contingent Event Pairs from Film Scenes</a></p>
<p>Author: Zhichao Hu ; Elahe Rahimtoroghi ; Larissa Munishkina ; Reid Swanson ; Marilyn A. Walker</p><p>Abstract: Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning. Researchers in NLP have tackled modeling such expectations from a range of perspectives, including treating it as the inference of the CONTINGENT discourse relation, or as a type of common-sense causal reasoning. Our approach is to model likelihood between events by drawing on several of these lines of previous work. We implement and evaluate different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another. We refine event pairs that we learn from a corpus of film scene descriptions utilizing web search counts, and evaluate our results by collecting human judgments ofcontingency. Our results indicate that the use of web search counts increases the av- , erage accuracy of our best method to 85.64% over a baseline of 50%, as compared to an average accuracy of 75. 15% without web search.</p><p>4 0.26279843 <a title="118-tfidf-4" href="./emnlp-2013-Building_Event_Threads_out_of_Multiple_News_Articles.html">41 emnlp-2013-Building Event Threads out of Multiple News Articles</a></p>
<p>Author: Xavier Tannier ; Veronique Moriceau</p><p>Abstract: We present an approach for building multidocument event threads from a large corpus of newswire articles. An event thread is basically a succession of events belonging to the same story. It helps the reader to contextualize the information contained in a single article, by navigating backward or forward in the thread from this article. A specific effort is also made on the detection of reactions to a particular event. In order to build these event threads, we use a cascade of classifiers and other modules, taking advantage of the redundancy of information in the newswire corpus. We also share interesting comments concerning our manual annotation procedure for building a training and testing set1.</p><p>5 0.23241854 <a title="118-tfidf-5" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>Author: Jun-Ping Ng ; Min-Yen Kan ; Ziheng Lin ; Wei Feng ; Bin Chen ; Jian Su ; Chew Lim Tan</p><p>Abstract: In this paper we classify the temporal relations between pairs of events on an article-wide basis. This is in contrast to much of the existing literature which focuses on just event pairs which are found within the same or adjacent sentences. To achieve this, we leverage on discourse analysis as we believe that it provides more useful semantic information than typical lexico-syntactic features. We propose the use of several discourse analysis frameworks, including 1) Rhetorical Structure Theory (RST), 2) PDTB-styled discourse relations, and 3) topical text segmentation. We explain how features derived from these frameworks can be effectively used with support vector machines (SVM) paired with convolution kernels. Experiments show that our proposal is effective in improving on the state-of-the-art significantly by as much as 16% in terms of F1, even if we only adopt less-than-perfect automatic discourse analyzers and parsers. Making use of more accurate discourse analysis can further boost gains to 35%.</p><p>6 0.22386843 <a title="118-tfidf-6" href="./emnlp-2013-Event-Based_Time_Label_Propagation_for_Automatic_Dating_of_News_Articles.html">74 emnlp-2013-Event-Based Time Label Propagation for Automatic Dating of News Articles</a></p>
<p>7 0.19353172 <a title="118-tfidf-7" href="./emnlp-2013-Optimized_Event_Storyline_Generation_based_on_Mixture-Event-Aspect_Model.html">147 emnlp-2013-Optimized Event Storyline Generation based on Mixture-Event-Aspect Model</a></p>
<p>8 0.18306121 <a title="118-tfidf-8" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>9 0.14798017 <a title="118-tfidf-9" href="./emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model.html">75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</a></p>
<p>10 0.11268673 <a title="118-tfidf-10" href="./emnlp-2013-Effectiveness_and_Efficiency_of_Open_Relation_Extraction.html">68 emnlp-2013-Effectiveness and Efficiency of Open Relation Extraction</a></p>
<p>11 0.11156759 <a title="118-tfidf-11" href="./emnlp-2013-Generating_Coherent_Event_Schemas_at_Scale.html">90 emnlp-2013-Generating Coherent Event Schemas at Scale</a></p>
<p>12 0.10306189 <a title="118-tfidf-12" href="./emnlp-2013-Easy_Victories_and_Uphill_Battles_in_Coreference_Resolution.html">67 emnlp-2013-Easy Victories and Uphill Battles in Coreference Resolution</a></p>
<p>13 0.094918989 <a title="118-tfidf-13" href="./emnlp-2013-A_Constrained_Latent_Variable_Model_for_Coreference_Resolution.html">1 emnlp-2013-A Constrained Latent Variable Model for Coreference Resolution</a></p>
<p>14 0.088522248 <a title="118-tfidf-14" href="./emnlp-2013-Error-Driven_Analysis_of_Challenges_in_Coreference_Resolution.html">73 emnlp-2013-Error-Driven Analysis of Challenges in Coreference Resolution</a></p>
<p>15 0.087166719 <a title="118-tfidf-15" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>16 0.085041307 <a title="118-tfidf-16" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>17 0.081658944 <a title="118-tfidf-17" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>18 0.072198033 <a title="118-tfidf-18" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>19 0.069598615 <a title="118-tfidf-19" href="./emnlp-2013-Detection_of_Product_Comparisons_-_How_Far_Does_an_Out-of-the-Box_Semantic_Role_Labeling_System_Take_You%3F.html">62 emnlp-2013-Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?</a></p>
<p>20 0.062098611 <a title="118-tfidf-20" href="./emnlp-2013-Joint_Coreference_Resolution_and_Named-Entity_Linking_with_Multi-Pass_Sieves.html">112 emnlp-2013-Joint Coreference Resolution and Named-Entity Linking with Multi-Pass Sieves</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.215), (1, 0.264), (2, 0.02), (3, 0.36), (4, 0.04), (5, -0.245), (6, -0.28), (7, -0.003), (8, -0.128), (9, 0.07), (10, 0.064), (11, 0.019), (12, 0.032), (13, -0.047), (14, 0.009), (15, -0.094), (16, -0.029), (17, 0.031), (18, -0.046), (19, 0.027), (20, 0.024), (21, -0.06), (22, -0.001), (23, 0.008), (24, 0.05), (25, 0.044), (26, 0.005), (27, 0.009), (28, -0.049), (29, 0.042), (30, -0.027), (31, -0.112), (32, -0.009), (33, -0.036), (34, -0.0), (35, 0.083), (36, 0.068), (37, 0.018), (38, -0.008), (39, 0.01), (40, -0.066), (41, 0.031), (42, 0.005), (43, -0.047), (44, 0.021), (45, -0.061), (46, -0.006), (47, -0.02), (48, 0.014), (49, -0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98649138 <a title="118-lsi-1" href="./emnlp-2013-Learning_Biological_Processes_with_Global_Constraints.html">118 emnlp-2013-Learning Biological Processes with Global Constraints</a></p>
<p>Author: Aju Thalappillil Scaria ; Jonathan Berant ; Mengqiu Wang ; Peter Clark ; Justin Lewis ; Brittany Harding ; Christopher D. Manning</p><p>Abstract: Biological processes are complex phenomena involving a series of events that are related to one another through various relationships. Systems that can understand and reason over biological processes would dramatically improve the performance of semantic applications involving inference such as question answering (QA) – specifically “How? ” and “Why? ” questions. In this paper, we present the task of process extraction, in which events within a process and the relations between the events are automatically extracted from text. We represent processes by graphs whose edges describe a set oftemporal, causal and co-reference event-event relations, and characterize the structural properties of these graphs (e.g., the graphs are connected). Then, we present a method for extracting relations between the events, which exploits these structural properties by performing joint in- ference over the set of extracted relations. On a novel dataset containing 148 descriptions of biological processes (released with this paper), we show significant improvement comparing to baselines that disregard process structure.</p><p>2 0.87904525 <a title="118-lsi-2" href="./emnlp-2013-Unsupervised_Induction_of_Contingent_Event_Pairs_from_Film_Scenes.html">192 emnlp-2013-Unsupervised Induction of Contingent Event Pairs from Film Scenes</a></p>
<p>Author: Zhichao Hu ; Elahe Rahimtoroghi ; Larissa Munishkina ; Reid Swanson ; Marilyn A. Walker</p><p>Abstract: Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning. Researchers in NLP have tackled modeling such expectations from a range of perspectives, including treating it as the inference of the CONTINGENT discourse relation, or as a type of common-sense causal reasoning. Our approach is to model likelihood between events by drawing on several of these lines of previous work. We implement and evaluate different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another. We refine event pairs that we learn from a corpus of film scene descriptions utilizing web search counts, and evaluate our results by collecting human judgments ofcontingency. Our results indicate that the use of web search counts increases the av- , erage accuracy of our best method to 85.64% over a baseline of 50%, as compared to an average accuracy of 75. 15% without web search.</p><p>3 0.80957502 <a title="118-lsi-3" href="./emnlp-2013-Building_Event_Threads_out_of_Multiple_News_Articles.html">41 emnlp-2013-Building Event Threads out of Multiple News Articles</a></p>
<p>Author: Xavier Tannier ; Veronique Moriceau</p><p>Abstract: We present an approach for building multidocument event threads from a large corpus of newswire articles. An event thread is basically a succession of events belonging to the same story. It helps the reader to contextualize the information contained in a single article, by navigating backward or forward in the thread from this article. A specific effort is also made on the detection of reactions to a particular event. In order to build these event threads, we use a cascade of classifiers and other modules, taking advantage of the redundancy of information in the newswire corpus. We also share interesting comments concerning our manual annotation procedure for building a training and testing set1.</p><p>4 0.78835177 <a title="118-lsi-4" href="./emnlp-2013-Event-Based_Time_Label_Propagation_for_Automatic_Dating_of_News_Articles.html">74 emnlp-2013-Event-Based Time Label Propagation for Automatic Dating of News Articles</a></p>
<p>Author: Tao Ge ; Baobao Chang ; Sujian Li ; Zhifang Sui</p><p>Abstract: Since many applications such as timeline summaries and temporal IR involving temporal analysis rely on document timestamps, the task of automatic dating of documents has been increasingly important. Instead of using feature-based methods as conventional models, our method attempts to date documents in a year level by exploiting relative temporal relations between documents and events, which are very effective for dating documents. Based on this intuition, we proposed an eventbased time label propagation model called confidence boosting in which time label information can be propagated between documents and events on a bipartite graph. The experiments show that our event-based propagation model can predict document timestamps in high accuracy and the model combined with a MaxEnt classifier outperforms the state-ofthe-art method for this task especially when the size of the training set is small.</p><p>5 0.72462982 <a title="118-lsi-5" href="./emnlp-2013-A_Unified_Model_for_Topics%2C_Events_and_Users_on_Twitter.html">16 emnlp-2013-A Unified Model for Topics, Events and Users on Twitter</a></p>
<p>Author: Qiming Diao ; Jing Jiang</p><p>Abstract: With the rapid growth of social media, Twitter has become one of the most widely adopted platforms for people to post short and instant message. On the one hand, people tweets about their daily lives, and on the other hand, when major events happen, people also follow and tweet about them. Moreover, people’s posting behaviors on events are often closely tied to their personal interests. In this paper, we try to model topics, events and users on Twitter in a unified way. We propose a model which combines an LDA-like topic model and the Recurrent Chinese Restaurant Process to capture topics and events. We further propose a duration-based regularization component to find bursty events. We also propose to use event-topic affinity vectors to model the asso- . ciation between events and topics. Our experiments shows that our model can accurately identify meaningful events and the event-topic affinity vectors are effective for event recommendation and grouping events by topics.</p><p>6 0.64899862 <a title="118-lsi-6" href="./emnlp-2013-Exploiting_Discourse_Analysis_for_Article-Wide_Temporal_Classification.html">76 emnlp-2013-Exploiting Discourse Analysis for Article-Wide Temporal Classification</a></p>
<p>7 0.61908656 <a title="118-lsi-7" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>8 0.58592874 <a title="118-lsi-8" href="./emnlp-2013-Optimized_Event_Storyline_Generation_based_on_Mixture-Event-Aspect_Model.html">147 emnlp-2013-Optimized Event Storyline Generation based on Mixture-Event-Aspect Model</a></p>
<p>9 0.45037112 <a title="118-lsi-9" href="./emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model.html">75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</a></p>
<p>10 0.38153645 <a title="118-lsi-10" href="./emnlp-2013-Effectiveness_and_Efficiency_of_Open_Relation_Extraction.html">68 emnlp-2013-Effectiveness and Efficiency of Open Relation Extraction</a></p>
<p>11 0.34838465 <a title="118-lsi-11" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>12 0.34636229 <a title="118-lsi-12" href="./emnlp-2013-Generating_Coherent_Event_Schemas_at_Scale.html">90 emnlp-2013-Generating Coherent Event Schemas at Scale</a></p>
<p>13 0.33844525 <a title="118-lsi-13" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>14 0.31437829 <a title="118-lsi-14" href="./emnlp-2013-The_VerbCorner_Project%3A_Toward_an_Empirically-Based_Semantic_Decomposition_of_Verbs.html">183 emnlp-2013-The VerbCorner Project: Toward an Empirically-Based Semantic Decomposition of Verbs</a></p>
<p>15 0.29386008 <a title="118-lsi-15" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>16 0.25769117 <a title="118-lsi-16" href="./emnlp-2013-The_Topology_of_Semantic_Knowledge.html">182 emnlp-2013-The Topology of Semantic Knowledge</a></p>
<p>17 0.24416265 <a title="118-lsi-17" href="./emnlp-2013-A_temporal_model_of_text_periodicities_using_Gaussian_Processes.html">18 emnlp-2013-A temporal model of text periodicities using Gaussian Processes</a></p>
<p>18 0.23818268 <a title="118-lsi-18" href="./emnlp-2013-Mining_New_Business_Opportunities%3A_Identifying_Trend_related_Products_by_Leveraging_Commercial_Intents_from_Microblogs.html">131 emnlp-2013-Mining New Business Opportunities: Identifying Trend related Products by Leveraging Commercial Intents from Microblogs</a></p>
<p>19 0.23110905 <a title="118-lsi-19" href="./emnlp-2013-Joint_Coreference_Resolution_and_Named-Entity_Linking_with_Multi-Pass_Sieves.html">112 emnlp-2013-Joint Coreference Resolution and Named-Entity Linking with Multi-Pass Sieves</a></p>
<p>20 0.22667243 <a title="118-lsi-20" href="./emnlp-2013-A_Constrained_Latent_Variable_Model_for_Coreference_Resolution.html">1 emnlp-2013-A Constrained Latent Variable Model for Coreference Resolution</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.028), (9, 0.025), (18, 0.028), (22, 0.124), (30, 0.059), (47, 0.344), (50, 0.017), (51, 0.132), (66, 0.018), (71, 0.013), (75, 0.068), (77, 0.014), (90, 0.018), (96, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76999199 <a title="118-lda-1" href="./emnlp-2013-Learning_Biological_Processes_with_Global_Constraints.html">118 emnlp-2013-Learning Biological Processes with Global Constraints</a></p>
<p>Author: Aju Thalappillil Scaria ; Jonathan Berant ; Mengqiu Wang ; Peter Clark ; Justin Lewis ; Brittany Harding ; Christopher D. Manning</p><p>Abstract: Biological processes are complex phenomena involving a series of events that are related to one another through various relationships. Systems that can understand and reason over biological processes would dramatically improve the performance of semantic applications involving inference such as question answering (QA) – specifically “How? ” and “Why? ” questions. In this paper, we present the task of process extraction, in which events within a process and the relations between the events are automatically extracted from text. We represent processes by graphs whose edges describe a set oftemporal, causal and co-reference event-event relations, and characterize the structural properties of these graphs (e.g., the graphs are connected). Then, we present a method for extracting relations between the events, which exploits these structural properties by performing joint in- ference over the set of extracted relations. On a novel dataset containing 148 descriptions of biological processes (released with this paper), we show significant improvement comparing to baselines that disregard process structure.</p><p>2 0.71112722 <a title="118-lda-2" href="./emnlp-2013-A_Log-Linear_Model_for_Unsupervised_Text_Normalization.html">9 emnlp-2013-A Log-Linear Model for Unsupervised Text Normalization</a></p>
<p>Author: Yi Yang ; Jacob Eisenstein</p><p>Abstract: We present a unified unsupervised statistical model for text normalization. The relationship between standard and non-standard tokens is characterized by a log-linear model, permitting arbitrary features. The weights of these features are trained in a maximumlikelihood framework, employing a novel sequential Monte Carlo training algorithm to overcome the large label space, which would be impractical for traditional dynamic programming solutions. This model is implemented in a normalization system called UNLOL, which achieves the best known results on two normalization datasets, outperforming more complex systems. We use the output of UNLOL to automatically normalize a large corpus of social media text, revealing a set of coherent orthographic styles that underlie online language variation.</p><p>3 0.66089565 <a title="118-lda-3" href="./emnlp-2013-Word_Level_Language_Identification_in_Online_Multilingual_Communication.html">204 emnlp-2013-Word Level Language Identification in Online Multilingual Communication</a></p>
<p>Author: Dong Nguyen ; A. Seza Dogruoz</p><p>Abstract: Multilingual speakers switch between languages in online and spoken communication. Analyses of large scale multilingual data require automatic language identification at the word level. For our experiments with multilingual online discussions, we first tag the language of individual words using language models and dictionaries. Secondly, we incorporate context to improve the performance. We achieve an accuracy of 98%. Besides word level accuracy, we use two new metrics to evaluate this task.</p><p>4 0.49414924 <a title="118-lda-4" href="./emnlp-2013-Building_Event_Threads_out_of_Multiple_News_Articles.html">41 emnlp-2013-Building Event Threads out of Multiple News Articles</a></p>
<p>Author: Xavier Tannier ; Veronique Moriceau</p><p>Abstract: We present an approach for building multidocument event threads from a large corpus of newswire articles. An event thread is basically a succession of events belonging to the same story. It helps the reader to contextualize the information contained in a single article, by navigating backward or forward in the thread from this article. A specific effort is also made on the detection of reactions to a particular event. In order to build these event threads, we use a cascade of classifiers and other modules, taking advantage of the redundancy of information in the newswire corpus. We also share interesting comments concerning our manual annotation procedure for building a training and testing set1.</p><p>5 0.48707548 <a title="118-lda-5" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>Author: Zhiyuan Chen ; Arjun Mukherjee ; Bing Liu ; Meichun Hsu ; Malu Castellanos ; Riddhiman Ghosh</p><p>Abstract: Aspect extraction is one of the key tasks in sentiment analysis. In recent years, statistical models have been used for the task. However, such models without any domain knowledge often produce aspects that are not interpretable in applications. To tackle the issue, some knowledge-based topic models have been proposed, which allow the user to input some prior domain knowledge to generate coherent aspects. However, existing knowledge-based topic models have several major shortcomings, e.g., little work has been done to incorporate the cannot-link type of knowledge or to automatically adjust the number of topics based on domain knowledge. This paper proposes a more advanced topic model, called MC-LDA (LDA with m-set and c-set), to address these problems, which is based on an Extended generalized Pólya urn (E-GPU) model (which is also proposed in this paper). Experiments on real-life product reviews from a variety of domains show that MCLDA outperforms the existing state-of-the-art models markedly.</p><p>6 0.48395759 <a title="118-lda-6" href="./emnlp-2013-Multi-Domain_Adaptation_for_SMT_Using_Multi-Task_Learning.html">136 emnlp-2013-Multi-Domain Adaptation for SMT Using Multi-Task Learning</a></p>
<p>7 0.47953326 <a title="118-lda-7" href="./emnlp-2013-Event-Based_Time_Label_Propagation_for_Automatic_Dating_of_News_Articles.html">74 emnlp-2013-Event-Based Time Label Propagation for Automatic Dating of News Articles</a></p>
<p>8 0.47645751 <a title="118-lda-8" href="./emnlp-2013-Summarizing_Complex_Events%3A_a_Cross-Modal_Solution_of_Storylines_Extraction_and_Reconstruction.html">179 emnlp-2013-Summarizing Complex Events: a Cross-Modal Solution of Storylines Extraction and Reconstruction</a></p>
<p>9 0.47295538 <a title="118-lda-9" href="./emnlp-2013-Collective_Personal_Profile_Summarization_with_Social_Networks.html">48 emnlp-2013-Collective Personal Profile Summarization with Social Networks</a></p>
<p>10 0.47268361 <a title="118-lda-10" href="./emnlp-2013-Appropriately_Incorporating_Statistical_Significance_in_PMI.html">25 emnlp-2013-Appropriately Incorporating Statistical Significance in PMI</a></p>
<p>11 0.46224368 <a title="118-lda-11" href="./emnlp-2013-Classifying_Message_Board_Posts_with_an_Extracted_Lexicon_of_Patient_Attributes.html">46 emnlp-2013-Classifying Message Board Posts with an Extracted Lexicon of Patient Attributes</a></p>
<p>12 0.46193874 <a title="118-lda-12" href="./emnlp-2013-Document_Summarization_via_Guided_Sentence_Compression.html">65 emnlp-2013-Document Summarization via Guided Sentence Compression</a></p>
<p>13 0.46140915 <a title="118-lda-13" href="./emnlp-2013-Automatic_Domain_Partitioning_for_Multi-Domain_Learning.html">29 emnlp-2013-Automatic Domain Partitioning for Multi-Domain Learning</a></p>
<p>14 0.45981282 <a title="118-lda-14" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>15 0.45935774 <a title="118-lda-15" href="./emnlp-2013-Leveraging_Lexical_Cohesion_and_Disruption_for_Topic_Segmentation.html">124 emnlp-2013-Leveraging Lexical Cohesion and Disruption for Topic Segmentation</a></p>
<p>16 0.45699754 <a title="118-lda-16" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>17 0.45684251 <a title="118-lda-17" href="./emnlp-2013-Deep_Learning_for_Chinese_Word_Segmentation_and_POS_Tagging.html">56 emnlp-2013-Deep Learning for Chinese Word Segmentation and POS Tagging</a></p>
<p>18 0.45678502 <a title="118-lda-18" href="./emnlp-2013-An_Empirical_Study_Of_Semi-Supervised_Chinese_Word_Segmentation_Using_Co-Training.html">21 emnlp-2013-An Empirical Study Of Semi-Supervised Chinese Word Segmentation Using Co-Training</a></p>
<p>19 0.45617643 <a title="118-lda-19" href="./emnlp-2013-Exploiting_Zero_Pronouns_to_Improve_Chinese_Coreference_Resolution.html">80 emnlp-2013-Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</a></p>
<p>20 0.45589542 <a title="118-lda-20" href="./emnlp-2013-Latent_Anaphora_Resolution_for_Cross-Lingual_Pronoun_Prediction.html">117 emnlp-2013-Latent Anaphora Resolution for Cross-Lingual Pronoun Prediction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
