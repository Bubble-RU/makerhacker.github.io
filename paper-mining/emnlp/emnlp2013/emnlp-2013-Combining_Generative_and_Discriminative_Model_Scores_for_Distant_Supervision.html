<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>49 emnlp-2013-Combining Generative and Discriminative Model Scores for Distant Supervision</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-49" href="#">emnlp2013-49</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>49 emnlp-2013-Combining Generative and Discriminative Model Scores for Distant Supervision</h1>
<br/><p>Source: <a title="emnlp-2013-49-pdf" href="http://aclweb.org/anthology//D/D13/D13-1003.pdf">pdf</a></p><p>Author: Benjamin Roth ; Dietrich Klakow</p><p>Abstract: Distant supervision is a scheme to generate noisy training data for relation extraction by aligning entities of a knowledge base with text. In this work we combine the output of a discriminative at-least-one learner with that of a generative hierarchical topic model to reduce the noise in distant supervision data. The combination significantly increases the ranking quality of extracted facts and achieves state-of-the-art extraction performance in an end-to-end setting. A simple linear interpolation of the model scores performs better than a parameter-free scheme based on nondominated sorting.</p><p>Reference: <a title="emnlp-2013-49-reference" href="../emnlp2013_reference/emnlp-2013-Combining_Generative_and_Discriminative_Model_Scores_for_Distant_Supervision_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Combining Generative and Discriminative Model Scores for Distant Supervision Benjamin Roth, Dietrich Klakow Saarland University Spoken Language Systems Saarbr¨ ucken, Germany  {ben j amin . [sent-1, score-0.03]
</p><p>2 Abstract Distant supervision is a scheme to generate noisy training data for relation extraction by aligning entities of a knowledge base with text. [sent-3, score-0.628]
</p><p>3 In this work we combine the output of a discriminative at-least-one learner with that of a generative hierarchical topic model to reduce the noise in distant supervision data. [sent-4, score-0.83]
</p><p>4 The combination significantly increases the ranking quality of extracted facts and achieves state-of-the-art extraction performance in an end-to-end setting. [sent-5, score-0.254]
</p><p>5 A simple linear interpolation of the model scores performs better than a parameter-free scheme based on nondominated sorting. [sent-6, score-0.258]
</p><p>6 1 Introduction Relation extraction is the task of finding relational  facts in unstructured text and putting them into a structured (tabularized) knowledge base. [sent-7, score-0.169]
</p><p>7 Training machine learning algorithms for relation extraction requires training data. [sent-8, score-0.208]
</p><p>8 Manual annotation of training data is laborious and costly, however, the knowledge base may already partially be filled with instances from the relations. [sent-10, score-0.06]
</p><p>9 This is utilized by a scheme known as distant supervision (DS) (Mintz et al. [sent-11, score-0.499]
</p><p>10 , 2009): text is automatically labeled by aligning (matching) pairs of entities that are contained in a knowledge base with their textual occurrences. [sent-12, score-0.134]
</p><p>11 , 2012): If the tuple place_o f_bi rth (Mi chae l Jacks on , Gary ) is contained in the knowledge base, one matching  context could be: Michael Jackson was born in Gary . [sent-18, score-0.131]
</p><p>12 Clearly, only the first context indeed expresses the relation and should be labeled accordingly. [sent-24, score-0.178]
</p><p>13 Three basic approaches have been proposed to deal with noisy distant supervision instances: The discriminative at-least-one approach (Riedel et al. [sent-25, score-0.571]
</p><p>14 , 2010), that requires that at least one of the matches for a relation-entity tuple indeed expresses the relation; The generative approach (Alfonseca et al. [sent-26, score-0.136]
</p><p>15 , 2012) that separates relation-specific distributions from noise distributions by using hierarchical topic models; And the pattern correlation approach (Takamatsu et al. [sent-27, score-0.412]
</p><p>16 , 2012) that assumes that contexts which match argument pairs have a high overlap in argument pairs with other patterns expressing the relation. [sent-28, score-0.316]
</p><p>17 We score  surface patterns and show that combining the two approaches results in a better ranking quality of relational facts. [sent-30, score-0.334]
</p><p>18 In an end-to-end evaluation we set a threshold on the pattern scores and apply the patProSce aedt ilne,gs W oafs th ieng 2t0o1n3, C USonAf,e1r e8n-c2e1 o Onc Etombpeir i 2c0a1l3 M. [sent-31, score-0.155]
</p><p>19 c th2o0d1s3 in A Nssaotcuiaratilo Lna fnogru Caogmep Purtoacteiosnsianlg L,i pnag ueis t 2ic4s–29,  Figure 1: Hierarchical topic models. [sent-33, score-0.088]
</p><p>20 Although the surface patterns are very simple (only strings of tokens), they achieve state-of-the-art extraction results. [sent-36, score-0.238]
</p><p>21 1 At-Least-One Models The original form of distant supervision (Mintz et al. [sent-38, score-0.45]
</p><p>22 , 2009) assumes all sentences containing an entity pair to be potential patterns for the relation holding between the entities. [sent-39, score-0.425]
</p><p>23 A variety of models relax this assumption and only presume that at least one of the entity pair occurrences is a textual manifestation of the relation. [sent-40, score-0.105]
</p><p>24 The first proposed model with an atleast-one learner is that of Riedel et al. [sent-41, score-0.059]
</p><p>25 It consists of a factor graph that includes binary variables for contexts, and groups contexts together for each entity pair. [sent-44, score-0.116]
</p><p>26 , 2011) can be viewed as a multi-label extension of (Riedel et al. [sent-46, score-0.047]
</p><p>27 2 Hierarchical Topic Model The hierarchical topic model (HierTopics) by Alfonseca et al. [sent-51, score-0.151]
</p><p>28 (2012) models the distant supervision data by a generative model. [sent-52, score-0.52]
</p><p>29 For each corpus match of an entity pair in the knowledge base, the corresponding surface pattern is assumed to be typical for either the entity pair, the relation, or neither. [sent-53, score-0.295]
</p><p>30 This principle is  then used to infer distributions over patterns of one of the following types: 1. [sent-54, score-0.214]
</p><p>31 The generative process assumes that for each argument pair in the knowledge base, all patterns are generated by first choosing a hidden variable z which can take on three values, B for background, R for relation and P for pair. [sent-60, score-0.508]
</p><p>32 Corresponding vocabulary distributions (φbg, φrel , φpair) for generating the context patterns are chosen according to the value of z. [sent-61, score-0.214]
</p><p>33 The Dirichlet-smoothed vocabulary distributions are shared on the respective levels. [sent-62, score-0.039]
</p><p>34 1 Generative Model We use a feature-based extension (Roth and Klakow,  2013) of Alfonseca et al. [sent-65, score-0.047]
</p><p>35 A variable x represents a choice of B, R or P for every pattern, i. [sent-68, score-0.036]
</p><p>36 Each feature is generated conditioned on a second variable z ∈ {B, R, P}, i. [sent-71, score-0.036]
</p><p>37 there are as many nvdari vaabrlieasb z f zor ∈ a pattern as t,h i. [sent-73, score-0.119]
</p><p>38 First, the hidden variable x is generated, then all z variables are generated for the corresponding features (see Figure 1). [sent-76, score-0.036]
</p><p>39 The values B, R or P of z depend on the corresponding x by a transition distribution:  P(Zi= z|Xj(i)= x) =(p1−sapms2aem,e, oift hze=r wi xse where features at indices iare mapped to the corresponding pattern indices by a function j(i); psame is set to . [sent-77, score-0.213]
</p><p>40 99 to enforce the correspondence between pattern and feature topics. [sent-78, score-0.119]
</p><p>41 2 Discriminative Model As a second feature-based model, we employ a per-  ceptron model that enforces constraints on the labels for patterns (Roth and Klakow, 2013). [sent-80, score-0.175]
</p><p>42 The model consists of log-linear factors for the set of relations 1The hyper-parameters used for the feature-based topic model are α = (1, 1, 1) and β = (. [sent-81, score-0.088]
</p><p>43 PRro absa wbieliltlie ass afo fra a orre lfaotrio tnhe r given a s (ennote renclaet pattern s are calculated by normaliPzing over log-linear factors defined as fr (s) = exp (Pi φi (s, r)θi), with φ(s, r) the feature vector for sePntence s and label assignment r, and θr the featureP weight vector. [sent-86, score-0.2]
</p><p>44 The learner is directed by the following semantics: First, for a sentence s that has a distant supervision match for relation r, relation r should have a higher probability than any other relation r0 ∈ R \ r. [sent-87, score-0.944]
</p><p>45 As extractions are expected to be noisy, high probabilities for NIL are enforced by a second constraint: NIL must have a higher probability than any relation r0 ∈ R \ r. [sent-88, score-0.187]
</p><p>46 Third, at least one DS sentence for an argument pair i rsd expected to express the corresponding relation r. [sent-89, score-0.227]
</p><p>47 For sentences s for an entity pair belonging to relation r, this can be written as the following constraints: ∀s,r0 : P(r|s) > P(r0|s) ∧ P(NIL|s) > P(r0|s) ∃s : P(r|s) > P(NIL|s) The violatio∃n o: fP any o>f t(hNe aLbo|sv)e constraints triggers a perceptron update. [sent-90, score-0.342]
</p><p>48 e (2012) paantd) aggregated over all pattern occurrences: For the topic model, the number of times the relation-specific topic has been sampled for a pattern is divided by n(pat), the  number of times the same pattern has been observed. [sent-94, score-0.533]
</p><p>49 Analogously for the perceptron, the number of times a pattern co-occurs with entity pairs for r is multiplied by the perceptron score and divided by n(pat). [sent-95, score-0.282]
</p><p>50 2: Score combination by non-dominated sorting: indicate patterns on the Pareto-frontier, which are highest. [sent-98, score-0.219]
</p><p>51 (5·Pn((rp|sa,tθ,r))+·PP((rN|sI,Lθ)|s,θ)) ·  •  The topic model and perceptron approaches are based on plausible yet fundamentally different principles of modeling noise without direct supervision. [sent-101, score-0.279]
</p><p>52 We use two schemes to obtain a combined ranking from the two model scores: The first is a ranking based on non-dominated sorting by successively computing the Pareto-frontier of the 2-dimensional  score vectors (Borzsony et al. [sent-104, score-0.252]
</p><p>53 , 2012) to combine the translation quality metrics BLEU, RIBES and NTER, each of which is based on different principles. [sent-110, score-0.084]
</p><p>54 In the context of machine translation it has been found to outperform a linear interpolation of the metrics and to be more stable to non-smooth metrics and noncomparable scalings. [sent-111, score-0.275]
</p><p>55 We compare non-dominated sorting with a simple linear interpolation with uniform weights. [sent-112, score-0.291]
</p><p>56 1 Ranking-Based Evaluation Evaluation is done on the ranking quality according to TAC KBP gold annotations (Ji et al. [sent-114, score-0.1]
</p><p>57 , 2010) of extracted facts from all TAC KBP queries from 2009-  2011 and the TAC KBP 2009-201 1 corpora. [sent-115, score-0.085]
</p><p>58 First, candidate sentences are retrieved in which the query entity and a second entity with the appropriate type are contained. [sent-116, score-0.142]
</p><p>59 Candidate sentences are then used to provide answer candidates if one of the extracted patterns matches. [sent-117, score-0.248]
</p><p>60 The answer candidates are ranked according to the score of the matching pattern. [sent-118, score-0.129]
</p><p>61 The basis for pattern extraction is the noisy DS training data of a top-3 ranked system in TAC KBP 2012 (Roth et al. [sent-119, score-0.288]
</p><p>62 The retrieval component of this system is used to obtain sentence and answer candidates (ranked according to their respective pattern scores). [sent-121, score-0.192]
</p><p>63 Evaluation results are reported as averages over per-relation results of the standard ranking metrics mean average precision (map), geometric map (gmap), precision at 5 and at 10 (p@5, p@10). [sent-122, score-0.118]
</p><p>64 The maximum-likelihood estimator (MLE) baseline scores patterns by the relative frequency they occur with a certain relation. [sent-123, score-0.211]
</p><p>65 The hierarchical topic (hier orig) as described in Alfonseca et al. [sent-124, score-0.151]
</p><p>66 (2012) increases the scores under most metrics, however the increase is only significant for p@5 and p@ 10. [sent-125, score-0.036]
</p><p>67 The feature-based extension of the topic model  (hier feat) has significantly better ranking quality. [sent-126, score-0.202]
</p><p>68 Slightly better scores are obtained by the at-leastone perceptron learner. [sent-127, score-0.128]
</p><p>69 It is interesting to see that the model combinations both by non-dominated sorting perc+hier (pareto) as well as uniform interpolation perc+hier (itpl) give a further increase in ranking 27  (paired t-test,  p  < 0. [sent-128, score-0.325]
</p><p>70 Figure 3 shows the Precision/Recall curves of the basic models and the linear interpolation. [sent-136, score-0.033]
</p><p>71 On the P/R curve, the linear interpolation is equal or better than the single methods on all recall levels. [sent-137, score-0.173]
</p><p>72 2 End-To-End Evaluation We evaluate the extraction quality of the induced perc+hier (itpl) patterns in an end-to-end setting. [sent-139, score-0.271]
</p><p>73 (2012) evaluation is done using a subset of queries from the TAC KBP 2010 and 2011evaluation. [sent-143, score-0.038]
</p><p>74 Only those answers are considered in scoring that are contained in a list of possible answers from their candidates (reducing the number of gold answers from 1601 to 576 and thereby considerably increasing the value of reported recall). [sent-145, score-0.303]
</p><p>75 For evaluating our patterns, we take the same queries for testing as Surdeanu et al. [sent-146, score-0.038]
</p><p>76 (2012) and take those sentences that contain query entities and slot filler candidates according to NEtags. [sent-150, score-0.121]
</p><p>77 We filter out all candidates that are not contained in the list of candidates considered in (Surdeanu et al. [sent-151, score-0.181]
</p><p>78 , 2012), and use the same reduced set of 576 gold answers as the key. [sent-152, score-0.065]
</p><p>79 3 on held-out development data and take all patterns with higher scores. [sent-154, score-0.175]
</p><p>80 Table 2 shows that results obtained with the induced  patterns compare well with state-of-the-art relation extraction systems. [sent-155, score-0.383]
</p><p>81 3 Illustration: Top-Ranked Patterns Figure 4 shows top-ranked patterns for per:title and org:top members employees, the two relations with most answers in the gold annotations. [sent-162, score-0.289]
</p><p>82 0 if the patterns occurs only with the relation in question this includes all cases where the pattern is only found once in the corpus. [sent-164, score-0.439]
</p><p>83 While this could be circumvented by frequency thresholding, we leave the long tail of the data as it is and let the algorithm deal with both frequent and infrequent patterns. [sent-165, score-0.035]
</p><p>84 One can see that while the maximum likelihood patterns contain some reasonable relational contexts, they are less prototypical and more prone to distant supervision errors. [sent-166, score-0.717]
</p><p>85 The patterns scored high –  by the proposed combination generalize better, variation at the top is achieved by re-combining  ele-  ments that carry relational meaning ( “is an ”, “vice president”,  “president director”) or are closely cor-  related to the particular relation. [sent-167, score-0.278]
</p><p>86 a feature-based  extension  model, and an at-least-one  of a hierarchical perceptron. [sent-170, score-0.11]
</p><p>87 topic  Interpola-  tion increases the quality of extractions and achieves state-of-the-art extraction performance. [sent-171, score-0.226]
</p><p>88 A combination scheme based on non-dominated  sorting, that  was inspired by work on combining machine translation metrics, was not as good as a simple linear combination of scores. [sent-172, score-0.17]
</p><p>89 We think that the good re-  sults motivate research into more integrated combinations of noise reduction approaches. [sent-173, score-0.064]
</p><p>90 Pattern learning for relation extraction with a hierarchical topic model. [sent-177, score-0.359]
</p><p>91 Knowledgebased weak supervision for information extraction of overlapping relations. [sent-198, score-0.285]
</p><p>92 Overview of the tac 2010 knowledge base population track. [sent-202, score-0.277]
</p><p>93 Featurebased models for improving the quality of noisy training data for relation extraction. [sent-216, score-0.228]
</p><p>94 Generalizing from freebase and patterns using distant supervi29 sion for slot filling. [sent-221, score-0.451]
</p><p>95 Reducing wrong labels in distant supervision for relation extraction. [sent-230, score-0.595]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hier', 0.313), ('distant', 0.228), ('supervision', 0.222), ('tac', 0.217), ('kbp', 0.212), ('perc', 0.208), ('nil', 0.176), ('patterns', 0.175), ('itpl', 0.174), ('mle', 0.165), ('relation', 0.145), ('interpolation', 0.14), ('pat', 0.138), ('roth', 0.134), ('alfonseca', 0.128), ('klakow', 0.121), ('pattern', 0.119), ('sorting', 0.118), ('takamatsu', 0.104), ('surdeanu', 0.1), ('perceptron', 0.092), ('gary', 0.091), ('topic', 0.088), ('president', 0.087), ('employees', 0.083), ('riedel', 0.081), ('dietrich', 0.077), ('mintz', 0.077), ('candidates', 0.073), ('rfo', 0.073), ('chairman', 0.073), ('director', 0.073), ('film', 0.073), ('entity', 0.071), ('generative', 0.07), ('borzsony', 0.069), ('hiertopics', 0.069), ('multir', 0.069), ('orig', 0.069), ('ranking', 0.067), ('answers', 0.065), ('noise', 0.064), ('born', 0.063), ('extraction', 0.063), ('hierarchical', 0.063), ('jackson', 0.06), ('mimlre', 0.06), ('base', 0.06), ('learner', 0.059), ('relational', 0.059), ('benjamin', 0.057), ('vice', 0.056), ('ranked', 0.056), ('godfrey', 0.055), ('joe', 0.055), ('yao', 0.053), ('ass', 0.051), ('metrics', 0.051), ('noisy', 0.05), ('ds', 0.05), ('members', 0.049), ('scheme', 0.049), ('slot', 0.048), ('argument', 0.048), ('extension', 0.047), ('org', 0.047), ('facts', 0.047), ('limin', 0.046), ('contexts', 0.045), ('duh', 0.044), ('combination', 0.044), ('title', 0.043), ('rp', 0.042), ('extractions', 0.042), ('hoffmann', 0.042), ('sv', 0.041), ('distributions', 0.039), ('aligning', 0.039), ('queries', 0.038), ('discriminative', 0.036), ('scores', 0.036), ('variable', 0.036), ('contained', 0.035), ('deal', 0.035), ('principles', 0.035), ('pair', 0.034), ('likelihood', 0.033), ('expresses', 0.033), ('tuple', 0.033), ('quality', 0.033), ('linear', 0.033), ('indices', 0.032), ('oift', 0.03), ('featurebased', 0.03), ('lieberman', 0.03), ('fra', 0.03), ('sketched', 0.03), ('ril', 0.03), ('adi', 0.03), ('amin', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="49-tfidf-1" href="./emnlp-2013-Combining_Generative_and_Discriminative_Model_Scores_for_Distant_Supervision.html">49 emnlp-2013-Combining Generative and Discriminative Model Scores for Distant Supervision</a></p>
<p>Author: Benjamin Roth ; Dietrich Klakow</p><p>Abstract: Distant supervision is a scheme to generate noisy training data for relation extraction by aligning entities of a knowledge base with text. In this work we combine the output of a discriminative at-least-one learner with that of a generative hierarchical topic model to reduce the noise in distant supervision data. The combination significantly increases the ranking quality of extracted facts and achieves state-of-the-art extraction performance in an end-to-end setting. A simple linear interpolation of the model scores performs better than a parameter-free scheme based on nondominated sorting.</p><p>2 0.18865259 <a title="49-tfidf-2" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>Author: Xiao Cheng ; Dan Roth</p><p>Abstract: Wikification, commonly referred to as Disambiguation to Wikipedia (D2W), is the task of identifying concepts and entities in text and disambiguating them into the most specific corresponding Wikipedia pages. Previous approaches to D2W focused on the use of local and global statistics over the given text, Wikipedia articles and its link structures, to evaluate context compatibility among a list of probable candidates. However, these methods fail (often, embarrassingly), when some level of text understanding is needed to support Wikification. In this paper we introduce a novel approach to Wikification by incorporating, along with statistical methods, richer relational analysis of the text. We provide an extensible, efficient and modular Integer Linear Programming (ILP) formulation of Wikification that incorporates the entity-relation inference problem, and show that the ability to identify relations in text helps both candi- date generation and ranking Wikipedia titles considerably. Our results show significant improvements in both Wikification and the TAC Entity Linking task.</p><p>3 0.14198449 <a title="49-tfidf-3" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>Author: Jason Weston ; Antoine Bordes ; Oksana Yakhnenko ; Nicolas Usunier</p><p>Abstract: This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over methods that rely on text features alone.</p><p>4 0.13473056 <a title="49-tfidf-4" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>Author: Oier Lopez de Lacalle ; Mirella Lapata</p><p>Abstract: In this paper we present an unsupervised approach to relational information extraction. Our model partitions tuples representing an observed syntactic relationship between two named entities (e.g., “X was born in Y” and “X is from Y”) into clusters corresponding to underlying semantic relation types (e.g., BornIn, Located). Our approach incorporates general domain knowledge which we encode as First Order Logic rules and automatically combine with a topic model developed specifically for the relation extraction task. Evaluation results on the ACE 2007 English Relation Detection and Categorization (RDC) task show that our model outperforms competitive unsupervised approaches by a wide margin and is able to produce clusters shaped by both the data and the rules.</p><p>5 0.1137825 <a title="49-tfidf-5" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>Author: Zhengyan He ; Shujie Liu ; Yang Song ; Mu Li ; Ming Zhou ; Houfeng Wang</p><p>Abstract: Entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base. Recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes. We propose a fast collective disambiguation approach based on stacking. First, we train a local predictor g0 with learning to rank as base learner, to generate initial ranking list of candidates. Second, top k candidates of related instances are searched for constructing expressive global coherence features. A global predictor g1 is trained in the augmented feature space and stacking is employed to tackle the train/test mismatch problem. The proposed method is fast and easy to implement. Experiments show its effectiveness over various algorithms on several public datasets. By learning a rich semantic relatedness measure be- . tween entity categories and context document, performance is further improved.</p><p>6 0.083874151 <a title="49-tfidf-6" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>7 0.083318949 <a title="49-tfidf-7" href="./emnlp-2013-Automatic_Feature_Engineering_for_Answer_Selection_and_Extraction.html">31 emnlp-2013-Automatic Feature Engineering for Answer Selection and Extraction</a></p>
<p>8 0.075420365 <a title="49-tfidf-8" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>9 0.074709862 <a title="49-tfidf-9" href="./emnlp-2013-Effectiveness_and_Efficiency_of_Open_Relation_Extraction.html">68 emnlp-2013-Effectiveness and Efficiency of Open Relation Extraction</a></p>
<p>10 0.071293503 <a title="49-tfidf-10" href="./emnlp-2013-Semantic_Parsing_on_Freebase_from_Question-Answer_Pairs.html">166 emnlp-2013-Semantic Parsing on Freebase from Question-Answer Pairs</a></p>
<p>11 0.070122227 <a title="49-tfidf-11" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>12 0.069488272 <a title="49-tfidf-12" href="./emnlp-2013-Event_Schema_Induction_with_a_Probabilistic_Entity-Driven_Model.html">75 emnlp-2013-Event Schema Induction with a Probabilistic Entity-Driven Model</a></p>
<p>13 0.069291696 <a title="49-tfidf-13" href="./emnlp-2013-Two-Stage_Method_for_Large-Scale_Acquisition_of_Contradiction_Pattern_Pairs_using_Entailment.html">189 emnlp-2013-Two-Stage Method for Large-Scale Acquisition of Contradiction Pattern Pairs using Entailment</a></p>
<p>14 0.063515253 <a title="49-tfidf-14" href="./emnlp-2013-Online_Learning_for_Inexact_Hypergraph_Search.html">141 emnlp-2013-Online Learning for Inexact Hypergraph Search</a></p>
<p>15 0.061980616 <a title="49-tfidf-15" href="./emnlp-2013-Improvements_to_the_Bayesian_Topic_N-Gram_Models.html">100 emnlp-2013-Improvements to the Bayesian Topic N-Gram Models</a></p>
<p>16 0.060909286 <a title="49-tfidf-16" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>17 0.058114506 <a title="49-tfidf-17" href="./emnlp-2013-Exploiting_Domain_Knowledge_in_Aspect_Extraction.html">77 emnlp-2013-Exploiting Domain Knowledge in Aspect Extraction</a></p>
<p>18 0.057262387 <a title="49-tfidf-18" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>19 0.05565457 <a title="49-tfidf-19" href="./emnlp-2013-Document_Summarization_via_Guided_Sentence_Compression.html">65 emnlp-2013-Document Summarization via Guided Sentence Compression</a></p>
<p>20 0.052993637 <a title="49-tfidf-20" href="./emnlp-2013-Unsupervised_Induction_of_Contingent_Event_Pairs_from_Film_Scenes.html">192 emnlp-2013-Unsupervised Induction of Contingent Event Pairs from Film Scenes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.211), (1, 0.083), (2, 0.042), (3, 0.073), (4, -0.005), (5, 0.063), (6, 0.01), (7, 0.122), (8, 0.131), (9, -0.019), (10, 0.1), (11, -0.047), (12, -0.117), (13, 0.065), (14, -0.101), (15, -0.061), (16, 0.064), (17, 0.067), (18, 0.116), (19, 0.091), (20, -0.065), (21, 0.096), (22, -0.01), (23, 0.045), (24, -0.073), (25, 0.051), (26, 0.056), (27, -0.024), (28, -0.077), (29, 0.101), (30, -0.06), (31, 0.015), (32, 0.049), (33, 0.065), (34, 0.07), (35, 0.071), (36, -0.034), (37, -0.048), (38, 0.077), (39, 0.052), (40, -0.079), (41, 0.068), (42, -0.086), (43, 0.189), (44, -0.034), (45, 0.014), (46, 0.023), (47, 0.02), (48, 0.073), (49, -0.146)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94903308 <a title="49-lsi-1" href="./emnlp-2013-Combining_Generative_and_Discriminative_Model_Scores_for_Distant_Supervision.html">49 emnlp-2013-Combining Generative and Discriminative Model Scores for Distant Supervision</a></p>
<p>Author: Benjamin Roth ; Dietrich Klakow</p><p>Abstract: Distant supervision is a scheme to generate noisy training data for relation extraction by aligning entities of a knowledge base with text. In this work we combine the output of a discriminative at-least-one learner with that of a generative hierarchical topic model to reduce the noise in distant supervision data. The combination significantly increases the ranking quality of extracted facts and achieves state-of-the-art extraction performance in an end-to-end setting. A simple linear interpolation of the model scores performs better than a parameter-free scheme based on nondominated sorting.</p><p>2 0.72289413 <a title="49-lsi-2" href="./emnlp-2013-Relational_Inference_for_Wikification.html">160 emnlp-2013-Relational Inference for Wikification</a></p>
<p>Author: Xiao Cheng ; Dan Roth</p><p>Abstract: Wikification, commonly referred to as Disambiguation to Wikipedia (D2W), is the task of identifying concepts and entities in text and disambiguating them into the most specific corresponding Wikipedia pages. Previous approaches to D2W focused on the use of local and global statistics over the given text, Wikipedia articles and its link structures, to evaluate context compatibility among a list of probable candidates. However, these methods fail (often, embarrassingly), when some level of text understanding is needed to support Wikification. In this paper we introduce a novel approach to Wikification by incorporating, along with statistical methods, richer relational analysis of the text. We provide an extensible, efficient and modular Integer Linear Programming (ILP) formulation of Wikification that incorporates the entity-relation inference problem, and show that the ability to identify relations in text helps both candi- date generation and ranking Wikipedia titles considerably. Our results show significant improvements in both Wikification and the TAC Entity Linking task.</p><p>3 0.65363497 <a title="49-lsi-3" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>Author: Jason Weston ; Antoine Bordes ; Oksana Yakhnenko ; Nicolas Usunier</p><p>Abstract: This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over methods that rely on text features alone.</p><p>4 0.62121528 <a title="49-lsi-4" href="./emnlp-2013-Unsupervised_Relation_Extraction_with_General_Domain_Knowledge.html">194 emnlp-2013-Unsupervised Relation Extraction with General Domain Knowledge</a></p>
<p>Author: Oier Lopez de Lacalle ; Mirella Lapata</p><p>Abstract: In this paper we present an unsupervised approach to relational information extraction. Our model partitions tuples representing an observed syntactic relationship between two named entities (e.g., “X was born in Y” and “X is from Y”) into clusters corresponding to underlying semantic relation types (e.g., BornIn, Located). Our approach incorporates general domain knowledge which we encode as First Order Logic rules and automatically combine with a topic model developed specifically for the relation extraction task. Evaluation results on the ACE 2007 English Relation Detection and Categorization (RDC) task show that our model outperforms competitive unsupervised approaches by a wide margin and is able to produce clusters shaped by both the data and the rules.</p><p>5 0.61422688 <a title="49-lsi-5" href="./emnlp-2013-Effectiveness_and_Efficiency_of_Open_Relation_Extraction.html">68 emnlp-2013-Effectiveness and Efficiency of Open Relation Extraction</a></p>
<p>Author: Filipe Mesquita ; Jordan Schmidek ; Denilson Barbosa</p><p>Abstract: A large number of Open Relation Extraction approaches have been proposed recently, covering a wide range of NLP machinery, from “shallow” (e.g., part-of-speech tagging) to “deep” (e.g., semantic role labeling–SRL). A natural question then is what is the tradeoff between NLP depth (and associated computational cost) versus effectiveness. This paper presents a fair and objective experimental comparison of 8 state-of-the-art approaches over 5 different datasets, and sheds some light on the issue. The paper also describes a novel method, EXEMPLAR, which adapts ideas from SRL to less costly NLP machinery, resulting in substantial gains both in efficiency and effectiveness, over binary and n-ary relation extraction tasks.</p><p>6 0.60210711 <a title="49-lsi-6" href="./emnlp-2013-Two-Stage_Method_for_Large-Scale_Acquisition_of_Contradiction_Pattern_Pairs_using_Entailment.html">189 emnlp-2013-Two-Stage Method for Large-Scale Acquisition of Contradiction Pattern Pairs using Entailment</a></p>
<p>7 0.55642438 <a title="49-lsi-7" href="./emnlp-2013-Efficient_Collective_Entity_Linking_with_Stacking.html">69 emnlp-2013-Efficient Collective Entity Linking with Stacking</a></p>
<p>8 0.49747324 <a title="49-lsi-8" href="./emnlp-2013-Predicting_the_Presence_of_Discourse_Connectives.html">152 emnlp-2013-Predicting the Presence of Discourse Connectives</a></p>
<p>9 0.48942459 <a title="49-lsi-9" href="./emnlp-2013-Exploiting_Multiple_Sources_for_Open-Domain_Hypernym_Discovery.html">79 emnlp-2013-Exploiting Multiple Sources for Open-Domain Hypernym Discovery</a></p>
<p>10 0.46244472 <a title="49-lsi-10" href="./emnlp-2013-Harvesting_Parallel_News_Streams_to_Generate_Paraphrases_of_Event_Relations.html">93 emnlp-2013-Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</a></p>
<p>11 0.4620375 <a title="49-lsi-11" href="./emnlp-2013-Joint_Bootstrapping_of_Corpus_Annotations_and_Entity_Types.html">110 emnlp-2013-Joint Bootstrapping of Corpus Annotations and Entity Types</a></p>
<p>12 0.42845619 <a title="49-lsi-12" href="./emnlp-2013-Joint_Learning_and_Inference_for_Grammatical_Error_Correction.html">114 emnlp-2013-Joint Learning and Inference for Grammatical Error Correction</a></p>
<p>13 0.37135249 <a title="49-lsi-13" href="./emnlp-2013-Interpreting_Anaphoric_Shell_Nouns_using_Antecedents_of_Cataphoric_Shell_Nouns_as_Training_Data.html">108 emnlp-2013-Interpreting Anaphoric Shell Nouns using Antecedents of Cataphoric Shell Nouns as Training Data</a></p>
<p>14 0.36392614 <a title="49-lsi-14" href="./emnlp-2013-A_Hierarchical_Entity-Based_Approach_to_Structuralize_User_Generated_Content_in_Social_Media%3A_A_Case_of_Yahoo%21_Answers.html">7 emnlp-2013-A Hierarchical Entity-Based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</a></p>
<p>15 0.3542237 <a title="49-lsi-15" href="./emnlp-2013-Using_Soft_Constraints_in_Joint_Inference_for_Clinical_Concept_Recognition.html">198 emnlp-2013-Using Soft Constraints in Joint Inference for Clinical Concept Recognition</a></p>
<p>16 0.34257537 <a title="49-lsi-16" href="./emnlp-2013-Improvements_to_the_Bayesian_Topic_N-Gram_Models.html">100 emnlp-2013-Improvements to the Bayesian Topic N-Gram Models</a></p>
<p>17 0.33379596 <a title="49-lsi-17" href="./emnlp-2013-Automatic_Feature_Engineering_for_Answer_Selection_and_Extraction.html">31 emnlp-2013-Automatic Feature Engineering for Answer Selection and Extraction</a></p>
<p>18 0.33249366 <a title="49-lsi-18" href="./emnlp-2013-Implicit_Feature_Detection_via_a_Constrained_Topic_Model_and_SVM.html">99 emnlp-2013-Implicit Feature Detection via a Constrained Topic Model and SVM</a></p>
<p>19 0.32359621 <a title="49-lsi-19" href="./emnlp-2013-Improving_Learning_and_Inference_in_a_Large_Knowledge-Base_using_Latent_Syntactic_Cues.html">102 emnlp-2013-Improving Learning and Inference in a Large Knowledge-Base using Latent Syntactic Cues</a></p>
<p>20 0.31713468 <a title="49-lsi-20" href="./emnlp-2013-Naive_Bayes_Word_Sense_Induction.html">138 emnlp-2013-Naive Bayes Word Sense Induction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.018), (9, 0.011), (18, 0.563), (22, 0.025), (26, 0.011), (30, 0.048), (36, 0.013), (51, 0.137), (66, 0.024), (71, 0.011), (75, 0.034), (77, 0.011), (96, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8872239 <a title="49-lda-1" href="./emnlp-2013-Combining_Generative_and_Discriminative_Model_Scores_for_Distant_Supervision.html">49 emnlp-2013-Combining Generative and Discriminative Model Scores for Distant Supervision</a></p>
<p>Author: Benjamin Roth ; Dietrich Klakow</p><p>Abstract: Distant supervision is a scheme to generate noisy training data for relation extraction by aligning entities of a knowledge base with text. In this work we combine the output of a discriminative at-least-one learner with that of a generative hierarchical topic model to reduce the noise in distant supervision data. The combination significantly increases the ranking quality of extracted facts and achieves state-of-the-art extraction performance in an end-to-end setting. A simple linear interpolation of the model scores performs better than a parameter-free scheme based on nondominated sorting.</p><p>2 0.86424929 <a title="49-lda-2" href="./emnlp-2013-Semi-Supervised_Representation_Learning_for_Cross-Lingual_Text_Classification.html">169 emnlp-2013-Semi-Supervised Representation Learning for Cross-Lingual Text Classification</a></p>
<p>Author: Min Xiao ; Yuhong Guo</p><p>Abstract: Cross-lingual adaptation aims to learn a prediction model in a label-scarce target language by exploiting labeled data from a labelrich source language. An effective crosslingual adaptation system can substantially reduce the manual annotation effort required in many natural language processing tasks. In this paper, we propose a new cross-lingual adaptation approach for document classification based on learning cross-lingual discriminative distributed representations of words. Specifically, we propose to maximize the loglikelihood of the documents from both language domains under a cross-lingual logbilinear document model, while minimizing the prediction log-losses of labeled documents. We conduct extensive experiments on cross-lingual sentiment classification tasks of Amazon product reviews. Our experimental results demonstrate the efficacy of the pro- posed cross-lingual adaptation approach.</p><p>3 0.84835607 <a title="49-lda-3" href="./emnlp-2013-Efficient_Left-to-Right_Hierarchical_Phrase-Based_Translation_with_Improved_Reordering.html">71 emnlp-2013-Efficient Left-to-Right Hierarchical Phrase-Based Translation with Improved Reordering</a></p>
<p>Author: Maryam Siahbani ; Baskaran Sankaran ; Anoop Sarkar</p><p>Abstract: Left-to-right (LR) decoding (Watanabe et al., 2006b) is a promising decoding algorithm for hierarchical phrase-based translation (Hiero). It generates the target sentence by extending the hypotheses only on the right edge. LR decoding has complexity O(n2b) for input of n words and beam size b, compared to O(n3) for the CKY algorithm. It requires a single language model (LM) history for each target hypothesis rather than two LM histories per hypothesis as in CKY. In this paper we present an augmented LR decoding algorithm that builds on the original algorithm in (Watanabe et al., 2006b). Unlike that algorithm, using experiments over multiple language pairs we show two new results: our LR decoding algorithm provides demonstrably more efficient decoding than CKY Hiero, four times faster; and by introducing new distortion and reordering features for LR decoding, it maintains the same translation quality (as in BLEU scores) ob- tained phrase-based and CKY Hiero with the same translation model.</p><p>4 0.65415221 <a title="49-lda-4" href="./emnlp-2013-Learning_Latent_Word_Representations_for_Domain_Adaptation_using_Supervised_Word_Clustering.html">120 emnlp-2013-Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering</a></p>
<p>Author: Min Xiao ; Feipeng Zhao ; Yuhong Guo</p><p>Abstract: Domain adaptation has been popularly studied on exploiting labeled information from a source domain to learn a prediction model in a target domain. In this paper, we develop a novel representation learning approach to address domain adaptation for text classification with automatically induced discriminative latent features, which are generalizable across domains while informative to the prediction task. Specifically, we propose a hierarchical multinomial Naive Bayes model with latent variables to conduct supervised word clustering on labeled documents from both source and target domains, and then use the produced cluster distribution of each word as its latent feature representation for domain adaptation. We train this latent graphical model us- ing a simple expectation-maximization (EM) algorithm. We empirically evaluate the proposed method with both cross-domain document categorization tasks on Reuters-21578 dataset and cross-domain sentiment classification tasks on Amazon product review dataset. The experimental results demonstrate that our proposed approach achieves superior performance compared with alternative methods.</p><p>5 0.58652848 <a title="49-lda-5" href="./emnlp-2013-Scaling_Semantic_Parsers_with_On-the-Fly_Ontology_Matching.html">164 emnlp-2013-Scaling Semantic Parsers with On-the-Fly Ontology Matching</a></p>
<p>Author: Tom Kwiatkowski ; Eunsol Choi ; Yoav Artzi ; Luke Zettlemoyer</p><p>Abstract: We consider the challenge of learning semantic parsers that scale to large, open-domain problems, such as question answering with Freebase. In such settings, the sentences cover a wide variety of topics and include many phrases whose meaning is difficult to represent in a fixed target ontology. For example, even simple phrases such as ‘daughter’ and ‘number of people living in’ cannot be directly represented in Freebase, whose ontology instead encodes facts about gender, parenthood, and population. In this paper, we introduce a new semantic parsing approach that learns to resolve such ontological mismatches. The parser is learned from question-answer pairs, uses a probabilistic CCG to build linguistically motivated logicalform meaning representations, and includes an ontology matching model that adapts the output logical forms for each target ontology. Experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus.</p><p>6 0.48411289 <a title="49-lda-6" href="./emnlp-2013-Connecting_Language_and_Knowledge_Bases_with_Embedding_Models_for_Relation_Extraction.html">51 emnlp-2013-Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</a></p>
<p>7 0.47459462 <a title="49-lda-7" href="./emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</a></p>
<p>8 0.4715777 <a title="49-lda-8" href="./emnlp-2013-Discriminative_Improvements_to_Distributional_Sentence_Similarity.html">64 emnlp-2013-Discriminative Improvements to Distributional Sentence Similarity</a></p>
<p>9 0.44262469 <a title="49-lda-9" href="./emnlp-2013-Noise-Aware_Character_Alignment_for_Bootstrapping_Statistical_Machine_Transliteration_from_Bilingual_Corpora.html">139 emnlp-2013-Noise-Aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora</a></p>
<p>10 0.44150984 <a title="49-lda-10" href="./emnlp-2013-Two-Stage_Method_for_Large-Scale_Acquisition_of_Contradiction_Pattern_Pairs_using_Entailment.html">189 emnlp-2013-Two-Stage Method for Large-Scale Acquisition of Contradiction Pattern Pairs using Entailment</a></p>
<p>11 0.43952611 <a title="49-lda-11" href="./emnlp-2013-Simple_Customization_of_Recursive_Neural_Networks_for_Semantic_Relation_Classification.html">172 emnlp-2013-Simple Customization of Recursive Neural Networks for Semantic Relation Classification</a></p>
<p>12 0.43932739 <a title="49-lda-12" href="./emnlp-2013-Word_Level_Language_Identification_in_Online_Multilingual_Communication.html">204 emnlp-2013-Word Level Language Identification in Online Multilingual Communication</a></p>
<p>13 0.43265218 <a title="49-lda-13" href="./emnlp-2013-Flexible_and_Efficient_Hypergraph_Interactions_for_Joint_Hierarchical_and_Forest-to-String_Decoding.html">88 emnlp-2013-Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest-to-String Decoding</a></p>
<p>14 0.43008801 <a title="49-lda-14" href="./emnlp-2013-Recursive_Autoencoders_for_ITG-Based_Translation.html">157 emnlp-2013-Recursive Autoencoders for ITG-Based Translation</a></p>
<p>15 0.42562959 <a title="49-lda-15" href="./emnlp-2013-A_Synchronous_Context_Free_Grammar_for_Time_Normalization.html">14 emnlp-2013-A Synchronous Context Free Grammar for Time Normalization</a></p>
<p>16 0.419128 <a title="49-lda-16" href="./emnlp-2013-Exploring_Demographic_Language_Variations_to_Improve_Multilingual_Sentiment_Analysis_in_Social_Media.html">81 emnlp-2013-Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media</a></p>
<p>17 0.41853261 <a title="49-lda-17" href="./emnlp-2013-Cross-Lingual_Discriminative_Learning_of_Sequence_Models_with_Posterior_Regularization.html">53 emnlp-2013-Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization</a></p>
<p>18 0.41847637 <a title="49-lda-18" href="./emnlp-2013-Max-Violation_Perceptron_and_Forced_Decoding_for_Scalable_MT_Training.html">128 emnlp-2013-Max-Violation Perceptron and Forced Decoding for Scalable MT Training</a></p>
<p>19 0.41714817 <a title="49-lda-19" href="./emnlp-2013-Converting_Continuous-Space_Language_Models_into_N-Gram_Language_Models_for_Statistical_Machine_Translation.html">52 emnlp-2013-Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation</a></p>
<p>20 0.41500467 <a title="49-lda-20" href="./emnlp-2013-Exploring_Representations_from_Unlabeled_Data_with_Co-training_for_Chinese_Word_Segmentation.html">82 emnlp-2013-Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
