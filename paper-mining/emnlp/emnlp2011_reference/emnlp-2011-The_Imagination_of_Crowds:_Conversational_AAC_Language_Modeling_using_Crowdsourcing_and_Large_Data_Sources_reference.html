<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>133 emnlp-2011-The Imagination of Crowds: Conversational AAC Language Modeling using Crowdsourcing and Large Data Sources</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-133" href="../emnlp2011/emnlp-2011-The_Imagination_of_Crowds%3A_Conversational_AAC_Language_Modeling_using_Crowdsourcing_and_Large_Data_Sources.html">emnlp2011-133</a> <a title="emnlp-2011-133-reference" href="#">emnlp2011-133-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>133 emnlp-2011-The Imagination of Crowds: Conversational AAC Language Modeling using Crowdsourcing and Large Data Sources</h1>
<br/><p>Source: <a title="emnlp-2011-133-pdf" href="http://aclweb.org/anthology//D/D11/D11-1065.pdf">pdf</a></p><p>Author: Keith Vertanen ; Per Ola Kristensson</p><p>Abstract: Augmented and alternative communication (AAC) devices enable users with certain communication disabilities to participate in everyday conversations. Such devices often rely on statistical language models to improve text entry by offering word predictions. These predictions can be improved if the language model is trained on data that closely reflects the style of the users’ intended communications. Unfortunately, there is no large dataset consisting of genuine AAC messages. In this paper we demonstrate how we can crowdsource the creation of a large set of fictional AAC messages. We show that these messages model conversational AAC better than the currently used datasets based on telephone conversations or newswire text. We leverage our crowdsourced messages to intelligently select sentences from much larger sets of Twitter, blog and Usenet data. Compared to a model trained only on telephone transcripts, our best performing model reduced perplexity on three test sets of AAC-like communications by 60– 82% relative. This translated to a potential keystroke savings in a predictive keyboard interface of 5–1 1%.</p><br/>
<h2>reference text</h2><p>Bruce Baker, Katya Hill, and Richard Devylder. 2000. Core vocabulary is the same across environments. In California State University at Northridge Conference. David R. Beukelman, Kathryn M. Yorkston, Miguel Poblete, and Carlos Naranjo. 1984. Frequency of word occurrence in communication samples produced by adult communication aid users. Journal of Speech  and Hearing Disorders, 49:360–367. Tamara Broderick and David J. C. MacKay. 2009. Fast and flexible selection with a single switch. PLoS ONE, 4(10):e7481. Ivan Bulyko, Mari Ostendorf, Manhung Siu, Tim Ng, Andreas Stolcke, and O¨zg u¨r C ¸etin. 2007. Web resources for language modeling in conversational speech recognition. ACM Transactions on Speech and Language Processing, 5(1): 1–25. Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The ICWSM 2009 Spinn3r dataset. In Proceedings of the 3rd Annual Conference on Weblogs and Social Media. Ciprian Chelba, Thorsten Brants, Will Neveitt, and Peng Xu. 2010. Study on interaction between entropy pruning and Kneser-Ney smoothing. In Proceedings of the International Conference on Spoken Language Processing, pages 2422–2425. Stanley F. Chen and Joshua T. Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical report, Computer Science Group, Harvard University. Jianfeng Gao, Joshua Goodman, Mingjing Li, and KaiFu Lee. 2002. Toward a unified approach to statistical language modeling for chinese. ACM Transactions on Asian Language Information Processing, 1:3–33. Nestor Garay-Vitoria and Julio Abascal. 2006. Text prediction systems: A survey. Universal Access in the Information Society, 4: 188–203.  Nestor Garay-Vitoria and Julio Gonz a´lez-Abascal. 1997. Intelligent word-prediction to enhance text input rate. In Proceedings of the 2nd ACM International Conference on Intelligent User Interfaces, pages 241–244. J.J. Godfrey, E.C. Holliman, and J. McDaniel. 1992. SWITCHBOARD: Telephone speech corpus for research and development. Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing, pages 5 17–520. David Graff, Roni Rosenfeld, and Doug Pau. 1995. CSR-III text. Linguistic Data Consortium, Philadelphia, PA, USA. David Graff. 2003. English gigaword corpus. Linguistic Data Consortium, Philadelphia, PA, USA. Sheri Hunnicutt. 1989. Using syntactic and semantic information in a word prediction aid. In Proceedings of the 1st European Conference on Speech Communication and Technology, pages 1191–1 193. Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing, pages 181–184. 710 Gregory W. Lesher and Gerard J. Rinkus. 2002. Domain-specific word prediction for augmentative communication. In Proceedings of the RESNA 2002 Annual Conference. Jianhua Li and Graeme Hirst. 2005. Semantic knowl-  edge in word completion. In Proceedings of the 7th International ACM SIGACCESS Conference on Computers and Accessibility, pages 121–128. Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien, KerJiann Chen, and Lin-Shan Lee. 1997. Chinese language model adaptation based on document classification and multiple domain-specific language models. In Proceedings of the 5th European Conference on Speech Communication and Technology, pages 1463– 1466. Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the 48th Annual Meeting of the Association of Computational Linguistics, pages 220–224. Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of twitter conversations. In Proceedings of HLT-NAACL 2010, pages 172–180. Cyrus Shaoul and Chris Westbury. 2009. A USENET corpus (2005-2009). University of Alberta, Canada. Andreas Stolcke. 1998. Entropy-based pruning of backoff language models. In Proceedings of DARPA Broadcast News Transcription and Understanding Workshop, pages 270–274. Andreas Stolcke. 2002. SRILM an extensible language modeling toolkit. In Proceedings of the 7th Annual International Conference on Spoken Language Processing, pages 901–904. –  Keith Trnka, Debra Yarrington, and Christopher Pennington. 2006. Topic modeling in fringe word prediction for AAC. In Proceedings of the 11th ACM International Conference on Intelligent UserInterfaces, pages 276–278. Keith Trnka, John McCaw, Debra Yarrington, Kathleen F. McCoy, and Christopher Pennington. 2009. User interaction with word prediction: The effects of prediction quality. ACM Transactions on Accessible Computing, 1: 17: 1–17:34. Keith Trnka. 2008. Adaptive language modeling for word prediction. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Student Research Workshop, pages 61–66. Horabail Venkatagiri. 1999. Efficient keyboard layouts for sequential access in augmentative and alternative communication. Augmentative and Alternative Communication, 15(2): 126–134. Tonio Wandmacher and Jean-Yves Antoine. 2007. Methods to integrate a language model with semantic information for a word prediction component. Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 506–5 13. Tonio Wandmacher, Jean-Yves Antoine, Franck Poirier, and Jean-Paul D ´eparte. 2008. SIBYLLE, an assistive communication system adapting to the context and its user. ACM Transactions on Accessible Computing, 1:6: 1–6:30. D. J. Ward and D. J. C. MacKay. 2002. Fast hands-free writing by gaze direction. Nature, 418(6900):838. 711</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
