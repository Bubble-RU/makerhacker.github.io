<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>5 emnlp-2011-A Fast Re-scoring Strategy to Capture Long-Distance Dependencies</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-5" href="../emnlp2011/emnlp-2011-A_Fast_Re-scoring_Strategy_to_Capture_Long-Distance_Dependencies.html">emnlp2011-5</a> <a title="emnlp-2011-5-reference" href="#">emnlp2011-5-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>5 emnlp-2011-A Fast Re-scoring Strategy to Capture Long-Distance Dependencies</h1>
<br/><p>Source: <a title="emnlp-2011-5-pdf" href="http://aclweb.org/anthology//D/D11/D11-1103.pdf">pdf</a></p><p>Author: Anoop Deoras ; Tomas Mikolov ; Kenneth Church</p><p>Abstract: A re-scoring strategy is proposed that makes it feasible to capture more long-distance dependencies in the natural language. Two pass strategies have become popular in a number of recognition tasks such as ASR (automatic speech recognition), MT (machine translation) and OCR (optical character recognition). The first pass typically applies a weak language model (n-grams) to a lattice and the second pass applies a stronger language model to N best lists. The stronger language model is intended to capture more longdistance dependencies. The proposed method uses RNN-LM (recurrent neural network language model), which is a long span LM, to rescore word lattices in the second pass. A hill climbing method (iterative decoding) is proposed to search over islands of confusability in the word lattice. An evaluation based on Broadcast News shows speedups of 20 over basic N best re-scoring, and word error rate reduction of 8% (relative) on a highly competitive setup.</p><br/>
<h2>reference text</h2><p>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. OpenFst: A General and Efficient Weighted Finite-State Transducer Library. In Proceedings of the Ninth International Conference on Implementation and Application of Automata, (CIAA 2007), volume 4783 of Lecture Notes in Computer Science, pages 11–23. Springer. J. R. Bellegarda. 2000. Exploiting latent semantic information in statistical language modeling. Proceedings of IEEE, 88(8): 1279–1296. Yoshua Bengio, R ´ejean Ducharme, and Pascal Vincent. 2001. A Neural Probabilistic Language Model. In Proceedings of Advances in Neural Information Processing Systems. Peter Beyerlein. 1998. Discriminative Model Combination. In Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). Christopher M. Bishop. 2006. Pattern Recognition and  Machine Learning. Springer. Ciprian Chelba and Frederick Jelinek. 2000. Structured Language Modeling. Computer Speech andLanguage, 14(4):283–332. S. F. Chen, L. Mangu, B. Ramabhadran, R. Sarikaya, and A. Sethy. 2009. Scaling shrinkage-based language models. In Proc. of IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), pages 299–304. Noam Chomsky. 1957. Syntactic Structures. The Hague: Mouton. Yen-Lu Chow and Richard Schwartz. 1989. The N-Best algorithm: an efficient procedure for finding top N sentence hypotheses. In Proceedings of the workshop on Speech and Natural Language, HLT ’89, pages 199– 202, Stroudsburg, PA, USA. Association for Computational Linguistics. Kenneth Church. 2012. A Pendulum Swung Too Far. Linguistic Issues in Language Technology - LiLT. to appear. T.M. Cover and J.A.Thomas. 1991. Elements of Information Theory. John Wiley and Sons, Inc. N.Y. Anoop Deoras and Frederick Jelinek. 2009. Iterative Decoding: A Novel Re-Scoring Framework for Confusion Networks. In Proc. of IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), pages 282 –286. Anoop Deoras, Denis Filimonov, Mary Harper, and Fred  Jelinek. 2010. Model Combination for Speech Recognition using Empirical Bayes Risk Minimization. In Proc. of IEEE Workshop on Spoken Language Technology (SLT). 1126 Anoop Deoras, Tom a´ˇ s Mikolov, Stefan Kombrink, Martin Karafi´ at, and Sanjeev Khudanpur. 2011. Variational Approximation of Long-Span Language Models for LVCSR. In Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). Jeffery Elman. 1990. Finding Structure in Time. In Cognitive Science, volume 14, pages 179–21 1. Denis Filimonov and Mary Harper. 2009. A Joint Language Model with Fine-grain Syntactic Tags. In Proc. of 2009 Conference on Empirical Methods in Natural Language Processing. V. Goel and W. Byrne. 2000. Minimum Bayes Risk Automatic Speech Recognition. Computer, Speech and Language. Rukmini Iyer and Mari Ostendorf. 1999. Modeling Long Distance Dependence in Language: Topic Mixtures Versus Dynamic Cache Models. IEEE Transactions on Speech and Audio Processing, 7(1):30–39. Zhifei Li and Jason Eisner. 2009. First- and second-order expectation semirings with applications to minimum-  risk training on translation forests. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 40–5 1, Singapore, August. Lidia Luminita Mangu. 2000. Finding consensus in speech recognition. Ph.D. thesis, The Johns Hopkins University. Adviser-Brill, Eric. Tom a´ˇ s Mikolov, Martin Karafi´ at, Luk a´ˇ s Burget, Jan “Honza” Cˇernock y´, and Sanjeev Khudanpur. 2010. Recurrent Neural Network Based Language Model. In Proc. of the ICSLP-Interspeech. Tom a´ˇ s Mikolov, Anoop Deoras, Stefan Kombrink, Luk a´ˇ s Burget, and Jan “Honza” Cˇernock y´. 2011a. Empirical Evaluation and Combination of Advanced Language Modeling Techniques. In Proc. of Interspeech. Tom a´ˇ s Mikolov, Stefan Kombrink, Luk a´ˇ s Burget, Jan “Honza” Cˇernock y´, and Sanjeev Khudanpur. 2011b. Extensions of Recurrent Neural Network Language Model. In Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). Mehryar Mohri and Michael Riley. 2002. An Efficient Algorithm for the N-Best-Strings Problem. In Proceedings of the International Conference on Spoken Language Processing (ICSLP). M. Mohri, F.C.N. Pereira, and M. Riley. 2000. The design principles of a weighted finite-state transducer li-  brary. Theoretical Computer Science, 231:17-32. A. Ogawa, K. Takeda, and F. Itakura. 1998. Balancing Acoustic and Linguistic Probabilities. In Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). F. Richardson, M. Ostendorf, and J.R. Rohlicek. 1995. Lattice-based search strategies for large vocabulary speech recognition. In Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). Brian Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249–276. Roni Rosenfeld, Stanley F. Chen, and Xiaojin Zhu. 2001. Whole-Sentence Exponential Language Models: a Vehicle for Linguistic-Statistical Integration. Computer Speech and Language, 15(1). Roni Rosenfeld. 1997. A Whole Sentence Maximum Entropy Language Model. In Proc. of IEEE workshop on Automatic Speech Recognition and Understanding (ASRU), Santa Barbara, California, December. D.E. Rumelhart, G. E. Hinton, and R.J. Williams. 1986. Learning representations by back-propagating errors. Nature, 323:533–536. Holger Schwenk. 2007. Continuous space language models. Computer Speech and Language, 21(3):492– 518. C. E. Shannon. 1948. A Mathematical Theory of Communication. The Bell System Technical Journal, 27:379–423, 623–656. H. Soltau, G. Saon, and B. Kingsbury. 2010. The IBM Attila speech recognition toolkit. In Proc. of IEEE Workshop on Spoken Language Technology (SLT). Wen Wang and Mary Harper. 2002. The SuperARV language model: investigating the effectiveness of tightly integrating multiple knowledge sources. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). 1127</p>
<br/>
<br/><br/><br/></body>
</html>
