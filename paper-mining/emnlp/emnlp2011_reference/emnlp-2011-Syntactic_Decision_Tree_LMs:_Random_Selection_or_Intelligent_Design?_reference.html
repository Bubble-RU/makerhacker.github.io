<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-131" href="../emnlp2011/emnlp-2011-Syntactic_Decision_Tree_LMs%3A_Random_Selection_or_Intelligent_Design%3F.html">emnlp2011-131</a> <a title="emnlp-2011-131-reference" href="#">emnlp2011-131-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</h1>
<br/><p>Source: <a title="emnlp-2011-131-pdf" href="http://aclweb.org/anthology//D/D11/D11-1064.pdf">pdf</a></p><p>Author: Denis Filimonov ; Mary Harper</p><p>Abstract: Decision trees have been applied to a variety of NLP tasks, including language modeling, for their ability to handle a variety of attributes and sparse context space. Moreover, forests (collections of decision trees) have been shown to substantially outperform individual decision trees. In this work, we investigate methods for combining trees in a forest, as well as methods for diversifying trees for the task of syntactic language modeling. We show that our tree interpolation technique outperforms the standard method used in the literature, and that, on this particular task, restricting tree contexts in a principled way produces smaller and better forests, with the best achieving an 8% relative reduction in Word Error Rate over an n-gram baseline.</p><br/>
<h2>reference text</h2><p>Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, and Robert L. Mercer. 1990. A tree-based statistical language model for natural language speech recognition. Readings in speech recognition, pages 507–5 14. Srinivas Bangalore. 1996. ‘Almost parsing’ technique for language modeling. In Proceedings of the International Conference on Spoken Language Processing, volume 2, pages 1173–1 176. Jeff Bilmes and Katrin Kirchhoff. 2003. Factored language models and generalized parallel backoff. In Proceedings of HLT/NAACL, pages 4–6. Ciprian Chelba and Frederick Jelinek. 2000. Structured language modeling for speech recognition. CoRR. Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, pages 3 10– 318. 699  S. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon, H. Soltau, and G. Zweig. 2006. Advances in speech transcription at IBM under the DARPA EARS program. IEEE Transactions on Audio, Speech and Lan-  guage Processing, pages 1596–1608. Denis Filimonov and Mary Harper. 2009. A joint language model with fine-grain syntactic tags. In Proceedings of the EMNLP 2009. Denis Filimonov and Mary Harper. 2011. Generalized interpolation in decision tree LM. In Proceedings of the 49st Annual Meeting of the Association for Computational Linguistics. Peter Heeman. 1998. POS tagging versus classes in language modeling. In Sixth Workshop on Very Large Corpora. Zhongqiang Huang and Mary Harper. 2009. SelfTraining PCFG grammars with latent annotations across languages. In Proceedings of the EMNLP 2009. Frederick Jelinek and Robert L. Mercer. 1980. Interpolated estimation of markov source parameters from sparse data. In Proceedings of the Workshop on Pattern Recognition in Practice, pages 381–397. Sven Martin, Jorg Liermann, and Hermann Ney. 1998. Algorithms for bigram and trigram word clustering. In Speech Communication, pages 1253–1256. J. R. Quinlan. 1986. Induction of decision trees. Machine Learning, 1(1):81–106. Ronald Rosenfeld, Jaime Carbonell, and Alexander Rudnicky. 1994. Adaptive statistical language modeling: A maximum entropy approach. Technical report. Peng Xu. 2005. Random Forests and Data Sparseness  Problem in Language Modeling. Ph.D. thesis, Baltimore, Maryland, April. Imed Zitouni. 2007. Backoff hierarchical class ngram language models: effectiveness to model unseen events in speech recognition. Computer Speech & Language, 21(1):88–104.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
