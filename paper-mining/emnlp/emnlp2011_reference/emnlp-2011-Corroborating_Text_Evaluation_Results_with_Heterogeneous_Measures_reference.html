<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>36 emnlp-2011-Corroborating Text Evaluation Results with Heterogeneous Measures</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-36" href="../emnlp2011/emnlp-2011-Corroborating_Text_Evaluation_Results_with_Heterogeneous_Measures.html">emnlp2011-36</a> <a title="emnlp-2011-36-reference" href="#">emnlp2011-36-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>36 emnlp-2011-Corroborating Text Evaluation Results with Heterogeneous Measures</h1>
<br/><p>Source: <a title="emnlp-2011-36-pdf" href="http://aclweb.org/anthology//D/D11/D11-1042.pdf">pdf</a></p><p>Author: Enrique Amigo ; Julio Gonzalo ; Jesus Gimenez ; Felisa Verdejo</p><p>Abstract: Automatically produced texts (e.g. translations or summaries) are usually evaluated with n-gram based measures such as BLEU or ROUGE, while the wide set of more sophisticated measures that have been proposed in the last years remains largely ignored for practical purposes. In this paper we first present an indepth analysis of the state of the art in order to clarify this issue. After this, we formalize and verify empirically a set of properties that every text evaluation measure based on similarity to human-produced references satisfies. These properties imply that corroborating system improvements with additional measures always increases the overall reliability of the evaluation process. In addition, the greater the heterogeneity of the measures (which is measurable) the higher their combined reliability. These results support the use of heterogeneous measures in order to consolidate text evaluation results.</p><br/>
<h2>reference text</h2><p>Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita. 2001. Using Multiple Edit Distances to Automatically Rank Machine Translation Output. In Proceedings of Machine Translation Summit VIII, pages 15–20. Joshua Albrecht and Rebecca Hwa. 2007a. A Reexamination of Machine Learning Approaches for 464 Sentence-Level MT Evaluation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), pages 880–887. Joshua Albrecht and Rebecca Hwa. 2007b. Regression for Sentence-Level MT Evaluation with Pseudo Refer-  ences. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), pages 296–303. Enrique Amig o´, Julio Gonzalo, Anselmo Pe nas, and Felisa Verdejo. 2005. QARLA: a Framework for the Evaluation of Automatic Summarization. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 280–289. Enrique Amig o´, Jes u´s Gim e´nez, Julio Gonzalo, and Llu ı´s M `arquez. 2006. MT Evaluation: Human-Like vs. Human Acceptable. In Proceedings of the Joint 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL), pages 17– 24. Enrique Amig o´, Jes u´s Gim e´nez, Julio Gonzalo, and Felisa Verdejo. 2009. The contribution of linguistic features to automatic machine translation evaluation. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ’09, pages 306–3 14, Stroudsburg, PA, USA. Association for Computational Linguistics. Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Pro-  ceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization. Chris Callison-burch and Miles Osborne. 2006. Reevaluating the role of bleu in machine translation research. In In EACL, pages 249–256. Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2008. Further meta-evaluation of machine translation. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 70–106. Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 workshop on statistical machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation. Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Przybocki, and Omar Zaidan. 2010. Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 17–53. Revised August 2010. Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM: A maximum similarity metric for machine translation evaluation. In Proceedings of ACL-08: HLT, pages 55–62. Simon Corston-Oliver, Michael Gamon, and Chris  Brockett. 2001. A Machine Learning Approach to the Automatic Evaluation of Machine Translation. In Proceedings ofthe 39thAnnualMeeting oftheAssociation for Computational Linguistics (ACL), pages 140–147. Christopher Culy and Susanne Z. Riehemann. 2003. The Limits of N-gram Translation Evaluation Metrics. In Proceedings of MT-SUMMIT IX, pages 1–8. George Doddington. 2002. Automatic Evaluation of Machine Translation Quality Using N-gram CoOccurrence Statistics. In Proceedings of the 2nd International Conference on Human Language Technology, pages 138–145. Michael Gamon, Anthony Aue, and Martine Smets. 2005. Sentence-Level MT evaluation without reference translations: beyond language modeling. In Proceedings of EAMT, pages 103–1 11. Jes u´s Gim e´nez and Llu ı´s M `arquez. 2007. Linguistic Features for Automatic Evaluation of Heterogeneous MT Systems. In Proceedings of the ACL Workshop on Statistical Machine Translation, pages 256–264. Jes u´s Gim e´nez and Llu ı´s M `arquez. 2008. Heterogeneous Automatic MT Evaluation Through NonParametric Metric Combinations. In Proceedings of the Third International Joint Conference on Natural Language Processing (IJCNLP), pages 3 19–326. Jes u´s Gim e´nez and Llu ı´s M `arquez. 2010. Asiya: An Open Toolkit for Automatic Machine Translation (Meta-)Evaluation. The Prague Bulletin of Mathemat-  ical Linguistics, 1(94):77–86. Jes u´s Gim e´nez. 2008. Empirical Machine Translation and its Evaluation. Ph.D. thesis, Universitat Polit `ecnica de Catalunya. Tsutomu Hirao, Manabu Okumura, and Hideki Isozaki. 2005. Kernel-based approach for automatic evaluation of natural language generation technologies: Application to automatic summarization. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 145–152, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics. Petr Homola, Vladislav Kubo nˇ, and Pavel Pecina. 2009. A simple automatic mt evaluation metric. In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ’09, pages 33–36, Stroudsburg, PA, USA. Association for Computational Linguistics. Jeremy G. Kahn, Matthew Snover, and Mari Ostendorf. 2009. Expected Dependency Pair Match: Predicting 465 translation quality with expected syntactic structure. Machine Translation. Alex Kulesza and Stuart M. Shieber. 2004. A learning approach to improving sentence-level MT evaluation. In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine  Translation (TMI), pages 75–84. LDC. 2005. Linguistic Data Annotation Specification: Assessment of Adequacy and Fluency in Translations. Revision 1.5. Technical report, Linguistic Data Consortium. http : / /www . ldc .upenn .edu/P ro j ect s / T IDES / Trans lat ion / TransAs se s s 0 4 .pdf. Audrey Le and Mark Przybocki. 2005. NIST 2005 machine translation evaluation official results. In Official release of automatic evaluation scores for all submissions, August. Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006. CDER: Efficient MT Evaluation Using Block Movements. In Proceedings of 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 241–248. Chin-Yew Lin and Franz Josef Och. 2004a. Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statics. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL). Chin-Yew Lin and Franz Josef Och. 2004b. ORANGE: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation. In Proceedings of the 20th International Conference on Computational Linguistics (COLING). Chin-Yew Lin. 2004. Rouge: A Package for Automatic Evaluation of Summaries. In Marie-Francine  Moens and Stan Szpakowicz, editors, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain, July. Association for Computational Linguistics. Lucian Vlad Lita, Monica Rogati, and Alon Lavie. 2005. BLANC: Learning Evaluation Metrics for MT. In Proceedings of the Joint Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT-EMNLP), pages 740–747. Ding Liu and Daniel Gildea. 2005. Syntactic Features for Evaluation of Machine Translation. In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, pages 25–32. Ding Liu and Daniel Gildea. 2006. Stochastic Iterative Alignment for Machine Translation Evaluation. In Proceedings of the Joint 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL), pages 539–546. Ding Liu and Daniel Gildea. 2007. Source-Language Features and Maximum Correlation Training for Machine Translation Evaluation. In Proceedings of the 2007 Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 41–48. Dennis Mehay and Chris Brew. 2007. BLEUATRE:  Flattening Syntactic Dependencies for MT Evaluation. In Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI). I. Dan Melamed, Ryan Green, and Joseph P. Turian. 2003. Precision and Recall of Machine Translation. In Proceedings of the Joint Conference on Human Language Technology and the North American Chapter of the Association for Computational Linguistics (HLTNAACL). Sonja Nießen, Franz Josef Och, Gregor Leusch, and Hermann Ney. 2000. An Evaluation Tool for Machine Translation: Fast Evaluation for MT Research. In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC). Karolina Owczarzak, Josef van Genabith, and Andy Way. 2007a. Dependency-Based Automatic Evaluation for Machine Translation. In Proceedings of SSST, NAACL-HLT/AMTA Workshop on Syntax and Structure in Statistical Translation, pages 80–87. Karolina Owczarzak, Josef van Genabith, and Andy Way. 2007b. Labelled Dependencies in Machine Translation Evaluation. In Proceedings of the ACL Workshop on Statistical Machine Translation, pages 104–1 11. Karolina Owczarzak, Josef van Genabith, and Andy Way. 2008. Evaluating machine translation with lfg dependencies. Machine Translation, 21(2):95–1 19. Karolina Owczarzak. 2009. Depeval(summ):  dependency-based evaluation for automatic summaries. In ACL-IJCNLP ’09: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1, pages 190–198, Morristown, NJ, USA. Association for Computational Linguistics. Sebastian Pad o´, Michael Galley, Dan Jurafsky, and Christopher D. Manning. 2009. Robust machine translation evaluation with entailment features. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 297–305. K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001a. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 3 11–3 18, Philadelphia, jul. 466 Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001b. Bleu: a method for automatic evaluation of machine translation, RC22176. Technical report, IBM T.J. Watson Research Center. Michael Paul, Andrew Finch, and Eiichiro Sumita. 2007. Reducing Human Assessments of Machine Translation Quality to Binary Classifiers. In Proceedings of the 11th Conference on Theoretical and Methodologi-  cal Issues in Machine Translation (TMI). Maja Popovic and Hermann Ney. 2007. Word Error Rates: Decomposition over POS classes and Applications for Error Analysis. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 48–55, Prague, Czech Republic, June. Association for Computational Linguistics. Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency Treelet Translation: Syntactically Informed Phrasal SMT. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 271–279. Chris Quirk. 2004. Training a Sentence-Level Machine Translation Confidence Metric. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC), pages 825–828. Florence Reeder, Keith Miller, Jennifer Doyon, and John White. 2001. The Naming of Things and the Confusion of Tongues: an MT Metric. In Proceedings of the Workshop on MT Evaluation ”Who did what to whom? ” at Machine Translation Summit VIII, pages 55–59. Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas  (AMTA), pages 223–231. Christoph Tillmann, Stefan Vogel, Hermann Ney, A. Zubiaga, and H. Sawaf. 1997. Accelerated DP based Search for Statistical Translation. In Proceedings of European Conference on Speech Communication and Technology. Stephen Tratz and Eduard Hovy. 2008. Summarization evaluation using transformed basic elements. In In Proceedings of TAC-08. Gaithersburg, Maryland. Joseph Turian, Luke Shen, and I. Dan Melamed. 2003a. Evaluation of machine translation and its evaluation. In In Proceedings of MT Summit IX, pages 386–393. Joseph P. Turian, Luke Shen, and I. Dan Melamed. 2003b. Evaluation of Machine Translation and its Evaluation. In Proceedings of MT SUMMIT IX.</p>
<br/>
<br/><br/><br/></body>
</html>
