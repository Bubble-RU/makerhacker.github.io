<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>39 emnlp-2011-Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-39" href="../emnlp2011/emnlp-2011-Discovering_Morphological_Paradigms_from_Plain_Text_Using_a_Dirichlet_Process_Mixture_Model.html">emnlp2011-39</a> <a title="emnlp-2011-39-reference" href="#">emnlp2011-39-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>39 emnlp-2011-Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model</h1>
<br/><p>Source: <a title="emnlp-2011-39-pdf" href="http://aclweb.org/anthology//D/D11/D11-1057.pdf">pdf</a></p><p>Author: Markus Dreyer ; Jason Eisner</p><p>Abstract: We present an inference algorithm that organizes observed words (tokens) into structured inflectional paradigms (types). It also naturally predicts the spelling of unobserved forms that are missing from these paradigms, and discovers inflectional principles (grammar) that generalize to wholly unobserved words. Our Bayesian generative model of the data explicitly represents tokens, types, inflections, paradigms, and locally conditioned string edits. It assumes that inflected word tokens are generated from an infinite mixture of inflectional paradigms (string tuples). Each paradigm is sampled all at once from a graphical model, whose potential functions are weighted finitestate transducers with language-specific parameters to be learned. These assumptions naturally lead to an elegant empirical Bayes inference procedure that exploits Monte Carlo EM, belief propagation, and dynamic programming. Given 50–100 seed paradigms, adding a 10million-word corpus reduces prediction error for morphological inflections by up to 10%.</p><br/>
<h2>reference text</h2><p>A. C. Albright. 2002. The Identification of Bases in Morphological Paradigms. Ph.D. thesis, University of California, Los Angeles. D. Aldous. 1985. Exchangeability and related topics. École d’été de probabilités de Saint-Flour XIII, pages 1–198. C. E. Antoniak. 1974. Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems. Annals of Statistics, 2(6): 1152–1 174. R. H Baayen, R. Piepenbrock, and L. Gulikers. 1995. The CELEX lexical database (release 2)[cd-rom]. Philadelphia, PA: Linguistic Data Consortium, University of Pennsylvania [Distributor]. M. Baroni, J. Matiasek, and H. Trost. 2002. Unsupervised discovery of morphologically related words based on orthographic and semantic similarity. In Proc. of the ACL-02 Workshop on Morphological and Phonological Learning, pages 48–57. M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta. 2009. The WaCky Wide Web: A collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation, 43(3):209–226. David Blackwell and James B. MacQueen. 1973. Ferguson distributions via Pòlya urn schemes. The Annals of Statistics, 1(2):353–355, March.  David M. Blei and Peter I. Frazier. 2010. Distancedependent Chinese restaurant processes. In Proc. of ICML, pages 87–94. E. Chan. 2006. Learning probabilistic paradigms for morphology in a latent class model. In Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL, pages 69–78. M. Creutz and K. Lagus. 2005. Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1.0. Computer and Information Science, Report A, 81. H. Déjean. 1998. Morphemes as necessary concept for structures discovery from untagged corpora. In Proc. of the Joint Conferences on New Methods in Language Processing and Computational Natural Language Learning, pages 295–298. Markus Dreyer and Jason Eisner. 2009. Graphical models over multiple strings. In Proc. of EMNLP, Singapore, August. Markus Dreyer, Jason Smith, and Jason Eisner. 2008. Latent-variable modeling of string transductions with finite-state methods. In Proc. of EMNLP, Honolulu, Hawaii, October. Markus Dreyer. 2011. A Non-Parametric Model for the Discovery of Inflectional Paradigms from Plain Text 626  Using Graphical Models over Strings. Ph.D. thesis, Johns Hopkins University. T.S. Ferguson. 1973. A Bayesian analysis of some nonparametric problems. The annals ofstatistics, 1(2):209– 230. Y. Freund and R. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277–296. J. Goldsmith. 2001. Unsupervised learning of the morphology of a natural language. Computational Linguistics, 27(2): 153–198. S. Goldwater, T. Griffiths, and M. Johnson. 2006a. Interpolating between types and tokens by estimating power-law generators. In Proc. of NIPS, volume 18, pages 459–466. S. Goldwater, T. L. Griffiths, and M. Johnson. 2006b. Contextual dependencies in unsupervised word segmentation. In Proc. of COLING-ACL. P.J. Green. 1995. Reversible jump Markov chain Monte Carlo computation and Bayesian model determination. Biometrika, 82(4):71 1. M. A Hafer and S. F Weiss. 1974. Word segmentation by letter successor varieties. Information Storage and Retrieval, 10:371–385. Z. S. Harris. 1955. From phoneme to morpheme. Language, 31(2): 190–222. G.E. Hinton. 2002. Training products of experts by min-  imizing contrastive divergence. Neural Computation, 14(8): 1771–1800. Klara Janecki. 2000. 300 Polish Verbs. Barron’s Educational Series. E. T. Jaynes. 2003. Probability Theory: The Logic of Science. Cambridge Univ Press. Edited by Larry Bretthorst. D. Jurafsky, J. H. Martin, A. Kehler, K. Vander Linden, and N. Ward. 2000. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. MIT Press. M. Kay. 1987. Nonconcatenative finite-state morphology. In Proc. of EACL, pages 2–10. M. Kurimo, S. Virpioja, V. Turunen, and K. Lagus. 2010. Morpho Challenge competition 2005–2010: Evaluations and results. In Proc. of ACL SIGMORPHON, pages 87–95. P. H. Matthews. 1972. Inflectional Morphology: A Theoretical Study Based on Aspects of Latin Verb Conjugation. Cambridge University Press. Christian Monson, Jaime Carbonell, Alon Lavie, and Lori Levin. 2007. ParaMor: Minimally supervised induction of paradigm structure and morphological analysis. In Proc. of ACL SIGMORPHON, pages 117–125, June. and S. Goldwater. 2009. Improving morphology induction by learning spelling rules. In Proc. of IJCAI, pages 1531–1536. J. Pitman and M. Yor. 1997. The two-parameter PoissonJ. Naradowsky  Dirichlet distribution derived from a stable subordinator.  Annals of Probability, 25:855–900. P. Schone and D. Jurafsky. 2001. Knowledge-free induction of inflectional morphologies. In Proc. of NAACL, volume 183, pages 183–191. J. Sethuraman. 1994. A constructive definition of Dirichlet priors. Statistica Sinica, 4(2):639–650. N. A. Smith, D. A. Smith, and R. W. Tromble. 2005. Context-based morphological disambiguation with random fields. In Proceedings of HLT-EMNLP, pages 475–482, October. G. T. Stump. 2001 . Inflectional Morphology: A Theory of Paradigm Structure. Cambridge University Press. Y.W. Teh, M.I. Jordan, M.J. Beal, and D.M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476): 1566–158 1. Yee Whye Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proc. of ACL. K. Toutanova and R.C. Moore. 2002. Pronunciation modeling for improved spelling correction. In Proc. of ACL, pages 144–151. D. Yarowsky and R. Wicentowski. 2000. Minimally supervised morphological analysis by multimodal alignment. In Proc. of ACL, pages 207–216, October. 627</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
