<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-68" href="../emnlp2011/emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">emnlp2011-68</a> <a title="emnlp-2011-68-reference" href="#">emnlp2011-68-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</h1>
<br/><p>Source: <a title="emnlp-2011-68-pdf" href="http://aclweb.org/anthology//D/D11/D11-1102.pdf">pdf</a></p><p>Author: Marco Dinarelli ; Sophie Rosset</p><p>Abstract: Reranking models have been successfully applied to many tasks of Natural Language Processing. However, there are two aspects of this approach that need a deeper investigation: (i) Assessment of hypotheses generated for reranking at classification phase: baseline models generate a list of hypotheses and these are used for reranking without any assessment; (ii) Detection of cases where reranking models provide a worst result: the best hypothesis provided by the reranking model is assumed to be always the best result. In some cases the reranking model provides an incorrect hypothesis while the baseline best hypothesis is correct, especially when baseline models are accurate. In this paper we propose solutions for these two aspects: (i) a semantic inconsistency metric to select possibly more correct n-best hypotheses, from a large set generated by an SLU basiline model. The selected hypotheses are reranked applying a state-of-the-art model based on Partial Tree Kernels, which encode SLU hypotheses in Support Vector Machines with complex structured features; (ii) finally, we apply a decision strategy, based on confidence values, to select the final hypothesis between the first ranked hypothesis provided by the baseline SLU model and the first ranked hypothesis provided by the re-ranker. We show the effectiveness of these solutions presenting comparative results obtained reranking hypotheses generated by a very accurate Conditional Random Field model. We evaluate our approach on the French MEDIA corpus. The results show significant improvements with respect to current state-of-the-art and previous 1104 Sophie Rosset LIMSI-CNRS B.P. 133, 91403 Orsay Cedex France ro s set @ l ims i fr . re-ranking models.</p><br/>
<h2>reference text</h2><p>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: languageindependent named entity recognition. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003 - Volume 4, pages 142–147, Morristown, NJ, USA. Association for Computational  Linguistics. Ellen M. Voorhees. 2001. The trec question answering track. Nat. Lang. Eng., 7:361–378, December. X. Carreras and Lluis Marquez. 2005. Introduction to the conll-2005 shared task: Semantic role labeling. R. De Mori, F. Bechet, D. Hakkani-Tur, M. McTear, G. Riccardi, and G. Tur. 2008. Spoken language understanding: A survey. IEEE Signal Processing Magazine, 25:50–58. Sylvain Galliano, Guillaume Gravier, and Maura Chaubard. 2009. The ester 2 evaluation campaign for the rich transcription of french radio broadcasts. In Proceedings of the International Conference of the Speech Communication Assosiation (Interspeech), Brighton, U.K. E. Charniak and M. Johnson. 2005. Coarse-to-fine nbest parsing and maxent discriminative reranking. In Proceedings of the Conference of the Association for Computational Linguistics (ACL), page 363370, Ann Arbor, MI. Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistic (CL), 31(1):25–70. J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning (ICML), pages 282–289, Williamstown,  MA, USA, June. Brigitte Krenn and Christer Samuelsson. 1997. The linguist’s guide to statistics - don’t panic. Thomas Lavergne, Olivier Capp e´, and Fran ¸cois Yvon. 2010. Practical very large scale CRFs. In Proceedings the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 504–513. Association for Computational Linguistics, July. Stefan Hahn, Patrick Lehnen, Georg Heigold, and Hermann Ney. 2009. Optimizing crfs for slu tasks in various languages using modified training criteria. In Proceedings of the International Conference of the Speech Communication Assosiation (Interspeech), Brighton, U.K. Stefan Riezler and Alexander Vasserman. 2010. Incremental feature selection and l1 regularization for relaxed maximum-entropy modeling. 1114 Hui Zou and Trevor Hastie. 2005. Regularization and variable selection via the Elastic Net. Journal of the Royal Statistical Society B, 67:301–320. Andrew McCallum. 2003. Efficiently inducing features of conditional random fields. In 19th Conference on Uncertainty in Artificial Intelligence. Lawrence R. Rabiner. 1989. A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257–286.  Mark Johnson. 1998. Pcfg models of linguistic tree representations. Computational Linguistics, 24:613–632. Olivier Galibert, Ludovic Quintard, Sophie Rosset, Pierre Zweigenbaum, Claire Ndellec, Sophie Aubin, Laurent Gillard, Jean-Pierre Raysz, Delphine Pois, Xavier Tannier, Louise Delger, and Dominique Laurent. 2010. Named and specific entity detection in varied data: The quro named entity baseline evaluation. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), Valletta, Malta, may. European Language Resources Association (ELRA). Olivier Galibert. 2009. Approches et m ´ethodologiespour la automatique `a des questions adapt ´ees un cadre interactif en domaine ouvert. Ph.D. thesis, Universit Paris Sud, Orsay. G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw, S. Strassel, and R. Weischedel. 2004. The Automatic Content Extraction (ACE) Program–Tasks, Data, and Evaluation. Proceedings of LREC 2004, pages 837– 840. H ´el `ene Bonneau-Maynard, Christelle Ayache, F. Bechet, A Denis, A Kuhn, Fabrice Lef e`vre, D. Mostefa, M. Qugnard, S. Rosset, and J. Servan, S. Vilaneau.  r ´eponse  2006. Results of the french evalda-media evaluation campaign for literal understanding. In LREC, pages 2054–2059, Genoa, Italy, May. Christian Raymond, Frdric Bchet, Renato De Mori, and Graldine Damnati. 2006. On the use of finite state transducers for semantic interpretation. Speech Communication, 48(3-4):288–304, March-April. Christian Raymond and Giuseppe Riccardi. 2007. Generative and discriminative algorithms for spoken language understanding. In Proceedings of the International Conference of the Speech Communication Assosiation (Interspeech), pages 1605–1608, Antwerp, Belgium, August. Vladimir N. Vapnik. 1998. Statistical Learning Theory. John Wiley and Sons. T. J. Sejnowski and C. S. Rosenberg. 1987. Parallel networks that learn to pronounce English text. Complex Systems, 1:145–168. Stefan Hahn, Marco Dinarelli, Christian Raymond, Fabrice Lef e`vre, Patrick Lehen, Renato De Mori, Alessandro Moschitti, Hermann Ney, and Giuseppe Riccardi. 2010. Comparing stochastic approaches to spoken language understanding in multiple languages. IEEE Transactions on Audio, Speech and Language Processing (TASLP), 99. Marco Dinarelli, Alessandro Moschitti, and Giuseppe Riccardi. 2009b. Re-ranking models based on small training data for spoken language understanding. In  Conference of Empirical Methods for Natural Language Processing, pages 11–18, Singapore, August. Marco Dinarelli, Alessandro Moschitti, and Giuseppe Riccardi. 2010. Hypotheses Selection for Reranking Semantic Annotation. In IEEE Workshop of Spoken Language Technology (SLT), Berkeley, USA. Alessandro Moschitti. 2006. Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees. In Proceedings ofECML 2006, pages 3 18–329, Berlin, Germany. M. Collins and N. Duffy. 2002. New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete structures, and the voted perceptron. In Proceedings of the Association for Computational Linguistics, pages 263–270. Libin Shen, Anoop Sarkar, and Aravind K. Joshi. 2003. Using LTAG Based Features in Parse Reranking. In Proceedings of EMNLP’06. Herbrich, Ralf and Graepel, Thore and Obermayer, Klaus. 2000. Large Margin Rank Boundaries for Ordinal Regression. In Advances in Large Margin Classifiers. Libin Shen, and Aravind K. Joshi. 2003. An SVM Based Voting Algorithm with Application to Parse Reranking. In Proceedings of CoNLL 2003. Libin Shen, Anoop Sarkar, and Franz J. Och. 2004. Discriminative reranking for machine translation. In HLTNAACL, pages 177–184.  Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting-based parse reranking with subtree features. In Proceedings of ACL’05. Stefan Hahn, Patrick Lehnen, and Hermann Ney. 2008a. System combination for spoken language understanding. In Proceedings of the International Conference of the Speech Communication Assosiation (Interspeech), pages 236–239, Brisbane, Australia. J. G. Fiscus. 1997. A post-processing system to yield reduced word error rates: Recogniser output voting error reduction (ROVER). In Proceedings 1997 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), pages 347–352, Santa Barbara, CA, December. 1115 John Shawe-Taylor and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press. Michael Collins. 2000. Discriminative reranking for natural language parsing. In ICML, pages 175–182. Michael Collins and Nigel Duffy. 2001 . Convolution kernels for natural language. In Advances in Neural Information Processing Systems 14, pages 625–632. MIT Press. Rush, Alexander M. and Sontag, David and Collins, Michael and Jaakkola, Tommi. 2010. On dual decomposition and linear programming relaxations for nat-  ural language processing. In Empirical Methods for Natural Language Processing (EMNLP). Cambridge, Massachusetts, USA. Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2008. Tree kernels for semantic role labeling. Computational Linguistics, 34(2): 193–224. Alessandro Moschitti, Silvia Quarteroni, Roberto Basili, and Suresh Manandhar. 2007. Exploiting syntactic and shallow semantic kernels for question/answer classification. In Proceedings of ACL’07, Prague, Czech Republic. Alexander Yeh and Kelmeth Church. 2000. More accurate tests for the statistical significance of result differences.</p>
<br/>
<br/><br/><br/></body>
</html>
