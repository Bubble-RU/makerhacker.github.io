<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>126 emnlp-2012-Training Factored PCFGs with Expectation Propagation</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-126" href="../emnlp2012/emnlp-2012-Training_Factored_PCFGs_with_Expectation_Propagation.html">emnlp2012-126</a> <a title="emnlp-2012-126-reference" href="#">emnlp2012-126-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>126 emnlp-2012-Training Factored PCFGs with Expectation Propagation</h1>
<br/><p>Source: <a title="emnlp-2012-126-pdf" href="http://aclweb.org/anthology//D/D12/D12-1105.pdf">pdf</a></p><p>Author: David Hall ; Dan Klein</p><p>Abstract: PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar. We present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion. Our method works with linguisticallymotivated annotations, induced latent structure, lexicalization, or any mix of the three. We use a structured expectation propagation algorithm that makes use of the factored structure in two ways. First, by partitioning the factors, it speeds up parsing exponentially over the unfactored approach. Second, it minimizes the redundancy of the factors during training, improving accuracy over an independent approach. Using purely latent variable annotations, we can efficiently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the na¨ ıve approach. Combining latent, lexicalized, and unlexicalized anno- tations, our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank.</p><br/>
<h2>reference text</h2><br/>
<br/><br/><br/></body>
</html>
