<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>65 emnlp-2012-Improving NLP through Marginalization of Hidden Syntactic Structure</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-65" href="../emnlp2012/emnlp-2012-Improving_NLP_through_Marginalization_of_Hidden_Syntactic_Structure.html">emnlp2012-65</a> <a title="emnlp-2012-65-reference" href="#">emnlp2012-65-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>65 emnlp-2012-Improving NLP through Marginalization of Hidden Syntactic Structure</h1>
<br/><p>Source: <a title="emnlp-2012-65-pdf" href="http://aclweb.org/anthology//D/D12/D12-1074.pdf">pdf</a></p><p>Author: Jason Naradowsky ; Sebastian Riedel ; David Smith</p><p>Abstract: Many NLP tasks make predictions that are inherently coupled to syntactic relations, but for many languages the resources required to provide such syntactic annotations are unavailable. For others it is unclear exactly how much of the syntactic annotations can be effectively leveraged with current models, and what structures in the syntactic trees are most relevant to the current task. We propose a novel method which avoids the need for any syntactically annotated data when predicting a related NLP task. Our method couples latent syntactic representations, constrained to form valid dependency graphs or constituency parses, with the prediction task via specialized factors in a Markov random field. At both training and test time we marginalize over this hidden structure, learning the optimal latent representations for the problem. Results show that this approach provides significant gains over a syntactically uninformed baseline, outperforming models that observe syntax on an English relation extraction task, and performing comparably to them in semantic role labeling.</p><br/>
<h2>reference text</h2><br/>
<br/><br/><br/></body>
</html>
