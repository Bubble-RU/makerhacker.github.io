<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>130 emnlp-2012-Unambiguity Regularization for Unsupervised Learning of Probabilistic Grammars</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-130" href="../emnlp2012/emnlp-2012-Unambiguity_Regularization_for_Unsupervised_Learning_of_Probabilistic_Grammars.html">emnlp2012-130</a> <a title="emnlp-2012-130-reference" href="#">emnlp2012-130-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>130 emnlp-2012-Unambiguity Regularization for Unsupervised Learning of Probabilistic Grammars</h1>
<br/><p>Source: <a title="emnlp-2012-130-pdf" href="http://aclweb.org/anthology//D/D12/D12-1121.pdf">pdf</a></p><p>Author: Kewei Tu ; Vasant Honavar</p><p>Abstract: We introduce a novel approach named unambiguity regularization for unsupervised learning of probabilistic natural language grammars. The approach is based on the observation that natural language is remarkably unambiguous in the sense that only a tiny portion of the large number of possible parses of a natural language sentence are syntactically valid. We incorporate an inductive bias into grammar learning in favor of grammars that lead to unambiguous parses on natural language sentences. The resulting family of algorithms includes the expectation-maximization algorithm (EM) and its variant, Viterbi EM, as well as a so-called softmax-EM algorithm. The softmax-EM algorithm can be implemented with a simple and computationally efficient extension to standard EM. In our experiments of unsupervised dependency grammar learn- ing, we show that unambiguity regularization is beneficial to learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art.</p><br/>
<h2>reference text</h2><p>I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. Diaz de Ilarraza, A. Garmendia, , and M. Oronoz. 2003. Construction of a basque dependency treebank. In Proc. of the 2nd Workshop on Treebanks and Linguistic Theories (TLT). Susana Afonso, Eckhard Bick, Renato Haber, and Diana Santos. 2002. “floresta sint a´(c)tica”: a treebank for Portuguese. In Proceedings of the 3rd Intern. Conf. on Language Resources and Evaluation (LREC), pages 1968–1703. J. K. Baker. 1979. Trainable grammars for speech recognition. In Speech Communication Papers for the 97th Meeting of the Acoustical Society of America. Van Der Beek, G. Bouma, R. Malouf, G. Van Noord, and Rijksuniversiteit Groningen. 2002. The alpino dependency treebank. In In Computational Linguistics in the Netherlands (CLIN, pages 1686–1691 . 1333 Phil Blunsom and Trevor Cohn. 2010. Unsupervised induction of tree substitution grammars for dependency parsing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’ 10, pages 1204–1213, Stroudsburg, PA, USA. Association for Computational Linguistics.  Matthias Buch-Kromann, J ¨urgen Wedekind, , and Jakob Elming. 2007. The copenhagen danishenglish dependency treebank v. 2.0. http://www.buchkromann.dk/matthias/cdt2.0/. Stanley F. Chen. 1995. Bayesian grammar induction for language modeling. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics. Shay B. Cohen and Noah A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In HLT-NAACL, pages 74–82. Shay B. Cohen, Kevin Gimpel, and Noah A. Smith. 2008. Logistic normal priors for unsupervised probabilistic grammar induction. In NIPS, pages 321–328. Tomaz Erjavec, Darja Fiser, Simon Krek, and Nina Ledinek. 2010. The jos linguistically tagged corpus of slovene. In LREC. Jenny Rose Finkel, Trond Grenager, and Christopher D. Manning. 2007. The infinite tree. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 272–279. Association for Computational Linguistics, June. Kuzman Ganchev, Jo˜ ao Gra ¸ca, Jennifer Gillenwater, and Ben Taskar. 2010. Posterior regularization for structured latent variable models. Journal of Machine Learning Research, 11:2001–2049. Jennifer Gillenwater, Kuzman Ganchev, Jo˜ ao Gra ¸ca, Fer-  nando Pereira, and Ben Taskar. 2010. Sparsity in dependency grammar induction. In ACL ’10: Proceedings of the ACL 2010 Conference Short Papers, pages 194–199, Morristown, NJ, USA. Association for Computational Linguistics. Yves Grandvalet and Yoshua Bengio. 2005. Semisupervised learning by entropy minimization. In Lawrence K. Saul, Yair Weiss, and L e´on Bottou, editors, Advances in Neural Information Processing Systems 17, pages 529–536. MIT Press, Cambridge, MA. Jan Haji cˇ, Alena B¨ ohmov a´, Eva Haji cˇov a´, and Barbora Vidov ´a-Hladk´ a. 2000. The Prague Dependency Treebank: A Three-Level Annotation Scenario. In A. Abeill´ e, editor, Treebanks: Building and Using Parsed Corpora, pages 103–127. Amsterdam:Kluwer. Jan Haji cˇ, Otakar Smr zˇ, Petr Zem a´nek, Jan Sˇnaidauf, and Emanuel Beˇ ska. 2004. Prague arabic dependency treebank: Development in data and tools. In In Proc. of the NEMLAR Intern. Conf. on Arabic Language Resources and Tools, pages 110–1 17. William P. Headden, III, Mark Johnson, and David McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In HLTNAACL, pages 101–109. Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007. Bayesian inference for pcfgs via markov chain monte carlo. In HLT-NAACL, pages 139–146. Dan Klein and Christopher D. Manning. 2004. Corpus-  based induction of syntactic structure: Models of dependency and constituency. In Proceedings of ACL. Kenichi Kurihara and Taisuke Sato. 2004. An application of the variational Bayesian approach to probabilistic contextfree grammars. In IJCNLP-04 Workshop beyond shallow analyses. K. Lari and S. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 4:35–36. Percy Liang, Slav Petrov, Michael I. Jordan, and Dan Klein. 2007. The infinite pcfg using hierarchical Dirichlet processes. In Proceedings of EMNLPCoNLL, pages 688–697. Christopher D. Manning and Hinrich Sch u¨tze. 1999. Foundations of statistical natural language processing. MIT Press, Cambridge, MA, USA. Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Talbanken05: A Swedish Treebank with Phrase Structure and Dependency Annotation. In Proceedings of the fifth international conference on Language Resources and Evaluation (LREC2006), May 24-26, 2006, Genoa, Italy, pages 1392–1395. European Language Resource Association, Paris. Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Associ-  ation for Computational Linguistics, pages 433–440, Morristown, NJ, USA. Association for Computational Linguistics. Hoifung Poon and Pedro Domingos. 2011. Sum-product networks : A new deep architecture. In Proceedings 1334 of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence (UAI). Kenneth Rose. 1998. Deterministic annealing for clustering, compression, classification, regression, and related optimization problems. In Proceedings of the IEEE, pages 2210–2239. Noah A. Smith and Jason Eisner. 2004. Annealing techniques for unsupervised statistical language learning. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04, Stroudsburg, PA, USA. Association for Computational Linguistics. David A. Smith and Jason Eisner. 2007. Bootstrapping feature-rich dependency parsers with entropic priors. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 667–677, Prague, June. Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky, and Christopher D. Manning. 2010. Viterbi training  improves unsupervised dependency parsing. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL ’ 10, pages 9–17, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<br/>
<br/><br/><br/></body>
</html>
