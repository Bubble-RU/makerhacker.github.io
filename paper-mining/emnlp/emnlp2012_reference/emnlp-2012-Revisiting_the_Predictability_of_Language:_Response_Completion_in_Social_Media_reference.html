<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>114 emnlp-2012-Revisiting the Predictability of Language: Response Completion in Social Media</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2012" href="../home/emnlp2012_home.html">emnlp2012</a> <a title="emnlp-2012-114" href="../emnlp2012/emnlp-2012-Revisiting_the_Predictability_of_Language%3A_Response_Completion_in_Social_Media.html">emnlp2012-114</a> <a title="emnlp-2012-114-reference" href="#">emnlp2012-114-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>114 emnlp-2012-Revisiting the Predictability of Language: Response Completion in Social Media</h1>
<br/><p>Source: <a title="emnlp-2012-114-pdf" href="http://aclweb.org/anthology//D/D12/D12-1136.pdf">pdf</a></p><p>Author: Bo Pang ; Sujith Ravi</p><p>Abstract: The question “how predictable is English?” has long fascinated researchers. While prior work has focused on formal English typically used in news articles, we turn to texts generated by users in online settings that are more informal in nature. We are motivated by a novel application scenario: given the difficulty of typing on mobile devices, can we help reduce typing effort with message completion, especially in conversational settings? We propose a method for automatic response completion. Our approach models both the language used in responses and the specific context provided by the original message. Our experimental results on a large-scale dataset show that both components help reduce typing effort. We also perform an information-theoretic study in this setting and examine the entropy of user-generated content, especially in con- versational scenarios, to better understand predictability of user generated English.</p><br/>
<h2>reference text</h2><p>Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: An entity-based approach. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL’05. David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022. Peter F. Brown, Vincent J. Della Pietra, Robert L. Mercer, Stephen A. Della Pietra, and Jennifer C. Lai. 1992. An estimate of an upper bound for the entropy of English. Comput. Linguist., 18:3 1–40. Ivan Bulyko, Mari Ostendorf, and Andreas Stolcke. 2003. Getting more mileage from web text sources for conversational speech language modeling using classdependent mixtures. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: companion volume of the Proceedings of HLT-NAACL 2003–short papers - Volume 2, NAACL-Short ’03, pages 7–9. Thomas M. Cover and Roger C. King. 1978. A convergent gambling estimate of the entropy of English. IEEE Transactions on Information Theory, 24:413– 421. Steve Farmer, Richard Sproat, and Michael Witzel. 2004. The collapse of the Indus-script thesis: The myth of a  literate Harappan civilization. Electronic Journal of Vedic Studies, 11:379–423 and 623–656. Yijue How and Min-Yen Kan. 2005. Optimizing predictive text entry for short message service on mobile phones. In Proceedings of the Human Computer Interfaces International (HCII). Christina L. James and Kelly M. Reischel. 2001. Text input for mobile devices: comparing model prediction to actual performance. In Proceedings of the SIGCHI conference on Human factors in computing systems, CHI ’01, pages 365–371, New York, NY, USA. ACM. Henry Kucera and W. Nelson Francis. 1967. Computational analysis of present-day American English. Brown University Press. I. Scott MacKenzie and R. William Soukoreff. 2002. Text entry for mobile computing: Models and methods,theory and practice. Human-Computer Interaction, 17(2-3): 147–198. 1499 Qiaozhu Mei and Kenneth Church. 2008. Entropy of search logs: how hard is search? with personalization? with backoff? In Proceedings ofthe international conference on Web search and web data mining, WSDM ’08, pages 45–54. Hamid Moradi, Jerzy W. Grzymala-busse, and James A. Roberts. 1998. Entropy of english text: experiments with humans and a machine learning system based on  rough sets. Information Sciences, 104:3 1–47. Petteri Nurmi, Andreas Forsblom, Patrik Flor e´en, Peter Peltonen, and Petri Saarikko. 2009. Predictive text input in a mobile shopping assistant: methods and interface design. In Proceedings of the 14th international conference on Intelligent user interfaces, IUI ’09, pages 435–438, New York, NY, USA. ACM. Rajesh P. N. Rao, Nisha Yadav, Mayank N. Vahia, Hrishikesh Joglekar, R. Adhikari, and Iravatham Mahadevan. 2009. Entropic evidence for linguistic structure in the Indus script. Science. Alan Ritter, Colin Cherry, and William B. Dolan. 2011. Data-driven response generation in social media. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 583–593. Ronald Rosenfeld. 2000. Two decades of statistical language modeling: Where do we go from here? Proceedings of the IEEE, 88. Claude E. Shannon. 1948. A mathematical theory of communication. Bell System Technical Journal, 27:379–423 and 623–656. Claude E. Shannon. 1951. Prediction and entropy of printed English. Bell System Technical Journal, 30:50–64. W. J. Teahan and John G. Cleary. 1996. The entropy of English using PPM-based models. In In Data Compression Conference, pages 53–62. IEEE Computer  Society Press. Joseph Weizenbaum. 1966. Eliza: a computer program for the study of natural language communication between man and machine. Commun. ACM, 9:36–45. D. Yavuz. 1978. Zipf’s law and entropy (Corresp.). IEEE Transactions on Information Theory, 20:650. Xing Yi and James Allan. 2009. A comparative study of utilizing topic models for information retrieval. In Proceedings of the European Conference on IR Research on Advances in Information Retrieval, pages 29–41.</p>
<br/>
<br/><br/><br/></body>
</html>
