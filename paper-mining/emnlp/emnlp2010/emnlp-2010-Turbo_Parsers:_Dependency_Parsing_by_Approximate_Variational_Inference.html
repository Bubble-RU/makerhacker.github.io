<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>110 emnlp-2010-Turbo Parsers: Dependency Parsing by Approximate Variational Inference</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2010" href="../home/emnlp2010_home.html">emnlp2010</a> <a title="emnlp-2010-110" href="#">emnlp2010-110</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>110 emnlp-2010-Turbo Parsers: Dependency Parsing by Approximate Variational Inference</h1>
<br/><p>Source: <a title="emnlp-2010-110-pdf" href="http://aclweb.org/anthology//D/D10/D10-1004.pdf">pdf</a></p><p>Author: Andre Martins ; Noah Smith ; Eric Xing ; Pedro Aguiar ; Mario Figueiredo</p><p>Abstract: We present a unified view of two state-of-theart non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al. (2009). By representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, includ- ing CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages.</p><p>Reference: <a title="emnlp-2010-110-reference" href="../emnlp2010_reference/emnlp-2010-Turbo_Parsers%3A_Dependency_Parsing_by_Approximate_Variational_Inference_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 pt  Abstract We present a unified view of two state-of-theart non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al. [sent-12, score-0.274]
</p><p>2 By representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method. [sent-14, score-0.175]
</p><p>3 We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. [sent-15, score-0.159]
</p><p>4 The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, includ-  ing CRFs and structured SVMs. [sent-16, score-0.092]
</p><p>5 Often, inference with such models becomes computationally intractable, causing a demand for understanding and improving approximate parsing algorithms. [sent-22, score-0.115]
</p><p>6 In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al. [sent-23, score-0.399]
</p><p>7 While those two parsers are differently motivated, we show that both correspond to inference in 34 M a´rio A. [sent-25, score-0.071]
</p><p>8 pt a factor graph, and both optimize objective functions over local approximations of the marginal polytope. [sent-29, score-0.385]
</p><p>9 The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al. [sent-30, score-0.303]
</p><p>10 1 Our contributions are not limited to dependency parsing: we present a general method for inference in factor graphs with hard constraints (§2), which ienxt efancdtso some hcsom wbithina htoarrdial c ofanscttorarsin tcson (§s2id)e,rwe dh by Smith and Eisner (2008). [sent-34, score-0.383]
</p><p>11 After presenting a geometric view of the variational approximations underlying message-passing algorithms (§3), and closing tyhien gap bsesatwgee-epna tshsien tgw aol gaofroirtehmmesn (t§io3n),ed an parsers (§4), we consider the problem of learning the model parameters (§5). [sent-35, score-0.26]
</p><p>12 Our experiments (§6) show state-of-the-art performance on dependency parsing tbaeten-cohfm-taherk-asr. [sent-43, score-0.083]
</p><p>13 (1993) for which decoding algorithms are equivalent to running belief propagation in a graph with loops (McEliece et al. [sent-45, score-0.227]
</p><p>14 hFyor example, eianc uhn ylabeled dependency parsing, I is the number of candidate dependency arcs (quadratic in the sentence length), and each Yi = {0, 1}. [sent-62, score-0.181]
</p><p>15 1: it is a bipartite graph Gx comprised of variable nodes {1, . [sent-73, score-0.094]
</p><p>16 , I} and factor nodes pCr ∈ C, vwaritiha an edge connecting atnhed fiatcht ovra rnioadbeles nCod ∈e a Cnd, a fitahcto anr n eoddgee C co inffn ie ∈ nCg. [sent-76, score-0.175]
</p><p>17 Hence, vtharei afablce-  ×  tnoord graph Gx mctoakre nso explicit ft ihe ∈ dCi re. [sent-77, score-0.066]
</p><p>18 This requires hard constraint factors that rule out forbidden partial assignments by mapping them to zero potential values. [sent-88, score-0.25]
</p><p>19 See Table 1for an inventory of hard constraint factors used in this paper. [sent-89, score-0.275]
</p><p>20 We let the soft factor potentials take the form ΨC(x,yC) yC)), where ∈ Rd is a vector of parameters (shared across factors) a Rnd φC(x, yC) is a local feature vector. [sent-92, score-0.292]
</p><p>21 Smith and Eisner (2008) proposed a factor graph representation for dependency parsing (Fig. [sent-96, score-0.324]
</p><p>22 The graph has O(n2) variable nodes (n is the sentence length), one per candidate arc a hh, mi linking a head h and modifier m. [sent-98, score-0.329]
</p><p>23 Outputs are binary, nwkiithng ya = a1d dif hf arc a belongs to the dependency tree. [sent-99, score-0.233]
</p><p>24 There is a hard factor TREE connected to all variables, that constrains the overall arc configurations to form a spanning tree. [sent-100, score-0.396]
</p><p>25 There is a unary soft factor per arc, whose log-potential reflects the score of that arc. [sent-101, score-0.221]
</p><p>26 There are also O(n3) pairwise factors; their log-potentials reflect the scores of sibling and grandparent arcs. [sent-102, score-0.118]
</p><p>27 These factors create loops, thus calling for approximate inference. [sent-103, score-0.228]
</p><p>28 , 2005),2 and computing posterior marginals for all arcs takes O(n3) time via the matrix-tree theorem (Smith and Smith, 2007; Koo et al. [sent-105, score-0.18]
</p><p>29 Two of these factors (TREE and XOR) had been proposed by Smith and Eisner (2008); we provide further information (max-product messages, entropies, and local agreement constraints). [sent-123, score-0.251]
</p><p>30 This inventory covers many cases, since the above formulae can be extended to the case where some inputs are negated: just replace the corresponding messages by their reciprocal, vi by 1 vi, etc. [sent-125, score-0.107]
</p><p>31 This allows building factors NAND (an OR factor with negated inputs), IMPLY (a 2-input OR with the bfiyrs 1t input negated), and XOR-WITH-OUTPUT (an XOR factor with the last input negated). [sent-126, score-0.593]
</p><p>32 −  In sum-product BP, the messages take the form:3 Mi→C(yi) ∝ QD6=C MD→i(yi) (4) MC→i(yi) ∝ PQyC∼yiΨC(yC) Qj6=i Mj→C(yj). [sent-127, score-0.082]
</p><p>33 Upon convergence, variable and factor beliefs are computed as: τi (yi) ∝ QC MC→i (yi) (6) τC(yC) ∝ ΨQC(yC) Qi Mi→C(yi). [sent-130, score-0.244]
</p><p>34 (7) BP is exact when the factor gQraph is a tree: in the sum-product case, the beliefs in Eqs. [sent-131, score-0.216]
</p><p>35 6–7 correspond 3We employ the standard ∼ notation, where a summa-  tion/Wmaex iemmipzlaotiyon t hined setxaendd by yC ∼ yi means trheat a ait s uism over all yC with the i-th component held∼ ∼fixe yd and set to yi. [sent-132, score-0.203]
</p><p>36 In graphs with loops, BP is an approximate method, not guaranteed to converge, nicknamed loopy BP. [sent-134, score-0.258]
</p><p>37 We highlight a variational perspective of loopy BP in §3; fhoirg now we acroinatsiiodnearl algorithmic oisfs luoeosp. [sent-135, score-0.278]
</p><p>38 N BoPte in th §3at; computing the factor-to-variable messages for each factor C (Eq. [sent-136, score-0.257]
</p><p>39 Fortunately, for all the hard constraint factors in rows 3–5 of Table 1, this computation can be done in linear time (and polynomial for the TREE factor)—this extends results presented in Smith and Eisner (2008). [sent-138, score-0.25]
</p><p>40 4 4The insight behind these speed-ups is that messages on binary-valued potentials can be expressed as MC→i (yi) ∝ SIB  SIB  SIB  Figure 1: Factor graph corresponding to the dependency parsing model of Smith and Eisner (2008) with sibling and grandparent features. [sent-139, score-0.338]
</p><p>41 Circles denote variable nodes, and squares denote factor nodes. [sent-140, score-0.203]
</p><p>42 Note the loops created by the inclusion of pairwise factors (GRAND and SIB). [sent-141, score-0.296]
</p><p>43 Wt hee fnaemxti present an alternative parametrization for the distributions in Px in terms of factor marginals. [sent-149, score-0.235]
</p><p>44 We will see that each distribution can be seen as a point in the socalled marginal polytope (Wainwright and Jordan, 2008); this will pave the way for the variational representations to be derived next. [sent-150, score-0.434]
</p><p>45 A part is a pair hC, yCi, where C is a soft factor and yC a partial output assignQment. [sent-152, score-0.221]
</p><p>46 The marginal polytope is the convex hull5 of all the “valid” output indicator vectors: M(Gx) conv{χ(y) | y ∈ Y(x)}. [sent-167, score-0.368]
</p><p>47 Note that M(Gx) only depends on the factor graph Gx and the hard constraints (i. [sent-168, score-0.323]
</p><p>48 The following variational representation for the log-partition function (mentioned in Eq. [sent-190, score-0.128]
</p><p>49 ,P  µ  Par met r�spaceFactors�plaocge-p� o� t e�n tials�Marginal�polytope Figure 2: Dual parametrization of the distributions in Px. [sent-197, score-0.06]
</p><p>50 Our parameter space (left) is first linearly mapped to the space of factor log-potentials (middle). [sent-198, score-0.175]
</p><p>51 The latter is mapped to the marginal polytope M(Gx) (right). [sent-199, score-0.306]
</p><p>52 9 is convex and its solution is attained at the factor marginals, i. [sent-204, score-0.237]
</p><p>53 4  Approximate Inference & Turbo Parsing  We now show how the variational machinery just described relates to message-passing algorithms and provides a common framework for analyzing two recent dependency parsers. [sent-217, score-0.181]
</p><p>54 1 Loopy BP as a Variational Approximation For general factor graphs with loops, the marginal polytope M(Gx) cannot be compactly specified and the entropy term H(µ) lacks a closed form, rendering exact optimizations in Eqs. [sent-221, score-0.529]
</p><p>55 A popular approximate algorithm for marginal inference is sum-product loopy BP, which passes messages as described in §2 and, upon convergence, computes dbeeslicerfisb evdia Eqs. [sent-223, score-0.463]
</p><p>56 W, uerpeo loopy vBePrg exact, these beliefs would be the true marginals and hence a point in the marginal polytope M(Gx) . [sent-225, score-0.602]
</p><p>57 (2001) and others, who first analyzed loopy BP from a variational perspective. [sent-227, score-0.278]
</p><p>58 The following two approximations underlie loopy BP: •  The marginal polytope M(Gx) is approximated by tThhee local polytope tLo (Gx). [sent-228, score-0.75]
</p><p>59 TGhis is an outer bound; its name derives from the fact that it only imposes local agreement constraints ∀i, yi ∈ Yi, C ∈ C:  ,  Pyi τi(yi) = 1, PyC∼yi τC(yC) = τi(yi). [sent-229, score-0.327]
</p><p>60 The entropy H is replacPed by its Bethe approxiTmhaeti eonnt HBethe(τ) p,la (1 di)H(τi) + PC∈C H(τC), where di P= |{C | −i ∈ C} | is the nPumC∈bCer of factors connePc=ted | tCo t |h ei ∈ith C variable, H(Pτi) −Pyi τi (yi) log τi (yi) and H(τC) −  ,  PedIi =b1y  PyC τC(yCP) logτC(yC). [sent-236, score-0.197]
</p><p>61 −  ,  Any Pstationary point of sum-product BP is a local optimum of the variational problem in Eq. [sent-237, score-0.172]
</p><p>62 Table 1 shows closed form expressions for the local agreement constraints and entropies of some hard-constraint factors, obtained by invoking Eq. [sent-241, score-0.169]
</p><p>63 2 Two Dependency Turbo Parsers We next present our main contribution: a formal connection between two recent approximate dependency parsers, which at first sight appear unrelated. [sent-246, score-0.113]
</p><p>64 Recall that (i) Smith and Eisner (2008) proposed a factor graph (Fig. [sent-247, score-0.241]
</p><p>65 1) in which they run loopy BP, and that (ii) Martins et al. [sent-248, score-0.15]
</p><p>66 (2009) approximate parsing as the solution of a linear program. [sent-249, score-0.09]
</p><p>67 Here, we fill the blanks in the two approaches: we derive explicitly the variational problem addressed in (i) and we provide the underlying factor graph in (ii). [sent-250, score-0.4]
</p><p>68 This puts the two approaches side-by-side as approximate  methods for marginal and MAP inference. [sent-251, score-0.171]
</p><p>69 11) that ignore the loops in their graphical models, we dub them turbo parsers by analogy with error-correcting turbo decoders (see footnote 1). [sent-253, score-0.416]
</p><p>70 1—call it Gx—includes pairwise soft factors connecting sibling and grandparent arcs. [sent-256, score-0.332]
</p><p>71 6 We next characterize the local polytope L (Gx) and the Bethe approximation HBethe inherent in Smith and Eisner’s loopy BP algorithm. [sent-257, score-0.389]
</p><p>72 Let A be the set of candidate arcs, and P A2 the set of pairs of arcs that have factors. [sent-258, score-0.075]
</p><p>73 Siinc we tahll vτariab=les h are binary, we may write, for each a ∈ A, τa(1) = za and τa(0) = 1w za, owrh eearceh za i s∈ a Ava,r τiable constrained to [0, 1]. [sent-260, score-0.432]
</p><p>74 L1e −t zA hzaia∈A; the local agreement constraints at the TREE zfacitor (see Table 1) are written as zA ∈ Ztree(x), where Ztree(x) is the arborescence polytope, i. [sent-261, score-0.124]
</p><p>75 , the convex hull of all incidence vectors of dependency trees (Martins et al. [sent-263, score-0.145]
</p><p>76 The approximate  vPariational expression becomes log Zx (θ) ≈  maxz  θ>F(x)z + Htree(zA)  −X Ia;b(za,zb,zab) haX,bi ∈P  s. [sent-270, score-0.089]
</p><p>77 zab ≤ za, zab ≥≤ za  +  zab ≤ zb, zb −≤ 1, ∀ha, bi ∈ P,  τab(1, 1) = zab, τab(0, 0) = 1− za − zb + zab τab(1, 0) = za − zab, τab(0, 1) = zb − zab. [sent-272, score-1.455]
</p><p>78 onW ofe M noarw-  Ztree(x),( d12e-)  inequalities which, along with zA ∈ fine the local polytope L(Gx). [sent-275, score-0.239]
</p><p>79 As fo∈r t Zhe factor entropies, start by noting that the TREE-factor entropy Htree can be obtained in closed form by computing the marginals zA and the partition function Zx (θ) (via the matrix-tree theorem) and recalling the variational representation in Eq. [sent-276, score-0.437]
</p><p>80 We next construct a factor graph G0x and show that the LP relaxation corresponds to an optimization of the form in Eq. [sent-282, score-0.281]
</p><p>81 10, with the marginal polytope M(G0x) replaced by L (G0x). [sent-283, score-0.306]
</p><p>82 G0x includes the following auxiliary variable nodes: path variables hpijii=0,. [sent-284, score-0.093]
</p><p>83 ,n, which innoddiceas:te pwahtheth vearr iwabolreds j pdesicends from iin the dependency tree, and flow variables hfakia∈A,k=1,. [sent-290, score-0.175]
</p><p>84 ,n, pwehnicdhen ecvya ltureaete, to 1 f oifwf arc a l“ecsar hrfiesi flow” to k, i. [sent-293, score-0.18]
</p><p>85 , iff there is a path from the root to k that passes through a. [sent-295, score-0.065]
</p><p>86 , any word descends from the root and from itself, and arcs leaving a word carry no flow to that word. [sent-298, score-0.162]
</p><p>87 This can be done with unary hard constraint factors. [sent-299, score-0.082]
</p><p>88 3: • O(n) XOR factors, each connecting all arc variaOb(lnes) of the form {hh, mi }h=0,. [sent-302, score-0.235]
</p><p>89 Each factor p0k  yields a local agreement constraint (see Table 1):  Phn=0 zhh,mi = 1, •  m∈  {1,  . [sent-307, score-0.299]
</p><p>90 , n}  (16)  O(nP3) IMPLY factors, each expressing that if an arc ncarries flow, then that arc must be active. [sent-310, score-0.36]
</p><p>91 Such factors are OR factors with the first input negated, hence, the local agreement constraints are: ≤ za, a ∈ A, k ∈ {1, . [sent-311, score-0.46]
</p><p>92 (17) O(n2) XOR-WITH-OUTPUT factors, which impose the constraint that each path variable pmk is active if and only if exactly one incoming arc in {hh, mi }h=0,. [sent-315, score-0.399]
</p><p>93 Such factors are XOR factors with the last input negated, and hence their local constraints are:  fak  •  pmk •  = Phn=0 fhkh,mi, m, k ∈ {1,  . [sent-319, score-0.456]
</p><p>94 , n}  (18)  O(n2) XOPR-WITH-OUTPUT factors to impose the cOo(nnstraint that words don’t consume other words’ commodities; i. [sent-322, score-0.168]
</p><p>95 he to fk h hif6 f= exactly one outgoing arc in {hh, mi }m=1,. [sent-325, score-0.262]
</p><p>96 ,n carries flow to k:  =  phk  =  = Pnm=1 fhkh,mi, h, k ∈ {0, . [sent-328, score-0.087]
</p><p>97 (2009) in their multi-commodity flow model, for the configuration with siblings and grandparent features. [sent-340, score-0.169]
</p><p>98 7 They also considered a configuration with non-projectivity features—which fire if an arc is non-projective. [sent-341, score-0.212]
</p><p>99 8 That configuration can also be obtained here if variables {nhh,mi } are can  also  be  ob  7To be precise, the constraints of Martins et al. [sent-342, score-0.108]
</p><p>100 8An arc hh, mi is non-projective if there is some word in its span Anont a descending sf nroomn- phr (Kahane ef tt haelr. [sent-345, score-0.235]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('yc', 0.42), ('gx', 0.39), ('za', 0.216), ('yi', 0.203), ('polytope', 0.195), ('arc', 0.18), ('factor', 0.175), ('factors', 0.168), ('bp', 0.163), ('martins', 0.152), ('loopy', 0.15), ('turbo', 0.14), ('pr', 0.13), ('variational', 0.128), ('zab', 0.123), ('marginal', 0.111), ('zb', 0.105), ('marginals', 0.105), ('eisner', 0.103), ('loops', 0.09), ('xor', 0.09), ('hbethe', 0.088), ('flow', 0.087), ('messages', 0.082), ('hh', 0.075), ('negated', 0.075), ('arcs', 0.075), ('mc', 0.075), ('smith', 0.072), ('htree', 0.07), ('instituto', 0.07), ('sib', 0.07), ('yci', 0.07), ('ztree', 0.07), ('zx', 0.068), ('px', 0.068), ('graph', 0.066), ('convex', 0.062), ('parametrization', 0.06), ('hc', 0.06), ('approximate', 0.06), ('mi', 0.055), ('approximations', 0.055), ('ab', 0.055), ('dependency', 0.053), ('bethe', 0.053), ('grandparent', 0.05), ('graphs', 0.048), ('parsers', 0.046), ('soft', 0.046), ('entropies', 0.045), ('pyc', 0.045), ('local', 0.044), ('qi', 0.044), ('hard', 0.041), ('constraint', 0.041), ('qc', 0.041), ('beliefs', 0.041), ('constraints', 0.041), ('relaxation', 0.04), ('agreement', 0.039), ('pairwise', 0.038), ('belief', 0.038), ('aguiar', 0.035), ('csoft', 0.035), ('ecnico', 0.035), ('lisboa', 0.035), ('logzx', 0.035), ('phn', 0.035), ('pmk', 0.035), ('portugal', 0.035), ('pyi', 0.035), ('yedidia', 0.035), ('passes', 0.035), ('variables', 0.035), ('propagation', 0.033), ('configuration', 0.032), ('underlying', 0.031), ('map', 0.03), ('incoming', 0.03), ('hxe', 0.03), ('argmaxy', 0.03), ('hull', 0.03), ('family', 0.03), ('sibling', 0.03), ('path', 0.03), ('parsing', 0.03), ('partition', 0.029), ('log', 0.029), ('lp', 0.029), ('ia', 0.029), ('tree', 0.028), ('variable', 0.028), ('ex', 0.027), ('ey', 0.027), ('potentials', 0.027), ('outgoing', 0.027), ('crfs', 0.026), ('inference', 0.025), ('inventory', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="110-tfidf-1" href="./emnlp-2010-Turbo_Parsers%3A_Dependency_Parsing_by_Approximate_Variational_Inference.html">110 emnlp-2010-Turbo Parsers: Dependency Parsing by Approximate Variational Inference</a></p>
<p>Author: Andre Martins ; Noah Smith ; Eric Xing ; Pedro Aguiar ; Mario Figueiredo</p><p>Abstract: We present a unified view of two state-of-theart non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al. (2009). By representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, includ- ing CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages.</p><p>2 0.14058644 <a title="110-tfidf-2" href="./emnlp-2010-On_Dual_Decomposition_and_Linear_Programming_Relaxations_for_Natural_Language_Processing.html">88 emnlp-2010-On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing</a></p>
<p>Author: Alexander M Rush ; David Sontag ; Michael Collins ; Tommi Jaakkola</p><p>Abstract: This paper introduces dual decomposition as a framework for deriving inference algorithms for NLP problems. The approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. The approach provably solves a linear programming (LP) relaxation of the global inference problem. It leads to algorithms that are simple, in that they use existing decoding algorithms; efficient, in that they avoid exact algorithms for the full model; and often exact, in that empirically they often recover the correct solution in spite of using an LP relaxation. We give experimental results on two problems: 1) the combination of two lexicalized parsing models; and 2) the combination of a lexicalized parsing model and a trigram part-of-speech tagger.</p><p>3 0.11566744 <a title="110-tfidf-3" href="./emnlp-2010-Dual_Decomposition_for_Parsing_with_Non-Projective_Head_Automata.html">38 emnlp-2010-Dual Decomposition for Parsing with Non-Projective Head Automata</a></p>
<p>Author: Terry Koo ; Alexander M. Rush ; Michael Collins ; Tommi Jaakkola ; David Sontag</p><p>Abstract: This paper introduces algorithms for nonprojective parsing based on dual decomposition. We focus on parsing algorithms for nonprojective head automata, a generalization of head-automata models to non-projective structures. The dual decomposition algorithms are simple and efficient, relying on standard dynamic programming and minimum spanning tree algorithms. They provably solve an LP relaxation of the non-projective parsing problem. Empirically the LP relaxation is very often tight: for many languages, exact solutions are achieved on over 98% of test sentences. The accuracy of our models is higher than previous work on a broad range of datasets.</p><p>4 0.099147573 <a title="110-tfidf-4" href="./emnlp-2010-Efficient_Graph-Based_Semi-Supervised_Learning_of_Structured_Tagging_Models.html">41 emnlp-2010-Efficient Graph-Based Semi-Supervised Learning of Structured Tagging Models</a></p>
<p>Author: Amarnag Subramanya ; Slav Petrov ; Fernando Pereira</p><p>Abstract: We describe a new scalable algorithm for semi-supervised training of conditional random fields (CRF) and its application to partof-speech (POS) tagging. The algorithm uses a similarity graph to encourage similar ngrams to have similar POS tags. We demonstrate the efficacy of our approach on a domain adaptation task, where we assume that we have access to large amounts of unlabeled data from the target domain, but no additional labeled data. The similarity graph is used during training to smooth the state posteriors on the target domain. Standard inference can be used at test time. Our approach is able to scale to very large problems and yields significantly improved target domain accuracy.</p><p>5 0.073010519 <a title="110-tfidf-5" href="./emnlp-2010-Joint_Training_and_Decoding_Using_Virtual_Nodes_for_Cascaded_Segmentation_and_Tagging_Tasks.html">69 emnlp-2010-Joint Training and Decoding Using Virtual Nodes for Cascaded Segmentation and Tagging Tasks</a></p>
<p>Author: Xian Qian ; Qi Zhang ; Yaqian Zhou ; Xuanjing Huang ; Lide Wu</p><p>Abstract: Many sequence labeling tasks in NLP require solving a cascade of segmentation and tagging subtasks, such as Chinese POS tagging, named entity recognition, and so on. Traditional pipeline approaches usually suffer from error propagation. Joint training/decoding in the cross-product state space could cause too many parameters and high inference complexity. In this paper, we present a novel method which integrates graph structures of two subtasks into one using virtual nodes, and performs joint training and decoding in the factorized state space. Experimental evaluations on CoNLL 2000 shallow parsing data set and Fourth SIGHAN Bakeoff CTB POS tagging data set demonstrate the superiority of our method over cross-product, pipeline and candidate reranking approaches.</p><p>6 0.072276451 <a title="110-tfidf-6" href="./emnlp-2010-Using_Universal_Linguistic_Knowledge_to_Guide_Grammar_Induction.html">116 emnlp-2010-Using Universal Linguistic Knowledge to Guide Grammar Induction</a></p>
<p>7 0.061052695 <a title="110-tfidf-7" href="./emnlp-2010-A_Latent_Variable_Model_for_Geographic_Lexical_Variation.html">6 emnlp-2010-A Latent Variable Model for Geographic Lexical Variation</a></p>
<p>8 0.058387265 <a title="110-tfidf-8" href="./emnlp-2010-Discriminative_Word_Alignment_with_a_Function_Word_Reordering_Model.html">36 emnlp-2010-Discriminative Word Alignment with a Function Word Reordering Model</a></p>
<p>9 0.047665726 <a title="110-tfidf-9" href="./emnlp-2010-Incorporating_Content_Structure_into_Text_Analysis_Applications.html">64 emnlp-2010-Incorporating Content Structure into Text Analysis Applications</a></p>
<p>10 0.046528596 <a title="110-tfidf-10" href="./emnlp-2010-Uptraining_for_Accurate_Deterministic_Question_Parsing.html">115 emnlp-2010-Uptraining for Accurate Deterministic Question Parsing</a></p>
<p>11 0.046294346 <a title="110-tfidf-11" href="./emnlp-2010-Top-Down_Nearly-Context-Sensitive_Parsing.html">106 emnlp-2010-Top-Down Nearly-Context-Sensitive Parsing</a></p>
<p>12 0.045906611 <a title="110-tfidf-12" href="./emnlp-2010-Evaluating_the_Impact_of_Alternative_Dependency_Graph_Encodings_on_Solving_Event_Extraction_Tasks.html">46 emnlp-2010-Evaluating the Impact of Alternative Dependency Graph Encodings on Solving Event Extraction Tasks</a></p>
<p>13 0.043615907 <a title="110-tfidf-13" href="./emnlp-2010-It_Depends_on_the_Translation%3A_Unsupervised_Dependency_Parsing_via_Word_Alignment.html">67 emnlp-2010-It Depends on the Translation: Unsupervised Dependency Parsing via Word Alignment</a></p>
<p>14 0.039358307 <a title="110-tfidf-14" href="./emnlp-2010-Confidence_in_Structured-Prediction_Using_Confidence-Weighted_Models.html">30 emnlp-2010-Confidence in Structured-Prediction Using Confidence-Weighted Models</a></p>
<p>15 0.038287055 <a title="110-tfidf-15" href="./emnlp-2010-Automatic_Evaluation_of_Translation_Quality_for_Distant_Language_Pairs.html">22 emnlp-2010-Automatic Evaluation of Translation Quality for Distant Language Pairs</a></p>
<p>16 0.037448775 <a title="110-tfidf-16" href="./emnlp-2010-Mining_Name_Translations_from_Entity_Graph_Mapping.html">79 emnlp-2010-Mining Name Translations from Entity Graph Mapping</a></p>
<p>17 0.033522319 <a title="110-tfidf-17" href="./emnlp-2010-Soft_Syntactic_Constraints_for_Hierarchical_Phrase-Based_Translation_Using_Latent_Syntactic_Distributions.html">98 emnlp-2010-Soft Syntactic Constraints for Hierarchical Phrase-Based Translation Using Latent Syntactic Distributions</a></p>
<p>18 0.033465024 <a title="110-tfidf-18" href="./emnlp-2010-Simple_Type-Level_Unsupervised_POS_Tagging.html">97 emnlp-2010-Simple Type-Level Unsupervised POS Tagging</a></p>
<p>19 0.033056132 <a title="110-tfidf-19" href="./emnlp-2010-Two_Decades_of_Unsupervised_POS_Induction%3A_How_Far_Have_We_Come%3F.html">111 emnlp-2010-Two Decades of Unsupervised POS Induction: How Far Have We Come?</a></p>
<p>20 0.032578804 <a title="110-tfidf-20" href="./emnlp-2010-Learning_the_Relative_Usefulness_of_Questions_in_Community_QA.html">74 emnlp-2010-Learning the Relative Usefulness of Questions in Community QA</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.136), (1, 0.069), (2, 0.098), (3, -0.045), (4, 0.007), (5, 0.103), (6, -0.003), (7, 0.169), (8, -0.019), (9, -0.067), (10, -0.209), (11, 0.127), (12, -0.116), (13, -0.058), (14, -0.01), (15, 0.012), (16, -0.022), (17, -0.009), (18, 0.064), (19, -0.063), (20, -0.014), (21, -0.026), (22, -0.042), (23, -0.094), (24, 0.03), (25, -0.02), (26, 0.029), (27, 0.045), (28, -0.128), (29, -0.057), (30, -0.083), (31, 0.042), (32, 0.152), (33, -0.008), (34, -0.03), (35, -0.097), (36, -0.099), (37, -0.077), (38, 0.055), (39, -0.035), (40, 0.16), (41, -0.165), (42, 0.264), (43, -0.033), (44, 0.063), (45, 0.11), (46, -0.12), (47, -0.16), (48, -0.218), (49, 0.074)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97808248 <a title="110-lsi-1" href="./emnlp-2010-Turbo_Parsers%3A_Dependency_Parsing_by_Approximate_Variational_Inference.html">110 emnlp-2010-Turbo Parsers: Dependency Parsing by Approximate Variational Inference</a></p>
<p>Author: Andre Martins ; Noah Smith ; Eric Xing ; Pedro Aguiar ; Mario Figueiredo</p><p>Abstract: We present a unified view of two state-of-theart non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al. (2009). By representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, includ- ing CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages.</p><p>2 0.41449752 <a title="110-lsi-2" href="./emnlp-2010-On_Dual_Decomposition_and_Linear_Programming_Relaxations_for_Natural_Language_Processing.html">88 emnlp-2010-On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing</a></p>
<p>Author: Alexander M Rush ; David Sontag ; Michael Collins ; Tommi Jaakkola</p><p>Abstract: This paper introduces dual decomposition as a framework for deriving inference algorithms for NLP problems. The approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. The approach provably solves a linear programming (LP) relaxation of the global inference problem. It leads to algorithms that are simple, in that they use existing decoding algorithms; efficient, in that they avoid exact algorithms for the full model; and often exact, in that empirically they often recover the correct solution in spite of using an LP relaxation. We give experimental results on two problems: 1) the combination of two lexicalized parsing models; and 2) the combination of a lexicalized parsing model and a trigram part-of-speech tagger.</p><p>3 0.39455456 <a title="110-lsi-3" href="./emnlp-2010-Efficient_Graph-Based_Semi-Supervised_Learning_of_Structured_Tagging_Models.html">41 emnlp-2010-Efficient Graph-Based Semi-Supervised Learning of Structured Tagging Models</a></p>
<p>Author: Amarnag Subramanya ; Slav Petrov ; Fernando Pereira</p><p>Abstract: We describe a new scalable algorithm for semi-supervised training of conditional random fields (CRF) and its application to partof-speech (POS) tagging. The algorithm uses a similarity graph to encourage similar ngrams to have similar POS tags. We demonstrate the efficacy of our approach on a domain adaptation task, where we assume that we have access to large amounts of unlabeled data from the target domain, but no additional labeled data. The similarity graph is used during training to smooth the state posteriors on the target domain. Standard inference can be used at test time. Our approach is able to scale to very large problems and yields significantly improved target domain accuracy.</p><p>4 0.36013904 <a title="110-lsi-4" href="./emnlp-2010-Dual_Decomposition_for_Parsing_with_Non-Projective_Head_Automata.html">38 emnlp-2010-Dual Decomposition for Parsing with Non-Projective Head Automata</a></p>
<p>Author: Terry Koo ; Alexander M. Rush ; Michael Collins ; Tommi Jaakkola ; David Sontag</p><p>Abstract: This paper introduces algorithms for nonprojective parsing based on dual decomposition. We focus on parsing algorithms for nonprojective head automata, a generalization of head-automata models to non-projective structures. The dual decomposition algorithms are simple and efficient, relying on standard dynamic programming and minimum spanning tree algorithms. They provably solve an LP relaxation of the non-projective parsing problem. Empirically the LP relaxation is very often tight: for many languages, exact solutions are achieved on over 98% of test sentences. The accuracy of our models is higher than previous work on a broad range of datasets.</p><p>5 0.29545391 <a title="110-lsi-5" href="./emnlp-2010-Word_Sense_Induction__Disambiguation_Using_Hierarchical_Random_Graphs.html">124 emnlp-2010-Word Sense Induction  Disambiguation Using Hierarchical Random Graphs</a></p>
<p>Author: Ioannis Klapaftis ; Suresh Manandhar</p><p>Abstract: Graph-based methods have gained attention in many areas of Natural Language Processing (NLP) including Word Sense Disambiguation (WSD), text summarization, keyword extraction and others. Most of the work in these areas formulate their problem in a graph-based setting and apply unsupervised graph clustering to obtain a set of clusters. Recent studies suggest that graphs often exhibit a hierarchical structure that goes beyond simple flat clustering. This paper presents an unsupervised method for inferring the hierarchical grouping of the senses of a polysemous word. The inferred hierarchical structures are applied to the problem of word sense disambiguation, where we show that our method performs sig- nificantly better than traditional graph-based methods and agglomerative clustering yielding improvements over state-of-the-art WSD systems based on sense induction.</p><p>6 0.26770234 <a title="110-lsi-6" href="./emnlp-2010-Mining_Name_Translations_from_Entity_Graph_Mapping.html">79 emnlp-2010-Mining Name Translations from Entity Graph Mapping</a></p>
<p>7 0.25557297 <a title="110-lsi-7" href="./emnlp-2010-A_Latent_Variable_Model_for_Geographic_Lexical_Variation.html">6 emnlp-2010-A Latent Variable Model for Geographic Lexical Variation</a></p>
<p>8 0.2444118 <a title="110-lsi-8" href="./emnlp-2010-Using_Universal_Linguistic_Knowledge_to_Guide_Grammar_Induction.html">116 emnlp-2010-Using Universal Linguistic Knowledge to Guide Grammar Induction</a></p>
<p>9 0.24186821 <a title="110-lsi-9" href="./emnlp-2010-Automatic_Keyphrase_Extraction_via_Topic_Decomposition.html">23 emnlp-2010-Automatic Keyphrase Extraction via Topic Decomposition</a></p>
<p>10 0.2312701 <a title="110-lsi-10" href="./emnlp-2010-Confidence_in_Structured-Prediction_Using_Confidence-Weighted_Models.html">30 emnlp-2010-Confidence in Structured-Prediction Using Confidence-Weighted Models</a></p>
<p>11 0.22644864 <a title="110-lsi-11" href="./emnlp-2010-Joint_Training_and_Decoding_Using_Virtual_Nodes_for_Cascaded_Segmentation_and_Tagging_Tasks.html">69 emnlp-2010-Joint Training and Decoding Using Virtual Nodes for Cascaded Segmentation and Tagging Tasks</a></p>
<p>12 0.17786571 <a title="110-lsi-12" href="./emnlp-2010-Evaluating_the_Impact_of_Alternative_Dependency_Graph_Encodings_on_Solving_Event_Extraction_Tasks.html">46 emnlp-2010-Evaluating the Impact of Alternative Dependency Graph Encodings on Solving Event Extraction Tasks</a></p>
<p>13 0.16782829 <a title="110-lsi-13" href="./emnlp-2010-Using_Unknown_Word_Techniques_to_Learn_Known_Words.html">117 emnlp-2010-Using Unknown Word Techniques to Learn Known Words</a></p>
<p>14 0.16074206 <a title="110-lsi-14" href="./emnlp-2010-Automatic_Evaluation_of_Translation_Quality_for_Distant_Language_Pairs.html">22 emnlp-2010-Automatic Evaluation of Translation Quality for Distant Language Pairs</a></p>
<p>15 0.15915965 <a title="110-lsi-15" href="./emnlp-2010-Discriminative_Word_Alignment_with_a_Function_Word_Reordering_Model.html">36 emnlp-2010-Discriminative Word Alignment with a Function Word Reordering Model</a></p>
<p>16 0.15843333 <a title="110-lsi-16" href="./emnlp-2010-Discriminative_Sample_Selection_for_Statistical_Machine_Translation.html">35 emnlp-2010-Discriminative Sample Selection for Statistical Machine Translation</a></p>
<p>17 0.14838232 <a title="110-lsi-17" href="./emnlp-2010-Uptraining_for_Accurate_Deterministic_Question_Parsing.html">115 emnlp-2010-Uptraining for Accurate Deterministic Question Parsing</a></p>
<p>18 0.14578579 <a title="110-lsi-18" href="./emnlp-2010-Utilizing_Extra-Sentential_Context_for_Parsing.html">118 emnlp-2010-Utilizing Extra-Sentential Context for Parsing</a></p>
<p>19 0.14028271 <a title="110-lsi-19" href="./emnlp-2010-Unsupervised_Induction_of_Tree_Substitution_Grammars_for_Dependency_Parsing.html">113 emnlp-2010-Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing</a></p>
<p>20 0.1401263 <a title="110-lsi-20" href="./emnlp-2010-Hashing-Based_Approaches_to_Spelling_Correction_of_Personal_Names.html">56 emnlp-2010-Hashing-Based Approaches to Spelling Correction of Personal Names</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.01), (10, 0.014), (12, 0.031), (14, 0.023), (29, 0.096), (30, 0.012), (32, 0.017), (45, 0.293), (52, 0.034), (56, 0.072), (60, 0.028), (62, 0.076), (66, 0.052), (72, 0.034), (76, 0.014), (77, 0.018), (79, 0.016), (82, 0.015), (87, 0.023), (89, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83873916 <a title="110-lda-1" href="./emnlp-2010-Turbo_Parsers%3A_Dependency_Parsing_by_Approximate_Variational_Inference.html">110 emnlp-2010-Turbo Parsers: Dependency Parsing by Approximate Variational Inference</a></p>
<p>Author: Andre Martins ; Noah Smith ; Eric Xing ; Pedro Aguiar ; Mario Figueiredo</p><p>Abstract: We present a unified view of two state-of-theart non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al. (2009). By representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, includ- ing CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages.</p><p>2 0.53727132 <a title="110-lda-2" href="./emnlp-2010-Extracting_Opinion_Targets_in_a_Single_and_Cross-Domain_Setting_with_Conditional_Random_Fields.html">49 emnlp-2010-Extracting Opinion Targets in a Single and Cross-Domain Setting with Conditional Random Fields</a></p>
<p>Author: Niklas Jakob ; Iryna Gurevych</p><p>Abstract: In this paper, we focus on the opinion target extraction as part of the opinion mining task. We model the problem as an information extraction task, which we address based on Conditional Random Fields (CRF). As a baseline we employ the supervised algorithm by Zhuang et al. (2006), which represents the state-of-the-art on the employed data. We evaluate the algorithms comprehensively on datasets from four different domains annotated with individual opinion target instances on a sentence level. Furthermore, we investigate the performance of our CRF-based approach and the baseline in a single- and cross-domain opinion target extraction setting. Our CRF-based approach improves the performance by 0.077, 0.126, 0.071 and 0. 178 regarding F-Measure in the single-domain extraction in the four domains. In the crossdomain setting our approach improves the performance by 0.409, 0.242, 0.294 and 0.343 regarding F-Measure over the baseline.</p><p>3 0.46269873 <a title="110-lda-3" href="./emnlp-2010-Dual_Decomposition_for_Parsing_with_Non-Projective_Head_Automata.html">38 emnlp-2010-Dual Decomposition for Parsing with Non-Projective Head Automata</a></p>
<p>Author: Terry Koo ; Alexander M. Rush ; Michael Collins ; Tommi Jaakkola ; David Sontag</p><p>Abstract: This paper introduces algorithms for nonprojective parsing based on dual decomposition. We focus on parsing algorithms for nonprojective head automata, a generalization of head-automata models to non-projective structures. The dual decomposition algorithms are simple and efficient, relying on standard dynamic programming and minimum spanning tree algorithms. They provably solve an LP relaxation of the non-projective parsing problem. Empirically the LP relaxation is very often tight: for many languages, exact solutions are achieved on over 98% of test sentences. The accuracy of our models is higher than previous work on a broad range of datasets.</p><p>4 0.44705677 <a title="110-lda-4" href="./emnlp-2010-Learning_First-Order_Horn_Clauses_from_Web_Text.html">72 emnlp-2010-Learning First-Order Horn Clauses from Web Text</a></p>
<p>Author: Stefan Schoenmackers ; Jesse Davis ; Oren Etzioni ; Daniel Weld</p><p>Abstract: input. Even the entire Web corpus does not explicitly answer all questions, yet inference can uncover many implicit answers. But where do inference rules come from? This paper investigates the problem of learning inference rules from Web text in an unsupervised, domain-independent manner. The SHERLOCK system, described herein, is a first-order learner that acquires over 30,000 Horn clauses from Web text. SHERLOCK embodies several innovations, including a novel rule scoring function based on Statistical Relevance (Salmon et al., 1971) which is effective on ambiguous, noisy and incomplete Web extractions. Our experiments show that inference over the learned rules discovers three times as many facts (at precision 0.8) as the TEXTRUNNER system which merely extracts facts explicitly stated in Web text.</p><p>5 0.44060978 <a title="110-lda-5" href="./emnlp-2010-On_Dual_Decomposition_and_Linear_Programming_Relaxations_for_Natural_Language_Processing.html">88 emnlp-2010-On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing</a></p>
<p>Author: Alexander M Rush ; David Sontag ; Michael Collins ; Tommi Jaakkola</p><p>Abstract: This paper introduces dual decomposition as a framework for deriving inference algorithms for NLP problems. The approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. The approach provably solves a linear programming (LP) relaxation of the global inference problem. It leads to algorithms that are simple, in that they use existing decoding algorithms; efficient, in that they avoid exact algorithms for the full model; and often exact, in that empirically they often recover the correct solution in spite of using an LP relaxation. We give experimental results on two problems: 1) the combination of two lexicalized parsing models; and 2) the combination of a lexicalized parsing model and a trigram part-of-speech tagger.</p><p>6 0.42061818 <a title="110-lda-6" href="./emnlp-2010-Positional_Language_Models_for_Clinical_Information_Retrieval.html">90 emnlp-2010-Positional Language Models for Clinical Information Retrieval</a></p>
<p>7 0.4188745 <a title="110-lda-7" href="./emnlp-2010-A_Latent_Variable_Model_for_Geographic_Lexical_Variation.html">6 emnlp-2010-A Latent Variable Model for Geographic Lexical Variation</a></p>
<p>8 0.41528705 <a title="110-lda-8" href="./emnlp-2010-Context_Comparison_of_Bursty_Events_in_Web_Search_and_Online_Media.html">32 emnlp-2010-Context Comparison of Bursty Events in Web Search and Online Media</a></p>
<p>9 0.41069037 <a title="110-lda-9" href="./emnlp-2010-Title_Generation_with_Quasi-Synchronous_Grammar.html">105 emnlp-2010-Title Generation with Quasi-Synchronous Grammar</a></p>
<p>10 0.40826517 <a title="110-lda-10" href="./emnlp-2010-Assessing_Phrase-Based_Translation_Models_with_Oracle_Decoding.html">18 emnlp-2010-Assessing Phrase-Based Translation Models with Oracle Decoding</a></p>
<p>11 0.40406257 <a title="110-lda-11" href="./emnlp-2010-Multi-Document_Summarization_Using_A%2A_Search_and_Discriminative_Learning.html">82 emnlp-2010-Multi-Document Summarization Using A* Search and Discriminative Learning</a></p>
<p>12 0.40102813 <a title="110-lda-12" href="./emnlp-2010-Soft_Syntactic_Constraints_for_Hierarchical_Phrase-Based_Translation_Using_Latent_Syntactic_Distributions.html">98 emnlp-2010-Soft Syntactic Constraints for Hierarchical Phrase-Based Translation Using Latent Syntactic Distributions</a></p>
<p>13 0.40034363 <a title="110-lda-13" href="./emnlp-2010-Using_Universal_Linguistic_Knowledge_to_Guide_Grammar_Induction.html">116 emnlp-2010-Using Universal Linguistic Knowledge to Guide Grammar Induction</a></p>
<p>14 0.39956698 <a title="110-lda-14" href="./emnlp-2010-Hierarchical_Phrase-Based_Translation_Grammars_Extracted_from_Alignment_Posterior_Probabilities.html">57 emnlp-2010-Hierarchical Phrase-Based Translation Grammars Extracted from Alignment Posterior Probabilities</a></p>
<p>15 0.39891627 <a title="110-lda-15" href="./emnlp-2010-Minimum_Error_Rate_Training_by_Sampling_the_Translation_Lattice.html">78 emnlp-2010-Minimum Error Rate Training by Sampling the Translation Lattice</a></p>
<p>16 0.39725348 <a title="110-lda-16" href="./emnlp-2010-Towards_Conversation_Entailment%3A_An_Empirical_Investigation.html">107 emnlp-2010-Towards Conversation Entailment: An Empirical Investigation</a></p>
<p>17 0.39340484 <a title="110-lda-17" href="./emnlp-2010-Inducing_Probabilistic_CCG_Grammars_from_Logical_Form_with_Higher-Order_Unification.html">65 emnlp-2010-Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification</a></p>
<p>18 0.39108455 <a title="110-lda-18" href="./emnlp-2010-Joint_Training_and_Decoding_Using_Virtual_Nodes_for_Cascaded_Segmentation_and_Tagging_Tasks.html">69 emnlp-2010-Joint Training and Decoding Using Virtual Nodes for Cascaded Segmentation and Tagging Tasks</a></p>
<p>19 0.39073062 <a title="110-lda-19" href="./emnlp-2010-PEM%3A_A_Paraphrase_Evaluation_Metric_Exploiting_Parallel_Texts.html">89 emnlp-2010-PEM: A Paraphrase Evaluation Metric Exploiting Parallel Texts</a></p>
<p>20 0.39064509 <a title="110-lda-20" href="./emnlp-2010-It_Depends_on_the_Translation%3A_Unsupervised_Dependency_Parsing_via_Word_Alignment.html">67 emnlp-2010-It Depends on the Translation: Unsupervised Dependency Parsing via Word Alignment</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
