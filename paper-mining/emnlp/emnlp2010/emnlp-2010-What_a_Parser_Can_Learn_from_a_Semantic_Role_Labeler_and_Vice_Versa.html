<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>121 emnlp-2010-What a Parser Can Learn from a Semantic Role Labeler and Vice Versa</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2010" href="../home/emnlp2010_home.html">emnlp2010</a> <a title="emnlp-2010-121" href="#">emnlp2010-121</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>121 emnlp-2010-What a Parser Can Learn from a Semantic Role Labeler and Vice Versa</h1>
<br/><p>Source: <a title="emnlp-2010-121-pdf" href="http://aclweb.org/anthology//D/D10/D10-1072.pdf">pdf</a></p><p>Author: Stephen Boxwell ; Dennis Mehay ; Chris Brew</p><p>Abstract: In many NLP systems, there is a unidirectional flow of information in which a parser supplies input to a semantic role labeler. In this paper, we build a system that allows information to flow in both directions. We make use of semantic role predictions in choosing a single-best parse. This process relies on an averaged perceptron model to distinguish likely semantic roles from erroneous ones. Our system penalizes parses that give rise to low-scoring semantic roles. To explore the consequences of this we perform two experiments. First, we use a baseline generative model to produce n-best parses, which are then re-ordered by our semantic model. Second, we use a modified version of our semantic role labeler to predict semantic roles at parse time. The performance of this modified labeler is weaker than that of our best full SRL, because it is restricted to features that can be computed directly from the parser’s packed chart. For both experiments, the resulting semantic predictions are then used to select parses. Finally, we feed the selected parses produced by each experiment to the full version of our semantic role labeler. We find that SRL performance can be improved over this baseline by selecting parses with likely semantic roles.</p><p>Reference: <a title="emnlp-2010-121-reference" href="../emnlp2010_reference/emnlp-2010-What_a_Parser_Can_Learn_from_a_Semantic_Role_Labeler_and_Vice_Versa_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('np', 0.401), ('rol', 0.337), ('sem', 0.337), ('srl', 0.309), ('chart', 0.3), ('pars', 0.242), ('pack', 0.171), ('dcl', 0.155), ('ccg', 0.117), ('attach', 0.108), ('admir', 0.106), ('erron', 0.106), ('pcfg', 0.105), ('robin', 0.096), ('predict', 0.094), ('mehay', 0.093), ('mey', 0.088), ('nov', 0.079), ('nombank', 0.079), ('unpack', 0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="121-tfidf-1" href="./emnlp-2010-What_a_Parser_Can_Learn_from_a_Semantic_Role_Labeler_and_Vice_Versa.html">121 emnlp-2010-What a Parser Can Learn from a Semantic Role Labeler and Vice Versa</a></p>
<p>Author: Stephen Boxwell ; Dennis Mehay ; Chris Brew</p><p>Abstract: In many NLP systems, there is a unidirectional flow of information in which a parser supplies input to a semantic role labeler. In this paper, we build a system that allows information to flow in both directions. We make use of semantic role predictions in choosing a single-best parse. This process relies on an averaged perceptron model to distinguish likely semantic roles from erroneous ones. Our system penalizes parses that give rise to low-scoring semantic roles. To explore the consequences of this we perform two experiments. First, we use a baseline generative model to produce n-best parses, which are then re-ordered by our semantic model. Second, we use a modified version of our semantic role labeler to predict semantic roles at parse time. The performance of this modified labeler is weaker than that of our best full SRL, because it is restricted to features that can be computed directly from the parser’s packed chart. For both experiments, the resulting semantic predictions are then used to select parses. Finally, we feed the selected parses produced by each experiment to the full version of our semantic role labeler. We find that SRL performance can be improved over this baseline by selecting parses with likely semantic roles.</p><p>2 0.29528633 <a title="121-tfidf-2" href="./emnlp-2010-Joint_Inference_for_Bilingual_Semantic_Role_Labeling.html">68 emnlp-2010-Joint Inference for Bilingual Semantic Role Labeling</a></p>
<p>Author: Tao Zhuang ; Chengqing Zong</p><p>Abstract: We show that jointly performing semantic role labeling (SRL) on bitext can improve SRL results on both sides. In our approach, we use monolingual SRL systems to produce argument candidates for predicates in bitext at first. Then, we simultaneously generate SRL results for two sides of bitext using our joint inference model. Our model prefers the bilingual SRL result that is not only reasonable on each side of bitext, but also has more consistent argument structures between two sides. To evaluate the consistency between two argument structures, we also formulate a log-linear model to compute the probability of aligning two arguments. We have experimented with our model on Chinese-English parallel PropBank data. Using our joint inference model, F1 scores of SRL results on Chinese and English text achieve 79.53% and 77.87% respectively, which are 1.52 and 1.74 points higher than the results of baseline monolingual SRL combination systems respectively.</p><p>3 0.24479643 <a title="121-tfidf-3" href="./emnlp-2010-SRL-Based_Verb_Selection_for_ESL.html">95 emnlp-2010-SRL-Based Verb Selection for ESL</a></p>
<p>Author: Xiaohua Liu ; Bo Han ; Kuan Li ; Stephan Hyeonjun Stiller ; Ming Zhou</p><p>Abstract: In this paper we develop an approach to tackle the problem of verb selection for learners of English as a second language (ESL) by using features from the output of Semantic Role Labeling (SRL). Unlike existing approaches to verb selection that use local features such as n-grams, our approach exploits semantic features which explicitly model the usage context of the verb. The verb choice highly depends on its usage context which is not consistently captured by local features. We then combine these semantic features with other local features under the generalized perceptron learning framework. Experiments on both indomain and out-of-domain corpora show that our approach outperforms the baseline and achieves state-of-the-art performance. 1</p><p>4 0.23574485 <a title="121-tfidf-4" href="./emnlp-2010-Top-Down_Nearly-Context-Sensitive_Parsing.html">106 emnlp-2010-Top-Down Nearly-Context-Sensitive Parsing</a></p>
<p>Author: Eugene Charniak</p><p>Abstract: We present a new syntactic parser that works left-to-right and top down, thus maintaining a fully-connected parse tree for a few alternative parse hypotheses. All of the commonly used statistical parsers use context-free dynamic programming algorithms and as such work bottom up on the entire sentence. Thus they only find a complete fully connected parse at the very end. In contrast, both subjective and experimental evidence show that people understand a sentence word-to-word as they go along, or close to it. The constraint that the parser keeps one or more fully connected syntactic trees is intended to operationalize this cognitive fact. Our parser achieves a new best result for topdown parsers of 89.4%,a 20% error reduction over the previous single-parser best result for parsers of this type of 86.8% (Roark, 2001) . The improved performance is due to embracing the very large feature set available in exchange for giving up dynamic programming.</p><p>5 0.22549604 <a title="121-tfidf-5" href="./emnlp-2010-Inducing_Probabilistic_CCG_Grammars_from_Logical_Form_with_Higher-Order_Unification.html">65 emnlp-2010-Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification</a></p>
<p>Author: Tom Kwiatkowksi ; Luke Zettlemoyer ; Sharon Goldwater ; Mark Steedman</p><p>Abstract: This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning. Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method. The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences. We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations.</p><p>6 0.18976276 <a title="121-tfidf-6" href="./emnlp-2010-Utilizing_Extra-Sentential_Context_for_Parsing.html">118 emnlp-2010-Utilizing Extra-Sentential Context for Parsing</a></p>
<p>7 0.174024 <a title="121-tfidf-7" href="./emnlp-2010-Unsupervised_Parse_Selection_for_HPSG.html">114 emnlp-2010-Unsupervised Parse Selection for HPSG</a></p>
<p>8 0.14877412 <a title="121-tfidf-8" href="./emnlp-2010-Automatic_Discovery_of_Manner_Relations_and_its_Applications.html">21 emnlp-2010-Automatic Discovery of Manner Relations and its Applications</a></p>
<p>9 0.13759999 <a title="121-tfidf-9" href="./emnlp-2010-Efficient_Incremental_Decoding_for_Tree-to-String_Translation.html">42 emnlp-2010-Efficient Incremental Decoding for Tree-to-String Translation</a></p>
<p>10 0.13101287 <a title="121-tfidf-10" href="./emnlp-2010-Uptraining_for_Accurate_Deterministic_Question_Parsing.html">115 emnlp-2010-Uptraining for Accurate Deterministic Question Parsing</a></p>
<p>11 0.10436133 <a title="121-tfidf-11" href="./emnlp-2010-Soft_Syntactic_Constraints_for_Hierarchical_Phrase-Based_Translation_Using_Latent_Syntactic_Distributions.html">98 emnlp-2010-Soft Syntactic Constraints for Hierarchical Phrase-Based Translation Using Latent Syntactic Distributions</a></p>
<p>12 0.10384578 <a title="121-tfidf-12" href="./emnlp-2010-SCFG_Decoding_Without_Binarization.html">94 emnlp-2010-SCFG Decoding Without Binarization</a></p>
<p>13 0.092644699 <a title="121-tfidf-13" href="./emnlp-2010-Self-Training_with_Products_of_Latent_Variable_Grammars.html">96 emnlp-2010-Self-Training with Products of Latent Variable Grammars</a></p>
<p>14 0.086875722 <a title="121-tfidf-14" href="./emnlp-2010-Evaluating_the_Impact_of_Alternative_Dependency_Graph_Encodings_on_Solving_Event_Extraction_Tasks.html">46 emnlp-2010-Evaluating the Impact of Alternative Dependency Graph Encodings on Solving Event Extraction Tasks</a></p>
<p>15 0.080022313 <a title="121-tfidf-15" href="./emnlp-2010-Improved_Fully_Unsupervised_Parsing_with_Zoomed_Learning.html">60 emnlp-2010-Improved Fully Unsupervised Parsing with Zoomed Learning</a></p>
<p>16 0.078180984 <a title="121-tfidf-16" href="./emnlp-2010-It_Depends_on_the_Translation%3A_Unsupervised_Dependency_Parsing_via_Word_Alignment.html">67 emnlp-2010-It Depends on the Translation: Unsupervised Dependency Parsing via Word Alignment</a></p>
<p>17 0.077599853 <a title="121-tfidf-17" href="./emnlp-2010-Effects_of_Empty_Categories_on_Machine_Translation.html">40 emnlp-2010-Effects of Empty Categories on Machine Translation</a></p>
<p>18 0.076870956 <a title="121-tfidf-18" href="./emnlp-2010-Unsupervised_Induction_of_Tree_Substitution_Grammars_for_Dependency_Parsing.html">113 emnlp-2010-Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing</a></p>
<p>19 0.073562108 <a title="121-tfidf-19" href="./emnlp-2010-Confidence_in_Structured-Prediction_Using_Confidence-Weighted_Models.html">30 emnlp-2010-Confidence in Structured-Prediction Using Confidence-Weighted Models</a></p>
<p>20 0.065113492 <a title="121-tfidf-20" href="./emnlp-2010-Resolving_Event_Noun_Phrases_to_Their_Verbal_Mentions.html">93 emnlp-2010-Resolving Event Noun Phrases to Their Verbal Mentions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.237), (1, 0.169), (2, 0.29), (3, 0.352), (4, -0.085), (5, -0.003), (6, 0.3), (7, -0.058), (8, -0.009), (9, -0.2), (10, 0.119), (11, -0.04), (12, -0.094), (13, -0.096), (14, -0.112), (15, 0.094), (16, -0.011), (17, -0.013), (18, -0.04), (19, 0.134), (20, -0.036), (21, -0.067), (22, -0.023), (23, 0.174), (24, -0.11), (25, 0.024), (26, 0.006), (27, 0.004), (28, -0.004), (29, -0.052), (30, -0.04), (31, 0.017), (32, -0.065), (33, 0.026), (34, 0.036), (35, 0.045), (36, 0.058), (37, 0.091), (38, -0.081), (39, 0.017), (40, -0.008), (41, -0.013), (42, -0.047), (43, 0.052), (44, -0.055), (45, -0.024), (46, -0.058), (47, -0.025), (48, -0.04), (49, 0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9492957 <a title="121-lsi-1" href="./emnlp-2010-What_a_Parser_Can_Learn_from_a_Semantic_Role_Labeler_and_Vice_Versa.html">121 emnlp-2010-What a Parser Can Learn from a Semantic Role Labeler and Vice Versa</a></p>
<p>Author: Stephen Boxwell ; Dennis Mehay ; Chris Brew</p><p>Abstract: In many NLP systems, there is a unidirectional flow of information in which a parser supplies input to a semantic role labeler. In this paper, we build a system that allows information to flow in both directions. We make use of semantic role predictions in choosing a single-best parse. This process relies on an averaged perceptron model to distinguish likely semantic roles from erroneous ones. Our system penalizes parses that give rise to low-scoring semantic roles. To explore the consequences of this we perform two experiments. First, we use a baseline generative model to produce n-best parses, which are then re-ordered by our semantic model. Second, we use a modified version of our semantic role labeler to predict semantic roles at parse time. The performance of this modified labeler is weaker than that of our best full SRL, because it is restricted to features that can be computed directly from the parser’s packed chart. For both experiments, the resulting semantic predictions are then used to select parses. Finally, we feed the selected parses produced by each experiment to the full version of our semantic role labeler. We find that SRL performance can be improved over this baseline by selecting parses with likely semantic roles.</p><p>2 0.65384424 <a title="121-lsi-2" href="./emnlp-2010-Joint_Inference_for_Bilingual_Semantic_Role_Labeling.html">68 emnlp-2010-Joint Inference for Bilingual Semantic Role Labeling</a></p>
<p>Author: Tao Zhuang ; Chengqing Zong</p><p>Abstract: We show that jointly performing semantic role labeling (SRL) on bitext can improve SRL results on both sides. In our approach, we use monolingual SRL systems to produce argument candidates for predicates in bitext at first. Then, we simultaneously generate SRL results for two sides of bitext using our joint inference model. Our model prefers the bilingual SRL result that is not only reasonable on each side of bitext, but also has more consistent argument structures between two sides. To evaluate the consistency between two argument structures, we also formulate a log-linear model to compute the probability of aligning two arguments. We have experimented with our model on Chinese-English parallel PropBank data. Using our joint inference model, F1 scores of SRL results on Chinese and English text achieve 79.53% and 77.87% respectively, which are 1.52 and 1.74 points higher than the results of baseline monolingual SRL combination systems respectively.</p><p>3 0.62709439 <a title="121-lsi-3" href="./emnlp-2010-Inducing_Probabilistic_CCG_Grammars_from_Logical_Form_with_Higher-Order_Unification.html">65 emnlp-2010-Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification</a></p>
<p>Author: Tom Kwiatkowksi ; Luke Zettlemoyer ; Sharon Goldwater ; Mark Steedman</p><p>Abstract: This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning. Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method. The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences. We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations.</p><p>4 0.57316154 <a title="121-lsi-4" href="./emnlp-2010-Top-Down_Nearly-Context-Sensitive_Parsing.html">106 emnlp-2010-Top-Down Nearly-Context-Sensitive Parsing</a></p>
<p>Author: Eugene Charniak</p><p>Abstract: We present a new syntactic parser that works left-to-right and top down, thus maintaining a fully-connected parse tree for a few alternative parse hypotheses. All of the commonly used statistical parsers use context-free dynamic programming algorithms and as such work bottom up on the entire sentence. Thus they only find a complete fully connected parse at the very end. In contrast, both subjective and experimental evidence show that people understand a sentence word-to-word as they go along, or close to it. The constraint that the parser keeps one or more fully connected syntactic trees is intended to operationalize this cognitive fact. Our parser achieves a new best result for topdown parsers of 89.4%,a 20% error reduction over the previous single-parser best result for parsers of this type of 86.8% (Roark, 2001) . The improved performance is due to embracing the very large feature set available in exchange for giving up dynamic programming.</p><p>5 0.57200724 <a title="121-lsi-5" href="./emnlp-2010-Utilizing_Extra-Sentential_Context_for_Parsing.html">118 emnlp-2010-Utilizing Extra-Sentential Context for Parsing</a></p>
<p>Author: Jackie Chi Kit Cheung ; Gerald Penn</p><p>Abstract: Syntactic consistency is the preference to reuse a syntactic construction shortly after its appearance in a discourse. We present an analysis of the WSJ portion of the Penn Treebank, and show that syntactic consistency is pervasive across productions with various lefthand side nonterminals. Then, we implement a reranking constituent parser that makes use of extra-sentential context in its feature set. Using a linear-chain conditional random field, we improve parsing accuracy over the generative baseline parser on the Penn Treebank WSJ corpus, rivalling a similar model that does not make use of context. We show that the context-aware and the context-ignorant rerankers perform well on different subsets of the evaluation data, suggesting a combined approach would provide further improvement. We also compare parses made by models, and suggest that context can be useful for parsing by capturing structural dependencies between sentences as opposed to lexically governed dependencies.</p><p>6 0.56425864 <a title="121-lsi-6" href="./emnlp-2010-SRL-Based_Verb_Selection_for_ESL.html">95 emnlp-2010-SRL-Based Verb Selection for ESL</a></p>
<p>7 0.47310799 <a title="121-lsi-7" href="./emnlp-2010-Automatic_Discovery_of_Manner_Relations_and_its_Applications.html">21 emnlp-2010-Automatic Discovery of Manner Relations and its Applications</a></p>
<p>8 0.44357088 <a title="121-lsi-8" href="./emnlp-2010-Unsupervised_Parse_Selection_for_HPSG.html">114 emnlp-2010-Unsupervised Parse Selection for HPSG</a></p>
<p>9 0.43115851 <a title="121-lsi-9" href="./emnlp-2010-Improved_Fully_Unsupervised_Parsing_with_Zoomed_Learning.html">60 emnlp-2010-Improved Fully Unsupervised Parsing with Zoomed Learning</a></p>
<p>10 0.40896407 <a title="121-lsi-10" href="./emnlp-2010-Uptraining_for_Accurate_Deterministic_Question_Parsing.html">115 emnlp-2010-Uptraining for Accurate Deterministic Question Parsing</a></p>
<p>11 0.37254104 <a title="121-lsi-11" href="./emnlp-2010-Using_Unknown_Word_Techniques_to_Learn_Known_Words.html">117 emnlp-2010-Using Unknown Word Techniques to Learn Known Words</a></p>
<p>12 0.36637408 <a title="121-lsi-12" href="./emnlp-2010-Efficient_Incremental_Decoding_for_Tree-to-String_Translation.html">42 emnlp-2010-Efficient Incremental Decoding for Tree-to-String Translation</a></p>
<p>13 0.34021494 <a title="121-lsi-13" href="./emnlp-2010-Evaluating_the_Impact_of_Alternative_Dependency_Graph_Encodings_on_Solving_Event_Extraction_Tasks.html">46 emnlp-2010-Evaluating the Impact of Alternative Dependency Graph Encodings on Solving Event Extraction Tasks</a></p>
<p>14 0.33832374 <a title="121-lsi-14" href="./emnlp-2010-SCFG_Decoding_Without_Binarization.html">94 emnlp-2010-SCFG Decoding Without Binarization</a></p>
<p>15 0.3284851 <a title="121-lsi-15" href="./emnlp-2010-Effects_of_Empty_Categories_on_Machine_Translation.html">40 emnlp-2010-Effects of Empty Categories on Machine Translation</a></p>
<p>16 0.3126919 <a title="121-lsi-16" href="./emnlp-2010-A_Unified_Framework_for_Scope_Learning_via_Simplified_Shallow_Semantic_Parsing.html">15 emnlp-2010-A Unified Framework for Scope Learning via Simplified Shallow Semantic Parsing</a></p>
<p>17 0.30649307 <a title="121-lsi-17" href="./emnlp-2010-Soft_Syntactic_Constraints_for_Hierarchical_Phrase-Based_Translation_Using_Latent_Syntactic_Distributions.html">98 emnlp-2010-Soft Syntactic Constraints for Hierarchical Phrase-Based Translation Using Latent Syntactic Distributions</a></p>
<p>18 0.26513225 <a title="121-lsi-18" href="./emnlp-2010-It_Depends_on_the_Translation%3A_Unsupervised_Dependency_Parsing_via_Word_Alignment.html">67 emnlp-2010-It Depends on the Translation: Unsupervised Dependency Parsing via Word Alignment</a></p>
<p>19 0.25830618 <a title="121-lsi-19" href="./emnlp-2010-Confidence_in_Structured-Prediction_Using_Confidence-Weighted_Models.html">30 emnlp-2010-Confidence in Structured-Prediction Using Confidence-Weighted Models</a></p>
<p>20 0.2565808 <a title="121-lsi-20" href="./emnlp-2010-Dual_Decomposition_for_Parsing_with_Non-Projective_Head_Automata.html">38 emnlp-2010-Dual Decomposition for Parsing with Non-Projective Head Automata</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.01), (4, 0.039), (7, 0.053), (25, 0.032), (31, 0.012), (35, 0.088), (38, 0.028), (39, 0.185), (44, 0.013), (46, 0.019), (47, 0.078), (49, 0.038), (53, 0.035), (54, 0.24), (58, 0.014), (62, 0.013), (71, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77782071 <a title="121-lda-1" href="./emnlp-2010-What_a_Parser_Can_Learn_from_a_Semantic_Role_Labeler_and_Vice_Versa.html">121 emnlp-2010-What a Parser Can Learn from a Semantic Role Labeler and Vice Versa</a></p>
<p>Author: Stephen Boxwell ; Dennis Mehay ; Chris Brew</p><p>Abstract: In many NLP systems, there is a unidirectional flow of information in which a parser supplies input to a semantic role labeler. In this paper, we build a system that allows information to flow in both directions. We make use of semantic role predictions in choosing a single-best parse. This process relies on an averaged perceptron model to distinguish likely semantic roles from erroneous ones. Our system penalizes parses that give rise to low-scoring semantic roles. To explore the consequences of this we perform two experiments. First, we use a baseline generative model to produce n-best parses, which are then re-ordered by our semantic model. Second, we use a modified version of our semantic role labeler to predict semantic roles at parse time. The performance of this modified labeler is weaker than that of our best full SRL, because it is restricted to features that can be computed directly from the parser’s packed chart. For both experiments, the resulting semantic predictions are then used to select parses. Finally, we feed the selected parses produced by each experiment to the full version of our semantic role labeler. We find that SRL performance can be improved over this baseline by selecting parses with likely semantic roles.</p><p>2 0.67566526 <a title="121-lda-2" href="./emnlp-2010-A_Unified_Framework_for_Scope_Learning_via_Simplified_Shallow_Semantic_Parsing.html">15 emnlp-2010-A Unified Framework for Scope Learning via Simplified Shallow Semantic Parsing</a></p>
<p>Author: Qiaoming Zhu ; Junhui Li ; Hongling Wang ; Guodong Zhou</p><p>Abstract: This paper approaches the scope learning problem via simplified shallow semantic parsing. This is done by regarding the cue as the predicate and mapping its scope into several constituents as the arguments of the cue. Evaluation on the BioScope corpus shows that the structural information plays a critical role in capturing the relationship between a cue and its dominated arguments. It also shows that our parsing approach significantly outperforms the state-of-the-art chunking ones. Although our parsing approach is only evaluated on negation and speculation scope learning here, it is portable to other kinds of scope learning. 1</p><p>3 0.64115942 <a title="121-lda-3" href="./emnlp-2010-Collective_Cross-Document_Relation_Extraction_Without_Labelled_Data.html">28 emnlp-2010-Collective Cross-Document Relation Extraction Without Labelled Data</a></p>
<p>Author: Limin Yao ; Sebastian Riedel ; Andrew McCallum</p><p>Abstract: We present a novel approach to relation extraction that integrates information across documents, performs global inference and requires no labelled text. In particular, we tackle relation extraction and entity identification jointly. We use distant supervision to train a factor graph model for relation extraction based on an existing knowledge base (Freebase, derived in parts from Wikipedia). For inference we run an efficient Gibbs sampler that leads to linear time joint inference. We evaluate our approach both for an indomain (Wikipedia) and a more realistic outof-domain (New York Times Corpus) setting. For the in-domain setting, our joint model leads to 4% higher precision than an isolated local approach, but has no advantage over a pipeline. For the out-of-domain data, we benefit strongly from joint modelling, and observe improvements in precision of 13% over the pipeline, and 15% over the isolated baseline.</p><p>4 0.62082881 <a title="121-lda-4" href="./emnlp-2010-Uptraining_for_Accurate_Deterministic_Question_Parsing.html">115 emnlp-2010-Uptraining for Accurate Deterministic Question Parsing</a></p>
<p>Author: Slav Petrov ; Pi-Chuan Chang ; Michael Ringgaard ; Hiyan Alshawi</p><p>Abstract: It is well known that parsing accuracies drop significantly on out-of-domain data. What is less known is that some parsers suffer more from domain shifts than others. We show that dependency parsers have more difficulty parsing questions than constituency parsers. In particular, deterministic shift-reduce dependency parsers, which are of highest interest for practical applications because of their linear running time, drop to 60% labeled accuracy on a question test set. We propose an uptraining procedure in which a deterministic parser is trained on the output of a more accurate, but slower, latent variable constituency parser (converted to dependencies). Uptraining with 100K unlabeled questions achieves results comparable to having 2K labeled questions for training. With 100K unlabeled and 2K labeled questions, uptraining is able to improve parsing accuracy to 84%, closing the gap between in-domain and out-of-domain performance.</p><p>5 0.61031091 <a title="121-lda-5" href="./emnlp-2010-Measuring_Distributional_Similarity_in_Context.html">77 emnlp-2010-Measuring Distributional Similarity in Context</a></p>
<p>Author: Georgiana Dinu ; Mirella Lapata</p><p>Abstract: The computation of meaning similarity as operationalized by vector-based models has found widespread use in many tasks ranging from the acquisition of synonyms and paraphrases to word sense disambiguation and textual entailment. Vector-based models are typically directed at representing words in isolation and thus best suited for measuring similarity out of context. In his paper we propose a probabilistic framework for measuring similarity in context. Central to our approach is the intuition that word meaning is represented as a probability distribution over a set of latent senses and is modulated by context. Experimental results on lexical substitution and word similarity show that our algorithm outperforms previously proposed models.</p><p>6 0.58641052 <a title="121-lda-6" href="./emnlp-2010-Function-Based_Question_Classification_for_General_QA.html">51 emnlp-2010-Function-Based Question Classification for General QA</a></p>
<p>7 0.55460113 <a title="121-lda-7" href="./emnlp-2010-Enhancing_Domain_Portability_of_Chinese_Segmentation_Model_Using_Chi-Square_Statistics_and_Bootstrapping.html">43 emnlp-2010-Enhancing Domain Portability of Chinese Segmentation Model Using Chi-Square Statistics and Bootstrapping</a></p>
<p>8 0.54722208 <a title="121-lda-8" href="./emnlp-2010-Joint_Inference_for_Bilingual_Semantic_Role_Labeling.html">68 emnlp-2010-Joint Inference for Bilingual Semantic Role Labeling</a></p>
<p>9 0.54245889 <a title="121-lda-9" href="./emnlp-2010-SCFG_Decoding_Without_Binarization.html">94 emnlp-2010-SCFG Decoding Without Binarization</a></p>
<p>10 0.53484577 <a title="121-lda-10" href="./emnlp-2010-Top-Down_Nearly-Context-Sensitive_Parsing.html">106 emnlp-2010-Top-Down Nearly-Context-Sensitive Parsing</a></p>
<p>11 0.53461987 <a title="121-lda-11" href="./emnlp-2010-Using_Unknown_Word_Techniques_to_Learn_Known_Words.html">117 emnlp-2010-Using Unknown Word Techniques to Learn Known Words</a></p>
<p>12 0.52945852 <a title="121-lda-12" href="./emnlp-2010-Improved_Fully_Unsupervised_Parsing_with_Zoomed_Learning.html">60 emnlp-2010-Improved Fully Unsupervised Parsing with Zoomed Learning</a></p>
<p>13 0.52844375 <a title="121-lda-13" href="./emnlp-2010-Learning_First-Order_Horn_Clauses_from_Web_Text.html">72 emnlp-2010-Learning First-Order Horn Clauses from Web Text</a></p>
<p>14 0.52787948 <a title="121-lda-14" href="./emnlp-2010-Dual_Decomposition_for_Parsing_with_Non-Projective_Head_Automata.html">38 emnlp-2010-Dual Decomposition for Parsing with Non-Projective Head Automata</a></p>
<p>15 0.52718115 <a title="121-lda-15" href="./emnlp-2010-A_Mixture_Model_with_Sharing_for_Lexical_Semantics.html">7 emnlp-2010-A Mixture Model with Sharing for Lexical Semantics</a></p>
<p>16 0.52528912 <a title="121-lda-16" href="./emnlp-2010-Automatic_Discovery_of_Manner_Relations_and_its_Applications.html">21 emnlp-2010-Automatic Discovery of Manner Relations and its Applications</a></p>
<p>17 0.51438856 <a title="121-lda-17" href="./emnlp-2010-Unsupervised_Induction_of_Tree_Substitution_Grammars_for_Dependency_Parsing.html">113 emnlp-2010-Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing</a></p>
<p>18 0.51277727 <a title="121-lda-18" href="./emnlp-2010-A_Multi-Pass_Sieve_for_Coreference_Resolution.html">8 emnlp-2010-A Multi-Pass Sieve for Coreference Resolution</a></p>
<p>19 0.50904441 <a title="121-lda-19" href="./emnlp-2010-A_Tree_Kernel-Based_Unified_Framework_for_Chinese_Zero_Anaphora_Resolution.html">14 emnlp-2010-A Tree Kernel-Based Unified Framework for Chinese Zero Anaphora Resolution</a></p>
<p>20 0.50683635 <a title="121-lda-20" href="./emnlp-2010-Automatic_Detection_and_Classification_of_Social_Events.html">20 emnlp-2010-Automatic Detection and Classification of Social Events</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
