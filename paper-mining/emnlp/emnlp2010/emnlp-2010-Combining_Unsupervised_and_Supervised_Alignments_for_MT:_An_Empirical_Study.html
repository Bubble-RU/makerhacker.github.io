<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>29 emnlp-2010-Combining Unsupervised and Supervised Alignments for MT: An Empirical Study</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2010" href="../home/emnlp2010_home.html">emnlp2010</a> <a title="emnlp-2010-29" href="#">emnlp2010-29</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>29 emnlp-2010-Combining Unsupervised and Supervised Alignments for MT: An Empirical Study</h1>
<br/><p>Source: <a title="emnlp-2010-29-pdf" href="http://aclweb.org/anthology//D/D10/D10-1065.pdf">pdf</a></p><p>Author: Jinxi Xu ; Antti-Veikko Rosti</p><p>Abstract: Word alignment plays a central role in statistical MT (SMT) since almost all SMT systems extract translation rules from word aligned parallel training data. While most SMT systems use unsupervised algorithms (e.g. GIZA++) for training word alignment, supervised methods, which exploit a small amount of human-aligned data, have become increasingly popular recently. This work empirically studies the performance of these two classes of alignment algorithms and explores strategies to combine them to improve overall system performance. We used two unsupervised aligners, GIZA++ and HMM, and one supervised aligner, ITG, in this study. To avoid language and genre specific conclusions, we ran experiments on test sets consisting of two language pairs (Chinese-to-English and Arabicto-English) and two genres (newswire and weblog). Results show that the two classes of algorithms achieve the same level of MT perfor- mance. Modest improvements were achieved by taking the union of the translation grammars extracted from different alignments. Significant improvements (around 1.0 in BLEU) were achieved by combining outputs of different systems trained with different alignments. The improvements are consistent across languages and genres.</p><p>Reference: <a title="emnlp-2010-29-reference" href="../emnlp2010_reference/emnlp-2010-Combining_Unsupervised_and_Supervised_Alignments_for_MT%3A_An_Empirical_Study_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Word alignment plays a central role in statistical MT (SMT) since almost all SMT systems extract translation rules from word aligned parallel training data. [sent-3, score-0.532]
</p><p>2 This work empirically studies the performance of these two classes of alignment algorithms and explores strategies to combine them to improve overall system performance. [sent-7, score-0.382]
</p><p>3 To avoid language and genre specific conclusions, we ran experiments on test sets consisting of two language pairs (Chinese-to-English and Arabicto-English) and two genres (newswire and weblog). [sent-9, score-0.172]
</p><p>4 Modest improvements were achieved by taking the union of the translation grammars extracted from different alignments. [sent-11, score-0.216]
</p><p>5 0 in BLEU) were achieved by combining outputs of different systems trained with different alignments. [sent-13, score-0.149]
</p><p>6 1 Introduction Word alignment plays a central role in training statistical machine translation (SMT) systems since almost all SMT systems extract translation rules from word aligned parallel training data. [sent-15, score-0.671]
</p><p>7 Until recently, 667 most SMT systems used GIZA++ (Och and Ney, 2003), an unsupervised algorithm, for aligning parallel training data. [sent-16, score-0.131]
</p><p>8 The main objective of this work is to show the two classes (unsupervised and supervised) of algorithms are complementary and combining them  will improve overall system performance. [sent-21, score-0.189]
</p><p>9 The use of human aligned training data allows supervised methods such as ITG to more accurately align frequent words, such as the alignments of Chinese particles (e. [sent-22, score-0.342]
</p><p>10 On the other hand, supervised methods can be affected by suboptimal alignments in hand-aligned data. [sent-29, score-0.3]
</p><p>11 For example, the hand-aligned data used in our experiments contain some coarse-grained alignments (e. [sent-30, score-0.258]
</p><p>12 “lianhe guo” to “United Nations”) although fine-grained alignments (“lian-he” to “United” and “guo” to “Nations”) are usually more appropriate for SMT. [sent-32, score-0.258]
</p><p>13 We explored two techniques to combine different alignment algorithms. [sent-37, score-0.256]
</p><p>14 One is to take the union of the translation rules extracted from alignments produced by different aligners. [sent-38, score-0.486]
</p><p>15 This is motivated by studies that showed that the coverage of translation  rules is critical to SMT (DeNeefe et al. [sent-39, score-0.21]
</p><p>16 tc ho2d0s10 in A Nsastoucira tlio Lnan fogru Cagoem Ppruotcaetisosninagl, L pinag eusis 6t6ic7s–673, other method is to combine the outputs of different MT systems trained using different aligners. [sent-43, score-0.137]
</p><p>17 Assuming different systems make independent errors, system combination can generate a better translation than those of individual systems through voting (Rosti et al. [sent-44, score-0.29]
</p><p>18 Past studies of combining alternative alignments focused on minimizing alignment errors, usually by merging alternative alignments for a sentence pair into a single alignment with the fewest number of incorrect alignment links (Ayan and Dorr, 2006). [sent-47, score-1.361]
</p><p>19 In contrast, our work is based on the assumption that perfect word alignment is impossible due to the intrinsic difficulty of the problem, and it is more effective to resolve translation ambiguities at later stages of the MT pipeline. [sent-48, score-0.335]
</p><p>20 A main focus of much previous work on word alignments is on theoretical aspects of the proposed algorithms. [sent-49, score-0.258]
</p><p>21 Our system was trained on a large amount of training data and evalu-  ated on multiple languages (Chinese-to-English and Arabic-to-English) and multiple genres (newswire and weblog). [sent-51, score-0.174]
</p><p>22 Section 3 describes the two methods used to combine these aligners to improve MT. [sent-57, score-0.233]
</p><p>23 2  Alignment Algorithms  We used three aligners in this work: GIZA++ (Och and Ney, 2003), jointly trained HMM (Liang et al. [sent-62, score-0.233]
</p><p>24 Given a sentence pair e f, −  iBt soewekns tth ael alignment a vtehnat am seaxntiemnizcees p tahier probability P(f, a|e). [sent-67, score-0.224]
</p><p>25 As in most previous studies using 668 GIZA++, we ran GIZA++ in both directions, from e to f and from f to e, and symmetrized the bidirectional alignments into one, using a method similar to the grow-diagonal-final method described in Och and Ney (2003). [sent-68, score-0.372]
</p><p>26 The jointly trained HMM aligner, or HMM for short, is also unsupervised but it uses a small amount of hand-aligned data to tweak a few high level parameters. [sent-70, score-0.116]
</p><p>27 The ITG aligner is a supervised method whose parameters are tuned to optimize alignment accuracy on hand-aligned data. [sent-72, score-0.503]
</p><p>28 It uses the inversion transduction grammar (ITG) (Wu, 1997) to narrow the space of possible alignments. [sent-73, score-0.149]
</p><p>29 Since the ITG aligner uses features extracted from HMM alignments, HMM was run as a prepossessing step in our experiments. [sent-74, score-0.179]
</p><p>30 Both the HMM and ITG aligners are publicly available1. [sent-75, score-0.201]
</p><p>31 3  Methods of Combining Alternative Alignments for MT  We explored two methods of combining alternative alignments for MT. [sent-76, score-0.346]
</p><p>32 One is to extract translation rules from the three alternative alignments and take the union of the three sets of rules as the single translation grammar. [sent-77, score-0.699]
</p><p>33 Procedurally, this is done by concatenating the alignment files before extracting translation rules. [sent-78, score-0.335]
</p><p>34 This method greatly increases the coverage of the rules, as the unioned translation grammar has about 80% more rules than the ones extracted from the individual alignment in our experiments. [sent-80, score-0.705]
</p><p>35 The other is to use system combination to combine outputs of systems trained using different aligners. [sent-82, score-0.26]
</p><p>36 Due to differences in the alignment algorithms, these systems would produce different hypotheses with independent errors. [sent-83, score-0.312]
</p><p>37 Combining a diverse set of hypotheses could improve overall system performance. [sent-84, score-0.111]
</p><p>38 While system combination is a well-known technique, to our knowledge this work is the first to apply it to explicitly exploit complementary align-  ment algorithms on a large scale. [sent-85, score-0.217]
</p><p>39 Since system combination is an established technique, here we only briefly discuss our system com1http://code. [sent-86, score-0.174]
</p><p>40 In this work, we use incremental hypothesis alignment with flexible matching (Rosti et al. [sent-91, score-0.224]
</p><p>41 This lattice is expanded with an unpruned bigram language model and the system combination weights are tuned directly to maximize the BLEU score of the 1-best decoding outputs. [sent-95, score-0.27]
</p><p>42 , 2008), one of the top performing systems in the NIST 2009 MT evaluation, and trained it with very large amounts of parallel and language model data. [sent-99, score-0.129]
</p><p>43 The system used large sets of discriminatively tuned features (up to 55,000 on Arabic) inspired by the work of Chiang et al. [sent-100, score-0.109]
</p><p>44 To avoid drawing language, genre, and metric specific conclusions, we experimented with two language pairs, Arabic-English and ChineseEnglish, and two genres, newswire and weblog, and report both BLEU (Papineni et al. [sent-102, score-0.09]
</p><p>45 Systems were tuned to maximize BLEU on the tuning set using a procedure described in Devlin (2009). [sent-105, score-0.086]
</p><p>46 Due to the size of the parallel corpora, we divided them 669 into five chunks and aligned them in parallel to save time. [sent-108, score-0.18]
</p><p>47 For longer sentences, we used HMM alignments instead, which were conveniently generated in the preprocessing step of ITG aligner. [sent-110, score-0.258]
</p><p>48 The same 5-gram LM was also used for re-scoring system combination results. [sent-113, score-0.123]
</p><p>49 For each combination of language pair and genre, we used three development sets: • Tune, which was used to tune parameters of iTnudinveid,u walh MichT systems. [sent-114, score-0.113]
</p><p>50 •  SysCombTune, which was used to tune pa-  rSaymseCteorsm bofT system choicmhbi wnaatsio uns. [sent-116, score-0.092]
</p><p>51 Te st, which was the blind test corpus for measuring performances eo bfl binodth t einstd civoridpuuasl systems and system combination. [sent-118, score-0.112]
</p><p>52 We re-tokenized the corpora using our tokenizers and projected the LDC alignments to our tokenization heuristically. [sent-125, score-0.258]
</p><p>53 Case insensitive BLEU and TER scores for Arabic newswire, Arabic weblog, Chinese newswire, and Chinese weblog are shown in Tables 2, 3, 4, and 5, respectively2. [sent-142, score-0.172]
</p><p>54 The BLEU scores on the Te st set are fairly similar but the ordering between different alignment algorithms is mixed between different languages and genres. [sent-143, score-0.289]
</p><p>55 To compare the two alignment combination strategies, we trained a system using the union of the rules extracted from the alternative alignments (union in the tables) and a combination of the three baseline system outputs (3 sys comb in the tables). [sent-144, score-1.024]
</p><p>56 The system with the unioned grammar was also added  as an additional system in the combination marked by 4 sys comb. [sent-145, score-0.544]
</p><p>57 As seen in the tables, unioned grammar and system combination improve MT on both languages (Arabic and Chinese) and both genres (newswire and weblog). [sent-146, score-0.526]
</p><p>58 While there are improvements on both Sys CombTune and Te st, the results on SysCombTune are not totally fair since it was used for tuning system combination weights and as validation for optimizing weights of the MT systems. [sent-147, score-0.197]
</p><p>59 (We did not show scores on Tune because systems were directly tuned on it. [sent-149, score-0.086]
</p><p>60 For unioned grammar, the overall improvement in BLEU is modest, ranging from 0. [sent-151, score-0.251]
</p><p>61 6 point 2Dagger (†) indicates statistically better results than the best individual alignment system. [sent-153, score-0.224]
</p><p>62 Double dagger (‡) indicates statistically better results than both best individual alignment and unioned grammar. [sent-154, score-0.475]
</p><p>63 Bold indicates best Test set performance among individual alignment systems. [sent-155, score-0.224]
</p><p>64 The TER improvements are mostly explained by the hypothesis alignment algorithm which is closely related to TER scoring (Rosti et al. [sent-165, score-0.27]
</p><p>65 The results are interesting because all three baseline systems (GIZA++, HMM and ITG) are identical except for the word alignments used in rule extraction. [sent-167, score-0.286]
</p><p>66 The results confirm that the aligners are indeed complementary, as we conjectured earlier. [sent-168, score-0.201]
</p><p>67 Also, the four-system combination yields consistent gains over the three-system combination, suggesting that the system using the unioned grammar is somewhat complementary to  the three baseline systems. [sent-169, score-0.495]
</p><p>68 The statistical test indicates that both the three and four system combinations are significantly better than the single best alignment system for all languages and genres in BLEU and TER. [sent-170, score-0.417]
</p><p>69 In most cases, they are also significantly better than unioned grammar. [sent-171, score-0.251]
</p><p>70 Somewhat surprisingly, the GIZA++ trained system is slightly better than the ITG trained system on all genres but Chinese weblog. [sent-172, score-0.257]
</p><p>71 (For long sentences, we had to settle for HMM alignments for computing reasons. [sent-175, score-0.258]
</p><p>72 46‡ Table 2: MT results on Arabic newswire (see footnote 2). [sent-202, score-0.161]
</p><p>73 53‡ Table 3: MT results on Arabic weblog (see footnote 2). [sent-227, score-0.243]
</p><p>74 57‡  Table 4: MT results on Chinese newswire (see footnote 2). [sent-252, score-0.161]
</p><p>75 36‡ Table 5: MT results on Chinese weblog (see footnote 2). [sent-277, score-0.243]
</p><p>76 Suppose on a common data set, the sets of alignment links produced by two aligners are A and B, we compute their agreement as (|A T B|/|A| + |A T Be c |/|B| )/2. [sent-280, score-0.425]
</p><p>77 Due to the large differences between the aligners, significantly more rules were extracted with the unioned grammar method in our experiments. [sent-286, score-0.37]
</p><p>78 However, for computing reasons, we kept the beam size of the decoder constant despite the increase in grammar size, potentially pruning out good theories. [sent-289, score-0.088]
</p><p>79 6  Related Work  Ayan and Dorr (2006) described a method to minimize alignment errors by combining alternative  alignments into a single alignment for each sentence pair. [sent-292, score-0.794]
</p><p>80 Deng and Zhou (2009) used the number of extractable translation pairs as the objective function for alignment combination. [sent-293, score-0.335]
</p><p>81 (2003) used heuristics to merge the bidirectional GIZA++ alignments into a single alignment. [sent-295, score-0.288]
</p><p>82 Despite differences in algorithms and objective functions in these studies, they all attempted to produce a single final alignment for each sentence pair. [sent-296, score-0.258]
</p><p>83 In comparison, all alternative alignments are directly used by the translation system in this work. [sent-297, score-0.464]
</p><p>84 The unioned grammar method in this work is very similar to Gim e´nez and M `arquez (2005), which combined phrase pairs extracted from different alignments into a single phrase table. [sent-298, score-0.57]
</p><p>85 The difference from that work is that our focus is to leverage complementary alignment algorithms, while theirs was to leverage alignments of different lexical units produced by the same aligner. [sent-299, score-0.542]
</p><p>86 The theory behind the GIZA++ aligner was due to  Brown et al. [sent-303, score-0.179]
</p><p>87 The use of HMM for word alignment can be traced as far back as to Vogel et al. [sent-308, score-0.224]
</p><p>88 The HMM aligner used in this work was due to Liang et al. [sent-310, score-0.179]
</p><p>89 It refined the original HMM alignment algorithm by jointly training two HMMs, one in each direction. [sent-312, score-0.224]
</p><p>90 Furthermore, it used a small amount of supervised data to tweak some high level parameters, although it did not directly use the supervised data in training. [sent-313, score-0.134]
</p><p>91 7  Conclusions  We explored two methods to exploit complementary alignment algorithms. [sent-314, score-0.284]
</p><p>92 One is to extract translation rules from all alternative alignments. [sent-315, score-0.213]
</p><p>93 The other is to combine outputs of different MT systems trained using different aligners. [sent-316, score-0.137]
</p><p>94 Experiments on two language pairs and two genres show consistent improvements over the baseline systems. [sent-317, score-0.137]
</p><p>95 Minimum Bayes risk combination of translation hypotheses from alternative morphological decompositions. [sent-341, score-0.287]
</p><p>96 Statistical significance tests for IJCNLP  machine translation evaluation. [sent-370, score-0.111]
</p><p>97 Incremental hypothesis alignment with flexible matching for building confusion networks: BBN system description for WMT09 system combination task. [sent-394, score-0.439]
</p><p>98 A new string-to-dependency machine translation algorithm with a target dependency language model. [sent-398, score-0.111]
</p><p>99 A study of translation edit rate with targeted human annotation. [sent-402, score-0.111]
</p><p>100 Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. [sent-410, score-0.157]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('itg', 0.457), ('alignments', 0.258), ('giza', 0.252), ('unioned', 0.251), ('alignment', 0.224), ('hmm', 0.205), ('aligners', 0.201), ('aligner', 0.179), ('ter', 0.176), ('weblog', 0.172), ('bleu', 0.167), ('rosti', 0.15), ('mt', 0.134), ('translation', 0.111), ('syscomb', 0.1), ('syscombtunetest', 0.1), ('arabic', 0.099), ('genres', 0.091), ('newswire', 0.09), ('ayan', 0.086), ('smt', 0.085), ('combination', 0.072), ('footnote', 0.071), ('parallel', 0.069), ('grammar', 0.061), ('hypotheses', 0.06), ('complementary', 0.06), ('union', 0.059), ('tuned', 0.058), ('sys', 0.058), ('rules', 0.058), ('och', 0.056), ('ney', 0.054), ('bbn', 0.054), ('haghighi', 0.052), ('system', 0.051), ('chinese', 0.051), ('fazil', 0.05), ('gim', 0.05), ('guo', 0.05), ('necip', 0.05), ('syscombtune', 0.05), ('tweak', 0.05), ('unpruned', 0.05), ('transduction', 0.047), ('improvements', 0.046), ('outputs', 0.045), ('combining', 0.044), ('alternative', 0.044), ('gispert', 0.043), ('nations', 0.043), ('nez', 0.043), ('ran', 0.043), ('te', 0.043), ('aligned', 0.042), ('north', 0.042), ('supervised', 0.042), ('confusion', 0.041), ('tune', 0.041), ('chapter', 0.041), ('studies', 0.041), ('inversion', 0.041), ('gale', 0.041), ('lattice', 0.039), ('bonnie', 0.039), ('genre', 0.038), ('dorr', 0.036), ('chineseenglish', 0.036), ('deng', 0.036), ('agreements', 0.036), ('jinxi', 0.036), ('matsoukas', 0.036), ('spyros', 0.036), ('american', 0.035), ('xu', 0.035), ('unsupervised', 0.034), ('liang', 0.034), ('lm', 0.034), ('algorithms', 0.034), ('blind', 0.033), ('deneefe', 0.033), ('combine', 0.032), ('trained', 0.032), ('ijcnlp', 0.032), ('etc', 0.032), ('afnlp', 0.032), ('st', 0.031), ('networks', 0.03), ('views', 0.03), ('bidirectional', 0.03), ('establish', 0.03), ('technologies', 0.03), ('united', 0.029), ('tuning', 0.028), ('systems', 0.028), ('pages', 0.028), ('beam', 0.027), ('vogel', 0.027), ('modest', 0.027), ('snover', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999905 <a title="29-tfidf-1" href="./emnlp-2010-Combining_Unsupervised_and_Supervised_Alignments_for_MT%3A_An_Empirical_Study.html">29 emnlp-2010-Combining Unsupervised and Supervised Alignments for MT: An Empirical Study</a></p>
<p>Author: Jinxi Xu ; Antti-Veikko Rosti</p><p>Abstract: Word alignment plays a central role in statistical MT (SMT) since almost all SMT systems extract translation rules from word aligned parallel training data. While most SMT systems use unsupervised algorithms (e.g. GIZA++) for training word alignment, supervised methods, which exploit a small amount of human-aligned data, have become increasingly popular recently. This work empirically studies the performance of these two classes of alignment algorithms and explores strategies to combine them to improve overall system performance. We used two unsupervised aligners, GIZA++ and HMM, and one supervised aligner, ITG, in this study. To avoid language and genre specific conclusions, we ran experiments on test sets consisting of two language pairs (Chinese-to-English and Arabicto-English) and two genres (newswire and weblog). Results show that the two classes of algorithms achieve the same level of MT perfor- mance. Modest improvements were achieved by taking the union of the translation grammars extracted from different alignments. Significant improvements (around 1.0 in BLEU) were achieved by combining outputs of different systems trained with different alignments. The improvements are consistent across languages and genres.</p><p>2 0.26324418 <a title="29-tfidf-2" href="./emnlp-2010-Hierarchical_Phrase-Based_Translation_Grammars_Extracted_from_Alignment_Posterior_Probabilities.html">57 emnlp-2010-Hierarchical Phrase-Based Translation Grammars Extracted from Alignment Posterior Probabilities</a></p>
<p>Author: Adria de Gispert ; Juan Pino ; William Byrne</p><p>Abstract: We report on investigations into hierarchical phrase-based translation grammars based on rules extracted from posterior distributions over alignments of the parallel text. Rather than restrict rule extraction to a single alignment, such as Viterbi, we instead extract rules based on posterior distributions provided by the HMM word-to-word alignmentmodel. We define translation grammars progressively by adding classes of rules to a basic phrase-based system. We assess these grammars in terms of their expressive power, measured by their ability to align the parallel text from which their rules are extracted, and the quality of the translations they yield. In Chinese-to-English translation, we find that rule extraction from posteriors gives translation improvements. We also find that grammars with rules with only one nonterminal, when extracted from posteri- ors, can outperform more complex grammars extracted from Viterbi alignments. Finally, we show that the best way to exploit source-totarget and target-to-source alignment models is to build two separate systems and combine their output translation lattices.</p><p>3 0.24395779 <a title="29-tfidf-3" href="./emnlp-2010-Discriminative_Word_Alignment_with_a_Function_Word_Reordering_Model.html">36 emnlp-2010-Discriminative Word Alignment with a Function Word Reordering Model</a></p>
<p>Author: Hendra Setiawan ; Chris Dyer ; Philip Resnik</p><p>Abstract: We address the modeling, parameter estimation and search challenges that arise from the introduction of reordering models that capture non-local reordering in alignment modeling. In particular, we introduce several reordering models that utilize (pairs of) function words as contexts for alignment reordering. To address the parameter estimation challenge, we propose to estimate these reordering models from a relatively small amount of manuallyaligned corpora. To address the search challenge, we devise an iterative local search algorithm that stochastically explores reordering possibilities. By capturing non-local reordering phenomena, our proposed alignment model bears a closer resemblance to stateof-the-art translation model. Empirical results show significant improvements in alignment quality as well as in translation performance over baselines in a large-scale ChineseEnglish translation task.</p><p>4 0.14893979 <a title="29-tfidf-4" href="./emnlp-2010-A_Fast_Fertility_Hidden_Markov_Model_for_Word_Alignment_Using_MCMC.html">3 emnlp-2010-A Fast Fertility Hidden Markov Model for Word Alignment Using MCMC</a></p>
<p>Author: Shaojun Zhao ; Daniel Gildea</p><p>Abstract: A word in one language can be translated to zero, one, or several words in other languages. Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.</p><p>5 0.13030286 <a title="29-tfidf-5" href="./emnlp-2010-Improving_Translation_via_Targeted_Paraphrasing.html">63 emnlp-2010-Improving Translation via Targeted Paraphrasing</a></p>
<p>Author: Philip Resnik ; Olivia Buzek ; Chang Hu ; Yakov Kronrod ; Alex Quinn ; Benjamin B. Bederson</p><p>Abstract: Targeted paraphrasing is a new approach to the problem of obtaining cost-effective, reasonable quality translation that makes use of simple and inexpensive human computations by monolingual speakers in combination with machine translation. The key insight behind the process is that it is possible to spot likely translation errors with only monolingual knowledge of the target language, and it is possible to generate alternative ways to say the same thing (i.e. paraphrases) with only monolingual knowledge of the source language. Evaluations demonstrate that this approach can yield substantial improvements in translation quality.</p><p>6 0.12952548 <a title="29-tfidf-6" href="./emnlp-2010-A_Hybrid_Morpheme-Word_Representation_for_Machine_Translation_of_Morphologically_Rich_Languages.html">5 emnlp-2010-A Hybrid Morpheme-Word Representation for Machine Translation of Morphologically Rich Languages</a></p>
<p>7 0.12718189 <a title="29-tfidf-7" href="./emnlp-2010-It_Depends_on_the_Translation%3A_Unsupervised_Dependency_Parsing_via_Word_Alignment.html">67 emnlp-2010-It Depends on the Translation: Unsupervised Dependency Parsing via Word Alignment</a></p>
<p>8 0.12717997 <a title="29-tfidf-8" href="./emnlp-2010-EMNLP_044.html">39 emnlp-2010-EMNLP 044</a></p>
<p>9 0.12548457 <a title="29-tfidf-9" href="./emnlp-2010-Statistical_Machine_Translation_with_a_Factorized_Grammar.html">99 emnlp-2010-Statistical Machine Translation with a Factorized Grammar</a></p>
<p>10 0.12468075 <a title="29-tfidf-10" href="./emnlp-2010-Assessing_Phrase-Based_Translation_Models_with_Oracle_Decoding.html">18 emnlp-2010-Assessing Phrase-Based Translation Models with Oracle Decoding</a></p>
<p>11 0.12261742 <a title="29-tfidf-11" href="./emnlp-2010-Minimum_Error_Rate_Training_by_Sampling_the_Translation_Lattice.html">78 emnlp-2010-Minimum Error Rate Training by Sampling the Translation Lattice</a></p>
<p>12 0.11315382 <a title="29-tfidf-12" href="./emnlp-2010-Discriminative_Sample_Selection_for_Statistical_Machine_Translation.html">35 emnlp-2010-Discriminative Sample Selection for Statistical Machine Translation</a></p>
<p>13 0.10904373 <a title="29-tfidf-13" href="./emnlp-2010-Facilitating_Translation_Using_Source_Language_Paraphrase_Lattices.html">50 emnlp-2010-Facilitating Translation Using Source Language Paraphrase Lattices</a></p>
<p>14 0.10830522 <a title="29-tfidf-14" href="./emnlp-2010-Automatic_Evaluation_of_Translation_Quality_for_Distant_Language_Pairs.html">22 emnlp-2010-Automatic Evaluation of Translation Quality for Distant Language Pairs</a></p>
<p>15 0.10387108 <a title="29-tfidf-15" href="./emnlp-2010-Cross_Language_Text_Classification_by_Model_Translation_and_Semi-Supervised_Learning.html">33 emnlp-2010-Cross Language Text Classification by Model Translation and Semi-Supervised Learning</a></p>
<p>16 0.097838357 <a title="29-tfidf-16" href="./emnlp-2010-Crouching_Dirichlet%2C_Hidden_Markov_Model%3A_Unsupervised_POS_Tagging_with_Context_Local_Tag_Generation.html">34 emnlp-2010-Crouching Dirichlet, Hidden Markov Model: Unsupervised POS Tagging with Context Local Tag Generation</a></p>
<p>17 0.090016641 <a title="29-tfidf-17" href="./emnlp-2010-Example-Based_Paraphrasing_for_Improved_Phrase-Based_Statistical_Machine_Translation.html">47 emnlp-2010-Example-Based Paraphrasing for Improved Phrase-Based Statistical Machine Translation</a></p>
<p>18 0.078462191 <a title="29-tfidf-18" href="./emnlp-2010-Soft_Syntactic_Constraints_for_Hierarchical_Phrase-Based_Translation_Using_Latent_Syntactic_Distributions.html">98 emnlp-2010-Soft Syntactic Constraints for Hierarchical Phrase-Based Translation Using Latent Syntactic Distributions</a></p>
<p>19 0.078349538 <a title="29-tfidf-19" href="./emnlp-2010-Enhancing_Mention_Detection_Using_Projection_via_Aligned_Corpora.html">44 emnlp-2010-Enhancing Mention Detection Using Projection via Aligned Corpora</a></p>
<p>20 0.077102579 <a title="29-tfidf-20" href="./emnlp-2010-Simple_Type-Level_Unsupervised_POS_Tagging.html">97 emnlp-2010-Simple Type-Level Unsupervised POS Tagging</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.286), (1, -0.249), (2, 0.064), (3, -0.145), (4, -0.006), (5, -0.281), (6, -0.082), (7, -0.039), (8, 0.053), (9, 0.106), (10, -0.057), (11, 0.064), (12, 0.063), (13, 0.017), (14, 0.015), (15, 0.034), (16, 0.007), (17, 0.001), (18, -0.083), (19, -0.119), (20, -0.076), (21, 0.032), (22, 0.14), (23, 0.025), (24, 0.162), (25, -0.071), (26, 0.014), (27, 0.004), (28, 0.007), (29, 0.022), (30, -0.036), (31, 0.029), (32, -0.047), (33, -0.031), (34, -0.033), (35, -0.029), (36, 0.022), (37, -0.069), (38, 0.009), (39, 0.084), (40, -0.001), (41, -0.024), (42, -0.034), (43, 0.034), (44, -0.043), (45, -0.036), (46, -0.025), (47, -0.125), (48, -0.072), (49, 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96060258 <a title="29-lsi-1" href="./emnlp-2010-Combining_Unsupervised_and_Supervised_Alignments_for_MT%3A_An_Empirical_Study.html">29 emnlp-2010-Combining Unsupervised and Supervised Alignments for MT: An Empirical Study</a></p>
<p>Author: Jinxi Xu ; Antti-Veikko Rosti</p><p>Abstract: Word alignment plays a central role in statistical MT (SMT) since almost all SMT systems extract translation rules from word aligned parallel training data. While most SMT systems use unsupervised algorithms (e.g. GIZA++) for training word alignment, supervised methods, which exploit a small amount of human-aligned data, have become increasingly popular recently. This work empirically studies the performance of these two classes of alignment algorithms and explores strategies to combine them to improve overall system performance. We used two unsupervised aligners, GIZA++ and HMM, and one supervised aligner, ITG, in this study. To avoid language and genre specific conclusions, we ran experiments on test sets consisting of two language pairs (Chinese-to-English and Arabicto-English) and two genres (newswire and weblog). Results show that the two classes of algorithms achieve the same level of MT perfor- mance. Modest improvements were achieved by taking the union of the translation grammars extracted from different alignments. Significant improvements (around 1.0 in BLEU) were achieved by combining outputs of different systems trained with different alignments. The improvements are consistent across languages and genres.</p><p>2 0.72089076 <a title="29-lsi-2" href="./emnlp-2010-Discriminative_Word_Alignment_with_a_Function_Word_Reordering_Model.html">36 emnlp-2010-Discriminative Word Alignment with a Function Word Reordering Model</a></p>
<p>Author: Hendra Setiawan ; Chris Dyer ; Philip Resnik</p><p>Abstract: We address the modeling, parameter estimation and search challenges that arise from the introduction of reordering models that capture non-local reordering in alignment modeling. In particular, we introduce several reordering models that utilize (pairs of) function words as contexts for alignment reordering. To address the parameter estimation challenge, we propose to estimate these reordering models from a relatively small amount of manuallyaligned corpora. To address the search challenge, we devise an iterative local search algorithm that stochastically explores reordering possibilities. By capturing non-local reordering phenomena, our proposed alignment model bears a closer resemblance to stateof-the-art translation model. Empirical results show significant improvements in alignment quality as well as in translation performance over baselines in a large-scale ChineseEnglish translation task.</p><p>3 0.67271107 <a title="29-lsi-3" href="./emnlp-2010-Hierarchical_Phrase-Based_Translation_Grammars_Extracted_from_Alignment_Posterior_Probabilities.html">57 emnlp-2010-Hierarchical Phrase-Based Translation Grammars Extracted from Alignment Posterior Probabilities</a></p>
<p>Author: Adria de Gispert ; Juan Pino ; William Byrne</p><p>Abstract: We report on investigations into hierarchical phrase-based translation grammars based on rules extracted from posterior distributions over alignments of the parallel text. Rather than restrict rule extraction to a single alignment, such as Viterbi, we instead extract rules based on posterior distributions provided by the HMM word-to-word alignmentmodel. We define translation grammars progressively by adding classes of rules to a basic phrase-based system. We assess these grammars in terms of their expressive power, measured by their ability to align the parallel text from which their rules are extracted, and the quality of the translations they yield. In Chinese-to-English translation, we find that rule extraction from posteriors gives translation improvements. We also find that grammars with rules with only one nonterminal, when extracted from posteri- ors, can outperform more complex grammars extracted from Viterbi alignments. Finally, we show that the best way to exploit source-totarget and target-to-source alignment models is to build two separate systems and combine their output translation lattices.</p><p>4 0.60439587 <a title="29-lsi-4" href="./emnlp-2010-A_Fast_Fertility_Hidden_Markov_Model_for_Word_Alignment_Using_MCMC.html">3 emnlp-2010-A Fast Fertility Hidden Markov Model for Word Alignment Using MCMC</a></p>
<p>Author: Shaojun Zhao ; Daniel Gildea</p><p>Abstract: A word in one language can be translated to zero, one, or several words in other languages. Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.</p><p>5 0.54961306 <a title="29-lsi-5" href="./emnlp-2010-A_Hybrid_Morpheme-Word_Representation_for_Machine_Translation_of_Morphologically_Rich_Languages.html">5 emnlp-2010-A Hybrid Morpheme-Word Representation for Machine Translation of Morphologically Rich Languages</a></p>
<p>Author: Minh-Thang Luong ; Preslav Nakov ; Min-Yen Kan</p><p>Abstract: We propose a language-independent approach for improving statistical machine translation for morphologically rich languages using a hybrid morpheme-word representation where the basic unit of translation is the morpheme, but word boundaries are respected at all stages of the translation process. Our model extends the classic phrase-based model by means of (1) word boundary-aware morpheme-level phrase extraction, (2) minimum error-rate training for a morpheme-level translation model using word-level BLEU, and (3) joint scoring with morpheme- and word-level language models. Further improvements are achieved by combining our model with the classic one. The evaluation on English to Finnish using Europarl (714K sentence pairs; 15.5M English words) shows statistically significant improvements over the classic model based on BLEU and human judgments.</p><p>6 0.51447517 <a title="29-lsi-6" href="./emnlp-2010-Automatic_Evaluation_of_Translation_Quality_for_Distant_Language_Pairs.html">22 emnlp-2010-Automatic Evaluation of Translation Quality for Distant Language Pairs</a></p>
<p>7 0.50365174 <a title="29-lsi-7" href="./emnlp-2010-Statistical_Machine_Translation_with_a_Factorized_Grammar.html">99 emnlp-2010-Statistical Machine Translation with a Factorized Grammar</a></p>
<p>8 0.49405888 <a title="29-lsi-8" href="./emnlp-2010-Assessing_Phrase-Based_Translation_Models_with_Oracle_Decoding.html">18 emnlp-2010-Assessing Phrase-Based Translation Models with Oracle Decoding</a></p>
<p>9 0.4657779 <a title="29-lsi-9" href="./emnlp-2010-It_Depends_on_the_Translation%3A_Unsupervised_Dependency_Parsing_via_Word_Alignment.html">67 emnlp-2010-It Depends on the Translation: Unsupervised Dependency Parsing via Word Alignment</a></p>
<p>10 0.448385 <a title="29-lsi-10" href="./emnlp-2010-Minimum_Error_Rate_Training_by_Sampling_the_Translation_Lattice.html">78 emnlp-2010-Minimum Error Rate Training by Sampling the Translation Lattice</a></p>
<p>11 0.42184654 <a title="29-lsi-11" href="./emnlp-2010-Discriminative_Sample_Selection_for_Statistical_Machine_Translation.html">35 emnlp-2010-Discriminative Sample Selection for Statistical Machine Translation</a></p>
<p>12 0.41230151 <a title="29-lsi-12" href="./emnlp-2010-EMNLP_044.html">39 emnlp-2010-EMNLP 044</a></p>
<p>13 0.39236301 <a title="29-lsi-13" href="./emnlp-2010-Improving_Translation_via_Targeted_Paraphrasing.html">63 emnlp-2010-Improving Translation via Targeted Paraphrasing</a></p>
<p>14 0.38406095 <a title="29-lsi-14" href="./emnlp-2010-Maximum_Entropy_Based_Phrase_Reordering_for_Hierarchical_Phrase-Based_Translation.html">76 emnlp-2010-Maximum Entropy Based Phrase Reordering for Hierarchical Phrase-Based Translation</a></p>
<p>15 0.36177251 <a title="29-lsi-15" href="./emnlp-2010-Further_Meta-Evaluation_of_Broad-Coverage_Surface_Realization.html">52 emnlp-2010-Further Meta-Evaluation of Broad-Coverage Surface Realization</a></p>
<p>16 0.33722207 <a title="29-lsi-16" href="./emnlp-2010-Handling_Noisy_Queries_in_Cross_Language_FAQ_Retrieval.html">55 emnlp-2010-Handling Noisy Queries in Cross Language FAQ Retrieval</a></p>
<p>17 0.33473939 <a title="29-lsi-17" href="./emnlp-2010-Crouching_Dirichlet%2C_Hidden_Markov_Model%3A_Unsupervised_POS_Tagging_with_Context_Local_Tag_Generation.html">34 emnlp-2010-Crouching Dirichlet, Hidden Markov Model: Unsupervised POS Tagging with Context Local Tag Generation</a></p>
<p>18 0.32275182 <a title="29-lsi-18" href="./emnlp-2010-Soft_Syntactic_Constraints_for_Hierarchical_Phrase-Based_Translation_Using_Latent_Syntactic_Distributions.html">98 emnlp-2010-Soft Syntactic Constraints for Hierarchical Phrase-Based Translation Using Latent Syntactic Distributions</a></p>
<p>19 0.30853477 <a title="29-lsi-19" href="./emnlp-2010-Facilitating_Translation_Using_Source_Language_Paraphrase_Lattices.html">50 emnlp-2010-Facilitating Translation Using Source Language Paraphrase Lattices</a></p>
<p>20 0.30457917 <a title="29-lsi-20" href="./emnlp-2010-Towards_Conversation_Entailment%3A_An_Empirical_Investigation.html">107 emnlp-2010-Towards Conversation Entailment: An Empirical Investigation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.017), (12, 0.029), (29, 0.198), (30, 0.03), (32, 0.02), (52, 0.123), (56, 0.062), (62, 0.015), (66, 0.109), (72, 0.031), (76, 0.022), (86, 0.221), (89, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81264967 <a title="29-lda-1" href="./emnlp-2010-Combining_Unsupervised_and_Supervised_Alignments_for_MT%3A_An_Empirical_Study.html">29 emnlp-2010-Combining Unsupervised and Supervised Alignments for MT: An Empirical Study</a></p>
<p>Author: Jinxi Xu ; Antti-Veikko Rosti</p><p>Abstract: Word alignment plays a central role in statistical MT (SMT) since almost all SMT systems extract translation rules from word aligned parallel training data. While most SMT systems use unsupervised algorithms (e.g. GIZA++) for training word alignment, supervised methods, which exploit a small amount of human-aligned data, have become increasingly popular recently. This work empirically studies the performance of these two classes of alignment algorithms and explores strategies to combine them to improve overall system performance. We used two unsupervised aligners, GIZA++ and HMM, and one supervised aligner, ITG, in this study. To avoid language and genre specific conclusions, we ran experiments on test sets consisting of two language pairs (Chinese-to-English and Arabicto-English) and two genres (newswire and weblog). Results show that the two classes of algorithms achieve the same level of MT perfor- mance. Modest improvements were achieved by taking the union of the translation grammars extracted from different alignments. Significant improvements (around 1.0 in BLEU) were achieved by combining outputs of different systems trained with different alignments. The improvements are consistent across languages and genres.</p><p>2 0.73503286 <a title="29-lda-2" href="./emnlp-2010-Hierarchical_Phrase-Based_Translation_Grammars_Extracted_from_Alignment_Posterior_Probabilities.html">57 emnlp-2010-Hierarchical Phrase-Based Translation Grammars Extracted from Alignment Posterior Probabilities</a></p>
<p>Author: Adria de Gispert ; Juan Pino ; William Byrne</p><p>Abstract: We report on investigations into hierarchical phrase-based translation grammars based on rules extracted from posterior distributions over alignments of the parallel text. Rather than restrict rule extraction to a single alignment, such as Viterbi, we instead extract rules based on posterior distributions provided by the HMM word-to-word alignmentmodel. We define translation grammars progressively by adding classes of rules to a basic phrase-based system. We assess these grammars in terms of their expressive power, measured by their ability to align the parallel text from which their rules are extracted, and the quality of the translations they yield. In Chinese-to-English translation, we find that rule extraction from posteriors gives translation improvements. We also find that grammars with rules with only one nonterminal, when extracted from posteri- ors, can outperform more complex grammars extracted from Viterbi alignments. Finally, we show that the best way to exploit source-totarget and target-to-source alignment models is to build two separate systems and combine their output translation lattices.</p><p>3 0.72276801 <a title="29-lda-3" href="./emnlp-2010-Soft_Syntactic_Constraints_for_Hierarchical_Phrase-Based_Translation_Using_Latent_Syntactic_Distributions.html">98 emnlp-2010-Soft Syntactic Constraints for Hierarchical Phrase-Based Translation Using Latent Syntactic Distributions</a></p>
<p>Author: Zhongqiang Huang ; Martin Cmejrek ; Bowen Zhou</p><p>Abstract: In this paper, we present a novel approach to enhance hierarchical phrase-based machine translation systems with linguistically motivated syntactic features. Rather than directly using treebank categories as in previous studies, we learn a set of linguistically-guided latent syntactic categories automatically from a source-side parsed, word-aligned parallel corpus, based on the hierarchical structure among phrase pairs as well as the syntactic structure of the source side. In our model, each X nonterminal in a SCFG rule is decorated with a real-valued feature vector computed based on its distribution of latent syntactic categories. These feature vectors are utilized at decod- ing time to measure the similarity between the syntactic analysis of the source side and the syntax of the SCFG rules that are applied to derive translations. Our approach maintains the advantages of hierarchical phrase-based translation systems while at the same time naturally incorporates soft syntactic constraints.</p><p>4 0.69794285 <a title="29-lda-4" href="./emnlp-2010-Discriminative_Word_Alignment_with_a_Function_Word_Reordering_Model.html">36 emnlp-2010-Discriminative Word Alignment with a Function Word Reordering Model</a></p>
<p>Author: Hendra Setiawan ; Chris Dyer ; Philip Resnik</p><p>Abstract: We address the modeling, parameter estimation and search challenges that arise from the introduction of reordering models that capture non-local reordering in alignment modeling. In particular, we introduce several reordering models that utilize (pairs of) function words as contexts for alignment reordering. To address the parameter estimation challenge, we propose to estimate these reordering models from a relatively small amount of manuallyaligned corpora. To address the search challenge, we devise an iterative local search algorithm that stochastically explores reordering possibilities. By capturing non-local reordering phenomena, our proposed alignment model bears a closer resemblance to stateof-the-art translation model. Empirical results show significant improvements in alignment quality as well as in translation performance over baselines in a large-scale ChineseEnglish translation task.</p><p>5 0.69634992 <a title="29-lda-5" href="./emnlp-2010-PEM%3A_A_Paraphrase_Evaluation_Metric_Exploiting_Parallel_Texts.html">89 emnlp-2010-PEM: A Paraphrase Evaluation Metric Exploiting Parallel Texts</a></p>
<p>Author: Chang Liu ; Daniel Dahlmeier ; Hwee Tou Ng</p><p>Abstract: We present PEM, the first fully automatic metric to evaluate the quality of paraphrases, and consequently, that of paraphrase generation systems. Our metric is based on three criteria: adequacy, fluency, and lexical dissimilarity. The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments.</p><p>6 0.69338256 <a title="29-lda-6" href="./emnlp-2010-Measuring_Distributional_Similarity_in_Context.html">77 emnlp-2010-Measuring Distributional Similarity in Context</a></p>
<p>7 0.69239539 <a title="29-lda-7" href="./emnlp-2010-Statistical_Machine_Translation_with_a_Factorized_Grammar.html">99 emnlp-2010-Statistical Machine Translation with a Factorized Grammar</a></p>
<p>8 0.69128686 <a title="29-lda-8" href="./emnlp-2010-Assessing_Phrase-Based_Translation_Models_with_Oracle_Decoding.html">18 emnlp-2010-Assessing Phrase-Based Translation Models with Oracle Decoding</a></p>
<p>9 0.6854775 <a title="29-lda-9" href="./emnlp-2010-Nouns_are_Vectors%2C_Adjectives_are_Matrices%3A_Representing_Adjective-Noun_Constructions_in_Semantic_Space.html">87 emnlp-2010-Nouns are Vectors, Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space</a></p>
<p>10 0.68525821 <a title="29-lda-10" href="./emnlp-2010-Crouching_Dirichlet%2C_Hidden_Markov_Model%3A_Unsupervised_POS_Tagging_with_Context_Local_Tag_Generation.html">34 emnlp-2010-Crouching Dirichlet, Hidden Markov Model: Unsupervised POS Tagging with Context Local Tag Generation</a></p>
<p>11 0.68424463 <a title="29-lda-11" href="./emnlp-2010-Minimum_Error_Rate_Training_by_Sampling_the_Translation_Lattice.html">78 emnlp-2010-Minimum Error Rate Training by Sampling the Translation Lattice</a></p>
<p>12 0.68264079 <a title="29-lda-12" href="./emnlp-2010-A_Mixture_Model_with_Sharing_for_Lexical_Semantics.html">7 emnlp-2010-A Mixture Model with Sharing for Lexical Semantics</a></p>
<p>13 0.68255305 <a title="29-lda-13" href="./emnlp-2010-Simple_Type-Level_Unsupervised_POS_Tagging.html">97 emnlp-2010-Simple Type-Level Unsupervised POS Tagging</a></p>
<p>14 0.68238479 <a title="29-lda-14" href="./emnlp-2010-Self-Training_with_Products_of_Latent_Variable_Grammars.html">96 emnlp-2010-Self-Training with Products of Latent Variable Grammars</a></p>
<p>15 0.68208617 <a title="29-lda-15" href="./emnlp-2010-Further_Meta-Evaluation_of_Broad-Coverage_Surface_Realization.html">52 emnlp-2010-Further Meta-Evaluation of Broad-Coverage Surface Realization</a></p>
<p>16 0.66700536 <a title="29-lda-16" href="./emnlp-2010-It_Depends_on_the_Translation%3A_Unsupervised_Dependency_Parsing_via_Word_Alignment.html">67 emnlp-2010-It Depends on the Translation: Unsupervised Dependency Parsing via Word Alignment</a></p>
<p>17 0.66345406 <a title="29-lda-17" href="./emnlp-2010-EMNLP_044.html">39 emnlp-2010-EMNLP 044</a></p>
<p>18 0.66102701 <a title="29-lda-18" href="./emnlp-2010-Translingual_Document_Representations_from_Discriminative_Projections.html">109 emnlp-2010-Translingual Document Representations from Discriminative Projections</a></p>
<p>19 0.65952271 <a title="29-lda-19" href="./emnlp-2010-Maximum_Entropy_Based_Phrase_Reordering_for_Hierarchical_Phrase-Based_Translation.html">76 emnlp-2010-Maximum Entropy Based Phrase Reordering for Hierarchical Phrase-Based Translation</a></p>
<p>20 0.65821385 <a title="29-lda-20" href="./emnlp-2010-Efficient_Incremental_Decoding_for_Tree-to-String_Translation.html">42 emnlp-2010-Efficient Incremental Decoding for Tree-to-String Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
