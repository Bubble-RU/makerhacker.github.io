<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>7 emnlp-2010-A Mixture Model with Sharing for Lexical Semantics</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2010" href="../home/emnlp2010_home.html">emnlp2010</a> <a title="emnlp-2010-7" href="#">emnlp2010-7</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>7 emnlp-2010-A Mixture Model with Sharing for Lexical Semantics</h1>
<br/><p>Source: <a title="emnlp-2010-7-pdf" href="http://aclweb.org/anthology//D/D10/D10-1114.pdf">pdf</a></p><p>Author: Joseph Reisinger ; Raymond Mooney</p><p>Abstract: We introduce tiered clustering, a mixture model capable of accounting for varying degrees of shared (context-independent) feature structure, and demonstrate its applicability to inferring distributed representations of word meaning. Common tasks in lexical semantics such as word relatedness or selectional preference can benefit from modeling such structure: Polysemous word usage is often governed by some common background metaphoric usage (e.g. the senses of line or run), and likewise modeling the selectional preference of verbs relies on identifying commonalities shared by their typical arguments. Tiered clustering can also be viewed as a form of soft feature selection, where features that do not contribute meaningfully to the clustering can be excluded. We demonstrate the applicability of tiered clustering, highlighting particular cases where modeling shared structure is beneficial and where it can be detrimental.</p><p>Reference: <a title="emnlp-2010-7-reference" href="../emnlp2010_reference/emnlp-2010-A_Mixture_Model_with_Sharing_for_Lexical_Semantics_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract We introduce tiered clustering, a mixture model capable of accounting for varying degrees of shared (context-independent) feature structure, and demonstrate its applicability to inferring distributed representations of word meaning. [sent-4, score-0.817]
</p><p>2 Common tasks in lexical semantics such as word relatedness or selectional preference can benefit from modeling such structure: Polysemous word usage is often governed by some common background metaphoric usage (e. [sent-5, score-0.603]
</p><p>3 the senses of line or run), and likewise modeling the selectional preference of verbs relies on identifying commonalities shared by their typical arguments. [sent-7, score-0.379]
</p><p>4 Tiered clustering can also be viewed as a form of soft feature selection, where features that do  not contribute meaningfully to the clustering can be excluded. [sent-8, score-0.495]
</p><p>5 We demonstrate the applicability of tiered clustering, highlighting particular cases where modeling shared structure is beneficial and where it can be detrimental. [sent-9, score-0.627]
</p><p>6 Such vector-space representations of meaning induce measures ofword similarity that can be tuned to correlate well with judgements made by humans. [sent-13, score-0.246]
</p><p>7 In this paper, we introduce tiered clustering, a novel probabilistic model of the shared structure often neglected in clustering problems. [sent-30, score-0.859]
</p><p>8 Tiered clustering performs soft feature selection, allocat-  ing features between a Dirichlet Process clustering model and a background model consisting of a single component. [sent-31, score-0.682]
</p><p>9 The background model accounts for features commonly shared by all occurrences (i. [sent-32, score-0.291]
</p><p>10 context-independent feature variation), while the clustering model accounts for variation in word usage (i. [sent-34, score-0.327]
</p><p>11 Using the tiered clustering model, we derive a multi-prototype representation capable of capturing varying degrees of sharing between word senses, and demonstrate its effectiveness in lexical semantic tasks where such sharing is desirable. [sent-37, score-0.956]
</p><p>12 In particular we show that tiered clustering outperforms the multi-prototype approach for (1) selectional preference (Resnik, 1997; Pantel et al. [sent-38, score-1.041]
</p><p>13 c od2s01 in0 N Aastsuorcaialt Lioan g foura Cgeom Prpoucteastisoin ga,l p Laignegsui 1s1ti7c3s–1 82, ing the typical filler of an argument slot of a verb, and (2) word-relatedness in the presence of highly polysemous words. [sent-43, score-0.186]
</p><p>14 Such models can be evaluated based on their correlation with human-reported lexical similarity judgements using e. [sent-54, score-0.371]
</p><p>15 Each boxed set shows the most common background (shared) features, and each prototype captures one thematic usage of the word. [sent-65, score-0.387]
</p><p>16 For example, wizard is broken up into a background cluster describing features common to all usages of the word (e. [sent-66, score-0.302]
</p><p>17 Distributional methods have also proven to be a powerful approach to modeling selectional preference (Pad o´ et al. [sent-72, score-0.236]
</p><p>18 In this section we briefly introduce a version of the multiprototype model based on the Dirichlet Process Mix-  Ñ  ture Model (DPMM), capable of inferring automatically the number of prototypes necessary for each word (Rasmussen, 2000). [sent-80, score-0.218]
</p><p>19 Multiple prototypes for each word w are generated by clustering feature vectors derived from each occurrence c P Cpwq in a large textual corpus an ocdc collecting Pthe C resulting acrlugsete tre cteuantlro coidrsπk , k P r1, Kws . [sent-82, score-0.401]
</p><p>20 The DPMM is an infinite capacity model capable of assigning data to a variable, but finite number of clusters Kw, with probability of assignment to cluster k proportional to the number of data points previously assigned to k. [sent-86, score-0.271]
</p><p>21 Using this model,  pwq  vpcq  the number of clusters no longer needs to be fixed a priori, allowing the model to allocate expressivity dynamically to concepts with richer structure. [sent-88, score-0.174]
</p><p>22 Such a model naturally allows the word representation to allocate additional capacity for highly polysemous words, with the number of clusters growing logarithmically with the number of occurrences. [sent-89, score-0.312]
</p><p>23 4  Tiered Clustering  Tiered clustering allocates features between two submodels: a (context-dependent) DPMM and a single (context-independent) background component. [sent-92, score-0.462]
</p><p>24 This model is similar structurally to the feature selective clustering model proposed by Law et al. [sent-93, score-0.263]
</p><p>25 However, instead of allocating entire feature dimensions between model and background compo1175  Figurβbeac1l:kug∞sPrtoelura! [sent-95, score-0.252]
</p><p>26 At a high level, the tiered model can be viewed as a combination of a multi-prototype model and a single-prototype back-off model. [sent-99, score-0.573]
</p><p>27 Concretely, each word occurrence wd first selects a cluster φd from the DPMM; then each feature wi,d is generated from either the background model φback or the selected cluster φd, determined by the tier  indicator zi,d. [sent-101, score-0.58]
</p><p>28 Since the background topic is shared across all occurrences, it can account for features with contextindependent variance, such as stop words and other high-frequency noise, as well as the central tendency of the collection (Table 1). [sent-104, score-0.273]
</p><p>29 5 Measuring Semantic Similarity Due to its richer representational structure, computing similarity in the multi-prototype model is less straightforward than in the single prototype case. [sent-108, score-0.265]
</p><p>30 Reisinger and Mooney (2010) found that simply averaging all similarity scores over all pairs of prototypes (sampled from the cluster distributions) performs reasonably well and is robust to noise. [sent-109, score-0.331]
</p><p>31 As cluster sizes become more uniform, AvgSim tends towards the single prototype similarity,1 hence the effectiveness of AvgSim stems from boosting the influence of small clusters. [sent-115, score-0.286]
</p><p>32 Tiered clustering representations offer more possibilities for computing semantic similarity than  multi-prototype, as the background prototype can be treated separately from the other prototypes. [sent-116, score-0.788]
</p><p>33 We make use of a simple sum of the distance between the two background components, and the AvgSim of the two sets of clustering components. [sent-117, score-0.419]
</p><p>34 2 Evaluation Methodology We evaluate the tiered clustering model on two problems from lexical semantics: word relatedness and selectional preference. [sent-126, score-1.043]
</p><p>35 For the word relatedness 1This can be problematic for certain clustering methods that specify uniform priors over cluster sizes; however the DPMM naturally exhibits a linear decay in cluster sizes with the Er# clusters of size Ms η{M. [sent-127, score-0.65]
</p><p>36 evaluation, we compared the predicted similarity of word pairs from each model to two collections ofhuman similarity judgements: WordSim-353 (Finkelstein et al. [sent-132, score-0.226]
</p><p>37 For selectional preference, we employ the Pad o´ dataset, which contains 211 verb-noun pairs with human similarity judgements for how plausible the noun is for each argument of the verb (2 arguments per verb, corresponding roughly to subject and object). [sent-139, score-0.433]
</p><p>38 In all cases correlation with human judgements is computed using Spearman’s nonparametric rank correlation (ρ) with average human judgements (Agirre et al. [sent-143, score-0.554]
</p><p>39 Finally, semantic similarity between word pairs is computed using cosine distance (‘2-normalized dot-product). [sent-148, score-0.196]
</p><p>40 4 Feature Pruning Feature pruning is one of the most significant factors in obtaining high correlation with human similarity judgements using vector-space models, and has been suggested as one way to improve sense disambiguation for polysemous verbs (Xue et al. [sent-150, score-0.585]
</p><p>41 In this section, we calibrate the single prototype and multiprototype methods on WS-353, reaching the limit of human and oracle performance and demonstrating robust performance gains even with semantically impoverished features. [sent-152, score-0.258]
</p><p>42 75 correlation on WS-353 using only unigram collocations and ρ 0. [sent-154, score-0.165]
</p><p>43 77 using a fixed-K multi-  prototype representation (Figure 3; Reisinger and Mooney, 2010). [sent-155, score-0.171]
</p><p>44 This result rivals average human performance, obtaining correlation near that of the supervised oracle approach of Agirre et al. [sent-156, score-0.165]
</p><p>45 The optimal pruning cutoff depends on the feature weighting and number of prototypes as well as the feature representation. [sent-158, score-0.214]
</p><p>46 Figure 4 breaks down the similarity pairs into four quantiles for each data set and then shows correlation separately for each quantile. [sent-161, score-0.398]
</p><p>47 In general ratings for highly similar (dissimilar) pairs are more predictable (quantiles 1 and 4) than middle similarity pairs (quantiles 2, 3). [sent-166, score-0.202]
</p><p>48 3  in semantic distance are easier for those Feature pruning improves correlations in quantiles 2–4 while reducing correlation in quantile 1 (lowest similarity). [sent-169, score-0.482]
</p><p>49 7  Results  We evaluate four models: (1) the standard singleprototype approach, (2) the DPMM multi-prototype approach outlined in §3, (3) a simple combinataipopnr oofa cthhe multi-prototype a (3nd) single-prototype approaches and (4) the tiered clustering approach (§4). [sent-171, score-0.805]
</p><p>50 Unless otherwise specified, both DPMM multi-prototype and tiered clustering  (MP+SP)4  c ionutont 5s,5 q  3The fact that the per-quantile correlation is significantly lower than the full correlation e. [sent-174, score-1.166]
</p><p>51 4(MP+SP) Tiered clustering’s ability to model both shared and idiosyncratic structure can be easily approximated by using the single prototype model as the shared component and multi-prototype model as the clustering. [sent-177, score-0.279]
</p><p>52 However, unlike in the tiered model, all features are assigned to both components. [sent-178, score-0.573]
</p><p>53 In general tf-idf features are the most sensitive to pruning level, yielding the highest correlation for moderate levels of pruning and significantly lower correlation than other representations without pruning. [sent-184, score-0.576]
</p><p>54 1, and tiered clustering uses α 10 for the background/clustering allocation smoother. [sent-190, score-0.805]
</p><p>55 In general the approaches incorporating multiple prototypes outperform single prototype (ρ 0. [sent-193, score-0.255]
</p><p>56 The tiered clustering model does not significantly outperform either the multi-prototype or MP+SP models on the full set, but yields significantly higher correlation on the high-polysemy set. [sent-197, score-1.032]
</p><p>57 The tiered model generates more clusters than DPMM multi-prototype (27. [sent-198, score-0.682]
</p><p>58 8), despite using the same hyperparameter settings: Since words commonly shared across clusters have been allocated to the background component, the cluster components have less overlap and hence the model naturally allocates more clusters. [sent-201, score-0.586]
</p><p>59 Examples of the tiered clusterings for several 1178 Method Single prototype high polysemy Multi-prototype high polysemy MP+SP  100  ErCs  background  73. [sent-202, score-1.351]
</p><p>60 All refers to the full set of pairs, high polysemy refers to the top 20% of pairs, ranked by sense count. [sent-238, score-0.232]
</p><p>61 ErCs is the average number ofclusters employed by each method and background is the average percentage of features allocated by the tiered model to the background cluster. [sent-239, score-0.995]
</p><p>62 In general the background component does indeed capture commonalities between all the sense clusters (e. [sent-242, score-0.387]
</p><p>63 all wizards use magic) and hence the tiered clusters are more semantically pure. [sent-244, score-0.682]
</p><p>64 Compared to the tiered clustering results in Table 1the multi-prototype clusters are significantly less pure for thematically poly-  semous words such as radio and wizard. [sent-250, score-0.945]
</p><p>65 On WN-Evocation, the single prototype and multi-prototype do not differ significantly in terms of correlation (ρ 0. [sent-253, score-0.367]
</p><p>66 201 respectively; Table 5), while SP+MP yields significantly lower correlation (ρ 0. [sent-255, score-0.196]
</p><p>67 176), and the tiered model yields significantly higher correlation (ρ 0. [sent-256, score-0.769]
</p><p>68 Restricting to the top 20% of pairs with highest human similarity judgements yields similar outcomes, with single prototype, multi-prototype and SP+MP statistically indistinguishable (ρ 0. [sent-258, score-0.244]
</p><p>69 235), and tiered clustering yielding significantly higher correlation (ρ 0. [sent-261, score-1.04]
</p><p>70 Likewise tiered clustering achieves the most significant gains on the high polysemy subset. [sent-263, score-0.997]
</p><p>71 Method Single prototype high polysemy Multi-prototype high polysemy  ρ ? [sent-319, score-0.555]
</p><p>72 4  -  MP+SP high polysemy Tiered high polysemy  19. [sent-336, score-0.384]
</p><p>73 The background component of the tiered clustering model can capture such general argument structure. [sent-358, score-0.992]
</p><p>74 We model each verb argument slot in the Pad o´ set with a separate tiered clustering model, separating terms co-occurring with the target verb according to which slot they fill. [sent-359, score-0.961]
</p><p>75 On the Pad o´ set, the performance of the DPMM multi-prototype approach breaks down and it yields significantly lower correlation with human norms than the single prototype (ρ 0. [sent-360, score-0.367]
</p><p>76 Furthermore combining with the single prototype does not significantly change its performance (ρ 0. [sent-364, score-0.202]
</p><p>77 Moving to the tiered model, however, yields significant improvements in correlation over the other models (ρ 0. [sent-366, score-0.738]
</p><p>78 294), primarily improving correlation in the  case of highly polysemous verbs and arguments. [sent-367, score-0.303]
</p><p>79 8  Discussion and Future Work  We have demonstrated a novel model for distributional lexical semantics capable of capturing both shared (context-independent) and idiosyncratic (context-dependent) structure in a set of word occurrences. [sent-368, score-0.203]
</p><p>80 The benefits of this tiered model were most pronounced on a selectional preference task, where there is significant shared structure imposed by conditioning on the verb. [sent-369, score-0.863]
</p><p>81 Although our results on the Pad o´ are not state of the art,6 we believe this to be due to the impoverished vector-space design; tiered clustering can be applied to more expressive vector spaces, such as those incorporating dependency parse and FrameNet features. [sent-370, score-0.805]
</p><p>82 One potential explanation for the superior performance of the tiered model vs. [sent-371, score-0.573]
</p><p>83 the DPMM multiprototype model is simply that it allocates more clusters to represent each word (Reisinger and Mooney, 2010). [sent-372, score-0.239]
</p><p>84 The additional clusters do  not provide more semantic content due to significant background similarity. [sent-375, score-0.36]
</p><p>85 Finally, the DPMM multi-prototype and tiered clustering models allocate clusters based on the variance of the underlying data set. [sent-376, score-0.979]
</p><p>86 33) between the number of clusters allocated by the DPMM and the number of word senses found in WordNet. [sent-379, score-0.195]
</p><p>87 This result is most likely due to our use of unigram context window features, which induce clustering based on thematic rather than syntactic differences. [sent-380, score-0.232]
</p><p>88 (Future Work) The word similarity experiments can be expanded by breaking pairs down further into highly homonymous and highly polysemous pairs, using e. [sent-382, score-0.336]
</p><p>89 With this data it would be interesting to validate the hypothesis that the percentage of features allocated to the background cluster is correlated with the degree of homonymy. [sent-385, score-0.35]
</p><p>90 The basic tiered clustering can be extended with additional background tiers, allocating more expressivity to model background feature variation. [sent-386, score-1.244]
</p><p>91 topic model (all background tiers) and a pure clustering model and may be reasonable when there is believed to be more background structure (e. [sent-392, score-0.638]
</p><p>92 9  Conclusions  This paper introduced a simple probabilistic model of tiered clustering inspired by feature selective clustering that leverages feature exchangeability to  allocate data features between a clustering model and shared component. [sent-408, score-1.484]
</p><p>93 The ability to model background variation, or shared structure, is shown to be beneficial for modeling words with high polysemy, yielding increased correlation with human similarity judgements modeling word relatedness and selectional preference. [sent-409, score-0.889]
</p><p>94 Furthermore, the tiered clustering model is shown to significantly outperform related models, yielding qualitatively more precise clusters. [sent-410, score-0.875]
</p><p>95 Word occurrence features are then drawn from a combination of a single cluster component indicated by cd and the background topic. [sent-415, score-0.356]
</p><p>96 7 The update rule for the latent tier indicator z is similar to the update rule for 2-topic LDA, with the background component as the first topic and the second topic being determined by the per-word-occurrence cluster indicator c. [sent-421, score-0.409]
</p><p>97 pi,dq is shorthand for the set z tzi,du, is the number of occurrences of word w in topic t not counting wi,d and is the number of features in occurrence d assigned to topic t, not counting wi,d. [sent-430, score-0.168]
</p><p>98 Likewise sampling the cluster indicators conditioned on the data ppcd |w, c? [sent-431, score-0.199]
</p><p>99 dq  is the number of occurrences  signed to k not including d,  Ñnpkdq is the vector  asof  counts of words from occurrence  wd  assigned to  7Effectively, the tiered clustering model is a special case of the nested Chinese Restaurant Process with the tree depth fixed to two (Blei et al. [sent-452, score-0.978]
</p><p>100 A study on similarity and relatedness using distributional and Wordnet-based approaches. [sent-461, score-0.232]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tiered', 0.573), ('dpmm', 0.236), ('clustering', 0.232), ('polysemy', 0.192), ('background', 0.187), ('prototype', 0.171), ('correlation', 0.165), ('selectional', 0.159), ('pad', 0.154), ('cluster', 0.115), ('judgements', 0.112), ('clusters', 0.109), ('polysemous', 0.106), ('mp', 0.104), ('reisinger', 0.104), ('quantiles', 0.101), ('similarity', 0.094), ('multiprototype', 0.087), ('ppcd', 0.084), ('quantile', 0.084), ('prototypes', 0.084), ('relatedness', 0.079), ('preference', 0.077), ('sp', 0.073), ('pruning', 0.068), ('avgsim', 0.068), ('ercs', 0.068), ('evocation', 0.068), ('allocate', 0.065), ('semantic', 0.064), ('mooney', 0.061), ('gabrilovich', 0.06), ('distributional', 0.059), ('finkelstein', 0.058), ('pantel', 0.058), ('occurrence', 0.054), ('shared', 0.054), ('commonalities', 0.051), ('markovitch', 0.051), ('occurrences', 0.05), ('allocated', 0.048), ('slot', 0.048), ('capable', 0.047), ('austin', 0.045), ('spearman', 0.045), ('allocates', 0.043), ('tier', 0.043), ('semantics', 0.043), ('restaurant', 0.042), ('dirichlet', 0.042), ('dp', 0.041), ('degrees', 0.04), ('sense', 0.04), ('representations', 0.04), ('yielding', 0.039), ('law', 0.039), ('senses', 0.038), ('pairs', 0.038), ('clusterings', 0.036), ('erk', 0.036), ('wd', 0.035), ('variation', 0.035), ('allocating', 0.034), ('dq', 0.034), ('exchangeability', 0.034), ('gdelen', 0.034), ('gorman', 0.034), ('guadalupe', 0.034), ('herda', 0.034), ('hindle', 0.034), ('homonymous', 0.034), ('ilarity', 0.034), ('magic', 0.034), ('nq', 0.034), ('ntpdq', 0.034), ('pado', 0.034), ('parsons', 0.034), ('raters', 0.034), ('sanborn', 0.034), ('selectionally', 0.034), ('shafto', 0.034), ('ulrike', 0.034), ('agirre', 0.034), ('cp', 0.034), ('pas', 0.034), ('sch', 0.033), ('highly', 0.032), ('accounting', 0.032), ('sebastian', 0.032), ('topic', 0.032), ('lda', 0.032), ('feature', 0.031), ('significantly', 0.031), ('griffiths', 0.031), ('verb', 0.03), ('wordnet', 0.03), ('hyperparameter', 0.03), ('landauer', 0.03), ('patrick', 0.03), ('usage', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="7-tfidf-1" href="./emnlp-2010-A_Mixture_Model_with_Sharing_for_Lexical_Semantics.html">7 emnlp-2010-A Mixture Model with Sharing for Lexical Semantics</a></p>
<p>Author: Joseph Reisinger ; Raymond Mooney</p><p>Abstract: We introduce tiered clustering, a mixture model capable of accounting for varying degrees of shared (context-independent) feature structure, and demonstrate its applicability to inferring distributed representations of word meaning. Common tasks in lexical semantics such as word relatedness or selectional preference can benefit from modeling such structure: Polysemous word usage is often governed by some common background metaphoric usage (e.g. the senses of line or run), and likewise modeling the selectional preference of verbs relies on identifying commonalities shared by their typical arguments. Tiered clustering can also be viewed as a form of soft feature selection, where features that do not contribute meaningfully to the clustering can be excluded. We demonstrate the applicability of tiered clustering, highlighting particular cases where modeling shared structure is beneficial and where it can be detrimental.</p><p>2 0.14307366 <a title="7-tfidf-2" href="./emnlp-2010-Two_Decades_of_Unsupervised_POS_Induction%3A_How_Far_Have_We_Come%3F.html">111 emnlp-2010-Two Decades of Unsupervised POS Induction: How Far Have We Come?</a></p>
<p>Author: Christos Christodoulopoulos ; Sharon Goldwater ; Mark Steedman</p><p>Abstract: Part-of-speech (POS) induction is one of the most popular tasks in research on unsupervised NLP. Many different methods have been proposed, yet comparisons are difficult to make since there is little consensus on evaluation framework, and many papers evaluate against only one or two competitor systems. Here we evaluate seven different POS induction systems spanning nearly 20 years of work, using a variety of measures. We show that some of the oldest (and simplest) systems stand up surprisingly well against more recent approaches. Since most of these systems were developed and tested using data from the WSJ corpus, we compare their generalization abil- ities by testing on both WSJ and the multilingual Multext-East corpus. Finally, we introduce the idea of evaluating systems based on their ability to produce cluster prototypes that are useful as input to a prototype-driven learner. In most cases, the prototype-driven learner outperforms the unsupervised system used to initialize it, yielding state-of-the-art results on WSJ and improvements on nonEnglish corpora.</p><p>3 0.14107576 <a title="7-tfidf-3" href="./emnlp-2010-Measuring_Distributional_Similarity_in_Context.html">77 emnlp-2010-Measuring Distributional Similarity in Context</a></p>
<p>Author: Georgiana Dinu ; Mirella Lapata</p><p>Abstract: The computation of meaning similarity as operationalized by vector-based models has found widespread use in many tasks ranging from the acquisition of synonyms and paraphrases to word sense disambiguation and textual entailment. Vector-based models are typically directed at representing words in isolation and thus best suited for measuring similarity out of context. In his paper we propose a probabilistic framework for measuring similarity in context. Central to our approach is the intuition that word meaning is represented as a probability distribution over a set of latent senses and is modulated by context. Experimental results on lexical substitution and word similarity show that our algorithm outperforms previously proposed models.</p><p>4 0.1102255 <a title="7-tfidf-4" href="./emnlp-2010-Inducing_Word_Senses_to_Improve_Web_Search_Result_Clustering.html">66 emnlp-2010-Inducing Word Senses to Improve Web Search Result Clustering</a></p>
<p>Author: Roberto Navigli ; Giuseppe Crisafulli</p><p>Abstract: In this paper, we present a novel approach to Web search result clustering based on the automatic discovery of word senses from raw text, a task referred to as Word Sense Induction (WSI). We first acquire the senses (i.e., meanings) of a query by means of a graphbased clustering algorithm that exploits cycles (triangles and squares) in the co-occurrence graph of the query. Then we cluster the search results based on their semantic similarity to the induced word senses. Our experiments, conducted on datasets of ambiguous queries, show that our approach improves search result clustering in terms of both clustering quality and degree of diversification.</p><p>5 0.11008683 <a title="7-tfidf-5" href="./emnlp-2010-Clustering-Based_Stratified_Seed_Sampling_for_Semi-Supervised_Relation_Classification.html">27 emnlp-2010-Clustering-Based Stratified Seed Sampling for Semi-Supervised Relation Classification</a></p>
<p>Author: Longhua Qian ; Guodong Zhou</p><p>Abstract: Seed sampling is critical in semi-supervised learning. This paper proposes a clusteringbased stratified seed sampling approach to semi-supervised learning. First, various clustering algorithms are explored to partition the unlabeled instances into different strata with each stratum represented by a center. Then, diversity-motivated intra-stratum sampling is adopted to choose the center and additional instances from each stratum to form the unlabeled seed set for an oracle to annotate. Finally, the labeled seed set is fed into a bootstrapping procedure as the initial labeled data. We systematically evaluate our stratified bootstrapping approach in the semantic relation classification subtask of the ACE RDC (Relation Detection and Classification) task. In particular, we compare various clustering algorithms on the stratified bootstrapping performance. Experimental results on the ACE RDC 2004 corpus show that our clusteringbased stratified bootstrapping approach achieves the best F1-score of 75.9 on the subtask of semantic relation classification, approaching the one with golden clustering.</p><p>6 0.093331322 <a title="7-tfidf-6" href="./emnlp-2010-NLP_on_Spoken_Documents_Without_ASR.html">84 emnlp-2010-NLP on Spoken Documents Without ASR</a></p>
<p>7 0.071966708 <a title="7-tfidf-7" href="./emnlp-2010-Nouns_are_Vectors%2C_Adjectives_are_Matrices%3A_Representing_Adjective-Noun_Constructions_in_Semantic_Space.html">87 emnlp-2010-Nouns are Vectors, Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space</a></p>
<p>8 0.071644284 <a title="7-tfidf-8" href="./emnlp-2010-Latent-Descriptor_Clustering_for_Unsupervised_POS_Induction.html">71 emnlp-2010-Latent-Descriptor Clustering for Unsupervised POS Induction</a></p>
<p>9 0.069865316 <a title="7-tfidf-9" href="./emnlp-2010-Word_Sense_Induction__Disambiguation_Using_Hierarchical_Random_Graphs.html">124 emnlp-2010-Word Sense Induction  Disambiguation Using Hierarchical Random Graphs</a></p>
<p>10 0.063857757 <a title="7-tfidf-10" href="./emnlp-2010-Unsupervised_Discovery_of_Negative_Categories_in_Lexicon_Bootstrapping.html">112 emnlp-2010-Unsupervised Discovery of Negative Categories in Lexicon Bootstrapping</a></p>
<p>11 0.058464974 <a title="7-tfidf-11" href="./emnlp-2010-Automatic_Evaluation_of_Translation_Quality_for_Distant_Language_Pairs.html">22 emnlp-2010-Automatic Evaluation of Translation Quality for Distant Language Pairs</a></p>
<p>12 0.056735184 <a title="7-tfidf-12" href="./emnlp-2010-Simple_Type-Level_Unsupervised_POS_Tagging.html">97 emnlp-2010-Simple Type-Level Unsupervised POS Tagging</a></p>
<p>13 0.055594094 <a title="7-tfidf-13" href="./emnlp-2010-Holistic_Sentiment_Analysis_Across_Languages%3A_Multilingual_Supervised_Latent_Dirichlet_Allocation.html">58 emnlp-2010-Holistic Sentiment Analysis Across Languages: Multilingual Supervised Latent Dirichlet Allocation</a></p>
<p>14 0.055044468 <a title="7-tfidf-14" href="./emnlp-2010-A_Semi-Supervised_Method_to_Learn_and_Construct_Taxonomies_Using_the_Web.html">12 emnlp-2010-A Semi-Supervised Method to Learn and Construct Taxonomies Using the Web</a></p>
<p>15 0.05312404 <a title="7-tfidf-15" href="./emnlp-2010-A_Multi-Pass_Sieve_for_Coreference_Resolution.html">8 emnlp-2010-A Multi-Pass Sieve for Coreference Resolution</a></p>
<p>16 0.0514239 <a title="7-tfidf-16" href="./emnlp-2010-SRL-Based_Verb_Selection_for_ESL.html">95 emnlp-2010-SRL-Based Verb Selection for ESL</a></p>
<p>17 0.050421331 <a title="7-tfidf-17" href="./emnlp-2010-A_Latent_Variable_Model_for_Geographic_Lexical_Variation.html">6 emnlp-2010-A Latent Variable Model for Geographic Lexical Variation</a></p>
<p>18 0.049371284 <a title="7-tfidf-18" href="./emnlp-2010-Evaluating_Models_of_Latent_Document_Semantics_in_the_Presence_of_OCR_Errors.html">45 emnlp-2010-Evaluating Models of Latent Document Semantics in the Presence of OCR Errors</a></p>
<p>19 0.048982434 <a title="7-tfidf-19" href="./emnlp-2010-Storing_the_Web_in_Memory%3A_Space_Efficient_Language_Models_with_Constant_Time_Retrieval.html">101 emnlp-2010-Storing the Web in Memory: Space Efficient Language Models with Constant Time Retrieval</a></p>
<p>20 0.048052356 <a title="7-tfidf-20" href="./emnlp-2010-Staying_Informed%3A_Supervised_and_Semi-Supervised_Multi-View_Topical_Analysis_of_Ideological_Perspective.html">100 emnlp-2010-Staying Informed: Supervised and Semi-Supervised Multi-View Topical Analysis of Ideological Perspective</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.174), (1, 0.127), (2, -0.083), (3, 0.048), (4, 0.034), (5, 0.034), (6, -0.221), (7, -0.001), (8, 0.191), (9, 0.092), (10, 0.096), (11, -0.081), (12, -0.102), (13, -0.117), (14, -0.014), (15, -0.224), (16, -0.023), (17, -0.104), (18, 0.117), (19, -0.106), (20, -0.007), (21, 0.083), (22, -0.034), (23, -0.064), (24, 0.125), (25, 0.03), (26, 0.087), (27, 0.045), (28, 0.043), (29, 0.003), (30, 0.001), (31, -0.001), (32, -0.035), (33, 0.081), (34, 0.017), (35, 0.075), (36, -0.022), (37, 0.128), (38, -0.149), (39, -0.021), (40, -0.009), (41, 0.141), (42, 0.034), (43, -0.139), (44, -0.0), (45, 0.038), (46, 0.101), (47, -0.019), (48, 0.133), (49, -0.002)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95785034 <a title="7-lsi-1" href="./emnlp-2010-A_Mixture_Model_with_Sharing_for_Lexical_Semantics.html">7 emnlp-2010-A Mixture Model with Sharing for Lexical Semantics</a></p>
<p>Author: Joseph Reisinger ; Raymond Mooney</p><p>Abstract: We introduce tiered clustering, a mixture model capable of accounting for varying degrees of shared (context-independent) feature structure, and demonstrate its applicability to inferring distributed representations of word meaning. Common tasks in lexical semantics such as word relatedness or selectional preference can benefit from modeling such structure: Polysemous word usage is often governed by some common background metaphoric usage (e.g. the senses of line or run), and likewise modeling the selectional preference of verbs relies on identifying commonalities shared by their typical arguments. Tiered clustering can also be viewed as a form of soft feature selection, where features that do not contribute meaningfully to the clustering can be excluded. We demonstrate the applicability of tiered clustering, highlighting particular cases where modeling shared structure is beneficial and where it can be detrimental.</p><p>2 0.54374713 <a title="7-lsi-2" href="./emnlp-2010-Two_Decades_of_Unsupervised_POS_Induction%3A_How_Far_Have_We_Come%3F.html">111 emnlp-2010-Two Decades of Unsupervised POS Induction: How Far Have We Come?</a></p>
<p>Author: Christos Christodoulopoulos ; Sharon Goldwater ; Mark Steedman</p><p>Abstract: Part-of-speech (POS) induction is one of the most popular tasks in research on unsupervised NLP. Many different methods have been proposed, yet comparisons are difficult to make since there is little consensus on evaluation framework, and many papers evaluate against only one or two competitor systems. Here we evaluate seven different POS induction systems spanning nearly 20 years of work, using a variety of measures. We show that some of the oldest (and simplest) systems stand up surprisingly well against more recent approaches. Since most of these systems were developed and tested using data from the WSJ corpus, we compare their generalization abil- ities by testing on both WSJ and the multilingual Multext-East corpus. Finally, we introduce the idea of evaluating systems based on their ability to produce cluster prototypes that are useful as input to a prototype-driven learner. In most cases, the prototype-driven learner outperforms the unsupervised system used to initialize it, yielding state-of-the-art results on WSJ and improvements on nonEnglish corpora.</p><p>3 0.51255447 <a title="7-lsi-3" href="./emnlp-2010-Measuring_Distributional_Similarity_in_Context.html">77 emnlp-2010-Measuring Distributional Similarity in Context</a></p>
<p>Author: Georgiana Dinu ; Mirella Lapata</p><p>Abstract: The computation of meaning similarity as operationalized by vector-based models has found widespread use in many tasks ranging from the acquisition of synonyms and paraphrases to word sense disambiguation and textual entailment. Vector-based models are typically directed at representing words in isolation and thus best suited for measuring similarity out of context. In his paper we propose a probabilistic framework for measuring similarity in context. Central to our approach is the intuition that word meaning is represented as a probability distribution over a set of latent senses and is modulated by context. Experimental results on lexical substitution and word similarity show that our algorithm outperforms previously proposed models.</p><p>4 0.49725235 <a title="7-lsi-4" href="./emnlp-2010-Inducing_Word_Senses_to_Improve_Web_Search_Result_Clustering.html">66 emnlp-2010-Inducing Word Senses to Improve Web Search Result Clustering</a></p>
<p>Author: Roberto Navigli ; Giuseppe Crisafulli</p><p>Abstract: In this paper, we present a novel approach to Web search result clustering based on the automatic discovery of word senses from raw text, a task referred to as Word Sense Induction (WSI). We first acquire the senses (i.e., meanings) of a query by means of a graphbased clustering algorithm that exploits cycles (triangles and squares) in the co-occurrence graph of the query. Then we cluster the search results based on their semantic similarity to the induced word senses. Our experiments, conducted on datasets of ambiguous queries, show that our approach improves search result clustering in terms of both clustering quality and degree of diversification.</p><p>5 0.47826037 <a title="7-lsi-5" href="./emnlp-2010-NLP_on_Spoken_Documents_Without_ASR.html">84 emnlp-2010-NLP on Spoken Documents Without ASR</a></p>
<p>Author: Mark Dredze ; Aren Jansen ; Glen Coppersmith ; Ken Church</p><p>Abstract: There is considerable interest in interdisciplinary combinations of automatic speech recognition (ASR), machine learning, natural language processing, text classification and information retrieval. Many of these boxes, especially ASR, are often based on considerable linguistic resources. We would like to be able to process spoken documents with few (if any) resources. Moreover, connecting black boxes in series tends to multiply errors, especially when the key terms are out-ofvocabulary (OOV). The proposed alternative applies text processing directly to the speech without a dependency on ASR. The method finds long (∼ 1 sec) repetitions in speech, fainndd scl luostnegrs t∼he 1m sinecto) pseudo-terms (roughly phrases). Document clustering and classification work surprisingly well on pseudoterms; performance on a Switchboard task approaches a baseline using gold standard man- ual transcriptions.</p><p>6 0.45564505 <a title="7-lsi-6" href="./emnlp-2010-Clustering-Based_Stratified_Seed_Sampling_for_Semi-Supervised_Relation_Classification.html">27 emnlp-2010-Clustering-Based Stratified Seed Sampling for Semi-Supervised Relation Classification</a></p>
<p>7 0.424449 <a title="7-lsi-7" href="./emnlp-2010-Word_Sense_Induction__Disambiguation_Using_Hierarchical_Random_Graphs.html">124 emnlp-2010-Word Sense Induction  Disambiguation Using Hierarchical Random Graphs</a></p>
<p>8 0.38534552 <a title="7-lsi-8" href="./emnlp-2010-Nouns_are_Vectors%2C_Adjectives_are_Matrices%3A_Representing_Adjective-Noun_Constructions_in_Semantic_Space.html">87 emnlp-2010-Nouns are Vectors, Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space</a></p>
<p>9 0.33444846 <a title="7-lsi-9" href="./emnlp-2010-A_Multi-Pass_Sieve_for_Coreference_Resolution.html">8 emnlp-2010-A Multi-Pass Sieve for Coreference Resolution</a></p>
<p>10 0.31999022 <a title="7-lsi-10" href="./emnlp-2010-Latent-Descriptor_Clustering_for_Unsupervised_POS_Induction.html">71 emnlp-2010-Latent-Descriptor Clustering for Unsupervised POS Induction</a></p>
<p>11 0.31122687 <a title="7-lsi-11" href="./emnlp-2010-Evaluating_Models_of_Latent_Document_Semantics_in_the_Presence_of_OCR_Errors.html">45 emnlp-2010-Evaluating Models of Latent Document Semantics in the Presence of OCR Errors</a></p>
<p>12 0.29917216 <a title="7-lsi-12" href="./emnlp-2010-Storing_the_Web_in_Memory%3A_Space_Efficient_Language_Models_with_Constant_Time_Retrieval.html">101 emnlp-2010-Storing the Web in Memory: Space Efficient Language Models with Constant Time Retrieval</a></p>
<p>13 0.29050234 <a title="7-lsi-13" href="./emnlp-2010-Predicting_the_Semantic_Compositionality_of_Prefix_Verbs.html">92 emnlp-2010-Predicting the Semantic Compositionality of Prefix Verbs</a></p>
<p>14 0.22656067 <a title="7-lsi-14" href="./emnlp-2010-A_Simple_Domain-Independent_Probabilistic_Approach_to_Generation.html">13 emnlp-2010-A Simple Domain-Independent Probabilistic Approach to Generation</a></p>
<p>15 0.22194968 <a title="7-lsi-15" href="./emnlp-2010-Staying_Informed%3A_Supervised_and_Semi-Supervised_Multi-View_Topical_Analysis_of_Ideological_Perspective.html">100 emnlp-2010-Staying Informed: Supervised and Semi-Supervised Multi-View Topical Analysis of Ideological Perspective</a></p>
<p>16 0.21849765 <a title="7-lsi-16" href="./emnlp-2010-Unsupervised_Discovery_of_Negative_Categories_in_Lexicon_Bootstrapping.html">112 emnlp-2010-Unsupervised Discovery of Negative Categories in Lexicon Bootstrapping</a></p>
<p>17 0.21716569 <a title="7-lsi-17" href="./emnlp-2010-A_Semi-Supervised_Method_to_Learn_and_Construct_Taxonomies_Using_the_Web.html">12 emnlp-2010-A Semi-Supervised Method to Learn and Construct Taxonomies Using the Web</a></p>
<p>18 0.20589216 <a title="7-lsi-18" href="./emnlp-2010-Further_Meta-Evaluation_of_Broad-Coverage_Surface_Realization.html">52 emnlp-2010-Further Meta-Evaluation of Broad-Coverage Surface Realization</a></p>
<p>19 0.20029587 <a title="7-lsi-19" href="./emnlp-2010-A_Latent_Variable_Model_for_Geographic_Lexical_Variation.html">6 emnlp-2010-A Latent Variable Model for Geographic Lexical Variation</a></p>
<p>20 0.19827789 <a title="7-lsi-20" href="./emnlp-2010-SRL-Based_Verb_Selection_for_ESL.html">95 emnlp-2010-SRL-Based Verb Selection for ESL</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.014), (10, 0.012), (12, 0.035), (17, 0.014), (22, 0.01), (27, 0.113), (28, 0.132), (29, 0.175), (30, 0.048), (32, 0.021), (52, 0.027), (56, 0.064), (62, 0.016), (66, 0.121), (72, 0.04), (76, 0.025), (77, 0.012), (87, 0.024), (89, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89332289 <a title="7-lda-1" href="./emnlp-2010-Efficient_Incremental_Decoding_for_Tree-to-String_Translation.html">42 emnlp-2010-Efficient Incremental Decoding for Tree-to-String Translation</a></p>
<p>Author: Liang Huang ; Haitao Mi</p><p>Abstract: Syntax-based translation models should in principle be efficient with polynomially-sized search space, but in practice they are often embarassingly slow, partly due to the cost of language model integration. In this paper we borrow from phrase-based decoding the idea to generate a translation incrementally left-to-right, and show that for tree-to-string models, with a clever encoding of derivation history, this method runs in averagecase polynomial-time in theory, and lineartime with beam search in practice (whereas phrase-based decoding is exponential-time in theory and quadratic-time in practice). Experiments show that, with comparable translation quality, our tree-to-string system (in Python) can run more than 30 times faster than the phrase-based system Moses (in C++).</p><p>same-paper 2 0.88007718 <a title="7-lda-2" href="./emnlp-2010-A_Mixture_Model_with_Sharing_for_Lexical_Semantics.html">7 emnlp-2010-A Mixture Model with Sharing for Lexical Semantics</a></p>
<p>Author: Joseph Reisinger ; Raymond Mooney</p><p>Abstract: We introduce tiered clustering, a mixture model capable of accounting for varying degrees of shared (context-independent) feature structure, and demonstrate its applicability to inferring distributed representations of word meaning. Common tasks in lexical semantics such as word relatedness or selectional preference can benefit from modeling such structure: Polysemous word usage is often governed by some common background metaphoric usage (e.g. the senses of line or run), and likewise modeling the selectional preference of verbs relies on identifying commonalities shared by their typical arguments. Tiered clustering can also be viewed as a form of soft feature selection, where features that do not contribute meaningfully to the clustering can be excluded. We demonstrate the applicability of tiered clustering, highlighting particular cases where modeling shared structure is beneficial and where it can be detrimental.</p><p>3 0.77765667 <a title="7-lda-3" href="./emnlp-2010-PEM%3A_A_Paraphrase_Evaluation_Metric_Exploiting_Parallel_Texts.html">89 emnlp-2010-PEM: A Paraphrase Evaluation Metric Exploiting Parallel Texts</a></p>
<p>Author: Chang Liu ; Daniel Dahlmeier ; Hwee Tou Ng</p><p>Abstract: We present PEM, the first fully automatic metric to evaluate the quality of paraphrases, and consequently, that of paraphrase generation systems. Our metric is based on three criteria: adequacy, fluency, and lexical dissimilarity. The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments.</p><p>4 0.76707947 <a title="7-lda-4" href="./emnlp-2010-Hierarchical_Phrase-Based_Translation_Grammars_Extracted_from_Alignment_Posterior_Probabilities.html">57 emnlp-2010-Hierarchical Phrase-Based Translation Grammars Extracted from Alignment Posterior Probabilities</a></p>
<p>Author: Adria de Gispert ; Juan Pino ; William Byrne</p><p>Abstract: We report on investigations into hierarchical phrase-based translation grammars based on rules extracted from posterior distributions over alignments of the parallel text. Rather than restrict rule extraction to a single alignment, such as Viterbi, we instead extract rules based on posterior distributions provided by the HMM word-to-word alignmentmodel. We define translation grammars progressively by adding classes of rules to a basic phrase-based system. We assess these grammars in terms of their expressive power, measured by their ability to align the parallel text from which their rules are extracted, and the quality of the translations they yield. In Chinese-to-English translation, we find that rule extraction from posteriors gives translation improvements. We also find that grammars with rules with only one nonterminal, when extracted from posteri- ors, can outperform more complex grammars extracted from Viterbi alignments. Finally, we show that the best way to exploit source-totarget and target-to-source alignment models is to build two separate systems and combine their output translation lattices.</p><p>5 0.76667237 <a title="7-lda-5" href="./emnlp-2010-Crouching_Dirichlet%2C_Hidden_Markov_Model%3A_Unsupervised_POS_Tagging_with_Context_Local_Tag_Generation.html">34 emnlp-2010-Crouching Dirichlet, Hidden Markov Model: Unsupervised POS Tagging with Context Local Tag Generation</a></p>
<p>Author: Taesun Moon ; Katrin Erk ; Jason Baldridge</p><p>Abstract: We define the crouching Dirichlet, hidden Markov model (CDHMM), an HMM for partof-speech tagging which draws state prior distributions for each local document context. This simple modification of the HMM takes advantage of the dichotomy in natural language between content and function words. In contrast, a standard HMM draws all prior distributions once over all states and it is known to perform poorly in unsupervised and semisupervised POS tagging. This modification significantly improves unsupervised POS tagging performance across several measures on five data sets for four languages. We also show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM+) is surprisingly effective.</p><p>6 0.76366627 <a title="7-lda-6" href="./emnlp-2010-Nouns_are_Vectors%2C_Adjectives_are_Matrices%3A_Representing_Adjective-Noun_Constructions_in_Semantic_Space.html">87 emnlp-2010-Nouns are Vectors, Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space</a></p>
<p>7 0.76320535 <a title="7-lda-7" href="./emnlp-2010-Minimum_Error_Rate_Training_by_Sampling_the_Translation_Lattice.html">78 emnlp-2010-Minimum Error Rate Training by Sampling the Translation Lattice</a></p>
<p>8 0.76109445 <a title="7-lda-8" href="./emnlp-2010-Measuring_Distributional_Similarity_in_Context.html">77 emnlp-2010-Measuring Distributional Similarity in Context</a></p>
<p>9 0.75446314 <a title="7-lda-9" href="./emnlp-2010-Statistical_Machine_Translation_with_a_Factorized_Grammar.html">99 emnlp-2010-Statistical Machine Translation with a Factorized Grammar</a></p>
<p>10 0.75044876 <a title="7-lda-10" href="./emnlp-2010-Simple_Type-Level_Unsupervised_POS_Tagging.html">97 emnlp-2010-Simple Type-Level Unsupervised POS Tagging</a></p>
<p>11 0.74723744 <a title="7-lda-11" href="./emnlp-2010-Self-Training_with_Products_of_Latent_Variable_Grammars.html">96 emnlp-2010-Self-Training with Products of Latent Variable Grammars</a></p>
<p>12 0.74713093 <a title="7-lda-12" href="./emnlp-2010-Improving_Translation_via_Targeted_Paraphrasing.html">63 emnlp-2010-Improving Translation via Targeted Paraphrasing</a></p>
<p>13 0.74674392 <a title="7-lda-13" href="./emnlp-2010-Using_Universal_Linguistic_Knowledge_to_Guide_Grammar_Induction.html">116 emnlp-2010-Using Universal Linguistic Knowledge to Guide Grammar Induction</a></p>
<p>14 0.74634564 <a title="7-lda-14" href="./emnlp-2010-Soft_Syntactic_Constraints_for_Hierarchical_Phrase-Based_Translation_Using_Latent_Syntactic_Distributions.html">98 emnlp-2010-Soft Syntactic Constraints for Hierarchical Phrase-Based Translation Using Latent Syntactic Distributions</a></p>
<p>15 0.74622744 <a title="7-lda-15" href="./emnlp-2010-Inducing_Probabilistic_CCG_Grammars_from_Logical_Form_with_Higher-Order_Unification.html">65 emnlp-2010-Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification</a></p>
<p>16 0.74464869 <a title="7-lda-16" href="./emnlp-2010-A_Latent_Variable_Model_for_Geographic_Lexical_Variation.html">6 emnlp-2010-A Latent Variable Model for Geographic Lexical Variation</a></p>
<p>17 0.74359947 <a title="7-lda-17" href="./emnlp-2010-Assessing_Phrase-Based_Translation_Models_with_Oracle_Decoding.html">18 emnlp-2010-Assessing Phrase-Based Translation Models with Oracle Decoding</a></p>
<p>18 0.74352747 <a title="7-lda-18" href="./emnlp-2010-Title_Generation_with_Quasi-Synchronous_Grammar.html">105 emnlp-2010-Title Generation with Quasi-Synchronous Grammar</a></p>
<p>19 0.74176973 <a title="7-lda-19" href="./emnlp-2010-It_Depends_on_the_Translation%3A_Unsupervised_Dependency_Parsing_via_Word_Alignment.html">67 emnlp-2010-It Depends on the Translation: Unsupervised Dependency Parsing via Word Alignment</a></p>
<p>20 0.74135566 <a title="7-lda-20" href="./emnlp-2010-Translingual_Document_Representations_from_Discriminative_Projections.html">109 emnlp-2010-Translingual Document Representations from Discriminative Projections</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
