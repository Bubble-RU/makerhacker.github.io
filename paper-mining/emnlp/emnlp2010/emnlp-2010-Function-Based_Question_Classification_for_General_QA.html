<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>51 emnlp-2010-Function-Based Question Classification for General QA</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2010" href="../home/emnlp2010_home.html">emnlp2010</a> <a title="emnlp-2010-51" href="#">emnlp2010-51</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>51 emnlp-2010-Function-Based Question Classification for General QA</h1>
<br/><p>Source: <a title="emnlp-2010-51-pdf" href="http://aclweb.org/anthology//D/D10/D10-1109.pdf">pdf</a></p><p>Author: Fan Bu ; Xingwei Zhu ; Yu Hao ; Xiaoyan Zhu</p><p>Abstract: In contrast with the booming increase of internet data, state-of-art QA (question answering) systems, otherwise, concerned data from specific domains or resources such as search engine snippets, online forums and Wikipedia in a somewhat isolated way. Users may welcome a more general QA system for its capability to answer questions of various sources, integrated from existed specialized sub-QA engines. In this framework, question classification is the primary task. However, the current paradigms of question classification were focused on some specified type of questions, i.e. factoid questions, which are inappropriate for the general QA. In this paper, we propose a new question classification paradigm, which includes a question taxonomy suitable to the general QA and a question classifier based on MLN (Markov logic network), where rule-based methods and statistical methods are unified into a single framework in a fuzzy discriminative learning approach. Experiments show that our method outperforms traditional question classification approaches.</p><p>Reference: <a title="emnlp-2010-51-reference" href="../emnlp2010_reference/emnlp-2010-Function-Based_Question_Classification_for_General_QA_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Users may welcome a more general QA system for its capability to answer questions of various sources, integrated from existed specialized sub-QA engines. [sent-13, score-0.508]
</p><p>2 However, the current paradigms of question classification were focused on some specified type of questions, i. [sent-15, score-0.347]
</p><p>3 1 Introduction During a long period of time, researches on question answering are mainly focused on finding short and concise answers from plain text for factoid questions driven by annual trackes such as CLEF, TREC and NTCIR. [sent-20, score-0.906]
</p><p>4 However, people usually ask more complex questions in real world which cannot be handled by these QA systems tailored to factoid questions. [sent-21, score-0.756]
</p><p>5 For example, the answers for factoid questions  can be extracted from webpages with high accuracy, definition questions can be answered by corresponding articles in wikipedia(Ye et al. [sent-26, score-1.009]
</p><p>6 , 2009) while community question answering services provide comprehensive answers for complex questions(Jeon et al. [sent-27, score-0.369]
</p><p>7 It will greatly enhance the overall performance if we can classify questions into several types, distribute each type of questions to suitable sources and trigger corresponding strategy to summarize returned answers. [sent-29, score-0.959]
</p><p>8 Question classification (QC) in factoid QA is to provide constraints on answer types that allows further processing to pinpoint and verify the answer (Li and Roth, 2004). [sent-30, score-0.467]
</p><p>9 Usually, questions are classified into a fine grained content-based taxonomy(e. [sent-31, score-0.421]
</p><p>10 To guide question distribution and answer summarization, questions are classified according to their functions instead of contents. [sent-35, score-0.782]
</p><p>11 Motivated by related work on user goal classification(Broder, 2002; Rose and Levinson, 2004) , we propose a function-based question classification category tailored to general QA. [sent-36, score-0.399]
</p><p>12 To classify questions effectively, we unify rulebased methods and statistical methods into a single framework. [sent-41, score-0.423]
</p><p>13 We generate strict patterns from functional words and soft patterns from content words. [sent-43, score-0.821]
</p><p>14 Each strict pattern is a regular expression while each soft pattern is a bi-gram cluster. [sent-44, score-0.569]
</p><p>15 The matching degree is either 0 or 1 for strict pattern and between 0 and 1for soft pattern. [sent-46, score-0.482]
</p><p>16 In this paper, we propose fuzzy discriminative weight learning of Markov logic network. [sent-50, score-0.417]
</p><p>17 This method takes degrees  of confidence of each evidence predicates into account thus can model the matching degrees between questions and soft patterns. [sent-51, score-0.715]
</p><p>18 The remainder of this paper is organized as follows: In the next section we review related work on question classification, query classification and Markov logic network. [sent-52, score-0.535]
</p><p>19 Section 4 introduces fuzzy discriminative weight learning of MLN and our methodology to extract strict and soft patterns. [sent-54, score-0.619]
</p><p>20 With the booming of internet, researches on question answering are becoming more practical. [sent-63, score-0.362]
</p><p>21 Most taxonomies proposed are focused on factoid questions, such as UIUC taxonomy (Li and Roth, 2002). [sent-64, score-0.428]
</p><p>22 To classify questions effectively, Researchers have proposed features of different levels, such as lexical features, syntactic features (Nguyen et al. [sent-67, score-0.423]
</p><p>23 (2008) describe a three-layers cQA oriented question taxonomy and use it to determine the expected best answer types and summarize answers. [sent-79, score-0.629]
</p><p>24 Other than Navigational, Informational and Transactional, the first layer contains a new Social category which represents the questions that do not intend to get an answer but to elicit interaction with other people. [sent-80, score-0.508]
</p><p>25 Markov logic network (MLN) (Richardson and Domingos, 2006) is a general model combining  first-order logic and probabilistic graphical models in a single representation. [sent-83, score-0.459]
</p><p>26 ListPeople ask these questions for a list of an-List Nobel price swers. [sent-90, score-0.517]
</p><p>27 ReasonPeople ask these questions for opinions or ex-Is it good to drink planations. [sent-95, score-0.517]
</p><p>28 SolutionPeople ask these questions for problem shoot-What should I do ing. [sent-101, score-0.517]
</p><p>29 DefinitionPeople ask these questions for description ofWho is Lady Gaga? [sent-106, score-0.517]
</p><p>30 NavigationPeople ask these questions for finding web-Where can I download sites or resources. [sent-112, score-0.517]
</p><p>31 -norm regu-  formulas  generated  first-order logic induction system and prevent overfitting. [sent-116, score-0.396]
</p><p>32 First, questions can be distributed into suitable QA subsystems according to their types. [sent-121, score-0.419]
</p><p>33 Second, we can employ suitable answer summarization strategy for each question type. [sent-122, score-0.392]
</p><p>34 At first glance, classifying questions onto this taxonomy seems a solved problem for English ques1121 tions because of interrogative words. [sent-125, score-0.625]
</p><p>35 From table 2 we can see two questions in Chinese share same function word “怎 么样” but have different types. [sent-128, score-0.388]
</p><p>36 Compared to Wendy Lehnert’s or Arthur Graesser’s taxonomy, our taxonomy is more practical on providing useful information for question  TablQeuT2ey:spteiTown CHhWoi大nwehs家ta怎odc么觉qou样e得sykStoRi做阿Kuoenl宫ast凡hni保o达gnhkPa鸡r怎oef么丁sACavmh样？ietcakfru? [sent-140, score-0.478]
</p><p>37 Compared to ours, taxonomy is too much focused on factoid Apart from Description, all coarse types can be mapped into Fact. [sent-143, score-0.413]
</p><p>38 But it is hard to automatically classify questions into that taxonomy, especially for types Constant, Dynamic and Social. [sent-146, score-0.454]
</p><p>39 To examine reasonableness of our taxonomy, we select and manually annotate 5800 frequent asked questions from Baidu Zhidao (see Section 5. [sent-148, score-0.388]
</p><p>40 5 percent of questions can be categorized into our taxonomy. [sent-153, score-0.388]
</p><p>41 1 we can see that navigational questions take a substantial proportion in cQA data. [sent-159, score-0.444]
</p><p>42 In this section, we propose a new question classification methodology which combines rule-based  methods and statistical methods by Markov logic network. [sent-171, score-0.517]
</p><p>43 First, the questions posted on online communities are casually written which cannot be accurately parsed by NLP tools, especially for Chinese. [sent-173, score-0.388]
</p><p>44 The construction of strict patterns and soft patterns will be shown in 4. [sent-177, score-0.745]
</p><p>45 1 Markov Logic Network A first-order knowledge base contains a set of formulas constructed from logic operators and symbols for predicates, constants, variables and functions. [sent-183, score-0.396]
</p><p>46 Formally, Markov logic network is defined as follows: Definition 1 (Richardson & Domingos 2004) A Markov logic network L is a set of pairs ( , ), where is a formula in first-order logic and is a real number. [sent-192, score-0.846]
</p><p>47 There is an edge between two nodes of iff the corresponding grounding predicates appear together in at least one grounding of one formula in . [sent-200, score-0.361]
</p><p>48 In discriminative weight learning, ground atom set is partitioned into a set of evidence atoms and a set of query atoms . [sent-203, score-0.434]
</p><p>49 In this paper, we propose fuzzy discriminative weight learning which can take the prior confidence of each evidence atom into account. [sent-205, score-0.38]
</p><p>50 2 Strict Patterns In our question classification task, we find function words are much more discriminative and less sparse than content words. [sent-213, score-0.416]
</p><p>51 Therefore, we extract strict patterns from function words and soft patterns from content words. [sent-214, score-0.821]
</p><p>52 1, in line 5-9, if a pattern in question with type is found in , we just update the frequency of in , else is added to with only freq. [sent-230, score-0.373]
</p><p>53 two patterns and are similar iff ∀q Qmat chP q p ⇔ mat chP q p , in which mat chP is defined in) S⇔ec tmioant 4ch. [sent-233, score-0.463]
</p><p>54 Since a large number of patterns are generated, it is unpractical to evaluate all of them by Markov logic network. [sent-235, score-0.42]
</p><p>55 We sort patterns by information gain and only choose top “good” patterns in line 11-12 ofAlg. [sent-236, score-0.41]
</p><p>56 The information gain IG of a pattern is defined as IG 푃  (푝)∑푃  (푡∣  푝)  log  푃  (  in which is the number of question types, is the probability of a question having type , (or 푝)) is the probability of a question matching(or not matching) pattern . [sent-239, score-0.966]
</p><p>57 (or 푝)) is the probability of a question having type given the condition that the question matches(or does not match) pattern . [sent-240, score-0.614]
</p><p>58 The more questions a pattern matches and the more unevenly the matched questions distribute among questions types, the higher IG will be. [sent-243, score-1.358]
</p><p>59 Moreover, questions given by users may be incomplete and contain not function words. [sent-248, score-0.388]
</p><p>60 To solve this problem, we build soft patterns on question set. [sent-252, score-0.62]
</p><p>61 Each question is represented by a  weighted vector of content bi-grams in which the weight is bi-gram frequency. [sent-253, score-0.364]
</p><p>62 Unlike strict patterns, a question can match a soft pattern to some extent. [sent-259, score-0.687]
</p><p>63 Also, soft patterns can be pre-filtered by information gain described in 4. [sent-262, score-0.413]
</p><p>64 The main query predicate is Type ( q, t ) , which is true iff question q has type t. [sent-266, score-0.46]
</p><p>65 For strict pat-  terns, the evidence predicate Mat chP ( q, p ) is true iff question q is matched by strict pattern p. [sent-267, score-0.823]
</p><p>66 For soft patterns, the evidence predicate type pair (p,t), if the proportion of type t in questions matching p is less than a minimum requirement , we remove corresponding formula from MLN. [sent-269, score-0.941]
</p><p>67 Similarly, we incorporate soft patterns by Mat chC ( q, +c )  Type ( q, +t )  Type ( q, t ’ )  Our weight learner use -regularization (Huynh and Mooney, 2008) to select formulas and prevent overfitting. [sent-270, score-0.626]
</p><p>68 Since we do not model relations among questions, the derived markov network can be broken up into separated subgraphs by questions and the gradient of CLL(Eq. [sent-276, score-0.567]
</p><p>69 For the sake of efficacy, for each pattern- QA system(the system which can potentially answer 1125  all kinds of questions utilizing data from heterogeneous sources) released at present. [sent-279, score-0.508]
</p><p>70 Alteratively, we test our methodology on cQA data based on observation that questions on cQA services are of various length, domain independent and wrote informally(even with grammar mistakes). [sent-280, score-0.46]
</p><p>71 To build training set, we randomly select 5800 frequent-asked questions from Baidu Zhidao. [sent-283, score-0.388]
</p><p>72 Then we ask 10 native-speakers to annotate these questions according to question title and question description. [sent-285, score-1.03]
</p><p>73 If an annotator cannot judge type from question title, he can view the question description. [sent-286, score-0.565]
</p><p>74 If type can be judged from the description, the question title will be replaced by a sentence selected from it. [sent-287, score-0.327]
</p><p>75 To examine the generalization capabilities, the test data is composed of 700 questions randomly selected from Baidu Zhidao and 700 questions from  Sina iAsk. [sent-295, score-0.776]
</p><p>76 We extract bi-grams from questions on training data as features. [sent-300, score-0.388]
</p><p>77 We denote this method as “MB”; MLN with strict patterns and bi-grams. [sent-309, score-0.366]
</p><p>78 We ask two native-speakers to write strict patterns for each type. [sent-310, score-0.495]
</p><p>79 We first select top 3000 patterns by information gain, merge these patterns with hand-crafted ones and combine similar patterns. [sent-316, score-0.376]
</p><p>80 Then we represent these patterns by formulas and learn the weight of each formula by MLN. [sent-317, score-0.559]
</p><p>81 To incorporate content information, we extract bi-grams from questions with function words removed and remove the ones with frequency lower than two. [sent-319, score-0.464]
</p><p>82 We denote this method as “MSB”; MLN with strict patterns and soft patterns. [sent-322, score-0.557]
</p><p>83 To incorporate content information, We cluster questions on training data with similarity threshold and get 2588 clusters(soft patterns) which are represented by 3491 formulas. [sent-323, score-0.499]
</p><p>84 We these soft patterns with strict patterns extracted in “MSB”, which add up to 7370 formulas. [sent-324, score-0.745]
</p><p>85 A question is classified into easy set iff it contains function-words. [sent-327, score-0.331]
</p><p>86 From the results we can see that all methods perform better on easy questions and MLN outperforms SVM using same bigram features. [sent-331, score-0.388]
</p><p>87 It is because the strict patterns for this two types are simple and effective. [sent-347, score-0.397]
</p><p>88 A handful of patterns could cover a wide range of questions with high precision. [sent-348, score-0.576]
</p><p>89 It is difficult to distinguish Fact from List because strict patterns for these two types are partly overlap each other. [sent-349, score-0.397]
</p><p>90 The recall of Definition is very low because many definition questions on test set are short and only consists of content words(e. [sent-352, score-0.505]
</p><p>91 This shortage could be remedied by building strict patterns on POStagging sequence. [sent-355, score-0.366]
</p><p>92 From the results we can see that soft patterns are consistent with our ordinary intuitions. [sent-360, score-0.379]
</p><p>93 For exam-  ple, if user ask a questions about “TV series”, he is likely to ask for recommendation of recent TV series and the question have a great chance to be List. [sent-361, score-0.942]
</p><p>94 If user ask questions about “lose weight”, he probably ask something like “How can I lose weight fast? [sent-362, score-0.748]
</p><p>95 6  Conclusion and Future Work  We have proposed a new question taxonomy tailored to general QA on heterogeneous sources. [sent-366, score-0.53]
</p><p>96 This taxonomy provide indispensable information for question distribution and answer summarization. [sent-367, score-0.598]
</p><p>97 We build strict patterns and soft patterns to represent the information in function words and content words. [sent-368, score-0.821]
</p><p>98 Also, fuzzy discriminative weight learning is proposed for unifying strict and soft patterns into Markov logic network. [sent-369, score-0.974]
</p><p>99 Also, since questions on  online communities are classified into categories by topic, we plan to perform joint question type inference on function-based taxonomy as well as topicbased taxonomy by Markov logic. [sent-373, score-1.191]
</p><p>100 Finding similar questions in large question and answer archives. [sent-449, score-0.749]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('questions', 0.388), ('mln', 0.327), ('question', 0.241), ('taxonomy', 0.237), ('formulas', 0.2), ('logic', 0.196), ('soft', 0.191), ('patterns', 0.188), ('strict', 0.178), ('qa', 0.169), ('factoid', 0.145), ('ask', 0.129), ('fuzzy', 0.126), ('formula', 0.124), ('answer', 0.12), ('markov', 0.112), ('baidu', 0.109), ('cqa', 0.109), ('mat', 0.109), ('atom', 0.093), ('uiuc', 0.093), ('chp', 0.091), ('zhidao', 0.091), ('domingos', 0.091), ('ground', 0.083), ('pattern', 0.077), ('content', 0.076), ('grounding', 0.073), ('richardson', 0.068), ('network', 0.067), ('informational', 0.062), ('distribute', 0.062), ('predicate', 0.06), ('iff', 0.057), ('navigational', 0.056), ('type', 0.055), ('user', 0.055), ('graesser', 0.055), ('inghua', 0.055), ('levinson', 0.055), ('msb', 0.055), ('sina', 0.055), ('singla', 0.055), ('unevenly', 0.055), ('tailored', 0.052), ('classification', 0.051), ('discriminative', 0.048), ('answers', 0.047), ('weight', 0.047), ('query', 0.047), ('researches', 0.047), ('mss', 0.047), ('huynh', 0.047), ('intents', 0.047), ('jansen', 0.047), ('taxonomies', 0.046), ('dot', 0.046), ('regular', 0.046), ('services', 0.043), ('world', 0.042), ('atoms', 0.042), ('rose', 0.041), ('definition', 0.041), ('zhu', 0.041), ('answering', 0.038), ('goals', 0.037), ('barak', 0.036), ('booming', 0.036), ('booth', 0.036), ('cll', 0.036), ('eighteen', 0.036), ('frakes', 0.036), ('generatively', 0.036), ('transactional', 0.036), ('unpractical', 0.036), ('nguyen', 0.036), ('mb', 0.036), ('matching', 0.036), ('classify', 0.035), ('cluster', 0.035), ('gain', 0.034), ('ig', 0.034), ('predicates', 0.034), ('confidence', 0.034), ('classified', 0.033), ('svm', 0.033), ('moschitti', 0.033), ('evidence', 0.032), ('types', 0.031), ('suitable', 0.031), ('li', 0.031), ('tsinghua', 0.031), ('kb', 0.031), ('mai', 0.031), ('groundings', 0.031), ('title', 0.031), ('president', 0.03), ('methodology', 0.029), ('annotator', 0.028), ('poon', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="51-tfidf-1" href="./emnlp-2010-Function-Based_Question_Classification_for_General_QA.html">51 emnlp-2010-Function-Based Question Classification for General QA</a></p>
<p>Author: Fan Bu ; Xingwei Zhu ; Yu Hao ; Xiaoyan Zhu</p><p>Abstract: In contrast with the booming increase of internet data, state-of-art QA (question answering) systems, otherwise, concerned data from specific domains or resources such as search engine snippets, online forums and Wikipedia in a somewhat isolated way. Users may welcome a more general QA system for its capability to answer questions of various sources, integrated from existed specialized sub-QA engines. In this framework, question classification is the primary task. However, the current paradigms of question classification were focused on some specified type of questions, i.e. factoid questions, which are inappropriate for the general QA. In this paper, we propose a new question classification paradigm, which includes a question taxonomy suitable to the general QA and a question classifier based on MLN (Markov logic network), where rule-based methods and statistical methods are unified into a single framework in a fuzzy discriminative learning approach. Experiments show that our method outperforms traditional question classification approaches.</p><p>2 0.26518166 <a title="51-tfidf-2" href="./emnlp-2010-Learning_the_Relative_Usefulness_of_Questions_in_Community_QA.html">74 emnlp-2010-Learning the Relative Usefulness of Questions in Community QA</a></p>
<p>Author: Razvan Bunescu ; Yunfeng Huang</p><p>Abstract: We present a machine learning approach for the task of ranking previously answered questions in a question repository with respect to their relevance to a new, unanswered reference question. The ranking model is trained on a collection of question groups manually annotated with a partial order relation reflecting the relative utility of questions inside each group. Based on a set of meaning and structure aware features, the new ranking model is able to substantially outperform more straightforward, unsupervised similarity measures.</p><p>3 0.13324372 <a title="51-tfidf-3" href="./emnlp-2010-A_Semi-Supervised_Method_to_Learn_and_Construct_Taxonomies_Using_the_Web.html">12 emnlp-2010-A Semi-Supervised Method to Learn and Construct Taxonomies Using the Web</a></p>
<p>Author: Zornitsa Kozareva ; Eduard Hovy</p><p>Abstract: Although many algorithms have been developed to harvest lexical resources, few organize the mined terms into taxonomies. We propose (1) a semi-supervised algorithm that uses a root concept, a basic level concept, and recursive surface patterns to learn automatically from the Web hyponym-hypernym pairs subordinated to the root; (2) a Web based concept positioning procedure to validate the learned pairs’ is-a relations; and (3) a graph algorithm that derives from scratch the integrated taxonomy structure of all the terms. Comparing results with WordNet, we find that the algorithm misses some concepts and links, but also that it discovers many additional ones lacking in WordNet. We evaluate the taxonomization power of our method on reconstructing parts of the WordNet taxonomy. Experiments show that starting from scratch, the algorithm can reconstruct 62% of the WordNet taxonomy for the regions tested.</p><p>4 0.12369016 <a title="51-tfidf-4" href="./emnlp-2010-Uptraining_for_Accurate_Deterministic_Question_Parsing.html">115 emnlp-2010-Uptraining for Accurate Deterministic Question Parsing</a></p>
<p>Author: Slav Petrov ; Pi-Chuan Chang ; Michael Ringgaard ; Hiyan Alshawi</p><p>Abstract: It is well known that parsing accuracies drop significantly on out-of-domain data. What is less known is that some parsers suffer more from domain shifts than others. We show that dependency parsers have more difficulty parsing questions than constituency parsers. In particular, deterministic shift-reduce dependency parsers, which are of highest interest for practical applications because of their linear running time, drop to 60% labeled accuracy on a question test set. We propose an uptraining procedure in which a deterministic parser is trained on the output of a more accurate, but slower, latent variable constituency parser (converted to dependencies). Uptraining with 100K unlabeled questions achieves results comparable to having 2K labeled questions for training. With 100K unlabeled and 2K labeled questions, uptraining is able to improve parsing accuracy to 84%, closing the gap between in-domain and out-of-domain performance.</p><p>5 0.078442805 <a title="51-tfidf-5" href="./emnlp-2010-Handling_Noisy_Queries_in_Cross_Language_FAQ_Retrieval.html">55 emnlp-2010-Handling Noisy Queries in Cross Language FAQ Retrieval</a></p>
<p>Author: Danish Contractor ; Govind Kothari ; Tanveer Faruquie ; L V Subramaniam ; Sumit Negi</p><p>Abstract: Recent times have seen a tremendous growth in mobile based data services that allow people to use Short Message Service (SMS) to access these data services. In a multilingual society it is essential that data services that were developed for a specific language be made accessible through other local languages also. In this paper, we present a service that allows a user to query a FrequentlyAsked-Questions (FAQ) database built in a local language (Hindi) using Noisy SMS English queries. The inherent noise in the SMS queries, along with the language mismatch makes this a challenging problem. We handle these two problems by formulating the query similarity over FAQ questions as a combinatorial search problem where the search space consists of combinations of dictionary variations of the noisy query and its top-N translations. We demonstrate the effectiveness of our approach on a real-life dataset.</p><p>6 0.074913979 <a title="51-tfidf-6" href="./emnlp-2010-Learning_First-Order_Horn_Clauses_from_Web_Text.html">72 emnlp-2010-Learning First-Order Horn Clauses from Web Text</a></p>
<p>7 0.066686139 <a title="51-tfidf-7" href="./emnlp-2010-Top-Down_Nearly-Context-Sensitive_Parsing.html">106 emnlp-2010-Top-Down Nearly-Context-Sensitive Parsing</a></p>
<p>8 0.062637106 <a title="51-tfidf-8" href="./emnlp-2010-What%27s_with_the_Attitude%3F_Identifying_Sentences_with_Attitude_in_Online_Discussions.html">120 emnlp-2010-What's with the Attitude? Identifying Sentences with Attitude in Online Discussions</a></p>
<p>9 0.061656531 <a title="51-tfidf-9" href="./emnlp-2010-Incorporating_Content_Structure_into_Text_Analysis_Applications.html">64 emnlp-2010-Incorporating Content Structure into Text Analysis Applications</a></p>
<p>10 0.056562092 <a title="51-tfidf-10" href="./emnlp-2010-Efficient_Graph-Based_Semi-Supervised_Learning_of_Structured_Tagging_Models.html">41 emnlp-2010-Efficient Graph-Based Semi-Supervised Learning of Structured Tagging Models</a></p>
<p>11 0.056039672 <a title="51-tfidf-11" href="./emnlp-2010-Improving_Gender_Classification_of_Blog_Authors.html">61 emnlp-2010-Improving Gender Classification of Blog Authors</a></p>
<p>12 0.054800205 <a title="51-tfidf-12" href="./emnlp-2010-Context_Comparison_of_Bursty_Events_in_Web_Search_and_Online_Media.html">32 emnlp-2010-Context Comparison of Bursty Events in Web Search and Online Media</a></p>
<p>13 0.054234967 <a title="51-tfidf-13" href="./emnlp-2010-Constraints_Based_Taxonomic_Relation_Classification.html">31 emnlp-2010-Constraints Based Taxonomic Relation Classification</a></p>
<p>14 0.052190054 <a title="51-tfidf-14" href="./emnlp-2010-Automatic_Detection_and_Classification_of_Social_Events.html">20 emnlp-2010-Automatic Detection and Classification of Social Events</a></p>
<p>15 0.051622517 <a title="51-tfidf-15" href="./emnlp-2010-Learning_Recurrent_Event_Queries_for_Web_Search.html">73 emnlp-2010-Learning Recurrent Event Queries for Web Search</a></p>
<p>16 0.047866073 <a title="51-tfidf-16" href="./emnlp-2010-Inducing_Word_Senses_to_Improve_Web_Search_Result_Clustering.html">66 emnlp-2010-Inducing Word Senses to Improve Web Search Result Clustering</a></p>
<p>17 0.044057805 <a title="51-tfidf-17" href="./emnlp-2010-Cross_Language_Text_Classification_by_Model_Translation_and_Semi-Supervised_Learning.html">33 emnlp-2010-Cross Language Text Classification by Model Translation and Semi-Supervised Learning</a></p>
<p>18 0.043475866 <a title="51-tfidf-18" href="./emnlp-2010-Soft_Syntactic_Constraints_for_Hierarchical_Phrase-Based_Translation_Using_Latent_Syntactic_Distributions.html">98 emnlp-2010-Soft Syntactic Constraints for Hierarchical Phrase-Based Translation Using Latent Syntactic Distributions</a></p>
<p>19 0.04231194 <a title="51-tfidf-19" href="./emnlp-2010-Better_Punctuation_Prediction_with_Dynamic_Conditional_Random_Fields.html">25 emnlp-2010-Better Punctuation Prediction with Dynamic Conditional Random Fields</a></p>
<p>20 0.041925773 <a title="51-tfidf-20" href="./emnlp-2010-An_Approach_of_Generating_Personalized_Views_from_Normalized_Electronic_Dictionaries_%3A_A_Practical_Experiment_on_Arabic_Language.html">16 emnlp-2010-An Approach of Generating Personalized Views from Normalized Electronic Dictionaries : A Practical Experiment on Arabic Language</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.174), (1, 0.116), (2, -0.033), (3, 0.185), (4, 0.06), (5, 0.077), (6, -0.026), (7, 0.101), (8, -0.042), (9, 0.114), (10, -0.06), (11, -0.095), (12, 0.224), (13, -0.05), (14, 0.036), (15, 0.403), (16, -0.269), (17, -0.15), (18, 0.174), (19, -0.066), (20, -0.092), (21, 0.072), (22, -0.047), (23, 0.032), (24, 0.096), (25, -0.043), (26, 0.193), (27, -0.039), (28, -0.044), (29, -0.057), (30, 0.021), (31, -0.031), (32, -0.025), (33, -0.08), (34, -0.08), (35, 0.023), (36, 0.103), (37, -0.007), (38, 0.015), (39, -0.038), (40, -0.041), (41, 0.052), (42, -0.066), (43, -0.02), (44, 0.002), (45, 0.068), (46, 0.024), (47, 0.075), (48, 0.038), (49, -0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98659611 <a title="51-lsi-1" href="./emnlp-2010-Function-Based_Question_Classification_for_General_QA.html">51 emnlp-2010-Function-Based Question Classification for General QA</a></p>
<p>Author: Fan Bu ; Xingwei Zhu ; Yu Hao ; Xiaoyan Zhu</p><p>Abstract: In contrast with the booming increase of internet data, state-of-art QA (question answering) systems, otherwise, concerned data from specific domains or resources such as search engine snippets, online forums and Wikipedia in a somewhat isolated way. Users may welcome a more general QA system for its capability to answer questions of various sources, integrated from existed specialized sub-QA engines. In this framework, question classification is the primary task. However, the current paradigms of question classification were focused on some specified type of questions, i.e. factoid questions, which are inappropriate for the general QA. In this paper, we propose a new question classification paradigm, which includes a question taxonomy suitable to the general QA and a question classifier based on MLN (Markov logic network), where rule-based methods and statistical methods are unified into a single framework in a fuzzy discriminative learning approach. Experiments show that our method outperforms traditional question classification approaches.</p><p>2 0.91540986 <a title="51-lsi-2" href="./emnlp-2010-Learning_the_Relative_Usefulness_of_Questions_in_Community_QA.html">74 emnlp-2010-Learning the Relative Usefulness of Questions in Community QA</a></p>
<p>Author: Razvan Bunescu ; Yunfeng Huang</p><p>Abstract: We present a machine learning approach for the task of ranking previously answered questions in a question repository with respect to their relevance to a new, unanswered reference question. The ranking model is trained on a collection of question groups manually annotated with a partial order relation reflecting the relative utility of questions inside each group. Based on a set of meaning and structure aware features, the new ranking model is able to substantially outperform more straightforward, unsupervised similarity measures.</p><p>3 0.4957794 <a title="51-lsi-3" href="./emnlp-2010-Handling_Noisy_Queries_in_Cross_Language_FAQ_Retrieval.html">55 emnlp-2010-Handling Noisy Queries in Cross Language FAQ Retrieval</a></p>
<p>Author: Danish Contractor ; Govind Kothari ; Tanveer Faruquie ; L V Subramaniam ; Sumit Negi</p><p>Abstract: Recent times have seen a tremendous growth in mobile based data services that allow people to use Short Message Service (SMS) to access these data services. In a multilingual society it is essential that data services that were developed for a specific language be made accessible through other local languages also. In this paper, we present a service that allows a user to query a FrequentlyAsked-Questions (FAQ) database built in a local language (Hindi) using Noisy SMS English queries. The inherent noise in the SMS queries, along with the language mismatch makes this a challenging problem. We handle these two problems by formulating the query similarity over FAQ questions as a combinatorial search problem where the search space consists of combinations of dictionary variations of the noisy query and its top-N translations. We demonstrate the effectiveness of our approach on a real-life dataset.</p><p>4 0.37722462 <a title="51-lsi-4" href="./emnlp-2010-A_Semi-Supervised_Method_to_Learn_and_Construct_Taxonomies_Using_the_Web.html">12 emnlp-2010-A Semi-Supervised Method to Learn and Construct Taxonomies Using the Web</a></p>
<p>Author: Zornitsa Kozareva ; Eduard Hovy</p><p>Abstract: Although many algorithms have been developed to harvest lexical resources, few organize the mined terms into taxonomies. We propose (1) a semi-supervised algorithm that uses a root concept, a basic level concept, and recursive surface patterns to learn automatically from the Web hyponym-hypernym pairs subordinated to the root; (2) a Web based concept positioning procedure to validate the learned pairs’ is-a relations; and (3) a graph algorithm that derives from scratch the integrated taxonomy structure of all the terms. Comparing results with WordNet, we find that the algorithm misses some concepts and links, but also that it discovers many additional ones lacking in WordNet. We evaluate the taxonomization power of our method on reconstructing parts of the WordNet taxonomy. Experiments show that starting from scratch, the algorithm can reconstruct 62% of the WordNet taxonomy for the regions tested.</p><p>5 0.36113316 <a title="51-lsi-5" href="./emnlp-2010-Uptraining_for_Accurate_Deterministic_Question_Parsing.html">115 emnlp-2010-Uptraining for Accurate Deterministic Question Parsing</a></p>
<p>Author: Slav Petrov ; Pi-Chuan Chang ; Michael Ringgaard ; Hiyan Alshawi</p><p>Abstract: It is well known that parsing accuracies drop significantly on out-of-domain data. What is less known is that some parsers suffer more from domain shifts than others. We show that dependency parsers have more difficulty parsing questions than constituency parsers. In particular, deterministic shift-reduce dependency parsers, which are of highest interest for practical applications because of their linear running time, drop to 60% labeled accuracy on a question test set. We propose an uptraining procedure in which a deterministic parser is trained on the output of a more accurate, but slower, latent variable constituency parser (converted to dependencies). Uptraining with 100K unlabeled questions achieves results comparable to having 2K labeled questions for training. With 100K unlabeled and 2K labeled questions, uptraining is able to improve parsing accuracy to 84%, closing the gap between in-domain and out-of-domain performance.</p><p>6 0.26003805 <a title="51-lsi-6" href="./emnlp-2010-What%27s_with_the_Attitude%3F_Identifying_Sentences_with_Attitude_in_Online_Discussions.html">120 emnlp-2010-What's with the Attitude? Identifying Sentences with Attitude in Online Discussions</a></p>
<p>7 0.23808652 <a title="51-lsi-7" href="./emnlp-2010-Constraints_Based_Taxonomic_Relation_Classification.html">31 emnlp-2010-Constraints Based Taxonomic Relation Classification</a></p>
<p>8 0.21824753 <a title="51-lsi-8" href="./emnlp-2010-Improving_Gender_Classification_of_Blog_Authors.html">61 emnlp-2010-Improving Gender Classification of Blog Authors</a></p>
<p>9 0.20508124 <a title="51-lsi-9" href="./emnlp-2010-Top-Down_Nearly-Context-Sensitive_Parsing.html">106 emnlp-2010-Top-Down Nearly-Context-Sensitive Parsing</a></p>
<p>10 0.20403156 <a title="51-lsi-10" href="./emnlp-2010-Learning_First-Order_Horn_Clauses_from_Web_Text.html">72 emnlp-2010-Learning First-Order Horn Clauses from Web Text</a></p>
<p>11 0.19169487 <a title="51-lsi-11" href="./emnlp-2010-Positional_Language_Models_for_Clinical_Information_Retrieval.html">90 emnlp-2010-Positional Language Models for Clinical Information Retrieval</a></p>
<p>12 0.18703023 <a title="51-lsi-12" href="./emnlp-2010-Better_Punctuation_Prediction_with_Dynamic_Conditional_Random_Fields.html">25 emnlp-2010-Better Punctuation Prediction with Dynamic Conditional Random Fields</a></p>
<p>13 0.18322249 <a title="51-lsi-13" href="./emnlp-2010-Efficient_Graph-Based_Semi-Supervised_Learning_of_Structured_Tagging_Models.html">41 emnlp-2010-Efficient Graph-Based Semi-Supervised Learning of Structured Tagging Models</a></p>
<p>14 0.17474367 <a title="51-lsi-14" href="./emnlp-2010-Domain_Adaptation_of_Rule-Based_Annotators_for_Named-Entity_Recognition_Tasks.html">37 emnlp-2010-Domain Adaptation of Rule-Based Annotators for Named-Entity Recognition Tasks</a></p>
<p>15 0.16715561 <a title="51-lsi-15" href="./emnlp-2010-Incorporating_Content_Structure_into_Text_Analysis_Applications.html">64 emnlp-2010-Incorporating Content Structure into Text Analysis Applications</a></p>
<p>16 0.16245824 <a title="51-lsi-16" href="./emnlp-2010-Two_Decades_of_Unsupervised_POS_Induction%3A_How_Far_Have_We_Come%3F.html">111 emnlp-2010-Two Decades of Unsupervised POS Induction: How Far Have We Come?</a></p>
<p>17 0.15149716 <a title="51-lsi-17" href="./emnlp-2010-A_Mixture_Model_with_Sharing_for_Lexical_Semantics.html">7 emnlp-2010-A Mixture Model with Sharing for Lexical Semantics</a></p>
<p>18 0.14800452 <a title="51-lsi-18" href="./emnlp-2010-An_Approach_of_Generating_Personalized_Views_from_Normalized_Electronic_Dictionaries_%3A_A_Practical_Experiment_on_Arabic_Language.html">16 emnlp-2010-An Approach of Generating Personalized Views from Normalized Electronic Dictionaries : A Practical Experiment on Arabic Language</a></p>
<p>19 0.1477825 <a title="51-lsi-19" href="./emnlp-2010-Context_Comparison_of_Bursty_Events_in_Web_Search_and_Online_Media.html">32 emnlp-2010-Context Comparison of Bursty Events in Web Search and Online Media</a></p>
<p>20 0.14503315 <a title="51-lsi-20" href="./emnlp-2010-Evaluating_Models_of_Latent_Document_Semantics_in_the_Presence_of_OCR_Errors.html">45 emnlp-2010-Evaluating Models of Latent Document Semantics in the Presence of OCR Errors</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.017), (10, 0.069), (12, 0.04), (29, 0.061), (30, 0.034), (32, 0.018), (52, 0.033), (56, 0.065), (62, 0.016), (66, 0.093), (72, 0.057), (76, 0.041), (77, 0.019), (87, 0.016), (88, 0.328), (89, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73101807 <a title="51-lda-1" href="./emnlp-2010-Function-Based_Question_Classification_for_General_QA.html">51 emnlp-2010-Function-Based Question Classification for General QA</a></p>
<p>Author: Fan Bu ; Xingwei Zhu ; Yu Hao ; Xiaoyan Zhu</p><p>Abstract: In contrast with the booming increase of internet data, state-of-art QA (question answering) systems, otherwise, concerned data from specific domains or resources such as search engine snippets, online forums and Wikipedia in a somewhat isolated way. Users may welcome a more general QA system for its capability to answer questions of various sources, integrated from existed specialized sub-QA engines. In this framework, question classification is the primary task. However, the current paradigms of question classification were focused on some specified type of questions, i.e. factoid questions, which are inappropriate for the general QA. In this paper, we propose a new question classification paradigm, which includes a question taxonomy suitable to the general QA and a question classifier based on MLN (Markov logic network), where rule-based methods and statistical methods are unified into a single framework in a fuzzy discriminative learning approach. Experiments show that our method outperforms traditional question classification approaches.</p><p>2 0.42217749 <a title="51-lda-2" href="./emnlp-2010-A_Hybrid_Morpheme-Word_Representation_for_Machine_Translation_of_Morphologically_Rich_Languages.html">5 emnlp-2010-A Hybrid Morpheme-Word Representation for Machine Translation of Morphologically Rich Languages</a></p>
<p>Author: Minh-Thang Luong ; Preslav Nakov ; Min-Yen Kan</p><p>Abstract: We propose a language-independent approach for improving statistical machine translation for morphologically rich languages using a hybrid morpheme-word representation where the basic unit of translation is the morpheme, but word boundaries are respected at all stages of the translation process. Our model extends the classic phrase-based model by means of (1) word boundary-aware morpheme-level phrase extraction, (2) minimum error-rate training for a morpheme-level translation model using word-level BLEU, and (3) joint scoring with morpheme- and word-level language models. Further improvements are achieved by combining our model with the classic one. The evaluation on English to Finnish using Europarl (714K sentence pairs; 15.5M English words) shows statistically significant improvements over the classic model based on BLEU and human judgments.</p><p>3 0.41827384 <a title="51-lda-3" href="./emnlp-2010-Learning_the_Relative_Usefulness_of_Questions_in_Community_QA.html">74 emnlp-2010-Learning the Relative Usefulness of Questions in Community QA</a></p>
<p>Author: Razvan Bunescu ; Yunfeng Huang</p><p>Abstract: We present a machine learning approach for the task of ranking previously answered questions in a question repository with respect to their relevance to a new, unanswered reference question. The ranking model is trained on a collection of question groups manually annotated with a partial order relation reflecting the relative utility of questions inside each group. Based on a set of meaning and structure aware features, the new ranking model is able to substantially outperform more straightforward, unsupervised similarity measures.</p><p>4 0.40548187 <a title="51-lda-4" href="./emnlp-2010-Discriminative_Sample_Selection_for_Statistical_Machine_Translation.html">35 emnlp-2010-Discriminative Sample Selection for Statistical Machine Translation</a></p>
<p>Author: Sankaranarayanan Ananthakrishnan ; Rohit Prasad ; David Stallard ; Prem Natarajan</p><p>Abstract: Production of parallel training corpora for the development of statistical machine translation (SMT) systems for resource-poor languages usually requires extensive manual effort. Active sample selection aims to reduce the labor, time, and expense incurred in producing such resources, attaining a given performance benchmark with the smallest possible training corpus by choosing informative, nonredundant source sentences from an available candidate pool for manual translation. We present a novel, discriminative sample selection strategy that preferentially selects batches of candidate sentences with constructs that lead to erroneous translations on a held-out development set. The proposed strategy supports a built-in diversity mechanism that reduces redundancy in the selected batches. Simulation experiments on English-to-Pashto and Spanish-to-English translation tasks demon- strate the superiority of the proposed approach to a number of competing techniques, such as random selection, dissimilarity-based selection, as well as a recently proposed semisupervised active learning strategy.</p><p>5 0.40415567 <a title="51-lda-5" href="./emnlp-2010-Constraints_Based_Taxonomic_Relation_Classification.html">31 emnlp-2010-Constraints Based Taxonomic Relation Classification</a></p>
<p>Author: Quang Do ; Dan Roth</p><p>Abstract: Determining whether two terms in text have an ancestor relation (e.g. Toyota and car) or a sibling relation (e.g. Toyota and Honda) is an essential component of textual inference in NLP applications such as Question Answering, Summarization, and Recognizing Textual Entailment. Significant work has been done on developing stationary knowledge sources that could potentially support these tasks, but these resources often suffer from low coverage, noise, and are inflexible when needed to support terms that are not identical to those placed in them, making their use as general purpose background knowledge resources difficult. In this paper, rather than building a stationary hierarchical structure of terms and relations, we describe a system that, given two terms, determines the taxonomic relation between them using a machine learning-based approach that makes use of existing resources. Moreover, we develop a global constraint opti- mization inference process and use it to leverage an existing knowledge base also to enforce relational constraints among terms and thus improve the classifier predictions. Our experimental evaluation shows that our approach significantly outperforms other systems built upon existing well-known knowledge sources.</p><p>6 0.40082479 <a title="51-lda-6" href="./emnlp-2010-Title_Generation_with_Quasi-Synchronous_Grammar.html">105 emnlp-2010-Title Generation with Quasi-Synchronous Grammar</a></p>
<p>7 0.40075791 <a title="51-lda-7" href="./emnlp-2010-Assessing_Phrase-Based_Translation_Models_with_Oracle_Decoding.html">18 emnlp-2010-Assessing Phrase-Based Translation Models with Oracle Decoding</a></p>
<p>8 0.40020162 <a title="51-lda-8" href="./emnlp-2010-What%27s_with_the_Attitude%3F_Identifying_Sentences_with_Attitude_in_Online_Discussions.html">120 emnlp-2010-What's with the Attitude? Identifying Sentences with Attitude in Online Discussions</a></p>
<p>9 0.39906275 <a title="51-lda-9" href="./emnlp-2010-Context_Comparison_of_Bursty_Events_in_Web_Search_and_Online_Media.html">32 emnlp-2010-Context Comparison of Bursty Events in Web Search and Online Media</a></p>
<p>10 0.39906147 <a title="51-lda-10" href="./emnlp-2010-Inducing_Probabilistic_CCG_Grammars_from_Logical_Form_with_Higher-Order_Unification.html">65 emnlp-2010-Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification</a></p>
<p>11 0.39829507 <a title="51-lda-11" href="./emnlp-2010-Extracting_Opinion_Targets_in_a_Single_and_Cross-Domain_Setting_with_Conditional_Random_Fields.html">49 emnlp-2010-Extracting Opinion Targets in a Single and Cross-Domain Setting with Conditional Random Fields</a></p>
<p>12 0.3970038 <a title="51-lda-12" href="./emnlp-2010-Towards_Conversation_Entailment%3A_An_Empirical_Investigation.html">107 emnlp-2010-Towards Conversation Entailment: An Empirical Investigation</a></p>
<p>13 0.39641082 <a title="51-lda-13" href="./emnlp-2010-Joint_Training_and_Decoding_Using_Virtual_Nodes_for_Cascaded_Segmentation_and_Tagging_Tasks.html">69 emnlp-2010-Joint Training and Decoding Using Virtual Nodes for Cascaded Segmentation and Tagging Tasks</a></p>
<p>14 0.39605048 <a title="51-lda-14" href="./emnlp-2010-Minimum_Error_Rate_Training_by_Sampling_the_Translation_Lattice.html">78 emnlp-2010-Minimum Error Rate Training by Sampling the Translation Lattice</a></p>
<p>15 0.39598638 <a title="51-lda-15" href="./emnlp-2010-It_Depends_on_the_Translation%3A_Unsupervised_Dependency_Parsing_via_Word_Alignment.html">67 emnlp-2010-It Depends on the Translation: Unsupervised Dependency Parsing via Word Alignment</a></p>
<p>16 0.3949962 <a title="51-lda-16" href="./emnlp-2010-Multi-Document_Summarization_Using_A%2A_Search_and_Discriminative_Learning.html">82 emnlp-2010-Multi-Document Summarization Using A* Search and Discriminative Learning</a></p>
<p>17 0.39473361 <a title="51-lda-17" href="./emnlp-2010-Non-Isomorphic_Forest_Pair_Translation.html">86 emnlp-2010-Non-Isomorphic Forest Pair Translation</a></p>
<p>18 0.39275718 <a title="51-lda-18" href="./emnlp-2010-Automatic_Detection_and_Classification_of_Social_Events.html">20 emnlp-2010-Automatic Detection and Classification of Social Events</a></p>
<p>19 0.39150342 <a title="51-lda-19" href="./emnlp-2010-Predicting_the_Semantic_Compositionality_of_Prefix_Verbs.html">92 emnlp-2010-Predicting the Semantic Compositionality of Prefix Verbs</a></p>
<p>20 0.39130768 <a title="51-lda-20" href="./emnlp-2010-Improving_Translation_via_Targeted_Paraphrasing.html">63 emnlp-2010-Improving Translation via Targeted Paraphrasing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
