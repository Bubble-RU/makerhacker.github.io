<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>94 emnlp-2010-SCFG Decoding Without Binarization</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2010" href="../home/emnlp2010_home.html">emnlp2010</a> <a title="emnlp-2010-94" href="#">emnlp2010-94</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>94 emnlp-2010-SCFG Decoding Without Binarization</h1>
<br/><p>Source: <a title="emnlp-2010-94-pdf" href="http://aclweb.org/anthology//D/D10/D10-1063.pdf">pdf</a></p><p>Author: Mark Hopkins ; Greg Langmead</p><p>Abstract: Conventional wisdom dictates that synchronous context-free grammars (SCFGs) must be converted to Chomsky Normal Form (CNF) to ensure cubic time decoding. For arbitrary SCFGs, this is typically accomplished via the synchronous binarization technique of (Zhang et al., 2006). A drawback to this approach is that it inflates the constant factors associated with decoding, and thus the practical running time. (DeNero et al., 2009) tackle this problem by defining a superset of CNF called Lexical Normal Form (LNF), which also supports cubic time decoding under certain implicit assumptions. In this paper, we make these assumptions explicit, and in doing so, show that LNF can be further expanded to a broader class of grammars (called “scope3”) that also supports cubic-time decoding. By simply pruning non-scope-3 rules from a GHKM-extracted grammar, we obtain better translation performance than synchronous binarization.</p><p>Reference: <a title="emnlp-2010-94-reference" href="../emnlp2010_reference/emnlp-2010-SCFG_Decoding_Without_Binarization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Conventional wisdom dictates that synchronous context-free grammars (SCFGs) must be converted to Chomsky Normal Form (CNF) to ensure cubic time decoding. [sent-3, score-0.209]
</p><p>2 For arbitrary SCFGs, this is typically accomplished via the synchronous binarization technique of (Zhang et al. [sent-4, score-0.115]
</p><p>3 A drawback to this approach is that it inflates the constant factors associated with decoding, and thus the practical running time. [sent-6, score-0.069]
</p><p>4 , 2009) tackle this problem by defining a superset of CNF called Lexical Normal Form (LNF), which also supports cubic time decoding under certain implicit assumptions. [sent-8, score-0.254]
</p><p>5 In this paper, we make these assumptions explicit, and in doing so, show that LNF can be further expanded to a broader class of grammars (called “scope3”) that also supports cubic-time decoding. [sent-9, score-0.03]
</p><p>6 By simply pruning non-scope-3 rules from a GHKM-extracted grammar, we obtain better  translation performance than synchronous binarization. [sent-10, score-0.138]
</p><p>7 1 Introduction At the heart of bottom-up chart parsing (Younger, 1967) is the following combinatorial problem. [sent-11, score-0.153]
</p><p>8 We have a context-free grammar (CFG) rule (for instance, S → NP VP PP) and an input sentence of length n (for instance, “)o ann dth aen f ianspt jet snktie nocfe mr smith”). [sent-12, score-0.82]
</p><p>9 During chart parsing, we need to apply the rule to all relevant subspans of the input sen? [sent-13, score-0.204]
</p><p>10 For this particular rule, there are application contexts, i. [sent-17, score-0.057]
</p><p>11 t parsing is at least linear in this quantity, it will take  ? [sent-24, score-0.038]
</p><p>12 646  NNonPP t VVhPPePfaPstPPjetskiofmrsmith …  …  …  NPVPPP  choice point  choice poin…t …  …  choice point  choice point  …  NPNPVVPPPPPP  Figu? [sent-26, score-0.16]
</p><p>13 application contexts for the CFG rule “S → NP VP P? [sent-31, score-0.199]
</p><p>14 wahpepreli n ti so tnh ceo length ofofr rt thhee input s reunlete “nSce →. [sent-33, score-0.031]
</p><p>15 at least O( ) = O(n4) time if we include this rule in our g? [sent-36, score-0.089]
</p><p>16 In CNF, all rules have the form X → Y Z or X → x, where x is a terminthael faonrdm X, Y, →Z Y are noron Xte →rmi nxa,l ws. [sent-40, score-0.069]
</p><p>17 rt parsing is = O(n3) when applied to CNF grammars. [sent-46, score-0.069]
</p><p>18 A disadvantage to CNF conversion is that it increases both the overall number of rules and the  ? [sent-47, score-0.069]
</p><p>19 This inflation of the “grammar constant” does not affect the asymptotic runtime, but can have a significant impact on the performance in practice. [sent-52, score-0.051]
</p><p>20 There are O(n) application contexts for CFG rule “S → the NPB of NNP”, and Oco(nnte2x) application cruolnete “xSts →for t hCeF GN rBule o f“S N N→P ”th,e a nJdJ NPB o)f a NppNlPica”t, ioifn we assume tohra tC FthGe input “sSent →enc teh eh JasJ  length n and contains no repeated words. [sent-56, score-0.298]
</p><p>21 LNF is a superclass of CNF that also allows rules whose right-hand sides have no consecutive nonterminals. [sent-59, score-0.103]
</p><p>22 The intuition is that the terminals provide anchors that limit the applicability of a given rule. [sent-60, score-0.023]
</p><p>23 For instance, consider the rule NP → the NPB of NNP. [sent-61, score-0.089]
</p><p>24 Because trhulee t NerPmi →na tlsh ec NonPsBtra oinf our choices, uthreer 2e. [sent-63, score-0.045]
</p><p>25 The implicit assumption is that input sentences will not repeat the same word more than a small constant number of times. [sent-65, score-0.077]
</p><p>26 If we make the explicit assumption that all words of an input sentence are unique, then there are O(n2) application contexts for a “no consecutive nonterminals” rule. [sent-66, score-0.196]
</p><p>27 Thus under this assumption, the running time of chart parsing is still O(n3) when applied to LNF grammars. [sent-67, score-0.196]
</p><p>28 But once we make this assumption explicit, it becomes clear that we can go even further than LNF and still maintain the cubic bound on the runtime. [sent-68, score-0.182]
</p><p>29 ) T application contexts, due to the anchoring effect of the  terminals. [sent-71, score-0.057]
</p><p>30 In general, for a rule of the form X → γ, ttheremrei are a. [sent-72, score-0.089]
</p><p>31 t I mn gosent O(np) application contexts, w →he rγe, p is the number of consecutive nonterminal pairs in 647 the string X ·γ· X (where X is an arbitrary nonterminal). [sent-73, score-0.186]
</p><p>32 Wrineg r eXfe ·rγ t·o X p as tehree scope no fa a i rutrlaer. [sent-74, score-0.054]
</p><p>33 y T nhounst ecrhmaritparsing runs in time O(nscope(G)), where scope(G) is the maximum scope of any of the rules in CFG G. [sent-75, score-0.123]
</p><p>34 Specifically, any scope-3 grammar can be decoded in cubic time. [sent-76, score-0.192]
</p><p>35 , 2009), the target of our interest is synchronous context-free grammar (SCFG) decoding with rules extracted using the GHKM algorithm (Galley et al. [sent-78, score-0.25]
</p><p>36 In practice, it turns out that only a small percentage of the lexical rules in our system have scope greater than 3. [sent-80, score-0.123]
</p><p>37 By simply removing these rules from the grammar, we can maintain the cubic running time of chart parsing without any kind of binarization. [sent-81, score-0.42]
</p><p>38 , 2009), we maintain the synchronous property of the grammar, and thus can integrate language model scoring into chart parsing. [sent-85, score-0.224]
</p><p>39 Finally, a system without binarized rules is considerably simpler to build and maintain. [sent-86, score-0.092]
</p><p>40 We show that this approach gives us better practical performance than a mature system that binarizes using the technique of (Zhang et al. [sent-87, score-0.023]
</p><p>41 2  Preliminaries  Assume we have a global vocabulary of symbols, containing the reserved substitution symbol ♦. [sent-89, score-0.031]
</p><p>42 We will typically use space-delimited quotations to represent example sentences, e. [sent-91, score-0.023]
</p><p>43 “the fast jet ski” rather than hthe, fast,jet, skii. [sent-93, score-0.54]
</p><p>44 Define the rank of a sentence as the count of its ♦ symbols. [sent-98, score-0.026]
</p><p>45 , sk) to denote the substitution of k sentences s1, . [sent-102, score-0.031]
</p><p>46 For instance, if s = “the ♦ ♦ of ♦”, then SUB (s, “fast”, “jet ski”, “mr smith”) = “the fast jet ski of mr smith”. [sent-106, score-1.263]
</p><p>47 To refer to a subsentence, define a span as a pair  <  [a, b] of nonnegative integers such that a b. [sent-107, score-0.042]
</p><p>48 , sni and a span [a, b] such tah saet nbt ≤ n, sd =efin hes s[a,b] = hsa+1 , . [sent-111, score-0.025]
</p><p>49 Both derive the sentence SUB(“on ♦”, SUB( “the ♦ ♦ of ♦”, “fast”, “jet ski”, “mr smith”) ) = “on the fast jet ski of mr smith”. [sent-116, score-1.263]
</p><p>50 The SCFG derivation simultaneously derives the auxiliary sentence “sur le jet ski vite de m smith”. [sent-117, score-1.351]
</p><p>51 3  Minimum Derivation Cost  Chart parsing solves a problem which we will re-  fer to as Minimum Derivation Cost. [sent-118, score-0.061]
</p><p>52 Because we want our results to be applicable to both CFG decoding and SCFG decoding with an integrated language model, we will provide a somewhat more abstract formulation of chart parsing than usual. [sent-119, score-0.273]
</p><p>53 A derivation is a tree of CFG rules, constructed so that the preconditions (the RHS nonterminals) of any rule match the postconditions (the LHS nonterminal) of its child rules. [sent-121, score-0.369]
</p><p>54 The purpose of a derivation is to derive a sentence, which is obtained through recursive substitution. [sent-122, score-0.221]
</p><p>55 In the example, we substitute “fast”, “jet ski”, and “mr smith” into the lexical pattern “the ♦ ♦ of ♦” to obtain “the fast jet ski of mr smith”. [sent-123, score-1.297]
</p><p>56 Then we substitute this result into the lexical pattern “on ♦” to obtain “on the fast jet ski of mr smith”. [sent-124, score-1.297]
</p><p>57 The cost of a derivation is simply the sum of the base costs of its rules. [sent-125, score-0.349]
</p><p>58 Thus the cost of the CFG derivation in Figure 3 is C1 + C2 + C3 + C4 + C5, where C1 is the base cost of rule “PP → on NP”, etc. [sent-126, score-0.544]
</p><p>59 Notice thaits th thise cboasste can tb oef d riuslteri “bPuPted → locally ”to, ethtce. [sent-127, score-0.07]
</p><p>60 An SCFG derivation is similar to a CFG deriva-  648  NP->PthPC2e->J oNnN oPfN PC1 J ->Cfa3stN ->jetskCi4N P->mrsmC5ith Figure 4: The cost of the CFG derivation in Figure 3 is C1 + C2 + C3 + C4 + C5, where C1 is the base cost of rule “PP → on NP”, etc. [sent-129, score-0.765]
</p><p>61 Notice that this cost can be doifs trurilbeut “ePdP locally NtoP Pt”he, entco. [sent-130, score-0.151]
</p><p>62 For instance, the SCFG derivation in Figure 3 derives the sentence pair h “on the fast jet ski ourfe mr smith”, “hseu sre nlet jet esk pia ivrit eh “doen m sem fiatsht” j i. [sent-133, score-2.089]
</p><p>63 kIni  omfa cmhrin sem translation, o jeftten sk we twea dnet mthe s mcoitsht ”o if. [sent-134, score-0.142]
</p><p>64 th Ien SCFG derivation to include a language model cost for this second sentence. [sent-136, score-0.327]
</p><p>65 For example, the cost ofthe SCFG derivation in Figure 3 might be C1+C2+C3+ C4+C5 +LM(sur le) +LM(le jet) +LM(jet ski) + LM(ski de) + LM(de m) + LM(m smith), where LM is the negative log of a 2-gram language model. [sent-137, score-0.327]
</p><p>66 This new cost function can also be distributed locally to the nodes of the derivation, as shown in Figure 5. [sent-138, score-0.151]
</p><p>67 Formally, define a carry as a sentence of rank 0. [sent-141, score-0.026]
</p><p>68 In order to provide a chart parsing formulation that applies to both CFG decoding and SCFG decoding with an integrated language model, we need abstract definitions of rule and derivation that capture the above concepts of pattern, postcondition, preconditions, cost, and carries. [sent-142, score-0.583]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ski', 0.505), ('jet', 0.436), ('cnf', 0.238), ('derivation', 0.221), ('mr', 0.218), ('scfg', 0.212), ('cfg', 0.21), ('lnf', 0.208), ('npb', 0.119), ('chart', 0.115), ('cubic', 0.115), ('smith', 0.114), ('lm', 0.108), ('cost', 0.106), ('fast', 0.104), ('sub', 0.093), ('np', 0.09), ('rule', 0.089), ('sur', 0.089), ('synchronous', 0.069), ('rules', 0.069), ('jj', 0.068), ('nnp', 0.065), ('decoding', 0.06), ('preconditions', 0.059), ('scfgs', 0.059), ('application', 0.057), ('derives', 0.056), ('scope', 0.054), ('contexts', 0.053), ('nonterminal', 0.053), ('grammar', 0.052), ('asymptotic', 0.051), ('vite', 0.051), ('normal', 0.049), ('sk', 0.048), ('pp', 0.048), ('denero', 0.047), ('binarization', 0.046), ('chomsky', 0.046), ('sem', 0.046), ('locally', 0.045), ('running', 0.043), ('eh', 0.042), ('mn', 0.042), ('ont', 0.042), ('nonnegative', 0.042), ('maintain', 0.04), ('choice', 0.04), ('nn', 0.038), ('parsing', 0.038), ('substitute', 0.034), ('nonterminals', 0.034), ('consecutive', 0.034), ('le', 0.032), ('rt', 0.031), ('substitution', 0.031), ('supports', 0.03), ('demonstration', 0.029), ('assumption', 0.027), ('constant', 0.026), ('rank', 0.026), ('saet', 0.025), ('preliminaries', 0.025), ('wisdom', 0.025), ('sre', 0.025), ('decoded', 0.025), ('jst', 0.025), ('younger', 0.025), ('dnet', 0.025), ('greg', 0.025), ('nocfe', 0.025), ('efin', 0.025), ('enc', 0.025), ('gn', 0.025), ('hthe', 0.025), ('ib', 0.025), ('lhs', 0.025), ('rhs', 0.025), ('superset', 0.025), ('thise', 0.025), ('simultaneously', 0.025), ('explicit', 0.025), ('de', 0.025), ('implicit', 0.024), ('tence', 0.023), ('oco', 0.023), ('mthe', 0.023), ('quotations', 0.023), ('binarized', 0.023), ('anchors', 0.023), ('ghkm', 0.023), ('oinf', 0.023), ('fer', 0.023), ('xte', 0.023), ('inflate', 0.023), ('mature', 0.023), ('ec', 0.022), ('vp', 0.022), ('base', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="94-tfidf-1" href="./emnlp-2010-SCFG_Decoding_Without_Binarization.html">94 emnlp-2010-SCFG Decoding Without Binarization</a></p>
<p>Author: Mark Hopkins ; Greg Langmead</p><p>Abstract: Conventional wisdom dictates that synchronous context-free grammars (SCFGs) must be converted to Chomsky Normal Form (CNF) to ensure cubic time decoding. For arbitrary SCFGs, this is typically accomplished via the synchronous binarization technique of (Zhang et al., 2006). A drawback to this approach is that it inflates the constant factors associated with decoding, and thus the practical running time. (DeNero et al., 2009) tackle this problem by defining a superset of CNF called Lexical Normal Form (LNF), which also supports cubic time decoding under certain implicit assumptions. In this paper, we make these assumptions explicit, and in doing so, show that LNF can be further expanded to a broader class of grammars (called “scope3”) that also supports cubic-time decoding. By simply pruning non-scope-3 rules from a GHKM-extracted grammar, we obtain better translation performance than synchronous binarization.</p><p>2 0.15882677 <a title="94-tfidf-2" href="./emnlp-2010-Soft_Syntactic_Constraints_for_Hierarchical_Phrase-Based_Translation_Using_Latent_Syntactic_Distributions.html">98 emnlp-2010-Soft Syntactic Constraints for Hierarchical Phrase-Based Translation Using Latent Syntactic Distributions</a></p>
<p>Author: Zhongqiang Huang ; Martin Cmejrek ; Bowen Zhou</p><p>Abstract: In this paper, we present a novel approach to enhance hierarchical phrase-based machine translation systems with linguistically motivated syntactic features. Rather than directly using treebank categories as in previous studies, we learn a set of linguistically-guided latent syntactic categories automatically from a source-side parsed, word-aligned parallel corpus, based on the hierarchical structure among phrase pairs as well as the syntactic structure of the source side. In our model, each X nonterminal in a SCFG rule is decorated with a real-valued feature vector computed based on its distribution of latent syntactic categories. These feature vectors are utilized at decod- ing time to measure the similarity between the syntactic analysis of the source side and the syntax of the SCFG rules that are applied to derive translations. Our approach maintains the advantages of hierarchical phrase-based translation systems while at the same time naturally incorporates soft syntactic constraints.</p><p>3 0.095489763 <a title="94-tfidf-3" href="./emnlp-2010-Efficient_Incremental_Decoding_for_Tree-to-String_Translation.html">42 emnlp-2010-Efficient Incremental Decoding for Tree-to-String Translation</a></p>
<p>Author: Liang Huang ; Haitao Mi</p><p>Abstract: Syntax-based translation models should in principle be efficient with polynomially-sized search space, but in practice they are often embarassingly slow, partly due to the cost of language model integration. In this paper we borrow from phrase-based decoding the idea to generate a translation incrementally left-to-right, and show that for tree-to-string models, with a clever encoding of derivation history, this method runs in averagecase polynomial-time in theory, and lineartime with beam search in practice (whereas phrase-based decoding is exponential-time in theory and quadratic-time in practice). Experiments show that, with comparable translation quality, our tree-to-string system (in Python) can run more than 30 times faster than the phrase-based system Moses (in C++).</p><p>4 0.094806589 <a title="94-tfidf-4" href="./emnlp-2010-Hierarchical_Phrase-Based_Translation_Grammars_Extracted_from_Alignment_Posterior_Probabilities.html">57 emnlp-2010-Hierarchical Phrase-Based Translation Grammars Extracted from Alignment Posterior Probabilities</a></p>
<p>Author: Adria de Gispert ; Juan Pino ; William Byrne</p><p>Abstract: We report on investigations into hierarchical phrase-based translation grammars based on rules extracted from posterior distributions over alignments of the parallel text. Rather than restrict rule extraction to a single alignment, such as Viterbi, we instead extract rules based on posterior distributions provided by the HMM word-to-word alignmentmodel. We define translation grammars progressively by adding classes of rules to a basic phrase-based system. We assess these grammars in terms of their expressive power, measured by their ability to align the parallel text from which their rules are extracted, and the quality of the translations they yield. In Chinese-to-English translation, we find that rule extraction from posteriors gives translation improvements. We also find that grammars with rules with only one nonterminal, when extracted from posteri- ors, can outperform more complex grammars extracted from Viterbi alignments. Finally, we show that the best way to exploit source-totarget and target-to-source alignment models is to build two separate systems and combine their output translation lattices.</p><p>5 0.091339238 <a title="94-tfidf-5" href="./emnlp-2010-What_a_Parser_Can_Learn_from_a_Semantic_Role_Labeler_and_Vice_Versa.html">121 emnlp-2010-What a Parser Can Learn from a Semantic Role Labeler and Vice Versa</a></p>
<p>Author: Stephen Boxwell ; Dennis Mehay ; Chris Brew</p><p>Abstract: In many NLP systems, there is a unidirectional flow of information in which a parser supplies input to a semantic role labeler. In this paper, we build a system that allows information to flow in both directions. We make use of semantic role predictions in choosing a single-best parse. This process relies on an averaged perceptron model to distinguish likely semantic roles from erroneous ones. Our system penalizes parses that give rise to low-scoring semantic roles. To explore the consequences of this we perform two experiments. First, we use a baseline generative model to produce n-best parses, which are then re-ordered by our semantic model. Second, we use a modified version of our semantic role labeler to predict semantic roles at parse time. The performance of this modified labeler is weaker than that of our best full SRL, because it is restricted to features that can be computed directly from the parser’s packed chart. For both experiments, the resulting semantic predictions are then used to select parses. Finally, we feed the selected parses produced by each experiment to the full version of our semantic role labeler. We find that SRL performance can be improved over this baseline by selecting parses with likely semantic roles.</p><p>6 0.087964565 <a title="94-tfidf-6" href="./emnlp-2010-Unsupervised_Induction_of_Tree_Substitution_Grammars_for_Dependency_Parsing.html">113 emnlp-2010-Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing</a></p>
<p>7 0.067831248 <a title="94-tfidf-7" href="./emnlp-2010-Non-Isomorphic_Forest_Pair_Translation.html">86 emnlp-2010-Non-Isomorphic Forest Pair Translation</a></p>
<p>8 0.062237728 <a title="94-tfidf-8" href="./emnlp-2010-On_Dual_Decomposition_and_Linear_Programming_Relaxations_for_Natural_Language_Processing.html">88 emnlp-2010-On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing</a></p>
<p>9 0.054344401 <a title="94-tfidf-9" href="./emnlp-2010-Top-Down_Nearly-Context-Sensitive_Parsing.html">106 emnlp-2010-Top-Down Nearly-Context-Sensitive Parsing</a></p>
<p>10 0.043120265 <a title="94-tfidf-10" href="./emnlp-2010-Statistical_Machine_Translation_with_a_Factorized_Grammar.html">99 emnlp-2010-Statistical Machine Translation with a Factorized Grammar</a></p>
<p>11 0.043056894 <a title="94-tfidf-11" href="./emnlp-2010-Using_Universal_Linguistic_Knowledge_to_Guide_Grammar_Induction.html">116 emnlp-2010-Using Universal Linguistic Knowledge to Guide Grammar Induction</a></p>
<p>12 0.040881198 <a title="94-tfidf-12" href="./emnlp-2010-EMNLP_044.html">39 emnlp-2010-EMNLP 044</a></p>
<p>13 0.039379571 <a title="94-tfidf-13" href="./emnlp-2010-Inducing_Probabilistic_CCG_Grammars_from_Logical_Form_with_Higher-Order_Unification.html">65 emnlp-2010-Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification</a></p>
<p>14 0.035701238 <a title="94-tfidf-14" href="./emnlp-2010-Utilizing_Extra-Sentential_Context_for_Parsing.html">118 emnlp-2010-Utilizing Extra-Sentential Context for Parsing</a></p>
<p>15 0.033333965 <a title="94-tfidf-15" href="./emnlp-2010-Maximum_Entropy_Based_Phrase_Reordering_for_Hierarchical_Phrase-Based_Translation.html">76 emnlp-2010-Maximum Entropy Based Phrase Reordering for Hierarchical Phrase-Based Translation</a></p>
<p>16 0.033018254 <a title="94-tfidf-16" href="./emnlp-2010-Learning_First-Order_Horn_Clauses_from_Web_Text.html">72 emnlp-2010-Learning First-Order Horn Clauses from Web Text</a></p>
<p>17 0.03261495 <a title="94-tfidf-17" href="./emnlp-2010-%22Poetic%22_Statistical_Machine_Translation%3A_Rhyme_and_Meter.html">1 emnlp-2010-"Poetic" Statistical Machine Translation: Rhyme and Meter</a></p>
<p>18 0.03076851 <a title="94-tfidf-18" href="./emnlp-2010-A_Unified_Framework_for_Scope_Learning_via_Simplified_Shallow_Semantic_Parsing.html">15 emnlp-2010-A Unified Framework for Scope Learning via Simplified Shallow Semantic Parsing</a></p>
<p>19 0.030229641 <a title="94-tfidf-19" href="./emnlp-2010-Title_Generation_with_Quasi-Synchronous_Grammar.html">105 emnlp-2010-Title Generation with Quasi-Synchronous Grammar</a></p>
<p>20 0.029607652 <a title="94-tfidf-20" href="./emnlp-2010-Self-Training_with_Products_of_Latent_Variable_Grammars.html">96 emnlp-2010-Self-Training with Products of Latent Variable Grammars</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.122), (1, -0.028), (2, 0.182), (3, -0.003), (4, 0.136), (5, 0.056), (6, 0.011), (7, -0.047), (8, -0.122), (9, -0.093), (10, 0.082), (11, -0.045), (12, -0.054), (13, -0.038), (14, -0.009), (15, -0.082), (16, -0.069), (17, -0.12), (18, -0.004), (19, 0.007), (20, -0.038), (21, -0.098), (22, 0.067), (23, -0.066), (24, -0.071), (25, -0.013), (26, 0.049), (27, 0.157), (28, 0.002), (29, -0.005), (30, 0.156), (31, 0.01), (32, -0.16), (33, 0.03), (34, -0.126), (35, 0.239), (36, 0.011), (37, -0.198), (38, -0.015), (39, -0.171), (40, 0.003), (41, 0.068), (42, 0.069), (43, 0.175), (44, -0.132), (45, 0.048), (46, -0.107), (47, 0.116), (48, 0.008), (49, 0.286)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96896541 <a title="94-lsi-1" href="./emnlp-2010-SCFG_Decoding_Without_Binarization.html">94 emnlp-2010-SCFG Decoding Without Binarization</a></p>
<p>Author: Mark Hopkins ; Greg Langmead</p><p>Abstract: Conventional wisdom dictates that synchronous context-free grammars (SCFGs) must be converted to Chomsky Normal Form (CNF) to ensure cubic time decoding. For arbitrary SCFGs, this is typically accomplished via the synchronous binarization technique of (Zhang et al., 2006). A drawback to this approach is that it inflates the constant factors associated with decoding, and thus the practical running time. (DeNero et al., 2009) tackle this problem by defining a superset of CNF called Lexical Normal Form (LNF), which also supports cubic time decoding under certain implicit assumptions. In this paper, we make these assumptions explicit, and in doing so, show that LNF can be further expanded to a broader class of grammars (called “scope3”) that also supports cubic-time decoding. By simply pruning non-scope-3 rules from a GHKM-extracted grammar, we obtain better translation performance than synchronous binarization.</p><p>2 0.52600199 <a title="94-lsi-2" href="./emnlp-2010-Soft_Syntactic_Constraints_for_Hierarchical_Phrase-Based_Translation_Using_Latent_Syntactic_Distributions.html">98 emnlp-2010-Soft Syntactic Constraints for Hierarchical Phrase-Based Translation Using Latent Syntactic Distributions</a></p>
<p>Author: Zhongqiang Huang ; Martin Cmejrek ; Bowen Zhou</p><p>Abstract: In this paper, we present a novel approach to enhance hierarchical phrase-based machine translation systems with linguistically motivated syntactic features. Rather than directly using treebank categories as in previous studies, we learn a set of linguistically-guided latent syntactic categories automatically from a source-side parsed, word-aligned parallel corpus, based on the hierarchical structure among phrase pairs as well as the syntactic structure of the source side. In our model, each X nonterminal in a SCFG rule is decorated with a real-valued feature vector computed based on its distribution of latent syntactic categories. These feature vectors are utilized at decod- ing time to measure the similarity between the syntactic analysis of the source side and the syntax of the SCFG rules that are applied to derive translations. Our approach maintains the advantages of hierarchical phrase-based translation systems while at the same time naturally incorporates soft syntactic constraints.</p><p>3 0.32621861 <a title="94-lsi-3" href="./emnlp-2010-Efficient_Incremental_Decoding_for_Tree-to-String_Translation.html">42 emnlp-2010-Efficient Incremental Decoding for Tree-to-String Translation</a></p>
<p>Author: Liang Huang ; Haitao Mi</p><p>Abstract: Syntax-based translation models should in principle be efficient with polynomially-sized search space, but in practice they are often embarassingly slow, partly due to the cost of language model integration. In this paper we borrow from phrase-based decoding the idea to generate a translation incrementally left-to-right, and show that for tree-to-string models, with a clever encoding of derivation history, this method runs in averagecase polynomial-time in theory, and lineartime with beam search in practice (whereas phrase-based decoding is exponential-time in theory and quadratic-time in practice). Experiments show that, with comparable translation quality, our tree-to-string system (in Python) can run more than 30 times faster than the phrase-based system Moses (in C++).</p><p>4 0.29394805 <a title="94-lsi-4" href="./emnlp-2010-Unsupervised_Induction_of_Tree_Substitution_Grammars_for_Dependency_Parsing.html">113 emnlp-2010-Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing</a></p>
<p>Author: Phil Blunsom ; Trevor Cohn</p><p>Abstract: Inducing a grammar directly from text is one of the oldest and most challenging tasks in Computational Linguistics. Significant progress has been made for inducing dependency grammars, however the models employed are overly simplistic, particularly in comparison to supervised parsing models. In this paper we present an approach to dependency grammar induction using tree substitution grammar which is capable of learning large dependency fragments and thereby better modelling the text. We define a hierarchical non-parametric Pitman-Yor Process prior which biases towards a small grammar with simple productions. This approach significantly improves the state-of-the-art, when measured by head attachment accuracy.</p><p>5 0.28025198 <a title="94-lsi-5" href="./emnlp-2010-Non-Isomorphic_Forest_Pair_Translation.html">86 emnlp-2010-Non-Isomorphic Forest Pair Translation</a></p>
<p>Author: Hui Zhang ; Min Zhang ; Haizhou Li ; Eng Siong Chng</p><p>Abstract: This paper studies two issues, non-isomorphic structure translation and target syntactic structure usage, for statistical machine translation in the context of forest-based tree to tree sequence translation. For the first issue, we propose a novel non-isomorphic translation framework to capture more non-isomorphic structure mappings than traditional tree-based and tree-sequence-based translation methods. For the second issue, we propose a parallel space searching method to generate hypothesis using tree-to-string model and evaluate its syntactic goodness using tree-to-tree/tree sequence model. This not only reduces the search complexity by merging spurious-ambiguity translation paths and solves the data sparseness issue in training, but also serves as a syntax-based target language model for better grammatical generation. Experiment results on the benchmark data show our proposed two solutions are very effective, achieving significant performance improvement over baselines when applying to different translation models.</p><p>6 0.2583589 <a title="94-lsi-6" href="./emnlp-2010-Hierarchical_Phrase-Based_Translation_Grammars_Extracted_from_Alignment_Posterior_Probabilities.html">57 emnlp-2010-Hierarchical Phrase-Based Translation Grammars Extracted from Alignment Posterior Probabilities</a></p>
<p>7 0.22216396 <a title="94-lsi-7" href="./emnlp-2010-What_a_Parser_Can_Learn_from_a_Semantic_Role_Labeler_and_Vice_Versa.html">121 emnlp-2010-What a Parser Can Learn from a Semantic Role Labeler and Vice Versa</a></p>
<p>8 0.2212352 <a title="94-lsi-8" href="./emnlp-2010-A_Unified_Framework_for_Scope_Learning_via_Simplified_Shallow_Semantic_Parsing.html">15 emnlp-2010-A Unified Framework for Scope Learning via Simplified Shallow Semantic Parsing</a></p>
<p>9 0.21817833 <a title="94-lsi-9" href="./emnlp-2010-Word-Based_Dialect_Identification_with_Georeferenced_Rules.html">123 emnlp-2010-Word-Based Dialect Identification with Georeferenced Rules</a></p>
<p>10 0.21001597 <a title="94-lsi-10" href="./emnlp-2010-Storing_the_Web_in_Memory%3A_Space_Efficient_Language_Models_with_Constant_Time_Retrieval.html">101 emnlp-2010-Storing the Web in Memory: Space Efficient Language Models with Constant Time Retrieval</a></p>
<p>11 0.17169152 <a title="94-lsi-11" href="./emnlp-2010-Top-Down_Nearly-Context-Sensitive_Parsing.html">106 emnlp-2010-Top-Down Nearly-Context-Sensitive Parsing</a></p>
<p>12 0.15944941 <a title="94-lsi-12" href="./emnlp-2010-Title_Generation_with_Quasi-Synchronous_Grammar.html">105 emnlp-2010-Title Generation with Quasi-Synchronous Grammar</a></p>
<p>13 0.15406784 <a title="94-lsi-13" href="./emnlp-2010-On_Dual_Decomposition_and_Linear_Programming_Relaxations_for_Natural_Language_Processing.html">88 emnlp-2010-On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing</a></p>
<p>14 0.15061694 <a title="94-lsi-14" href="./emnlp-2010-Using_Universal_Linguistic_Knowledge_to_Guide_Grammar_Induction.html">116 emnlp-2010-Using Universal Linguistic Knowledge to Guide Grammar Induction</a></p>
<p>15 0.14761104 <a title="94-lsi-15" href="./emnlp-2010-Resolving_Event_Noun_Phrases_to_Their_Verbal_Mentions.html">93 emnlp-2010-Resolving Event Noun Phrases to Their Verbal Mentions</a></p>
<p>16 0.1431904 <a title="94-lsi-16" href="./emnlp-2010-EMNLP_044.html">39 emnlp-2010-EMNLP 044</a></p>
<p>17 0.13736041 <a title="94-lsi-17" href="./emnlp-2010-Learning_First-Order_Horn_Clauses_from_Web_Text.html">72 emnlp-2010-Learning First-Order Horn Clauses from Web Text</a></p>
<p>18 0.13245721 <a title="94-lsi-18" href="./emnlp-2010-A_Semi-Supervised_Approach_to_Improve_Classification_of_Infrequent_Discourse_Relations_Using_Feature_Vector_Extension.html">11 emnlp-2010-A Semi-Supervised Approach to Improve Classification of Infrequent Discourse Relations Using Feature Vector Extension</a></p>
<p>19 0.12841597 <a title="94-lsi-19" href="./emnlp-2010-Improving_Gender_Classification_of_Blog_Authors.html">61 emnlp-2010-Improving Gender Classification of Blog Authors</a></p>
<p>20 0.12837379 <a title="94-lsi-20" href="./emnlp-2010-%22Poetic%22_Statistical_Machine_Translation%3A_Rhyme_and_Meter.html">1 emnlp-2010-"Poetic" Statistical Machine Translation: Rhyme and Meter</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(12, 0.02), (29, 0.104), (30, 0.018), (32, 0.017), (35, 0.378), (52, 0.045), (56, 0.072), (62, 0.025), (66, 0.045), (72, 0.022), (76, 0.066), (77, 0.011), (79, 0.013), (87, 0.016), (89, 0.017), (96, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77550966 <a title="94-lda-1" href="./emnlp-2010-SCFG_Decoding_Without_Binarization.html">94 emnlp-2010-SCFG Decoding Without Binarization</a></p>
<p>Author: Mark Hopkins ; Greg Langmead</p><p>Abstract: Conventional wisdom dictates that synchronous context-free grammars (SCFGs) must be converted to Chomsky Normal Form (CNF) to ensure cubic time decoding. For arbitrary SCFGs, this is typically accomplished via the synchronous binarization technique of (Zhang et al., 2006). A drawback to this approach is that it inflates the constant factors associated with decoding, and thus the practical running time. (DeNero et al., 2009) tackle this problem by defining a superset of CNF called Lexical Normal Form (LNF), which also supports cubic time decoding under certain implicit assumptions. In this paper, we make these assumptions explicit, and in doing so, show that LNF can be further expanded to a broader class of grammars (called “scope3”) that also supports cubic-time decoding. By simply pruning non-scope-3 rules from a GHKM-extracted grammar, we obtain better translation performance than synchronous binarization.</p><p>2 0.37393761 <a title="94-lda-2" href="./emnlp-2010-Soft_Syntactic_Constraints_for_Hierarchical_Phrase-Based_Translation_Using_Latent_Syntactic_Distributions.html">98 emnlp-2010-Soft Syntactic Constraints for Hierarchical Phrase-Based Translation Using Latent Syntactic Distributions</a></p>
<p>Author: Zhongqiang Huang ; Martin Cmejrek ; Bowen Zhou</p><p>Abstract: In this paper, we present a novel approach to enhance hierarchical phrase-based machine translation systems with linguistically motivated syntactic features. Rather than directly using treebank categories as in previous studies, we learn a set of linguistically-guided latent syntactic categories automatically from a source-side parsed, word-aligned parallel corpus, based on the hierarchical structure among phrase pairs as well as the syntactic structure of the source side. In our model, each X nonterminal in a SCFG rule is decorated with a real-valued feature vector computed based on its distribution of latent syntactic categories. These feature vectors are utilized at decod- ing time to measure the similarity between the syntactic analysis of the source side and the syntax of the SCFG rules that are applied to derive translations. Our approach maintains the advantages of hierarchical phrase-based translation systems while at the same time naturally incorporates soft syntactic constraints.</p><p>3 0.33652931 <a title="94-lda-3" href="./emnlp-2010-Title_Generation_with_Quasi-Synchronous_Grammar.html">105 emnlp-2010-Title Generation with Quasi-Synchronous Grammar</a></p>
<p>Author: Kristian Woodsend ; Yansong Feng ; Mirella Lapata</p><p>Abstract: The task of selecting information and rendering it appropriately appears in multiple contexts in summarization. In this paper we present a model that simultaneously optimizes selection and rendering preferences. The model operates over a phrase-based representation of the source document which we obtain by merging PCFG parse trees and dependency graphs. Selection preferences for individual phrases are learned discriminatively, while a quasi-synchronous grammar (Smith and Eisner, 2006) captures rendering preferences such as paraphrases and compressions. Based on an integer linear programming formulation, the model learns to generate summaries that satisfy both types of preferences, while ensuring that length, topic coverage and grammar constraints are met. Experiments on headline and image caption generation show that our method obtains state-of-the-art performance using essentially the same model for both tasks without any major modifications.</p><p>4 0.3349632 <a title="94-lda-4" href="./emnlp-2010-Hierarchical_Phrase-Based_Translation_Grammars_Extracted_from_Alignment_Posterior_Probabilities.html">57 emnlp-2010-Hierarchical Phrase-Based Translation Grammars Extracted from Alignment Posterior Probabilities</a></p>
<p>Author: Adria de Gispert ; Juan Pino ; William Byrne</p><p>Abstract: We report on investigations into hierarchical phrase-based translation grammars based on rules extracted from posterior distributions over alignments of the parallel text. Rather than restrict rule extraction to a single alignment, such as Viterbi, we instead extract rules based on posterior distributions provided by the HMM word-to-word alignmentmodel. We define translation grammars progressively by adding classes of rules to a basic phrase-based system. We assess these grammars in terms of their expressive power, measured by their ability to align the parallel text from which their rules are extracted, and the quality of the translations they yield. In Chinese-to-English translation, we find that rule extraction from posteriors gives translation improvements. We also find that grammars with rules with only one nonterminal, when extracted from posteri- ors, can outperform more complex grammars extracted from Viterbi alignments. Finally, we show that the best way to exploit source-totarget and target-to-source alignment models is to build two separate systems and combine their output translation lattices.</p><p>5 0.33144367 <a title="94-lda-5" href="./emnlp-2010-Inducing_Probabilistic_CCG_Grammars_from_Logical_Form_with_Higher-Order_Unification.html">65 emnlp-2010-Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification</a></p>
<p>Author: Tom Kwiatkowksi ; Luke Zettlemoyer ; Sharon Goldwater ; Mark Steedman</p><p>Abstract: This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning. Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method. The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences. We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations.</p><p>6 0.33124039 <a title="94-lda-6" href="./emnlp-2010-A_Latent_Variable_Model_for_Geographic_Lexical_Variation.html">6 emnlp-2010-A Latent Variable Model for Geographic Lexical Variation</a></p>
<p>7 0.33059654 <a title="94-lda-7" href="./emnlp-2010-Effects_of_Empty_Categories_on_Machine_Translation.html">40 emnlp-2010-Effects of Empty Categories on Machine Translation</a></p>
<p>8 0.33001384 <a title="94-lda-8" href="./emnlp-2010-PEM%3A_A_Paraphrase_Evaluation_Metric_Exploiting_Parallel_Texts.html">89 emnlp-2010-PEM: A Paraphrase Evaluation Metric Exploiting Parallel Texts</a></p>
<p>9 0.32866332 <a title="94-lda-9" href="./emnlp-2010-Minimum_Error_Rate_Training_by_Sampling_the_Translation_Lattice.html">78 emnlp-2010-Minimum Error Rate Training by Sampling the Translation Lattice</a></p>
<p>10 0.32798696 <a title="94-lda-10" href="./emnlp-2010-Assessing_Phrase-Based_Translation_Models_with_Oracle_Decoding.html">18 emnlp-2010-Assessing Phrase-Based Translation Models with Oracle Decoding</a></p>
<p>11 0.32796288 <a title="94-lda-11" href="./emnlp-2010-Efficient_Incremental_Decoding_for_Tree-to-String_Translation.html">42 emnlp-2010-Efficient Incremental Decoding for Tree-to-String Translation</a></p>
<p>12 0.32628411 <a title="94-lda-12" href="./emnlp-2010-Classifying_Dialogue_Acts_in_One-on-One_Live_Chats.html">26 emnlp-2010-Classifying Dialogue Acts in One-on-One Live Chats</a></p>
<p>13 0.32517752 <a title="94-lda-13" href="./emnlp-2010-Using_Universal_Linguistic_Knowledge_to_Guide_Grammar_Induction.html">116 emnlp-2010-Using Universal Linguistic Knowledge to Guide Grammar Induction</a></p>
<p>14 0.32279661 <a title="94-lda-14" href="./emnlp-2010-Nouns_are_Vectors%2C_Adjectives_are_Matrices%3A_Representing_Adjective-Noun_Constructions_in_Semantic_Space.html">87 emnlp-2010-Nouns are Vectors, Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space</a></p>
<p>15 0.32161167 <a title="94-lda-15" href="./emnlp-2010-Multi-Document_Summarization_Using_A%2A_Search_and_Discriminative_Learning.html">82 emnlp-2010-Multi-Document Summarization Using A* Search and Discriminative Learning</a></p>
<p>16 0.32133391 <a title="94-lda-16" href="./emnlp-2010-Towards_Conversation_Entailment%3A_An_Empirical_Investigation.html">107 emnlp-2010-Towards Conversation Entailment: An Empirical Investigation</a></p>
<p>17 0.31906459 <a title="94-lda-17" href="./emnlp-2010-A_Mixture_Model_with_Sharing_for_Lexical_Semantics.html">7 emnlp-2010-A Mixture Model with Sharing for Lexical Semantics</a></p>
<p>18 0.31818917 <a title="94-lda-18" href="./emnlp-2010-Turbo_Parsers%3A_Dependency_Parsing_by_Approximate_Variational_Inference.html">110 emnlp-2010-Turbo Parsers: Dependency Parsing by Approximate Variational Inference</a></p>
<p>19 0.31810302 <a title="94-lda-19" href="./emnlp-2010-Crouching_Dirichlet%2C_Hidden_Markov_Model%3A_Unsupervised_POS_Tagging_with_Context_Local_Tag_Generation.html">34 emnlp-2010-Crouching Dirichlet, Hidden Markov Model: Unsupervised POS Tagging with Context Local Tag Generation</a></p>
<p>20 0.31791094 <a title="94-lda-20" href="./emnlp-2010-Context_Comparison_of_Bursty_Events_in_Web_Search_and_Online_Media.html">32 emnlp-2010-Context Comparison of Bursty Events in Web Search and Online Media</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
