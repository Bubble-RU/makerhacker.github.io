<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>25 emnlp-2010-Better Punctuation Prediction with Dynamic Conditional Random Fields</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2010" href="../home/emnlp2010_home.html">emnlp2010</a> <a title="emnlp-2010-25" href="#">emnlp2010-25</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>25 emnlp-2010-Better Punctuation Prediction with Dynamic Conditional Random Fields</h1>
<br/><p>Source: <a title="emnlp-2010-25-pdf" href="http://aclweb.org/anthology//D/D10/D10-1018.pdf">pdf</a></p><p>Author: Wei Lu ; Hwee Tou Ng</p><p>Abstract: This paper focuses on the task of inserting punctuation symbols into transcribed conversational speech texts, without relying on prosodic cues. We investigate limitations associated with previous methods, and propose a novel approach based on dynamic conditional random fields. Different from previous work, our proposed approach is designed to jointly perform both sentence boundary and sentence type prediction, and punctuation prediction on speech utterances. We performed evaluations on a transcribed conversational speech domain consisting of both English and Chinese texts. Empirical results show that our method outperforms an approach based on linear-chain conditional random fields and other previous approaches.</p><p>Reference: <a title="emnlp-2010-25-reference" href="../emnlp2010_reference/emnlp-2010-Better_Punctuation_Prediction_with_Dynamic_Conditional_Random_Fields_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('punctu', 0.482), ('btec', 0.271), ('iwslt', 0.257), ('crf', 0.243), ('symbol', 0.212), ('qnin', 0.203), ('asr', 0.199), ('ut', 0.158), ('prosod', 0.157), ('lay', 0.153), ('dein', 0.135), ('ct', 0.135), ('sutton', 0.122), ('predict', 0.11), ('transl', 0.102), ('speech', 0.101), ('hid', 0.098), ('cliqu', 0.097), ('field', 0.096), ('pleas', 0.094)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="25-tfidf-1" href="./emnlp-2010-Better_Punctuation_Prediction_with_Dynamic_Conditional_Random_Fields.html">25 emnlp-2010-Better Punctuation Prediction with Dynamic Conditional Random Fields</a></p>
<p>Author: Wei Lu ; Hwee Tou Ng</p><p>Abstract: This paper focuses on the task of inserting punctuation symbols into transcribed conversational speech texts, without relying on prosodic cues. We investigate limitations associated with previous methods, and propose a novel approach based on dynamic conditional random fields. Different from previous work, our proposed approach is designed to jointly perform both sentence boundary and sentence type prediction, and punctuation prediction on speech utterances. We performed evaluations on a transcribed conversational speech domain consisting of both English and Chinese texts. Empirical results show that our method outperforms an approach based on linear-chain conditional random fields and other previous approaches.</p><p>2 0.25075814 <a title="25-tfidf-2" href="./emnlp-2010-Lessons_Learned_in_Part-of-Speech_Tagging_of_Conversational_Speech.html">75 emnlp-2010-Lessons Learned in Part-of-Speech Tagging of Conversational Speech</a></p>
<p>Author: Vladimir Eidelman ; Zhongqiang Huang ; Mary Harper</p><p>Abstract: This paper examines tagging models for spontaneous English speech transcripts. We analyze the performance of state-of-the-art tagging models, either generative or discriminative, left-to-right or bidirectional, with or without latent annotations, together with the use of ToBI break indexes and several methods for segmenting the speech transcripts (i.e., conversation side, speaker turn, or humanannotated sentence). Based on these studies, we observe that: (1) bidirectional models tend to achieve better accuracy levels than left-toright models, (2) generative models seem to perform somewhat better than discriminative models on this task, and (3) prosody improves tagging performance of models on conversation sides, but has much less impact on smaller segments. We conclude that, although the use of break indexes can indeed significantly im- prove performance over baseline models without them on conversation sides, tagging accuracy improves more by using smaller segments, for which the impact of the break indexes is marginal.</p><p>3 0.17531039 <a title="25-tfidf-3" href="./emnlp-2010-Efficient_Graph-Based_Semi-Supervised_Learning_of_Structured_Tagging_Models.html">41 emnlp-2010-Efficient Graph-Based Semi-Supervised Learning of Structured Tagging Models</a></p>
<p>Author: Amarnag Subramanya ; Slav Petrov ; Fernando Pereira</p><p>Abstract: We describe a new scalable algorithm for semi-supervised training of conditional random fields (CRF) and its application to partof-speech (POS) tagging. The algorithm uses a similarity graph to encourage similar ngrams to have similar POS tags. We demonstrate the efficacy of our approach on a domain adaptation task, where we assume that we have access to large amounts of unlabeled data from the target domain, but no additional labeled data. The similarity graph is used during training to smooth the state posteriors on the target domain. Standard inference can be used at test time. Our approach is able to scale to very large problems and yields significantly improved target domain accuracy.</p><p>4 0.1529177 <a title="25-tfidf-4" href="./emnlp-2010-Enhancing_Domain_Portability_of_Chinese_Segmentation_Model_Using_Chi-Square_Statistics_and_Bootstrapping.html">43 emnlp-2010-Enhancing Domain Portability of Chinese Segmentation Model Using Chi-Square Statistics and Bootstrapping</a></p>
<p>Author: Baobao Chang ; Dongxu Han</p><p>Abstract: Almost all Chinese language processing tasks involve word segmentation of the language input as their first steps, thus robust and reliable segmentation techniques are always required to make sure those tasks wellperformed. In recent years, machine learning and sequence labeling models such as Conditional Random Fields (CRFs) are often used in segmenting Chinese texts. Compared with traditional lexicon-driven models, machine learned models achieve higher F-measure scores. But machine learned models heavily depend on training materials. Although they can effectively process texts from the same domain as the training texts, they perform relatively poorly when texts from new domains are to be processed. In this paper, we propose to use Ï‡2 statistics when training an SVM-HMM based segmentation model to im- prove its ability to recall OOV words and then use bootstrapping strategies to maintain its ability to recall IV words. Experiments show the approach proposed in this paper enhances the domain portability of the Chinese word segmentation model and prevents drastic decline in performance when processing texts across domains.</p><p>5 0.12341104 <a title="25-tfidf-5" href="./emnlp-2010-Classifying_Dialogue_Acts_in_One-on-One_Live_Chats.html">26 emnlp-2010-Classifying Dialogue Acts in One-on-One Live Chats</a></p>
<p>Author: Su Nam Kim ; Lawrence Cavedon ; Timothy Baldwin</p><p>Abstract: We explore the task of automatically classifying dialogue acts in 1-on-1 online chat forums, an increasingly popular means of providing customer service. In particular, we investigate the effectiveness of various features and machine learners for this task. While a simple bag-of-words approach provides a solid baseline, we find that adding information from dialogue structure and inter-utterance dependency provides some increase in performance; learners that account for sequential dependencies (CRFs) show the best performance. We report our results from testing using a corpus of chat dialogues derived from online shopping customer-feedback data.</p><p>6 0.11626454 <a title="25-tfidf-6" href="./emnlp-2010-Cross_Language_Text_Classification_by_Model_Translation_and_Semi-Supervised_Learning.html">33 emnlp-2010-Cross Language Text Classification by Model Translation and Semi-Supervised Learning</a></p>
<p>7 0.10790714 <a title="25-tfidf-7" href="./emnlp-2010-NLP_on_Spoken_Documents_Without_ASR.html">84 emnlp-2010-NLP on Spoken Documents Without ASR</a></p>
<p>8 0.099960938 <a title="25-tfidf-8" href="./emnlp-2010-Confidence_in_Structured-Prediction_Using_Confidence-Weighted_Models.html">30 emnlp-2010-Confidence in Structured-Prediction Using Confidence-Weighted Models</a></p>
<p>9 0.095083304 <a title="25-tfidf-9" href="./emnlp-2010-Using_Universal_Linguistic_Knowledge_to_Guide_Grammar_Induction.html">116 emnlp-2010-Using Universal Linguistic Knowledge to Guide Grammar Induction</a></p>
<p>10 0.092685498 <a title="25-tfidf-10" href="./emnlp-2010-A_Game-Theoretic_Approach_to_Generating_Spatial_Descriptions.html">4 emnlp-2010-A Game-Theoretic Approach to Generating Spatial Descriptions</a></p>
<p>11 0.092060246 <a title="25-tfidf-11" href="./emnlp-2010-Joint_Training_and_Decoding_Using_Virtual_Nodes_for_Cascaded_Segmentation_and_Tagging_Tasks.html">69 emnlp-2010-Joint Training and Decoding Using Virtual Nodes for Cascaded Segmentation and Tagging Tasks</a></p>
<p>12 0.090035722 <a title="25-tfidf-12" href="./emnlp-2010-Incorporating_Content_Structure_into_Text_Analysis_Applications.html">64 emnlp-2010-Incorporating Content Structure into Text Analysis Applications</a></p>
<p>13 0.087162316 <a title="25-tfidf-13" href="./emnlp-2010-Minimum_Error_Rate_Training_by_Sampling_the_Translation_Lattice.html">78 emnlp-2010-Minimum Error Rate Training by Sampling the Translation Lattice</a></p>
<p>14 0.085471854 <a title="25-tfidf-14" href="./emnlp-2010-Towards_Conversation_Entailment%3A_An_Empirical_Investigation.html">107 emnlp-2010-Towards Conversation Entailment: An Empirical Investigation</a></p>
<p>15 0.084828615 <a title="25-tfidf-15" href="./emnlp-2010-Simple_Type-Level_Unsupervised_POS_Tagging.html">97 emnlp-2010-Simple Type-Level Unsupervised POS Tagging</a></p>
<p>16 0.073676504 <a title="25-tfidf-16" href="./emnlp-2010-Improving_Translation_via_Targeted_Paraphrasing.html">63 emnlp-2010-Improving Translation via Targeted Paraphrasing</a></p>
<p>17 0.0735064 <a title="25-tfidf-17" href="./emnlp-2010-Assessing_Phrase-Based_Translation_Models_with_Oracle_Decoding.html">18 emnlp-2010-Assessing Phrase-Based Translation Models with Oracle Decoding</a></p>
<p>18 0.069033973 <a title="25-tfidf-18" href="./emnlp-2010-Example-Based_Paraphrasing_for_Improved_Phrase-Based_Statistical_Machine_Translation.html">47 emnlp-2010-Example-Based Paraphrasing for Improved Phrase-Based Statistical Machine Translation</a></p>
<p>19 0.068680368 <a title="25-tfidf-19" href="./emnlp-2010-Discriminative_Word_Alignment_with_a_Function_Word_Reordering_Model.html">36 emnlp-2010-Discriminative Word Alignment with a Function Word Reordering Model</a></p>
<p>20 0.066201121 <a title="25-tfidf-20" href="./emnlp-2010-Utilizing_Extra-Sentential_Context_for_Parsing.html">118 emnlp-2010-Utilizing Extra-Sentential Context for Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.247), (1, 0.063), (2, -0.022), (3, -0.084), (4, -0.048), (5, -0.189), (6, 0.044), (7, 0.27), (8, -0.025), (9, 0.186), (10, -0.235), (11, -0.067), (12, -0.185), (13, -0.087), (14, 0.05), (15, 0.024), (16, -0.078), (17, -0.006), (18, -0.087), (19, -0.003), (20, -0.042), (21, -0.049), (22, 0.122), (23, 0.107), (24, 0.014), (25, -0.072), (26, -0.053), (27, 0.038), (28, -0.137), (29, -0.137), (30, -0.05), (31, 0.031), (32, -0.098), (33, -0.068), (34, 0.118), (35, 0.077), (36, -0.143), (37, -0.164), (38, -0.06), (39, -0.045), (40, 0.097), (41, 0.039), (42, -0.032), (43, 0.006), (44, -0.205), (45, 0.132), (46, 0.03), (47, -0.032), (48, -0.045), (49, -0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93303031 <a title="25-lsi-1" href="./emnlp-2010-Better_Punctuation_Prediction_with_Dynamic_Conditional_Random_Fields.html">25 emnlp-2010-Better Punctuation Prediction with Dynamic Conditional Random Fields</a></p>
<p>Author: Wei Lu ; Hwee Tou Ng</p><p>Abstract: This paper focuses on the task of inserting punctuation symbols into transcribed conversational speech texts, without relying on prosodic cues. We investigate limitations associated with previous methods, and propose a novel approach based on dynamic conditional random fields. Different from previous work, our proposed approach is designed to jointly perform both sentence boundary and sentence type prediction, and punctuation prediction on speech utterances. We performed evaluations on a transcribed conversational speech domain consisting of both English and Chinese texts. Empirical results show that our method outperforms an approach based on linear-chain conditional random fields and other previous approaches.</p><p>2 0.71554184 <a title="25-lsi-2" href="./emnlp-2010-Lessons_Learned_in_Part-of-Speech_Tagging_of_Conversational_Speech.html">75 emnlp-2010-Lessons Learned in Part-of-Speech Tagging of Conversational Speech</a></p>
<p>Author: Vladimir Eidelman ; Zhongqiang Huang ; Mary Harper</p><p>Abstract: This paper examines tagging models for spontaneous English speech transcripts. We analyze the performance of state-of-the-art tagging models, either generative or discriminative, left-to-right or bidirectional, with or without latent annotations, together with the use of ToBI break indexes and several methods for segmenting the speech transcripts (i.e., conversation side, speaker turn, or humanannotated sentence). Based on these studies, we observe that: (1) bidirectional models tend to achieve better accuracy levels than left-toright models, (2) generative models seem to perform somewhat better than discriminative models on this task, and (3) prosody improves tagging performance of models on conversation sides, but has much less impact on smaller segments. We conclude that, although the use of break indexes can indeed significantly im- prove performance over baseline models without them on conversation sides, tagging accuracy improves more by using smaller segments, for which the impact of the break indexes is marginal.</p><p>3 0.48996475 <a title="25-lsi-3" href="./emnlp-2010-Efficient_Graph-Based_Semi-Supervised_Learning_of_Structured_Tagging_Models.html">41 emnlp-2010-Efficient Graph-Based Semi-Supervised Learning of Structured Tagging Models</a></p>
<p>Author: Amarnag Subramanya ; Slav Petrov ; Fernando Pereira</p><p>Abstract: We describe a new scalable algorithm for semi-supervised training of conditional random fields (CRF) and its application to partof-speech (POS) tagging. The algorithm uses a similarity graph to encourage similar ngrams to have similar POS tags. We demonstrate the efficacy of our approach on a domain adaptation task, where we assume that we have access to large amounts of unlabeled data from the target domain, but no additional labeled data. The similarity graph is used during training to smooth the state posteriors on the target domain. Standard inference can be used at test time. Our approach is able to scale to very large problems and yields significantly improved target domain accuracy.</p><p>4 0.42225972 <a title="25-lsi-4" href="./emnlp-2010-Enhancing_Domain_Portability_of_Chinese_Segmentation_Model_Using_Chi-Square_Statistics_and_Bootstrapping.html">43 emnlp-2010-Enhancing Domain Portability of Chinese Segmentation Model Using Chi-Square Statistics and Bootstrapping</a></p>
<p>Author: Baobao Chang ; Dongxu Han</p><p>Abstract: Almost all Chinese language processing tasks involve word segmentation of the language input as their first steps, thus robust and reliable segmentation techniques are always required to make sure those tasks wellperformed. In recent years, machine learning and sequence labeling models such as Conditional Random Fields (CRFs) are often used in segmenting Chinese texts. Compared with traditional lexicon-driven models, machine learned models achieve higher F-measure scores. But machine learned models heavily depend on training materials. Although they can effectively process texts from the same domain as the training texts, they perform relatively poorly when texts from new domains are to be processed. In this paper, we propose to use Ï‡2 statistics when training an SVM-HMM based segmentation model to im- prove its ability to recall OOV words and then use bootstrapping strategies to maintain its ability to recall IV words. Experiments show the approach proposed in this paper enhances the domain portability of the Chinese word segmentation model and prevents drastic decline in performance when processing texts across domains.</p><p>5 0.4032743 <a title="25-lsi-5" href="./emnlp-2010-Classifying_Dialogue_Acts_in_One-on-One_Live_Chats.html">26 emnlp-2010-Classifying Dialogue Acts in One-on-One Live Chats</a></p>
<p>Author: Su Nam Kim ; Lawrence Cavedon ; Timothy Baldwin</p><p>Abstract: We explore the task of automatically classifying dialogue acts in 1-on-1 online chat forums, an increasingly popular means of providing customer service. In particular, we investigate the effectiveness of various features and machine learners for this task. While a simple bag-of-words approach provides a solid baseline, we find that adding information from dialogue structure and inter-utterance dependency provides some increase in performance; learners that account for sequential dependencies (CRFs) show the best performance. We report our results from testing using a corpus of chat dialogues derived from online shopping customer-feedback data.</p><p>6 0.34792453 <a title="25-lsi-6" href="./emnlp-2010-Confidence_in_Structured-Prediction_Using_Confidence-Weighted_Models.html">30 emnlp-2010-Confidence in Structured-Prediction Using Confidence-Weighted Models</a></p>
<p>7 0.33662856 <a title="25-lsi-7" href="./emnlp-2010-NLP_on_Spoken_Documents_Without_ASR.html">84 emnlp-2010-NLP on Spoken Documents Without ASR</a></p>
<p>8 0.33493128 <a title="25-lsi-8" href="./emnlp-2010-Training_Continuous_Space_Language_Models%3A_Some_Practical_Issues.html">108 emnlp-2010-Training Continuous Space Language Models: Some Practical Issues</a></p>
<p>9 0.32787639 <a title="25-lsi-9" href="./emnlp-2010-A_Game-Theoretic_Approach_to_Generating_Spatial_Descriptions.html">4 emnlp-2010-A Game-Theoretic Approach to Generating Spatial Descriptions</a></p>
<p>10 0.32611334 <a title="25-lsi-10" href="./emnlp-2010-Effects_of_Empty_Categories_on_Machine_Translation.html">40 emnlp-2010-Effects of Empty Categories on Machine Translation</a></p>
<p>11 0.29561594 <a title="25-lsi-11" href="./emnlp-2010-An_Efficient_Algorithm_for_Unsupervised_Word_Segmentation_with_Branching_Entropy_and_MDL.html">17 emnlp-2010-An Efficient Algorithm for Unsupervised Word Segmentation with Branching Entropy and MDL</a></p>
<p>12 0.27873755 <a title="25-lsi-12" href="./emnlp-2010-Using_Universal_Linguistic_Knowledge_to_Guide_Grammar_Induction.html">116 emnlp-2010-Using Universal Linguistic Knowledge to Guide Grammar Induction</a></p>
<p>13 0.27555317 <a title="25-lsi-13" href="./emnlp-2010-Towards_Conversation_Entailment%3A_An_Empirical_Investigation.html">107 emnlp-2010-Towards Conversation Entailment: An Empirical Investigation</a></p>
<p>14 0.27046677 <a title="25-lsi-14" href="./emnlp-2010-Simple_Type-Level_Unsupervised_POS_Tagging.html">97 emnlp-2010-Simple Type-Level Unsupervised POS Tagging</a></p>
<p>15 0.26465559 <a title="25-lsi-15" href="./emnlp-2010-Incorporating_Content_Structure_into_Text_Analysis_Applications.html">64 emnlp-2010-Incorporating Content Structure into Text Analysis Applications</a></p>
<p>16 0.2537677 <a title="25-lsi-16" href="./emnlp-2010-Assessing_Phrase-Based_Translation_Models_with_Oracle_Decoding.html">18 emnlp-2010-Assessing Phrase-Based Translation Models with Oracle Decoding</a></p>
<p>17 0.24280341 <a title="25-lsi-17" href="./emnlp-2010-Joint_Training_and_Decoding_Using_Virtual_Nodes_for_Cascaded_Segmentation_and_Tagging_Tasks.html">69 emnlp-2010-Joint Training and Decoding Using Virtual Nodes for Cascaded Segmentation and Tagging Tasks</a></p>
<p>18 0.24278262 <a title="25-lsi-18" href="./emnlp-2010-Cross_Language_Text_Classification_by_Model_Translation_and_Semi-Supervised_Learning.html">33 emnlp-2010-Cross Language Text Classification by Model Translation and Semi-Supervised Learning</a></p>
<p>19 0.22659168 <a title="25-lsi-19" href="./emnlp-2010-Minimum_Error_Rate_Training_by_Sampling_the_Translation_Lattice.html">78 emnlp-2010-Minimum Error Rate Training by Sampling the Translation Lattice</a></p>
<p>20 0.22464925 <a title="25-lsi-20" href="./emnlp-2010-Fusing_Eye_Gaze_with_Speech_Recognition_Hypotheses_to_Resolve_Exophoric_References_in_Situated_Dialogue.html">53 emnlp-2010-Fusing Eye Gaze with Speech Recognition Hypotheses to Resolve Exophoric References in Situated Dialogue</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.016), (7, 0.043), (25, 0.045), (31, 0.027), (35, 0.024), (38, 0.523), (39, 0.029), (47, 0.093), (49, 0.029), (54, 0.013), (58, 0.018), (61, 0.01), (62, 0.015), (81, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93028331 <a title="25-lda-1" href="./emnlp-2010-WikiWars%3A_A_New_Corpus_for_Research_on_Temporal_Expressions.html">122 emnlp-2010-WikiWars: A New Corpus for Research on Temporal Expressions</a></p>
<p>Author: Pawel Mazur ; Robert Dale</p><p>Abstract: The reliable extraction of knowledge from text requires an appropriate treatment of the time at which reported events take place. Unfortunately, there are very few annotated data sets that support the development of techniques for event time-stamping and tracking the progression of time through a narrative. In this paper, we present a new corpus of temporally-rich documents sourced from English Wikipedia, which we have annotated with TIMEX2 tags. The corpus contains around 120000 tokens, and 2600 TIMEX2 expressions, thus comparing favourably in size to other existing corpora used in these areas. We describe the prepa- ration of the corpus, and compare the profile of the data with other existing temporally annotated corpora. We also report the results obtained when we use DANTE, our temporal expression tagger, to process this corpus, and point to where further work is required. The corpus is publicly available for research purposes.</p><p>2 0.80873972 <a title="25-lda-2" href="./emnlp-2010-Example-Based_Paraphrasing_for_Improved_Phrase-Based_Statistical_Machine_Translation.html">47 emnlp-2010-Example-Based Paraphrasing for Improved Phrase-Based Statistical Machine Translation</a></p>
<p>Author: Aurelien Max</p><p>Abstract: In this article, an original view on how to improve phrase translation estimates is proposed. This proposal is grounded on two main ideas: first, that appropriate examples of a given phrase should participate more in building its translation distribution; second, that paraphrases can be used to better estimate this distribution. Initial experiments provide evidence of the potential of our approach and its implementation for effectively improving translation performance.</p><p>same-paper 3 0.80623126 <a title="25-lda-3" href="./emnlp-2010-Better_Punctuation_Prediction_with_Dynamic_Conditional_Random_Fields.html">25 emnlp-2010-Better Punctuation Prediction with Dynamic Conditional Random Fields</a></p>
<p>Author: Wei Lu ; Hwee Tou Ng</p><p>Abstract: This paper focuses on the task of inserting punctuation symbols into transcribed conversational speech texts, without relying on prosodic cues. We investigate limitations associated with previous methods, and propose a novel approach based on dynamic conditional random fields. Different from previous work, our proposed approach is designed to jointly perform both sentence boundary and sentence type prediction, and punctuation prediction on speech utterances. We performed evaluations on a transcribed conversational speech domain consisting of both English and Chinese texts. Empirical results show that our method outperforms an approach based on linear-chain conditional random fields and other previous approaches.</p><p>4 0.54692364 <a title="25-lda-4" href="./emnlp-2010-Classifying_Dialogue_Acts_in_One-on-One_Live_Chats.html">26 emnlp-2010-Classifying Dialogue Acts in One-on-One Live Chats</a></p>
<p>Author: Su Nam Kim ; Lawrence Cavedon ; Timothy Baldwin</p><p>Abstract: We explore the task of automatically classifying dialogue acts in 1-on-1 online chat forums, an increasingly popular means of providing customer service. In particular, we investigate the effectiveness of various features and machine learners for this task. While a simple bag-of-words approach provides a solid baseline, we find that adding information from dialogue structure and inter-utterance dependency provides some increase in performance; learners that account for sequential dependencies (CRFs) show the best performance. We report our results from testing using a corpus of chat dialogues derived from online shopping customer-feedback data.</p><p>5 0.5422281 <a title="25-lda-5" href="./emnlp-2010-Fusing_Eye_Gaze_with_Speech_Recognition_Hypotheses_to_Resolve_Exophoric_References_in_Situated_Dialogue.html">53 emnlp-2010-Fusing Eye Gaze with Speech Recognition Hypotheses to Resolve Exophoric References in Situated Dialogue</a></p>
<p>Author: Zahar Prasov ; Joyce Y. Chai</p><p>Abstract: In situated dialogue humans often utter linguistic expressions that refer to extralinguistic entities in the environment. Correctly resolving these references is critical yet challenging for artificial agents partly due to their limited speech recognition and language understanding capabilities. Motivated by psycholinguistic studies demonstrating a tight link between language production and human eye gaze, we have developed approaches that integrate naturally occurring human eye gaze with speech recognition hypotheses to resolve exophoric references in situated dialogue in a virtual world. In addition to incorporating eye gaze with the best recognized spoken hypothesis, we developed an algorithm to also handle multiple hypotheses modeled as word confusion networks. Our empirical results demonstrate that incorporating eye gaze with recognition hypotheses consistently outperforms the results obtained from processing recognition hypotheses alone. Incorporating eye gaze with word confusion networks further improves performance.</p><p>6 0.50281447 <a title="25-lda-6" href="./emnlp-2010-Lessons_Learned_in_Part-of-Speech_Tagging_of_Conversational_Speech.html">75 emnlp-2010-Lessons Learned in Part-of-Speech Tagging of Conversational Speech</a></p>
<p>7 0.50018686 <a title="25-lda-7" href="./emnlp-2010-Maximum_Entropy_Based_Phrase_Reordering_for_Hierarchical_Phrase-Based_Translation.html">76 emnlp-2010-Maximum Entropy Based Phrase Reordering for Hierarchical Phrase-Based Translation</a></p>
<p>8 0.48919484 <a title="25-lda-8" href="./emnlp-2010-A_Simple_Domain-Independent_Probabilistic_Approach_to_Generation.html">13 emnlp-2010-A Simple Domain-Independent Probabilistic Approach to Generation</a></p>
<p>9 0.48517838 <a title="25-lda-9" href="./emnlp-2010-PEM%3A_A_Paraphrase_Evaluation_Metric_Exploiting_Parallel_Texts.html">89 emnlp-2010-PEM: A Paraphrase Evaluation Metric Exploiting Parallel Texts</a></p>
<p>10 0.46871454 <a title="25-lda-10" href="./emnlp-2010-NLP_on_Spoken_Documents_Without_ASR.html">84 emnlp-2010-NLP on Spoken Documents Without ASR</a></p>
<p>11 0.45752934 <a title="25-lda-11" href="./emnlp-2010-An_Efficient_Algorithm_for_Unsupervised_Word_Segmentation_with_Branching_Entropy_and_MDL.html">17 emnlp-2010-An Efficient Algorithm for Unsupervised Word Segmentation with Branching Entropy and MDL</a></p>
<p>12 0.45744461 <a title="25-lda-12" href="./emnlp-2010-Assessing_Phrase-Based_Translation_Models_with_Oracle_Decoding.html">18 emnlp-2010-Assessing Phrase-Based Translation Models with Oracle Decoding</a></p>
<p>13 0.4568646 <a title="25-lda-13" href="./emnlp-2010-Exploiting_Conversation_Structure_in_Unsupervised_Topic_Segmentation_for_Emails.html">48 emnlp-2010-Exploiting Conversation Structure in Unsupervised Topic Segmentation for Emails</a></p>
<p>14 0.45384088 <a title="25-lda-14" href="./emnlp-2010-Context_Comparison_of_Bursty_Events_in_Web_Search_and_Online_Media.html">32 emnlp-2010-Context Comparison of Bursty Events in Web Search and Online Media</a></p>
<p>15 0.45317572 <a title="25-lda-15" href="./emnlp-2010-EMNLP_044.html">39 emnlp-2010-EMNLP 044</a></p>
<p>16 0.45204175 <a title="25-lda-16" href="./emnlp-2010-Title_Generation_with_Quasi-Synchronous_Grammar.html">105 emnlp-2010-Title Generation with Quasi-Synchronous Grammar</a></p>
<p>17 0.4442223 <a title="25-lda-17" href="./emnlp-2010-Incorporating_Content_Structure_into_Text_Analysis_Applications.html">64 emnlp-2010-Incorporating Content Structure into Text Analysis Applications</a></p>
<p>18 0.43692797 <a title="25-lda-18" href="./emnlp-2010-Soft_Syntactic_Constraints_for_Hierarchical_Phrase-Based_Translation_Using_Latent_Syntactic_Distributions.html">98 emnlp-2010-Soft Syntactic Constraints for Hierarchical Phrase-Based Translation Using Latent Syntactic Distributions</a></p>
<p>19 0.43179026 <a title="25-lda-19" href="./emnlp-2010-A_Hybrid_Morpheme-Word_Representation_for_Machine_Translation_of_Morphologically_Rich_Languages.html">5 emnlp-2010-A Hybrid Morpheme-Word Representation for Machine Translation of Morphologically Rich Languages</a></p>
<p>20 0.43175444 <a title="25-lda-20" href="./emnlp-2010-Tense_Sense_Disambiguation%3A_A_New_Syntactic_Polysemy_Task.html">103 emnlp-2010-Tense Sense Disambiguation: A New Syntactic Polysemy Task</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
