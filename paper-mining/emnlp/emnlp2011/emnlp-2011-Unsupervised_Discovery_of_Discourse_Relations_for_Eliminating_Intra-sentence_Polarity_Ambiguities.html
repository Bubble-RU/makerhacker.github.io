<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>142 emnlp-2011-Unsupervised Discovery of Discourse Relations for Eliminating Intra-sentence Polarity Ambiguities</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-142" href="#">emnlp2011-142</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>142 emnlp-2011-Unsupervised Discovery of Discourse Relations for Eliminating Intra-sentence Polarity Ambiguities</h1>
<br/><p>Source: <a title="emnlp-2011-142-pdf" href="http://aclweb.org/anthology//D/D11/D11-1015.pdf">pdf</a></p><p>Author: Lanjun Zhou ; Binyang Li ; Wei Gao ; Zhongyu Wei ; Kam-Fai Wong</p><p>Abstract: Polarity classification of opinionated sentences with both positive and negative sentiments1 is a key challenge in sentiment analysis. This paper presents a novel unsupervised method for discovering intra-sentence level discourse relations for eliminating polarity ambiguities. Firstly, a discourse scheme with discourse constraints on polarity was defined empirically based on Rhetorical Structure Theory (RST). Then, a small set of cuephrase-based patterns were utilized to collect a large number of discourse instances which were later converted to semantic sequential representations (SSRs). Finally, an unsupervised method was adopted to generate, weigh and filter new SSRs without cue phrases for recognizing discourse relations. Experimental results showed that the proposed methods not only effectively recognized the defined discourse relations but also achieved significant improvement by integrating discourse information in sentence-level polarity classification.</p><p>Reference: <a title="emnlp-2011-142-reference" href="../emnlp2011_reference/emnlp-2011-Unsupervised_Discovery_of_Discourse_Relations_for_Eliminating_Intra-sentence_Polarity_Ambiguities_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This paper presents a novel unsupervised method for discovering intra-sentence level discourse relations for eliminating polarity ambiguities. [sent-5, score-1.048]
</p><p>2 Firstly, a discourse scheme with discourse constraints on polarity was defined empirically based on Rhetorical Structure Theory (RST). [sent-6, score-1.535]
</p><p>3 Then, a small set of cuephrase-based patterns were utilized to collect a large number of discourse instances which were later converted to semantic sequential representations (SSRs). [sent-7, score-0.719]
</p><p>4 Finally, an unsupervised method was adopted to generate, weigh and filter new SSRs without cue phrases for recognizing discourse relations. [sent-8, score-0.78]
</p><p>5 Experimental results showed that the proposed methods not only effectively recognized the defined  discourse relations but also achieved significant improvement by integrating discourse information in sentence-level polarity classification. [sent-9, score-1.577]
</p><p>6 1 Introduction As an important task of sentiment analysis, polarity classification is critically affected by discourse structure (Polanyi and Zaenen, 2006). [sent-10, score-1.055]
</p><p>7 , 2008) and proved that the utilization of discourse relations could improve the performance of polarity classification on dialogues (Somasundaran et al. [sent-13, score-1.01]
</p><p>8 However, cur1Defined  as  ambiguous sentences in this  paper  162 rent state-of-the-art methods for sentence-level polarity classification are facing difficulties in ascertaining the polarity of some sentences. [sent-15, score-0.716]
</p><p>9 Example (a) is difficult for existing polarity classification methods for two reasons: (1) the number of positive expressions is less than negative expressions; (2) the importance of each sentiment expression is unknown. [sent-19, score-0.444]
</p><p>10 Existing sentence-level polarity classification methods ignoring discourse structure often give wrong results for these sentences. [sent-26, score-0.935]
</p><p>11 (n and s denote nucleus and satellite segment, respectively) art method (Xu and Kit, 2010) in NTCIR-8 Chinese MOAT as the baseline polarity classifier (BPC) in this paper. [sent-33, score-0.633]
</p><p>12 In this paper, we focused on the automation of recognizing intra-sentence level discourse relations for polarity classification. [sent-35, score-1.005]
</p><p>13 Based on the previous work of Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), a discourse scheme with discourse constraints on polarity was defined empirically (see Section 3). [sent-36, score-1.535]
</p><p>14 From a raw corpus, a small set of cuephrase-based patterns were used to collect discourse instances. [sent-38, score-0.599]
</p><p>15 Experimental results showed that the proposed methods could effectively recognize the defined discourse relations and achieve significant improvement in sentence-level polarity classification comparing to BPC. [sent-41, score-1.01]
</p><p>16 Section 3 presents the discourse scheme with discourse constraints on polarity. [sent-44, score-1.2]
</p><p>17 2  Related Work  Research on polarity classification were generally conducted on 4 levels: document-level (Pang et al. [sent-47, score-0.362]
</p><p>18 There was little research focusing on the automatic recognition of intra-sentence level discourse 163 relations for sentiment analysis in the literature. [sent-52, score-0.73]
</p><p>19 Polanyi and Zaenen (2006) argued that valence calculation is critically affected by discourse structure. [sent-53, score-0.639]
</p><p>20 Nevertheless, they did not propose a computational model for their discourse scheme. [sent-56, score-0.573]
</p><p>21 Nonetheless, contrastive relations were only one type of discourse relations which may help polarity classification. [sent-58, score-1.058]
</p><p>22 (2008) modeled polarity reversal using HCRFs integrated with inter-sentence discourse structures. [sent-60, score-0.908]
</p><p>23 However, our work is on intra-  sentence level and our purpose is not to find polarity reversals but trying to adapt general discourse schemes (e. [sent-61, score-0.908]
</p><p>24 , RST) to help determine the overall polarity of ambiguous sentences. [sent-63, score-0.354]
</p><p>25 , 2009), which proposed opinion frames as a representation ofdiscourse-level associations on dialogue and modeled the scheme to improve opinion polarity classification. [sent-66, score-0.482]
</p><p>26 Our work differs from their approaches in two key aspects: (1) we distinguished nucleus and satellite in discourse but opinion frames did not; (2) our method for discourse discovery was unsupervisedwhile their method needed annotated data. [sent-68, score-1.51]
</p><p>27 Most research works about discourse classification were not related to sentiment analysis. [sent-69, score-0.682]
</p><p>28 Supervised discourse classification methods (Soricut and Marcu, 2003; Duverle and Prendinger, 2009) needed manually annotated data. [sent-70, score-0.6]
</p><p>29 Marcu and Echihabi (2002) presented an unsupervised method to recognize discourse relations held between arbitrary  spans of text. [sent-71, score-0.675]
</p><p>30 They showed that lexical pairs extracted from massive amount of data can have a major impact on discourse classification. [sent-72, score-0.573]
</p><p>31 Thus, in additional to lexical features, we incorporated sequential and semantic information in proposed method for discourse relation classification. [sent-76, score-0.644]
</p><p>32 3  Discourse Scheme for Eliminating Polarity Ambiguities  Since not all of the discourse relations in RST would help eliminate polarity ambiguities, the discourse scheme defined in this paper was on a much coarser level. [sent-78, score-1.59]
</p><p>33 In order to ascertain which relations should be included in our scheme, 500 ambiguous sentences were randomly chosen from NTCIR MOAT Chinese corpus and the most common discourse relations for connecting independent clauses  in compound sentences were annotated. [sent-79, score-0.819]
</p><p>34 We found that 13 relations from RST occupied about 70% of the annotated discourse relations which may help eliminate polarity ambiguities. [sent-80, score-1.058]
</p><p>35 Inspired by Marcu and Echihabi (2002), to construct relatively lownoise discourse instances for unsupervised methods using cue phrases, we grouped the 13 relations into the following 5 relations: Contrast is a union of Antithesis, Concession, Otherwise and Contrast from RST. [sent-81, score-0.819]
</p><p>36 Cue-phrase-based patterns could find only limited number of discourse instances with high precision (Marcu and Echihabi, 2002). [sent-88, score-0.643]
</p><p>37 Thus, we proposed a language independent unsupervised method to identify discourse relations without cue phrases while maintaining relatively high precision. [sent-92, score-0.795]
</p><p>38 For each discourse relation, we started with several cuephrase-based patterns and collected a large number of discourse instances from raw corpus. [sent-93, score-1.216]
</p><p>39 Then, discourse instances were converted to semantic sequential representations (SSRs). [sent-94, score-0.661]
</p><p>40 1  Gathering and representing discourse instances A discourse instance, denoted by Di, consists of two successive segments (Di[1] , Di[2] ) within a sentence. [sent-98, score-1.296]
</p><p>41 Table 1 listed some examples of cue phrases for each discourse relation. [sent-127, score-0.673]
</p><p>42 "CUE1" indicated satellite segments and "CUE2" indicated nucleus segments. [sent-133, score-0.417]
</p><p>43 Note that we did not distinguish satellite from nucleus for Continuation in this paper because the polarity could be determined by either segment. [sent-134, score-0.663]
</p><p>44 To simplify the problem of discourse segmentation, we split compound sentences into discourse segments using commas and semicolons. [sent-136, score-1.239]
</p><p>45 Although we collected discourse instances from compound sentences only, the number of instances for each discourse relation was large enough for the proposed unsupervised method. [sent-137, score-1.327]
</p><p>46 In order to incorporate lexical and semantic information in our method, we represented each word in a discourse instance using a part-of-speech tag, a semantic label and a sentiment tag. [sent-139, score-0.655]
</p><p>47 The next problem became how to start from current SSRs and generate new SSRs for recognizing discourse relations without cue phrases. [sent-151, score-0.77]
</p><p>48 that di and dj could generate a common SSR if and only if the orders of nucleus segment and satellite segment were the same. [sent-179, score-0.626]
</p><p>49 As a result, for each discourse relation rn, a corresponding common SSR set Sn could be obtained by adopting match(di, dj) where i j for all discourse instances. [sent-185, score-1.23]
</p><p>50 An advantage eof i match(d1 , d2) was eth ina-t the generated common SSRs preserved the sequential structure of original discourse instances. [sent-186, score-0.655]
</p><p>51 And common SSRs allows us to build high precision discourse classifiers (See Section 5). [sent-187, score-0.611]
</p><p>52 For example, the adverb "very" in "very brilliant" of D1 was not important for discourse recognition. [sent-189, score-0.573]
</p><p>53 Nouns (except for named entities) and verbs were most representative words in discourse recognition (Marcu and Echihabi, 2002). [sent-196, score-0.573]
</p><p>54 In addition, adjectives and adverbs appearing in sentiment lexicons were  important for polarity classification. [sent-197, score-0.417]
</p><p>55 The common SSR set Sn for each discourse relation rn could be directly used in SSR-based unsupervised classifiers or be employed as effective features in supervised methods. [sent-211, score-0.684]
</p><p>56 7n2)%c e  Table 4: Distribution of discourse relations on NTC-7. [sent-214, score-0.648]
</p><p>57 Others represents discourse relations not included in our discourse scheme. [sent-215, score-1.221]
</p><p>58 1 Annotation work and Data We extracted all compound sentences which may contain the defined discourse relations from opinionated sentences (neutral ones were dropped) of NT-  CIR7 MOAT simplified Chinese training data. [sent-217, score-0.73]
</p><p>59 1,225 discourse instances were extracted and two annotators were trained to annotate discourse relations according to the discourse scheme defined in Section 3. [sent-218, score-1.872]
</p><p>60 Note that we annotate both explicit and implicit discourse relations. [sent-219, score-0.573]
</p><p>61 Table 4 showed the distribution of annotated discourse relations based on the inter-annotator agreement. [sent-223, score-0.648]
</p><p>62 The proportion of occurrences of each discourse relations varied greatly. [sent-224, score-0.648]
</p><p>63 The experiments of this paper were performed using the following data sets: NTC-7 contained manually annotated discourse instances (shown in Table 4). [sent-226, score-0.636]
</p><p>64 The experiments of discourse identification were performed on this data set. [sent-227, score-0.573]
</p><p>65 The experiments of polarity ambiguity elimination using the identified discourse relations were performed on this  data set. [sent-229, score-1.002]
</p><p>66 Given a discourse instance Di, the probabilities: P(rk | (Di[1] , Di[2] )) for each relation rk were estimated|( on all text from XINHUA. [sent-238, score-0.639]
</p><p>67 Then, the most likely discourse relation was determined by taking the maximum over argmaxk{P(rk |(Di[1] , Di[2] )}. [sent-239, score-0.649]
</p><p>68 cSSR used both cue-phrase-based patterns together with common SSRs for recognizing discourse  relations. [sent-240, score-0.659]
</p><p>69 Common SSRs were mined from discourse instances extracted from XINHUA using cuephrase-based patterns. [sent-241, score-0.667]
</p><p>70 SVM was trained utilizing cue phrases, probabilities from M&E;, topic similarity, structure overlap, polarity of segments and mined common SSRs (Optional). [sent-243, score-0.596]
</p><p>71 The nucleus and satellite information is  Figure 2: Influences of different values of minconf to the performance of cSSR acquired by cSSR if a segment pair could match a cSSR. [sent-250, score-0.507]
</p><p>72 (2) Apply discourse constraints on polarity to ascertain the polarity for each discourse instance. [sent-252, score-1.855]
</p><p>73 There may be conflicts between polarities acquired by BPC and discourse constraints on polarity (e. [sent-253, score-0.966]
</p><p>74 , Two segments with the same polarity holding a Contrast relation). [sent-255, score-0.486]
</p><p>75 To handle this problem, we chose the segment with higher polarity confidence and adjusted the polarity of the other segment using discourse constraints on polarity. [sent-256, score-1.439]
</p><p>76 (3) If there was more than one discourse instance in a single sentence, the overall polarity of the sentence was determined by voting of polarities from each discourse instance under the majority rule. [sent-257, score-1.549]
</p><p>77 Table 5 presented the experimental results for discourse relation classification. [sent-274, score-0.619]
</p><p>78 8 VS10M R9231s  Table 6: Performance of integrating discourse classifiers and constraints to polarity classification. [sent-280, score-0.949]
</p><p>79 On the other side, M&E; which only considered word pairs between two segments of discourse instances got a higher recall with a large drop of precision. [sent-285, score-0.713]
</p><p>80 The drop of precision may be caused  by the neglect ofstructural and semantic information of discourse instances. [sent-286, score-0.596]
</p><p>81 Actually, cSSR outperformed Baseline in all discourse relations except for Contrast. [sent-291, score-0.648]
</p><p>82 Table 6 presented the performance of integrating discourse classifiers to polarity classification. [sent-299, score-0.929]
</p><p>83 For Baseline and cSSR, the information of nucleus and satellite could be obtained directly from cue-  Table 5: Performance of recognizing discourse relations. [sent-300, score-0.893]
</p><p>84 For SVM+cSSR, the nucleus and satellite information was acquired by cSSR if a segment pair could match a cSSR. [sent-302, score-0.396]
</p><p>85 It's clear that the performance of polarity classification was enhanced with the improvement of discourse relation recogni-  tion. [sent-304, score-0.981]
</p><p>86 M&E; was not included in this experiment because the performance of polarity classification was decreased by the mis-classified discourse relations. [sent-305, score-0.935]
</p><p>87 It's straightforward that these words were insignificant in discourse relation classification purpose. [sent-325, score-0.646]
</p><p>88 0), the proposed method successfully edo wwonr weighed ,th teh e w porrodpso swehdic mh were  Figure 3: Improvement from individual discourse relations. [sent-337, score-0.573]
</p><p>89 Contribution of different discourse relations  We also analyzed the contribution of different discourse relations in eliminating polarity ambiguities. [sent-343, score-1.669]
</p><p>90 Referto Figure 3, the improvement ofpolarity classification mainly came from three discourse relations: Contrast, Continuation and Cause. [sent-344, score-0.6]
</p><p>91 It was straightforward that Contrast relation could eliminate polarity ambiguities because it held between two segments with opposite polarities. [sent-345, score-0.483]
</p><p>92 However, recall Table 4, although Cause occurred more often than Contrast, only a part of discourse instances holding Cause relation contained two segments with the opposite polarities. [sent-347, score-0.833]
</p><p>93 Consequently, the polarity of the second segment should be negative. [sent-355, score-0.405]
</p><p>94 6  Conclusions and Future work  This paper focused on unsupervised discovery of intra-sentence discourse relations for sentence level polarity classification. [sent-356, score-1.01]
</p><p>95 We firstly presented a discourse scheme based on empirical observations. [sent-357, score-0.607]
</p><p>96 Then, an unsupervised method was proposed starting from a small set of cue-phrase-based patterns to mine high quality common SSRs for each discourse relation. [sent-358, score-0.664]
</p><p>97 Experimental results showed that our methods not only effectively recognized discourse relations but also achieved significant improvement (p<0. [sent-360, score-0.648]
</p><p>98 A novel discourse parser based on support vector machine classification. [sent-407, score-0.573]
</p><p>99 Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification. [sent-495, score-1.057]
</p><p>100 Sentence level discourse parsing using syntactic and lexical information. [sent-502, score-0.573]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('discourse', 0.573), ('ssrs', 0.5), ('polarity', 0.335), ('cssr', 0.222), ('nucleus', 0.153), ('satellite', 0.145), ('ssr', 0.133), ('minconf', 0.111), ('continuation', 0.104), ('cue', 0.1), ('mismatches', 0.088), ('sentiment', 0.082), ('holding', 0.078), ('moat', 0.078), ('ssri', 0.078), ('di', 0.075), ('dj', 0.075), ('relations', 0.075), ('segments', 0.073), ('segment', 0.07), ('bpc', 0.067), ('conf', 0.057), ('weighing', 0.052), ('mined', 0.05), ('rst', 0.048), ('opinion', 0.047), ('relation', 0.046), ('somasundaran', 0.045), ('instances', 0.044), ('sn', 0.042), ('rhetorical', 0.04), ('opinionated', 0.04), ('common', 0.038), ('eliminating', 0.038), ('polarities', 0.038), ('echihabi', 0.036), ('confidence', 0.036), ('chinese', 0.035), ('prp', 0.035), ('scheme', 0.034), ('marcu', 0.034), ('asher', 0.033), ('inproceedings', 0.033), ('denoted', 0.033), ('utilized', 0.032), ('cause', 0.031), ('svm', 0.031), ('weigh', 0.03), ('determined', 0.03), ('ambiguities', 0.029), ('um', 0.029), ('adopted', 0.028), ('match', 0.028), ('calculation', 0.028), ('unsupervised', 0.027), ('classification', 0.027), ('polanyi', 0.026), ('patterns', 0.026), ('sequential', 0.025), ('indicated', 0.023), ('drop', 0.023), ('bos', 0.023), ('wiebe', 0.022), ('simplified', 0.022), ('banned', 0.022), ('boris', 0.022), ('hated', 0.022), ('loved', 0.022), ('ofaverage', 0.022), ('ofdiscourse', 0.022), ('sadamitsu', 0.022), ('ufm', 0.022), ('recognizing', 0.022), ('jj', 0.022), ('hong', 0.021), ('filtering', 0.021), ('integrating', 0.021), ('dropped', 0.02), ('compound', 0.02), ('rk', 0.02), ('maintaining', 0.02), ('constraints', 0.02), ('contrast', 0.02), ('che', 0.019), ('preserved', 0.019), ('frames', 0.019), ('converted', 0.019), ('affected', 0.019), ('critically', 0.019), ('elimination', 0.019), ('duverle', 0.019), ('miltsakaki', 0.019), ('ascertain', 0.019), ('corrupted', 0.019), ('brilliant', 0.019), ('ntcir', 0.019), ('contained', 0.019), ('kong', 0.019), ('nn', 0.019), ('ambiguous', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000011 <a title="142-tfidf-1" href="./emnlp-2011-Unsupervised_Discovery_of_Discourse_Relations_for_Eliminating_Intra-sentence_Polarity_Ambiguities.html">142 emnlp-2011-Unsupervised Discovery of Discourse Relations for Eliminating Intra-sentence Polarity Ambiguities</a></p>
<p>Author: Lanjun Zhou ; Binyang Li ; Wei Gao ; Zhongyu Wei ; Kam-Fai Wong</p><p>Abstract: Polarity classification of opinionated sentences with both positive and negative sentiments1 is a key challenge in sentiment analysis. This paper presents a novel unsupervised method for discovering intra-sentence level discourse relations for eliminating polarity ambiguities. Firstly, a discourse scheme with discourse constraints on polarity was defined empirically based on Rhetorical Structure Theory (RST). Then, a small set of cuephrase-based patterns were utilized to collect a large number of discourse instances which were later converted to semantic sequential representations (SSRs). Finally, an unsupervised method was adopted to generate, weigh and filter new SSRs without cue phrases for recognizing discourse relations. Experimental results showed that the proposed methods not only effectively recognized the defined discourse relations but also achieved significant improvement by integrating discourse information in sentence-level polarity classification.</p><p>2 0.258028 <a title="142-tfidf-2" href="./emnlp-2011-Modelling_Discourse_Relations_for_Arabic.html">94 emnlp-2011-Modelling Discourse Relations for Arabic</a></p>
<p>Author: Amal Al-Saif ; Katja Markert</p><p>Abstract: We present the first algorithms to automatically identify explicit discourse connectives and the relations they signal for Arabic text. First we show that, for Arabic news, most adjacent sentences are connected via explicit connectives in contrast to English, making the treatment of explicit discourse connectives for Arabic highly important. We also show that explicit Arabic discourse connectives are far more ambiguous than English ones, making their treatment challenging. In the second part of the paper, we present supervised algorithms to address automatic discourse connective identification and discourse relation recognition. Our connective identifier based on gold standard syntactic features achieves almost human performance. In addition, an identifier based solely on simple lexical and automatically derived morphological and POS features performs with high reliability, essential for languages that do not have high-quality parsers yet. Our algorithm for recognizing discourse relations performs significantly better than a baseline based on the connective surface string alone and therefore reduces the ambiguity in explicit connective interpretation.</p><p>3 0.20342599 <a title="142-tfidf-3" href="./emnlp-2011-Minimally_Supervised_Event_Causality_Identification.html">92 emnlp-2011-Minimally Supervised Event Causality Identification</a></p>
<p>Author: Quang Do ; Yee Seng Chan ; Dan Roth</p><p>Abstract: This paper develops a minimally supervised approach, based on focused distributional similarity methods and discourse connectives, for identifying of causality relations between events in context. While it has been shown that distributional similarity can help identifying causality, we observe that discourse connectives and the particular discourse relation they evoke in context provide additional information towards determining causality between events. We show that combining discourse relation predictions and distributional similarity methods in a global inference procedure provides additional improvements towards determining event causality.</p><p>4 0.12486804 <a title="142-tfidf-4" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>Author: Amit Dubey ; Frank Keller ; Patrick Sturt</p><p>Abstract: This paper introduces a psycholinguistic model of sentence processing which combines a Hidden Markov Model noun phrase chunker with a co-reference classifier. Both models are fully incremental and generative, giving probabilities of lexical elements conditional upon linguistic structure. This allows us to compute the information theoretic measure of surprisal, which is known to correlate with human processing effort. We evaluate our surprisal predictions on the Dundee corpus of eye-movement data show that our model achieve a better fit with human reading times than a syntax-only model which does not have access to co-reference information.</p><p>5 0.11438113 <a title="142-tfidf-5" href="./emnlp-2011-Compositional_Matrix-Space_Models_for_Sentiment_Analysis.html">30 emnlp-2011-Compositional Matrix-Space Models for Sentiment Analysis</a></p>
<p>Author: Ainur Yessenalina ; Claire Cardie</p><p>Abstract: We present a general learning-based approach for phrase-level sentiment analysis that adopts an ordinal sentiment scale and is explicitly compositional in nature. Thus, we can model the compositional effects required for accurate assignment of phrase-level sentiment. For example, combining an adverb (e.g., “very”) with a positive polar adjective (e.g., “good”) produces a phrase (“very good”) with increased polarity over the adjective alone. Inspired by recent work on distributional approaches to compositionality, we model each word as a matrix and combine words using iterated matrix multiplication, which allows for the modeling of both additive and multiplicative semantic effects. Although the multiplication-based matrix-space framework has been shown to be a theoretically elegant way to model composition (Rudolph and Giesbrecht, 2010), training such models has to be done carefully: the optimization is nonconvex and requires a good initial starting point. This paper presents the first such algorithm for learning a matrix-space model for semantic composition. In the context of the phrase-level sentiment analysis task, our experimental results show statistically significant improvements in performance over a bagof-words model.</p><p>6 0.094828919 <a title="142-tfidf-6" href="./emnlp-2011-Predicting_Thread_Discourse_Structure_over_Technical_Web_Forums.html">105 emnlp-2011-Predicting Thread Discourse Structure over Technical Web Forums</a></p>
<p>7 0.090810485 <a title="142-tfidf-7" href="./emnlp-2011-Semi-Supervised_Recursive_Autoencoders_for_Predicting_Sentiment_Distributions.html">120 emnlp-2011-Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</a></p>
<p>8 0.087198325 <a title="142-tfidf-8" href="./emnlp-2011-Learning_General_Connotation_of_Words_using_Graph-based_Algorithms.html">81 emnlp-2011-Learning General Connotation of Words using Graph-based Algorithms</a></p>
<p>9 0.084741041 <a title="142-tfidf-9" href="./emnlp-2011-Structural_Opinion_Mining_for_Graph-based_Sentiment_Representation.html">126 emnlp-2011-Structural Opinion Mining for Graph-based Sentiment Representation</a></p>
<p>10 0.080246054 <a title="142-tfidf-10" href="./emnlp-2011-Harnessing_WordNet_Senses_for_Supervised_Sentiment_Classification.html">63 emnlp-2011-Harnessing WordNet Senses for Supervised Sentiment Classification</a></p>
<p>11 0.076977462 <a title="142-tfidf-11" href="./emnlp-2011-Learning_the_Information_Status_of_Noun_Phrases_in_Spoken_Dialogues.html">84 emnlp-2011-Learning the Information Status of Noun Phrases in Spoken Dialogues</a></p>
<p>12 0.074631304 <a title="142-tfidf-12" href="./emnlp-2011-Cooooooooooooooollllllllllllll%21%21%21%21%21%21%21%21%21%21%21%21%21%21_Using_Word_Lengthening_to_Detect_Sentiment_in_Microblogs.html">33 emnlp-2011-Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using Word Lengthening to Detect Sentiment in Microblogs</a></p>
<p>13 0.051367719 <a title="142-tfidf-13" href="./emnlp-2011-A_Weakly-supervised_Approach_to_Argumentative_Zoning_of_Scientific_Documents.html">12 emnlp-2011-A Weakly-supervised Approach to Argumentative Zoning of Scientific Documents</a></p>
<p>14 0.048627079 <a title="142-tfidf-14" href="./emnlp-2011-Generating_Subsequent_Reference_in_Shared_Visual_Scenes%3A_Computation_vs_Re-Use.html">62 emnlp-2011-Generating Subsequent Reference in Shared Visual Scenes: Computation vs Re-Use</a></p>
<p>15 0.046954475 <a title="142-tfidf-15" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>16 0.046383858 <a title="142-tfidf-16" href="./emnlp-2011-Data-Driven_Response_Generation_in_Social_Media.html">38 emnlp-2011-Data-Driven Response Generation in Social Media</a></p>
<p>17 0.042383093 <a title="142-tfidf-17" href="./emnlp-2011-A_generative_model_for_unsupervised_discovery_of_relations_and_argument_classes_from_clinical_texts.html">14 emnlp-2011-A generative model for unsupervised discovery of relations and argument classes from clinical texts</a></p>
<p>18 0.041203734 <a title="142-tfidf-18" href="./emnlp-2011-Relation_Acquisition_using_Word_Classes_and_Partial_Patterns.html">113 emnlp-2011-Relation Acquisition using Word Classes and Partial Patterns</a></p>
<p>19 0.040664166 <a title="142-tfidf-19" href="./emnlp-2011-Relation_Extraction_with_Relation_Topics.html">114 emnlp-2011-Relation Extraction with Relation Topics</a></p>
<p>20 0.040252328 <a title="142-tfidf-20" href="./emnlp-2011-Active_Learning_with_Amazon_Mechanical_Turk.html">17 emnlp-2011-Active Learning with Amazon Mechanical Turk</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.132), (1, -0.14), (2, -0.046), (3, 0.075), (4, 0.189), (5, -0.184), (6, 0.137), (7, 0.144), (8, -0.116), (9, -0.431), (10, -0.149), (11, 0.002), (12, 0.047), (13, 0.131), (14, 0.044), (15, -0.132), (16, 0.085), (17, 0.062), (18, 0.057), (19, -0.003), (20, 0.015), (21, 0.008), (22, 0.06), (23, 0.079), (24, 0.01), (25, -0.051), (26, -0.054), (27, -0.011), (28, -0.036), (29, 0.027), (30, 0.032), (31, -0.076), (32, 0.056), (33, 0.024), (34, 0.03), (35, -0.054), (36, 0.0), (37, -0.034), (38, -0.008), (39, 0.044), (40, 0.026), (41, -0.016), (42, -0.01), (43, -0.05), (44, 0.012), (45, -0.021), (46, 0.061), (47, 0.001), (48, -0.047), (49, -0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97292829 <a title="142-lsi-1" href="./emnlp-2011-Unsupervised_Discovery_of_Discourse_Relations_for_Eliminating_Intra-sentence_Polarity_Ambiguities.html">142 emnlp-2011-Unsupervised Discovery of Discourse Relations for Eliminating Intra-sentence Polarity Ambiguities</a></p>
<p>Author: Lanjun Zhou ; Binyang Li ; Wei Gao ; Zhongyu Wei ; Kam-Fai Wong</p><p>Abstract: Polarity classification of opinionated sentences with both positive and negative sentiments1 is a key challenge in sentiment analysis. This paper presents a novel unsupervised method for discovering intra-sentence level discourse relations for eliminating polarity ambiguities. Firstly, a discourse scheme with discourse constraints on polarity was defined empirically based on Rhetorical Structure Theory (RST). Then, a small set of cuephrase-based patterns were utilized to collect a large number of discourse instances which were later converted to semantic sequential representations (SSRs). Finally, an unsupervised method was adopted to generate, weigh and filter new SSRs without cue phrases for recognizing discourse relations. Experimental results showed that the proposed methods not only effectively recognized the defined discourse relations but also achieved significant improvement by integrating discourse information in sentence-level polarity classification.</p><p>2 0.89845365 <a title="142-lsi-2" href="./emnlp-2011-Modelling_Discourse_Relations_for_Arabic.html">94 emnlp-2011-Modelling Discourse Relations for Arabic</a></p>
<p>Author: Amal Al-Saif ; Katja Markert</p><p>Abstract: We present the first algorithms to automatically identify explicit discourse connectives and the relations they signal for Arabic text. First we show that, for Arabic news, most adjacent sentences are connected via explicit connectives in contrast to English, making the treatment of explicit discourse connectives for Arabic highly important. We also show that explicit Arabic discourse connectives are far more ambiguous than English ones, making their treatment challenging. In the second part of the paper, we present supervised algorithms to address automatic discourse connective identification and discourse relation recognition. Our connective identifier based on gold standard syntactic features achieves almost human performance. In addition, an identifier based solely on simple lexical and automatically derived morphological and POS features performs with high reliability, essential for languages that do not have high-quality parsers yet. Our algorithm for recognizing discourse relations performs significantly better than a baseline based on the connective surface string alone and therefore reduces the ambiguity in explicit connective interpretation.</p><p>3 0.77283889 <a title="142-lsi-3" href="./emnlp-2011-Minimally_Supervised_Event_Causality_Identification.html">92 emnlp-2011-Minimally Supervised Event Causality Identification</a></p>
<p>Author: Quang Do ; Yee Seng Chan ; Dan Roth</p><p>Abstract: This paper develops a minimally supervised approach, based on focused distributional similarity methods and discourse connectives, for identifying of causality relations between events in context. While it has been shown that distributional similarity can help identifying causality, we observe that discourse connectives and the particular discourse relation they evoke in context provide additional information towards determining causality between events. We show that combining discourse relation predictions and distributional similarity methods in a global inference procedure provides additional improvements towards determining event causality.</p><p>4 0.46747202 <a title="142-lsi-4" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>Author: Amit Dubey ; Frank Keller ; Patrick Sturt</p><p>Abstract: This paper introduces a psycholinguistic model of sentence processing which combines a Hidden Markov Model noun phrase chunker with a co-reference classifier. Both models are fully incremental and generative, giving probabilities of lexical elements conditional upon linguistic structure. This allows us to compute the information theoretic measure of surprisal, which is known to correlate with human processing effort. We evaluate our surprisal predictions on the Dundee corpus of eye-movement data show that our model achieve a better fit with human reading times than a syntax-only model which does not have access to co-reference information.</p><p>5 0.32694134 <a title="142-lsi-5" href="./emnlp-2011-Learning_the_Information_Status_of_Noun_Phrases_in_Spoken_Dialogues.html">84 emnlp-2011-Learning the Information Status of Noun Phrases in Spoken Dialogues</a></p>
<p>Author: Altaf Rahman ; Vincent Ng</p><p>Abstract: An entity in a dialogue may be old, new, or mediated/inferrable with respect to the hearer’s beliefs. Knowing the information status of the entities participating in a dialogue can therefore facilitate its interpretation. We address the under-investigated problem of automatically determining the information status of discourse entities. Specifically, we extend Nissim’s (2006) machine learning approach to information-status determination with lexical and structured features, and exploit learned knowledge of the information status of each discourse entity for coreference resolution. Experimental results on a set of Switchboard dialogues reveal that (1) incorporating our proposed features into Nissim’s feature set enables our system to achieve stateof-the-art performance on information-status classification, and (2) the resulting information can be used to improve the performance of learning-based coreference resolvers.</p><p>6 0.32313457 <a title="142-lsi-6" href="./emnlp-2011-Learning_General_Connotation_of_Words_using_Graph-based_Algorithms.html">81 emnlp-2011-Learning General Connotation of Words using Graph-based Algorithms</a></p>
<p>7 0.3219921 <a title="142-lsi-7" href="./emnlp-2011-Predicting_Thread_Discourse_Structure_over_Technical_Web_Forums.html">105 emnlp-2011-Predicting Thread Discourse Structure over Technical Web Forums</a></p>
<p>8 0.29425466 <a title="142-lsi-8" href="./emnlp-2011-Compositional_Matrix-Space_Models_for_Sentiment_Analysis.html">30 emnlp-2011-Compositional Matrix-Space Models for Sentiment Analysis</a></p>
<p>9 0.27666649 <a title="142-lsi-9" href="./emnlp-2011-Cooooooooooooooollllllllllllll%21%21%21%21%21%21%21%21%21%21%21%21%21%21_Using_Word_Lengthening_to_Detect_Sentiment_in_Microblogs.html">33 emnlp-2011-Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using Word Lengthening to Detect Sentiment in Microblogs</a></p>
<p>10 0.27540237 <a title="142-lsi-10" href="./emnlp-2011-Structural_Opinion_Mining_for_Graph-based_Sentiment_Representation.html">126 emnlp-2011-Structural Opinion Mining for Graph-based Sentiment Representation</a></p>
<p>11 0.27183521 <a title="142-lsi-11" href="./emnlp-2011-Semi-Supervised_Recursive_Autoencoders_for_Predicting_Sentiment_Distributions.html">120 emnlp-2011-Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</a></p>
<p>12 0.25471863 <a title="142-lsi-12" href="./emnlp-2011-Harnessing_WordNet_Senses_for_Supervised_Sentiment_Classification.html">63 emnlp-2011-Harnessing WordNet Senses for Supervised Sentiment Classification</a></p>
<p>13 0.21218632 <a title="142-lsi-13" href="./emnlp-2011-A_Weakly-supervised_Approach_to_Argumentative_Zoning_of_Scientific_Documents.html">12 emnlp-2011-A Weakly-supervised Approach to Argumentative Zoning of Scientific Documents</a></p>
<p>14 0.15778579 <a title="142-lsi-14" href="./emnlp-2011-Generating_Subsequent_Reference_in_Shared_Visual_Scenes%3A_Computation_vs_Re-Use.html">62 emnlp-2011-Generating Subsequent Reference in Shared Visual Scenes: Computation vs Re-Use</a></p>
<p>15 0.15738042 <a title="142-lsi-15" href="./emnlp-2011-Relation_Acquisition_using_Word_Classes_and_Partial_Patterns.html">113 emnlp-2011-Relation Acquisition using Word Classes and Partial Patterns</a></p>
<p>16 0.1525328 <a title="142-lsi-16" href="./emnlp-2011-Data-Driven_Response_Generation_in_Social_Media.html">38 emnlp-2011-Data-Driven Response Generation in Social Media</a></p>
<p>17 0.14449856 <a title="142-lsi-17" href="./emnlp-2011-Active_Learning_with_Amazon_Mechanical_Turk.html">17 emnlp-2011-Active Learning with Amazon Mechanical Turk</a></p>
<p>18 0.13350722 <a title="142-lsi-18" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>19 0.12913549 <a title="142-lsi-19" href="./emnlp-2011-Relation_Extraction_with_Relation_Topics.html">114 emnlp-2011-Relation Extraction with Relation Topics</a></p>
<p>20 0.1290894 <a title="142-lsi-20" href="./emnlp-2011-Identifying_Relations_for_Open_Information_Extraction.html">70 emnlp-2011-Identifying Relations for Open Information Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(15, 0.019), (23, 0.1), (36, 0.013), (37, 0.031), (45, 0.049), (48, 0.382), (53, 0.019), (54, 0.017), (57, 0.017), (62, 0.027), (64, 0.017), (66, 0.04), (69, 0.023), (79, 0.034), (82, 0.011), (87, 0.012), (96, 0.047), (97, 0.016), (98, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75320053 <a title="142-lda-1" href="./emnlp-2011-Unsupervised_Discovery_of_Discourse_Relations_for_Eliminating_Intra-sentence_Polarity_Ambiguities.html">142 emnlp-2011-Unsupervised Discovery of Discourse Relations for Eliminating Intra-sentence Polarity Ambiguities</a></p>
<p>Author: Lanjun Zhou ; Binyang Li ; Wei Gao ; Zhongyu Wei ; Kam-Fai Wong</p><p>Abstract: Polarity classification of opinionated sentences with both positive and negative sentiments1 is a key challenge in sentiment analysis. This paper presents a novel unsupervised method for discovering intra-sentence level discourse relations for eliminating polarity ambiguities. Firstly, a discourse scheme with discourse constraints on polarity was defined empirically based on Rhetorical Structure Theory (RST). Then, a small set of cuephrase-based patterns were utilized to collect a large number of discourse instances which were later converted to semantic sequential representations (SSRs). Finally, an unsupervised method was adopted to generate, weigh and filter new SSRs without cue phrases for recognizing discourse relations. Experimental results showed that the proposed methods not only effectively recognized the defined discourse relations but also achieved significant improvement by integrating discourse information in sentence-level polarity classification.</p><p>2 0.59693396 <a title="142-lda-2" href="./emnlp-2011-Harnessing_WordNet_Senses_for_Supervised_Sentiment_Classification.html">63 emnlp-2011-Harnessing WordNet Senses for Supervised Sentiment Classification</a></p>
<p>Author: Balamurali AR ; Aditya Joshi ; Pushpak Bhattacharyya</p><p>Abstract: Traditional approaches to sentiment classification rely on lexical features, syntax-based features or a combination of the two. We propose semantic features using word senses for a supervised document-level sentiment classifier. To highlight the benefit of sense-based features, we compare word-based representation of documents with a sense-based representation where WordNet senses of the words are used as features. In addition, we highlight the benefit of senses by presenting a part-ofspeech-wise effect on sentiment classification. Finally, we show that even if a WSD engine disambiguates between a limited set of words in a document, a sentiment classifier still performs better than what it does in absence of sense annotation. Since word senses used as features show promise, we also examine the possibility of using similarity metrics defined on WordNet to address the problem of not finding a sense in the training corpus. We per- form experiments using three popular similarity metrics to mitigate the effect of unknown synsets in a test corpus by replacing them with similar synsets from the training corpus. The results show promising improvement with respect to the baseline.</p><p>3 0.53482068 <a title="142-lda-3" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>Author: Amit Dubey ; Frank Keller ; Patrick Sturt</p><p>Abstract: This paper introduces a psycholinguistic model of sentence processing which combines a Hidden Markov Model noun phrase chunker with a co-reference classifier. Both models are fully incremental and generative, giving probabilities of lexical elements conditional upon linguistic structure. This allows us to compute the information theoretic measure of surprisal, which is known to correlate with human processing effort. We evaluate our surprisal predictions on the Dundee corpus of eye-movement data show that our model achieve a better fit with human reading times than a syntax-only model which does not have access to co-reference information.</p><p>4 0.36741674 <a title="142-lda-4" href="./emnlp-2011-Modelling_Discourse_Relations_for_Arabic.html">94 emnlp-2011-Modelling Discourse Relations for Arabic</a></p>
<p>Author: Amal Al-Saif ; Katja Markert</p><p>Abstract: We present the first algorithms to automatically identify explicit discourse connectives and the relations they signal for Arabic text. First we show that, for Arabic news, most adjacent sentences are connected via explicit connectives in contrast to English, making the treatment of explicit discourse connectives for Arabic highly important. We also show that explicit Arabic discourse connectives are far more ambiguous than English ones, making their treatment challenging. In the second part of the paper, we present supervised algorithms to address automatic discourse connective identification and discourse relation recognition. Our connective identifier based on gold standard syntactic features achieves almost human performance. In addition, an identifier based solely on simple lexical and automatically derived morphological and POS features performs with high reliability, essential for languages that do not have high-quality parsers yet. Our algorithm for recognizing discourse relations performs significantly better than a baseline based on the connective surface string alone and therefore reduces the ambiguity in explicit connective interpretation.</p><p>5 0.34677115 <a title="142-lda-5" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Noah A. Smith</p><p>Abstract: We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.</p><p>6 0.34168518 <a title="142-lda-6" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>7 0.33690605 <a title="142-lda-7" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>8 0.33579785 <a title="142-lda-8" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<p>9 0.33578691 <a title="142-lda-9" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>10 0.33576691 <a title="142-lda-10" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>11 0.33511263 <a title="142-lda-11" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>12 0.33320943 <a title="142-lda-12" href="./emnlp-2011-Named_Entity_Recognition_in_Tweets%3A_An_Experimental_Study.html">98 emnlp-2011-Named Entity Recognition in Tweets: An Experimental Study</a></p>
<p>13 0.33245996 <a title="142-lda-13" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>14 0.33188692 <a title="142-lda-14" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>15 0.33161664 <a title="142-lda-15" href="./emnlp-2011-Semi-Supervised_Recursive_Autoencoders_for_Predicting_Sentiment_Distributions.html">120 emnlp-2011-Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</a></p>
<p>16 0.33155096 <a title="142-lda-16" href="./emnlp-2011-Lateen_EM%3A_Unsupervised_Training_with_Multiple_Objectives%2C_Applied_to_Dependency_Grammar_Induction.html">79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</a></p>
<p>17 0.33131319 <a title="142-lda-17" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>18 0.33090472 <a title="142-lda-18" href="./emnlp-2011-Structural_Opinion_Mining_for_Graph-based_Sentiment_Representation.html">126 emnlp-2011-Structural Opinion Mining for Graph-based Sentiment Representation</a></p>
<p>19 0.32935253 <a title="142-lda-19" href="./emnlp-2011-Learning_General_Connotation_of_Words_using_Graph-based_Algorithms.html">81 emnlp-2011-Learning General Connotation of Words using Graph-based Algorithms</a></p>
<p>20 0.32935229 <a title="142-lda-20" href="./emnlp-2011-Compositional_Matrix-Space_Models_for_Sentiment_Analysis.html">30 emnlp-2011-Compositional Matrix-Space Models for Sentiment Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
