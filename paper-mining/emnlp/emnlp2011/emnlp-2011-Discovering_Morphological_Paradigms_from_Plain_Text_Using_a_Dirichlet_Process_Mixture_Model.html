<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>39 emnlp-2011-Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-39" href="#">emnlp2011-39</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>39 emnlp-2011-Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model</h1>
<br/><p>Source: <a title="emnlp-2011-39-pdf" href="http://aclweb.org/anthology//D/D11/D11-1057.pdf">pdf</a></p><p>Author: Markus Dreyer ; Jason Eisner</p><p>Abstract: We present an inference algorithm that organizes observed words (tokens) into structured inflectional paradigms (types). It also naturally predicts the spelling of unobserved forms that are missing from these paradigms, and discovers inflectional principles (grammar) that generalize to wholly unobserved words. Our Bayesian generative model of the data explicitly represents tokens, types, inflections, paradigms, and locally conditioned string edits. It assumes that inflected word tokens are generated from an infinite mixture of inflectional paradigms (string tuples). Each paradigm is sampled all at once from a graphical model, whose potential functions are weighted finitestate transducers with language-specific parameters to be learned. These assumptions naturally lead to an elegant empirical Bayes inference procedure that exploits Monte Carlo EM, belief propagation, and dynamic programming. Given 50–100 seed paradigms, adding a 10million-word corpus reduces prediction error for morphological inflections by up to 10%.</p><p>Reference: <a title="emnlp-2011-39-reference" href="../emnlp2011_reference/emnlp-2011-Discovering_Morphological_Paradigms_from_Plain_Text_Using_a_Dirichlet_Process_Mixture_Model_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 mdreyer@ sdl com Abstract We present an inference algorithm that organizes observed words (tokens) into structured inflectional paradigms (types). [sent-2, score-0.673]
</p><p>2 It also naturally predicts the spelling of unobserved forms that are missing from these paradigms, and discovers inflectional principles (grammar) that generalize to wholly unobserved words. [sent-3, score-0.411]
</p><p>3 It assumes that inflected word tokens are generated from an infinite mixture of inflectional paradigms (string tuples). [sent-5, score-0.895]
</p><p>4 Given 50–100 seed paradigms, adding a 10million-word corpus reduces prediction error for morphological inflections by up to 10%. [sent-8, score-0.424]
</p><p>5 That is, a single lexeme may be realized in a corpus as many different word types, which are differently inflected for person, number, gender, tense, mood, etc. [sent-13, score-0.473]
</p><p>6 A token ofbroken might be tagged as (1) a VERB and more specifically as (2) the past participle inflection of (3) the abstract lexeme Reconstructing the latent lexemes and inflections allows the features of other statistical models to consider them. [sent-32, score-1.151]
</p><p>7 2We use cursive font for abstract lexemes to emphasize that they are atomic objects that do not decompose into letters. [sent-37, score-0.257]
</p><p>8 Types: In carrying out the above, we will reconstruct specific morphological paradigms of the language. [sent-41, score-0.662]
</p><p>9 A paradigm is a grid of all the inflected forms of some lexeme, as illustrated in Table 1. [sent-42, score-0.328]
</p><p>10 Our reconstructed paradigms will include our predictions of inflected forms that were never observed in the corpus. [sent-43, score-0.673]
</p><p>11 We learn a prior distribution over inflectional paradigms by learning (e. [sent-46, score-0.694]
</p><p>12 Our basic strategy is Monte Carlo EM, so these parameters tell us how to guess the paradigms (Monte Carlo E step), then these reconstructed paradigms tell us how to reestimate the parameters (M step), and so on iteratively. [sent-52, score-1.027]
</p><p>13 We use a few supervised paradigms to initialize the parameters and help reestimate them. [sent-53, score-0.498]
</p><p>14 2 Modeling Morphological Paradigms A paradigm such as Table 1 describes how some abstract lexeme (•b&r; …a†k)is expressed in German. [sent-71, score-0.525]
</p><p>15 4 We evaluate whole paradigms as linguistic objects, following word-and-paradigm or realizational morphology (Matthews, 1972; Stump, 2001). [sent-72, score-0.599]
</p><p>16 That is, we presume that some language-specific distribution p(π) defines whether a paradigm π is a grammatical—and  a priori likely—way for a lexeme to express itself in the language. [sent-73, score-0.576]
</p><p>17 Figure 1: A distribution over paradigms modeled as an MRF over 7 strings. [sent-96, score-0.549]
</p><p>18 1 illustrates the result of belief propagation on a simple MRF whose factors relate all inflected forms to a common (possibly unobserved) lemma, but not directly to one Our method could be used with any p(π). [sent-105, score-0.274]
</p><p>19 3 Modeling the Lexicon (types) Dreyer and Eisner (2009) learned partially observing some paradigms (type data). [sent-110, score-0.498]
</p><p>20 That work, while rather accurate at predicting inflected forms, sometimes erred: it predicted spellings that never occurred in text, even for forms that “should” be common. [sent-111, score-0.376]
</p><p>21 We therefore need a model for generating tokens— a probabilistic lexicon that specifies which inflections of which lexemes are common, and how they are spelled. [sent-113, score-0.545]
</p><p>22 Choose parameters This defines p(π) : which paradigms are likely a priori. [sent-116, score-0.498]
</p><p>23 For each lexeme, choose a paradigm that will be used to express the lexeme orthographically. [sent-125, score-0.525]
</p><p>24 This chooses a countable number of lexemes to have positive probability in the language, and decides which ones are most common. [sent-129, score-0.333]
</p><p>25 For the lexeme •t†h’i„n†k, this might choose to make 1stperson singular more common than for typical verbs. [sent-131, score-0.401]
</p><p>26 ) VERB paradigms is languagespecific; we currently assume it is given by a linguist, along with a few supervised VERB paradigms. [sent-136, score-0.498]
</p><p>27 For example, a token of brechen may have been chosen by choosing frequent lexeme •b&r; …a†kfrom the VERB lexicon; then choosing 1st-person plural given •b&r; …a†k;and finally looking up that inflection’s spelling in •b&r; …a†k’sparadigm. [sent-140, score-0.648]
</p><p>28 Gradually, similar tokens get “clustered” into paradigms (section 4). [sent-148, score-0.557]
</p><p>29 Each corpus token ihas been tagged with part of speech ti, lexeme ‘i and inflection si. [sent-156, score-0.651]
</p><p>30 , which locked the corresponding type spelling in the paradigm to the spelling w1 = bri cht ; similarly for and Now w7 is about to be reanalyzed. [sent-158, score-0.399]
</p><p>31 First, because of the current analyses of and the 3rd-person spellings of •b&r; …a†kare already con-  ˆ  strained to match w1 and w3 (the “lock” icon). [sent-161, score-0.262]
</p><p>32 In short, given the other analyses, we know which inflected lexemes in the lexicon are likely, and how likely each one is to be spelled as brechen. [sent-169, score-0.475]
</p><p>33 This lets us compute the relative probabilities of the possible analyses of token so that the Gibbs sampler can accordingly choose one of these analyses at random. [sent-170, score-0.299]
</p><p>34 For example, having tagged w5 =  θ~estimate,  θ~  ‚„  619 springt as s5 = 2nd-person plural may strengthen our estimated probability that 2nd-person spellings tend to end in -t. [sent-174, score-0.311]
</p><p>35 3 Collapsed Representation of the Lexicon The lexicon is collapsed out of our sampler, in the sense that we do not represent a single guess about the infinitely many lexeme probabilities and paradigms. [sent-181, score-0.516]
</p><p>36 2 names its lexemes as •b&r; …a†kand •j’u„m’p for expository purposes, but of course the sampler cannot reconstruct such labels. [sent-185, score-0.412]
</p><p>37 Formally, these labels are collapsed out, and we represent lexemes as anonymous objects. [sent-186, score-0.294]
</p><p>38 Tokens and are tagged with the same anonymous lexeme (which will correspond to sitting  at the same table in a Chinese restaurant process). [sent-187, score-0.494]
</p><p>39 For each lexeme ‘ and inflection s, we maintain pointers to any tokens currently tagged with the slot (‘, s). [sent-188, score-0.681]
</p><p>40 But if some spellings in ‘’s paradigm are known, store a truncated distribution that enumerates the 25 most likely spellings for (‘, s), according to loopy belief propagation within the paradigm. [sent-193, score-0.705]
</p><p>41 All such ‘ share the same marginal distribution over spellings of (‘, s) : the marginal of the prior p(π). [sent-196, score-0.252]
</p><p>42 7This character trigram model is fast to build if p(π) is deA hash table based on cases 1 and 2 can now be used to rapidly map any word w to a list of slots of existing lexemes that might plausibly have generated w. [sent-201, score-0.29]
</p><p>43 If it chooses the “novel lexeme” option, we create an arbitrary new lexeme object in memory. [sent-204, score-0.418]
</p><p>44 The number of explicitly represented lexemes is always finite (at most the number of corpus tokens). [sent-205, score-0.294]
</p><p>45 After all, each word token was obtained by randomly sampling a weighted paradigm (i. [sent-208, score-0.266]
</p><p>46 Just as each Gaussian in a Gaussian mixture is  a distribution over all points Rn, each weighted paradigm is a distribution over all spellings Σ∗ (but assigns probability > 0 to only a finite subset of Σ∗). [sent-211, score-0.572]
</p><p>47 A language might have one probable lexeme from whose paradigm all these words were sampled. [sent-215, score-0.525]
</p><p>48 It is much less likely to have several probable lexemes that all coincidentally chose spellings that started with dis combobul at -. [sent-216, score-0.487]
</p><p>49 Of course, our model is sensitive to more than shared prefixes, and it does not merely cluster words into a paradigm but assigns them to particular inflectional slots in the paradigm. [sent-219, score-0.331]
</p><p>50 If not, one could still try belief propagation; or one could approximate by estimating a language model from the spellings associated with slot s by cases 1and 2. [sent-222, score-0.303]
</p><p>51 This avoids placing a prior bound on the number of lexemes or paradigms in the language. [sent-225, score-0.755]
</p><p>52 We assume that a natural language has an infinite lexicon, although most lexemes have sufficiently low probability that they have not been used in our training corpus or even in human history (yet). [sent-226, score-0.33]
</p><p>53 3 instead broke this into two steps: it first generated a distribution over countably many lexemes (step 2), and then generated a weighted paradigm for each lexeme (steps 3–4). [sent-231, score-0.87]
</p><p>54 This construction keeps distinct lexemes separate even if they happen to have identical paradigms (polysemy). [sent-232, score-0.755]
</p><p>55 A linguist also specifies features of the inflections: the grid layout in Table 1 shows that 4 of the 12 inflections in SVERB share the “2nd-person” feature. [sent-244, score-0.274]
</p><p>56 8We denote inflections by s because they represent “slots” in paradigms (or, in the metaphor of section 6. [sent-245, score-0.71]
</p><p>57 A paradigm for t ∈ T is a mapping π : St → Σ∗, specifying a spelling f∈or T each inflection iπn St. [sent-248, score-0.402]
</p><p>58 A lexeme ‘ is an abstract element of some lexical space L. [sent-250, score-0.372]
</p><p>59 Lexemes have no internal semantic structure: thLe only question we can ask about a lexeme is whether it is equal to some other lexeme. [sent-251, score-0.372]
</p><p>60 There is no upper bound on how many lexemes can be discovered in a text corpus; L is infinite. [sent-252, score-0.257]
</p><p>61 For each of the infinitely many lexemes ‘ ∈ L, and each t ∈ T, the paradigm πt,‘ is‘ a ∈fu Lnction St → ∈Σ∗. [sent-273, score-0.41]
</p><p>62 3), Gt(‘) denotes tag t’s distribution over lexemes ‘ ∈ L, while Ht,‘(s) denotes the tagged lexeme (t, ‘) ’∈s dListribution over inflections s ∈ St. [sent-279, score-0.979]
</p><p>63 2 Paradigms p(πt,‘ | For each t ∈ T, let Dt(π) denote the distribution over paradigms that was presented in section 2. [sent-293, score-0.549]
</p><p>64 Dt is fully specified by our graphical model for paradigms of part of speech t, together with its parameters generated above. [sent-295, score-0.539]
</p><p>65 It considers spellings: DVERB describes what verb paradigms typically look like in the language (e. [sent-297, score-0.541]
</p><p>66 θ~ as  θ~may  For each possible tagged lexeme (t, ‘), we now draw a paradigm πt,‘ from Dt. [sent-302, score-0.572]
</p><p>67 Most of these lexemes will end up having probability 0 in the language. [sent-303, score-0.287]
</p><p>68 If αt is small, then Gt will tend to have the property that most of its probability mass falls on relatively few of the lexemes in Lt {‘ ∈ L : Gt(‘) > 0}. [sent-309, score-0.287]
</p><p>69 For G to be a uniform distribution over an infinite lexeme set L, we need L to be uncountable. [sent-311, score-0.466]
</p><p>70 SLo each lexeme ‘ ∈ L is selected by Lat most one tag t. [sent-313, score-0.412]
</p><p>71 =def  each  lexeme  ‘  9For example, L =def [0, 1], so that•b&r; …a†kismerely a suggestive nickname for a lexeme such as 0. [sent-314, score-0.744]
</p><p>72 4 Inflectional Distributions p(Ht,‘ | αt0) For each tagged lexeme (t, ‘), the language specifies some distribution Ht,‘ over its inflections. [sent-321, score-0.47]
</p><p>73 But in general, to discover tags and inflections simultaneously, we can suppose that the tag sequence t~(and its length n) are generated by a Markov model, with tag bigram or trigram probabilities specified by some parameters τ. [sent-336, score-0.292]
</p><p>74 A lexeme token depends on  its tag: draw ‘i from Gti, so p(‘i | Gti ) = Gti (‘i). [sent-340, score-0.449]
</p><p>75 7 Inflections p(si | Hti,‘i) An inflection slot depends on its tagged lexeme: we draw si from Hti,‘i , so p(si | Hti,‘i ) = Hti,‘i (si). [sent-342, score-0.337]
</p><p>76 11 Given the tag, lexeme, and inflection at position i, we generate the word wi simply by looking up its spelling in the appropriate paradigm. [sent-345, score-0.287]
</p><p>77 Metaphorically, each table) ‘} |i n+ C αhinese restaurant t has a fixed, finite set of seats corresponding to the inflections s ∈ St. [sent-363, score-0.381]
</p><p>78 When customer ichooses to sit at table ‘i, she also chooses a seat si at that table (see Fig. [sent-365, score-0.27]
</p><p>79 2), choosing either an already occupied seat with probability proportional to the number of customers already in that seat, or else a random seat (sampled from Hti and not necessarily empty) with probability proportional to αt0i . [sent-366, score-0.402]
</p><p>80 9) that a new customer in restaurant ti chooses table ‘i, given the other customers in that restaurant (and αti)13 • the probability (from section 6. [sent-374, score-0.337]
</p><p>81 9) that a new customer at table ‘i chooses seat si, given the other customers at that table (and and αt0i)13 • the probability (from section 3. [sent-375, score-0.287]
</p><p>82 We assume that our supervised list of observed paradigms was generated by sampling from  Gt. [sent-383, score-0.534]
</p><p>83 12Actually, to improve mixing time, we choose a currently active lexeme ‘ uniformly at random, make all customers {i : ‘i = ‘} invisible, and sequentially guess where they are sitting. [sent-388, score-0.477]
</p><p>84 14Implying that they are assigned to lexemes with nonnegligible probability. [sent-390, score-0.257]
</p><p>85 Best numbers per set of seed paradigms in bold (statistically significant on our large test set under a paired permutation test, p < 0. [sent-408, score-0.601]
</p><p>86 For seed and test paradigms we used verbal inflectional paradigms from the CELEX morphological database (Baayen et al. [sent-415, score-1.353]
</p><p>87 Each split sampled 100 paradigms as seed data and used the remaining 5,415 paradigms for evaluation. [sent-424, score-1.128]
</p><p>88 16 100 further paradigms were held out for future use. [sent-430, score-0.498]
</p><p>89 17Since these seed paradigms are sampled uniformly from a set of CELEX paradigms, most of them are regular. [sent-431, score-0.63]
</p><p>90 Adding a corpus of 10 million words to these 100 paradigms reduces the error rate by 8. [sent-442, score-0.529]
</p><p>91 18 We also tested a baseline that simply inflects each morphological form according to the basic regular German inflection pattern; this reaches an accuracy of only 84. [sent-449, score-0.264]
</p><p>92 All methods do best on the huge number of 18Considering the 63,778 distinct spellings from all of our 5,615 CELEX paradigms, we find that the smaller corpus contains 7,376 spellings and the 10 larger corpus contains 13,572. [sent-458, score-0.402]
</p><p>93 Note that, by using a corpus, we even improve our prediction accuracy for forms and spellings that are not found in the corpus, i. [sent-478, score-0.275]
</p><p>94 (a) We find all paradigms that consist of novel spellings only, i. [sent-484, score-0.699]
</p><p>95 While the integrated graphical models over strings (Dreyer and Eisner, 2009) can learn some basic morphology from the seed paradigms, the added corpus plays an important role in correcting its mistakes, especially for the more frequent, irregular verb forms. [sent-506, score-0.356]
</p><p>96 Chan (2006) learns sets of morphologically related words; he calls these sets paradigms but notes that they are not substructured entities, in contrast to the paradigms we model in this paper. [sent-519, score-1.025]
</p><p>97 We ran our sampler over a large corpus (10 million words), inferring everything jointly and reducing the prediction error for morphological inflections by up to 10%. [sent-535, score-0.452]
</p><p>98 While we focus here on inflectional morphology and evaluate the results in isolation, we regard the present work as a significant step toward a larger generative model under which Bayesian inference would reconstruct other relationships as well (e. [sent-539, score-0.331]
</p><p>99 Learning probabilistic paradigms for morphology in a latent class model. [sent-599, score-0.599]
</p><p>100 ParaMor: Minimally supervised induction of paradigm structure and morphological analysis. [sent-732, score-0.262]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('paradigms', 0.498), ('lexeme', 0.372), ('lexemes', 0.257), ('inflections', 0.212), ('spellings', 0.201), ('inflection', 0.155), ('paradigm', 0.153), ('inflectional', 0.145), ('appendix', 0.125), ('dreyer', 0.123), ('morphological', 0.109), ('seed', 0.103), ('inflected', 0.101), ('morphology', 0.101), ('sampler', 0.1), ('seat', 0.1), ('spelling', 0.094), ('si', 0.087), ('token', 0.077), ('lexicon', 0.076), ('restaurant', 0.075), ('gt', 0.075), ('forms', 0.074), ('customers', 0.074), ('brechen', 0.072), ('celex', 0.072), ('gti', 0.072), ('nonconcatenative', 0.072), ('irregular', 0.068), ('monte', 0.067), ('carlo', 0.062), ('ht', 0.062), ('bins', 0.062), ('hti', 0.062), ('analyses', 0.061), ('tokens', 0.059), ('concatenative', 0.057), ('seats', 0.057), ('lemma', 0.056), ('reconstruct', 0.055), ('gibbs', 0.055), ('belief', 0.054), ('dirichlet', 0.051), ('distribution', 0.051), ('unobserved', 0.049), ('mixture', 0.049), ('slot', 0.048), ('tagged', 0.047), ('chooses', 0.046), ('propagation', 0.045), ('bayesian', 0.044), ('infinite', 0.043), ('verb', 0.043), ('goldwater', 0.042), ('spelled', 0.041), ('mrf', 0.041), ('graphical', 0.041), ('tag', 0.04), ('samples', 0.039), ('wi', 0.038), ('finite', 0.037), ('collapsed', 0.037), ('dpmm', 0.037), ('countably', 0.037), ('icon', 0.037), ('bin', 0.037), ('annals', 0.037), ('customer', 0.037), ('sampling', 0.036), ('posterior', 0.034), ('proportional', 0.034), ('linguist', 0.033), ('slots', 0.033), ('markus', 0.033), ('baroni', 0.033), ('plural', 0.033), ('variables', 0.033), ('grammar', 0.032), ('lt', 0.032), ('participle', 0.031), ('wacky', 0.031), ('guess', 0.031), ('xs', 0.031), ('million', 0.031), ('inference', 0.03), ('probability', 0.03), ('sampled', 0.029), ('morphologically', 0.029), ('singular', 0.029), ('abus', 0.029), ('bri', 0.029), ('cht', 0.029), ('combobul', 0.029), ('combobulat', 0.029), ('gaussians', 0.029), ('hafer', 0.029), ('kand', 0.029), ('kurimo', 0.029), ('layout', 0.029), ('monson', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="39-tfidf-1" href="./emnlp-2011-Discovering_Morphological_Paradigms_from_Plain_Text_Using_a_Dirichlet_Process_Mixture_Model.html">39 emnlp-2011-Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model</a></p>
<p>Author: Markus Dreyer ; Jason Eisner</p><p>Abstract: We present an inference algorithm that organizes observed words (tokens) into structured inflectional paradigms (types). It also naturally predicts the spelling of unobserved forms that are missing from these paradigms, and discovers inflectional principles (grammar) that generalize to wholly unobserved words. Our Bayesian generative model of the data explicitly represents tokens, types, inflections, paradigms, and locally conditioned string edits. It assumes that inflected word tokens are generated from an infinite mixture of inflectional paradigms (string tuples). Each paradigm is sampled all at once from a graphical model, whose potential functions are weighted finitestate transducers with language-specific parameters to be learned. These assumptions naturally lead to an elegant empirical Bayes inference procedure that exploits Monte Carlo EM, belief propagation, and dynamic programming. Given 50–100 seed paradigms, adding a 10million-word corpus reduces prediction error for morphological inflections by up to 10%.</p><p>2 0.23576793 <a title="39-tfidf-2" href="./emnlp-2011-Lexical_Generalization_in_CCG_Grammar_Induction_for_Semantic_Parsing.html">87 emnlp-2011-Lexical Generalization in CCG Grammar Induction for Semantic Parsing</a></p>
<p>Author: Tom Kwiatkowski ; Luke Zettlemoyer ; Sharon Goldwater ; Mark Steedman</p><p>Abstract: We consider the problem of learning factored probabilistic CCG grammars for semantic parsing from data containing sentences paired with logical-form meaning representations. Traditional CCG lexicons list lexical items that pair words and phrases with syntactic and semantic content. Such lexicons can be inefficient when words appear repeatedly with closely related lexical content. In this paper, we introduce factored lexicons, which include both lexemes to model word meaning and templates to model systematic variation in word usage. We also present an algorithm for learning factored CCG lexicons, along with a probabilistic parse-selection model. Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance greatly from the lexical factoring. benefits</p><p>3 0.12741284 <a title="39-tfidf-3" href="./emnlp-2011-Universal_Morphological_Analysis_using_Structured_Nearest_Neighbor_Prediction.html">140 emnlp-2011-Universal Morphological Analysis using Structured Nearest Neighbor Prediction</a></p>
<p>Author: Young-Bum Kim ; Joao Graca ; Benjamin Snyder</p><p>Abstract: In this paper, we consider the problem of unsupervised morphological analysis from a new angle. Past work has endeavored to design unsupervised learning methods which explicitly or implicitly encode inductive biases appropriate to the task at hand. We propose instead to treat morphological analysis as a structured prediction problem, where languages with labeled data serve as training examples for unlabeled languages, without the assumption of parallel data. We define a universal morphological feature space in which every language and its morphological analysis reside. We develop a novel structured nearest neighbor prediction method which seeks to find the morphological analysis for each unlabeled lan- guage which lies as close as possible in the feature space to a training language. We apply our model to eight inflecting languages, and induce nominal morphology with substantially higher accuracy than a traditional, MDLbased approach. Our analysis indicates that accuracy continues to improve substantially as the number of training languages increases.</p><p>4 0.12545982 <a title="39-tfidf-4" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>Author: Christos Christodoulopoulos ; Sharon Goldwater ; Mark Steedman</p><p>Abstract: In this paper we present a fully unsupervised syntactic class induction system formulated as a Bayesian multinomial mixture model, where each word type is constrained to belong to a single class. By using a mixture model rather than a sequence model (e.g., HMM), we are able to easily add multiple kinds of features, including those at both the type level (morphology features) and token level (context and alignment features, the latter from parallel corpora). Using only context features, our system yields results comparable to state-of-the art, far better than a similar model without the one-class-per-type constraint. Using the additional features provides added benefit, and our final system outperforms the best published results on most of the 25 corpora tested.</p><p>5 0.10817471 <a title="39-tfidf-5" href="./emnlp-2011-Non-parametric_Bayesian_Segmentation_of_Japanese_Noun_Phrases.html">99 emnlp-2011-Non-parametric Bayesian Segmentation of Japanese Noun Phrases</a></p>
<p>Author: Yugo Murawaki ; Sadao Kurohashi</p><p>Abstract: A key factor of high quality word segmentation for Japanese is a high-coverage dictionary, but it is costly to manually build such a lexical resource. Although external lexical resources for human readers are potentially good knowledge sources, they have not been utilized due to differences in segmentation criteria. To supplement a morphological dictionary with these resources, we propose a new task of Japanese noun phrase segmentation. We apply non-parametric Bayesian language models to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text. For inference, we propose a novel block sampling procedure named hybrid type-based sampling, which has the ability to directly escape a local optimum that is not too distant from the global optimum. Experiments show that the proposed method efficiently corrects the initial segmentation given by a morphological ana- lyzer.</p><p>6 0.058817323 <a title="39-tfidf-6" href="./emnlp-2011-Unsupervised_Structure_Prediction_with_Non-Parallel_Multilingual_Guidance.html">146 emnlp-2011-Unsupervised Structure Prediction with Non-Parallel Multilingual Guidance</a></p>
<p>7 0.056974057 <a title="39-tfidf-7" href="./emnlp-2011-Unsupervised_Information_Extraction_with_Distributional_Prior_Knowledge.html">143 emnlp-2011-Unsupervised Information Extraction with Distributional Prior Knowledge</a></p>
<p>8 0.054669559 <a title="39-tfidf-8" href="./emnlp-2011-Exploiting_Syntactic_and_Distributional_Information_for_Spelling_Correction_with_Web-Scale_N-gram_Models.html">55 emnlp-2011-Exploiting Syntactic and Distributional Information for Spelling Correction with Web-Scale N-gram Models</a></p>
<p>9 0.0505387 <a title="39-tfidf-9" href="./emnlp-2011-Semantic_Topic_Models%3A_Combining_Word_Distributional_Statistics_and_Dictionary_Definitions.html">119 emnlp-2011-Semantic Topic Models: Combining Word Distributional Statistics and Dictionary Definitions</a></p>
<p>10 0.049073141 <a title="39-tfidf-10" href="./emnlp-2011-Unsupervised_Dependency_Parsing_without_Gold_Part-of-Speech_Tags.html">141 emnlp-2011-Unsupervised Dependency Parsing without Gold Part-of-Speech Tags</a></p>
<p>11 0.048534065 <a title="39-tfidf-11" href="./emnlp-2011-Harnessing_WordNet_Senses_for_Supervised_Sentiment_Classification.html">63 emnlp-2011-Harnessing WordNet Senses for Supervised Sentiment Classification</a></p>
<p>12 0.048144195 <a title="39-tfidf-12" href="./emnlp-2011-Identification_of_Multi-word_Expressions_by_Combining_Multiple_Linguistic_Information_Sources.html">69 emnlp-2011-Identification of Multi-word Expressions by Combining Multiple Linguistic Information Sources</a></p>
<p>13 0.047646005 <a title="39-tfidf-13" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>14 0.045069855 <a title="39-tfidf-14" href="./emnlp-2011-Literal_and_Metaphorical_Sense_Identification_through_Concrete_and_Abstract_Context.html">91 emnlp-2011-Literal and Metaphorical Sense Identification through Concrete and Abstract Context</a></p>
<p>15 0.044988967 <a title="39-tfidf-15" href="./emnlp-2011-Cooooooooooooooollllllllllllll%21%21%21%21%21%21%21%21%21%21%21%21%21%21_Using_Word_Lengthening_to_Detect_Sentiment_in_Microblogs.html">33 emnlp-2011-Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using Word Lengthening to Detect Sentiment in Microblogs</a></p>
<p>16 0.044637293 <a title="39-tfidf-16" href="./emnlp-2011-Optimizing_Semantic_Coherence_in_Topic_Models.html">101 emnlp-2011-Optimizing Semantic Coherence in Topic Models</a></p>
<p>17 0.044027191 <a title="39-tfidf-17" href="./emnlp-2011-Simple_Effective_Decipherment_via_Combinatorial_Optimization.html">122 emnlp-2011-Simple Effective Decipherment via Combinatorial Optimization</a></p>
<p>18 0.043818191 <a title="39-tfidf-18" href="./emnlp-2011-Bootstrapped_Named_Entity_Recognition_for_Product_Attribute_Extraction.html">23 emnlp-2011-Bootstrapped Named Entity Recognition for Product Attribute Extraction</a></p>
<p>19 0.041443609 <a title="39-tfidf-19" href="./emnlp-2011-Joint_Models_for_Chinese_POS_Tagging_and_Dependency_Parsing.html">75 emnlp-2011-Joint Models for Chinese POS Tagging and Dependency Parsing</a></p>
<p>20 0.040350728 <a title="39-tfidf-20" href="./emnlp-2011-Multilayer_Sequence_Labeling.html">96 emnlp-2011-Multilayer Sequence Labeling</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.172), (1, -0.035), (2, -0.064), (3, 0.007), (4, 0.031), (5, 0.078), (6, -0.169), (7, 0.016), (8, -0.155), (9, -0.002), (10, -0.068), (11, 0.112), (12, -0.239), (13, -0.126), (14, -0.12), (15, -0.003), (16, -0.133), (17, -0.069), (18, 0.088), (19, 0.07), (20, -0.156), (21, -0.041), (22, 0.135), (23, 0.073), (24, -0.091), (25, 0.055), (26, -0.005), (27, -0.132), (28, 0.057), (29, -0.055), (30, -0.003), (31, -0.159), (32, -0.096), (33, -0.119), (34, 0.242), (35, -0.026), (36, 0.067), (37, -0.075), (38, -0.087), (39, 0.024), (40, 0.076), (41, 0.142), (42, -0.131), (43, -0.099), (44, -0.016), (45, -0.01), (46, 0.145), (47, -0.058), (48, -0.049), (49, 0.117)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94869006 <a title="39-lsi-1" href="./emnlp-2011-Discovering_Morphological_Paradigms_from_Plain_Text_Using_a_Dirichlet_Process_Mixture_Model.html">39 emnlp-2011-Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model</a></p>
<p>Author: Markus Dreyer ; Jason Eisner</p><p>Abstract: We present an inference algorithm that organizes observed words (tokens) into structured inflectional paradigms (types). It also naturally predicts the spelling of unobserved forms that are missing from these paradigms, and discovers inflectional principles (grammar) that generalize to wholly unobserved words. Our Bayesian generative model of the data explicitly represents tokens, types, inflections, paradigms, and locally conditioned string edits. It assumes that inflected word tokens are generated from an infinite mixture of inflectional paradigms (string tuples). Each paradigm is sampled all at once from a graphical model, whose potential functions are weighted finitestate transducers with language-specific parameters to be learned. These assumptions naturally lead to an elegant empirical Bayes inference procedure that exploits Monte Carlo EM, belief propagation, and dynamic programming. Given 50–100 seed paradigms, adding a 10million-word corpus reduces prediction error for morphological inflections by up to 10%.</p><p>2 0.57787734 <a title="39-lsi-2" href="./emnlp-2011-Lexical_Generalization_in_CCG_Grammar_Induction_for_Semantic_Parsing.html">87 emnlp-2011-Lexical Generalization in CCG Grammar Induction for Semantic Parsing</a></p>
<p>Author: Tom Kwiatkowski ; Luke Zettlemoyer ; Sharon Goldwater ; Mark Steedman</p><p>Abstract: We consider the problem of learning factored probabilistic CCG grammars for semantic parsing from data containing sentences paired with logical-form meaning representations. Traditional CCG lexicons list lexical items that pair words and phrases with syntactic and semantic content. Such lexicons can be inefficient when words appear repeatedly with closely related lexical content. In this paper, we introduce factored lexicons, which include both lexemes to model word meaning and templates to model systematic variation in word usage. We also present an algorithm for learning factored CCG lexicons, along with a probabilistic parse-selection model. Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance greatly from the lexical factoring. benefits</p><p>3 0.52100194 <a title="39-lsi-3" href="./emnlp-2011-Universal_Morphological_Analysis_using_Structured_Nearest_Neighbor_Prediction.html">140 emnlp-2011-Universal Morphological Analysis using Structured Nearest Neighbor Prediction</a></p>
<p>Author: Young-Bum Kim ; Joao Graca ; Benjamin Snyder</p><p>Abstract: In this paper, we consider the problem of unsupervised morphological analysis from a new angle. Past work has endeavored to design unsupervised learning methods which explicitly or implicitly encode inductive biases appropriate to the task at hand. We propose instead to treat morphological analysis as a structured prediction problem, where languages with labeled data serve as training examples for unlabeled languages, without the assumption of parallel data. We define a universal morphological feature space in which every language and its morphological analysis reside. We develop a novel structured nearest neighbor prediction method which seeks to find the morphological analysis for each unlabeled lan- guage which lies as close as possible in the feature space to a training language. We apply our model to eight inflecting languages, and induce nominal morphology with substantially higher accuracy than a traditional, MDLbased approach. Our analysis indicates that accuracy continues to improve substantially as the number of training languages increases.</p><p>4 0.46205139 <a title="39-lsi-4" href="./emnlp-2011-Unsupervised_Information_Extraction_with_Distributional_Prior_Knowledge.html">143 emnlp-2011-Unsupervised Information Extraction with Distributional Prior Knowledge</a></p>
<p>Author: Cane Wing-ki Leung ; Jing Jiang ; Kian Ming A. Chai ; Hai Leong Chieu ; Loo-Nin Teow</p><p>Abstract: We address the task of automatic discovery of information extraction template from a given text collection. Our approach clusters candidate slot fillers to identify meaningful template slots. We propose a generative model that incorporates distributional prior knowledge to help distribute candidates in a document into appropriate slots. Empirical results suggest that the proposed prior can bring substantial improvements to our task as compared to a K-means baseline and a Gaussian mixture model baseline. Specifically, the proposed prior has shown to be effective when coupled with discriminative features of the candidates.</p><p>5 0.45998979 <a title="39-lsi-5" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>Author: Christos Christodoulopoulos ; Sharon Goldwater ; Mark Steedman</p><p>Abstract: In this paper we present a fully unsupervised syntactic class induction system formulated as a Bayesian multinomial mixture model, where each word type is constrained to belong to a single class. By using a mixture model rather than a sequence model (e.g., HMM), we are able to easily add multiple kinds of features, including those at both the type level (morphology features) and token level (context and alignment features, the latter from parallel corpora). Using only context features, our system yields results comparable to state-of-the art, far better than a similar model without the one-class-per-type constraint. Using the additional features provides added benefit, and our final system outperforms the best published results on most of the 25 corpora tested.</p><p>6 0.37042168 <a title="39-lsi-6" href="./emnlp-2011-Non-parametric_Bayesian_Segmentation_of_Japanese_Noun_Phrases.html">99 emnlp-2011-Non-parametric Bayesian Segmentation of Japanese Noun Phrases</a></p>
<p>7 0.3281416 <a title="39-lsi-7" href="./emnlp-2011-Literal_and_Metaphorical_Sense_Identification_through_Concrete_and_Abstract_Context.html">91 emnlp-2011-Literal and Metaphorical Sense Identification through Concrete and Abstract Context</a></p>
<p>8 0.28585869 <a title="39-lsi-8" href="./emnlp-2011-Unsupervised_Structure_Prediction_with_Non-Parallel_Multilingual_Guidance.html">146 emnlp-2011-Unsupervised Structure Prediction with Non-Parallel Multilingual Guidance</a></p>
<p>9 0.25010702 <a title="39-lsi-9" href="./emnlp-2011-Exploiting_Syntactic_and_Distributional_Information_for_Spelling_Correction_with_Web-Scale_N-gram_Models.html">55 emnlp-2011-Exploiting Syntactic and Distributional Information for Spelling Correction with Web-Scale N-gram Models</a></p>
<p>10 0.2315267 <a title="39-lsi-10" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>11 0.23101127 <a title="39-lsi-11" href="./emnlp-2011-Multilayer_Sequence_Labeling.html">96 emnlp-2011-Multilayer Sequence Labeling</a></p>
<p>12 0.21649085 <a title="39-lsi-12" href="./emnlp-2011-Analyzing_Methods_for_Improving_Precision_of_Pivot_Based_Bilingual_Dictionaries.html">18 emnlp-2011-Analyzing Methods for Improving Precision of Pivot Based Bilingual Dictionaries</a></p>
<p>13 0.20269305 <a title="39-lsi-13" href="./emnlp-2011-Identification_of_Multi-word_Expressions_by_Combining_Multiple_Linguistic_Information_Sources.html">69 emnlp-2011-Identification of Multi-word Expressions by Combining Multiple Linguistic Information Sources</a></p>
<p>14 0.19003539 <a title="39-lsi-14" href="./emnlp-2011-Bootstrapped_Named_Entity_Recognition_for_Product_Attribute_Extraction.html">23 emnlp-2011-Bootstrapped Named Entity Recognition for Product Attribute Extraction</a></p>
<p>15 0.1889969 <a title="39-lsi-15" href="./emnlp-2011-Exact_Inference_for_Generative_Probabilistic_Non-Projective_Dependency_Parsing.html">52 emnlp-2011-Exact Inference for Generative Probabilistic Non-Projective Dependency Parsing</a></p>
<p>16 0.18879846 <a title="39-lsi-16" href="./emnlp-2011-Computing_Logical_Form_on_Regulatory_Texts.html">32 emnlp-2011-Computing Logical Form on Regulatory Texts</a></p>
<p>17 0.18863413 <a title="39-lsi-17" href="./emnlp-2011-Structured_Sparsity_in_Structured_Prediction.html">129 emnlp-2011-Structured Sparsity in Structured Prediction</a></p>
<p>18 0.18775529 <a title="39-lsi-18" href="./emnlp-2011-Harnessing_WordNet_Senses_for_Supervised_Sentiment_Classification.html">63 emnlp-2011-Harnessing WordNet Senses for Supervised Sentiment Classification</a></p>
<p>19 0.18715945 <a title="39-lsi-19" href="./emnlp-2011-A_Cascaded_Classification_Approach_to_Semantic_Head_Recognition.html">2 emnlp-2011-A Cascaded Classification Approach to Semantic Head Recognition</a></p>
<p>20 0.18496607 <a title="39-lsi-20" href="./emnlp-2011-Syntactic_Decision_Tree_LMs%3A_Random_Selection_or_Intelligent_Design%3F.html">131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(15, 0.039), (23, 0.082), (36, 0.036), (37, 0.023), (45, 0.075), (53, 0.012), (54, 0.038), (57, 0.019), (61, 0.285), (62, 0.023), (64, 0.041), (66, 0.076), (69, 0.015), (79, 0.038), (82, 0.027), (90, 0.011), (94, 0.011), (96, 0.043), (98, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76075232 <a title="39-lda-1" href="./emnlp-2011-Discovering_Morphological_Paradigms_from_Plain_Text_Using_a_Dirichlet_Process_Mixture_Model.html">39 emnlp-2011-Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model</a></p>
<p>Author: Markus Dreyer ; Jason Eisner</p><p>Abstract: We present an inference algorithm that organizes observed words (tokens) into structured inflectional paradigms (types). It also naturally predicts the spelling of unobserved forms that are missing from these paradigms, and discovers inflectional principles (grammar) that generalize to wholly unobserved words. Our Bayesian generative model of the data explicitly represents tokens, types, inflections, paradigms, and locally conditioned string edits. It assumes that inflected word tokens are generated from an infinite mixture of inflectional paradigms (string tuples). Each paradigm is sampled all at once from a graphical model, whose potential functions are weighted finitestate transducers with language-specific parameters to be learned. These assumptions naturally lead to an elegant empirical Bayes inference procedure that exploits Monte Carlo EM, belief propagation, and dynamic programming. Given 50–100 seed paradigms, adding a 10million-word corpus reduces prediction error for morphological inflections by up to 10%.</p><p>2 0.70221609 <a title="39-lda-2" href="./emnlp-2011-Rumor_has_it%3A_Identifying_Misinformation_in_Microblogs.html">117 emnlp-2011-Rumor has it: Identifying Misinformation in Microblogs</a></p>
<p>Author: Vahed Qazvinian ; Emily Rosengren ; Dragomir R. Radev ; Qiaozhu Mei</p><p>Abstract: A rumor is commonly defined as a statement whose true value is unverifiable. Rumors may spread misinformation (false information) or disinformation (deliberately false information) on a network of people. Identifying rumors is crucial in online social media where large amounts of information are easily spread across a large network by sources with unverified authority. In this paper, we address the problem of rumor detection in microblogs and explore the effectiveness of 3 categories of features: content-based, network-based, and microblog-specific memes for correctly identifying rumors. Moreover, we show how these features are also effective in identifying disinformers, users who endorse a rumor and further help it to spread. We perform our experiments on more than 10,000 manually annotated tweets collected from Twitter and show how our retrieval model achieves more than 0.95 in Mean Average Precision (MAP). Fi- nally, we believe that our dataset is the first large-scale dataset on rumor detection. It can open new dimensions in analyzing online misinformation and other aspects of microblog conversations.</p><p>3 0.47624385 <a title="39-lda-3" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>Author: Amit Dubey ; Frank Keller ; Patrick Sturt</p><p>Abstract: This paper introduces a psycholinguistic model of sentence processing which combines a Hidden Markov Model noun phrase chunker with a co-reference classifier. Both models are fully incremental and generative, giving probabilities of lexical elements conditional upon linguistic structure. This allows us to compute the information theoretic measure of surprisal, which is known to correlate with human processing effort. We evaluate our surprisal predictions on the Dundee corpus of eye-movement data show that our model achieve a better fit with human reading times than a syntax-only model which does not have access to co-reference information.</p><p>4 0.46622962 <a title="39-lda-4" href="./emnlp-2011-Universal_Morphological_Analysis_using_Structured_Nearest_Neighbor_Prediction.html">140 emnlp-2011-Universal Morphological Analysis using Structured Nearest Neighbor Prediction</a></p>
<p>Author: Young-Bum Kim ; Joao Graca ; Benjamin Snyder</p><p>Abstract: In this paper, we consider the problem of unsupervised morphological analysis from a new angle. Past work has endeavored to design unsupervised learning methods which explicitly or implicitly encode inductive biases appropriate to the task at hand. We propose instead to treat morphological analysis as a structured prediction problem, where languages with labeled data serve as training examples for unlabeled languages, without the assumption of parallel data. We define a universal morphological feature space in which every language and its morphological analysis reside. We develop a novel structured nearest neighbor prediction method which seeks to find the morphological analysis for each unlabeled lan- guage which lies as close as possible in the feature space to a training language. We apply our model to eight inflecting languages, and induce nominal morphology with substantially higher accuracy than a traditional, MDLbased approach. Our analysis indicates that accuracy continues to improve substantially as the number of training languages increases.</p><p>5 0.46576726 <a title="39-lda-5" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>Author: Christos Christodoulopoulos ; Sharon Goldwater ; Mark Steedman</p><p>Abstract: In this paper we present a fully unsupervised syntactic class induction system formulated as a Bayesian multinomial mixture model, where each word type is constrained to belong to a single class. By using a mixture model rather than a sequence model (e.g., HMM), we are able to easily add multiple kinds of features, including those at both the type level (morphology features) and token level (context and alignment features, the latter from parallel corpora). Using only context features, our system yields results comparable to state-of-the art, far better than a similar model without the one-class-per-type constraint. Using the additional features provides added benefit, and our final system outperforms the best published results on most of the 25 corpora tested.</p><p>6 0.46240449 <a title="39-lda-6" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>7 0.45995492 <a title="39-lda-7" href="./emnlp-2011-Semi-Supervised_Recursive_Autoencoders_for_Predicting_Sentiment_Distributions.html">120 emnlp-2011-Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</a></p>
<p>8 0.45203117 <a title="39-lda-8" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>9 0.45001709 <a title="39-lda-9" href="./emnlp-2011-Unsupervised_Dependency_Parsing_without_Gold_Part-of-Speech_Tags.html">141 emnlp-2011-Unsupervised Dependency Parsing without Gold Part-of-Speech Tags</a></p>
<p>10 0.44554591 <a title="39-lda-10" href="./emnlp-2011-Cross-Cutting_Models_of_Lexical_Semantics.html">37 emnlp-2011-Cross-Cutting Models of Lexical Semantics</a></p>
<p>11 0.44474 <a title="39-lda-11" href="./emnlp-2011-Exploring_Supervised_LDA_Models_for_Assigning_Attributes_to_Adjective-Noun_Phrases.html">56 emnlp-2011-Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases</a></p>
<p>12 0.44436365 <a title="39-lda-12" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>13 0.44046548 <a title="39-lda-13" href="./emnlp-2011-Non-parametric_Bayesian_Segmentation_of_Japanese_Noun_Phrases.html">99 emnlp-2011-Non-parametric Bayesian Segmentation of Japanese Noun Phrases</a></p>
<p>14 0.44034681 <a title="39-lda-14" href="./emnlp-2011-Multiword_Expression_Identification_with_Tree_Substitution_Grammars%3A_A_Parsing_tour_de_force_with_French.html">97 emnlp-2011-Multiword Expression Identification with Tree Substitution Grammars: A Parsing tour de force with French</a></p>
<p>15 0.4382565 <a title="39-lda-15" href="./emnlp-2011-Latent_Vector_Weighting_for_Word_Meaning_in_Context.html">80 emnlp-2011-Latent Vector Weighting for Word Meaning in Context</a></p>
<p>16 0.43609974 <a title="39-lda-16" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>17 0.43439451 <a title="39-lda-17" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<p>18 0.43397149 <a title="39-lda-18" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>19 0.43255273 <a title="39-lda-19" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>20 0.43194917 <a title="39-lda-20" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
