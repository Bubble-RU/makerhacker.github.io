<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 emnlp-2011-Active Learning with Amazon Mechanical Turk</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-17" href="#">emnlp2011-17</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>17 emnlp-2011-Active Learning with Amazon Mechanical Turk</h1>
<br/><p>Source: <a title="emnlp-2011-17-pdf" href="http://aclweb.org/anthology//D/D11/D11-1143.pdf">pdf</a></p><p>Author: Florian Laws ; Christian Scheible ; Hinrich Schutze</p><p>Abstract: Supervised classification needs large amounts of annotated training data that is expensive to create. Two approaches that reduce the cost of annotation are active learning and crowdsourcing. However, these two approaches have not been combined successfully to date. We evaluate the utility of active learning in crowdsourcing on two tasks, named entity recognition and sentiment detection, and show that active learning outperforms random selection of annotation examples in a noisy crowdsourcing scenario.</p><p>Reference: <a title="emnlp-2011-17-reference" href="../emnlp2011_reference/emnlp-2011-Active_Learning_with_Amazon_Mechanical_Turk_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Two approaches that reduce the cost of annotation are active learning and crowdsourcing. [sent-4, score-0.479]
</p><p>2 We evaluate the utility of active learning in crowdsourcing on two tasks, named entity recognition and sentiment detection, and show that active learning outperforms random selection of annotation examples in a noisy crowdsourcing scenario. [sent-6, score-1.636]
</p><p>3 Recently, crowdsourcing services like Amazon Mechanical Turk (MTurk) have become available as an alternative that offers acquisition of non-expert annotations at low cost. [sent-9, score-0.328]
</p><p>4 The cost of MTurk annotation is low, but a consequence of using non-expert annotators is much lower annotation quality. [sent-11, score-0.583]
</p><p>5 AL reduces annotation effort by setting up an annotation loop where, starting from a small seed set, only the maximally informative examples are chosen for annotation. [sent-14, score-0.541]
</p><p>6 Until recently, most AL studies focused on simulating the annotation process by using already available gold standard data. [sent-21, score-0.276]
</p><p>7 For this reason, some authors have questioned the applicability of AL to noisy annotation scenarios such as MTurk (Baldridge and Palmer, 2009; Rehbein et al. [sent-23, score-0.315]
</p><p>8 AL and crowdsourcing are complementary approaches: AL reduces the number of annotations used while crowdsourcing reduces the cost per annotation. [sent-25, score-0.68]
</p><p>9 Our main contribution in this paper is that we show for the first time that AL is significantly better than randomly selected annotation examples in a real crowdsourcing annotation scenario. [sent-27, score-0.701]
</p><p>10 Our experiments directly address two tasks, named entity recognition and sentiment detection, but our Proce dEindgisnb oufr tgh e, 2 S0c1o1tl Canodn,f eUrKen,c Jeuol yn 2 E7m–3p1ir,ic 2a0l1 M1. [sent-28, score-0.351]
</p><p>11 We also show that the effectiveness of MTurk annotation with AL can be further enhanced by using two techniques that increase label quality: adaptive voting and fragment recovery. [sent-31, score-0.679]
</p><p>12 (2010) choose an annotation interface where annotators have to drag the mouse to select entities. [sent-41, score-0.349]
</p><p>13 Carpenter and Poesio (2010) argue that dragging is less convenient for workers than marking tokens. [sent-42, score-0.269]
</p><p>14 Another important difference is that previous –  –  studies on NER have used data sets for which no “linguistic” gold annotation is available. [sent-44, score-0.276]
</p><p>15 (2005) were among the first to investigate the effect of actively sampled instances on agreement of labels and annotation time. [sent-49, score-0.312]
</p><p>16 (2010) investigate AL with human expert annotators for word sense disambiguation, but do not find convincing evidence that AL reduces annotation cost in a realistic (non-simulated) annotation scenario. [sent-55, score-0.645]
</p><p>17 (2010) carried out experiments 1547  on sentiment active learning through crowdsourcing. [sent-57, score-0.451]
</p><p>18 However, in a crowdsourcing scenario, it is not possible to ask specific annotators for a label, as crowdsourcing workers join and leave the site. [sent-60, score-0.799]
</p><p>19 We are not aware of any study that shows that AL is significantly better than a simple baseline of having annotators annotate randomly selected examples in a highly noisy annotation setting like crowdsourcing. [sent-63, score-0.389]
</p><p>20 While AL generally is superior to this baseline in simulated experiments, it is not clear that this result carries over to crowdsourcing annotation. [sent-64, score-0.287]
</p><p>21 3  Annotation System  One fundamental design criterion for our annotation system was the ability to select examples in real time to support, e. [sent-66, score-0.277]
</p><p>22 First, the administrator can manage annotation experiments using a web interface and publish annotation tasks associated with an experiment on MTurk. [sent-73, score-0.472]
</p><p>23 Second, the frontend web application presents annotation tasks to MTurk workers. [sent-75, score-0.324]
</p><p>24 An external question contains an URL to our frontend web application, which is queried when a worker views an annotation task. [sent-78, score-0.55]
</p><p>25 The backend component is responsible for selection of an example to be annotated in response to a worker’s request for an annotation task. [sent-80, score-0.387]
</p><p>26 The backend implements a diverse choice of random and active selection strategies as well as the multilabeling strategies described in section 3. [sent-81, score-0.513]
</p><p>27 Lowercase tokens are prelabeled with “O” (no named entity), but annotators are encouraged to change this label if the token is in fact part of an entity phrase. [sent-88, score-0.335]
</p><p>28 For sentiment annotation, we found in preliminary experiments that using simple radio button selection for the choice of the document label (pos it ive or negat ive) leads to a very high amount of spam submissions, taking the overall classification accuracy down to around 55%. [sent-89, score-0.422]
</p><p>29 1 Concurrent example selection AL works by setting up an interactive annotation loop where at each iteration, the most informative example is selected for annotation. [sent-93, score-0.33]
</p><p>30 However, batch selection might not give the optimum selection (examples in a batch are likely to be redundant, see Brinker (2003)) and wait times can still occur between one batch and the next. [sent-103, score-0.349]
</p><p>31 When performing annotation with MTurk, wait  Pˆ  times are unacceptable. [sent-104, score-0.294]
</p><p>32 Thus, we perform the retraining and uncertainty rescoring concurrently with the annotation user interface. [sent-105, score-0.314]
</p><p>33 The annotation user interface takes the most informative example from the pool and presents it to the annotator. [sent-107, score-0.364]
</p><p>34 In this way, annotation and example selection can run in parallel. [sent-110, score-0.287]
</p><p>35 2 Adaptive voting and fragment recovery MTurk labels often have a high error rate. [sent-114, score-0.446]
</p><p>36 A common strategy for improving label quality is to acquire multiple labels by different workers for each example and then consolidate the annotations into a single label of higher quality. [sent-115, score-0.677]
</p><p>37 , +f = using fragments; sentiment budget 1130 for run 1, sentiment  budget 1756 averaged over 2 runs. [sent-119, score-0.66]
</p><p>38 voting and is adaptive in the number of repeated annotations. [sent-120, score-0.345]
</p><p>39 Then majority voting is performed for each token individually. [sent-122, score-0.319]
</p><p>40 1 Experiments In our NER experiments, we have workers reannotate the English corpus of the CoNLL-2003 NER shared task. [sent-136, score-0.304]
</p><p>41 We chose this corpus to be able to compare crowdsourced annotations with gold standard 2It can take a while in this scheme for annotators to agree on a final annotation for a sentence. [sent-137, score-0.474]
</p><p>42 We make tentative labels of a sentence available to the classifier immediately and replace them with the final labels once voting is completed. [sent-138, score-0.473]
</p><p>43 The sentiment detection task was modeled after a well-known document analysis setup for sentiment classification, introduced by Pang et al. [sent-146, score-0.526]
</p><p>44 We use their corpus of 1000 positive and 1000 negative movie reviews and the Stanford maximum entropy classifier (Manning and Klein, 2003) to predict the sentiment label of each document d from a unigram representation of d. [sent-148, score-0.367]
</p><p>45 We compare random sampling (RS) and AL in combination with the proposed voting and fragment strategies with different parameters. [sent-152, score-0.48]
</p><p>46 We chose voting with at most d = 5 repetitions as our main reannotation strategy for both random and active sampling for NER annotation. [sent-156, score-0.591]
</p><p>47 We always compare two strategies for the same annotation budget. [sent-168, score-0.281]
</p><p>48 For example, the number of training sentences in Table 1 differ in the two relevant columns, but all strategies compared use exactly the same annotation budget (5820, 693 1, 1130,  and 1756, respectively). [sent-169, score-0.348]
</p><p>49 2000 sentences or 450 documents would not have been meaningful; therefore we chose to run an extra experiment with the single annotation strategy to match this up with the budgets of the voting strategies. [sent-172, score-0.588]
</p><p>50 2 Results For sentiment detection, worker accuracy or label quality the percentage of correctly annotated documents is 74. [sent-175, score-0.637]
</p><p>51 In contrast, for NER, worker accuracy the percentage ofnon-O tokens annotated correctly is only 51. [sent-177, score-0.297]
</p><p>52 Adaptive voting and –  –  –  –  1550 fragment recovery manage to recover a small part of the lost performance (lines 2–4); each of the three F1 scores is significantly better than the one above it as indicated by † (Approximate Randomization iTte sast (Noreen, 1989; (CAhpinpcrhoxorim eat al. [sent-185, score-0.354]
</p><p>53 Adaptive voting and fragment recovery again increase worker accuracy (lines 6–8) although total improvement of 3. [sent-191, score-0.553]
</p><p>54 We carried out two runs of the same experiment for sentiment to validate our first positive result since –  –  the difference between the two conditions is not as large as in NER (Figure 1, top right). [sent-199, score-0.291]
</p><p>55 It is likely that some of them can be learned well through random sampling at first; however, active learning can gain accuracy over time because it selects examples with more difficult clues. [sent-206, score-0.34]
</p><p>56 In Figure 1 (bottom), we compare single annotation with adaptive voting. [sent-207, score-0.321]
</p><p>57 Adaptive voting trades quantity of sampled sentences for quality of labels and thus incurs higher net costs per sentence. [sent-209, score-0.396]
</p><p>58 For NER (Figure 1, bottom left), the single annotation strategy has a faster start; so for small budgets, covering a somewhat larger portion of the sample space  is beneficial. [sent-211, score-0.286]
</p><p>59 For sentiment (Figure 1, bottom right), results are similar: voting has no benefit initially, but as finding maximally informative examples to annotate becomes harder in later stages of learning, adaptive voting gains an advantage over single annotations. [sent-217, score-0.954]
</p><p>60 6% accuracy for sentiment (averaged over two runs at budget 1756). [sent-219, score-0.358]
</p><p>61 3 Annotation time per token Most AL work assumes constant cost per annotation unit. [sent-222, score-0.34]
</p><p>62 In annotation with MTurk, cost is not a function 1551 of annotation time because workers are paid a fixed amount per HIT. [sent-226, score-0.78]
</p><p>63 Nevertheless, annotation time plays a part in whether workers are willing to work on a given task for the offered reward. [sent-227, score-0.489]
</p><p>64 This is particularly problematic for NER since workers have to examine each token individually. [sent-228, score-0.318]
</p><p>65 We therefore investigate for NER whether the time MTurk workers spend on annotating sentences differs for random vs. [sent-229, score-0.366]
</p><p>66 We first compute median and mean annotation times and number of tokens per sentence: strategy  mse dci/asnentemnce an toaklelns/sre qntueinrce d  rAanLdom 1 7 . [sent-231, score-0.326]
</p><p>67 # uppercase tokens well as sentences with slightly more uppercase tokens that require annotation. [sent-238, score-0.376]
</p><p>68 (ii) The noisy labels result in bad intermediate models that then select suboptimal examples to be annotated next. [sent-257, score-0.271]
</p><p>69 First, we preserve the sequence of sentences chosen by our AL experiments on MTurk, with 5voting for NER and 4-voting for sentiment but replace the noisy worker-provided labels by gold labels. [sent-260, score-0.476]
</p><p>70 The performance of classifiers trained on this sequence is the dashed line “MTurk selection, gold labels” in Figure 3 for NER (left) and sentiment (right). [sent-261, score-0.319]
</p><p>71 Here, the selection too is controlled by gold labels, so the selection has a noiseless classifier available for scoring and can perform optimal uncertainty selection. [sent-263, score-0.282]
</p><p>72 For a fair comparison, we adjust the batchsize to be equal to the average staleness of a selected example in concurrent MTurk active learning. [sent-269, score-0.317]
</p><p>73 For our concurrent NER system, the average staleness of an example was about 12 (min: 1, max: 40), for sentiment it was about 2. [sent-272, score-0.392]
</p><p>74 (2010) because there are more annotators accessing our system at the same time via MTurk but not as high for sentiment since documents are longer and retraining the sentiment classifier is faster. [sent-274, score-0.715]
</p><p>75 We cannot compare on cost here since we do not know what the persentence cost of a “gold” expert annotation is. [sent-280, score-0.398]
</p><p>76 We attribute this to the fact that the quality of the labels is higher in sentiment than in NER. [sent-291, score-0.415]
</p><p>77 Our initial experiments on sentiment were all negative (showing no improvement of AL compared to random) because label quality was too low. [sent-292, score-0.382]
</p><p>78 5  Worker Quality  So far we have assumed that all workers provide annotations of the same quality. [sent-296, score-0.368]
</p><p>79 Figure 4 shows plots of worker accuracy as a function of worker productivity (number  of annotated examples). [sent-298, score-0.429]
</p><p>80 Some workers submit only one or two HITs just to try out the task. [sent-299, score-0.319]
</p><p>81 For NER, the majority of workers submit between 5 and 10 sentences, with label qualities between 0. [sent-300, score-0.404]
</p><p>82 For sentiment, most workers submit 1 to 5 documents, with label qualities between 0. [sent-305, score-0.378]
</p><p>83 While quality for highly productive workers is mediocre in our experiments, other researchers have found extremely bad quality for their most prolific workers (Callison-Burch, 2009). [sent-309, score-0.684]
</p><p>84 Some of these workers might be spammers who try to submit answers with automatic scripts. [sent-310, score-0.349]
</p><p>85 We encountered some spammers that our heuristics did not detect (shown in the bottom-right areas of Figure 4, left), but the voting mechanism was able to mitigate their negative influence. [sent-311, score-0.274]
</p><p>86 Given the large variation in Figure 4, using worker quality in crowdsourcing for improved training set creation seems promising. [sent-312, score-0.488]
</p><p>87 1 Blocking low-quality workers A simple approach is to refuse annotations from workers that have been determined to provide low quality answers. [sent-315, score-0.697]
</p><p>88 While the voting strategy prevented a performance decrease with bad annotations, it needed to expend many extra annotations for correction. [sent-320, score-0.408]
</p><p>89 When low-quality workers are less active, as in the AL dataset, we find no meaningful performance increase for low cutoffs up to 0. [sent-322, score-0.314]
</p><p>90 2  Trusting high-quality workers  The complementary approach is to take annotations from highly rated workers at face value and immediately accept them as the correct label, bypassing the voting procedure. [sent-328, score-0.941]
</p><p>91 Bypassing saves the cost of 1554 repeated annotation of the same sentence. [sent-329, score-0.291]
</p><p>92 Figure 5 shows learning curves for two bypass thresholds on worker quality (measured as proportion of correct non-O tokens) for random (c) and AL (d). [sent-330, score-0.356]
</p><p>93 While our method of sample selection for AL proved to be quite robust even in the presence of noise, higher quality labels do have an influence on the sample selection (see section 4. [sent-342, score-0.286]
</p><p>94 6  Conclusion  We have investigated the use of AL in a real-life annotation experiment with human annotators instead of traditional simulations with gold labels for (a) (b)  Cost  (c)  Cost  (d)  Cost Cost Figure 5: Blocking low-quality workers: (a) random, (b) AL. [sent-352, score-0.485]
</p><p>95 The annotation was performed using MTurk in an AL framework that features concurrent example selection without wait times. [sent-355, score-0.421]
</p><p>96 We also evaluated two strategies, adaptive voting and fragment recovery, to improve label quality at low additional cost. [sent-356, score-0.519]
</p><p>97 This is clear evidence that active learning and crowdsourcing are complementary methods for lowering annotation cost and should be used together in training set creation for natural language processing tasks. [sent-362, score-0.708]
</p><p>98 We have also conducted oracle experiments that show that further performance gains and cost savings can be achieved by using information about worker quality. [sent-363, score-0.296]
</p><p>99 Using crowdsourcing and active learning to track sentiment in online media. [sent-374, score-0.68]
</p><p>100 Investigating the effects of selective sampling on the annotation task. [sent-409, score-0.273]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mturk', 0.342), ('ner', 0.299), ('al', 0.296), ('workers', 0.269), ('sentiment', 0.263), ('voting', 0.244), ('crowdsourcing', 0.229), ('annotation', 0.22), ('worker', 0.199), ('active', 0.188), ('uppercase', 0.121), ('frontend', 0.104), ('adaptive', 0.101), ('annotations', 0.099), ('labels', 0.092), ('haertel', 0.086), ('wait', 0.074), ('annotators', 0.072), ('cost', 0.071), ('pool', 0.069), ('backend', 0.069), ('donmez', 0.069), ('staleness', 0.069), ('tokens', 0.067), ('budget', 0.067), ('random', 0.067), ('selection', 0.067), ('noisy', 0.065), ('strategies', 0.061), ('quality', 0.06), ('budgets', 0.06), ('bypassing', 0.06), ('concurrent', 0.06), ('label', 0.059), ('simulated', 0.058), ('gold', 0.056), ('recovery', 0.055), ('fragment', 0.055), ('mechanical', 0.055), ('sampling', 0.053), ('ringger', 0.052), ('submit', 0.05), ('token', 0.049), ('uncertainty', 0.047), ('amazon', 0.047), ('retraining', 0.047), ('batch', 0.047), ('classifier', 0.045), ('simulations', 0.045), ('informativeness', 0.045), ('cutoffs', 0.045), ('rehbein', 0.045), ('named', 0.044), ('lewis', 0.044), ('entity', 0.044), ('informative', 0.043), ('hit', 0.042), ('hachey', 0.04), ('strategy', 0.039), ('brew', 0.037), ('expert', 0.036), ('turk', 0.036), ('agreeing', 0.035), ('blocking', 0.035), ('lawson', 0.035), ('pinar', 0.035), ('reannotate', 0.035), ('robbie', 0.035), ('scheible', 0.035), ('schein', 0.035), ('secs', 0.035), ('tomanek', 0.035), ('voyer', 0.035), ('noise', 0.035), ('ive', 0.033), ('laws', 0.033), ('lines', 0.032), ('examples', 0.032), ('interface', 0.032), ('annotated', 0.031), ('queue', 0.03), ('baldridge', 0.03), ('annotating', 0.03), ('bypass', 0.03), ('retrained', 0.03), ('ipeirotis', 0.03), ('questioned', 0.03), ('spammers', 0.03), ('runs', 0.028), ('bottom', 0.027), ('alexis', 0.027), ('carpenter', 0.027), ('crowdsourced', 0.027), ('queried', 0.027), ('reduces', 0.026), ('oracle', 0.026), ('bad', 0.026), ('majority', 0.026), ('documents', 0.025), ('select', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="17-tfidf-1" href="./emnlp-2011-Active_Learning_with_Amazon_Mechanical_Turk.html">17 emnlp-2011-Active Learning with Amazon Mechanical Turk</a></p>
<p>Author: Florian Laws ; Christian Scheible ; Hinrich Schutze</p><p>Abstract: Supervised classification needs large amounts of annotated training data that is expensive to create. Two approaches that reduce the cost of annotation are active learning and crowdsourcing. However, these two approaches have not been combined successfully to date. We evaluate the utility of active learning in crowdsourcing on two tasks, named entity recognition and sentiment detection, and show that active learning outperforms random selection of annotation examples in a noisy crowdsourcing scenario.</p><p>2 0.18384168 <a title="17-tfidf-2" href="./emnlp-2011-Divide_and_Conquer%3A_Crowdsourcing_the_Creation_of_Cross-Lingual_Textual_Entailment_Corpora.html">42 emnlp-2011-Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora</a></p>
<p>Author: Matteo Negri ; Luisa Bentivogli ; Yashar Mehdad ; Danilo Giampiccolo ; Alessandro Marchetti</p><p>Abstract: We address the creation of cross-lingual textual entailment corpora by means of crowdsourcing. Our goal is to define a cheap and replicable data collection methodology that minimizes the manual work done by expert annotators, without resorting to preprocessing tools or already annotated monolingual datasets. In line with recent works emphasizing the need of large-scale annotation efforts for textual entailment, our work aims to: i) tackle the scarcity of data available to train and evaluate systems, and ii) promote the recourse to crowdsourcing as an effective way to reduce the costs of data collection without sacrificing quality. We show that a complex data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators. The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German.</p><p>3 0.16918598 <a title="17-tfidf-3" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>Author: Burr Settles</p><p>Abstract: This paper describes DUALIST, an active learning annotation paradigm which solicits and learns from labels on both features (e.g., words) and instances (e.g., documents). We present a novel semi-supervised training algorithm developed for this setting, which is (1) fast enough to support real-time interactive speeds, and (2) at least as accurate as preexisting methods for learning with mixed feature and instance labels. Human annotators in user studies were able to produce near-stateof-the-art classifiers—on several corpora in a variety of application domains—with only a few minutes of effort.</p><p>4 0.16030636 <a title="17-tfidf-4" href="./emnlp-2011-Compositional_Matrix-Space_Models_for_Sentiment_Analysis.html">30 emnlp-2011-Compositional Matrix-Space Models for Sentiment Analysis</a></p>
<p>Author: Ainur Yessenalina ; Claire Cardie</p><p>Abstract: We present a general learning-based approach for phrase-level sentiment analysis that adopts an ordinal sentiment scale and is explicitly compositional in nature. Thus, we can model the compositional effects required for accurate assignment of phrase-level sentiment. For example, combining an adverb (e.g., “very”) with a positive polar adjective (e.g., “good”) produces a phrase (“very good”) with increased polarity over the adjective alone. Inspired by recent work on distributional approaches to compositionality, we model each word as a matrix and combine words using iterated matrix multiplication, which allows for the modeling of both additive and multiplicative semantic effects. Although the multiplication-based matrix-space framework has been shown to be a theoretically elegant way to model composition (Rudolph and Giesbrecht, 2010), training such models has to be done carefully: the optimization is nonconvex and requires a good initial starting point. This paper presents the first such algorithm for learning a matrix-space model for semantic composition. In the context of the phrase-level sentiment analysis task, our experimental results show statistically significant improvements in performance over a bagof-words model.</p><p>5 0.15638943 <a title="17-tfidf-5" href="./emnlp-2011-Semi-Supervised_Recursive_Autoencoders_for_Predicting_Sentiment_Distributions.html">120 emnlp-2011-Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</a></p>
<p>Author: Richard Socher ; Jeffrey Pennington ; Eric H. Huang ; Andrew Y. Ng ; Christopher D. Manning</p><p>Abstract: We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model’s ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.</p><p>6 0.13997267 <a title="17-tfidf-6" href="./emnlp-2011-Harnessing_WordNet_Senses_for_Supervised_Sentiment_Classification.html">63 emnlp-2011-Harnessing WordNet Senses for Supervised Sentiment Classification</a></p>
<p>7 0.13902332 <a title="17-tfidf-7" href="./emnlp-2011-Cooooooooooooooollllllllllllll%21%21%21%21%21%21%21%21%21%21%21%21%21%21_Using_Word_Lengthening_to_Detect_Sentiment_in_Microblogs.html">33 emnlp-2011-Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using Word Lengthening to Detect Sentiment in Microblogs</a></p>
<p>8 0.13347371 <a title="17-tfidf-8" href="./emnlp-2011-The_Imagination_of_Crowds%3A_Conversational_AAC_Language_Modeling_using_Crowdsourcing_and_Large_Data_Sources.html">133 emnlp-2011-The Imagination of Crowds: Conversational AAC Language Modeling using Crowdsourcing and Large Data Sources</a></p>
<p>9 0.12118008 <a title="17-tfidf-9" href="./emnlp-2011-A_Non-negative_Matrix_Factorization_Based_Approach_for_Active_Dual_Supervision_from_Document_and_Word_Labels.html">9 emnlp-2011-A Non-negative Matrix Factorization Based Approach for Active Dual Supervision from Document and Word Labels</a></p>
<p>10 0.10540193 <a title="17-tfidf-10" href="./emnlp-2011-Discriminating_Gender_on_Twitter.html">41 emnlp-2011-Discriminating Gender on Twitter</a></p>
<p>11 0.080482766 <a title="17-tfidf-11" href="./emnlp-2011-Bootstrapped_Named_Entity_Recognition_for_Product_Attribute_Extraction.html">23 emnlp-2011-Bootstrapped Named Entity Recognition for Product Attribute Extraction</a></p>
<p>12 0.078936078 <a title="17-tfidf-12" href="./emnlp-2011-Named_Entity_Recognition_in_Tweets%3A_An_Experimental_Study.html">98 emnlp-2011-Named Entity Recognition in Tweets: An Experimental Study</a></p>
<p>13 0.0775778 <a title="17-tfidf-13" href="./emnlp-2011-Learning_General_Connotation_of_Words_using_Graph-based_Algorithms.html">81 emnlp-2011-Learning General Connotation of Words using Graph-based Algorithms</a></p>
<p>14 0.075945392 <a title="17-tfidf-14" href="./emnlp-2011-A_Weakly-supervised_Approach_to_Argumentative_Zoning_of_Scientific_Documents.html">12 emnlp-2011-A Weakly-supervised Approach to Argumentative Zoning of Scientific Documents</a></p>
<p>15 0.075363614 <a title="17-tfidf-15" href="./emnlp-2011-Structural_Opinion_Mining_for_Graph-based_Sentiment_Representation.html">126 emnlp-2011-Structural Opinion Mining for Graph-based Sentiment Representation</a></p>
<p>16 0.059451479 <a title="17-tfidf-16" href="./emnlp-2011-Extreme_Extraction_-_Machine_Reading_in_a_Week.html">57 emnlp-2011-Extreme Extraction - Machine Reading in a Week</a></p>
<p>17 0.057562198 <a title="17-tfidf-17" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>18 0.05625641 <a title="17-tfidf-18" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<p>19 0.053048141 <a title="17-tfidf-19" href="./emnlp-2011-Identifying_and_Following_Expert_Investors_in_Stock_Microblogs.html">71 emnlp-2011-Identifying and Following Expert Investors in Stock Microblogs</a></p>
<p>20 0.052235518 <a title="17-tfidf-20" href="./emnlp-2011-Unsupervised_Dependency_Parsing_without_Gold_Part-of-Speech_Tags.html">141 emnlp-2011-Unsupervised Dependency Parsing without Gold Part-of-Speech Tags</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.201), (1, -0.217), (2, 0.134), (3, 0.064), (4, 0.221), (5, 0.024), (6, 0.042), (7, -0.023), (8, -0.064), (9, 0.098), (10, 0.04), (11, -0.048), (12, -0.05), (13, 0.086), (14, -0.022), (15, 0.024), (16, 0.181), (17, -0.157), (18, -0.188), (19, 0.123), (20, 0.046), (21, 0.196), (22, 0.067), (23, -0.077), (24, -0.158), (25, -0.098), (26, -0.139), (27, 0.161), (28, 0.112), (29, -0.12), (30, 0.086), (31, 0.086), (32, 0.095), (33, 0.039), (34, 0.127), (35, 0.075), (36, 0.008), (37, -0.066), (38, -0.029), (39, -0.002), (40, -0.134), (41, -0.06), (42, -0.094), (43, 0.018), (44, -0.057), (45, -0.01), (46, 0.003), (47, 0.009), (48, -0.003), (49, -0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96633321 <a title="17-lsi-1" href="./emnlp-2011-Active_Learning_with_Amazon_Mechanical_Turk.html">17 emnlp-2011-Active Learning with Amazon Mechanical Turk</a></p>
<p>Author: Florian Laws ; Christian Scheible ; Hinrich Schutze</p><p>Abstract: Supervised classification needs large amounts of annotated training data that is expensive to create. Two approaches that reduce the cost of annotation are active learning and crowdsourcing. However, these two approaches have not been combined successfully to date. We evaluate the utility of active learning in crowdsourcing on two tasks, named entity recognition and sentiment detection, and show that active learning outperforms random selection of annotation examples in a noisy crowdsourcing scenario.</p><p>2 0.69780844 <a title="17-lsi-2" href="./emnlp-2011-Divide_and_Conquer%3A_Crowdsourcing_the_Creation_of_Cross-Lingual_Textual_Entailment_Corpora.html">42 emnlp-2011-Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora</a></p>
<p>Author: Matteo Negri ; Luisa Bentivogli ; Yashar Mehdad ; Danilo Giampiccolo ; Alessandro Marchetti</p><p>Abstract: We address the creation of cross-lingual textual entailment corpora by means of crowdsourcing. Our goal is to define a cheap and replicable data collection methodology that minimizes the manual work done by expert annotators, without resorting to preprocessing tools or already annotated monolingual datasets. In line with recent works emphasizing the need of large-scale annotation efforts for textual entailment, our work aims to: i) tackle the scarcity of data available to train and evaluate systems, and ii) promote the recourse to crowdsourcing as an effective way to reduce the costs of data collection without sacrificing quality. We show that a complex data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators. The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German.</p><p>3 0.63732564 <a title="17-lsi-3" href="./emnlp-2011-The_Imagination_of_Crowds%3A_Conversational_AAC_Language_Modeling_using_Crowdsourcing_and_Large_Data_Sources.html">133 emnlp-2011-The Imagination of Crowds: Conversational AAC Language Modeling using Crowdsourcing and Large Data Sources</a></p>
<p>Author: Keith Vertanen ; Per Ola Kristensson</p><p>Abstract: Augmented and alternative communication (AAC) devices enable users with certain communication disabilities to participate in everyday conversations. Such devices often rely on statistical language models to improve text entry by offering word predictions. These predictions can be improved if the language model is trained on data that closely reflects the style of the users’ intended communications. Unfortunately, there is no large dataset consisting of genuine AAC messages. In this paper we demonstrate how we can crowdsource the creation of a large set of fictional AAC messages. We show that these messages model conversational AAC better than the currently used datasets based on telephone conversations or newswire text. We leverage our crowdsourced messages to intelligently select sentences from much larger sets of Twitter, blog and Usenet data. Compared to a model trained only on telephone transcripts, our best performing model reduced perplexity on three test sets of AAC-like communications by 60– 82% relative. This translated to a potential keystroke savings in a predictive keyboard interface of 5–1 1%.</p><p>4 0.49744704 <a title="17-lsi-4" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>Author: Burr Settles</p><p>Abstract: This paper describes DUALIST, an active learning annotation paradigm which solicits and learns from labels on both features (e.g., words) and instances (e.g., documents). We present a novel semi-supervised training algorithm developed for this setting, which is (1) fast enough to support real-time interactive speeds, and (2) at least as accurate as preexisting methods for learning with mixed feature and instance labels. Human annotators in user studies were able to produce near-stateof-the-art classifiers—on several corpora in a variety of application domains—with only a few minutes of effort.</p><p>5 0.47023326 <a title="17-lsi-5" href="./emnlp-2011-Semi-Supervised_Recursive_Autoencoders_for_Predicting_Sentiment_Distributions.html">120 emnlp-2011-Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</a></p>
<p>Author: Richard Socher ; Jeffrey Pennington ; Eric H. Huang ; Andrew Y. Ng ; Christopher D. Manning</p><p>Abstract: We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model’s ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.</p><p>6 0.45391488 <a title="17-lsi-6" href="./emnlp-2011-Cooooooooooooooollllllllllllll%21%21%21%21%21%21%21%21%21%21%21%21%21%21_Using_Word_Lengthening_to_Detect_Sentiment_in_Microblogs.html">33 emnlp-2011-Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using Word Lengthening to Detect Sentiment in Microblogs</a></p>
<p>7 0.42560789 <a title="17-lsi-7" href="./emnlp-2011-Compositional_Matrix-Space_Models_for_Sentiment_Analysis.html">30 emnlp-2011-Compositional Matrix-Space Models for Sentiment Analysis</a></p>
<p>8 0.41475981 <a title="17-lsi-8" href="./emnlp-2011-A_Non-negative_Matrix_Factorization_Based_Approach_for_Active_Dual_Supervision_from_Document_and_Word_Labels.html">9 emnlp-2011-A Non-negative Matrix Factorization Based Approach for Active Dual Supervision from Document and Word Labels</a></p>
<p>9 0.38482147 <a title="17-lsi-9" href="./emnlp-2011-A_Weakly-supervised_Approach_to_Argumentative_Zoning_of_Scientific_Documents.html">12 emnlp-2011-A Weakly-supervised Approach to Argumentative Zoning of Scientific Documents</a></p>
<p>10 0.37977913 <a title="17-lsi-10" href="./emnlp-2011-Harnessing_WordNet_Senses_for_Supervised_Sentiment_Classification.html">63 emnlp-2011-Harnessing WordNet Senses for Supervised Sentiment Classification</a></p>
<p>11 0.33309886 <a title="17-lsi-11" href="./emnlp-2011-Learning_General_Connotation_of_Words_using_Graph-based_Algorithms.html">81 emnlp-2011-Learning General Connotation of Words using Graph-based Algorithms</a></p>
<p>12 0.31667405 <a title="17-lsi-12" href="./emnlp-2011-Bootstrapped_Named_Entity_Recognition_for_Product_Attribute_Extraction.html">23 emnlp-2011-Bootstrapped Named Entity Recognition for Product Attribute Extraction</a></p>
<p>13 0.30972379 <a title="17-lsi-13" href="./emnlp-2011-Discriminating_Gender_on_Twitter.html">41 emnlp-2011-Discriminating Gender on Twitter</a></p>
<p>14 0.22469746 <a title="17-lsi-14" href="./emnlp-2011-Enhancing_Chinese_Word_Segmentation_Using_Unlabeled_Data.html">48 emnlp-2011-Enhancing Chinese Word Segmentation Using Unlabeled Data</a></p>
<p>15 0.2192378 <a title="17-lsi-15" href="./emnlp-2011-Multilayer_Sequence_Labeling.html">96 emnlp-2011-Multilayer Sequence Labeling</a></p>
<p>16 0.21783306 <a title="17-lsi-16" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>17 0.21556534 <a title="17-lsi-17" href="./emnlp-2011-Named_Entity_Recognition_in_Tweets%3A_An_Experimental_Study.html">98 emnlp-2011-Named Entity Recognition in Tweets: An Experimental Study</a></p>
<p>18 0.21297054 <a title="17-lsi-18" href="./emnlp-2011-Unsupervised_Information_Extraction_with_Distributional_Prior_Knowledge.html">143 emnlp-2011-Unsupervised Information Extraction with Distributional Prior Knowledge</a></p>
<p>19 0.20672032 <a title="17-lsi-19" href="./emnlp-2011-Learning_Local_Content_Shift_Detectors_from_Document-level_Information.html">82 emnlp-2011-Learning Local Content Shift Detectors from Document-level Information</a></p>
<p>20 0.19974899 <a title="17-lsi-20" href="./emnlp-2011-Improving_Bilingual_Projections_via_Sparse_Covariance_Matrices.html">73 emnlp-2011-Improving Bilingual Projections via Sparse Covariance Matrices</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(15, 0.013), (23, 0.165), (32, 0.281), (36, 0.025), (37, 0.049), (45, 0.072), (53, 0.014), (54, 0.027), (57, 0.015), (62, 0.014), (64, 0.012), (66, 0.029), (69, 0.021), (79, 0.039), (82, 0.03), (87, 0.014), (90, 0.016), (96, 0.053), (98, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90002769 <a title="17-lda-1" href="./emnlp-2011-Linear_Text_Segmentation_Using_Affinity_Propagation.html">88 emnlp-2011-Linear Text Segmentation Using Affinity Propagation</a></p>
<p>Author: Anna Kazantseva ; Stan Szpakowicz</p><p>Abstract: This paper presents a new algorithm for linear text segmentation. It is an adaptation of Affinity Propagation, a state-of-the-art clustering algorithm in the framework of factor graphs. Affinity Propagation for Segmentation, or APS, receives a set of pairwise similarities between data points and produces segment boundaries and segment centres data points which best describe all other data points within the segment. APS iteratively passes messages in a cyclic factor graph, until convergence. Each iteration works with information on all available similarities, resulting in highquality results. APS scales linearly for realistic segmentation tasks. We derive the algorithm from the original Affinity Propagation formu– lation, and evaluate its performance on topical text segmentation in comparison with two state-of-the art segmenters. The results suggest that APS performs on par with or outperforms these two very competitive baselines.</p><p>same-paper 2 0.78874326 <a title="17-lda-2" href="./emnlp-2011-Active_Learning_with_Amazon_Mechanical_Turk.html">17 emnlp-2011-Active Learning with Amazon Mechanical Turk</a></p>
<p>Author: Florian Laws ; Christian Scheible ; Hinrich Schutze</p><p>Abstract: Supervised classification needs large amounts of annotated training data that is expensive to create. Two approaches that reduce the cost of annotation are active learning and crowdsourcing. However, these two approaches have not been combined successfully to date. We evaluate the utility of active learning in crowdsourcing on two tasks, named entity recognition and sentiment detection, and show that active learning outperforms random selection of annotation examples in a noisy crowdsourcing scenario.</p><p>3 0.58431381 <a title="17-lda-3" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Noah A. Smith</p><p>Abstract: We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.</p><p>4 0.57611525 <a title="17-lda-4" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>Author: Keith Hall ; Ryan McDonald ; Jason Katz-Brown ; Michael Ringgaard</p><p>Abstract: We present an online learning algorithm for training parsers which allows for the inclusion of multiple objective functions. The primary example is the extension of a standard supervised parsing objective function with additional loss-functions, either based on intrinsic parsing quality or task-specific extrinsic measures of quality. Our empirical results show how this approach performs for two dependency parsing algorithms (graph-based and transition-based parsing) and how it achieves increased performance on multiple target tasks including reordering for machine translation and parser adaptation.</p><p>5 0.57587349 <a title="17-lda-5" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>Author: Jason Katz-Brown ; Slav Petrov ; Ryan McDonald ; Franz Och ; David Talbot ; Hiroshi Ichikawa ; Masakazu Seno ; Hideto Kazawa</p><p>Abstract: We propose a simple training regime that can improve the extrinsic performance of a parser, given only a corpus of sentences and a way to automatically evaluate the extrinsic quality of a candidate parse. We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system. We use a corpus of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress.</p><p>6 0.57410836 <a title="17-lda-6" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>7 0.57022744 <a title="17-lda-7" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>8 0.56914103 <a title="17-lda-8" href="./emnlp-2011-Efficient_Subsampling_for_Training_Complex_Language_Models.html">46 emnlp-2011-Efficient Subsampling for Training Complex Language Models</a></p>
<p>9 0.56807685 <a title="17-lda-9" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>10 0.56747234 <a title="17-lda-10" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<p>11 0.56717068 <a title="17-lda-11" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>12 0.56631821 <a title="17-lda-12" href="./emnlp-2011-Lateen_EM%3A_Unsupervised_Training_with_Multiple_Objectives%2C_Applied_to_Dependency_Grammar_Induction.html">79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</a></p>
<p>13 0.56625825 <a title="17-lda-13" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>14 0.56472903 <a title="17-lda-14" href="./emnlp-2011-Structural_Opinion_Mining_for_Graph-based_Sentiment_Representation.html">126 emnlp-2011-Structural Opinion Mining for Graph-based Sentiment Representation</a></p>
<p>15 0.56343442 <a title="17-lda-15" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>16 0.56290901 <a title="17-lda-16" href="./emnlp-2011-Named_Entity_Recognition_in_Tweets%3A_An_Experimental_Study.html">98 emnlp-2011-Named Entity Recognition in Tweets: An Experimental Study</a></p>
<p>17 0.56220686 <a title="17-lda-17" href="./emnlp-2011-Bootstrapped_Named_Entity_Recognition_for_Product_Attribute_Extraction.html">23 emnlp-2011-Bootstrapped Named Entity Recognition for Product Attribute Extraction</a></p>
<p>18 0.5597797 <a title="17-lda-18" href="./emnlp-2011-Generating_Aspect-oriented_Multi-Document_Summarization_with_Event-aspect_model.html">61 emnlp-2011-Generating Aspect-oriented Multi-Document Summarization with Event-aspect model</a></p>
<p>19 0.5592801 <a title="17-lda-19" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>20 0.55837137 <a title="17-lda-20" href="./emnlp-2011-Heuristic_Search_for_Non-Bottom-Up_Tree_Structure_Prediction.html">65 emnlp-2011-Heuristic Search for Non-Bottom-Up Tree Structure Prediction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
