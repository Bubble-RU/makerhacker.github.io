<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-131" href="#">emnlp2011-131</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</h1>
<br/><p>Source: <a title="emnlp-2011-131-pdf" href="http://aclweb.org/anthology//D/D11/D11-1064.pdf">pdf</a></p><p>Author: Denis Filimonov ; Mary Harper</p><p>Abstract: Decision trees have been applied to a variety of NLP tasks, including language modeling, for their ability to handle a variety of attributes and sparse context space. Moreover, forests (collections of decision trees) have been shown to substantially outperform individual decision trees. In this work, we investigate methods for combining trees in a forest, as well as methods for diversifying trees for the task of syntactic language modeling. We show that our tree interpolation technique outperforms the standard method used in the literature, and that, on this particular task, restricting tree contexts in a principled way produces smaller and better forests, with the best achieving an 8% relative reduction in Word Error Rate over an n-gram baseline.</p><p>Reference: <a title="emnlp-2011-131-reference" href="../emnlp2011_reference/emnlp-2011-Syntactic_Decision_Tree_LMs%3A_Random_Selection_or_Intelligent_Design%3F_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu Abstract Decision trees have been applied to a variety of NLP tasks, including language modeling, for their ability to handle a variety of attributes and sparse context space. [sent-4, score-0.371]
</p><p>2 Moreover, forests (collections of decision trees) have been shown to substantially outperform individual decision trees. [sent-5, score-0.667]
</p><p>3 In this work, we investigate methods for combining trees in a forest, as well as methods for diversifying trees for the task of syntactic language modeling. [sent-6, score-0.496]
</p><p>4 In Section 2, we describe the details of the syntactic decision tree LM. [sent-46, score-0.431]
</p><p>5 Construction of a single-tree model is difficult due to the inevitable greediness of the tree construction process and its tendency to overfit the data. [sent-47, score-0.196]
</p><p>6 This problem is often addressed by interpolating with lower order decision trees. [sent-48, score-0.299]
</p><p>7 In Section 3, we point out the inappropriateness of backoff methods borrowed from n-gram models for decision tree LMs and briefly describe a generalized interpolation for such models. [sent-49, score-1.061]
</p><p>8 , ΦN},  A decision tree provides us with a clustering func-  Φ(wii−−1n+1tii−−1n+1)  {Φ1,  tion → where N is the number ofclusters, and clusters Φk are disjoint subsets of the context space. [sent-58, score-0.477]
</p><p>9 1 Decision Tree Construction We use recursive partitioning to grow decision trees. [sent-62, score-0.305]
</p><p>10 Binary splits are often referred to as questions about the context because a binary partition can be represented by a binary function that decides whether an element of context space belongs to one  partition or the other. [sent-65, score-0.168]
</p><p>11 We utilize univariate questions where each question partitions the context on one attribute, e. [sent-66, score-0.174]
</p><p>12 The questions about words and tags are constructed differently: •  The questions q about the words are in the form q(x) ≡ wi+x ∈ bSo,u wt thheere w x diss an integer bremtqw(xee)n ≡ −n 1a∈nd S −1, aerned xS ⊂ Vn nist a seurb bseettowf etehen w−onrd + vocabulary V an . [sent-69, score-0.279]
</p><p>13 Because the algorithm is greedy and depends on the initialization, we construct 4 questions per word position using different random initializations of the Exchange algorithm. [sent-72, score-0.24]
</p><p>14 To estimate the probability at the backoff node (B in Figure 1), we can either use the probability from its grandparent node A or estimate it using a lower order tree (see Section 3), or combine the two. [sent-74, score-0.635]
</p><p>15 +  We have observed no noticeable difference between these methods, which suggests that only a small fraction of probability is estimated from these nodes; therefore, for simplicity, we use the probability estimated at the backoff node’s grandparent. [sent-75, score-0.383]
</p><p>16 •  To create questions about tags we create a hiTeroar ccrheaicteal clustering bofo ualtl tags wine tchree aftoerm a hoi-f a binary tree. [sent-76, score-0.223]
</p><p>17 In this tree, each leaf is an individual tag and each internal node is associated with the subset of tags that the node dominates. [sent-78, score-0.19]
</p><p>18 Questions about tags are constructed in the form q(x, k) ≡ ti+x ∈ Tk, where k is a tnhoede f oirnm mth qe( tag t)ree ≡ an td Tk ∈is tThe subset of tags associated with that node. [sent-79, score-0.16]
</p><p>19 Then, among the questions about attribute 693  Figure 1: A fragment of the decision tree with a backoff node. [sent-85, score-0.782]
</p><p>20 To account for unseen words, we add the backoff node B. [sent-88, score-0.326]
</p><p>21 Note that the tree induction algorithm can also be used to construct trees without tags:  p(wi|wii−−n1+1)  ≈  p(wi|Φ(wii−−n1+1))  We refer to this model as the word-tree model. [sent-91, score-0.414]
</p><p>22 By comparing syntactic and word-tree models, we are able to separate the effects of decision tree modeling and syntactic information on language modeling by comparing both models to an n-gram baseline. [sent-92, score-0.54]
</p><p>23 2 In-tree Smoothing A decision tree offers a hierarchy of clusterings that can be exploited for smoothing. [sent-94, score-0.403]
</p><p>24 ,  p(witi|Φ(wii−−1n+1tii−−n1+1))  p(witi|Φ(wii−−1n+2tii−−n1+2)),  and so on up until p(witi) (wwhii−chn i2s a +o2ne-node tree (essentially a unigram model). [sent-100, score-0.155]
</p><p>25 Although superficially similar to backoff in n-gram models, lower order decision trees differ substantially from lower order n-gram models and require different interpolation methods. [sent-101, score-1.185]
</p><p>26 In the next section, we discuss this difference and present a generalized interpolation that is more suitable for combining decision tree models. [sent-102, score-0.757]
</p><p>27 2 in a more generic way: ˜p (wi|w1i−1)  =  ρn(wi|Φn(w1i−1)) + (6) γ(Φn(wi1−1)) ·˜ p(wi|BOn−1(w1i−1))  where, ρn is a discounted distribution, Φn is a clustering function of order n, and γ(Φn(wi1−1)) is the backoff weight chosen to normalize the distribution. [sent-107, score-0.352]
</p><p>28 BOn−1 is the backoff clustering function of order n − 1, representing a reduction of context size. [sent-108, score-0.37]
</p><p>29 In the case of a decision tree model, the same b2ackoff function is typically used, but the clustering function can be arbitrary. [sent-112, score-0.441]
</p><p>30 6 is that the backoff context BOn−1 (w1i−1) allows for a more robust (but less informed) probability estimation than the con-  text cluster Φn(wi1−1). [sent-114, score-0.376]
</p><p>31 More precisely: ∀w1i−1,W  : W ∈ Φn(wi1−1) ⇒ W ∈ BOn−1(w1i−1) (7) that is, every word sequence W that belongs to a context cluster Φn(wi1−1), belongs to the same backoff cluster BOn−1 (w1i−1) (hence has the same backoff distribution). [sent-115, score-0.712]
</p><p>32 , a decision tree, the property is not necessarily satisfied. [sent-119, score-0.305]
</p><p>33 Let us consider what happens when we have two context sequences W and W0 that belong to the same cluster Φn(W) = Φn(W0) but different backoff clusters BOn−1 (W) BOn−1 (W0). [sent-121, score-0.351]
</p><p>34 For example: suppose we h(Wave) Φ(wi−2wi−1) = ({on}, {may,june}) and two corresponding backoff c({luosnte}r,s{: BayO,ju0 = ({may}) a cnodr rBesOpo00n = ({june}). [sent-122, score-0.273]
</p><p>35 However this would not be possible in the backoff scheme in Eq. [sent-126, score-0.273]
</p><p>36 Hence arbitrary clustering (an advantage of decision trees) leads to a violation of Property 7, which  wii−−n1+1,  =  is likely to produce a degradation in performance if backoff interpolation Eq. [sent-128, score-0.926]
</p><p>37 pn(wi |φn) is the probability distribution at the cluster φn iφn the tree of order n. [sent-132, score-0.222]
</p><p>38 This interpolation method is particularly useful as, unlike count-based discounting methods (e. [sent-133, score-0.326]
</p><p>39 In (Filimonov and Harper, 2011), we observed that because of the violation of Property 7 in decision tree models, the interpolation method of Eq. [sent-136, score-0.729]
</p><p>40 Instead we pro-  posed the following generalized form of linear interpolation:  ˜pn(wi|wi −−n1+1) =Pnm=1Pλmnm(=φ1mλ)m ·( pφmm()wi|φm)  (9)  Note that the recursive interpolation of Eq. [sent-138, score-0.387]
</p><p>41 8 can be reprPesented in this form with the additional constraint Pnm=1 λm(φm) = 1, which is not required in the genPeralized interpolation of Eq. [sent-139, score-0.297]
</p><p>42 9, individual trees do not have explicit higher-lower order relations, they are treated as a collection of trees, i. [sent-145, score-0.262]
</p><p>43 Naturally, to benefit from the forest model, its trees must differ in some way. [sent-148, score-0.339]
</p><p>44 Different trees can be created based on differences in the training data, differences in the  tree growing algorithm, or some non-determinism in the way the trees are constructed. [sent-149, score-0.623]
</p><p>45 (Xu, 2005) used randomization techniques to produce a large forest of decision trees that were combined as follows: 695  p(wi|wi −−n1+1) =M1mXM=1pm(wi|wi −−n1+1)  (10)  where M is the number ofdecision trees in the forest (he proposed M = 100) and pm is the m-th tree model4. [sent-150, score-1.201]
</p><p>46 Note that this type of interpolation assumes that each tree model is “equal” a priori and therefore is only appropriate when the tree models are grown in the same way (particularly, using the same order of context). [sent-151, score-0.741]
</p><p>47 (Xu, 2005) showed that, although each individual tree is a fairly weak model, their combination outperforms the n-gram baseline substantially. [sent-155, score-0.183]
</p><p>48 However, we find this approach impractical for online application of any sizable model: In our experiments, fourgram trees have approximately 1. [sent-156, score-0.518]
</p><p>49 It would be infeasible to apply a model consisting of more than a handful of such trees without distributed computing of some sort. [sent-158, score-0.27]
</p><p>50 Therefore, we pose the following question: If we can afford to have only a handful of trees in the model, what would be best approach to construct those trees? [sent-159, score-0.295]
</p><p>51 In the remainder of this section, we will describe the experimental setup, discuss and evaluate different ways of building decision tree forests for language modeling, and compare combination methods based on Eq. [sent-160, score-0.546]
</p><p>52 For the syntactic modeling, we used  tags comprised of the POS tags of the word and it’s head. [sent-169, score-0.184]
</p><p>53 We constructed two sets of decision trees (a joint syntactic model and a word-tree model) as described in Section 2. [sent-176, score-0.593]
</p><p>54 Each set was comprised of a fourgram tree with backoff trigram, bigram, and unigram trees. [sent-177, score-0.756]
</p><p>55 He found that when the Exchange algorithm was initialized randomly, the Bernoulli trial parameter did not matter; however, when the Exchange algorithm was initialized deterministically; lower values for the Bernoulli trial parameter r yielded better overall forest performance. [sent-189, score-0.212]
</p><p>56 By introducing Bernoulli trials, on the other hand, there is a choice to purposely degrade the quality of individual trees in the hope that additional diversity would enable their combination to compensate for the loss of quality in individual trees. [sent-198, score-0.353]
</p><p>57 Another way of introducing randomness to the tree construction without apparent degradation of individual tree quality is through varying the data, e. [sent-199, score-0.483]
</p><p>58 Let us take a closer look at the effect of different types of randomization on individual trees and their combinations. [sent-203, score-0.357]
</p><p>59 In the first set of experiments,  we compare the performance of a single undegraded fourgram tree9 with forests of fourgram trees grown randomly with Bernoulli trials. [sent-204, score-1.231]
</p><p>60 Having only sameorder trees in a forest allows us to apply interpolation of Eq. [sent-205, score-0.636]
</p><p>61 10 (used in (Xu, 2005)) and compare with the interpolation method presented in Eq. [sent-206, score-0.297]
</p><p>62 By comparing forests of different sizes with the baseline from Table 1, we are able to evaluate the effect of randomization in decision tree growing and assess the importance of the lower order trees. [sent-208, score-0.692]
</p><p>63 Note that, while an undegraded syntactic tree is better than the word tree, the situation is reversed when the trees are grown randomly. [sent-210, score-0.703]
</p><p>64 As we increase the number of random trees in the forest, the perplexity decreases as expected, with the interpolation method of Eq. [sent-212, score-0.748]
</p><p>65 Note that in the case of the word-tree model, it takes 4 random decision trees to reach the performance of a single undegraded tree, while in the joint model, even  8Here and henceforth, by “undegraded” we mean “according to the algorithm described in Section 2. [sent-215, score-0.731]
</p><p>66 Percentage numbers in parentheses denote the reduction of perplexity relative to the lower order model of the same type. [sent-235, score-0.298]
</p><p>67 1  Table 2: Perplexity numbers obtained using fourgram trees only. [sent-246, score-0.556]
</p><p>68 Note that “undgr” and “rnd” denote undegraded and randomly grown trees with Bernoulli trials, respectively, and the number indicates the number of trees in the forest. [sent-247, score-0.754]
</p><p>69 Also “baseline” refers to the fourgram models with lower order trees (from Table 1, Eq. [sent-248, score-0.6]
</p><p>70 5 trees are much worse than a single decision tree constructed without randomization. [sent-250, score-0.685]
</p><p>71 In Table 3, we evaluate forests of fourgram trees produced using randomizations without degrading the tree construction algorithm. [sent-252, score-0.898]
</p><p>72 All forests in this table use the interpolation method of Eq. [sent-254, score-0.44]
</p><p>73 Note that, while these perplexity numbers are substantially better than trees produced with Bernoulli trials in Table 2, they are still significantly worse than the baseline model from Table 1. [sent-256, score-0.563]
</p><p>74 These results suggest that, while it is beneficial to combine different decision trees, we should introduce differences to the tree construction process 697  # tre sExcwhongrd. [sent-257, score-0.444]
</p><p>75 07  Table 3: Perplexity numbers obtained using fourgram trees produced using random initialization of the Exchange algorithm (Exchng. [sent-265, score-0.614]
</p><p>76 Note that “baseline” refers to the fourgram models with lower order trees (from Table 1). [sent-267, score-0.6]
</p><p>77 without degrading the trees when introducing randomness, especially for joint models. [sent-270, score-0.338]
</p><p>78 In addition, lower order trees seem to play an important role for high quality model combination. [sent-271, score-0.285]
</p><p>79 3 Context-Restricted Forest As we have mentioned above, combining higher and  lower order decision trees produces much better results. [sent-273, score-0.533]
</p><p>80 A lower order decision tree is grown from a lower order context space, i. [sent-274, score-0.644]
</p><p>81 Note that in this case, rather than randomly ignoring contexts via Bernoulli trials at every node in the decision tree, we discard some context attributes upfront in a principled manner (i. [sent-277, score-0.497]
</p><p>82 , most distant context) and then grow the decision tree without degradation. [sent-279, score-0.427]
</p><p>83 In Table 4, we present the perplexity numbers for our standard model with additional trees. [sent-281, score-0.224]
</p><p>84 We denote context-restricted trees by their Markovian or-  ModelsizePPL 1w1t + 2w2t + 3w3t + 4w4t (*)294MB147. [sent-282, score-0.234]
</p><p>85 “bernoulli-rnd” and “datarnd” indicate fourgram trees randomized using Bernoulli trials and varying training data, respectively. [sent-287, score-0.623]
</p><p>86 The second column shows the combined size of decision trees in the forest. [sent-288, score-0.482]
</p><p>87 ders (words w and tags t independently), so 3w2t indicates a decision tree implementing the probability function: p(witi |wi−1wi−2ti−1). [sent-289, score-0.484]
</p><p>88 The fourgram joint model presented in Table 1 has four trees and is labeled with the formula “1w1t + 2w2t + 3w3t + 4w4t” in Table 4. [sent-290, score-0.553]
</p><p>89 The randomly grown trees (denoted “bernoulli-rnd”) are grown utilizing the full context 4w4t using the methods described in Section 4. [sent-291, score-0.476]
</p><p>90 All models utilize the generalized interpolation method described in Section 3. [sent-293, score-0.426]
</p><p>91 As can be seen in Table 4, adding undegraded trees consistently improves the performance of an already strong baseline, while adding random trees only increases the perplexity because their quality is worse than undegraded trees’ . [sent-295, score-1.051]
</p><p>92 Note that the last two rows are syntactic models using the interpolation method of Eq. [sent-309, score-0.356]
</p><p>93 In Table 5, we present WER results along with the corresponding perplexity numbers from Tables 1 and 4 for our lowest perplexity syntactic model, as well as the baselines (modified KN n-gram model and standard decision tree models using interpolation methods of Eq. [sent-319, score-1.169]
</p><p>94 9 substantially improves performance over the interpolation method of Eq. [sent-323, score-0.297]
</p><p>95 Adding four trees utilizing context restricted in different ways further reduces WER by 0. [sent-326, score-0.27]
</p><p>96 6  Conclusion  In this paper, we investigate various aspects of combining multiple decision trees in a single language model. [sent-331, score-0.482]
</p><p>97 9) for decision tree models proposed in (Filimonov and Harper, 2011) is in fact a forest in-  terpolation method rather than a backoff interpolation because, in Eq. [sent-333, score-1.109]
</p><p>98 9, models do not have explicit higher-lower order relation as they do in backoff interpolation (Eq. [sent-334, score-0.601]
</p><p>99 Thus, in this paper we investigate the question of how to construct decision trees so that their combination results in improved performance (under the assumption that computational tractability allows only a handful of decision trees in a forest). [sent-336, score-1.025]
</p><p>100 Additionally, we observe that simply restricting the context used to construct trees in different ways, not only produces smaller trees (because of the context reduction), but the resulting variations in trees also produce forests that are at least as good as forests of larger trees. [sent-340, score-1.085]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wii', 0.301), ('interpolation', 0.297), ('fourgram', 0.284), ('backoff', 0.273), ('decision', 0.248), ('witi', 0.243), ('trees', 0.234), ('wi', 0.197), ('perplexity', 0.186), ('undegraded', 0.183), ('exchange', 0.158), ('tree', 0.155), ('bernoulli', 0.151), ('forests', 0.143), ('filimonov', 0.14), ('bon', 0.14), ('forest', 0.105), ('trials', 0.105), ('grown', 0.103), ('wer', 0.099), ('harper', 0.096), ('randomization', 0.095), ('initializations', 0.087), ('asr', 0.083), ('questions', 0.073), ('lms', 0.062), ('property', 0.057), ('generalized', 0.057), ('tags', 0.056), ('attributes', 0.055), ('node', 0.053), ('heldout', 0.052), ('lower', 0.051), ('constructed', 0.048), ('mb', 0.047), ('comprised', 0.044), ('xu', 0.043), ('rescoring', 0.042), ('cluster', 0.042), ('acoustic', 0.041), ('construction', 0.041), ('degradation', 0.041), ('utilize', 0.041), ('degrading', 0.041), ('discounted', 0.041), ('pnm', 0.041), ('denis', 0.039), ('numbers', 0.038), ('clustering', 0.038), ('mary', 0.037), ('jelinek', 0.037), ('context', 0.036), ('handful', 0.036), ('lattices', 0.036), ('randomness', 0.035), ('bahl', 0.035), ('purposely', 0.035), ('bilmes', 0.035), ('heeman', 0.035), ('joint', 0.035), ('wsj', 0.034), ('lm', 0.034), ('recursive', 0.033), ('attribute', 0.033), ('folds', 0.033), ('stopping', 0.033), ('speech', 0.033), ('models', 0.031), ('random', 0.031), ('estimated', 0.03), ('violation', 0.029), ('discounting', 0.029), ('vocabulary', 0.029), ('individual', 0.028), ('trigram', 0.028), ('syntactic', 0.028), ('introducing', 0.028), ('chelba', 0.028), ('frederick', 0.028), ('kn', 0.028), ('trial', 0.028), ('tk', 0.028), ('initialization', 0.027), ('pn', 0.027), ('violated', 0.026), ('construct', 0.025), ('recursively', 0.025), ('modeling', 0.025), ('transcription', 0.025), ('pm', 0.025), ('probability', 0.025), ('greedy', 0.024), ('iy', 0.024), ('rosenfeld', 0.024), ('fluent', 0.024), ('grow', 0.024), ('partitions', 0.024), ('reduction', 0.023), ('variety', 0.023), ('belongs', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000012 <a title="131-tfidf-1" href="./emnlp-2011-Syntactic_Decision_Tree_LMs%3A_Random_Selection_or_Intelligent_Design%3F.html">131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</a></p>
<p>Author: Denis Filimonov ; Mary Harper</p><p>Abstract: Decision trees have been applied to a variety of NLP tasks, including language modeling, for their ability to handle a variety of attributes and sparse context space. Moreover, forests (collections of decision trees) have been shown to substantially outperform individual decision trees. In this work, we investigate methods for combining trees in a forest, as well as methods for diversifying trees for the task of syntactic language modeling. We show that our tree interpolation technique outperforms the standard method used in the literature, and that, on this particular task, restricting tree contexts in a principled way produces smaller and better forests, with the best achieving an 8% relative reduction in Word Error Rate over an n-gram baseline.</p><p>2 0.12399378 <a title="131-tfidf-2" href="./emnlp-2011-A_Fast_Re-scoring_Strategy_to_Capture_Long-Distance_Dependencies.html">5 emnlp-2011-A Fast Re-scoring Strategy to Capture Long-Distance Dependencies</a></p>
<p>Author: Anoop Deoras ; Tomas Mikolov ; Kenneth Church</p><p>Abstract: A re-scoring strategy is proposed that makes it feasible to capture more long-distance dependencies in the natural language. Two pass strategies have become popular in a number of recognition tasks such as ASR (automatic speech recognition), MT (machine translation) and OCR (optical character recognition). The first pass typically applies a weak language model (n-grams) to a lattice and the second pass applies a stronger language model to N best lists. The stronger language model is intended to capture more longdistance dependencies. The proposed method uses RNN-LM (recurrent neural network language model), which is a long span LM, to rescore word lattices in the second pass. A hill climbing method (iterative decoding) is proposed to search over islands of confusability in the word lattice. An evaluation based on Broadcast News shows speedups of 20 over basic N best re-scoring, and word error rate reduction of 8% (relative) on a highly competitive setup.</p><p>3 0.11429693 <a title="131-tfidf-3" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<p>Author: Amittai Axelrod ; Xiaodong He ; Jianfeng Gao</p><p>Abstract: Xiaodong He Microsoft Research Redmond, WA 98052 xiaohe @mi cro s o ft . com Jianfeng Gao Microsoft Research Redmond, WA 98052 j fgao @mi cro s o ft . com have its own argot, vocabulary or stylistic preferences, such that the corpus characteristics will necWe explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain. These sentences may be selected with simple cross-entropy based methods, of which we present three. As these sentences are not themselves identical to the in-domain data, we call them pseudo in-domain subcorpora. These subcorpora 1% the size of the original can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus. Performance is further improved when we use these domain-adapted models in combination with a true in-domain model. The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining in- and general-domain systems during decoding. – –</p><p>4 0.10095574 <a title="131-tfidf-4" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>Author: Reut Tsarfaty ; Joakim Nivre ; Evelina Andersson</p><p>Abstract: unkown-abstract</p><p>5 0.096421674 <a title="131-tfidf-5" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>Author: Xinyan Xiao ; Yang Liu ; Qun Liu ; Shouxun Lin</p><p>Abstract: Although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features, it is hard to scale up to large data due to decoding complexity. We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets.</p><p>6 0.095432527 <a title="131-tfidf-6" href="./emnlp-2011-Language_Models_for_Machine_Translation%3A_Original_vs._Translated_Texts.html">76 emnlp-2011-Language Models for Machine Translation: Original vs. Translated Texts</a></p>
<p>7 0.093433358 <a title="131-tfidf-7" href="./emnlp-2011-Statistical_Machine_Translation_with_Local_Language_Models.html">125 emnlp-2011-Statistical Machine Translation with Local Language Models</a></p>
<p>8 0.084209085 <a title="131-tfidf-8" href="./emnlp-2011-Third-order_Variational_Reranking_on_Packed-Shared_Dependency_Forests.html">134 emnlp-2011-Third-order Variational Reranking on Packed-Shared Dependency Forests</a></p>
<p>9 0.081324376 <a title="131-tfidf-9" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>10 0.064415373 <a title="131-tfidf-10" href="./emnlp-2011-Unsupervised_Dependency_Parsing_without_Gold_Part-of-Speech_Tags.html">141 emnlp-2011-Unsupervised Dependency Parsing without Gold Part-of-Speech Tags</a></p>
<p>11 0.063838758 <a title="131-tfidf-11" href="./emnlp-2011-Exploring_Supervised_LDA_Models_for_Assigning_Attributes_to_Adjective-Noun_Phrases.html">56 emnlp-2011-Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases</a></p>
<p>12 0.059835203 <a title="131-tfidf-12" href="./emnlp-2011-Joint_Models_for_Chinese_POS_Tagging_and_Dependency_Parsing.html">75 emnlp-2011-Joint Models for Chinese POS Tagging and Dependency Parsing</a></p>
<p>13 0.055505626 <a title="131-tfidf-13" href="./emnlp-2011-Using_Syntactic_and_Semantic_Structural_Kernels_for_Classifying_Definition_Questions_in_Jeopardy%21.html">147 emnlp-2011-Using Syntactic and Semantic Structural Kernels for Classifying Definition Questions in Jeopardy!</a></p>
<p>14 0.054737337 <a title="131-tfidf-14" href="./emnlp-2011-Inducing_Sentence_Structure_from_Parallel_Corpora_for_Reordering.html">74 emnlp-2011-Inducing Sentence Structure from Parallel Corpora for Reordering</a></p>
<p>15 0.05469634 <a title="131-tfidf-15" href="./emnlp-2011-Watermarking_the_Outputs_of_Structured_Prediction_with_an_application_in_Statistical_Machine_Translation..html">148 emnlp-2011-Watermarking the Outputs of Structured Prediction with an application in Statistical Machine Translation.</a></p>
<p>16 0.053714547 <a title="131-tfidf-16" href="./emnlp-2011-Efficient_Subsampling_for_Training_Complex_Language_Models.html">46 emnlp-2011-Efficient Subsampling for Training Complex Language Models</a></p>
<p>17 0.052769605 <a title="131-tfidf-17" href="./emnlp-2011-Unsupervised_Structure_Prediction_with_Non-Parallel_Multilingual_Guidance.html">146 emnlp-2011-Unsupervised Structure Prediction with Non-Parallel Multilingual Guidance</a></p>
<p>18 0.05252137 <a title="131-tfidf-18" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>19 0.052097891 <a title="131-tfidf-19" href="./emnlp-2011-Structured_Lexical_Similarity_via_Convolution_Kernels_on_Dependency_Trees.html">127 emnlp-2011-Structured Lexical Similarity via Convolution Kernels on Dependency Trees</a></p>
<p>20 0.051030379 <a title="131-tfidf-20" href="./emnlp-2011-Accurate_Parsing_with_Compact_Tree-Substitution_Grammars%3A_Double-DOP.html">16 emnlp-2011-Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.184), (1, 0.051), (2, -0.01), (3, -0.025), (4, -0.003), (5, 0.026), (6, -0.062), (7, -0.114), (8, 0.072), (9, -0.084), (10, 0.067), (11, 0.061), (12, -0.063), (13, 0.116), (14, -0.128), (15, 0.123), (16, 0.121), (17, 0.069), (18, -0.024), (19, 0.136), (20, 0.145), (21, -0.001), (22, 0.056), (23, -0.138), (24, -0.08), (25, -0.112), (26, 0.027), (27, -0.103), (28, 0.122), (29, 0.035), (30, -0.018), (31, -0.154), (32, -0.075), (33, -0.15), (34, -0.201), (35, -0.129), (36, -0.062), (37, -0.051), (38, -0.003), (39, -0.005), (40, 0.04), (41, 0.035), (42, -0.08), (43, -0.137), (44, 0.108), (45, 0.242), (46, 0.017), (47, -0.062), (48, 0.114), (49, 0.106)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95731735 <a title="131-lsi-1" href="./emnlp-2011-Syntactic_Decision_Tree_LMs%3A_Random_Selection_or_Intelligent_Design%3F.html">131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</a></p>
<p>Author: Denis Filimonov ; Mary Harper</p><p>Abstract: Decision trees have been applied to a variety of NLP tasks, including language modeling, for their ability to handle a variety of attributes and sparse context space. Moreover, forests (collections of decision trees) have been shown to substantially outperform individual decision trees. In this work, we investigate methods for combining trees in a forest, as well as methods for diversifying trees for the task of syntactic language modeling. We show that our tree interpolation technique outperforms the standard method used in the literature, and that, on this particular task, restricting tree contexts in a principled way produces smaller and better forests, with the best achieving an 8% relative reduction in Word Error Rate over an n-gram baseline.</p><p>2 0.59285659 <a title="131-lsi-2" href="./emnlp-2011-Efficient_Subsampling_for_Training_Complex_Language_Models.html">46 emnlp-2011-Efficient Subsampling for Training Complex Language Models</a></p>
<p>Author: Puyang Xu ; Asela Gunawardana ; Sanjeev Khudanpur</p><p>Abstract: We propose an efficient way to train maximum entropy language models (MELM) and neural network language models (NNLM). The advantage of the proposed method comes from a more robust and efficient subsampling technique. The original multi-class language modeling problem is transformed into a set of binary problems where each binary classifier predicts whether or not a particular word will occur. We show that the binarized model is as powerful as the standard model and allows us to aggressively subsample negative training examples without sacrificing predictive performance. Empirical results show that we can train MELM and NNLM at 1% ∼ 5% of the strtaaninda MrdE complexity LwMith a no %los ∼s 5in% performance.</p><p>3 0.52295756 <a title="131-lsi-3" href="./emnlp-2011-Language_Models_for_Machine_Translation%3A_Original_vs._Translated_Texts.html">76 emnlp-2011-Language Models for Machine Translation: Original vs. Translated Texts</a></p>
<p>Author: Gennadi Lembersky ; Noam Ordan ; Shuly Wintner</p><p>Abstract: We investigate the differences between language models compiled from original target-language texts and those compiled from texts manually translated to the target language. Corroborating established observations of Translation Studies, we demonstrate that the latter are significantly better predictors of translated sentences than the former, and hence fit the reference set better. Furthermore, translated texts yield better language models for statistical machine translation than original texts.</p><p>4 0.40595454 <a title="131-lsi-4" href="./emnlp-2011-Efficient_retrieval_of_tree_translation_examples_for_Syntax-Based_Machine_Translation.html">47 emnlp-2011-Efficient retrieval of tree translation examples for Syntax-Based Machine Translation</a></p>
<p>Author: Fabien Cromieres ; Sadao Kurohashi</p><p>Abstract: We propose an algorithm allowing to efficiently retrieve example treelets in a parsed tree database in order to allow on-the-fly extraction of syntactic translation rules. We also propose improvements of this algorithm allowing several kinds of flexible matchings.</p><p>5 0.3843427 <a title="131-lsi-5" href="./emnlp-2011-A_Fast_Re-scoring_Strategy_to_Capture_Long-Distance_Dependencies.html">5 emnlp-2011-A Fast Re-scoring Strategy to Capture Long-Distance Dependencies</a></p>
<p>Author: Anoop Deoras ; Tomas Mikolov ; Kenneth Church</p><p>Abstract: A re-scoring strategy is proposed that makes it feasible to capture more long-distance dependencies in the natural language. Two pass strategies have become popular in a number of recognition tasks such as ASR (automatic speech recognition), MT (machine translation) and OCR (optical character recognition). The first pass typically applies a weak language model (n-grams) to a lattice and the second pass applies a stronger language model to N best lists. The stronger language model is intended to capture more longdistance dependencies. The proposed method uses RNN-LM (recurrent neural network language model), which is a long span LM, to rescore word lattices in the second pass. A hill climbing method (iterative decoding) is proposed to search over islands of confusability in the word lattice. An evaluation based on Broadcast News shows speedups of 20 over basic N best re-scoring, and word error rate reduction of 8% (relative) on a highly competitive setup.</p><p>6 0.36766738 <a title="131-lsi-6" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>7 0.34923962 <a title="131-lsi-7" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<p>8 0.32739934 <a title="131-lsi-8" href="./emnlp-2011-Accurate_Parsing_with_Compact_Tree-Substitution_Grammars%3A_Double-DOP.html">16 emnlp-2011-Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP</a></p>
<p>9 0.31392083 <a title="131-lsi-9" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>10 0.30963978 <a title="131-lsi-10" href="./emnlp-2011-The_Imagination_of_Crowds%3A_Conversational_AAC_Language_Modeling_using_Crowdsourcing_and_Large_Data_Sources.html">133 emnlp-2011-The Imagination of Crowds: Conversational AAC Language Modeling using Crowdsourcing and Large Data Sources</a></p>
<p>11 0.29983079 <a title="131-lsi-11" href="./emnlp-2011-Generating_Subsequent_Reference_in_Shared_Visual_Scenes%3A_Computation_vs_Re-Use.html">62 emnlp-2011-Generating Subsequent Reference in Shared Visual Scenes: Computation vs Re-Use</a></p>
<p>12 0.29321545 <a title="131-lsi-12" href="./emnlp-2011-Third-order_Variational_Reranking_on_Packed-Shared_Dependency_Forests.html">134 emnlp-2011-Third-order Variational Reranking on Packed-Shared Dependency Forests</a></p>
<p>13 0.28811297 <a title="131-lsi-13" href="./emnlp-2011-Watermarking_the_Outputs_of_Structured_Prediction_with_an_application_in_Statistical_Machine_Translation..html">148 emnlp-2011-Watermarking the Outputs of Structured Prediction with an application in Statistical Machine Translation.</a></p>
<p>14 0.28358802 <a title="131-lsi-14" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>15 0.28095034 <a title="131-lsi-15" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>16 0.27185899 <a title="131-lsi-16" href="./emnlp-2011-Inducing_Sentence_Structure_from_Parallel_Corpora_for_Reordering.html">74 emnlp-2011-Inducing Sentence Structure from Parallel Corpora for Reordering</a></p>
<p>17 0.27096665 <a title="131-lsi-17" href="./emnlp-2011-Unsupervised_Information_Extraction_with_Distributional_Prior_Knowledge.html">143 emnlp-2011-Unsupervised Information Extraction with Distributional Prior Knowledge</a></p>
<p>18 0.2594561 <a title="131-lsi-18" href="./emnlp-2011-Unsupervised_Dependency_Parsing_without_Gold_Part-of-Speech_Tags.html">141 emnlp-2011-Unsupervised Dependency Parsing without Gold Part-of-Speech Tags</a></p>
<p>19 0.24936759 <a title="131-lsi-19" href="./emnlp-2011-Statistical_Machine_Translation_with_Local_Language_Models.html">125 emnlp-2011-Statistical Machine Translation with Local Language Models</a></p>
<p>20 0.24500728 <a title="131-lsi-20" href="./emnlp-2011-A_Simple_Word_Trigger_Method_for_Social_Tag_Suggestion.html">11 emnlp-2011-A Simple Word Trigger Method for Social Tag Suggestion</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.071), (36, 0.025), (37, 0.054), (45, 0.043), (53, 0.036), (54, 0.041), (57, 0.434), (62, 0.013), (64, 0.018), (66, 0.046), (69, 0.012), (79, 0.033), (82, 0.017), (90, 0.015), (96, 0.034), (98, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.85663247 <a title="131-lda-1" href="./emnlp-2011-Relation_Acquisition_using_Word_Classes_and_Partial_Patterns.html">113 emnlp-2011-Relation Acquisition using Word Classes and Partial Patterns</a></p>
<p>Author: Stijn De Saeger ; Kentaro Torisawa ; Masaaki Tsuchida ; Jun'ichi Kazama ; Chikara Hashimoto ; Ichiro Yamada ; Jong Hoon Oh ; Istvan Varga ; Yulan Yan</p><p>Abstract: This paper proposes a semi-supervised relation acquisition method that does not rely on extraction patterns (e.g. “X causes Y” for causal relations) but instead learns a combination of indirect evidence for the target relation semantic word classes and partial patterns. This method can extract long tail instances of semantic relations like causality from rare and complex expressions in a large Japanese Web corpus in extreme cases, patterns that occur only once in the entire corpus. Such patterns are beyond the reach ofcurrent pattern based methods. We show that our method performs on par with state-of-the-art pattern based methods, and maintains a reasonable level of accuracy even for instances — — acquired from infrequent patterns. This ability to acquire long tail instances is crucial for risk management and innovation, where an exhaustive database of high-level semantic relations like causation is of vital importance.</p><p>same-paper 2 0.84365624 <a title="131-lda-2" href="./emnlp-2011-Syntactic_Decision_Tree_LMs%3A_Random_Selection_or_Intelligent_Design%3F.html">131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</a></p>
<p>Author: Denis Filimonov ; Mary Harper</p><p>Abstract: Decision trees have been applied to a variety of NLP tasks, including language modeling, for their ability to handle a variety of attributes and sparse context space. Moreover, forests (collections of decision trees) have been shown to substantially outperform individual decision trees. In this work, we investigate methods for combining trees in a forest, as well as methods for diversifying trees for the task of syntactic language modeling. We show that our tree interpolation technique outperforms the standard method used in the literature, and that, on this particular task, restricting tree contexts in a principled way produces smaller and better forests, with the best achieving an 8% relative reduction in Word Error Rate over an n-gram baseline.</p><p>3 0.83735108 <a title="131-lda-3" href="./emnlp-2011-Summarize_What_You_Are_Interested_In%3A_An_Optimization_Framework_for_Interactive_Personalized_Summarization.html">130 emnlp-2011-Summarize What You Are Interested In: An Optimization Framework for Interactive Personalized Summarization</a></p>
<p>Author: Rui Yan ; Jian-Yun Nie ; Xiaoming Li</p><p>Abstract: Most traditional summarization methods treat their outputs as static and plain texts, which fail to capture user interests during summarization because the generated summaries are the same for different users. However, users have individual preferences on a particular source document collection and obviously a universal summary for all users might not always be satisfactory. Hence we investigate an important and challenging problem in summary generation, i.e., Interactive Personalized Summarization (IPS), which generates summaries in an interactive and personalized manner. Given the source documents, IPS captures user interests by enabling interactive clicks and incorporates personalization by modeling captured reader preference. We develop . experimental systems to compare 5 rival algorithms on 4 instinctively different datasets which amount to 5197 documents. Evaluation results in ROUGE metrics indicate the comparable performance between IPS and the best competing system but IPS produces summaries with much more user satisfaction according to evaluator ratings. Besides, low ROUGE consistency among these user preferred summaries indicates the existence of personalization.</p><p>4 0.46743304 <a title="131-lda-4" href="./emnlp-2011-Large-Scale_Noun_Compound_Interpretation_Using_Bootstrapping_and_the_Web_as_a_Corpus.html">78 emnlp-2011-Large-Scale Noun Compound Interpretation Using Bootstrapping and the Web as a Corpus</a></p>
<p>Author: Su Nam Kim ; Preslav Nakov</p><p>Abstract: Responding to the need for semantic lexical resources in natural language processing applications, we examine methods to acquire noun compounds (NCs), e.g., orange juice, together with suitable fine-grained semantic interpretations, e.g., squeezed from, which are directly usable as paraphrases. We employ bootstrapping and web statistics, and utilize the relationship between NCs and paraphrasing patterns to jointly extract NCs and such patterns in multiple alternating iterations. In evaluation, we found that having one compound noun fixed yields both a higher number of semantically interpreted NCs and improved accuracy due to stronger semantic restrictions.</p><p>5 0.4264296 <a title="131-lda-5" href="./emnlp-2011-Extreme_Extraction_-_Machine_Reading_in_a_Week.html">57 emnlp-2011-Extreme Extraction - Machine Reading in a Week</a></p>
<p>Author: Marjorie Freedman ; Lance Ramshaw ; Elizabeth Boschee ; Ryan Gabbard ; Gary Kratkiewicz ; Nicolas Ward ; Ralph Weischedel</p><p>Abstract: We report on empirical results in extreme extraction. It is extreme in that (1) from receipt of the ontology specifying the target concepts and relations, development is limited to one week and that (2) relatively little training data is assumed. We are able to surpass human recall and achieve an F1 of 0.5 1 on a question-answering task with less than 50 hours of effort using a hybrid approach that mixes active learning, bootstrapping, and limited (5 hours) manual rule writing. We compare the performance of three systems: extraction with handwritten rules, bootstrapped extraction, and a combination. We show that while the recall of the handwritten rules surpasses that of the learned system, the learned system is able to improve the overall recall and F1.</p><p>6 0.38505292 <a title="131-lda-6" href="./emnlp-2011-Class_Label_Enhancement_via_Related_Instances.html">26 emnlp-2011-Class Label Enhancement via Related Instances</a></p>
<p>7 0.38403261 <a title="131-lda-7" href="./emnlp-2011-Unsupervised_Learning_of_Selectional_Restrictions_and_Detection_of_Argument_Coercions.html">144 emnlp-2011-Unsupervised Learning of Selectional Restrictions and Detection of Argument Coercions</a></p>
<p>8 0.3819544 <a title="131-lda-8" href="./emnlp-2011-Using_Syntactic_and_Semantic_Structural_Kernels_for_Classifying_Definition_Questions_in_Jeopardy%21.html">147 emnlp-2011-Using Syntactic and Semantic Structural Kernels for Classifying Definition Questions in Jeopardy!</a></p>
<p>9 0.37846419 <a title="131-lda-9" href="./emnlp-2011-Identifying_Relations_for_Open_Information_Extraction.html">70 emnlp-2011-Identifying Relations for Open Information Extraction</a></p>
<p>10 0.37717164 <a title="131-lda-10" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>11 0.35830143 <a title="131-lda-11" href="./emnlp-2011-Personalized_Recommendation_of_User_Comments_via_Factor_Models.html">104 emnlp-2011-Personalized Recommendation of User Comments via Factor Models</a></p>
<p>12 0.34690124 <a title="131-lda-12" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>13 0.34625039 <a title="131-lda-13" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>14 0.34488648 <a title="131-lda-14" href="./emnlp-2011-Hierarchical_Verb_Clustering_Using_Graph_Factorization.html">67 emnlp-2011-Hierarchical Verb Clustering Using Graph Factorization</a></p>
<p>15 0.34216753 <a title="131-lda-15" href="./emnlp-2011-Bootstrapped_Named_Entity_Recognition_for_Product_Attribute_Extraction.html">23 emnlp-2011-Bootstrapped Named Entity Recognition for Product Attribute Extraction</a></p>
<p>16 0.33926919 <a title="131-lda-16" href="./emnlp-2011-Predicting_Thread_Discourse_Structure_over_Technical_Web_Forums.html">105 emnlp-2011-Predicting Thread Discourse Structure over Technical Web Forums</a></p>
<p>17 0.33923572 <a title="131-lda-17" href="./emnlp-2011-Harnessing_different_knowledge_sources_to_measure_semantic_relatedness_under_a_uniform_model.html">64 emnlp-2011-Harnessing different knowledge sources to measure semantic relatedness under a uniform model</a></p>
<p>18 0.33618987 <a title="131-lda-18" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>19 0.33531824 <a title="131-lda-19" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>20 0.33259639 <a title="131-lda-20" href="./emnlp-2011-Splitting_Noun_Compounds_via_Monolingual_and_Bilingual_Paraphrasing%3A_A_Study_on_Japanese_Katakana_Words.html">124 emnlp-2011-Splitting Noun Compounds via Monolingual and Bilingual Paraphrasing: A Study on Japanese Katakana Words</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
