<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>130 emnlp-2011-Summarize What You Are Interested In: An Optimization Framework for Interactive Personalized Summarization</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-130" href="#">emnlp2011-130</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>130 emnlp-2011-Summarize What You Are Interested In: An Optimization Framework for Interactive Personalized Summarization</h1>
<br/><p>Source: <a title="emnlp-2011-130-pdf" href="http://aclweb.org/anthology//D/D11/D11-1124.pdf">pdf</a></p><p>Author: Rui Yan ; Jian-Yun Nie ; Xiaoming Li</p><p>Abstract: Most traditional summarization methods treat their outputs as static and plain texts, which fail to capture user interests during summarization because the generated summaries are the same for different users. However, users have individual preferences on a particular source document collection and obviously a universal summary for all users might not always be satisfactory. Hence we investigate an important and challenging problem in summary generation, i.e., Interactive Personalized Summarization (IPS), which generates summaries in an interactive and personalized manner. Given the source documents, IPS captures user interests by enabling interactive clicks and incorporates personalization by modeling captured reader preference. We develop . experimental systems to compare 5 rival algorithms on 4 instinctively different datasets which amount to 5197 documents. Evaluation results in ROUGE metrics indicate the comparable performance between IPS and the best competing system but IPS produces summaries with much more user satisfaction according to evaluator ratings. Besides, low ROUGE consistency among these user preferred summaries indicates the existence of personalization.</p><p>Reference: <a title="emnlp-2011-130-reference" href="../emnlp2011_reference/emnlp-2011-Summarize_What_You_Are_Interested_In%3A_An_Optimization_Framework_for_Interactive_Personalized_Summarization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn  nie @ i ro  Abstract Most traditional summarization methods treat their outputs as static and plain texts, which fail to capture user interests during summarization because the generated summaries are the same for different users. [sent-4, score-1.659]
</p><p>2 However, users have individual preferences on a particular source document collection and obviously a universal summary for all users might not always be satisfactory. [sent-5, score-0.363]
</p><p>3 Hence we investigate an important and challenging problem in summary generation, i. [sent-6, score-0.149]
</p><p>4 , Interactive Personalized Summarization (IPS), which generates summaries in an interactive and personalized manner. [sent-8, score-0.532]
</p><p>5 Given the source documents, IPS captures user interests by enabling interactive clicks and incorporates personalization by modeling captured reader preference. [sent-9, score-1.198]
</p><p>6 experimental systems to compare 5 rival algorithms on 4 instinctively different datasets which amount to 5197 documents. [sent-11, score-0.081]
</p><p>7 Evaluation results in ROUGE metrics indicate the comparable performance between IPS and the best competing system but IPS produces summaries with much more user satisfaction according to evaluator ratings. [sent-12, score-0.521]
</p><p>8 Besides, low ROUGE consistency among these user preferred summaries indicates the existence of personalization. [sent-13, score-0.443]
</p><p>9 cn  livering the majority of information content from a document set, and hence is a necessity. [sent-17, score-0.083]
</p><p>10 Traditional summarization methods play an important role with the exponential document growth on the Web. [sent-18, score-0.329]
</p><p>11 However, for the readers, the impact of human interests has seldom been considered. [sent-19, score-0.434]
</p><p>12 Traditional summarization utilizes the same methodology to generate the same summary no matter who is reading. [sent-20, score-0.474]
</p><p>13 However, users may have bias on what they prefer to read due to their potential interests: they  need personalization. [sent-21, score-0.136]
</p><p>14 Therefore, traditional summarization methods are to some extent insufficient. [sent-22, score-0.375]
</p><p>15 Topic biased summarization tries for personalization by pre-defining human interests as several general categories, such as health or science. [sent-23, score-1.021]
</p><p>16 Readers are required to select their possible interests before summary generation so that the chosen topic has priority during summarization. [sent-24, score-0.636]
</p><p>17 Unfortunately, such topic biased summarization is not sufficient for two reasons: (1) interests cannot usually be accurately pre-defined by ambiguous topic categories and (2) user interests cannot always be foreknown. [sent-25, score-1.675]
</p><p>18 Often users do not really know what general ideas or detail information they are interested in until they read the summaries. [sent-26, score-0.195]
</p><p>19 Therefore, more flexible interactions are required to establish personalization. [sent-27, score-0.048]
</p><p>20 Due to all the insufficiencies of existed summarization approaches, we introduce a new multidocument summarization task of Interactive PersonIn the era of information explosion, people need new information to update their knowledge whilst information on Web is updating extremely fast. [sent-28, score-0.781]
</p><p>21 Multidocument summarization has been proposed to address such dilemma by producing a summary de1342  alized Summarization (IPS) and a novel solution for the task. [sent-29, score-0.484]
</p><p>22 Taking a document collection as input, the system outputs a summary aligned both with source corpus and with user personalization, which is captured by flexible human−system interactions. [sent-30, score-0.531]
</p><p>23 The contribution of IPS is manifold  by addressing following challenges: • The 1st challenge for IPS is to integrate user inte•re Tshtse i 1nstot cthraadlilteinongael summary components. [sent-34, score-0.432]
</p><p>24 We formulate the task into a balanced optimization framework via iterative substitution to generate summaries with maximum overall utilities. [sent-36, score-0.242]
</p><p>25 • The 2nd challenge is to capture user interests through dint cehraaclltieonng. [sent-37, score-0.629]
</p><p>26 e W ise develop an uinsteerra incttievremechanism of “click” and “examine” between readers and summaries and address sparse data by “click smoothing” under the scenario of few user clicks. [sent-38, score-0.62]
</p><p>27 In Section 3 we provide IPS overview, describe user interaction and optimize component combination with personalization. [sent-40, score-0.275]
</p><p>28 2  Related Work  Multi-Document Summarization (MDS) has drawn much attention in recent years and gained emphasis in conferences such as ACL, EMNLP and SIGIR, etc. [sent-43, score-0.037]
</p><p>29 General MDS can either be extractive or ab-  stractive. [sent-44, score-0.119]
</p><p>30 sentences, paragraphs) of the documents indicating their importance and then extracts top ranked ones, while the latter demands information fusion(e. [sent-47, score-0.034]
</p><p>31 Centroid-based method is one of the most popular extractive summarization method. [sent-51, score-0.414]
</p><p>32 Most recently, the graph-based ranking methods have been proposed to rank sentences or passages based on the “votes” or “recommendations” between each other. [sent-56, score-0.04]
</p><p>33 The graphbased methods first construct a graph representing the sentence relationships at different granularities and then evaluate the saliency score of the sentences based on the graph. [sent-57, score-0.109]
</p><p>34 improve the graph-ranking algorithm by differentiating intradocument and inter-document links between sen-  tences (2007b) and incorporate cluster information in the graph model to evaluate sentences (2008). [sent-60, score-0.07]
</p><p>35 To date, topics (or themes, clusters) in documents have been discovered and used for sentence selection for topic biased summarization (Wan and Yang, 2008; Gong and Liu, 2001). [sent-61, score-0.514]
</p><p>36 have proposed a manifold-ranking method to make uniform use of sentence-to-sentence and sentence-totopic relationships to generate topic biased summaries (2007a). [sent-63, score-0.458]
</p><p>37 in (2003) pre-define several topic concepts, assuming users will foresee their interested topics and then generate the topic biased summary. [sent-65, score-0.465]
</p><p>38 However, such assumption is not quite reasonable because user interests may not be forecasted, or pre-defined accurately as we have explained in last section. [sent-66, score-0.674]
</p><p>39 The above algorithms are usually traditional extensions of generic summarizers. [sent-67, score-0.08]
</p><p>40 They do not involve interactive mechanisms to capture reader interests, nor do they utilize user preference for personalization in summarization. [sent-68, score-0.785]
</p><p>41 in (2008) have proposed a summarization biased to neighboring reading context through anchor texts. [sent-70, score-0.482]
</p><p>42 However, such scenario does not apply to contexts with-  out human-edited anchor texts like Wikipedia they have used. [sent-71, score-0.102]
</p><p>43 Our approach can naturally and simultaneously take into account traditional summary elements and user interests and combine both in optimization under a wider practical scenario. [sent-72, score-0.896]
</p><p>44 3  Interactive Personalized Summarization  Personalization based on user preference can be captured via various alternative ways, such as eyetracking or mouse-tracking instruments used in (Guo and Agichtein, 2010). [sent-73, score-0.456]
</p><p>45 In this study, we utilize interactive user clicks/examinations for personalization. [sent-74, score-0.461]
</p><p>46 Unlike traditional summarization, IPS supports human−system interaction by clicking into the summary nse−nsteynstceems ianntde examining source c ionntote txhtes. [sent-75, score-0.309]
</p><p>47 s Tumheimplicit feedback of user clicks indicates what they are interested in and the system collects preference information to update summaries if readers wish to. [sent-76, score-0.843]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ips', 0.456), ('interests', 0.39), ('summarization', 0.295), ('user', 0.239), ('summaries', 0.204), ('personalization', 0.183), ('interactive', 0.181), ('summary', 0.149), ('personalized', 0.147), ('wan', 0.138), ('biased', 0.122), ('extractive', 0.119), ('readers', 0.109), ('clicks', 0.101), ('montr', 0.101), ('topic', 0.097), ('users', 0.09), ('multidocument', 0.087), ('eal', 0.087), ('peking', 0.087), ('pku', 0.087), ('traditional', 0.08), ('mds', 0.079), ('nie', 0.073), ('click', 0.069), ('preference', 0.068), ('anchor', 0.065), ('rouge', 0.065), ('radev', 0.062), ('captured', 0.061), ('interested', 0.059), ('yan', 0.053), ('cn', 0.049), ('flexible', 0.048), ('read', 0.046), ('accurately', 0.045), ('eyetracking', 0.044), ('clicking', 0.044), ('leuski', 0.044), ('evaluator', 0.044), ('manifold', 0.044), ('forecasted', 0.044), ('instruments', 0.044), ('seldom', 0.044), ('gong', 0.044), ('recherche', 0.044), ('instinctively', 0.044), ('lexpagerank', 0.044), ('mmr', 0.044), ('neats', 0.044), ('personin', 0.044), ('saliency', 0.044), ('xiaoming', 0.044), ('reader', 0.043), ('beijing', 0.041), ('utilize', 0.041), ('tarau', 0.04), ('textrank', 0.04), ('erkan', 0.04), ('dilemma', 0.04), ('scotland', 0.04), ('tences', 0.04), ('passages', 0.04), ('china', 0.038), ('optimization', 0.038), ('scenario', 0.037), ('rival', 0.037), ('qu', 0.037), ('era', 0.037), ('guo', 0.037), ('themes', 0.037), ('reformulation', 0.037), ('conferences', 0.037), ('rui', 0.037), ('mead', 0.037), ('fusion', 0.037), ('whilst', 0.037), ('interaction', 0.036), ('relationships', 0.035), ('agichtein', 0.034), ('reviewing', 0.034), ('static', 0.034), ('demands', 0.034), ('paragraphs', 0.034), ('satisfaction', 0.034), ('document', 0.034), ('pagerank', 0.033), ('collects', 0.033), ('goldstein', 0.033), ('op', 0.031), ('implementations', 0.031), ('votes', 0.031), ('ise', 0.031), ('health', 0.031), ('update', 0.03), ('universit', 0.03), ('graphbased', 0.03), ('utilizes', 0.03), ('mechanisms', 0.03), ('differentiating', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="130-tfidf-1" href="./emnlp-2011-Summarize_What_You_Are_Interested_In%3A_An_Optimization_Framework_for_Interactive_Personalized_Summarization.html">130 emnlp-2011-Summarize What You Are Interested In: An Optimization Framework for Interactive Personalized Summarization</a></p>
<p>Author: Rui Yan ; Jian-Yun Nie ; Xiaoming Li</p><p>Abstract: Most traditional summarization methods treat their outputs as static and plain texts, which fail to capture user interests during summarization because the generated summaries are the same for different users. However, users have individual preferences on a particular source document collection and obviously a universal summary for all users might not always be satisfactory. Hence we investigate an important and challenging problem in summary generation, i.e., Interactive Personalized Summarization (IPS), which generates summaries in an interactive and personalized manner. Given the source documents, IPS captures user interests by enabling interactive clicks and incorporates personalization by modeling captured reader preference. We develop . experimental systems to compare 5 rival algorithms on 4 instinctively different datasets which amount to 5197 documents. Evaluation results in ROUGE metrics indicate the comparable performance between IPS and the best competing system but IPS produces summaries with much more user satisfaction according to evaluator ratings. Besides, low ROUGE consistency among these user preferred summaries indicates the existence of personalization.</p><p>2 0.24681042 <a title="130-tfidf-2" href="./emnlp-2011-Timeline_Generation_through_Evolutionary_Trans-Temporal_Summarization.html">135 emnlp-2011-Timeline Generation through Evolutionary Trans-Temporal Summarization</a></p>
<p>Author: Rui Yan ; Liang Kong ; Congrui Huang ; Xiaojun Wan ; Xiaoming Li ; Yan Zhang</p><p>Abstract: We investigate an important and challenging problem in summary generation, i.e., Evolutionary Trans-Temporal Summarization (ETTS), which generates news timelines from massive data on the Internet. ETTS greatly facilitates fast news browsing and knowledge comprehension, and hence is a necessity. Given the collection oftime-stamped web documents related to the evolving news, ETTS aims to return news evolution along the timeline, consisting of individual but correlated summaries on each date. Existing summarization algorithms fail to utilize trans-temporal characteristics among these component summaries. We propose to model trans-temporal correlations among component summaries for timelines, using inter-date and intra-date sen- tence dependencies, and present a novel combination. We develop experimental systems to compare 5 rival algorithms on 6 instinctively different datasets which amount to 10251 documents. Evaluation results in ROUGE metrics indicate the effectiveness of the proposed approach based on trans-temporal information. 1</p><p>3 0.14311831 <a title="130-tfidf-3" href="./emnlp-2011-Generating_Aspect-oriented_Multi-Document_Summarization_with_Event-aspect_model.html">61 emnlp-2011-Generating Aspect-oriented Multi-Document Summarization with Event-aspect model</a></p>
<p>Author: Peng Li ; Yinglin Wang ; Wei Gao ; Jing Jiang</p><p>Abstract: In this paper, we propose a novel approach to automatic generation of aspect-oriented summaries from multiple documents. We first develop an event-aspect LDA model to cluster sentences into aspects. We then use extended LexRank algorithm to rank the sentences in each cluster. We use Integer Linear Programming for sentence selection. Key features of our method include automatic grouping of semantically related sentences and sentence ranking based on extension of random walk model. Also, we implement a new sentence compression algorithm which use dependency tree instead of parser tree. We compare our method with four baseline methods. Quantitative evaluation based on Rouge metric demonstrates the effectiveness and advantages of our method.</p><p>4 0.1225092 <a title="130-tfidf-4" href="./emnlp-2011-Personalized_Recommendation_of_User_Comments_via_Factor_Models.html">104 emnlp-2011-Personalized Recommendation of User Comments via Factor Models</a></p>
<p>Author: Deepak Agarwal ; Bee-Chung Chen ; Bo Pang</p><p>Abstract: In recent years, the amount of user-generated opinionated texts (e.g., reviews, user comments) continues to grow at a rapid speed: featured news stories on a major event easily attract thousands of user comments on a popular online News service. How to consume subjective information ofthis volume becomes an interesting and important research question. In contrast to previous work on review analysis that tried to filter or summarize information for a generic average user, we explore a different direction of enabling personalized recommendation of such information. For each user, our task is to rank the comments associated with a given article according to personalized user preference (i.e., whether the user is likely to like or dislike the comment). To this end, we propose a factor model that incorporates rater-comment and rater-author interactions simultaneously in a principled way. Our full model significantly outperforms strong baselines as well as related models that have been considered in previous work.</p><p>5 0.088877797 <a title="130-tfidf-5" href="./emnlp-2011-Bootstrapping_Semantic_Parsers_from_Conversations.html">24 emnlp-2011-Bootstrapping Semantic Parsers from Conversations</a></p>
<p>Author: Yoav Artzi ; Luke Zettlemoyer</p><p>Abstract: Conversations provide rich opportunities for interactive, continuous learning. When something goes wrong, a system can ask for clarification, rewording, or otherwise redirect the interaction to achieve its goals. In this paper, we present an approach for using conversational interactions of this type to induce semantic parsers. We demonstrate learning without any explicit annotation of the meanings of user utterances. Instead, we model meaning with latent variables, and introduce a loss function to measure how well potential meanings match the conversation. This loss drives the overall learning approach, which induces a weighted CCG grammar that could be used to automatically bootstrap the semantic analysis component in a complete dialog system. Experiments on DARPA Communicator conversational logs demonstrate effective learning, despite requiring no explicit mean- . ing annotations.</p><p>6 0.083634786 <a title="130-tfidf-6" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>7 0.080206901 <a title="130-tfidf-7" href="./emnlp-2011-Discriminating_Gender_on_Twitter.html">41 emnlp-2011-Discriminating Gender on Twitter</a></p>
<p>8 0.077494778 <a title="130-tfidf-8" href="./emnlp-2011-Ranking_Human_and_Machine_Summarization_Systems.html">110 emnlp-2011-Ranking Human and Machine Summarization Systems</a></p>
<p>9 0.075241096 <a title="130-tfidf-9" href="./emnlp-2011-Optimizing_Semantic_Coherence_in_Topic_Models.html">101 emnlp-2011-Optimizing Semantic Coherence in Topic Models</a></p>
<p>10 0.070418924 <a title="130-tfidf-10" href="./emnlp-2011-Semantic_Topic_Models%3A_Combining_Word_Distributional_Statistics_and_Dictionary_Definitions.html">119 emnlp-2011-Semantic Topic Models: Combining Word Distributional Statistics and Dictionary Definitions</a></p>
<p>11 0.063997947 <a title="130-tfidf-11" href="./emnlp-2011-Bayesian_Checking_for_Topic_Models.html">21 emnlp-2011-Bayesian Checking for Topic Models</a></p>
<p>12 0.053577654 <a title="130-tfidf-12" href="./emnlp-2011-Syntax-Based_Grammaticality_Improvement_using_CCG_and_Guided_Search.html">132 emnlp-2011-Syntax-Based Grammaticality Improvement using CCG and Guided Search</a></p>
<p>13 0.04558767 <a title="130-tfidf-13" href="./emnlp-2011-Corroborating_Text_Evaluation_Results_with_Heterogeneous_Measures.html">36 emnlp-2011-Corroborating Text Evaluation Results with Heterogeneous Measures</a></p>
<p>14 0.039941236 <a title="130-tfidf-14" href="./emnlp-2011-Cache-based_Document-level_Statistical_Machine_Translation.html">25 emnlp-2011-Cache-based Document-level Statistical Machine Translation</a></p>
<p>15 0.037966419 <a title="130-tfidf-15" href="./emnlp-2011-Rumor_has_it%3A_Identifying_Misinformation_in_Microblogs.html">117 emnlp-2011-Rumor has it: Identifying Misinformation in Microblogs</a></p>
<p>16 0.03583654 <a title="130-tfidf-16" href="./emnlp-2011-A_Simple_Word_Trigger_Method_for_Social_Tag_Suggestion.html">11 emnlp-2011-A Simple Word Trigger Method for Social Tag Suggestion</a></p>
<p>17 0.035723388 <a title="130-tfidf-17" href="./emnlp-2011-Data-Driven_Response_Generation_in_Social_Media.html">38 emnlp-2011-Data-Driven Response Generation in Social Media</a></p>
<p>18 0.033290617 <a title="130-tfidf-18" href="./emnlp-2011-Predicting_Thread_Discourse_Structure_over_Technical_Web_Forums.html">105 emnlp-2011-Predicting Thread Discourse Structure over Technical Web Forums</a></p>
<p>19 0.031805396 <a title="130-tfidf-19" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>20 0.030906081 <a title="130-tfidf-20" href="./emnlp-2011-Relation_Extraction_with_Relation_Topics.html">114 emnlp-2011-Relation Extraction with Relation Topics</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.124), (1, -0.11), (2, 0.008), (3, -0.151), (4, -0.024), (5, 0.093), (6, 0.003), (7, -0.073), (8, 0.069), (9, -0.019), (10, -0.139), (11, -0.278), (12, -0.157), (13, 0.026), (14, 0.353), (15, 0.063), (16, -0.212), (17, 0.009), (18, -0.007), (19, 0.129), (20, 0.09), (21, -0.006), (22, -0.051), (23, -0.109), (24, -0.037), (25, 0.015), (26, 0.1), (27, -0.005), (28, -0.08), (29, 0.126), (30, 0.024), (31, -0.046), (32, 0.153), (33, -0.022), (34, 0.034), (35, 0.109), (36, -0.068), (37, 0.084), (38, -0.02), (39, -0.074), (40, 0.076), (41, -0.033), (42, -0.178), (43, -0.025), (44, -0.12), (45, 0.059), (46, 0.134), (47, 0.08), (48, 0.062), (49, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98348963 <a title="130-lsi-1" href="./emnlp-2011-Summarize_What_You_Are_Interested_In%3A_An_Optimization_Framework_for_Interactive_Personalized_Summarization.html">130 emnlp-2011-Summarize What You Are Interested In: An Optimization Framework for Interactive Personalized Summarization</a></p>
<p>Author: Rui Yan ; Jian-Yun Nie ; Xiaoming Li</p><p>Abstract: Most traditional summarization methods treat their outputs as static and plain texts, which fail to capture user interests during summarization because the generated summaries are the same for different users. However, users have individual preferences on a particular source document collection and obviously a universal summary for all users might not always be satisfactory. Hence we investigate an important and challenging problem in summary generation, i.e., Interactive Personalized Summarization (IPS), which generates summaries in an interactive and personalized manner. Given the source documents, IPS captures user interests by enabling interactive clicks and incorporates personalization by modeling captured reader preference. We develop . experimental systems to compare 5 rival algorithms on 4 instinctively different datasets which amount to 5197 documents. Evaluation results in ROUGE metrics indicate the comparable performance between IPS and the best competing system but IPS produces summaries with much more user satisfaction according to evaluator ratings. Besides, low ROUGE consistency among these user preferred summaries indicates the existence of personalization.</p><p>2 0.85240495 <a title="130-lsi-2" href="./emnlp-2011-Timeline_Generation_through_Evolutionary_Trans-Temporal_Summarization.html">135 emnlp-2011-Timeline Generation through Evolutionary Trans-Temporal Summarization</a></p>
<p>Author: Rui Yan ; Liang Kong ; Congrui Huang ; Xiaojun Wan ; Xiaoming Li ; Yan Zhang</p><p>Abstract: We investigate an important and challenging problem in summary generation, i.e., Evolutionary Trans-Temporal Summarization (ETTS), which generates news timelines from massive data on the Internet. ETTS greatly facilitates fast news browsing and knowledge comprehension, and hence is a necessity. Given the collection oftime-stamped web documents related to the evolving news, ETTS aims to return news evolution along the timeline, consisting of individual but correlated summaries on each date. Existing summarization algorithms fail to utilize trans-temporal characteristics among these component summaries. We propose to model trans-temporal correlations among component summaries for timelines, using inter-date and intra-date sen- tence dependencies, and present a novel combination. We develop experimental systems to compare 5 rival algorithms on 6 instinctively different datasets which amount to 10251 documents. Evaluation results in ROUGE metrics indicate the effectiveness of the proposed approach based on trans-temporal information. 1</p><p>3 0.46941733 <a title="130-lsi-3" href="./emnlp-2011-Personalized_Recommendation_of_User_Comments_via_Factor_Models.html">104 emnlp-2011-Personalized Recommendation of User Comments via Factor Models</a></p>
<p>Author: Deepak Agarwal ; Bee-Chung Chen ; Bo Pang</p><p>Abstract: In recent years, the amount of user-generated opinionated texts (e.g., reviews, user comments) continues to grow at a rapid speed: featured news stories on a major event easily attract thousands of user comments on a popular online News service. How to consume subjective information ofthis volume becomes an interesting and important research question. In contrast to previous work on review analysis that tried to filter or summarize information for a generic average user, we explore a different direction of enabling personalized recommendation of such information. For each user, our task is to rank the comments associated with a given article according to personalized user preference (i.e., whether the user is likely to like or dislike the comment). To this end, we propose a factor model that incorporates rater-comment and rater-author interactions simultaneously in a principled way. Our full model significantly outperforms strong baselines as well as related models that have been considered in previous work.</p><p>4 0.43673399 <a title="130-lsi-4" href="./emnlp-2011-Ranking_Human_and_Machine_Summarization_Systems.html">110 emnlp-2011-Ranking Human and Machine Summarization Systems</a></p>
<p>Author: Peter Rankel ; John Conroy ; Eric Slud ; Dianne O'Leary</p><p>Abstract: The Text Analysis Conference (TAC) ranks summarization systems by their average score over a collection of document sets. We investigate the statistical appropriateness of this score and propose an alternative that better distinguishes between human and machine evaluation systems.</p><p>5 0.41574639 <a title="130-lsi-5" href="./emnlp-2011-Generating_Aspect-oriented_Multi-Document_Summarization_with_Event-aspect_model.html">61 emnlp-2011-Generating Aspect-oriented Multi-Document Summarization with Event-aspect model</a></p>
<p>Author: Peng Li ; Yinglin Wang ; Wei Gao ; Jing Jiang</p><p>Abstract: In this paper, we propose a novel approach to automatic generation of aspect-oriented summaries from multiple documents. We first develop an event-aspect LDA model to cluster sentences into aspects. We then use extended LexRank algorithm to rank the sentences in each cluster. We use Integer Linear Programming for sentence selection. Key features of our method include automatic grouping of semantically related sentences and sentence ranking based on extension of random walk model. Also, we implement a new sentence compression algorithm which use dependency tree instead of parser tree. We compare our method with four baseline methods. Quantitative evaluation based on Rouge metric demonstrates the effectiveness and advantages of our method.</p><p>6 0.29650116 <a title="130-lsi-6" href="./emnlp-2011-Bootstrapping_Semantic_Parsers_from_Conversations.html">24 emnlp-2011-Bootstrapping Semantic Parsers from Conversations</a></p>
<p>7 0.24738403 <a title="130-lsi-7" href="./emnlp-2011-Discriminating_Gender_on_Twitter.html">41 emnlp-2011-Discriminating Gender on Twitter</a></p>
<p>8 0.24538128 <a title="130-lsi-8" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>9 0.17778565 <a title="130-lsi-9" href="./emnlp-2011-Bayesian_Checking_for_Topic_Models.html">21 emnlp-2011-Bayesian Checking for Topic Models</a></p>
<p>10 0.1766852 <a title="130-lsi-10" href="./emnlp-2011-Optimizing_Semantic_Coherence_in_Topic_Models.html">101 emnlp-2011-Optimizing Semantic Coherence in Topic Models</a></p>
<p>11 0.17607184 <a title="130-lsi-11" href="./emnlp-2011-Rumor_has_it%3A_Identifying_Misinformation_in_Microblogs.html">117 emnlp-2011-Rumor has it: Identifying Misinformation in Microblogs</a></p>
<p>12 0.16581282 <a title="130-lsi-12" href="./emnlp-2011-The_Imagination_of_Crowds%3A_Conversational_AAC_Language_Modeling_using_Crowdsourcing_and_Large_Data_Sources.html">133 emnlp-2011-The Imagination of Crowds: Conversational AAC Language Modeling using Crowdsourcing and Large Data Sources</a></p>
<p>13 0.15836251 <a title="130-lsi-13" href="./emnlp-2011-Semantic_Topic_Models%3A_Combining_Word_Distributional_Statistics_and_Dictionary_Definitions.html">119 emnlp-2011-Semantic Topic Models: Combining Word Distributional Statistics and Dictionary Definitions</a></p>
<p>14 0.15545602 <a title="130-lsi-14" href="./emnlp-2011-Corroborating_Text_Evaluation_Results_with_Heterogeneous_Measures.html">36 emnlp-2011-Corroborating Text Evaluation Results with Heterogeneous Measures</a></p>
<p>15 0.14784281 <a title="130-lsi-15" href="./emnlp-2011-A_Simple_Word_Trigger_Method_for_Social_Tag_Suggestion.html">11 emnlp-2011-A Simple Word Trigger Method for Social Tag Suggestion</a></p>
<p>16 0.14398354 <a title="130-lsi-16" href="./emnlp-2011-Syntax-Based_Grammaticality_Improvement_using_CCG_and_Guided_Search.html">132 emnlp-2011-Syntax-Based Grammaticality Improvement using CCG and Guided Search</a></p>
<p>17 0.12302414 <a title="130-lsi-17" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>18 0.12033325 <a title="130-lsi-18" href="./emnlp-2011-Predicting_Thread_Discourse_Structure_over_Technical_Web_Forums.html">105 emnlp-2011-Predicting Thread Discourse Structure over Technical Web Forums</a></p>
<p>19 0.11252128 <a title="130-lsi-19" href="./emnlp-2011-Cache-based_Document-level_Statistical_Machine_Translation.html">25 emnlp-2011-Cache-based Document-level Statistical Machine Translation</a></p>
<p>20 0.10904858 <a title="130-lsi-20" href="./emnlp-2011-A_Non-negative_Matrix_Factorization_Based_Approach_for_Active_Dual_Supervision_from_Document_and_Word_Labels.html">9 emnlp-2011-A Non-negative Matrix Factorization Based Approach for Active Dual Supervision from Document and Word Labels</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.187), (36, 0.012), (37, 0.012), (45, 0.053), (57, 0.53), (62, 0.012), (64, 0.013), (66, 0.01), (79, 0.024), (96, 0.027), (98, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91748744 <a title="130-lda-1" href="./emnlp-2011-Summarize_What_You_Are_Interested_In%3A_An_Optimization_Framework_for_Interactive_Personalized_Summarization.html">130 emnlp-2011-Summarize What You Are Interested In: An Optimization Framework for Interactive Personalized Summarization</a></p>
<p>Author: Rui Yan ; Jian-Yun Nie ; Xiaoming Li</p><p>Abstract: Most traditional summarization methods treat their outputs as static and plain texts, which fail to capture user interests during summarization because the generated summaries are the same for different users. However, users have individual preferences on a particular source document collection and obviously a universal summary for all users might not always be satisfactory. Hence we investigate an important and challenging problem in summary generation, i.e., Interactive Personalized Summarization (IPS), which generates summaries in an interactive and personalized manner. Given the source documents, IPS captures user interests by enabling interactive clicks and incorporates personalization by modeling captured reader preference. We develop . experimental systems to compare 5 rival algorithms on 4 instinctively different datasets which amount to 5197 documents. Evaluation results in ROUGE metrics indicate the comparable performance between IPS and the best competing system but IPS produces summaries with much more user satisfaction according to evaluator ratings. Besides, low ROUGE consistency among these user preferred summaries indicates the existence of personalization.</p><p>2 0.8936246 <a title="130-lda-2" href="./emnlp-2011-Relation_Acquisition_using_Word_Classes_and_Partial_Patterns.html">113 emnlp-2011-Relation Acquisition using Word Classes and Partial Patterns</a></p>
<p>Author: Stijn De Saeger ; Kentaro Torisawa ; Masaaki Tsuchida ; Jun'ichi Kazama ; Chikara Hashimoto ; Ichiro Yamada ; Jong Hoon Oh ; Istvan Varga ; Yulan Yan</p><p>Abstract: This paper proposes a semi-supervised relation acquisition method that does not rely on extraction patterns (e.g. “X causes Y” for causal relations) but instead learns a combination of indirect evidence for the target relation semantic word classes and partial patterns. This method can extract long tail instances of semantic relations like causality from rare and complex expressions in a large Japanese Web corpus in extreme cases, patterns that occur only once in the entire corpus. Such patterns are beyond the reach ofcurrent pattern based methods. We show that our method performs on par with state-of-the-art pattern based methods, and maintains a reasonable level of accuracy even for instances — — acquired from infrequent patterns. This ability to acquire long tail instances is crucial for risk management and innovation, where an exhaustive database of high-level semantic relations like causation is of vital importance.</p><p>3 0.85446954 <a title="130-lda-3" href="./emnlp-2011-Syntactic_Decision_Tree_LMs%3A_Random_Selection_or_Intelligent_Design%3F.html">131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</a></p>
<p>Author: Denis Filimonov ; Mary Harper</p><p>Abstract: Decision trees have been applied to a variety of NLP tasks, including language modeling, for their ability to handle a variety of attributes and sparse context space. Moreover, forests (collections of decision trees) have been shown to substantially outperform individual decision trees. In this work, we investigate methods for combining trees in a forest, as well as methods for diversifying trees for the task of syntactic language modeling. We show that our tree interpolation technique outperforms the standard method used in the literature, and that, on this particular task, restricting tree contexts in a principled way produces smaller and better forests, with the best achieving an 8% relative reduction in Word Error Rate over an n-gram baseline.</p><p>4 0.51641512 <a title="130-lda-4" href="./emnlp-2011-Large-Scale_Noun_Compound_Interpretation_Using_Bootstrapping_and_the_Web_as_a_Corpus.html">78 emnlp-2011-Large-Scale Noun Compound Interpretation Using Bootstrapping and the Web as a Corpus</a></p>
<p>Author: Su Nam Kim ; Preslav Nakov</p><p>Abstract: Responding to the need for semantic lexical resources in natural language processing applications, we examine methods to acquire noun compounds (NCs), e.g., orange juice, together with suitable fine-grained semantic interpretations, e.g., squeezed from, which are directly usable as paraphrases. We employ bootstrapping and web statistics, and utilize the relationship between NCs and paraphrasing patterns to jointly extract NCs and such patterns in multiple alternating iterations. In evaluation, we found that having one compound noun fixed yields both a higher number of semantically interpreted NCs and improved accuracy due to stronger semantic restrictions.</p><p>5 0.49529898 <a title="130-lda-5" href="./emnlp-2011-Extreme_Extraction_-_Machine_Reading_in_a_Week.html">57 emnlp-2011-Extreme Extraction - Machine Reading in a Week</a></p>
<p>Author: Marjorie Freedman ; Lance Ramshaw ; Elizabeth Boschee ; Ryan Gabbard ; Gary Kratkiewicz ; Nicolas Ward ; Ralph Weischedel</p><p>Abstract: We report on empirical results in extreme extraction. It is extreme in that (1) from receipt of the ontology specifying the target concepts and relations, development is limited to one week and that (2) relatively little training data is assumed. We are able to surpass human recall and achieve an F1 of 0.5 1 on a question-answering task with less than 50 hours of effort using a hybrid approach that mixes active learning, bootstrapping, and limited (5 hours) manual rule writing. We compare the performance of three systems: extraction with handwritten rules, bootstrapped extraction, and a combination. We show that while the recall of the handwritten rules surpasses that of the learned system, the learned system is able to improve the overall recall and F1.</p><p>6 0.45042011 <a title="130-lda-6" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>7 0.44954523 <a title="130-lda-7" href="./emnlp-2011-Class_Label_Enhancement_via_Related_Instances.html">26 emnlp-2011-Class Label Enhancement via Related Instances</a></p>
<p>8 0.44799238 <a title="130-lda-8" href="./emnlp-2011-Identifying_Relations_for_Open_Information_Extraction.html">70 emnlp-2011-Identifying Relations for Open Information Extraction</a></p>
<p>9 0.43520898 <a title="130-lda-9" href="./emnlp-2011-Using_Syntactic_and_Semantic_Structural_Kernels_for_Classifying_Definition_Questions_in_Jeopardy%21.html">147 emnlp-2011-Using Syntactic and Semantic Structural Kernels for Classifying Definition Questions in Jeopardy!</a></p>
<p>10 0.43168044 <a title="130-lda-10" href="./emnlp-2011-Personalized_Recommendation_of_User_Comments_via_Factor_Models.html">104 emnlp-2011-Personalized Recommendation of User Comments via Factor Models</a></p>
<p>11 0.42948633 <a title="130-lda-11" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>12 0.41979763 <a title="130-lda-12" href="./emnlp-2011-Timeline_Generation_through_Evolutionary_Trans-Temporal_Summarization.html">135 emnlp-2011-Timeline Generation through Evolutionary Trans-Temporal Summarization</a></p>
<p>13 0.4176057 <a title="130-lda-13" href="./emnlp-2011-Splitting_Noun_Compounds_via_Monolingual_and_Bilingual_Paraphrasing%3A_A_Study_on_Japanese_Katakana_Words.html">124 emnlp-2011-Splitting Noun Compounds via Monolingual and Bilingual Paraphrasing: A Study on Japanese Katakana Words</a></p>
<p>14 0.41574904 <a title="130-lda-14" href="./emnlp-2011-Bootstrapped_Named_Entity_Recognition_for_Product_Attribute_Extraction.html">23 emnlp-2011-Bootstrapped Named Entity Recognition for Product Attribute Extraction</a></p>
<p>15 0.41536346 <a title="130-lda-15" href="./emnlp-2011-Unsupervised_Learning_of_Selectional_Restrictions_and_Detection_of_Argument_Coercions.html">144 emnlp-2011-Unsupervised Learning of Selectional Restrictions and Detection of Argument Coercions</a></p>
<p>16 0.41348827 <a title="130-lda-16" href="./emnlp-2011-Generating_Aspect-oriented_Multi-Document_Summarization_with_Event-aspect_model.html">61 emnlp-2011-Generating Aspect-oriented Multi-Document Summarization with Event-aspect model</a></p>
<p>17 0.40905675 <a title="130-lda-17" href="./emnlp-2011-Relation_Extraction_with_Relation_Topics.html">114 emnlp-2011-Relation Extraction with Relation Topics</a></p>
<p>18 0.408822 <a title="130-lda-18" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>19 0.40041092 <a title="130-lda-19" href="./emnlp-2011-Predicting_Thread_Discourse_Structure_over_Technical_Web_Forums.html">105 emnlp-2011-Predicting Thread Discourse Structure over Technical Web Forums</a></p>
<p>20 0.39361888 <a title="130-lda-20" href="./emnlp-2011-Rumor_has_it%3A_Identifying_Misinformation_in_Microblogs.html">117 emnlp-2011-Rumor has it: Identifying Misinformation in Microblogs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
