<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>55 emnlp-2011-Exploiting Syntactic and Distributional Information for Spelling Correction with Web-Scale N-gram Models</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-55" href="#">emnlp2011-55</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>55 emnlp-2011-Exploiting Syntactic and Distributional Information for Spelling Correction with Web-Scale N-gram Models</h1>
<br/><p>Source: <a title="emnlp-2011-55-pdf" href="http://aclweb.org/anthology//D/D11/D11-1119.pdf">pdf</a></p><p>Author: Wei Xu ; Joel Tetreault ; Martin Chodorow ; Ralph Grishman ; Le Zhao</p><p>Abstract: We propose a novel way of incorporating dependency parse and word co-occurrence information into a state-of-the-art web-scale ngram model for spelling correction. The syntactic and distributional information provides extra evidence in addition to that provided by a web-scale n-gram corpus and especially helps with data sparsity problems. Experimental results show that introducing syntactic features into n-gram based models significantly reduces errors by up to 12.4% over the current state-of-the-art. The word co-occurrence information shows potential but only improves overall accuracy slightly. 1</p><p>Reference: <a title="emnlp-2011-55-reference" href="../emnlp2011_reference/emnlp-2011-Exploiting_Syntactic_and_Distributional_Information_for_Spelling_Correction_with_Web-Scale_N-gram_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We propose a novel way of incorporating dependency parse and word co-occurrence information into a state-of-the-art web-scale ngram model for spelling correction. [sent-9, score-0.429]
</p><p>2 The syntactic and distributional information provides extra evidence in addition to that provided by a web-scale n-gram corpus and especially helps with data sparsity problems. [sent-10, score-0.126]
</p><p>3 Experimental results show that introducing syntactic features into n-gram based models significantly reduces errors by up to 12. [sent-11, score-0.095]
</p><p>4 1  Introduction  The function of context-sensitive text correction is  to identify word-choice errors in text (Bergsma et al. [sent-14, score-0.242]
</p><p>5 It can be viewed as a lexical disambiguation task (Lapata and Keller, 2005), where a system selects from a predefined confusion word set, such as {affect, effect} or {complement, compliment}, aansd { provides fthfeec tm}o sort appropriate ewnto,rd c ochmopicliem given the context. [sent-16, score-0.363]
</p><p>6 One of the top performing models of spelling correction (Bergsma et al. [sent-18, score-0.456]
</p><p>7 Second, if the target confusable word is rare, there will not be enough n-gram support or training data to render a confident decision. [sent-24, score-0.189]
</p><p>8 Take a sentence from The New York Times (NYT) for example: “‘This fellow’s won a war,’ the dean of the capital’s press corps, David Broder, announced on ‘Meet the Press’ after complimenting the president on the ‘great sense of authority and command’ he exhibited in a flight suit. [sent-26, score-0.203]
</p><p>9 The n-gram models decide solely based on the frequency of the bi-grams “after comple(i)menting” and “comple(i)menting the”, which are common usages for both words. [sent-28, score-0.099]
</p><p>10 We propose enhancing state-of-the-art web-scale n-gram models for spelling correction with syntactic structures and distributional information. [sent-41, score-0.549]
</p><p>11 For our work, we build on a baseline system that combines n-gram and lexical features (Bergsma et al. [sent-42, score-0.118]
</p><p>12 We show that the baseline system can be improved by augmenting it with dependency parse features. [sent-45, score-0.129]
</p><p>13 We show that the impact of parse features can be further augmented when combined with distributional information, specifically word cooccurrence information. [sent-47, score-0.339]
</p><p>14 In Sections 3 and 4, we discuss our methods for using parse features and word co-occurrence information. [sent-49, score-0.195]
</p><p>15 2  Related Work  A variety of approaches have been proposed for context-sensitive spelling correction ranging from semantic methods to machine learning classifiers to large-scale n-gram models. [sent-51, score-0.519]
</p><p>16 Some semantics-based systems have been developed based on an intuitive assumption that the in-  tended word is more likely to be semantically coherent with the context than is a spelling error. [sent-52, score-0.302]
</p><p>17 Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. [sent-60, score-0.159]
</p><p>18 The use of distributional similarity measures for  spelling correction has been previously explored in (Mohammad and Hist, 2006). [sent-61, score-0.549]
</p><p>19 In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model and with parse features, which allows the system to select words outside the local window for distributional similarity. [sent-62, score-0.373]
</p><p>20 In the prior work, the words for distributional similarity are constrained to the local window, and positional information of the words is not encoded. [sent-63, score-0.122]
</p><p>21 , 2009) has demonstrated that large-scale language modeling is extremely helpful for contextual spelling correction and other lexical disambiguation tasks. [sent-66, score-0.527]
</p><p>22 These systems make the word choice depending on how frequently each candidate word has been seen in the given context in web-scale data. [sent-67, score-0.12]
</p><p>23 From the Google Web N-gram Corpus, they retrieve counts of n-grams of different sizes (2-5) and positions that span the target word w0 within a window of 9 words. [sent-71, score-0.181]
</p><p>24 For example, for the following sentence: “The system tried to decide {among, between} the two confus1292  able words. [sent-72, score-0.143]
</p><p>25 ”, the method would extract the five 5gram patterns, shown below in Figure 2, where w0 can be either word in the confusion set {among, between} i tnh tehri sw particular example. [sent-73, score-0.297]
</p><p>26 n Sdi two 2-grams, in total, 14 n-grams for each of the words in the confusion set. [sent-75, score-0.252]
</p><p>27 system tried to decide tried to decide to decide decide  w0 w0 w0 w0 w0  the the two the two confusable the two confusable words  We briefly describe three of Bergsma et al. [sent-76, score-0.595]
</p><p>28 (2010) augment the NG model with lexical features (described in detail in Section 3. [sent-86, score-0.087]
</p><p>29 (2009; 2010) restricted their experiments to only five confusion sets where the reported performance in (Golding and Roth, 1999) was below 90%: {among, between}, {amount, number}, {cite, sight, site}, {peace, piece} amndo {raise, rise}. [sent-89, score-0.252]
</p><p>30 However, the limited confusion word sets they evaluated  may not comprehensively represent the word usage errors that writers typically make. [sent-92, score-0.436]
</p><p>31 In our work, we show that parse features are effective when used directly in the classification mode (as opposed to as a final filter) to select the best correction regardless of whether or not the part-of-speech of the choices differ. [sent-97, score-0.349]
</p><p>32 Statistical parsers have also seen limited use in the sister tasks of preposition and article error detection (Hermet et al. [sent-98, score-0.141]
</p><p>33 In those instances where parsers have been used, they have mainly provided shallow analyses or relations involving specific target words, such as a preposition or verb. [sent-101, score-0.112]
</p><p>34 Unlike preposition errors, spelling errors can occur in any word. [sent-102, score-0.37]
</p><p>35 In this paper, we propose a novel way to incorporate the parse into spelling correction, applying the parser to sentences filled by each candidate word equivalently and extracting salient features. [sent-103, score-0.516]
</p><p>36 This overcomes two problem in the existing methods: 1) the parse trees of the same sentence filled by different confusion words can be different. [sent-104, score-0.392]
</p><p>37 However, in the test phase, we do not know which word should be put in the sentences to create parse features for test examples. [sent-105, score-0.195]
</p><p>38 , 2009; Rozovskaya and Roth, 2010) in the text correction field introduced artificial errors into training data to adapt the system to better handle ill-formed text. [sent-109, score-0.273]
</p><p>39 1 Baseline System  We chose one of the leading spelling correction systems, (Bergsma et al. [sent-112, score-0.456]
</p><p>40 As noted earlier, it is an SVM-based system combining web-scale n-gram counts (NG) and contextual words (LEX) as features. [sent-114, score-0.103]
</p><p>41 To simplify the explanation, throughout the paper, we will only consider the situation with two confusion words. [sent-115, score-0.252]
</p><p>42 The 1293  problem with more than two words in pre-defined confusion sets can be solved similarly by using a one-vs. [sent-116, score-0.252]
</p><p>43 As we mentioned in Section 2, NG features include log-counts of 3-to-5-gram patterns for each candidate word with the given context. [sent-118, score-0.127]
</p><p>44 LEX features can be broken down into three subcategories: 1) bag-of-words (words at all positions in a 9-word window around the target word), 2) indicators for the words preceding or following the target word, and 3) indicators for all n-grams and their positions. [sent-119, score-0.194]
</p><p>45 For the sentence “The system tried to decide {among, between} the two confusable words. [sent-120, score-0.245]
</p><p>46 , the two positional bigrams would be “decide” and “the”, and examples of the n-gram fea-  tures would be right-trigram = “among the two” and left-4-gram = “tried to decide between”. [sent-122, score-0.097]
</p><p>47 combination of 4 and 5  Each of these six classes of PAR features can contain zero to many values, since the target word can be involved in none to multiple grammatical relations and features of different filler words are merged together. [sent-133, score-0.191]
</p><p>48 In Table 1, we present the parse features for an example sentence. [sent-135, score-0.15]
</p><p>49 The parse features here are listed as string values, but are later converted into binary numbers in the vectors for the SVM model. [sent-136, score-0.15]
</p><p>50 4  Distributional Word Co-occurrence  Though lexical and parse features are complementary to n-gram models, they are learned from a normal training corpus and may not have enough coverage due to data sparsity. [sent-137, score-0.185]
</p><p>51 The other option is to make use of high-order word co-occurrence, which is included in many semantic word relatedness measures, such as Latent Semantic Analysis (LSA) (Landauer et al. [sent-142, score-0.119]
</p><p>52 Our intuition is to choose the confusion word which is most relevant to a given context. [sent-145, score-0.297]
</p><p>53 , and the relevance between two words as a function Relevance(w1, w2), which can either be calculated from word co-occurrence or Random Indexing. [sent-149, score-0.158]
</p><p>54 The score of each candidate word c in the confusion set given a context with meaningful words M is calculated by the following formula: Score(c) =  X mX∈M  Relevance(c,m)  In this paper, we experiment with first-order word co-occurrence and Random Indexing as relevance measures. [sent-150, score-0.485]
</p><p>55 And we define salient contextual words as heads or complements in the dependency relations with the target word. [sent-151, score-0.156]
</p><p>56 In this way, we use the parse information to constrain the two distribution models. [sent-152, score-0.098]
</p><p>57 1 First-order Word Co-occurrence The relevance based on first-order word cooccurrence is calculated from the Google Web 5gram Corpus in a fashion similar to how we dealt with n-gram counts in the previous section. [sent-157, score-0.245]
</p><p>58 Take the sentence at the beginning of this section for example, where  only the words “a” and “friends” are related to the target word (either “complement” or “compliment”) by either relevance measure. [sent-162, score-0.2]
</p><p>59 The relevance based on Random Indexing for (complement, friends) is 0. [sent-163, score-0.113]
</p><p>60 Meanwhile, the relevance based on first order word co-occurrence for (compliment, friends) is 7. [sent-166, score-0.158]
</p><p>61 The system with either kind of relevance outputs “compliment”. [sent-171, score-0.144]
</p><p>62 3 System Combination Since the numeric measurement of word cooccurrence is not as specific as the PAR features and less trustworthy, adding word co-occurrence information as features into the classifier along with ngram counts, lexical and parse features will hurt the overall performance. [sent-173, score-0.492]
</p><p>63 5  Evaluation  We evaluate the effectiveness of syntactic and distributional information on spelling correction. [sent-177, score-0.35]
</p><p>64 We compare our results against two baselines: 1) MAJOR chooses the most frequent candidate from the 1295  confusion set in the training corpus, and 2) Bergsma et al. [sent-179, score-0.282]
</p><p>65 who use the same source as training data for the lexical features, our training data (for both lexical and parse features) comes from larger and more diverse news sources. [sent-191, score-0.168]
</p><p>66 We evaluate our systems on 5 confusion sets from Bergsma et al. [sent-194, score-0.252]
</p><p>67 (2009; 2010) and 9 commonly confused word pairs with moderate frequency in daily usage (randomly selected from those listed in En-  glish educational resources2). [sent-195, score-0.145]
</p><p>68 For each confusable word pair, sentences that contain either of the words are extracted to form training and test data. [sent-198, score-0.147]
</p><p>69 For frequently occurring confusion word sets used by Bergsma et al. [sent-200, score-0.297]
</p><p>70 For the 9 less frequent confusion word sets, we extract all the unique examples for training and testing from the above sources. [sent-204, score-0.333]
</p><p>71 The spelling correction system is evaluated by measuring its accuracy in comparison to the gold standard in test data. [sent-205, score-0.487]
</p><p>72 2 Experimental Results We present the results for each set separately because each set may behave very differently, depending upon its frequency, part-of-speech, number of senses and other differences between the words in each confusion set. [sent-211, score-0.252]
</p><p>73 The overall accuracy across confusion sets is also presented to show the effectiveness of different approaches. [sent-212, score-0.252]
</p><p>74 1 Effectiveness of Parse Features We exploit the n-gram counts (NG), lexical features (LEX) of Bergsma et al. [sent-218, score-0.123]
</p><p>75 (2010) and our own parse features (PAR) in linear SVM models. [sent-219, score-0.15]
</p><p>76 As shown in Table 3, by exploiting our unique parse features, for the total 14 confusion sets, the accuracy increases on 12 sets and decreases on 2 sets. [sent-221, score-0.35]
</p><p>77 Overall, the spelling correction accuracy improves an ab1296  solute 1. [sent-222, score-0.456]
</p><p>78 The second comparison is to see how parse features interact with n-gram count features in a supervised classifier. [sent-226, score-0.202]
</p><p>79 As shown in Table 3, the parse features proved to be beneficial when augmenting this baseline, except for the decrease in accuracy on adverse, averse by only 2 cases out of 368, and among, between by 2 cases out of 10227. [sent-229, score-0.181]
</p><p>80 For all other confusion sets, parse features decrease the error rate by as much as 2. [sent-230, score-0.469]
</p><p>81 Improvements are statistically significant on all confusion sets together, although for each separate set, improvements are significant on only 5 sets, in part due to an insufficient number of test cases. [sent-233, score-0.252]
</p><p>82 We also noticed that lexical features are not always helpful when added to n-gram count features, even for in-domain applications (i. [sent-235, score-0.087]
</p><p>83 However, lexical and parse features together show more significant and constant improvement over n-gram count-based models, as marked by α. [sent-238, score-0.185]
</p><p>84 Of the six systems, every system that uses parse features gets the example correct in Section 1, “complementing the president”; LEX by itself also gets the example correct, but NG and NG+LEX fail. [sent-239, score-0.181]
</p><p>85 For all 12 confusion sets, distributional word co-occurrence information improves 9 sets and hurts 5 sets. [sent-256, score-0.39]
</p><p>86 We believe there are two reasons why Random Indexing fared worse than first-order word  co-occurrence: 1) Random Indexing considers cooccurrence on a document level, while our firstorder word co-occurrence is limited to a 5-word window context. [sent-261, score-0.199]
</p><p>87 The latter is more suitable to contextsensitive spelling correction. [sent-262, score-0.257]
</p><p>88 2) The model for Random Indexing is trained on a relatively small size corpus compared to the web-scale data we used to get n-gram count features for the classifier and thus is not able to introduce much new evidence besides the information carried by NG+LEX+PAR features. [sent-263, score-0.085]
</p><p>89 It introduces new errors because it simply favors the word that co-occurred more often regardless of other factors. [sent-267, score-0.088]
</p><p>90 Its impact is also limited because it is only considered when classifiers with NG+LEX+PAR features are not confident. [sent-268, score-0.086]
</p><p>91 52% %)α α* Table 3: Spelling correction precision (%), impact of adding parse features SVM trained on 1G words of news text, tested on 9-months of NYT data. [sent-277, score-0.349]
</p><p>92 (2009; 2010), no morphological variants of the words are used in evaluation 1298  6 We  Conclusions propose  a  novel approach that  uses  parse  features and lexical features together to improve the performance of web-scale n-gram models for spelling correction. [sent-290, score-0.494]
</p><p>93 This method is especially adaptive when less training data are available, which is the case for confusable words that are not very frequently used. [sent-291, score-0.102]
</p><p>94 We also investigate the effectiveness of incorporating web-scale word co-occurrence and corpus-based semantic word relatedness (Random Indexing). [sent-292, score-0.119]
</p><p>95 It will be interesting to see if the usage of the word “compliment” in “complimenting the president” can be estimated by considering similar usages in the corpus, such as “complimenting the student” or by creating an n-gram database of synset patterns. [sent-296, score-0.127]
</p><p>96 A classifier-based approach to preposition and determiner error correction in L2 English. [sent-360, score-0.305]
</p><p>97 A Bayesian hybrid method for context sensitive spelling correction. [sent-382, score-0.257]
</p><p>98 Detecting errors in English article usage by non-native speakers. [sent-386, score-0.129]
</p><p>99 The Word-Space Model: using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces. [sent-426, score-0.093]
</p><p>100 Using parse features for preposition selection and error detection. [sent-436, score-0.256]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bergsma', 0.472), ('compliment', 0.262), ('lex', 0.26), ('spelling', 0.257), ('confusion', 0.252), ('correction', 0.199), ('indexing', 0.192), ('par', 0.175), ('ng', 0.157), ('complement', 0.136), ('golding', 0.131), ('friends', 0.119), ('relevance', 0.113), ('nyt', 0.11), ('complimenting', 0.109), ('confusable', 0.102), ('tetreault', 0.102), ('parse', 0.098), ('president', 0.094), ('distributional', 0.093), ('felice', 0.087), ('preposition', 0.07), ('decide', 0.068), ('chodorow', 0.066), ('liblinear', 0.065), ('onstar', 0.065), ('joel', 0.063), ('window', 0.058), ('svm', 0.053), ('features', 0.052), ('cooccurrence', 0.051), ('usage', 0.051), ('educational', 0.049), ('martin', 0.049), ('complementing', 0.047), ('word', 0.045), ('carlson', 0.045), ('tried', 0.044), ('salient', 0.044), ('comple', 0.044), ('driver', 0.044), ('elmi', 0.044), ('hermet', 0.044), ('higgins', 0.044), ('notifies', 0.044), ('passenger', 0.044), ('rachele', 0.044), ('errors', 0.043), ('filled', 0.042), ('target', 0.042), ('google', 0.04), ('landauer', 0.04), ('random', 0.038), ('collision', 0.037), ('cuny', 0.037), ('izumi', 0.037), ('rozovskaya', 0.037), ('deerwester', 0.037), ('dligach', 0.037), ('dmitriy', 0.037), ('menting', 0.037), ('vue', 0.037), ('whitelaw', 0.037), ('rating', 0.037), ('sekine', 0.037), ('service', 0.037), ('counts', 0.036), ('error', 0.036), ('contextual', 0.036), ('testing', 0.036), ('lexical', 0.035), ('article', 0.035), ('roth', 0.035), ('classifiers', 0.034), ('complements', 0.034), ('front', 0.034), ('mangu', 0.034), ('vehicle', 0.034), ('safety', 0.034), ('esl', 0.034), ('budanitsky', 0.034), ('crash', 0.034), ('cucerzan', 0.034), ('sparsity', 0.033), ('logistic', 0.033), ('classifier', 0.033), ('regression', 0.032), ('fan', 0.031), ('pulman', 0.031), ('usages', 0.031), ('shane', 0.031), ('decrease', 0.031), ('system', 0.031), ('candidate', 0.03), ('york', 0.03), ('brants', 0.03), ('positional', 0.029), ('gamon', 0.029), ('semantic', 0.029), ('ngram', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="55-tfidf-1" href="./emnlp-2011-Exploiting_Syntactic_and_Distributional_Information_for_Spelling_Correction_with_Web-Scale_N-gram_Models.html">55 emnlp-2011-Exploiting Syntactic and Distributional Information for Spelling Correction with Web-Scale N-gram Models</a></p>
<p>Author: Wei Xu ; Joel Tetreault ; Martin Chodorow ; Ralph Grishman ; Le Zhao</p><p>Abstract: We propose a novel way of incorporating dependency parse and word co-occurrence information into a state-of-the-art web-scale ngram model for spelling correction. The syntactic and distributional information provides extra evidence in addition to that provided by a web-scale n-gram corpus and especially helps with data sparsity problems. Experimental results show that introducing syntactic features into n-gram based models significantly reduces errors by up to 12.4% over the current state-of-the-art. The word co-occurrence information shows potential but only improves overall accuracy slightly. 1</p><p>2 0.18269326 <a title="55-tfidf-2" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>Author: Daniel Dahlmeier ; Hwee Tou Ng</p><p>Abstract: We present a novel approach for automatic collocation error correction in learner English which is based on paraphrases extracted from parallel corpora. Our key assumption is that collocation errors are often caused by semantic similarity in the first language (L1language) of the writer. An analysis of a large corpus of annotated learner English confirms this assumption. We evaluate our approach on real-world learner data and show that L1-induced paraphrases outperform traditional approaches based on edit distance, homophones, and WordNet synonyms.</p><p>3 0.12289885 <a title="55-tfidf-3" href="./emnlp-2011-Parse_Correction_with_Specialized_Models_for_Difficult_Attachment_Types.html">102 emnlp-2011-Parse Correction with Specialized Models for Difficult Attachment Types</a></p>
<p>Author: Enrique Henestroza Anguiano ; Marie Candito</p><p>Abstract: This paper develops a framework for syntactic dependency parse correction. Dependencies in an input parse tree are revised by selecting, for a given dependent, the best governor from within a small set of candidates. We use a discriminative linear ranking model to select the best governor from a group of candidates for a dependent, and our model includes a rich feature set that encodes syntactic structure in the input parse tree. The parse correction framework is parser-agnostic, and can correct attachments using either a generic model or specialized models tailored to difficult attachment types like coordination and pp-attachment. Our experiments show that parse correction, combining a generic model with specialized models for difficult attachment types, can successfully improve the quality of predicted parse trees output by sev- eral representative state-of-the-art dependency parsers for French.</p><p>4 0.078636825 <a title="55-tfidf-4" href="./emnlp-2011-A_Joint_Model_for_Extended_Semantic_Role_Labeling.html">7 emnlp-2011-A Joint Model for Extended Semantic Role Labeling</a></p>
<p>Author: Vivek Srikumar ; Dan Roth</p><p>Abstract: This paper presents a model that extends semantic role labeling. Existing approaches independently analyze relations expressed by verb predicates or those expressed as nominalizations. However, sentences express relations via other linguistic phenomena as well. Furthermore, these phenomena interact with each other, thus restricting the structures they articulate. In this paper, we use this intuition to define a joint inference model that captures the inter-dependencies between verb semantic role labeling and relations expressed using prepositions. The scarcity of jointly labeled data presents a crucial technical challenge for learning a joint model. The key strength of our model is that we use existing structure predictors as black boxes. By enforcing consistency constraints between their predictions, we show improvements in the performance of both tasks without retraining the individual models.</p><p>5 0.07104826 <a title="55-tfidf-5" href="./emnlp-2011-Semantic_Topic_Models%3A_Combining_Word_Distributional_Statistics_and_Dictionary_Definitions.html">119 emnlp-2011-Semantic Topic Models: Combining Word Distributional Statistics and Dictionary Definitions</a></p>
<p>Author: Weiwei Guo ; Mona Diab</p><p>Abstract: In this paper, we propose a novel topic model based on incorporating dictionary definitions. Traditional topic models treat words as surface strings without assuming predefined knowledge about word meaning. They infer topics only by observing surface word co-occurrence. However, the co-occurred words may not be semantically related in a manner that is relevant for topic coherence. Exploiting dictionary definitions explicitly in our model yields a better understanding of word semantics leading to better text modeling. We exploit WordNet as a lexical resource for sense definitions. We show that explicitly modeling word definitions helps improve performance significantly over the baseline for a text categorization task.</p><p>6 0.069265209 <a title="55-tfidf-6" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<p>7 0.06144115 <a title="55-tfidf-7" href="./emnlp-2011-A_Correction_Model_for_Word_Alignments.html">3 emnlp-2011-A Correction Model for Word Alignments</a></p>
<p>8 0.060758032 <a title="55-tfidf-8" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>9 0.059985761 <a title="55-tfidf-9" href="./emnlp-2011-A_Fast%2C_Accurate%2C_Non-Projective%2C_Semantically-Enriched_Parser.html">4 emnlp-2011-A Fast, Accurate, Non-Projective, Semantically-Enriched Parser</a></p>
<p>10 0.054669559 <a title="55-tfidf-10" href="./emnlp-2011-Discovering_Morphological_Paradigms_from_Plain_Text_Using_a_Dirichlet_Process_Mixture_Model.html">39 emnlp-2011-Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model</a></p>
<p>11 0.053262435 <a title="55-tfidf-11" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>12 0.049707532 <a title="55-tfidf-12" href="./emnlp-2011-Linguistic_Redundancy_in_Twitter.html">89 emnlp-2011-Linguistic Redundancy in Twitter</a></p>
<p>13 0.046934895 <a title="55-tfidf-13" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>14 0.046687584 <a title="55-tfidf-14" href="./emnlp-2011-Latent_Vector_Weighting_for_Word_Meaning_in_Context.html">80 emnlp-2011-Latent Vector Weighting for Word Meaning in Context</a></p>
<p>15 0.044471446 <a title="55-tfidf-15" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>16 0.044462815 <a title="55-tfidf-16" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>17 0.043728977 <a title="55-tfidf-17" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>18 0.043563839 <a title="55-tfidf-18" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>19 0.042332616 <a title="55-tfidf-19" href="./emnlp-2011-Harnessing_WordNet_Senses_for_Supervised_Sentiment_Classification.html">63 emnlp-2011-Harnessing WordNet Senses for Supervised Sentiment Classification</a></p>
<p>20 0.041963559 <a title="55-tfidf-20" href="./emnlp-2011-Statistical_Machine_Translation_with_Local_Language_Models.html">125 emnlp-2011-Statistical Machine Translation with Local Language Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.179), (1, -0.037), (2, -0.044), (3, 0.016), (4, -0.016), (5, 0.015), (6, -0.033), (7, 0.092), (8, 0.007), (9, 0.022), (10, 0.048), (11, -0.122), (12, 0.05), (13, 0.012), (14, -0.125), (15, 0.102), (16, -0.028), (17, -0.099), (18, 0.149), (19, -0.177), (20, -0.035), (21, 0.094), (22, 0.148), (23, 0.189), (24, -0.102), (25, 0.218), (26, 0.239), (27, 0.003), (28, 0.203), (29, 0.116), (30, 0.056), (31, -0.103), (32, 0.088), (33, 0.022), (34, 0.045), (35, 0.015), (36, 0.062), (37, -0.042), (38, 0.086), (39, 0.021), (40, -0.119), (41, -0.132), (42, -0.026), (43, -0.083), (44, -0.11), (45, 0.064), (46, 0.052), (47, -0.016), (48, -0.005), (49, -0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93355316 <a title="55-lsi-1" href="./emnlp-2011-Exploiting_Syntactic_and_Distributional_Information_for_Spelling_Correction_with_Web-Scale_N-gram_Models.html">55 emnlp-2011-Exploiting Syntactic and Distributional Information for Spelling Correction with Web-Scale N-gram Models</a></p>
<p>Author: Wei Xu ; Joel Tetreault ; Martin Chodorow ; Ralph Grishman ; Le Zhao</p><p>Abstract: We propose a novel way of incorporating dependency parse and word co-occurrence information into a state-of-the-art web-scale ngram model for spelling correction. The syntactic and distributional information provides extra evidence in addition to that provided by a web-scale n-gram corpus and especially helps with data sparsity problems. Experimental results show that introducing syntactic features into n-gram based models significantly reduces errors by up to 12.4% over the current state-of-the-art. The word co-occurrence information shows potential but only improves overall accuracy slightly. 1</p><p>2 0.84886515 <a title="55-lsi-2" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>Author: Daniel Dahlmeier ; Hwee Tou Ng</p><p>Abstract: We present a novel approach for automatic collocation error correction in learner English which is based on paraphrases extracted from parallel corpora. Our key assumption is that collocation errors are often caused by semantic similarity in the first language (L1language) of the writer. An analysis of a large corpus of annotated learner English confirms this assumption. We evaluate our approach on real-world learner data and show that L1-induced paraphrases outperform traditional approaches based on edit distance, homophones, and WordNet synonyms.</p><p>3 0.58643824 <a title="55-lsi-3" href="./emnlp-2011-Parse_Correction_with_Specialized_Models_for_Difficult_Attachment_Types.html">102 emnlp-2011-Parse Correction with Specialized Models for Difficult Attachment Types</a></p>
<p>Author: Enrique Henestroza Anguiano ; Marie Candito</p><p>Abstract: This paper develops a framework for syntactic dependency parse correction. Dependencies in an input parse tree are revised by selecting, for a given dependent, the best governor from within a small set of candidates. We use a discriminative linear ranking model to select the best governor from a group of candidates for a dependent, and our model includes a rich feature set that encodes syntactic structure in the input parse tree. The parse correction framework is parser-agnostic, and can correct attachments using either a generic model or specialized models tailored to difficult attachment types like coordination and pp-attachment. Our experiments show that parse correction, combining a generic model with specialized models for difficult attachment types, can successfully improve the quality of predicted parse trees output by sev- eral representative state-of-the-art dependency parsers for French.</p><p>4 0.38526753 <a title="55-lsi-4" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<p>Author: Sze-Meng Jojo Wong ; Mark Dras</p><p>Abstract: Attempts to profile authors according to their characteristics extracted from textual data, including native language, have drawn attention in recent years, via various machine learning approaches utilising mostly lexical features. Drawing on the idea of contrastive analysis, which postulates that syntactic errors in a text are to some extent influenced by the native language of an author, this paper explores the usefulness of syntactic features for native language identification. We take two types of parse substructure as features— horizontal slices of trees, and the more general feature schemas from discriminative parse reranking—and show that using this kind of syntactic feature results in an accuracy score in classification of seven native languages of around 80%, an error reduction of more than 30%.</p><p>5 0.30896184 <a title="55-lsi-5" href="./emnlp-2011-A_Correction_Model_for_Word_Alignments.html">3 emnlp-2011-A Correction Model for Word Alignments</a></p>
<p>Author: J. Scott McCarley ; Abraham Ittycheriah ; Salim Roukos ; Bing Xiang ; Jian-ming Xu</p><p>Abstract: Models of word alignment built as sequences of links have limited expressive power, but are easy to decode. Word aligners that model the alignment matrix can express arbitrary alignments, but are difficult to decode. We propose an alignment matrix model as a correction algorithm to an underlying sequencebased aligner. Then a greedy decoding algorithm enables the full expressive power of the alignment matrix formulation. Improved alignment performance is shown for all nine language pairs tested. The improved alignments also improved translation quality from Chinese to English and English to Italian.</p><p>6 0.29527611 <a title="55-lsi-6" href="./emnlp-2011-A_Cascaded_Classification_Approach_to_Semantic_Head_Recognition.html">2 emnlp-2011-A Cascaded Classification Approach to Semantic Head Recognition</a></p>
<p>7 0.28622329 <a title="55-lsi-7" href="./emnlp-2011-Unsupervised_Information_Extraction_with_Distributional_Prior_Knowledge.html">143 emnlp-2011-Unsupervised Information Extraction with Distributional Prior Knowledge</a></p>
<p>8 0.27660304 <a title="55-lsi-8" href="./emnlp-2011-Approximate_Scalable_Bounded_Space_Sketch_for_Large_Data_NLP.html">19 emnlp-2011-Approximate Scalable Bounded Space Sketch for Large Data NLP</a></p>
<p>9 0.26019388 <a title="55-lsi-9" href="./emnlp-2011-Harnessing_WordNet_Senses_for_Supervised_Sentiment_Classification.html">63 emnlp-2011-Harnessing WordNet Senses for Supervised Sentiment Classification</a></p>
<p>10 0.25944617 <a title="55-lsi-10" href="./emnlp-2011-Discovering_Morphological_Paradigms_from_Plain_Text_Using_a_Dirichlet_Process_Mixture_Model.html">39 emnlp-2011-Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model</a></p>
<p>11 0.25853011 <a title="55-lsi-11" href="./emnlp-2011-A_Joint_Model_for_Extended_Semantic_Role_Labeling.html">7 emnlp-2011-A Joint Model for Extended Semantic Role Labeling</a></p>
<p>12 0.22568126 <a title="55-lsi-12" href="./emnlp-2011-Bootstrapped_Named_Entity_Recognition_for_Product_Attribute_Extraction.html">23 emnlp-2011-Bootstrapped Named Entity Recognition for Product Attribute Extraction</a></p>
<p>13 0.22250353 <a title="55-lsi-13" href="./emnlp-2011-Latent_Vector_Weighting_for_Word_Meaning_in_Context.html">80 emnlp-2011-Latent Vector Weighting for Word Meaning in Context</a></p>
<p>14 0.21992579 <a title="55-lsi-14" href="./emnlp-2011-Computing_Logical_Form_on_Regulatory_Texts.html">32 emnlp-2011-Computing Logical Form on Regulatory Texts</a></p>
<p>15 0.21773003 <a title="55-lsi-15" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>16 0.21524416 <a title="55-lsi-16" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>17 0.21340805 <a title="55-lsi-17" href="./emnlp-2011-Enhancing_Chinese_Word_Segmentation_Using_Unlabeled_Data.html">48 emnlp-2011-Enhancing Chinese Word Segmentation Using Unlabeled Data</a></p>
<p>18 0.20723923 <a title="55-lsi-18" href="./emnlp-2011-Refining_the_Notions_of_Depth_and_Density_in_WordNet-based_Semantic_Similarity_Measures.html">112 emnlp-2011-Refining the Notions of Depth and Density in WordNet-based Semantic Similarity Measures</a></p>
<p>19 0.2023668 <a title="55-lsi-19" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>20 0.20004322 <a title="55-lsi-20" href="./emnlp-2011-Semantic_Topic_Models%3A_Combining_Word_Distributional_Statistics_and_Dictionary_Definitions.html">119 emnlp-2011-Semantic Topic Models: Combining Word Distributional Statistics and Dictionary Definitions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.11), (27, 0.013), (36, 0.038), (37, 0.017), (45, 0.121), (53, 0.025), (54, 0.034), (56, 0.308), (57, 0.017), (62, 0.022), (64, 0.02), (66, 0.034), (69, 0.011), (79, 0.035), (82, 0.021), (85, 0.01), (87, 0.014), (96, 0.044), (98, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74367118 <a title="55-lda-1" href="./emnlp-2011-Exploiting_Syntactic_and_Distributional_Information_for_Spelling_Correction_with_Web-Scale_N-gram_Models.html">55 emnlp-2011-Exploiting Syntactic and Distributional Information for Spelling Correction with Web-Scale N-gram Models</a></p>
<p>Author: Wei Xu ; Joel Tetreault ; Martin Chodorow ; Ralph Grishman ; Le Zhao</p><p>Abstract: We propose a novel way of incorporating dependency parse and word co-occurrence information into a state-of-the-art web-scale ngram model for spelling correction. The syntactic and distributional information provides extra evidence in addition to that provided by a web-scale n-gram corpus and especially helps with data sparsity problems. Experimental results show that introducing syntactic features into n-gram based models significantly reduces errors by up to 12.4% over the current state-of-the-art. The word co-occurrence information shows potential but only improves overall accuracy slightly. 1</p><p>2 0.51904565 <a title="55-lda-2" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>Author: Daniel Dahlmeier ; Hwee Tou Ng</p><p>Abstract: We present a novel approach for automatic collocation error correction in learner English which is based on paraphrases extracted from parallel corpora. Our key assumption is that collocation errors are often caused by semantic similarity in the first language (L1language) of the writer. An analysis of a large corpus of annotated learner English confirms this assumption. We evaluate our approach on real-world learner data and show that L1-induced paraphrases outperform traditional approaches based on edit distance, homophones, and WordNet synonyms.</p><p>3 0.50265014 <a title="55-lda-3" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>Author: Emily M. Bender ; Dan Flickinger ; Stephan Oepen ; Yi Zhang</p><p>Abstract: In order to obtain a fine-grained evaluation of parser accuracy over naturally occurring text, we study 100 examples each of ten reasonably frequent linguistic phenomena, randomly selected from a parsed version of the English Wikipedia. We construct a corresponding set of gold-standard target dependencies for these 1000 sentences, operationalize mappings to these targets from seven state-of-theart parsers, and evaluate the parsers against this data to measure their level of success in identifying these dependencies.</p><p>4 0.49338663 <a title="55-lda-4" href="./emnlp-2011-Cross-Cutting_Models_of_Lexical_Semantics.html">37 emnlp-2011-Cross-Cutting Models of Lexical Semantics</a></p>
<p>Author: Joseph Reisinger ; Raymond Mooney</p><p>Abstract: Context-dependent word similarity can be measured over multiple cross-cutting dimensions. For example, lung and breath are similar thematically, while authoritative and superficial occur in similar syntactic contexts, but share little semantic similarity. Both of these notions of similarity play a role in determining word meaning, and hence lexical semantic models must take them both into account. Towards this end, we develop a novel model, Multi-View Mixture (MVM), that represents words as multiple overlapping clusterings. MVM finds multiple data partitions based on different subsets of features, subject to the marginal constraint that feature subsets are distributed according to Latent Dirich- let Allocation. Intuitively, this constraint favors feature partitions that have coherent topical semantics. Furthermore, MVM uses soft feature assignment, hence the contribution of each data point to each clustering view is variable, isolating the impact of data only to views where they assign the most features. Through a series of experiments, we demonstrate the utility of MVM as an inductive bias for capturing relations between words that are intuitive to humans, outperforming related models such as Latent Dirichlet Allocation.</p><p>5 0.49233341 <a title="55-lda-5" href="./emnlp-2011-Semantic_Topic_Models%3A_Combining_Word_Distributional_Statistics_and_Dictionary_Definitions.html">119 emnlp-2011-Semantic Topic Models: Combining Word Distributional Statistics and Dictionary Definitions</a></p>
<p>Author: Weiwei Guo ; Mona Diab</p><p>Abstract: In this paper, we propose a novel topic model based on incorporating dictionary definitions. Traditional topic models treat words as surface strings without assuming predefined knowledge about word meaning. They infer topics only by observing surface word co-occurrence. However, the co-occurred words may not be semantically related in a manner that is relevant for topic coherence. Exploiting dictionary definitions explicitly in our model yields a better understanding of word semantics leading to better text modeling. We exploit WordNet as a lexical resource for sense definitions. We show that explicitly modeling word definitions helps improve performance significantly over the baseline for a text categorization task.</p><p>6 0.48836273 <a title="55-lda-6" href="./emnlp-2011-Cooooooooooooooollllllllllllll%21%21%21%21%21%21%21%21%21%21%21%21%21%21_Using_Word_Lengthening_to_Detect_Sentiment_in_Microblogs.html">33 emnlp-2011-Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using Word Lengthening to Detect Sentiment in Microblogs</a></p>
<p>7 0.48626858 <a title="55-lda-7" href="./emnlp-2011-Exploring_Supervised_LDA_Models_for_Assigning_Attributes_to_Adjective-Noun_Phrases.html">56 emnlp-2011-Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases</a></p>
<p>8 0.48619229 <a title="55-lda-8" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>9 0.48466709 <a title="55-lda-9" href="./emnlp-2011-Linking_Entities_to_a_Knowledge_Base_with_Query_Expansion.html">90 emnlp-2011-Linking Entities to a Knowledge Base with Query Expansion</a></p>
<p>10 0.47997314 <a title="55-lda-10" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>11 0.47987431 <a title="55-lda-11" href="./emnlp-2011-Harnessing_different_knowledge_sources_to_measure_semantic_relatedness_under_a_uniform_model.html">64 emnlp-2011-Harnessing different knowledge sources to measure semantic relatedness under a uniform model</a></p>
<p>12 0.47903591 <a title="55-lda-12" href="./emnlp-2011-Robust_Disambiguation_of_Named_Entities_in_Text.html">116 emnlp-2011-Robust Disambiguation of Named Entities in Text</a></p>
<p>13 0.47895718 <a title="55-lda-13" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>14 0.47872359 <a title="55-lda-14" href="./emnlp-2011-Named_Entity_Recognition_in_Tweets%3A_An_Experimental_Study.html">98 emnlp-2011-Named Entity Recognition in Tweets: An Experimental Study</a></p>
<p>15 0.4784691 <a title="55-lda-15" href="./emnlp-2011-Learning_General_Connotation_of_Words_using_Graph-based_Algorithms.html">81 emnlp-2011-Learning General Connotation of Words using Graph-based Algorithms</a></p>
<p>16 0.478333 <a title="55-lda-16" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>17 0.47797912 <a title="55-lda-17" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>18 0.47755423 <a title="55-lda-18" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>19 0.47439027 <a title="55-lda-19" href="./emnlp-2011-Better_Evaluation_Metrics_Lead_to_Better_Machine_Translation.html">22 emnlp-2011-Better Evaluation Metrics Lead to Better Machine Translation</a></p>
<p>20 0.47373638 <a title="55-lda-20" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
