<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>9 emnlp-2011-A Non-negative Matrix Factorization Based Approach for Active Dual Supervision from Document and Word Labels</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-9" href="#">emnlp2011-9</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>9 emnlp-2011-A Non-negative Matrix Factorization Based Approach for Active Dual Supervision from Document and Word Labels</h1>
<br/><p>Source: <a title="emnlp-2011-9-pdf" href="http://aclweb.org/anthology//D/D11/D11-1088.pdf">pdf</a></p><p>Author: Chao Shen ; Tao Li</p><p>Abstract: In active dual supervision, not only informative examples but also features are selected for labeling to build a high quality classifier with low cost. However, how to measure the informativeness for both examples and feature on the same scale has not been well solved. In this paper, we propose a non-negative matrix factorization based approach to address this issue. We first extend the matrix factorization framework to explicitly model the corresponding relationships between feature classes and examples classes. Then by making use of the reconstruction error, we propose a unified scheme to determine which feature or example a classifier is most likely to benefit from having labeled. Empirical results demonstrate the effectiveness of our proposed methods.</p><p>Reference: <a title="emnlp-2011-9-reference" href="../emnlp2011_reference/emnlp-2011-A_Non-negative_Matrix_Factorization_Based_Approach_for_Active_Dual_Supervision_from_Document_and_Word_Labels_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu }@  Abstract In active dual supervision, not only informative examples but also features are selected for labeling to build a high quality classifier with low cost. [sent-3, score-0.75]
</p><p>2 In this paper, we propose a non-negative matrix factorization based approach to address this issue. [sent-5, score-0.422]
</p><p>3 We first extend the matrix factorization framework to explicitly model the corresponding relationships between feature classes and examples classes. [sent-6, score-0.541]
</p><p>4 Then by making use of the reconstruction error, we propose a unified scheme to determine which feature or example a classifier is most likely to benefit from having labeled. [sent-7, score-0.563]
</p><p>5 Traditional approaches for active learning query the human experts to obtain the labels for intelligently chosen data samples. [sent-10, score-0.535]
</p><p>6 However, in text classification where the input data is generally represented as documentword matrices, human supervision can be obtained on both documents and words. [sent-11, score-0.437]
</p><p>7 In fact, different kinds of supervision generally have  different acquisition costs, different degrees of utility and are not mutually redundant (Sindhwani et al. [sent-15, score-0.43]
</p><p>8 Ideally, effective active learning schemes should be able to utilize different forms of supervision. [sent-17, score-0.325]
</p><p>9 To incorporate the supervision on words and documents at same time into the active learning scheme, recently an active dual supervision (or dual active learning) has been proposed (Melville and Sindhwani, 2009; Sindhwani et al. [sent-18, score-2.289]
</p><p>10 Comparing with traditional active learning which aims to select the most “informative” examples (e. [sent-20, score-0.33]
</p><p>11 , documents) for domain experts to label, active dual supervision selects both the “informative” examples (e. [sent-22, score-1.003]
</p><p>12 For active dual supervision to be effective, there are three important components: a) an underlying learning mechanism that is able to learn from both the labeled examples and features (i. [sent-27, score-1.068]
</p><p>13 ’s initial work on active dual supervision (Sindhwani et al. [sent-33, score-0.95]
</p><p>14 To trade-off between different types of supervision, a simple probabilistic interleaving scheme where the active learner probabilistically queries the example oracle and the feature oracle is used. [sent-36, score-0.863]
</p><p>15 , 2009) proposed a dual supervision method based on constrained nonnegative tri-factorization of the document-term matrix where the labeled features and examples are naturally incorporated as sets of constraints. [sent-40, score-1.075]
</p><p>16 Having a framework for incorporating dual-supervision based on matrix factorization, gives rise to the natural question of how to perform active dual super-  vision in this setting. [sent-41, score-0.831]
</p><p>17 Since rows and columns are treated equally in estimating the errors of matrix factorization, another question is can we address the scaling issue in comparing the value of feature labels and example labels. [sent-42, score-0.313]
</p><p>18 In this paper, we study the problem of active dual supervision using non-negative matrix trifactorization. [sent-43, score-1.145]
</p><p>19 Our work is based on the dual supervision framework using constrained non-negative trifactorization proposed in (Li et al. [sent-44, score-0.71]
</p><p>20 Then by making use of the reconstruction error criterion in matrix factorization, we propose a unified scheme to evaluate the value of feature and example labels. [sent-47, score-0.773]
</p><p>21 Instead of comparing the estimated performance increase of new feature labels or example labels, our proposed scheme assumes that a better supervision (a feature label or a example label) should lead to a more accurate reconstruction of the original data matrix. [sent-48, score-0.881]
</p><p>22 The experiments show that our proposed unified scheme to query selection (i. [sent-50, score-0.461]
</p><p>23 , feature/example selection for  labeling) outperforms the interleaving schemes and the scheme based on expected log gain. [sent-52, score-0.542]
</p><p>24 950 The rest of this paper is organized as follows: the related work is discussed in Section 2, and the dual supervision framework based on non-negative matrix tri-factorization is introduced in Section 3. [sent-53, score-0.864]
</p><p>25 We extend non-negative matrix tri-factorization to active learning settings in Section 4, and propose a unified scheme for query selection in Section 5. [sent-54, score-0.958]
</p><p>26 2  Related Work  We point the reader to a recent report (Settles, 2009) for an in-depth survey on active learning. [sent-56, score-0.281]
</p><p>27 Active Learning/Active Dual Supervision Most prior work in active learning has focused on pooledbased techniques, where examples from an unlabeled pool are selected for labeling (Cohn et al. [sent-58, score-0.415]
</p><p>28 With the study of learning from labeled fea-  tures, many research efforts on active learning with feature supervision are also reported (Melville et al. [sent-60, score-0.735]
</p><p>29 , 2009) performed active learning via feature labeling using several uncertainty reduction heuristics using the learning model developed in (Druck et al. [sent-66, score-0.454]
</p><p>30 , 2009) studied the problem of active dual supervision from examples and features using a graph-based dual supervision method with a simple probabilistic method for interleaving feature labels and example labels. [sent-69, score-2.026]
</p><p>31 In our work, we develop our active dual supervision framework using constrained non-negative tri-factorization and also propose a unified scheme to evaluate the value of feature and example labels. [sent-70, score-1.305]
</p><p>32 , 2010), which proposes a unified approach for the dual active learning problem using expected utility where the utility is defined as the log gain of the classification model with a new labeled document or word. [sent-72, score-1.281]
</p><p>33 Conceptually, our proposed unified scheme is a special case  of the expected utility framework where the utility is computed using the matrix reconstruction error. [sent-73, score-0.966]
</p><p>34 The utility based on the log gain of the classification model may not be reliable as small model changes resulted from a single additional example label or feature label may not be reflected in the classification performance (Attenberg et al. [sent-74, score-0.352]
</p><p>35 The empirical comparisons show that our proposed unified scheme based on reconstruction error outperforms the expected log gain. [sent-76, score-0.6]
</p><p>36 Dual Supervision Note that a learning method that is capable of performing dual supervision (i. [sent-77, score-0.69]
</p><p>37 , learning from both labeled examples and features) is the basis for active dual supervision. [sent-79, score-0.754]
</p><p>38 Dual supervision is a relatively new area of research and few methods have been developed for dual supervision. [sent-80, score-0.669]
</p><p>39 , 2008), a bipartite graph regularization model (GRADS) is used to diffuse label information along both sides of the document-term matrix and to perform dual supervision for semi-supervised sentiment analysis. [sent-82, score-1.021]
</p><p>40 Our work is based on the dual supervision framework using constrained non-negative tri-factorization. [sent-91, score-0.71]
</p><p>41 , 2009) extended it to incorporate labeled words and documents as dual supervision via two loss terms in the objective function of Tri-NMF as following: minF,G,S  kX − GSFTk2 +αtrace[(F − F0)TC1(F +β trace[(G − GF0)TC2((FG  F0)] − G0)]. [sent-95, score-0.825]
</p><p>42 Under the assumption that the i-th document class should correspond to the i-  =  th word class, S should be an approximate diagonal matrix, since the documents of i-th class is more likely to contain the words of the i-th class. [sent-109, score-0.347]
</p><p>43 Note that S is not an exact diagonal matrix, since a document of one class apparently can use words from other classes (especially G and F are required to be approximately orthogonal, which means the classification is rigorous). [sent-110, score-0.283]
</p><p>44 In active learning, the set of starting labeled documents or words is small, and this may generate an ill-formed S, leading to an incorrect alignment of word classes and document classes. [sent-113, score-0.552]
</p><p>45 , 2011): Fil = P(w = wi|zw = l), Gjk = P(d = dj |zd = k), Skl = P(zd = k, zw = l),  (4)  where w is word variable, d is document variable, and zw, zd are random variables indicating word class and document class respectively. [sent-117, score-0.396]
</p><p>46 Alternatively, to keep it simple, we ignore the known label information and just assume there exists a diagonal matrix S0 and two orthogonal matrices G, F, that GS0FT  ≈  X. [sent-122, score-0.401]
</p><p>47 1 Introduction An ideal active dual supervision scheme should be able to evaluate the value of acquiring labels for documents and words on the same scale. [sent-144, score-1.274]
</p><p>48 In the initial study of dual active supervision, different scores are used for documents and words (e. [sent-145, score-0.723]
</p><p>49 2 Reconstruction Error In our matrix factorization framework, rows and columns are treated equally in estimating the errors of matrix factorization, and the reconstruction error is thus a natural measure of utility. [sent-154, score-0.881]
</p><p>50 To select a new unlabeled document/word for labeling, we assume  953 that a good supervision should lead to a good constrained factorization for the document-term matrix, X ≈ GSFT. [sent-156, score-0.602]
</p><p>51 If the new query qj is a word and its lXabe ≈l i sG k, Fthen the new factorization is =  Gar∗jg=mk,inSGj∗=,Sk,F,Fkj∗X=k − GSFTk2  + αtrace[(G G−S GF0)TkC2(G − G0)] β trace[(F − F0,j=k)TC1(F − F0,j=k)] γ trace[(S − − S F0)T(S − S0)], (10) where F0,j=k is same as F0 except that F0,j=k (j, k) = 1. [sent-157, score-0.599]
</p><p>52 In other words, we obtained a new factorization using the labeled words. [sent-158, score-0.296]
</p><p>53 Similarly, if the new query qj is a document, then the new factorization is  + +  =  Gar∗jg=mk,inSGj∗=,Sk,F,Fkj∗X=k − GSFTk2  + αtrace[(G − G0,j=k)TC2(G − G0,j=k)] + β trace[(F −− G F0)TC1(F − F0)] + γ trace[(S − − S F0)T(S − S0)], (11) where G0,j=k is same as G0 except that G0,j=k (j, k) = 1. [sent-159, score-0.599]
</p><p>54 In other words, we obtained  a new factorization using the labeled documents. [sent-160, score-0.296]
</p><p>55 Then the new reconstruction error is RE(qj = k) = kX − Gj∗=kSj∗=kFj∗=kk2. [sent-161, score-0.264]
</p><p>56 When a query qj is a word, P(qj = k) is P(zw = k|w = wi)  =∝  FPi(kw∗ =P wjK=i1|zSwk=j, k)PjK=1P(zw= k,zd= j) (14)  otherwise, P(zd = k|d = di)  =∝  PGi(kd∗ =P djKi=|z1dS=jk k. [sent-164, score-0.372]
</p><p>57 3  Algorithm Description  Computational Improvement: It can be computa-  tionally intensive if the reconstruction error is computed for all unknown documents and words. [sent-166, score-0.376]
</p><p>58 Algorithm 1 Active Dual Supervision Algorithm Based on Matrix Factorization INPUT: X, document-word matrix; F0, current labeled words; G0, current labeled documents; O, the oracle OUTPUT: G, classification result for all documents in X 1. [sent-171, score-0.305]
</p><p>59 First we iteratively use the updating rules of Equation 7 to obtain the factorization G, F, S based on initial labeled documents and words. [sent-176, score-0.383]
</p><p>60 Then to select a new query, for each unlabeled document or word in the pool and for each possible class, we compute the reconstruction error 954 with new supervision (using the current factorization results as initialization values). [sent-177, score-0.926]
</p><p>61 By using sparse matrix multiplications and avoiding dense intermediate matrices, updating F, S, G each takes O(k2 (m + n) + kz) time per iteration which scales linearly with the dimensions and density of the data matrix (Li et al. [sent-182, score-0.39]
</p><p>62 To those words with labels, the word oracle returns its label; otherwise, the oracle returns a “don’t know” response (no word label is obtained for learning, but the word is excluded from the following query selection). [sent-193, score-0.279]
</p><p>63 , 2009) where features are roughly 5 times cheaper to label than examples, so we assume the cost is 1for a word query and is 5 for  a document query. [sent-202, score-0.302]
</p><p>64 (a) baseball-hockey  (b) ibm-mac  (c) med-space Figure 1: Comparing the performance ofdual supervision via Tri-NMF w/ and w/o the constraint on S. [sent-204, score-0.314]
</p><p>65 2We do not perform fine tuning on the parameters  since the main objective of the paper is to demonstrate the effectiveness of matrix factorization based methods for dual active supervision. [sent-205, score-1.058]
</p><p>66 2  Experimental Results  Effect of Constraints NMF  on S in Constrained  Figure 1 demonstrates  the effectiveness  Triof  dual supervision with explicit class alignment via Tri-NMF as described in Section 4. [sent-208, score-0.721]
</p><p>67 When there are enough labeled documents and words, the constraints on S have a relative small impact on the performance of dual supervision. [sent-209, score-0.511]
</p><p>68 However, in the beginning phase of active learning, the labeled dataset can be small (such as 10 labeled documents and 10 labeled words). [sent-210, score-0.575]
</p><p>69 In this case, without the constraint of S, the matrix factorization may generate incorrect class alignment, thus lead to almost random classification results (around 50% accuracy), as shown in Figure 1, and further make unreasonable the following evaluation of queries. [sent-211, score-0.51]
</p><p>70 Comparing Query Selection Approaches Figure 2 compares our proposed unified scheme (denoted as Expected-reconstruction-error) with the following baselines using Tri-NMF as the classifier for dual supervision: (1). [sent-212, score-0.662]
</p><p>71 Interleaved-uncertainty which first selects feature query by certainty and sample query by uncertainty and then combines the two types of queries using an interleaving scheme. [sent-213, score-0.741]
</p><p>72 The interleaving probability (probability to select the query as a document) is set as 0. [sent-214, score-0.438]
</p><p>73 Expected-reconstruction-error outperforms interleaving schemes with all the different interleaving probability values with which we experimented. [sent-221, score-0.603]
</p><p>74 Although log gain is a finer-grained utility measure of classifier performance than accuracy and has a good performance in the setting with a large set of starting labeled documents (e. [sent-223, score-0.343]
</p><p>75 Different from the Expected-loggain, Expected-reconstruction-error estimates the utility using the matrix reconstruction error, making use of information of all documents and words, including those unlabeled. [sent-226, score-0.625]
</p><p>76 the Unified Scheme To further demonstrate the benefit of the proposed unified scheme , we compare it with its interleaved version: Interleaved-expected-  cuyAar0. [sent-228, score-0.309]
</p><p>77 8642ni70 Labeinlg Cost  (b) ibm-mac  Labeinlg Cost  (c) med-space  Figure 2: Comparing the different query selection approaches in active learning via Tri-NMF with dual supervision. [sent-234, score-0.833]
</p><p>78 42r8670 (b) ibm-mac Figure  3: Comparing  (c) med-space  the unified and interleaving scheme based on reconstruction error. [sent-241, score-0.802]
</p><p>79 construction-error which computes the utility of a query using the reconstruction error, but uses inter-  leaving scheme to decide which type of query to select. [sent-242, score-0.789]
</p><p>80 We experiment with different interleaving probability values ranging from 0. [sent-243, score-0.29]
</p><p>81 From Figure 3, the optimal interleaving probability value varies on different datasets. [sent-246, score-0.29]
</p><p>82 8 is among the optimal interleaving probability values on baseball-hockey dataset but performs poorly on ibm-mac dataset. [sent-248, score-0.29]
</p><p>83 This observation also illustrates the need for a unified scheme, because of the difficulty in choosing the optimal interleaving probability value. [sent-249, score-0.425]
</p><p>84 Although the proposed unified scheme is not significantly better than its interleaving counterparts for all interleaving probability values on all datasets, it avoids the bad choices. [sent-250, score-0.865]
</p><p>85 Figure 5 presents the sequence of different query types selected by our unified scheme and it clearly demonstrates the distribution patterns of different query types. [sent-251, score-0.581]
</p><p>86 Interleaving uncertainty using GRADS It should be pointed out that our unified scheme for query selection based on reconstruction error does not rely on the estimation of model performance on training data and can be  easily integrated with other dual supervision mod-  ycuAar 0 . [sent-257, score-1.456]
</p><p>87 5780 (b) ibm-mac  (c) med-space  Figure 4: GRADS with reconstruction error and interleaving uncertainty. [sent-265, score-0.554]
</p><p>88 Figure 4 shows the comparison of GRADS using the interleaved scheme with an interleaving probability of 0. [sent-268, score-0.464]
</p><p>89 5, and using our unified scheme based on reconstruction error. [sent-269, score-0.512]
</p><p>90 Among the 3 datasets we used, the reconstruction error based approach outperforms the interleaving scheme on baseball-hockey and ibmmac, and has similar performance with the interleaving scheme on med-space. [sent-270, score-1.144]
</p><p>91 Labeling Cost  Figure 6: Comparing active dual supervision using matrix factorization with GRADS on sentiment analysis. [sent-271, score-1.448]
</p><p>92 957 7  Conclusions  In this paper, we study the problem of active dual supervision, and propose a matrix tri-factorization based approach to address the issue, how to evaluate labeling benifit of different types of queries (examples or features) in the same scale. [sent-276, score-0.896]
</p><p>93 Following extending the nonnegative matrix tri-factorization to the active dual supervision setting, we use the reconstruction error to evaluate the value of feature and example labels. [sent-277, score-1.511]
</p><p>94 On the equivalence between non-negative matrix factorization and probabilistic latent semantic indexing. [sent-317, score-0.422]
</p><p>95 Document classification through interactive supervision of document and term labels. [sent-341, score-0.426]
</p><p>96 A non-negative matrix tri-factorization approach to sentiment classification with lexical prior knowledge. [sent-356, score-0.307]
</p><p>97 Employing EM and pool-based active learning for text classification. [sent-364, score-0.302]
</p><p>98 Active dual supervision: Reducing the cost of annotating examples and features. [sent-371, score-0.418]
</p><p>99 Uncertainty sampling and transductive experimental design for active dual supervision. [sent-458, score-0.662]
</p><p>100 Support vector machine active learning with applications to text classification. [sent-465, score-0.302]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dual', 0.355), ('supervision', 0.314), ('interleaving', 0.29), ('active', 0.281), ('sindhwani', 0.27), ('reconstruction', 0.227), ('factorization', 0.227), ('qj', 0.224), ('matrix', 0.195), ('trace', 0.195), ('scheme', 0.15), ('query', 0.148), ('unified', 0.135), ('grads', 0.135), ('melville', 0.131), ('utility', 0.116), ('zw', 0.087), ('documents', 0.087), ('druck', 0.085), ('labeinlg', 0.084), ('diagonal', 0.08), ('ck', 0.079), ('document', 0.076), ('sentiment', 0.076), ('nonnegative', 0.073), ('attenberg', 0.073), ('labeled', 0.069), ('uncertainty', 0.062), ('labels', 0.06), ('orthogonal', 0.055), ('zd', 0.053), ('class', 0.052), ('gjk', 0.051), ('sjk', 0.051), ('ding', 0.047), ('oracle', 0.044), ('fg', 0.044), ('kx', 0.043), ('label', 0.043), ('constrained', 0.041), ('labeling', 0.04), ('certainty', 0.039), ('theorem', 0.039), ('classes', 0.039), ('li', 0.038), ('bipartite', 0.038), ('eu', 0.038), ('error', 0.037), ('classification', 0.036), ('raghavan', 0.035), ('cost', 0.035), ('jk', 0.034), ('fq', 0.034), ('gar', 0.034), ('gq', 0.034), ('jg', 0.034), ('kkt', 0.034), ('sandler', 0.034), ('slk', 0.034), ('zha', 0.034), ('xk', 0.032), ('equation', 0.031), ('settles', 0.031), ('comparing', 0.029), ('fjk', 0.029), ('pjk', 0.029), ('xxt', 0.029), ('feature', 0.029), ('matrices', 0.028), ('examples', 0.028), ('selection', 0.028), ('acquiring', 0.027), ('expected', 0.027), ('godbole', 0.026), ('transductive', 0.026), ('experts', 0.025), ('pool', 0.025), ('unknown', 0.025), ('queries', 0.025), ('gain', 0.025), ('feedback', 0.025), ('interleaved', 0.024), ('sq', 0.024), ('informative', 0.024), ('log', 0.024), ('relationships', 0.023), ('schemes', 0.023), ('costs', 0.023), ('discovery', 0.023), ('rl', 0.023), ('zaidan', 0.023), ('shen', 0.022), ('classifier', 0.022), ('learning', 0.021), ('acm', 0.02), ('unlabeled', 0.02), ('cluster', 0.02), ('update', 0.02), ('databases', 0.02), ('tnhe', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="9-tfidf-1" href="./emnlp-2011-A_Non-negative_Matrix_Factorization_Based_Approach_for_Active_Dual_Supervision_from_Document_and_Word_Labels.html">9 emnlp-2011-A Non-negative Matrix Factorization Based Approach for Active Dual Supervision from Document and Word Labels</a></p>
<p>Author: Chao Shen ; Tao Li</p><p>Abstract: In active dual supervision, not only informative examples but also features are selected for labeling to build a high quality classifier with low cost. However, how to measure the informativeness for both examples and feature on the same scale has not been well solved. In this paper, we propose a non-negative matrix factorization based approach to address this issue. We first extend the matrix factorization framework to explicitly model the corresponding relationships between feature classes and examples classes. Then by making use of the reconstruction error, we propose a unified scheme to determine which feature or example a classifier is most likely to benefit from having labeled. Empirical results demonstrate the effectiveness of our proposed methods.</p><p>2 0.22615574 <a title="9-tfidf-2" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>Author: Burr Settles</p><p>Abstract: This paper describes DUALIST, an active learning annotation paradigm which solicits and learns from labels on both features (e.g., words) and instances (e.g., documents). We present a novel semi-supervised training algorithm developed for this setting, which is (1) fast enough to support real-time interactive speeds, and (2) at least as accurate as preexisting methods for learning with mixed feature and instance labels. Human annotators in user studies were able to produce near-stateof-the-art classifiers—on several corpora in a variety of application domains—with only a few minutes of effort.</p><p>3 0.16266125 <a title="9-tfidf-3" href="./emnlp-2011-Entire_Relaxation_Path_for_Maximum_Entropy_Problems.html">49 emnlp-2011-Entire Relaxation Path for Maximum Entropy Problems</a></p>
<p>Author: Moshe Dubiner ; Yoram Singer</p><p>Abstract: We discuss and analyze the problem of finding a distribution that minimizes the relative entropy to a prior distribution while satisfying max-norm constraints with respect to an observed distribution. This setting generalizes the classical maximum entropy problems as it relaxes the standard constraints on the observed values. We tackle the problem by introducing a re-parametrization in which the unknown distribution is distilled to a single scalar. We then describe a homotopy between the relaxation parameter and the distribution characterizing parameter. The homotopy also reveals an aesthetic symmetry between the prior distribution and the observed distribution. We then use the reformulated problem to describe a space and time efficient algorithm for tracking the entire relaxation path. Our derivations are based on a compact geomet- ric view of the relaxation path as a piecewise linear function in a two dimensional space of the relaxation-characterization parameters. We demonstrate the usability of our approach by applying the problem to Zipfian distributions over a large alphabet.</p><p>4 0.13412872 <a title="9-tfidf-4" href="./emnlp-2011-Semi-Supervised_Recursive_Autoencoders_for_Predicting_Sentiment_Distributions.html">120 emnlp-2011-Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</a></p>
<p>Author: Richard Socher ; Jeffrey Pennington ; Eric H. Huang ; Andrew Y. Ng ; Christopher D. Manning</p><p>Abstract: We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model’s ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.</p><p>5 0.12447324 <a title="9-tfidf-5" href="./emnlp-2011-Latent_Vector_Weighting_for_Word_Meaning_in_Context.html">80 emnlp-2011-Latent Vector Weighting for Word Meaning in Context</a></p>
<p>Author: Tim Van de Cruys ; Thierry Poibeau ; Anna Korhonen</p><p>Abstract: This paper presents a novel method for the computation of word meaning in context. We make use of a factorization model in which words, together with their window-based context words and their dependency relations, are linked to latent dimensions. The factorization model allows us to determine which dimensions are important for a particular context, and adapt the dependency-based feature vector of the word accordingly. The evaluation on a lexical substitution task carried out for both English and French – indicates that our approach is able to reach better results than state-of-the-art methods in lexical substitution, while at the same time providing more accurate meaning representations. –</p><p>6 0.12118008 <a title="9-tfidf-6" href="./emnlp-2011-Active_Learning_with_Amazon_Mechanical_Turk.html">17 emnlp-2011-Active Learning with Amazon Mechanical Turk</a></p>
<p>7 0.10368689 <a title="9-tfidf-7" href="./emnlp-2011-Dual_Decomposition_with_Many_Overlapping_Components.html">45 emnlp-2011-Dual Decomposition with Many Overlapping Components</a></p>
<p>8 0.10126399 <a title="9-tfidf-8" href="./emnlp-2011-A_Weakly-supervised_Approach_to_Argumentative_Zoning_of_Scientific_Documents.html">12 emnlp-2011-A Weakly-supervised Approach to Argumentative Zoning of Scientific Documents</a></p>
<p>9 0.097461157 <a title="9-tfidf-9" href="./emnlp-2011-Compositional_Matrix-Space_Models_for_Sentiment_Analysis.html">30 emnlp-2011-Compositional Matrix-Space Models for Sentiment Analysis</a></p>
<p>10 0.089113906 <a title="9-tfidf-10" href="./emnlp-2011-Exact_Decoding_of_Phrase-Based_Translation_Models_through_Lagrangian_Relaxation.html">51 emnlp-2011-Exact Decoding of Phrase-Based Translation Models through Lagrangian Relaxation</a></p>
<p>11 0.078485265 <a title="9-tfidf-11" href="./emnlp-2011-Linking_Entities_to_a_Knowledge_Base_with_Query_Expansion.html">90 emnlp-2011-Linking Entities to a Knowledge Base with Query Expansion</a></p>
<p>12 0.074808367 <a title="9-tfidf-12" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<p>13 0.065652475 <a title="9-tfidf-13" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>14 0.061364673 <a title="9-tfidf-14" href="./emnlp-2011-Collaborative_Ranking%3A_A_Case_Study_on_Entity_Linking.html">29 emnlp-2011-Collaborative Ranking: A Case Study on Entity Linking</a></p>
<p>15 0.058924701 <a title="9-tfidf-15" href="./emnlp-2011-Multilayer_Sequence_Labeling.html">96 emnlp-2011-Multilayer Sequence Labeling</a></p>
<p>16 0.055834927 <a title="9-tfidf-16" href="./emnlp-2011-Harnessing_WordNet_Senses_for_Supervised_Sentiment_Classification.html">63 emnlp-2011-Harnessing WordNet Senses for Supervised Sentiment Classification</a></p>
<p>17 0.054861389 <a title="9-tfidf-17" href="./emnlp-2011-Improving_Bilingual_Projections_via_Sparse_Covariance_Matrices.html">73 emnlp-2011-Improving Bilingual Projections via Sparse Covariance Matrices</a></p>
<p>18 0.051993143 <a title="9-tfidf-18" href="./emnlp-2011-Structural_Opinion_Mining_for_Graph-based_Sentiment_Representation.html">126 emnlp-2011-Structural Opinion Mining for Graph-based Sentiment Representation</a></p>
<p>19 0.050040625 <a title="9-tfidf-19" href="./emnlp-2011-A_Correction_Model_for_Word_Alignments.html">3 emnlp-2011-A Correction Model for Word Alignments</a></p>
<p>20 0.049832828 <a title="9-tfidf-20" href="./emnlp-2011-Structured_Sparsity_in_Structured_Prediction.html">129 emnlp-2011-Structured Sparsity in Structured Prediction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.179), (1, -0.143), (2, -0.014), (3, 0.034), (4, 0.203), (5, 0.002), (6, 0.044), (7, -0.235), (8, -0.106), (9, 0.166), (10, 0.014), (11, -0.205), (12, 0.093), (13, -0.128), (14, -0.062), (15, -0.178), (16, 0.11), (17, -0.014), (18, -0.234), (19, 0.058), (20, -0.044), (21, 0.091), (22, -0.06), (23, 0.061), (24, -0.134), (25, 0.002), (26, 0.026), (27, 0.125), (28, -0.073), (29, 0.117), (30, 0.005), (31, -0.157), (32, 0.059), (33, 0.108), (34, -0.065), (35, -0.083), (36, 0.11), (37, 0.039), (38, -0.055), (39, 0.055), (40, 0.101), (41, 0.096), (42, 0.036), (43, 0.029), (44, 0.015), (45, 0.064), (46, -0.046), (47, 0.004), (48, 0.061), (49, 0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.965801 <a title="9-lsi-1" href="./emnlp-2011-A_Non-negative_Matrix_Factorization_Based_Approach_for_Active_Dual_Supervision_from_Document_and_Word_Labels.html">9 emnlp-2011-A Non-negative Matrix Factorization Based Approach for Active Dual Supervision from Document and Word Labels</a></p>
<p>Author: Chao Shen ; Tao Li</p><p>Abstract: In active dual supervision, not only informative examples but also features are selected for labeling to build a high quality classifier with low cost. However, how to measure the informativeness for both examples and feature on the same scale has not been well solved. In this paper, we propose a non-negative matrix factorization based approach to address this issue. We first extend the matrix factorization framework to explicitly model the corresponding relationships between feature classes and examples classes. Then by making use of the reconstruction error, we propose a unified scheme to determine which feature or example a classifier is most likely to benefit from having labeled. Empirical results demonstrate the effectiveness of our proposed methods.</p><p>2 0.72419155 <a title="9-lsi-2" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>Author: Burr Settles</p><p>Abstract: This paper describes DUALIST, an active learning annotation paradigm which solicits and learns from labels on both features (e.g., words) and instances (e.g., documents). We present a novel semi-supervised training algorithm developed for this setting, which is (1) fast enough to support real-time interactive speeds, and (2) at least as accurate as preexisting methods for learning with mixed feature and instance labels. Human annotators in user studies were able to produce near-stateof-the-art classifiers—on several corpora in a variety of application domains—with only a few minutes of effort.</p><p>3 0.60758895 <a title="9-lsi-3" href="./emnlp-2011-A_Weakly-supervised_Approach_to_Argumentative_Zoning_of_Scientific_Documents.html">12 emnlp-2011-A Weakly-supervised Approach to Argumentative Zoning of Scientific Documents</a></p>
<p>Author: Yufan Guo ; Anna Korhonen ; Thierry Poibeau</p><p>Abstract: Documents Anna Korhonen Thierry Poibeau Computer Laboratory LaTTiCe, UMR8094 University of Cambridge, UK CNRS & ENS, France alk2 3 @ cam . ac .uk thierry .po ibeau @ ens . fr tific literature according to categories of information structure (or discourse, rhetorical, argumentative or Argumentative Zoning (AZ) analysis of the argumentative structure of a scientific paper has proved useful for a number of information access tasks. Current approaches to AZ rely on supervised machine learning (ML). – – Requiring large amounts of annotated data, these approaches are expensive to develop and port to different domains and tasks. A potential solution to this problem is to use weaklysupervised ML instead. We investigate the performance of four weakly-supervised classifiers on scientific abstract data annotated for multiple AZ classes. Our best classifier based on the combination of active learning and selftraining outperforms our best supervised classifier, yielding a high accuracy of 81% when using just 10% of the labeled data. This result suggests that weakly-supervised learning could be employed to improve the practical applicability and portability of AZ across different information access tasks.</p><p>4 0.58369923 <a title="9-lsi-4" href="./emnlp-2011-Entire_Relaxation_Path_for_Maximum_Entropy_Problems.html">49 emnlp-2011-Entire Relaxation Path for Maximum Entropy Problems</a></p>
<p>Author: Moshe Dubiner ; Yoram Singer</p><p>Abstract: We discuss and analyze the problem of finding a distribution that minimizes the relative entropy to a prior distribution while satisfying max-norm constraints with respect to an observed distribution. This setting generalizes the classical maximum entropy problems as it relaxes the standard constraints on the observed values. We tackle the problem by introducing a re-parametrization in which the unknown distribution is distilled to a single scalar. We then describe a homotopy between the relaxation parameter and the distribution characterizing parameter. The homotopy also reveals an aesthetic symmetry between the prior distribution and the observed distribution. We then use the reformulated problem to describe a space and time efficient algorithm for tracking the entire relaxation path. Our derivations are based on a compact geomet- ric view of the relaxation path as a piecewise linear function in a two dimensional space of the relaxation-characterization parameters. We demonstrate the usability of our approach by applying the problem to Zipfian distributions over a large alphabet.</p><p>5 0.44357225 <a title="9-lsi-5" href="./emnlp-2011-Active_Learning_with_Amazon_Mechanical_Turk.html">17 emnlp-2011-Active Learning with Amazon Mechanical Turk</a></p>
<p>Author: Florian Laws ; Christian Scheible ; Hinrich Schutze</p><p>Abstract: Supervised classification needs large amounts of annotated training data that is expensive to create. Two approaches that reduce the cost of annotation are active learning and crowdsourcing. However, these two approaches have not been combined successfully to date. We evaluate the utility of active learning in crowdsourcing on two tasks, named entity recognition and sentiment detection, and show that active learning outperforms random selection of annotation examples in a noisy crowdsourcing scenario.</p><p>6 0.37760171 <a title="9-lsi-6" href="./emnlp-2011-Semi-Supervised_Recursive_Autoencoders_for_Predicting_Sentiment_Distributions.html">120 emnlp-2011-Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</a></p>
<p>7 0.35985923 <a title="9-lsi-7" href="./emnlp-2011-Dual_Decomposition_with_Many_Overlapping_Components.html">45 emnlp-2011-Dual Decomposition with Many Overlapping Components</a></p>
<p>8 0.31513846 <a title="9-lsi-8" href="./emnlp-2011-Latent_Vector_Weighting_for_Word_Meaning_in_Context.html">80 emnlp-2011-Latent Vector Weighting for Word Meaning in Context</a></p>
<p>9 0.31182241 <a title="9-lsi-9" href="./emnlp-2011-Exact_Decoding_of_Phrase-Based_Translation_Models_through_Lagrangian_Relaxation.html">51 emnlp-2011-Exact Decoding of Phrase-Based Translation Models through Lagrangian Relaxation</a></p>
<p>10 0.29018399 <a title="9-lsi-10" href="./emnlp-2011-Compositional_Matrix-Space_Models_for_Sentiment_Analysis.html">30 emnlp-2011-Compositional Matrix-Space Models for Sentiment Analysis</a></p>
<p>11 0.27360147 <a title="9-lsi-11" href="./emnlp-2011-Learning_Local_Content_Shift_Detectors_from_Document-level_Information.html">82 emnlp-2011-Learning Local Content Shift Detectors from Document-level Information</a></p>
<p>12 0.2730667 <a title="9-lsi-12" href="./emnlp-2011-Collaborative_Ranking%3A_A_Case_Study_on_Entity_Linking.html">29 emnlp-2011-Collaborative Ranking: A Case Study on Entity Linking</a></p>
<p>13 0.26224115 <a title="9-lsi-13" href="./emnlp-2011-Extreme_Extraction_-_Machine_Reading_in_a_Week.html">57 emnlp-2011-Extreme Extraction - Machine Reading in a Week</a></p>
<p>14 0.2553288 <a title="9-lsi-14" href="./emnlp-2011-Harnessing_WordNet_Senses_for_Supervised_Sentiment_Classification.html">63 emnlp-2011-Harnessing WordNet Senses for Supervised Sentiment Classification</a></p>
<p>15 0.25024721 <a title="9-lsi-15" href="./emnlp-2011-Enhancing_Chinese_Word_Segmentation_Using_Unlabeled_Data.html">48 emnlp-2011-Enhancing Chinese Word Segmentation Using Unlabeled Data</a></p>
<p>16 0.2495641 <a title="9-lsi-16" href="./emnlp-2011-Improving_Bilingual_Projections_via_Sparse_Covariance_Matrices.html">73 emnlp-2011-Improving Bilingual Projections via Sparse Covariance Matrices</a></p>
<p>17 0.2424355 <a title="9-lsi-17" href="./emnlp-2011-Personalized_Recommendation_of_User_Comments_via_Factor_Models.html">104 emnlp-2011-Personalized Recommendation of User Comments via Factor Models</a></p>
<p>18 0.2406715 <a title="9-lsi-18" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>19 0.24063028 <a title="9-lsi-19" href="./emnlp-2011-Class_Label_Enhancement_via_Related_Instances.html">26 emnlp-2011-Class Label Enhancement via Related Instances</a></p>
<p>20 0.23600341 <a title="9-lsi-20" href="./emnlp-2011-Linking_Entities_to_a_Knowledge_Base_with_Query_Expansion.html">90 emnlp-2011-Linking Entities to a Knowledge Base with Query Expansion</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.017), (15, 0.014), (23, 0.112), (36, 0.026), (37, 0.034), (39, 0.327), (45, 0.076), (53, 0.014), (54, 0.031), (57, 0.016), (62, 0.017), (64, 0.017), (66, 0.056), (69, 0.024), (79, 0.032), (82, 0.022), (87, 0.012), (96, 0.053), (98, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.85618645 <a title="9-lda-1" href="./emnlp-2011-Domain-Assisted_Product_Aspect_Hierarchy_Generation%3A_Towards_Hierarchical_Organization_of_Unstructured_Consumer_Reviews.html">43 emnlp-2011-Domain-Assisted Product Aspect Hierarchy Generation: Towards Hierarchical Organization of Unstructured Consumer Reviews</a></p>
<p>Author: Jianxing Yu ; Zheng-Jun Zha ; Meng Wang ; Kai Wang ; Tat-Seng Chua</p><p>Abstract: This paper presents a domain-assisted approach to organize various aspects of a product into a hierarchy by integrating domain knowledge (e.g., the product specifications), as well as consumer reviews. Based on the derived hierarchy, we generate a hierarchical organization of consumer reviews on various product aspects and aggregate consumer opinions on these aspects. With such organization, user can easily grasp the overview of consumer reviews. Furthermore, we apply the hierarchy to the task of implicit aspect identification which aims to infer implicit aspects of the reviews that do not explicitly express those aspects but actually comment on them. The experimental results on 11popular products in four domains demonstrate the effectiveness of our approach.</p><p>same-paper 2 0.71088457 <a title="9-lda-2" href="./emnlp-2011-A_Non-negative_Matrix_Factorization_Based_Approach_for_Active_Dual_Supervision_from_Document_and_Word_Labels.html">9 emnlp-2011-A Non-negative Matrix Factorization Based Approach for Active Dual Supervision from Document and Word Labels</a></p>
<p>Author: Chao Shen ; Tao Li</p><p>Abstract: In active dual supervision, not only informative examples but also features are selected for labeling to build a high quality classifier with low cost. However, how to measure the informativeness for both examples and feature on the same scale has not been well solved. In this paper, we propose a non-negative matrix factorization based approach to address this issue. We first extend the matrix factorization framework to explicitly model the corresponding relationships between feature classes and examples classes. Then by making use of the reconstruction error, we propose a unified scheme to determine which feature or example a classifier is most likely to benefit from having labeled. Empirical results demonstrate the effectiveness of our proposed methods.</p><p>3 0.65170664 <a title="9-lda-3" href="./emnlp-2011-Cache-based_Document-level_Statistical_Machine_Translation.html">25 emnlp-2011-Cache-based Document-level Statistical Machine Translation</a></p>
<p>Author: Zhengxian Gong ; Min Zhang ; Guodong Zhou</p><p>Abstract: Statistical machine translation systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time, ignoring document-level information. In this paper, we propose a cache-based approach to document-level translation. Since caches mainly depend on relevant data to supervise subsequent decisions, it is critical to fill the caches with highly-relevant data of a reasonable size. In this paper, we present three kinds of caches to store relevant document-level information: 1) a dynamic cache, which stores bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document; 2) a static cache, which stores relevant bilingual phrase pairs extracted from similar bilingual document pairs (i.e. source documents similar to the test document and their corresponding target documents) in the training parallel corpus; 3) a topic cache, which stores the target-side topic words related with the test document in the source-side. In particular, three new features are designed to explore various kinds of document-level information in above three kinds of caches. Evaluation shows the effectiveness of our cache-based approach to document-level translation with the performance improvement of 0.8 1 in BLUE score over Moses. Especially, detailed analysis and discussion are presented to give new insights to document-level translation. 1</p><p>4 0.44822809 <a title="9-lda-4" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>Author: Christos Christodoulopoulos ; Sharon Goldwater ; Mark Steedman</p><p>Abstract: In this paper we present a fully unsupervised syntactic class induction system formulated as a Bayesian multinomial mixture model, where each word type is constrained to belong to a single class. By using a mixture model rather than a sequence model (e.g., HMM), we are able to easily add multiple kinds of features, including those at both the type level (morphology features) and token level (context and alignment features, the latter from parallel corpora). Using only context features, our system yields results comparable to state-of-the art, far better than a similar model without the one-class-per-type constraint. Using the additional features provides added benefit, and our final system outperforms the best published results on most of the 25 corpora tested.</p><p>5 0.44596848 <a title="9-lda-5" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>Author: Burr Settles</p><p>Abstract: This paper describes DUALIST, an active learning annotation paradigm which solicits and learns from labels on both features (e.g., words) and instances (e.g., documents). We present a novel semi-supervised training algorithm developed for this setting, which is (1) fast enough to support real-time interactive speeds, and (2) at least as accurate as preexisting methods for learning with mixed feature and instance labels. Human annotators in user studies were able to produce near-stateof-the-art classifiers—on several corpora in a variety of application domains—with only a few minutes of effort.</p><p>6 0.44569406 <a title="9-lda-6" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>7 0.44226679 <a title="9-lda-7" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>8 0.43814987 <a title="9-lda-8" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>9 0.43697527 <a title="9-lda-9" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>10 0.43550646 <a title="9-lda-10" href="./emnlp-2011-Cross-Cutting_Models_of_Lexical_Semantics.html">37 emnlp-2011-Cross-Cutting Models of Lexical Semantics</a></p>
<p>11 0.4354555 <a title="9-lda-11" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>12 0.43461794 <a title="9-lda-12" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>13 0.43384814 <a title="9-lda-13" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>14 0.43319547 <a title="9-lda-14" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>15 0.43187931 <a title="9-lda-15" href="./emnlp-2011-Named_Entity_Recognition_in_Tweets%3A_An_Experimental_Study.html">98 emnlp-2011-Named Entity Recognition in Tweets: An Experimental Study</a></p>
<p>16 0.43109852 <a title="9-lda-16" href="./emnlp-2011-Lateen_EM%3A_Unsupervised_Training_with_Multiple_Objectives%2C_Applied_to_Dependency_Grammar_Induction.html">79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</a></p>
<p>17 0.4310967 <a title="9-lda-17" href="./emnlp-2011-Discovering_Morphological_Paradigms_from_Plain_Text_Using_a_Dirichlet_Process_Mixture_Model.html">39 emnlp-2011-Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model</a></p>
<p>18 0.43011299 <a title="9-lda-18" href="./emnlp-2011-Exploring_Supervised_LDA_Models_for_Assigning_Attributes_to_Adjective-Noun_Phrases.html">56 emnlp-2011-Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases</a></p>
<p>19 0.42957345 <a title="9-lda-19" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<p>20 0.42957333 <a title="9-lda-20" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
