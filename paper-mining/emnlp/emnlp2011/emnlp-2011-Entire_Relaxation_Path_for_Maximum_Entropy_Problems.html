<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>49 emnlp-2011-Entire Relaxation Path for Maximum Entropy Problems</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-49" href="#">emnlp2011-49</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>49 emnlp-2011-Entire Relaxation Path for Maximum Entropy Problems</h1>
<br/><p>Source: <a title="emnlp-2011-49-pdf" href="http://aclweb.org/anthology//D/D11/D11-1087.pdf">pdf</a></p><p>Author: Moshe Dubiner ; Yoram Singer</p><p>Abstract: We discuss and analyze the problem of finding a distribution that minimizes the relative entropy to a prior distribution while satisfying max-norm constraints with respect to an observed distribution. This setting generalizes the classical maximum entropy problems as it relaxes the standard constraints on the observed values. We tackle the problem by introducing a re-parametrization in which the unknown distribution is distilled to a single scalar. We then describe a homotopy between the relaxation parameter and the distribution characterizing parameter. The homotopy also reveals an aesthetic symmetry between the prior distribution and the observed distribution. We then use the reformulated problem to describe a space and time efficient algorithm for tracking the entire relaxation path. Our derivations are based on a compact geomet- ric view of the relaxation path as a piecewise linear function in a two dimensional space of the relaxation-characterization parameters. We demonstrate the usability of our approach by applying the problem to Zipfian distributions over a large alphabet.</p><p>Reference: <a title="emnlp-2011-49-reference" href="../emnlp2011_reference/emnlp-2011-Entire_Relaxation_Path_for_Maximum_Entropy_Problems_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pj', 0.539), ('qj', 0.462), ('relax', 0.294), ('homotop', 0.247), ('pjn', 0.231), ('dual', 0.154), ('aesthet', 0.148), ('entrop', 0.127), ('reform', 0.121), ('mjpj', 0.099), ('mjud', 0.099), ('pujs', 0.099), ('lagrang', 0.095), ('piecew', 0.085), ('multiply', 0.079), ('uj', 0.077), ('symmetry', 0.071), ('zipf', 0.071), ('track', 0.07), ('jx', 0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="49-tfidf-1" href="./emnlp-2011-Entire_Relaxation_Path_for_Maximum_Entropy_Problems.html">49 emnlp-2011-Entire Relaxation Path for Maximum Entropy Problems</a></p>
<p>Author: Moshe Dubiner ; Yoram Singer</p><p>Abstract: We discuss and analyze the problem of finding a distribution that minimizes the relative entropy to a prior distribution while satisfying max-norm constraints with respect to an observed distribution. This setting generalizes the classical maximum entropy problems as it relaxes the standard constraints on the observed values. We tackle the problem by introducing a re-parametrization in which the unknown distribution is distilled to a single scalar. We then describe a homotopy between the relaxation parameter and the distribution characterizing parameter. The homotopy also reveals an aesthetic symmetry between the prior distribution and the observed distribution. We then use the reformulated problem to describe a space and time efficient algorithm for tracking the entire relaxation path. Our derivations are based on a compact geomet- ric view of the relaxation path as a piecewise linear function in a two dimensional space of the relaxation-characterization parameters. We demonstrate the usability of our approach by applying the problem to Zipfian distributions over a large alphabet.</p><p>2 0.19183591 <a title="49-tfidf-2" href="./emnlp-2011-A_Non-negative_Matrix_Factorization_Based_Approach_for_Active_Dual_Supervision_from_Document_and_Word_Labels.html">9 emnlp-2011-A Non-negative Matrix Factorization Based Approach for Active Dual Supervision from Document and Word Labels</a></p>
<p>Author: Chao Shen ; Tao Li</p><p>Abstract: In active dual supervision, not only informative examples but also features are selected for labeling to build a high quality classifier with low cost. However, how to measure the informativeness for both examples and feature on the same scale has not been well solved. In this paper, we propose a non-negative matrix factorization based approach to address this issue. We first extend the matrix factorization framework to explicitly model the corresponding relationships between feature classes and examples classes. Then by making use of the reconstruction error, we propose a unified scheme to determine which feature or example a classifier is most likely to benefit from having labeled. Empirical results demonstrate the effectiveness of our proposed methods.</p><p>3 0.18806341 <a title="49-tfidf-3" href="./emnlp-2011-Exact_Decoding_of_Phrase-Based_Translation_Models_through_Lagrangian_Relaxation.html">51 emnlp-2011-Exact Decoding of Phrase-Based Translation Models through Lagrangian Relaxation</a></p>
<p>Author: Yin-Wen Chang ; Michael Collins</p><p>Abstract: This paper describes an algorithm for exact decoding of phrase-based translation models, based on Lagrangian relaxation. The method recovers exact solutions, with certificates of optimality, on over 99% of test examples. The method is much more efficient than approaches based on linear programming (LP) or integer linear programming (ILP) solvers: these methods are not feasible for anything other than short sentences. We compare our method to MOSES (Koehn et al., 2007), and give precise estimates of the number and magnitude of search errors that MOSES makes.</p><p>4 0.12217823 <a title="49-tfidf-4" href="./emnlp-2011-Dual_Decomposition_with_Many_Overlapping_Components.html">45 emnlp-2011-Dual Decomposition with Many Overlapping Components</a></p>
<p>Author: Andre Martins ; Noah Smith ; Mario Figueiredo ; Pedro Aguiar</p><p>Abstract: Dual decomposition has been recently proposed as a way of combining complementary models, with a boost in predictive power. However, in cases where lightweight decompositions are not readily available (e.g., due to the presence of rich features or logical constraints), the original subgradient algorithm is inefficient. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1</p><p>5 0.10241692 <a title="49-tfidf-5" href="./emnlp-2011-Minimally_Supervised_Event_Causality_Identification.html">92 emnlp-2011-Minimally Supervised Event Causality Identification</a></p>
<p>Author: Quang Do ; Yee Seng Chan ; Dan Roth</p><p>Abstract: This paper develops a minimally supervised approach, based on focused distributional similarity methods and discourse connectives, for identifying of causality relations between events in context. While it has been shown that distributional similarity can help identifying causality, we observe that discourse connectives and the particular discourse relation they evoke in context provide additional information towards determining causality between events. We show that combining discourse relation predictions and distributional similarity methods in a global inference procedure provides additional improvements towards determining event causality.</p><p>6 0.088999495 <a title="49-tfidf-6" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>7 0.050361551 <a title="49-tfidf-7" href="./emnlp-2011-Structural_Opinion_Mining_for_Graph-based_Sentiment_Representation.html">126 emnlp-2011-Structural Opinion Mining for Graph-based Sentiment Representation</a></p>
<p>8 0.048014276 <a title="49-tfidf-8" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<p>9 0.0471044 <a title="49-tfidf-9" href="./emnlp-2011-Optimal_Search_for_Minimum_Error_Rate_Training.html">100 emnlp-2011-Optimal Search for Minimum Error Rate Training</a></p>
<p>10 0.041739061 <a title="49-tfidf-10" href="./emnlp-2011-Structured_Sparsity_in_Structured_Prediction.html">129 emnlp-2011-Structured Sparsity in Structured Prediction</a></p>
<p>11 0.040505059 <a title="49-tfidf-11" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>12 0.035428833 <a title="49-tfidf-12" href="./emnlp-2011-A_Fast_Re-scoring_Strategy_to_Capture_Long-Distance_Dependencies.html">5 emnlp-2011-A Fast Re-scoring Strategy to Capture Long-Distance Dependencies</a></p>
<p>13 0.033816732 <a title="49-tfidf-13" href="./emnlp-2011-Relaxed_Cross-lingual_Projection_of_Constituent_Syntax.html">115 emnlp-2011-Relaxed Cross-lingual Projection of Constituent Syntax</a></p>
<p>14 0.033690248 <a title="49-tfidf-14" href="./emnlp-2011-Multilayer_Sequence_Labeling.html">96 emnlp-2011-Multilayer Sequence Labeling</a></p>
<p>15 0.031380035 <a title="49-tfidf-15" href="./emnlp-2011-Predicting_a_Scientific_Communitys_Response_to_an_Article.html">106 emnlp-2011-Predicting a Scientific Communitys Response to an Article</a></p>
<p>16 0.028460063 <a title="49-tfidf-16" href="./emnlp-2011-Improving_Bilingual_Projections_via_Sparse_Covariance_Matrices.html">73 emnlp-2011-Improving Bilingual Projections via Sparse Covariance Matrices</a></p>
<p>17 0.027099337 <a title="49-tfidf-17" href="./emnlp-2011-Exploiting_Syntactic_and_Distributional_Information_for_Spelling_Correction_with_Web-Scale_N-gram_Models.html">55 emnlp-2011-Exploiting Syntactic and Distributional Information for Spelling Correction with Web-Scale N-gram Models</a></p>
<p>18 0.026416399 <a title="49-tfidf-18" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>19 0.026304401 <a title="49-tfidf-19" href="./emnlp-2011-Latent_Vector_Weighting_for_Word_Meaning_in_Context.html">80 emnlp-2011-Latent Vector Weighting for Word Meaning in Context</a></p>
<p>20 0.025990162 <a title="49-tfidf-20" href="./emnlp-2011-Unsupervised_Dependency_Parsing_without_Gold_Part-of-Speech_Tags.html">141 emnlp-2011-Unsupervised Dependency Parsing without Gold Part-of-Speech Tags</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.102), (1, -0.022), (2, -0.055), (3, 0.022), (4, 0.069), (5, -0.118), (6, 0.049), (7, 0.023), (8, 0.094), (9, 0.173), (10, 0.32), (11, -0.118), (12, 0.102), (13, -0.011), (14, 0.127), (15, 0.173), (16, -0.06), (17, 0.007), (18, -0.117), (19, 0.03), (20, 0.027), (21, 0.027), (22, -0.006), (23, -0.043), (24, -0.023), (25, 0.067), (26, 0.015), (27, 0.029), (28, -0.068), (29, 0.077), (30, 0.033), (31, -0.009), (32, -0.121), (33, 0.133), (34, -0.012), (35, 0.144), (36, 0.047), (37, 0.032), (38, 0.019), (39, 0.126), (40, -0.053), (41, -0.057), (42, -0.0), (43, 0.061), (44, -0.059), (45, 0.1), (46, 0.034), (47, -0.039), (48, 0.1), (49, 0.122)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96935081 <a title="49-lsi-1" href="./emnlp-2011-Entire_Relaxation_Path_for_Maximum_Entropy_Problems.html">49 emnlp-2011-Entire Relaxation Path for Maximum Entropy Problems</a></p>
<p>Author: Moshe Dubiner ; Yoram Singer</p><p>Abstract: We discuss and analyze the problem of finding a distribution that minimizes the relative entropy to a prior distribution while satisfying max-norm constraints with respect to an observed distribution. This setting generalizes the classical maximum entropy problems as it relaxes the standard constraints on the observed values. We tackle the problem by introducing a re-parametrization in which the unknown distribution is distilled to a single scalar. We then describe a homotopy between the relaxation parameter and the distribution characterizing parameter. The homotopy also reveals an aesthetic symmetry between the prior distribution and the observed distribution. We then use the reformulated problem to describe a space and time efficient algorithm for tracking the entire relaxation path. Our derivations are based on a compact geomet- ric view of the relaxation path as a piecewise linear function in a two dimensional space of the relaxation-characterization parameters. We demonstrate the usability of our approach by applying the problem to Zipfian distributions over a large alphabet.</p><p>2 0.6132552 <a title="49-lsi-2" href="./emnlp-2011-Dual_Decomposition_with_Many_Overlapping_Components.html">45 emnlp-2011-Dual Decomposition with Many Overlapping Components</a></p>
<p>Author: Andre Martins ; Noah Smith ; Mario Figueiredo ; Pedro Aguiar</p><p>Abstract: Dual decomposition has been recently proposed as a way of combining complementary models, with a boost in predictive power. However, in cases where lightweight decompositions are not readily available (e.g., due to the presence of rich features or logical constraints), the original subgradient algorithm is inefficient. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1</p><p>3 0.52865613 <a title="49-lsi-3" href="./emnlp-2011-Exact_Decoding_of_Phrase-Based_Translation_Models_through_Lagrangian_Relaxation.html">51 emnlp-2011-Exact Decoding of Phrase-Based Translation Models through Lagrangian Relaxation</a></p>
<p>Author: Yin-Wen Chang ; Michael Collins</p><p>Abstract: This paper describes an algorithm for exact decoding of phrase-based translation models, based on Lagrangian relaxation. The method recovers exact solutions, with certificates of optimality, on over 99% of test examples. The method is much more efficient than approaches based on linear programming (LP) or integer linear programming (ILP) solvers: these methods are not feasible for anything other than short sentences. We compare our method to MOSES (Koehn et al., 2007), and give precise estimates of the number and magnitude of search errors that MOSES makes.</p><p>4 0.50496805 <a title="49-lsi-4" href="./emnlp-2011-A_Non-negative_Matrix_Factorization_Based_Approach_for_Active_Dual_Supervision_from_Document_and_Word_Labels.html">9 emnlp-2011-A Non-negative Matrix Factorization Based Approach for Active Dual Supervision from Document and Word Labels</a></p>
<p>Author: Chao Shen ; Tao Li</p><p>Abstract: In active dual supervision, not only informative examples but also features are selected for labeling to build a high quality classifier with low cost. However, how to measure the informativeness for both examples and feature on the same scale has not been well solved. In this paper, we propose a non-negative matrix factorization based approach to address this issue. We first extend the matrix factorization framework to explicitly model the corresponding relationships between feature classes and examples classes. Then by making use of the reconstruction error, we propose a unified scheme to determine which feature or example a classifier is most likely to benefit from having labeled. Empirical results demonstrate the effectiveness of our proposed methods.</p><p>5 0.31459358 <a title="49-lsi-5" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>Author: Burr Settles</p><p>Abstract: This paper describes DUALIST, an active learning annotation paradigm which solicits and learns from labels on both features (e.g., words) and instances (e.g., documents). We present a novel semi-supervised training algorithm developed for this setting, which is (1) fast enough to support real-time interactive speeds, and (2) at least as accurate as preexisting methods for learning with mixed feature and instance labels. Human annotators in user studies were able to produce near-stateof-the-art classifiers—on several corpora in a variety of application domains—with only a few minutes of effort.</p><p>6 0.25180802 <a title="49-lsi-6" href="./emnlp-2011-Lateen_EM%3A_Unsupervised_Training_with_Multiple_Objectives%2C_Applied_to_Dependency_Grammar_Induction.html">79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</a></p>
<p>7 0.22715224 <a title="49-lsi-7" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<p>8 0.22042213 <a title="49-lsi-8" href="./emnlp-2011-Minimally_Supervised_Event_Causality_Identification.html">92 emnlp-2011-Minimally Supervised Event Causality Identification</a></p>
<p>9 0.19372067 <a title="49-lsi-9" href="./emnlp-2011-Optimal_Search_for_Minimum_Error_Rate_Training.html">100 emnlp-2011-Optimal Search for Minimum Error Rate Training</a></p>
<p>10 0.1743945 <a title="49-lsi-10" href="./emnlp-2011-Improving_Bilingual_Projections_via_Sparse_Covariance_Matrices.html">73 emnlp-2011-Improving Bilingual Projections via Sparse Covariance Matrices</a></p>
<p>11 0.16193587 <a title="49-lsi-11" href="./emnlp-2011-Literal_and_Metaphorical_Sense_Identification_through_Concrete_and_Abstract_Context.html">91 emnlp-2011-Literal and Metaphorical Sense Identification through Concrete and Abstract Context</a></p>
<p>12 0.15270768 <a title="49-lsi-12" href="./emnlp-2011-Structural_Opinion_Mining_for_Graph-based_Sentiment_Representation.html">126 emnlp-2011-Structural Opinion Mining for Graph-based Sentiment Representation</a></p>
<p>13 0.14617066 <a title="49-lsi-13" href="./emnlp-2011-Unsupervised_Information_Extraction_with_Distributional_Prior_Knowledge.html">143 emnlp-2011-Unsupervised Information Extraction with Distributional Prior Knowledge</a></p>
<p>14 0.12939595 <a title="49-lsi-14" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>15 0.12494078 <a title="49-lsi-15" href="./emnlp-2011-Personalized_Recommendation_of_User_Comments_via_Factor_Models.html">104 emnlp-2011-Personalized Recommendation of User Comments via Factor Models</a></p>
<p>16 0.1224148 <a title="49-lsi-16" href="./emnlp-2011-Learning_General_Connotation_of_Words_using_Graph-based_Algorithms.html">81 emnlp-2011-Learning General Connotation of Words using Graph-based Algorithms</a></p>
<p>17 0.12060992 <a title="49-lsi-17" href="./emnlp-2011-Relation_Extraction_with_Relation_Topics.html">114 emnlp-2011-Relation Extraction with Relation Topics</a></p>
<p>18 0.12056025 <a title="49-lsi-18" href="./emnlp-2011-Lexical_Co-occurrence%2C_Statistical_Significance%2C_and_Word_Association.html">86 emnlp-2011-Lexical Co-occurrence, Statistical Significance, and Word Association</a></p>
<p>19 0.1204847 <a title="49-lsi-19" href="./emnlp-2011-A_Fast_Re-scoring_Strategy_to_Capture_Long-Distance_Dependencies.html">5 emnlp-2011-A Fast Re-scoring Strategy to Capture Long-Distance Dependencies</a></p>
<p>20 0.1152158 <a title="49-lsi-20" href="./emnlp-2011-Relaxed_Cross-lingual_Projection_of_Constituent_Syntax.html">115 emnlp-2011-Relaxed Cross-lingual Projection of Constituent Syntax</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.016), (27, 0.053), (37, 0.031), (41, 0.024), (42, 0.019), (50, 0.022), (51, 0.452), (68, 0.223), (96, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.69164801 <a title="49-lda-1" href="./emnlp-2011-Entire_Relaxation_Path_for_Maximum_Entropy_Problems.html">49 emnlp-2011-Entire Relaxation Path for Maximum Entropy Problems</a></p>
<p>Author: Moshe Dubiner ; Yoram Singer</p><p>Abstract: We discuss and analyze the problem of finding a distribution that minimizes the relative entropy to a prior distribution while satisfying max-norm constraints with respect to an observed distribution. This setting generalizes the classical maximum entropy problems as it relaxes the standard constraints on the observed values. We tackle the problem by introducing a re-parametrization in which the unknown distribution is distilled to a single scalar. We then describe a homotopy between the relaxation parameter and the distribution characterizing parameter. The homotopy also reveals an aesthetic symmetry between the prior distribution and the observed distribution. We then use the reformulated problem to describe a space and time efficient algorithm for tracking the entire relaxation path. Our derivations are based on a compact geomet- ric view of the relaxation path as a piecewise linear function in a two dimensional space of the relaxation-characterization parameters. We demonstrate the usability of our approach by applying the problem to Zipfian distributions over a large alphabet.</p><p>2 0.54991829 <a title="49-lda-2" href="./emnlp-2011-A_Word_Reordering_Model_for_Improved_Machine_Translation.html">13 emnlp-2011-A Word Reordering Model for Improved Machine Translation</a></p>
<p>Author: Karthik Visweswariah ; Rajakrishnan Rajkumar ; Ankur Gandhe ; Ananthakrishnan Ramanathan ; Jiri Navratil</p><p>Abstract: Preordering of source side sentences has proved to be useful in improving statistical machine translation. Most work has used a parser in the source language along with rules to map the source language word order into the target language word order. The requirement to have a source language parser is a major drawback, which we seek to overcome in this paper. Instead of using a parser and then using rules to order the source side sentence we learn a model that can directly reorder source side sentences to match target word order using a small parallel corpus with highquality word alignments. Our model learns pairwise costs of a word immediately preced- ing another word. We use the Lin-Kernighan heuristic to find the best source reordering efficiently during training and testing and show that it suffices to provide good quality reordering. We show gains in translation performance based on our reordering model for translating from Hindi to English, Urdu to English (with a public dataset), and English to Hindi. For English to Hindi we show that our technique achieves better performance than a method that uses rules applied to the source side English parse.</p><p>3 0.45757622 <a title="49-lda-3" href="./emnlp-2011-Bootstrapped_Named_Entity_Recognition_for_Product_Attribute_Extraction.html">23 emnlp-2011-Bootstrapped Named Entity Recognition for Product Attribute Extraction</a></p>
<p>Author: Duangmanee Putthividhya ; Junling Hu</p><p>Abstract: We present a named entity recognition (NER) system for extracting product attributes and values from listing titles. Information extraction from short listing titles present a unique challenge, with the lack of informative context and grammatical structure. In this work, we combine supervised NER with bootstrapping to expand the seed list, and output normalized results. Focusing on listings from eBay’s clothing and shoes categories, our bootstrapped NER system is able to identify new brands corresponding to spelling variants and typographical errors of the known brands, as well as identifying novel brands. Among the top 300 new brands predicted, our system achieves 90.33% precision. To output normalized attribute values, we explore several string comparison algorithms and found n-gram substring matching to work well in practice.</p><p>4 0.45717087 <a title="49-lda-4" href="./emnlp-2011-A_Correction_Model_for_Word_Alignments.html">3 emnlp-2011-A Correction Model for Word Alignments</a></p>
<p>Author: J. Scott McCarley ; Abraham Ittycheriah ; Salim Roukos ; Bing Xiang ; Jian-ming Xu</p><p>Abstract: Models of word alignment built as sequences of links have limited expressive power, but are easy to decode. Word aligners that model the alignment matrix can express arbitrary alignments, but are difficult to decode. We propose an alignment matrix model as a correction algorithm to an underlying sequencebased aligner. Then a greedy decoding algorithm enables the full expressive power of the alignment matrix formulation. Improved alignment performance is shown for all nine language pairs tested. The improved alignments also improved translation quality from Chinese to English and English to Italian.</p><p>5 0.45576033 <a title="49-lda-5" href="./emnlp-2011-Lateen_EM%3A_Unsupervised_Training_with_Multiple_Objectives%2C_Applied_to_Dependency_Grammar_Induction.html">79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: We present new training methods that aim to mitigate local optima and slow convergence in unsupervised training by using additional imperfect objectives. In its simplest form, lateen EM alternates between the two objectives of ordinary “soft” and “hard” expectation maximization (EM) algorithms. Switching objectives when stuck can help escape local optima. We find that applying a single such alternation already yields state-of-the-art results for English dependency grammar induction. More elaborate lateen strategies track both objectives, with each validating the moves proposed by the other. Disagreements can signal earlier opportunities to switch or terminate, saving iterations. De-emphasizing fixed points in these ways eliminates some guesswork from tuning EM. An evaluation against a suite of unsupervised dependency parsing tasks, for a vari- ety of languages, showed that lateen strategies significantly speed up training of both EM algorithms, and improve accuracy for hard EM.</p><p>6 0.45439178 <a title="49-lda-6" href="./emnlp-2011-Optimizing_Semantic_Coherence_in_Topic_Models.html">101 emnlp-2011-Optimizing Semantic Coherence in Topic Models</a></p>
<p>7 0.4542492 <a title="49-lda-7" href="./emnlp-2011-Exploring_Supervised_LDA_Models_for_Assigning_Attributes_to_Adjective-Noun_Phrases.html">56 emnlp-2011-Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases</a></p>
<p>8 0.45417872 <a title="49-lda-8" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>9 0.45414206 <a title="49-lda-9" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>10 0.45408151 <a title="49-lda-10" href="./emnlp-2011-Latent_Vector_Weighting_for_Word_Meaning_in_Context.html">80 emnlp-2011-Latent Vector Weighting for Word Meaning in Context</a></p>
<p>11 0.45382643 <a title="49-lda-11" href="./emnlp-2011-Splitting_Noun_Compounds_via_Monolingual_and_Bilingual_Paraphrasing%3A_A_Study_on_Japanese_Katakana_Words.html">124 emnlp-2011-Splitting Noun Compounds via Monolingual and Bilingual Paraphrasing: A Study on Japanese Katakana Words</a></p>
<p>12 0.45347139 <a title="49-lda-12" href="./emnlp-2011-Structural_Opinion_Mining_for_Graph-based_Sentiment_Representation.html">126 emnlp-2011-Structural Opinion Mining for Graph-based Sentiment Representation</a></p>
<p>13 0.45336491 <a title="49-lda-13" href="./emnlp-2011-Cooooooooooooooollllllllllllll%21%21%21%21%21%21%21%21%21%21%21%21%21%21_Using_Word_Lengthening_to_Detect_Sentiment_in_Microblogs.html">33 emnlp-2011-Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using Word Lengthening to Detect Sentiment in Microblogs</a></p>
<p>14 0.45331383 <a title="49-lda-14" href="./emnlp-2011-Lexical_Co-occurrence%2C_Statistical_Significance%2C_and_Word_Association.html">86 emnlp-2011-Lexical Co-occurrence, Statistical Significance, and Word Association</a></p>
<p>15 0.45328426 <a title="49-lda-15" href="./emnlp-2011-The_Imagination_of_Crowds%3A_Conversational_AAC_Language_Modeling_using_Crowdsourcing_and_Large_Data_Sources.html">133 emnlp-2011-The Imagination of Crowds: Conversational AAC Language Modeling using Crowdsourcing and Large Data Sources</a></p>
<p>16 0.45327571 <a title="49-lda-16" href="./emnlp-2011-Enhancing_Chinese_Word_Segmentation_Using_Unlabeled_Data.html">48 emnlp-2011-Enhancing Chinese Word Segmentation Using Unlabeled Data</a></p>
<p>17 0.45318308 <a title="49-lda-17" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>18 0.45299804 <a title="49-lda-18" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>19 0.45297143 <a title="49-lda-19" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>20 0.45288801 <a title="49-lda-20" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
