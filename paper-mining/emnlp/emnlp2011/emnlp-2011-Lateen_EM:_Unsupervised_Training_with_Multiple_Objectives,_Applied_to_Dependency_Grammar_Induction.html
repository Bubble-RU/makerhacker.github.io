<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-79" href="#">emnlp2011-79</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</h1>
<br/><p>Source: <a title="emnlp-2011-79-pdf" href="http://aclweb.org/anthology//D/D11/D11-1117.pdf">pdf</a></p><p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: We present new training methods that aim to mitigate local optima and slow convergence in unsupervised training by using additional imperfect objectives. In its simplest form, lateen EM alternates between the two objectives of ordinary “soft” and “hard” expectation maximization (EM) algorithms. Switching objectives when stuck can help escape local optima. We find that applying a single such alternation already yields state-of-the-art results for English dependency grammar induction. More elaborate lateen strategies track both objectives, with each validating the moves proposed by the other. Disagreements can signal earlier opportunities to switch or terminate, saving iterations. De-emphasizing fixed points in these ways eliminates some guesswork from tuning EM. An evaluation against a suite of unsupervised dependency parsing tasks, for a vari- ety of languages, showed that lateen strategies significantly speed up training of both EM algorithms, and improve accuracy for hard EM.</p><p>Reference: <a title="emnlp-2011-79-reference" href="../emnlp2011_reference/emnlp-2011-Lateen_EM%3A_Unsupervised_Training_with_Multiple_Objectives%2C_Applied_to_Dependency_Grammar_Induction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We present new training methods that aim to mitigate local optima and slow convergence in unsupervised training by using additional imperfect objectives. [sent-8, score-0.178]
</p><p>2 In its simplest form, lateen EM alternates between the two objectives of ordinary “soft” and “hard” expectation maximization (EM) algorithms. [sent-9, score-0.955]
</p><p>3 Switching objectives when stuck can help escape local optima. [sent-10, score-0.244]
</p><p>4 We find that applying a single such alternation already yields state-of-the-art results for English dependency grammar induction. [sent-11, score-0.111]
</p><p>5 More elaborate lateen strategies track both objectives, with each validating the moves proposed by the other. [sent-12, score-0.826]
</p><p>6 An evaluation against a suite of unsupervised dependency parsing tasks, for a vari-  ety of languages, showed that lateen strategies significantly speed up training of both EM algorithms, and improve accuracy for hard EM. [sent-15, score-0.969]
</p><p>7 1269 Another attractive feature that helped make EM instrumental (Meng, 2007) is its initial efficiency: Training tends to begin with large steps in a parameter space, sometimes bypassing many local optima at once. [sent-21, score-0.161]
</p><p>8 Next, its convergence rate necessarily suffers: Disproportion-  ately many (and ever-smaller) steps are needed to finally approach this fixed point, which is almost invariably a local optimum. [sent-23, score-0.134]
</p><p>9 Deciding when to terminate EM often involves guesswork; and finding ways out of local optima requires trial and error. [sent-24, score-0.139]
</p><p>10 Unsupervised objectives are, at best, loosely correlated with extrinsic performance (Pereira and Schabes, 1992; Merialdo, 1994; Liang and Klein, 2008, inter alia). [sent-26, score-0.181]
</p><p>11 The name “lateen” comes from the sea triangular lateen sails —  can take wind on either side, enabling sailing vessels to tack (see Figure 1). [sent-32, score-0.897]
</p><p>12 As a captain can’t count on favorable winds, so an unsupervised learner can’t rely on co-operative gradients:  soft EM maximizes  ProceedEindgisnb oufr tghhe, 2 S0c1o1tl Canodn,f eUrKen,c Jeuol yn 2 E7m–3p1ir,ic 2a0l1 M1. [sent-33, score-0.149]
</p><p>13 tc ho2d0s11 in A Nsasotuciraatlio Lnan fogru Cagoem Ppruotcaetisosninagl, L pinagguesis 1ti2c6s9–1280,  Figure 1: A triangular sail atop a traditional Arab sailing vessel, the dhow (right). [sent-35, score-0.105]
</p><p>14 But the efficient lateen sail worked like a wing (with high pressure on one side and low pressure on the other), allowing a ship to go almost directly into a headwind. [sent-37, score-0.857]
</p><p>15 1  likelihoods of observed data across assignments to hidden variables, whereas hard EM focuses on most likely completions. [sent-40, score-0.121]
</p><p>16 2 These objectives are plausible, yet both can be provably “wrong” (Spitkovsky et al. [sent-41, score-0.139]
</p><p>17 Thus, it is permissible for lateen EM t2o0 maneuver . [sent-44, score-0.77]
</p><p>18 2  The Lateen Family of Algorithms  We propose several strategies that use a secondary objective to improve over standard EM training. [sent-46, score-0.337]
</p><p>19 For hard EM, the secondary objective is that of soft EM; and vice versa if soft EM is the primary algorithm. [sent-47, score-0.762]
</p><p>20 1 Algorithm #1: Simple Lateen EM Simple lateen EM begins by running standard EM to convergence, using a user-supplied initial model, primary objective and definition of convergence. [sent-49, score-1.042]
</p><p>21 A single lateen alternation involves two phases: (i) retraining using the secondary objective, starting from the previous converged solution (once again iterating until convergence, but now of the secondary objective); 1Partially adapted from http ://www . [sent-51, score-1.209]
</p><p>22 h2a)rd d EM; i onn our case, bthi etr corresponding objective is Spitkovsky et al. [sent-62, score-0.108]
</p><p>23 1270 and (ii) retraining using the primary objective again, starting from the latest converged solution (once more to convergence of the primary objective). [sent-65, score-0.472]
</p><p>24 The algorithm stops upon failing to sufficiently improve the primary objective across alternations (applying the standard convergence criterion end-to-end) and returns the best of all models re-estimated during training (as judged by the primary objective). [sent-66, score-0.512]
</p><p>25 2 Algorithm #2: Shallow Lateen EM Same as algorithm #1, but switches back to optimizing the primary objective after a single step with the secondary, during phase (i) of all lateen alternations. [sent-68, score-0.987]
</p><p>26 Thus, the algorithm alternates between optimizing a primary objective to convergence, then stepping away, using one iteration of the secondary optimizer. [sent-69, score-0.411]
</p><p>27 3 Algorithm #3: Early-Stopping Lateen EM This variant runs standard EM but quits early if the secondary objective suffers. [sent-71, score-0.344]
</p><p>28 , a “small-enough” change in the primary objective) with any adverse change of the secondary (i. [sent-74, score-0.257]
</p><p>29 Early-switching lateen EM halts primary optimizers as soon as they hurt the secondary objective and stops secondary optimizers once they harm the primary objective. [sent-80, score-1.537]
</p><p>30 This algorithm terminates when it fails to sufficiently improve the primary objective across a full alternation. [sent-81, score-0.217]
</p><p>31 5 Algorithm #5: Partly-Switching Lateen EM Same as algorithm #4, but again iterating primary objectives to convergence, as in algorithm #1; secondary optimizers still continue to terminate early. [sent-83, score-0.507]
</p><p>32 3  The Task and Study #1  We chose to test the impact of these five lateen algorithms on unsupervised dependency parsing a task in which EM plays an important role (Paskin, 2001 ; Klein and Manning, 2004; Gillenwater et al. [sent-84, score-0.815]
</p><p>33 This entailed two sets of experiments: In study #1, we tested whether single alternations of simple lateen EM (as defined in §2. [sent-86, score-0.809]
</p><p>34 354AonSec-  tion 23 of WSJ (all sentences) for recent state-of-the-art systems and our two experiments (one unlexicalized and one lexicalized) with a single alternation of lateen EM. [sent-90, score-0.866]
</p><p>35 In study #2, we introduced a more sophisticated methodology that uses factorial designs and regressions to evaluate lateen strategies with unsupervised dependency parsing in many languages, after also controlling for other important sources of variation. [sent-92, score-1.023]
</p><p>36 This model was trained using hard EM on WSJ45 (WSJ sentences up to length 45) until successive changes in per-token cross-entropy fell below 2−20 bits (Spitkovsky et al. [sent-95, score-0.14]
</p><p>37 ,o 2d0e1l 0hba;d 2 in01d0eead, converged, by running 10 steps of hard EM on WSJ45 and verifying that its objective did not change much. [sent-98, score-0.26]
</p><p>38 Next, we applied a single alternation of simple lateen EM: first running soft EM (this took 101 steps, using the same termination criterion), followed by hard EM (again to convergence another 23 iterations). [sent-99, score-1.26]
</p><p>39 The result was a decrease in hard EM’s cross-entropy, from 3. [sent-100, score-0.099]
</p><p>40 4 Our first experiment showed that lateen EM holds promise for simple models. [sent-106, score-0.77]
</p><p>41 , 1993) into unlabeled reference dependency parses using deterministic “head-percolation” rules (Collins,  1999); sentence root symbols (but not punctuation) arcs count towards accuracies (Paskin, 2001; Klein and Manning, 2004). [sent-114, score-0.131]
</p><p>42 ’s (2009) method (also the approach 1271 parses; this took 24 steps with hard EM. [sent-116, score-0.122]
</p><p>43 We then applied another single lateen alternation: This time, soft EM ran for 37 steps, hard EM took another 14, and the new model again improved, by 1. [sent-117, score-1.018]
</p><p>44 This last model is competitive with the state-ofthe-art; moreover, gains from single applications of simple lateen alternations (2. [sent-123, score-0.809]
</p><p>45 4  Methodology for Study #2  Study #1 suggests that lateen EM can improve grammar induction in English. [sent-127, score-0.796]
</p><p>46 We therefore use a factorial experimental design and regression analyses with a variety of lateen strategies. [sent-129, score-0.884]
</p><p>47 Two regressions one predicting accuracy, the other, the number of iterations capture the effects that la—  —  teen algorithms have on performance and efficiency, relative to standard EM training. [sent-130, score-0.159]
</p><p>48 We also explored the impact from the quality of an initial model (using both uniform and ad hoc initializers), the choice of a primary objective (i. [sent-132, score-0.217]
</p><p>49 , soft or hard EM), and the quantity and complexity oftraining data (shorter versus both short and long sentences). [sent-134, score-0.248]
</p><p>50 5 Scoring Function We report directed accuracies fractions of correctly guessed (unlabeled) dependency arcs, including arcs from sentence root symbols, as is standard —  practice (Paskin, 2001; Klein and Manning, 2004). [sent-157, score-0.137]
</p><p>51 5  Experiments  We now summarize our baseline models and briefly review the proposed lateen algorithms. [sent-160, score-0.77]
</p><p>52 For details of the default systems (standard soft and hard EM), all control variables and both regressions (against final accuracies and iteration counts) see Appendix A. [sent-161, score-0.374]
</p><p>53 1 Baseline Models We tested a total of six baseline models, experimenting with two types of alternatives: (i) strategies that perturb stuck models directly, by smoothing, ignoring secondary objectives; and (ii) shallow applications of a single EM step, ignoring convergence. [sent-163, score-0.275]
</p><p>54 Baseline B1 alternates running standard EM to convergence and smoothing. [sent-164, score-0.184]
</p><p>55 Another shallow baseline, B3, alternates single steps of soft 8With the exception of Arabic ’07, from which we discarded a single sentence containing 145 non-punctuation tokens. [sent-166, score-0.245]
</p><p>56 9  and hard Three such baselines begin with hard EM (marked with the subscript h); and three more start with soft EM (marked with the subscript s). [sent-168, score-0.404]
</p><p>57 2 Lateen Models Ten models, A{1, 2, 3, 4, 5}{h,s}, correspond to our lateen algorithms #1–5 (§2), starting with either hard or sno aftl gEoMrit’hs objective, §to2 b,e s tuarsteidn as wthiteh primary. [sent-170, score-0.909]
</p><p>58 6  Results  dency accuracy (∆a) and multiplicative changes in the number of iterations before terminating (∆i) for all baseline models and lateen algorithms, relative to standard training: soft EM (left) and hard EM (right). [sent-171, score-1.176]
</p><p>59 5% higher, on average, compared to standard Viterbi training; A1s is only 30% slower than standard soft EM, but does not impact its accuracy at all, on average. [sent-184, score-0.281]
</p><p>60 Viterbi EM converges after 47 iterations, 9It approximates a mixture (the average of soft and hard objectives) — a natural comparison, computable via gradients and standard optimization algorithms, such as L-BFGS (Liu and Nocedal, 1989). [sent-186, score-0.298]
</p><p>61 We did not explore exact interpolations, however, because replacing EM is itself a significant confounder, even with unchanged objectives (Berg-Kirkpatrick et al. [sent-187, score-0.139]
</p><p>62 The two curves are primary and secondary objectives (soft EM’s lies below, as sentence yields are at least as likely as parse trees): shaded regions indicate iterations of hard EM (primary); and annotated values are measurements upon each optimizer’s convergence (soft EM’s are parenthesized). [sent-198, score-0.61]
</p><p>63 Three alternations of lateen EM (totaling 265 iterations) further decrease the primary objective to 3. [sent-203, score-1.026]
</p><p>64 5% higher, on average, compared to standard Viterbi training; A2s is again 30% slower than standard soft EM and —  also has no measurable impact on parsing accuracy. [sent-211, score-0.281]
</p><p>65 3 A3{h,s} Early-Stopping Lateen EM Both A3h and A3s run 30% faster, on average, than standard training with hard or soft EM; and neither heuristic causes a statistical change to accuracy. [sent-213, score-0.273]
</p><p>66 Table 3 shows accuracies and iteration counts for 10 (of 23) train/test splits that terminate early with A3s (in one particular, example setting). [sent-214, score-0.159]
</p><p>67 These runs are nearly twice as fast, and only two score (slightly) lower, compared to standard training using soft EM. [sent-215, score-0.201]
</p><p>68 0% higher, on average, compared to standard Viterbi training; A4s is, in fact, 20% faster than standard soft EM, but still has no measurable impact on accuracy. [sent-219, score-0.22]
</p><p>69 9% higher, on average, compared to standard Viterbi training; A5s is 20% slower than soft EM, but, again, no more accurate. [sent-223, score-0.235]
</p><p>70 1273 CoNLL Year Soft EM A3s —  ation counts for the 10 (of 23) train/test splits affected by early termination (setting: soft EM’s primary objective, trained using shorter sentences and ad-hoc initialization). [sent-225, score-0.386]
</p><p>71 7  Discussion  Lateen strategies improve dependency grammar induction in several ways. [sent-226, score-0.105]
</p><p>72 This technique could be used to (more) fairly compare learners with radically different objectives (e. [sent-228, score-0.139]
</p><p>73 The second benefit is improved performance, but only starting with hard EM. [sent-231, score-0.117]
</p><p>74 Initial local optima discovered by soft EM are such that the impact on accuracy of all subsequent heuristics is indistinguishable from noise (it’s not even negative). [sent-232, score-0.265]
</p><p>75 But for hard EM, lateen strategies consistently improve accuracy by 1. [sent-233, score-0.946]
</p><p>76 5% as an algorithm follows the secondary objective longer (a single step, until the primary objective gets worse, or to convergence). [sent-236, score-0.473]
</p><p>77 Our results suggest that soft EM should use early termination to improve efficiency. [sent-237, score-0.252]
</p><p>78 Hard EM, by contrast, could use any lateen strategy to improve either efficiency or performance, or to strike a balance. [sent-238, score-0.807]
</p><p>79 1 Avoiding and/or Escaping Local Attractors Simple lateen EM is similar to Dhillon et al. [sent-240, score-0.77]
</p><p>80 Their “ping-pong” strategy alternates batch and incremental EM, exploits the strong points of each, and improves a shared objective at every step. [sent-242, score-0.154]
</p><p>81 Unlike generalized (GEM) variants (Neal and  Hinton, 1999), lateen EM uses multiple objectives: it sacrifices the primary in the short run, to escape local optima; in the long run, it also does no harm, by construction (as it returns the best model seen). [sent-243, score-0.94]
</p><p>82 Of the meta-heuristics that use more than a standard, scalar objective, deterministic annealing (DA) (Rose, 1998) is closest to lateen EM. [sent-244, score-0.808]
</p><p>83 DA perturbs objective functions, instead of manipulating solutions directly. [sent-245, score-0.108]
</p><p>84 Smith and Eisner (2004) employed DA to improve part-of-speech disambiguation, but found that objectives had to be further “skewed,” using domain knowledge, before it helped (constituent) grammar induction. [sent-249, score-0.187]
</p><p>85 (For this reason, we did not experiment with DA, despite its strong similarities to lateen EM. [sent-250, score-0.77]
</p><p>86 ) Smith and Eisner (2004) used a “temperature” β to anneal a flat uniform distribution (β = 0) into soft EM’s non-convex objective (β = 1). [sent-251, score-0.257]
</p><p>87 In their framework, hard EM corresponds to β −→ ∞, so the algorithms differ only in their β-schedule: DA’s is continuous, from 0 to 1; la-  teen EM’s is a discrete alternation, of 1 and +∞. [sent-252, score-0.15]
</p><p>88 10One can think of this as a kind of “beam search” (Lowerre, 1976), with soft EM expanding and hard EM pruning a frontier. [sent-266, score-0.248]
</p><p>89 1274 Early-stopping lateen EM tethers termination to a sign change in the direction ofa secondary objective,  ,  similarly to (cross-)validation (Stone, 1974; Geisser, 1975; Arlot and Celisse, 2010), but without splitting data it trains using all examples, at all times. [sent-267, score-0.985]
</p><p>90 In contrast, lateen EM uses the same data, features, model and essentially the same algorithms, changing only their objective functions: it makes no assumptions, but guarantees not to harm the primary objective. [sent-272, score-1.036]
</p><p>91 Some of these distinctions have become blurred —  with time: an objective  Collins  and Singer (1999) introduced  function  into co-training;  (also based on agreement)  Goldman  and Zhou (2000),  Ng  and Cardie (2003) and Chan et al. [sent-273, score-0.108]
</p><p>92 9  Conclusions and Future Work  Lateen strategies can improve performance and efficiency for dependency grammar induction with the DMV. [sent-287, score-0.142]
</p><p>93 Early-stopping lateen EM is 30% faster than standard training, without affecting accuracy it reduces guesswork in terminating EM. [sent-288, score-0.896]
</p><p>94 At the other extreme, simple lateen EM is slower, but significantly improves accuracy by 5. [sent-289, score-0.791]
</p><p>95 5%, on average for hard EM, escaping some of its local optima. [sent-290, score-0.156]
</p><p>96 It would be interesting to apply lateen algorithms to advanced parsing models (Blunsom and Cohn, 2010; Headden et al. [sent-291, score-0.792]
</p><p>97 14We used full factorial designs for clarity of exposition. [sent-301, score-0.123]
</p><p>98 But many fewer experiments would suffice, especially in regression models without interaction terms: for the more efficient fractional factorial designs, as well as for randomized block designs and full factorial designs, see Montgomery (2005, Ch. [sent-302, score-0.237]
</p><p>99 In the efficiency regression, dependent variables are logarithms of the numbers of iterations. [sent-316, score-0.121]
</p><p>100 The undirected and integrated factors only affect the regression for accuracies (see Table 4, left); remaining factors participate also in the running times regression (see Table 4, right). [sent-324, score-0.2]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lateen', 0.77), ('em', 0.382), ('soft', 0.149), ('secondary', 0.148), ('objectives', 0.139), ('primary', 0.109), ('objective', 0.108), ('hard', 0.099), ('convergence', 0.083), ('termination', 0.067), ('factorial', 0.067), ('optima', 0.067), ('spitkovsky', 0.064), ('alternation', 0.062), ('slower', 0.061), ('paskin', 0.061), ('bpt', 0.057), ('strategies', 0.056), ('designs', 0.056), ('alia', 0.056), ('accuracies', 0.054), ('regressions', 0.051), ('optimizers', 0.049), ('regression', 0.047), ('alternates', 0.046), ('viterbi', 0.046), ('terminate', 0.044), ('stuck', 0.044), ('guesswork', 0.043), ('sail', 0.043), ('sails', 0.043), ('inter', 0.042), ('alternations', 0.039), ('klein', 0.038), ('efficiency', 0.037), ('terminating', 0.037), ('logarithms', 0.037), ('sailing', 0.037), ('early', 0.036), ('arcs', 0.035), ('unlexicalized', 0.034), ('escape', 0.033), ('appendix', 0.033), ('iterations', 0.032), ('wsj', 0.03), ('running', 0.03), ('da', 0.029), ('harm', 0.029), ('attractors', 0.029), ('escaping', 0.029), ('halting', 0.029), ('tacking', 0.029), ('teen', 0.029), ('local', 0.028), ('runs', 0.027), ('shallow', 0.027), ('dependent', 0.026), ('converged', 0.026), ('grammar', 0.026), ('standard', 0.025), ('gradients', 0.025), ('splits', 0.025), ('goldman', 0.025), ('triangular', 0.025), ('manning', 0.024), ('predictors', 0.024), ('dependency', 0.023), ('steps', 0.023), ('undirected', 0.022), ('redundancies', 0.022), ('hiyan', 0.022), ('wind', 0.022), ('pressure', 0.022), ('helped', 0.022), ('headden', 0.022), ('likelihoods', 0.022), ('changes', 0.022), ('algorithms', 0.022), ('smith', 0.021), ('accuracy', 0.021), ('variables', 0.021), ('begin', 0.021), ('criterion', 0.021), ('multiplicative', 0.021), ('optimizer', 0.021), ('measurable', 0.021), ('eisner', 0.02), ('lexicalized', 0.02), ('guarantees', 0.02), ('smoothed', 0.02), ('views', 0.02), ('annealing', 0.019), ('retraining', 0.019), ('bits', 0.019), ('valentin', 0.019), ('deterministic', 0.019), ('starting', 0.018), ('stops', 0.018), ('iterating', 0.018), ('subscript', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="79-tfidf-1" href="./emnlp-2011-Lateen_EM%3A_Unsupervised_Training_with_Multiple_Objectives%2C_Applied_to_Dependency_Grammar_Induction.html">79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: We present new training methods that aim to mitigate local optima and slow convergence in unsupervised training by using additional imperfect objectives. In its simplest form, lateen EM alternates between the two objectives of ordinary “soft” and “hard” expectation maximization (EM) algorithms. Switching objectives when stuck can help escape local optima. We find that applying a single such alternation already yields state-of-the-art results for English dependency grammar induction. More elaborate lateen strategies track both objectives, with each validating the moves proposed by the other. Disagreements can signal earlier opportunities to switch or terminate, saving iterations. De-emphasizing fixed points in these ways eliminates some guesswork from tuning EM. An evaluation against a suite of unsupervised dependency parsing tasks, for a vari- ety of languages, showed that lateen strategies significantly speed up training of both EM algorithms, and improve accuracy for hard EM.</p><p>2 0.10732554 <a title="79-tfidf-2" href="./emnlp-2011-Unsupervised_Dependency_Parsing_without_Gold_Part-of-Speech_Tags.html">141 emnlp-2011-Unsupervised Dependency Parsing without Gold Part-of-Speech Tags</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Angel X. Chang ; Daniel Jurafsky</p><p>Abstract: We show that categories induced by unsupervised word clustering can surpass the performance of gold part-of-speech tags in dependency grammar induction. Unlike classic clustering algorithms, our method allows a word to have different tags in different contexts. In an ablative analysis, we first demonstrate that this context-dependence is crucial to the superior performance of gold tags — requiring a word to always have the same part-ofspeech significantly degrades the performance of manual tags in grammar induction, eliminating the advantage that human annotation has over unsupervised tags. We then introduce a sequence modeling technique that combines the output of a word clustering algorithm with context-colored noise, to allow words to be tagged differently in different contexts. With these new induced tags as input, our state-of- the-art dependency grammar inducer achieves 59. 1% directed accuracy on Section 23 (all sentences) of the Wall Street Journal (WSJ) corpus — 0.7% higher than using gold tags.</p><p>3 0.068741791 <a title="79-tfidf-3" href="./emnlp-2011-Unsupervised_Structure_Prediction_with_Non-Parallel_Multilingual_Guidance.html">146 emnlp-2011-Unsupervised Structure Prediction with Non-Parallel Multilingual Guidance</a></p>
<p>Author: Shay B. Cohen ; Dipanjan Das ; Noah A. Smith</p><p>Abstract: We describe a method for prediction of linguistic structure in a language for which only unlabeled data is available, using annotated data from a set of one or more helper languages. Our approach is based on a model that locally mixes between supervised models from the helper languages. Parallel data is not used, allowing the technique to be applied even in domains where human-translated texts are unavailable. We obtain state-of-theart performance for two tasks of structure prediction: unsupervised part-of-speech tagging and unsupervised dependency parsing.</p><p>4 0.055700976 <a title="79-tfidf-4" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>Author: Keith Hall ; Ryan McDonald ; Jason Katz-Brown ; Michael Ringgaard</p><p>Abstract: We present an online learning algorithm for training parsers which allows for the inclusion of multiple objective functions. The primary example is the extension of a standard supervised parsing objective function with additional loss-functions, either based on intrinsic parsing quality or task-specific extrinsic measures of quality. Our empirical results show how this approach performs for two dependency parsing algorithms (graph-based and transition-based parsing) and how it achieves increased performance on multiple target tasks including reordering for machine translation and parser adaptation.</p><p>5 0.047316637 <a title="79-tfidf-5" href="./emnlp-2011-Multi-Source_Transfer_of_Delexicalized_Dependency_Parsers.html">95 emnlp-2011-Multi-Source Transfer of Delexicalized Dependency Parsers</a></p>
<p>Author: Ryan McDonald ; Slav Petrov ; Keith Hall</p><p>Abstract: We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source lan- guages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.</p><p>6 0.044276357 <a title="79-tfidf-6" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>7 0.043801356 <a title="79-tfidf-7" href="./emnlp-2011-A_Fast%2C_Accurate%2C_Non-Projective%2C_Semantically-Enriched_Parser.html">4 emnlp-2011-A Fast, Accurate, Non-Projective, Semantically-Enriched Parser</a></p>
<p>8 0.041217253 <a title="79-tfidf-8" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>9 0.038339309 <a title="79-tfidf-9" href="./emnlp-2011-Minimum_Imputed-Risk%3A_Unsupervised_Discriminative_Training_for_Machine_Translation.html">93 emnlp-2011-Minimum Imputed-Risk: Unsupervised Discriminative Training for Machine Translation</a></p>
<p>10 0.035664324 <a title="79-tfidf-10" href="./emnlp-2011-Accurate_Parsing_with_Compact_Tree-Substitution_Grammars%3A_Double-DOP.html">16 emnlp-2011-Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP</a></p>
<p>11 0.034310482 <a title="79-tfidf-11" href="./emnlp-2011-Multilayer_Sequence_Labeling.html">96 emnlp-2011-Multilayer Sequence Labeling</a></p>
<p>12 0.034303546 <a title="79-tfidf-12" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>13 0.033779148 <a title="79-tfidf-13" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>14 0.032904066 <a title="79-tfidf-14" href="./emnlp-2011-Optimal_Search_for_Minimum_Error_Rate_Training.html">100 emnlp-2011-Optimal Search for Minimum Error Rate Training</a></p>
<p>15 0.031743538 <a title="79-tfidf-15" href="./emnlp-2011-Personalized_Recommendation_of_User_Comments_via_Factor_Models.html">104 emnlp-2011-Personalized Recommendation of User Comments via Factor Models</a></p>
<p>16 0.031019231 <a title="79-tfidf-16" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>17 0.028887287 <a title="79-tfidf-17" href="./emnlp-2011-Enhancing_Chinese_Word_Segmentation_Using_Unlabeled_Data.html">48 emnlp-2011-Enhancing Chinese Word Segmentation Using Unlabeled Data</a></p>
<p>18 0.028651232 <a title="79-tfidf-18" href="./emnlp-2011-Discovering_Morphological_Paradigms_from_Plain_Text_Using_a_Dirichlet_Process_Mixture_Model.html">39 emnlp-2011-Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model</a></p>
<p>19 0.028017467 <a title="79-tfidf-19" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>20 0.027878057 <a title="79-tfidf-20" href="./emnlp-2011-Exact_Decoding_of_Phrase-Based_Translation_Models_through_Lagrangian_Relaxation.html">51 emnlp-2011-Exact Decoding of Phrase-Based Translation Models through Lagrangian Relaxation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.115), (1, 0.022), (2, -0.017), (3, 0.07), (4, -0.003), (5, 0.05), (6, -0.052), (7, -0.044), (8, -0.053), (9, 0.017), (10, -0.03), (11, 0.002), (12, 0.006), (13, 0.006), (14, 0.018), (15, 0.035), (16, 0.026), (17, 0.028), (18, -0.019), (19, 0.085), (20, 0.009), (21, 0.077), (22, 0.028), (23, 0.023), (24, 0.073), (25, 0.07), (26, -0.078), (27, 0.045), (28, -0.036), (29, 0.013), (30, -0.028), (31, 0.021), (32, 0.033), (33, 0.005), (34, -0.048), (35, -0.059), (36, -0.082), (37, -0.051), (38, 0.141), (39, 0.16), (40, 0.012), (41, -0.048), (42, 0.017), (43, -0.026), (44, -0.109), (45, 0.129), (46, 0.264), (47, -0.278), (48, -0.288), (49, -0.11)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96062642 <a title="79-lsi-1" href="./emnlp-2011-Lateen_EM%3A_Unsupervised_Training_with_Multiple_Objectives%2C_Applied_to_Dependency_Grammar_Induction.html">79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: We present new training methods that aim to mitigate local optima and slow convergence in unsupervised training by using additional imperfect objectives. In its simplest form, lateen EM alternates between the two objectives of ordinary “soft” and “hard” expectation maximization (EM) algorithms. Switching objectives when stuck can help escape local optima. We find that applying a single such alternation already yields state-of-the-art results for English dependency grammar induction. More elaborate lateen strategies track both objectives, with each validating the moves proposed by the other. Disagreements can signal earlier opportunities to switch or terminate, saving iterations. De-emphasizing fixed points in these ways eliminates some guesswork from tuning EM. An evaluation against a suite of unsupervised dependency parsing tasks, for a vari- ety of languages, showed that lateen strategies significantly speed up training of both EM algorithms, and improve accuracy for hard EM.</p><p>2 0.5284068 <a title="79-lsi-2" href="./emnlp-2011-Unsupervised_Dependency_Parsing_without_Gold_Part-of-Speech_Tags.html">141 emnlp-2011-Unsupervised Dependency Parsing without Gold Part-of-Speech Tags</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Angel X. Chang ; Daniel Jurafsky</p><p>Abstract: We show that categories induced by unsupervised word clustering can surpass the performance of gold part-of-speech tags in dependency grammar induction. Unlike classic clustering algorithms, our method allows a word to have different tags in different contexts. In an ablative analysis, we first demonstrate that this context-dependence is crucial to the superior performance of gold tags — requiring a word to always have the same part-ofspeech significantly degrades the performance of manual tags in grammar induction, eliminating the advantage that human annotation has over unsupervised tags. We then introduce a sequence modeling technique that combines the output of a word clustering algorithm with context-colored noise, to allow words to be tagged differently in different contexts. With these new induced tags as input, our state-of- the-art dependency grammar inducer achieves 59. 1% directed accuracy on Section 23 (all sentences) of the Wall Street Journal (WSJ) corpus — 0.7% higher than using gold tags.</p><p>3 0.40001941 <a title="79-lsi-3" href="./emnlp-2011-Unsupervised_Information_Extraction_with_Distributional_Prior_Knowledge.html">143 emnlp-2011-Unsupervised Information Extraction with Distributional Prior Knowledge</a></p>
<p>Author: Cane Wing-ki Leung ; Jing Jiang ; Kian Ming A. Chai ; Hai Leong Chieu ; Loo-Nin Teow</p><p>Abstract: We address the task of automatic discovery of information extraction template from a given text collection. Our approach clusters candidate slot fillers to identify meaningful template slots. We propose a generative model that incorporates distributional prior knowledge to help distribute candidates in a document into appropriate slots. Empirical results suggest that the proposed prior can bring substantial improvements to our task as compared to a K-means baseline and a Gaussian mixture model baseline. Specifically, the proposed prior has shown to be effective when coupled with discriminative features of the candidates.</p><p>4 0.29954058 <a title="79-lsi-4" href="./emnlp-2011-Hierarchical_Verb_Clustering_Using_Graph_Factorization.html">67 emnlp-2011-Hierarchical Verb Clustering Using Graph Factorization</a></p>
<p>Author: Lin Sun ; Anna Korhonen</p><p>Abstract: Most previous research on verb clustering has focussed on acquiring flat classifications from corpus data, although many manually built classifications are taxonomic in nature. Also Natural Language Processing (NLP) applications benefit from taxonomic classifications because they vary in terms of the granularity they require from a classification. We introduce a new clustering method called Hierarchical Graph Factorization Clustering (HGFC) and extend it so that it is optimal for the task. Our results show that HGFC outperforms the frequently used agglomerative clustering on a hierarchical test set extracted from VerbNet, and that it yields state-of-the-art performance also on a flat test set. We demonstrate how the method can be used to acquire novel classifications as well as to extend existing ones on the basis of some prior knowledge about the classification.</p><p>5 0.2943927 <a title="79-lsi-5" href="./emnlp-2011-Unsupervised_Structure_Prediction_with_Non-Parallel_Multilingual_Guidance.html">146 emnlp-2011-Unsupervised Structure Prediction with Non-Parallel Multilingual Guidance</a></p>
<p>Author: Shay B. Cohen ; Dipanjan Das ; Noah A. Smith</p><p>Abstract: We describe a method for prediction of linguistic structure in a language for which only unlabeled data is available, using annotated data from a set of one or more helper languages. Our approach is based on a model that locally mixes between supervised models from the helper languages. Parallel data is not used, allowing the technique to be applied even in domains where human-translated texts are unavailable. We obtain state-of-theart performance for two tasks of structure prediction: unsupervised part-of-speech tagging and unsupervised dependency parsing.</p><p>6 0.22407743 <a title="79-lsi-6" href="./emnlp-2011-Minimum_Imputed-Risk%3A_Unsupervised_Discriminative_Training_for_Machine_Translation.html">93 emnlp-2011-Minimum Imputed-Risk: Unsupervised Discriminative Training for Machine Translation</a></p>
<p>7 0.22251911 <a title="79-lsi-7" href="./emnlp-2011-Accurate_Parsing_with_Compact_Tree-Substitution_Grammars%3A_Double-DOP.html">16 emnlp-2011-Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP</a></p>
<p>8 0.21426181 <a title="79-lsi-8" href="./emnlp-2011-A_Weakly-supervised_Approach_to_Argumentative_Zoning_of_Scientific_Documents.html">12 emnlp-2011-A Weakly-supervised Approach to Argumentative Zoning of Scientific Documents</a></p>
<p>9 0.2082407 <a title="79-lsi-9" href="./emnlp-2011-Corpus-Guided_Sentence_Generation_of_Natural_Images.html">34 emnlp-2011-Corpus-Guided Sentence Generation of Natural Images</a></p>
<p>10 0.20820017 <a title="79-lsi-10" href="./emnlp-2011-Entire_Relaxation_Path_for_Maximum_Entropy_Problems.html">49 emnlp-2011-Entire Relaxation Path for Maximum Entropy Problems</a></p>
<p>11 0.20357014 <a title="79-lsi-11" href="./emnlp-2011-Literal_and_Metaphorical_Sense_Identification_through_Concrete_and_Abstract_Context.html">91 emnlp-2011-Literal and Metaphorical Sense Identification through Concrete and Abstract Context</a></p>
<p>12 0.19088523 <a title="79-lsi-12" href="./emnlp-2011-Multi-Source_Transfer_of_Delexicalized_Dependency_Parsers.html">95 emnlp-2011-Multi-Source Transfer of Delexicalized Dependency Parsers</a></p>
<p>13 0.17636484 <a title="79-lsi-13" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>14 0.17056793 <a title="79-lsi-14" href="./emnlp-2011-A_Simple_Word_Trigger_Method_for_Social_Tag_Suggestion.html">11 emnlp-2011-A Simple Word Trigger Method for Social Tag Suggestion</a></p>
<p>15 0.16867757 <a title="79-lsi-15" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>16 0.16588372 <a title="79-lsi-16" href="./emnlp-2011-A_Cascaded_Classification_Approach_to_Semantic_Head_Recognition.html">2 emnlp-2011-A Cascaded Classification Approach to Semantic Head Recognition</a></p>
<p>17 0.16433331 <a title="79-lsi-17" href="./emnlp-2011-Predicting_a_Scientific_Communitys_Response_to_an_Article.html">106 emnlp-2011-Predicting a Scientific Communitys Response to an Article</a></p>
<p>18 0.16425064 <a title="79-lsi-18" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>19 0.16405818 <a title="79-lsi-19" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>20 0.16388002 <a title="79-lsi-20" href="./emnlp-2011-Linear_Text_Segmentation_Using_Affinity_Propagation.html">88 emnlp-2011-Linear Text Segmentation Using Affinity Propagation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(15, 0.01), (17, 0.235), (23, 0.145), (36, 0.028), (37, 0.029), (45, 0.05), (53, 0.024), (54, 0.036), (57, 0.011), (62, 0.032), (64, 0.051), (66, 0.077), (69, 0.021), (79, 0.04), (82, 0.018), (90, 0.023), (96, 0.057), (98, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86111557 <a title="79-lda-1" href="./emnlp-2011-Predicting_Thread_Discourse_Structure_over_Technical_Web_Forums.html">105 emnlp-2011-Predicting Thread Discourse Structure over Technical Web Forums</a></p>
<p>Author: Li Wang ; Marco Lui ; Su Nam Kim ; Joakim Nivre ; Timothy Baldwin</p><p>Abstract: Online discussion forums are a valuable means for users to resolve specific information needs, both interactively for the participants and statically for users who search/browse over historical thread data. However, the complex structure of forum threads can make it difficult for users to extract relevant information. The discourse structure of web forum threads, in the form of labelled dependency relationships between posts, has the potential to greatly improve information access over web forum archives. In this paper, we present the task of parsing user forum threads to determine the labelled dependencies between posts. Three methods, including a dependency parsing approach, are proposed to jointly classify the links (relationships) between posts and the dialogue act (type) of each link. The proposed methods significantly surpass an informed baseline. We also experiment with “in situ” classification of evolving threads, and establish that our best methods are able to perform equivalently well over partial threads as complete threads.</p><p>same-paper 2 0.79054064 <a title="79-lda-2" href="./emnlp-2011-Lateen_EM%3A_Unsupervised_Training_with_Multiple_Objectives%2C_Applied_to_Dependency_Grammar_Induction.html">79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: We present new training methods that aim to mitigate local optima and slow convergence in unsupervised training by using additional imperfect objectives. In its simplest form, lateen EM alternates between the two objectives of ordinary “soft” and “hard” expectation maximization (EM) algorithms. Switching objectives when stuck can help escape local optima. We find that applying a single such alternation already yields state-of-the-art results for English dependency grammar induction. More elaborate lateen strategies track both objectives, with each validating the moves proposed by the other. Disagreements can signal earlier opportunities to switch or terminate, saving iterations. De-emphasizing fixed points in these ways eliminates some guesswork from tuning EM. An evaluation against a suite of unsupervised dependency parsing tasks, for a vari- ety of languages, showed that lateen strategies significantly speed up training of both EM algorithms, and improve accuracy for hard EM.</p><p>3 0.63425332 <a title="79-lda-3" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Noah A. Smith</p><p>Abstract: We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.</p><p>4 0.63321728 <a title="79-lda-4" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>Author: Christos Christodoulopoulos ; Sharon Goldwater ; Mark Steedman</p><p>Abstract: In this paper we present a fully unsupervised syntactic class induction system formulated as a Bayesian multinomial mixture model, where each word type is constrained to belong to a single class. By using a mixture model rather than a sequence model (e.g., HMM), we are able to easily add multiple kinds of features, including those at both the type level (morphology features) and token level (context and alignment features, the latter from parallel corpora). Using only context features, our system yields results comparable to state-of-the art, far better than a similar model without the one-class-per-type constraint. Using the additional features provides added benefit, and our final system outperforms the best published results on most of the 25 corpora tested.</p><p>5 0.61690456 <a title="79-lda-5" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<p>Author: Sebastian Riedel ; Andrew McCallum</p><p>Abstract: Extracting biomedical events from literature has attracted much recent attention. The bestperforming systems so far have been pipelines of simple subtask-specific local classifiers. A natural drawback of such approaches are cascading errors introduced in early stages of the pipeline. We present three joint models of increasing complexity designed to overcome this problem. The first model performs joint trigger and argument extraction, and lends itself to a simple, efficient and exact inference algorithm. The second model captures correlations between events, while the third model ensures consistency between arguments of the same event. Inference in these models is kept tractable through dual decomposition. The first two models outperform the previous best joint approaches and are very competitive with respect to the current state-of-theart. The third model yields the best results reported so far on the BioNLP 2009 shared task, the BioNLP 2011 Genia task and the BioNLP 2011Infectious Diseases task.</p><p>6 0.61646479 <a title="79-lda-6" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>7 0.61447018 <a title="79-lda-7" href="./emnlp-2011-Universal_Morphological_Analysis_using_Structured_Nearest_Neighbor_Prediction.html">140 emnlp-2011-Universal Morphological Analysis using Structured Nearest Neighbor Prediction</a></p>
<p>8 0.61338079 <a title="79-lda-8" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>9 0.61205995 <a title="79-lda-9" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>10 0.61073619 <a title="79-lda-10" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>11 0.60741544 <a title="79-lda-11" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>12 0.60467857 <a title="79-lda-12" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>13 0.60329407 <a title="79-lda-13" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>14 0.60024279 <a title="79-lda-14" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>15 0.59928632 <a title="79-lda-15" href="./emnlp-2011-Tuning_as_Ranking.html">138 emnlp-2011-Tuning as Ranking</a></p>
<p>16 0.59727931 <a title="79-lda-16" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>17 0.59709674 <a title="79-lda-17" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>18 0.59697104 <a title="79-lda-18" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<p>19 0.59605992 <a title="79-lda-19" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>20 0.59390408 <a title="79-lda-20" href="./emnlp-2011-Third-order_Variational_Reranking_on_Packed-Shared_Dependency_Forests.html">134 emnlp-2011-Third-order Variational Reranking on Packed-Shared Dependency Forests</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
