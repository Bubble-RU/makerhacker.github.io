<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>73 emnlp-2011-Improving Bilingual Projections via Sparse Covariance Matrices</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-73" href="#">emnlp2011-73</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>73 emnlp-2011-Improving Bilingual Projections via Sparse Covariance Matrices</h1>
<br/><p>Source: <a title="emnlp-2011-73-pdf" href="http://aclweb.org/anthology//D/D11/D11-1086.pdf">pdf</a></p><p>Author: Jagadeesh Jagarlamudi ; Raghavendra Udupa ; Hal Daume III ; Abhijit Bhole</p><p>Abstract: Mapping documents into an interlingual representation can help bridge the language barrier of cross-lingual corpora. Many existing approaches are based on word co-occurrences extracted from aligned training data, represented as a covariance matrix. In theory, such a covariance matrix should represent semantic equivalence, and should be highly sparse. Unfortunately, the presence of noise leads to dense covariance matrices which in turn leads to suboptimal document representations. In this paper, we explore techniques to recover the desired sparsity in covariance matrices in two ways. First, we explore word association measures and bilingual dictionaries to weigh the word pairs. Later, we explore different selection strategies to remove the noisy pairs based on the association scores. Our experimental results on the task of aligning comparable documents shows the efficacy of sparse covariance matrices on two data sets from two different language pairs.</p><p>Reference: <a title="emnlp-2011-73-reference" href="../emnlp2011_reference/emnlp-2011-Improving_Bilingual_Projections_via_Sparse_Covariance_Matrices_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Many existing approaches are based on word co-occurrences extracted from aligned training data, represented as a covariance matrix. [sent-6, score-0.725]
</p><p>2 In theory, such a covariance matrix should represent semantic equivalence, and should be highly sparse. [sent-7, score-0.721]
</p><p>3 Unfortunately, the presence of noise leads to dense covariance matrices which in turn leads to suboptimal document representations. [sent-8, score-1.009]
</p><p>4 In  this paper, we explore techniques to recover the desired sparsity in covariance matrices in two ways. [sent-9, score-1.063]
</p><p>5 First, we explore word association measures and bilingual dictionaries to weigh the word pairs. [sent-10, score-0.345]
</p><p>6 Later, we explore different selection strategies to remove the noisy pairs based on the association scores. [sent-11, score-0.287]
</p><p>7 Our experimental results on the task of aligning comparable documents shows the efficacy of sparse covariance matrices on two data sets from two different language pairs. [sent-12, score-1.093]
</p><p>8 Most of the existing approaches use manually aligned document pairs to find a common subspace in which the aligned document pairs are maximally correlated. [sent-19, score-0.384]
</p><p>9 The discriminative approaches capture essential word co-occurrences in terms of two monolingual covariance matrices and a cross-covariance matrix. [sent-29, score-1.044]
</p><p>10 Subsequently, they use these covariance matrices to find projection directions in each language such that aligned documents lie close to each other (Sec. [sent-30, score-1.133]
</p><p>11 The strong reliance of these approaches on the covariance matrices leads to problems, especially with the noisy data caused either by the noisy words in a document or the noisy document alignments. [sent-32, score-1.198]
</p><p>12 In this paper, we address the problem of identifying and removing noisy entries in the covariance matrices. [sent-39, score-0.723]
</p><p>13 In the first stage, we explore the use of word association measures such as Mutual Information (MI) and Yule’s ω (Reis and Judd, 2000) in computing the strength of a word pair (Sec. [sent-41, score-0.183]
</p><p>14 We also explore the use of bilingual dictionaries developed from cleaner resources such as parallel data. [sent-44, score-0.232]
</p><p>15 In the second stage, we use the association strengths in filtering out the noisy word pairs from the covariance matrices. [sent-45, score-0.809]
</p><p>16 We evaluate the utility of sparse covariance matrices in improving the bilingual projections incrementally (Sec. [sent-49, score-1.073]
</p><p>17 We found that sparsifying the covariance matrices helps in general, but using cleaner resource such bilingual dictionaries performed best. [sent-55, score-1.204]
</p><p>18 We mainly focus on representing the solution of CCA in terms of covariance matrices. [sent-57, score-0.629]
</p><p>19 Given a training data of n aligned document pairs, CCA finds projection directions for each language, so that the documents when projected along these directions are maximally correlated (Hotelling, 1936). [sent-59, score-0.355]
</p><p>20 (1) where Cxx = XXT, Cyy = Y YT are the monolingual covariance matrices, Cxy = XYT is the crosscovariance matrix and λ is the regularization parameter. [sent-73, score-0.887]
</p><p>21 Using these eigenvectors as columns, we form the projection matrices A and B. [sent-74, score-0.397]
</p><p>22 These projection matrices are used to map documents in both the languages into interlingual representation. [sent-75, score-0.49]
</p><p>23 So, every non-zero entry of the crosscovariance matrix restricts the choice of the projection directions. [sent-85, score-0.217]
</p><p>24 Every occurrence of a noisy word will have a non-zero contribution towards the covariance matrix making it dense, which in turn prevents the selection of appropriate projection directions. [sent-88, score-0.973]
</p><p>25 In this section, we describe some techniques to recover the sparsity by removing the noisy entries from the covariance matrices. [sent-89, score-0.878]
</p><p>26 We break this task into two sub problems: computing an association score for every word pair and then using an appropriate strategy to identify the noisy pairs based on their weights. [sent-90, score-0.18]
</p><p>27 This measure uses information about the occurrence of a word pair in  aligned documents and doesn’t use other statistics such as ‘how often this pair doesn ’t co-occur together’ and so on. [sent-105, score-0.195]
</p><p>28 2 Mutual Information Association measures like covariance and Pointwise Mutual Information, which only use the frequency with which a word pair co-occurs, often overestimate the strength of low frequent words (Moore, 2004). [sent-108, score-0.697]
</p><p>29 We treat  the occurrence of a word in a document slightly different from others, we treat a word as occurring in a document if it has occurred more than its average frequency in the corpus. [sent-112, score-0.208]
</p><p>30 4 Bilingual Dictionary The above three association measures use the same training data that is available to compute the covariance matrices in CCA. [sent-122, score-0.969]
</p><p>31 Thus, their utility in bringing additional information, which is not captured by the covariance matrices, is arguable (our experiments show that they are indeed helpful). [sent-123, score-0.629]
</p><p>32 Moreover, they use document level co-occurrence information which is coarse compared to the cooccurrence at sentence level or the translational information provided by a bilingual dictionary. [sent-124, score-0.195]
</p><p>33 So, we use bilingual dictionaries as our final resource to weigh the word co-occurrences. [sent-125, score-0.193]
</p><p>34 While the first three association measures can also be applied to monolingual data, bilingual dictionary can’t be used for weighting monolingual word pairs. [sent-132, score-0.619]
</p><p>35 So in this case, we use either of the above mentioned techniques for weighting monolingual word pairs. [sent-133, score-0.206]
</p><p>36 1 Thresholding A straight forward way to remove the noisy word co-occurrences is to zero out the entries of the cross-covariance matrix that are lower than a threshold. [sent-139, score-0.217]
</p><p>37 So, iPf we want to remove some of the entries of the covariance matrix with minimal change in the value of the objective function, then the optimal choice is to sort the entries of the covariance matrix and filter out the less confident word pairs. [sent-142, score-1.545]
</p><p>38 4 Monolingual Augmentation  The above three selection strategies operate on the covariance matrices independently. [sent-176, score-0.986]
</p><p>39 Specifically, we propose to augment the set of selected bilingual word pairs using the monolingual word pairs. [sent-178, score-0.356]
</p><p>40 We first use any of the above mentioned strategies to select bilingual and monolingual word pairs. [sent-179, score-0.327]
</p><p>41 Let Ixy, Ixx and Iyy be the binary matrices that indicate the selected word pairs based on the bilingual and monolingual association scores. [sent-180, score-0.628]
</p><p>42 Then the monolingual augmentation strategy updates Ixy in the following way: Ixy ← Binarize(IxxIxyIyy) i. [sent-181, score-0.242]
</p><p>43 , we multiply Ixy with the monolingual selection matrices and then binarize the resulting matrix. [sent-183, score-0.453]
</p><p>44 Our monolingual augmentation is motivated by the following probabilistic interpretation: P(x,y) =  XP(x|x′)P(y|y′)P(x′,y′) xX′ X,y′  which can be rewritten as P ← TxP(Ty)T where wTxh acnhd c Tany are monolingual Psta ←te t Transition matrices. [sent-184, score-0.415]
</p><p>45 3  Our Approach  In this section we summarize our approach for the task of finding aligned documents from a crosslingual comparable corpora. [sent-186, score-0.176]
</p><p>46 The training phase involves finding projection directions for documents of both the languages. [sent-187, score-0.19]
</p><p>47 We compute the covariance matrices using the training data. [sent-188, score-0.878]
</p><p>48 2) to recover the sparseness in either only the cross-covariance or all of the covariance matrices. [sent-193, score-0.677]
</p><p>49 Let Ixy, Ixx and Iyy be the binary matrices which represent the word pairs that are selected based on the chosen sparsification technique. [sent-194, score-0.379]
</p><p>50 Let A and B be the matrices formed with top eigenvectors of Eq. [sent-199, score-0.303]
</p><p>51 These projection matrices are used to map documents into the interlingual representation. [sent-201, score-0.49]
</p><p>52 During the testing, given an English document x, finding an aligned Spanish document involves solv-  ing:  argmyaxrxT? [sent-205, score-0.211]
</p><p>53 1 Experimental Setup We experiment with the task of finding aligned documents from a cross-lingual comparable corpora. [sent-213, score-0.176]
</p><p>54 As the corpora are comparable, some documents in one collection have a comparable document in the other collection. [sent-215, score-0.184]
</p><p>55 We evaluate our idea of sparsifying the covariance matrices incrementally. [sent-218, score-1.002]
</p><p>56 That leaves us 12 different ways for sparsifying the covariance matrices, with each method having parameters to control the amount of sparseness. [sent-222, score-0.753]
</p><p>57 We use the true feature correspondences to form the cross-covariance selection matrix Ixy (Sec. [sent-237, score-0.208]
</p><p>58 For this experiment, we use the full monolingual covariance matrices. [sent-240, score-0.764]
</p><p>59 We tokenize the documents, retain only the most frequent 2000 words in each language and convert the docu-  Figure 2: Comparison of the word association measures along with different selection criteria. [sent-254, score-0.191]
</p><p>60 The x-axis plots the number of non-zero entries in the covariance matrices and the y-axis plots the accuracy oftop-ranked document. [sent-255, score-1.002]
</p><p>61 2 shows the performance of these different combinations with varying levels of sparsity in the covariance matrices. [sent-263, score-0.764]
</p><p>62 We start with 2000 non-zero entries in the covariance matrices and experiment up to 20,000 non-zero entries. [sent-265, score-0.914]
</p><p>63 Since our data set has 2000 words in each language, 2000 non-zero entries in a covariance matrix implies that, on an average, every word is associated with only one word. [sent-266, score-0.788]
</p><p>64 selecting more number of elements in the covariance matrices, increases the performance slightly and then decreases again. [sent-270, score-0.629]
</p><p>65 From the figure, it  936 seems that sparsifying the covariance matrices might help in improving the performance of the task. [sent-271, score-1.002]
</p><p>66 This suggests that, apart from the weighting of the word pairs, appropriate selection of the word pairs is also equally important. [sent-274, score-0.208]
</p><p>67 From this figure, we observe that Mutual Information and Yule’s ω perform competitively but they consistently outperform models that use covariance as the association measure. [sent-276, score-0.714]
</p><p>68 2 Amount of Sparsity In the previous experiment, we used same level of sparsity for all the covariance matrices, i. [sent-280, score-0.736]
</p><p>69 same number of associations were selected for each word in all the three covariance matrices. [sent-282, score-0.66]
</p><p>70 In the following experiment, we use different levels of sparsity for the individual covariance matrices. [sent-283, score-0.764]
</p><p>71 In the Yule+Match combination, we use Yule’s ω association measure for weighting the word pairs and use matching for selection. [sent-286, score-0.203]
</p><p>72 In the Dictionary+Match combination, we use bilingual dictionary for sparsifying cross-covariance matrix, i. [sent-287, score-0.311]
</p><p>73 And for monolingual word pairs, we use MI for weighting and matching for word pair selection. [sent-290, score-0.278]
</p><p>74 For each level of sparsity of the cross-covariance matrix, we experiment with different levels of sparsity on the monolingual covariance matrices. [sent-291, score-1.006]
</p><p>75 ‘Only XY’ indicates we use the full monolingual covariance matrices. [sent-292, score-0.764]
</p><p>76 ‘Aug’ indicates that we use monolingual augmentation to refine the sparsity of the cross-covariance matrix (Sec. [sent-295, score-0.441]
</p><p>77 From both the figures 3(a) and 3(b), we observe that ‘Only XY’ run (dark blue) performs poorly compared to the other runs, indicating that sparsifying all the covariance matrices is better than spar-  sifying only the cross-covariance  matrix. [sent-299, score-1.002]
</p><p>78 Matching is used as selection criteria for all the covariance matrices. [sent-303, score-0.698]
</p><p>79 The x-axis plots the threshold on bilingual translation probability and it determines the sparsity of Cxy. [sent-305, score-0.3]
</p><p>80 Figure 3: Comparison of Yule+Match and Dictionary+Match combination with different levels of sparsity for the covariance matrices. [sent-307, score-0.764]
</p><p>81 In both the figures, the x-axis plots the sparsity of the cross-covariance matrix and for each value we try different levels of sparsity on the monolingual covariance matrices (which are grouped together). [sent-308, score-1.391]
</p><p>82 In both the above experiments, the performance bars are very similar when we use MI instead of Yule and vice versa for weighting monolingual word pairs. [sent-322, score-0.249]
</p><p>83 Yule(l)+Match(k), where l ∈ {2, 3} is the number Yofu Spanish wcho(rkd)s, wallhoewreed l f∈or { e2a,3ch} English wmoberdr and vice versa and k=2 is the number of monolingual word associations for each word. [sent-327, score-0.209]
</p><p>84 Again, we run these combinatainodns t wyi kth monolingual augmentation eidseen ctoimfiebdi by Dictionary+Match(k)+Aug. [sent-331, score-0.242]
</p><p>85 of English document and the second half of its aligned foreign language document (Mimno et al. [sent-377, score-0.211]
</p><p>86 From the results, it is clear that sparsifying the covariance matrices helps improving the accuracies significantly. [sent-400, score-1.002]
</p><p>87 This indicates that using fine granular information such as a bilingual dictionary gleaned from an external source is very helpful in improving the accuracies. [sent-402, score-0.234]
</p><p>88 Among the  models that rely solely on the training data, models that use monolingual augmentation performed better on Wikipedia data set, while models that do not use augmentation performed better on Europarl data sets. [sent-403, score-0.349]
</p><p>89 As the documents become comparable, we need to use monolingual statistics to refine the bilingual statistics. [sent-405, score-0.326]
</p><p>90 This conforms with our initial hunch that, when the training data is clean the covariance matrices tend to be less noisy. [sent-407, score-0.906]
</p><p>91 5  Discussion  In this paper, we have proposed the idea of sparsifyng covariance matrices to improve bilingual projection directions. [sent-408, score-1.094]
</p><p>92 We are not aware of any NLP research that attempts to recover the sparseness of the covariance matrices to improve the projection directions. [sent-409, score-1.02]
</p><p>93 Their objective is to find projection directions such that the original documents are represented as a sparse vectors in the common sub-space. [sent-411, score-0.263]
</p><p>94 Another seemingly relevant but different direction is the sparse covariance matrix selection research (Banerjee et al. [sent-412, score-0.863]
</p><p>95 The objective in this work is to find matrices such that the inverse of the covariance matrix is sparse which has applications in Gaussian processes. [sent-414, score-1.043]
</p><p>96 Our experimental results show that using external information such as bilingual dictionaries which is gleaned from cleaner resources brings significant improvements. [sent-416, score-0.249]
</p><p>97 Moreover, we also observe that computing word pair association measures from the same training data along with an appropriate selection criteria can also yield significant improvements. [sent-417, score-0.191]
</p><p>98 This is certainly encouraging and in future we would like to explore more sophisticated techniques to recover the sparsity based on the training data itself. [sent-418, score-0.185]
</p><p>99 Sparse covariance selection 939 CoRR,  via robust maximum likelihood estimation. [sent-436, score-0.698]
</p><p>100 From bilingual dictionaries to interlin-  gual document representations. [sent-504, score-0.235]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('covariance', 0.629), ('yule', 0.311), ('cca', 0.268), ('matrices', 0.249), ('monolingual', 0.135), ('sparsifying', 0.124), ('bilingual', 0.122), ('ixy', 0.109), ('augmentation', 0.107), ('sparsity', 0.107), ('spanish', 0.106), ('thresholding', 0.097), ('projection', 0.094), ('iij', 0.093), ('matrix', 0.092), ('jagarlamudi', 0.08), ('interlingual', 0.078), ('document', 0.073), ('sparse', 0.073), ('fj', 0.07), ('documents', 0.069), ('selection', 0.069), ('aligned', 0.065), ('dictionary', 0.065), ('mi', 0.065), ('opca', 0.062), ('sparsification', 0.062), ('match', 0.059), ('noisy', 0.058), ('europarl', 0.054), ('association', 0.054), ('aug', 0.054), ('eigenvectors', 0.054), ('hal', 0.051), ('jagadeesh', 0.048), ('recover', 0.048), ('correspondences', 0.047), ('daum', 0.047), ('atxytb', 0.047), ('cyy', 0.047), ('gleaned', 0.047), ('ixx', 0.047), ('iyy', 0.047), ('reis', 0.047), ('platt', 0.045), ('synthetic', 0.045), ('plots', 0.044), ('vice', 0.043), ('comparable', 0.042), ('ei', 0.041), ('matching', 0.041), ('canonical', 0.04), ('dictionaries', 0.04), ('cxx', 0.04), ('cleaner', 0.04), ('daume', 0.04), ('weighting', 0.04), ('strategies', 0.039), ('rewritten', 0.038), ('measures', 0.037), ('pairs', 0.037), ('mutual', 0.036), ('entries', 0.036), ('subspace', 0.034), ('subsequently', 0.032), ('transliteration', 0.032), ('aligning', 0.031), ('word', 0.031), ('atxxta', 0.031), ('ballesteros', 0.031), ('bty', 0.031), ('competitively', 0.031), ('crosscovariance', 0.031), ('hardoon', 0.031), ('hermjakob', 0.031), ('jonker', 0.031), ('judd', 0.031), ('ravindra', 0.031), ('vinokourov', 0.031), ('ytb', 0.031), ('zeroing', 0.031), ('noise', 0.031), ('doesn', 0.03), ('mimno', 0.03), ('explore', 0.03), ('levels', 0.028), ('clean', 0.028), ('xy', 0.028), ('correlation', 0.027), ('dense', 0.027), ('translation', 0.027), ('directions', 0.027), ('iii', 0.027), ('rai', 0.027), ('tfidf', 0.027), ('vu', 0.027), ('neighbour', 0.027), ('bel', 0.027), ('cxy', 0.027), ('raghavendra', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="73-tfidf-1" href="./emnlp-2011-Improving_Bilingual_Projections_via_Sparse_Covariance_Matrices.html">73 emnlp-2011-Improving Bilingual Projections via Sparse Covariance Matrices</a></p>
<p>Author: Jagadeesh Jagarlamudi ; Raghavendra Udupa ; Hal Daume III ; Abhijit Bhole</p><p>Abstract: Mapping documents into an interlingual representation can help bridge the language barrier of cross-lingual corpora. Many existing approaches are based on word co-occurrences extracted from aligned training data, represented as a covariance matrix. In theory, such a covariance matrix should represent semantic equivalence, and should be highly sparse. Unfortunately, the presence of noise leads to dense covariance matrices which in turn leads to suboptimal document representations. In this paper, we explore techniques to recover the desired sparsity in covariance matrices in two ways. First, we explore word association measures and bilingual dictionaries to weigh the word pairs. Later, we explore different selection strategies to remove the noisy pairs based on the association scores. Our experimental results on the task of aligning comparable documents shows the efficacy of sparse covariance matrices on two data sets from two different language pairs.</p><p>2 0.096924864 <a title="73-tfidf-2" href="./emnlp-2011-SMT_Helps_Bitext_Dependency_Parsing.html">118 emnlp-2011-SMT Helps Bitext Dependency Parsing</a></p>
<p>Author: Wenliang Chen ; Jun'ichi Kazama ; Min Zhang ; Yoshimasa Tsuruoka ; Yujie Zhang ; Yiou Wang ; Kentaro Torisawa ; Haizhou Li</p><p>Abstract: We propose a method to improve the accuracy of parsing bilingual texts (bitexts) with the help of statistical machine translation (SMT) systems. Previous bitext parsing methods use human-annotated bilingual treebanks that are hard to obtain. Instead, our approach uses an auto-generated bilingual treebank to produce bilingual constraints. However, because the auto-generated bilingual treebank contains errors, the bilingual constraints are noisy. To overcome this problem, we use large-scale unannotated data to verify the constraints and design a set of effective bilingual features for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT.</p><p>3 0.08405678 <a title="73-tfidf-3" href="./emnlp-2011-Analyzing_Methods_for_Improving_Precision_of_Pivot_Based_Bilingual_Dictionaries.html">18 emnlp-2011-Analyzing Methods for Improving Precision of Pivot Based Bilingual Dictionaries</a></p>
<p>Author: Xabier Saralegi ; Iker Manterola ; Inaki San Vicente</p><p>Abstract: An A-C bilingual dictionary can be inferred by merging A-B and B-C dictionaries using B as pivot. However, polysemous pivot words often produce wrong translation candidates. This paper analyzes two methods for pruning wrong candidates: one based on exploiting the structure of the source dictionaries, and the other based on distributional similarity computed from comparable corpora. As both methods depend exclusively on easily available resources, they are well suited to less resourced languages. We studied whether these two techniques complement each other given that they are based on different paradigms. We also researched combining them by looking for the best adequacy depending on various application scenarios. ,</p><p>4 0.06985645 <a title="73-tfidf-4" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<p>Author: Amittai Axelrod ; Xiaodong He ; Jianfeng Gao</p><p>Abstract: Xiaodong He Microsoft Research Redmond, WA 98052 xiaohe @mi cro s o ft . com Jianfeng Gao Microsoft Research Redmond, WA 98052 j fgao @mi cro s o ft . com have its own argot, vocabulary or stylistic preferences, such that the corpus characteristics will necWe explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain. These sentences may be selected with simple cross-entropy based methods, of which we present three. As these sentences are not themselves identical to the in-domain data, we call them pseudo in-domain subcorpora. These subcorpora 1% the size of the original can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus. Performance is further improved when we use these domain-adapted models in combination with a true in-domain model. The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining in- and general-domain systems during decoding. – –</p><p>5 0.069551393 <a title="73-tfidf-5" href="./emnlp-2011-A_Correction_Model_for_Word_Alignments.html">3 emnlp-2011-A Correction Model for Word Alignments</a></p>
<p>Author: J. Scott McCarley ; Abraham Ittycheriah ; Salim Roukos ; Bing Xiang ; Jian-ming Xu</p><p>Abstract: Models of word alignment built as sequences of links have limited expressive power, but are easy to decode. Word aligners that model the alignment matrix can express arbitrary alignments, but are difficult to decode. We propose an alignment matrix model as a correction algorithm to an underlying sequencebased aligner. Then a greedy decoding algorithm enables the full expressive power of the alignment matrix formulation. Improved alignment performance is shown for all nine language pairs tested. The improved alignments also improved translation quality from Chinese to English and English to Italian.</p><p>6 0.068639763 <a title="73-tfidf-6" href="./emnlp-2011-Compositional_Matrix-Space_Models_for_Sentiment_Analysis.html">30 emnlp-2011-Compositional Matrix-Space Models for Sentiment Analysis</a></p>
<p>7 0.067647204 <a title="73-tfidf-7" href="./emnlp-2011-Cache-based_Document-level_Statistical_Machine_Translation.html">25 emnlp-2011-Cache-based Document-level Statistical Machine Translation</a></p>
<p>8 0.058988106 <a title="73-tfidf-8" href="./emnlp-2011-Structured_Sparsity_in_Structured_Prediction.html">129 emnlp-2011-Structured Sparsity in Structured Prediction</a></p>
<p>9 0.058537479 <a title="73-tfidf-9" href="./emnlp-2011-Simple_Effective_Decipherment_via_Combinatorial_Optimization.html">122 emnlp-2011-Simple Effective Decipherment via Combinatorial Optimization</a></p>
<p>10 0.058432471 <a title="73-tfidf-10" href="./emnlp-2011-Relaxed_Cross-lingual_Projection_of_Constituent_Syntax.html">115 emnlp-2011-Relaxed Cross-lingual Projection of Constituent Syntax</a></p>
<p>11 0.057624768 <a title="73-tfidf-11" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>12 0.055718567 <a title="73-tfidf-12" href="./emnlp-2011-Latent_Vector_Weighting_for_Word_Meaning_in_Context.html">80 emnlp-2011-Latent Vector Weighting for Word Meaning in Context</a></p>
<p>13 0.054861389 <a title="73-tfidf-13" href="./emnlp-2011-A_Non-negative_Matrix_Factorization_Based_Approach_for_Active_Dual_Supervision_from_Document_and_Word_Labels.html">9 emnlp-2011-A Non-negative Matrix Factorization Based Approach for Active Dual Supervision from Document and Word Labels</a></p>
<p>14 0.053999275 <a title="73-tfidf-14" href="./emnlp-2011-Unsupervised_Structure_Prediction_with_Non-Parallel_Multilingual_Guidance.html">146 emnlp-2011-Unsupervised Structure Prediction with Non-Parallel Multilingual Guidance</a></p>
<p>15 0.053438168 <a title="73-tfidf-15" href="./emnlp-2011-Multi-Source_Transfer_of_Delexicalized_Dependency_Parsers.html">95 emnlp-2011-Multi-Source Transfer of Delexicalized Dependency Parsers</a></p>
<p>16 0.053232808 <a title="73-tfidf-16" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>17 0.048423968 <a title="73-tfidf-17" href="./emnlp-2011-Improved_Transliteration_Mining_Using_Graph_Reinforcement.html">72 emnlp-2011-Improved Transliteration Mining Using Graph Reinforcement</a></p>
<p>18 0.047960546 <a title="73-tfidf-18" href="./emnlp-2011-Multilayer_Sequence_Labeling.html">96 emnlp-2011-Multilayer Sequence Labeling</a></p>
<p>19 0.047312215 <a title="73-tfidf-19" href="./emnlp-2011-Lexical_Co-occurrence%2C_Statistical_Significance%2C_and_Word_Association.html">86 emnlp-2011-Lexical Co-occurrence, Statistical Significance, and Word Association</a></p>
<p>20 0.045809619 <a title="73-tfidf-20" href="./emnlp-2011-Minimum_Imputed-Risk%3A_Unsupervised_Discriminative_Training_for_Machine_Translation.html">93 emnlp-2011-Minimum Imputed-Risk: Unsupervised Discriminative Training for Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.165), (1, -0.002), (2, -0.005), (3, -0.101), (4, 0.048), (5, 0.056), (6, -0.04), (7, 0.018), (8, -0.152), (9, 0.052), (10, -0.016), (11, -0.03), (12, 0.056), (13, 0.08), (14, 0.004), (15, 0.013), (16, 0.004), (17, 0.023), (18, -0.165), (19, -0.115), (20, -0.05), (21, -0.206), (22, -0.049), (23, 0.013), (24, 0.089), (25, -0.111), (26, 0.078), (27, 0.097), (28, 0.011), (29, 0.124), (30, 0.095), (31, 0.058), (32, -0.058), (33, 0.029), (34, 0.072), (35, -0.019), (36, 0.014), (37, 0.142), (38, 0.007), (39, 0.106), (40, -0.128), (41, -0.06), (42, -0.079), (43, 0.045), (44, -0.02), (45, -0.024), (46, -0.096), (47, -0.082), (48, -0.002), (49, -0.077)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93621039 <a title="73-lsi-1" href="./emnlp-2011-Improving_Bilingual_Projections_via_Sparse_Covariance_Matrices.html">73 emnlp-2011-Improving Bilingual Projections via Sparse Covariance Matrices</a></p>
<p>Author: Jagadeesh Jagarlamudi ; Raghavendra Udupa ; Hal Daume III ; Abhijit Bhole</p><p>Abstract: Mapping documents into an interlingual representation can help bridge the language barrier of cross-lingual corpora. Many existing approaches are based on word co-occurrences extracted from aligned training data, represented as a covariance matrix. In theory, such a covariance matrix should represent semantic equivalence, and should be highly sparse. Unfortunately, the presence of noise leads to dense covariance matrices which in turn leads to suboptimal document representations. In this paper, we explore techniques to recover the desired sparsity in covariance matrices in two ways. First, we explore word association measures and bilingual dictionaries to weigh the word pairs. Later, we explore different selection strategies to remove the noisy pairs based on the association scores. Our experimental results on the task of aligning comparable documents shows the efficacy of sparse covariance matrices on two data sets from two different language pairs.</p><p>2 0.59629875 <a title="73-lsi-2" href="./emnlp-2011-SMT_Helps_Bitext_Dependency_Parsing.html">118 emnlp-2011-SMT Helps Bitext Dependency Parsing</a></p>
<p>Author: Wenliang Chen ; Jun'ichi Kazama ; Min Zhang ; Yoshimasa Tsuruoka ; Yujie Zhang ; Yiou Wang ; Kentaro Torisawa ; Haizhou Li</p><p>Abstract: We propose a method to improve the accuracy of parsing bilingual texts (bitexts) with the help of statistical machine translation (SMT) systems. Previous bitext parsing methods use human-annotated bilingual treebanks that are hard to obtain. Instead, our approach uses an auto-generated bilingual treebank to produce bilingual constraints. However, because the auto-generated bilingual treebank contains errors, the bilingual constraints are noisy. To overcome this problem, we use large-scale unannotated data to verify the constraints and design a set of effective bilingual features for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT.</p><p>3 0.55784738 <a title="73-lsi-3" href="./emnlp-2011-Analyzing_Methods_for_Improving_Precision_of_Pivot_Based_Bilingual_Dictionaries.html">18 emnlp-2011-Analyzing Methods for Improving Precision of Pivot Based Bilingual Dictionaries</a></p>
<p>Author: Xabier Saralegi ; Iker Manterola ; Inaki San Vicente</p><p>Abstract: An A-C bilingual dictionary can be inferred by merging A-B and B-C dictionaries using B as pivot. However, polysemous pivot words often produce wrong translation candidates. This paper analyzes two methods for pruning wrong candidates: one based on exploiting the structure of the source dictionaries, and the other based on distributional similarity computed from comparable corpora. As both methods depend exclusively on easily available resources, they are well suited to less resourced languages. We studied whether these two techniques complement each other given that they are based on different paradigms. We also researched combining them by looking for the best adequacy depending on various application scenarios. ,</p><p>4 0.53681529 <a title="73-lsi-4" href="./emnlp-2011-Cache-based_Document-level_Statistical_Machine_Translation.html">25 emnlp-2011-Cache-based Document-level Statistical Machine Translation</a></p>
<p>Author: Zhengxian Gong ; Min Zhang ; Guodong Zhou</p><p>Abstract: Statistical machine translation systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time, ignoring document-level information. In this paper, we propose a cache-based approach to document-level translation. Since caches mainly depend on relevant data to supervise subsequent decisions, it is critical to fill the caches with highly-relevant data of a reasonable size. In this paper, we present three kinds of caches to store relevant document-level information: 1) a dynamic cache, which stores bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document; 2) a static cache, which stores relevant bilingual phrase pairs extracted from similar bilingual document pairs (i.e. source documents similar to the test document and their corresponding target documents) in the training parallel corpus; 3) a topic cache, which stores the target-side topic words related with the test document in the source-side. In particular, three new features are designed to explore various kinds of document-level information in above three kinds of caches. Evaluation shows the effectiveness of our cache-based approach to document-level translation with the performance improvement of 0.8 1 in BLUE score over Moses. Especially, detailed analysis and discussion are presented to give new insights to document-level translation. 1</p><p>5 0.42651662 <a title="73-lsi-5" href="./emnlp-2011-Lexical_Co-occurrence%2C_Statistical_Significance%2C_and_Word_Association.html">86 emnlp-2011-Lexical Co-occurrence, Statistical Significance, and Word Association</a></p>
<p>Author: Dipak L. Chaudhari ; Om P. Damani ; Srivatsan Laxman</p><p>Abstract: Om P. Damani Srivatsan Laxman Computer Science and Engg. Microsoft Research India IIT Bombay Bangalore damani @ cse . i . ac . in itb s laxman@mi cro s o ft . com of words that co-occur in a large number of docuLexical co-occurrence is an important cue for detecting word associations. We propose a new measure of word association based on a new notion of statistical significance for lexical co-occurrences. Existing measures typically rely on global unigram frequencies to determine expected co-occurrence counts. In- stead, we focus only on documents that contain both terms (of a candidate word-pair) and ask if the distribution of the observed spans of the word-pair resembles that under a random null model. This would imply that the words in the pair are not related strongly enough for one word to influence placement of the other. However, if the words are found to occur closer together than explainable by the null model, then we hypothesize a more direct association between the words. Through extensive empirical evaluation on most of the publicly available benchmark data sets, we show the advantages of our measure over existing co-occurrence measures.</p><p>6 0.42548651 <a title="73-lsi-6" href="./emnlp-2011-Approximate_Scalable_Bounded_Space_Sketch_for_Large_Data_NLP.html">19 emnlp-2011-Approximate Scalable Bounded Space Sketch for Large Data NLP</a></p>
<p>7 0.40637511 <a title="73-lsi-7" href="./emnlp-2011-A_Correction_Model_for_Word_Alignments.html">3 emnlp-2011-A Correction Model for Word Alignments</a></p>
<p>8 0.39937481 <a title="73-lsi-8" href="./emnlp-2011-Improved_Transliteration_Mining_Using_Graph_Reinforcement.html">72 emnlp-2011-Improved Transliteration Mining Using Graph Reinforcement</a></p>
<p>9 0.3972986 <a title="73-lsi-9" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>10 0.3754417 <a title="73-lsi-10" href="./emnlp-2011-Relaxed_Cross-lingual_Projection_of_Constituent_Syntax.html">115 emnlp-2011-Relaxed Cross-lingual Projection of Constituent Syntax</a></p>
<p>11 0.36930311 <a title="73-lsi-11" href="./emnlp-2011-Watermarking_the_Outputs_of_Structured_Prediction_with_an_application_in_Statistical_Machine_Translation..html">148 emnlp-2011-Watermarking the Outputs of Structured Prediction with an application in Statistical Machine Translation.</a></p>
<p>12 0.34585956 <a title="73-lsi-12" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<p>13 0.34430048 <a title="73-lsi-13" href="./emnlp-2011-Minimum_Imputed-Risk%3A_Unsupervised_Discriminative_Training_for_Machine_Translation.html">93 emnlp-2011-Minimum Imputed-Risk: Unsupervised Discriminative Training for Machine Translation</a></p>
<p>14 0.34335026 <a title="73-lsi-14" href="./emnlp-2011-Unsupervised_Information_Extraction_with_Distributional_Prior_Knowledge.html">143 emnlp-2011-Unsupervised Information Extraction with Distributional Prior Knowledge</a></p>
<p>15 0.3362627 <a title="73-lsi-15" href="./emnlp-2011-Structured_Sparsity_in_Structured_Prediction.html">129 emnlp-2011-Structured Sparsity in Structured Prediction</a></p>
<p>16 0.31517419 <a title="73-lsi-16" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>17 0.30616894 <a title="73-lsi-17" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>18 0.29413411 <a title="73-lsi-18" href="./emnlp-2011-Simple_Effective_Decipherment_via_Combinatorial_Optimization.html">122 emnlp-2011-Simple Effective Decipherment via Combinatorial Optimization</a></p>
<p>19 0.28845316 <a title="73-lsi-19" href="./emnlp-2011-Compositional_Matrix-Space_Models_for_Sentiment_Analysis.html">30 emnlp-2011-Compositional Matrix-Space Models for Sentiment Analysis</a></p>
<p>20 0.28377515 <a title="73-lsi-20" href="./emnlp-2011-Heuristic_Search_for_Non-Bottom-Up_Tree_Structure_Prediction.html">65 emnlp-2011-Heuristic Search for Non-Bottom-Up Tree Structure Prediction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.083), (36, 0.016), (37, 0.023), (45, 0.098), (53, 0.036), (54, 0.03), (62, 0.017), (64, 0.035), (66, 0.033), (69, 0.394), (79, 0.048), (82, 0.031), (85, 0.01), (87, 0.01), (96, 0.024), (98, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83444709 <a title="73-lda-1" href="./emnlp-2011-Exact_Decoding_of_Phrase-Based_Translation_Models_through_Lagrangian_Relaxation.html">51 emnlp-2011-Exact Decoding of Phrase-Based Translation Models through Lagrangian Relaxation</a></p>
<p>Author: Yin-Wen Chang ; Michael Collins</p><p>Abstract: This paper describes an algorithm for exact decoding of phrase-based translation models, based on Lagrangian relaxation. The method recovers exact solutions, with certificates of optimality, on over 99% of test examples. The method is much more efficient than approaches based on linear programming (LP) or integer linear programming (ILP) solvers: these methods are not feasible for anything other than short sentences. We compare our method to MOSES (Koehn et al., 2007), and give precise estimates of the number and magnitude of search errors that MOSES makes.</p><p>2 0.80611223 <a title="73-lda-2" href="./emnlp-2011-Random_Walk_Inference_and_Learning_in_A_Large_Scale_Knowledge_Base.html">109 emnlp-2011-Random Walk Inference and Learning in A Large Scale Knowledge Base</a></p>
<p>Author: Ni Lao ; Tom Mitchell ; William W. Cohen</p><p>Abstract: t om . We consider the problem of performing learning and inference in a large scale knowledge base containing imperfect knowledge with incomplete coverage. We show that a soft inference procedure based on a combination of constrained, weighted, random walks through the knowledge base graph can be used to reliably infer new beliefs for the knowledge base. More specifically, we show that the system can learn to infer different target relations by tuning the weights associated with random walks that follow different paths through the graph, using a version of the Path Ranking Algorithm (Lao and Cohen, 2010b). We apply this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by NELL, a never-ending language learner (Carlson et al., 2010). This new system improves significantly over NELL’s earlier Horn-clause learning and inference method: it obtains nearly double the precision at rank 100, and the new learning method is also applicable to many more inference tasks.</p><p>same-paper 3 0.76662064 <a title="73-lda-3" href="./emnlp-2011-Improving_Bilingual_Projections_via_Sparse_Covariance_Matrices.html">73 emnlp-2011-Improving Bilingual Projections via Sparse Covariance Matrices</a></p>
<p>Author: Jagadeesh Jagarlamudi ; Raghavendra Udupa ; Hal Daume III ; Abhijit Bhole</p><p>Abstract: Mapping documents into an interlingual representation can help bridge the language barrier of cross-lingual corpora. Many existing approaches are based on word co-occurrences extracted from aligned training data, represented as a covariance matrix. In theory, such a covariance matrix should represent semantic equivalence, and should be highly sparse. Unfortunately, the presence of noise leads to dense covariance matrices which in turn leads to suboptimal document representations. In this paper, we explore techniques to recover the desired sparsity in covariance matrices in two ways. First, we explore word association measures and bilingual dictionaries to weigh the word pairs. Later, we explore different selection strategies to remove the noisy pairs based on the association scores. Our experimental results on the task of aligning comparable documents shows the efficacy of sparse covariance matrices on two data sets from two different language pairs.</p><p>4 0.74051642 <a title="73-lda-4" href="./emnlp-2011-Computation_of_Infix_Probabilities_for_Probabilistic_Context-Free_Grammars.html">31 emnlp-2011-Computation of Infix Probabilities for Probabilistic Context-Free Grammars</a></p>
<p>Author: Mark-Jan Nederhof ; Giorgio Satta</p><p>Abstract: The notion of infix probability has been introduced in the literature as a generalization of the notion of prefix (or initial substring) probability, motivated by applications in speech recognition and word error correction. For the case where a probabilistic context-free grammar is used as language model, methods for the computation of infix probabilities have been presented in the literature, based on various simplifying assumptions. Here we present a solution that applies to the problem in its full generality.</p><p>5 0.55480152 <a title="73-lda-5" href="./emnlp-2011-Hierarchical_Phrase-based_Translation_Representations.html">66 emnlp-2011-Hierarchical Phrase-based Translation Representations</a></p>
<p>Author: Gonzalo Iglesias ; Cyril Allauzen ; William Byrne ; Adria de Gispert ; Michael Riley</p><p>Abstract: This paper compares several translation representations for a synchronous context-free grammar parse including CFGs/hypergraphs, finite-state automata (FSA), and pushdown automata (PDA). The representation choice is shown to determine the form and complexity of target LM intersection and shortest-path algorithms that follow. Intersection, shortest path, FSA expansion and RTN replacement algorithms are presented for PDAs. Chinese-toEnglish translation experiments using HiFST and HiPDT, FSA and PDA-based decoders, are presented using admissible (or exact) search, possible for HiFST with compact SCFG rulesets and HiPDT with compact LMs. For large rulesets with large LMs, we introduce a two-pass search strategy which we then analyze in terms of search errors and translation performance.</p><p>6 0.43542072 <a title="73-lda-6" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>7 0.43507481 <a title="73-lda-7" href="./emnlp-2011-Entire_Relaxation_Path_for_Maximum_Entropy_Problems.html">49 emnlp-2011-Entire Relaxation Path for Maximum Entropy Problems</a></p>
<p>8 0.43100983 <a title="73-lda-8" href="./emnlp-2011-Heuristic_Search_for_Non-Bottom-Up_Tree_Structure_Prediction.html">65 emnlp-2011-Heuristic Search for Non-Bottom-Up Tree Structure Prediction</a></p>
<p>9 0.41964734 <a title="73-lda-9" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>10 0.41862211 <a title="73-lda-10" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>11 0.41244903 <a title="73-lda-11" href="./emnlp-2011-A_Correction_Model_for_Word_Alignments.html">3 emnlp-2011-A Correction Model for Word Alignments</a></p>
<p>12 0.40354016 <a title="73-lda-12" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<p>13 0.40319392 <a title="73-lda-13" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>14 0.40224829 <a title="73-lda-14" href="./emnlp-2011-Exploring_Supervised_LDA_Models_for_Assigning_Attributes_to_Adjective-Noun_Phrases.html">56 emnlp-2011-Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases</a></p>
<p>15 0.40166879 <a title="73-lda-15" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>16 0.39898247 <a title="73-lda-16" href="./emnlp-2011-Simple_Effective_Decipherment_via_Combinatorial_Optimization.html">122 emnlp-2011-Simple Effective Decipherment via Combinatorial Optimization</a></p>
<p>17 0.39891586 <a title="73-lda-17" href="./emnlp-2011-Cache-based_Document-level_Statistical_Machine_Translation.html">25 emnlp-2011-Cache-based Document-level Statistical Machine Translation</a></p>
<p>18 0.39802447 <a title="73-lda-18" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>19 0.39713833 <a title="73-lda-19" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>20 0.39662316 <a title="73-lda-20" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
