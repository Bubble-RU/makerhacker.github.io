<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>56 emnlp-2011-Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-56" href="#">emnlp2011-56</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>56 emnlp-2011-Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases</h1>
<br/><p>Source: <a title="emnlp-2011-56-pdf" href="http://aclweb.org/anthology//D/D11/D11-1050.pdf">pdf</a></p><p>Author: Matthias Hartung ; Anette Frank</p><p>Abstract: This paper introduces an attribute selection task as a way to characterize the inherent meaning of property-denoting adjectives in adjective-noun phrases, such as e.g. hot in hot summer denoting the attribute TEMPERATURE, rather than TASTE. We formulate this task in a vector space model that represents adjectives and nouns as vectors in a semantic space defined over possible attributes. The vectors incorporate latent semantic information obtained from two variants of LDA topic models. Our LDA models outperform previous approaches on a small set of 10 attributes with considerable gains on sparse representations, which highlights the strong smoothing power of LDA models. For the first time, we extend the attribute selection task to a new data set with more than 200 classes. We observe that large-scale attribute selection is a hard problem, but a subset of attributes performs robustly on the large scale as well. Again, the LDA models outperform the VSM baseline.</p><p>Reference: <a title="emnlp-2011-56-reference" href="../emnlp2011_reference/emnlp-2011-Exploring_Supervised_LDA_Models_for_Assigning_Attributes_to_Adjective-Noun_Phrases_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract This paper introduces an attribute selection task as a way to characterize the inherent meaning of property-denoting adjectives in adjective-noun phrases, such as e. [sent-2, score-0.747]
</p><p>2 hot in hot summer denoting the attribute TEMPERATURE, rather than TASTE. [sent-4, score-0.63]
</p><p>3 We formulate this task in a vector space model that represents adjectives and nouns as vectors in a semantic space defined over possible attributes. [sent-5, score-0.336]
</p><p>4 The vectors incorporate latent semantic information obtained from two variants of LDA topic models. [sent-6, score-0.259]
</p><p>5 Our LDA models outperform previous approaches on a small set of 10 attributes with considerable gains on sparse representations, which highlights the strong smoothing power of LDA models. [sent-7, score-0.544]
</p><p>6 For the first time, we extend the attribute selection task to a new data set with more than 200 classes. [sent-8, score-0.555]
</p><p>7 We observe that large-scale attribute selection is a hard problem, but a subset of attributes performs robustly on the large scale as well. [sent-9, score-1.045]
</p><p>8 ), as well as latent variable models (LVMs) which aggregate distributional observations in ’hidden’, or latent variables, thereby reducing the dimensionality of the 540 . [sent-16, score-0.243]
</p><p>9 A topic that is increasingly studied in distributional semantics is the semantics of adjectives, both  O´  in isolation (Almuhareb, 2006) and in compositional adjective-noun phrases (Hartung and Frank, 2010; Guevara, 2010; Baroni and Zamparelli, 2010). [sent-23, score-0.39]
</p><p>10 In this paper, we propose a new approach to a problem we denote as attribute selection: The task is to predict the hidden attribute meaning expressed by a property-denoting adjective in composition with a noun. [sent-24, score-1.19]
</p><p>11 , may denote attributes such as TEMPERATURE, TASTE or EMOTIONALITY. [sent-27, score-0.49]
</p><p>12 These adjective meanings can be combined with nouns such as tea, soup or debate, which can be characterized in terms of attributes as well. [sent-28, score-0.657]
</p><p>13 The goal of the task is to determine the hidden attribute meaning predicated over the noun in a given adjective-noun phrase, as illustrated in (1). [sent-29, score-0.558]
</p><p>14 TEMPERATURE(summer) = hot It is by way of the composition of adjective and noun that specific attributes are selected from the adjective’s space of possible attribute meanings, and typically lead to a disambiguation of the adjective and possibly the noun. [sent-32, score-1.33]
</p><p>15 Hartung and Frank (2010) were the first to model this insight in a VSM by representing the meaning of adjectives and nouns in se-  mantic vectors defined over attributes. [sent-33, score-0.243]
</p><p>16 ec th2o0d1s1 i Ans Nsoactuiartaioln La fonrg Cuaogmep Purtoatcieosnsainlg L,in pgaugies ti 5c4s0–5 1,  vector composition, such that the ‘hidden’ attribute meaning of the phrase can be ‘selected’ as a prominent component from the composed vector. [sent-36, score-0.615]
</p><p>17 1 for the adjective enormous ( e~) in combination with the noun ball (~b), with alternative composition operations: vector multiplication (×) and addition (+). [sent-38, score-0.337]
</p><p>18 In the present paper we offer a new approach to this formalization of the compositional meaning of adjectives and nouns that owes to both distributional  VSMs and LVMs. [sent-40, score-0.334]
</p><p>19 (2003)) to train an attribute model that captures semantic information encoded in adjectives and nouns independently of one another. [sent-43, score-0.673]
</p><p>20 Following Hartung and Frank (2010), this model is embedded into a VSM that employs vector composition to combine the meaning of adjectives and nouns. [sent-44, score-0.299]
</p><p>21 We present two variants of LDA that differ in the way attributes are associated with the induced LDA topics: Controled LDA (C-LDA) and Labeled LDA (L-LDA; Ramage et al. [sent-45, score-0.49]
</p><p>22 (ii) While these prior approaches were restricted to a confined set of 10 attributes, we will we apply our  1The figure is adopted from the distributional setting of Hartung and Frank (2010), with component values defined by pattern frequency counts for the chosen attribute nouns. [sent-49, score-0.636]
</p><p>23 In Section 3 we introduce two LDA models for attribute selection: C-LDA and L-LDA. [sent-53, score-0.509]
</p><p>24 Section 4 describes the settings for two experiments: In the first experiment, we perform attribute selection confined to a space of 10 attributes to compare against prior work. [sent-54, score-1.112]
</p><p>25 In the second setting we perform attribute selection on a large scale, using 206 attributes. [sent-55, score-0.555]
</p><p>26 Almuhareb (2006) aims at capturing the relationship  between adjectives and attributes based on lexicosyntactic patterns, such as the ATTR of the * is ADJ. [sent-59, score-0.597]
</p><p>27 Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. [sent-61, score-0.246]
</p><p>28 While these works adhere to a purely latent representation of meaning, Hartung and Frank (2010) include attributes as symbolic ‘hidden’ meanings of adjectives, nouns and adjective-noun phrases in a distributional VSM. [sent-62, score-0.771]
</p><p>29 Finally, a large body of work dealing with compositionality in distributional frameworks is not confined to the special case of adjective-noun composition (Mitchell and Lapata (2008), Rudolph and Giesbrecht (2010), i. [sent-63, score-0.263]
</p><p>30 The idea to integrate topic models and VSMs goes back to Mitchell and Lapata (2009) who build a distributional model  with dimensions set to topics over bag-of-words features. [sent-86, score-0.421]
</p><p>31 In their setting, LDA merely serves the purpose of dimensionality reduction, whereas our particular motivation is to use topics as probabilistic indicators for the prediction of attributes as semantic target categories in adjective-noun composition. [sent-87, score-0.71]
</p><p>32 Their results indicate that a multiplicative setting works best for vector composition in word-based models, while vector addition is better suited for topic vectors. [sent-90, score-0.392]
</p><p>33 We apply a similar idea to the attribute selection problem: we collect pseudo-documents that characterize attributes by adjectives and nouns that co542 occur with the attribute nouns in local contextual relations. [sent-98, score-1.784]
</p><p>34 The topic distributions obtained from fitting an LDA model to the collection of these pseudodocuments can then be injected into semantic vector representations for adjectives and nouns. [sent-99, score-0.405]
</p><p>35 In its original statement, LDA is a fully unsupervised process (apart from the desired number of topics which has to be specified in advance) that estimates topic distributions over documents θd and topic-word distributions φt with topics represented as latent variables. [sent-100, score-0.488]
</p><p>36 For attribute selection, the LDA-inferred topics need to be linked to semantic attributes. [sent-104, score-0.642]
</p><p>37 For instance, it is unlikely to observe a dependency path linking the adjective mature to the attribute MATURITY. [sent-110, score-0.6]
</p><p>38 Consequently, the final attribute model is expected to assign a (small) positive probability to the relation between mature and MATURITY without observing it in the training data. [sent-113, score-0.518]
</p><p>39 The contents of the pseudo-documents are selected along syntactic dependency paths linking each attribute noun to meaningful context words (adjectives and nouns). [sent-118, score-0.534]
</p><p>40 , yields a pseudo-document for the attribute noun SPEED containing car and fast. [sent-121, score-0.533]
</p><p>41 Though we are ultimately interested in triples of attributes, adjectives and nouns that define the compositional semantics of adjective-noun phrases (cf. [sent-124, score-0.286]
</p><p>42 (1)), C-LDA is only exposed to binary tuples between attributes and adjectives or nouns, respectively. [sent-125, score-0.597]
</p><p>43 Presenting LDA with pseudo-documents that characterize individual target attributes imports supervision into the LDA process in two respects: the estimated topic proportions P(t|d) will be highly attribute-specific, raonpdo similarly so f)or w tihlle topic gdhilsytributions P(w|t). [sent-127, score-0.894]
</p><p>44 d Moreover, since C-LDA collects pseudo-documents focused on individual target attributes, we are able to link external categories to the generative process by heuristically labeling pseudo-documents with their respective attribute as target category. [sent-130, score-0.578]
</p><p>45 Thus, we approximate P(w|a), the probability of a word given an attribute, by P(w|d) as boabbtaili ntyed o ffro am w LorDdA g: attribute,  by  P  ×  2The dependency paths, together with the set of attribute nouns of interest, have to be manually specified. [sent-131, score-0.54]
</p><p>46 More specifically, L-LDA extends the generative process of LDA by constraining the topic distributions over documents to only those topics that  θ(d)  Λ(d). [sent-165, score-0.318]
</p><p>47 correspond to the document’s set of labels This is done by projecting the parameter vector of the Dirichlet topic prior α to a lower-dimensional vector α(d) whose topic dimensions correspond to the document labels. [sent-166, score-0.479]
</p><p>48 KThis matrix is used in step 6 to project the Dirichlet topic prior α to a lower-dimensional vector , whose topic dimensions correspond to the document labels. [sent-170, score-0.421]
</p><p>49 In our instantiation of L-LDA, we collect pseudodocuments for attributes exactly as for C-LDA. [sent-172, score-0.53]
</p><p>50 Documents are labeled with exactly one category, the attribute noun. [sent-173, score-0.505]
</p><p>51 Particularly, a word might be associated with some of the topics underlying an attribute, but not with all of them, and an attribute can be characterized by multiple topics. [sent-178, score-0.595]
</p><p>52 It selects all attributes that lead to an increase of entropy when suppressed from the vector representation. [sent-192, score-0.548]
</p><p>53 We evaluate the performance of the VSMs based on C-LDA and L-LDA in two experimental settings, contrasting the problem of attribute selection on semantic spaces of radically different dimen-  sionality, using sets of 10 vs. [sent-200, score-0.581]
</p><p>54 We evaluate against two gold standards consisting of adjective-noun phrases (or adjective-noun pairs) and their associated attribute meanings. [sent-203, score-0.512]
</p><p>55 DEPVSM is similar to PATTVSM; however, it relies on dependency paths that connect the target elements and attributes in local contexts. [sent-213, score-0.547]
</p><p>56 Data set for attribute selection over a large semantic space (206 attributes). [sent-223, score-0.602]
</p><p>57 In the second experiment, we max out the attribute selection task to a much larger set of attributes in order to analyze the difficulty of the task on more representative data. [sent-224, score-1.045]
</p><p>58 We automatically construct a data set of adjective-noun phrases labeled with appropriate attributes from WordNet 3. [sent-225, score-0.555]
</p><p>59 We first extract all adjectives that are linked to at least one attribute synset by the att ribute relation. [sent-227, score-0.604]
</p><p>60 Next, we run the glosses of these adjectives (3592 in number) through TreeTagger (Schmid, 1994) to find examples of adjectives modifying nouns in attributive constructions. [sent-228, score-0.278]
</p><p>61 The resulting adjective-noun phrases are labeled with the attribute label linked to the given adjective sense. [sent-229, score-0.644]
</p><p>62 t xt 5If an attribute provides only one example, this was added to the development set. [sent-240, score-0.476]
</p><p>63 1 Experiment 1 In Experiment 1, we evaluate the performance of C-LDA and L-LDA on the attribute selection task over 10 attributes against the pattern-based and dependency-based models PATTVSM and DEPVSM as competitive baselines. [sent-245, score-1.078]
</p><p>64 Given that C-LDA and L-LDA estimate attribute-specific topic distributions in the structured pseudo-documents under different assumptions regarding the correspondence of attributes and topics (cf. [sent-247, score-0.783]
</p><p>65 1 Attribute Selection for 10 Attributes Tables 1 and 2 summarize the results for at-  tribute selection over 10 attributes against the labeled adjective-noun pairs in the test set, using ESel and MPC as selection functions on vectors composed by multiplication (Table 1) and addition (Table 2). [sent-254, score-0.808]
</p><p>66 The results reported for C-LDA correspond to the best performing model (with number of topics set to 42, as this setting yields the best and most constant results over both composition operators). [sent-255, score-0.25]
</p><p>67 Compared to the LDA models, the VSM baselines 206 attributes, while all models were trained on 262 attributes obtained from WordNet in the first extraction step. [sent-267, score-0.523]
</p><p>68 Figures 3 and 4 display the overall performance curve ranging over different topic numbers for CLDAESel,+ and C-LDAESel, compared to the remaining models that are not dependent on topic size. [sent-300, score-0.325]
</p><p>69 For topic numbers smaller than the attribute set size, C-LDA underperforms, for obvious reasons. [sent-301, score-0.622]
</p><p>70 Topcis  Figure 4: Performance of C-LDAESel,+ for different topic numbers, compared against all other models dition at topic ranges larger than 10. [sent-304, score-0.325]
</p><p>71 2 Experiment 2 Experiment 2 is designed attributes to be modeled, both LDA models and the in the attribute selection  to max out the space of to assess the capacity of DEPVSM baseline model task on a large attribute  space. [sent-337, score-1.611]
</p><p>72 1 Large-scale Attribute Selection Table 5 (column all) displays the performance of all models on attribute selection over a range of 206 7We did not apply PATTVSM to this large-scale experiment, as only poor performance can be expected. [sent-341, score-0.588]
</p><p>73 yC1057-DLAESel on 206 (all) and 73 property attributes (property)  ×  Table 6: Attribute selection on 206 attributes (all) and 73 property attributes (property); performance figures of CLDAESel, for best attributes (F>0) dimensions, contrasting vector addition and multi-  plication. [sent-346, score-2.217]
</p><p>74 As the overall performance is close to 0 for both composition methods, no parameter setting can be identified as particularly suited for this large-scale attribute selection task. [sent-348, score-0.656]
</p><p>75 2 Focused Evaluation and Data Analysis To gain a deeper insight into the modeling capacity of the LDA models for this large-scale selection task, Table 6 (column all) presents a partial evaluation of attributes that could be assigned to adjectivenoun pairs with an f-score >0 by C-LDAESel, . [sent-352, score-0.662]
</p><p>76 LDAESel,  ×  in Experiment 2  the LDA models on this large attribute space, it is remarkable that C-LDA is able to induce distinctive topic distributions for a number of attributes with up to 0. [sent-357, score-1.173]
</p><p>77 Raising the attribute selection task from 10 to 206 attributes poses a true challenge to our models, by the sheer size and diversity of the semantic space  considered. [sent-360, score-1.092]
</p><p>78 As seen above, C-LDA achieves relatively high performance figures on selected attributes (cf. [sent-369, score-0.49]
</p><p>79 In order to identify what makes these attributes different from others that resist successful modeling, we investigated three factors: (i) the amount of training data available for each attribute, (ii) the ambiguity rate per attribute, and (iii) their ontological subtype. [sent-372, score-0.525]
</p><p>80 (i) Measuring the dependence between training data size and f-score per attribute shows that a large amount of training data is generally helpful, but not the decisive factor (Pearson’s r = 0. [sent-373, score-0.503]
</p><p>81 In fact, the qualitative analysis in Table 7 shows that C-LDA is capable of assigning meaningful attributes to adjective-noun phrases not only in easy, but also ambiguous cases (cf. [sent-379, score-0.526]
</p><p>82 shallow water, where DEPTH is the only attribute provided for shallow in WordNet vs. [sent-380, score-0.476]
</p><p>83 2 are rather diverse, including concepts such as HEIGHT, KINDNESS or INDIVIDUALITY, we observe a high number of attributes from Exp. [sent-383, score-0.49]
</p><p>84 Given that they are categorized into the property class in WordNet9, we presume that the varying performance across attributes might be influenced by their ontological subtype. [sent-387, score-0.585]
</p><p>85 2, with training data limited to the 73 attributes pertaining to the property subtype in WordNet. [sent-389, score-0.55]
</p><p>86 ×  9WordNet  separates  attributes into properties, qualities and  among several others. [sent-400, score-0.49]
</p><p>87 Again, a more detailed analysis is given in Table 6 (column property), showing the performance of the best individual property attributes (F>0) in the restricted experiment. [sent-402, score-0.55]
</p><p>88 Average performance of the best property attributes with F>0, individually, amounts to F=0. [sent-403, score-0.55]
</p><p>89 column all), nearly all property attributes benefit from model training on selective data. [sent-406, score-0.573]
</p><p>90 Thus, apparently, some of the adjectives associated with non-property attributes in the full set provide some discriminative power that is helpful to distinguish property types. [sent-408, score-0.657]
</p><p>91 In a qualitative analysis of the 133 non-property attributes filtered out in this experiment, we find that the WordNet-SUMO mapping (Niles, 2003) does not provide differentiating definitions for about 60% of these attributes, linking them instead to a single subjective assessment attribute. [sent-409, score-0.49]
</p><p>92 6  Conclusion  This paper explored the use of LDA topic models in a semantic labeling task that predicts attributes as ’hidden’ meanings in the compositional semantics of adjective-noun phrases. [sent-411, score-0.795]
</p><p>93 LDA topic models are expected to alleviate sparsity problems of distributional VSMs as encountered in prior work, by incorporating latent semantic information about attribute nouns. [sent-412, score-0.844]
</p><p>94 We proposed two LDA models for the attribute selection task that import supervision for a target category parameter in different ways: L-LDA (Ramage et al. [sent-415, score-0.639]
</p><p>95 Combining standard LDA topic modeling with a means of interpreting the induced topics relative to a set of external categories, C-LDA offers greater flexibility and expressiveness. [sent-422, score-0.289]
</p><p>96 Our experimental results show that modeling attributes as latent or explicit topics with C-LDA and L-LDA, respectively, outperforms the purely distributional baseline model DEPVSM and PATTVSM of prior work. [sent-423, score-0.793]
</p><p>97 However,  we obtain respectable results on a subset of attributes denoting properties, where C-LDA performs best in quantitative performance measures. [sent-429, score-0.513]
</p><p>98 It yields highest f-scores in full and partial evaluation both with the full-size attribute model, and when training and testing is restricted to property attributes. [sent-430, score-0.566]
</p><p>99 Data analysis indicates that our models perform more robustly on concrete attributes in contrast to abstract attribute types that lack clear categorization. [sent-432, score-0.999]
</p><p>100 This suggests that our approach to attribute selection is most appropriate for detecting attributes that reflect clear ontological distinctions. [sent-433, score-1.08]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('attributes', 0.49), ('attribute', 0.476), ('lda', 0.378), ('hartung', 0.241), ('vsm', 0.184), ('topic', 0.146), ('esel', 0.12), ('vsms', 0.12), ('topics', 0.119), ('adjectives', 0.107), ('pattvsm', 0.107), ('composition', 0.101), ('distributional', 0.09), ('frank', 0.086), ('adjective', 0.082), ('depvsm', 0.08), ('mpc', 0.08), ('selection', 0.079), ('multiplication', 0.069), ('nouns', 0.064), ('property', 0.06), ('vector', 0.058), ('baroni', 0.055), ('almuhareb', 0.053), ('llda', 0.053), ('pattvsmesel', 0.053), ('hot', 0.051), ('ramage', 0.051), ('latent', 0.048), ('confined', 0.046), ('clda', 0.046), ('wordnet', 0.042), ('temperature', 0.042), ('mature', 0.042), ('attr', 0.042), ('blei', 0.042), ('compositional', 0.04), ('anette', 0.04), ('cldaesel', 0.04), ('controled', 0.04), ('lvms', 0.04), ('pseudodocuments', 0.04), ('semantics', 0.039), ('vectors', 0.039), ('document', 0.038), ('capacity', 0.036), ('phrases', 0.036), ('ontological', 0.035), ('taste', 0.035), ('mitchell', 0.034), ('meaning', 0.033), ('models', 0.033), ('dimensions', 0.033), ('proportions', 0.033), ('matthias', 0.031), ('paths', 0.031), ('yields', 0.03), ('multiplicative', 0.029), ('summer', 0.029), ('labeled', 0.029), ('characterize', 0.028), ('distributions', 0.028), ('arattr', 0.027), ('decisive', 0.027), ('disclda', 0.027), ('ldaesel', 0.027), ('maturity', 0.027), ('topcis', 0.027), ('tpattr', 0.027), ('noun', 0.027), ('dirichlet', 0.026), ('semantic', 0.026), ('compositionality', 0.026), ('target', 0.026), ('sparsity', 0.025), ('categories', 0.025), ('generative', 0.025), ('experiment', 0.025), ('supervision', 0.025), ('dimensionality', 0.024), ('inherent', 0.024), ('regard', 0.024), ('prominent', 0.024), ('selectional', 0.024), ('component', 0.024), ('modeling', 0.024), ('denoting', 0.023), ('column', 0.023), ('superscripts', 0.023), ('guevara', 0.023), ('tribute', 0.023), ('hidden', 0.022), ('purely', 0.022), ('ritter', 0.022), ('meanings', 0.021), ('lapata', 0.021), ('space', 0.021), ('sparse', 0.021), ('linked', 0.021), ('jeff', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="56-tfidf-1" href="./emnlp-2011-Exploring_Supervised_LDA_Models_for_Assigning_Attributes_to_Adjective-Noun_Phrases.html">56 emnlp-2011-Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases</a></p>
<p>Author: Matthias Hartung ; Anette Frank</p><p>Abstract: This paper introduces an attribute selection task as a way to characterize the inherent meaning of property-denoting adjectives in adjective-noun phrases, such as e.g. hot in hot summer denoting the attribute TEMPERATURE, rather than TASTE. We formulate this task in a vector space model that represents adjectives and nouns as vectors in a semantic space defined over possible attributes. The vectors incorporate latent semantic information obtained from two variants of LDA topic models. Our LDA models outperform previous approaches on a small set of 10 attributes with considerable gains on sparse representations, which highlights the strong smoothing power of LDA models. For the first time, we extend the attribute selection task to a new data set with more than 200 classes. We observe that large-scale attribute selection is a hard problem, but a subset of attributes performs robustly on the large scale as well. Again, the LDA models outperform the VSM baseline.</p><p>2 0.21461436 <a title="56-tfidf-2" href="./emnlp-2011-Semantic_Topic_Models%3A_Combining_Word_Distributional_Statistics_and_Dictionary_Definitions.html">119 emnlp-2011-Semantic Topic Models: Combining Word Distributional Statistics and Dictionary Definitions</a></p>
<p>Author: Weiwei Guo ; Mona Diab</p><p>Abstract: In this paper, we propose a novel topic model based on incorporating dictionary definitions. Traditional topic models treat words as surface strings without assuming predefined knowledge about word meaning. They infer topics only by observing surface word co-occurrence. However, the co-occurred words may not be semantically related in a manner that is relevant for topic coherence. Exploiting dictionary definitions explicitly in our model yields a better understanding of word semantics leading to better text modeling. We exploit WordNet as a lexical resource for sense definitions. We show that explicitly modeling word definitions helps improve performance significantly over the baseline for a text categorization task.</p><p>3 0.1872372 <a title="56-tfidf-3" href="./emnlp-2011-Optimizing_Semantic_Coherence_in_Topic_Models.html">101 emnlp-2011-Optimizing Semantic Coherence in Topic Models</a></p>
<p>Author: David Mimno ; Hanna Wallach ; Edmund Talley ; Miriam Leenders ; Andrew McCallum</p><p>Abstract: Latent variable models have the potential to add value to large document collections by discovering interpretable, low-dimensional subspaces. In order for people to use such models, however, they must trust them. Unfortunately, typical dimensionality reduction methods for text, such as latent Dirichlet allocation, often produce low-dimensional subspaces (topics) that are obviously flawed to human domain experts. The contributions of this paper are threefold: (1) An analysis of the ways in which topics can be flawed; (2) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data; (3) a novel statistical topic model based on this metric that significantly improves topic quality in a large-scale document collection from the National Institutes of Health (NIH).</p><p>4 0.16413039 <a title="56-tfidf-4" href="./emnlp-2011-Generating_Subsequent_Reference_in_Shared_Visual_Scenes%3A_Computation_vs_Re-Use.html">62 emnlp-2011-Generating Subsequent Reference in Shared Visual Scenes: Computation vs Re-Use</a></p>
<p>Author: Jette Viethen ; Robert Dale ; Markus Guhe</p><p>Abstract: Traditional computational approaches to referring expression generation operate in a deliberate manner, choosing the attributes to be included on the basis of their ability to distinguish the intended referent from its distractors. However, work in psycholinguistics suggests that speakers align their referring expressions with those used previously in the discourse, implying less deliberate choice and more subconscious reuse. This raises the question as to which is a more accurate characterisation of what people do. Using a corpus of dialogues containing 16,358 referring expressions, we explore this question via the generation of subsequent references in shared visual scenes. We use a machine learning approach to referring expression generation and demonstrate that incorporating features that correspond to the computational tradition does not match human referring behaviour as well as using features corresponding to the process of alignment. The results support the view that the traditional model of referring expression generation that is widely assumed in work on natural language generation may not in fact be correct; our analysis may also help explain the oft-observed redundancy found in humanproduced referring expressions.</p><p>5 0.16351198 <a title="56-tfidf-5" href="./emnlp-2011-Bootstrapped_Named_Entity_Recognition_for_Product_Attribute_Extraction.html">23 emnlp-2011-Bootstrapped Named Entity Recognition for Product Attribute Extraction</a></p>
<p>Author: Duangmanee Putthividhya ; Junling Hu</p><p>Abstract: We present a named entity recognition (NER) system for extracting product attributes and values from listing titles. Information extraction from short listing titles present a unique challenge, with the lack of informative context and grammatical structure. In this work, we combine supervised NER with bootstrapping to expand the seed list, and output normalized results. Focusing on listings from eBay’s clothing and shoes categories, our bootstrapped NER system is able to identify new brands corresponding to spelling variants and typographical errors of the known brands, as well as identifying novel brands. Among the top 300 new brands predicted, our system achieves 90.33% precision. To output normalized attribute values, we explore several string comparison algorithms and found n-gram substring matching to work well in practice.</p><p>6 0.15601274 <a title="56-tfidf-6" href="./emnlp-2011-Bayesian_Checking_for_Topic_Models.html">21 emnlp-2011-Bayesian Checking for Topic Models</a></p>
<p>7 0.13486262 <a title="56-tfidf-7" href="./emnlp-2011-Cross-Cutting_Models_of_Lexical_Semantics.html">37 emnlp-2011-Cross-Cutting Models of Lexical Semantics</a></p>
<p>8 0.13298477 <a title="56-tfidf-8" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>9 0.11115523 <a title="56-tfidf-9" href="./emnlp-2011-Unsupervised_Learning_of_Selectional_Restrictions_and_Detection_of_Argument_Coercions.html">144 emnlp-2011-Unsupervised Learning of Selectional Restrictions and Detection of Argument Coercions</a></p>
<p>10 0.09820085 <a title="56-tfidf-10" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>11 0.071368068 <a title="56-tfidf-11" href="./emnlp-2011-Latent_Vector_Weighting_for_Word_Meaning_in_Context.html">80 emnlp-2011-Latent Vector Weighting for Word Meaning in Context</a></p>
<p>12 0.070118263 <a title="56-tfidf-12" href="./emnlp-2011-Compositional_Matrix-Space_Models_for_Sentiment_Analysis.html">30 emnlp-2011-Compositional Matrix-Space Models for Sentiment Analysis</a></p>
<p>13 0.068823934 <a title="56-tfidf-13" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>14 0.063838758 <a title="56-tfidf-14" href="./emnlp-2011-Syntactic_Decision_Tree_LMs%3A_Random_Selection_or_Intelligent_Design%3F.html">131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</a></p>
<p>15 0.062494338 <a title="56-tfidf-15" href="./emnlp-2011-A_generative_model_for_unsupervised_discovery_of_relations_and_argument_classes_from_clinical_texts.html">14 emnlp-2011-A generative model for unsupervised discovery of relations and argument classes from clinical texts</a></p>
<p>16 0.061855171 <a title="56-tfidf-16" href="./emnlp-2011-Relation_Extraction_with_Relation_Topics.html">114 emnlp-2011-Relation Extraction with Relation Topics</a></p>
<p>17 0.061026543 <a title="56-tfidf-17" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>18 0.056168947 <a title="56-tfidf-18" href="./emnlp-2011-Generating_Aspect-oriented_Multi-Document_Summarization_with_Event-aspect_model.html">61 emnlp-2011-Generating Aspect-oriented Multi-Document Summarization with Event-aspect model</a></p>
<p>19 0.04873661 <a title="56-tfidf-19" href="./emnlp-2011-A_Cascaded_Classification_Approach_to_Semantic_Head_Recognition.html">2 emnlp-2011-A Cascaded Classification Approach to Semantic Head Recognition</a></p>
<p>20 0.046607438 <a title="56-tfidf-20" href="./emnlp-2011-Named_Entity_Recognition_in_Tweets%3A_An_Experimental_Study.html">98 emnlp-2011-Named Entity Recognition in Tweets: An Experimental Study</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.177), (1, -0.158), (2, -0.193), (3, -0.21), (4, -0.029), (5, 0.295), (6, 0.076), (7, 0.045), (8, 0.102), (9, -0.059), (10, -0.016), (11, 0.067), (12, 0.002), (13, -0.027), (14, -0.176), (15, -0.041), (16, 0.156), (17, 0.019), (18, -0.03), (19, 0.003), (20, 0.14), (21, 0.031), (22, 0.01), (23, -0.086), (24, 0.078), (25, -0.081), (26, 0.195), (27, -0.089), (28, -0.095), (29, -0.026), (30, -0.158), (31, 0.207), (32, 0.098), (33, -0.021), (34, 0.075), (35, -0.11), (36, 0.082), (37, -0.026), (38, 0.003), (39, -0.09), (40, -0.045), (41, 0.049), (42, 0.037), (43, -0.166), (44, -0.046), (45, 0.017), (46, 0.074), (47, 0.017), (48, 0.037), (49, -0.12)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95352018 <a title="56-lsi-1" href="./emnlp-2011-Exploring_Supervised_LDA_Models_for_Assigning_Attributes_to_Adjective-Noun_Phrases.html">56 emnlp-2011-Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases</a></p>
<p>Author: Matthias Hartung ; Anette Frank</p><p>Abstract: This paper introduces an attribute selection task as a way to characterize the inherent meaning of property-denoting adjectives in adjective-noun phrases, such as e.g. hot in hot summer denoting the attribute TEMPERATURE, rather than TASTE. We formulate this task in a vector space model that represents adjectives and nouns as vectors in a semantic space defined over possible attributes. The vectors incorporate latent semantic information obtained from two variants of LDA topic models. Our LDA models outperform previous approaches on a small set of 10 attributes with considerable gains on sparse representations, which highlights the strong smoothing power of LDA models. For the first time, we extend the attribute selection task to a new data set with more than 200 classes. We observe that large-scale attribute selection is a hard problem, but a subset of attributes performs robustly on the large scale as well. Again, the LDA models outperform the VSM baseline.</p><p>2 0.68792868 <a title="56-lsi-2" href="./emnlp-2011-Cross-Cutting_Models_of_Lexical_Semantics.html">37 emnlp-2011-Cross-Cutting Models of Lexical Semantics</a></p>
<p>Author: Joseph Reisinger ; Raymond Mooney</p><p>Abstract: Context-dependent word similarity can be measured over multiple cross-cutting dimensions. For example, lung and breath are similar thematically, while authoritative and superficial occur in similar syntactic contexts, but share little semantic similarity. Both of these notions of similarity play a role in determining word meaning, and hence lexical semantic models must take them both into account. Towards this end, we develop a novel model, Multi-View Mixture (MVM), that represents words as multiple overlapping clusterings. MVM finds multiple data partitions based on different subsets of features, subject to the marginal constraint that feature subsets are distributed according to Latent Dirich- let Allocation. Intuitively, this constraint favors feature partitions that have coherent topical semantics. Furthermore, MVM uses soft feature assignment, hence the contribution of each data point to each clustering view is variable, isolating the impact of data only to views where they assign the most features. Through a series of experiments, we demonstrate the utility of MVM as an inductive bias for capturing relations between words that are intuitive to humans, outperforming related models such as Latent Dirichlet Allocation.</p><p>3 0.58879226 <a title="56-lsi-3" href="./emnlp-2011-Bootstrapped_Named_Entity_Recognition_for_Product_Attribute_Extraction.html">23 emnlp-2011-Bootstrapped Named Entity Recognition for Product Attribute Extraction</a></p>
<p>Author: Duangmanee Putthividhya ; Junling Hu</p><p>Abstract: We present a named entity recognition (NER) system for extracting product attributes and values from listing titles. Information extraction from short listing titles present a unique challenge, with the lack of informative context and grammatical structure. In this work, we combine supervised NER with bootstrapping to expand the seed list, and output normalized results. Focusing on listings from eBay’s clothing and shoes categories, our bootstrapped NER system is able to identify new brands corresponding to spelling variants and typographical errors of the known brands, as well as identifying novel brands. Among the top 300 new brands predicted, our system achieves 90.33% precision. To output normalized attribute values, we explore several string comparison algorithms and found n-gram substring matching to work well in practice.</p><p>4 0.56386697 <a title="56-lsi-4" href="./emnlp-2011-Generating_Subsequent_Reference_in_Shared_Visual_Scenes%3A_Computation_vs_Re-Use.html">62 emnlp-2011-Generating Subsequent Reference in Shared Visual Scenes: Computation vs Re-Use</a></p>
<p>Author: Jette Viethen ; Robert Dale ; Markus Guhe</p><p>Abstract: Traditional computational approaches to referring expression generation operate in a deliberate manner, choosing the attributes to be included on the basis of their ability to distinguish the intended referent from its distractors. However, work in psycholinguistics suggests that speakers align their referring expressions with those used previously in the discourse, implying less deliberate choice and more subconscious reuse. This raises the question as to which is a more accurate characterisation of what people do. Using a corpus of dialogues containing 16,358 referring expressions, we explore this question via the generation of subsequent references in shared visual scenes. We use a machine learning approach to referring expression generation and demonstrate that incorporating features that correspond to the computational tradition does not match human referring behaviour as well as using features corresponding to the process of alignment. The results support the view that the traditional model of referring expression generation that is widely assumed in work on natural language generation may not in fact be correct; our analysis may also help explain the oft-observed redundancy found in humanproduced referring expressions.</p><p>5 0.46513379 <a title="56-lsi-5" href="./emnlp-2011-Semantic_Topic_Models%3A_Combining_Word_Distributional_Statistics_and_Dictionary_Definitions.html">119 emnlp-2011-Semantic Topic Models: Combining Word Distributional Statistics and Dictionary Definitions</a></p>
<p>Author: Weiwei Guo ; Mona Diab</p><p>Abstract: In this paper, we propose a novel topic model based on incorporating dictionary definitions. Traditional topic models treat words as surface strings without assuming predefined knowledge about word meaning. They infer topics only by observing surface word co-occurrence. However, the co-occurred words may not be semantically related in a manner that is relevant for topic coherence. Exploiting dictionary definitions explicitly in our model yields a better understanding of word semantics leading to better text modeling. We exploit WordNet as a lexical resource for sense definitions. We show that explicitly modeling word definitions helps improve performance significantly over the baseline for a text categorization task.</p><p>6 0.4437322 <a title="56-lsi-6" href="./emnlp-2011-Optimizing_Semantic_Coherence_in_Topic_Models.html">101 emnlp-2011-Optimizing Semantic Coherence in Topic Models</a></p>
<p>7 0.43791905 <a title="56-lsi-7" href="./emnlp-2011-Bayesian_Checking_for_Topic_Models.html">21 emnlp-2011-Bayesian Checking for Topic Models</a></p>
<p>8 0.39196223 <a title="56-lsi-8" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>9 0.3882018 <a title="56-lsi-9" href="./emnlp-2011-Unsupervised_Learning_of_Selectional_Restrictions_and_Detection_of_Argument_Coercions.html">144 emnlp-2011-Unsupervised Learning of Selectional Restrictions and Detection of Argument Coercions</a></p>
<p>10 0.29234356 <a title="56-lsi-10" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>11 0.24786638 <a title="56-lsi-11" href="./emnlp-2011-Latent_Vector_Weighting_for_Word_Meaning_in_Context.html">80 emnlp-2011-Latent Vector Weighting for Word Meaning in Context</a></p>
<p>12 0.22242773 <a title="56-lsi-12" href="./emnlp-2011-A_Cascaded_Classification_Approach_to_Semantic_Head_Recognition.html">2 emnlp-2011-A Cascaded Classification Approach to Semantic Head Recognition</a></p>
<p>13 0.20865715 <a title="56-lsi-13" href="./emnlp-2011-Syntactic_Decision_Tree_LMs%3A_Random_Selection_or_Intelligent_Design%3F.html">131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</a></p>
<p>14 0.19936219 <a title="56-lsi-14" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>15 0.17611243 <a title="56-lsi-15" href="./emnlp-2011-Learning_Local_Content_Shift_Detectors_from_Document-level_Information.html">82 emnlp-2011-Learning Local Content Shift Detectors from Document-level Information</a></p>
<p>16 0.17610589 <a title="56-lsi-16" href="./emnlp-2011-Compositional_Matrix-Space_Models_for_Sentiment_Analysis.html">30 emnlp-2011-Compositional Matrix-Space Models for Sentiment Analysis</a></p>
<p>17 0.16414915 <a title="56-lsi-17" href="./emnlp-2011-Approximate_Scalable_Bounded_Space_Sketch_for_Large_Data_NLP.html">19 emnlp-2011-Approximate Scalable Bounded Space Sketch for Large Data NLP</a></p>
<p>18 0.16407254 <a title="56-lsi-18" href="./emnlp-2011-Lexical_Co-occurrence%2C_Statistical_Significance%2C_and_Word_Association.html">86 emnlp-2011-Lexical Co-occurrence, Statistical Significance, and Word Association</a></p>
<p>19 0.16250086 <a title="56-lsi-19" href="./emnlp-2011-Improving_Bilingual_Projections_via_Sparse_Covariance_Matrices.html">73 emnlp-2011-Improving Bilingual Projections via Sparse Covariance Matrices</a></p>
<p>20 0.15375957 <a title="56-lsi-20" href="./emnlp-2011-Generating_Aspect-oriented_Multi-Document_Summarization_with_Event-aspect_model.html">61 emnlp-2011-Generating Aspect-oriented Multi-Document Summarization with Event-aspect model</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.073), (27, 0.011), (36, 0.018), (37, 0.018), (45, 0.123), (52, 0.278), (53, 0.024), (54, 0.028), (57, 0.018), (62, 0.017), (64, 0.018), (66, 0.072), (69, 0.017), (79, 0.051), (82, 0.038), (87, 0.015), (90, 0.013), (94, 0.019), (96, 0.038), (98, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77805114 <a title="56-lda-1" href="./emnlp-2011-Exploring_Supervised_LDA_Models_for_Assigning_Attributes_to_Adjective-Noun_Phrases.html">56 emnlp-2011-Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases</a></p>
<p>Author: Matthias Hartung ; Anette Frank</p><p>Abstract: This paper introduces an attribute selection task as a way to characterize the inherent meaning of property-denoting adjectives in adjective-noun phrases, such as e.g. hot in hot summer denoting the attribute TEMPERATURE, rather than TASTE. We formulate this task in a vector space model that represents adjectives and nouns as vectors in a semantic space defined over possible attributes. The vectors incorporate latent semantic information obtained from two variants of LDA topic models. Our LDA models outperform previous approaches on a small set of 10 attributes with considerable gains on sparse representations, which highlights the strong smoothing power of LDA models. For the first time, we extend the attribute selection task to a new data set with more than 200 classes. We observe that large-scale attribute selection is a hard problem, but a subset of attributes performs robustly on the large scale as well. Again, the LDA models outperform the VSM baseline.</p><p>2 0.75091791 <a title="56-lda-2" href="./emnlp-2011-Discriminating_Gender_on_Twitter.html">41 emnlp-2011-Discriminating Gender on Twitter</a></p>
<p>Author: John D. Burger ; John Henderson ; George Kim ; Guido Zarrella</p><p>Abstract: Accurate prediction of demographic attributes from social media and other informal online content is valuable for marketing, personalization, and legal investigation. This paper describes the construction of a large, multilingual dataset labeled with gender, and investigates statistical models for determining the gender of uncharacterized Twitter users. We explore several different classifier types on this dataset. We show the degree to which classifier accuracy varies based on tweet volumes as well as when various kinds of profile metadata are included in the models. We also perform a large-scale human assessment using Amazon Mechanical Turk. Our methods significantly out-perform both baseline models and almost all humans on the same task.</p><p>3 0.7395469 <a title="56-lda-3" href="./emnlp-2011-Cross-Cutting_Models_of_Lexical_Semantics.html">37 emnlp-2011-Cross-Cutting Models of Lexical Semantics</a></p>
<p>Author: Joseph Reisinger ; Raymond Mooney</p><p>Abstract: Context-dependent word similarity can be measured over multiple cross-cutting dimensions. For example, lung and breath are similar thematically, while authoritative and superficial occur in similar syntactic contexts, but share little semantic similarity. Both of these notions of similarity play a role in determining word meaning, and hence lexical semantic models must take them both into account. Towards this end, we develop a novel model, Multi-View Mixture (MVM), that represents words as multiple overlapping clusterings. MVM finds multiple data partitions based on different subsets of features, subject to the marginal constraint that feature subsets are distributed according to Latent Dirich- let Allocation. Intuitively, this constraint favors feature partitions that have coherent topical semantics. Furthermore, MVM uses soft feature assignment, hence the contribution of each data point to each clustering view is variable, isolating the impact of data only to views where they assign the most features. Through a series of experiments, we demonstrate the utility of MVM as an inductive bias for capturing relations between words that are intuitive to humans, outperforming related models such as Latent Dirichlet Allocation.</p><p>4 0.54258329 <a title="56-lda-4" href="./emnlp-2011-Cooooooooooooooollllllllllllll%21%21%21%21%21%21%21%21%21%21%21%21%21%21_Using_Word_Lengthening_to_Detect_Sentiment_in_Microblogs.html">33 emnlp-2011-Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using Word Lengthening to Detect Sentiment in Microblogs</a></p>
<p>Author: Samuel Brody ; Nicholas Diakopoulos</p><p>Abstract: We present an automatic method which leverages word lengthening to adapt a sentiment lexicon specifically for Twitter and similar social messaging networks. The contributions of the paper are as follows. First, we call attention to lengthening as a widespread phenomenon in microblogs and social messaging, and demonstrate the importance of handling it correctly. We then show that lengthening is strongly associated with subjectivity and sentiment. Finally, we present an automatic method which leverages this association to detect domain-specific sentiment- and emotionbearing words. We evaluate our method by comparison to human judgments, and analyze its strengths and weaknesses. Our results are of interest to anyone analyzing sentiment in microblogs and social networks, whether for research or commercial purposes.</p><p>5 0.53954619 <a title="56-lda-5" href="./emnlp-2011-Identifying_and_Following_Expert_Investors_in_Stock_Microblogs.html">71 emnlp-2011-Identifying and Following Expert Investors in Stock Microblogs</a></p>
<p>Author: Roy Bar-Haim ; Elad Dinur ; Ronen Feldman ; Moshe Fresko ; Guy Goldstein</p><p>Abstract: Information published in online stock investment message boards, and more recently in stock microblogs, is considered highly valuable by many investors. Previous work focused on aggregation of sentiment from all users. However, in this work we show that it is beneficial to distinguish expert users from non-experts. We propose a general framework for identifying expert investors, and use it as a basis for several models that predict stock rise from stock microblogging messages (stock tweets). In particular, we present two methods that combine expert identification and per-user unsupervised learning. These methods were shown to achieve relatively high precision in predicting stock rise, and significantly outperform our baseline. In addition, our work provides an in-depth analysis of the content and potential usefulness of stock tweets.</p><p>6 0.53846121 <a title="56-lda-6" href="./emnlp-2011-Twitter_Catches_The_Flu%3A_Detecting_Influenza_Epidemics_using_Twitter.html">139 emnlp-2011-Twitter Catches The Flu: Detecting Influenza Epidemics using Twitter</a></p>
<p>7 0.53841329 <a title="56-lda-7" href="./emnlp-2011-Rumor_has_it%3A_Identifying_Misinformation_in_Microblogs.html">117 emnlp-2011-Rumor has it: Identifying Misinformation in Microblogs</a></p>
<p>8 0.53601569 <a title="56-lda-8" href="./emnlp-2011-Named_Entity_Recognition_in_Tweets%3A_An_Experimental_Study.html">98 emnlp-2011-Named Entity Recognition in Tweets: An Experimental Study</a></p>
<p>9 0.51545972 <a title="56-lda-9" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>10 0.51278657 <a title="56-lda-10" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>11 0.51076448 <a title="56-lda-11" href="./emnlp-2011-Linguistic_Redundancy_in_Twitter.html">89 emnlp-2011-Linguistic Redundancy in Twitter</a></p>
<p>12 0.50652367 <a title="56-lda-12" href="./emnlp-2011-Semantic_Topic_Models%3A_Combining_Word_Distributional_Statistics_and_Dictionary_Definitions.html">119 emnlp-2011-Semantic Topic Models: Combining Word Distributional Statistics and Dictionary Definitions</a></p>
<p>13 0.49089971 <a title="56-lda-13" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>14 0.48639411 <a title="56-lda-14" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>15 0.4856683 <a title="56-lda-15" href="./emnlp-2011-Linking_Entities_to_a_Knowledge_Base_with_Query_Expansion.html">90 emnlp-2011-Linking Entities to a Knowledge Base with Query Expansion</a></p>
<p>16 0.48236415 <a title="56-lda-16" href="./emnlp-2011-Approximate_Scalable_Bounded_Space_Sketch_for_Large_Data_NLP.html">19 emnlp-2011-Approximate Scalable Bounded Space Sketch for Large Data NLP</a></p>
<p>17 0.47710517 <a title="56-lda-17" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>18 0.47677553 <a title="56-lda-18" href="./emnlp-2011-Data-Driven_Response_Generation_in_Social_Media.html">38 emnlp-2011-Data-Driven Response Generation in Social Media</a></p>
<p>19 0.47601962 <a title="56-lda-19" href="./emnlp-2011-Harnessing_different_knowledge_sources_to_measure_semantic_relatedness_under_a_uniform_model.html">64 emnlp-2011-Harnessing different knowledge sources to measure semantic relatedness under a uniform model</a></p>
<p>20 0.4733417 <a title="56-lda-20" href="./emnlp-2011-Discovering_Morphological_Paradigms_from_Plain_Text_Using_a_Dirichlet_Process_Mixture_Model.html">39 emnlp-2011-Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
