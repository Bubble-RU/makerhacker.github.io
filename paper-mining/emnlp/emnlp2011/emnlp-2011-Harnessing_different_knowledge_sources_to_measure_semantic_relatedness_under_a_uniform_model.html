<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>64 emnlp-2011-Harnessing different knowledge sources to measure semantic relatedness under a uniform model</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-64" href="#">emnlp2011-64</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>64 emnlp-2011-Harnessing different knowledge sources to measure semantic relatedness under a uniform model</h1>
<br/><p>Source: <a title="emnlp-2011-64-pdf" href="http://aclweb.org/anthology//D/D11/D11-1092.pdf">pdf</a></p><p>Author: Ziqi Zhang ; Anna Lisa Gentile ; Fabio Ciravegna</p><p>Abstract: Measuring semantic relatedness between words or concepts is a crucial process to many Natural Language Processing tasks. Exiting methods exploit semantic evidence from a single knowledge source, and are predominantly evaluated only in the general domain. This paper introduces a method of harnessing different knowledge sources under a uniform model for measuring semantic relatedness between words or concepts. Using Wikipedia and WordNet as examples, and evaluated in both the general and biomedical domains, it successfully combines strengths from both knowledge sources and outperforms stateof-the-art on many datasets.</p><p>Reference: <a title="emnlp-2011-64-reference" href="../emnlp2011_reference/emnlp-2011-Harnessing_different_knowledge_sources_to_measure_semantic_relatedness_under_a_uniform_model_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Harnessing different knowledge sources to measure semantic relatedness under a uniform model Ziqi Zhang Anna Lisa Gentile Fabio Ciravegna Department of Computer Science, University of Sheffield 211Portobello, Regent Court Sheffield, S1 4DP z . [sent-1, score-0.504]
</p><p>2 uk Abstract Measuring semantic relatedness between words or concepts is a crucial process to many Natural Language Processing tasks. [sent-12, score-0.386]
</p><p>3 Exiting methods exploit semantic evidence from a single knowledge source, and are predominantly evaluated only in the general domain. [sent-13, score-0.181]
</p><p>4 This paper introduces a method of harnessing different knowledge sources under a uniform model for measuring semantic relatedness between words or concepts. [sent-14, score-0.636]
</p><p>5 Using Wikipedia and WordNet as examples, and evaluated in both the general and biomedical domains, it successfully combines strengths from both knowledge sources and outperforms stateof-the-art on many datasets. [sent-15, score-0.418]
</p><p>6 1 Introduction  Semantic relatedness (SR) measures how much two (strings of) words or concepts are related by encompassing all kinds of relations between them (Strube and Ponzetto, 2006). [sent-16, score-0.361]
</p><p>7 In the biomedical domain, SR is an important technique for discovering gene functions and interactions (Wu et al. [sent-20, score-0.15]
</p><p>8 There is an abundant literature on measuring SR between words or concepts. [sent-23, score-0.027]
</p><p>9 Typically, these methods extract semantic evidence of words and concepts from a background knowledge source, 991 uk  . [sent-24, score-0.383]
</p><p>10 The knowledge sources can be unstructured documents or (semi-)structured resources such as Wikipedia, WordNet, and domain specific ontologies (e. [sent-27, score-0.593]
</p><p>11 , the Gene In this paper, we identify two issues that have  Ontology1). [sent-29, score-0.032]
</p><p>12 First, existing works typically employ a single knowledge source of semantic evidence. [sent-31, score-0.204]
</p><p>13 , 2010) has shown that the accuracy of an SR method differs depending on the choice of the knowledge sources, and there is no conclusion which knowledge source is superior to others. [sent-33, score-0.252]
</p><p>14 (2010) argue that this indicates different knowledge sources may complement each other. [sent-35, score-0.359]
</p><p>15 Second, the majority of SR methods have been evaluated in general domains only, except a few earlier WordNet-based methods that have been adapted to biomedical ontologies and evaluated in that domain (Lord et al. [sent-36, score-0.313]
</p><p>16 Given the significant attention that SR has received in specific domains (Pesquita et al. [sent-40, score-0.082]
</p><p>17 , 2007), evaluation of SR methods in specific domains is increasingly important. [sent-41, score-0.116]
</p><p>18 This paper addresses these issues by proposing a generic and uniform model for computing SR between words or concepts using multiple knowledge sources, and evaluating the proposed method in both general and specific domains. [sent-42, score-0.458]
</p><p>19 The method combines and integrates semantic evidence of words or concepts extracted from any  knowledge source in a generic graph representation, with which the SR between concepts or words is computed. [sent-43, score-0.757]
</p><p>20 Using two of the most popular general-domain knowledge sources, 1 http://www. [sent-44, score-0.099]
</p><p>21 2011 Wikipedia and WordNet as examples, the method is evaluated on 7 benchmarking datasets, including three datasets from the biomedical domain and four from the general domain. [sent-47, score-0.242]
</p><p>22 Section 2 discusses related work; Section 3 presents the proposed method; Section 4 describes the experiments and evaluation; Section 5 discusses results and findings; Section 6  concludes this paper. [sent-50, score-0.062]
</p><p>23 1 SR methods Methods for computing SR can be classified into path based, Information Content (IC) based, statistical and hybrid methods. [sent-52, score-0.037]
</p><p>24 , 1989; Wu and Palmer, 1994) measure SR between words or concepts as a function of their distance in a semantic network, usually calculated based on the path connecting the words or concepts by certain semantic (typically is-a) links. [sent-54, score-0.637]
</p><p>25 , 2004) assess relatedness between words or concepts by the amount of information they share, usually determined by a higher level concept that subsumes both concepts in a taxonomic structure. [sent-57, score-0.74]
</p><p>26 Statistical methods measure relatedness between words or concepts based on their distribution of contextual evidence. [sent-58, score-0.335]
</p><p>27 This can be formalized as cooccurrence statistics collected from unstructured documents (Chen et al. [sent-59, score-0.207]
</p><p>28 , 2006), or  distributional concept or word vectors with features extracted from either unstructured documents (Harrington, 2010; Wojtinnek and Pulman, 2011) or (semi-)structured knowledge 992 resources (Agirre et al. [sent-61, score-0.478]
</p><p>29 (2008) derive a taxonomy of terms from unstructured documents by applying hierarchical clustering based on corpus statistics, then apply path based method on this taxonomy to compute SR. [sent-69, score-0.347]
</p><p>30 Han and Zhao (2010) use one IC based method and two statistical methods to compute SR, then derive an aggregated score. [sent-70, score-0.037]
</p><p>31 2  SR knowledge sources and domains  Computing SR requires background knowledge about concepts or words, which can be extracted from unstructured corpora, semi-structured and structured knowledge resources. [sent-72, score-1.035]
</p><p>32 Unstructured  corpora are easier to create and cheaper to maintain, however, semantic relations between words or concepts are implicit. [sent-73, score-0.284]
</p><p>33 , 2006) that exploit unstructured corpora typically depend on distributional statistics, and thus may ignore important semantic evidences present in (semi-)structured knowledge sources (Pan and Farrell, 2007). [sent-76, score-0.523]
</p><p>34 , 2008; Wojtinnek and Pulman, 2011) propose to pre-process a corpus to learn a semantic network, with which SR is computed. [sent-78, score-0.051]
</p><p>35 This creates high pre-processing cost; also, the choice of corpus and its size often have a direct correlation with the accuracy of SR methods (Batet et al. [sent-79, score-0.03]
</p><p>36 (Semi-)Structured knowledge sources on the other hand, organize semantic knowledge about concepts and words explicitly and interlink them with semantic relations. [sent-81, score-0.736]
</p><p>37 They have been popular choices in the studies of SR, and they include lexical resources such as WordNet, Wiktionary, and (semi-)structured encyclopedic resources such as Wikipedia. [sent-82, score-0.061]
</p><p>38 WordNet has been used in earlier studies (Hirst and St-Onge, 1998; Jiang and Conrath, 1997; Lin, 1998; Leacock and Chodorow 1998; Resnik, 1995; Seco et al. [sent-83, score-0.061]
</p><p>39 , 2004; Wu and  Palmer, 1994) and is still a preferred knowledge source in recent works (Agirre et al. [sent-84, score-0.153]
</p><p>40 However, its effectiveness may be hindered by its lack of coverage of specialized lexicons and domain specific concepts (Strube and Ponzetto, 2006; Zhang et al. [sent-86, score-0.358]
</p><p>41 Wikipedia and Wiktionary are collaboratively maintained knowledge sources and therefore may overcome this limitation. [sent-88, score-0.357]
</p><p>42 Wikipedia in particular, is found to have reasonable coverage of many domains (Holloway et al. [sent-89, score-0.133]
</p><p>43 It has become increasingly popular in SR studies recently. [sent-91, score-0.095]
</p><p>44 However, research (Zesch and Gurevych, 2010) have shown that methods based on Wikipedia have no clear advantage over WordNet-based methods on some general domain datasets in terms of accuracy, while Zhang et al. [sent-92, score-0.097]
</p><p>45 (2010) argue that different knowledge sources may complement each other, and SR methods may benefit from harnessing different knowledge sources. [sent-93, score-0.563]
</p><p>46 , 2008) have adapted state-of-the-art to domain specific knowledge sources (e. [sent-98, score-0.345]
</p><p>47 , the Gene Ontology, the MeSH2) and evaluated them therein. [sent-100, score-0.031]
</p><p>48 Despite these efforts, a large proportion of state-of-the-art is still  only evaluated in the general domain. [sent-101, score-0.031]
</p><p>49 3  SR methods similar to this work  Few works have attempted at combining different knowledge sources in SR studies, especially (semi)structured knowledge sources. [sent-103, score-0.401]
</p><p>50 The closest studies are Han and Zhao (2010) and Tsang and Stevenson (2010). [sent-104, score-0.061]
</p><p>51 Han and Zhao firstly compute SR between words using three state-of-the-art SR methods separately. [sent-105, score-0.06]
</p><p>52 Next, one score is chosen subject to an arbitrary preference order, and used to create a connected graph of weighted edges between words. [sent-106, score-0.042]
</p><p>53 A recursive function is then applied to the graph to compute final SR scores between words. [sent-107, score-0.042]
</p><p>54 Essentially, each SR method is applied in isolation and features from different sources are used separately with each distinctive method. [sent-108, score-0.282]
</p><p>55 Tsang and Stevenson (2010) combine WordNet and unstructured documents by weighing each word found in WordNet using its frequency observed in a large corpus. [sent-110, score-0.207]
</p><p>56 1), each word or word segment is searched in each knowledge source to identify their contexts that is specific to that knowledge source. [sent-118, score-0.344]
</p><p>57 We define a context as the representation of meaning or a concept for a word. [sent-119, score-0.172]
</p><p>58 2), for each concept of an  input word, features are extracted from its context and a graph representation of each concept and their features is created. [sent-122, score-0.386]
</p><p>59 3), cross-source contexts are mapped where they refer to the same concept, thus their features from different sources can be combined to derive an enriched representation. [sent-124, score-0.278]
</p><p>60 This creates a final, uniform graph representation where input words are connected by shared features of their underlying candidate concepts. [sent-125, score-0.162]
</p><p>61 4) the graph is submitted to a generic algorithm to compute SR between words. [sent-127, score-0.087]
</p><p>62 In the following, we discuss details with respect to different types of knowledge sources, while focusing on Wikipedia and WordNet in our experiments for two reasons. [sent-128, score-0.099]
</p><p>63 First, they are used by the majority of SR methods and are therefore most representative knowledge sources. [sent-129, score-0.099]
</p><p>64 Second, they have strongly distinctive and complementary characteristics, which make ideal testbeds for the requirements. [sent-130, score-0.079]
</p><p>65 On one hand, WordNet is a lexical resource containing rich and strict semantic relations between words, but lacks coverage of specialized vocabularies. [sent-131, score-0.169]
</p><p>66 On the other hand, Wikipedia is a semi-structured resource with good coverage of domains and named entities, but the  semantic knowledge is organized in a looser way. [sent-132, score-0.348]
</p><p>67 1 Context retrieval Given a pair of words or word segments, we firstly identify contexts representing the underlying meanings or concepts from each knowledge source. [sent-134, score-0.498]
</p><p>68 For lexical resources, this could be distinctive word senses. [sent-135, score-0.079]
</p><p>69 , “cat”), and be the set of k concepts of w extracted from WordNet. [sent-140, score-0.233]
</p><p>70 Thus we search for underlying articles that describe different concepts. [sent-142, score-0.041]
</p><p>71 If a single nondisambiguation page describing a concept is returned, the concept is selected and the retrieval is complete. [sent-144, score-0.413]
</p><p>72 In the second case, a disambiguation page linking to all possible concept pages may be returned. [sent-145, score-0.241]
</p><p>73 This page lists all underlying concepts and entities referenced by w as links and a short description with each link. [sent-146, score-0.401]
</p><p>74 In this case, we always  keep the first concept page, which is found often to be the most common sense of the word; additionally, we select other concept pages whose short descriptions contain the word w. [sent-147, score-0.344]
</p><p>75 We do not select all linked pages because many of these in fact link to a concept relevant to w, but not necessarily a candidate sense of w. [sent-148, score-0.172]
</p><p>76 Thirdly, if no pages are returned for w, we search for the most relevant page using w as keyword(s) in an inverted index of all Wikipedia pages (e. [sent-149, score-0.121]
</p><p>77 We denote concepts retrieved from Wikipedia as For unstructured sources such as documents, a simple approach could be defining a word context as a text passage around each occurrence of w, and grouping similar contexts of w as representation of its underlying meanings, or concepts. [sent-152, score-0.773]
</p><p>78 (2008) and Harrington (2010) may be applied to extract a lexical network of words, whereby similar methods to WordNet can be applied. [sent-154, score-0.033]
</p><p>79 2 Feature extraction and representation Next, for each concept identified from a knowledge source, features are extracted from their 994  corresponding contexts. [sent-157, score-0.271]
</p><p>80 Figure 1 shows an example representation of a concept and its Wikipedia features:  Words from page titles and redirection links (can be considered as synonyms)  Words from categories, used as higher level hypernyms in some studies (Zesch et al. [sent-160, score-0.334]
</p><p>81 , 2010; Strube and Ponzetto, 2006)  Words from outgoing links  mammal” using different types of features extracted from Wikipedia. [sent-161, score-0.032]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sr', 0.67), ('concepts', 0.233), ('sources', 0.203), ('concept', 0.172), ('unstructured', 0.17), ('pozo', 0.168), ('wikipedia', 0.161), ('zesch', 0.135), ('wordnet', 0.126), ('harnessing', 0.105), ('ponzetto', 0.105), ('relatedness', 0.102), ('harrington', 0.101), ('knowledge', 0.099), ('dcs', 0.087), ('gurevych', 0.087), ('biomedical', 0.085), ('domains', 0.082), ('distinctive', 0.079), ('strube', 0.079), ('ic', 0.075), ('han', 0.073), ('zhang', 0.071), ('page', 0.069), ('leacock', 0.068), ('cilibrasi', 0.067), ('lord', 0.067), ('seco', 0.067), ('tsang', 0.067), ('vitanyi', 0.067), ('wojtinnek', 0.067), ('gene', 0.065), ('chodorow', 0.062), ('studies', 0.061), ('firstly', 0.06), ('retrieved', 0.059), ('sheffield', 0.058), ('datasets', 0.054), ('zhao', 0.054), ('source', 0.054), ('semantic', 0.051), ('coverage', 0.051), ('structured', 0.05), ('resnik', 0.049), ('uniform', 0.049), ('wiktionary', 0.049), ('pulman', 0.049), ('conrath', 0.049), ('matsuo', 0.046), ('generic', 0.045), ('agirre', 0.043), ('stevenson', 0.043), ('domain', 0.043), ('graph', 0.042), ('ontologies', 0.041), ('thirdly', 0.041), ('hirst', 0.041), ('underlying', 0.041), ('contexts', 0.038), ('pedersen', 0.038), ('wu', 0.038), ('documents', 0.037), ('derive', 0.037), ('path', 0.037), ('resource', 0.036), ('increasingly', 0.034), ('network', 0.033), ('taxonomy', 0.033), ('links', 0.032), ('connecting', 0.032), ('jiang', 0.032), ('issues', 0.032), ('evaluated', 0.031), ('discusses', 0.031), ('specialized', 0.031), ('complement', 0.03), ('creates', 0.03), ('pan', 0.029), ('maintained', 0.029), ('looser', 0.029), ('benchmarking', 0.029), ('circle', 0.029), ('gouws', 0.029), ('lisa', 0.029), ('markovitch', 0.029), ('passage', 0.029), ('segment', 0.028), ('measuring', 0.027), ('meanings', 0.027), ('palmer', 0.027), ('argue', 0.027), ('returned', 0.026), ('encompassing', 0.026), ('collaboratively', 0.026), ('wk', 0.026), ('searched', 0.026), ('referenced', 0.026), ('inverted', 0.026), ('court', 0.026), ('harness', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="64-tfidf-1" href="./emnlp-2011-Harnessing_different_knowledge_sources_to_measure_semantic_relatedness_under_a_uniform_model.html">64 emnlp-2011-Harnessing different knowledge sources to measure semantic relatedness under a uniform model</a></p>
<p>Author: Ziqi Zhang ; Anna Lisa Gentile ; Fabio Ciravegna</p><p>Abstract: Measuring semantic relatedness between words or concepts is a crucial process to many Natural Language Processing tasks. Exiting methods exploit semantic evidence from a single knowledge source, and are predominantly evaluated only in the general domain. This paper introduces a method of harnessing different knowledge sources under a uniform model for measuring semantic relatedness between words or concepts. Using Wikipedia and WordNet as examples, and evaluated in both the general and biomedical domains, it successfully combines strengths from both knowledge sources and outperforms stateof-the-art on many datasets.</p><p>2 0.082468823 <a title="64-tfidf-2" href="./emnlp-2011-Relation_Extraction_with_Relation_Topics.html">114 emnlp-2011-Relation Extraction with Relation Topics</a></p>
<p>Author: Chang Wang ; James Fan ; Aditya Kalyanpur ; David Gondek</p><p>Abstract: This paper describes a novel approach to the semantic relation detection problem. Instead of relying only on the training instances for a new relation, we leverage the knowledge learned from previously trained relation detectors. Specifically, we detect a new semantic relation by projecting the new relation’s training instances onto a lower dimension topic space constructed from existing relation detectors through a three step process. First, we construct a large relation repository of more than 7,000 relations from Wikipedia. Second, we construct a set of non-redundant relation topics defined at multiple scales from the relation repository to characterize the existing relations. Similar to the topics defined over words, each relation topic is an interpretable multinomial distribution over the existing relations. Third, we integrate the relation topics in a kernel function, and use it together with SVM to construct detectors for new relations. The experimental results on Wikipedia and ACE data have confirmed that backgroundknowledge-based topics generated from the Wikipedia relation repository can significantly improve the performance over the state-of-theart relation detection approaches.</p><p>3 0.081492051 <a title="64-tfidf-3" href="./emnlp-2011-Harnessing_WordNet_Senses_for_Supervised_Sentiment_Classification.html">63 emnlp-2011-Harnessing WordNet Senses for Supervised Sentiment Classification</a></p>
<p>Author: Balamurali AR ; Aditya Joshi ; Pushpak Bhattacharyya</p><p>Abstract: Traditional approaches to sentiment classification rely on lexical features, syntax-based features or a combination of the two. We propose semantic features using word senses for a supervised document-level sentiment classifier. To highlight the benefit of sense-based features, we compare word-based representation of documents with a sense-based representation where WordNet senses of the words are used as features. In addition, we highlight the benefit of senses by presenting a part-ofspeech-wise effect on sentiment classification. Finally, we show that even if a WSD engine disambiguates between a limited set of words in a document, a sentiment classifier still performs better than what it does in absence of sense annotation. Since word senses used as features show promise, we also examine the possibility of using similarity metrics defined on WordNet to address the problem of not finding a sense in the training corpus. We per- form experiments using three popular similarity metrics to mitigate the effect of unknown synsets in a test corpus by replacing them with similar synsets from the training corpus. The results show promising improvement with respect to the baseline.</p><p>4 0.074177772 <a title="64-tfidf-4" href="./emnlp-2011-Robust_Disambiguation_of_Named_Entities_in_Text.html">116 emnlp-2011-Robust Disambiguation of Named Entities in Text</a></p>
<p>Author: Johannes Hoffart ; Mohamed Amir Yosef ; Ilaria Bordino ; Hagen Furstenau ; Manfred Pinkal ; Marc Spaniol ; Bilyana Taneva ; Stefan Thater ; Gerhard Weikum</p><p>Abstract: Disambiguating named entities in naturallanguage text maps mentions of ambiguous names onto canonical entities like people or places, registered in a knowledge base such as DBpedia or YAGO. This paper presents a robust method for collective disambiguation, by harnessing context from knowledge bases and using a new form of coherence graph. It unifies prior approaches into a comprehensive framework that combines three measures: the prior probability of an entity being mentioned, the similarity between the contexts of a mention and a candidate entity, as well as the coherence among candidate entities for all mentions together. The method builds a weighted graph of mentions and candidate entities, and computes a dense subgraph that approximates the best joint mention-entity mapping. Experiments show that the new method significantly outperforms prior methods in terms of accuracy, with robust behavior across a variety of inputs.</p><p>5 0.06139591 <a title="64-tfidf-5" href="./emnlp-2011-Analyzing_Methods_for_Improving_Precision_of_Pivot_Based_Bilingual_Dictionaries.html">18 emnlp-2011-Analyzing Methods for Improving Precision of Pivot Based Bilingual Dictionaries</a></p>
<p>Author: Xabier Saralegi ; Iker Manterola ; Inaki San Vicente</p><p>Abstract: An A-C bilingual dictionary can be inferred by merging A-B and B-C dictionaries using B as pivot. However, polysemous pivot words often produce wrong translation candidates. This paper analyzes two methods for pruning wrong candidates: one based on exploiting the structure of the source dictionaries, and the other based on distributional similarity computed from comparable corpora. As both methods depend exclusively on easily available resources, they are well suited to less resourced languages. We studied whether these two techniques complement each other given that they are based on different paradigms. We also researched combining them by looking for the best adequacy depending on various application scenarios. ,</p><p>6 0.058761157 <a title="64-tfidf-6" href="./emnlp-2011-Class_Label_Enhancement_via_Related_Instances.html">26 emnlp-2011-Class Label Enhancement via Related Instances</a></p>
<p>7 0.056755017 <a title="64-tfidf-7" href="./emnlp-2011-Refining_the_Notions_of_Depth_and_Density_in_WordNet-based_Semantic_Similarity_Measures.html">112 emnlp-2011-Refining the Notions of Depth and Density in WordNet-based Semantic Similarity Measures</a></p>
<p>8 0.05171359 <a title="64-tfidf-8" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<p>9 0.051333122 <a title="64-tfidf-9" href="./emnlp-2011-Lexical_Co-occurrence%2C_Statistical_Significance%2C_and_Word_Association.html">86 emnlp-2011-Lexical Co-occurrence, Statistical Significance, and Word Association</a></p>
<p>10 0.049924374 <a title="64-tfidf-10" href="./emnlp-2011-Semantic_Topic_Models%3A_Combining_Word_Distributional_Statistics_and_Dictionary_Definitions.html">119 emnlp-2011-Semantic Topic Models: Combining Word Distributional Statistics and Dictionary Definitions</a></p>
<p>11 0.046402346 <a title="64-tfidf-11" href="./emnlp-2011-A_generative_model_for_unsupervised_discovery_of_relations_and_argument_classes_from_clinical_texts.html">14 emnlp-2011-A generative model for unsupervised discovery of relations and argument classes from clinical texts</a></p>
<p>12 0.041328397 <a title="64-tfidf-12" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>13 0.039521873 <a title="64-tfidf-13" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>14 0.037199877 <a title="64-tfidf-14" href="./emnlp-2011-Exploiting_Syntactic_and_Distributional_Information_for_Spelling_Correction_with_Web-Scale_N-gram_Models.html">55 emnlp-2011-Exploiting Syntactic and Distributional Information for Spelling Correction with Web-Scale N-gram Models</a></p>
<p>15 0.037187703 <a title="64-tfidf-15" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>16 0.037153549 <a title="64-tfidf-16" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>17 0.034507219 <a title="64-tfidf-17" href="./emnlp-2011-Random_Walk_Inference_and_Learning_in_A_Large_Scale_Knowledge_Base.html">109 emnlp-2011-Random Walk Inference and Learning in A Large Scale Knowledge Base</a></p>
<p>18 0.033814382 <a title="64-tfidf-18" href="./emnlp-2011-Structured_Lexical_Similarity_via_Convolution_Kernels_on_Dependency_Trees.html">127 emnlp-2011-Structured Lexical Similarity via Convolution Kernels on Dependency Trees</a></p>
<p>19 0.031552356 <a title="64-tfidf-19" href="./emnlp-2011-Learning_Local_Content_Shift_Detectors_from_Document-level_Information.html">82 emnlp-2011-Learning Local Content Shift Detectors from Document-level Information</a></p>
<p>20 0.031399459 <a title="64-tfidf-20" href="./emnlp-2011-Extreme_Extraction_-_Machine_Reading_in_a_Week.html">57 emnlp-2011-Extreme Extraction - Machine Reading in a Week</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.121), (1, -0.073), (2, -0.075), (3, -0.035), (4, 0.003), (5, -0.035), (6, 0.035), (7, 0.02), (8, 0.019), (9, 0.061), (10, 0.053), (11, 0.0), (12, -0.018), (13, 0.051), (14, 0.011), (15, 0.016), (16, -0.066), (17, 0.031), (18, 0.008), (19, -0.083), (20, -0.062), (21, -0.169), (22, 0.016), (23, 0.031), (24, -0.01), (25, 0.04), (26, 0.072), (27, 0.108), (28, 0.12), (29, -0.148), (30, 0.156), (31, 0.087), (32, -0.159), (33, 0.176), (34, -0.135), (35, -0.011), (36, -0.009), (37, -0.107), (38, 0.182), (39, -0.086), (40, 0.128), (41, 0.097), (42, -0.017), (43, 0.099), (44, 0.009), (45, 0.067), (46, 0.288), (47, 0.312), (48, -0.057), (49, -0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97366905 <a title="64-lsi-1" href="./emnlp-2011-Harnessing_different_knowledge_sources_to_measure_semantic_relatedness_under_a_uniform_model.html">64 emnlp-2011-Harnessing different knowledge sources to measure semantic relatedness under a uniform model</a></p>
<p>Author: Ziqi Zhang ; Anna Lisa Gentile ; Fabio Ciravegna</p><p>Abstract: Measuring semantic relatedness between words or concepts is a crucial process to many Natural Language Processing tasks. Exiting methods exploit semantic evidence from a single knowledge source, and are predominantly evaluated only in the general domain. This paper introduces a method of harnessing different knowledge sources under a uniform model for measuring semantic relatedness between words or concepts. Using Wikipedia and WordNet as examples, and evaluated in both the general and biomedical domains, it successfully combines strengths from both knowledge sources and outperforms stateof-the-art on many datasets.</p><p>2 0.50445145 <a title="64-lsi-2" href="./emnlp-2011-Refining_the_Notions_of_Depth_and_Density_in_WordNet-based_Semantic_Similarity_Measures.html">112 emnlp-2011-Refining the Notions of Depth and Density in WordNet-based Semantic Similarity Measures</a></p>
<p>Author: Tong Wang ; Graeme Hirst</p><p>Abstract: We re-investigate the rationale for and the effectiveness of adopting the notions of depth and density in WordNet-based semantic similarity measures. We show that the intuition for including these notions in WordNet-based similarity measures does not always stand up to empirical examination. In particular, the traditional definitions of depth and density as ordinal integer values in the hierarchical structure of WordNet does not always correlate with human judgment of lexical semantic similarity, which imposes strong limitations on their contribution to an accurate similarity measure. We thus propose several novel definitions of depth and density, which yield significant improvement in degree of correlation with similarity. When used in WordNet-based semantic similarity measures, the new definitions consistently improve performance on a task of correlating with human judgment.</p><p>3 0.41808039 <a title="64-lsi-3" href="./emnlp-2011-Class_Label_Enhancement_via_Related_Instances.html">26 emnlp-2011-Class Label Enhancement via Related Instances</a></p>
<p>Author: Zornitsa Kozareva ; Konstantin Voevodski ; Shanghua Teng</p><p>Abstract: Class-instance label propagation algorithms have been successfully used to fuse information from multiple sources in order to enrich a set of unlabeled instances with class labels. Yet, nobody has explored the relationships between the instances themselves to enhance an initial set of class-instance pairs. We propose two graph-theoretic methods (centrality and regularization), which start with a small set of labeled class-instance pairs and use the instance-instance network to extend the class labels to all instances in the network. We carry out a comparative study with state-of-the-art knowledge harvesting algorithm and show that our approach can learn additional class labels while maintaining high accuracy. We conduct a comparative study between class-instance and instance-instance graphs used to propagate the class labels and show that the latter one achieves higher accuracy.</p><p>4 0.41206047 <a title="64-lsi-4" href="./emnlp-2011-Analyzing_Methods_for_Improving_Precision_of_Pivot_Based_Bilingual_Dictionaries.html">18 emnlp-2011-Analyzing Methods for Improving Precision of Pivot Based Bilingual Dictionaries</a></p>
<p>Author: Xabier Saralegi ; Iker Manterola ; Inaki San Vicente</p><p>Abstract: An A-C bilingual dictionary can be inferred by merging A-B and B-C dictionaries using B as pivot. However, polysemous pivot words often produce wrong translation candidates. This paper analyzes two methods for pruning wrong candidates: one based on exploiting the structure of the source dictionaries, and the other based on distributional similarity computed from comparable corpora. As both methods depend exclusively on easily available resources, they are well suited to less resourced languages. We studied whether these two techniques complement each other given that they are based on different paradigms. We also researched combining them by looking for the best adequacy depending on various application scenarios. ,</p><p>5 0.39847121 <a title="64-lsi-5" href="./emnlp-2011-Harnessing_WordNet_Senses_for_Supervised_Sentiment_Classification.html">63 emnlp-2011-Harnessing WordNet Senses for Supervised Sentiment Classification</a></p>
<p>Author: Balamurali AR ; Aditya Joshi ; Pushpak Bhattacharyya</p><p>Abstract: Traditional approaches to sentiment classification rely on lexical features, syntax-based features or a combination of the two. We propose semantic features using word senses for a supervised document-level sentiment classifier. To highlight the benefit of sense-based features, we compare word-based representation of documents with a sense-based representation where WordNet senses of the words are used as features. In addition, we highlight the benefit of senses by presenting a part-ofspeech-wise effect on sentiment classification. Finally, we show that even if a WSD engine disambiguates between a limited set of words in a document, a sentiment classifier still performs better than what it does in absence of sense annotation. Since word senses used as features show promise, we also examine the possibility of using similarity metrics defined on WordNet to address the problem of not finding a sense in the training corpus. We per- form experiments using three popular similarity metrics to mitigate the effect of unknown synsets in a test corpus by replacing them with similar synsets from the training corpus. The results show promising improvement with respect to the baseline.</p><p>6 0.37776259 <a title="64-lsi-6" href="./emnlp-2011-Robust_Disambiguation_of_Named_Entities_in_Text.html">116 emnlp-2011-Robust Disambiguation of Named Entities in Text</a></p>
<p>7 0.3117649 <a title="64-lsi-7" href="./emnlp-2011-Learning_Local_Content_Shift_Detectors_from_Document-level_Information.html">82 emnlp-2011-Learning Local Content Shift Detectors from Document-level Information</a></p>
<p>8 0.29394829 <a title="64-lsi-8" href="./emnlp-2011-Lexical_Co-occurrence%2C_Statistical_Significance%2C_and_Word_Association.html">86 emnlp-2011-Lexical Co-occurrence, Statistical Significance, and Word Association</a></p>
<p>9 0.27450636 <a title="64-lsi-9" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>10 0.24704668 <a title="64-lsi-10" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<p>11 0.22867593 <a title="64-lsi-11" href="./emnlp-2011-A_Cascaded_Classification_Approach_to_Semantic_Head_Recognition.html">2 emnlp-2011-A Cascaded Classification Approach to Semantic Head Recognition</a></p>
<p>12 0.19907796 <a title="64-lsi-12" href="./emnlp-2011-Semantic_Topic_Models%3A_Combining_Word_Distributional_Statistics_and_Dictionary_Definitions.html">119 emnlp-2011-Semantic Topic Models: Combining Word Distributional Statistics and Dictionary Definitions</a></p>
<p>13 0.18249959 <a title="64-lsi-13" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>14 0.18010099 <a title="64-lsi-14" href="./emnlp-2011-Linear_Text_Segmentation_Using_Affinity_Propagation.html">88 emnlp-2011-Linear Text Segmentation Using Affinity Propagation</a></p>
<p>15 0.17767185 <a title="64-lsi-15" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>16 0.17666723 <a title="64-lsi-16" href="./emnlp-2011-Literal_and_Metaphorical_Sense_Identification_through_Concrete_and_Abstract_Context.html">91 emnlp-2011-Literal and Metaphorical Sense Identification through Concrete and Abstract Context</a></p>
<p>17 0.16902794 <a title="64-lsi-17" href="./emnlp-2011-Exploiting_Syntactic_and_Distributional_Information_for_Spelling_Correction_with_Web-Scale_N-gram_Models.html">55 emnlp-2011-Exploiting Syntactic and Distributional Information for Spelling Correction with Web-Scale N-gram Models</a></p>
<p>18 0.16811953 <a title="64-lsi-18" href="./emnlp-2011-Random_Walk_Inference_and_Learning_in_A_Large_Scale_Knowledge_Base.html">109 emnlp-2011-Random Walk Inference and Learning in A Large Scale Knowledge Base</a></p>
<p>19 0.16504976 <a title="64-lsi-19" href="./emnlp-2011-Relation_Extraction_with_Relation_Topics.html">114 emnlp-2011-Relation Extraction with Relation Topics</a></p>
<p>20 0.16492996 <a title="64-lsi-20" href="./emnlp-2011-Learning_the_Information_Status_of_Noun_Phrases_in_Spoken_Dialogues.html">84 emnlp-2011-Learning the Information Status of Noun Phrases in Spoken Dialogues</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(15, 0.011), (23, 0.079), (33, 0.34), (36, 0.031), (37, 0.027), (45, 0.129), (53, 0.032), (54, 0.026), (57, 0.012), (62, 0.034), (64, 0.033), (66, 0.029), (79, 0.035), (82, 0.024), (87, 0.013), (96, 0.034), (98, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74578184 <a title="64-lda-1" href="./emnlp-2011-Harnessing_different_knowledge_sources_to_measure_semantic_relatedness_under_a_uniform_model.html">64 emnlp-2011-Harnessing different knowledge sources to measure semantic relatedness under a uniform model</a></p>
<p>Author: Ziqi Zhang ; Anna Lisa Gentile ; Fabio Ciravegna</p><p>Abstract: Measuring semantic relatedness between words or concepts is a crucial process to many Natural Language Processing tasks. Exiting methods exploit semantic evidence from a single knowledge source, and are predominantly evaluated only in the general domain. This paper introduces a method of harnessing different knowledge sources under a uniform model for measuring semantic relatedness between words or concepts. Using Wikipedia and WordNet as examples, and evaluated in both the general and biomedical domains, it successfully combines strengths from both knowledge sources and outperforms stateof-the-art on many datasets.</p><p>2 0.66505843 <a title="64-lda-2" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>Author: Kristian Woodsend ; Mirella Lapata</p><p>Abstract: Text simplification aims to rewrite text into simpler versions, and thus make information accessible to a broader audience. Most previous work simplifies sentences using handcrafted rules aimed at splitting long sentences, or substitutes difficult words using a predefined dictionary. This paper presents a datadriven model based on quasi-synchronous grammar, a formalism that can naturally capture structural mismatches and complex rewrite operations. We describe how such a grammar can be induced from Wikipedia and propose an integer linear programming model for selecting the most appropriate simplification from the space of possible rewrites generated by the grammar. We show experimentally that our method creates simplifications that significantly reduce the reading difficulty ofthe input, while maintaining grammaticality and preserving its meaning.</p><p>3 0.4492411 <a title="64-lda-3" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>Author: Emily M. Bender ; Dan Flickinger ; Stephan Oepen ; Yi Zhang</p><p>Abstract: In order to obtain a fine-grained evaluation of parser accuracy over naturally occurring text, we study 100 examples each of ten reasonably frequent linguistic phenomena, randomly selected from a parsed version of the English Wikipedia. We construct a corresponding set of gold-standard target dependencies for these 1000 sentences, operationalize mappings to these targets from seven state-of-theart parsers, and evaluate the parsers against this data to measure their level of success in identifying these dependencies.</p><p>4 0.43585986 <a title="64-lda-4" href="./emnlp-2011-Linking_Entities_to_a_Knowledge_Base_with_Query_Expansion.html">90 emnlp-2011-Linking Entities to a Knowledge Base with Query Expansion</a></p>
<p>Author: Swapna Gottipati ; Jing Jiang</p><p>Abstract: In this paper we present a novel approach to entity linking based on a statistical language model-based information retrieval with query expansion. We use both local contexts and global world knowledge to expand query language models. We place a strong emphasis on named entities in the local contexts and explore a positional language model to weigh them differently based on their distances to the query. Our experiments on the TAC-KBP 2010 data show that incorporating such contextual information indeed aids in disambiguating the named entities and consistently improves the entity linking performance. Compared with the official results from KBP 2010 participants, our system shows competitive performance.</p><p>5 0.42910892 <a title="64-lda-5" href="./emnlp-2011-Approximate_Scalable_Bounded_Space_Sketch_for_Large_Data_NLP.html">19 emnlp-2011-Approximate Scalable Bounded Space Sketch for Large Data NLP</a></p>
<p>Author: Amit Goyal ; Hal Daume III</p><p>Abstract: We exploit sketch techniques, especially the Count-Min sketch, a memory, and time efficient framework which approximates the frequency of a word pair in the corpus without explicitly storing the word pair itself. These methods use hashing to deal with massive amounts of streaming text. We apply CountMin sketch to approximate word pair counts and exhibit their effectiveness on three important NLP tasks. Our experiments demonstrate that on all of the three tasks, we get performance comparable to Exact word pair counts setting and state-of-the-art system. Our method scales to 49 GB of unzipped web data using bounded space of 2 billion counters (8 GB memory).</p><p>6 0.42887896 <a title="64-lda-6" href="./emnlp-2011-Semantic_Topic_Models%3A_Combining_Word_Distributional_Statistics_and_Dictionary_Definitions.html">119 emnlp-2011-Semantic Topic Models: Combining Word Distributional Statistics and Dictionary Definitions</a></p>
<p>7 0.42419514 <a title="64-lda-7" href="./emnlp-2011-Cross-Cutting_Models_of_Lexical_Semantics.html">37 emnlp-2011-Cross-Cutting Models of Lexical Semantics</a></p>
<p>8 0.41844085 <a title="64-lda-8" href="./emnlp-2011-Cooooooooooooooollllllllllllll%21%21%21%21%21%21%21%21%21%21%21%21%21%21_Using_Word_Lengthening_to_Detect_Sentiment_in_Microblogs.html">33 emnlp-2011-Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using Word Lengthening to Detect Sentiment in Microblogs</a></p>
<p>9 0.41784599 <a title="64-lda-9" href="./emnlp-2011-Exploring_Supervised_LDA_Models_for_Assigning_Attributes_to_Adjective-Noun_Phrases.html">56 emnlp-2011-Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases</a></p>
<p>10 0.41496399 <a title="64-lda-10" href="./emnlp-2011-Lexical_Co-occurrence%2C_Statistical_Significance%2C_and_Word_Association.html">86 emnlp-2011-Lexical Co-occurrence, Statistical Significance, and Word Association</a></p>
<p>11 0.41338286 <a title="64-lda-11" href="./emnlp-2011-Bayesian_Checking_for_Topic_Models.html">21 emnlp-2011-Bayesian Checking for Topic Models</a></p>
<p>12 0.41176382 <a title="64-lda-12" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>13 0.40988109 <a title="64-lda-13" href="./emnlp-2011-Exploiting_Syntactic_and_Distributional_Information_for_Spelling_Correction_with_Web-Scale_N-gram_Models.html">55 emnlp-2011-Exploiting Syntactic and Distributional Information for Spelling Correction with Web-Scale N-gram Models</a></p>
<p>14 0.40893206 <a title="64-lda-14" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>15 0.40891945 <a title="64-lda-15" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>16 0.40541592 <a title="64-lda-16" href="./emnlp-2011-Optimizing_Semantic_Coherence_in_Topic_Models.html">101 emnlp-2011-Optimizing Semantic Coherence in Topic Models</a></p>
<p>17 0.405283 <a title="64-lda-17" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>18 0.40445265 <a title="64-lda-18" href="./emnlp-2011-Discovering_Morphological_Paradigms_from_Plain_Text_Using_a_Dirichlet_Process_Mixture_Model.html">39 emnlp-2011-Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model</a></p>
<p>19 0.40168363 <a title="64-lda-19" href="./emnlp-2011-Robust_Disambiguation_of_Named_Entities_in_Text.html">116 emnlp-2011-Robust Disambiguation of Named Entities in Text</a></p>
<p>20 0.40137643 <a title="64-lda-20" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
