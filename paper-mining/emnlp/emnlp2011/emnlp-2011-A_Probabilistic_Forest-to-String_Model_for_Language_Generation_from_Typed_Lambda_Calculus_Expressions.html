<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-10" href="#">emnlp2011-10</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</h1>
<br/><p>Source: <a title="emnlp-2011-10-pdf" href="http://aclweb.org/anthology//D/D11/D11-1149.pdf">pdf</a></p><p>Author: Wei Lu ; Hwee Tou Ng</p><p>Abstract: This paper describes a novel probabilistic approach for generating natural language sentences from their underlying semantics in the form of typed lambda calculus. The approach is built on top of a novel reduction-based weighted synchronous context free grammar formalism, which facilitates the transformation process from typed lambda calculus into natural language sentences. Sentences can then be generated based on such grammar rules with a log-linear model. To acquire such grammar rules automatically in an unsupervised manner, we also propose a novel approach with a generative model, which maps from sub-expressions of logical forms to word sequences in natural language sentences. Experiments on benchmark datasets for both English and Chinese generation tasks yield significant improvements over results obtained by two state-of-the-art machine translation models, in terms of both automatic metrics and human evaluation.</p><p>Reference: <a title="emnlp-2011-10-reference" href="../emnlp2011_reference/emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 sg  Abstract This paper describes a novel probabilistic approach for generating natural language sentences from their underlying semantics in the form of typed lambda calculus. [sent-4, score-0.48]
</p><p>2 The approach is built on top of a novel reduction-based weighted synchronous context free grammar formalism, which facilitates the transformation process from typed lambda calculus into natural language sentences. [sent-5, score-0.912]
</p><p>3 Sentences can then be generated based on such grammar rules with a log-linear model. [sent-6, score-0.318]
</p><p>4 To acquire such grammar rules automatically in an unsupervised manner, we also propose a novel approach with a generative model, which maps from sub-expressions of logical forms to word sequences in natural language sentences. [sent-7, score-0.806]
</p><p>5 Experiments on benchmark datasets for both English and Chinese generation tasks yield significant improvements over results obtained by two state-of-the-art machine translation models, in terms of both automatic metrics  and human evaluation. [sent-8, score-0.25]
</p><p>6 1 Introduction This work focuses on the task of generating natural language sentences from their underlying meaning representations in the form of formal logical expressions (typed lambda calculus). [sent-9, score-0.76]
</p><p>7 Many early approaches to generation from logical forms make use of rule-based methods (Wang, 1980; Shieber et al. [sent-10, score-0.502]
</p><p>8 Recent approaches start to employ corpusbased probabilistic methods, but many of them assume the underlying meaning representations are of 1611 specific forms such as variable-free tree-structured representations (Wong and Mooney, 2007a; Lu et al. [sent-12, score-0.428]
</p><p>9 However, unlike text to text translation, which has been extensively studied in the machine translation community, translating from logical forms into text presents additional challenges. [sent-18, score-0.414]
</p><p>10 Specifically, logical forms such as λ-expressions may have complex internal structures and variable dependencies across subexpressions. [sent-19, score-0.422]
</p><p>11 Problems arise when performing automatic acquisition of a translation lexicon, as well as performing lexical selection and surface realization during generation. [sent-20, score-0.209]
</p><p>12 To our best knowledge, this is the first probabilistic model for generating sentences from the lambda calculus encodings of their underlying formal meaning representations, that concerns both surface realization and lexical acquisition. [sent-26, score-0.661]
</p><p>13 2  Related Work  The task of language generation from logical forms has a long history. [sent-28, score-0.502]
</p><p>14 Wang (1980) presented an approach for generation from an extended predicate  logic formalism using hand-written rules. [sent-30, score-0.238]
</p><p>15 (1990) presented a semantic head-driven approach for generation from logical forms based on rules written in Prolog. [sent-32, score-0.67]
</p><p>16 Shemtov (1996) presented a system for generation of multiple paraphrases from ambiguous logical forms. [sent-33, score-0.42]
</p><p>17 Langkilde (2000) presented a probabilistic model for generation from a packed forest meaning representation, without concerning lexical acquisition. [sent-34, score-0.736]
</p><p>18 Specifically, we are not aware of any prior work that handles both automatic unsupervised lexical acquisition and surface realization for generation from logical forms in a single framework. [sent-35, score-0.63]
</p><p>19 Another line of research efforts focused on the task of language generation from other meaning representation formalisms. [sent-36, score-0.358]
</p><p>20 Wong and Mooney (2007a) as well as Chen and Mooney (2008) made use of synchronous grammars to transform a variablefree tree-structured meaning representation into sentences. [sent-37, score-0.432]
</p><p>21 (2009) presented a language generation model using the same meaning representation based on tree conditional random fields. [sent-39, score-0.491]
</p><p>22 (2010) presented a domain-independent probabilistic approach for generation from database entries. [sent-41, score-0.21]
</p><p>23 Recently there are also substantial research efforts on the task of mapping natural language to meaning 1612 representations in various formalisms the inverse task of language generation called semantic parsing. [sent-43, score-0.402]
</p><p>24 (2008), in which we presented ajoint generative process that produces a hybrid tree structure containing words, syntactic structures, and meaning representations, where the meaning representations are in a variable-free tree-structured form. [sent-48, score-0.653]
</p><p>25 One important property of the model in our prior work is that it induces a hybrid tree structure automatically in an unsupervised manner, which reveals the correspondences between natural language word sequences and semantic elements. [sent-49, score-0.268]
</p><p>26 The model in turn serves as the basis for inducing the synchronous grammar rules later. [sent-51, score-0.471]
</p><p>27 (2008), a generative model was presented to model the process that jointly generates both natural language sentences and their underlying meaning representations of a variable-free treestructured form. [sent-53, score-0.432]
</p><p>28 The model was defined over a hybrid tree, which consists of meaning representation tokens as internal nodes and natural language words as leaves. [sent-54, score-0.327]
</p><p>29 One limitation of the hybrid tree model is that it assumes a single fixed tree structure for the meaning representation. [sent-55, score-0.441]
</p><p>30 The internal nodes of a meaning representation tree involve λ-expressions which are not necessarily of variable-free form; 2. [sent-59, score-0.411]
</p><p>31 The meaning representation has a packed forest representation, rather than a single deterministic tree structure. [sent-60, score-0.722]
</p><p>32 1 Packed λ-Meaning Forest We represent a λ-expression with a packed forest of meaning representation trees (called λ-meaning forest). [sent-62, score-0.589]
</p><p>33 Multiple different meaning representation trees (called λ-meaning trees) can be extracted from the same λ-meaning forest, but they all convey equivalent semantics via reductions, as discussed next. [sent-63, score-0.334]
</p><p>34 Constructing a λ-meaning forest for a given λexpression requires decomposition of a complete λexpression into semantically complete and syntactically correct sub-expressions in a principled manner. [sent-64, score-0.238]
</p><p>35 s W αe- ethqueniv ab lueilndt the λ-meaning forest based on the expressions h, f,  and g. [sent-71, score-0.286]
</p><p>36 In practice, we develop a BUILDFOREST(e) procedure which recursively builds λ-forests by applying restricted higher-order unification rules on top of the λ-expression e. [sent-72, score-0.227]
</p><p>37 Note that this process provides a semantically equivalent packed forest representation of the original λ-expression, without altering its semantics in any way. [sent-78, score-0.589]
</p><p>38 csotnattaein(xs exponentially many tree structures which all convey the same semantics. [sent-95, score-0.233]
</p><p>39 w4  w2  τc : πc  w3  w5  Figure 1: The joint generative process of both λ-meaning tree and its corresponding natural language sentence, which results in a λ-hybrid tree. [sent-111, score-0.284]
</p><p>40 The generative process for a sentence together with its corresponding λ-meaning tree is illustrated in Figure 1, which results in a λ-hybrid tree. [sent-112, score-0.284]
</p><p>41 e1  that e1: miss r runs through the mississippi  Figure 2: One example λ-hybrid tree for the sentence “give me the states bordering states that the mississippi runs through” together  with its logical form “λx0. [sent-151, score-1.112]
</p><p>42 The probability associated with generation of the subtree that spans the sub-sentence “that the mississippi runs through” can be written as:  P? [sent-157, score-0.489]
</p><p>43 , 2008); and 3) meaning representation (MR) model parameters ρ, which model the generation process from one λ-production to its child λ-productions. [sent-167, score-0.432]
</p><p>44 Since we allow a packed λ-meaning forest representation rather than a fixed tree structure, the MR model parameters ρ in this work should be estimated with the inside-outside algorithm as well, rather than being estimated directly from the training data by simple counting, as was done in Lu et al. [sent-169, score-0.596]
</p><p>45 1 The Grammar We use a weighted synchronous context free grammar (SCFG) (Aho and Ullman, 1969), which was previously used in Chiang (2007) for hierarchical phrase-based machine translation. [sent-175, score-0.427]
</p><p>46 The grammar is defined as follows: τ → hpλ , hw , ∼i (1) where τ is the type associated with the λ-production pλ4, and hw is a sequence consisting of natural language words intermixed with types. [sent-176, score-0.582]
</p><p>47 We allow a maximum of two nonterminal symbols in each synchronous rule, as was also assumed in Chiang (2007), which makes the grammar a binary SCFG. [sent-180, score-0.342]
</p><p>48 A derivation with the above two synchronous  rules results in the following λ-expression with its natural language counterpart:  paired  4Since type is already indicated by τ, we avoid redundancy by omitting it when writing pλ, without loss of information. [sent-185, score-0.331]
</p><p>49 loc(miss r,x) ,that the mississippi runs throughE where the source side λ-expression is constructed from the application λy. [sent-196, score-0.243]
</p><p>50 loc(miss r, x), the above rule in fact gives one candidate translation “that the mississippi runs through”. [sent-201, score-0.324]
</p><p>51 3  Grammar Induction  Automatic induction of the grammar rules as described above from training data (which consists of pairs of λ-expressions and natural language sentences) is a challenging task. [sent-222, score-0.36]
</p><p>52 However, unlike texts, logical forms have complex internal structures and variable dependencies across sub-expressions. [sent-229, score-0.422]
</p><p>53 It is not obvious how to establish alignments between logical terms and texts with such alignment models. [sent-230, score-0.311]
</p><p>54 Fortunately, the generative model for λ-hybrid tree introduced in Section 3 explicitly models the mappings from λ-sub-expressions to (possibly discontiguous) word sequences with a joint generative process. [sent-231, score-0.359]
</p><p>55 This motivates us to extract grammar rules from the λ-hybrid trees. [sent-232, score-0.318]
</p><p>56 Thus, we first find the Viterbi λ-hybrid trees for all training instances,  subtree rooted by  construction  e1  : miss r gets “abstracted” by its type e. [sent-233, score-0.245]
</p><p>57 based on the learned parameters of the generative λhybrid tree model. [sent-235, score-0.246]
</p><p>58 Next, we extract grammar rules on top of these  λ-hybrid trees. [sent-236, score-0.318]
</p><p>59 Specifically, we extract the following three types of synchronous grammar rules, with examples given in Figure 3: 1. [sent-237, score-0.342]
</p><p>60 Subtree rules: These rules are constructed from a complete subtree of the λ-hybrid tree. [sent-240, score-0.206]
</p><p>61 Two-level λ-hybrid sequence rules: These rules are constructed from a tree fragment with one of its grandchild subtrees (the subtree rooted by one of its grandchild nodes) being abstracted with its type only. [sent-243, score-0.586]
</p><p>62 Figure 4 gives an example based on a tree fragment of the λ-hybrid tree in Figure 2. [sent-245, score-0.266]
</p><p>63 In fact, if the semantics conveyed by the grandchild subtree serves as its argument, we will obtain the exact  complete semantics of the current subtree. [sent-248, score-0.332]
</p><p>64 1616 The overall algorithm for learning the grammar rules is sketched in Figure 5. [sent-251, score-0.318]
</p><p>65 Thus, in this work, we make use of a bottom-up dynamic programming chart-parsing algorithm that works directly on translating forest nodes into target natural language words. [sent-260, score-0.241]
</p><p>66 The algorithm is similar to that of Langkilde (2000) for generation from an underlying packed semantic forest. [sent-261, score-0.453]
</p><p>67 In order to accommodate type 2 and type 3 rules as discussed in Section 4. [sent-263, score-0.227]
</p><p>68 2) h ← FINDHYBRIDTREE(f, s, θ¯) It finds the most probable λ-hybrid tree h contain-  •  ing the given f-s pair, under the generative model parameters θ¯. [sent-271, score-0.246]
</p><p>69 3) Γh ← EXTRACTRULES(h) It takes in a λ-hybrid tree h, and extracts a set of grammar rules Γh out of it. [sent-274, score-0.451]
</p><p>70 a • For each (fi , si) ∈ (f, s), find the most probabFoler λ-hybrid tree) h∈i, ( afn,sd) t,h fiennd de xthtreac mto otshte grammar rules from it: hi = FINDHYBRIDTREE(fi, si, ¯θ∗) Γ = Γ ∪ EXTRACTRULES(hi) 3. [sent-282, score-0.318]
</p><p>71 Figure 5: The algorithm for learning the grammar rules  5  Experiments  For experiments,  we evaluated on the GEOQUERY  dataset, which consists of 880 queries on U. [sent-284, score-0.318]
</p><p>72 We further split the learning set into two portions, where 500 instances are used for training the models, which includes induction of grammar rules, training a language model, and computing feature values, and the remaining 100 instances are used for tuning the feature weights. [sent-295, score-0.231]
</p><p>73 As we have mentioned earlier, we are not aware of any previous work that performs generation from  formal logical forms that concerns both lexical acquisition and surface realization. [sent-296, score-0.548]
</p><p>74 It is not obvious how to adopt their algorithm in our context where content selection is not required but the more complex logical semantic representation is used as input. [sent-299, score-0.353]
</p><p>75 Alternatively, analogous to the work of Wong and Mooney (2007a), we could first parse the λexpressions into binary tree structures with a deterministic procedure, and then linearize the tree structure as a sequence. [sent-331, score-0.385]
</p><p>76 As for our system, during the grammar learning phase, we initialize the generative model parameters with output from the IBM alignment model 1 (Brown et al. [sent-333, score-0.362]
</p><p>77 , 1993)8, and run the λ-hybrid tree generative model with the unigram emission assumption for 10 iterations, followed by another 10 iterations with the bigram assumption. [sent-334, score-0.246]
</p><p>78 Grammar rules are then extracted based on the λ-hybrid trees obtained from such learned generative model parameters. [sent-335, score-0.242]
</p><p>79 As we can observe, the way that a meaning representation tree is linearized has a significant impact on the translation performance. [sent-338, score-0.403]
</p><p>80 8We assume word unigrams are generated from free variables, quantifiers, and logical connectives in IBM model 1. [sent-341, score-0.336]
</p><p>81 1618 Our system, on the other hand, employs a packed forest representation for λ-expressions. [sent-342, score-0.463]
</p><p>82 Therefore, it eliminates the ordering constraint by encompassing exponentially many possible tree structures during both the alignment and decoding stage. [sent-343, score-0.281]
</p><p>83 Second, in order to assess the importance of the two types of novel rules subtree rules (type 2) and two-level λ-hybrid sequence rules (type 3), we also conducted experiments without these rules for generation. [sent-400, score-0.635]
</p><p>84 4  Experiments on Variable-free Meaning Representations Finally, we also assess the effectiveness of our model on an alternative meaning representation formalism in the form of variable-free tree structures. [sent-405, score-0.391]
</p><p>85 Soweeever, since such forms present monotonous struc-  tures, and thus give less alternative options in the higher-order unification-based decomposition process, it prevents the algorithm from creating many disjunctive nodes in the packed forest. [sent-420, score-0.367]
</p><p>86 It is thus hypothesized that the advantages of the packed forest representation could not be fully exploited with such a meaning representation formalism. [sent-421, score-0.652]
</p><p>87 Next, we performed grammar induction with the complete training data of that fold, and used the learned feature weights for decoding of the test instances. [sent-425, score-0.276]
</p><p>88 Our algorithm works well by introducing additional new types of synchronous rules that are able to capture longer range dependencies. [sent-436, score-0.282]
</p><p>89 WASP−1++, on the other hand, also makes use of a synchronous parsing-based statistical machine translation approach. [sent-437, score-0.234]
</p><p>90 In contrast, our model directly performs alignment and translation from a packed forest representation to a sentence. [sent-439, score-0.604]
</p><p>91 r47ytI8aS056Ti89o7ns Conclusions and Future Work  In this work, we presented a novel algorithm for generating natural language sentences from their under1620 lying semantics in the form of typed lambda calculus. [sent-447, score-0.396]
</p><p>92 We tackled the problem by introducing a novel reduction-based weighted synchronous context-free grammar formalism, which allows sentence generation with a log-linear model. [sent-448, score-0.59]
</p><p>93 In addition, we proposed a novel generative model that jointly gener-  ates lambda calculus expressions and natural language sentences. [sent-449, score-0.566]
</p><p>94 Furthermore, we have demonstrated that the model can also effectively handle inputs with a variablefree version of meaning representation. [sent-452, score-0.213]
</p><p>95 We believe the algorithm used for inducing the reduction-based synchronous grammar rules may find applications in other research problems, such as statistical machine translation and phrasal synchronous grammar induction. [sent-453, score-0.894]
</p><p>96 Inducing probabilistic CCG grammars from logical form with higher-order unification. [sent-620, score-0.331]
</p><p>97 A generative model for parsing natural language to meaning representations. [sent-673, score-0.239]
</p><p>98 Learning synchronous grammars for semantic parsing with lambda calculus. [sent-782, score-0.415]
</p><p>99 Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. [sent-795, score-0.292]
</p><p>100 Online learning of relaxed CCG grammars for parsing to logical form. [sent-802, score-0.29]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('logical', 0.251), ('lu', 0.216), ('packed', 0.202), ('forest', 0.198), ('grammar', 0.189), ('lambda', 0.184), ('wong', 0.178), ('mississippi', 0.178), ('generation', 0.169), ('synchronous', 0.153), ('mooney', 0.151), ('kwiatkowski', 0.147), ('calculus', 0.139), ('hw', 0.134), ('tree', 0.133), ('rules', 0.129), ('zettlemoyer', 0.128), ('buildforest', 0.127), ('meaning', 0.126), ('miss', 0.119), ('chiang', 0.115), ('generative', 0.113), ('angeli', 0.11), ('unification', 0.098), ('expressions', 0.088), ('semantics', 0.088), ('forms', 0.082), ('realization', 0.082), ('typed', 0.082), ('translation', 0.081), ('grandchild', 0.079), ('subtree', 0.077), ('inorder', 0.076), ('intermixed', 0.076), ('linearize', 0.076), ('wasp', 0.076), ('formalism', 0.069), ('representations', 0.068), ('shieber', 0.065), ('runs', 0.065), ('representation', 0.063), ('chinese', 0.062), ('alignment', 0.06), ('loc', 0.06), ('convey', 0.057), ('evaluators', 0.055), ('preorder', 0.055), ('moses', 0.052), ('ter', 0.052), ('joshua', 0.052), ('bordering', 0.051), ('extractrules', 0.051), ('findhybridtree', 0.051), ('functor', 0.051), ('huet', 0.051), ('throughe', 0.051), ('variablefree', 0.051), ('type', 0.049), ('refers', 0.049), ('hybrid', 0.049), ('expression', 0.048), ('koehn', 0.048), ('free', 0.048), ('correspondences', 0.047), ('internal', 0.046), ('surface', 0.046), ('ibm', 0.045), ('decoding', 0.045), ('langkilde', 0.044), ('robocup', 0.044), ('treestructured', 0.044), ('underlying', 0.043), ('nodes', 0.043), ('structures', 0.043), ('induction', 0.042), ('novel', 0.042), ('probabilistic', 0.041), ('fold', 0.041), ('decomposition', 0.04), ('reductions', 0.04), ('kuhlmann', 0.04), ('abstracted', 0.04), ('geoquery', 0.04), ('kate', 0.04), ('bleu', 0.039), ('grammars', 0.039), ('semantic', 0.039), ('pages', 0.038), ('fluency', 0.038), ('process', 0.038), ('connectives', 0.037), ('mda', 0.037), ('plm', 0.037), ('quantifiers', 0.037), ('toolkit', 0.037), ('weighted', 0.037), ('child', 0.036), ('inputs', 0.036), ('galley', 0.036), ('states', 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="10-tfidf-1" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>Author: Wei Lu ; Hwee Tou Ng</p><p>Abstract: This paper describes a novel probabilistic approach for generating natural language sentences from their underlying semantics in the form of typed lambda calculus. The approach is built on top of a novel reduction-based weighted synchronous context free grammar formalism, which facilitates the transformation process from typed lambda calculus into natural language sentences. Sentences can then be generated based on such grammar rules with a log-linear model. To acquire such grammar rules automatically in an unsupervised manner, we also propose a novel approach with a generative model, which maps from sub-expressions of logical forms to word sequences in natural language sentences. Experiments on benchmark datasets for both English and Chinese generation tasks yield significant improvements over results obtained by two state-of-the-art machine translation models, in terms of both automatic metrics and human evaluation.</p><p>2 0.22319451 <a title="10-tfidf-2" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>Author: Xinyan Xiao ; Yang Liu ; Qun Liu ; Shouxun Lin</p><p>Abstract: Although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features, it is hard to scale up to large data due to decoding complexity. We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets.</p><p>3 0.19335645 <a title="10-tfidf-3" href="./emnlp-2011-Lexical_Generalization_in_CCG_Grammar_Induction_for_Semantic_Parsing.html">87 emnlp-2011-Lexical Generalization in CCG Grammar Induction for Semantic Parsing</a></p>
<p>Author: Tom Kwiatkowski ; Luke Zettlemoyer ; Sharon Goldwater ; Mark Steedman</p><p>Abstract: We consider the problem of learning factored probabilistic CCG grammars for semantic parsing from data containing sentences paired with logical-form meaning representations. Traditional CCG lexicons list lexical items that pair words and phrases with syntactic and semantic content. Such lexicons can be inefficient when words appear repeatedly with closely related lexical content. In this paper, we introduce factored lexicons, which include both lexemes to model word meaning and templates to model systematic variation in word usage. We also present an algorithm for learning factored CCG lexicons, along with a probabilistic parse-selection model. Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance greatly from the lexical factoring. benefits</p><p>4 0.15716743 <a title="10-tfidf-4" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>Author: Juri Ganitkevitch ; Chris Callison-Burch ; Courtney Napoles ; Benjamin Van Durme</p><p>Abstract: Previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora. However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems.</p><p>5 0.15702847 <a title="10-tfidf-5" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>Author: Benjamin Borschinger ; Bevan K. Jones ; Mark Johnson</p><p>Abstract: It is often assumed that ‘grounded’ learning tasks are beyond the scope of grammatical inference techniques. In this paper, we show that the grounded task of learning a semantic parser from ambiguous training data as discussed in Kim and Mooney (2010) can be reduced to a Probabilistic Context-Free Grammar learning task in a way that gives state of the art results. We further show that additionally letting our model learn the language’s canonical word order improves its performance and leads to the highest semantic parsing f-scores previously reported in the literature.1</p><p>6 0.14789766 <a title="10-tfidf-6" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>7 0.14754148 <a title="10-tfidf-7" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>8 0.14731102 <a title="10-tfidf-8" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>9 0.14432353 <a title="10-tfidf-9" href="./emnlp-2011-Tuning_as_Ranking.html">138 emnlp-2011-Tuning as Ranking</a></p>
<p>10 0.13663188 <a title="10-tfidf-10" href="./emnlp-2011-Bootstrapping_Semantic_Parsers_from_Conversations.html">24 emnlp-2011-Bootstrapping Semantic Parsers from Conversations</a></p>
<p>11 0.125659 <a title="10-tfidf-11" href="./emnlp-2011-Better_Evaluation_Metrics_Lead_to_Better_Machine_Translation.html">22 emnlp-2011-Better Evaluation Metrics Lead to Better Machine Translation</a></p>
<p>12 0.10095681 <a title="10-tfidf-12" href="./emnlp-2011-Third-order_Variational_Reranking_on_Packed-Shared_Dependency_Forests.html">134 emnlp-2011-Third-order Variational Reranking on Packed-Shared Dependency Forests</a></p>
<p>13 0.10041107 <a title="10-tfidf-13" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>14 0.097442254 <a title="10-tfidf-14" href="./emnlp-2011-Statistical_Machine_Translation_with_Local_Language_Models.html">125 emnlp-2011-Statistical Machine Translation with Local Language Models</a></p>
<p>15 0.097338334 <a title="10-tfidf-15" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>16 0.092476398 <a title="10-tfidf-16" href="./emnlp-2011-A_Correction_Model_for_Word_Alignments.html">3 emnlp-2011-A Correction Model for Word Alignments</a></p>
<p>17 0.091862015 <a title="10-tfidf-17" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>18 0.09079697 <a title="10-tfidf-18" href="./emnlp-2011-Data-Driven_Response_Generation_in_Social_Media.html">38 emnlp-2011-Data-Driven Response Generation in Social Media</a></p>
<p>19 0.085362844 <a title="10-tfidf-19" href="./emnlp-2011-Minimum_Imputed-Risk%3A_Unsupervised_Discriminative_Training_for_Machine_Translation.html">93 emnlp-2011-Minimum Imputed-Risk: Unsupervised Discriminative Training for Machine Translation</a></p>
<p>20 0.084920086 <a title="10-tfidf-20" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.333), (1, 0.147), (2, 0.026), (3, -0.112), (4, 0.038), (5, -0.12), (6, -0.157), (7, 0.02), (8, 0.19), (9, -0.033), (10, -0.123), (11, 0.192), (12, -0.142), (13, -0.188), (14, -0.003), (15, -0.026), (16, 0.026), (17, -0.014), (18, -0.039), (19, 0.062), (20, -0.026), (21, 0.055), (22, -0.117), (23, 0.095), (24, 0.045), (25, -0.104), (26, 0.083), (27, 0.09), (28, 0.148), (29, -0.084), (30, -0.056), (31, -0.051), (32, 0.119), (33, -0.092), (34, -0.055), (35, -0.037), (36, 0.008), (37, -0.001), (38, -0.045), (39, 0.105), (40, 0.124), (41, -0.058), (42, 0.013), (43, 0.052), (44, 0.013), (45, -0.01), (46, 0.027), (47, 0.013), (48, -0.015), (49, -0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95043671 <a title="10-lsi-1" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>Author: Wei Lu ; Hwee Tou Ng</p><p>Abstract: This paper describes a novel probabilistic approach for generating natural language sentences from their underlying semantics in the form of typed lambda calculus. The approach is built on top of a novel reduction-based weighted synchronous context free grammar formalism, which facilitates the transformation process from typed lambda calculus into natural language sentences. Sentences can then be generated based on such grammar rules with a log-linear model. To acquire such grammar rules automatically in an unsupervised manner, we also propose a novel approach with a generative model, which maps from sub-expressions of logical forms to word sequences in natural language sentences. Experiments on benchmark datasets for both English and Chinese generation tasks yield significant improvements over results obtained by two state-of-the-art machine translation models, in terms of both automatic metrics and human evaluation.</p><p>2 0.67081946 <a title="10-lsi-2" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>Author: Xinyan Xiao ; Yang Liu ; Qun Liu ; Shouxun Lin</p><p>Abstract: Although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features, it is hard to scale up to large data due to decoding complexity. We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets.</p><p>3 0.65988982 <a title="10-lsi-3" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>Author: Benjamin Borschinger ; Bevan K. Jones ; Mark Johnson</p><p>Abstract: It is often assumed that ‘grounded’ learning tasks are beyond the scope of grammatical inference techniques. In this paper, we show that the grounded task of learning a semantic parser from ambiguous training data as discussed in Kim and Mooney (2010) can be reduced to a Probabilistic Context-Free Grammar learning task in a way that gives state of the art results. We further show that additionally letting our model learn the language’s canonical word order improves its performance and leads to the highest semantic parsing f-scores previously reported in the literature.1</p><p>4 0.59350908 <a title="10-lsi-4" href="./emnlp-2011-Computing_Logical_Form_on_Regulatory_Texts.html">32 emnlp-2011-Computing Logical Form on Regulatory Texts</a></p>
<p>Author: Nikhil Dinesh ; Aravind Joshi ; Insup Lee</p><p>Abstract: The computation of logical form has been proposed as an intermediate step in the translation of sentences to logic. Logical form encodes the resolution of scope ambiguities. In this paper, we describe experiments on a modestsized corpus of regulation annotated with a novel variant of logical form, called abstract syntax trees (ASTs). The main step in computing ASTs is to order scope-taking operators. A learning model for ranking is adapted for this ordering. We design features by studying the problem ofcomparing the scope ofone operator to another. The scope comparisons are used to compute ASTs, with an F-score of 90.6% on the set of ordering decisons.</p><p>5 0.57559544 <a title="10-lsi-5" href="./emnlp-2011-Lexical_Generalization_in_CCG_Grammar_Induction_for_Semantic_Parsing.html">87 emnlp-2011-Lexical Generalization in CCG Grammar Induction for Semantic Parsing</a></p>
<p>Author: Tom Kwiatkowski ; Luke Zettlemoyer ; Sharon Goldwater ; Mark Steedman</p><p>Abstract: We consider the problem of learning factored probabilistic CCG grammars for semantic parsing from data containing sentences paired with logical-form meaning representations. Traditional CCG lexicons list lexical items that pair words and phrases with syntactic and semantic content. Such lexicons can be inefficient when words appear repeatedly with closely related lexical content. In this paper, we introduce factored lexicons, which include both lexemes to model word meaning and templates to model systematic variation in word usage. We also present an algorithm for learning factored CCG lexicons, along with a probabilistic parse-selection model. Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance greatly from the lexical factoring. benefits</p><p>6 0.54961079 <a title="10-lsi-6" href="./emnlp-2011-Bootstrapping_Semantic_Parsers_from_Conversations.html">24 emnlp-2011-Bootstrapping Semantic Parsers from Conversations</a></p>
<p>7 0.5297904 <a title="10-lsi-7" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>8 0.46509996 <a title="10-lsi-8" href="./emnlp-2011-Tuning_as_Ranking.html">138 emnlp-2011-Tuning as Ranking</a></p>
<p>9 0.45443249 <a title="10-lsi-9" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>10 0.44659507 <a title="10-lsi-10" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>11 0.44625601 <a title="10-lsi-11" href="./emnlp-2011-Efficient_retrieval_of_tree_translation_examples_for_Syntax-Based_Machine_Translation.html">47 emnlp-2011-Efficient retrieval of tree translation examples for Syntax-Based Machine Translation</a></p>
<p>12 0.44518068 <a title="10-lsi-12" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>13 0.43653154 <a title="10-lsi-13" href="./emnlp-2011-Accurate_Parsing_with_Compact_Tree-Substitution_Grammars%3A_Double-DOP.html">16 emnlp-2011-Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP</a></p>
<p>14 0.41029552 <a title="10-lsi-14" href="./emnlp-2011-Data-Driven_Response_Generation_in_Social_Media.html">38 emnlp-2011-Data-Driven Response Generation in Social Media</a></p>
<p>15 0.39905015 <a title="10-lsi-15" href="./emnlp-2011-Heuristic_Search_for_Non-Bottom-Up_Tree_Structure_Prediction.html">65 emnlp-2011-Heuristic Search for Non-Bottom-Up Tree Structure Prediction</a></p>
<p>16 0.39638293 <a title="10-lsi-16" href="./emnlp-2011-Optimal_Search_for_Minimum_Error_Rate_Training.html">100 emnlp-2011-Optimal Search for Minimum Error Rate Training</a></p>
<p>17 0.388616 <a title="10-lsi-17" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>18 0.38551 <a title="10-lsi-18" href="./emnlp-2011-Better_Evaluation_Metrics_Lead_to_Better_Machine_Translation.html">22 emnlp-2011-Better Evaluation Metrics Lead to Better Machine Translation</a></p>
<p>19 0.37719181 <a title="10-lsi-19" href="./emnlp-2011-Third-order_Variational_Reranking_on_Packed-Shared_Dependency_Forests.html">134 emnlp-2011-Third-order Variational Reranking on Packed-Shared Dependency Forests</a></p>
<p>20 0.37064737 <a title="10-lsi-20" href="./emnlp-2011-Unsupervised_Learning_of_Selectional_Restrictions_and_Detection_of_Argument_Coercions.html">144 emnlp-2011-Unsupervised Learning of Selectional Restrictions and Detection of Argument Coercions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.11), (36, 0.022), (37, 0.021), (45, 0.047), (53, 0.026), (54, 0.434), (57, 0.013), (62, 0.025), (64, 0.016), (65, 0.013), (66, 0.02), (69, 0.019), (79, 0.058), (82, 0.018), (87, 0.013), (90, 0.021), (96, 0.024), (98, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94178253 <a title="10-lda-1" href="./emnlp-2011-Ranking_Human_and_Machine_Summarization_Systems.html">110 emnlp-2011-Ranking Human and Machine Summarization Systems</a></p>
<p>Author: Peter Rankel ; John Conroy ; Eric Slud ; Dianne O'Leary</p><p>Abstract: The Text Analysis Conference (TAC) ranks summarization systems by their average score over a collection of document sets. We investigate the statistical appropriateness of this score and propose an alternative that better distinguishes between human and machine evaluation systems.</p><p>same-paper 2 0.89763582 <a title="10-lda-2" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>Author: Wei Lu ; Hwee Tou Ng</p><p>Abstract: This paper describes a novel probabilistic approach for generating natural language sentences from their underlying semantics in the form of typed lambda calculus. The approach is built on top of a novel reduction-based weighted synchronous context free grammar formalism, which facilitates the transformation process from typed lambda calculus into natural language sentences. Sentences can then be generated based on such grammar rules with a log-linear model. To acquire such grammar rules automatically in an unsupervised manner, we also propose a novel approach with a generative model, which maps from sub-expressions of logical forms to word sequences in natural language sentences. Experiments on benchmark datasets for both English and Chinese generation tasks yield significant improvements over results obtained by two state-of-the-art machine translation models, in terms of both automatic metrics and human evaluation.</p><p>3 0.85988814 <a title="10-lda-3" href="./emnlp-2011-Hierarchical_Verb_Clustering_Using_Graph_Factorization.html">67 emnlp-2011-Hierarchical Verb Clustering Using Graph Factorization</a></p>
<p>Author: Lin Sun ; Anna Korhonen</p><p>Abstract: Most previous research on verb clustering has focussed on acquiring flat classifications from corpus data, although many manually built classifications are taxonomic in nature. Also Natural Language Processing (NLP) applications benefit from taxonomic classifications because they vary in terms of the granularity they require from a classification. We introduce a new clustering method called Hierarchical Graph Factorization Clustering (HGFC) and extend it so that it is optimal for the task. Our results show that HGFC outperforms the frequently used agglomerative clustering on a hierarchical test set extracted from VerbNet, and that it yields state-of-the-art performance also on a flat test set. We demonstrate how the method can be used to acquire novel classifications as well as to extend existing ones on the basis of some prior knowledge about the classification.</p><p>4 0.51223963 <a title="10-lda-4" href="./emnlp-2011-Better_Evaluation_Metrics_Lead_to_Better_Machine_Translation.html">22 emnlp-2011-Better Evaluation Metrics Lead to Better Machine Translation</a></p>
<p>Author: Chang Liu ; Daniel Dahlmeier ; Hwee Tou Ng</p><p>Abstract: Many machine translation evaluation metrics have been proposed after the seminal BLEU metric, and many among them have been found to consistently outperform BLEU, demonstrated by their better correlations with human judgment. It has long been the hope that by tuning machine translation systems against these new generation metrics, advances in automatic machine translation evaluation can lead directly to advances in automatic machine translation. However, to date there has been no unambiguous report that these new metrics can improve a state-of-theart machine translation system over its BLEUtuned baseline. In this paper, we demonstrate that tuning Joshua, a hierarchical phrase-based statistical machine translation system, with the TESLA metrics results in significantly better humanjudged translation quality than the BLEUtuned baseline. TESLA-M in particular is simple and performs well in practice on large datasets. We release all our implementation under an open source license. It is our hope that this work will encourage the machine translation community to finally move away from BLEU as the unquestioned default and to consider the new generation metrics when tuning their systems.</p><p>5 0.5115397 <a title="10-lda-5" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>Author: Yang Gao ; Philipp Koehn ; Alexandra Birch</p><p>Abstract: Long-distance reordering remains one of the biggest challenges facing machine translation. We derive soft constraints from the source dependency parsing to directly address the reordering problem for the hierarchical phrasebased model. Our approach significantly improves Chinese–English machine translation on a large-scale task by 0.84 BLEU points on average. Moreover, when we switch the tuning function from BLEU to the LRscore which promotes reordering, we observe total improvements of 1.21 BLEU, 1.30 LRscore and 3.36 TER over the baseline. On average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially effective for long-distance reodering.</p><p>6 0.51035249 <a title="10-lda-6" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>7 0.50247455 <a title="10-lda-7" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>8 0.49746546 <a title="10-lda-8" href="./emnlp-2011-Lexical_Generalization_in_CCG_Grammar_Induction_for_Semantic_Parsing.html">87 emnlp-2011-Lexical Generalization in CCG Grammar Induction for Semantic Parsing</a></p>
<p>9 0.48246551 <a title="10-lda-9" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>10 0.47583306 <a title="10-lda-10" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>11 0.47386503 <a title="10-lda-11" href="./emnlp-2011-Efficient_retrieval_of_tree_translation_examples_for_Syntax-Based_Machine_Translation.html">47 emnlp-2011-Efficient retrieval of tree translation examples for Syntax-Based Machine Translation</a></p>
<p>12 0.47125727 <a title="10-lda-12" href="./emnlp-2011-Third-order_Variational_Reranking_on_Packed-Shared_Dependency_Forests.html">134 emnlp-2011-Third-order Variational Reranking on Packed-Shared Dependency Forests</a></p>
<p>13 0.47125381 <a title="10-lda-13" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>14 0.46756876 <a title="10-lda-14" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>15 0.4621532 <a title="10-lda-15" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>16 0.45834738 <a title="10-lda-16" href="./emnlp-2011-Data-Driven_Response_Generation_in_Social_Media.html">38 emnlp-2011-Data-Driven Response Generation in Social Media</a></p>
<p>17 0.45197704 <a title="10-lda-17" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>18 0.44596651 <a title="10-lda-18" href="./emnlp-2011-Hierarchical_Phrase-based_Translation_Representations.html">66 emnlp-2011-Hierarchical Phrase-based Translation Representations</a></p>
<p>19 0.44504693 <a title="10-lda-19" href="./emnlp-2011-Multiword_Expression_Identification_with_Tree_Substitution_Grammars%3A_A_Parsing_tour_de_force_with_French.html">97 emnlp-2011-Multiword Expression Identification with Tree Substitution Grammars: A Parsing tour de force with French</a></p>
<p>20 0.44014201 <a title="10-lda-20" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
