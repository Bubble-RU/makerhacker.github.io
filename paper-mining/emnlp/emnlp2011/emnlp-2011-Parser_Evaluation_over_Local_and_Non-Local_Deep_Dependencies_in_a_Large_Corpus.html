<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-103" href="#">emnlp2011-103</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</h1>
<br/><p>Source: <a title="emnlp-2011-103-pdf" href="http://aclweb.org/anthology//D/D11/D11-1037.pdf">pdf</a></p><p>Author: Emily M. Bender ; Dan Flickinger ; Stephan Oepen ; Yi Zhang</p><p>Abstract: In order to obtain a fine-grained evaluation of parser accuracy over naturally occurring text, we study 100 examples each of ten reasonably frequent linguistic phenomena, randomly selected from a parsed version of the English Wikipedia. We construct a corresponding set of gold-standard target dependencies for these 1000 sentences, operationalize mappings to these targets from seven state-of-theart parsers, and evaluate the parsers against this data to measure their level of success in identifying these dependencies.</p><p>Reference: <a title="emnlp-2011-103-reference" href="../emnlp2011_reference/emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  ,  Abstract In order to obtain a fine-grained evaluation of parser accuracy over naturally occurring text, we study 100 examples each of ten reasonably frequent linguistic phenomena, randomly selected from a parsed version of the English Wikipedia. [sent-4, score-0.21]
</p><p>2 We construct a corresponding set of gold-standard target dependencies for these 1000 sentences, operationalize mappings to these targets from seven state-of-theart parsers, and evaluate the parsers against this data to measure their level of success in identifying these dependencies. [sent-5, score-0.427]
</p><p>3 We suggest a selection of ten linguistic phenomena that we believe (a) occur with reasonably high frequency in running text and (b) have the  potential to shed some light on the depths of linguistic analysis. [sent-20, score-0.445]
</p><p>4 We quantify the frequency of these constructions in the English Wikipedia, then annotate 100 example sentences for each phenomenon with gold-standard dependencies reflecting core properties of the phenomena of interest. [sent-21, score-0.676]
</p><p>5 This gold standard is then used to estimate the recall of these dependencies by seven commonly used parsers, providing the basis for a qualitative discussion of the state of the art in parsing for English. [sent-22, score-0.337]
</p><p>6 2  Background  All parsing systems embody knowledge about possible and probable pairings of strings and corresponding linguistic structure. [sent-25, score-0.166]
</p><p>7 1 Approaches to parsing Source of linguistic knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al. [sent-34, score-0.189]
</p><p>8 Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e. [sent-38, score-0.172]
</p><p>9 (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowledge (e. [sent-45, score-0.186]
</p><p>10 There tends to be a correlation between the methodology used in the acquisition of linguistic  knowledge and the complexity of representations: in the creation of a mostly hand-crafted treebank like the PTB, representations have to be simple enough for human annotators to reliably manipulate. [sent-60, score-0.245]
</p><p>11 Deriving more complex representations typically presupposes further computational support, often involving some hand-crafted linguistic knowledge—which can take the form of mappings from PTB-like representations to “richer” grammatical frameworks (as in the line of work by O’Donovan et al. [sent-61, score-0.251]
</p><p>12 2 In principle, one might expect that richer representations allow parsers to capture complex syntactic or semantic dependencies more explicitly. [sent-67, score-0.416]
</p><p>13 At the same time, such “deeper” relations may still be recoverable (to some degree) from comparatively simple parser outputs, as demonstrated for unbounded dependency extraction from strictly local syntactic dependency trees by Nivre et al. [sent-68, score-0.315]
</p><p>14 2  An armada of parsers  Stanford Parser (Klein and Manning, 2003) is a probabilistic parser which can produce both phrase structure trees and grammatical relations (syntactic dependencies). [sent-71, score-0.25]
</p><p>15 The parsing model we evaluate is the 2A noteworthy exception to this correlation is the annotated corpus of Zettlemoyer and Collins (2005), which pairs surface strings from the realm of natural language database interfaces directly with semantic representations in lambda calculus. [sent-72, score-0.173]
</p><p>16 We chose Stanford Parser from among the state-of-the-art PTB-derived parsers for its support for grammatical relations as an alternate interface representation. [sent-75, score-0.145]
</p><p>17 (2010), this combination gives the best parsing accuracy in terms of Stanford dependencies on the PTB. [sent-81, score-0.294]
</p><p>18 In addition to CCG derivations, the C&C; parser can directly output a variant of grammatical relations. [sent-88, score-0.144]
</p><p>19 Unlike other parsers in our mix, RASP did not build on PTB data in either its PoS tagging 3This hand-crafted grammar is distinct from the ERG, despite sharing the general framework of HPSG. [sent-92, score-0.193]
</p><p>20 The parser uses an edgefactored model and searches for a maximal spanning tree that connects all the words in a sentence into a dependency tree. [sent-97, score-0.191]
</p><p>21 3  Phenomena  In this section we summarize the ten phenomena we explore and our motivations for choosing them. [sent-107, score-0.313]
</p><p>22 Our goal was to find phenomena where the relevant dependencies are relatively subtle, such that more linguistic knowledge is beneficial in order to retrieve them. [sent-108, score-0.577]
</p><p>23 Though this set is of course only a sampling, these phenomena illustrate the richness of structure, both local and non-local, involved in the mapping  from English strings to their meanings. [sent-109, score-0.317]
</p><p>24 We discuss the phenomena in four sets and then briefly review their representation in the Penn Treebank. [sent-110, score-0.274]
</p><p>25 1 Long distance dependencies Three of our phenomena can be classified as involving long-distance dependencies: finite that-less relatives clauses (‘barerel’), tough adjectives (‘tough’) and right node raising (‘rnr’). [sent-112, score-0.894]
</p><p>26 While the majority of our phenomena involve local dependencies, we include these long-distance dependency types because they are challenging for parsers and enable more direct comparison with the work of Rimell et al. [sent-116, score-0.507]
</p><p>27 In contrast, our rnr category is somewhat narrower than Rimell et al. [sent-119, score-0.185]
</p><p>28 Part of the difficulty in retrieving long-distance dependencies is that the so-called extraction site is not overtly marked in the string. [sent-121, score-0.237]
</p><p>29 2 Non-dependencies Two of our phenomena crucially look for the lack of dependencies. [sent-131, score-0.274]
</p><p>30 Words involved in the relevant dependencies are highlighted in italics (dependents) and boldface (heads). [sent-133, score-0.237]
</p><p>31 The expletive it only appears when it is licensed by a specific construction (such as extraposition, (4)) or selecting head. [sent-137, score-0.148]
</p><p>32 If the goal of parsing is to recover from the surface string the dependencies capturing who did what to whom, expletive it should not feature in any of those dependencies. [sent-138, score-0.442]
</p><p>33 Likewise, instances of expletive it should be detected and discarded in reference resolution. [sent-139, score-0.148]
</p><p>34 We  hypothesize that detecting expletive it requires encoding linguistic knowledge about its licensers. [sent-140, score-0.214]
</p><p>35 4 Subtle arguments Our final three phenomena involve ways in which verbal arguments can be more difficult to identify than in ordinary finite clauses. [sent-156, score-0.465]
</p><p>36 These include detecting the arguments of verbal gerunds (‘vger’), the interleaving of arguments and adjuncts (‘argadj’) and raising/control (‘control’) constructions. [sent-157, score-0.198]
</p><p>37 The argadj examples are of interest because English typically prefers to have direct objects directly adjacent to the selecting verb. [sent-166, score-0.148]
</p><p>38 Nonetheless, phenomena such as parentheticals and heavy-NP shift (Arnold et al. [sent-167, score-0.274]
</p><p>39 Finally, we turn to raising and control verbs (‘control’) (e. [sent-170, score-0.18]
</p><p>40 These verbs select for an infinitival VP complement and stipulate that another of their arguments (subject or direct object in the examples we explore) is 401 identified with the unrealized subject position of the infinitival VP. [sent-174, score-0.215]
</p><p>41 5 Penn Treebank representations We investigated the representation of these 10 phenomena in the PTB (Marcus et al. [sent-179, score-0.347]
</p><p>42 , 1995) to determine how the relevant dependencies were intended to be represented. [sent-181, score-0.237]
</p><p>43 We then used Ghodke and Bird’s (2010) Treebank Search to find examples of the intended annotations as well as potential examples of the phenomena an-  notated differently, to get a sense of the consistency of the annotation from both precision and recall perspectives. [sent-182, score-0.385]
</p><p>44 In this study, we take the phrase structure trees of the PTB to represent dependencies based on reasonable identification of heads. [sent-183, score-0.237]
</p><p>45 The barerel, vpart, and absol phenomena are completely unproblematic, with their relevant dependencies explicitly and reliably represented. [sent-184, score-0.729]
</p><p>46 In addition, the tough construction is reliably annotated, though one of the dependencies we take to be central is not directly represented: The missing argument is linked to a null wh head at the left edge of the complement of the tough predicate, rather than to its subject. [sent-185, score-0.835]
</p><p>47 Two further phenomena (rnr and vger) are essentially correctly represented: the representations of the dependencies are explicit and mostly but not entirely consistently applied. [sent-186, score-0.584]
</p><p>48 Two out of a sample of 20 examples annotated as containing rnr did not, and two out of a sample of 35 non-rnr-annotated coordinations actually contained rnr. [sent-187, score-0.185]
</p><p>49 For vger the primary problem is with the PoS tagging, where the gerund is sometimes given a nominal tag, contrary to PTB guidelines, though the structure above it conforms. [sent-188, score-0.219]
</p><p>50 In the case ofobject control, while the guide5Distinguishing between raising and control requires further lexical knowledge and is another example of a “nondependency” (in the raising examples). [sent-190, score-0.268]
</p><p>51 In the case of itexpl, the adjoined ( S (-NONE-*EXP * ) ) indicating an expletive use of it is applied consistently for extraposition (as prescribed in the guidelines). [sent-193, score-0.185]
</p><p>52 However, the set of lexical licensers of the expletive is incomplete. [sent-194, score-0.148]
</p><p>53 For argadj we run into the problem that the PTB does not explicitly distinguish between post-verbal modifiers and verbal complements in the way that they are attached. [sent-195, score-0.22]
</p><p>54 We note that the PTB representations of many of these phenomena (barerel, tough, rnr, argadj, control, itexpl) involve empty elements and/or function tags. [sent-202, score-0.388]
</p><p>55 The second is to highlight the difficulty of producing consistent annotations of any complexity as well as the hurdles faced by a hand-annotation approach when attempting to scale a resource to more complex representations and/or additional phenomena (though cf. [sent-206, score-0.4]
</p><p>56 12%546 tough rnr itexpl vpart ned absol vger argadj control  0. [sent-212, score-1.379]
</p><p>57 78%  175 1263 402 765 349 963 679 1346 124  Table 1: Relative frequencies of phenomena matches in Wikipedia, and number of candidate strings vetted. [sent-221, score-0.317]
</p><p>58 These candidates were then hand-vetted in sequence by two annotators to identify, for each phenomenon, 100 examples that do in fact involve the  phenomenon in question and which are both grammatical and free of typos. [sent-223, score-0.2]
</p><p>59 For the control and tough phenomena hardly any filtering for complexity was applied, hence these can serve as indicators of the rate of genuine false positives. [sent-230, score-0.646]
</p><p>60 For phenomena that partially overlap with those of Rimell et al. [sent-231, score-0.274]
</p><p>61 2 Annotation format We annotated up to two dependency triples per phenomenon instance, identifying the heads and dependents by the surface form of the head words in the sentence suffixed with a number indicating word position (see Table 2). [sent-234, score-0.3]
</p><p>62 6 Some strings contain more than one instance of the phenomenon they illustrate;  in these cases, multiple sets of dependencies are 6As  the parsers differ in tokenization strategies, our evaluation script treats these position IDs as approximate indicators. [sent-235, score-0.593]
</p><p>63 7 (12) (absol) main clause predicate MOD absolutive predicate 9. [sent-253, score-0.171]
</p><p>64 In addition, some strings evince more than one of the phenomena we are studying. [sent-262, score-0.317]
</p><p>65 However, we only annotate the dependencies associated with the phenomenon the string was selected to represent. [sent-263, score-0.357]
</p><p>66 Finally, in examples with coordinated heads or dependents, we recorded separate dependencies for each conjunct. [sent-264, score-0.281]
</p><p>67 In total, we annotated 2127 dependency triples for the 1000 sentences, including 253 negative dependencies (see below). [sent-265, score-0.323]
</p><p>68 Table 3 outlines  the dependencies annotated for each phenomenon. [sent-266, score-0.237]
</p><p>69 As these sets of targets are disjunctive, these conventions should have the effect of increasing measured parser performance. [sent-268, score-0.226]
</p><p>70 580 (27%) of the annotated dependencies had at least one disjunction. [sent-269, score-0.237]
</p><p>71 Before beginning annotation on each phenomenon, we agreed on which dependencies to annotate. [sent-273, score-0.295]
</p><p>72 The annotation conventions address how to handle co-  ordination, semantically empty auxiliaries, passives and similar orthogonal phenomena. [sent-275, score-0.175]
</p><p>73 are not directly  comparable  While these outputs with each other, we  were able to associate our manually-annotated  tar-  get dependencies with parser-specific dependencies, by defining sets of phenomenon-specific regular expressions for each parser. [sent-287, score-0.237]
</p><p>74 3 with a relevant portion of the dependencies produced for the example in Table 2. [sent-293, score-0.237]
</p><p>75 Here the C&C; dependency ( ncsub j pas sed 4 Act 1 ) matches the first target in the 7We do not count typographical errors or incompletely specified conventions as failures of inter-annotator agreement. [sent-294, score-0.24]
</p><p>76 They are specific to each phenomenon, as we did not attempt to write a general dependency converter, but rather to discover what patterns of dependency rela-  tions describe the phenomenon when it is correctly identified by each parser. [sent-298, score-0.292]
</p><p>77 8 In total, we wrote 364 regular expressions to handle the output of the seven parsers, allowing some leeway in the role labels used by a parser for any given target dependency. [sent-300, score-0.148]
</p><p>78 The order of the columns within a phenomenon follows the order of the dependency descriptions in Table 3: For each pair, the dependency type with the higher score for the majority of the parsers is shown first (to the left). [sent-306, score-0.398]
</p><p>79 The phenomena themselves are also arranged according to increasing (average) difficulty. [sent-307, score-0.274]
</p><p>80 itexpl only has one column, as we annotated just one dependency per instance here. [sent-308, score-0.234]
</p><p>81 Figure 1: Individual dependency recall for seven  parsers over  ten phenomena. [sent-313, score-0.274]
</p><p>82 We observe fairly high recall of the dependencies for vpart and vger (with the exception of RASP), and high recall for both dependencies representing control for five systems. [sent-315, score-0.88]
</p><p>83 While Enju, Stanford, MST,  and RASP all found between 70 and 85% of the dependency between the adjective and its complement in the tough construction, only Enju and XLE represented the dependency between the subject of the adjective and the gap inside the adjective’s complement. [sent-316, score-0.568]
</p><p>84 For the remaining phenomena, each parser performed markedly worse on one dependency type, compared to the other. [sent-317, score-0.191]
</p><p>85 No system scored higher than 33% on the harder of the two dependencies in rnror absol, and Stanford, MST, and RASP all scored below 25% on the harder dependency in barerel. [sent-319, score-0.323]
</p><p>86 Only XLE scored higher than 10% on the second dependency for ned and higher than 50% for itexpl. [sent-320, score-0.172]
</p><p>87 1, it is clear that even the best ofthese parsers fail to correctly identify a large number ofrelevant dependencies associated with linguistic phenomena that occur with reasonable frequency 405 in the Wikipedia. [sent-322, score-0.683]
</p><p>88 For the two longdistance phenomena that overlap with those studied in Rimell et al. [sent-324, score-0.274]
</p><p>89 9 Our evaluation over Wikipedia examples thus shows the same relative lack of success in recovering longdistance dependencies that they found for WSJ sentences. [sent-326, score-0.237]
</p><p>90 The systems did better on relatively wellstudied phenomena including control, vger, and vpart, but had less success with the rest, even though all but two of those remaining phenomena involve syntactically local dependencies (as indicated in Table 3). [sent-327, score-0.826]
</p><p>91 , our average scores for each parser across the dependencies for these phenomena are within 12 points of those reported by Rimell et al. [sent-331, score-0.616]
</p><p>92 5, there is tension between developing sufficiently complex representations to capture linguistic phenomena and keeping an annotation scheme simple enough that it can be reliably produced by humans, in the case of handannotation. [sent-336, score-0.523]
</p><p>93 This allows us to include both local and non-local dependencies not represented or not reliably encoded in the PTB, enabling us to evaluate parser performance with more precision over a wider range of linguistic phenomena. [sent-348, score-0.46]
</p><p>94 The first is qualitative: Where previous work showed that overall Parseval numbers hide difficulties with long-distance dependencies, our results show that there are multiple kinds of reasonably frequent local dependencies which are also difficult for the current standard approaches to parsing. [sent-350, score-0.237]
</p><p>95 found two phenomena which were virtually unanalyzed (recall below 10%) for one or two parsers each, we found eight phenomena which were virtually unanalyzed by at least one system, including two unanalyzed by five and one by six. [sent-352, score-0.876]
</p><p>96 Thus we have shown that the dependencies being missed by typical modern approaches to parsing are more varied and more numerous than 406  previously thought. [sent-354, score-0.294]
</p><p>97 8  Conclusion  We have presented a detailed construction-focused evaluation of seven parsers over 10 phenomena, with 1000 examples drawn from English Wikipedia. [sent-355, score-0.149]
</p><p>98 Our results demonstrate that significant opportunities remain for parser improvement, and highlight specific challenges that remain invisible in aggregate parser evaluation (e. [sent-358, score-0.21]
</p><p>99 (2009) and expand the range of phenomena considered in such evaluations. [sent-364, score-0.274]
</p><p>100 Bracketing guidelines for treebank II style Penn treebank project. [sent-377, score-0.164]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('phenomena', 0.274), ('tough', 0.24), ('ptb', 0.237), ('dependencies', 0.237), ('rimell', 0.223), ('rnr', 0.185), ('absol', 0.166), ('vger', 0.166), ('argadj', 0.148), ('erg', 0.148), ('expletive', 0.148), ('itexpl', 0.148), ('vpart', 0.148), ('xle', 0.148), ('barerel', 0.129), ('phenomenon', 0.12), ('parsers', 0.106), ('parser', 0.105), ('rasp', 0.101), ('enju', 0.095), ('control', 0.092), ('raising', 0.088), ('grammar', 0.087), ('ned', 0.086), ('dependency', 0.086), ('conventions', 0.08), ('flickinger', 0.079), ('oepen', 0.074), ('unanalyzed', 0.074), ('representations', 0.073), ('verbal', 0.072), ('linguistic', 0.066), ('complement', 0.066), ('ncmod', 0.064), ('particle', 0.063), ('annotation', 0.058), ('predicate', 0.058), ('hpsg', 0.058), ('parsing', 0.057), ('guidelines', 0.056), ('absolutive', 0.055), ('donovan', 0.055), ('infinitival', 0.055), ('pargram', 0.055), ('relatives', 0.055), ('wsj', 0.054), ('treebank', 0.054), ('gerund', 0.053), ('annotations', 0.053), ('reliably', 0.052), ('wikipedia', 0.05), ('dependents', 0.05), ('briscoe', 0.05), ('gerunds', 0.048), ('dept', 0.048), ('kaplan', 0.048), ('tracy', 0.048), ('script', 0.047), ('stanford', 0.047), ('adjective', 0.045), ('constructions', 0.045), ('heads', 0.044), ('seven', 0.043), ('bare', 0.043), ('mod', 0.043), ('adjunct', 0.043), ('maxwell', 0.043), ('strings', 0.043), ('ccg', 0.042), ('coordination', 0.041), ('targets', 0.041), ('involve', 0.041), ('genuine', 0.04), ('cahill', 0.04), ('tokenization', 0.04), ('arguments', 0.039), ('grammatical', 0.039), ('ten', 0.039), ('raised', 0.038), ('unbounded', 0.038), ('riezler', 0.037), ('alios', 0.037), ('aoife', 0.037), ('beek', 0.037), ('complementation', 0.037), ('crew', 0.037), ('downstairs', 0.037), ('extraposition', 0.037), ('ghodke', 0.037), ('huddleston', 0.037), ('incompletely', 0.037), ('ncsub', 0.037), ('obscuring', 0.037), ('passives', 0.037), ('upstairs', 0.037), ('wikiwoods', 0.037), ('xmod', 0.037), ('np', 0.036), ('king', 0.036), ('hockenmaier', 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999893 <a title="103-tfidf-1" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>Author: Emily M. Bender ; Dan Flickinger ; Stephan Oepen ; Yi Zhang</p><p>Abstract: In order to obtain a fine-grained evaluation of parser accuracy over naturally occurring text, we study 100 examples each of ten reasonably frequent linguistic phenomena, randomly selected from a parsed version of the English Wikipedia. We construct a corresponding set of gold-standard target dependencies for these 1000 sentences, operationalize mappings to these targets from seven state-of-theart parsers, and evaluate the parsers against this data to measure their level of success in identifying these dependencies.</p><p>2 0.17525852 <a title="103-tfidf-2" href="./emnlp-2011-A_Fast%2C_Accurate%2C_Non-Projective%2C_Semantically-Enriched_Parser.html">4 emnlp-2011-A Fast, Accurate, Non-Projective, Semantically-Enriched Parser</a></p>
<p>Author: Stephen Tratz ; Eduard Hovy</p><p>Abstract: Dependency parsers are critical components within many NLP systems. However, currently available dependency parsers each exhibit at least one of several weaknesses, including high running time, limited accuracy, vague dependency labels, and lack of nonprojectivity support. Furthermore, no commonly used parser provides additional shallow semantic interpretation, such as preposition sense disambiguation and noun compound interpretation. In this paper, we present a new dependency-tree conversion of the Penn Treebank along with its associated fine-grain dependency labels and a fast, accurate parser trained on it. We explain how a non-projective extension to shift-reduce parsing can be incorporated into non-directional easy-first parsing. The parser performs well when evaluated on the standard test section of the Penn Treebank, outperforming several popular open source dependency parsers; it is, to the best of our knowledge, the first dependency parser capable of parsing more than 75 sentences per second at over 93% accuracy.</p><p>3 0.15119846 <a title="103-tfidf-3" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>Author: Reut Tsarfaty ; Joakim Nivre ; Evelina Andersson</p><p>Abstract: unkown-abstract</p><p>4 0.1358524 <a title="103-tfidf-4" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>Author: Keith Hall ; Ryan McDonald ; Jason Katz-Brown ; Michael Ringgaard</p><p>Abstract: We present an online learning algorithm for training parsers which allows for the inclusion of multiple objective functions. The primary example is the extension of a standard supervised parsing objective function with additional loss-functions, either based on intrinsic parsing quality or task-specific extrinsic measures of quality. Our empirical results show how this approach performs for two dependency parsing algorithms (graph-based and transition-based parsing) and how it achieves increased performance on multiple target tasks including reordering for machine translation and parser adaptation.</p><p>5 0.1268768 <a title="103-tfidf-5" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Noah A. Smith</p><p>Abstract: We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.</p><p>6 0.11354123 <a title="103-tfidf-6" href="./emnlp-2011-Multi-Source_Transfer_of_Delexicalized_Dependency_Parsers.html">95 emnlp-2011-Multi-Source Transfer of Delexicalized Dependency Parsers</a></p>
<p>7 0.09931083 <a title="103-tfidf-7" href="./emnlp-2011-Unsupervised_Dependency_Parsing_without_Gold_Part-of-Speech_Tags.html">141 emnlp-2011-Unsupervised Dependency Parsing without Gold Part-of-Speech Tags</a></p>
<p>8 0.09361773 <a title="103-tfidf-8" href="./emnlp-2011-A_Joint_Model_for_Extended_Semantic_Role_Labeling.html">7 emnlp-2011-A Joint Model for Extended Semantic Role Labeling</a></p>
<p>9 0.092280887 <a title="103-tfidf-9" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>10 0.08946351 <a title="103-tfidf-10" href="./emnlp-2011-Parse_Correction_with_Specialized_Models_for_Difficult_Attachment_Types.html">102 emnlp-2011-Parse Correction with Specialized Models for Difficult Attachment Types</a></p>
<p>11 0.086105689 <a title="103-tfidf-11" href="./emnlp-2011-Semi-supervised_CCG_Lexicon_Extension.html">121 emnlp-2011-Semi-supervised CCG Lexicon Extension</a></p>
<p>12 0.084920086 <a title="103-tfidf-12" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>13 0.083985709 <a title="103-tfidf-13" href="./emnlp-2011-Joint_Models_for_Chinese_POS_Tagging_and_Dependency_Parsing.html">75 emnlp-2011-Joint Models for Chinese POS Tagging and Dependency Parsing</a></p>
<p>14 0.083492108 <a title="103-tfidf-14" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>15 0.077934049 <a title="103-tfidf-15" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<p>16 0.077781469 <a title="103-tfidf-16" href="./emnlp-2011-Syntax-Based_Grammaticality_Improvement_using_CCG_and_Guided_Search.html">132 emnlp-2011-Syntax-Based Grammaticality Improvement using CCG and Guided Search</a></p>
<p>17 0.07755474 <a title="103-tfidf-17" href="./emnlp-2011-Lexical_Generalization_in_CCG_Grammar_Induction_for_Semantic_Parsing.html">87 emnlp-2011-Lexical Generalization in CCG Grammar Induction for Semantic Parsing</a></p>
<p>18 0.076259673 <a title="103-tfidf-18" href="./emnlp-2011-Accurate_Parsing_with_Compact_Tree-Substitution_Grammars%3A_Double-DOP.html">16 emnlp-2011-Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP</a></p>
<p>19 0.07317438 <a title="103-tfidf-19" href="./emnlp-2011-Unsupervised_Structure_Prediction_with_Non-Parallel_Multilingual_Guidance.html">146 emnlp-2011-Unsupervised Structure Prediction with Non-Parallel Multilingual Guidance</a></p>
<p>20 0.07305856 <a title="103-tfidf-20" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.27), (1, 0.053), (2, -0.063), (3, 0.218), (4, -0.031), (5, 0.045), (6, -0.052), (7, 0.076), (8, 0.114), (9, -0.004), (10, -0.034), (11, -0.03), (12, 0.001), (13, -0.024), (14, 0.049), (15, 0.065), (16, 0.082), (17, -0.085), (18, 0.047), (19, -0.07), (20, 0.011), (21, -0.029), (22, 0.053), (23, -0.015), (24, -0.126), (25, -0.093), (26, -0.03), (27, 0.107), (28, -0.064), (29, -0.046), (30, -0.018), (31, 0.008), (32, 0.024), (33, -0.079), (34, -0.047), (35, -0.006), (36, 0.013), (37, 0.017), (38, 0.001), (39, -0.077), (40, 0.003), (41, 0.084), (42, 0.033), (43, 0.064), (44, -0.019), (45, -0.09), (46, -0.005), (47, 0.004), (48, -0.078), (49, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95567805 <a title="103-lsi-1" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>Author: Emily M. Bender ; Dan Flickinger ; Stephan Oepen ; Yi Zhang</p><p>Abstract: In order to obtain a fine-grained evaluation of parser accuracy over naturally occurring text, we study 100 examples each of ten reasonably frequent linguistic phenomena, randomly selected from a parsed version of the English Wikipedia. We construct a corresponding set of gold-standard target dependencies for these 1000 sentences, operationalize mappings to these targets from seven state-of-theart parsers, and evaluate the parsers against this data to measure their level of success in identifying these dependencies.</p><p>2 0.81183636 <a title="103-lsi-2" href="./emnlp-2011-A_Fast%2C_Accurate%2C_Non-Projective%2C_Semantically-Enriched_Parser.html">4 emnlp-2011-A Fast, Accurate, Non-Projective, Semantically-Enriched Parser</a></p>
<p>Author: Stephen Tratz ; Eduard Hovy</p><p>Abstract: Dependency parsers are critical components within many NLP systems. However, currently available dependency parsers each exhibit at least one of several weaknesses, including high running time, limited accuracy, vague dependency labels, and lack of nonprojectivity support. Furthermore, no commonly used parser provides additional shallow semantic interpretation, such as preposition sense disambiguation and noun compound interpretation. In this paper, we present a new dependency-tree conversion of the Penn Treebank along with its associated fine-grain dependency labels and a fast, accurate parser trained on it. We explain how a non-projective extension to shift-reduce parsing can be incorporated into non-directional easy-first parsing. The parser performs well when evaluated on the standard test section of the Penn Treebank, outperforming several popular open source dependency parsers; it is, to the best of our knowledge, the first dependency parser capable of parsing more than 75 sentences per second at over 93% accuracy.</p><p>3 0.73243821 <a title="103-lsi-3" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>Author: Reut Tsarfaty ; Joakim Nivre ; Evelina Andersson</p><p>Abstract: unkown-abstract</p><p>4 0.61674577 <a title="103-lsi-4" href="./emnlp-2011-Parse_Correction_with_Specialized_Models_for_Difficult_Attachment_Types.html">102 emnlp-2011-Parse Correction with Specialized Models for Difficult Attachment Types</a></p>
<p>Author: Enrique Henestroza Anguiano ; Marie Candito</p><p>Abstract: This paper develops a framework for syntactic dependency parse correction. Dependencies in an input parse tree are revised by selecting, for a given dependent, the best governor from within a small set of candidates. We use a discriminative linear ranking model to select the best governor from a group of candidates for a dependent, and our model includes a rich feature set that encodes syntactic structure in the input parse tree. The parse correction framework is parser-agnostic, and can correct attachments using either a generic model or specialized models tailored to difficult attachment types like coordination and pp-attachment. Our experiments show that parse correction, combining a generic model with specialized models for difficult attachment types, can successfully improve the quality of predicted parse trees output by sev- eral representative state-of-the-art dependency parsers for French.</p><p>5 0.59148133 <a title="103-lsi-5" href="./emnlp-2011-Multi-Source_Transfer_of_Delexicalized_Dependency_Parsers.html">95 emnlp-2011-Multi-Source Transfer of Delexicalized Dependency Parsers</a></p>
<p>Author: Ryan McDonald ; Slav Petrov ; Keith Hall</p><p>Abstract: We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source lan- guages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.</p><p>6 0.57036698 <a title="103-lsi-6" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>7 0.54080635 <a title="103-lsi-7" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<p>8 0.52547163 <a title="103-lsi-8" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>9 0.49833143 <a title="103-lsi-9" href="./emnlp-2011-Semi-supervised_CCG_Lexicon_Extension.html">121 emnlp-2011-Semi-supervised CCG Lexicon Extension</a></p>
<p>10 0.47558588 <a title="103-lsi-10" href="./emnlp-2011-Accurate_Parsing_with_Compact_Tree-Substitution_Grammars%3A_Double-DOP.html">16 emnlp-2011-Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP</a></p>
<p>11 0.47507787 <a title="103-lsi-11" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>12 0.47268277 <a title="103-lsi-12" href="./emnlp-2011-Syntax-Based_Grammaticality_Improvement_using_CCG_and_Guided_Search.html">132 emnlp-2011-Syntax-Based Grammaticality Improvement using CCG and Guided Search</a></p>
<p>13 0.46960416 <a title="103-lsi-13" href="./emnlp-2011-Lexical_Generalization_in_CCG_Grammar_Induction_for_Semantic_Parsing.html">87 emnlp-2011-Lexical Generalization in CCG Grammar Induction for Semantic Parsing</a></p>
<p>14 0.44658813 <a title="103-lsi-14" href="./emnlp-2011-Exact_Inference_for_Generative_Probabilistic_Non-Projective_Dependency_Parsing.html">52 emnlp-2011-Exact Inference for Generative Probabilistic Non-Projective Dependency Parsing</a></p>
<p>15 0.44222391 <a title="103-lsi-15" href="./emnlp-2011-Unsupervised_Dependency_Parsing_without_Gold_Part-of-Speech_Tags.html">141 emnlp-2011-Unsupervised Dependency Parsing without Gold Part-of-Speech Tags</a></p>
<p>16 0.43521178 <a title="103-lsi-16" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>17 0.43441686 <a title="103-lsi-17" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>18 0.42379069 <a title="103-lsi-18" href="./emnlp-2011-Relaxed_Cross-lingual_Projection_of_Constituent_Syntax.html">115 emnlp-2011-Relaxed Cross-lingual Projection of Constituent Syntax</a></p>
<p>19 0.42080009 <a title="103-lsi-19" href="./emnlp-2011-A_Joint_Model_for_Extended_Semantic_Role_Labeling.html">7 emnlp-2011-A Joint Model for Extended Semantic Role Labeling</a></p>
<p>20 0.41038778 <a title="103-lsi-20" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.086), (36, 0.025), (37, 0.018), (45, 0.444), (53, 0.021), (54, 0.022), (57, 0.016), (62, 0.021), (64, 0.022), (66, 0.039), (69, 0.012), (79, 0.06), (82, 0.018), (90, 0.029), (96, 0.071), (98, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98557729 <a title="103-lda-1" href="./emnlp-2011-Bayesian_Checking_for_Topic_Models.html">21 emnlp-2011-Bayesian Checking for Topic Models</a></p>
<p>Author: David Mimno ; David Blei</p><p>Abstract: Real document collections do not fit the independence assumptions asserted by most statistical topic models, but how badly do they violate them? We present a Bayesian method for measuring how well a topic model fits a corpus. Our approach is based on posterior predictive checking, a method for diagnosing Bayesian models in user-defined ways. Our method can identify where a topic model fits the data, where it falls short, and in which directions it might be improved.</p><p>2 0.98015457 <a title="103-lda-2" href="./emnlp-2011-Lexical_Co-occurrence%2C_Statistical_Significance%2C_and_Word_Association.html">86 emnlp-2011-Lexical Co-occurrence, Statistical Significance, and Word Association</a></p>
<p>Author: Dipak L. Chaudhari ; Om P. Damani ; Srivatsan Laxman</p><p>Abstract: Om P. Damani Srivatsan Laxman Computer Science and Engg. Microsoft Research India IIT Bombay Bangalore damani @ cse . i . ac . in itb s laxman@mi cro s o ft . com of words that co-occur in a large number of docuLexical co-occurrence is an important cue for detecting word associations. We propose a new measure of word association based on a new notion of statistical significance for lexical co-occurrences. Existing measures typically rely on global unigram frequencies to determine expected co-occurrence counts. In- stead, we focus only on documents that contain both terms (of a candidate word-pair) and ask if the distribution of the observed spans of the word-pair resembles that under a random null model. This would imply that the words in the pair are not related strongly enough for one word to influence placement of the other. However, if the words are found to occur closer together than explainable by the null model, then we hypothesize a more direct association between the words. Through extensive empirical evaluation on most of the publicly available benchmark data sets, we show the advantages of our measure over existing co-occurrence measures.</p><p>3 0.97171378 <a title="103-lda-3" href="./emnlp-2011-Approximate_Scalable_Bounded_Space_Sketch_for_Large_Data_NLP.html">19 emnlp-2011-Approximate Scalable Bounded Space Sketch for Large Data NLP</a></p>
<p>Author: Amit Goyal ; Hal Daume III</p><p>Abstract: We exploit sketch techniques, especially the Count-Min sketch, a memory, and time efficient framework which approximates the frequency of a word pair in the corpus without explicitly storing the word pair itself. These methods use hashing to deal with massive amounts of streaming text. We apply CountMin sketch to approximate word pair counts and exhibit their effectiveness on three important NLP tasks. Our experiments demonstrate that on all of the three tasks, we get performance comparable to Exact word pair counts setting and state-of-the-art system. Our method scales to 49 GB of unzipped web data using bounded space of 2 billion counters (8 GB memory).</p><p>4 0.93896866 <a title="103-lda-4" href="./emnlp-2011-Linking_Entities_to_a_Knowledge_Base_with_Query_Expansion.html">90 emnlp-2011-Linking Entities to a Knowledge Base with Query Expansion</a></p>
<p>Author: Swapna Gottipati ; Jing Jiang</p><p>Abstract: In this paper we present a novel approach to entity linking based on a statistical language model-based information retrieval with query expansion. We use both local contexts and global world knowledge to expand query language models. We place a strong emphasis on named entities in the local contexts and explore a positional language model to weigh them differently based on their distances to the query. Our experiments on the TAC-KBP 2010 data show that incorporating such contextual information indeed aids in disambiguating the named entities and consistently improves the entity linking performance. Compared with the official results from KBP 2010 participants, our system shows competitive performance.</p><p>same-paper 5 0.93406498 <a title="103-lda-5" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>Author: Emily M. Bender ; Dan Flickinger ; Stephan Oepen ; Yi Zhang</p><p>Abstract: In order to obtain a fine-grained evaluation of parser accuracy over naturally occurring text, we study 100 examples each of ten reasonably frequent linguistic phenomena, randomly selected from a parsed version of the English Wikipedia. We construct a corresponding set of gold-standard target dependencies for these 1000 sentences, operationalize mappings to these targets from seven state-of-theart parsers, and evaluate the parsers against this data to measure their level of success in identifying these dependencies.</p><p>6 0.81894779 <a title="103-lda-6" href="./emnlp-2011-Semantic_Topic_Models%3A_Combining_Word_Distributional_Statistics_and_Dictionary_Definitions.html">119 emnlp-2011-Semantic Topic Models: Combining Word Distributional Statistics and Dictionary Definitions</a></p>
<p>7 0.81751525 <a title="103-lda-7" href="./emnlp-2011-Optimizing_Semantic_Coherence_in_Topic_Models.html">101 emnlp-2011-Optimizing Semantic Coherence in Topic Models</a></p>
<p>8 0.77553564 <a title="103-lda-8" href="./emnlp-2011-Harnessing_different_knowledge_sources_to_measure_semantic_relatedness_under_a_uniform_model.html">64 emnlp-2011-Harnessing different knowledge sources to measure semantic relatedness under a uniform model</a></p>
<p>9 0.77502221 <a title="103-lda-9" href="./emnlp-2011-Cross-Cutting_Models_of_Lexical_Semantics.html">37 emnlp-2011-Cross-Cutting Models of Lexical Semantics</a></p>
<p>10 0.74750847 <a title="103-lda-10" href="./emnlp-2011-Cooooooooooooooollllllllllllll%21%21%21%21%21%21%21%21%21%21%21%21%21%21_Using_Word_Lengthening_to_Detect_Sentiment_in_Microblogs.html">33 emnlp-2011-Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using Word Lengthening to Detect Sentiment in Microblogs</a></p>
<p>11 0.7371195 <a title="103-lda-11" href="./emnlp-2011-Exploring_Supervised_LDA_Models_for_Assigning_Attributes_to_Adjective-Noun_Phrases.html">56 emnlp-2011-Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases</a></p>
<p>12 0.71204096 <a title="103-lda-12" href="./emnlp-2011-Exploiting_Syntactic_and_Distributional_Information_for_Spelling_Correction_with_Web-Scale_N-gram_Models.html">55 emnlp-2011-Exploiting Syntactic and Distributional Information for Spelling Correction with Web-Scale N-gram Models</a></p>
<p>13 0.69531226 <a title="103-lda-13" href="./emnlp-2011-Improving_Bilingual_Projections_via_Sparse_Covariance_Matrices.html">73 emnlp-2011-Improving Bilingual Projections via Sparse Covariance Matrices</a></p>
<p>14 0.69004333 <a title="103-lda-14" href="./emnlp-2011-Robust_Disambiguation_of_Named_Entities_in_Text.html">116 emnlp-2011-Robust Disambiguation of Named Entities in Text</a></p>
<p>15 0.68932128 <a title="103-lda-15" href="./emnlp-2011-Learning_General_Connotation_of_Words_using_Graph-based_Algorithms.html">81 emnlp-2011-Learning General Connotation of Words using Graph-based Algorithms</a></p>
<p>16 0.66824484 <a title="103-lda-16" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>17 0.66675788 <a title="103-lda-17" href="./emnlp-2011-Efficient_retrieval_of_tree_translation_examples_for_Syntax-Based_Machine_Translation.html">47 emnlp-2011-Efficient retrieval of tree translation examples for Syntax-Based Machine Translation</a></p>
<p>18 0.66574603 <a title="103-lda-18" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>19 0.66564178 <a title="103-lda-19" href="./emnlp-2011-Literal_and_Metaphorical_Sense_Identification_through_Concrete_and_Abstract_Context.html">91 emnlp-2011-Literal and Metaphorical Sense Identification through Concrete and Abstract Context</a></p>
<p>20 0.66131681 <a title="103-lda-20" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
