<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>99 emnlp-2011-Non-parametric Bayesian Segmentation of Japanese Noun Phrases</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-99" href="#">emnlp2011-99</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>99 emnlp-2011-Non-parametric Bayesian Segmentation of Japanese Noun Phrases</h1>
<br/><p>Source: <a title="emnlp-2011-99-pdf" href="http://aclweb.org/anthology//D/D11/D11-1056.pdf">pdf</a></p><p>Author: Yugo Murawaki ; Sadao Kurohashi</p><p>Abstract: A key factor of high quality word segmentation for Japanese is a high-coverage dictionary, but it is costly to manually build such a lexical resource. Although external lexical resources for human readers are potentially good knowledge sources, they have not been utilized due to differences in segmentation criteria. To supplement a morphological dictionary with these resources, we propose a new task of Japanese noun phrase segmentation. We apply non-parametric Bayesian language models to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text. For inference, we propose a novel block sampling procedure named hybrid type-based sampling, which has the ability to directly escape a local optimum that is not too distant from the global optimum. Experiments show that the proposed method efficiently corrects the initial segmentation given by a morphological ana- lyzer.</p><p>Reference: <a title="emnlp-2011-99-reference" href="../emnlp2011_reference/emnlp-2011-Non-parametric_Bayesian_Segmentation_of_Japanese_Noun_Phrases_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp  ,  Abstract A key factor of high quality word segmentation for Japanese is a high-coverage dictionary, but it is costly to manually build such a lexical resource. [sent-4, score-0.477]
</p><p>2 Although external lexical resources for human readers are potentially good knowledge sources, they have not been utilized due to differences in segmentation criteria. [sent-5, score-0.466]
</p><p>3 To supplement a morphological dictionary with these resources, we propose a new task of Japanese noun phrase segmentation. [sent-6, score-0.462]
</p><p>4 We apply non-parametric Bayesian language models to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text. [sent-7, score-0.312]
</p><p>5 For inference, we propose a novel block sampling procedure named hybrid type-based sampling, which has the ability to directly escape a local optimum that is not too distant from the global optimum. [sent-8, score-0.49]
</p><p>6 Experiments show that the proposed method efficiently corrects the initial segmentation given by a morphological ana-  lyzer. [sent-9, score-0.674]
</p><p>7 1 Introduction Word segmentation is the first step of natural language processing for Japanese, Chinese and Thai because they do not delimit words by white-space. [sent-10, score-0.427]
</p><p>8 Among them,  encyclopedias are especially important in that they contain a lot of terms that a morphological dictionary fails to cover. [sent-18, score-0.342]
</p><p>9 According to our segmentation criteria, it consists of two words “常山” (tsuneyama) and “城” (jou). [sent-21, score-0.427]
</p><p>10 However, the morphological analyzer wrongly segments it into “常” (tsune) and “山城” (yamashiro) because “常山” (tsuneyama) is an unknown word. [sent-22, score-0.523]
</p><p>11 To do this, we examine the main text of the entry, on the assumption that if the noun phrase in question consists of more than one word, its constituents appear in the main text either freely or as part of other noun phrases. [sent-25, score-0.415]
</p><p>12 The bigram model alleviates a problem of the unigram model, that is, a tendency to misidentify a sequence of words in common collocations as a single word. [sent-33, score-0.381]
</p><p>13 However, type-based sampling is not easily applicable to the bigram model owing to sparsity and its dependence on latent assignments. [sent-36, score-0.438]
</p><p>14 We propose a hybrid type-based sampling procedure, which combines the Metropolis-Hastings algorithm with Gibbs sampling. [sent-37, score-0.332]
</p><p>15 This greatly eases the sampling procedure while retaining the efficiency of typebased sampling. [sent-40, score-0.329]
</p><p>16 Experiments show that the pro-  posed method quickly corrects the initial segmentation given by a morphological analyzer. [sent-41, score-0.674]
</p><p>17 2  Related Work  Japanese Morphological Analysis and Lexical Acquisition Word segmentation for Japanese is usually solved as the joint task of segmentation and part-of-speech tagging, which is called morphological analysis (Kurohashi et al. [sent-42, score-1.054]
</p><p>18 Since, unlike Chinese and Thai, Japanese is rich in morphology, morphological regularity can be used to determine if an unknown word candidate in text is indeed the word to be acquired. [sent-53, score-0.316]
</p><p>19 Noun phrases can hardly be distinguished from single nouns because in Japanese, no morphological marker is attached to join nouns to form a noun phrase. [sent-55, score-0.406]
</p><p>20 The assumption that language is a se-  quence of invariant words fails to capture rich morphology, as our segmentation criteria specify that each verb or adjective consists of an invariant stem and an ending that changes its form according to its grammatical roles. [sent-59, score-0.497]
</p><p>21 NER performance may be affected by segmentation errors in morphological analysis involving unknown words. [sent-69, score-0.743]
</p><p>22 Chinese word segmentation is often formalized as a character tagging problem (Xue, 2003). [sent-70, score-0.476]
</p><p>23 3  Japanese Noun Phrase Segmentation  Our goal is to overcome the unknown word problem in morphological analysis by utilizing existing resources such as dictionaries and encyclopedias for human readers. [sent-86, score-0.397]
</p><p>24 If the noun phrase in question consists of more than one word, its constituents would appear in the text either freely or as part of other noun phrases. [sent-96, score-0.415]
</p><p>25 We obtain the segmentation of an entry noun phrase by considering the segmentation of the whole 1http : / / s ource forge . [sent-97, score-1.136]
</p><p>26 One may instead consider a pipeline approach in which we first extract noun phrases in text and then identify boundaries within these noun phrases. [sent-100, score-0.439]
</p><p>27 However, noun phrases in text are not trivially identifiable in the case that they contain unknown words as their constituents. [sent-101, score-0.322]
</p><p>28 For example, the analyzer erroneously segments the word “ち ん す う ” (chiNsukou) into “ちん” (chiN) and “す う ” (sukou), and since the latter is misidentified as a verb, the incorrect noun phrase “ち ん” (chiN) is extracted. [sent-102, score-0.408]
</p><p>29 We have a morphological analyzer with a dictionary that covers frequent words. [sent-103, score-0.468]
</p><p>30 For this reason, we like to use the segmentation given by the analyzer as the initial state and to make small changes to them to get  こ  こ  a desired output. [sent-105, score-0.728]
</p><p>31 As the annotated corpus encodes our segmentation criteria, it can be used to force the models to stick with our segmentation criteria. [sent-107, score-0.933]
</p><p>32 We concentrate on segmentation in this paper, but we also need to assign a POS tag to each constituent word and to incorporate segmented noun phrases into the dictionary of the morphological analyzer. [sent-108, score-0.968]
</p><p>33 3 4  Non-parametric Bayesian Language Models  To correct the initial segmentation given by the analyzer, we use non-parametric Bayesian language models that have been applied to unsupervised word segmentation (Goldwater et al. [sent-110, score-0.901]
</p><p>34 1 Unigram Model In the unigram model, a word in the corpus  wi  is  generated as follows: G|α0, P0  ∼  DP(α0, P0)  wi|G  ∼  G  3Fortunately, the morphological analyzer JUMAN is capable of handling phrases, each of which consists of more than one word. [sent-115, score-0.587]
</p><p>35 In  preliminary experiments, we found that the unigram model often interpreted a noun phrase as a single word, even in the case that its constituents frequently appeared in text. [sent-123, score-0.375]
</p><p>36 2 Bigram Model The problem of the unigram model can be alleviated by the bigram model based on a hierarchical Dirichlet process (Goldwater et al. [sent-125, score-0.304]
</p><p>37 In the bigram model, word wi is generated as follows: G|α0, P0 ∼ DP(α0, P0) Hl |α1 , G ∼ DP(α1, G) wi |wi−1 = l, Hl ∼ Hl Marginalizing out G and Hl, we can again explain the model with the Chinese restaurant process. [sent-127, score-0.322]
</p><p>38 Unlike the unigram model, however, the bigram model depends on the latent table assignments z−i. [sent-128, score-0.304]
</p><p>39 4 Mixing an Annotated Corpus An annotated corpus can be used to force the models to stick with our segmentation criteria. [sent-150, score-0.506]
</p><p>40 A  straightforward way to do this is to mix it with raw text while fixing the segmentation during inference (Mochihashi et al. [sent-151, score-0.427]
</p><p>41 Similarly, the back-off mixing bigram model replaces P1 in (2)  with  P1BM 5  = λIPP1 + (1 − λIP)P2REF. [sent-161, score-0.41]
</p><p>42 Inference  Collapsed Gibbs sampling is widely used to find an optimal segmentation (Goldwater et al. [sent-162, score-0.675]
</p><p>43 In this section, we first show that simple collapsed sampling can hardly escape the initial segmentation. [sent-164, score-0.495]
</p><p>44 To address this problem, we apply a block sampling algorithm named type-based sampling (Liang et al. [sent-165, score-0.549]
</p><p>45 Since type-based sampling is not applicable to the bigram model, we propose a novel sampling procedure for the bigram model, which we call hybrid type-based sampling. [sent-167, score-0.96]
</p><p>46 1 Collapsed Sampling In collapsed Gibbs sampling, the sampler repeatedly samples every possible boundary position, conditioned on the current state of the rest of the corpus. [sent-169, score-0.354]
</p><p>47 This property is especially problematic in our settings where the initial segmentation is given by a morphological analyzer. [sent-174, score-0.674]
</p><p>48 Since the analyzer deterministically segments text using pre-defined parameters, the resultant segmentation is fairly consistent. [sent-175, score-0.634]
</p><p>49 For this reason, the initial segmentation is usually chosen at random (Goldwater et al. [sent-179, score-0.474]
</p><p>50 Sentence-based block sampling is also susceptible to consistent initialization (Liang et al. [sent-181, score-0.355]
</p><p>51 2 Type-based Sampling To achieve fast convergence, we adopt a block sampling algorithm named type-based sampling (Liang et al. [sent-184, score-0.549]
</p><p>52 Type-based sampling takes advantage of the exchangeability of multiple positions with the same type. [sent-188, score-0.338]
</p><p>53 (2010) used random initialization, we take particular note of the possibility of efficiently correcting the consistent segmentation by the analyzer. [sent-194, score-0.427]
</p><p>54 Type-based sampling is, however, not applicable to the bigram model for two reasons. [sent-195, score-0.438]
</p><p>55 Strictly speaking, we need to update the model counts even when sampling one position because the observation of the bigram ⟨wlw1⟩, for example, may vafafteicotn th oef probability P2 (w2 |h− , ⟨wlw1⟩). [sent-202, score-0.438]
</p><p>56 (2009) approximate t|hhe probability by not updating the model counts in collapsed Gibbs sampling (i. [sent-204, score-0.385]
</p><p>57 This is motivated by the observation that although the joint sampling of a large number of positions is computationally expensive, the proposal is accepted very infrequently. [sent-255, score-0.454]
</p><p>58 Similarly, we can impose our trivial rules of segmentation on the model. [sent-262, score-0.427]
</p><p>59 For each entry of Wikipedia, we regarded the title as a noun phrase and used both the title and main text for segmentation. [sent-266, score-0.418]
</p><p>60 We separately applied our segmentation procedure to each entry. [sent-267, score-0.427]
</p><p>61 We applied both the title and main text to the morphological analyzer JUMAN5 to get an initial segmentation. [sent-274, score-0.522]
</p><p>62 If the resultant segmentation conflicted with markup information, we overrode the former. [sent-275, score-0.488]
</p><p>63 The initial segmentation was also used as the baseline. [sent-276, score-0.474]
</p><p>64 The first condition ensures that there are segmentation ambiguities. [sent-293, score-0.427]
</p><p>65 Models We compared the unigram and bigram models. [sent-301, score-0.304]
</p><p>66 As for inference procedures, we used collapsed Gibbs sampling (CL) for both models, typebased sampling (TB) for the unigram model and hybrid type-based sampling (HTB) for the bigram model. [sent-302, score-1.35]
</p><p>67 We tested two mixing methods of the annotated corpus, direct mixing (DM) and back-off mixing (BM). [sent-303, score-0.704]
</p><p>68 The unigram model has one Dirichlet process concentration hyperparameter α0 and the bigram model has α0 and α1. [sent-307, score-0.403]
</p><p>69 Kyot o% 2 0Univers ity% 2 0 Text % 2 0 Corpus Table 1: Results of segmentation of entry titles (F-score (precision/recall)). [sent-329, score-0.508]
</p><p>70 Evaluation Metrics We evaluated the segmentation accuracy of 500 entry titles. [sent-330, score-0.508]
</p><p>71 We report the score of the most frequent  segmentation among 10 samples. [sent-332, score-0.427]
</p><p>72 2 Results Table 1 shows segmentation accuracy of various models. [sent-340, score-0.427]
</p><p>73 As suggested by relatively low precision, unknown words tend to be over-segmented by the morphological analyzer. [sent-344, score-0.316]
</p><p>74 In the best hyperparameter settings, the back-off mixing bigram model with hybrid type-based sam-  612 pling (bigram + HTB + BM) significantly outperformed the baseline and achieved the best F-score. [sent-345, score-0.549]
</p><p>75 It is simply because it did not change the initial segmentation a lot. [sent-349, score-0.474]
</p><p>76 In contrast, type-based sampling (+TB) brought large moves to the unigram model and significantly hurt accuracy. [sent-350, score-0.362]
</p><p>77 When combined with (hybrid) type-based sampling (+TB/+HTB), back-off mixing (+BM) increased accuracy from the corresponding nonmixing models. [sent-352, score-0.468]
</p><p>78 To our surprise, collapsed sampling with mixing models (+CL, +DM/+BM) outperformed the baseline. [sent-355, score-0.605]
</p><p>79 7 A diff is defined as the number of character-based disagreements between the baseline segmentation and a model output. [sent-361, score-0.427]
</p><p>80 We can see that collapsed sampling was almost unable to escape the initial state. [sent-363, score-0.495]
</p><p>81 With type-based sampling (+TB), the unigram model went further than the bigram model, but to an undesired direction. [sent-364, score-0.587]
</p><p>82 The bigram model with hybrid type-based sampling (bigram + HTB) converged in few itera-  tions. [sent-365, score-0.522]
</p><p>83 Although the model with random initialization (+RAND) converged to a nearby point, the initial segmentation by the morphological analyzer realized a bit faster convergence and better accuracy. [sent-366, score-0.935]
</p><p>84 However, this seems wasteful, given that a large portion of text has only marginal influence on the segmentation of the noun phrase in question. [sent-374, score-0.628]
</p><p>85 We sampled a boundary only if the corresponding local area contained a substring of the noun phrase in question. [sent-376, score-0.3]
</p><p>86 However, the difference of convergence speed is obvious in the iteration-based comparison although (hybrid) type-based sampling takes several times longer than collapsed sampling in the current na¨ ıve implementation. [sent-381, score-0.633]
</p><p>87 5 Discussion Figure 4 shows some segmentations corrected by the back-off mixing bigram model with hybrid typebased sampling. [sent-391, score-0.616]
</p><p>88 In Japanese, people often change the script to derive a proper noun from a common noun, which a na¨ ıve analyzer fails to recognize. [sent-396, score-0.361]
</p><p>89 As hiragana is mainly used to write function words and other basic words, segmentation errors concerning hiragana often bring disastrous effects on applications of morphological analysis. [sent-413, score-0.907]
</p><p>90 Most improvements come from correction of over-segmentation because the initial segmentation by the analyzer shows a tendency of oversegmentation. [sent-415, score-0.717]
</p><p>91 On the other hand, the segmentation failed when our assumption about constituents does not hold. [sent-418, score-0.487]
</p><p>92 We adopted nonparametric Bayesian language models and proposed hybrid type-based sampling that can efficiently correct segmentation given by the morphological analyzer. [sent-421, score-0.997]
</p><p>93 Although supervised segmentation is very competitive, we showed that it can be supplemented with our unsupervised approach. [sent-422, score-0.427]
</p><p>94 For example, in unknown word acquisition (Murawaki and Kurohashi, 2008), noun phrases are often acquired from text as single words. [sent-425, score-0.362]
</p><p>95 In the future we will assign a POS tag to each word in order to use segmented noun phrases in morphological analysis. [sent-427, score-0.48]
</p><p>96 Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling. [sent-513, score-0.427]
</p><p>97 Online acquisition of Japanese unknown morphemes using morphological constraints. [sent-518, score-0.356]
</p><p>98 Chinese segmentation and new word detection using conditional random fields. [sent-544, score-0.427]
</p><p>99 The unknown word problem: a morphological analysis of Japanese using maximum entropy aided by a dictionary. [sent-558, score-0.316]
</p><p>100 Bayesian semi-supervised Chinese word segmentation for statistical machine translation. [sent-563, score-0.427]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('segmentation', 0.427), ('sampling', 0.248), ('mixing', 0.22), ('analyzer', 0.207), ('japanese', 0.206), ('morphological', 0.2), ('bigram', 0.19), ('noun', 0.154), ('goldwater', 0.142), ('hiragana', 0.14), ('collapsed', 0.137), ('tsuneyama', 0.122), ('unknown', 0.116), ('unigram', 0.114), ('mochihashi', 0.102), ('murawaki', 0.102), ('zerogram', 0.102), ('boundary', 0.099), ('positions', 0.09), ('bm', 0.088), ('hybrid', 0.084), ('entry', 0.081), ('encyclopedias', 0.081), ('htb', 0.081), ('typebased', 0.081), ('proposal', 0.08), ('boundaries', 0.079), ('segmented', 0.074), ('kurohashi', 0.073), ('bayesian', 0.073), ('sampler', 0.071), ('asahara', 0.07), ('katakana', 0.07), ('gibbs', 0.07), ('title', 0.068), ('wi', 0.066), ('yuji', 0.063), ('escape', 0.063), ('kudo', 0.062), ('dictionary', 0.061), ('markup', 0.061), ('tsune', 0.061), ('constituents', 0.06), ('chinese', 0.059), ('hl', 0.059), ('ip', 0.055), ('hyperparameter', 0.055), ('initialization', 0.054), ('block', 0.053), ('acceptance', 0.052), ('phrases', 0.052), ('segment', 0.051), ('jp', 0.05), ('character', 0.049), ('skip', 0.047), ('initial', 0.047), ('phrase', 0.047), ('state', 0.047), ('tb', 0.046), ('annotated', 0.044), ('concentration', 0.044), ('iob', 0.044), ('liang', 0.043), ('optimum', 0.042), ('encyclopedic', 0.041), ('segmentations', 0.041), ('entries', 0.041), ('chin', 0.041), ('escobar', 0.041), ('iterat', 0.041), ('jr', 0.041), ('kansai', 0.041), ('misidentify', 0.041), ('shinsuke', 0.041), ('tsuboi', 0.041), ('yamashiro', 0.041), ('yugo', 0.041), ('acquisition', 0.04), ('hyperparameters', 0.04), ('matsumoto', 0.04), ('median', 0.039), ('hh', 0.039), ('external', 0.039), ('nonparametric', 0.038), ('dp', 0.038), ('wl', 0.037), ('sadao', 0.037), ('accepted', 0.036), ('sharon', 0.036), ('tendency', 0.036), ('gazetteer', 0.035), ('adaptor', 0.035), ('stick', 0.035), ('undesired', 0.035), ('kyot', 0.035), ('characterbased', 0.035), ('wiki', 0.035), ('station', 0.035), ('invariant', 0.035), ('juman', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999809 <a title="99-tfidf-1" href="./emnlp-2011-Non-parametric_Bayesian_Segmentation_of_Japanese_Noun_Phrases.html">99 emnlp-2011-Non-parametric Bayesian Segmentation of Japanese Noun Phrases</a></p>
<p>Author: Yugo Murawaki ; Sadao Kurohashi</p><p>Abstract: A key factor of high quality word segmentation for Japanese is a high-coverage dictionary, but it is costly to manually build such a lexical resource. Although external lexical resources for human readers are potentially good knowledge sources, they have not been utilized due to differences in segmentation criteria. To supplement a morphological dictionary with these resources, we propose a new task of Japanese noun phrase segmentation. We apply non-parametric Bayesian language models to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text. For inference, we propose a novel block sampling procedure named hybrid type-based sampling, which has the ability to directly escape a local optimum that is not too distant from the global optimum. Experiments show that the proposed method efficiently corrects the initial segmentation given by a morphological ana- lyzer.</p><p>2 0.20101565 <a title="99-tfidf-2" href="./emnlp-2011-Enhancing_Chinese_Word_Segmentation_Using_Unlabeled_Data.html">48 emnlp-2011-Enhancing Chinese Word Segmentation Using Unlabeled Data</a></p>
<p>Author: Weiwei Sun ; Jia Xu</p><p>Abstract: This paper investigates improving supervised word segmentation accuracy with unlabeled data. Both large-scale in-domain data and small-scale document text are considered. We present a unified solution to include features derived from unlabeled data to a discriminative learning model. For the large-scale data, we derive string statistics from Gigaword to assist a character-based segmenter. In addition, we introduce the idea about transductive, document-level segmentation, which is designed to improve the system recall for out-ofvocabulary (OOV) words which appear more than once inside a document. Novel features1 result in relative error reductions of 13.8% and 15.4% in terms of F-score and the recall of OOV words respectively.</p><p>3 0.18006976 <a title="99-tfidf-3" href="./emnlp-2011-Splitting_Noun_Compounds_via_Monolingual_and_Bilingual_Paraphrasing%3A_A_Study_on_Japanese_Katakana_Words.html">124 emnlp-2011-Splitting Noun Compounds via Monolingual and Bilingual Paraphrasing: A Study on Japanese Katakana Words</a></p>
<p>Author: Nobuhiro Kaji ; Masaru Kitsuregawa</p><p>Abstract: Word boundaries within noun compounds are not marked by white spaces in a number of languages, unlike in English, and it is beneficial for various NLP applications to split such noun compounds. In the case of Japanese, noun compounds made up of katakana words (i.e., transliterated foreign words) are particularly difficult to split, because katakana words are highly productive and are often outof-vocabulary. To overcome this difficulty, we propose using monolingual and bilingual paraphrases of katakana noun compounds for identifying word boundaries. Experiments demonstrated that splitting accuracy is substantially improved by extracting such paraphrases from unlabeled textual data, the Web in our case, and then using that information for constructing splitting models.</p><p>4 0.13043705 <a title="99-tfidf-4" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>Author: Christos Christodoulopoulos ; Sharon Goldwater ; Mark Steedman</p><p>Abstract: In this paper we present a fully unsupervised syntactic class induction system formulated as a Bayesian multinomial mixture model, where each word type is constrained to belong to a single class. By using a mixture model rather than a sequence model (e.g., HMM), we are able to easily add multiple kinds of features, including those at both the type level (morphology features) and token level (context and alignment features, the latter from parallel corpora). Using only context features, our system yields results comparable to state-of-the art, far better than a similar model without the one-class-per-type constraint. Using the additional features provides added benefit, and our final system outperforms the best published results on most of the 25 corpora tested.</p><p>5 0.12758477 <a title="99-tfidf-5" href="./emnlp-2011-Universal_Morphological_Analysis_using_Structured_Nearest_Neighbor_Prediction.html">140 emnlp-2011-Universal Morphological Analysis using Structured Nearest Neighbor Prediction</a></p>
<p>Author: Young-Bum Kim ; Joao Graca ; Benjamin Snyder</p><p>Abstract: In this paper, we consider the problem of unsupervised morphological analysis from a new angle. Past work has endeavored to design unsupervised learning methods which explicitly or implicitly encode inductive biases appropriate to the task at hand. We propose instead to treat morphological analysis as a structured prediction problem, where languages with labeled data serve as training examples for unlabeled languages, without the assumption of parallel data. We define a universal morphological feature space in which every language and its morphological analysis reside. We develop a novel structured nearest neighbor prediction method which seeks to find the morphological analysis for each unlabeled lan- guage which lies as close as possible in the feature space to a training language. We apply our model to eight inflecting languages, and induce nominal morphology with substantially higher accuracy than a traditional, MDLbased approach. Our analysis indicates that accuracy continues to improve substantially as the number of training languages increases.</p><p>6 0.1145964 <a title="99-tfidf-6" href="./emnlp-2011-Linear_Text_Segmentation_Using_Affinity_Propagation.html">88 emnlp-2011-Linear Text Segmentation Using Affinity Propagation</a></p>
<p>7 0.10817471 <a title="99-tfidf-7" href="./emnlp-2011-Discovering_Morphological_Paradigms_from_Plain_Text_Using_a_Dirichlet_Process_Mixture_Model.html">39 emnlp-2011-Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model</a></p>
<p>8 0.087913185 <a title="99-tfidf-8" href="./emnlp-2011-Joint_Models_for_Chinese_POS_Tagging_and_Dependency_Parsing.html">75 emnlp-2011-Joint Models for Chinese POS Tagging and Dependency Parsing</a></p>
<p>9 0.087150469 <a title="99-tfidf-9" href="./emnlp-2011-Named_Entity_Recognition_in_Tweets%3A_An_Experimental_Study.html">98 emnlp-2011-Named Entity Recognition in Tweets: An Experimental Study</a></p>
<p>10 0.072319746 <a title="99-tfidf-10" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>11 0.071999989 <a title="99-tfidf-11" href="./emnlp-2011-Relation_Acquisition_using_Word_Classes_and_Partial_Patterns.html">113 emnlp-2011-Relation Acquisition using Word Classes and Partial Patterns</a></p>
<p>12 0.057781879 <a title="99-tfidf-12" href="./emnlp-2011-Semantic_Topic_Models%3A_Combining_Word_Distributional_Statistics_and_Dictionary_Definitions.html">119 emnlp-2011-Semantic Topic Models: Combining Word Distributional Statistics and Dictionary Definitions</a></p>
<p>13 0.05216768 <a title="99-tfidf-13" href="./emnlp-2011-Statistical_Machine_Translation_with_Local_Language_Models.html">125 emnlp-2011-Statistical Machine Translation with Local Language Models</a></p>
<p>14 0.051638428 <a title="99-tfidf-14" href="./emnlp-2011-Analyzing_Methods_for_Improving_Precision_of_Pivot_Based_Bilingual_Dictionaries.html">18 emnlp-2011-Analyzing Methods for Improving Precision of Pivot Based Bilingual Dictionaries</a></p>
<p>15 0.049867641 <a title="99-tfidf-15" href="./emnlp-2011-Bootstrapped_Named_Entity_Recognition_for_Product_Attribute_Extraction.html">23 emnlp-2011-Bootstrapped Named Entity Recognition for Product Attribute Extraction</a></p>
<p>16 0.049652949 <a title="99-tfidf-16" href="./emnlp-2011-Unsupervised_Structure_Prediction_with_Non-Parallel_Multilingual_Guidance.html">146 emnlp-2011-Unsupervised Structure Prediction with Non-Parallel Multilingual Guidance</a></p>
<p>17 0.047790591 <a title="99-tfidf-17" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>18 0.047547668 <a title="99-tfidf-18" href="./emnlp-2011-Syntactic_Decision_Tree_LMs%3A_Random_Selection_or_Intelligent_Design%3F.html">131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</a></p>
<p>19 0.047369625 <a title="99-tfidf-19" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>20 0.047326166 <a title="99-tfidf-20" href="./emnlp-2011-Active_Learning_with_Amazon_Mechanical_Turk.html">17 emnlp-2011-Active Learning with Amazon Mechanical Turk</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.206), (1, -0.029), (2, -0.052), (3, -0.011), (4, -0.032), (5, 0.049), (6, -0.131), (7, 0.006), (8, -0.294), (9, 0.039), (10, 0.034), (11, 0.05), (12, -0.077), (13, 0.094), (14, -0.2), (15, -0.143), (16, -0.152), (17, -0.109), (18, 0.284), (19, 0.212), (20, -0.009), (21, -0.097), (22, -0.104), (23, 0.016), (24, -0.097), (25, -0.075), (26, -0.106), (27, 0.051), (28, -0.006), (29, 0.015), (30, 0.026), (31, -0.018), (32, 0.099), (33, -0.117), (34, -0.013), (35, 0.084), (36, 0.081), (37, -0.089), (38, 0.009), (39, -0.127), (40, -0.049), (41, 0.008), (42, 0.172), (43, 0.037), (44, -0.068), (45, 0.043), (46, -0.008), (47, -0.055), (48, 0.09), (49, 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97234648 <a title="99-lsi-1" href="./emnlp-2011-Non-parametric_Bayesian_Segmentation_of_Japanese_Noun_Phrases.html">99 emnlp-2011-Non-parametric Bayesian Segmentation of Japanese Noun Phrases</a></p>
<p>Author: Yugo Murawaki ; Sadao Kurohashi</p><p>Abstract: A key factor of high quality word segmentation for Japanese is a high-coverage dictionary, but it is costly to manually build such a lexical resource. Although external lexical resources for human readers are potentially good knowledge sources, they have not been utilized due to differences in segmentation criteria. To supplement a morphological dictionary with these resources, we propose a new task of Japanese noun phrase segmentation. We apply non-parametric Bayesian language models to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text. For inference, we propose a novel block sampling procedure named hybrid type-based sampling, which has the ability to directly escape a local optimum that is not too distant from the global optimum. Experiments show that the proposed method efficiently corrects the initial segmentation given by a morphological ana- lyzer.</p><p>2 0.74300474 <a title="99-lsi-2" href="./emnlp-2011-Enhancing_Chinese_Word_Segmentation_Using_Unlabeled_Data.html">48 emnlp-2011-Enhancing Chinese Word Segmentation Using Unlabeled Data</a></p>
<p>Author: Weiwei Sun ; Jia Xu</p><p>Abstract: This paper investigates improving supervised word segmentation accuracy with unlabeled data. Both large-scale in-domain data and small-scale document text are considered. We present a unified solution to include features derived from unlabeled data to a discriminative learning model. For the large-scale data, we derive string statistics from Gigaword to assist a character-based segmenter. In addition, we introduce the idea about transductive, document-level segmentation, which is designed to improve the system recall for out-ofvocabulary (OOV) words which appear more than once inside a document. Novel features1 result in relative error reductions of 13.8% and 15.4% in terms of F-score and the recall of OOV words respectively.</p><p>3 0.63907951 <a title="99-lsi-3" href="./emnlp-2011-Splitting_Noun_Compounds_via_Monolingual_and_Bilingual_Paraphrasing%3A_A_Study_on_Japanese_Katakana_Words.html">124 emnlp-2011-Splitting Noun Compounds via Monolingual and Bilingual Paraphrasing: A Study on Japanese Katakana Words</a></p>
<p>Author: Nobuhiro Kaji ; Masaru Kitsuregawa</p><p>Abstract: Word boundaries within noun compounds are not marked by white spaces in a number of languages, unlike in English, and it is beneficial for various NLP applications to split such noun compounds. In the case of Japanese, noun compounds made up of katakana words (i.e., transliterated foreign words) are particularly difficult to split, because katakana words are highly productive and are often outof-vocabulary. To overcome this difficulty, we propose using monolingual and bilingual paraphrases of katakana noun compounds for identifying word boundaries. Experiments demonstrated that splitting accuracy is substantially improved by extracting such paraphrases from unlabeled textual data, the Web in our case, and then using that information for constructing splitting models.</p><p>4 0.50485009 <a title="99-lsi-4" href="./emnlp-2011-Linear_Text_Segmentation_Using_Affinity_Propagation.html">88 emnlp-2011-Linear Text Segmentation Using Affinity Propagation</a></p>
<p>Author: Anna Kazantseva ; Stan Szpakowicz</p><p>Abstract: This paper presents a new algorithm for linear text segmentation. It is an adaptation of Affinity Propagation, a state-of-the-art clustering algorithm in the framework of factor graphs. Affinity Propagation for Segmentation, or APS, receives a set of pairwise similarities between data points and produces segment boundaries and segment centres data points which best describe all other data points within the segment. APS iteratively passes messages in a cyclic factor graph, until convergence. Each iteration works with information on all available similarities, resulting in highquality results. APS scales linearly for realistic segmentation tasks. We derive the algorithm from the original Affinity Propagation formu– lation, and evaluate its performance on topical text segmentation in comparison with two state-of-the art segmenters. The results suggest that APS performs on par with or outperforms these two very competitive baselines.</p><p>5 0.45384684 <a title="99-lsi-5" href="./emnlp-2011-Universal_Morphological_Analysis_using_Structured_Nearest_Neighbor_Prediction.html">140 emnlp-2011-Universal Morphological Analysis using Structured Nearest Neighbor Prediction</a></p>
<p>Author: Young-Bum Kim ; Joao Graca ; Benjamin Snyder</p><p>Abstract: In this paper, we consider the problem of unsupervised morphological analysis from a new angle. Past work has endeavored to design unsupervised learning methods which explicitly or implicitly encode inductive biases appropriate to the task at hand. We propose instead to treat morphological analysis as a structured prediction problem, where languages with labeled data serve as training examples for unlabeled languages, without the assumption of parallel data. We define a universal morphological feature space in which every language and its morphological analysis reside. We develop a novel structured nearest neighbor prediction method which seeks to find the morphological analysis for each unlabeled lan- guage which lies as close as possible in the feature space to a training language. We apply our model to eight inflecting languages, and induce nominal morphology with substantially higher accuracy than a traditional, MDLbased approach. Our analysis indicates that accuracy continues to improve substantially as the number of training languages increases.</p><p>6 0.43674898 <a title="99-lsi-6" href="./emnlp-2011-Discovering_Morphological_Paradigms_from_Plain_Text_Using_a_Dirichlet_Process_Mixture_Model.html">39 emnlp-2011-Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model</a></p>
<p>7 0.42329344 <a title="99-lsi-7" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>8 0.29766867 <a title="99-lsi-8" href="./emnlp-2011-Named_Entity_Recognition_in_Tweets%3A_An_Experimental_Study.html">98 emnlp-2011-Named Entity Recognition in Tweets: An Experimental Study</a></p>
<p>9 0.28077865 <a title="99-lsi-9" href="./emnlp-2011-Bootstrapped_Named_Entity_Recognition_for_Product_Attribute_Extraction.html">23 emnlp-2011-Bootstrapped Named Entity Recognition for Product Attribute Extraction</a></p>
<p>10 0.26073304 <a title="99-lsi-10" href="./emnlp-2011-A_Cascaded_Classification_Approach_to_Semantic_Head_Recognition.html">2 emnlp-2011-A Cascaded Classification Approach to Semantic Head Recognition</a></p>
<p>11 0.24621654 <a title="99-lsi-11" href="./emnlp-2011-Large-Scale_Noun_Compound_Interpretation_Using_Bootstrapping_and_the_Web_as_a_Corpus.html">78 emnlp-2011-Large-Scale Noun Compound Interpretation Using Bootstrapping and the Web as a Corpus</a></p>
<p>12 0.2378787 <a title="99-lsi-12" href="./emnlp-2011-Minimum_Imputed-Risk%3A_Unsupervised_Discriminative_Training_for_Machine_Translation.html">93 emnlp-2011-Minimum Imputed-Risk: Unsupervised Discriminative Training for Machine Translation</a></p>
<p>13 0.22992511 <a title="99-lsi-13" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>14 0.22500969 <a title="99-lsi-14" href="./emnlp-2011-Joint_Models_for_Chinese_POS_Tagging_and_Dependency_Parsing.html">75 emnlp-2011-Joint Models for Chinese POS Tagging and Dependency Parsing</a></p>
<p>15 0.22044644 <a title="99-lsi-15" href="./emnlp-2011-Approximate_Scalable_Bounded_Space_Sketch_for_Large_Data_NLP.html">19 emnlp-2011-Approximate Scalable Bounded Space Sketch for Large Data NLP</a></p>
<p>16 0.2160053 <a title="99-lsi-16" href="./emnlp-2011-Unsupervised_Information_Extraction_with_Distributional_Prior_Knowledge.html">143 emnlp-2011-Unsupervised Information Extraction with Distributional Prior Knowledge</a></p>
<p>17 0.20842959 <a title="99-lsi-17" href="./emnlp-2011-Identification_of_Multi-word_Expressions_by_Combining_Multiple_Linguistic_Information_Sources.html">69 emnlp-2011-Identification of Multi-word Expressions by Combining Multiple Linguistic Information Sources</a></p>
<p>18 0.20179939 <a title="99-lsi-18" href="./emnlp-2011-Multilayer_Sequence_Labeling.html">96 emnlp-2011-Multilayer Sequence Labeling</a></p>
<p>19 0.20107 <a title="99-lsi-19" href="./emnlp-2011-Relation_Acquisition_using_Word_Classes_and_Partial_Patterns.html">113 emnlp-2011-Relation Acquisition using Word Classes and Partial Patterns</a></p>
<p>20 0.19835757 <a title="99-lsi-20" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(15, 0.387), (23, 0.124), (36, 0.034), (37, 0.017), (45, 0.074), (53, 0.018), (54, 0.028), (57, 0.034), (62, 0.015), (64, 0.024), (66, 0.039), (69, 0.015), (79, 0.044), (82, 0.016), (87, 0.012), (90, 0.011), (96, 0.024), (98, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79482591 <a title="99-lda-1" href="./emnlp-2011-Non-parametric_Bayesian_Segmentation_of_Japanese_Noun_Phrases.html">99 emnlp-2011-Non-parametric Bayesian Segmentation of Japanese Noun Phrases</a></p>
<p>Author: Yugo Murawaki ; Sadao Kurohashi</p><p>Abstract: A key factor of high quality word segmentation for Japanese is a high-coverage dictionary, but it is costly to manually build such a lexical resource. Although external lexical resources for human readers are potentially good knowledge sources, they have not been utilized due to differences in segmentation criteria. To supplement a morphological dictionary with these resources, we propose a new task of Japanese noun phrase segmentation. We apply non-parametric Bayesian language models to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text. For inference, we propose a novel block sampling procedure named hybrid type-based sampling, which has the ability to directly escape a local optimum that is not too distant from the global optimum. Experiments show that the proposed method efficiently corrects the initial segmentation given by a morphological ana- lyzer.</p><p>2 0.75067329 <a title="99-lda-2" href="./emnlp-2011-Semi-Supervised_Recursive_Autoencoders_for_Predicting_Sentiment_Distributions.html">120 emnlp-2011-Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</a></p>
<p>Author: Richard Socher ; Jeffrey Pennington ; Eric H. Huang ; Andrew Y. Ng ; Christopher D. Manning</p><p>Abstract: We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model’s ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.</p><p>3 0.58024341 <a title="99-lda-3" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>Author: Amit Dubey ; Frank Keller ; Patrick Sturt</p><p>Abstract: This paper introduces a psycholinguistic model of sentence processing which combines a Hidden Markov Model noun phrase chunker with a co-reference classifier. Both models are fully incremental and generative, giving probabilities of lexical elements conditional upon linguistic structure. This allows us to compute the information theoretic measure of surprisal, which is known to correlate with human processing effort. We evaluate our surprisal predictions on the Dundee corpus of eye-movement data show that our model achieve a better fit with human reading times than a syntax-only model which does not have access to co-reference information.</p><p>4 0.43846118 <a title="99-lda-4" href="./emnlp-2011-Splitting_Noun_Compounds_via_Monolingual_and_Bilingual_Paraphrasing%3A_A_Study_on_Japanese_Katakana_Words.html">124 emnlp-2011-Splitting Noun Compounds via Monolingual and Bilingual Paraphrasing: A Study on Japanese Katakana Words</a></p>
<p>Author: Nobuhiro Kaji ; Masaru Kitsuregawa</p><p>Abstract: Word boundaries within noun compounds are not marked by white spaces in a number of languages, unlike in English, and it is beneficial for various NLP applications to split such noun compounds. In the case of Japanese, noun compounds made up of katakana words (i.e., transliterated foreign words) are particularly difficult to split, because katakana words are highly productive and are often outof-vocabulary. To overcome this difficulty, we propose using monolingual and bilingual paraphrases of katakana noun compounds for identifying word boundaries. Experiments demonstrated that splitting accuracy is substantially improved by extracting such paraphrases from unlabeled textual data, the Web in our case, and then using that information for constructing splitting models.</p><p>5 0.43652767 <a title="99-lda-5" href="./emnlp-2011-Discovering_Morphological_Paradigms_from_Plain_Text_Using_a_Dirichlet_Process_Mixture_Model.html">39 emnlp-2011-Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model</a></p>
<p>Author: Markus Dreyer ; Jason Eisner</p><p>Abstract: We present an inference algorithm that organizes observed words (tokens) into structured inflectional paradigms (types). It also naturally predicts the spelling of unobserved forms that are missing from these paradigms, and discovers inflectional principles (grammar) that generalize to wholly unobserved words. Our Bayesian generative model of the data explicitly represents tokens, types, inflections, paradigms, and locally conditioned string edits. It assumes that inflected word tokens are generated from an infinite mixture of inflectional paradigms (string tuples). Each paradigm is sampled all at once from a graphical model, whose potential functions are weighted finitestate transducers with language-specific parameters to be learned. These assumptions naturally lead to an elegant empirical Bayes inference procedure that exploits Monte Carlo EM, belief propagation, and dynamic programming. Given 50–100 seed paradigms, adding a 10million-word corpus reduces prediction error for morphological inflections by up to 10%.</p><p>6 0.42859969 <a title="99-lda-6" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>7 0.41282722 <a title="99-lda-7" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>8 0.41231754 <a title="99-lda-8" href="./emnlp-2011-Structural_Opinion_Mining_for_Graph-based_Sentiment_Representation.html">126 emnlp-2011-Structural Opinion Mining for Graph-based Sentiment Representation</a></p>
<p>9 0.40306693 <a title="99-lda-9" href="./emnlp-2011-Multiword_Expression_Identification_with_Tree_Substitution_Grammars%3A_A_Parsing_tour_de_force_with_French.html">97 emnlp-2011-Multiword Expression Identification with Tree Substitution Grammars: A Parsing tour de force with French</a></p>
<p>10 0.40168613 <a title="99-lda-10" href="./emnlp-2011-Rumor_has_it%3A_Identifying_Misinformation_in_Microblogs.html">117 emnlp-2011-Rumor has it: Identifying Misinformation in Microblogs</a></p>
<p>11 0.40127274 <a title="99-lda-11" href="./emnlp-2011-Harnessing_WordNet_Senses_for_Supervised_Sentiment_Classification.html">63 emnlp-2011-Harnessing WordNet Senses for Supervised Sentiment Classification</a></p>
<p>12 0.39627576 <a title="99-lda-12" href="./emnlp-2011-Large-Scale_Cognate_Recovery.html">77 emnlp-2011-Large-Scale Cognate Recovery</a></p>
<p>13 0.3936165 <a title="99-lda-13" href="./emnlp-2011-Personalized_Recommendation_of_User_Comments_via_Factor_Models.html">104 emnlp-2011-Personalized Recommendation of User Comments via Factor Models</a></p>
<p>14 0.39011103 <a title="99-lda-14" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>15 0.38946918 <a title="99-lda-15" href="./emnlp-2011-Unsupervised_Discovery_of_Discourse_Relations_for_Eliminating_Intra-sentence_Polarity_Ambiguities.html">142 emnlp-2011-Unsupervised Discovery of Discourse Relations for Eliminating Intra-sentence Polarity Ambiguities</a></p>
<p>16 0.38942751 <a title="99-lda-16" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>17 0.38848245 <a title="99-lda-17" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>18 0.38709751 <a title="99-lda-18" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>19 0.38677585 <a title="99-lda-19" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>20 0.38471842 <a title="99-lda-20" href="./emnlp-2011-Named_Entity_Recognition_in_Tweets%3A_An_Experimental_Study.html">98 emnlp-2011-Named Entity Recognition in Tweets: An Experimental Study</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
