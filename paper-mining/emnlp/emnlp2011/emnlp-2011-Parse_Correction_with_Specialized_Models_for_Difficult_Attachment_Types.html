<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>102 emnlp-2011-Parse Correction with Specialized Models for Difficult Attachment Types</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-102" href="#">emnlp2011-102</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>102 emnlp-2011-Parse Correction with Specialized Models for Difficult Attachment Types</h1>
<br/><p>Source: <a title="emnlp-2011-102-pdf" href="http://aclweb.org/anthology//D/D11/D11-1113.pdf">pdf</a></p><p>Author: Enrique Henestroza Anguiano ; Marie Candito</p><p>Abstract: This paper develops a framework for syntactic dependency parse correction. Dependencies in an input parse tree are revised by selecting, for a given dependent, the best governor from within a small set of candidates. We use a discriminative linear ranking model to select the best governor from a group of candidates for a dependent, and our model includes a rich feature set that encodes syntactic structure in the input parse tree. The parse correction framework is parser-agnostic, and can correct attachments using either a generic model or specialized models tailored to difficult attachment types like coordination and pp-attachment. Our experiments show that parse correction, combining a generic model with specialized models for difficult attachment types, can successfully improve the quality of predicted parse trees output by sev- eral representative state-of-the-art dependency parsers for French.</p><p>Reference: <a title="emnlp-2011-102-reference" href="../emnlp2011_reference/emnlp-2011-Parse_Correction_with_Specialized_Models_for_Difficult_Attachment_Types_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 fr ,  marie  Abstract This paper develops a framework for syntactic dependency parse correction. [sent-2, score-0.332]
</p><p>2 Dependencies in an input parse tree are revised by selecting, for a given dependent, the best governor from within a small set of candidates. [sent-3, score-0.645]
</p><p>3 We use a discriminative linear ranking model to select the best governor from a group of candidates for a dependent, and our model includes a rich feature set that encodes syntactic structure in the input parse tree. [sent-4, score-0.623]
</p><p>4 The parse correction framework is parser-agnostic, and can correct attachments using either a generic model or specialized models tailored to difficult attachment types like coordination and pp-attachment. [sent-5, score-1.306]
</p><p>5 Our experiments show that parse correction, combining a generic model with specialized models for difficult attachment types, can successfully improve the quality of predicted parse trees output by sev-  eral representative state-of-the-art dependency parsers for French. [sent-6, score-1.187]
</p><p>6 1 Introduction In syntactic dependency parse correction, attachments in an input parse tree are revised by selecting, for a given dependent, the best governor from within a small set of candidates. [sent-7, score-1.14]
</p><p>7 The motivation behind parse correction is that attachment decisions, especially traditionally difficult ones like pp-attachment and coordination, may require substantial contextual information in order to be made accurately. [sent-8, score-0.754]
</p><p>8 Because syntactic dependency parsers predict the parse tree for an entire sentence, they may not be able to take 1222 . [sent-9, score-0.458]
</p><p>9 Assuming nonetheless that a predicted parse tree is mostly accurate, parse correction can revise difficult attachments by using the predicted tree’s syntactic structure to restrict the set of candidate governors and extract a rich set of features to help select among  them. [sent-13, score-1.549]
</p><p>10 Parse correction is also appealing because it is parser-agnostic: it can be trained to correct the output of any dependency parser. [sent-14, score-0.506]
</p><p>11 In Section 2 we discuss work related to parse correction, pp-attachment and coordination resolution. [sent-15, score-0.405]
</p><p>12 In Section 3 we discuss dependency structure and various statistical dependency parsing approaches. [sent-16, score-0.3]
</p><p>13 In Section 4 we introduce the parse correction framework, and Section 5 describes the features and learning model used in our implementation. [sent-17, score-0.578]
</p><p>14 In Section 6 we present experiments in which parse correction revises the predicted parse trees of four state-of-the-art dependency parsers for French. [sent-18, score-1.163]
</p><p>15 Hall and Nov a´k (2005), working on Czech, define a neighborhood of candidate governors centered around the original governor of a dependent, and a Maximum Entropy model determines the probability of each candidate-dependent attachment. [sent-24, score-0.734]
</p><p>16 Attardi and Dell’Orletta (2009) investigate reverse revision: a left-to-right transition-based model is first used to parse a sentence, then a right-to-left transition-based model is run with additional features taken from the left-toright model’s predicted parse. [sent-28, score-0.323]
</p><p>17 While their approach is similar to parse correction in that it uses a predicted parse to inform a subsequent processing step, this information is used to improve a second parser rather than a model for correcting errors. [sent-30, score-0.937]
</p><p>18 McDonald and Pereira (2006) consider a method for recovering non-projective attachments from a graph representation of a sentence, in which an optimal projective parse tree has been identified. [sent-31, score-0.496]
</p><p>19 As a process that revises the output of a syntactic parser, parse reranking is also similar to parse correction. [sent-34, score-0.561]
</p><p>20 the work of Charniak and Johnson (2005) and of Collins and Koo (2005)), parse reranking is concerned with the reordering of n-best ranked parse trees output by  a syntactic parser. [sent-37, score-0.578]
</p><p>21 Parse correction has a number of advantages compared to reranking: it can be 1223 used with parsers that do not output n-best ranked parses, it can be easily restricted to specific attachment types, and its output space of parse trees is not limited to those appearing in an n-best list. [sent-38, score-0.912]
</p><p>22 However, parse reranking has the advantage of selecting the globally optimal parse for a sentence from an nbest list, while parse correction makes only locally optimal revisions in the predicted parse for a sentence. [sent-39, score-1.398]
</p><p>23 Examples consist oftuples ofthe form (v, n1,p, n2), where either v or n1 is the true governor of the pp comprising p and n2, and the task is to choose between v and n1. [sent-42, score-0.376]
</p><p>24 With parse correction, candidate governors are identified automatically with no (v, n1,p, n2) restriction, and for several representative parsers we find that parse correction improves pp-attachment performance. [sent-44, score-1.122]
</p><p>25 Resnik (1999) uses semantic similarity to resolve nounphrase coordination of the form (n1, cc, n2, n3), where the coordinating conjunction cc coordinates either the heads n1 and n2 or the heads n1 and n3. [sent-46, score-0.493]
</p><p>26 Finally, other work has focused on introducing specialized features for coordination into existing syn-  tactic parsing models (Hogan, 2007). [sent-50, score-0.398]
</p><p>27 Our approach is novel with respect to previous work by directly modeling the correction of coordination errors made by general-purpose dependency parsers. [sent-51, score-0.658]
</p><p>28 ouvrit  Elle  porte  avec  la  cl´ e  la Figure 1: An unlabeled dependency tree for: Elle ouvrit la porte avec la cl´ e. [sent-52, score-0.347]
</p><p>29 An edge from a governor to a dependent indicates, roughly, that the presence of the dependent is syntactically legitimated by the governor. [sent-55, score-0.573]
</p><p>30 Additionally, an alternative method for obtaining the dependency parse for a sentence is to parse the sentence with a constituency-based parser and then use an automatic process to convert the output into dependency structure. [sent-63, score-0.734]
</p><p>31 (2005), the parsing process selects the globally optimal parse tree from a graph containing attachments (directed edges) between each pair of words (nodes) in a sentence. [sent-75, score-0.539]
</p><p>32 It finds the k-best scoring parse trees, both during training and at parse time, where the score of a tree is the sum of the scores of its factors (consisting of one or more linked edges). [sent-76, score-0.584]
</p><p>33 3 Constituency-Based Parsing Beyond the two main approaches to dependency parsing, there is also the approach of constituencybased parsing followed by a conversion step to dependency structure. [sent-80, score-0.3]
</p><p>34 (2010a): (i) A constituency parse tree is output by the BerkeleyParser, which has been trained to learn a probabilistic context-free grammar with latent annotations (Petrov et al. [sent-82, score-0.307]
</p><p>35 1225  INPUT: Predicted parse tree T LOOP: For each chosen dependent d ∈ D • Identify candidates Cd from T  • Predict cˆ = acrg ∈m CadxS(c,d,T) Update T{gov(d) ← c} OUTPUT: Corrected version of parse tree T •  Figure 2: The parse correction algorithm. [sent-99, score-1.27]
</p><p>36 4  Parse Correction  The parse correction algorithm is a post-processing step to dependency parsing, where attachments from the predicted parse tree of a sentence are corrected by considering alternative candidate governors for each dependent. [sent-106, score-1.554]
</p><p>37 The input is the predicted parse T of a sentence. [sent-108, score-0.323]
</p><p>38 From T a set D of dependent nodes are chosen for attachment correction. [sent-109, score-0.263]
</p><p>39 se Ft Cd oafc hcan dd ∈ida Dte governors from T is identified, and then the highest scoring c ∈ Cd, using a function S(c, d, T), is assigned as tche ∈ new governor of d in T. [sent-111, score-0.6]
</p><p>40 Pseudo-code for parse correction is shown in Figure 2. [sent-112, score-0.578]
</p><p>41 However, one advantage of parse correction is its ability to focus on specific attachment types, so an additional criterion for choosing dependents is to look separately at those dependents that correspond to difficult attachment types. [sent-117, score-1.207]
</p><p>42 Analyzing errors made by the dependency parsers introduced in Section 3 on the development set of the FTB, we observe that two major sources of error across different parsers are coordination and ppattachment. [sent-118, score-0.443]
</p><p>43 Coordination accounts for around 10% of incorrect attachments and has an error rate ranging from 30 − 40%, while pp-attachment accounts finorg a frrooumnd 3 030 −% 4 o0f% %in,c worhrielect papt-taatcthamchemntesn atn adc hcoasu an error rate of around 15%. [sent-119, score-0.279]
</p><p>44 Given the FTB annotation scheme, coordination can be corrected by changing the governor (first conjunct) of the coordinating conjunction that governs the second conjunct, and pp-  attachment can be corrected by changing the governor of the preposition that heads the pp. [sent-121, score-1.505]
</p><p>45 4 We thus train specialized corrective models for when the dependents are coordinating conjunctions and prepositions, in addition to a generic corrective model that can be applied to any dependent. [sent-122, score-1.002]
</p><p>46 2 Identifying Candidate Governors The set of candidate governors Cd for a dependent d can be chosen in different ways. [sent-124, score-0.369]
</p><p>47 One method is to let every other node in T be a candidate governor for d. [sent-125, score-0.383]
</p><p>48 Hall and Nov a´k (2005) define a neighborhood as a set of nodes Nm(d) around the original predicted governor co of d, where Nm(d) includes all nodes in the 4The FTB handles pp-attachment in a typical fashion, but coordination may be handled differently by other schemes (e. [sent-127, score-0.771]
</p><p>49 1226  parse tree T within graph distance m of d that pass through co. [sent-133, score-0.273]
</p><p>50 They find that around 2/3 of the incorrect attachments in the output ofCzech parses can be corrected by selecting the best governor from within N3 (d). [sent-134, score-0.697]
</p><p>51 Similarly, in oracle experiments reported in section 6, we find that around 1/2 of coordination and pp-attachments in the output of French parses can be corrected by selecting the best governor from within N3 (d). [sent-135, score-0.793]
</p><p>52 3 Scoring Candidate Governors A new governor for a dependent d is predicted by selecting the highest scoring candidate c ∈ Cd according gto th a fhuignhcetisotn s S(c, d, T), iwdahtiech c ta ∈k Ces into  c  account features over c, d, and the parse tree T. [sent-141, score-0.924]
</p><p>53 4 Algorithm Complexity The time complexity of our algorithm is O(n) in the length n of the input sentence, which is consistent with past work on parse correction by Hall and Nov a´k (2005) and by Attardi and Ciaramita (2007). [sent-145, score-0.578]
</p><p>54 For each such dependent d, the algorithm uses a linear model to select a new governor after extracting features for a local set of candidate governors Cd, whose size does not dependent on n in the average case. [sent-149, score-0.826]
</p><p>55 7 Locality in candidate governor identification and feature extraction preserves linear time complexity in the overall algorithm. [sent-150, score-0.383]
</p><p>56 We extract separate training sets for each type of dependent we wish to correct (generic, prepositions, coordinating conjunctions). [sent-154, score-0.28]
</p><p>57 Given p, then for each token d we wish to correct in a sentence in the training section, we note its true governor gd in the gold  parse tree of the sentence, identify a set of candidate governors Cd in the predicted parse T, and get feature vectors {Φ(c, d, T) : c ∈ Cd}. [sent-155, score-1.225]
</p><p>58 2 Feature Space In order to learn an effective scoring function, we use a rich feature space F that encodes syntactic context surrounding a candidate-dependent pair (c, d) within a parse tree T. [sent-157, score-0.321]
</p><p>59 flat trees) could lead to cases where |Cd |=n, but for linguistically coherent parse trees |Cd | is rwathheerre O(km), wbuhte froer k li nisg uthiset average -arity pofa syntactic parse trees and m is the neighborhood distance used. [sent-163, score-0.614]
</p><p>60 This is typically used in parse reranking (Charniak and Johnson, 2005), where for each sentence the model must select the correct parse from within an n-best list. [sent-181, score-0.497]
</p><p>61 , |X | · Get feature vectors { x~t,c : c ∈ Cdt } · GGeett tfreuaetu governor gt ∈x Cdt · LGeett h trtu =e g argmax (∈ w~ Ct−1 · xt,c) c∈Cdt −{gt} · Let mt = ( w~t−1 · xt,gt ) − ( w~t−1 · xt,ht ) IF: mt < 1  w~ avg  · Let τt= min‰C , k~ xt,g1t−−~m xtt,htk2? [sent-192, score-0.382]
</p><p>62 9 In our ranking approach to parse correction (PARanking), the weight vector is trained to select the true governor from a set of candidates Cd for a dependent d. [sent-195, score-1.133]
</p><p>63 The training set X is defined such that the tth instance is a collection of feature vectors { x~t,c = Φ(c, dt, Tt) : c ∈ Cdt }, where Cdt is the c{ ~xandidate set for th)e dependent dt hweirthein C the predicted parse Tt, and the class is the true governor gt. [sent-196, score-0.815]
</p><p>64 10 For each training iteration t, the margin is defined as 9We considered a binary training approach to parse correction in which the model is trained to independently classify candidates as true or false governors, as used by Hall and Nov a´k (2005). [sent-200, score-0.643]
</p><p>65 6  Experiments  We present experiments where we applied parse correction to the output of four state-of-the-art dependency parsers for French. [sent-211, score-0.797]
</p><p>66 To train our parse correction models, we generated specialized training sets corresponding to each parser by doing 10-fold jackknifing on the FTB training set (cf. [sent-213, score-0.782]
</p><p>67 Each parser was run on the FTB dev and test sets, providing baseline unlabeled attachment score (UAS) results and output parse trees to be corrected. [sent-216, score-0.52]
</p><p>68 1 Oracles and Neighborhood Size To determine candidate neighborhood size, we considered an oracle scoring function that always selects the true governor of a dependent if it appears  in the set of candidate governors, and otherwise selects the predicted governor. [sent-218, score-0.941]
</p><p>69 4 Table 1: Parse correction oracle UAS (%) for different neighborhood sizes, by dependent type (coordinating conjunctions, prepositions, or all dependents). [sent-240, score-0.63]
</p><p>70 We also compared the oracle for parse correction with an oracle for parse reranking, in which the parse with the highest UAS for a sentence is selected from the top-100 parses output by MSTParser. [sent-242, score-1.227]
</p><p>71 We found that for MSTParser, the oracle for parse correction using neighborhood size m=3 (95. [sent-243, score-0.733]
</p><p>72 6% UAS) is comparable to the oracle for parse reranking using the top-100 parses (95. [sent-244, score-0.382]
</p><p>73 This is an encouraging result, showing that parse correction is capable of the same improvement as parse reranking without  needing to process an n-best list of parses. [sent-246, score-0.856]
</p><p>74 We found that P=3 with CE-reduction allowed for the most compactness without sacrificing correction performance, for all of our corrective models. [sent-249, score-0.585]
</p><p>75 Additionally, K=2 worked well for the coordinating conjunction models, while K=10 worked well for the preposition and generic models. [sent-250, score-0.352]
</p><p>76 023456*  Table 2: Coordinating conjunction, preposition, and overall UAS (%) by corrective configuration on the test set. [sent-264, score-0.283]
</p><p>77 The PA-Ranking aggressiveness parameter C was  set to 1 for our experiments, while the rounds parameter R was tuned separately for each corrective model using the dev set. [sent-268, score-0.333]
</p><p>78 For our final tests, we applied each combination of parser + corrective configuration by sequentially revising all dependents in the output parse that had a relevant POS tag given the corrective configuration. [sent-269, score-0.951]
</p><p>79 In the FTB test set, this amounted to an evaluation over 5,706 preposition tokens, 801 coordinating conjunction tokens, and 31,404 overall (non-punctuation) tokens. [sent-270, score-0.287]
</p><p>80 The overall UAS of each parser (except BohnetParser) was significantly improved under both corrective configurations. [sent-273, score-0.262]
</p><p>81 tion performed as well as, and in most cases better than, the generic configuration, indicating the usefulness of specialized models and features for  difficult attachment types. [sent-277, score-0.379]
</p><p>82 Interestingly, the lower the baseline parser’s UAS, the larger the overall improvement from parse correction under the specialized configuration: MaltParser had the lowest baseline and the highest error reduction (6. [sent-278, score-0.716]
</p><p>83 It may be that the additional errors made by a low-baseline parser, compared to a high-baseline parser, involve relatively simpler attachments that parse correction can better model. [sent-283, score-0.741]
</p><p>84 Parse correction achieved significant improvements for coordination resolution under the specialized configuration for each parser. [sent-284, score-0.74]
</p><p>85 The result for MSTParser was  surprising: although it had the second-highest baseline overall UAS, it shared the lowest baseline coordinating conjunction UAS and had the highest error reduction with parse correction. [sent-291, score-0.47]
</p><p>86 An explanation for this result is that the annotation scheme for coordination structure in the dependency FTB has the first conjunct governing the coordinating conjunction, which governs the second conjunct. [sent-292, score-0.533]
</p><p>87 BohnetParser, which uses general 2-edge factors, can consider full coordination structures and consequently has a much higher baseline coordinating conjunction UAS than MSTParser. [sent-295, score-0.437]
</p><p>88 Parse correction achieved significant but modest improvements in pp-attachment performance under the specialized configuration for MaltParser and BerkeleyParser. [sent-296, score-0.554]
</p><p>89 However, parse correction did not significantly improve pp-attachment performance for MSTParser or BohnetParser, the two parsers that had the highest baseline preposition UAS (around 1230  w→cMco→diwficatwio→n TwypeMods  Berkel yCOPovre opr dasl 214218 06317491034 31 103. [sent-297, score-0.731]
</p><p>90 In addition to evaluating UAS improvements for parse correction, we took a closer look at the best corrective configuration (specialized) and analyzed the types of attachment modifications made (Table 3). [sent-310, score-0.687]
</p><p>91 Parse correction is thus conservative in the number of modifications made, and rather accurate when it does decide to modify an attachment. [sent-312, score-0.359]
</p><p>92 A rough version of parse correction in the specialized configuration took around 200s (for each parser). [sent-316, score-0.869]
</p><p>93 An interesting result is that parse correction improves MaltParser the most while retaining an overall time complexity of O(n), compared to O(n3) or higher for the other parsers. [sent-317, score-0.578]
</p><p>94 This suggests that linear-time transition-based parsing and parse correction could combine to form an attractive system that improves parsing performance while retaining high speed. [sent-318, score-0.726]
</p><p>95 7  Conclusion  We have developed a parse correction framework for syntactic dependency parsing that uses specialized models for difficult attachment types. [sent-319, score-1.079]
</p><p>96 Candidate governors for a given dependent are identified in a neighborhood around the predicted governor, and a scoring function selects the best governor. [sent-320, score-0.648]
</p><p>97 We used discriminative linear ranking models with features encoding syntactic context, and we tested parse cor-  rection on coordination, pp-attachment, and generic dependencies in the outputs of four representative statistical dependency parsers for French. [sent-321, score-0.502]
</p><p>98 Parse correction achieved improvements in unlabeled attachment score for three out of the four parsers, with MaltParser seeing the greatest improvement. [sent-322, score-0.506]
</p><p>99 Since both MaltParser and parse correction run in O(n) time, a combined system could prove useful in situations where high parsing speed is required. [sent-323, score-0.652]
</p><p>100 Future work on parse correction might focus on developing specialized models for other difficult attachment types, such as verb-phrase attachment (verb dependents account for around 15% of incorrect attachments across all four parsers). [sent-324, score-1.413]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('correction', 0.359), ('governor', 0.341), ('corrective', 0.226), ('parse', 0.219), ('governors', 0.211), ('coordination', 0.186), ('uas', 0.184), ('cd', 0.174), ('coordinating', 0.164), ('attachments', 0.163), ('dependents', 0.153), ('bohnetparser', 0.151), ('attachment', 0.147), ('specialized', 0.138), ('ftb', 0.13), ('mstparser', 0.123), ('dependent', 0.116), ('dependency', 0.113), ('predicted', 0.104), ('berkeleyparser', 0.104), ('maltparser', 0.101), ('combo', 0.091), ('candito', 0.091), ('nov', 0.091), ('cdt', 0.09), ('conjunction', 0.087), ('neighborhood', 0.082), ('attardi', 0.078), ('parsing', 0.074), ('french', 0.073), ('oracle', 0.073), ('parsers', 0.072), ('corrected', 0.07), ('generic', 0.065), ('projective', 0.06), ('combos', 0.06), ('projectivity', 0.06), ('reranking', 0.059), ('around', 0.058), ('denis', 0.058), ('revision', 0.058), ('configuration', 0.057), ('tree', 0.054), ('prepositions', 0.052), ('scoring', 0.048), ('trees', 0.047), ('atterer', 0.045), ('berkel', 0.045), ('dasl', 0.045), ('inria', 0.045), ('revise', 0.045), ('pos', 0.044), ('factors', 0.044), ('candidate', 0.042), ('avg', 0.041), ('benchmarking', 0.039), ('aggressiveness', 0.039), ('hall', 0.039), ('took', 0.038), ('dev', 0.037), ('mcdonald', 0.037), ('preposition', 0.036), ('parser', 0.036), ('linearly', 0.036), ('conjunct', 0.035), ('governs', 0.035), ('shimbo', 0.035), ('true', 0.035), ('output', 0.034), ('coordinate', 0.034), ('ranking', 0.033), ('corrects', 0.033), ('hara', 0.033), ('ciaramita', 0.033), ('michigan', 0.033), ('parses', 0.031), ('revised', 0.031), ('buffer', 0.031), ('rounds', 0.031), ('conjunctions', 0.03), ('avec', 0.03), ('cgov', 0.03), ('elle', 0.03), ('henestroza', 0.03), ('incrementing', 0.03), ('jackknifing', 0.03), ('lefff', 0.03), ('olteanu', 0.03), ('ouvrit', 0.03), ('padsl', 0.03), ('pdasl', 0.03), ('polka', 0.03), ('porte', 0.03), ('revises', 0.03), ('sagot', 0.03), ('candidates', 0.03), ('difficult', 0.029), ('dobj', 0.029), ('selects', 0.029), ('heads', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="102-tfidf-1" href="./emnlp-2011-Parse_Correction_with_Specialized_Models_for_Difficult_Attachment_Types.html">102 emnlp-2011-Parse Correction with Specialized Models for Difficult Attachment Types</a></p>
<p>Author: Enrique Henestroza Anguiano ; Marie Candito</p><p>Abstract: This paper develops a framework for syntactic dependency parse correction. Dependencies in an input parse tree are revised by selecting, for a given dependent, the best governor from within a small set of candidates. We use a discriminative linear ranking model to select the best governor from a group of candidates for a dependent, and our model includes a rich feature set that encodes syntactic structure in the input parse tree. The parse correction framework is parser-agnostic, and can correct attachments using either a generic model or specialized models tailored to difficult attachment types like coordination and pp-attachment. Our experiments show that parse correction, combining a generic model with specialized models for difficult attachment types, can successfully improve the quality of predicted parse trees output by sev- eral representative state-of-the-art dependency parsers for French.</p><p>2 0.16026253 <a title="102-tfidf-2" href="./emnlp-2011-A_Fast%2C_Accurate%2C_Non-Projective%2C_Semantically-Enriched_Parser.html">4 emnlp-2011-A Fast, Accurate, Non-Projective, Semantically-Enriched Parser</a></p>
<p>Author: Stephen Tratz ; Eduard Hovy</p><p>Abstract: Dependency parsers are critical components within many NLP systems. However, currently available dependency parsers each exhibit at least one of several weaknesses, including high running time, limited accuracy, vague dependency labels, and lack of nonprojectivity support. Furthermore, no commonly used parser provides additional shallow semantic interpretation, such as preposition sense disambiguation and noun compound interpretation. In this paper, we present a new dependency-tree conversion of the Penn Treebank along with its associated fine-grain dependency labels and a fast, accurate parser trained on it. We explain how a non-projective extension to shift-reduce parsing can be incorporated into non-directional easy-first parsing. The parser performs well when evaluated on the standard test section of the Penn Treebank, outperforming several popular open source dependency parsers; it is, to the best of our knowledge, the first dependency parser capable of parsing more than 75 sentences per second at over 93% accuracy.</p><p>3 0.13964951 <a title="102-tfidf-3" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>Author: Reut Tsarfaty ; Joakim Nivre ; Evelina Andersson</p><p>Abstract: unkown-abstract</p><p>4 0.13416848 <a title="102-tfidf-4" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>Author: Keith Hall ; Ryan McDonald ; Jason Katz-Brown ; Michael Ringgaard</p><p>Abstract: We present an online learning algorithm for training parsers which allows for the inclusion of multiple objective functions. The primary example is the extension of a standard supervised parsing objective function with additional loss-functions, either based on intrinsic parsing quality or task-specific extrinsic measures of quality. Our empirical results show how this approach performs for two dependency parsing algorithms (graph-based and transition-based parsing) and how it achieves increased performance on multiple target tasks including reordering for machine translation and parser adaptation.</p><p>5 0.12620574 <a title="102-tfidf-5" href="./emnlp-2011-Multi-Source_Transfer_of_Delexicalized_Dependency_Parsers.html">95 emnlp-2011-Multi-Source Transfer of Delexicalized Dependency Parsers</a></p>
<p>Author: Ryan McDonald ; Slav Petrov ; Keith Hall</p><p>Abstract: We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source lan- guages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.</p><p>6 0.12289885 <a title="102-tfidf-6" href="./emnlp-2011-Exploiting_Syntactic_and_Distributional_Information_for_Spelling_Correction_with_Web-Scale_N-gram_Models.html">55 emnlp-2011-Exploiting Syntactic and Distributional Information for Spelling Correction with Web-Scale N-gram Models</a></p>
<p>7 0.11934938 <a title="102-tfidf-7" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>8 0.11451517 <a title="102-tfidf-8" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>9 0.10697189 <a title="102-tfidf-9" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>10 0.10358962 <a title="102-tfidf-10" href="./emnlp-2011-Joint_Models_for_Chinese_POS_Tagging_and_Dependency_Parsing.html">75 emnlp-2011-Joint Models for Chinese POS Tagging and Dependency Parsing</a></p>
<p>11 0.10181699 <a title="102-tfidf-11" href="./emnlp-2011-Multiword_Expression_Identification_with_Tree_Substitution_Grammars%3A_A_Parsing_tour_de_force_with_French.html">97 emnlp-2011-Multiword Expression Identification with Tree Substitution Grammars: A Parsing tour de force with French</a></p>
<p>12 0.08946351 <a title="102-tfidf-12" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>13 0.086433358 <a title="102-tfidf-13" href="./emnlp-2011-A_Correction_Model_for_Word_Alignments.html">3 emnlp-2011-A Correction Model for Word Alignments</a></p>
<p>14 0.083285309 <a title="102-tfidf-14" href="./emnlp-2011-Exact_Inference_for_Generative_Probabilistic_Non-Projective_Dependency_Parsing.html">52 emnlp-2011-Exact Inference for Generative Probabilistic Non-Projective Dependency Parsing</a></p>
<p>15 0.074320845 <a title="102-tfidf-15" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<p>16 0.073679954 <a title="102-tfidf-16" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>17 0.073201053 <a title="102-tfidf-17" href="./emnlp-2011-Third-order_Variational_Reranking_on_Packed-Shared_Dependency_Forests.html">134 emnlp-2011-Third-order Variational Reranking on Packed-Shared Dependency Forests</a></p>
<p>18 0.066415995 <a title="102-tfidf-18" href="./emnlp-2011-A_Joint_Model_for_Extended_Semantic_Role_Labeling.html">7 emnlp-2011-A Joint Model for Extended Semantic Role Labeling</a></p>
<p>19 0.063400716 <a title="102-tfidf-19" href="./emnlp-2011-Unsupervised_Dependency_Parsing_without_Gold_Part-of-Speech_Tags.html">141 emnlp-2011-Unsupervised Dependency Parsing without Gold Part-of-Speech Tags</a></p>
<p>20 0.056154493 <a title="102-tfidf-20" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.234), (1, 0.093), (2, -0.039), (3, 0.214), (4, -0.034), (5, 0.076), (6, -0.031), (7, 0.049), (8, 0.085), (9, -0.002), (10, 0.072), (11, -0.145), (12, 0.067), (13, 0.048), (14, -0.007), (15, 0.064), (16, 0.045), (17, -0.105), (18, 0.17), (19, -0.172), (20, 0.036), (21, 0.122), (22, 0.069), (23, 0.052), (24, 0.02), (25, 0.073), (26, 0.154), (27, 0.02), (28, 0.11), (29, 0.078), (30, 0.084), (31, -0.115), (32, 0.092), (33, -0.009), (34, 0.037), (35, -0.006), (36, 0.058), (37, -0.025), (38, -0.076), (39, -0.023), (40, -0.088), (41, 0.085), (42, -0.009), (43, -0.028), (44, -0.005), (45, 0.083), (46, -0.017), (47, 0.091), (48, -0.013), (49, 0.002)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93616033 <a title="102-lsi-1" href="./emnlp-2011-Parse_Correction_with_Specialized_Models_for_Difficult_Attachment_Types.html">102 emnlp-2011-Parse Correction with Specialized Models for Difficult Attachment Types</a></p>
<p>Author: Enrique Henestroza Anguiano ; Marie Candito</p><p>Abstract: This paper develops a framework for syntactic dependency parse correction. Dependencies in an input parse tree are revised by selecting, for a given dependent, the best governor from within a small set of candidates. We use a discriminative linear ranking model to select the best governor from a group of candidates for a dependent, and our model includes a rich feature set that encodes syntactic structure in the input parse tree. The parse correction framework is parser-agnostic, and can correct attachments using either a generic model or specialized models tailored to difficult attachment types like coordination and pp-attachment. Our experiments show that parse correction, combining a generic model with specialized models for difficult attachment types, can successfully improve the quality of predicted parse trees output by sev- eral representative state-of-the-art dependency parsers for French.</p><p>2 0.70156795 <a title="102-lsi-2" href="./emnlp-2011-Exploiting_Syntactic_and_Distributional_Information_for_Spelling_Correction_with_Web-Scale_N-gram_Models.html">55 emnlp-2011-Exploiting Syntactic and Distributional Information for Spelling Correction with Web-Scale N-gram Models</a></p>
<p>Author: Wei Xu ; Joel Tetreault ; Martin Chodorow ; Ralph Grishman ; Le Zhao</p><p>Abstract: We propose a novel way of incorporating dependency parse and word co-occurrence information into a state-of-the-art web-scale ngram model for spelling correction. The syntactic and distributional information provides extra evidence in addition to that provided by a web-scale n-gram corpus and especially helps with data sparsity problems. Experimental results show that introducing syntactic features into n-gram based models significantly reduces errors by up to 12.4% over the current state-of-the-art. The word co-occurrence information shows potential but only improves overall accuracy slightly. 1</p><p>3 0.61478937 <a title="102-lsi-3" href="./emnlp-2011-A_Fast%2C_Accurate%2C_Non-Projective%2C_Semantically-Enriched_Parser.html">4 emnlp-2011-A Fast, Accurate, Non-Projective, Semantically-Enriched Parser</a></p>
<p>Author: Stephen Tratz ; Eduard Hovy</p><p>Abstract: Dependency parsers are critical components within many NLP systems. However, currently available dependency parsers each exhibit at least one of several weaknesses, including high running time, limited accuracy, vague dependency labels, and lack of nonprojectivity support. Furthermore, no commonly used parser provides additional shallow semantic interpretation, such as preposition sense disambiguation and noun compound interpretation. In this paper, we present a new dependency-tree conversion of the Penn Treebank along with its associated fine-grain dependency labels and a fast, accurate parser trained on it. We explain how a non-projective extension to shift-reduce parsing can be incorporated into non-directional easy-first parsing. The parser performs well when evaluated on the standard test section of the Penn Treebank, outperforming several popular open source dependency parsers; it is, to the best of our knowledge, the first dependency parser capable of parsing more than 75 sentences per second at over 93% accuracy.</p><p>4 0.61291409 <a title="102-lsi-4" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>Author: Daniel Dahlmeier ; Hwee Tou Ng</p><p>Abstract: We present a novel approach for automatic collocation error correction in learner English which is based on paraphrases extracted from parallel corpora. Our key assumption is that collocation errors are often caused by semantic similarity in the first language (L1language) of the writer. An analysis of a large corpus of annotated learner English confirms this assumption. We evaluate our approach on real-world learner data and show that L1-induced paraphrases outperform traditional approaches based on edit distance, homophones, and WordNet synonyms.</p><p>5 0.57846344 <a title="102-lsi-5" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>Author: Reut Tsarfaty ; Joakim Nivre ; Evelina Andersson</p><p>Abstract: unkown-abstract</p><p>6 0.55194563 <a title="102-lsi-6" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>7 0.53147215 <a title="102-lsi-7" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>8 0.48954096 <a title="102-lsi-8" href="./emnlp-2011-Exact_Inference_for_Generative_Probabilistic_Non-Projective_Dependency_Parsing.html">52 emnlp-2011-Exact Inference for Generative Probabilistic Non-Projective Dependency Parsing</a></p>
<p>9 0.48737547 <a title="102-lsi-9" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<p>10 0.45391208 <a title="102-lsi-10" href="./emnlp-2011-Multi-Source_Transfer_of_Delexicalized_Dependency_Parsers.html">95 emnlp-2011-Multi-Source Transfer of Delexicalized Dependency Parsers</a></p>
<p>11 0.42759007 <a title="102-lsi-11" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>12 0.38544548 <a title="102-lsi-12" href="./emnlp-2011-Third-order_Variational_Reranking_on_Packed-Shared_Dependency_Forests.html">134 emnlp-2011-Third-order Variational Reranking on Packed-Shared Dependency Forests</a></p>
<p>13 0.37673023 <a title="102-lsi-13" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>14 0.35521886 <a title="102-lsi-14" href="./emnlp-2011-Joint_Models_for_Chinese_POS_Tagging_and_Dependency_Parsing.html">75 emnlp-2011-Joint Models for Chinese POS Tagging and Dependency Parsing</a></p>
<p>15 0.34817752 <a title="102-lsi-15" href="./emnlp-2011-A_Correction_Model_for_Word_Alignments.html">3 emnlp-2011-A Correction Model for Word Alignments</a></p>
<p>16 0.34731561 <a title="102-lsi-16" href="./emnlp-2011-Multiword_Expression_Identification_with_Tree_Substitution_Grammars%3A_A_Parsing_tour_de_force_with_French.html">97 emnlp-2011-Multiword Expression Identification with Tree Substitution Grammars: A Parsing tour de force with French</a></p>
<p>17 0.27302542 <a title="102-lsi-17" href="./emnlp-2011-Relaxed_Cross-lingual_Projection_of_Constituent_Syntax.html">115 emnlp-2011-Relaxed Cross-lingual Projection of Constituent Syntax</a></p>
<p>18 0.26968142 <a title="102-lsi-18" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>19 0.26684138 <a title="102-lsi-19" href="./emnlp-2011-Latent_Vector_Weighting_for_Word_Meaning_in_Context.html">80 emnlp-2011-Latent Vector Weighting for Word Meaning in Context</a></p>
<p>20 0.25447056 <a title="102-lsi-20" href="./emnlp-2011-SMT_Helps_Bitext_Dependency_Parsing.html">118 emnlp-2011-SMT Helps Bitext Dependency Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.081), (36, 0.487), (37, 0.019), (45, 0.041), (53, 0.018), (54, 0.014), (62, 0.012), (64, 0.024), (66, 0.035), (79, 0.038), (82, 0.034), (90, 0.026), (96, 0.062), (98, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89509058 <a title="102-lda-1" href="./emnlp-2011-Multilayer_Sequence_Labeling.html">96 emnlp-2011-Multilayer Sequence Labeling</a></p>
<p>Author: Ai Azuma ; Yuji Matsumoto</p><p>Abstract: In this paper, we describe a novel approach to cascaded learning and inference on sequences. We propose a weakly joint learning model on cascaded inference on sequences, called multilayer sequence labeling. In this model, inference on sequences is modeled as cascaded decision. However, the decision on a sequence labeling sequel to other decisions utilizes the features on the preceding results as marginalized by the probabilistic models on them. It is not novel itself, but our idea central to this paper is that the probabilistic models on succeeding labeling are viewed as indirectly depending on the probabilistic models on preceding analyses. We also propose two types of efficient dynamic programming which are required in the gradient-based optimization of an objective function. One of the dynamic programming algorithms resembles back propagation algorithm for mul- tilayer feed-forward neural networks. The other is a generalized version of the forwardbackward algorithm. We also report experiments of cascaded part-of-speech tagging and chunking of English sentences and show effectiveness of the proposed method.</p><p>same-paper 2 0.87636274 <a title="102-lda-2" href="./emnlp-2011-Parse_Correction_with_Specialized_Models_for_Difficult_Attachment_Types.html">102 emnlp-2011-Parse Correction with Specialized Models for Difficult Attachment Types</a></p>
<p>Author: Enrique Henestroza Anguiano ; Marie Candito</p><p>Abstract: This paper develops a framework for syntactic dependency parse correction. Dependencies in an input parse tree are revised by selecting, for a given dependent, the best governor from within a small set of candidates. We use a discriminative linear ranking model to select the best governor from a group of candidates for a dependent, and our model includes a rich feature set that encodes syntactic structure in the input parse tree. The parse correction framework is parser-agnostic, and can correct attachments using either a generic model or specialized models tailored to difficult attachment types like coordination and pp-attachment. Our experiments show that parse correction, combining a generic model with specialized models for difficult attachment types, can successfully improve the quality of predicted parse trees output by sev- eral representative state-of-the-art dependency parsers for French.</p><p>3 0.75807512 <a title="102-lda-3" href="./emnlp-2011-Identification_of_Multi-word_Expressions_by_Combining_Multiple_Linguistic_Information_Sources.html">69 emnlp-2011-Identification of Multi-word Expressions by Combining Multiple Linguistic Information Sources</a></p>
<p>Author: Yulia Tsvetkov ; Shuly Wintner</p><p>Abstract: We propose an architecture for expressing various linguistically-motivated features that help identify multi-word expressions in natural language texts. The architecture combines various linguistically-motivated classification features in a Bayesian Network. We introduce novel ways for computing many of these features, and manually define linguistically-motivated interrelationships among them, which the Bayesian network models. Our methodology is almost entirely unsupervised and completely languageindependent; it relies on few language resources and is thus suitable for a large number of languages. Furthermore, unlike much recent work, our approach can identify expressions of various types and syntactic con- structions. We demonstrate a significant improvement in identification accuracy, compared with less sophisticated baselines.</p><p>4 0.46485946 <a title="102-lda-4" href="./emnlp-2011-Tuning_as_Ranking.html">138 emnlp-2011-Tuning as Ranking</a></p>
<p>Author: Mark Hopkins ; Jonathan May</p><p>Abstract: We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chiang et al., 2008b), PRO is easy to implement. It uses off-the-shelf linear binary classifier software and can be built on top of an existing MERT framework in a matter of hours. We establish PRO’s scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.</p><p>5 0.41532573 <a title="102-lda-5" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>Author: Marco Dinarelli ; Sophie Rosset</p><p>Abstract: Reranking models have been successfully applied to many tasks of Natural Language Processing. However, there are two aspects of this approach that need a deeper investigation: (i) Assessment of hypotheses generated for reranking at classification phase: baseline models generate a list of hypotheses and these are used for reranking without any assessment; (ii) Detection of cases where reranking models provide a worst result: the best hypothesis provided by the reranking model is assumed to be always the best result. In some cases the reranking model provides an incorrect hypothesis while the baseline best hypothesis is correct, especially when baseline models are accurate. In this paper we propose solutions for these two aspects: (i) a semantic inconsistency metric to select possibly more correct n-best hypotheses, from a large set generated by an SLU basiline model. The selected hypotheses are reranked applying a state-of-the-art model based on Partial Tree Kernels, which encode SLU hypotheses in Support Vector Machines with complex structured features; (ii) finally, we apply a decision strategy, based on confidence values, to select the final hypothesis between the first ranked hypothesis provided by the baseline SLU model and the first ranked hypothesis provided by the re-ranker. We show the effectiveness of these solutions presenting comparative results obtained reranking hypotheses generated by a very accurate Conditional Random Field model. We evaluate our approach on the French MEDIA corpus. The results show significant improvements with respect to current state-of-the-art and previous 1104 Sophie Rosset LIMSI-CNRS B.P. 133, 91403 Orsay Cedex France ro s set @ l ims i fr . re-ranking models.</p><p>6 0.39659619 <a title="102-lda-6" href="./emnlp-2011-Third-order_Variational_Reranking_on_Packed-Shared_Dependency_Forests.html">134 emnlp-2011-Third-order Variational Reranking on Packed-Shared Dependency Forests</a></p>
<p>7 0.39245826 <a title="102-lda-7" href="./emnlp-2011-Syntax-Based_Grammaticality_Improvement_using_CCG_and_Guided_Search.html">132 emnlp-2011-Syntax-Based Grammaticality Improvement using CCG and Guided Search</a></p>
<p>8 0.38524428 <a title="102-lda-8" href="./emnlp-2011-Linear_Text_Segmentation_Using_Affinity_Propagation.html">88 emnlp-2011-Linear Text Segmentation Using Affinity Propagation</a></p>
<p>9 0.36968988 <a title="102-lda-9" href="./emnlp-2011-Heuristic_Search_for_Non-Bottom-Up_Tree_Structure_Prediction.html">65 emnlp-2011-Heuristic Search for Non-Bottom-Up Tree Structure Prediction</a></p>
<p>10 0.36823046 <a title="102-lda-10" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>11 0.36423615 <a title="102-lda-11" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>12 0.36039612 <a title="102-lda-12" href="./emnlp-2011-Multiword_Expression_Identification_with_Tree_Substitution_Grammars%3A_A_Parsing_tour_de_force_with_French.html">97 emnlp-2011-Multiword Expression Identification with Tree Substitution Grammars: A Parsing tour de force with French</a></p>
<p>13 0.35896695 <a title="102-lda-13" href="./emnlp-2011-Structural_Opinion_Mining_for_Graph-based_Sentiment_Representation.html">126 emnlp-2011-Structural Opinion Mining for Graph-based Sentiment Representation</a></p>
<p>14 0.3482908 <a title="102-lda-14" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>15 0.34316912 <a title="102-lda-15" href="./emnlp-2011-Efficient_retrieval_of_tree_translation_examples_for_Syntax-Based_Machine_Translation.html">47 emnlp-2011-Efficient retrieval of tree translation examples for Syntax-Based Machine Translation</a></p>
<p>16 0.34083113 <a title="102-lda-16" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<p>17 0.3383745 <a title="102-lda-17" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>18 0.33418167 <a title="102-lda-18" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>19 0.33377099 <a title="102-lda-19" href="./emnlp-2011-Unsupervised_Information_Extraction_with_Distributional_Prior_Knowledge.html">143 emnlp-2011-Unsupervised Information Extraction with Distributional Prior Knowledge</a></p>
<p>20 0.3335962 <a title="102-lda-20" href="./emnlp-2011-Exploiting_Syntactic_and_Distributional_Information_for_Spelling_Correction_with_Web-Scale_N-gram_Models.html">55 emnlp-2011-Exploiting Syntactic and Distributional Information for Spelling Correction with Web-Scale N-gram Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
