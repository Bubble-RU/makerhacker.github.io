<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-83" href="#">emnlp2011-83</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</h1>
<br/><p>Source: <a title="emnlp-2011-83-pdf" href="http://aclweb.org/anthology//D/D11/D11-1108.pdf">pdf</a></p><p>Author: Juri Ganitkevitch ; Chris Callison-Burch ; Courtney Napoles ; Benjamin Van Durme</p><p>Abstract: Previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora. However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems.</p><p>Reference: <a title="emnlp-2011-83-reference" href="../emnlp2011_reference/emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('paraphras', 0.689), ('compress', 0.289), ('madnan', 0.182), ('np', 0.147), ('nontermin', 0.128), ('jj', 0.114), ('scfg', 0.11), ('pivot', 0.104), ('synchron', 0.103), ('transl', 0.098), ('leak', 0.09), ('paraphrast', 0.09), ('barzilay', 0.088), ('parallel', 0.086), ('vp', 0.084), ('cohn', 0.083), ('gramm', 0.074), ('bil', 0.073), ('cnt', 0.072), ('decontamin', 0.072)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999827 <a title="83-tfidf-1" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>Author: Juri Ganitkevitch ; Chris Callison-Burch ; Courtney Napoles ; Benjamin Van Durme</p><p>Abstract: Previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora. However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems.</p><p>2 0.65112138 <a title="83-tfidf-2" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>Author: Prodromos Malakasiotis ; Ion Androutsopoulos</p><p>Abstract: We present a method that paraphrases a given sentence by first generating candidate paraphrases and then ranking (or classifying) them. The candidates are generated by applying existing paraphrasing rules extracted from parallel corpora. The ranking component considers not only the overall quality of the rules that produced each candidate, but also the extent to which they preserve grammaticality and meaning in the particular context of the input sentence, as well as the degree to which the candidate differs from the input. We experimented with both a Maximum Entropy classifier and an SVR ranker. Experimental results show that incorporating features from an existing paraphrase recognizer in the ranking component improves performance, and that our overall method compares well against a state of the art paraphrase generator, when paraphrasing rules apply to the input sentences. We also propose a new methodology to evaluate the ranking components of generate-and-rank paraphrase generators, which evaluates them across different combinations of weights for grammaticality, meaning preservation, and diversity. The paper is accompanied by a paraphrasing dataset we constructed for evaluations of this kind.</p><p>3 0.21222088 <a title="83-tfidf-3" href="./emnlp-2011-Latent_Vector_Weighting_for_Word_Meaning_in_Context.html">80 emnlp-2011-Latent Vector Weighting for Word Meaning in Context</a></p>
<p>Author: Tim Van de Cruys ; Thierry Poibeau ; Anna Korhonen</p><p>Abstract: This paper presents a novel method for the computation of word meaning in context. We make use of a factorization model in which words, together with their window-based context words and their dependency relations, are linked to latent dimensions. The factorization model allows us to determine which dimensions are important for a particular context, and adapt the dependency-based feature vector of the word accordingly. The evaluation on a lexical substitution task carried out for both English and French – indicates that our approach is able to reach better results than state-of-the-art methods in lexical substitution, while at the same time providing more accurate meaning representations. –</p><p>4 0.1939081 <a title="83-tfidf-4" href="./emnlp-2011-Splitting_Noun_Compounds_via_Monolingual_and_Bilingual_Paraphrasing%3A_A_Study_on_Japanese_Katakana_Words.html">124 emnlp-2011-Splitting Noun Compounds via Monolingual and Bilingual Paraphrasing: A Study on Japanese Katakana Words</a></p>
<p>Author: Nobuhiro Kaji ; Masaru Kitsuregawa</p><p>Abstract: Word boundaries within noun compounds are not marked by white spaces in a number of languages, unlike in English, and it is beneficial for various NLP applications to split such noun compounds. In the case of Japanese, noun compounds made up of katakana words (i.e., transliterated foreign words) are particularly difficult to split, because katakana words are highly productive and are often outof-vocabulary. To overcome this difficulty, we propose using monolingual and bilingual paraphrases of katakana noun compounds for identifying word boundaries. Experiments demonstrated that splitting accuracy is substantially improved by extracting such paraphrases from unlabeled textual data, the Web in our case, and then using that information for constructing splitting models.</p><p>5 0.17516942 <a title="83-tfidf-5" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>Author: Jiajun Zhang ; Feifei Zhai ; Chengqing Zong</p><p>Abstract: Due to its explicit modeling of the grammaticality of the output via target-side syntax, the string-to-tree model has been shown to be one of the most successful syntax-based translation models. However, a major limitation of this model is that it does not utilize any useful syntactic information on the source side. In this paper, we analyze the difficulties of incorporating source syntax in a string-totree model. We then propose a new way to use the source syntax in a fuzzy manner, both in source syntactic annotation and in rule matching. We further explore three algorithms in rule matching: 0-1 matching, likelihood matching, and deep similarity matching. Our method not only guarantees grammatical output with an explicit target tree, but also enables the system to choose the proper translation rules via fuzzy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1</p><p>6 0.1666093 <a title="83-tfidf-6" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>7 0.15096484 <a title="83-tfidf-7" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>8 0.14912938 <a title="83-tfidf-8" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>9 0.13426442 <a title="83-tfidf-9" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>10 0.13053498 <a title="83-tfidf-10" href="./emnlp-2011-Large-Scale_Noun_Compound_Interpretation_Using_Bootstrapping_and_the_Web_as_a_Corpus.html">78 emnlp-2011-Large-Scale Noun Compound Interpretation Using Bootstrapping and the Web as a Corpus</a></p>
<p>11 0.12420143 <a title="83-tfidf-11" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>12 0.12283943 <a title="83-tfidf-12" href="./emnlp-2011-Better_Evaluation_Metrics_Lead_to_Better_Machine_Translation.html">22 emnlp-2011-Better Evaluation Metrics Lead to Better Machine Translation</a></p>
<p>13 0.12066562 <a title="83-tfidf-13" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>14 0.11451996 <a title="83-tfidf-14" href="./emnlp-2011-Generating_Aspect-oriented_Multi-Document_Summarization_with_Event-aspect_model.html">61 emnlp-2011-Generating Aspect-oriented Multi-Document Summarization with Event-aspect model</a></p>
<p>15 0.10366851 <a title="83-tfidf-15" href="./emnlp-2011-Statistical_Machine_Translation_with_Local_Language_Models.html">125 emnlp-2011-Statistical Machine Translation with Local Language Models</a></p>
<p>16 0.10294909 <a title="83-tfidf-16" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>17 0.098258048 <a title="83-tfidf-17" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<p>18 0.09792129 <a title="83-tfidf-18" href="./emnlp-2011-Divide_and_Conquer%3A_Crowdsourcing_the_Creation_of_Cross-Lingual_Textual_Entailment_Corpora.html">42 emnlp-2011-Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora</a></p>
<p>19 0.089257136 <a title="83-tfidf-19" href="./emnlp-2011-Data-Driven_Response_Generation_in_Social_Media.html">38 emnlp-2011-Data-Driven Response Generation in Social Media</a></p>
<p>20 0.086694717 <a title="83-tfidf-20" href="./emnlp-2011-Analyzing_Methods_for_Improving_Precision_of_Pivot_Based_Bilingual_Dictionaries.html">18 emnlp-2011-Analyzing Methods for Improving Precision of Pivot Based Bilingual Dictionaries</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.293), (1, 0.126), (2, 0.033), (3, -0.53), (4, -0.074), (5, 0.336), (6, -0.254), (7, 0.035), (8, 0.167), (9, 0.106), (10, 0.06), (11, 0.008), (12, -0.043), (13, -0.002), (14, -0.022), (15, 0.012), (16, -0.088), (17, -0.034), (18, -0.028), (19, 0.129), (20, -0.174), (21, -0.062), (22, 0.114), (23, -0.037), (24, 0.047), (25, -0.032), (26, 0.003), (27, -0.048), (28, -0.058), (29, -0.093), (30, 0.061), (31, 0.01), (32, 0.006), (33, -0.034), (34, -0.051), (35, -0.029), (36, -0.022), (37, 0.095), (38, 0.051), (39, 0.042), (40, 0.025), (41, -0.0), (42, -0.125), (43, -0.044), (44, -0.029), (45, 0.06), (46, 0.021), (47, 0.033), (48, -0.003), (49, 0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95760626 <a title="83-lsi-1" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>Author: Prodromos Malakasiotis ; Ion Androutsopoulos</p><p>Abstract: We present a method that paraphrases a given sentence by first generating candidate paraphrases and then ranking (or classifying) them. The candidates are generated by applying existing paraphrasing rules extracted from parallel corpora. The ranking component considers not only the overall quality of the rules that produced each candidate, but also the extent to which they preserve grammaticality and meaning in the particular context of the input sentence, as well as the degree to which the candidate differs from the input. We experimented with both a Maximum Entropy classifier and an SVR ranker. Experimental results show that incorporating features from an existing paraphrase recognizer in the ranking component improves performance, and that our overall method compares well against a state of the art paraphrase generator, when paraphrasing rules apply to the input sentences. We also propose a new methodology to evaluate the ranking components of generate-and-rank paraphrase generators, which evaluates them across different combinations of weights for grammaticality, meaning preservation, and diversity. The paper is accompanied by a paraphrasing dataset we constructed for evaluations of this kind.</p><p>same-paper 2 0.95223421 <a title="83-lsi-2" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>Author: Juri Ganitkevitch ; Chris Callison-Burch ; Courtney Napoles ; Benjamin Van Durme</p><p>Abstract: Previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora. However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems.</p><p>3 0.51402664 <a title="83-lsi-3" href="./emnlp-2011-Splitting_Noun_Compounds_via_Monolingual_and_Bilingual_Paraphrasing%3A_A_Study_on_Japanese_Katakana_Words.html">124 emnlp-2011-Splitting Noun Compounds via Monolingual and Bilingual Paraphrasing: A Study on Japanese Katakana Words</a></p>
<p>Author: Nobuhiro Kaji ; Masaru Kitsuregawa</p><p>Abstract: Word boundaries within noun compounds are not marked by white spaces in a number of languages, unlike in English, and it is beneficial for various NLP applications to split such noun compounds. In the case of Japanese, noun compounds made up of katakana words (i.e., transliterated foreign words) are particularly difficult to split, because katakana words are highly productive and are often outof-vocabulary. To overcome this difficulty, we propose using monolingual and bilingual paraphrases of katakana noun compounds for identifying word boundaries. Experiments demonstrated that splitting accuracy is substantially improved by extracting such paraphrases from unlabeled textual data, the Web in our case, and then using that information for constructing splitting models.</p><p>4 0.43685111 <a title="83-lsi-4" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>Author: Kristian Woodsend ; Mirella Lapata</p><p>Abstract: Text simplification aims to rewrite text into simpler versions, and thus make information accessible to a broader audience. Most previous work simplifies sentences using handcrafted rules aimed at splitting long sentences, or substitutes difficult words using a predefined dictionary. This paper presents a datadriven model based on quasi-synchronous grammar, a formalism that can naturally capture structural mismatches and complex rewrite operations. We describe how such a grammar can be induced from Wikipedia and propose an integer linear programming model for selecting the most appropriate simplification from the space of possible rewrites generated by the grammar. We show experimentally that our method creates simplifications that significantly reduce the reading difficulty ofthe input, while maintaining grammaticality and preserving its meaning.</p><p>5 0.41646266 <a title="83-lsi-5" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>Author: Daniel Dahlmeier ; Hwee Tou Ng</p><p>Abstract: We present a novel approach for automatic collocation error correction in learner English which is based on paraphrases extracted from parallel corpora. Our key assumption is that collocation errors are often caused by semantic similarity in the first language (L1language) of the writer. An analysis of a large corpus of annotated learner English confirms this assumption. We evaluate our approach on real-world learner data and show that L1-induced paraphrases outperform traditional approaches based on edit distance, homophones, and WordNet synonyms.</p><p>6 0.38767561 <a title="83-lsi-6" href="./emnlp-2011-Latent_Vector_Weighting_for_Word_Meaning_in_Context.html">80 emnlp-2011-Latent Vector Weighting for Word Meaning in Context</a></p>
<p>7 0.37566289 <a title="83-lsi-7" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>8 0.33191288 <a title="83-lsi-8" href="./emnlp-2011-Divide_and_Conquer%3A_Crowdsourcing_the_Creation_of_Cross-Lingual_Textual_Entailment_Corpora.html">42 emnlp-2011-Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora</a></p>
<p>9 0.32948276 <a title="83-lsi-9" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>10 0.32373685 <a title="83-lsi-10" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>11 0.31711456 <a title="83-lsi-11" href="./emnlp-2011-Large-Scale_Noun_Compound_Interpretation_Using_Bootstrapping_and_the_Web_as_a_Corpus.html">78 emnlp-2011-Large-Scale Noun Compound Interpretation Using Bootstrapping and the Web as a Corpus</a></p>
<p>12 0.29702833 <a title="83-lsi-12" href="./emnlp-2011-Minimum_Imputed-Risk%3A_Unsupervised_Discriminative_Training_for_Machine_Translation.html">93 emnlp-2011-Minimum Imputed-Risk: Unsupervised Discriminative Training for Machine Translation</a></p>
<p>13 0.2758224 <a title="83-lsi-13" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>14 0.26541084 <a title="83-lsi-14" href="./emnlp-2011-Better_Evaluation_Metrics_Lead_to_Better_Machine_Translation.html">22 emnlp-2011-Better Evaluation Metrics Lead to Better Machine Translation</a></p>
<p>15 0.25101787 <a title="83-lsi-15" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>16 0.25005275 <a title="83-lsi-16" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>17 0.24921532 <a title="83-lsi-17" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<p>18 0.23836227 <a title="83-lsi-18" href="./emnlp-2011-Cache-based_Document-level_Statistical_Machine_Translation.html">25 emnlp-2011-Cache-based Document-level Statistical Machine Translation</a></p>
<p>19 0.23611766 <a title="83-lsi-19" href="./emnlp-2011-Feature-Rich_Language-Independent_Syntax-Based_Alignment_for_Statistical_Machine_Translation.html">60 emnlp-2011-Feature-Rich Language-Independent Syntax-Based Alignment for Statistical Machine Translation</a></p>
<p>20 0.23556206 <a title="83-lsi-20" href="./emnlp-2011-Analyzing_Methods_for_Improving_Precision_of_Pivot_Based_Bilingual_Dictionaries.html">18 emnlp-2011-Analyzing Methods for Improving Precision of Pivot Based Bilingual Dictionaries</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.047), (10, 0.308), (13, 0.044), (27, 0.036), (37, 0.108), (41, 0.041), (42, 0.039), (50, 0.085), (68, 0.156), (96, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.70224667 <a title="83-lda-1" href="./emnlp-2011-Linguistic_Redundancy_in_Twitter.html">89 emnlp-2011-Linguistic Redundancy in Twitter</a></p>
<p>Author: Fabio Massimo Zanzotto ; Marco Pennaccchiotti ; Kostas Tsioutsiouliklis</p><p>Abstract: In the last few years, the interest of the research community in micro-blogs and social media services, such as Twitter, is growing exponentially. Yet, so far not much attention has been paid on a key characteristic of microblogs: the high level of information redundancy. The aim of this paper is to systematically approach this problem by providing an operational definition of redundancy. We cast redundancy in the framework of Textual Entailment Recognition. We also provide quantitative evidence on the pervasiveness of redundancy in Twitter, and describe a dataset of redundancy-annotated tweets. Finally, we present a general purpose system for identifying redundant tweets. An extensive quantitative evaluation shows that our system successfully solves the redundancy detection task, improving over baseline systems with statistical significance.</p><p>same-paper 2 0.67642325 <a title="83-lda-2" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>Author: Juri Ganitkevitch ; Chris Callison-Burch ; Courtney Napoles ; Benjamin Van Durme</p><p>Abstract: Previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora. However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems.</p><p>3 0.62344289 <a title="83-lda-3" href="./emnlp-2011-Unsupervised_Learning_of_Selectional_Restrictions_and_Detection_of_Argument_Coercions.html">144 emnlp-2011-Unsupervised Learning of Selectional Restrictions and Detection of Argument Coercions</a></p>
<p>Author: Kirk Roberts ; Sanda Harabagiu</p><p>Abstract: Metonymic language is a pervasive phenomenon. Metonymic type shifting, or argument type coercion, results in a selectional restriction violation where the argument’s semantic class differs from the class the predicate expects. In this paper we present an unsupervised method that learns the selectional restriction of arguments and enables the detection of argument coercion. This method also generates an enhanced probabilistic resolution of logical metonymies. The experimental results indicate substantial improvements the detection of coercions and the ranking of metonymic interpretations.</p><p>4 0.55052304 <a title="83-lda-4" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>Author: Xinyan Xiao ; Yang Liu ; Qun Liu ; Shouxun Lin</p><p>Abstract: Although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features, it is hard to scale up to large data due to decoding complexity. We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets.</p><p>5 0.54840684 <a title="83-lda-5" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>Author: Jiajun Zhang ; Feifei Zhai ; Chengqing Zong</p><p>Abstract: Due to its explicit modeling of the grammaticality of the output via target-side syntax, the string-to-tree model has been shown to be one of the most successful syntax-based translation models. However, a major limitation of this model is that it does not utilize any useful syntactic information on the source side. In this paper, we analyze the difficulties of incorporating source syntax in a string-totree model. We then propose a new way to use the source syntax in a fuzzy manner, both in source syntactic annotation and in rule matching. We further explore three algorithms in rule matching: 0-1 matching, likelihood matching, and deep similarity matching. Our method not only guarantees grammatical output with an explicit target tree, but also enables the system to choose the proper translation rules via fuzzy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1</p><p>6 0.54701257 <a title="83-lda-6" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>7 0.54406983 <a title="83-lda-7" href="./emnlp-2011-Relation_Acquisition_using_Word_Classes_and_Partial_Patterns.html">113 emnlp-2011-Relation Acquisition using Word Classes and Partial Patterns</a></p>
<p>8 0.54276526 <a title="83-lda-8" href="./emnlp-2011-Unsupervised_Discovery_of_Discourse_Relations_for_Eliminating_Intra-sentence_Polarity_Ambiguities.html">142 emnlp-2011-Unsupervised Discovery of Discourse Relations for Eliminating Intra-sentence Polarity Ambiguities</a></p>
<p>9 0.54111481 <a title="83-lda-9" href="./emnlp-2011-Extreme_Extraction_-_Machine_Reading_in_a_Week.html">57 emnlp-2011-Extreme Extraction - Machine Reading in a Week</a></p>
<p>10 0.53812236 <a title="83-lda-10" href="./emnlp-2011-Relation_Extraction_with_Relation_Topics.html">114 emnlp-2011-Relation Extraction with Relation Topics</a></p>
<p>11 0.5377332 <a title="83-lda-11" href="./emnlp-2011-Better_Evaluation_Metrics_Lead_to_Better_Machine_Translation.html">22 emnlp-2011-Better Evaluation Metrics Lead to Better Machine Translation</a></p>
<p>12 0.53624797 <a title="83-lda-12" href="./emnlp-2011-Corroborating_Text_Evaluation_Results_with_Heterogeneous_Measures.html">36 emnlp-2011-Corroborating Text Evaluation Results with Heterogeneous Measures</a></p>
<p>13 0.53537166 <a title="83-lda-13" href="./emnlp-2011-Harnessing_different_knowledge_sources_to_measure_semantic_relatedness_under_a_uniform_model.html">64 emnlp-2011-Harnessing different knowledge sources to measure semantic relatedness under a uniform model</a></p>
<p>14 0.53460145 <a title="83-lda-14" href="./emnlp-2011-Data-Driven_Response_Generation_in_Social_Media.html">38 emnlp-2011-Data-Driven Response Generation in Social Media</a></p>
<p>15 0.53412521 <a title="83-lda-15" href="./emnlp-2011-Heuristic_Search_for_Non-Bottom-Up_Tree_Structure_Prediction.html">65 emnlp-2011-Heuristic Search for Non-Bottom-Up Tree Structure Prediction</a></p>
<p>16 0.53379321 <a title="83-lda-16" href="./emnlp-2011-Lexical_Generalization_in_CCG_Grammar_Induction_for_Semantic_Parsing.html">87 emnlp-2011-Lexical Generalization in CCG Grammar Induction for Semantic Parsing</a></p>
<p>17 0.53327823 <a title="83-lda-17" href="./emnlp-2011-Watermarking_the_Outputs_of_Structured_Prediction_with_an_application_in_Statistical_Machine_Translation..html">148 emnlp-2011-Watermarking the Outputs of Structured Prediction with an application in Statistical Machine Translation.</a></p>
<p>18 0.53327233 <a title="83-lda-18" href="./emnlp-2011-Minimally_Supervised_Event_Causality_Identification.html">92 emnlp-2011-Minimally Supervised Event Causality Identification</a></p>
<p>19 0.53271109 <a title="83-lda-19" href="./emnlp-2011-Tuning_as_Ranking.html">138 emnlp-2011-Tuning as Ranking</a></p>
<p>20 0.53204691 <a title="83-lda-20" href="./emnlp-2011-Efficient_retrieval_of_tree_translation_examples_for_Syntax-Based_Machine_Translation.html">47 emnlp-2011-Efficient retrieval of tree translation examples for Syntax-Based Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
