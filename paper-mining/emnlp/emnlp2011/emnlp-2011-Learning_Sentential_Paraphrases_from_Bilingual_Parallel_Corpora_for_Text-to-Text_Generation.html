<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-83" href="#">emnlp2011-83</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</h1>
<br/><p>Source: <a title="emnlp-2011-83-pdf" href="http://aclweb.org/anthology//D/D11/D11-1108.pdf">pdf</a></p><p>Author: Juri Ganitkevitch ; Chris Callison-Burch ; Courtney Napoles ; Benjamin Van Durme</p><p>Abstract: Previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora. However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems.</p><p>Reference: <a title="emnlp-2011-83-reference" href="../emnlp2011_reference/emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. [sent-2, score-0.265]
</p><p>2 We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. [sent-3, score-1.032]
</p><p>3 We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems. [sent-5, score-0.977]
</p><p>4 Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. [sent-7, score-0.404]
</p><p>5 In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al. [sent-8, score-0.315]
</p><p>6 A variety of different types of corpora (and semantic equivalence cues) have been used to automatically induce paraphrase collections for English (Madnani and Dorr, 2010). [sent-17, score-0.315]
</p><p>7 Perhaps the most natural type of corpus for this task is a monolingual parallel text, which allows sentential paraphrases to be extracted since the sentence pairs in such corpora are perfect paraphrases of each other (Barzilay and McKeown, 2001 ; Pang et al. [sent-18, score-1.073]
</p><p>8 While rich syntactic paraphrases have been learned from monolingual parallel corpora, they suffer from very limited data availability and thus have poor coverage. [sent-20, score-0.602]
</p><p>9 Other methods obtain paraphrases from raw monolingual text by relying on distributional similarity (Lin and Pantel, 2001 ; Bhagat and Ravichandran, 2008). [sent-21, score-0.462]
</p><p>10 The coverage of paraphrase lexica extracted from bitexts has been shown to outperform that obtained from other sources (Zhao et al. [sent-26, score-0.315]
</p><p>11 While there have been efforts pursuing the extraction of more powerful paraphrases (Madnani et al. [sent-28, score-0.404]
</p><p>12 , 2008b), it is not yet clear to what extent sentential paraphrases can be induced from bitexts. [sent-30, score-0.511]
</p><p>13 In this paper we: •  •  Extend the bilingual pivoting approach to paraphrase in thdeu bcitiloinng utoa produce g ric ahp syntactic paraphrases. [sent-31, score-0.551]
</p><p>14 Perform a thorough analysis of the types of paraphrases we oubghtai ann aalnyds sdis ocfu tshse th tyep para-  phrastic transformations we are capable of capturing. [sent-32, score-0.464]
</p><p>15 •  •  2  Describe how training paradigms for syntactDice/sscernibteent hiaolw paraphrase maroaddeilgsm ssho fuolrd s yben ttaaci-lored to different text-to-text generation tasks. [sent-33, score-0.392]
</p><p>16 Related Work  Madnani and Dorr (2010) survey a variety of datadriven paraphrasing techniques, categorizing them based on the type of data that they use. [sent-35, score-0.172]
</p><p>17 , 2004), monolingual parallel corpora (Barzilay and McKeown, 2001 ; Pang et al. [sent-38, score-0.158]
</p><p>18 , 2003), and bilingual parallel corpora (Bannard and Callison-Burch, 2005; Madnani et al. [sent-39, score-0.2]
</p><p>19 Paraphrase extraction using bilingual parallel cor-  pora was proposed by Bannard and Callison-Burch (2005) who induced paraphrases using techniques from phrase-based statistical machine translation (Koehn et al. [sent-43, score-0.714]
</p><p>20 After extracting a bilingual 1169 phrase table, English paraphrases are obtained by pivoting through foreign language phrases. [sent-45, score-0.647]
</p><p>21 Since many paraphrases can be extracted for a phrase, Bannard and Callison-Burch rank them using a paraphrase probability defined in terms of the translation model probabilities p(f|e) and p(e|f) : p(e2|e1)  = Xp(e2,f|e1)  =  (1)  Xf  Xp(e2|f,e1)p(f|e1) Xf  ≈ Xp(e2|f)p(f|e1). [sent-46, score-0.829]
</p><p>22 Xf  (2) (3)  Several subsequent efforts extended the bilingual pivoting technique, many of which introduced elements of more contemporary syntax-based approaches to statistical machine translation. [sent-47, score-0.196]
</p><p>23 (2007) extended the technique to hier-  archical phrase-based machine translation (Chiang, 2005), which is formally a synchronous context-free grammar (SCFG) and thus can be thought of as a paraphrase grammar. [sent-49, score-0.622]
</p><p>24 The paraphrase grammar can paraphrase (or “decode”) input sentences using an SCFG decoder, like the Hiero, Joshua or cdec MT systems (Chiang, 2007; Li et al. [sent-50, score-0.712]
</p><p>25 Callison-Burch (2008) introduced syntactic constraints by labeling all phrases and paraphrases (even non-constituent phrases) with CCGinspired slash categories (Steedman and Baldridge, 2011), an approach similar to Zollmann and Venugopal (2006)’s syntax-augmented machine translation (SAMT). [sent-55, score-0.597]
</p><p>26 (2008b) added slots to bilingually extracted  paraphrase patterns that were labeled with part-ofspeech tags, but not larger syntactic constituents. [sent-60, score-0.393]
</p><p>27 Before the shift to statistical natural language processing, paraphrasing was often treated as syntactic transformations or by parsing and then generating from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). [sent-61, score-0.332]
</p><p>28 Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al. [sent-62, score-0.519]
</p><p>29 , 2009) with (3) a concrete example of the adaptation procedure for the task of paraphrase-based sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008; Cohn and Lapata, 2009). [sent-74, score-0.331]
</p><p>30 3  SCFGs in Translation  The model we use in our paraphrasing approach is a syntactically informed synchronous context-free grammar (SCFG). [sent-75, score-0.369]
</p><p>31 Extraction methods vary on whether they extract only minimal rules for phrases dominated by nodes in the parse tree, or more complex rules that include non-constituent phrases. [sent-79, score-0.151]
</p><p>32 cNT (α) }  constitutes a one-to-one correspondency function between the nonterminals in γ and α. [sent-86, score-0.174]
</p><p>33 e  ≥  Rule Extraction Phrase-based approaches to statistical machine translation (and their successors) extract pairs of (e, f) phrases from automatically word-aligned parallel sentences. [sent-88, score-0.253]
</p><p>34 These phrase extraction heuristics have been extended so that they extract synchronous grammar rules (Galley et al. [sent-91, score-0.251]
</p><p>35 The nonterminal symbol that is the left-hand side of the SCFG rule is then determined by the syntactic constituent that dominates e (in this case NP). [sent-99, score-0.181]
</p><p>36 To introduce nonterminals into the right-hand side of the rule, we can apply rules extracted over sub-phrases of f,synchronously substituting the corresponding nonterminal symbol for the sub-phrases on both sides. [sent-100, score-0.216]
</p><p>37 The synchronous substitution applied to f and e then yields the correspondency ∼. [sent-101, score-0.192]
</p><p>38 oOnndee significant differentiating factor between the competing ways ofextracting SCFG rules is whether the extraction method generates rules only for constituent phrases that are dominated by a node in the parse tree (Galley et al. [sent-102, score-0.151]
</p><p>39 Although the synchronous trees are unlike the derivations found in the Penn Treebank, their yield is a good translation of the German. [sent-115, score-0.225]
</p><p>40 4  SCFGs in Paraphrasing  Rule Extraction To create a paraphrase grammar from a translation grammar, we extend the syntactically informed pivot approach of Callison-Burch (2008) to the SCFG model. [sent-121, score-0.556]
</p><p>41 Feature Functions In the computation of the features ϕ~ from ϕ~1 and ϕ~2 we follow the approximation in Equation 3, which yields lexical and phrasal paraphrase probability features. [sent-124, score-0.381]
</p><p>42 Another indicator feature, δreorder, fires if the rule swaps the order of two nonterminals, which enables us to promote more complex paraphrases that require structural reordering. [sent-126, score-0.48]
</p><p>43 Decoding With this, paraphrasing becomes an English-to-English translation problem which can be formulated similarly to Equation 5 as:  eˆ2 ≈ yield(argd∈Dm(ae2x,e1)p(d,e2|e1)). [sent-127, score-0.282]
</p><p>44 Figure 3 shows an example derivation produced as a result of applying our paraphrase rules in the decoding process. [sent-128, score-0.409]
</p><p>45 Another advantage of using the decoder from statistical machine translation is that n-gram language models, which have been shown to be useful in natural language generation (Langkilde and Knight, 1998), are already well integrated (Huang and Chiang, 2007). [sent-129, score-0.227]
</p><p>46 5  Analysis  A key motivation for the use of syntactic paraphrases over their phrasal counterparts is their potential to capture meaning-preserving linguistic transformations in a more general fashion. [sent-130, score-0.57]
</p><p>47 A phrasal system is limited to memorizing fully lexicalized transformations in its paraphrase table, resulting in poor generalization capabilities. [sent-131, score-0.441]
</p><p>48 By contrast, a syntactic paraphrasing system intuitively should be able to address this issue and learn well-formed and generic patterns  that can be easily applied to unseen data. [sent-132, score-0.212]
</p><p>49 1172  CDNPNNSNPJJVPDTDT+NNPNPNNP  twelve  cartoons  insulting  the  prophet mohammad  12 of the cartoons that are offensive to the prophet mohammad  CDNPNNSNPJJVPDTDT+NNPNPNNP Paraphrase Rule Lexical paraphrase: JJ → offensive | insulting Reduced relative clause: NP → NP that VP | NP VP Pred. [sent-133, score-0.384]
</p><p>50 A few of the rules applied in the parse are show in the left column, with the pivot phrases that gave rise to them on the right. [sent-135, score-0.146]
</p><p>51 To put this expectation to the test, we investigate  how our grammar captures a number of well-known paraphrastic transformations. [sent-136, score-0.178]
</p><p>52 1 Table 1 shows the transformations along with examples of the generic grammar rules our system learns to represent them. [sent-137, score-0.196]
</p><p>53 When given a transformation to extract a syntactic paraphrase for, we want to find rules that neither under- nor over-generalize. [sent-138, score-0.409]
</p><p>54 The paraphrases implementing thepossessive rule and the dative shift shown in Table 1 are a good examples of this: the two noun-phrase arguments to the expressions are abstracted to nonterminals while each rule’s lexicalization provides an appropriate frame of evidence for the transform. [sent-140, score-0.714]
</p><p>55 give decontamination equipment to Japan give Japan decontamination equipment provide decontamination equipment to Japan ? [sent-148, score-0.462]
</p><p>56 provide Japan decontamination equipment Note how our system extracts a dative shift rule for to give and a rule that both shifts and substitutes a more appropriate verb for to provide. [sent-149, score-0.443]
</p><p>57 The use of syntactic nonterminals in our paraphrase rules to capture complex transforms also makes it possible to impose constraints on their application. [sent-150, score-0.506]
</p><p>58 (2007) do not impose any constraints on how the nonterminal X can be realized, their equivalent of the topicalization rule would massively overgeneralize:  S → X1, X2 . [sent-152, score-0.141]
</p><p>59 For instance it is hard to extract generic paraphrases for all instances of passivization, since our syntactic model currently has no means of representing the morphological changes that the verb undergoes: the reactor leaks radiation radiation is leaking from the reactor . [sent-156, score-0.718]
</p><p>60 Still, for cases where the verb’s morphology does not change, we manage to learn a rule: 1173 the radiation that the reactor had leaked the radiation which leaked from the reactor . [sent-157, score-0.35]
</p><p>61 Another example of a deficiency in our synchronous grammar models are light verb constructs such as: to take a walk to walk . [sent-158, score-0.197]
</p><p>62 Here, a noun is transformed into the corresponding verb something our synchronous syntactic CFGs –  are not able to capture except through memorization. [sent-159, score-0.155]
</p><p>63 This is a surprising result which shows that bilingual parallel corpora can be used to learn sentential paraphrases, and that they are a viable alternative to other data sources like monolingual parallel corpora, which more obviously contain sentential paraphrases, but are scarce. [sent-161, score-0.572]
</p><p>64 6  Text-to-Text Applications  The core of many text-to-text generation tasks is sentential paraphrasing, augmented with specific constraints or goals. [sent-162, score-0.184]
</p><p>65 Since our model borrows much of its machinery from statistical machine translation a sentential rewriting problem itself – it is straightforward to use our paraphrase grammars to generate new sentences using SMT’s decoding and parameter optimization techniques. [sent-163, score-0.572]
</p><p>66 Each individual text-to-text application requires that our framework be adapted in several ways, by specifying: •  •  •  •  •  A mechanism for extracting synchronous grammar rules (in this paper we argue that pivot-based paraphrasing is widely applicable). [sent-166, score-0.466]
</p><p>67 In the remainder of this section, we illustrate how our bilingually extracted paraphrases can be adapted to perform sentence compression, which is the task of reducing the length of sentence while preserving  its core meaning. [sent-175, score-0.485]
</p><p>68 Most previous approaches to sentence compression focused only on the deletion of a subset of words from the sentence (Knight and Marcu, 2002). [sent-176, score-0.381]
</p><p>69 While these help quantify how good a paraphrase is in general, they do not make any statement on task-specific things such as the change in language complexity or text length. [sent-180, score-0.315]
</p><p>70 To make this information available to the decoder, we enhance our paraphrases with four compression-targeted features. [sent-181, score-0.404]
</p><p>71 We add the count features csrc and ctgt, indicating the number of words on either side of the rule as well as two difference features: cdcount = ctgt − csrc and the analogously computed difference i−n cthe average word length in characters, cdavg. [sent-182, score-0.19]
</p><p>72 2 Objective Function Given our paraphrasing system’s connection to SMT, the naive/obvious choice for parameter optimization would be to optimize for BLEU over a  set of paraphrases, for instance parallel English reference translations for a machine translation task (Madnani et al. [sent-184, score-0.382]
</p><p>73 Naively optimizing for BLEU, however, will result in a trivial paraphrasing system heavily biased towards producing identity “paraphrases”. [sent-188, score-0.172]
</p><p>74 Moreover, BLEU does not provide a mechanism for directly specifying a per-sentence compression rate, which is desirable for the compression task. [sent-190, score-0.662]
</p><p>75 Instead, we propose PR´ECIS, an objective function tailored to the text compression task: PR´ECISλ,ϕ(I, C, R)  e(1−c/r)  ? [sent-191, score-0.331]
</p><p>76 3 Development Data To tune the parameters of our paraphrase system for sentence compression, we need an appropriate corpus of reference compressions. [sent-202, score-0.315]
</p><p>77 Since our model is designed to compress by paraphrasing rather than deletion, the commonly used deletion-based compression data sets like the Ziff-Davis corpus are not suitable. [sent-203, score-0.503]
</p><p>78 We have thus created a corpus of compression paraphrases. [sent-204, score-0.331]
</p><p>79 We further retain only those sentence pairs where the compression rate cr falls in the range 0. [sent-206, score-0.371]
</p><p>80 3 Table 4 shows a set of pairwise comparisons for compression rates ≈ 0. [sent-212, score-0.37]
</p><p>81 We see that going from a mHiperreos-sbiaosned ra t eos a syntactic paraphrase grammar yields a significant improvement in grammaticality. [sent-214, score-0.437]
</p><p>82 Further augmenting the grammar with deletion rules significantly helps retain the core meaning at compression rates this high,  however compared to the un-augmented syntactic system grammaticality scores drop. [sent-216, score-0.641]
</p><p>83 In Table 3 we compare our system to the ILP approach at a modest compression rate of ≈ 0. [sent-218, score-0.371]
</p><p>84 57 Table 3: Results of the human evaluation on longer com-  pressions: pairwise compression rates (CR), meaning and grammaticality scores. [sent-237, score-0.415]
</p><p>85 2903 Table 4: Human evaluation for shorter compressions and for variations of our paraphrase system. [sent-273, score-0.373]
</p><p>86 These results indicate that, over a variety of compression rates, our  framework for text-to-text generation is performing as well as or better than specifically tailored stateof-the-art methods. [sent-281, score-0.408]
</p><p>87 We see that both the paraphrase and ILP systems produce good quality results, with the paraphrase system retaining the meaning of the source sentence more accurately. [sent-283, score-0.63]
</p><p>88 7  Conclusion  In this work we introduced a method to learn syntactically informed paraphrases from bilingual parallel texts. [sent-284, score-0.604]
</p><p>89 We demonstrated when our paraphrasing system was adapted to do sentence compression, it achieved results competitive with state-of-the-art compression systems with only minimal effort. [sent-286, score-0.546]
</p><p>90 Acknowledgments We would like to thank Trevor Cohn for kindly providing us with the T3 compression system. [sent-287, score-0.331]
</p><p>91 Large scale acquisition of paraphrases for learning surface patterns. [sent-330, score-0.404]
</p><p>92 Large margin synchronous generation and its application to sentence compression. [sent-358, score-0.192]
</p><p>93 Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. [sent-375, score-0.415]
</p><p>94 Generation of single-sentence paraphrases from predicate/argument structure using lexico-grammatical resources. [sent-422, score-0.404]
</p><p>95 Generating phrasal and sentential paraphrases: A survey of data-driven methods. [sent-447, score-0.173]
</p><p>96 Using paraphrases for parameter tuning in statistical machine translation. [sent-451, score-0.404]
</p><p>97 Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences. [sent-493, score-0.404]
</p><p>98 Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. [sent-546, score-0.2]
</p><p>99 Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems. [sent-559, score-0.15]
</p><p>100 Pivot approach for extracting paraphrase  patterns from bilingual corpora. [sent-567, score-0.415]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('paraphrases', 0.404), ('compression', 0.331), ('paraphrase', 0.315), ('madnani', 0.194), ('paraphrasing', 0.172), ('np', 0.156), ('jj', 0.121), ('scfg', 0.117), ('synchronous', 0.115), ('translation', 0.11), ('sentential', 0.107), ('parallel', 0.1), ('bilingual', 0.1), ('nonterminals', 0.097), ('paraphrastic', 0.096), ('pivoting', 0.096), ('barzilay', 0.094), ('vp', 0.089), ('cohn', 0.088), ('grammar', 0.082), ('generation', 0.077), ('cnt', 0.077), ('correspondency', 0.077), ('dative', 0.077), ('decontamination', 0.077), ('equipment', 0.077), ('reactor', 0.077), ('rule', 0.076), ('cd', 0.074), ('chris', 0.072), ('zhao', 0.069), ('bannard', 0.067), ('phrasal', 0.066), ('nonterminal', 0.065), ('bleu', 0.065), ('nns', 0.062), ('shift', 0.06), ('transformations', 0.06), ('radiation', 0.06), ('mckeown', 0.059), ('monolingual', 0.058), ('compressions', 0.058), ('ecis', 0.058), ('insulting', 0.058), ('napoles', 0.058), ('offensive', 0.058), ('poetry', 0.058), ('regina', 0.054), ('rules', 0.054), ('venugopal', 0.052), ('knight', 0.05), ('deletion', 0.05), ('bhagat', 0.05), ('pivot', 0.049), ('kathleen', 0.049), ('quirk', 0.048), ('chiang', 0.047), ('zollmann', 0.047), ('juri', 0.047), ('foreign', 0.047), ('lapata', 0.046), ('shiqi', 0.045), ('nitin', 0.045), ('grammaticality', 0.045), ('ganitkevitch', 0.045), ('ravichandran', 0.043), ('adapted', 0.043), ('phrases', 0.043), ('proceedings', 0.042), ('kevin', 0.042), ('xf', 0.042), ('ilp', 0.041), ('syntactic', 0.04), ('decoder', 0.04), ('trevor', 0.04), ('rate', 0.04), ('decoding', 0.04), ('vb', 0.039), ('rates', 0.039), ('anick', 0.038), ('beleidigend', 0.038), ('bilingually', 0.038), ('cartoons', 0.038), ('cdnpnnsnpjjvpdtdt', 0.038), ('csrc', 0.038), ('ctgt', 0.038), ('dangerous', 0.038), ('dem', 0.038), ('dorfes', 0.038), ('epnn', 0.038), ('gef', 0.038), ('hrlich', 0.038), ('kozlowski', 0.038), ('leaked', 0.038), ('meteer', 0.038), ('nicht', 0.038), ('nnen', 0.038), ('nnpnpnnp', 0.038), ('passivization', 0.038), ('prophet', 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="83-tfidf-1" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>Author: Juri Ganitkevitch ; Chris Callison-Burch ; Courtney Napoles ; Benjamin Van Durme</p><p>Abstract: Previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora. However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems.</p><p>2 0.41885543 <a title="83-tfidf-2" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>Author: Prodromos Malakasiotis ; Ion Androutsopoulos</p><p>Abstract: We present a method that paraphrases a given sentence by first generating candidate paraphrases and then ranking (or classifying) them. The candidates are generated by applying existing paraphrasing rules extracted from parallel corpora. The ranking component considers not only the overall quality of the rules that produced each candidate, but also the extent to which they preserve grammaticality and meaning in the particular context of the input sentence, as well as the degree to which the candidate differs from the input. We experimented with both a Maximum Entropy classifier and an SVR ranker. Experimental results show that incorporating features from an existing paraphrase recognizer in the ranking component improves performance, and that our overall method compares well against a state of the art paraphrase generator, when paraphrasing rules apply to the input sentences. We also propose a new methodology to evaluate the ranking components of generate-and-rank paraphrase generators, which evaluates them across different combinations of weights for grammaticality, meaning preservation, and diversity. The paper is accompanied by a paraphrasing dataset we constructed for evaluations of this kind.</p><p>3 0.18001924 <a title="83-tfidf-3" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>Author: Jiajun Zhang ; Feifei Zhai ; Chengqing Zong</p><p>Abstract: Due to its explicit modeling of the grammaticality of the output via target-side syntax, the string-to-tree model has been shown to be one of the most successful syntax-based translation models. However, a major limitation of this model is that it does not utilize any useful syntactic information on the source side. In this paper, we analyze the difficulties of incorporating source syntax in a string-totree model. We then propose a new way to use the source syntax in a fuzzy manner, both in source syntactic annotation and in rule matching. We further explore three algorithms in rule matching: 0-1 matching, likelihood matching, and deep similarity matching. Our method not only guarantees grammatical output with an explicit target tree, but also enables the system to choose the proper translation rules via fuzzy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1</p><p>4 0.15716743 <a title="83-tfidf-4" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>Author: Wei Lu ; Hwee Tou Ng</p><p>Abstract: This paper describes a novel probabilistic approach for generating natural language sentences from their underlying semantics in the form of typed lambda calculus. The approach is built on top of a novel reduction-based weighted synchronous context free grammar formalism, which facilitates the transformation process from typed lambda calculus into natural language sentences. Sentences can then be generated based on such grammar rules with a log-linear model. To acquire such grammar rules automatically in an unsupervised manner, we also propose a novel approach with a generative model, which maps from sub-expressions of logical forms to word sequences in natural language sentences. Experiments on benchmark datasets for both English and Chinese generation tasks yield significant improvements over results obtained by two state-of-the-art machine translation models, in terms of both automatic metrics and human evaluation.</p><p>5 0.15093212 <a title="83-tfidf-5" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>Author: Xinyan Xiao ; Yang Liu ; Qun Liu ; Shouxun Lin</p><p>Abstract: Although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features, it is hard to scale up to large data due to decoding complexity. We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets.</p><p>6 0.13123788 <a title="83-tfidf-6" href="./emnlp-2011-Generating_Aspect-oriented_Multi-Document_Summarization_with_Event-aspect_model.html">61 emnlp-2011-Generating Aspect-oriented Multi-Document Summarization with Event-aspect model</a></p>
<p>7 0.12845482 <a title="83-tfidf-7" href="./emnlp-2011-Splitting_Noun_Compounds_via_Monolingual_and_Bilingual_Paraphrasing%3A_A_Study_on_Japanese_Katakana_Words.html">124 emnlp-2011-Splitting Noun Compounds via Monolingual and Bilingual Paraphrasing: A Study on Japanese Katakana Words</a></p>
<p>8 0.12411115 <a title="83-tfidf-8" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>9 0.11730335 <a title="83-tfidf-9" href="./emnlp-2011-Better_Evaluation_Metrics_Lead_to_Better_Machine_Translation.html">22 emnlp-2011-Better Evaluation Metrics Lead to Better Machine Translation</a></p>
<p>10 0.11651698 <a title="83-tfidf-10" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>11 0.11587283 <a title="83-tfidf-11" href="./emnlp-2011-Latent_Vector_Weighting_for_Word_Meaning_in_Context.html">80 emnlp-2011-Latent Vector Weighting for Word Meaning in Context</a></p>
<p>12 0.10914947 <a title="83-tfidf-12" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>13 0.10615042 <a title="83-tfidf-13" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>14 0.10444063 <a title="83-tfidf-14" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<p>15 0.1041429 <a title="83-tfidf-15" href="./emnlp-2011-Statistical_Machine_Translation_with_Local_Language_Models.html">125 emnlp-2011-Statistical Machine Translation with Local Language Models</a></p>
<p>16 0.094626226 <a title="83-tfidf-16" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>17 0.088766165 <a title="83-tfidf-17" href="./emnlp-2011-SMT_Helps_Bitext_Dependency_Parsing.html">118 emnlp-2011-SMT Helps Bitext Dependency Parsing</a></p>
<p>18 0.087776281 <a title="83-tfidf-18" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>19 0.087527439 <a title="83-tfidf-19" href="./emnlp-2011-Data-Driven_Response_Generation_in_Social_Media.html">38 emnlp-2011-Data-Driven Response Generation in Social Media</a></p>
<p>20 0.086375028 <a title="83-tfidf-20" href="./emnlp-2011-Minimum_Imputed-Risk%3A_Unsupervised_Discriminative_Training_for_Machine_Translation.html">93 emnlp-2011-Minimum Imputed-Risk: Unsupervised Discriminative Training for Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.291), (1, 0.15), (2, 0.061), (3, -0.287), (4, 0.017), (5, -0.182), (6, -0.196), (7, 0.253), (8, 0.065), (9, 0.166), (10, -0.112), (11, -0.005), (12, 0.263), (13, 0.03), (14, 0.173), (15, -0.168), (16, -0.015), (17, -0.099), (18, -0.012), (19, 0.053), (20, -0.01), (21, 0.15), (22, 0.059), (23, -0.135), (24, 0.047), (25, 0.033), (26, -0.072), (27, -0.08), (28, 0.031), (29, -0.042), (30, -0.02), (31, -0.033), (32, -0.111), (33, 0.011), (34, 0.042), (35, -0.045), (36, 0.049), (37, -0.037), (38, -0.039), (39, -0.005), (40, 0.12), (41, -0.005), (42, 0.068), (43, -0.071), (44, 0.057), (45, 0.05), (46, -0.001), (47, 0.008), (48, 0.002), (49, -0.001)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95113856 <a title="83-lsi-1" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>Author: Juri Ganitkevitch ; Chris Callison-Burch ; Courtney Napoles ; Benjamin Van Durme</p><p>Abstract: Previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora. However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems.</p><p>2 0.92925966 <a title="83-lsi-2" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>Author: Prodromos Malakasiotis ; Ion Androutsopoulos</p><p>Abstract: We present a method that paraphrases a given sentence by first generating candidate paraphrases and then ranking (or classifying) them. The candidates are generated by applying existing paraphrasing rules extracted from parallel corpora. The ranking component considers not only the overall quality of the rules that produced each candidate, but also the extent to which they preserve grammaticality and meaning in the particular context of the input sentence, as well as the degree to which the candidate differs from the input. We experimented with both a Maximum Entropy classifier and an SVR ranker. Experimental results show that incorporating features from an existing paraphrase recognizer in the ranking component improves performance, and that our overall method compares well against a state of the art paraphrase generator, when paraphrasing rules apply to the input sentences. We also propose a new methodology to evaluate the ranking components of generate-and-rank paraphrase generators, which evaluates them across different combinations of weights for grammaticality, meaning preservation, and diversity. The paper is accompanied by a paraphrasing dataset we constructed for evaluations of this kind.</p><p>3 0.54023492 <a title="83-lsi-3" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>Author: Kristian Woodsend ; Mirella Lapata</p><p>Abstract: Text simplification aims to rewrite text into simpler versions, and thus make information accessible to a broader audience. Most previous work simplifies sentences using handcrafted rules aimed at splitting long sentences, or substitutes difficult words using a predefined dictionary. This paper presents a datadriven model based on quasi-synchronous grammar, a formalism that can naturally capture structural mismatches and complex rewrite operations. We describe how such a grammar can be induced from Wikipedia and propose an integer linear programming model for selecting the most appropriate simplification from the space of possible rewrites generated by the grammar. We show experimentally that our method creates simplifications that significantly reduce the reading difficulty ofthe input, while maintaining grammaticality and preserving its meaning.</p><p>4 0.46852371 <a title="83-lsi-4" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>Author: Jiajun Zhang ; Feifei Zhai ; Chengqing Zong</p><p>Abstract: Due to its explicit modeling of the grammaticality of the output via target-side syntax, the string-to-tree model has been shown to be one of the most successful syntax-based translation models. However, a major limitation of this model is that it does not utilize any useful syntactic information on the source side. In this paper, we analyze the difficulties of incorporating source syntax in a string-totree model. We then propose a new way to use the source syntax in a fuzzy manner, both in source syntactic annotation and in rule matching. We further explore three algorithms in rule matching: 0-1 matching, likelihood matching, and deep similarity matching. Our method not only guarantees grammatical output with an explicit target tree, but also enables the system to choose the proper translation rules via fuzzy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1</p><p>5 0.43539977 <a title="83-lsi-5" href="./emnlp-2011-Splitting_Noun_Compounds_via_Monolingual_and_Bilingual_Paraphrasing%3A_A_Study_on_Japanese_Katakana_Words.html">124 emnlp-2011-Splitting Noun Compounds via Monolingual and Bilingual Paraphrasing: A Study on Japanese Katakana Words</a></p>
<p>Author: Nobuhiro Kaji ; Masaru Kitsuregawa</p><p>Abstract: Word boundaries within noun compounds are not marked by white spaces in a number of languages, unlike in English, and it is beneficial for various NLP applications to split such noun compounds. In the case of Japanese, noun compounds made up of katakana words (i.e., transliterated foreign words) are particularly difficult to split, because katakana words are highly productive and are often outof-vocabulary. To overcome this difficulty, we propose using monolingual and bilingual paraphrases of katakana noun compounds for identifying word boundaries. Experiments demonstrated that splitting accuracy is substantially improved by extracting such paraphrases from unlabeled textual data, the Web in our case, and then using that information for constructing splitting models.</p><p>6 0.41342509 <a title="83-lsi-6" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>7 0.40694383 <a title="83-lsi-7" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>8 0.36695004 <a title="83-lsi-8" href="./emnlp-2011-Divide_and_Conquer%3A_Crowdsourcing_the_Creation_of_Cross-Lingual_Textual_Entailment_Corpora.html">42 emnlp-2011-Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora</a></p>
<p>9 0.34263536 <a title="83-lsi-9" href="./emnlp-2011-Minimum_Imputed-Risk%3A_Unsupervised_Discriminative_Training_for_Machine_Translation.html">93 emnlp-2011-Minimum Imputed-Risk: Unsupervised Discriminative Training for Machine Translation</a></p>
<p>10 0.34042546 <a title="83-lsi-10" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>11 0.33200184 <a title="83-lsi-11" href="./emnlp-2011-Latent_Vector_Weighting_for_Word_Meaning_in_Context.html">80 emnlp-2011-Latent Vector Weighting for Word Meaning in Context</a></p>
<p>12 0.31218487 <a title="83-lsi-12" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>13 0.31212685 <a title="83-lsi-13" href="./emnlp-2011-Generating_Aspect-oriented_Multi-Document_Summarization_with_Event-aspect_model.html">61 emnlp-2011-Generating Aspect-oriented Multi-Document Summarization with Event-aspect model</a></p>
<p>14 0.31049672 <a title="83-lsi-14" href="./emnlp-2011-Data-Driven_Response_Generation_in_Social_Media.html">38 emnlp-2011-Data-Driven Response Generation in Social Media</a></p>
<p>15 0.30905211 <a title="83-lsi-15" href="./emnlp-2011-Better_Evaluation_Metrics_Lead_to_Better_Machine_Translation.html">22 emnlp-2011-Better Evaluation Metrics Lead to Better Machine Translation</a></p>
<p>16 0.30726808 <a title="83-lsi-16" href="./emnlp-2011-Cache-based_Document-level_Statistical_Machine_Translation.html">25 emnlp-2011-Cache-based Document-level Statistical Machine Translation</a></p>
<p>17 0.30145839 <a title="83-lsi-17" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<p>18 0.2868644 <a title="83-lsi-18" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>19 0.28600118 <a title="83-lsi-19" href="./emnlp-2011-SMT_Helps_Bitext_Dependency_Parsing.html">118 emnlp-2011-SMT Helps Bitext Dependency Parsing</a></p>
<p>20 0.27897865 <a title="83-lsi-20" href="./emnlp-2011-Accurate_Parsing_with_Compact_Tree-Substitution_Grammars%3A_Double-DOP.html">16 emnlp-2011-Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.115), (35, 0.023), (36, 0.028), (37, 0.023), (45, 0.038), (53, 0.037), (54, 0.052), (57, 0.022), (62, 0.021), (64, 0.03), (65, 0.012), (66, 0.072), (69, 0.022), (79, 0.059), (82, 0.021), (88, 0.264), (90, 0.019), (96, 0.04), (98, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76554531 <a title="83-lda-1" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>Author: Juri Ganitkevitch ; Chris Callison-Burch ; Courtney Napoles ; Benjamin Van Durme</p><p>Abstract: Previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora. However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems.</p><p>2 0.5393815 <a title="83-lda-2" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>Author: Jiajun Zhang ; Feifei Zhai ; Chengqing Zong</p><p>Abstract: Due to its explicit modeling of the grammaticality of the output via target-side syntax, the string-to-tree model has been shown to be one of the most successful syntax-based translation models. However, a major limitation of this model is that it does not utilize any useful syntactic information on the source side. In this paper, we analyze the difficulties of incorporating source syntax in a string-totree model. We then propose a new way to use the source syntax in a fuzzy manner, both in source syntactic annotation and in rule matching. We further explore three algorithms in rule matching: 0-1 matching, likelihood matching, and deep similarity matching. Our method not only guarantees grammatical output with an explicit target tree, but also enables the system to choose the proper translation rules via fuzzy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1</p><p>3 0.52523363 <a title="83-lda-3" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Noah A. Smith</p><p>Abstract: We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.</p><p>4 0.52471179 <a title="83-lda-4" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>Author: Christos Christodoulopoulos ; Sharon Goldwater ; Mark Steedman</p><p>Abstract: In this paper we present a fully unsupervised syntactic class induction system formulated as a Bayesian multinomial mixture model, where each word type is constrained to belong to a single class. By using a mixture model rather than a sequence model (e.g., HMM), we are able to easily add multiple kinds of features, including those at both the type level (morphology features) and token level (context and alignment features, the latter from parallel corpora). Using only context features, our system yields results comparable to state-of-the art, far better than a similar model without the one-class-per-type constraint. Using the additional features provides added benefit, and our final system outperforms the best published results on most of the 25 corpora tested.</p><p>5 0.51526207 <a title="83-lda-5" href="./emnlp-2011-Universal_Morphological_Analysis_using_Structured_Nearest_Neighbor_Prediction.html">140 emnlp-2011-Universal Morphological Analysis using Structured Nearest Neighbor Prediction</a></p>
<p>Author: Young-Bum Kim ; Joao Graca ; Benjamin Snyder</p><p>Abstract: In this paper, we consider the problem of unsupervised morphological analysis from a new angle. Past work has endeavored to design unsupervised learning methods which explicitly or implicitly encode inductive biases appropriate to the task at hand. We propose instead to treat morphological analysis as a structured prediction problem, where languages with labeled data serve as training examples for unlabeled languages, without the assumption of parallel data. We define a universal morphological feature space in which every language and its morphological analysis reside. We develop a novel structured nearest neighbor prediction method which seeks to find the morphological analysis for each unlabeled lan- guage which lies as close as possible in the feature space to a training language. We apply our model to eight inflecting languages, and induce nominal morphology with substantially higher accuracy than a traditional, MDLbased approach. Our analysis indicates that accuracy continues to improve substantially as the number of training languages increases.</p><p>6 0.51464456 <a title="83-lda-6" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>7 0.51344377 <a title="83-lda-7" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>8 0.51278383 <a title="83-lda-8" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>9 0.51249665 <a title="83-lda-9" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>10 0.50941271 <a title="83-lda-10" href="./emnlp-2011-Better_Evaluation_Metrics_Lead_to_Better_Machine_Translation.html">22 emnlp-2011-Better Evaluation Metrics Lead to Better Machine Translation</a></p>
<p>11 0.50934935 <a title="83-lda-11" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>12 0.50873899 <a title="83-lda-12" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>13 0.50661314 <a title="83-lda-13" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<p>14 0.50627458 <a title="83-lda-14" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>15 0.50575644 <a title="83-lda-15" href="./emnlp-2011-Lateen_EM%3A_Unsupervised_Training_with_Multiple_Objectives%2C_Applied_to_Dependency_Grammar_Induction.html">79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</a></p>
<p>16 0.50396496 <a title="83-lda-16" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>17 0.50377256 <a title="83-lda-17" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>18 0.50291377 <a title="83-lda-18" href="./emnlp-2011-Hierarchical_Phrase-based_Translation_Representations.html">66 emnlp-2011-Hierarchical Phrase-based Translation Representations</a></p>
<p>19 0.50265503 <a title="83-lda-19" href="./emnlp-2011-Lexical_Generalization_in_CCG_Grammar_Induction_for_Semantic_Parsing.html">87 emnlp-2011-Lexical Generalization in CCG Grammar Induction for Semantic Parsing</a></p>
<p>20 0.50151956 <a title="83-lda-20" href="./emnlp-2011-Multiword_Expression_Identification_with_Tree_Substitution_Grammars%3A_A_Parsing_tour_de_force_with_French.html">97 emnlp-2011-Multiword Expression Identification with Tree Substitution Grammars: A Parsing tour de force with French</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
