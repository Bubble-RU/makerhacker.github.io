<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>146 emnlp-2011-Unsupervised Structure Prediction with Non-Parallel Multilingual Guidance</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-146" href="#">emnlp2011-146</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>146 emnlp-2011-Unsupervised Structure Prediction with Non-Parallel Multilingual Guidance</h1>
<br/><p>Source: <a title="emnlp-2011-146-pdf" href="http://aclweb.org/anthology//D/D11/D11-1005.pdf">pdf</a></p><p>Author: Shay B. Cohen ; Dipanjan Das ; Noah A. Smith</p><p>Abstract: We describe a method for prediction of linguistic structure in a language for which only unlabeled data is available, using annotated data from a set of one or more helper languages. Our approach is based on a model that locally mixes between supervised models from the helper languages. Parallel data is not used, allowing the technique to be applied even in domains where human-translated texts are unavailable. We obtain state-of-theart performance for two tasks of structure prediction: unsupervised part-of-speech tagging and unsupervised dependency parsing.</p><p>Reference: <a title="emnlp-2011-146-reference" href="../emnlp2011_reference/emnlp-2011-Unsupervised_Structure_Prediction_with_Non-Parallel_Multilingual_Guidance_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  ,  ,  Abstract We describe a method for prediction of linguistic structure in a language for which only unlabeled data is available, using annotated data from a set of one or more helper languages. [sent-5, score-0.645]
</p><p>2 Our approach is based on a model that locally mixes between supervised models from the helper languages. [sent-6, score-0.682]
</p><p>3 We obtain state-of-theart performance for two tasks of structure prediction: unsupervised part-of-speech tagging and unsupervised dependency parsing. [sent-8, score-0.369]
</p><p>4 1 Introduction A major focus of recent NLP research has involved unsupervised learning of structure such as POS  tag sequences and parse trees (Klein and Manning, 2004; Johnson et al. [sent-9, score-0.213]
</p><p>5 1 In this paper, we present an approach to using  annotated data from one or more languages (helper languages) to learn models for another language that lacks annotated data (the target language). [sent-19, score-0.169]
</p><p>6 We focus on generative probabilistic models parameterized by multinomial distributions. [sent-24, score-0.167]
</p><p>7 We begin with supervised maximum likelihood estimates for models of the helper languages. [sent-25, score-0.648]
</p><p>8 In the second stage, we learn a model for the target language using unannotated data, maximizing likelihood over interpolations of the helper language models’ distributions. [sent-26, score-0.616]
</p><p>9 c e2th0o1d1s A ins Nocaitautiroanl L foarn Cguoamgpeu Ptartoicoensaslin Lgin,g puaigsetisc 5s0–61, drift down phylogenetic trees (Berg-Kirkpatrick and Klein, 2010) is comparable, but the practical assumption of supervised helper languages is new to this work. [sent-32, score-0.759]
</p><p>10 (2010) used universal syntactic categories and rules to improve grammar induction, but their model required expert handwritten rules as constraints. [sent-34, score-0.271]
</p><p>11 Herein, we specifically focus on two problems in linguistic structure prediction: unsupervised POS tagging and unsupervised dependency grammar induction. [sent-35, score-0.474]
</p><p>12 We also experiment with unsupervised learning of dependency structures from words, by combining our tagger and parser. [sent-38, score-0.221]
</p><p>13 Select a set of L helper languages for which there exists annotated data hD1, . [sent-43, score-0.756]
</p><p>14 The link between the languages is achieved through coarsegrained categories, which are now now commonplace (and arguably central to any theory of natural language syntax). [sent-75, score-0.166]
</p><p>15 A key novel contribution is the use of helper languages for initialization, and of unsupervised learning to learn the contribution of each helper language to that initialization (step 3). [sent-76, score-1.499]
</p><p>16 3  Interpolated Multilingual Probabilistic Context-Free Grammars  Our focus in this paper is on models that consist of multinomial distributions that have relationships between them through a generative process such as  a probabilistic context-free grammar (PCFG). [sent-78, score-0.275]
</p><p>17 The shaded area corresponds to a convex hull inside the probability simplex, indicating a mixture of the parameters of the four languages shown in the figure. [sent-80, score-0.409]
</p><p>18 Each such estimate , for 1 ≤ ‘ ≤ L, corresponds to a the maximum lik,e floirho 1od ≤ e ‘st i≤ma Lte, c boarsreedsp on asn tnoot aa ttheed data for the ‘th helper language. [sent-94, score-0.703]
</p><p>19 Then, to create a model for new language, we define a new set of parameters θ as:  θ(‘)  θk,i=XLβ‘,kθ(k‘,)i,  (4)  X‘=1  52 where β is the set of coefficients that we will now be interested in estimating (instePad of directly estimating θ). [sent-95, score-0.274]
</p><p>20 nd T dependency grammar induction we experiment with in §6. [sent-107, score-0.265]
</p><p>21 We assume that there exists L set of parameters for this PCFG each corresponding to a helper language. [sent-110, score-0.661]
</p><p>22 where rule A  θ(‘) →A α  (5)  is the probability associated with αin the ‘th helper language. [sent-128, score-0.616]
</p><p>23 At each point, the derivational process of this PCFG uses the nonterminal’s specific β coefficients to choose one of the helper languages. [sent-129, score-0.739]
</p><p>24 Such a construction allows more syntactic variability in the language we are trying to estimate, originating in the syntax of the various helper languages. [sent-134, score-0.616]
</p><p>25 These derivations will now include L K features of the form g‘,k (x, y), corresponding Kto a actouurnest o off th thee event o gf choosing the ‘th mixture component for multinomial k. [sent-140, score-0.441]
</p><p>26 ” 4  Inference and Parameter Estimation  The main building block commonly required for unsupervised learning in NLP is that of computing feature expectations for a given model. [sent-145, score-0.208]
</p><p>27 (2010) found that replacing traditional multinomial parameterizations with locally normalized, feature-based log-linear models was advantageous. [sent-162, score-0.17]
</p><p>28 For such a feature-rich model, our mul-  tilingual modeling framework still substitutes θ with a mixture of supervised multinomials for L helper languages as in Eq. [sent-164, score-1.122]
</p><p>29 However, for computational convenience, we also reparametrize the mixture coefficients β:  β‘,k=P‘L0e=x1pexγp‘,kγ‘0,k  (10)  Here, each γ‘,k is an Punconstrained parameter, and the above “softmax” transformation ensures that β lies within the probability simplex for context k. [sent-166, score-0.422]
</p><p>30 In addition to these estimation techniques, which are based on the optimization of the log-likelihood, we also consider a trivially simple technique for estimating β: setting βl,k to the uniform weight L−1,  where L is the number of helper languages. [sent-173, score-0.821]
</p><p>31 Whenever a probability θi within a multinomial dWishterinbeuvteiorn a in prvoolbvaebsi a coarse-grained category c as an event (i. [sent-176, score-0.188]
</p><p>32 4During this expansion process for a coarse event, we tried adding random noise to |λt−θ1i(c)| and renormalizing, to break  symmetry between the fine events, but that was found to be harmful in preliminary experiments. [sent-180, score-0.238]
</p><p>33 54 The result of this expansion is a model in the desired family; we use it to initialize conventional unsupervised parameter estimation. [sent-181, score-0.216]
</p><p>34 Lexical parameters, if any, do not undergo this expansion process, and they are estimated anew in the fine grained model during unsupervised learning, and are initialized using standard methods. [sent-182, score-0.209]
</p><p>35 We first note the characteristics of the datasets and the universal POS tags used in multilingual modeling. [sent-184, score-0.401]
</p><p>36 1 Data For our experiments, we fixed a set of four helper languages with relatively large amounts of data, displaying nontrivial linguistic diversity: Czech (Slavic), English (West-Germanic), German (WestGermanic), and Italian (Romance). [sent-186, score-0.727]
</p><p>37 This was the only set of helper languages we tested; improvements are likely possible. [sent-190, score-0.727]
</p><p>38 We leave an exploration of helper language choice (a subset selection problem) to future research, instead demonstrating that the concept has merit. [sent-191, score-0.616]
</p><p>39 Following standard practice, in unsupervised grammar induction experiments we remove punctuation and then eliminate sentences from the data of length greater than 10. [sent-197, score-0.295]
</p><p>40 These follow recent work by Das and Petrov (201 1) on unsupervised POS tagging in a multilingual setting with parallel data, and have been described in detail by Petrov et al. [sent-204, score-0.334]
</p><p>41 While there might be some controversy about what an appropriate universal tag set should include, these 12 categories (or a subset) cover the most frequent parts of speech and exist in one form or another in all of the languages that we studied. [sent-206, score-0.381]
</p><p>42 For each language in our data, a mapping from the fine-grained treebank POS tags to these universal  POS tags was constructed manually by Petrov et al. [sent-207, score-0.456]
</p><p>43 1 Model The model is a hidden Markov model (HMM), which has been popular for unsupervised tagging tasks (Merialdo, 1994; Elworthy, 1994; Smith and Eisner, 2005; Berg-Kirkpatrick et al. [sent-213, score-0.181]
</p><p>44 These locally normalized log-linear models can look at various aspects of the observation x given a tag y, or the pair of tags in a transition, incorporating overlapping features. [sent-217, score-0.26]
</p><p>45 (2010)  used only a single indicator feature of a tag pair, essentially equating to a traditional multinomial distribution. [sent-222, score-0.24]
</p><p>46 Since only the unlexicalized transition distributions are common across multiple languages, assuming that they all use a set of universal POS tags, akin to Eq. [sent-224, score-0.284]
</p><p>47 4, we can have a multilingual version of the transition distributions, by incorporating supervised helper transition probabilities. [sent-225, score-0.989]
</p><p>48 Thus, we can write:  θy→y0=‘XL=1β‘,yθy(→‘)y0  (11)  We use the above expression to replace the transition distributions, obtaining a multilingual mixture version of the model. [sent-226, score-0.5]
</p><p>49 Here, the transition probabilities  θy(→‘)y0  for the ‘th helper language are fixed after  beingy →esytimated using maximum likelihood estimation on the helper language’s treebank. [sent-227, score-1.385]
</p><p>50 g93 Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary constructed from the training section of the corresponding treebank. [sent-255, score-0.52]
</p><p>51 “Mixture+DG” is the model where multilingual mixture coefficients β of helper languages are estimated using coarse tags (§4), followed by expansion m(§5ul),t lainndg uthalen m initializing iDciGen wtsit βh tohfe h expanded turaagnesist aiorne parameters. [sent-258, score-1.605]
</p><p>52 “inUgn ciofoarrmse+ tDagGs” ( §is4 )th, efo case ewdh beyre e xβp are soent (to§ 1)/,4 a, ntdra tnhseintio innsi oiafl helper languages are pmanixdeedd, expanded, paanrda mtheetner sD. [sent-259, score-0.727]
</p><p>53 In case of (b), the tag dictionary solves the problem of tag identification and performance is measured using per word POS accuracy. [sent-262, score-0.285]
</p><p>54 (2010), while for the mixture case, we computed gradients with respect to γ, the unconstrained parameters used to express the mixture coefficients β (see Eq. [sent-267, score-0.674]
</p><p>55 0or1 our multilingual nm oofd tehle (step 3u ien §2), we similarly sampled trielainl gvuaalule ms ofrdoeml ( N(0, 0. [sent-274, score-0.215]
</p><p>56 01) ; as explained irena §2, coarse umnpivleerds falr tags are ,u0s. [sent-277, score-0.229]
</p><p>57 After the mixture parameters γ are estimated, we  compute the mixture probabilities β using Eq. [sent-279, score-0.551]
</p><p>58 56 Next, for each tag pair y, y0, we compute θy→y0, which are the coarse transition probabilities interpolated using β, given the helper languages. [sent-281, score-0.954]
</p><p>59 Finally, we train a feature-HMM by initializing its transition parameters with natural logarithms of the expanded θ parameters, and the emission parameters using small random real values sampled from N(0, 0. [sent-283, score-0.315]
</p><p>60 This implies that the lexicalized emissNio(n0 parameters η itmhaptl were previously easlitzimeda etemd sinthe coarse multilingual model are thrown away and not used for initialization; instead standard initialization is used. [sent-285, score-0.382]
</p><p>61 In this baseline, we set the number of HMM states to the number of fine-grained treebank tags for the given language. [sent-296, score-0.173]
</p><p>62 The first initializes training of the target language’s POS model using a uniform mixture ofthe helper language models (i. [sent-298, score-0.962]
</p><p>63 , each β‘,y = L1 = 14), and expansion from coarse-grained to fine-grained POS tags as described in §5. [sent-300, score-0.179]
</p><p>64 ” The second version estimates the mixture coefficients to maximize likelihood, then expands the POS tags (§5), using the result to initialize training of the  ftiangasl (m§5o)d,e ul. [sent-302, score-0.548]
</p><p>65 o”f No Tag Dictionary For each of the above configurations, we ran purely unsupervised training without a tag dictionary, and evaluated using one-to-one mapping accuracy constraining at most one HMM state to map to a unique treebank tag in the test data, using maximum bipartite matching. [sent-304, score-0.433]
</p><p>66 8 With a Tag Dictionary We also ran a second version of each experimental configuration, where we used a tag dictionary to restrict the possible path sequences of the HMM during both learning and inference. [sent-306, score-0.181]
</p><p>67 This tag dictionary was constructed only from the training section of a given language’s treebank. [sent-307, score-0.181]
</p><p>68 For this experiment we removed punctuation from the training and test data, enabling direct use within the dependency grammar induction experiments. [sent-309, score-0.265]
</p><p>69 We did not choose the other variant, many-to-one mapping accuracy, because quite often the metric mapped several HMM states to one treebank tag, leaving many treebank tags unaccounted for. [sent-311, score-0.259]
</p><p>70 Without a tag dictionary, in eight out of ten cases, either Uniform+DG or Mixture+DG outperforms the monolingual baseline (Table 2a). [sent-315, score-0.194]
</p><p>71 For six of these eight languages, the latter model where the mixture coefficients are learned automatically fares better than uniform weighting. [sent-316, score-0.469]
</p><p>72 With a tag dictionary, the multilingual variants outperform the baseline in seven out of ten cases, and the learned mixture outperforms or matches the uniform mixture in five of those seven (Table 2b). [sent-317, score-0.908]
</p><p>73 4 Dependency Grammar Induction We next describe experiments for dependency grammar induction. [sent-319, score-0.184]
</p><p>74 As the basic grammatical model, we adopt the dependency model with valence (Klein and Manning, 2004), which forms the basis for stateof-the-art results for dependency grammar induction in various settings (Cohen and Smith, 2009;  Spitkovsky et al. [sent-320, score-0.344]
</p><p>75 Uniform and Mixture behave similarly, with a slight advantage to the trained mixture setting. [sent-330, score-0.253]
</p><p>76 Using EM to train the mixture coefficients more often hurts than helps (six languages out of  ten). [sent-331, score-0.487]
</p><p>77 It is well known that likelihood does not cor9Its  supervised performance is still far from the supervised state of the art in dependency parsing. [sent-332, score-0.173]
</p><p>78 Table 3: Results for dependency grammar induction given gold-standard POS tags, reported as attachment accuracy (fraction of parents which are correct). [sent-333, score-0.265]
</p><p>79 Projection of the learned mixture coefficients  through PCA. [sent-338, score-0.376]
</p><p>80 relate with the true accuracy measurement, and so it is unsurprising that this holds in the constrained mixture family as well. [sent-345, score-0.283]
</p><p>81 In future work, a different parametrization of the mixture coefficients, through features, or perhaps a Bayesian prior on the weights, might lead to an objective that better simulates accuracy. [sent-346, score-0.253]
</p><p>82 Table 3 shows that even uniform mixture coefficients are sufficient to obtain accuracy which supercedes most unsupervised baselines. [sent-347, score-0.578]
</p><p>83 Our experiments also show that multilingual learning performs better for dependency grammar induction than part-of-speech tagging. [sent-351, score-0.418]
</p><p>84 The transition matrix in partof-speech tagging largely depends on word order in the various helper languages, which differs greatly. [sent-353, score-0.782]
</p><p>85 This means that a mixture of transition matrices will not necessarily yield a meaningful transition matrix. [sent-354, score-0.441]
</p><p>86 However, for dependency grammar, there are certain universal dependencies which appear in all helper languages, and therefore, a mixture between multinomials for these dependencies still yields a useful multinomial. [sent-355, score-1.26]
</p><p>87 5 Inducing Dependencies from Words Finally, we combine the models for POS tagging and grammar induction to perform grammar induction directly from words, instead of gold-standard POS tags. [sent-357, score-0.444]
</p><p>88 With a tag dictionary, learn a fine-grained POS tagging model unsupervised, using either DG or Mixture+DG as described in §6. [sent-359, score-0.176]
</p><p>89 956g0  Table  4:  Results for dependency  grammar induction over words. [sent-372, score-0.265]
</p><p>90 Given the two models, we infer POS tags on the test data using DG or Mixture+DG to get a lattice (Joint) or a sequence (Pipeline) and then parse using the model from the previous The resulting dependency trees are evaluated against the gold standard. [sent-394, score-0.201]
</p><p>91 In almost all cases, joint decoding of tags and trees performs better than the pipeline. [sent-397, score-0.166]
</p><p>92 Even though our part-of-speech tagger with multilingual guidance outperforms the completely unsupervised baseline, there is not always an advantage of using this multilingually guided partof-speech tagger for dependency grammar induction. [sent-398, score-0.558]
</p><p>93 For Turkish, Japanese, Slovene and Dutch, our unsupervised learner from words outperforms unsupervised parsing using gold-standard part-of-speech tags. [sent-399, score-0.218]
</p><p>94 Earlier work that induced part-of-speech tags and then performed unsupervised parsing in a pipeline includes Klein and Manning (2004) and Smith (2006). [sent-402, score-0.283]
</p><p>95 7  Conclusion  We presented an approach to exploiting annotated data in helper languages to infer part-of-speech tagging and dependency parsing models in a different, target language, without parallel data. [sent-405, score-0.907]
</p><p>96 We also described a way to do joint decoding of part-of-speech tags and dependencies which performs better than a pipeline. [sent-407, score-0.204]
</p><p>97 Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. [sent-463, score-0.291]
</p><p>98 Improving unsupervised dependency parsing with richer contexts and smoothing. [sent-520, score-0.188]
</p><p>99 From baby steps to leapfrog: How “less is more” in unsupervised dependency parsing. [sent-676, score-0.188]
</p><p>100 Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora. [sent-688, score-0.183]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('helper', 0.616), ('mixture', 0.253), ('multilingual', 0.153), ('dg', 0.149), ('multinomial', 0.136), ('universal', 0.126), ('coefficients', 0.123), ('tags', 0.122), ('pcfg', 0.117), ('languages', 0.111), ('multinomials', 0.11), ('unsupervised', 0.109), ('pos', 0.107), ('coarse', 0.107), ('grammar', 0.105), ('tag', 0.104), ('expectations', 0.099), ('transition', 0.094), ('uniform', 0.093), ('smith', 0.089), ('aa', 0.087), ('cohen', 0.081), ('induction', 0.081), ('dependency', 0.079), ('dictionary', 0.077), ('tagging', 0.072), ('hmm', 0.072), ('petrov', 0.068), ('klein', 0.068), ('naseem', 0.065), ('das', 0.062), ('estimation', 0.059), ('dmv', 0.058), ('snyder', 0.057), ('expansion', 0.057), ('em', 0.056), ('coarsegrained', 0.055), ('gillenwater', 0.055), ('estimating', 0.053), ('pipeline', 0.052), ('ten', 0.052), ('event', 0.052), ('nonterminal', 0.052), ('treebank', 0.051), ('initialize', 0.05), ('derivatives', 0.05), ('hg', 0.05), ('initialization', 0.047), ('conjoined', 0.046), ('simplex', 0.046), ('guidance', 0.046), ('parameters', 0.045), ('decoding', 0.044), ('avg', 0.043), ('tying', 0.043), ('fine', 0.043), ('montemagni', 0.042), ('ngiextfh', 0.042), ('udm', 0.042), ('hthee', 0.041), ('categories', 0.04), ('emission', 0.039), ('xl', 0.039), ('ganchev', 0.039), ('monolingual', 0.038), ('dependencies', 0.038), ('egt', 0.037), ('initializer', 0.037), ('dutch', 0.036), ('italian', 0.036), ('mapping', 0.035), ('distributions', 0.034), ('locally', 0.034), ('denoted', 0.034), ('ien', 0.033), ('interpolated', 0.033), ('tiger', 0.033), ('headden', 0.033), ('tagger', 0.033), ('supervised', 0.032), ('buchholz', 0.032), ('expanded', 0.032), ('understood', 0.032), ('parameterized', 0.031), ('danish', 0.031), ('slovene', 0.031), ('backbone', 0.031), ('initializing', 0.031), ('symmetry', 0.031), ('lexicalized', 0.03), ('projection', 0.03), ('state', 0.03), ('unlexicalized', 0.03), ('family', 0.03), ('mcdonald', 0.03), ('posterior', 0.029), ('sampled', 0.029), ('annotated', 0.029), ('lr', 0.029), ('pcfgs', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999893 <a title="146-tfidf-1" href="./emnlp-2011-Unsupervised_Structure_Prediction_with_Non-Parallel_Multilingual_Guidance.html">146 emnlp-2011-Unsupervised Structure Prediction with Non-Parallel Multilingual Guidance</a></p>
<p>Author: Shay B. Cohen ; Dipanjan Das ; Noah A. Smith</p><p>Abstract: We describe a method for prediction of linguistic structure in a language for which only unlabeled data is available, using annotated data from a set of one or more helper languages. Our approach is based on a model that locally mixes between supervised models from the helper languages. Parallel data is not used, allowing the technique to be applied even in domains where human-translated texts are unavailable. We obtain state-of-theart performance for two tasks of structure prediction: unsupervised part-of-speech tagging and unsupervised dependency parsing.</p><p>2 0.22591098 <a title="146-tfidf-2" href="./emnlp-2011-Multi-Source_Transfer_of_Delexicalized_Dependency_Parsers.html">95 emnlp-2011-Multi-Source Transfer of Delexicalized Dependency Parsers</a></p>
<p>Author: Ryan McDonald ; Slav Petrov ; Keith Hall</p><p>Abstract: We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source lan- guages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.</p><p>3 0.18868764 <a title="146-tfidf-3" href="./emnlp-2011-Unsupervised_Dependency_Parsing_without_Gold_Part-of-Speech_Tags.html">141 emnlp-2011-Unsupervised Dependency Parsing without Gold Part-of-Speech Tags</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Angel X. Chang ; Daniel Jurafsky</p><p>Abstract: We show that categories induced by unsupervised word clustering can surpass the performance of gold part-of-speech tags in dependency grammar induction. Unlike classic clustering algorithms, our method allows a word to have different tags in different contexts. In an ablative analysis, we first demonstrate that this context-dependence is crucial to the superior performance of gold tags — requiring a word to always have the same part-ofspeech significantly degrades the performance of manual tags in grammar induction, eliminating the advantage that human annotation has over unsupervised tags. We then introduce a sequence modeling technique that combines the output of a word clustering algorithm with context-colored noise, to allow words to be tagged differently in different contexts. With these new induced tags as input, our state-of- the-art dependency grammar inducer achieves 59. 1% directed accuracy on Section 23 (all sentences) of the Wall Street Journal (WSJ) corpus — 0.7% higher than using gold tags.</p><p>4 0.15950106 <a title="146-tfidf-4" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>Author: Christos Christodoulopoulos ; Sharon Goldwater ; Mark Steedman</p><p>Abstract: In this paper we present a fully unsupervised syntactic class induction system formulated as a Bayesian multinomial mixture model, where each word type is constrained to belong to a single class. By using a mixture model rather than a sequence model (e.g., HMM), we are able to easily add multiple kinds of features, including those at both the type level (morphology features) and token level (context and alignment features, the latter from parallel corpora). Using only context features, our system yields results comparable to state-of-the art, far better than a similar model without the one-class-per-type constraint. Using the additional features provides added benefit, and our final system outperforms the best published results on most of the 25 corpora tested.</p><p>5 0.15663446 <a title="146-tfidf-5" href="./emnlp-2011-Universal_Morphological_Analysis_using_Structured_Nearest_Neighbor_Prediction.html">140 emnlp-2011-Universal Morphological Analysis using Structured Nearest Neighbor Prediction</a></p>
<p>Author: Young-Bum Kim ; Joao Graca ; Benjamin Snyder</p><p>Abstract: In this paper, we consider the problem of unsupervised morphological analysis from a new angle. Past work has endeavored to design unsupervised learning methods which explicitly or implicitly encode inductive biases appropriate to the task at hand. We propose instead to treat morphological analysis as a structured prediction problem, where languages with labeled data serve as training examples for unlabeled languages, without the assumption of parallel data. We define a universal morphological feature space in which every language and its morphological analysis reside. We develop a novel structured nearest neighbor prediction method which seeks to find the morphological analysis for each unlabeled lan- guage which lies as close as possible in the feature space to a training language. We apply our model to eight inflecting languages, and induce nominal morphology with substantially higher accuracy than a traditional, MDLbased approach. Our analysis indicates that accuracy continues to improve substantially as the number of training languages increases.</p><p>6 0.14866047 <a title="146-tfidf-6" href="./emnlp-2011-Joint_Models_for_Chinese_POS_Tagging_and_Dependency_Parsing.html">75 emnlp-2011-Joint Models for Chinese POS Tagging and Dependency Parsing</a></p>
<p>7 0.13896364 <a title="146-tfidf-7" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>8 0.11993639 <a title="146-tfidf-8" href="./emnlp-2011-Relaxed_Cross-lingual_Projection_of_Constituent_Syntax.html">115 emnlp-2011-Relaxed Cross-lingual Projection of Constituent Syntax</a></p>
<p>9 0.095363989 <a title="146-tfidf-9" href="./emnlp-2011-Statistical_Machine_Translation_with_Local_Language_Models.html">125 emnlp-2011-Statistical Machine Translation with Local Language Models</a></p>
<p>10 0.093655936 <a title="146-tfidf-10" href="./emnlp-2011-A_Fast%2C_Accurate%2C_Non-Projective%2C_Semantically-Enriched_Parser.html">4 emnlp-2011-A Fast, Accurate, Non-Projective, Semantically-Enriched Parser</a></p>
<p>11 0.081785314 <a title="146-tfidf-11" href="./emnlp-2011-Exact_Inference_for_Generative_Probabilistic_Non-Projective_Dependency_Parsing.html">52 emnlp-2011-Exact Inference for Generative Probabilistic Non-Projective Dependency Parsing</a></p>
<p>12 0.077066623 <a title="146-tfidf-12" href="./emnlp-2011-Accurate_Parsing_with_Compact_Tree-Substitution_Grammars%3A_Double-DOP.html">16 emnlp-2011-Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP</a></p>
<p>13 0.075319186 <a title="146-tfidf-13" href="./emnlp-2011-Computation_of_Infix_Probabilities_for_Probabilistic_Context-Free_Grammars.html">31 emnlp-2011-Computation of Infix Probabilities for Probabilistic Context-Free Grammars</a></p>
<p>14 0.073508941 <a title="146-tfidf-14" href="./emnlp-2011-A_Simple_Word_Trigger_Method_for_Social_Tag_Suggestion.html">11 emnlp-2011-A Simple Word Trigger Method for Social Tag Suggestion</a></p>
<p>15 0.07317438 <a title="146-tfidf-15" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>16 0.07239034 <a title="146-tfidf-16" href="./emnlp-2011-Structured_Sparsity_in_Structured_Prediction.html">129 emnlp-2011-Structured Sparsity in Structured Prediction</a></p>
<p>17 0.071894728 <a title="146-tfidf-17" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>18 0.068741791 <a title="146-tfidf-18" href="./emnlp-2011-Lateen_EM%3A_Unsupervised_Training_with_Multiple_Objectives%2C_Applied_to_Dependency_Grammar_Induction.html">79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</a></p>
<p>19 0.067855299 <a title="146-tfidf-19" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>20 0.067070171 <a title="146-tfidf-20" href="./emnlp-2011-Multilayer_Sequence_Labeling.html">96 emnlp-2011-Multilayer Sequence Labeling</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.273), (1, 0.084), (2, -0.054), (3, 0.175), (4, -0.049), (5, 0.173), (6, -0.18), (7, -0.05), (8, -0.199), (9, -0.024), (10, -0.109), (11, 0.124), (12, 0.044), (13, 0.073), (14, 0.124), (15, 0.046), (16, 0.071), (17, 0.081), (18, -0.106), (19, 0.11), (20, -0.031), (21, -0.047), (22, 0.077), (23, 0.026), (24, 0.09), (25, 0.18), (26, -0.01), (27, -0.099), (28, -0.029), (29, -0.131), (30, -0.019), (31, -0.0), (32, -0.016), (33, 0.053), (34, -0.034), (35, 0.067), (36, 0.014), (37, 0.025), (38, 0.054), (39, 0.047), (40, 0.018), (41, -0.046), (42, -0.103), (43, 0.011), (44, -0.004), (45, -0.008), (46, 0.004), (47, 0.003), (48, 0.008), (49, -0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95799017 <a title="146-lsi-1" href="./emnlp-2011-Unsupervised_Structure_Prediction_with_Non-Parallel_Multilingual_Guidance.html">146 emnlp-2011-Unsupervised Structure Prediction with Non-Parallel Multilingual Guidance</a></p>
<p>Author: Shay B. Cohen ; Dipanjan Das ; Noah A. Smith</p><p>Abstract: We describe a method for prediction of linguistic structure in a language for which only unlabeled data is available, using annotated data from a set of one or more helper languages. Our approach is based on a model that locally mixes between supervised models from the helper languages. Parallel data is not used, allowing the technique to be applied even in domains where human-translated texts are unavailable. We obtain state-of-theart performance for two tasks of structure prediction: unsupervised part-of-speech tagging and unsupervised dependency parsing.</p><p>2 0.72140074 <a title="146-lsi-2" href="./emnlp-2011-Unsupervised_Dependency_Parsing_without_Gold_Part-of-Speech_Tags.html">141 emnlp-2011-Unsupervised Dependency Parsing without Gold Part-of-Speech Tags</a></p>
<p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Angel X. Chang ; Daniel Jurafsky</p><p>Abstract: We show that categories induced by unsupervised word clustering can surpass the performance of gold part-of-speech tags in dependency grammar induction. Unlike classic clustering algorithms, our method allows a word to have different tags in different contexts. In an ablative analysis, we first demonstrate that this context-dependence is crucial to the superior performance of gold tags — requiring a word to always have the same part-ofspeech significantly degrades the performance of manual tags in grammar induction, eliminating the advantage that human annotation has over unsupervised tags. We then introduce a sequence modeling technique that combines the output of a word clustering algorithm with context-colored noise, to allow words to be tagged differently in different contexts. With these new induced tags as input, our state-of- the-art dependency grammar inducer achieves 59. 1% directed accuracy on Section 23 (all sentences) of the Wall Street Journal (WSJ) corpus — 0.7% higher than using gold tags.</p><p>3 0.71606338 <a title="146-lsi-3" href="./emnlp-2011-Multi-Source_Transfer_of_Delexicalized_Dependency_Parsers.html">95 emnlp-2011-Multi-Source Transfer of Delexicalized Dependency Parsers</a></p>
<p>Author: Ryan McDonald ; Slav Petrov ; Keith Hall</p><p>Abstract: We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source lan- guages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.</p><p>4 0.64793378 <a title="146-lsi-4" href="./emnlp-2011-Universal_Morphological_Analysis_using_Structured_Nearest_Neighbor_Prediction.html">140 emnlp-2011-Universal Morphological Analysis using Structured Nearest Neighbor Prediction</a></p>
<p>Author: Young-Bum Kim ; Joao Graca ; Benjamin Snyder</p><p>Abstract: In this paper, we consider the problem of unsupervised morphological analysis from a new angle. Past work has endeavored to design unsupervised learning methods which explicitly or implicitly encode inductive biases appropriate to the task at hand. We propose instead to treat morphological analysis as a structured prediction problem, where languages with labeled data serve as training examples for unlabeled languages, without the assumption of parallel data. We define a universal morphological feature space in which every language and its morphological analysis reside. We develop a novel structured nearest neighbor prediction method which seeks to find the morphological analysis for each unlabeled lan- guage which lies as close as possible in the feature space to a training language. We apply our model to eight inflecting languages, and induce nominal morphology with substantially higher accuracy than a traditional, MDLbased approach. Our analysis indicates that accuracy continues to improve substantially as the number of training languages increases.</p><p>5 0.62720907 <a title="146-lsi-5" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>Author: Christos Christodoulopoulos ; Sharon Goldwater ; Mark Steedman</p><p>Abstract: In this paper we present a fully unsupervised syntactic class induction system formulated as a Bayesian multinomial mixture model, where each word type is constrained to belong to a single class. By using a mixture model rather than a sequence model (e.g., HMM), we are able to easily add multiple kinds of features, including those at both the type level (morphology features) and token level (context and alignment features, the latter from parallel corpora). Using only context features, our system yields results comparable to state-of-the art, far better than a similar model without the one-class-per-type constraint. Using the additional features provides added benefit, and our final system outperforms the best published results on most of the 25 corpora tested.</p><p>6 0.59203064 <a title="146-lsi-6" href="./emnlp-2011-Relaxed_Cross-lingual_Projection_of_Constituent_Syntax.html">115 emnlp-2011-Relaxed Cross-lingual Projection of Constituent Syntax</a></p>
<p>7 0.54639381 <a title="146-lsi-7" href="./emnlp-2011-Joint_Models_for_Chinese_POS_Tagging_and_Dependency_Parsing.html">75 emnlp-2011-Joint Models for Chinese POS Tagging and Dependency Parsing</a></p>
<p>8 0.51947033 <a title="146-lsi-8" href="./emnlp-2011-A_Simple_Word_Trigger_Method_for_Social_Tag_Suggestion.html">11 emnlp-2011-A Simple Word Trigger Method for Social Tag Suggestion</a></p>
<p>9 0.44310024 <a title="146-lsi-9" href="./emnlp-2011-Lateen_EM%3A_Unsupervised_Training_with_Multiple_Objectives%2C_Applied_to_Dependency_Grammar_Induction.html">79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</a></p>
<p>10 0.43943509 <a title="146-lsi-10" href="./emnlp-2011-Accurate_Parsing_with_Compact_Tree-Substitution_Grammars%3A_Double-DOP.html">16 emnlp-2011-Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP</a></p>
<p>11 0.3907679 <a title="146-lsi-11" href="./emnlp-2011-Discovering_Morphological_Paradigms_from_Plain_Text_Using_a_Dirichlet_Process_Mixture_Model.html">39 emnlp-2011-Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model</a></p>
<p>12 0.37094408 <a title="146-lsi-12" href="./emnlp-2011-Statistical_Machine_Translation_with_Local_Language_Models.html">125 emnlp-2011-Statistical Machine Translation with Local Language Models</a></p>
<p>13 0.36486009 <a title="146-lsi-13" href="./emnlp-2011-Structured_Sparsity_in_Structured_Prediction.html">129 emnlp-2011-Structured Sparsity in Structured Prediction</a></p>
<p>14 0.36206573 <a title="146-lsi-14" href="./emnlp-2011-Multilayer_Sequence_Labeling.html">96 emnlp-2011-Multilayer Sequence Labeling</a></p>
<p>15 0.34698728 <a title="146-lsi-15" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>16 0.34481081 <a title="146-lsi-16" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>17 0.34158313 <a title="146-lsi-17" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>18 0.33533722 <a title="146-lsi-18" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>19 0.33241123 <a title="146-lsi-19" href="./emnlp-2011-Computation_of_Infix_Probabilities_for_Probabilistic_Context-Free_Grammars.html">31 emnlp-2011-Computation of Infix Probabilities for Probabilistic Context-Free Grammars</a></p>
<p>20 0.32668731 <a title="146-lsi-20" href="./emnlp-2011-A_Fast%2C_Accurate%2C_Non-Projective%2C_Semantically-Enriched_Parser.html">4 emnlp-2011-A Fast, Accurate, Non-Projective, Semantically-Enriched Parser</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.073), (36, 0.026), (37, 0.02), (45, 0.058), (53, 0.031), (54, 0.029), (57, 0.011), (62, 0.015), (64, 0.411), (66, 0.065), (69, 0.022), (79, 0.062), (82, 0.034), (90, 0.019), (96, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92242295 <a title="146-lda-1" href="./emnlp-2011-Dual_Decomposition_with_Many_Overlapping_Components.html">45 emnlp-2011-Dual Decomposition with Many Overlapping Components</a></p>
<p>Author: Andre Martins ; Noah Smith ; Mario Figueiredo ; Pedro Aguiar</p><p>Abstract: Dual decomposition has been recently proposed as a way of combining complementary models, with a boost in predictive power. However, in cases where lightweight decompositions are not readily available (e.g., due to the presence of rich features or logical constraints), the original subgradient algorithm is inefficient. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1</p><p>2 0.87101567 <a title="146-lda-2" href="./emnlp-2011-Multi-Source_Transfer_of_Delexicalized_Dependency_Parsers.html">95 emnlp-2011-Multi-Source Transfer of Delexicalized Dependency Parsers</a></p>
<p>Author: Ryan McDonald ; Slav Petrov ; Keith Hall</p><p>Abstract: We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source lan- guages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.</p><p>same-paper 3 0.87003374 <a title="146-lda-3" href="./emnlp-2011-Unsupervised_Structure_Prediction_with_Non-Parallel_Multilingual_Guidance.html">146 emnlp-2011-Unsupervised Structure Prediction with Non-Parallel Multilingual Guidance</a></p>
<p>Author: Shay B. Cohen ; Dipanjan Das ; Noah A. Smith</p><p>Abstract: We describe a method for prediction of linguistic structure in a language for which only unlabeled data is available, using annotated data from a set of one or more helper languages. Our approach is based on a model that locally mixes between supervised models from the helper languages. Parallel data is not used, allowing the technique to be applied even in domains where human-translated texts are unavailable. We obtain state-of-theart performance for two tasks of structure prediction: unsupervised part-of-speech tagging and unsupervised dependency parsing.</p><p>4 0.52875382 <a title="146-lda-4" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<p>Author: Sebastian Riedel ; Andrew McCallum</p><p>Abstract: Extracting biomedical events from literature has attracted much recent attention. The bestperforming systems so far have been pipelines of simple subtask-specific local classifiers. A natural drawback of such approaches are cascading errors introduced in early stages of the pipeline. We present three joint models of increasing complexity designed to overcome this problem. The first model performs joint trigger and argument extraction, and lends itself to a simple, efficient and exact inference algorithm. The second model captures correlations between events, while the third model ensures consistency between arguments of the same event. Inference in these models is kept tractable through dual decomposition. The first two models outperform the previous best joint approaches and are very competitive with respect to the current state-of-theart. The third model yields the best results reported so far on the BioNLP 2009 shared task, the BioNLP 2011 Genia task and the BioNLP 2011Infectious Diseases task.</p><p>5 0.51513207 <a title="146-lda-5" href="./emnlp-2011-Simple_Effective_Decipherment_via_Combinatorial_Optimization.html">122 emnlp-2011-Simple Effective Decipherment via Combinatorial Optimization</a></p>
<p>Author: Taylor Berg-Kirkpatrick ; Dan Klein</p><p>Abstract: We present a simple objective function that when optimized yields accurate solutions to both decipherment and cognate pair identification problems. The objective simultaneously scores a matching between two alphabets and a matching between two lexicons, each in a different language. We introduce a simple coordinate descent procedure that efficiently finds effective solutions to the resulting combinatorial optimization problem. Our system requires only a list of words in both languages as input, yet it competes with and surpasses several state-of-the-art systems that are both substantially more complex and make use of more information.</p><p>6 0.49415565 <a title="146-lda-6" href="./emnlp-2011-Universal_Morphological_Analysis_using_Structured_Nearest_Neighbor_Prediction.html">140 emnlp-2011-Universal Morphological Analysis using Structured Nearest Neighbor Prediction</a></p>
<p>7 0.49073821 <a title="146-lda-7" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>8 0.47468343 <a title="146-lda-8" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>9 0.47042996 <a title="146-lda-9" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>10 0.46774498 <a title="146-lda-10" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>11 0.45303354 <a title="146-lda-11" href="./emnlp-2011-Large-Scale_Cognate_Recovery.html">77 emnlp-2011-Large-Scale Cognate Recovery</a></p>
<p>12 0.44394356 <a title="146-lda-12" href="./emnlp-2011-A_Simple_Word_Trigger_Method_for_Social_Tag_Suggestion.html">11 emnlp-2011-A Simple Word Trigger Method for Social Tag Suggestion</a></p>
<p>13 0.44366285 <a title="146-lda-13" href="./emnlp-2011-Relaxed_Cross-lingual_Projection_of_Constituent_Syntax.html">115 emnlp-2011-Relaxed Cross-lingual Projection of Constituent Syntax</a></p>
<p>14 0.43853766 <a title="146-lda-14" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>15 0.43809536 <a title="146-lda-15" href="./emnlp-2011-Harnessing_different_knowledge_sources_to_measure_semantic_relatedness_under_a_uniform_model.html">64 emnlp-2011-Harnessing different knowledge sources to measure semantic relatedness under a uniform model</a></p>
<p>16 0.43791026 <a title="146-lda-16" href="./emnlp-2011-A_Word_Reordering_Model_for_Improved_Machine_Translation.html">13 emnlp-2011-A Word Reordering Model for Improved Machine Translation</a></p>
<p>17 0.43607891 <a title="146-lda-17" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>18 0.43195915 <a title="146-lda-18" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>19 0.43184832 <a title="146-lda-19" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<p>20 0.43135041 <a title="146-lda-20" href="./emnlp-2011-Language_Models_for_Machine_Translation%3A_Original_vs._Translated_Texts.html">76 emnlp-2011-Language Models for Machine Translation: Original vs. Translated Texts</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
