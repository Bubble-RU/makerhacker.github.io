<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-85" href="#">emnlp2011-85</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</h1>
<br/><p>Source: <a title="emnlp-2011-85-pdf" href="http://aclweb.org/anthology//D/D11/D11-1038.pdf">pdf</a></p><p>Author: Kristian Woodsend ; Mirella Lapata</p><p>Abstract: Text simplification aims to rewrite text into simpler versions, and thus make information accessible to a broader audience. Most previous work simplifies sentences using handcrafted rules aimed at splitting long sentences, or substitutes difficult words using a predefined dictionary. This paper presents a datadriven model based on quasi-synchronous grammar, a formalism that can naturally capture structural mismatches and complex rewrite operations. We describe how such a grammar can be induced from Wikipedia and propose an integer linear programming model for selecting the most appropriate simplification from the space of possible rewrites generated by the grammar. We show experimentally that our method creates simplifications that significantly reduce the reading difficulty ofthe input, while maintaining grammaticality and preserving its meaning.</p><p>Reference: <a title="emnlp-2011-85-reference" href="../emnlp2011_reference/emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract Text simplification aims to rewrite text into simpler versions, and thus make information accessible to a broader audience. [sent-4, score-0.657]
</p><p>2 Most previous work simplifies sentences using handcrafted rules aimed at splitting long sentences, or substitutes difficult words using a predefined dictionary. [sent-5, score-0.215]
</p><p>3 We describe how such a grammar can be induced from Wikipedia and propose an integer linear programming model for selecting the most appropriate simplification from the space of possible rewrites generated by the grammar. [sent-7, score-0.56]
</p><p>4 We show experimentally that our method creates simplifications that significantly reduce the reading difficulty  ofthe input, while maintaining grammaticality and preserving its meaning. [sent-8, score-0.351]
</p><p>5 1 Introduction Sentence simplification is perhaps one of the oldest text rewriting problems. [sent-9, score-0.484]
</p><p>6 Given a source sentence, the goal is to create a grammatical target that is easier to read with simpler vocabulary and syntactic structure. [sent-10, score-0.294]
</p><p>7 An example is shown in Table 1 involving a broad spectrum of rewrite operations such as deletion, substitution, insertion, and reordering. [sent-11, score-0.198]
</p><p>8 The popularity of the simplification task stems from its potential relevance to various applications. [sent-12, score-0.415]
</p><p>9 uk  Table 1: Example of a source sentence (top) and its simplification (bottom). [sent-18, score-0.54]
</p><p>10 A simplification component could be also used as a preprocessing step to improve the performance of parsers (Chandrasekar et al. [sent-21, score-0.415]
</p><p>11 , rewrite rules), but also be able to combine them to generate new  text, in a simpler language. [sent-27, score-0.242]
</p><p>12 The task is also distinct from sentence compression as it aims to render a sentence more accessible while preserving its meaning. [sent-28, score-0.174]
</p><p>13 In fact, one of the commonest simplification operations is sentence splitting which usually produces longer rather than shorter output! [sent-30, score-0.648]
</p><p>14 In this paper we propose a sentence simplification model that is able to handle structural mismatches and complex rewriting operations. [sent-35, score-0.54]
</p><p>15 Rather than postulating a strictly syn-  chronous structure over the source and target sentences, QG identifies a “sloppy” alignment of parse trees assuming that the target tree is in some way “inspired by” the source tree. [sent-37, score-0.349]
</p><p>16 Given a source tree, it finds the best target tree licensed by the grammar subject to constraints such as sentence length and reading ease. [sent-39, score-0.377]
</p><p>17 Contrary to most previous approaches (see the discussion in Section 2) which rely heavily on hand-crafted rules, our model learns simplification rewrites automatically from examples of source-target sentences. [sent-42, score-0.453]
</p><p>18 We exploit Wikipedia and create a (parallel) simplification corpus in two ways: by aligning MainEW sentences to their SimpleEW counterparts, and by extracting training instances from SimpleEW revision histories, thus leveraging Wikipedia’s collaborative editing process. [sent-47, score-0.584]
</p><p>19 Our experimental results demonstrate that a simplification model can be learned from Wikipedia 410 data alone without any manual effort. [sent-48, score-0.415]
</p><p>20 Perhaps unsurprisingly, the quality of the QG grammar rules greatly improves when these are learned from revision histories which are less noisy than sentence alignments. [sent-49, score-0.47]
</p><p>21 , 2010) our model yields significantly simpler output that is both grammatical and meaning preserving. [sent-51, score-0.22]
</p><p>22 2  Related Work  Sentence simplification has attracted a great deal of attention due to its potential impact on society. [sent-52, score-0.415]
</p><p>23 The literature is rife with attempts to simplify text using mostly hand-crafted syntactic rules aimed at splitting long and complicated sentences into several simpler ones (Carroll et al. [sent-53, score-0.382]
</p><p>24 Other work focuses on lexical simplifications and substitutes difficult words by more common WordNet synonyms or paraphrases found in a predefined dictionary (Devlin, 1999; Inui et al. [sent-56, score-0.191]
</p><p>25 (2010) explore data-driven methods to learn lexical simplifications from Wikipedia revision histories. [sent-60, score-0.325]
</p><p>26 (2010) also use Wikipedia to learn a sentence simplification model which is able to perform four rewrite operations, namely substitution, reordering, splitting, and deletion. [sent-63, score-0.605]
</p><p>27 m Tehneti torannedsl ftoiounr rewrite operations whose probabilities are estimated from a parallel corpus of MainEW and SimpleEW sentences using an expectation maximization algorithm. [sent-66, score-0.233]
</p><p>28 Their decoder translates sentences into simpler alternatives by greedily selecting the branch in the source tree with the highest probability. [sent-67, score-0.248]
</p><p>29 Our own work formulates sentence simplification in the framework of Quasi-synchronous grammar (QG, Smith and Eisner 2006). [sent-68, score-0.547]
</p><p>30 QG allows to describe non-isomorphic tree pairs (the grammar rules can comprise trees of arbitrary depth, and fragments can be mapped) and is thus suited to text-rewriting tasks which typically involve a number of local modifications to the input text. [sent-69, score-0.205]
</p><p>31 We use quasi-synchronous grammar to learn a wide range of rewrite operations capturing both lexical and structural simplifi-  cations naturally without any additional rule engineering. [sent-70, score-0.329]
</p><p>32 , substitution or splitting) are not modeled explicitly; instead, we leave it up to our grammar extraction algorithm to learn appropriate rules that reflect the training data. [sent-75, score-0.24]
</p><p>33 Furthermore, our learning framework is not limited to simplification and could be easily adapted to other rewriting tasks. [sent-79, score-0.484]
</p><p>34 3  Sentence Simplification Model  Our model takes a single sentence as input and creates a version that is simpler to read. [sent-85, score-0.193]
</p><p>35 Given an input sentence, our model decon-  structs it into component phrases and clauses, each of which is simplified (lexically and structurally) through QG rewrite rules. [sent-94, score-0.215]
</p><p>36 We generate all possible simplifications for a given input and use the ILP to find the best target subject to grammaticality constraints. [sent-95, score-0.303]
</p><p>37 In what follows we first detail how we extract QG rewrite rules as these form the backbone of our model and then formulate the ILP proper. [sent-96, score-0.227]
</p><p>38 A grammar node in the target tree T2 is modeled on a subset of nodes in the source tree, with a rather loose alignment between the trees. [sent-103, score-0.416]
</p><p>39 We take aligned sentence pairs represented as phrase structure trees and build up a list of leaf node  alignments based on lexical identity. [sent-104, score-0.314]
</p><p>40 QG rules are created from aligned nodes above the leaf node level if the all the nodes in the target tree can be explained using nodes from the source. [sent-106, score-0.577]
</p><p>41 Syntactic simplification rules Each QG rule describes the transformations required from source to target phrase sub-trees. [sent-109, score-0.717]
</p><p>42 Lexical simplification rules Lexical substitutions are an important part of simplification. [sent-122, score-0.567]
</p><p>43 We learn them from aligned sub-trees, in the same way as described above for syntax rules, by allowing a small number of lexical substitutions to be present in the rules, and provided they do not include proper nouns. [sent-123, score-0.174]
</p><p>44 (2010) learn lexical simplifications without taking syntactic context into account. [sent-126, score-0.191]
</p><p>45 For rules where the syntactic structures of  the source and target sub-trees match, and the only 412 difference is a lexical substitution, we construct a more general rule by extracting the words and corresponding POS tags involved in the substitution. [sent-128, score-0.291]
</p><p>46 Then at the generation stage, identifying suitable rules depends only on the substitution words, rather than the surrounding syntactic context. [sent-129, score-0.196]
</p><p>47 Sentence splitting rules Another important simplification technique is to split syntactically complicated sentences into several shorter ones. [sent-131, score-0.656]
</p><p>48 To learn QG rules for this operation, the source sentence is aligned with two consecutive target sentences. [sent-132, score-0.379]
</p><p>49 Rather than expecting to discover a split point in the source sentence, we attempt to identify a node in the source parse tree that contributes to both of the two target sentences. [sent-133, score-0.321]
</p><p>50 Our intuition is that one of the target sentences will follow the general syntactic structure of the source sentence. [sent-134, score-0.178]
</p><p>51 A node in the source sentence parse tree will be aligned with a (similar but  simpler) node in the main target sentence, but at the same time it will fully explain the other target sentence, which we term the auxiliary sentence. [sent-136, score-0.61]
</p><p>52 The resulting QG rule is a tuple of three phrase structure elements: the source node, the node in the target main sentence (the top level of this node is typically the same as that of the source node), and the phrase structure of the entire auxiliary sentence. [sent-139, score-0.587]
</p><p>53 This formalism is able to capture the operations required to split sentences containing coordinate or subordinate clauses, parenthetical content, relative clauses and apposition. [sent-141, score-0.17]
</p><p>54 2 ILP-based Generation We cast the problem of finding a suitable target simplification given a source sentence as an integer linear program (ILP). [sent-144, score-0.677]
</p><p>55 Specifically, simplified text is created from source sentence parse trees by identi-  fying and applying QG grammar rules. [sent-145, score-0.282]
</p><p>56 The generation process starts at the root node of the parse tree, applying QG rules to subtrees until leaf nodes are reached. [sent-147, score-0.258]
</p><p>57 We do not use the Bayesian probability model proposed by Smith and Eisner (2006) to identify the best sequence of simplification rules. [sent-148, score-0.415]
</p><p>58 Instead, where there is more than one matching rule, and so more than one simplification is possible, the alternatives are all generated and incorporated into the target phrase structure tree. [sent-149, score-0.528]
</p><p>59 The ILP model operates over this phrase structure tree and selects the phrase nodes from which to form the target output. [sent-150, score-0.249]
</p><p>60 Applying the QG rules on the source sentence generates a number of auxiliary sentences. [sent-151, score-0.286]
</p><p>61 Let P be the set of nodes in the phrase structure trees of the auxiliary sentences, and Ps ⊂ P be the set of nodes in each sentence s ∈ S. [sent-153, score-0.285]
</p><p>62 In a similar fashion, the sets Ai ⊂ S, ∀i ∈ P capture the indices of any auxiliary sentences t∈hat depend on the presence of node i. [sent-156, score-0.176]
</p><p>63 (1a)  xj  →  xi  ∀i ∈ P, j ∈ Di  (1b)  xi  → ys  ∀i ∈ P, s ∈ Ai  (1c)  xi  → ys  ∀s ∈ S, i∈ Ps  (1d)  j∑∈Cixj= xi  ∀i ∈C, j ∈Ci  (1e)  s∑∈Syi≥ 1  (1f)  ∈ {0, 1}  ∀i ∈ P  (1g)  ys ∈ {0, 1}  ∀s ∈ S. [sent-165, score-0.196]
</p><p>64 Each phrase is locally given a rewrite penalty gi, where common lexical substitutions, rewrites and simplifications are penalized less (as we trust them more), compared to rarer QG rules. [sent-167, score-0.402]
</p><p>65 as seen in the training data, and Nr the number of times all suitable rules for this  gi  phrase node were seen. [sent-174, score-0.263]
</p><p>66 Intuitively, texts ought to be simpler if they correspond to low reading levels. [sent-179, score-0.174]
</p><p>67 A commonly used reading level measure is the Flesch-Kincaid Grade Level (FKGL) index which estimates readability as a combination of the average number of syllables per word and the average number of words per sentence. [sent-180, score-0.214]
</p><p>68 i∈S  i∈P  When positive, this indicates that sentences are shorter than target, and contributes positively to the readability objective whilst encouraging the application of sentence splitting and deletion-based QG rules. [sent-185, score-0.289]
</p><p>69 We can use the two target parameters (wps and spw) to control how much simplification the ILP should apply. [sent-188, score-0.489]
</p><p>70 We decided not to incorporate any such constraints into our model, as we did not want to place limitations on the simplification of original content. [sent-214, score-0.415]
</p><p>71 4  Experimental Setup  In this section we present our experimental setup for assessing the performance of the simplification model described above. [sent-215, score-0.415]
</p><p>72 Grammar Extraction QG rules were learned from revision histories and an aligned simplification corpus, which we obtained from snapshots4 of MainEW and SimpleEW. [sent-217, score-0.84]
</p><p>73 ARCoelirvgipnsoeirodanSy3n31t a26cticLe2x96i 9calSp2l1i58t4 ing  Table 2: Number of QG rules extracted (after removing singletons) from revision-based and aligned corpora. [sent-223, score-0.18]
</p><p>74 We identified suitable revisions for simplification by selecting those where the author had mentioned a keyword (such as simple, clarification or grammar) in the revision comments. [sent-224, score-0.636]
</p><p>75 With regard to  the aligned simplification corpus, we paired 15,000 articles from SimpleEW and MainEW following the language link within the snapshot files. [sent-229, score-0.502]
</p><p>76 Within the paired articles, we identified aligned sentences using macro alignment (at paragraph level) then micro alignment (at sentence level), using tf. [sent-230, score-0.232]
</p><p>77 QG rules were created by aligning nodes in these sentences as described earlier. [sent-233, score-0.189]
</p><p>78 A breakdown of the number and type of rules we obtained from the revision and aligned corpora (after removing rules appearing only once) is given in Table 2. [sent-234, score-0.407]
</p><p>79 Rules (1)–(3) involve syntactic simplification and rules (4)–(6) involve sentence splitting. [sent-236, score-0.564]
</p><p>80 Examples of common lexical simplifications found by our grammar are: “discovered” → “found”, “defeated” → “mwmoanr against”, “may refer to “”f → “d”c,ou “ldde mean”, “original” → “first”, “requires” → →“n “eceodus”ld. [sent-237, score-0.267]
</p><p>81 The transftoerrm ar eo sfh noowdnes a sfr tohme t Sp teo h SsoTu (cfeo,r t example) rely on tnhseapplication of syntactic simplification rules rules. [sent-247, score-0.508]
</p><p>82 of target words per sentence (wps) was set to 8, and syllables per word (spw) to 1. [sent-249, score-0.193]
</p><p>83 The solution was converted into a sentence by removing nodes not chosen from the tree representation, then concatenating the remaining leaf nodes in order. [sent-253, score-0.245]
</p><p>84 We evaluated two versions of our model, one with rewrite rules acquired from revision histories of simplified documents and another one with rules extracted from MainEW-SimpleEW aligned sentences. [sent-258, score-0.76]
</p><p>85 (2010)6 who also learn simplification rules from Wikipedia, and a simple baseline that uses solely lexical simplifications7 provided by the SimpleEW editor “SpencerK” (Spencer Kelly). [sent-260, score-0.536]
</p><p>86 An obvious idea would be to treat sentence simplification as an English-to-English translation problem and use an off-the-shelf system like Moses8 for the task. [sent-261, score-0.471]
</p><p>87 (2010) show that Moses performs poorly, it cannot model rewrite operations that split sentences or drop words and in most cases generates output identical 6We are grateful to Zhemin Zhu for providing us with his test set and the output of his system. [sent-263, score-0.313]
</p><p>88 Intuitively, readability measures ought to be suitable for assessing the output of simplification systems. [sent-271, score-0.572]
</p><p>89 TERp also allows for stem, synonym, and paraphrase substitutions which are common rewrite operations in simplification. [sent-279, score-0.298]
</p><p>90 In the first experiment participants were presented with a source sentence and its target simplification and asked to rate  ×  whether the latter was easier to read compared to the source. [sent-285, score-0.614]
</p><p>91 In the third experiment, they judged how well the simplification preserved the meaning of the source. [sent-287, score-0.444]
</p><p>92 The first column reports the FKGL readability index of the source sentences (MainEW), of their target simplifications (SimpleEW) and the output of four models: a simple baseline that relies on lexical substitution (SpencerK), Zhu et al. [sent-299, score-0.565]
</p><p>93 ’s (2010) model, and two versions of our model, one trained on revision histories (RevILP) and another one trained on the MainEW-SimpleEW aligned corpus (AlignILP). [sent-300, score-0.359]
</p><p>94 Table 6 reports the average ratings for Simplicity (is the target sentence simpler than the source? [sent-338, score-0.238]
</p><p>95 The QG rules obtained from the revision histories produce better output compared to the aligned corpus. [sent-358, score-0.465]
</p><p>96 As revision histories are created by Wikipedia contributors, they tend to be a more accurate data source than aligned sentences which are obtained via an automatic and unavoidably noisy  procedure. [sent-359, score-0.468]
</p><p>97 Our results also show that a more general model not restricted to specific rewrite operations like Zhu et al. [sent-360, score-0.198]
</p><p>98 We also wanted to see whether a simplification model trained on Wikipedia could be applied to another domain. [sent-362, score-0.415]
</p><p>99 We have presented an all-purpose simplification model without a target 13http : //www. [sent-371, score-0.489]
</p><p>100 For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia. [sent-555, score-0.191]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('simplification', 0.415), ('qg', 0.414), ('simpleew', 0.322), ('revilp', 0.209), ('mainew', 0.177), ('zhu', 0.164), ('simplifications', 0.163), ('ilp', 0.156), ('revision', 0.134), ('rewrite', 0.134), ('spencerk', 0.129), ('histories', 0.111), ('simpler', 0.108), ('wikipedia', 0.104), ('alignilp', 0.096), ('terp', 0.096), ('rules', 0.093), ('aligned', 0.087), ('splitting', 0.087), ('readability', 0.085), ('simplified', 0.081), ('yatskar', 0.08), ('grammar', 0.076), ('target', 0.074), ('node', 0.073), ('substitution', 0.071), ('rewriting', 0.069), ('source', 0.069), ('auxiliary', 0.068), ('grammaticality', 0.066), ('reading', 0.066), ('operations', 0.064), ('fkgl', 0.064), ('hsy', 0.064), ('spw', 0.064), ('woodsend', 0.064), ('wps', 0.064), ('syllables', 0.063), ('nodes', 0.061), ('simplify', 0.059), ('substitutions', 0.059), ('sentence', 0.056), ('siddharthan', 0.055), ('revisions', 0.055), ('chandrasekar', 0.048), ('devlin', 0.048), ('vickrey', 0.048), ('smith', 0.043), ('grammatical', 0.043), ('nelken', 0.042), ('grade', 0.042), ('paraphrase', 0.041), ('output', 0.04), ('phrase', 0.039), ('rewrites', 0.038), ('hw', 0.038), ('tree', 0.036), ('barzilay', 0.036), ('clauses', 0.036), ('sentences', 0.035), ('compression', 0.035), ('formalism', 0.035), ('suitable', 0.032), ('achterberg', 0.032), ('beigman', 0.032), ('hsd', 0.032), ('isy', 0.032), ('klebanov', 0.032), ('kristian', 0.032), ('quasisynchronous', 0.032), ('rani', 0.032), ('technische', 0.032), ('unavoidably', 0.032), ('vandalism', 0.032), ('zhemin', 0.032), ('integer', 0.031), ('leaf', 0.031), ('bleu', 0.03), ('creates', 0.029), ('di', 0.029), ('meaning', 0.029), ('lexical', 0.028), ('xi', 0.028), ('ys', 0.028), ('yamangil', 0.028), ('alignment', 0.027), ('versions', 0.027), ('rule', 0.027), ('preserving', 0.027), ('shorter', 0.026), ('suntec', 0.026), ('sapporo', 0.026), ('gi', 0.026), ('simplicity', 0.026), ('constraint', 0.025), ('deletion', 0.025), ('carroll', 0.025), ('anova', 0.025), ('tukey', 0.025), ('nastase', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="85-tfidf-1" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>Author: Kristian Woodsend ; Mirella Lapata</p><p>Abstract: Text simplification aims to rewrite text into simpler versions, and thus make information accessible to a broader audience. Most previous work simplifies sentences using handcrafted rules aimed at splitting long sentences, or substitutes difficult words using a predefined dictionary. This paper presents a datadriven model based on quasi-synchronous grammar, a formalism that can naturally capture structural mismatches and complex rewrite operations. We describe how such a grammar can be induced from Wikipedia and propose an integer linear programming model for selecting the most appropriate simplification from the space of possible rewrites generated by the grammar. We show experimentally that our method creates simplifications that significantly reduce the reading difficulty ofthe input, while maintaining grammaticality and preserving its meaning.</p><p>2 0.14058898 <a title="85-tfidf-2" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Noah A. Smith</p><p>Abstract: We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.</p><p>3 0.12411115 <a title="85-tfidf-3" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>Author: Juri Ganitkevitch ; Chris Callison-Burch ; Courtney Napoles ; Benjamin Van Durme</p><p>Abstract: Previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora. However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems.</p><p>4 0.092875451 <a title="85-tfidf-4" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>Author: Prodromos Malakasiotis ; Ion Androutsopoulos</p><p>Abstract: We present a method that paraphrases a given sentence by first generating candidate paraphrases and then ranking (or classifying) them. The candidates are generated by applying existing paraphrasing rules extracted from parallel corpora. The ranking component considers not only the overall quality of the rules that produced each candidate, but also the extent to which they preserve grammaticality and meaning in the particular context of the input sentence, as well as the degree to which the candidate differs from the input. We experimented with both a Maximum Entropy classifier and an SVR ranker. Experimental results show that incorporating features from an existing paraphrase recognizer in the ranking component improves performance, and that our overall method compares well against a state of the art paraphrase generator, when paraphrasing rules apply to the input sentences. We also propose a new methodology to evaluate the ranking components of generate-and-rank paraphrase generators, which evaluates them across different combinations of weights for grammaticality, meaning preservation, and diversity. The paper is accompanied by a paraphrasing dataset we constructed for evaluations of this kind.</p><p>5 0.091862015 <a title="85-tfidf-5" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>Author: Wei Lu ; Hwee Tou Ng</p><p>Abstract: This paper describes a novel probabilistic approach for generating natural language sentences from their underlying semantics in the form of typed lambda calculus. The approach is built on top of a novel reduction-based weighted synchronous context free grammar formalism, which facilitates the transformation process from typed lambda calculus into natural language sentences. Sentences can then be generated based on such grammar rules with a log-linear model. To acquire such grammar rules automatically in an unsupervised manner, we also propose a novel approach with a generative model, which maps from sub-expressions of logical forms to word sequences in natural language sentences. Experiments on benchmark datasets for both English and Chinese generation tasks yield significant improvements over results obtained by two state-of-the-art machine translation models, in terms of both automatic metrics and human evaluation.</p><p>6 0.090369016 <a title="85-tfidf-6" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>7 0.087940894 <a title="85-tfidf-7" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>8 0.069561534 <a title="85-tfidf-8" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>9 0.06636861 <a title="85-tfidf-9" href="./emnlp-2011-Multi-Source_Transfer_of_Delexicalized_Dependency_Parsers.html">95 emnlp-2011-Multi-Source Transfer of Delexicalized Dependency Parsers</a></p>
<p>10 0.062314931 <a title="85-tfidf-10" href="./emnlp-2011-Latent_Vector_Weighting_for_Word_Meaning_in_Context.html">80 emnlp-2011-Latent Vector Weighting for Word Meaning in Context</a></p>
<p>11 0.061407596 <a title="85-tfidf-11" href="./emnlp-2011-A_Correction_Model_for_Word_Alignments.html">3 emnlp-2011-A Correction Model for Word Alignments</a></p>
<p>12 0.061194118 <a title="85-tfidf-12" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>13 0.060822748 <a title="85-tfidf-13" href="./emnlp-2011-Generating_Aspect-oriented_Multi-Document_Summarization_with_Event-aspect_model.html">61 emnlp-2011-Generating Aspect-oriented Multi-Document Summarization with Event-aspect model</a></p>
<p>14 0.060700711 <a title="85-tfidf-14" href="./emnlp-2011-A_Word_Reordering_Model_for_Improved_Machine_Translation.html">13 emnlp-2011-A Word Reordering Model for Improved Machine Translation</a></p>
<p>15 0.058890816 <a title="85-tfidf-15" href="./emnlp-2011-Syntax-Based_Grammaticality_Improvement_using_CCG_and_Guided_Search.html">132 emnlp-2011-Syntax-Based Grammaticality Improvement using CCG and Guided Search</a></p>
<p>16 0.056739155 <a title="85-tfidf-16" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>17 0.054491915 <a title="85-tfidf-17" href="./emnlp-2011-Exact_Decoding_of_Phrase-Based_Translation_Models_through_Lagrangian_Relaxation.html">51 emnlp-2011-Exact Decoding of Phrase-Based Translation Models through Lagrangian Relaxation</a></p>
<p>18 0.053711839 <a title="85-tfidf-18" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>19 0.053183645 <a title="85-tfidf-19" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>20 0.05152173 <a title="85-tfidf-20" href="./emnlp-2011-Relation_Extraction_with_Relation_Topics.html">114 emnlp-2011-Relation Extraction with Relation Topics</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.214), (1, 0.08), (2, -0.001), (3, -0.062), (4, 0.011), (5, -0.062), (6, -0.06), (7, 0.076), (8, 0.066), (9, 0.075), (10, -0.05), (11, 0.015), (12, 0.078), (13, -0.001), (14, 0.099), (15, -0.077), (16, -0.013), (17, 0.052), (18, 0.009), (19, -0.025), (20, 0.031), (21, 0.018), (22, -0.063), (23, -0.048), (24, -0.043), (25, 0.033), (26, 0.009), (27, -0.014), (28, 0.048), (29, -0.088), (30, 0.135), (31, 0.013), (32, 0.07), (33, -0.075), (34, 0.008), (35, -0.135), (36, -0.018), (37, -0.0), (38, 0.154), (39, -0.086), (40, 0.086), (41, 0.083), (42, 0.04), (43, 0.036), (44, 0.104), (45, -0.173), (46, 0.089), (47, -0.007), (48, -0.13), (49, -0.07)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90546179 <a title="85-lsi-1" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>Author: Kristian Woodsend ; Mirella Lapata</p><p>Abstract: Text simplification aims to rewrite text into simpler versions, and thus make information accessible to a broader audience. Most previous work simplifies sentences using handcrafted rules aimed at splitting long sentences, or substitutes difficult words using a predefined dictionary. This paper presents a datadriven model based on quasi-synchronous grammar, a formalism that can naturally capture structural mismatches and complex rewrite operations. We describe how such a grammar can be induced from Wikipedia and propose an integer linear programming model for selecting the most appropriate simplification from the space of possible rewrites generated by the grammar. We show experimentally that our method creates simplifications that significantly reduce the reading difficulty ofthe input, while maintaining grammaticality and preserving its meaning.</p><p>2 0.52963138 <a title="85-lsi-2" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>Author: Juri Ganitkevitch ; Chris Callison-Burch ; Courtney Napoles ; Benjamin Van Durme</p><p>Abstract: Previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora. However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems.</p><p>3 0.50477815 <a title="85-lsi-3" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>Author: Prodromos Malakasiotis ; Ion Androutsopoulos</p><p>Abstract: We present a method that paraphrases a given sentence by first generating candidate paraphrases and then ranking (or classifying) them. The candidates are generated by applying existing paraphrasing rules extracted from parallel corpora. The ranking component considers not only the overall quality of the rules that produced each candidate, but also the extent to which they preserve grammaticality and meaning in the particular context of the input sentence, as well as the degree to which the candidate differs from the input. We experimented with both a Maximum Entropy classifier and an SVR ranker. Experimental results show that incorporating features from an existing paraphrase recognizer in the ranking component improves performance, and that our overall method compares well against a state of the art paraphrase generator, when paraphrasing rules apply to the input sentences. We also propose a new methodology to evaluate the ranking components of generate-and-rank paraphrase generators, which evaluates them across different combinations of weights for grammaticality, meaning preservation, and diversity. The paper is accompanied by a paraphrasing dataset we constructed for evaluations of this kind.</p><p>4 0.4969399 <a title="85-lsi-4" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Noah A. Smith</p><p>Abstract: We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.</p><p>5 0.46706811 <a title="85-lsi-5" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>Author: Jun Xie ; Haitao Mi ; Qun Liu</p><p>Abstract: Dependency structure, as a first step towards semantics, is believed to be helpful to improve translation quality. However, previous works on dependency structure based models typically resort to insertion operations to complete translations, which make it difficult to specify ordering information in translation rules. In our model of this paper, we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents relations and the target side as strings. The head-dependents rules require only substitution operation, thus our model requires no heuristics or separate ordering models of the previous works to control the word order of translations. Large-scale experiments show that our model performs well on long distance reordering, and outperforms the state- of-the-art constituency-to-string model (+1.47 BLEU on average) and hierarchical phrasebased model (+0.46 BLEU on average) on two Chinese-English NIST test sets without resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models.</p><p>6 0.45617601 <a title="85-lsi-6" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>7 0.44049644 <a title="85-lsi-7" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>8 0.39359081 <a title="85-lsi-8" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>9 0.36621284 <a title="85-lsi-9" href="./emnlp-2011-Harnessing_different_knowledge_sources_to_measure_semantic_relatedness_under_a_uniform_model.html">64 emnlp-2011-Harnessing different knowledge sources to measure semantic relatedness under a uniform model</a></p>
<p>10 0.36378017 <a title="85-lsi-10" href="./emnlp-2011-A_Correction_Model_for_Word_Alignments.html">3 emnlp-2011-A Correction Model for Word Alignments</a></p>
<p>11 0.35863206 <a title="85-lsi-11" href="./emnlp-2011-Relaxed_Cross-lingual_Projection_of_Constituent_Syntax.html">115 emnlp-2011-Relaxed Cross-lingual Projection of Constituent Syntax</a></p>
<p>12 0.34103692 <a title="85-lsi-12" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>13 0.33609864 <a title="85-lsi-13" href="./emnlp-2011-Identifying_Relations_for_Open_Information_Extraction.html">70 emnlp-2011-Identifying Relations for Open Information Extraction</a></p>
<p>14 0.33452666 <a title="85-lsi-14" href="./emnlp-2011-Learning_Local_Content_Shift_Detectors_from_Document-level_Information.html">82 emnlp-2011-Learning Local Content Shift Detectors from Document-level Information</a></p>
<p>15 0.33029944 <a title="85-lsi-15" href="./emnlp-2011-Multi-Source_Transfer_of_Delexicalized_Dependency_Parsers.html">95 emnlp-2011-Multi-Source Transfer of Delexicalized Dependency Parsers</a></p>
<p>16 0.32000998 <a title="85-lsi-16" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>17 0.31840044 <a title="85-lsi-17" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>18 0.31218898 <a title="85-lsi-18" href="./emnlp-2011-Inducing_Sentence_Structure_from_Parallel_Corpora_for_Reordering.html">74 emnlp-2011-Inducing Sentence Structure from Parallel Corpora for Reordering</a></p>
<p>19 0.300814 <a title="85-lsi-19" href="./emnlp-2011-Generating_Aspect-oriented_Multi-Document_Summarization_with_Event-aspect_model.html">61 emnlp-2011-Generating Aspect-oriented Multi-Document Summarization with Event-aspect model</a></p>
<p>20 0.29872435 <a title="85-lsi-20" href="./emnlp-2011-Generating_Subsequent_Reference_in_Shared_Visual_Scenes%3A_Computation_vs_Re-Use.html">62 emnlp-2011-Generating Subsequent Reference in Shared Visual Scenes: Computation vs Re-Use</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(15, 0.018), (23, 0.099), (33, 0.287), (36, 0.027), (37, 0.029), (45, 0.056), (53, 0.023), (54, 0.034), (57, 0.017), (62, 0.018), (64, 0.04), (66, 0.044), (69, 0.032), (79, 0.055), (82, 0.02), (90, 0.034), (96, 0.043), (98, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73610437 <a title="85-lda-1" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>Author: Kristian Woodsend ; Mirella Lapata</p><p>Abstract: Text simplification aims to rewrite text into simpler versions, and thus make information accessible to a broader audience. Most previous work simplifies sentences using handcrafted rules aimed at splitting long sentences, or substitutes difficult words using a predefined dictionary. This paper presents a datadriven model based on quasi-synchronous grammar, a formalism that can naturally capture structural mismatches and complex rewrite operations. We describe how such a grammar can be induced from Wikipedia and propose an integer linear programming model for selecting the most appropriate simplification from the space of possible rewrites generated by the grammar. We show experimentally that our method creates simplifications that significantly reduce the reading difficulty ofthe input, while maintaining grammaticality and preserving its meaning.</p><p>2 0.7219438 <a title="85-lda-2" href="./emnlp-2011-Harnessing_different_knowledge_sources_to_measure_semantic_relatedness_under_a_uniform_model.html">64 emnlp-2011-Harnessing different knowledge sources to measure semantic relatedness under a uniform model</a></p>
<p>Author: Ziqi Zhang ; Anna Lisa Gentile ; Fabio Ciravegna</p><p>Abstract: Measuring semantic relatedness between words or concepts is a crucial process to many Natural Language Processing tasks. Exiting methods exploit semantic evidence from a single knowledge source, and are predominantly evaluated only in the general domain. This paper introduces a method of harnessing different knowledge sources under a uniform model for measuring semantic relatedness between words or concepts. Using Wikipedia and WordNet as examples, and evaluated in both the general and biomedical domains, it successfully combines strengths from both knowledge sources and outperforms stateof-the-art on many datasets.</p><p>3 0.47401845 <a title="85-lda-3" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Noah A. Smith</p><p>Abstract: We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.</p><p>4 0.47017103 <a title="85-lda-4" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>Author: Christos Christodoulopoulos ; Sharon Goldwater ; Mark Steedman</p><p>Abstract: In this paper we present a fully unsupervised syntactic class induction system formulated as a Bayesian multinomial mixture model, where each word type is constrained to belong to a single class. By using a mixture model rather than a sequence model (e.g., HMM), we are able to easily add multiple kinds of features, including those at both the type level (morphology features) and token level (context and alignment features, the latter from parallel corpora). Using only context features, our system yields results comparable to state-of-the art, far better than a similar model without the one-class-per-type constraint. Using the additional features provides added benefit, and our final system outperforms the best published results on most of the 25 corpora tested.</p><p>5 0.46475139 <a title="85-lda-5" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>Author: Amit Dubey ; Frank Keller ; Patrick Sturt</p><p>Abstract: This paper introduces a psycholinguistic model of sentence processing which combines a Hidden Markov Model noun phrase chunker with a co-reference classifier. Both models are fully incremental and generative, giving probabilities of lexical elements conditional upon linguistic structure. This allows us to compute the information theoretic measure of surprisal, which is known to correlate with human processing effort. We evaluate our surprisal predictions on the Dundee corpus of eye-movement data show that our model achieve a better fit with human reading times than a syntax-only model which does not have access to co-reference information.</p><p>6 0.46044439 <a title="85-lda-6" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>7 0.45901772 <a title="85-lda-7" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<p>8 0.45757592 <a title="85-lda-8" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>9 0.45755339 <a title="85-lda-9" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>10 0.45634252 <a title="85-lda-10" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>11 0.45594072 <a title="85-lda-11" href="./emnlp-2011-Hierarchical_Phrase-based_Translation_Representations.html">66 emnlp-2011-Hierarchical Phrase-based Translation Representations</a></p>
<p>12 0.45484728 <a title="85-lda-12" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>13 0.45396531 <a title="85-lda-13" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>14 0.45217693 <a title="85-lda-14" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>15 0.4519318 <a title="85-lda-15" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<p>16 0.45171988 <a title="85-lda-16" href="./emnlp-2011-Multiword_Expression_Identification_with_Tree_Substitution_Grammars%3A_A_Parsing_tour_de_force_with_French.html">97 emnlp-2011-Multiword Expression Identification with Tree Substitution Grammars: A Parsing tour de force with French</a></p>
<p>17 0.45124388 <a title="85-lda-17" href="./emnlp-2011-Lexical_Generalization_in_CCG_Grammar_Induction_for_Semantic_Parsing.html">87 emnlp-2011-Lexical Generalization in CCG Grammar Induction for Semantic Parsing</a></p>
<p>18 0.45062974 <a title="85-lda-18" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>19 0.44939575 <a title="85-lda-19" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>20 0.4492878 <a title="85-lda-20" href="./emnlp-2011-Syntax-Based_Grammaticality_Improvement_using_CCG_and_Guided_Search.html">132 emnlp-2011-Syntax-Based Grammaticality Improvement using CCG and Guided Search</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
