<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>69 emnlp-2011-Identification of Multi-word Expressions by Combining Multiple Linguistic Information Sources</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-69" href="#">emnlp2011-69</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>69 emnlp-2011-Identification of Multi-word Expressions by Combining Multiple Linguistic Information Sources</h1>
<br/><p>Source: <a title="emnlp-2011-69-pdf" href="http://aclweb.org/anthology//D/D11/D11-1077.pdf">pdf</a></p><p>Author: Yulia Tsvetkov ; Shuly Wintner</p><p>Abstract: We propose an architecture for expressing various linguistically-motivated features that help identify multi-word expressions in natural language texts. The architecture combines various linguistically-motivated classification features in a Bayesian Network. We introduce novel ways for computing many of these features, and manually define linguistically-motivated interrelationships among them, which the Bayesian network models. Our methodology is almost entirely unsupervised and completely languageindependent; it relies on few language resources and is thus suitable for a large number of languages. Furthermore, unlike much recent work, our approach can identify expressions of various types and syntactic con- structions. We demonstrate a significant improvement in identification accuracy, compared with less sophisticated baselines.</p><p>Reference: <a title="emnlp-2011-69-reference" href="../emnlp2011_reference/emnlp-2011-Identification_of_Multi-word_Expressions_by_Combining_Multiple_Linguistic_Information_Sources_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 yul i t svet kov@ gmai l com a Abstract We propose an architecture for expressing various linguistically-motivated features that help identify multi-word expressions in natural language texts. [sent-3, score-0.191]
</p><p>2 The architecture combines various linguistically-motivated classification features in a Bayesian Network. [sent-4, score-0.169]
</p><p>3 We introduce novel ways for computing many of these features, and manually define linguistically-motivated interrelationships among them, which the Bayesian network models. [sent-5, score-0.094]
</p><p>4 Our methodology is almost entirely unsupervised and completely languageindependent; it relies on few language resources and is thus suitable for a large number of languages. [sent-6, score-0.081]
</p><p>5 Furthermore, unlike much recent work, our approach can identify expressions of various types and syntactic con-  structions. [sent-7, score-0.093]
</p><p>6 They are a heterogeneous class of constructions with diverse sets of characteristics, distinguished by their idiosyncratic behavior. [sent-14, score-0.085]
</p><p>7 Morphologically, some MWEs allow some of their constituents to freely inflect while restricting (or preventing) the inflection of other constituents. [sent-15, score-0.1]
</p><p>8 In some cases MWEs may allow constituents to undergo non-standard morphological inflections that 836 Shuly Wintner Department of Computer Science University of Haifa shuly@ c s hai fa ac i l  . [sent-16, score-0.196]
</p><p>9 We propose a novel architecture for identifying MWEs of various types and syntactic categories in monolingual corpora. [sent-25, score-0.132]
</p><p>10 Unlike much existing work, which focuses on a particular syntactic construction, our approach addresses MWEs of all types by focusing on the general idiosyncratic properties of MWEs rather than on specific properties of each sub-class thereof. [sent-26, score-0.144]
</p><p>11 The architecture uses Bayesian  Networks (BN) to express multiple interdependent linguistically-motivated features. [sent-28, score-0.075]
</p><p>12 We augment these by features that reflect collocation measures. [sent-31, score-0.114]
</p><p>13 This is a directed graph, whose nodes express the features used for classification, and whose edges deProce Ed iningbsu orfg th ,e S 2c0o1tl1an Cdo,n UfeKr,en Jcuely on 27 E–m31p,ir 2ic0a1l1 M. [sent-33, score-0.087]
</p><p>14 It combines statistics with a large array of linguisticallymotivated features, organized in an architecture that  reflects interdependencies among the features. [sent-39, score-0.211]
</p><p>15 The methodology we advocate is thus language-independent, requiring relatively few language resources, and is therefore optimal for medium-density languages (Varga et al. [sent-42, score-0.099]
</p><p>16 These feature focus on the expression of linguistic idiosyncrasies of various types, a phenomenon typical of MWEs. [sent-45, score-0.134]
</p><p>17 We propose novel computational modeling of many of these features; in particular, we account for the morphological idiosyncrasy of MWEs using a histogram of the number of inflected forms, in a technique that draws from image processing. [sent-46, score-0.234]
</p><p>18 Third, we advocate the use of Bayesian Networks as a mechanism for expressing manually-crafted dependencies among features; the use of BN significantly improves the classification accuracy. [sent-47, score-0.081]
</p><p>19 1 Our evaluation shows that the use of linguistically-motivated features results in reduction of 23% of the errors compared with a collocation baseline; organizing the knowledge in a BN reduces the error rate by additional 8. [sent-49, score-0.114]
</p><p>20 After discussing related work in the next section, we describe in Section 3 the methodology we propose, including a detailed discussion of the features and their implementation. [sent-51, score-0.081]
</p><p>21 He shows that combining different collocation measures using  standard statistical classification methods improves over using a single collocation measure. [sent-57, score-0.176]
</p><p>22 , 2007) suggest that some collocation measures (especially PMI and Log-likelihood) are superior to others for identifying MWEs. [sent-60, score-0.094]
</p><p>23 Hybrid methods that combine word statistics with linguistic information exploit morphological, syntactic and semantic idiosyncrasies to extract idiomatic MWEs. [sent-63, score-0.076]
</p><p>24 Several works address the lexical fixedness or syntactic fixedness of (certain types of) MWEs in order to extract them from texts. [sent-67, score-0.136]
</p><p>25 An expression is considered lexically fixed if replacing any of its con-  stituents by a semantically (and syntactically) similar word generally results in an invalid or literal expression. [sent-68, score-0.166]
</p><p>26 For example, Van de Cruys and Villada Moir o´n (2007) use lexical fixedness to extract Dutch Verb-Noun idiomatic combinations (VNICs). [sent-70, score-0.109]
</p><p>27 Our approach only requires morphological processing and a bilingual dictionary, which are more readilyavailable for several languages. [sent-76, score-0.085]
</p><p>28 Several properties of Hebrew MWEs are de-  scribed by Al-Haj (2010); Al-Haj and Wintner (2010) use them in order to construct an SVM-based classifier that can distinguish between MWE and non-MWE noun-noun constructions in Hebrew. [sent-78, score-0.088]
</p><p>29 The features of the SVM reflect several morphological and morpho-syntactic properties of such constructions. [sent-79, score-0.149]
</p><p>30 Recently, Tsvetkov and Wintner (2010b) introduced a general methodology for extracting MWEs from bilingual corpora, and applied it to Hebrew. [sent-83, score-0.089]
</p><p>31 We use this  methodology to extract both positive and negative instances for our training set in the current work; but we extrapolate the results much further by extending the method to monolingual corpora, which are typically much larger than bilingual ones. [sent-86, score-0.116]
</p><p>32 The features used for classification were of two kinds: (1) various collocation measures; (2) bi-grams aligned together by an auto838 matic word aligner applied to a parallel (PortugueseEnglish) corpus. [sent-94, score-0.203]
</p><p>33 A BN was used to combine the predictions of the various features on the test set, but the structure of the network is not described. [sent-95, score-0.12]
</p><p>34 However, the BN does not play any special role in this work,  and its structure does not reflect any insights or intuitions on the structure of the problem domain or on interdependencies among features. [sent-97, score-0.17]
</p><p>35 In particular, we also believe that collocation measures are highly important for this task, but cannot completely solve the problem: linguistically-motivated features are mandatory in order to improve the accuracy of the classifier. [sent-99, score-0.114]
</p><p>36 In this work we focus on various properties of different types ofMWEs, and define general features that may accurately apply to some, but not necessarily all of them. [sent-100, score-0.131]
</p><p>37 An architecture of Bayesian Networks is optimal for this task: it enables us to define weighted dependencies among features, such that certain features are more significant for identifying some class of MWEs, whereas others are more prominent in identifying other classes. [sent-101, score-0.178]
</p><p>38 As we show below, this architecture results in significant improvements over a more na¨ ıve combination of features. [sent-102, score-0.075]
</p><p>39 2 Several properties of MWEs make this task challenging: MWEs exhibit idiosyncrasies on a variety of levels, orthographic, morphological, syntactic and of course semantic (Al-Haj, 2010). [sent-105, score-0.084]
</p><p>40 •  BN can encode not only statistical data, but also prior adnoemnaciond knowledge taantdis thicumaldana intuitions, in the form of interdependencies among features. [sent-114, score-0.095]
</p><p>41 2 Linguistically-motivated Features Based on the observations of Al-Haj (2010), we define several linguistically-motivated features that are aimed at capturing some of the unique properties of MWEs. [sent-117, score-0.1]
</p><p>42 While many idiosyncratic properties of MWEs have been previously studied, we introduce novel ways to express those properties as computable features informing a classifier. [sent-118, score-0.167]
</p><p>43 839 We define a binary feature, DASH, whose value is 1 iff the dash character appears in some surface form of the candidate MWE. [sent-129, score-0.129]
</p><p>44 We define a feature, HAPAX, whose value is a binary vector with 1in the i-th place iff the i-th word of the candidate is not in the lexicon, and does not occur in other bi-grams at the same location. [sent-132, score-0.129]
</p><p>45 Frozen form MWE constituents sometimes occur in one fixed, frozen form. [sent-135, score-0.098]
</p><p>46 We define a feature, FROZEN, whose value is a binary vector with 1in the i-th place iff the i-th word of the candidate never inflects in the context of this expression. [sent-136, score-0.129]
</p><p>47 Example: bit xwlim (house-of sick-people) “hospital”; the noun xwlim must be in the plural in this MWE. [sent-137, score-0.095]
</p><p>48 Partial morphological inflection In some cases, MWE constituents undergo a (strict but non-empty) subset of the full inflections that they would undergo in isolation. [sent-138, score-0.294]
</p><p>49 We compute a histogram of the distribution in the corpus of all the possible surface forms of each constituent of an MWE candidate. [sent-141, score-0.131]
</p><p>50 Such histograms can compactly  represent distributional information on morphological behavior, in the same way that histograms of the distribution of gray levels in a picture are used to represent the picture itself. [sent-142, score-0.164]
</p><p>51 Our assumption is that the inflection histograms of non-MWEs are more uniform than the histograms of MWEs, in which some inflections may be more frequent and others may be altogether missing. [sent-143, score-0.201]
</p><p>52 Of course, restrictions on the histogram may stem from the part of speech of the expression; such constraints are captured by dependencies in the BN structure. [sent-144, score-0.131]
</p><p>53 Since each MWE is idiosyncratic in its own way, we do not expect the histograms of MWEs to have some specific pattern, except non-uniformity. [sent-145, score-0.101]
</p><p>54 Offline, we compute the average histogram for positive and negative examples: The average histogram of MWEs is shorter and less uniform than the average histogram of nonMWEs. [sent-147, score-0.42]
</p><p>55 We define as feature, HIST, the L1 (Manhattan) distance between the histogram of the candidate and the closest average histogram. [sent-148, score-0.187]
</p><p>56 For example, the MWE bit mepv (house-of law) “court” occurs in the following inflected forms: bit hmepv “the court” (75%); bit mepv “a court” (15%); bti hmepv “the courts” (8%); and bti mepv “courts” (2%). [sent-149, score-0.584]
</p><p>57 The histogram for this candidate is thus (75, 15, 8, 2). [sent-150, score-0.159]
</p><p>58 In contrast, the non-MWE txwm mepv (domain-of law) “domain of the law”, which is syntactically identical, occurs in nine different inflected forms, and its sorted histogram is (59, 14, 7, 7, 5, 2, 2, 2, 2). [sent-151, score-0.293]
</p><p>59 We first compute a histogram of the frequencies of words following each candidate MWE. [sent-156, score-0.159]
</p><p>60 We trim the tail of the histogram by removing words whose frequency is lower than 0. [sent-157, score-0.163]
</p><p>61 Offline, we compute the same histograms for positive and negative examples and average them as above. [sent-159, score-0.082]
</p><p>62 The value of CONTEXT is 1 iff the histogram of the candidate is closer (in terms of L1 distance) to the positive average. [sent-160, score-0.227]
</p><p>63 For example, the histogram of bit mepv “court” includes 15 values, dominated by bit mepv yliwn “supreme court” (20%) and bit mepv mxwzi “district court” (13%), followed by contexts whose frequency ranges between 5% and 0. [sent-161, score-0.613]
</p><p>64 In contrast, the non-MWE txwm mepv “domain-of law” has a much shorter histogram, namely (12, 11, 6) : over 70% of the words following this expression occur less than 0. [sent-163, score-0.204]
</p><p>65 We use  a dictionary to generate word-by-word translations of candidate MWEs to English, and check the number of occurrences of the English literal translation in a large English corpus. [sent-169, score-0.126]
</p><p>66 We expect non-MWEs to have some literal translational equivalent (possibly with frequency that correlates with their frequency in Hebrew), whereas for MWEs we expect no (or few) literal translations. [sent-171, score-0.196]
</p><p>67 We define a binary feature, TRANS, whose value is 1 iff some literal translation of the candidate occurs more than 5 times in the corpus. [sent-172, score-0.227]
</p><p>68 For example, the MWE htxtn ym (marry with) “marry” is literally translated as with marry, marry with, together marry and marry together, none of which occurs in the corpus. [sent-173, score-0.243]
</p><p>69 Collocation As a baseline, statistical association measure, we use a heuristic variant of pointwise mutual information (PMI), promoting also collocations whose constituents are frequent (Tsvetkov and Wintner, 2010b). [sent-174, score-0.083]
</p><p>70 3  Feature Interdependencies Expressed as a Bayesian Network A Bayesian Network (Jensen and Nielsen, 2007) is organized as a graph whose nodes are random variables and whose edges represent interdependencies among those variables. [sent-177, score-0.159]
</p><p>71 This facilitates the expression of domain knowledge (and intuitions, beliefs, etc. [sent-179, score-0.107]
</p><p>72 a classification device: training amounts to computing the joint probability distribution of the training set, whereas classification maximizes the posterior probability of the particular node (variable) being queried. [sent-182, score-0.08]
</p><p>73 For MWE identification we define a BN whose nodes correspond to the features described in Section 3. [sent-183, score-0.117]
</p><p>74 The POS of an expression influences its morphological inflection, hence the edges from POS to HIST and to FROZEN. [sent-190, score-0.122]
</p><p>75 841 Hapaxes clearly affect all statistical metrics, hence the edge from HAPAX to PMI, and also the existence of literal translation, since if a word is not in the lexicon, it does not have a translation, hence the edge from HAPAX to TRANS. [sent-192, score-0.098]
</p><p>76 Also, we assume that there is a correlation between the frequency (and PMI) of a candidate and whether or not a literal translation of the expression exists, hence the edge from PMI to TRANS. [sent-193, score-0.194]
</p><p>77 The core idea behind this method is that MWEs tend to be translated in nonliteral ways; in a parallel corpus, words that are 1:1 aligned typically indicate literal translations and are hence unlikely constituents of MWEs. [sent-230, score-0.213]
</p><p>78 We thus divide the set into four classes: aligned bi-grams with high PMI score, aligned bi-grams with low PMI score, misaligned with high PMI and misaligned with low PMI. [sent-233, score-0.3]
</p><p>79 Aligned bi-grams, independently of their PMI score, are more likely non-MWEs; high-PMI misaligned bi-grams are very likely MWEs; and the status of low-PMI misaligned bi-grams is unclear, and must be further investigated. [sent-234, score-0.218]
</p><p>80 Additionally, we assume that the 2,203 misaligned bi-grams with high PMI scores are likely MWEs. [sent-239, score-0.109]
</p><p>81 As for the set of over 61,000 misaligned low-PMI bi-grams, certainly many of them are non-MWEs, but some may be MWEs, and we 842 are interested in including them as positive examples of MWEs with low PMI scores. [sent-240, score-0.136]
</p><p>82 2; one (BN-auto) in which we train a BN but let Weka determine its structure (using the K2 algorithm); and one (BN) in which we train a Bayesian Network whose structure reflects manually-crafted linguisticallymotivated knowledge, as depicted in Figure 1. [sent-252, score-0.106]
</p><p>83 It is the combination of linguistically-motivated features with feature interdependencies reflecting domain knowledge that contribute to the best performance. [sent-277, score-0.118]
</p><p>84 They clearly demonstrate that the linguistically-motivated features we define provide a significant improvement in classification accuracy over the baseline PMI measure. [sent-287, score-0.091]
</p><p>85 The first non-MWE (false positive) occurs in the 50th place on the list; it is crpt niqwla “France Nicolas”, which is obviously a sub-sequence of the larger MWE, neia crpt niqwla srqwzi “French president Nicolas Sarkozy”. [sent-308, score-0.108]
</p><p>86 5  Conclusions and future work  We presented a novel architecture for identifying MWEs in text corpora. [sent-313, score-0.101]
</p><p>87 This is reflected in two ways in our work: by defining computable features that reflect different facets of irregularities; and by framing the features as part of a larger Bayesian Network that accounts for interdependencies among them. [sent-315, score-0.164]
</p><p>88 The modular architecture of BN facilitates easy exploration with more features. [sent-319, score-0.114]
</p><p>89 However, it is possible to literally translate a MWE candidate to English and rely on the English WordNet for generating synonyms of the literal translation. [sent-322, score-0.126]
</p><p>90 The assumption is that if a back-translated expression has a high PMI, the original candidate is very likely not a MWE. [sent-324, score-0.096]
</p><p>91 Identifying multi-word expressions by leveraging morphological and syntactic idiosyncrasy. [sent-334, score-0.116]
</p><p>92 Hebrew multiword expressions: Linguistic properties, lexical representation, morphological processing, and automatic acquisition. [sent-339, score-0.178]
</p><p>93 A measure of syntactic flexibility for automatically identifying multiword expressions in corpora. [sent-347, score-0.212]
</p><p>94 A hybrid approach to improve bilingual multiword expression extraction. [sent-371, score-0.249]
</p><p>95 A hybrid approach for functional expression identification in a japanese reading assistant. [sent-394, score-0.128]
</p><p>96 Using a bayesian network induction approach for text categorization. [sent-425, score-0.149]
</p><p>97 Comparing and combining a semantic tagger and a statistical tool for mwe extraction. [sent-444, score-0.318]
</p><p>98 An evaluation of methods for the extraction of multiword expressions. [sent-448, score-0.124]
</p><p>99 Validation and evaluation of automatically acquired multiword expressions for grammar engineering. [sent-484, score-0.186]
</p><p>100 A hybrid approach for the identification of multiword expressions. [sent-488, score-0.184]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mwes', 0.666), ('mwe', 0.318), ('wintner', 0.21), ('bn', 0.199), ('pmi', 0.191), ('tsvetkov', 0.14), ('histogram', 0.131), ('multiword', 0.124), ('xmwe', 0.122), ('hebrew', 0.122), ('mepv', 0.109), ('misaligned', 0.109), ('literal', 0.098), ('interdependencies', 0.095), ('bayesian', 0.083), ('marry', 0.081), ('architecture', 0.075), ('shuly', 0.07), ('collocation', 0.068), ('expression', 0.068), ('fixedness', 0.068), ('network', 0.066), ('court', 0.064), ('expressions', 0.062), ('methodology', 0.058), ('histograms', 0.055), ('morphological', 0.054), ('hapax', 0.054), ('ramisch', 0.053), ('constituents', 0.051), ('inflection', 0.049), ('undergo', 0.049), ('properties', 0.049), ('frozen', 0.047), ('idiosyncratic', 0.046), ('xk', 0.045), ('villavicencio', 0.042), ('inflections', 0.042), ('idiomatic', 0.041), ('iff', 0.041), ('bit', 0.041), ('aligned', 0.041), ('advocate', 0.041), ('linguisticallymotivated', 0.041), ('peshkin', 0.041), ('classification', 0.04), ('networks', 0.04), ('facilitates', 0.039), ('constructions', 0.039), ('weka', 0.037), ('aline', 0.035), ('hist', 0.035), ('idiosyncrasies', 0.035), ('yulia', 0.035), ('identification', 0.034), ('depicted', 0.033), ('bannard', 0.032), ('whose', 0.032), ('various', 0.031), ('bilingual', 0.031), ('law', 0.029), ('candidate', 0.028), ('define', 0.028), ('intuitions', 0.028), ('positive', 0.027), ('bti', 0.027), ('calado', 0.027), ('courts', 0.027), ('crpt', 0.027), ('denoyer', 0.027), ('erman', 0.027), ('hazelbeck', 0.027), ('hmepv', 0.027), ('idiart', 0.027), ('niqwla', 0.027), ('savova', 0.027), ('txwm', 0.027), ('villada', 0.027), ('virginia', 0.027), ('vnics', 0.027), ('weller', 0.027), ('xwlim', 0.027), ('andr', 0.026), ('inflected', 0.026), ('hybrid', 0.026), ('identifying', 0.026), ('svm', 0.025), ('insights', 0.024), ('reflect', 0.023), ('cruys', 0.023), ('haifa', 0.023), ('idiosyncrasy', 0.023), ('lam', 0.023), ('nonliteral', 0.023), ('piao', 0.023), ('viktor', 0.023), ('features', 0.023), ('carlos', 0.023), ('organizing', 0.023), ('completely', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="69-tfidf-1" href="./emnlp-2011-Identification_of_Multi-word_Expressions_by_Combining_Multiple_Linguistic_Information_Sources.html">69 emnlp-2011-Identification of Multi-word Expressions by Combining Multiple Linguistic Information Sources</a></p>
<p>Author: Yulia Tsvetkov ; Shuly Wintner</p><p>Abstract: We propose an architecture for expressing various linguistically-motivated features that help identify multi-word expressions in natural language texts. The architecture combines various linguistically-motivated classification features in a Bayesian Network. We introduce novel ways for computing many of these features, and manually define linguistically-motivated interrelationships among them, which the Bayesian network models. Our methodology is almost entirely unsupervised and completely languageindependent; it relies on few language resources and is thus suitable for a large number of languages. Furthermore, unlike much recent work, our approach can identify expressions of various types and syntactic con- structions. We demonstrate a significant improvement in identification accuracy, compared with less sophisticated baselines.</p><p>2 0.54356664 <a title="69-tfidf-2" href="./emnlp-2011-Multiword_Expression_Identification_with_Tree_Substitution_Grammars%3A_A_Parsing_tour_de_force_with_French.html">97 emnlp-2011-Multiword Expression Identification with Tree Substitution Grammars: A Parsing tour de force with French</a></p>
<p>Author: Spence Green ; Marie-Catherine de Marneffe ; John Bauer ; Christopher D. Manning</p><p>Abstract: Multiword expressions (MWE), a known nuisance for both linguistics and NLP, blur the lines between syntax and semantics. Previous work on MWE identification has relied primarily on surface statistics, which perform poorly for longer MWEs and cannot model discontinuous expressions. To address these problems, we show that even the simplest parsing models can effectively identify MWEs of arbitrary length, and that Tree Substitution Grammars achieve the best results. Our experiments show a 36.4% F1 absolute improvement for French over an n-gram surface statistics baseline, currently the predominant method for MWE identification. Our models are useful for several NLP tasks in which MWE pre-grouping has improved accuracy. 1</p><p>3 0.067562468 <a title="69-tfidf-3" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>Author: Daniel Dahlmeier ; Hwee Tou Ng</p><p>Abstract: We present a novel approach for automatic collocation error correction in learner English which is based on paraphrases extracted from parallel corpora. Our key assumption is that collocation errors are often caused by semantic similarity in the first language (L1language) of the writer. An analysis of a large corpus of annotated learner English confirms this assumption. We evaluate our approach on real-world learner data and show that L1-induced paraphrases outperform traditional approaches based on edit distance, homophones, and WordNet synonyms.</p><p>4 0.065212511 <a title="69-tfidf-4" href="./emnlp-2011-A_Cascaded_Classification_Approach_to_Semantic_Head_Recognition.html">2 emnlp-2011-A Cascaded Classification Approach to Semantic Head Recognition</a></p>
<p>Author: Lukas Michelbacher ; Alok Kothari ; Martin Forst ; Christina Lioma ; Hinrich Schutze</p><p>Abstract: Most NLP systems use tokenization as part of preprocessing. Generally, tokenizers are based on simple heuristics and do not recognize multi-word units (MWUs) like hot dog or black hole unless a precompiled list of MWUs is available. In this paper, we propose a new cascaded model for detecting MWUs of arbitrary length for tokenization, focusing on noun phrases in the physics domain. We adopt a classification approach because unlike other work on MWUs – tokenization requires a completely automatic approach. We achieve an accuracy of 68% for recognizing non-compositional MWUs and show that our MWU recognizer improves retrieval performance when used as part of an information retrieval system. – 1</p><p>5 0.055128627 <a title="69-tfidf-5" href="./emnlp-2011-Language_Models_for_Machine_Translation%3A_Original_vs._Translated_Texts.html">76 emnlp-2011-Language Models for Machine Translation: Original vs. Translated Texts</a></p>
<p>Author: Gennadi Lembersky ; Noam Ordan ; Shuly Wintner</p><p>Abstract: We investigate the differences between language models compiled from original target-language texts and those compiled from texts manually translated to the target language. Corroborating established observations of Translation Studies, we demonstrate that the latter are significantly better predictors of translated sentences than the former, and hence fit the reference set better. Furthermore, translated texts yield better language models for statistical machine translation than original texts.</p><p>6 0.053621672 <a title="69-tfidf-6" href="./emnlp-2011-Approximate_Scalable_Bounded_Space_Sketch_for_Large_Data_NLP.html">19 emnlp-2011-Approximate Scalable Bounded Space Sketch for Large Data NLP</a></p>
<p>7 0.052967366 <a title="69-tfidf-7" href="./emnlp-2011-Literal_and_Metaphorical_Sense_Identification_through_Concrete_and_Abstract_Context.html">91 emnlp-2011-Literal and Metaphorical Sense Identification through Concrete and Abstract Context</a></p>
<p>8 0.051866658 <a title="69-tfidf-8" href="./emnlp-2011-Universal_Morphological_Analysis_using_Structured_Nearest_Neighbor_Prediction.html">140 emnlp-2011-Universal Morphological Analysis using Structured Nearest Neighbor Prediction</a></p>
<p>9 0.048144195 <a title="69-tfidf-9" href="./emnlp-2011-Discovering_Morphological_Paradigms_from_Plain_Text_Using_a_Dirichlet_Process_Mixture_Model.html">39 emnlp-2011-Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model</a></p>
<p>10 0.047021132 <a title="69-tfidf-10" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>11 0.04259409 <a title="69-tfidf-11" href="./emnlp-2011-Non-parametric_Bayesian_Segmentation_of_Japanese_Noun_Phrases.html">99 emnlp-2011-Non-parametric Bayesian Segmentation of Japanese Noun Phrases</a></p>
<p>12 0.038159024 <a title="69-tfidf-12" href="./emnlp-2011-Simple_Effective_Decipherment_via_Combinatorial_Optimization.html">122 emnlp-2011-Simple Effective Decipherment via Combinatorial Optimization</a></p>
<p>13 0.037687771 <a title="69-tfidf-13" href="./emnlp-2011-Unsupervised_Semantic_Role_Induction_with_Graph_Partitioning.html">145 emnlp-2011-Unsupervised Semantic Role Induction with Graph Partitioning</a></p>
<p>14 0.035619423 <a title="69-tfidf-14" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>15 0.035075605 <a title="69-tfidf-15" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>16 0.0345948 <a title="69-tfidf-16" href="./emnlp-2011-Relation_Acquisition_using_Word_Classes_and_Partial_Patterns.html">113 emnlp-2011-Relation Acquisition using Word Classes and Partial Patterns</a></p>
<p>17 0.034467634 <a title="69-tfidf-17" href="./emnlp-2011-Lexical_Co-occurrence%2C_Statistical_Significance%2C_and_Word_Association.html">86 emnlp-2011-Lexical Co-occurrence, Statistical Significance, and Word Association</a></p>
<p>18 0.032517362 <a title="69-tfidf-18" href="./emnlp-2011-Structural_Opinion_Mining_for_Graph-based_Sentiment_Representation.html">126 emnlp-2011-Structural Opinion Mining for Graph-based Sentiment Representation</a></p>
<p>19 0.032508291 <a title="69-tfidf-19" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>20 0.032148197 <a title="69-tfidf-20" href="./emnlp-2011-Analyzing_Methods_for_Improving_Precision_of_Pivot_Based_Bilingual_Dictionaries.html">18 emnlp-2011-Analyzing Methods for Improving Precision of Pivot Based Bilingual Dictionaries</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.151), (1, -0.01), (2, -0.043), (3, 0.021), (4, 0.025), (5, 0.043), (6, -0.241), (7, 0.2), (8, -0.104), (9, -0.227), (10, 0.654), (11, -0.133), (12, -0.115), (13, -0.241), (14, 0.143), (15, -0.079), (16, -0.04), (17, 0.001), (18, -0.066), (19, -0.01), (20, -0.013), (21, 0.003), (22, -0.056), (23, -0.058), (24, 0.079), (25, -0.077), (26, -0.044), (27, -0.063), (28, 0.001), (29, -0.001), (30, 0.013), (31, 0.012), (32, 0.008), (33, 0.043), (34, 0.052), (35, -0.018), (36, -0.02), (37, 0.003), (38, 0.011), (39, 0.007), (40, 0.043), (41, 0.009), (42, 0.018), (43, -0.011), (44, -0.011), (45, -0.014), (46, -0.028), (47, 0.017), (48, -0.001), (49, -0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92985213 <a title="69-lsi-1" href="./emnlp-2011-Identification_of_Multi-word_Expressions_by_Combining_Multiple_Linguistic_Information_Sources.html">69 emnlp-2011-Identification of Multi-word Expressions by Combining Multiple Linguistic Information Sources</a></p>
<p>Author: Yulia Tsvetkov ; Shuly Wintner</p><p>Abstract: We propose an architecture for expressing various linguistically-motivated features that help identify multi-word expressions in natural language texts. The architecture combines various linguistically-motivated classification features in a Bayesian Network. We introduce novel ways for computing many of these features, and manually define linguistically-motivated interrelationships among them, which the Bayesian network models. Our methodology is almost entirely unsupervised and completely languageindependent; it relies on few language resources and is thus suitable for a large number of languages. Furthermore, unlike much recent work, our approach can identify expressions of various types and syntactic con- structions. We demonstrate a significant improvement in identification accuracy, compared with less sophisticated baselines.</p><p>2 0.84734297 <a title="69-lsi-2" href="./emnlp-2011-Multiword_Expression_Identification_with_Tree_Substitution_Grammars%3A_A_Parsing_tour_de_force_with_French.html">97 emnlp-2011-Multiword Expression Identification with Tree Substitution Grammars: A Parsing tour de force with French</a></p>
<p>Author: Spence Green ; Marie-Catherine de Marneffe ; John Bauer ; Christopher D. Manning</p><p>Abstract: Multiword expressions (MWE), a known nuisance for both linguistics and NLP, blur the lines between syntax and semantics. Previous work on MWE identification has relied primarily on surface statistics, which perform poorly for longer MWEs and cannot model discontinuous expressions. To address these problems, we show that even the simplest parsing models can effectively identify MWEs of arbitrary length, and that Tree Substitution Grammars achieve the best results. Our experiments show a 36.4% F1 absolute improvement for French over an n-gram surface statistics baseline, currently the predominant method for MWE identification. Our models are useful for several NLP tasks in which MWE pre-grouping has improved accuracy. 1</p><p>3 0.17924969 <a title="69-lsi-3" href="./emnlp-2011-A_Cascaded_Classification_Approach_to_Semantic_Head_Recognition.html">2 emnlp-2011-A Cascaded Classification Approach to Semantic Head Recognition</a></p>
<p>Author: Lukas Michelbacher ; Alok Kothari ; Martin Forst ; Christina Lioma ; Hinrich Schutze</p><p>Abstract: Most NLP systems use tokenization as part of preprocessing. Generally, tokenizers are based on simple heuristics and do not recognize multi-word units (MWUs) like hot dog or black hole unless a precompiled list of MWUs is available. In this paper, we propose a new cascaded model for detecting MWUs of arbitrary length for tokenization, focusing on noun phrases in the physics domain. We adopt a classification approach because unlike other work on MWUs – tokenization requires a completely automatic approach. We achieve an accuracy of 68% for recognizing non-compositional MWUs and show that our MWU recognizer improves retrieval performance when used as part of an information retrieval system. – 1</p><p>4 0.1722831 <a title="69-lsi-4" href="./emnlp-2011-Literal_and_Metaphorical_Sense_Identification_through_Concrete_and_Abstract_Context.html">91 emnlp-2011-Literal and Metaphorical Sense Identification through Concrete and Abstract Context</a></p>
<p>Author: Peter Turney ; Yair Neuman ; Dan Assaf ; Yohai Cohen</p><p>Abstract: Metaphor is ubiquitous in text, even in highly technical text. Correct inference about textual entailment requires computers to distinguish the literal and metaphorical senses of a word. Past work has treated this problem as a classical word sense disambiguation task. In this paper, we take a new approach, based on research in cognitive linguistics that views metaphor as a method for transferring knowledge from a familiar, well-understood, or concrete domain to an unfamiliar, less understood, or more abstract domain. This view leads to the hypothesis that metaphorical word usage is correlated with the degree of abstractness of the word’s context. We introduce an algorithm that uses this hypothesis to classify a word sense in a given context as either literal (de- notative) or metaphorical (connotative). We evaluate this algorithm with a set of adjectivenoun phrases (e.g., in dark comedy, the adjective dark is used metaphorically; in dark hair, it is used literally) and with the TroFi (Trope Finder) Example Base of literal and nonliteral usage for fifty verbs. We achieve state-of-theart performance on both datasets.</p><p>5 0.17224041 <a title="69-lsi-5" href="./emnlp-2011-Approximate_Scalable_Bounded_Space_Sketch_for_Large_Data_NLP.html">19 emnlp-2011-Approximate Scalable Bounded Space Sketch for Large Data NLP</a></p>
<p>Author: Amit Goyal ; Hal Daume III</p><p>Abstract: We exploit sketch techniques, especially the Count-Min sketch, a memory, and time efficient framework which approximates the frequency of a word pair in the corpus without explicitly storing the word pair itself. These methods use hashing to deal with massive amounts of streaming text. We apply CountMin sketch to approximate word pair counts and exhibit their effectiveness on three important NLP tasks. Our experiments demonstrate that on all of the three tasks, we get performance comparable to Exact word pair counts setting and state-of-the-art system. Our method scales to 49 GB of unzipped web data using bounded space of 2 billion counters (8 GB memory).</p><p>6 0.1569194 <a title="69-lsi-6" href="./emnlp-2011-Accurate_Parsing_with_Compact_Tree-Substitution_Grammars%3A_Double-DOP.html">16 emnlp-2011-Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP</a></p>
<p>7 0.13390352 <a title="69-lsi-7" href="./emnlp-2011-Discovering_Morphological_Paradigms_from_Plain_Text_Using_a_Dirichlet_Process_Mixture_Model.html">39 emnlp-2011-Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model</a></p>
<p>8 0.13295527 <a title="69-lsi-8" href="./emnlp-2011-Language_Models_for_Machine_Translation%3A_Original_vs._Translated_Texts.html">76 emnlp-2011-Language Models for Machine Translation: Original vs. Translated Texts</a></p>
<p>9 0.12982951 <a title="69-lsi-9" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>10 0.12965339 <a title="69-lsi-10" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>11 0.12610503 <a title="69-lsi-11" href="./emnlp-2011-Analyzing_Methods_for_Improving_Precision_of_Pivot_Based_Bilingual_Dictionaries.html">18 emnlp-2011-Analyzing Methods for Improving Precision of Pivot Based Bilingual Dictionaries</a></p>
<p>12 0.11980335 <a title="69-lsi-12" href="./emnlp-2011-Watermarking_the_Outputs_of_Structured_Prediction_with_an_application_in_Statistical_Machine_Translation..html">148 emnlp-2011-Watermarking the Outputs of Structured Prediction with an application in Statistical Machine Translation.</a></p>
<p>13 0.11974093 <a title="69-lsi-13" href="./emnlp-2011-Universal_Morphological_Analysis_using_Structured_Nearest_Neighbor_Prediction.html">140 emnlp-2011-Universal Morphological Analysis using Structured Nearest Neighbor Prediction</a></p>
<p>14 0.11294931 <a title="69-lsi-14" href="./emnlp-2011-Lexical_Co-occurrence%2C_Statistical_Significance%2C_and_Word_Association.html">86 emnlp-2011-Lexical Co-occurrence, Statistical Significance, and Word Association</a></p>
<p>15 0.1088876 <a title="69-lsi-15" href="./emnlp-2011-Generating_Subsequent_Reference_in_Shared_Visual_Scenes%3A_Computation_vs_Re-Use.html">62 emnlp-2011-Generating Subsequent Reference in Shared Visual Scenes: Computation vs Re-Use</a></p>
<p>16 0.10835182 <a title="69-lsi-16" href="./emnlp-2011-Computing_Logical_Form_on_Regulatory_Texts.html">32 emnlp-2011-Computing Logical Form on Regulatory Texts</a></p>
<p>17 0.10817826 <a title="69-lsi-17" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>18 0.10792157 <a title="69-lsi-18" href="./emnlp-2011-Structural_Opinion_Mining_for_Graph-based_Sentiment_Representation.html">126 emnlp-2011-Structural Opinion Mining for Graph-based Sentiment Representation</a></p>
<p>19 0.10722919 <a title="69-lsi-19" href="./emnlp-2011-Improving_Bilingual_Projections_via_Sparse_Covariance_Matrices.html">73 emnlp-2011-Improving Bilingual Projections via Sparse Covariance Matrices</a></p>
<p>20 0.10684559 <a title="69-lsi-20" href="./emnlp-2011-Learning_General_Connotation_of_Words_using_Graph-based_Algorithms.html">81 emnlp-2011-Learning General Connotation of Words using Graph-based Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.088), (36, 0.417), (37, 0.018), (45, 0.08), (53, 0.021), (54, 0.031), (57, 0.018), (61, 0.012), (62, 0.02), (64, 0.02), (66, 0.031), (69, 0.012), (79, 0.04), (80, 0.011), (82, 0.013), (90, 0.013), (94, 0.01), (96, 0.035), (98, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91967833 <a title="69-lda-1" href="./emnlp-2011-Multilayer_Sequence_Labeling.html">96 emnlp-2011-Multilayer Sequence Labeling</a></p>
<p>Author: Ai Azuma ; Yuji Matsumoto</p><p>Abstract: In this paper, we describe a novel approach to cascaded learning and inference on sequences. We propose a weakly joint learning model on cascaded inference on sequences, called multilayer sequence labeling. In this model, inference on sequences is modeled as cascaded decision. However, the decision on a sequence labeling sequel to other decisions utilizes the features on the preceding results as marginalized by the probabilistic models on them. It is not novel itself, but our idea central to this paper is that the probabilistic models on succeeding labeling are viewed as indirectly depending on the probabilistic models on preceding analyses. We also propose two types of efficient dynamic programming which are required in the gradient-based optimization of an objective function. One of the dynamic programming algorithms resembles back propagation algorithm for mul- tilayer feed-forward neural networks. The other is a generalized version of the forwardbackward algorithm. We also report experiments of cascaded part-of-speech tagging and chunking of English sentences and show effectiveness of the proposed method.</p><p>2 0.89984381 <a title="69-lda-2" href="./emnlp-2011-Parse_Correction_with_Specialized_Models_for_Difficult_Attachment_Types.html">102 emnlp-2011-Parse Correction with Specialized Models for Difficult Attachment Types</a></p>
<p>Author: Enrique Henestroza Anguiano ; Marie Candito</p><p>Abstract: This paper develops a framework for syntactic dependency parse correction. Dependencies in an input parse tree are revised by selecting, for a given dependent, the best governor from within a small set of candidates. We use a discriminative linear ranking model to select the best governor from a group of candidates for a dependent, and our model includes a rich feature set that encodes syntactic structure in the input parse tree. The parse correction framework is parser-agnostic, and can correct attachments using either a generic model or specialized models tailored to difficult attachment types like coordination and pp-attachment. Our experiments show that parse correction, combining a generic model with specialized models for difficult attachment types, can successfully improve the quality of predicted parse trees output by sev- eral representative state-of-the-art dependency parsers for French.</p><p>same-paper 3 0.81538659 <a title="69-lda-3" href="./emnlp-2011-Identification_of_Multi-word_Expressions_by_Combining_Multiple_Linguistic_Information_Sources.html">69 emnlp-2011-Identification of Multi-word Expressions by Combining Multiple Linguistic Information Sources</a></p>
<p>Author: Yulia Tsvetkov ; Shuly Wintner</p><p>Abstract: We propose an architecture for expressing various linguistically-motivated features that help identify multi-word expressions in natural language texts. The architecture combines various linguistically-motivated classification features in a Bayesian Network. We introduce novel ways for computing many of these features, and manually define linguistically-motivated interrelationships among them, which the Bayesian network models. Our methodology is almost entirely unsupervised and completely languageindependent; it relies on few language resources and is thus suitable for a large number of languages. Furthermore, unlike much recent work, our approach can identify expressions of various types and syntactic con- structions. We demonstrate a significant improvement in identification accuracy, compared with less sophisticated baselines.</p><p>4 0.51992691 <a title="69-lda-4" href="./emnlp-2011-Tuning_as_Ranking.html">138 emnlp-2011-Tuning as Ranking</a></p>
<p>Author: Mark Hopkins ; Jonathan May</p><p>Abstract: We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chiang et al., 2008b), PRO is easy to implement. It uses off-the-shelf linear binary classifier software and can be built on top of an existing MERT framework in a matter of hours. We establish PRO’s scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.</p><p>5 0.48245981 <a title="69-lda-5" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>Author: Marco Dinarelli ; Sophie Rosset</p><p>Abstract: Reranking models have been successfully applied to many tasks of Natural Language Processing. However, there are two aspects of this approach that need a deeper investigation: (i) Assessment of hypotheses generated for reranking at classification phase: baseline models generate a list of hypotheses and these are used for reranking without any assessment; (ii) Detection of cases where reranking models provide a worst result: the best hypothesis provided by the reranking model is assumed to be always the best result. In some cases the reranking model provides an incorrect hypothesis while the baseline best hypothesis is correct, especially when baseline models are accurate. In this paper we propose solutions for these two aspects: (i) a semantic inconsistency metric to select possibly more correct n-best hypotheses, from a large set generated by an SLU basiline model. The selected hypotheses are reranked applying a state-of-the-art model based on Partial Tree Kernels, which encode SLU hypotheses in Support Vector Machines with complex structured features; (ii) finally, we apply a decision strategy, based on confidence values, to select the final hypothesis between the first ranked hypothesis provided by the baseline SLU model and the first ranked hypothesis provided by the re-ranker. We show the effectiveness of these solutions presenting comparative results obtained reranking hypotheses generated by a very accurate Conditional Random Field model. We evaluate our approach on the French MEDIA corpus. The results show significant improvements with respect to current state-of-the-art and previous 1104 Sophie Rosset LIMSI-CNRS B.P. 133, 91403 Orsay Cedex France ro s set @ l ims i fr . re-ranking models.</p><p>6 0.46140099 <a title="69-lda-6" href="./emnlp-2011-Syntax-Based_Grammaticality_Improvement_using_CCG_and_Guided_Search.html">132 emnlp-2011-Syntax-Based Grammaticality Improvement using CCG and Guided Search</a></p>
<p>7 0.45389241 <a title="69-lda-7" href="./emnlp-2011-Third-order_Variational_Reranking_on_Packed-Shared_Dependency_Forests.html">134 emnlp-2011-Third-order Variational Reranking on Packed-Shared Dependency Forests</a></p>
<p>8 0.44411308 <a title="69-lda-8" href="./emnlp-2011-Heuristic_Search_for_Non-Bottom-Up_Tree_Structure_Prediction.html">65 emnlp-2011-Heuristic Search for Non-Bottom-Up Tree Structure Prediction</a></p>
<p>9 0.43401396 <a title="69-lda-9" href="./emnlp-2011-Linear_Text_Segmentation_Using_Affinity_Propagation.html">88 emnlp-2011-Linear Text Segmentation Using Affinity Propagation</a></p>
<p>10 0.42672899 <a title="69-lda-10" href="./emnlp-2011-Multiword_Expression_Identification_with_Tree_Substitution_Grammars%3A_A_Parsing_tour_de_force_with_French.html">97 emnlp-2011-Multiword Expression Identification with Tree Substitution Grammars: A Parsing tour de force with French</a></p>
<p>11 0.42532435 <a title="69-lda-11" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>12 0.42449668 <a title="69-lda-12" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>13 0.42030829 <a title="69-lda-13" href="./emnlp-2011-Efficient_retrieval_of_tree_translation_examples_for_Syntax-Based_Machine_Translation.html">47 emnlp-2011-Efficient retrieval of tree translation examples for Syntax-Based Machine Translation</a></p>
<p>14 0.41845542 <a title="69-lda-14" href="./emnlp-2011-Structural_Opinion_Mining_for_Graph-based_Sentiment_Representation.html">126 emnlp-2011-Structural Opinion Mining for Graph-based Sentiment Representation</a></p>
<p>15 0.4179275 <a title="69-lda-15" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>16 0.41623449 <a title="69-lda-16" href="./emnlp-2011-Discovering_Morphological_Paradigms_from_Plain_Text_Using_a_Dirichlet_Process_Mixture_Model.html">39 emnlp-2011-Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model</a></p>
<p>17 0.41571158 <a title="69-lda-17" href="./emnlp-2011-Exploiting_Syntactic_and_Distributional_Information_for_Spelling_Correction_with_Web-Scale_N-gram_Models.html">55 emnlp-2011-Exploiting Syntactic and Distributional Information for Spelling Correction with Web-Scale N-gram Models</a></p>
<p>18 0.41141769 <a title="69-lda-18" href="./emnlp-2011-Rumor_has_it%3A_Identifying_Misinformation_in_Microblogs.html">117 emnlp-2011-Rumor has it: Identifying Misinformation in Microblogs</a></p>
<p>19 0.40947199 <a title="69-lda-19" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<p>20 0.4049288 <a title="69-lda-20" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
