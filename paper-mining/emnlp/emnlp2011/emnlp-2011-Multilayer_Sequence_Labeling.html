<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>96 emnlp-2011-Multilayer Sequence Labeling</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-96" href="#">emnlp2011-96</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>96 emnlp-2011-Multilayer Sequence Labeling</h1>
<br/><p>Source: <a title="emnlp-2011-96-pdf" href="http://aclweb.org/anthology//D/D11/D11-1058.pdf">pdf</a></p><p>Author: Ai Azuma ; Yuji Matsumoto</p><p>Abstract: In this paper, we describe a novel approach to cascaded learning and inference on sequences. We propose a weakly joint learning model on cascaded inference on sequences, called multilayer sequence labeling. In this model, inference on sequences is modeled as cascaded decision. However, the decision on a sequence labeling sequel to other decisions utilizes the features on the preceding results as marginalized by the probabilistic models on them. It is not novel itself, but our idea central to this paper is that the probabilistic models on succeeding labeling are viewed as indirectly depending on the probabilistic models on preceding analyses. We also propose two types of efficient dynamic programming which are required in the gradient-based optimization of an objective function. One of the dynamic programming algorithms resembles back propagation algorithm for mul- tilayer feed-forward neural networks. The other is a generalized version of the forwardbackward algorithm. We also report experiments of cascaded part-of-speech tagging and chunking of English sentences and show effectiveness of the proposed method.</p><p>Reference: <a title="emnlp-2011-96-reference" href="../emnlp2011_reference/emnlp-2011-Multilayer_Sequence_Labeling_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We propose a weakly joint learning model on cascaded inference on sequences, called multilayer sequence labeling. [sent-4, score-0.322]
</p><p>2 In this model, inference on sequences is modeled as cascaded decision. [sent-5, score-0.192]
</p><p>3 However, the decision on a sequence labeling sequel to other decisions utilizes the features on the preceding results as marginalized by the probabilistic models on them. [sent-6, score-0.583]
</p><p>4 It is not novel itself, but our idea central to this paper is that the probabilistic models on succeeding labeling are viewed as indirectly depending on the probabilistic models on preceding analyses. [sent-7, score-0.238]
</p><p>5 We also propose two types of efficient dynamic programming which are required in the gradient-based optimization of an objective function. [sent-8, score-0.29]
</p><p>6 We also report experiments of cascaded part-of-speech tagging and chunking of English sentences and show effectiveness of the proposed method. [sent-11, score-0.348]
</p><p>7 Sequence labeling is the simplest subclass of structured prediction problems. [sent-15, score-0.257]
</p><p>8 In sequence labeling, the most likely one 628 among all the possible label sequences is predicted for a given input. [sent-16, score-0.297]
</p><p>9 Although sequence labeling is the simplest subclass, a lot of real-world tasks are modeled as problems of this simplest subclass. [sent-17, score-0.405]
</p><p>10 Many models have been proposed  for sequence labeling tasks, such as Hidden Markov Models (HMM), Conditional Random Fields (CRF) (Lafferty et al. [sent-19, score-0.301]
</p><p>11 A cascade of predictions here means the situation in which some of predictions are made based upon the results of other predictions. [sent-25, score-0.31]
</p><p>12 For example, in NLP, we perform named entity recognition or base-phrase chunking for given sentences based on part-of-speech (POS) labels predicted by another sequence labeler. [sent-27, score-0.436]
</p><p>13 Therefore, many tasks in NLP are modeled as a cascade of sequence predictions. [sent-29, score-0.319]
</p><p>14 If a prediction is based upon the result of another prediction, we call the former upper stage and the latter lower stage. [sent-30, score-0.44]
</p><p>15 Methods pursued for a cascade of predictions including sequence predictions, of course–, are desired to perform certain types of capability. [sent-31, score-0.384]
</p><p>16 Another is backward information propagation, that is, the rich annotated data on an upper stage should affect the models on lower stages retroactively. [sent-36, score-0.638]
</p><p>17 Many current systems for a cascade of sequence predictions adopt a simple 1-best feed-forward approach. [sent-37, score-0.384]
</p><p>18 They simply take the most likely output at each prediction stage and transfer it to the next upper stage. [sent-38, score-0.473]
</p><p>19 Such a framework can maximize reusability of existing sequence labeling systems. [sent-39, score-0.301]
</p><p>20 The essence of this orientation is that the labeler on an upper stage utilizes the information of all the possible output candidates on lower stages. [sent-44, score-0.673]
</p><p>21 However, the size of the output space can become quite large in sequence labeling. [sent-45, score-0.23]
</p><p>22 (2006), a cascades of sequence predictions is viewed as a Bayesian network, and sample sequences are drawn at each stage according to the output distribution. [sent-49, score-0.599]
</p><p>23 In the method proposed in Bunescu (2008), an upper labeler uses the probabilities marginalized on the parts of the output sequences on lower stages as weights for the features. [sent-51, score-0.857]
</p><p>24 The weighted features are integrated in the model of the labeler on the upper stage. [sent-52, score-0.369]
</p><p>25 It enables simultaneous learning and estimation of multiple sequence labelings on the same input sequences, where time slices of the outputs of all the out sequences are regularly aligned. [sent-60, score-0.439]
</p><p>26 Moreover, it only considers the cases where labels of an input sequence and all output sequences are regularly aligned. [sent-63, score-0.48]
</p><p>27 It is not clear how to build a joint labeling model which handles irregular output label  sequences like semi-Markov models (Sarawagi and Cohen, 2005). [sent-64, score-0.411]
</p><p>28 In this paper, we propose a middle ground for a cascade of sequence predictions. [sent-65, score-0.319]
</p><p>29 We first assume that the model on all the sequence labeling stages is probabilistic one. [sent-67, score-0.435]
</p><p>30 In modeling of an upper stage, a feature is weighted by the marginal probability of the fragment of the outputs from a lower stage. [sent-68, score-0.375]
</p><p>31 Features integrated in the model on each stage are weighted by the marginal probabilities of the fragments of the outputs on lower stages. [sent-71, score-0.36]
</p><p>32 So, if the output distributions on lower stages change, the marginal probabilities of any fragments also change, and this in turn can change the value of the features on the upper stage. [sent-72, score-0.524]
</p><p>33 In other words, the features on an upper stage indirectly depend on the models on the lower stages. [sent-73, score-0.49]
</p><p>34 Based on this intuition, the learning procedure of the model on an upper stage can affect not only direct model parameters, but also the weights of the features by changing the model on  the lower stages. [sent-74, score-0.598]
</p><p>35 Supervised learning based on annotated data on an upper stage may affect the model or model parameters on the lower stages. [sent-75, score-0.481]
</p><p>36 It could be said that the information of annotation data on an upper stage is propagated back to the model on lower stages. [sent-76, score-0.48]
</p><p>37 In Section 3, we propose an optimization procedure according to the intuition noted above. [sent-78, score-0.23]
</p><p>38 The proposed method shows some improvements on a real-world task in comparison with ordinary methods. [sent-80, score-0.23]
</p><p>39 Hereafter, for the sake of simplicity, we only describe the simplest case in which there are just two stages, one lower stage of sequence labeling named L1 and one upper stage of sequence labeling named L2. [sent-82, score-1.351]
</p><p>40 L2 is also a sequence labeling stage for the same  input x and the output of L1. [sent-84, score-0.655]
</p><p>41 The model for L1 per se is the same as ordinary ones for sequence labeling. [sent-89, score-0.369]
</p><p>42 (2)  ∑∈Y1 It is worth noting that this formalization subsumes both directed and undirected linear-chain graphical models, which are the most typical models for sequence labeling, including HMM and CRF. [sent-109, score-0.297]
</p><p>43 In such configuration, all the possible successful paths defined in our notation have strict one-to-one correspondence to all the possible joint assignments of labels in linear-chain graphical models. [sent-111, score-0.186]
</p><p>44 Next, we formalize the probabilistic model on the upper stage L2. [sent-113, score-0.42]
</p><p>45 A feature on an arc e2 ∈ E2 can access local characteristics of the confide∈nc Ee-rated superposition of the L1’s outputs, in addition to the information of the input x. [sent-117, score-0.329]
</p><p>46 To formulate local characteristics of the superposition of the L1’s outputs, we first define output features of L1, denoted by h⟨1,k1′,e1⟩ ∈ R (k′1 ∈ K1′, e1 ∈ E1). [sent-118, score-0.238]
</p><p>47 Before the output features are integrated into the model for L2, they all are confidence-rated with respect to P1, that is, each output feature h⟨1,k1′,e1⟩ is numerically rated by the estimated probabilities summed over the sequences emitting that feature. [sent-120, score-0.374]
</p><p>48 Here, the notation ∑y1∼e1 represents the summation over sequen∑ces ∼ceonsistent with an arc e1 ∈ E1, that is, ∑the summation over the set {y1 ∈ Y1 | e1 ∈ y1}. [sent-122, score-0.199]
</p><p>49 The input features for P2 on an arc e2 ∈ E2 are permitted ptou arbitrarily combine the infor∈ma Etion of x and the L1’s marginalized output features ¯h1, in addition to the local characteristics of the arc at hand e2. [sent-124, score-0.742]
</p><p>50 In summary, an input feature for L2 on an arc e2 ∈ E2 is of the form  f⟨2,k2,e2,x⟩(h¯1(θ1))  ∈ R  (k2 ∈ K2)  ,  (5)  where K2 is th(e index) set of the input feature types fwohr L2. [sent-125, score-0.333]
</p><p>51 KTo make the optimization procedure feasible, smoothness condition on any L2’s input feature is assumed with respect to all the L1 output features, that is,  ’s ∂∂fh¯⟨2⟨,1k,2k1′,e,e21,x⟩⟩is always guaranteed to exist for  631 ∀k′1, e1, k2, e2. [sent-126, score-0.477]
</p><p>52 P2 is viewed not only as the function of the ordinary direct parameters θ2 but also as the function of θ1, which represents the parameters for the L1’s model, through the intermediate variables ¯h1. [sent-130, score-0.23]
</p><p>53 So optimization procedure on P2 may affect the determination of the values not only of the direct parameters θ2 but also of the indirect ones θ1 . [sent-131, score-0.31]
</p><p>54 3  Optimization Algorithm  In this section, we describe optimization procedure for the model formulated in the previous section. [sent-135, score-0.23]
</p><p>55 Optimization procedure repeatedly searches a direction in the parameter space which is ascendent with respect to the objective function, and updates the parameter values into that direction by small advances. [sent-179, score-0.272]
</p><p>56 Many existing optimization routines like steepest descent or conjugation gradient do that job only by giving the objective value and gra-  dients on parameter values to be updated. [sent-180, score-0.435]
</p><p>57 So, the optimization problem here boils down to the calculation of the objective value and gradients on given parameter values. [sent-181, score-0.421]
</p><p>58 Before entering the detailed description of the algorithm for calculating the objective function and gradients, we note the functional relations among the objective function and previously defined variables. [sent-182, score-0.304]
</p><p>59 The diagram shown in Figure 1 illustrates the functional relations among the parameters, input and output feature functions, models, and objective function. [sent-183, score-0.328]
</p><p>60 The value of the 632 objective function on given parameter values can be calculated in order of the arrows shown in the diagram. [sent-185, score-0.203]
</p><p>61 The functional relations illustrated in the Figure 1 ensure some forms of the chain rule of differentiation among the variables. [sent-187, score-0.204]
</p><p>62 These two directions of stepwise computation are analogous to the forward and back propagation for multilayer feedforward neural networks, respectively. [sent-189, score-0.27]
</p><p>63 Algorithm 1 shows the whole picture of the gradient-based optimization procedure for our model. [sent-190, score-0.23]
</p><p>64 The values of marginalized output features h¯⟨1,x⟩ can be calculated by (3). [sent-192, score-0.412]
</p><p>65 Because they are the simple marginals of features, the ordinary forwardbackward algorithm (hereafter, abbreviated as “FB”) on G1 offers an efficient way to calculate their values. [sent-193, score-0.441]
</p><p>66 ca Fuisnea they are no dnidff ethreennt Lfr aomre the ordinary log-likelihood computation. [sent-196, score-0.23]
</p><p>67 The terms and in (11) become the same forms that appear in the ordinary CRF optimization, i. [sent-199, score-0.23]
</p><p>68 (12) These calculations are performed by the ordinary FB on G1 and G2, respectively. [sent-202, score-0.23]
</p><p>69 As described in the previous section, it is assumed  that the values of the second factor∂∂f⟨h¯21,x⟩is guaranteed to exists for any given θ1, and the procedure for calculating them is fixed in advance. [sent-207, score-0.243]
</p><p>70 In other  words, ∂θ∂⟨1L,2k1⟩ bec∑omes the covariance between the k1-th input feature for L1 and the hypothetical feature h′⟨1,e1⟩  d≡ef∂h¯∂⟨1L,2e1⟩  · h⟨1,e1⟩. [sent-214, score-0.23]
</p><p>71 The second term of (18) can be calculated by the ordinary F-B because it consists of the marginals of arc features. [sent-216, score-0.464]
</p><p>72 d≡ef  It is easy for calcul)ating the value  AD transforms  this F-B into  anoth[er algorit]hffmflll for calculating the differentiation w. [sent-226, score-0.199]
</p><p>73 see  Li and Eisner (2009) and  634 numbers, the arithmetic operations and the exponential function are generalized to the dual numbers, and the ordinary F-B is also generalized to the dual numbers. [sent-233, score-0.478]
</p><p>74 The final line in the loop of Algorithm 1 can be implemented by various optimization routines and line search algorithms. [sent-236, score-0.223]
</p><p>75 The time and space complexity to compute the objective and gradient values for given parameter vectors θ1 , θ2 is the same as that for that for Bunescu (2008), up to a constant factor. [sent-237, score-0.212]
</p><p>76 The task is to annotate the POS tags and to perform base-phrase chunking on English sentences. [sent-240, score-0.255]
</p><p>77 Base-phrase chunking is a task to classify con-  tinuous subsequences of words into syntactic categories. [sent-241, score-0.255]
</p><p>78 This task is performed by annotating a chunking label on each word (Ramshaw and Marcus, 1995). [sent-242, score-0.314]
</p><p>79 The types of chunking label consist of “Begin-Category”, which represents the beginning of a chunk, “Inside-Category”, which represents the inside of a chunk, and “Other. [sent-243, score-0.314]
</p><p>80 ” Usually, POS labeling runs first before base-phrase chunking is performed. [sent-244, score-0.417]
</p><p>81 Therefore, this task is a typical interesting case where a sequence labeling depends on the output from other sequence labelers. [sent-245, score-0.531]
</p><p>82 The number of the label types used in base-phrase chunking is equal to 23. [sent-251, score-0.375]
</p><p>83 We compare the proposed method to two existing sequence labeling methods as baselines. [sent-252, score-0.301]
</p><p>84 This labeler is a simple CRF and learned by ordinary optimization procedure. [sent-254, score-0.535]
</p><p>85 A simple CRF model is learned for the chunking labeling, on the input sentences and the most likely POS label sequences predicted by the already learned POS labeler. [sent-256, score-0.471]
</p><p>86 ” The other baseline method has a CRF model for the chunking labeling, which uses the marginalized features offered by the POS labeler. [sent-258, score-0.499]
</p><p>87 However, the parameters of the POS labeler  are fixed in the training of the chunking model. [sent-259, score-0.397]
</p><p>88 In “CRF + CRF-BP,” the objective function for joint learning (10) is not guaranteed to be convex, so optimization procedure is sensible to the initial configuration of the model parameters. [sent-267, score-0.371]
</p><p>89 Although we only described the formalization and optimization procedure ofthe models with arc features, We use node features in the experiment. [sent-270, score-0.542]
</p><p>90 All node features are combined with the corresponding node label (POS or chunking label) feature. [sent-272, score-0.464]
</p><p>91 All arc features are combined with the feature of the corresponding  instantiated on each time slice in five character window. [sent-273, score-0.301]
</p><p>92 output features for “CRF  + CRF-MF”  and “CRF  ‡  arc label pair. [sent-274, score-0.331]
</p><p>93 †  features are  features are not used in POS labeler, and marginalized as  + CRF-BP. [sent-275, score-0.294]
</p><p>94 From Table 2, the proposed method significantly outperforms two baseline methods on chunking performance. [sent-279, score-0.255]
</p><p>95 Although the improvement on POS labeling performance by the proposed method “CRF + CRF-BP” is not significant, it might show  that optimization procedure provides some form of backward information propagation in comparison to “CRF + CRF-MF. [sent-280, score-0.535]
</p><p>96 ” 5  Conclusions  In this paper, we adopt the method to weight features on an upper sequence labeling stage by the marginalized probabilities estimated by the model on lower stages. [sent-281, score-0.985]
</p><p>97 We also point out that the model on an upper stage is considered to depend on the model on lower stages indirectly. [sent-282, score-0.536]
</p><p>98 In addition, we propose optimization procedure that enables the joint optimization of the multiple models on the different level of stages. [sent-283, score-0.393]
</p><p>99 Con-  ditional random fields: Probabilistic models for menting and labeling sequence data. [sent-328, score-0.301]
</p><p>100 Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. [sent-366, score-0.339]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ef', 0.264), ('chunking', 0.255), ('crf', 0.249), ('ordinary', 0.23), ('stage', 0.205), ('marginalized', 0.194), ('cascade', 0.18), ('upper', 0.177), ('optimization', 0.163), ('labeling', 0.162), ('labeler', 0.142), ('dag', 0.142), ('sequence', 0.139), ('arc', 0.131), ('differentiation', 0.117), ('bunescu', 0.105), ('sequences', 0.099), ('stages', 0.096), ('calculate', 0.095), ('cascaded', 0.093), ('output', 0.091), ('jacobian', 0.09), ('multilayer', 0.09), ('gradients', 0.087), ('objective', 0.086), ('propagation', 0.082), ('calculating', 0.082), ('formalization', 0.081), ('slice', 0.077), ('prev', 0.077), ('pos', 0.07), ('dual', 0.068), ('notation', 0.068), ('procedure', 0.067), ('predictions', 0.065), ('marginals', 0.065), ('omitted', 0.062), ('exp', 0.061), ('backward', 0.061), ('equal', 0.061), ('corliss', 0.06), ('covariances', 0.06), ('dlog', 0.06), ('routines', 0.06), ('sink', 0.06), ('superposition', 0.06), ('label', 0.059), ('input', 0.058), ('forward', 0.058), ('lower', 0.058), ('guaranteed', 0.055), ('sake', 0.052), ('expectation', 0.052), ('simplest', 0.052), ('marginal', 0.052), ('imaginary', 0.051), ('regularly', 0.051), ('forwardbackward', 0.051), ('functional', 0.05), ('hereafter', 0.05), ('features', 0.05), ('node', 0.05), ('gradient', 0.047), ('featu', 0.047), ('slices', 0.047), ('sarawagi', 0.047), ('nara', 0.047), ('outputs', 0.045), ('calculation', 0.045), ('src', 0.043), ('subscripts', 0.043), ('covariance', 0.043), ('subclass', 0.043), ('fh', 0.043), ('hypothetical', 0.043), ('sutton', 0.043), ('feature', 0.043), ('labels', 0.042), ('fields', 0.042), ('affect', 0.041), ('dynamic', 0.041), ('fl', 0.041), ('back', 0.04), ('directed', 0.04), ('parameter', 0.04), ('successful', 0.039), ('values', 0.039), ('calculated', 0.038), ('dot', 0.038), ('chunk', 0.038), ('arithmetic', 0.038), ('ramshaw', 0.038), ('probabilistic', 0.038), ('networks', 0.038), ('generalized', 0.037), ('chain', 0.037), ('marcus', 0.037), ('characteristics', 0.037), ('graphical', 0.037), ('totally', 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="96-tfidf-1" href="./emnlp-2011-Multilayer_Sequence_Labeling.html">96 emnlp-2011-Multilayer Sequence Labeling</a></p>
<p>Author: Ai Azuma ; Yuji Matsumoto</p><p>Abstract: In this paper, we describe a novel approach to cascaded learning and inference on sequences. We propose a weakly joint learning model on cascaded inference on sequences, called multilayer sequence labeling. In this model, inference on sequences is modeled as cascaded decision. However, the decision on a sequence labeling sequel to other decisions utilizes the features on the preceding results as marginalized by the probabilistic models on them. It is not novel itself, but our idea central to this paper is that the probabilistic models on succeeding labeling are viewed as indirectly depending on the probabilistic models on preceding analyses. We also propose two types of efficient dynamic programming which are required in the gradient-based optimization of an objective function. One of the dynamic programming algorithms resembles back propagation algorithm for mul- tilayer feed-forward neural networks. The other is a generalized version of the forwardbackward algorithm. We also report experiments of cascaded part-of-speech tagging and chunking of English sentences and show effectiveness of the proposed method.</p><p>2 0.1046921 <a title="96-tfidf-2" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>Author: Marco Dinarelli ; Sophie Rosset</p><p>Abstract: Reranking models have been successfully applied to many tasks of Natural Language Processing. However, there are two aspects of this approach that need a deeper investigation: (i) Assessment of hypotheses generated for reranking at classification phase: baseline models generate a list of hypotheses and these are used for reranking without any assessment; (ii) Detection of cases where reranking models provide a worst result: the best hypothesis provided by the reranking model is assumed to be always the best result. In some cases the reranking model provides an incorrect hypothesis while the baseline best hypothesis is correct, especially when baseline models are accurate. In this paper we propose solutions for these two aspects: (i) a semantic inconsistency metric to select possibly more correct n-best hypotheses, from a large set generated by an SLU basiline model. The selected hypotheses are reranked applying a state-of-the-art model based on Partial Tree Kernels, which encode SLU hypotheses in Support Vector Machines with complex structured features; (ii) finally, we apply a decision strategy, based on confidence values, to select the final hypothesis between the first ranked hypothesis provided by the baseline SLU model and the first ranked hypothesis provided by the re-ranker. We show the effectiveness of these solutions presenting comparative results obtained reranking hypotheses generated by a very accurate Conditional Random Field model. We evaluate our approach on the French MEDIA corpus. The results show significant improvements with respect to current state-of-the-art and previous 1104 Sophie Rosset LIMSI-CNRS B.P. 133, 91403 Orsay Cedex France ro s set @ l ims i fr . re-ranking models.</p><p>3 0.097998723 <a title="96-tfidf-3" href="./emnlp-2011-Joint_Models_for_Chinese_POS_Tagging_and_Dependency_Parsing.html">75 emnlp-2011-Joint Models for Chinese POS Tagging and Dependency Parsing</a></p>
<p>Author: Zhenghua Li ; Min Zhang ; Wanxiang Che ; Ting Liu ; Wenliang Chen ; Haizhou Li</p><p>Abstract: Part-of-speech (POS) is an indispensable feature in dependency parsing. Current research usually models POS tagging and dependency parsing independently. This may suffer from error propagation problem. Our experiments show that parsing accuracy drops by about 6% when using automatic POS tags instead of gold ones. To solve this issue, this paper proposes a solution by jointly optimizing POS tagging and dependency parsing in a unique model. We design several joint models and their corresponding decoding algorithms to incorporate different feature sets. We further present an effective pruning strategy to reduce the search space of candidate POS tags, leading to significant improvement of parsing speed. Experimental results on Chinese Penn Treebank 5 show that our joint models significantly improve the state-of-the-art parsing accuracy by about 1.5%. Detailed analysis shows that the joint method is able to choose such POS tags that are more helpful and discriminative from parsing viewpoint. This is the fundamental reason of parsing accuracy improvement.</p><p>4 0.097028896 <a title="96-tfidf-4" href="./emnlp-2011-Structured_Sparsity_in_Structured_Prediction.html">129 emnlp-2011-Structured Sparsity in Structured Prediction</a></p>
<p>Author: Andre Martins ; Noah Smith ; Mario Figueiredo ; Pedro Aguiar</p><p>Abstract: Linear models have enjoyed great success in structured prediction in NLP. While a lot of progress has been made on efficient training with several loss functions, the problem of endowing learners with a mechanism for feature selection is still unsolved. Common approaches employ ad hoc filtering or L1regularization; both ignore the structure of the feature space, preventing practicioners from encoding structural prior knowledge. We fill this gap by adopting regularizers that promote structured sparsity, along with efficient algorithms to handle them. Experiments on three tasks (chunking, entity recognition, and dependency parsing) show gains in performance, compactness, and model interpretability.</p><p>5 0.088117473 <a title="96-tfidf-5" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>Author: Reut Tsarfaty ; Joakim Nivre ; Evelina Andersson</p><p>Abstract: unkown-abstract</p><p>6 0.072153889 <a title="96-tfidf-6" href="./emnlp-2011-Exact_Decoding_of_Phrase-Based_Translation_Models_through_Lagrangian_Relaxation.html">51 emnlp-2011-Exact Decoding of Phrase-Based Translation Models through Lagrangian Relaxation</a></p>
<p>7 0.071895011 <a title="96-tfidf-7" href="./emnlp-2011-Universal_Morphological_Analysis_using_Structured_Nearest_Neighbor_Prediction.html">140 emnlp-2011-Universal Morphological Analysis using Structured Nearest Neighbor Prediction</a></p>
<p>8 0.067889243 <a title="96-tfidf-8" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>9 0.067070171 <a title="96-tfidf-9" href="./emnlp-2011-Unsupervised_Structure_Prediction_with_Non-Parallel_Multilingual_Guidance.html">146 emnlp-2011-Unsupervised Structure Prediction with Non-Parallel Multilingual Guidance</a></p>
<p>10 0.066083334 <a title="96-tfidf-10" href="./emnlp-2011-A_Joint_Model_for_Extended_Semantic_Role_Labeling.html">7 emnlp-2011-A Joint Model for Extended Semantic Role Labeling</a></p>
<p>11 0.064202577 <a title="96-tfidf-11" href="./emnlp-2011-Unsupervised_Semantic_Role_Induction_with_Graph_Partitioning.html">145 emnlp-2011-Unsupervised Semantic Role Induction with Graph Partitioning</a></p>
<p>12 0.063985199 <a title="96-tfidf-12" href="./emnlp-2011-Dual_Decomposition_with_Many_Overlapping_Components.html">45 emnlp-2011-Dual Decomposition with Many Overlapping Components</a></p>
<p>13 0.062998526 <a title="96-tfidf-13" href="./emnlp-2011-Minimum_Imputed-Risk%3A_Unsupervised_Discriminative_Training_for_Machine_Translation.html">93 emnlp-2011-Minimum Imputed-Risk: Unsupervised Discriminative Training for Machine Translation</a></p>
<p>14 0.062418684 <a title="96-tfidf-14" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>15 0.062355414 <a title="96-tfidf-15" href="./emnlp-2011-Statistical_Machine_Translation_with_Local_Language_Models.html">125 emnlp-2011-Statistical Machine Translation with Local Language Models</a></p>
<p>16 0.061054472 <a title="96-tfidf-16" href="./emnlp-2011-Exact_Inference_for_Generative_Probabilistic_Non-Projective_Dependency_Parsing.html">52 emnlp-2011-Exact Inference for Generative Probabilistic Non-Projective Dependency Parsing</a></p>
<p>17 0.060509995 <a title="96-tfidf-17" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>18 0.059461202 <a title="96-tfidf-18" href="./emnlp-2011-Bootstrapped_Named_Entity_Recognition_for_Product_Attribute_Extraction.html">23 emnlp-2011-Bootstrapped Named Entity Recognition for Product Attribute Extraction</a></p>
<p>19 0.058924701 <a title="96-tfidf-19" href="./emnlp-2011-A_Non-negative_Matrix_Factorization_Based_Approach_for_Active_Dual_Supervision_from_Document_and_Word_Labels.html">9 emnlp-2011-A Non-negative Matrix Factorization Based Approach for Active Dual Supervision from Document and Word Labels</a></p>
<p>20 0.057039116 <a title="96-tfidf-20" href="./emnlp-2011-Optimal_Search_for_Minimum_Error_Rate_Training.html">100 emnlp-2011-Optimal Search for Minimum Error Rate Training</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.221), (1, -0.001), (2, -0.035), (3, 0.05), (4, 0.022), (5, -0.006), (6, -0.061), (7, -0.202), (8, -0.027), (9, 0.008), (10, 0.026), (11, -0.054), (12, -0.022), (13, 0.044), (14, -0.12), (15, -0.084), (16, -0.043), (17, -0.01), (18, 0.025), (19, 0.042), (20, 0.023), (21, 0.027), (22, 0.042), (23, -0.009), (24, 0.137), (25, -0.001), (26, -0.006), (27, 0.088), (28, -0.056), (29, -0.007), (30, 0.053), (31, 0.109), (32, -0.092), (33, -0.015), (34, 0.16), (35, 0.051), (36, 0.022), (37, 0.094), (38, 0.058), (39, 0.049), (40, 0.028), (41, -0.093), (42, -0.125), (43, -0.013), (44, 0.289), (45, -0.016), (46, 0.036), (47, 0.059), (48, -0.062), (49, -0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95672208 <a title="96-lsi-1" href="./emnlp-2011-Multilayer_Sequence_Labeling.html">96 emnlp-2011-Multilayer Sequence Labeling</a></p>
<p>Author: Ai Azuma ; Yuji Matsumoto</p><p>Abstract: In this paper, we describe a novel approach to cascaded learning and inference on sequences. We propose a weakly joint learning model on cascaded inference on sequences, called multilayer sequence labeling. In this model, inference on sequences is modeled as cascaded decision. However, the decision on a sequence labeling sequel to other decisions utilizes the features on the preceding results as marginalized by the probabilistic models on them. It is not novel itself, but our idea central to this paper is that the probabilistic models on succeeding labeling are viewed as indirectly depending on the probabilistic models on preceding analyses. We also propose two types of efficient dynamic programming which are required in the gradient-based optimization of an objective function. One of the dynamic programming algorithms resembles back propagation algorithm for mul- tilayer feed-forward neural networks. The other is a generalized version of the forwardbackward algorithm. We also report experiments of cascaded part-of-speech tagging and chunking of English sentences and show effectiveness of the proposed method.</p><p>2 0.67785144 <a title="96-lsi-2" href="./emnlp-2011-Structured_Sparsity_in_Structured_Prediction.html">129 emnlp-2011-Structured Sparsity in Structured Prediction</a></p>
<p>Author: Andre Martins ; Noah Smith ; Mario Figueiredo ; Pedro Aguiar</p><p>Abstract: Linear models have enjoyed great success in structured prediction in NLP. While a lot of progress has been made on efficient training with several loss functions, the problem of endowing learners with a mechanism for feature selection is still unsolved. Common approaches employ ad hoc filtering or L1regularization; both ignore the structure of the feature space, preventing practicioners from encoding structural prior knowledge. We fill this gap by adopting regularizers that promote structured sparsity, along with efficient algorithms to handle them. Experiments on three tasks (chunking, entity recognition, and dependency parsing) show gains in performance, compactness, and model interpretability.</p><p>3 0.47055548 <a title="96-lsi-3" href="./emnlp-2011-Class_Label_Enhancement_via_Related_Instances.html">26 emnlp-2011-Class Label Enhancement via Related Instances</a></p>
<p>Author: Zornitsa Kozareva ; Konstantin Voevodski ; Shanghua Teng</p><p>Abstract: Class-instance label propagation algorithms have been successfully used to fuse information from multiple sources in order to enrich a set of unlabeled instances with class labels. Yet, nobody has explored the relationships between the instances themselves to enhance an initial set of class-instance pairs. We propose two graph-theoretic methods (centrality and regularization), which start with a small set of labeled class-instance pairs and use the instance-instance network to extend the class labels to all instances in the network. We carry out a comparative study with state-of-the-art knowledge harvesting algorithm and show that our approach can learn additional class labels while maintaining high accuracy. We conduct a comparative study between class-instance and instance-instance graphs used to propagate the class labels and show that the latter one achieves higher accuracy.</p><p>4 0.46261755 <a title="96-lsi-4" href="./emnlp-2011-Unsupervised_Information_Extraction_with_Distributional_Prior_Knowledge.html">143 emnlp-2011-Unsupervised Information Extraction with Distributional Prior Knowledge</a></p>
<p>Author: Cane Wing-ki Leung ; Jing Jiang ; Kian Ming A. Chai ; Hai Leong Chieu ; Loo-Nin Teow</p><p>Abstract: We address the task of automatic discovery of information extraction template from a given text collection. Our approach clusters candidate slot fillers to identify meaningful template slots. We propose a generative model that incorporates distributional prior knowledge to help distribute candidates in a document into appropriate slots. Empirical results suggest that the proposed prior can bring substantial improvements to our task as compared to a K-means baseline and a Gaussian mixture model baseline. Specifically, the proposed prior has shown to be effective when coupled with discriminative features of the candidates.</p><p>5 0.46076599 <a title="96-lsi-5" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>Author: Marco Dinarelli ; Sophie Rosset</p><p>Abstract: Reranking models have been successfully applied to many tasks of Natural Language Processing. However, there are two aspects of this approach that need a deeper investigation: (i) Assessment of hypotheses generated for reranking at classification phase: baseline models generate a list of hypotheses and these are used for reranking without any assessment; (ii) Detection of cases where reranking models provide a worst result: the best hypothesis provided by the reranking model is assumed to be always the best result. In some cases the reranking model provides an incorrect hypothesis while the baseline best hypothesis is correct, especially when baseline models are accurate. In this paper we propose solutions for these two aspects: (i) a semantic inconsistency metric to select possibly more correct n-best hypotheses, from a large set generated by an SLU basiline model. The selected hypotheses are reranked applying a state-of-the-art model based on Partial Tree Kernels, which encode SLU hypotheses in Support Vector Machines with complex structured features; (ii) finally, we apply a decision strategy, based on confidence values, to select the final hypothesis between the first ranked hypothesis provided by the baseline SLU model and the first ranked hypothesis provided by the re-ranker. We show the effectiveness of these solutions presenting comparative results obtained reranking hypotheses generated by a very accurate Conditional Random Field model. We evaluate our approach on the French MEDIA corpus. The results show significant improvements with respect to current state-of-the-art and previous 1104 Sophie Rosset LIMSI-CNRS B.P. 133, 91403 Orsay Cedex France ro s set @ l ims i fr . re-ranking models.</p><p>6 0.42909446 <a title="96-lsi-6" href="./emnlp-2011-Bootstrapped_Named_Entity_Recognition_for_Product_Attribute_Extraction.html">23 emnlp-2011-Bootstrapped Named Entity Recognition for Product Attribute Extraction</a></p>
<p>7 0.40600103 <a title="96-lsi-7" href="./emnlp-2011-Joint_Models_for_Chinese_POS_Tagging_and_Dependency_Parsing.html">75 emnlp-2011-Joint Models for Chinese POS Tagging and Dependency Parsing</a></p>
<p>8 0.38400984 <a title="96-lsi-8" href="./emnlp-2011-Optimal_Search_for_Minimum_Error_Rate_Training.html">100 emnlp-2011-Optimal Search for Minimum Error Rate Training</a></p>
<p>9 0.38067773 <a title="96-lsi-9" href="./emnlp-2011-Dual_Decomposition_with_Many_Overlapping_Components.html">45 emnlp-2011-Dual Decomposition with Many Overlapping Components</a></p>
<p>10 0.37178552 <a title="96-lsi-10" href="./emnlp-2011-A_Joint_Model_for_Extended_Semantic_Role_Labeling.html">7 emnlp-2011-A Joint Model for Extended Semantic Role Labeling</a></p>
<p>11 0.36156449 <a title="96-lsi-11" href="./emnlp-2011-A_Weakly-supervised_Approach_to_Argumentative_Zoning_of_Scientific_Documents.html">12 emnlp-2011-A Weakly-supervised Approach to Argumentative Zoning of Scientific Documents</a></p>
<p>12 0.34732556 <a title="96-lsi-12" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>13 0.34038082 <a title="96-lsi-13" href="./emnlp-2011-Predicting_a_Scientific_Communitys_Response_to_an_Article.html">106 emnlp-2011-Predicting a Scientific Communitys Response to an Article</a></p>
<p>14 0.33676633 <a title="96-lsi-14" href="./emnlp-2011-Unsupervised_Semantic_Role_Induction_with_Graph_Partitioning.html">145 emnlp-2011-Unsupervised Semantic Role Induction with Graph Partitioning</a></p>
<p>15 0.33031473 <a title="96-lsi-15" href="./emnlp-2011-Learning_Local_Content_Shift_Detectors_from_Document-level_Information.html">82 emnlp-2011-Learning Local Content Shift Detectors from Document-level Information</a></p>
<p>16 0.32979202 <a title="96-lsi-16" href="./emnlp-2011-Enhancing_Chinese_Word_Segmentation_Using_Unlabeled_Data.html">48 emnlp-2011-Enhancing Chinese Word Segmentation Using Unlabeled Data</a></p>
<p>17 0.32956204 <a title="96-lsi-17" href="./emnlp-2011-Unsupervised_Structure_Prediction_with_Non-Parallel_Multilingual_Guidance.html">146 emnlp-2011-Unsupervised Structure Prediction with Non-Parallel Multilingual Guidance</a></p>
<p>18 0.32844934 <a title="96-lsi-18" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>19 0.32601759 <a title="96-lsi-19" href="./emnlp-2011-Linear_Text_Segmentation_Using_Affinity_Propagation.html">88 emnlp-2011-Linear Text Segmentation Using Affinity Propagation</a></p>
<p>20 0.32296389 <a title="96-lsi-20" href="./emnlp-2011-Exact_Inference_for_Generative_Probabilistic_Non-Projective_Dependency_Parsing.html">52 emnlp-2011-Exact Inference for Generative Probabilistic Non-Projective Dependency Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.105), (36, 0.52), (37, 0.024), (45, 0.036), (53, 0.014), (54, 0.011), (57, 0.013), (62, 0.016), (64, 0.029), (66, 0.021), (69, 0.022), (79, 0.036), (82, 0.029), (90, 0.016), (96, 0.035), (98, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90681303 <a title="96-lda-1" href="./emnlp-2011-Multilayer_Sequence_Labeling.html">96 emnlp-2011-Multilayer Sequence Labeling</a></p>
<p>Author: Ai Azuma ; Yuji Matsumoto</p><p>Abstract: In this paper, we describe a novel approach to cascaded learning and inference on sequences. We propose a weakly joint learning model on cascaded inference on sequences, called multilayer sequence labeling. In this model, inference on sequences is modeled as cascaded decision. However, the decision on a sequence labeling sequel to other decisions utilizes the features on the preceding results as marginalized by the probabilistic models on them. It is not novel itself, but our idea central to this paper is that the probabilistic models on succeeding labeling are viewed as indirectly depending on the probabilistic models on preceding analyses. We also propose two types of efficient dynamic programming which are required in the gradient-based optimization of an objective function. One of the dynamic programming algorithms resembles back propagation algorithm for mul- tilayer feed-forward neural networks. The other is a generalized version of the forwardbackward algorithm. We also report experiments of cascaded part-of-speech tagging and chunking of English sentences and show effectiveness of the proposed method.</p><p>2 0.87355703 <a title="96-lda-2" href="./emnlp-2011-Parse_Correction_with_Specialized_Models_for_Difficult_Attachment_Types.html">102 emnlp-2011-Parse Correction with Specialized Models for Difficult Attachment Types</a></p>
<p>Author: Enrique Henestroza Anguiano ; Marie Candito</p><p>Abstract: This paper develops a framework for syntactic dependency parse correction. Dependencies in an input parse tree are revised by selecting, for a given dependent, the best governor from within a small set of candidates. We use a discriminative linear ranking model to select the best governor from a group of candidates for a dependent, and our model includes a rich feature set that encodes syntactic structure in the input parse tree. The parse correction framework is parser-agnostic, and can correct attachments using either a generic model or specialized models tailored to difficult attachment types like coordination and pp-attachment. Our experiments show that parse correction, combining a generic model with specialized models for difficult attachment types, can successfully improve the quality of predicted parse trees output by sev- eral representative state-of-the-art dependency parsers for French.</p><p>3 0.76262075 <a title="96-lda-3" href="./emnlp-2011-Identification_of_Multi-word_Expressions_by_Combining_Multiple_Linguistic_Information_Sources.html">69 emnlp-2011-Identification of Multi-word Expressions by Combining Multiple Linguistic Information Sources</a></p>
<p>Author: Yulia Tsvetkov ; Shuly Wintner</p><p>Abstract: We propose an architecture for expressing various linguistically-motivated features that help identify multi-word expressions in natural language texts. The architecture combines various linguistically-motivated classification features in a Bayesian Network. We introduce novel ways for computing many of these features, and manually define linguistically-motivated interrelationships among them, which the Bayesian network models. Our methodology is almost entirely unsupervised and completely languageindependent; it relies on few language resources and is thus suitable for a large number of languages. Furthermore, unlike much recent work, our approach can identify expressions of various types and syntactic con- structions. We demonstrate a significant improvement in identification accuracy, compared with less sophisticated baselines.</p><p>4 0.47510698 <a title="96-lda-4" href="./emnlp-2011-Tuning_as_Ranking.html">138 emnlp-2011-Tuning as Ranking</a></p>
<p>Author: Mark Hopkins ; Jonathan May</p><p>Abstract: We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chiang et al., 2008b), PRO is easy to implement. It uses off-the-shelf linear binary classifier software and can be built on top of an existing MERT framework in a matter of hours. We establish PRO’s scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.</p><p>5 0.42443737 <a title="96-lda-5" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>Author: Marco Dinarelli ; Sophie Rosset</p><p>Abstract: Reranking models have been successfully applied to many tasks of Natural Language Processing. However, there are two aspects of this approach that need a deeper investigation: (i) Assessment of hypotheses generated for reranking at classification phase: baseline models generate a list of hypotheses and these are used for reranking without any assessment; (ii) Detection of cases where reranking models provide a worst result: the best hypothesis provided by the reranking model is assumed to be always the best result. In some cases the reranking model provides an incorrect hypothesis while the baseline best hypothesis is correct, especially when baseline models are accurate. In this paper we propose solutions for these two aspects: (i) a semantic inconsistency metric to select possibly more correct n-best hypotheses, from a large set generated by an SLU basiline model. The selected hypotheses are reranked applying a state-of-the-art model based on Partial Tree Kernels, which encode SLU hypotheses in Support Vector Machines with complex structured features; (ii) finally, we apply a decision strategy, based on confidence values, to select the final hypothesis between the first ranked hypothesis provided by the baseline SLU model and the first ranked hypothesis provided by the re-ranker. We show the effectiveness of these solutions presenting comparative results obtained reranking hypotheses generated by a very accurate Conditional Random Field model. We evaluate our approach on the French MEDIA corpus. The results show significant improvements with respect to current state-of-the-art and previous 1104 Sophie Rosset LIMSI-CNRS B.P. 133, 91403 Orsay Cedex France ro s set @ l ims i fr . re-ranking models.</p><p>6 0.40005854 <a title="96-lda-6" href="./emnlp-2011-Third-order_Variational_Reranking_on_Packed-Shared_Dependency_Forests.html">134 emnlp-2011-Third-order Variational Reranking on Packed-Shared Dependency Forests</a></p>
<p>7 0.39602122 <a title="96-lda-7" href="./emnlp-2011-Syntax-Based_Grammaticality_Improvement_using_CCG_and_Guided_Search.html">132 emnlp-2011-Syntax-Based Grammaticality Improvement using CCG and Guided Search</a></p>
<p>8 0.38857651 <a title="96-lda-8" href="./emnlp-2011-Linear_Text_Segmentation_Using_Affinity_Propagation.html">88 emnlp-2011-Linear Text Segmentation Using Affinity Propagation</a></p>
<p>9 0.38028058 <a title="96-lda-9" href="./emnlp-2011-Heuristic_Search_for_Non-Bottom-Up_Tree_Structure_Prediction.html">65 emnlp-2011-Heuristic Search for Non-Bottom-Up Tree Structure Prediction</a></p>
<p>10 0.37773472 <a title="96-lda-10" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>11 0.36891302 <a title="96-lda-11" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>12 0.36736909 <a title="96-lda-12" href="./emnlp-2011-Structural_Opinion_Mining_for_Graph-based_Sentiment_Representation.html">126 emnlp-2011-Structural Opinion Mining for Graph-based Sentiment Representation</a></p>
<p>13 0.35782751 <a title="96-lda-13" href="./emnlp-2011-Multiword_Expression_Identification_with_Tree_Substitution_Grammars%3A_A_Parsing_tour_de_force_with_French.html">97 emnlp-2011-Multiword Expression Identification with Tree Substitution Grammars: A Parsing tour de force with French</a></p>
<p>14 0.34453899 <a title="96-lda-14" href="./emnlp-2011-Rumor_has_it%3A_Identifying_Misinformation_in_Microblogs.html">117 emnlp-2011-Rumor has it: Identifying Misinformation in Microblogs</a></p>
<p>15 0.34346625 <a title="96-lda-15" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>16 0.34198183 <a title="96-lda-16" href="./emnlp-2011-Efficient_retrieval_of_tree_translation_examples_for_Syntax-Based_Machine_Translation.html">47 emnlp-2011-Efficient retrieval of tree translation examples for Syntax-Based Machine Translation</a></p>
<p>17 0.34193066 <a title="96-lda-17" href="./emnlp-2011-Unsupervised_Information_Extraction_with_Distributional_Prior_Knowledge.html">143 emnlp-2011-Unsupervised Information Extraction with Distributional Prior Knowledge</a></p>
<p>18 0.3410503 <a title="96-lda-18" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>19 0.34091857 <a title="96-lda-19" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>20 0.34046334 <a title="96-lda-20" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
