<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>118 emnlp-2011-SMT Helps Bitext Dependency Parsing</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-118" href="#">emnlp2011-118</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>118 emnlp-2011-SMT Helps Bitext Dependency Parsing</h1>
<br/><p>Source: <a title="emnlp-2011-118-pdf" href="http://aclweb.org/anthology//D/D11/D11-1007.pdf">pdf</a></p><p>Author: Wenliang Chen ; Jun'ichi Kazama ; Min Zhang ; Yoshimasa Tsuruoka ; Yujie Zhang ; Yiou Wang ; Kentaro Torisawa ; Haizhou Li</p><p>Abstract: We propose a method to improve the accuracy of parsing bilingual texts (bitexts) with the help of statistical machine translation (SMT) systems. Previous bitext parsing methods use human-annotated bilingual treebanks that are hard to obtain. Instead, our approach uses an auto-generated bilingual treebank to produce bilingual constraints. However, because the auto-generated bilingual treebank contains errors, the bilingual constraints are noisy. To overcome this problem, we use large-scale unannotated data to verify the constraints and design a set of effective bilingual features for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT.</p><p>Reference: <a title="emnlp-2011-118-reference" href="../emnlp2011_reference/emnlp-2011-SMT_Helps_Bitext_Dependency_Parsing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ijpo  ,  ,  ,  ,  ,  Abstract We propose a method to improve the accuracy of parsing bilingual texts (bitexts) with the help of statistical machine translation (SMT) systems. [sent-14, score-0.642]
</p><p>2 Previous bitext parsing methods use human-annotated bilingual treebanks that are hard to obtain. [sent-15, score-0.807]
</p><p>3 Instead, our approach uses an auto-generated bilingual treebank to produce bilingual constraints. [sent-16, score-1.113]
</p><p>4 However, because the auto-generated bilingual treebank contains errors, the bilingual constraints are noisy. [sent-17, score-1.172]
</p><p>5 To overcome this problem, we use large-scale unannotated data to verify the constraints and design a set of effective bilingual features for parsing models based on the verified results. [sent-18, score-1.069]
</p><p>6 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al. [sent-22, score-0.581]
</p><p>7 In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. [sent-26, score-0.279]
</p><p>8 Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. [sent-28, score-1.329]
</p><p>9 Their method uses bilingual treebanks that have human-annotated tree structures on both sides. [sent-30, score-0.625]
</p><p>10 It uses another type of bilingual treebanks that have tree structures on the source sentences and their human-translated sentences. [sent-33, score-0.699]
</p><p>11 (2010) also used bilingual treebanks and made use of tree structures on the target side. [sent-35, score-0.69]
</p><p>12 However, the bilingual treebanks are hard to obtain, partly because of the high cost of human translation. [sent-36, score-0.625]
</p><p>13 On the other hand, many large-scale monolingual treebanks exist, such as the Penn English Treebank (PTB) (Marcus et al. [sent-38, score-0.259]
</p><p>14 In this paper, we propose a bitext parsing approach in which we produce the bilingual constraints on existing monolingual treebanks with the help of SMT systems. [sent-40, score-1.052]
</p><p>15 In our approach, we first use an SMT system  to translate the sentences of a source monolingual treebank into the target language. [sent-42, score-0.396]
</p><p>16 Then, the target sentences are parsed by a parser trained on a target monolingual treebank. [sent-43, score-0.362]
</p><p>17 We then obtain a bilingual treebank that has human annotated trees on the source side and auto-generated trees on the target side. [sent-44, score-0.768]
</p><p>18 c e2th0o1d1s A ins Nocaitautiroanl L foarn Cguoamgpeu Ptartoicoensaslin Lgin,g puaigsetisc 7s3–83, target side are not perfect, we expect that we can improve bitext parsing performance by using this newly auto-generated bilingual treebank. [sent-47, score-0.831]
</p><p>19 Then we can produce a set of bilingual constraints between the two sides. [sent-49, score-0.581]
</p><p>20 To overcome this problem, we verify the constraints by using large-scale unannotated monolingual sentences and bilingual sentence pairs. [sent-51, score-0.915]
</p><p>21 Finally, we design a set of bilingual features based on the verified results for parsing models. [sent-52, score-0.856]
</p><p>22 Our approach uses existing resources including monolingual treebanks to train monolingual parsers  on both sides, bilingual unannotated data to train SMT systems and to extract bilingual subtrees, and target monolingual unannotated data to extract monolingual subtrees. [sent-53, score-2.018]
</p><p>23 In summary, we make the following contributions: • We propose an approach that uses an autogenerated bilingual atcreheb tahnakt sreasthe arn tuhtaonhuman-annotated bilingual treebanks used in previous studies (Burkett and Klein, 2008; Huang et al. [sent-54, score-1.147]
</p><p>24 The auto-generated bilingual treebank is built with the help of SMT systems. [sent-57, score-0.621]
</p><p>25 •  •  We verify the unreliable constraints by using tWhee existing large-scale eun coannsntortaaintetds dbayta u sainndg design a set of effective bilingual features over the verified results. [sent-58, score-0.933]
</p><p>26 Section 4 describes a set of bilingual features based on the bilingual constraints and  Section 5 describes how to use large-scale unannotated data to verify the bilingual constraints and define another set of bilingual features based on the verified results. [sent-70, score-2.669]
</p><p>27 2  Motivation  Here, bitext parsing is the task of parsing source sentences with the help of their corresponding translations. [sent-73, score-0.345]
</p><p>28 ttaa  ggaaoodduu  ppiinnggjjiiaa  llee yyuu lliippeenngg zzoonnggllii (b)  ddee hhuuiittaann jjiieegguuoo  Figure 1: Input and output of our approach In bitext parsing, some ambiguities exist on the source side, but they may be unambiguous on the target side. [sent-122, score-0.238]
</p><p>29 ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de liliang he jiqiao PN VV DT! [sent-153, score-1.548]
</p><p>30 ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de liliang he jiqiao (b)  ? [sent-178, score-1.548]
</p><p>31 The figure shows that the translation can provide useful bilingual constraints. [sent-203, score-0.553]
</p><p>32 However, there are few humanannotated bilingual treebanks and the existing bilingual treebanks are usually small. [sent-207, score-1.279]
</p><p>33 So we want to use existing resources to generate a bilingual treebank with the help of SMT systems. [sent-211, score-0.621]
</p><p>34 We hope to improve source side parsing by using this newly built bilingual treebank. [sent-212, score-0.726]
</p><p>35 ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai  de liliang he jiqiao  He hoped that all the athletes would fully demonstrate the strength and skill that they cultivate daily  Figure 3: Example of human translation  ? [sent-247, score-1.79]
</p><p>36 ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai  de liliang he jiqiao  he! [sent-281, score-1.548]
</p><p>37 skil s Figure 4: Example of Moses translation Figure 4 shows an example of a translation using a Moses-based system, where the target sentence is parsed by a monolingual target parser. [sent-299, score-0.433]
</p><p>38 From this example, although the sentences and parse trees on the target side are not perfect, we still can explore useful information to improve bitext parsing. [sent-303, score-0.274]
</p><p>39 In this paper, we focus on how to design a method to verify such unreliable bilingual constraints. [sent-304, score-0.599]
</p><p>40 One is monolingual features based on the source sentences. [sent-311, score-0.24]
</p><p>41 The other one is bilingual features (described in Sections 4 and 5) that consider the bilingual constraints. [sent-313, score-1.078]
</p><p>42 We call the parser with the monolingual features on the source side Parsers, and the parser with the monolingual features on the target side Parsert. [sent-314, score-0.669]
</p><p>43 4  Original bilingual features  In this paper, we generate two types of bilingual features, original and verified bilingual features. [sent-315, score-1.841]
</p><p>44 The original bilingual features (described in this section) are based on the bilingual constraints without being verified by large-scale unannotated data. [sent-316, score-1.455]
</p><p>45 And the verified bilingual features (described in Section 5) are based on the bilingual constraints verified by using large-scale unannotated data. [sent-317, score-1.696]
</p><p>46 1 Auto-generated bilingual treebank Assuming that we have monolingual treebanks on the source side, an SMT system that can translate the source sentences into the target language, and a  Parsert trained on the target monolingual treebank. [sent-319, score-1.292]
</p><p>47 We first translate the sentences of the source monolingual treebank into the target language using the SMT system. [sent-320, score-0.396]
</p><p>48 We define a binary function for this bilingual constraint: Fbn(rsn : rtk), where n and k refers to the types of the dependencies (2 for bigram and 3 for trigram). [sent-335, score-0.64]
</p><p>49 For example, in rs2 : rt3, rs2 is a bigram dependency on the source side and rt3 is a trigram dependency on the target side. [sent-336, score-0.449]
</p><p>50 1 Bigram constraint function: Fb2 For rs2, we consider two types of bilingual constraints. [sent-339, score-0.568]
</p><p>51 taxiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilaide! [sent-399, score-0.951]
</p><p>52 skil s Figure 5: Example of bilingual constraints (2to2)  ? [sent-420, score-0.639]
</p><p>53 ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de liliang he jiqiao  he! [sent-468, score-1.548]
</p><p>54 skil s Figure 6: Example of bilingual constraints (2to3) 4. [sent-486, score-0.639]
</p><p>55 taxiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilaide! [sent-543, score-0.951]
</p><p>56 skil s  Figure 7: Example of bilingual constraints (3to3) 4. [sent-564, score-0.639]
</p><p>57 4 Original bilingual features We define original bilingual features based on the bilingual constraint functions and the bilingual reordering function. [sent-573, score-2.202]
</p><p>58 h F Firb s2o ti-,Do Tradibreli r,Ff1er:aoOitur geisnalh SbF ielcbi no3 g,nuD da-ilor fe,idaFetur oefi satures We use an example to show how to generate the original bilingual features in practice. [sent-576, score-0.556]
</p><p>59 In Figure 4, we want to define the bilingual features for the bigram dependency (rs2) between “发挥(fahui)” and “技巧(jiqiao)”. [sent-577, score-0.691]
</p><p>60 f 5  Verified bilingual features  However, because the bilingual treebank is generated automatically, using the bilingual constraints alone is not reliable. [sent-581, score-1.728]
</p><p>61 More specifically, rtk of the constraint is verified by checking a list of target monolingual subtrees and rsn : rtk is verified by checking a list of bilingual subtrees. [sent-583, score-1.952]
</p><p>62 The subtrees are extracted from the large-scale unannotated data. [sent-584, score-0.238]
</p><p>63 The basic idea is as follows: if the dependency structures of a bilingual constraint can be found in the list of the target monolingual subtrees 1For the second order features, Dir is the combination of the directions of two dependencies. [sent-585, score-1.008]
</p><p>64 or bilingual subtrees, this constraint will probably be reliable. [sent-586, score-0.568]
</p><p>65 We first parse the large-scale unannotated monolingual and bilingual data. [sent-587, score-0.755]
</p><p>66 Subsequently, we extract the monolingual and bilingual subtrees from the parsed data. [sent-588, score-0.866]
</p><p>67 We then verify the bilingual constraints using the extracted subtrees. [sent-589, score-0.658]
</p><p>68 Finally, we generate the bilingual features based on the verified results for the parsing models. [sent-590, score-0.856]
</p><p>69 (2009) proposed a simple method to extract subtrees from large-scale monolingual data and used them as features to improve monolingual parsing. [sent-595, score-0.507]
</p><p>70 Following their method, we parse large unannotated data with the Parsert and obtain the subtree list (STt) on the target side. [sent-596, score-0.229]
</p><p>71 We extract two types of subtrees: bigram (two words) subtree and trigram (three words) subtree. [sent-597, score-0.243]
</p><p>72 Figure 8: Example of monolingual subtree extraction From the dependency tree in Figure 8-(a), we obtain the subtrees shown in Figure 8-(b) where the  first three are bigram subtrees and the last one is a trigram subtree. [sent-606, score-0.779]
</p><p>73 After extraction, we obtain the subtree list STt that includes two sets, one for bigram subtrees, and the other one for trigram subtrees. [sent-607, score-0.243]
</p><p>74 2  Verified target constraint function: Fvt(rtk) We use the extracted target subtrees to verify the rtk of the bilingual constraints. [sent-617, score-1.167]
</p><p>75 If the rtk is included in STt, function Fvt(rtk) = Type(rtk), otherwise Fvt(rtk) = ZERO. [sent-619, score-0.231]
</p><p>76 3 Bilingual subtrees We extract bilingual subtrees from a bilingual corpus, which is parsed by the Parsers and Parsert on both sides. [sent-624, score-1.393]
</p><p>77 We extract three types of bilingual subtrees: bigram-bigram (stbi22), bigram-trigram (stbi23), and trigram-trigram (stbi33) subtrees. [sent-625, score-0.522]
</p><p>78 For example, stbi22 consists of a bigram subtree on the source side and a bigram subtree on the target side. [sent-626, score-0.505]
</p><p>79 tu dent Figure 9: Example of bilingual subtree extraction From the dependency tree in Figure 9-(a), we obtain the bilingual subtrees shown in Figure 9(b). [sent-643, score-1.35]
</p><p>80 Figure 9-(b) shows the extracted bigram-bigram  bilingual subtrees. [sent-644, score-0.522]
</p><p>81 4  Verified bilingual constraint function:  Fvb(rbink)  We use the extracted bilingual subtrees to verify the rsn : rtk (rbink in short) of the bilingual constraints. [sent-649, score-2.139]
</p><p>82 rsn and rtk form a candidate bilingual subtree stbink. [sent-650, score-0.898]
</p><p>83 2 Verified bilingual features Then, we define another set of bilingual features by combining the verified constraint functions. [sent-653, score-1.399]
</p><p>84 We call these bilingual features ‘verified bilingual features’ . [sent-654, score-1.078]
</p><p>85 Table 2 lists the verified bilingual features used in our experiments, where each line defines a feature template that is a combination of functions. [sent-655, score-0.823]
</p><p>86 We use an example to show how to generate the verified bilingual features in practice. [sent-656, score-0.797]
</p><p>87 In Figure 4, we want to define the verified features for the bigram dependency (rs2) between “发挥(fahui)” and “技 巧(jiqiao)”. [sent-657, score-0.41]
</p><p>88 Suppose we can find rt3 in STt with label MF and can not find the candidate bilingual subtree in STbi. [sent-660, score-0.609]
</p><p>89 Note that we did not use human translation on the English side of this bilingual treebank to train our new parsers. [sent-668, score-0.684]
</p><p>90 To extract bilingual subtrees, we used the FBIS corpus and an additional bilingual corpus containing 800,000 sentence pairs from the training data of NIST MT08 evaluation campaign. [sent-712, score-1.044]
</p><p>91 We used PAG to refer to our parsers trained on the auto-generated bilingual treebank. [sent-727, score-0.55]
</p><p>92 29 points for the second-order model by adding the verified bilingual features. [sent-739, score-0.763]
</p><p>93 If we used the original bilingual features (PAGo), the system dropped 0. [sent-743, score-0.556]
</p><p>94 This indicated that the verified bilingual constraints did provide useful information for the parsing models. [sent-746, score-0.911]
</p><p>95 These facts indicated that our approach obtained the benefits from the verified constraints, while using the bilingual constraints alone was enough for ORACLE. [sent-751, score-0.852]
</p><p>96 Types HA and AG denote training on human-annotated and auto-generated bilingual treebanks respectively. [sent-797, score-0.625]
</p><p>97 (2009), Chen2010BI refers to the result of using bilingual features in Chen et al. [sent-800, score-0.597]
</p><p>98 (2010), our approach used an auto-generated bilingual treebank while theirs used a human-annotated bilingual treebank. [sent-809, score-1.113]
</p><p>99 Although we trained our parser on an auto-  generated bilingual treebank, we achieved an accuracy comparable to the systems trained on humanannotated bilingual treebanks on the standard test data. [sent-814, score-1.201]
</p><p>100 Moreover, our approach continued to provide improvement over the baseline systems when we used a much larger monolingual treebank (over 50,000 sentences) where target human translations are not available and very hard to construct. [sent-815, score-0.29]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bilingual', 0.522), ('jiqiao', 0.26), ('fahui', 0.245), ('verified', 0.241), ('rtk', 0.231), ('subtrees', 0.161), ('peiyu', 0.159), ('monolingual', 0.156), ('quanti', 0.144), ('yundongyuan', 0.144), ('fvt', 0.13), ('liliang', 0.13), ('rbink', 0.13), ('bitext', 0.123), ('chongfeng', 0.115), ('fvb', 0.115), ('pingshi', 0.115), ('treebanks', 0.103), ('parsert', 0.101), ('qilai', 0.101), ('xiwang', 0.101), ('subtree', 0.087), ('pag', 0.087), ('stt', 0.087), ('trigram', 0.079), ('smt', 0.079), ('verify', 0.077), ('unannotated', 0.077), ('bigram', 0.077), ('chen', 0.075), ('athletes', 0.073), ('burkett', 0.07), ('treebank', 0.069), ('fbis', 0.068), ('target', 0.065), ('ctb', 0.063), ('side', 0.062), ('constraints', 0.059), ('parsing', 0.059), ('dependency', 0.058), ('diri', 0.058), ('rsn', 0.058), ('skil', 0.058), ('skill', 0.058), ('source', 0.05), ('huang', 0.05), ('bllip', 0.05), ('constraint', 0.046), ('wenliang', 0.045), ('cultivate', 0.043), ('righti', 0.043), ('bitexts', 0.042), ('ful', 0.042), ('refers', 0.041), ('vv', 0.039), ('skills', 0.037), ('alignment', 0.037), ('strength', 0.037), ('kazama', 0.035), ('klein', 0.035), ('features', 0.034), ('secondorder', 0.034), ('ta', 0.034), ('hope', 0.033), ('translate', 0.032), ('chinese', 0.032), ('country', 0.031), ('translation', 0.031), ('indicated', 0.03), ('mcdonald', 0.03), ('kentaro', 0.03), ('help', 0.03), ('uas', 0.029), ('latest', 0.029), ('gtran', 0.029), ('humanannotated', 0.029), ('pago', 0.029), ('qilaide', 0.029), ('taxiwang', 0.029), ('yujie', 0.029), ('play', 0.028), ('carreras', 0.028), ('fro', 0.028), ('links', 0.028), ('parsers', 0.028), ('parsed', 0.027), ('template', 0.026), ('ichi', 0.026), ('aligner', 0.026), ('singapore', 0.026), ('ptb', 0.025), ('parser', 0.025), ('yoshimasa', 0.025), ('kruengkrai', 0.025), ('yiou', 0.025), ('twhee', 0.025), ('span', 0.025), ('denero', 0.024), ('checks', 0.024), ('sentences', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="118-tfidf-1" href="./emnlp-2011-SMT_Helps_Bitext_Dependency_Parsing.html">118 emnlp-2011-SMT Helps Bitext Dependency Parsing</a></p>
<p>Author: Wenliang Chen ; Jun'ichi Kazama ; Min Zhang ; Yoshimasa Tsuruoka ; Yujie Zhang ; Yiou Wang ; Kentaro Torisawa ; Haizhou Li</p><p>Abstract: We propose a method to improve the accuracy of parsing bilingual texts (bitexts) with the help of statistical machine translation (SMT) systems. Previous bitext parsing methods use human-annotated bilingual treebanks that are hard to obtain. Instead, our approach uses an auto-generated bilingual treebank to produce bilingual constraints. However, because the auto-generated bilingual treebank contains errors, the bilingual constraints are noisy. To overcome this problem, we use large-scale unannotated data to verify the constraints and design a set of effective bilingual features for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT.</p><p>2 0.1882188 <a title="118-tfidf-2" href="./emnlp-2011-Cache-based_Document-level_Statistical_Machine_Translation.html">25 emnlp-2011-Cache-based Document-level Statistical Machine Translation</a></p>
<p>Author: Zhengxian Gong ; Min Zhang ; Guodong Zhou</p><p>Abstract: Statistical machine translation systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time, ignoring document-level information. In this paper, we propose a cache-based approach to document-level translation. Since caches mainly depend on relevant data to supervise subsequent decisions, it is critical to fill the caches with highly-relevant data of a reasonable size. In this paper, we present three kinds of caches to store relevant document-level information: 1) a dynamic cache, which stores bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document; 2) a static cache, which stores relevant bilingual phrase pairs extracted from similar bilingual document pairs (i.e. source documents similar to the test document and their corresponding target documents) in the training parallel corpus; 3) a topic cache, which stores the target-side topic words related with the test document in the source-side. In particular, three new features are designed to explore various kinds of document-level information in above three kinds of caches. Evaluation shows the effectiveness of our cache-based approach to document-level translation with the performance improvement of 0.8 1 in BLUE score over Moses. Especially, detailed analysis and discussion are presented to give new insights to document-level translation. 1</p><p>3 0.12130979 <a title="118-tfidf-3" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<p>Author: Amittai Axelrod ; Xiaodong He ; Jianfeng Gao</p><p>Abstract: Xiaodong He Microsoft Research Redmond, WA 98052 xiaohe @mi cro s o ft . com Jianfeng Gao Microsoft Research Redmond, WA 98052 j fgao @mi cro s o ft . com have its own argot, vocabulary or stylistic preferences, such that the corpus characteristics will necWe explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain. These sentences may be selected with simple cross-entropy based methods, of which we present three. As these sentences are not themselves identical to the in-domain data, we call them pseudo in-domain subcorpora. These subcorpora 1% the size of the original can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus. Performance is further improved when we use these domain-adapted models in combination with a true in-domain model. The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining in- and general-domain systems during decoding. – –</p><p>4 0.11763655 <a title="118-tfidf-4" href="./emnlp-2011-Relaxed_Cross-lingual_Projection_of_Constituent_Syntax.html">115 emnlp-2011-Relaxed Cross-lingual Projection of Constituent Syntax</a></p>
<p>Author: Wenbin Jiang ; Qun Liu ; Yajuan Lv</p><p>Abstract: We propose a relaxed correspondence assumption for cross-lingual projection of constituent syntax, which allows a supposed constituent of the target sentence to correspond to an unrestricted treelet in the source parse. Such a relaxed assumption fundamentally tolerates the syntactic non-isomorphism between languages, and enables us to learn the target-language-specific syntactic idiosyncrasy rather than a strained grammar directly projected from the source language syntax. Based on this assumption, a novel constituency projection method is also proposed in order to induce a projected constituent treebank from the source-parsed bilingual corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers.</p><p>5 0.098109812 <a title="118-tfidf-5" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Noah A. Smith</p><p>Abstract: We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.</p><p>6 0.096924864 <a title="118-tfidf-6" href="./emnlp-2011-Improving_Bilingual_Projections_via_Sparse_Covariance_Matrices.html">73 emnlp-2011-Improving Bilingual Projections via Sparse Covariance Matrices</a></p>
<p>7 0.096312664 <a title="118-tfidf-7" href="./emnlp-2011-Multi-Source_Transfer_of_Delexicalized_Dependency_Parsers.html">95 emnlp-2011-Multi-Source Transfer of Delexicalized Dependency Parsers</a></p>
<p>8 0.094618775 <a title="118-tfidf-8" href="./emnlp-2011-Analyzing_Methods_for_Improving_Precision_of_Pivot_Based_Bilingual_Dictionaries.html">18 emnlp-2011-Analyzing Methods for Improving Precision of Pivot Based Bilingual Dictionaries</a></p>
<p>9 0.089800544 <a title="118-tfidf-9" href="./emnlp-2011-Joint_Models_for_Chinese_POS_Tagging_and_Dependency_Parsing.html">75 emnlp-2011-Joint Models for Chinese POS Tagging and Dependency Parsing</a></p>
<p>10 0.088766165 <a title="118-tfidf-10" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>11 0.08802513 <a title="118-tfidf-11" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>12 0.083310291 <a title="118-tfidf-12" href="./emnlp-2011-Minimum_Imputed-Risk%3A_Unsupervised_Discriminative_Training_for_Machine_Translation.html">93 emnlp-2011-Minimum Imputed-Risk: Unsupervised Discriminative Training for Machine Translation</a></p>
<p>13 0.079108156 <a title="118-tfidf-13" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>14 0.07009238 <a title="118-tfidf-14" href="./emnlp-2011-A_Fast%2C_Accurate%2C_Non-Projective%2C_Semantically-Enriched_Parser.html">4 emnlp-2011-A Fast, Accurate, Non-Projective, Semantically-Enriched Parser</a></p>
<p>15 0.069272399 <a title="118-tfidf-15" href="./emnlp-2011-A_Correction_Model_for_Word_Alignments.html">3 emnlp-2011-A Correction Model for Word Alignments</a></p>
<p>16 0.068327792 <a title="118-tfidf-16" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>17 0.064805657 <a title="118-tfidf-17" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>18 0.062385779 <a title="118-tfidf-18" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>19 0.061035004 <a title="118-tfidf-19" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>20 0.060360979 <a title="118-tfidf-20" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.197), (1, 0.144), (2, 0.048), (3, -0.025), (4, -0.035), (5, 0.035), (6, -0.01), (7, 0.044), (8, -0.132), (9, 0.003), (10, -0.004), (11, 0.039), (12, 0.139), (13, 0.142), (14, 0.112), (15, 0.062), (16, 0.047), (17, -0.178), (18, -0.139), (19, -0.096), (20, -0.065), (21, -0.215), (22, -0.177), (23, 0.046), (24, 0.046), (25, -0.242), (26, 0.076), (27, 0.005), (28, 0.025), (29, 0.229), (30, 0.002), (31, 0.014), (32, -0.108), (33, -0.014), (34, 0.165), (35, 0.009), (36, -0.013), (37, -0.059), (38, -0.075), (39, 0.007), (40, 0.064), (41, -0.023), (42, -0.017), (43, -0.004), (44, -0.037), (45, 0.067), (46, 0.017), (47, -0.027), (48, 0.011), (49, 0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95196038 <a title="118-lsi-1" href="./emnlp-2011-SMT_Helps_Bitext_Dependency_Parsing.html">118 emnlp-2011-SMT Helps Bitext Dependency Parsing</a></p>
<p>Author: Wenliang Chen ; Jun'ichi Kazama ; Min Zhang ; Yoshimasa Tsuruoka ; Yujie Zhang ; Yiou Wang ; Kentaro Torisawa ; Haizhou Li</p><p>Abstract: We propose a method to improve the accuracy of parsing bilingual texts (bitexts) with the help of statistical machine translation (SMT) systems. Previous bitext parsing methods use human-annotated bilingual treebanks that are hard to obtain. Instead, our approach uses an auto-generated bilingual treebank to produce bilingual constraints. However, because the auto-generated bilingual treebank contains errors, the bilingual constraints are noisy. To overcome this problem, we use large-scale unannotated data to verify the constraints and design a set of effective bilingual features for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT.</p><p>2 0.79557669 <a title="118-lsi-2" href="./emnlp-2011-Cache-based_Document-level_Statistical_Machine_Translation.html">25 emnlp-2011-Cache-based Document-level Statistical Machine Translation</a></p>
<p>Author: Zhengxian Gong ; Min Zhang ; Guodong Zhou</p><p>Abstract: Statistical machine translation systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time, ignoring document-level information. In this paper, we propose a cache-based approach to document-level translation. Since caches mainly depend on relevant data to supervise subsequent decisions, it is critical to fill the caches with highly-relevant data of a reasonable size. In this paper, we present three kinds of caches to store relevant document-level information: 1) a dynamic cache, which stores bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document; 2) a static cache, which stores relevant bilingual phrase pairs extracted from similar bilingual document pairs (i.e. source documents similar to the test document and their corresponding target documents) in the training parallel corpus; 3) a topic cache, which stores the target-side topic words related with the test document in the source-side. In particular, three new features are designed to explore various kinds of document-level information in above three kinds of caches. Evaluation shows the effectiveness of our cache-based approach to document-level translation with the performance improvement of 0.8 1 in BLUE score over Moses. Especially, detailed analysis and discussion are presented to give new insights to document-level translation. 1</p><p>3 0.6063605 <a title="118-lsi-3" href="./emnlp-2011-Analyzing_Methods_for_Improving_Precision_of_Pivot_Based_Bilingual_Dictionaries.html">18 emnlp-2011-Analyzing Methods for Improving Precision of Pivot Based Bilingual Dictionaries</a></p>
<p>Author: Xabier Saralegi ; Iker Manterola ; Inaki San Vicente</p><p>Abstract: An A-C bilingual dictionary can be inferred by merging A-B and B-C dictionaries using B as pivot. However, polysemous pivot words often produce wrong translation candidates. This paper analyzes two methods for pruning wrong candidates: one based on exploiting the structure of the source dictionaries, and the other based on distributional similarity computed from comparable corpora. As both methods depend exclusively on easily available resources, they are well suited to less resourced languages. We studied whether these two techniques complement each other given that they are based on different paradigms. We also researched combining them by looking for the best adequacy depending on various application scenarios. ,</p><p>4 0.59188008 <a title="118-lsi-4" href="./emnlp-2011-Improving_Bilingual_Projections_via_Sparse_Covariance_Matrices.html">73 emnlp-2011-Improving Bilingual Projections via Sparse Covariance Matrices</a></p>
<p>Author: Jagadeesh Jagarlamudi ; Raghavendra Udupa ; Hal Daume III ; Abhijit Bhole</p><p>Abstract: Mapping documents into an interlingual representation can help bridge the language barrier of cross-lingual corpora. Many existing approaches are based on word co-occurrences extracted from aligned training data, represented as a covariance matrix. In theory, such a covariance matrix should represent semantic equivalence, and should be highly sparse. Unfortunately, the presence of noise leads to dense covariance matrices which in turn leads to suboptimal document representations. In this paper, we explore techniques to recover the desired sparsity in covariance matrices in two ways. First, we explore word association measures and bilingual dictionaries to weigh the word pairs. Later, we explore different selection strategies to remove the noisy pairs based on the association scores. Our experimental results on the task of aligning comparable documents shows the efficacy of sparse covariance matrices on two data sets from two different language pairs.</p><p>5 0.49272576 <a title="118-lsi-5" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<p>Author: Amittai Axelrod ; Xiaodong He ; Jianfeng Gao</p><p>Abstract: Xiaodong He Microsoft Research Redmond, WA 98052 xiaohe @mi cro s o ft . com Jianfeng Gao Microsoft Research Redmond, WA 98052 j fgao @mi cro s o ft . com have its own argot, vocabulary or stylistic preferences, such that the corpus characteristics will necWe explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain. These sentences may be selected with simple cross-entropy based methods, of which we present three. As these sentences are not themselves identical to the in-domain data, we call them pseudo in-domain subcorpora. These subcorpora 1% the size of the original can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus. Performance is further improved when we use these domain-adapted models in combination with a true in-domain model. The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining in- and general-domain systems during decoding. – –</p><p>6 0.47646919 <a title="118-lsi-6" href="./emnlp-2011-Relaxed_Cross-lingual_Projection_of_Constituent_Syntax.html">115 emnlp-2011-Relaxed Cross-lingual Projection of Constituent Syntax</a></p>
<p>7 0.40152305 <a title="118-lsi-7" href="./emnlp-2011-Minimum_Imputed-Risk%3A_Unsupervised_Discriminative_Training_for_Machine_Translation.html">93 emnlp-2011-Minimum Imputed-Risk: Unsupervised Discriminative Training for Machine Translation</a></p>
<p>8 0.36298329 <a title="118-lsi-8" href="./emnlp-2011-Multi-Source_Transfer_of_Delexicalized_Dependency_Parsers.html">95 emnlp-2011-Multi-Source Transfer of Delexicalized Dependency Parsers</a></p>
<p>9 0.34375179 <a title="118-lsi-9" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>10 0.29955336 <a title="118-lsi-10" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>11 0.29493752 <a title="118-lsi-11" href="./emnlp-2011-A_Fast%2C_Accurate%2C_Non-Projective%2C_Semantically-Enriched_Parser.html">4 emnlp-2011-A Fast, Accurate, Non-Projective, Semantically-Enriched Parser</a></p>
<p>12 0.29098135 <a title="118-lsi-12" href="./emnlp-2011-Inducing_Sentence_Structure_from_Parallel_Corpora_for_Reordering.html">74 emnlp-2011-Inducing Sentence Structure from Parallel Corpora for Reordering</a></p>
<p>13 0.27650306 <a title="118-lsi-13" href="./emnlp-2011-A_Correction_Model_for_Word_Alignments.html">3 emnlp-2011-A Correction Model for Word Alignments</a></p>
<p>14 0.26056704 <a title="118-lsi-14" href="./emnlp-2011-Data-Driven_Response_Generation_in_Social_Media.html">38 emnlp-2011-Data-Driven Response Generation in Social Media</a></p>
<p>15 0.25812203 <a title="118-lsi-15" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>16 0.25760442 <a title="118-lsi-16" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>17 0.24806891 <a title="118-lsi-17" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>18 0.24754862 <a title="118-lsi-18" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>19 0.24746679 <a title="118-lsi-19" href="./emnlp-2011-Language_Models_for_Machine_Translation%3A_Original_vs._Translated_Texts.html">76 emnlp-2011-Language Models for Machine Translation: Original vs. Translated Texts</a></p>
<p>20 0.24614774 <a title="118-lsi-20" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.106), (36, 0.017), (37, 0.019), (45, 0.05), (53, 0.013), (54, 0.018), (57, 0.025), (62, 0.017), (64, 0.06), (65, 0.422), (66, 0.015), (69, 0.014), (79, 0.04), (82, 0.021), (87, 0.021), (90, 0.015), (96, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.68868047 <a title="118-lda-1" href="./emnlp-2011-SMT_Helps_Bitext_Dependency_Parsing.html">118 emnlp-2011-SMT Helps Bitext Dependency Parsing</a></p>
<p>Author: Wenliang Chen ; Jun'ichi Kazama ; Min Zhang ; Yoshimasa Tsuruoka ; Yujie Zhang ; Yiou Wang ; Kentaro Torisawa ; Haizhou Li</p><p>Abstract: We propose a method to improve the accuracy of parsing bilingual texts (bitexts) with the help of statistical machine translation (SMT) systems. Previous bitext parsing methods use human-annotated bilingual treebanks that are hard to obtain. Instead, our approach uses an auto-generated bilingual treebank to produce bilingual constraints. However, because the auto-generated bilingual treebank contains errors, the bilingual constraints are noisy. To overcome this problem, we use large-scale unannotated data to verify the constraints and design a set of effective bilingual features for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT.</p><p>2 0.6669246 <a title="118-lda-2" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>Author: Jun Xie ; Haitao Mi ; Qun Liu</p><p>Abstract: Dependency structure, as a first step towards semantics, is believed to be helpful to improve translation quality. However, previous works on dependency structure based models typically resort to insertion operations to complete translations, which make it difficult to specify ordering information in translation rules. In our model of this paper, we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents relations and the target side as strings. The head-dependents rules require only substitution operation, thus our model requires no heuristics or separate ordering models of the previous works to control the word order of translations. Large-scale experiments show that our model performs well on long distance reordering, and outperforms the state- of-the-art constituency-to-string model (+1.47 BLEU on average) and hierarchical phrasebased model (+0.46 BLEU on average) on two Chinese-English NIST test sets without resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models.</p><p>3 0.34275243 <a title="118-lda-3" href="./emnlp-2011-Cache-based_Document-level_Statistical_Machine_Translation.html">25 emnlp-2011-Cache-based Document-level Statistical Machine Translation</a></p>
<p>Author: Zhengxian Gong ; Min Zhang ; Guodong Zhou</p><p>Abstract: Statistical machine translation systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time, ignoring document-level information. In this paper, we propose a cache-based approach to document-level translation. Since caches mainly depend on relevant data to supervise subsequent decisions, it is critical to fill the caches with highly-relevant data of a reasonable size. In this paper, we present three kinds of caches to store relevant document-level information: 1) a dynamic cache, which stores bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document; 2) a static cache, which stores relevant bilingual phrase pairs extracted from similar bilingual document pairs (i.e. source documents similar to the test document and their corresponding target documents) in the training parallel corpus; 3) a topic cache, which stores the target-side topic words related with the test document in the source-side. In particular, three new features are designed to explore various kinds of document-level information in above three kinds of caches. Evaluation shows the effectiveness of our cache-based approach to document-level translation with the performance improvement of 0.8 1 in BLUE score over Moses. Especially, detailed analysis and discussion are presented to give new insights to document-level translation. 1</p><p>4 0.34179613 <a title="118-lda-4" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>Author: Jiajun Zhang ; Feifei Zhai ; Chengqing Zong</p><p>Abstract: Due to its explicit modeling of the grammaticality of the output via target-side syntax, the string-to-tree model has been shown to be one of the most successful syntax-based translation models. However, a major limitation of this model is that it does not utilize any useful syntactic information on the source side. In this paper, we analyze the difficulties of incorporating source syntax in a string-totree model. We then propose a new way to use the source syntax in a fuzzy manner, both in source syntactic annotation and in rule matching. We further explore three algorithms in rule matching: 0-1 matching, likelihood matching, and deep similarity matching. Our method not only guarantees grammatical output with an explicit target tree, but also enables the system to choose the proper translation rules via fuzzy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1</p><p>5 0.34003454 <a title="118-lda-5" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>Author: Yang Gao ; Philipp Koehn ; Alexandra Birch</p><p>Abstract: Long-distance reordering remains one of the biggest challenges facing machine translation. We derive soft constraints from the source dependency parsing to directly address the reordering problem for the hierarchical phrasebased model. Our approach significantly improves Chinese–English machine translation on a large-scale task by 0.84 BLEU points on average. Moreover, when we switch the tuning function from BLEU to the LRscore which promotes reordering, we observe total improvements of 1.21 BLEU, 1.30 LRscore and 3.36 TER over the baseline. On average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially effective for long-distance reodering.</p><p>6 0.32343754 <a title="118-lda-6" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<p>7 0.31949005 <a title="118-lda-7" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>8 0.31717765 <a title="118-lda-8" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>9 0.31537208 <a title="118-lda-9" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>10 0.31305364 <a title="118-lda-10" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>11 0.31188425 <a title="118-lda-11" href="./emnlp-2011-Unsupervised_Structure_Prediction_with_Non-Parallel_Multilingual_Guidance.html">146 emnlp-2011-Unsupervised Structure Prediction with Non-Parallel Multilingual Guidance</a></p>
<p>12 0.31107366 <a title="118-lda-12" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>13 0.30920893 <a title="118-lda-13" href="./emnlp-2011-Lateen_EM%3A_Unsupervised_Training_with_Multiple_Objectives%2C_Applied_to_Dependency_Grammar_Induction.html">79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</a></p>
<p>14 0.3088735 <a title="118-lda-14" href="./emnlp-2011-Multi-Source_Transfer_of_Delexicalized_Dependency_Parsers.html">95 emnlp-2011-Multi-Source Transfer of Delexicalized Dependency Parsers</a></p>
<p>15 0.30877227 <a title="118-lda-15" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>16 0.30796254 <a title="118-lda-16" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>17 0.30692109 <a title="118-lda-17" href="./emnlp-2011-Named_Entity_Recognition_in_Tweets%3A_An_Experimental_Study.html">98 emnlp-2011-Named Entity Recognition in Tweets: An Experimental Study</a></p>
<p>18 0.30571532 <a title="118-lda-18" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>19 0.30541641 <a title="118-lda-19" href="./emnlp-2011-Joint_Models_for_Chinese_POS_Tagging_and_Dependency_Parsing.html">75 emnlp-2011-Joint Models for Chinese POS Tagging and Dependency Parsing</a></p>
<p>20 0.30531096 <a title="118-lda-20" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
