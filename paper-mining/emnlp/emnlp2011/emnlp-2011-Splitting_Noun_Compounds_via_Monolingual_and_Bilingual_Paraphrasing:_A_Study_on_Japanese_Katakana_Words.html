<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>124 emnlp-2011-Splitting Noun Compounds via Monolingual and Bilingual Paraphrasing: A Study on Japanese Katakana Words</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-124" href="#">emnlp2011-124</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>124 emnlp-2011-Splitting Noun Compounds via Monolingual and Bilingual Paraphrasing: A Study on Japanese Katakana Words</h1>
<br/><p>Source: <a title="emnlp-2011-124-pdf" href="http://aclweb.org/anthology//D/D11/D11-1089.pdf">pdf</a></p><p>Author: Nobuhiro Kaji ; Masaru Kitsuregawa</p><p>Abstract: Word boundaries within noun compounds are not marked by white spaces in a number of languages, unlike in English, and it is beneficial for various NLP applications to split such noun compounds. In the case of Japanese, noun compounds made up of katakana words (i.e., transliterated foreign words) are particularly difficult to split, because katakana words are highly productive and are often outof-vocabulary. To overcome this difficulty, we propose using monolingual and bilingual paraphrases of katakana noun compounds for identifying word boundaries. Experiments demonstrated that splitting accuracy is substantially improved by extracting such paraphrases from unlabeled textual data, the Web in our case, and then using that information for constructing splitting models.</p><p>Reference: <a title="emnlp-2011-124-reference" href="../emnlp2011_reference/emnlp-2011-Splitting_Noun_Compounds_via_Monolingual_and_Bilingual_Paraphrasing%3A_A_Study_on_Japanese_Katakana_Words_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 j p i Abstract Word boundaries within noun compounds are not marked by white spaces in a number of languages, unlike in English, and it is beneficial for various NLP applications to split such noun compounds. [sent-5, score-0.616]
</p><p>2 In the case of Japanese, noun compounds made up of katakana words (i. [sent-6, score-1.121]
</p><p>3 , transliterated foreign words) are particularly difficult to split, because katakana words are highly productive and are often outof-vocabulary. [sent-8, score-0.812]
</p><p>4 To overcome this difficulty, we propose using monolingual and bilingual paraphrases of katakana noun compounds for identifying word boundaries. [sent-9, score-1.357]
</p><p>5 Experiments demonstrated that splitting accuracy is substantially improved by extracting such paraphrases from unlabeled textual data, the Web  in our case, and then using that information for constructing splitting models. [sent-10, score-0.701]
</p><p>6 1 Japanese katakana words and noun compound splitting Borrowing is a major type of word formation in Japanese, and numerous foreign words (proper names or neologisms etc. [sent-12, score-1.333]
</p><p>7 Most borrowed words in modern Japanese are transliterations1 from English and they are referred to as katakana words because transliterated foreign words are primarily spelled by using katakana characters in the Japanese writing system. [sent-14, score-1.547]
</p><p>8 2 Compound1Some researchers use the term transcription rather than transliteration (Breen, 2009). [sent-15, score-0.218]
</p><p>9 Our terminology is based on studies on machine transliteration (Knight and Graehl, 1998). [sent-16, score-0.238]
</p><p>10 In particular, noun compounds are frequently produced by merging two or more nouns together. [sent-23, score-0.39]
</p><p>11 These two types of word formation yield a significant amount of katakana noun compounds, making Japanese a highly productive language. [sent-24, score-0.922]
</p><p>12 , German, Dutch and Korean), constituent words of compounds are not separated by white spaces, unlike in English. [sent-27, score-0.393]
</p><p>13 For example, compound splitting enables SMT systems to translate a compound on a word-by-word basis, even if the compound itself is not found in the translation table (Koehn and Knight, 2003; Dyer, 2009). [sent-29, score-0.705]
</p><p>14 In abbreviation recognition, the definition of an abbreviation is often in the form of a noun compound, and most abbreviation recognition algorithms assume that the definition is properly  segmented; see e. [sent-31, score-0.255]
</p><p>15 This has led NLP researchers to explore methods for splitting compounds, especially noun compounds, in various languages (Koehn and Knight, 2003; Nakazawa et al. [sent-35, score-0.387]
</p><p>16 The idea behind those methods is that compounds are basically split into constituent words when they are translated into English, where the compounded words are separated by white spaces, and hence splitting rules can be learned by discovering word alignments in bilingual resources. [sent-45, score-0.771]
</p><p>17 The largest obstacle that makes compound splitting difficult is the existence of out-of-vocabulary  words, which are not found in the abovementioned linguistic resources. [sent-46, score-0.411]
</p><p>18 In the Japanese case, it is known that katakana words constitute a large source ofout-of-vocabulary words (Brill et al. [sent-47, score-0.731]
</p><p>19 As we have discussed, katakana words are very productive, and thus we can no longer expect existent linguistic resources to have sufficient coverage. [sent-50, score-0.753]
</p><p>20 According to (Breen, 2009), as many as 20% of katakana words in news articles, which we think include less out-ofvocabulary words than Web and other noisy textual data, are out-of-vocabulary. [sent-51, score-0.757]
</p><p>21 Those katakana words often form noun compounds, and pose a substantial difficulty for Japanese text processing (Nakazawa et al. [sent-52, score-0.854]
</p><p>22 2 Paraphrases as implicit word boundaries To alleviate the errors caused by out-of-vocabulary words, we explored the use of unlabeled textual data for splitting katakana noun compounds. [sent-55, score-1.22]
</p><p>23 From a broad perspective, our approach can be seen as using paraphrases of noun compounds. [sent-59, score-0.27]
</p><p>24 As we will see in section 4 and 5, katakana noun compounds can be paraphrased into various forms that strongly indicate word boundaries within the original noun compound. [sent-60, score-1.353]
</p><p>25 This paper empirically demonstrates that splitting accuracy can be significantly improved by extracting such paraphrases from unlabeled text, the Web in our case, and then using that information for constructing splitting models. [sent-61, score-0.675]
</p><p>26 Section 4 explores monolin960 gual paraphrases that can be generated by inserting certain linguistic markers between constituent words of katakana noun compounds. [sent-63, score-1.049]
</p><p>27 Section 5, in turn, explores bilingual paraphrases (specifically, backtransliteration). [sent-64, score-0.193]
</p><p>28 Since katakana words are basically transliterations from English, back-transliterating katakana noun compounds is also useful for splitting. [sent-65, score-1.942]
</p><p>29 To avoid terminological confusion, mono-  lingual paraphrases are simply referred to as paraphrases and bilingual paraphrases are referred to as back-transliterations hereafter. [sent-66, score-0.547]
</p><p>30 We also confirmed that our method outperforms the previously proposed splitting methods by a wide margin. [sent-69, score-0.264]
</p><p>31 All these results strongly suggest the effectiveness of paraphrasing and back-transliteration for identifying word boundaries within katakana noun compounds. [sent-70, score-0.991]
</p><p>32 1 Compound splitting A common approach to splitting compounds without expensive linguistic resources is an unsupervised method based on word or string frequencies estimated from unlabeled text (Koehn and Knight, 2003; Ando and Lee, 2003; Schiller, 2005; Nakazawa et al. [sent-72, score-0.836]
</p><p>33 (2005) also investigated ways of splitting katakana noun compounds. [sent-75, score-1.118]
</p><p>34 Our approach can be seen as augmenting discriminative models of compound splitting with large external linguistic resources, i. [sent-79, score-0.411]
</p><p>35 Holz and Biemann (2008) proposed a method  for splitting and paraphrasing German compounds. [sent-91, score-0.325]
</p><p>36 Although we can use existing word segmentation systems for splitting katakana noun compounds, it is difficult to reach the desired accuracy, as we will empirically demonstrate in section 6. [sent-95, score-1.234]
</p><p>37 One reason for this is that katakana noun compounds often include out-of-vocabulary words, which are difficult for the existing segmentation systems to deal with. [sent-96, score-1.217]
</p><p>38 More importantly, we are unaware of any attempts to use paraphrases or transliterations for word segmentation in the same way as we do. [sent-100, score-0.329]
</p><p>39 Although these studies successfully demonstrated the usefulness of paraphrases for improving parsers, the connection between paraphrases and word segmentation (or noun compound splitting) was not at all discussed. [sent-102, score-0.7]
</p><p>40 Our method of using back-transliterations for splitting katakana noun compounds (section 5) is closely related to methods for mining transliteration from the Web text (Brill et al. [sent-103, score-1.603]
</p><p>41 What most differentiates these studies from our work is that their primary goal is to build a machine transliteration system or to build a bilingual dictionary itself; none of them explored splitting compounds. [sent-107, score-0.548]
</p><p>42 We formalize our task as a structure prediction problem that, given a katakana noun compound x, predicts the most probable splitting y∗. [sent-118, score-1.265]
</p><p>43 y∗= ayr∈gYm(xa)xw · φ(y), where Y (x) represents the set of all splitting options of x, φ(y) is a feature vector representation of y, and w is a weight vector to be estimated from labeled data. [sent-119, score-0.284]
</p><p>44 In addition to those basic features, we also employ paraphrases and back-transliterations of katakana noun compounds as features. [sent-128, score-1.296]
</p><p>45 TypeRuleExample  4  Paraphrasing  In this section,  we argue that paraphrases  of  katakana noun compounds provides useful information on word boundaries. [sent-136, score-1.288]
</p><p>46 1 Paraphrasing noun compounds A katakana noun compound can be paraphrased into various forms, some of which provide information on the word boundaries within the original compound. [sent-139, score-1.5]
</p><p>47 (1a) is in the form of a noun compound, within which the word boundary is ambiguous. [sent-144, score-0.181]
</p><p>48 In the Japanese writing system, the centered dot is sometimes, but not always, used to separate long katakana compounds for the  sake of readability. [sent-146, score-0.998]
</p><p>49 (1c) is the noun phrase generated from (1a) by inserting the possessive marker ‘の’, which can be translated as with in this context, between the constituent words. [sent-147, score-0.204]
</p><p>50 If we observe paraphrases of (1a) such as (1b) and (1c), we can guess that a word boundary exists between ‘ア チ ョ ビ (anchovy)’ and ‘パ パス タ (pasta)’. [sent-148, score-0.205]
</p><p>51 2 Paraphrase rules The above discussion led us to use paraphrase frequencies estimated from Web text for splitting katakana noun compounds. [sent-150, score-1.25]
</p><p>52 The left-hand term corresponds to a compound to be paraphrased and the right-hand term represents its paraphrase. [sent-155, score-0.18]
</p><p>53 As preprocessing, we use regular expressions to count the frequencies of all potential paraphrases of katakana noun compounds on the Web in advance. [sent-161, score-1.289]
</p><p>54 Given a candidate segmentation y at test time, we generate paraphrases of the noun compound by setting X1 = yi−1 and X2 = yi, and applying the paraphrase rules. [sent-166, score-0.6]
</p><p>55 5  Back-transliteration  Most katakana words are transliterations from English, where words are separated by white spaces. [sent-171, score-0.875]
</p><p>56 It is, therefore, reasonable to think that backtransliterating katakana noun compounds into English would provide information on word boundaries, in a similar way to paraphrasing. [sent-172, score-1.141]
</p><p>57 This section presents a method for extracting back-transliterations of katakana words from monolingual Web text, and establishing word alignments between those katakana and English words (Table 3). [sent-173, score-1.533]
</p><p>58 In what follows, the pair of katakana words and its English back-transliteration is referred to as a transliteration pair. [sent-174, score-0.979]
</p><p>59 If the transliteration pair is annotated with word alignment information as in Table 3, it is referred to as a word-aligned transliteration pair. [sent-175, score-0.524]
</p><p>60 Using word-aligned transliteration pairs extracted  from the Web text, we derive a binary feature indicating whether katakana word yi corresponds to a single English word. [sent-176, score-1.026]
</p><p>61 Additionally, we derive another feature indicating whether a katakana word 2gram yi−1yi corresponds to an English word 2-gram. [sent-177, score-0.791]
</p><p>62 We extract word-aligned transliteration pairs from 963 Table 3: Word-aligned transliteration pairs. [sent-190, score-0.436]
</p><p>63 ’ in the example, has ptoa r been identified, a dnedn (c) pre-parenthesis text, lew,h hicahs is a katakana noun compound in our case, has to be segmented into words. [sent-194, score-1.034]
</p><p>64 This is, however, not appropriate for our purpose, because pre-parenthesis text is a katakana noun compound, which is hard for existing systems to handle, and hence the alignment quality is inevitably affected by segmentation errors. [sent-199, score-0.988]
</p><p>65 Since transliterated katakana words preserve the pronunciation of the original English words to some extent (Knight and Graehl, 1998), we can discover the correspondences between substrings of the two languages based on phonetic similarity: (3) a. [sent-202, score-0.89]
</p><p>66 We can recognize that the katakana string ‘ジ ャ ン ’, which is the concatenation of the first two substrings in (3a), forms a single word because it corresponds to the English word junk, and so on. [sent-209, score-0.801]
</p><p>67 3 shows how to extract word-aligned transliteration pairs by using the probabilistic model. [sent-219, score-0.218]
</p><p>68 2 Phonetic similarity model To establish the substring alignment between katakana and Latin alphabet strings, we use the  probabilistic model proposed by (Jiampojamarn et al. [sent-221, score-0.836]
</p><p>69 Let f and e be katakana and alphabet strings, and A be the substring alignment between them. [sent-223, score-0.836]
</p><p>70 ン  964 The model parameters are estimated from a set of transliteration pairs (f, e) using the EM algorithm. [sent-236, score-0.218]
</p><p>71 Given a new transliteration pair (f, e), we can determine the substring alignment as  A∗= argAmaxlogp(f,e,A). [sent-244, score-0.323]
</p><p>72 In finding the substring alignment, a white space on the English side is used as a constraint, so that the English substring ei does not span a white space. [sent-245, score-0.228]
</p><p>73 3  Extracting word-aligned transliteration pairs The word-aligned transliteration pairs are extracted using the phonetic similarity model, as follows. [sent-247, score-0.486]
</p><p>74 First, candidate transliteration pairs (f, e) are extracted from the parenthetical expressions. [sent-248, score-0.278]
</p><p>75 This results in a list of wordaligned transliteration pairs (Table 3). [sent-256, score-0.24]
</p><p>76 1 Experimental setting To train the phonetic similarity model, we used a set of transliteration pairs extracted from the Wikipedia. [sent-259, score-0.268]
</p><p>77 5 We randomly extracted 5286 entries written in katakana from EDICT and manually annotated word boundaries by establishing word correspondences to their English transliterations. [sent-265, score-0.879]
</p><p>78 6 It is  the largest dictionary used for Japanese word segmentation, and it includes 19,885 katakana words. [sent-270, score-0.751]
</p><p>79 From the corpora, we extracted 14,966,205 (potential) paraphrases of katakana noun compounds together with their frequencies. [sent-273, score-1.268]
</p><p>80 The first frequency-based baseline, UNIGRAM, performs compound splitting based on a word 1-gram language model (Schiller, 2005; Alfonseca et al. [sent-289, score-0.431]
</p><p>81 The second frequency-based baseline, GMF, outputs the splitting option with the highest geometric mean frequency of the constituent words (Koehn and Knight, 2003):  y∗= ayr∈gmY(xa)xGMF(y) = ayrg∈mY(ax)x? [sent-292, score-0.312]
</p><p>82 Regarding the two state-of-the-art word segmentation systems, one is JUMAN,7 a rule-based word segmentation system (Kurohashi and Nagao, 1994), and the other is MECAB,8 a supervised word segmentation system based on CRFs (Kudo et al. [sent-315, score-0.348]
</p><p>83 Although the literature states that it is hard for existing systems to deal with katakana noun compounds (Nakazawa et al. [sent-318, score-1.121]
</p><p>84 The performance of the two word segmentation baselines (JUMAN and MECAB) is significantly worse in our task than in the standard word segmentation task, where nearly 99% precision and recall are reported (Kudo et al. [sent-337, score-0.232]
</p><p>85 This demonstrates that splitting a katakana noun compound is not at all a trivial task to resolve, even for the state-of-theart word segmentation systems. [sent-339, score-1.381]
</p><p>86 By analyzing the errors, we interestingly found that some of the erroneous splitting results are still acceptable to humans. [sent-341, score-0.264]
</p><p>87 Although the latter splitting may be useful in some applications, it is judged as wrong in our evaluation framework. [sent-343, score-0.264]
</p><p>88 This implies the importance of evaluating the splitting results in  プ  プ  some extrinsic tasks. [sent-344, score-0.264]
</p><p>89 4 Investigation on out-of-vocabulary words In our test data, 2681 out of the 5286 katakana noun compounds contained at least one out-of-vocabulary word that are not registered in NAIST-jdic. [sent-347, score-1.141]
</p><p>90 Table 6 illustrates the results of the supervised systems for those 2681 and the remaining 2605 katakana noun compounds (referred to as w/ OOV and w/o OOV data, respectively). [sent-348, score-1.121]
</p><p>91 This is consistent with our claim that outof-vocabulary words are a major source of errors in splitting noun compounds. [sent-350, score-0.387]
</p><p>92 6%) are in NAIST-jdic or wordaligned transliteration pairs extracted from the Web text. [sent-375, score-0.24]
</p><p>93 Figure 2 represents the number of distinct word-aligned transliteration pairs that were extracted from the Web corpora. [sent-397, score-0.218]
</p><p>94 We see that most of the extracted transliteration pairs have high confidence score. [sent-398, score-0.218]
</p><p>95 7  Conclusion  In this paper, we explored the idea of using monolingual and bilingual paraphrases for splitting katakana noun compounds in Japanese. [sent-399, score-1.601]
</p><p>96 The experiments demonstrated that our method significantly improves the splitting accuracy by a large margin in comparison with the previously proposed methods. [sent-400, score-0.264]
</p><p>97 This means that paraphrasing provides a simple and effective way of using unlabeled textual data for identifying implicit word boundaries within katakana noun compounds. [sent-401, score-1.017]
</p><p>98 Although our investigation was restricted to katakana noun compounds, one might expect that a similar approach would be useful for splitting other types of noun compounds (e. [sent-402, score-1.508]
</p><p>99 Parallel bilingual paraphrase rules for noun compounds: Concepts and rules for exploring Web language resources. [sent-474, score-0.304]
</p><p>100 Search engine statistics beyond the n-gram: Application to noun compound bracketing. [sent-498, score-0.27]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('katakana', 0.731), ('compounds', 0.267), ('splitting', 0.264), ('transliteration', 0.218), ('compound', 0.147), ('paraphrases', 0.147), ('nakazawa', 0.133), ('japanese', 0.13), ('noun', 0.123), ('segmentation', 0.096), ('paraphrase', 0.087), ('alfonseca', 0.084), ('substring', 0.067), ('transliterations', 0.066), ('anchovy', 0.064), ('pasta', 0.064), ('paraphrasing', 0.061), ('ap', 0.06), ('parenthetical', 0.06), ('oov', 0.058), ('boundaries', 0.056), ('transliterated', 0.055), ('junk', 0.051), ('phonetic', 0.05), ('tokyo', 0.05), ('constituent', 0.048), ('web', 0.048), ('white', 0.047), ('bilingual', 0.046), ('gmf', 0.044), ('abbreviation', 0.044), ('knight', 0.043), ('breen', 0.039), ('decompounding', 0.039), ('frequencybased', 0.039), ('holz', 0.039), ('inparenthesis', 0.039), ('alignment', 0.038), ('boundary', 0.038), ('nakov', 0.037), ('yi', 0.037), ('asian', 0.036), ('segmented', 0.033), ('jiampojamarn', 0.033), ('paraphrased', 0.033), ('marker', 0.033), ('separated', 0.031), ('english', 0.03), ('referred', 0.03), ('substrings', 0.03), ('koehn', 0.028), ('german', 0.028), ('marti', 0.028), ('establishing', 0.028), ('ayr', 0.028), ('basic', 0.028), ('brill', 0.026), ('productive', 0.026), ('degraded', 0.026), ('textual', 0.026), ('backtransliteration', 0.026), ('braschler', 0.026), ('dict', 0.026), ('edict', 0.026), ('gym', 0.026), ('junkfood', 0.026), ('kageura', 0.026), ('mecab', 0.026), ('neologisms', 0.026), ('preparenthesis', 0.026), ('schiller', 0.026), ('tkl', 0.026), ('tsujimura', 0.026), ('food', 0.025), ('okazaki', 0.025), ('cao', 0.025), ('xa', 0.025), ('perceptron', 0.025), ('rules', 0.024), ('correspondences', 0.024), ('basically', 0.024), ('kudo', 0.024), ('hearst', 0.024), ('monolingual', 0.023), ('existent', 0.022), ('bilac', 0.022), ('slaven', 0.022), ('juman', 0.022), ('kanji', 0.022), ('para', 0.022), ('wordaligned', 0.022), ('formation', 0.022), ('trivially', 0.021), ('frequencies', 0.021), ('feature', 0.02), ('studies', 0.02), ('word', 0.02), ('query', 0.02), ('biemann', 0.02), ('ando', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="124-tfidf-1" href="./emnlp-2011-Splitting_Noun_Compounds_via_Monolingual_and_Bilingual_Paraphrasing%3A_A_Study_on_Japanese_Katakana_Words.html">124 emnlp-2011-Splitting Noun Compounds via Monolingual and Bilingual Paraphrasing: A Study on Japanese Katakana Words</a></p>
<p>Author: Nobuhiro Kaji ; Masaru Kitsuregawa</p><p>Abstract: Word boundaries within noun compounds are not marked by white spaces in a number of languages, unlike in English, and it is beneficial for various NLP applications to split such noun compounds. In the case of Japanese, noun compounds made up of katakana words (i.e., transliterated foreign words) are particularly difficult to split, because katakana words are highly productive and are often outof-vocabulary. To overcome this difficulty, we propose using monolingual and bilingual paraphrases of katakana noun compounds for identifying word boundaries. Experiments demonstrated that splitting accuracy is substantially improved by extracting such paraphrases from unlabeled textual data, the Web in our case, and then using that information for constructing splitting models.</p><p>2 0.18006976 <a title="124-tfidf-2" href="./emnlp-2011-Non-parametric_Bayesian_Segmentation_of_Japanese_Noun_Phrases.html">99 emnlp-2011-Non-parametric Bayesian Segmentation of Japanese Noun Phrases</a></p>
<p>Author: Yugo Murawaki ; Sadao Kurohashi</p><p>Abstract: A key factor of high quality word segmentation for Japanese is a high-coverage dictionary, but it is costly to manually build such a lexical resource. Although external lexical resources for human readers are potentially good knowledge sources, they have not been utilized due to differences in segmentation criteria. To supplement a morphological dictionary with these resources, we propose a new task of Japanese noun phrase segmentation. We apply non-parametric Bayesian language models to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text. For inference, we propose a novel block sampling procedure named hybrid type-based sampling, which has the ability to directly escape a local optimum that is not too distant from the global optimum. Experiments show that the proposed method efficiently corrects the initial segmentation given by a morphological ana- lyzer.</p><p>3 0.16395913 <a title="124-tfidf-3" href="./emnlp-2011-Improved_Transliteration_Mining_Using_Graph_Reinforcement.html">72 emnlp-2011-Improved Transliteration Mining Using Graph Reinforcement</a></p>
<p>Author: Ali El Kahki ; Kareem Darwish ; Ahmed Saad El Din ; Mohamed Abd El-Wahab ; Ahmed Hefny ; Waleed Ammar</p><p>Abstract: Mining of transliterations from comparable or parallel text can enhance natural language processing applications such as machine translation and cross language information retrieval. This paper presents an enhanced transliteration mining technique that uses a generative graph reinforcement model to infer mappings between source and target character sequences. An initial set of mappings are learned through automatic alignment of transliteration pairs at character sequence level. Then, these mappings are modeled using a bipartite graph. A graph reinforcement algorithm is then used to enrich the graph by inferring additional mappings. During graph reinforcement, appropriate link reweighting is used to promote good mappings and to demote bad ones. The enhanced transliteration mining technique is tested in the context of mining transliterations from parallel Wikipedia titles in 4 alphabet-based languages pairs, namely English-Arabic, English-Russian, English-Hindi, and English-Tamil. The improvements in F1-measure over the baseline system were 18.7, 1.0, 4.5, and 32.5 basis points for the four language pairs respectively. The results herein outperform the best reported results in the literature by 2.6, 4.8, 0.8, and 4.1 basis points for respectively. the four language 1384 pairs</p><p>4 0.12947388 <a title="124-tfidf-4" href="./emnlp-2011-Large-Scale_Noun_Compound_Interpretation_Using_Bootstrapping_and_the_Web_as_a_Corpus.html">78 emnlp-2011-Large-Scale Noun Compound Interpretation Using Bootstrapping and the Web as a Corpus</a></p>
<p>Author: Su Nam Kim ; Preslav Nakov</p><p>Abstract: Responding to the need for semantic lexical resources in natural language processing applications, we examine methods to acquire noun compounds (NCs), e.g., orange juice, together with suitable fine-grained semantic interpretations, e.g., squeezed from, which are directly usable as paraphrases. We employ bootstrapping and web statistics, and utilize the relationship between NCs and paraphrasing patterns to jointly extract NCs and such patterns in multiple alternating iterations. In evaluation, we found that having one compound noun fixed yields both a higher number of semantically interpreted NCs and improved accuracy due to stronger semantic restrictions.</p><p>5 0.12845482 <a title="124-tfidf-5" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>Author: Juri Ganitkevitch ; Chris Callison-Burch ; Courtney Napoles ; Benjamin Van Durme</p><p>Abstract: Previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora. However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems.</p><p>6 0.12235826 <a title="124-tfidf-6" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>7 0.068157472 <a title="124-tfidf-7" href="./emnlp-2011-Enhancing_Chinese_Word_Segmentation_Using_Unlabeled_Data.html">48 emnlp-2011-Enhancing Chinese Word Segmentation Using Unlabeled Data</a></p>
<p>8 0.049156148 <a title="124-tfidf-8" href="./emnlp-2011-Relation_Acquisition_using_Word_Classes_and_Partial_Patterns.html">113 emnlp-2011-Relation Acquisition using Word Classes and Partial Patterns</a></p>
<p>9 0.046163414 <a title="124-tfidf-9" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>10 0.042292882 <a title="124-tfidf-10" href="./emnlp-2011-A_Correction_Model_for_Word_Alignments.html">3 emnlp-2011-A Correction Model for Word Alignments</a></p>
<p>11 0.042055149 <a title="124-tfidf-11" href="./emnlp-2011-SMT_Helps_Bitext_Dependency_Parsing.html">118 emnlp-2011-SMT Helps Bitext Dependency Parsing</a></p>
<p>12 0.041138288 <a title="124-tfidf-12" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>13 0.040819958 <a title="124-tfidf-13" href="./emnlp-2011-A_Fast%2C_Accurate%2C_Non-Projective%2C_Semantically-Enriched_Parser.html">4 emnlp-2011-A Fast, Accurate, Non-Projective, Semantically-Enriched Parser</a></p>
<p>14 0.038866524 <a title="124-tfidf-14" href="./emnlp-2011-Latent_Vector_Weighting_for_Word_Meaning_in_Context.html">80 emnlp-2011-Latent Vector Weighting for Word Meaning in Context</a></p>
<p>15 0.036647081 <a title="124-tfidf-15" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>16 0.03613368 <a title="124-tfidf-16" href="./emnlp-2011-Universal_Morphological_Analysis_using_Structured_Nearest_Neighbor_Prediction.html">140 emnlp-2011-Universal Morphological Analysis using Structured Nearest Neighbor Prediction</a></p>
<p>17 0.035176784 <a title="124-tfidf-17" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>18 0.035074241 <a title="124-tfidf-18" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<p>19 0.034418184 <a title="124-tfidf-19" href="./emnlp-2011-Analyzing_Methods_for_Improving_Precision_of_Pivot_Based_Bilingual_Dictionaries.html">18 emnlp-2011-Analyzing Methods for Improving Precision of Pivot Based Bilingual Dictionaries</a></p>
<p>20 0.033604167 <a title="124-tfidf-20" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.137), (1, 0.01), (2, -0.015), (3, -0.087), (4, -0.017), (5, -0.069), (6, -0.122), (7, 0.174), (8, -0.2), (9, 0.152), (10, -0.02), (11, -0.041), (12, 0.108), (13, 0.106), (14, -0.111), (15, -0.205), (16, -0.125), (17, -0.2), (18, 0.21), (19, 0.075), (20, 0.115), (21, -0.09), (22, -0.083), (23, -0.104), (24, -0.013), (25, -0.103), (26, -0.075), (27, 0.008), (28, 0.023), (29, -0.063), (30, -0.114), (31, 0.042), (32, 0.055), (33, -0.063), (34, -0.226), (35, -0.008), (36, -0.101), (37, 0.069), (38, -0.034), (39, 0.136), (40, 0.052), (41, 0.021), (42, 0.012), (43, -0.001), (44, -0.156), (45, -0.006), (46, 0.018), (47, -0.031), (48, 0.055), (49, -0.0)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.947366 <a title="124-lsi-1" href="./emnlp-2011-Splitting_Noun_Compounds_via_Monolingual_and_Bilingual_Paraphrasing%3A_A_Study_on_Japanese_Katakana_Words.html">124 emnlp-2011-Splitting Noun Compounds via Monolingual and Bilingual Paraphrasing: A Study on Japanese Katakana Words</a></p>
<p>Author: Nobuhiro Kaji ; Masaru Kitsuregawa</p><p>Abstract: Word boundaries within noun compounds are not marked by white spaces in a number of languages, unlike in English, and it is beneficial for various NLP applications to split such noun compounds. In the case of Japanese, noun compounds made up of katakana words (i.e., transliterated foreign words) are particularly difficult to split, because katakana words are highly productive and are often outof-vocabulary. To overcome this difficulty, we propose using monolingual and bilingual paraphrases of katakana noun compounds for identifying word boundaries. Experiments demonstrated that splitting accuracy is substantially improved by extracting such paraphrases from unlabeled textual data, the Web in our case, and then using that information for constructing splitting models.</p><p>2 0.5982604 <a title="124-lsi-2" href="./emnlp-2011-Improved_Transliteration_Mining_Using_Graph_Reinforcement.html">72 emnlp-2011-Improved Transliteration Mining Using Graph Reinforcement</a></p>
<p>Author: Ali El Kahki ; Kareem Darwish ; Ahmed Saad El Din ; Mohamed Abd El-Wahab ; Ahmed Hefny ; Waleed Ammar</p><p>Abstract: Mining of transliterations from comparable or parallel text can enhance natural language processing applications such as machine translation and cross language information retrieval. This paper presents an enhanced transliteration mining technique that uses a generative graph reinforcement model to infer mappings between source and target character sequences. An initial set of mappings are learned through automatic alignment of transliteration pairs at character sequence level. Then, these mappings are modeled using a bipartite graph. A graph reinforcement algorithm is then used to enrich the graph by inferring additional mappings. During graph reinforcement, appropriate link reweighting is used to promote good mappings and to demote bad ones. The enhanced transliteration mining technique is tested in the context of mining transliterations from parallel Wikipedia titles in 4 alphabet-based languages pairs, namely English-Arabic, English-Russian, English-Hindi, and English-Tamil. The improvements in F1-measure over the baseline system were 18.7, 1.0, 4.5, and 32.5 basis points for the four language pairs respectively. The results herein outperform the best reported results in the literature by 2.6, 4.8, 0.8, and 4.1 basis points for respectively. the four language 1384 pairs</p><p>3 0.5335294 <a title="124-lsi-3" href="./emnlp-2011-Non-parametric_Bayesian_Segmentation_of_Japanese_Noun_Phrases.html">99 emnlp-2011-Non-parametric Bayesian Segmentation of Japanese Noun Phrases</a></p>
<p>Author: Yugo Murawaki ; Sadao Kurohashi</p><p>Abstract: A key factor of high quality word segmentation for Japanese is a high-coverage dictionary, but it is costly to manually build such a lexical resource. Although external lexical resources for human readers are potentially good knowledge sources, they have not been utilized due to differences in segmentation criteria. To supplement a morphological dictionary with these resources, we propose a new task of Japanese noun phrase segmentation. We apply non-parametric Bayesian language models to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text. For inference, we propose a novel block sampling procedure named hybrid type-based sampling, which has the ability to directly escape a local optimum that is not too distant from the global optimum. Experiments show that the proposed method efficiently corrects the initial segmentation given by a morphological ana- lyzer.</p><p>4 0.47831321 <a title="124-lsi-4" href="./emnlp-2011-Large-Scale_Noun_Compound_Interpretation_Using_Bootstrapping_and_the_Web_as_a_Corpus.html">78 emnlp-2011-Large-Scale Noun Compound Interpretation Using Bootstrapping and the Web as a Corpus</a></p>
<p>Author: Su Nam Kim ; Preslav Nakov</p><p>Abstract: Responding to the need for semantic lexical resources in natural language processing applications, we examine methods to acquire noun compounds (NCs), e.g., orange juice, together with suitable fine-grained semantic interpretations, e.g., squeezed from, which are directly usable as paraphrases. We employ bootstrapping and web statistics, and utilize the relationship between NCs and paraphrasing patterns to jointly extract NCs and such patterns in multiple alternating iterations. In evaluation, we found that having one compound noun fixed yields both a higher number of semantically interpreted NCs and improved accuracy due to stronger semantic restrictions.</p><p>5 0.38957858 <a title="124-lsi-5" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>Author: Prodromos Malakasiotis ; Ion Androutsopoulos</p><p>Abstract: We present a method that paraphrases a given sentence by first generating candidate paraphrases and then ranking (or classifying) them. The candidates are generated by applying existing paraphrasing rules extracted from parallel corpora. The ranking component considers not only the overall quality of the rules that produced each candidate, but also the extent to which they preserve grammaticality and meaning in the particular context of the input sentence, as well as the degree to which the candidate differs from the input. We experimented with both a Maximum Entropy classifier and an SVR ranker. Experimental results show that incorporating features from an existing paraphrase recognizer in the ranking component improves performance, and that our overall method compares well against a state of the art paraphrase generator, when paraphrasing rules apply to the input sentences. We also propose a new methodology to evaluate the ranking components of generate-and-rank paraphrase generators, which evaluates them across different combinations of weights for grammaticality, meaning preservation, and diversity. The paper is accompanied by a paraphrasing dataset we constructed for evaluations of this kind.</p><p>6 0.38830519 <a title="124-lsi-6" href="./emnlp-2011-Enhancing_Chinese_Word_Segmentation_Using_Unlabeled_Data.html">48 emnlp-2011-Enhancing Chinese Word Segmentation Using Unlabeled Data</a></p>
<p>7 0.32222378 <a title="124-lsi-7" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>8 0.24180508 <a title="124-lsi-8" href="./emnlp-2011-Relation_Acquisition_using_Word_Classes_and_Partial_Patterns.html">113 emnlp-2011-Relation Acquisition using Word Classes and Partial Patterns</a></p>
<p>9 0.19640118 <a title="124-lsi-9" href="./emnlp-2011-A_Cascaded_Classification_Approach_to_Semantic_Head_Recognition.html">2 emnlp-2011-A Cascaded Classification Approach to Semantic Head Recognition</a></p>
<p>10 0.19168268 <a title="124-lsi-10" href="./emnlp-2011-Watermarking_the_Outputs_of_Structured_Prediction_with_an_application_in_Statistical_Machine_Translation..html">148 emnlp-2011-Watermarking the Outputs of Structured Prediction with an application in Statistical Machine Translation.</a></p>
<p>11 0.18183465 <a title="124-lsi-11" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<p>12 0.17293422 <a title="124-lsi-12" href="./emnlp-2011-Analyzing_Methods_for_Improving_Precision_of_Pivot_Based_Bilingual_Dictionaries.html">18 emnlp-2011-Analyzing Methods for Improving Precision of Pivot Based Bilingual Dictionaries</a></p>
<p>13 0.1622753 <a title="124-lsi-13" href="./emnlp-2011-Approximate_Scalable_Bounded_Space_Sketch_for_Large_Data_NLP.html">19 emnlp-2011-Approximate Scalable Bounded Space Sketch for Large Data NLP</a></p>
<p>14 0.15450384 <a title="124-lsi-14" href="./emnlp-2011-Improving_Bilingual_Projections_via_Sparse_Covariance_Matrices.html">73 emnlp-2011-Improving Bilingual Projections via Sparse Covariance Matrices</a></p>
<p>15 0.14986692 <a title="124-lsi-15" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>16 0.14927191 <a title="124-lsi-16" href="./emnlp-2011-Universal_Morphological_Analysis_using_Structured_Nearest_Neighbor_Prediction.html">140 emnlp-2011-Universal Morphological Analysis using Structured Nearest Neighbor Prediction</a></p>
<p>17 0.14570011 <a title="124-lsi-17" href="./emnlp-2011-SMT_Helps_Bitext_Dependency_Parsing.html">118 emnlp-2011-SMT Helps Bitext Dependency Parsing</a></p>
<p>18 0.13615508 <a title="124-lsi-18" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>19 0.13478376 <a title="124-lsi-19" href="./emnlp-2011-Extreme_Extraction_-_Machine_Reading_in_a_Week.html">57 emnlp-2011-Extreme Extraction - Machine Reading in a Week</a></p>
<p>20 0.13353853 <a title="124-lsi-20" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(15, 0.039), (23, 0.135), (36, 0.017), (37, 0.036), (45, 0.057), (53, 0.025), (54, 0.024), (57, 0.025), (62, 0.02), (64, 0.029), (66, 0.037), (69, 0.011), (72, 0.299), (79, 0.046), (82, 0.012), (85, 0.011), (96, 0.045), (98, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.72491693 <a title="124-lda-1" href="./emnlp-2011-Splitting_Noun_Compounds_via_Monolingual_and_Bilingual_Paraphrasing%3A_A_Study_on_Japanese_Katakana_Words.html">124 emnlp-2011-Splitting Noun Compounds via Monolingual and Bilingual Paraphrasing: A Study on Japanese Katakana Words</a></p>
<p>Author: Nobuhiro Kaji ; Masaru Kitsuregawa</p><p>Abstract: Word boundaries within noun compounds are not marked by white spaces in a number of languages, unlike in English, and it is beneficial for various NLP applications to split such noun compounds. In the case of Japanese, noun compounds made up of katakana words (i.e., transliterated foreign words) are particularly difficult to split, because katakana words are highly productive and are often outof-vocabulary. To overcome this difficulty, we propose using monolingual and bilingual paraphrases of katakana noun compounds for identifying word boundaries. Experiments demonstrated that splitting accuracy is substantially improved by extracting such paraphrases from unlabeled textual data, the Web in our case, and then using that information for constructing splitting models.</p><p>2 0.4998354 <a title="124-lda-2" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Noah A. Smith</p><p>Abstract: We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.</p><p>3 0.49862602 <a title="124-lda-3" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>Author: Christos Christodoulopoulos ; Sharon Goldwater ; Mark Steedman</p><p>Abstract: In this paper we present a fully unsupervised syntactic class induction system formulated as a Bayesian multinomial mixture model, where each word type is constrained to belong to a single class. By using a mixture model rather than a sequence model (e.g., HMM), we are able to easily add multiple kinds of features, including those at both the type level (morphology features) and token level (context and alignment features, the latter from parallel corpora). Using only context features, our system yields results comparable to state-of-the art, far better than a similar model without the one-class-per-type constraint. Using the additional features provides added benefit, and our final system outperforms the best published results on most of the 25 corpora tested.</p><p>4 0.49725756 <a title="124-lda-4" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>Author: Jason Katz-Brown ; Slav Petrov ; Ryan McDonald ; Franz Och ; David Talbot ; Hiroshi Ichikawa ; Masakazu Seno ; Hideto Kazawa</p><p>Abstract: We propose a simple training regime that can improve the extrinsic performance of a parser, given only a corpus of sentences and a way to automatically evaluate the extrinsic quality of a candidate parse. We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system. We use a corpus of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress.</p><p>5 0.49553201 <a title="124-lda-5" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>Author: Amit Dubey ; Frank Keller ; Patrick Sturt</p><p>Abstract: This paper introduces a psycholinguistic model of sentence processing which combines a Hidden Markov Model noun phrase chunker with a co-reference classifier. Both models are fully incremental and generative, giving probabilities of lexical elements conditional upon linguistic structure. This allows us to compute the information theoretic measure of surprisal, which is known to correlate with human processing effort. We evaluate our surprisal predictions on the Dundee corpus of eye-movement data show that our model achieve a better fit with human reading times than a syntax-only model which does not have access to co-reference information.</p><p>6 0.49400151 <a title="124-lda-6" href="./emnlp-2011-Semi-Supervised_Recursive_Autoencoders_for_Predicting_Sentiment_Distributions.html">120 emnlp-2011-Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</a></p>
<p>7 0.49170431 <a title="124-lda-7" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>8 0.49074057 <a title="124-lda-8" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>9 0.48972899 <a title="124-lda-9" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<p>10 0.48789832 <a title="124-lda-10" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>11 0.48646739 <a title="124-lda-11" href="./emnlp-2011-Non-parametric_Bayesian_Segmentation_of_Japanese_Noun_Phrases.html">99 emnlp-2011-Non-parametric Bayesian Segmentation of Japanese Noun Phrases</a></p>
<p>12 0.48631418 <a title="124-lda-12" href="./emnlp-2011-Lateen_EM%3A_Unsupervised_Training_with_Multiple_Objectives%2C_Applied_to_Dependency_Grammar_Induction.html">79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</a></p>
<p>13 0.48587388 <a title="124-lda-13" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>14 0.48544341 <a title="124-lda-14" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>15 0.48432943 <a title="124-lda-15" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>16 0.48376015 <a title="124-lda-16" href="./emnlp-2011-Named_Entity_Recognition_in_Tweets%3A_An_Experimental_Study.html">98 emnlp-2011-Named Entity Recognition in Tweets: An Experimental Study</a></p>
<p>17 0.48336366 <a title="124-lda-17" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>18 0.48073158 <a title="124-lda-18" href="./emnlp-2011-A_Word_Reordering_Model_for_Improved_Machine_Translation.html">13 emnlp-2011-A Word Reordering Model for Improved Machine Translation</a></p>
<p>19 0.48044869 <a title="124-lda-19" href="./emnlp-2011-Better_Evaluation_Metrics_Lead_to_Better_Machine_Translation.html">22 emnlp-2011-Better Evaluation Metrics Lead to Better Machine Translation</a></p>
<p>20 0.480434 <a title="124-lda-20" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
