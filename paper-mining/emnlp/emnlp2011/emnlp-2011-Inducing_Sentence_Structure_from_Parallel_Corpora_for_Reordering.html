<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>74 emnlp-2011-Inducing Sentence Structure from Parallel Corpora for Reordering</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-74" href="#">emnlp2011-74</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>74 emnlp-2011-Inducing Sentence Structure from Parallel Corpora for Reordering</h1>
<br/><p>Source: <a title="emnlp-2011-74-pdf" href="http://aclweb.org/anthology//D/D11/D11-1018.pdf">pdf</a></p><p>Author: John DeNero ; Jakob Uszkoreit</p><p>Abstract: When translating among languages that differ substantially in word order, machine translation (MT) systems benefit from syntactic preordering—an approach that uses features from a syntactic parse to permute source words into a target-language-like order. This paper presents a method for inducing parse trees automatically from a parallel corpus, instead of using a supervised parser trained on a treebank. These induced parses are used to preorder source sentences. We demonstrate that our induced parser is effective: it not only improves a state-of-the-art phrase-based system with integrated reordering, but also approaches the performance of a recent preordering method based on a supervised parser. These results show that the syntactic structure which is relevant to MT pre-ordering can be learned automatically from parallel text, thus establishing a new application for unsupervised grammar induction.</p><p>Reference: <a title="emnlp-2011-74-reference" href="../emnlp2011_reference/emnlp-2011-Inducing_Sentence_Structure_from_Parallel_Corpora_for_Reordering_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Inducing Sentence Structure from Parallel Corpora for Reordering  John DeNero Google Research denero @ google  . [sent-1, score-0.149]
</p><p>2 com  Abstract When translating among languages that differ substantially in word order, machine translation (MT) systems benefit from syntactic preordering—an approach that uses features from a syntactic parse to permute source words into a target-language-like order. [sent-2, score-0.421]
</p><p>3 This paper presents a method for inducing parse trees automatically from a parallel corpus, instead of using a supervised parser trained on a treebank. [sent-3, score-0.464]
</p><p>4 These induced parses are used to preorder source sentences. [sent-4, score-0.162]
</p><p>5 We demonstrate that our induced parser is effective: it not only improves a state-of-the-art phrase-based system with integrated reordering, but also approaches the performance of a recent preordering method based on a supervised parser. [sent-5, score-0.329]
</p><p>6 These results show that the syntactic structure which is relevant to MT pre-ordering can be  learned automatically from parallel text, thus establishing a new application for unsupervised grammar induction. [sent-6, score-0.3]
</p><p>7 1 Introduction Recent work in statistical machine translation (MT) has demonstrated the effectiveness of syntactic preordering: an approach that permutes source sentences into a target-like order as a pre-processing step, using features of a source-side syntactic parse (Collins et al. [sent-7, score-0.384]
</p><p>8 Syntactic pre-ordering is particularly effective at applying structural transformations, such as the ordering change from a subject-verb-object (SVO) language like English to a subject-object-verb (SOV) language like Japanese. [sent-10, score-0.092]
</p><p>9 However, state-of-the-art 193 Jakob Uszkoreit Google Research us z kore it @ google com  . [sent-11, score-0.115]
</p><p>10 pre-ordering methods require a supervised syntactic parser to provide structural information about each sentence. [sent-12, score-0.177]
</p><p>11 We propose a method that learns  both a parsing model and a reordering model directly from a word-aligned parallel corpus. [sent-13, score-0.62]
</p><p>12 Our approach, which we call Structure Induction for Reordering (STIR), requires no syntactic annotations to train, but approaches the performance of a recent syntactic pre-ordering method in a large-scale English-Japanese MT system. [sent-14, score-0.104]
</p><p>13 STIR predicts a pre-ordering via two pipelined models: (1) parsing and (2) tree reordering. [sent-15, score-0.276]
</p><p>14 The first model induces a binary parse, which defines the space of possible reorderings. [sent-16, score-0.063]
</p><p>15 In particular, only trees that properly separate verbs from their object noun phrases will license an SVO to SOV transformation. [sent-17, score-0.276]
</p><p>16 Our approach resembles work with binary synchronous grammars (Wu, 1997), but is distinct in its emphasis on monolingual parsing as a first phase, and in selecting reorderings without the aid of a target-side language model. [sent-19, score-0.414]
</p><p>17 The parsing model is trained to maximize the conditional likelihood of trees that license the reorderings implied by observed word alignments in a parallel corpus. [sent-20, score-0.824]
</p><p>18 This objective differs from those of previous grammar induction models, which typ-  ically focus on succinctly explaining the observed source language corpus via latent hierarchical structure (Pereira and Schabes, 1992; Klein and Manning, 2002). [sent-21, score-0.309]
</p><p>19 Our convex objective allows us to train a feature-rich log-linear parsing model, even without supervised treebank data. [sent-22, score-0.221]
</p><p>20 We evaluate STIR in a large-scale EnglishJapanese machine translation system. [sent-27, score-0.034]
</p><p>21 We measure how closely our predicted reorderings match those implied by hand-annotated word alignments. [sent-28, score-0.277]
</p><p>22 STIR approaches the performance of the state-of-the-art pre-ordering method described in Genzel (2010), which learns reordering rules for supervised treebank parses. [sent-29, score-0.442]
</p><p>23 84 BLEU over a standard phrase-based system  with an integrated reordering model. [sent-31, score-0.36]
</p><p>24 2  Parsing and Reordering Models  STIR consists of two pipelined log-linear models for parsing and reordering, as well as a third model for inducing trees from parallel corpora, trees that serve to train the first two models. [sent-32, score-0.686]
</p><p>25 For each aligned sentence pair in a parallel corpus, the parallel parsing model selects a binary tree t over the source sentence, such that t licenses the reordering pattern implied by the word alignment (Section 2. [sent-35, score-1.308]
</p><p>26 The monolingual parsing model is trained to generate t without inspecting the alignments or target sentences (Section 2. [sent-37, score-0.249]
</p><p>27 The tree reordering model is trained to locally permute t to produce the target order (Section 2. [sent-39, score-0.577]
</p><p>28 In the context of an MT system, the monolingual parser and tree reorderer are applied in sequence to pre-order source sentences. [sent-41, score-0.249]
</p><p>29 1 Unlabeled Binary Trees  Unlabeled binary trees are central to the STIR pipeline. [sent-43, score-0.178]
</p><p>30 Let [k, ‘) denote a span of indices of a 0indexed word sequence e, where i ∈ [k, ‘) if k ≤ i n < e‘. [sent-45, score-0.172]
</p><p>31 d [0, n) edqeuneontecse teh,e w rohoerte span covering kth ≤e whole sequence, where n = |e| . [sent-46, score-0.211]
</p><p>32 A tree t = (T , N) consists of a set of terminalA spans Tt =and ( Tno ,nN-te)r cmoinnsails spans a N s. [sent-47, score-0.42]
</p><p>33 e tE oafch te non194  Figure 1: The training and reordering pipeline for STIR contains three models. [sent-48, score-0.446]
</p><p>34 The inputs and outputs of each model are indicated by solid arrows, while dashed arrows indicate the source of training examples. [sent-49, score-0.162]
</p><p>35 The parallel parsing model provides tree and reordering examples that are used to train the other models. [sent-50, score-0.697]
</p><p>36 In an MT system, the  trained reordering pipeline (shaded) pre-orders a source sentence without target-side or alignment information. [sent-51, score-0.517]
</p><p>37 terminal span [k, ‘) ∈ N has a split point m, where tke < m < a‘n splits )t h∈e span sin ato sp clhitil pdo spans [k, m) and [m, ‘). [sent-52, score-0.824]
</p><p>38 Formally, a pair (T , N) is a well-formed tarnede over [0, n) mif:a •  •  •  The root span [0, n) ∈ T ∪ N. [sent-53, score-0.172]
</p><p>39 c Terminal spans T are disjoint, but cover [0, n). [sent-55, score-0.172]
</p><p>40 It is often convenient to refer to a split non-terminal triple (k, m, ‘) that include a non-terminal span [k, ‘) and its split point m. [sent-57, score-0.376]
</p><p>41 2 Parallel Parsing Model The first step in the STIR pipeline is to select a binary parse of each source sentence in a parallel cor-  pus, one which licenses the reordering implied by a word alignment. [sent-60, score-0.996]
</p><p>42 Let the triple (e, f,A) be an aligned sentence pair, ewth tehree e palned fe are Aw)o rbde sequences and A is a set of links (i, j) indicating that ei aligns aton fj. [sent-61, score-0.342]
</p><p>43 c(j) is the number of aligned words in f prior to position j. [sent-66, score-0.057]
</p><p>44 Ji = {j : (i, j) ∈ A0}  ∅  ,  and let ψ(i) = if ei is unaligned. [sent-69, score-0.048]
</p><p>45 We can extend tahnids projection fu ∅n cifti eon to spans [k, ‘) ofe via union:  ψ(k,‘) =  [ ψ(i)  . [sent-70, score-0.344]
</p><p>46 k≤[i<‘ We say that a span [k, ‘) aligns contiguously if ∀(i, j) ∈ A0, j ∈ ψ(k, ‘) implies i∈ [k, ‘) , which corresponds to the familiar definition that [k, ‘) is one side of an extractable phrase pair. [sent-71, score-0.436]
</p><p>47 Given this notion of projection, we can relate trees to alignments. [sent-73, score-0.148]
</p><p>48 A tree (T , N) over e respects an alignment mAen0 itsf. [sent-74, score-0.202]
</p><p>49 a lAl [k, ‘) ∈ NT) ∪ o v Ner align contiguously, anntd A foirf every (k, m, ‘), ∪th Ne projections ψ(k, m) and ψ(m, ‘) are adjacent. [sent-76, score-0.148]
</p><p>50 Projections are adjacent if the left bound of one is the right bound of the other, or if either is empty. [sent-77, score-0.088]
</p><p>51 The parallel parsing model is a linear model over trees that respect A0, which factors over spans. [sent-78, score-0.375]
</p><p>52 s(t)  =  X wTφT(k,‘) + X wNφN(k,m,‘)  [k,X‘)∈T  (k,mX,‘)∈N+  where the weight vector w = (wT wN) scores features φT on terminal spans and φN on non-terminal spans and their split points. [sent-79, score-0.522]
</p><p>53 Let s(k, ‘) be the score of the highest scoring binary tree over the span [k, ‘) that 195 respects A0. [sent-81, score-0.397]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stir', 0.57), ('reordering', 0.323), ('span', 0.172), ('spans', 0.172), ('preordering', 0.164), ('parallel', 0.159), ('implied', 0.155), ('license', 0.127), ('permute', 0.127), ('permutes', 0.127), ('terminal', 0.123), ('reorderings', 0.122), ('trees', 0.115), ('mt', 0.11), ('parsing', 0.101), ('licenses', 0.099), ('pipelined', 0.099), ('contiguously', 0.099), ('svo', 0.091), ('sov', 0.086), ('respects', 0.086), ('arrows', 0.086), ('projections', 0.086), ('wt', 0.082), ('pipeline', 0.078), ('google', 0.078), ('aligns', 0.077), ('source', 0.076), ('tree', 0.076), ('denero', 0.071), ('unaligned', 0.071), ('projection', 0.068), ('binary', 0.063), ('align', 0.062), ('triple', 0.062), ('inducing', 0.059), ('aligned', 0.057), ('monolingual', 0.057), ('split', 0.055), ('ordering', 0.055), ('mif', 0.055), ('ically', 0.055), ('aton', 0.055), ('englishjapanese', 0.055), ('ifc', 0.055), ('ofe', 0.055), ('pdo', 0.055), ('uszkoreit', 0.055), ('wn', 0.052), ('syntactic', 0.052), ('locally', 0.051), ('lal', 0.049), ('genzel', 0.049), ('tno', 0.049), ('succinctly', 0.049), ('tahnids', 0.049), ('extractable', 0.049), ('supervised', 0.048), ('ei', 0.048), ('preorder', 0.046), ('ear', 0.046), ('establishing', 0.046), ('inspecting', 0.046), ('jakob', 0.046), ('schabes', 0.046), ('induction', 0.045), ('alignments', 0.045), ('te', 0.045), ('bound', 0.044), ('perspective', 0.043), ('parse', 0.043), ('tehree', 0.043), ('pus', 0.043), ('sin', 0.043), ('grammar', 0.043), ('explaining', 0.041), ('alignment', 0.04), ('induced', 0.04), ('parser', 0.04), ('kth', 0.039), ('ig', 0.039), ('familiar', 0.039), ('shaded', 0.039), ('train', 0.038), ('learns', 0.037), ('com', 0.037), ('resembles', 0.037), ('integrated', 0.037), ('structural', 0.037), ('properly', 0.034), ('emphasis', 0.034), ('treebank', 0.034), ('translation', 0.034), ('transformations', 0.033), ('aw', 0.033), ('disjoint', 0.033), ('canonical', 0.033), ('relate', 0.033), ('sp', 0.032), ('convenient', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="74-tfidf-1" href="./emnlp-2011-Inducing_Sentence_Structure_from_Parallel_Corpora_for_Reordering.html">74 emnlp-2011-Inducing Sentence Structure from Parallel Corpora for Reordering</a></p>
<p>Author: John DeNero ; Jakob Uszkoreit</p><p>Abstract: When translating among languages that differ substantially in word order, machine translation (MT) systems benefit from syntactic preordering—an approach that uses features from a syntactic parse to permute source words into a target-language-like order. This paper presents a method for inducing parse trees automatically from a parallel corpus, instead of using a supervised parser trained on a treebank. These induced parses are used to preorder source sentences. We demonstrate that our induced parser is effective: it not only improves a state-of-the-art phrase-based system with integrated reordering, but also approaches the performance of a recent preordering method based on a supervised parser. These results show that the syntactic structure which is relevant to MT pre-ordering can be learned automatically from parallel text, thus establishing a new application for unsupervised grammar induction.</p><p>2 0.29915503 <a title="74-tfidf-2" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>Author: Jason Katz-Brown ; Slav Petrov ; Ryan McDonald ; Franz Och ; David Talbot ; Hiroshi Ichikawa ; Masakazu Seno ; Hideto Kazawa</p><p>Abstract: We propose a simple training regime that can improve the extrinsic performance of a parser, given only a corpus of sentences and a way to automatically evaluate the extrinsic quality of a candidate parse. We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system. We use a corpus of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress.</p><p>3 0.25664777 <a title="74-tfidf-3" href="./emnlp-2011-A_Word_Reordering_Model_for_Improved_Machine_Translation.html">13 emnlp-2011-A Word Reordering Model for Improved Machine Translation</a></p>
<p>Author: Karthik Visweswariah ; Rajakrishnan Rajkumar ; Ankur Gandhe ; Ananthakrishnan Ramanathan ; Jiri Navratil</p><p>Abstract: Preordering of source side sentences has proved to be useful in improving statistical machine translation. Most work has used a parser in the source language along with rules to map the source language word order into the target language word order. The requirement to have a source language parser is a major drawback, which we seek to overcome in this paper. Instead of using a parser and then using rules to order the source side sentence we learn a model that can directly reorder source side sentences to match target word order using a small parallel corpus with highquality word alignments. Our model learns pairwise costs of a word immediately preced- ing another word. We use the Lin-Kernighan heuristic to find the best source reordering efficiently during training and testing and show that it suffices to provide good quality reordering. We show gains in translation performance based on our reordering model for translating from Hindi to English, Urdu to English (with a public dataset), and English to Hindi. For English to Hindi we show that our technique achieves better performance than a method that uses rules applied to the source side English parse.</p><p>4 0.17363031 <a title="74-tfidf-4" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>Author: Yang Gao ; Philipp Koehn ; Alexandra Birch</p><p>Abstract: Long-distance reordering remains one of the biggest challenges facing machine translation. We derive soft constraints from the source dependency parsing to directly address the reordering problem for the hierarchical phrasebased model. Our approach significantly improves Chinese–English machine translation on a large-scale task by 0.84 BLEU points on average. Moreover, when we switch the tuning function from BLEU to the LRscore which promotes reordering, we observe total improvements of 1.21 BLEU, 1.30 LRscore and 3.36 TER over the baseline. On average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially effective for long-distance reodering.</p><p>5 0.12510827 <a title="74-tfidf-5" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>Author: Keith Hall ; Ryan McDonald ; Jason Katz-Brown ; Michael Ringgaard</p><p>Abstract: We present an online learning algorithm for training parsers which allows for the inclusion of multiple objective functions. The primary example is the extension of a standard supervised parsing objective function with additional loss-functions, either based on intrinsic parsing quality or task-specific extrinsic measures of quality. Our empirical results show how this approach performs for two dependency parsing algorithms (graph-based and transition-based parsing) and how it achieves increased performance on multiple target tasks including reordering for machine translation and parser adaptation.</p><p>6 0.11756361 <a title="74-tfidf-6" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>7 0.10830866 <a title="74-tfidf-7" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>8 0.095101506 <a title="74-tfidf-8" href="./emnlp-2011-Multi-Source_Transfer_of_Delexicalized_Dependency_Parsers.html">95 emnlp-2011-Multi-Source Transfer of Delexicalized Dependency Parsers</a></p>
<p>9 0.093755059 <a title="74-tfidf-9" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>10 0.093608297 <a title="74-tfidf-10" href="./emnlp-2011-Joint_Models_for_Chinese_POS_Tagging_and_Dependency_Parsing.html">75 emnlp-2011-Joint Models for Chinese POS Tagging and Dependency Parsing</a></p>
<p>11 0.091677628 <a title="74-tfidf-11" href="./emnlp-2011-Relaxed_Cross-lingual_Projection_of_Constituent_Syntax.html">115 emnlp-2011-Relaxed Cross-lingual Projection of Constituent Syntax</a></p>
<p>12 0.091408081 <a title="74-tfidf-12" href="./emnlp-2011-A_Correction_Model_for_Word_Alignments.html">3 emnlp-2011-A Correction Model for Word Alignments</a></p>
<p>13 0.070954531 <a title="74-tfidf-13" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>14 0.066037185 <a title="74-tfidf-14" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>15 0.061518744 <a title="74-tfidf-15" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>16 0.060918618 <a title="74-tfidf-16" href="./emnlp-2011-A_Fast%2C_Accurate%2C_Non-Projective%2C_Semantically-Enriched_Parser.html">4 emnlp-2011-A Fast, Accurate, Non-Projective, Semantically-Enriched Parser</a></p>
<p>17 0.060620219 <a title="74-tfidf-17" href="./emnlp-2011-Heuristic_Search_for_Non-Bottom-Up_Tree_Structure_Prediction.html">65 emnlp-2011-Heuristic Search for Non-Bottom-Up Tree Structure Prediction</a></p>
<p>18 0.056819119 <a title="74-tfidf-18" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>19 0.055734809 <a title="74-tfidf-19" href="./emnlp-2011-Statistical_Machine_Translation_with_Local_Language_Models.html">125 emnlp-2011-Statistical Machine Translation with Local Language Models</a></p>
<p>20 0.054737337 <a title="74-tfidf-20" href="./emnlp-2011-Syntactic_Decision_Tree_LMs%3A_Random_Selection_or_Intelligent_Design%3F.html">131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.228), (1, 0.233), (2, 0.105), (3, 0.101), (4, -0.058), (5, 0.091), (6, 0.281), (7, 0.146), (8, -0.033), (9, 0.043), (10, -0.018), (11, -0.039), (12, -0.082), (13, -0.049), (14, -0.034), (15, -0.151), (16, -0.043), (17, 0.095), (18, -0.086), (19, 0.065), (20, 0.05), (21, -0.056), (22, -0.016), (23, -0.069), (24, -0.056), (25, -0.022), (26, 0.024), (27, -0.087), (28, 0.173), (29, 0.003), (30, 0.03), (31, -0.003), (32, -0.062), (33, 0.054), (34, 0.022), (35, -0.032), (36, -0.014), (37, 0.052), (38, 0.004), (39, 0.036), (40, 0.01), (41, -0.118), (42, 0.018), (43, 0.012), (44, 0.112), (45, 0.024), (46, 0.078), (47, -0.059), (48, 0.094), (49, -0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94313246 <a title="74-lsi-1" href="./emnlp-2011-Inducing_Sentence_Structure_from_Parallel_Corpora_for_Reordering.html">74 emnlp-2011-Inducing Sentence Structure from Parallel Corpora for Reordering</a></p>
<p>Author: John DeNero ; Jakob Uszkoreit</p><p>Abstract: When translating among languages that differ substantially in word order, machine translation (MT) systems benefit from syntactic preordering—an approach that uses features from a syntactic parse to permute source words into a target-language-like order. This paper presents a method for inducing parse trees automatically from a parallel corpus, instead of using a supervised parser trained on a treebank. These induced parses are used to preorder source sentences. We demonstrate that our induced parser is effective: it not only improves a state-of-the-art phrase-based system with integrated reordering, but also approaches the performance of a recent preordering method based on a supervised parser. These results show that the syntactic structure which is relevant to MT pre-ordering can be learned automatically from parallel text, thus establishing a new application for unsupervised grammar induction.</p><p>2 0.82818133 <a title="74-lsi-2" href="./emnlp-2011-A_Word_Reordering_Model_for_Improved_Machine_Translation.html">13 emnlp-2011-A Word Reordering Model for Improved Machine Translation</a></p>
<p>Author: Karthik Visweswariah ; Rajakrishnan Rajkumar ; Ankur Gandhe ; Ananthakrishnan Ramanathan ; Jiri Navratil</p><p>Abstract: Preordering of source side sentences has proved to be useful in improving statistical machine translation. Most work has used a parser in the source language along with rules to map the source language word order into the target language word order. The requirement to have a source language parser is a major drawback, which we seek to overcome in this paper. Instead of using a parser and then using rules to order the source side sentence we learn a model that can directly reorder source side sentences to match target word order using a small parallel corpus with highquality word alignments. Our model learns pairwise costs of a word immediately preced- ing another word. We use the Lin-Kernighan heuristic to find the best source reordering efficiently during training and testing and show that it suffices to provide good quality reordering. We show gains in translation performance based on our reordering model for translating from Hindi to English, Urdu to English (with a public dataset), and English to Hindi. For English to Hindi we show that our technique achieves better performance than a method that uses rules applied to the source side English parse.</p><p>3 0.77563745 <a title="74-lsi-3" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>Author: Jason Katz-Brown ; Slav Petrov ; Ryan McDonald ; Franz Och ; David Talbot ; Hiroshi Ichikawa ; Masakazu Seno ; Hideto Kazawa</p><p>Abstract: We propose a simple training regime that can improve the extrinsic performance of a parser, given only a corpus of sentences and a way to automatically evaluate the extrinsic quality of a candidate parse. We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system. We use a corpus of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress.</p><p>4 0.56611872 <a title="74-lsi-4" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>Author: Yang Gao ; Philipp Koehn ; Alexandra Birch</p><p>Abstract: Long-distance reordering remains one of the biggest challenges facing machine translation. We derive soft constraints from the source dependency parsing to directly address the reordering problem for the hierarchical phrasebased model. Our approach significantly improves Chinese–English machine translation on a large-scale task by 0.84 BLEU points on average. Moreover, when we switch the tuning function from BLEU to the LRscore which promotes reordering, we observe total improvements of 1.21 BLEU, 1.30 LRscore and 3.36 TER over the baseline. On average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially effective for long-distance reodering.</p><p>5 0.45797306 <a title="74-lsi-5" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>Author: Keith Hall ; Ryan McDonald ; Jason Katz-Brown ; Michael Ringgaard</p><p>Abstract: We present an online learning algorithm for training parsers which allows for the inclusion of multiple objective functions. The primary example is the extension of a standard supervised parsing objective function with additional loss-functions, either based on intrinsic parsing quality or task-specific extrinsic measures of quality. Our empirical results show how this approach performs for two dependency parsing algorithms (graph-based and transition-based parsing) and how it achieves increased performance on multiple target tasks including reordering for machine translation and parser adaptation.</p><p>6 0.40504909 <a title="74-lsi-6" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>7 0.34762076 <a title="74-lsi-7" href="./emnlp-2011-A_Correction_Model_for_Word_Alignments.html">3 emnlp-2011-A Correction Model for Word Alignments</a></p>
<p>8 0.33995554 <a title="74-lsi-8" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>9 0.33605897 <a title="74-lsi-9" href="./emnlp-2011-Relaxed_Cross-lingual_Projection_of_Constituent_Syntax.html">115 emnlp-2011-Relaxed Cross-lingual Projection of Constituent Syntax</a></p>
<p>10 0.32622543 <a title="74-lsi-10" href="./emnlp-2011-Multi-Source_Transfer_of_Delexicalized_Dependency_Parsers.html">95 emnlp-2011-Multi-Source Transfer of Delexicalized Dependency Parsers</a></p>
<p>11 0.32592925 <a title="74-lsi-11" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>12 0.3062219 <a title="74-lsi-12" href="./emnlp-2011-Accurate_Parsing_with_Compact_Tree-Substitution_Grammars%3A_Double-DOP.html">16 emnlp-2011-Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP</a></p>
<p>13 0.3040736 <a title="74-lsi-13" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>14 0.30264005 <a title="74-lsi-14" href="./emnlp-2011-Heuristic_Search_for_Non-Bottom-Up_Tree_Structure_Prediction.html">65 emnlp-2011-Heuristic Search for Non-Bottom-Up Tree Structure Prediction</a></p>
<p>15 0.27709159 <a title="74-lsi-15" href="./emnlp-2011-SMT_Helps_Bitext_Dependency_Parsing.html">118 emnlp-2011-SMT Helps Bitext Dependency Parsing</a></p>
<p>16 0.27035373 <a title="74-lsi-16" href="./emnlp-2011-Syntactic_Decision_Tree_LMs%3A_Random_Selection_or_Intelligent_Design%3F.html">131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</a></p>
<p>17 0.26478618 <a title="74-lsi-17" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>18 0.26435643 <a title="74-lsi-18" href="./emnlp-2011-Lexical_Co-occurrence%2C_Statistical_Significance%2C_and_Word_Association.html">86 emnlp-2011-Lexical Co-occurrence, Statistical Significance, and Word Association</a></p>
<p>19 0.25913003 <a title="74-lsi-19" href="./emnlp-2011-Efficient_retrieval_of_tree_translation_examples_for_Syntax-Based_Machine_Translation.html">47 emnlp-2011-Efficient retrieval of tree translation examples for Syntax-Based Machine Translation</a></p>
<p>20 0.25805211 <a title="74-lsi-20" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.08), (36, 0.018), (37, 0.584), (45, 0.026), (54, 0.022), (62, 0.011), (64, 0.027), (66, 0.031), (69, 0.017), (79, 0.055), (96, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89274412 <a title="74-lda-1" href="./emnlp-2011-Inducing_Sentence_Structure_from_Parallel_Corpora_for_Reordering.html">74 emnlp-2011-Inducing Sentence Structure from Parallel Corpora for Reordering</a></p>
<p>Author: John DeNero ; Jakob Uszkoreit</p><p>Abstract: When translating among languages that differ substantially in word order, machine translation (MT) systems benefit from syntactic preordering—an approach that uses features from a syntactic parse to permute source words into a target-language-like order. This paper presents a method for inducing parse trees automatically from a parallel corpus, instead of using a supervised parser trained on a treebank. These induced parses are used to preorder source sentences. We demonstrate that our induced parser is effective: it not only improves a state-of-the-art phrase-based system with integrated reordering, but also approaches the performance of a recent preordering method based on a supervised parser. These results show that the syntactic structure which is relevant to MT pre-ordering can be learned automatically from parallel text, thus establishing a new application for unsupervised grammar induction.</p><p>2 0.88404852 <a title="74-lda-2" href="./emnlp-2011-A_Fast_Re-scoring_Strategy_to_Capture_Long-Distance_Dependencies.html">5 emnlp-2011-A Fast Re-scoring Strategy to Capture Long-Distance Dependencies</a></p>
<p>Author: Anoop Deoras ; Tomas Mikolov ; Kenneth Church</p><p>Abstract: A re-scoring strategy is proposed that makes it feasible to capture more long-distance dependencies in the natural language. Two pass strategies have become popular in a number of recognition tasks such as ASR (automatic speech recognition), MT (machine translation) and OCR (optical character recognition). The first pass typically applies a weak language model (n-grams) to a lattice and the second pass applies a stronger language model to N best lists. The stronger language model is intended to capture more longdistance dependencies. The proposed method uses RNN-LM (recurrent neural network language model), which is a long span LM, to rescore word lattices in the second pass. A hill climbing method (iterative decoding) is proposed to search over islands of confusability in the word lattice. An evaluation based on Broadcast News shows speedups of 20 over basic N best re-scoring, and word error rate reduction of 8% (relative) on a highly competitive setup.</p><p>3 0.77572483 <a title="74-lda-3" href="./emnlp-2011-A_Weakly-supervised_Approach_to_Argumentative_Zoning_of_Scientific_Documents.html">12 emnlp-2011-A Weakly-supervised Approach to Argumentative Zoning of Scientific Documents</a></p>
<p>Author: Yufan Guo ; Anna Korhonen ; Thierry Poibeau</p><p>Abstract: Documents Anna Korhonen Thierry Poibeau Computer Laboratory LaTTiCe, UMR8094 University of Cambridge, UK CNRS & ENS, France alk2 3 @ cam . ac .uk thierry .po ibeau @ ens . fr tific literature according to categories of information structure (or discourse, rhetorical, argumentative or Argumentative Zoning (AZ) analysis of the argumentative structure of a scientific paper has proved useful for a number of information access tasks. Current approaches to AZ rely on supervised machine learning (ML). – – Requiring large amounts of annotated data, these approaches are expensive to develop and port to different domains and tasks. A potential solution to this problem is to use weaklysupervised ML instead. We investigate the performance of four weakly-supervised classifiers on scientific abstract data annotated for multiple AZ classes. Our best classifier based on the combination of active learning and selftraining outperforms our best supervised classifier, yielding a high accuracy of 81% when using just 10% of the labeled data. This result suggests that weakly-supervised learning could be employed to improve the practical applicability and portability of AZ across different information access tasks.</p><p>4 0.42835042 <a title="74-lda-4" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>Author: Marco Dinarelli ; Sophie Rosset</p><p>Abstract: Reranking models have been successfully applied to many tasks of Natural Language Processing. However, there are two aspects of this approach that need a deeper investigation: (i) Assessment of hypotheses generated for reranking at classification phase: baseline models generate a list of hypotheses and these are used for reranking without any assessment; (ii) Detection of cases where reranking models provide a worst result: the best hypothesis provided by the reranking model is assumed to be always the best result. In some cases the reranking model provides an incorrect hypothesis while the baseline best hypothesis is correct, especially when baseline models are accurate. In this paper we propose solutions for these two aspects: (i) a semantic inconsistency metric to select possibly more correct n-best hypotheses, from a large set generated by an SLU basiline model. The selected hypotheses are reranked applying a state-of-the-art model based on Partial Tree Kernels, which encode SLU hypotheses in Support Vector Machines with complex structured features; (ii) finally, we apply a decision strategy, based on confidence values, to select the final hypothesis between the first ranked hypothesis provided by the baseline SLU model and the first ranked hypothesis provided by the re-ranker. We show the effectiveness of these solutions presenting comparative results obtained reranking hypotheses generated by a very accurate Conditional Random Field model. We evaluate our approach on the French MEDIA corpus. The results show significant improvements with respect to current state-of-the-art and previous 1104 Sophie Rosset LIMSI-CNRS B.P. 133, 91403 Orsay Cedex France ro s set @ l ims i fr . re-ranking models.</p><p>5 0.41982707 <a title="74-lda-5" href="./emnlp-2011-Efficient_Subsampling_for_Training_Complex_Language_Models.html">46 emnlp-2011-Efficient Subsampling for Training Complex Language Models</a></p>
<p>Author: Puyang Xu ; Asela Gunawardana ; Sanjeev Khudanpur</p><p>Abstract: We propose an efficient way to train maximum entropy language models (MELM) and neural network language models (NNLM). The advantage of the proposed method comes from a more robust and efficient subsampling technique. The original multi-class language modeling problem is transformed into a set of binary problems where each binary classifier predicts whether or not a particular word will occur. We show that the binarized model is as powerful as the standard model and allows us to aggressively subsample negative training examples without sacrificing predictive performance. Empirical results show that we can train MELM and NNLM at 1% ∼ 5% of the strtaaninda MrdE complexity LwMith a no %los ∼s 5in% performance.</p><p>6 0.3698709 <a title="74-lda-6" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>7 0.36434463 <a title="74-lda-7" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>8 0.35044977 <a title="74-lda-8" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>9 0.34342861 <a title="74-lda-9" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<p>10 0.34189928 <a title="74-lda-10" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>11 0.33631855 <a title="74-lda-11" href="./emnlp-2011-A_Word_Reordering_Model_for_Improved_Machine_Translation.html">13 emnlp-2011-A Word Reordering Model for Improved Machine Translation</a></p>
<p>12 0.33629993 <a title="74-lda-12" href="./emnlp-2011-Hierarchical_Phrase-based_Translation_Representations.html">66 emnlp-2011-Hierarchical Phrase-based Translation Representations</a></p>
<p>13 0.33379656 <a title="74-lda-13" href="./emnlp-2011-Third-order_Variational_Reranking_on_Packed-Shared_Dependency_Forests.html">134 emnlp-2011-Third-order Variational Reranking on Packed-Shared Dependency Forests</a></p>
<p>14 0.33371633 <a title="74-lda-14" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>15 0.33046025 <a title="74-lda-15" href="./emnlp-2011-Bootstrapped_Named_Entity_Recognition_for_Product_Attribute_Extraction.html">23 emnlp-2011-Bootstrapped Named Entity Recognition for Product Attribute Extraction</a></p>
<p>16 0.31953719 <a title="74-lda-16" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>17 0.31718391 <a title="74-lda-17" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>18 0.31524473 <a title="74-lda-18" href="./emnlp-2011-Cache-based_Document-level_Statistical_Machine_Translation.html">25 emnlp-2011-Cache-based Document-level Statistical Machine Translation</a></p>
<p>19 0.31208175 <a title="74-lda-19" href="./emnlp-2011-Compositional_Matrix-Space_Models_for_Sentiment_Analysis.html">30 emnlp-2011-Compositional Matrix-Space Models for Sentiment Analysis</a></p>
<p>20 0.31142581 <a title="74-lda-20" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
