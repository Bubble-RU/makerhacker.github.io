<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-58" href="#">emnlp2011-58</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</h1>
<br/><p>Source: <a title="emnlp-2011-58-pdf" href="http://aclweb.org/anthology//D/D11/D11-1081.pdf">pdf</a></p><p>Author: Xinyan Xiao ; Yang Liu ; Qun Liu ; Shouxun Lin</p><p>Abstract: Although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features, it is hard to scale up to large data due to decoding complexity. We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets.</p><p>Reference: <a title="emnlp-2011-58-reference" href="../emnlp2011_reference/emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. [sent-2, score-0.505]
</p><p>2 Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. [sent-3, score-0.884]
</p><p>3 The key idea (Section 4) is to initialize a reference derivation tree with maximum score by the help of word alignment, and then traverse the tree to generate the subset forest in linear time. [sent-28, score-1.092]
</p><p>4 Besides the efficiency improvement, such a forest allows us to train the model without resort-  translation. [sent-29, score-0.446]
</p><p>5 1  O(n6),  1Exactly, there are no reference derivations, since derivation is a latent variable in SMT. [sent-30, score-0.533]
</p><p>6 The reference translation is “the  denote a “reference”  derivation tree t1 which exactly yields  the reference translation. [sent-36, score-0.922]
</p><p>7 (2) Replacing e3 in t1 with e4 results a competing non-reference derivation t2, which fails to swap the order of X3,4. [sent-37, score-0.461]
</p><p>8 (3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3. [sent-38, score-0.533]
</p><p>9 Overall, both the generation of forests and the training algorithm are scalable, enabling us to train millions of features on large-scale data. [sent-45, score-0.308]
</p><p>10 We call γ and α as the source side and the target side of rule respectively. [sent-53, score-0.268]
</p><p>11 Here a rule means a phrase translation (Koehn et al. [sent-54, score-0.251]
</p><p>12 In context of SCFG, a derivation is a se881 quence of SCFG rules {ri}. [sent-57, score-0.417]
</p><p>13 , 2008; CLiF Gand ru Eisner, 2009) niss a compact representation of all the derivations for a given sentence under an SCFG (see Figure 1). [sent-59, score-0.344]
</p><p>14 A tree t in the forest corresponds to a derivation. [sent-60, score-0.426]
</p><p>15 More formally, a forest is a pair ⟨V, E⟩, where V is Mtheo rsee tf oorfm nodes, Efor eiss tth ies ase pta oirf hyperedge. [sent-62, score-0.362]
</p><p>16 eF oVr a given source sentence f = f1n, Each node v ∈ V is in the form Xi,j, which deno,te Esa tchhe n recognition of nonterminal X spanning the substring from the i through j (that is fi+1. [sent-63, score-0.268]
</p><p>17 t Etoa a single consequent node and corresponds to an SCFG rule r(e). [sent-68, score-0.28]
</p><p>18 3  Our Translation Forest  We use a translation forest that contains both “reference” derivations that potentially yield the reference translation and also some neighboring “nonreference” derivations that fail to produce the reference translation. [sent-69, score-1.731]
</p><p>19 Therefore, our forest only represents some of the derivations for a sentence given an SCFG rule table. [sent-70, score-0.849]
</p><p>20 The motivation of using such a forest is efficiency. [sent-71, score-0.362]
</p><p>21 The derivation tree t1represented by solid hyperedges is a reference derivation. [sent-74, score-0.721]
</p><p>22 We can construct a non-reference derivation by making small change to t1. [sent-75, score-0.316]
</p><p>23 By replacing the e3 of t1 with e4, we obtain a non-reference derivation tree t2. [sent-76, score-0.38]
</p><p>24 Thus, the derivation t2 fails to move the subject “police” to the behind of verb “shot dead”, resulting a wrong translation “the gunman was police shot dead”. [sent-80, score-0.691]
</p><p>25 Generally, our forest contains all the reference derivations RT for a sentence given a rule table, and some neighboring ano senn-rteefnecreen gcivee nde ar riuvaletio tanbsl e N, anTd , swohmiceh can hbeb odreifnigned no fnr-ormef eRreTnc . [sent-82, score-1.118]
</p><p>26 hMicohre ca formally, we fcraollm two hyperedges e1 and e2 are competing hyperedges, if their corresponding rules r(e1) = ⟨γ1, α1⟩ and r(e2) = ⟨γ2, α2⟩ :  γ1 = γ2 ∧ α1 = α2  (1)  This means they give different translations for a same source side. [sent-83, score-0.374]
</p><p>27 We use C(t) to represent the set ofcompeting derivations of tree t, and C(t,e) to represent the set of competing derivations of t if the competition occurs in hyperedge e in t. [sent-86, score-0.853]
</p><p>28 Given a rule table, the set of reference derivations RT for a sentence is determined. [sent-87, score-0.704]
</p><p>29 Then, the set of nRoTn-r feofre raen sceen edenrcieva istio dnest e NrmTin can T beh edne,fi tnheed s ferto mof nRoTn- :r ∪t∈RTC(t)  (3)  Overall, our forest is the compact representation of RT and NT . [sent-88, score-0.362]
</p><p>30 2The definition of derivation tree is similar to forest, except that the tree contains exactly one tree while forest contains exponentially trees. [sent-90, score-0.87]
</p><p>31 Starting form a reference derivation, we try to slightly change the derivation into a new reference derivation. [sent-93, score-0.75]
</p><p>32 During this process, we collect the competing derivations of reference derivations. [sent-94, score-0.626]
</p><p>33 We describe the details of local operators for changing a derivation in section 4. [sent-95, score-0.447]
</p><p>34 1, and then introduce the creation of initial refer-  ence derivation with max score in Section 4. [sent-96, score-0.316]
</p><p>35 For example, given derivation t1, we delete the node X0,1 and the related hyperedge e1 and e5. [sent-98, score-0.528]
</p><p>36 Fixing the other nodes and edges, we try to add a new edge e2 to create a new reference translation. [sent-99, score-0.355]
</p><p>37 In this case, if rule r2 really exists in our rule table, we get a new reference derivation t3. [sent-100, score-0.819]
</p><p>38 Notably, if r2 does not exist in the rule table, we fail to create a new reference derivation. [sent-103, score-0.464]
</p><p>39 In such case, we keep the origin derivation unchanged. [sent-104, score-0.372]
</p><p>40 Algorithm 1 shows the process of The input is a reference derivation t, and the output is a new derivation and the generated derivations. [sent-105, score-0.849]
</p><p>41 3  3For simplicity, we list all the trees, and do not compress them into a forest in practice. [sent-107, score-0.399]
</p><p>42 It is straight to extent the algorithm to get a compact forest for those generated derivations. [sent-108, score-0.397]
</p><p>43 (1) Applying lexicalize operator on the non-terminal node  X0,1 in (a) results  new derivation shown in (b). [sent-112, score-0.823]
</p><p>44 (2) When visiting bei in (b), the generalize operator changes the derivation into (c). [sent-113, score-0.652]
</p><p>45 a  The list used for storing forest is initialized with the input tree (line 2). [sent-114, score-0.475]
</p><p>46 For each node v, we first append the competing derivations C(t,e) to list, where e is incoming edge of v (lines 4-5). [sent-116, score-0.657]
</p><p>47 The operators returns a reference derivation tn (line 7). [sent-118, score-0.825]
</p><p>48 If it is new (line 8), we collect both the tn (line 9), and also the competing derivations C(tn, e′) of the new derivation on those edges e′ which only occur in the new derivation (lines 10-1 1). [sent-119, score-1.202]
</p><p>49 Finally, if the new derivation has a larger score, we will replace the origin derivation with new one (lines 12-13). [sent-120, score-0.688]
</p><p>50 Although there is a two-level loop for visiting nodes (line 3 and 6), each node is visited only one time in the inner loops. [sent-121, score-0.254]
</p><p>51 Considering that the number of source word (also leaf node here) is less than the total number of nodes and is more than ⌈(#node + 1)/2⌉ , the time complexity of tmhoe process (is# anlsood eli+ne1ar)/ /w2⌉it,h t hthee t mnuem cboemr polfe source word. [sent-123, score-0.318]
</p><p>52 1 Lexicalize and Generalize  The function OPERATE in Algorithm 1uses two operators to change a node: lexicalize and generalize. [sent-125, score-0.346]
</p><p>53 It moves away a nonterminal node and attaches the children of current node to its parent. [sent-128, score-0.321]
</p><p>54 In Figure 2(b), the node X0,1 is deleted, requiring a more lexicalized rule to be applied to the parent node X0,4 (one more terminal in the source side). [sent-129, score-0.509]
</p><p>55 We constrain the lexicalize operator to apply on pre-terminal nodes whose children are all terminal nodes. [sent-130, score-0.472]
</p><p>56 In contrast, the generalize operator works on terminal nodes and 883 inserts a nonterminal node between current node and its parent node. [sent-131, score-0.625]
</p><p>57 This operator generalizes over the continuous terminal sibling nodes left to the current node (including the current node). [sent-132, score-0.394]
</p><p>58 A new node X0,2 is inserted as the parent of node qiangshou and node bei. [sent-134, score-0.483]
</p><p>59 Suppose we want to lexicalize the node X0,1 in t1 of Figure 1, we first delete the node X0,1 and related edge e1 and e5, then we try to add the new  edge e2. [sent-136, score-0.561]
</p><p>60 Therefore, sometimes we may fail to create a new reference derivation (like r2 may not exist in the rule table). [sent-138, score-0.78]
</p><p>61 In such case, we keep the origin derivation unchanged. [sent-139, score-0.372]
</p><p>62 Considering the change of rules, the lexicalize operator deletes two rules and adds one new rule, while the generalize operator deletes one rule and adds two new rules. [sent-141, score-0.898]
</p><p>63 The different lies in that we use the operator for decoding where the rule table is fixing. [sent-145, score-0.399]
</p><p>64 2 Initialize a Reference Derivation The generation starts from an initial reference derivation with max score. [sent-147, score-0.591]
</p><p>65 This requires bi-parsing (Dyer, 2010) over the source sentence f and the reference translation e. [sent-148, score-0.409]
</p><p>66 If we only use the features as traditional SCFG systems, the biparsing may end with a derivation consists of some giant rules or rules with rare source/target sides, which is called degenerate solution (DeNero et al. [sent-157, score-0.614]
</p><p>67 That is because the translation rules with rare source/target sides always receive a very high translation probability. [sent-159, score-0.317]
</p><p>68 We add a prior score log(#rule) for each rule, where #rule is the number of occurrence of a rule, to reward frequent reusable rules and derivations with more rules. [sent-160, score-0.447]
</p><p>69 Finally, we may fail to create reference derivations due to the limitation in rule extraction. [sent-161, score-0.769]
</p><p>70 We also add these rules to the rule table, so as to make sure every sentence is reachable given the rule table. [sent-165, score-0.564]
</p><p>71 A source sentence is reachable given a rule table if reference derivations exists. [sent-166, score-0.887]
</p><p>72 (2008) algorithm, ensuring that the bi-parsing has at least one path to create a reference derivation. [sent-170, score-0.264]
</p><p>73 5 Training We use the forest to train a log-linear model with a latent variable as describe in Blunsom et al. [sent-172, score-0.401]
</p><p>74 The probability p(e|f) is the sum over all possible dTehreiv partoiobnasb: p(e|f) =  ∑ t∈△∑  p(t,e|f)  (4)  ∑(e,f)  where △(e, f) is the set of all possible derivations twhahte trrea △nsl(aet,ef f) i insto th e a sendt otf fi sa one ssusicbhl ede dreirviavtaiotino. [sent-174, score-0.305]
</p><p>75 n4 4Although the derivation is typically represent as d, we denotes it by t since our paper use tree to represent derivation. [sent-175, score-0.38]
</p><p>76 We first create an initial reference derivation for every training examples using bi-parsing (lines 4-5), and then online learn the parameters using SGD (lines 6-12). [sent-187, score-0.615]
</p><p>77 In practice, instead of storing all the derivations in a list, we traverse the tree twice. [sent-189, score-0.45]
</p><p>78 During training, we also change the derivations (line 10). [sent-191, score-0.305]
</p><p>79 The 8 features is the same as Chiang (2007) including 4 rule scores (direct and reverse translation scores; direct and reverse lexical translation scores); 1 target side language model score; 3 penalties for word counts, extracted rules and glue rule. [sent-203, score-0.5]
</p><p>80 RTRAIN denotes the reachable (given rule table without added rules) subset of TRAIN data. [sent-222, score-0.281]
</p><p>81 2, where rare rules with very high translation probability are selected as the reference derivations. [sent-233, score-0.426]
</p><p>82 3 in target side, where 186, 810 sentence pairs (36%) are reachable (without added rules in Section 4. [sent-240, score-0.278]
</p><p>83 We also compare our fast generation method with different data (only reachable or full data). [sent-265, score-0.241]
</p><p>84 Here a composed rule is a rule that can be produced by any other extracted rules. [sent-275, score-0.286]
</p><p>85 Since the rule table of the entire data is too large to be loaded to  the memory (even drop one-count rules), we remove many sentence pairs to create a much smaller data yet having a comparable performance with the entire data. [sent-280, score-0.299]
</p><p>86 We create minimum rules from a sentence pair, and count the number of source words in those minimum rules that are added rules. [sent-282, score-0.435]
</p><p>87 Meanwhile, such results also demonstrate that we can benefits from the forest generated by our fast  method instead of traditional CKY algorithm. [sent-312, score-0.455]
</p><p>88 This verifies our motivation to guarantee all sentence  Sentence Length  Figure 3: Plot of training times (including forest generation and SGD training) versus sentence length. [sent-315, score-0.533]
</p><p>89 Figure 3 shows training times (including forest generation and SGD training) versus sentence length. [sent-321, score-0.494]
</p><p>90 (1) Bi-parsing to initialize a reference derivation with max score. [sent-327, score-0.57]
</p><p>91 (2) Training procedure which generates a set of derivations to calculate the gradient and update parameters. [sent-328, score-0.426]
</p><p>92 For simplicity we do not compress the generated derivations into forests, therefore the size of resulting derivations is fairly small, which is about 265. [sent-333, score-0.647]
</p><p>93 Furthermore, we use lexical-  ize operator more often than generalize operator (the ration between them is 1. [sent-336, score-0.357]
</p><p>94 Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus 887 small) rules. [sent-338, score-0.718]
</p><p>95 For efficiency, we only use neighboring derivations for training. [sent-349, score-0.357]
</p><p>96 Furthermore, we focus on how to fast generate translation forest for training. [sent-353, score-0.515]
</p><p>97 8  Conclusion and Future Work  We have presented a fast generation algorithm for translation forest which contains both reference derivations and neighboring non-reference derivations for large-scale SMT discriminative training. [sent-360, score-1.515]
</p><p>98 In this paper, we define the forest based on competing derivations which only differ in one rule. [sent-365, score-0.771]
</p><p>99 There may be better classes of forest that can produce a better performance. [sent-366, score-0.362]
</p><p>100 Furthermore, since the generation of forests is quite general, it’s straight to apply our forest on other learning algorithms. [sent-368, score-0.547]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('forest', 0.362), ('derivation', 0.316), ('derivations', 0.305), ('reference', 0.217), ('lexicalize', 0.215), ('tn', 0.161), ('scfg', 0.161), ('operator', 0.155), ('rule', 0.143), ('reachable', 0.138), ('node', 0.137), ('operators', 0.131), ('hyperedges', 0.124), ('translation', 0.108), ('chiang', 0.108), ('blunsom', 0.106), ('competing', 0.104), ('rules', 0.101), ('gunman', 0.096), ('sgd', 0.096), ('police', 0.093), ('forests', 0.092), ('mert', 0.091), ('millions', 0.084), ('gradient', 0.075), ('append', 0.075), ('hyperedge', 0.075), ('watanabe', 0.073), ('bei', 0.072), ('qiangshou', 0.072), ('cky', 0.065), ('tree', 0.064), ('decoding', 0.063), ('discriminative', 0.063), ('dead', 0.062), ('visiting', 0.062), ('generation', 0.058), ('synchronous', 0.057), ('fail', 0.057), ('origin', 0.056), ('nodes', 0.055), ('perceptron', 0.055), ('phil', 0.054), ('lines', 0.053), ('neighboring', 0.052), ('minimum', 0.051), ('smt', 0.049), ('storing', 0.049), ('poon', 0.049), ('traditional', 0.048), ('biparsing', 0.048), ('jingfang', 0.048), ('och', 0.047), ('generalize', 0.047), ('create', 0.047), ('nonterminal', 0.047), ('terminal', 0.047), ('calculate', 0.046), ('fast', 0.045), ('source', 0.045), ('efficiency', 0.045), ('cohn', 0.044), ('rt', 0.044), ('tight', 0.044), ('eisner', 0.043), ('sampler', 0.042), ('partition', 0.041), ('xx', 0.041), ('fails', 0.041), ('reusable', 0.041), ('deletes', 0.041), ('chan', 0.04), ('side', 0.04), ('train', 0.039), ('sentence', 0.039), ('lies', 0.038), ('zhang', 0.038), ('franz', 0.037), ('initialize', 0.037), ('trevor', 0.037), ('visit', 0.037), ('compress', 0.037), ('germann', 0.037), ('shot', 0.037), ('complexity', 0.036), ('contrastive', 0.036), ('miles', 0.036), ('smith', 0.036), ('liang', 0.036), ('edge', 0.036), ('josef', 0.036), ('entire', 0.035), ('training', 0.035), ('milliseconds', 0.035), ('straight', 0.035), ('motivated', 0.034), ('fn', 0.034), ('globally', 0.034), ('overfitting', 0.034), ('traverse', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000013 <a title="58-tfidf-1" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>Author: Xinyan Xiao ; Yang Liu ; Qun Liu ; Shouxun Lin</p><p>Abstract: Although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features, it is hard to scale up to large data due to decoding complexity. We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets.</p><p>2 0.22319451 <a title="58-tfidf-2" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>Author: Wei Lu ; Hwee Tou Ng</p><p>Abstract: This paper describes a novel probabilistic approach for generating natural language sentences from their underlying semantics in the form of typed lambda calculus. The approach is built on top of a novel reduction-based weighted synchronous context free grammar formalism, which facilitates the transformation process from typed lambda calculus into natural language sentences. Sentences can then be generated based on such grammar rules with a log-linear model. To acquire such grammar rules automatically in an unsupervised manner, we also propose a novel approach with a generative model, which maps from sub-expressions of logical forms to word sequences in natural language sentences. Experiments on benchmark datasets for both English and Chinese generation tasks yield significant improvements over results obtained by two state-of-the-art machine translation models, in terms of both automatic metrics and human evaluation.</p><p>3 0.20659178 <a title="58-tfidf-3" href="./emnlp-2011-Third-order_Variational_Reranking_on_Packed-Shared_Dependency_Forests.html">134 emnlp-2011-Third-order Variational Reranking on Packed-Shared Dependency Forests</a></p>
<p>Author: Katsuhiko Hayashi ; Taro Watanabe ; Masayuki Asahara ; Yuji Matsumoto</p><p>Abstract: We propose a novel forest reranking algorithm for discriminative dependency parsing based on a variant of Eisner’s generative model. In our framework, we define two kinds of generative model for reranking. One is learned from training data offline and the other from a forest generated by a baseline parser on the fly. The final prediction in the reranking stage is performed using linear interpolation of these models and discriminative model. In order to efficiently train the model from and decode on a hypergraph data structure representing a forest, we apply extended inside/outside and Viterbi algorithms. Experimental results show that our proposed forest reranking algorithm achieves significant improvement when compared with conventional approaches.</p><p>4 0.2012784 <a title="58-tfidf-4" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>Author: Jiajun Zhang ; Feifei Zhai ; Chengqing Zong</p><p>Abstract: Due to its explicit modeling of the grammaticality of the output via target-side syntax, the string-to-tree model has been shown to be one of the most successful syntax-based translation models. However, a major limitation of this model is that it does not utilize any useful syntactic information on the source side. In this paper, we analyze the difficulties of incorporating source syntax in a string-totree model. We then propose a new way to use the source syntax in a fuzzy manner, both in source syntactic annotation and in rule matching. We further explore three algorithms in rule matching: 0-1 matching, likelihood matching, and deep similarity matching. Our method not only guarantees grammatical output with an explicit target tree, but also enables the system to choose the proper translation rules via fuzzy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1</p><p>5 0.17263833 <a title="58-tfidf-5" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>Author: Jun Xie ; Haitao Mi ; Qun Liu</p><p>Abstract: Dependency structure, as a first step towards semantics, is believed to be helpful to improve translation quality. However, previous works on dependency structure based models typically resort to insertion operations to complete translations, which make it difficult to specify ordering information in translation rules. In our model of this paper, we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents relations and the target side as strings. The head-dependents rules require only substitution operation, thus our model requires no heuristics or separate ordering models of the previous works to control the word order of translations. Large-scale experiments show that our model performs well on long distance reordering, and outperforms the state- of-the-art constituency-to-string model (+1.47 BLEU on average) and hierarchical phrasebased model (+0.46 BLEU on average) on two Chinese-English NIST test sets without resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models.</p><p>6 0.15093212 <a title="58-tfidf-6" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>7 0.14263982 <a title="58-tfidf-7" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>8 0.13857132 <a title="58-tfidf-8" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>9 0.12416626 <a title="58-tfidf-9" href="./emnlp-2011-Minimum_Imputed-Risk%3A_Unsupervised_Discriminative_Training_for_Machine_Translation.html">93 emnlp-2011-Minimum Imputed-Risk: Unsupervised Discriminative Training for Machine Translation</a></p>
<p>10 0.11719673 <a title="58-tfidf-10" href="./emnlp-2011-Heuristic_Search_for_Non-Bottom-Up_Tree_Structure_Prediction.html">65 emnlp-2011-Heuristic Search for Non-Bottom-Up Tree Structure Prediction</a></p>
<p>11 0.11226262 <a title="58-tfidf-11" href="./emnlp-2011-Statistical_Machine_Translation_with_Local_Language_Models.html">125 emnlp-2011-Statistical Machine Translation with Local Language Models</a></p>
<p>12 0.1006292 <a title="58-tfidf-12" href="./emnlp-2011-Better_Evaluation_Metrics_Lead_to_Better_Machine_Translation.html">22 emnlp-2011-Better Evaluation Metrics Lead to Better Machine Translation</a></p>
<p>13 0.10023093 <a title="58-tfidf-13" href="./emnlp-2011-Computing_Logical_Form_on_Regulatory_Texts.html">32 emnlp-2011-Computing Logical Form on Regulatory Texts</a></p>
<p>14 0.098810285 <a title="58-tfidf-14" href="./emnlp-2011-A_Correction_Model_for_Word_Alignments.html">3 emnlp-2011-A Correction Model for Word Alignments</a></p>
<p>15 0.096421674 <a title="58-tfidf-15" href="./emnlp-2011-Syntactic_Decision_Tree_LMs%3A_Random_Selection_or_Intelligent_Design%3F.html">131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</a></p>
<p>16 0.095581494 <a title="58-tfidf-16" href="./emnlp-2011-Exact_Decoding_of_Phrase-Based_Translation_Models_through_Lagrangian_Relaxation.html">51 emnlp-2011-Exact Decoding of Phrase-Based Translation Models through Lagrangian Relaxation</a></p>
<p>17 0.095268071 <a title="58-tfidf-17" href="./emnlp-2011-Optimal_Search_for_Minimum_Error_Rate_Training.html">100 emnlp-2011-Optimal Search for Minimum Error Rate Training</a></p>
<p>18 0.094037615 <a title="58-tfidf-18" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<p>19 0.085631363 <a title="58-tfidf-19" href="./emnlp-2011-Tuning_as_Ranking.html">138 emnlp-2011-Tuning as Ranking</a></p>
<p>20 0.085572243 <a title="58-tfidf-20" href="./emnlp-2011-Computation_of_Infix_Probabilities_for_Probabilistic_Context-Free_Grammars.html">31 emnlp-2011-Computation of Infix Probabilities for Probabilistic Context-Free Grammars</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.29), (1, 0.214), (2, 0.093), (3, -0.158), (4, 0.032), (5, -0.104), (6, -0.06), (7, -0.118), (8, 0.146), (9, -0.061), (10, 0.02), (11, 0.236), (12, -0.047), (13, -0.001), (14, 0.029), (15, -0.135), (16, -0.02), (17, 0.004), (18, 0.001), (19, 0.142), (20, 0.034), (21, 0.16), (22, -0.085), (23, 0.123), (24, 0.066), (25, -0.053), (26, 0.155), (27, 0.105), (28, 0.082), (29, -0.016), (30, 0.072), (31, -0.01), (32, -0.054), (33, -0.029), (34, 0.011), (35, -0.017), (36, -0.023), (37, 0.052), (38, -0.052), (39, -0.067), (40, 0.038), (41, -0.014), (42, -0.085), (43, 0.035), (44, -0.123), (45, 0.176), (46, -0.126), (47, 0.009), (48, -0.024), (49, 0.075)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96003538 <a title="58-lsi-1" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>Author: Xinyan Xiao ; Yang Liu ; Qun Liu ; Shouxun Lin</p><p>Abstract: Although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features, it is hard to scale up to large data due to decoding complexity. We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets.</p><p>2 0.65000021 <a title="58-lsi-2" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>Author: Wei Lu ; Hwee Tou Ng</p><p>Abstract: This paper describes a novel probabilistic approach for generating natural language sentences from their underlying semantics in the form of typed lambda calculus. The approach is built on top of a novel reduction-based weighted synchronous context free grammar formalism, which facilitates the transformation process from typed lambda calculus into natural language sentences. Sentences can then be generated based on such grammar rules with a log-linear model. To acquire such grammar rules automatically in an unsupervised manner, we also propose a novel approach with a generative model, which maps from sub-expressions of logical forms to word sequences in natural language sentences. Experiments on benchmark datasets for both English and Chinese generation tasks yield significant improvements over results obtained by two state-of-the-art machine translation models, in terms of both automatic metrics and human evaluation.</p><p>3 0.64610654 <a title="58-lsi-3" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>Author: Jiajun Zhang ; Feifei Zhai ; Chengqing Zong</p><p>Abstract: Due to its explicit modeling of the grammaticality of the output via target-side syntax, the string-to-tree model has been shown to be one of the most successful syntax-based translation models. However, a major limitation of this model is that it does not utilize any useful syntactic information on the source side. In this paper, we analyze the difficulties of incorporating source syntax in a string-totree model. We then propose a new way to use the source syntax in a fuzzy manner, both in source syntactic annotation and in rule matching. We further explore three algorithms in rule matching: 0-1 matching, likelihood matching, and deep similarity matching. Our method not only guarantees grammatical output with an explicit target tree, but also enables the system to choose the proper translation rules via fuzzy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1</p><p>4 0.61255902 <a title="58-lsi-4" href="./emnlp-2011-Third-order_Variational_Reranking_on_Packed-Shared_Dependency_Forests.html">134 emnlp-2011-Third-order Variational Reranking on Packed-Shared Dependency Forests</a></p>
<p>Author: Katsuhiko Hayashi ; Taro Watanabe ; Masayuki Asahara ; Yuji Matsumoto</p><p>Abstract: We propose a novel forest reranking algorithm for discriminative dependency parsing based on a variant of Eisner’s generative model. In our framework, we define two kinds of generative model for reranking. One is learned from training data offline and the other from a forest generated by a baseline parser on the fly. The final prediction in the reranking stage is performed using linear interpolation of these models and discriminative model. In order to efficiently train the model from and decode on a hypergraph data structure representing a forest, we apply extended inside/outside and Viterbi algorithms. Experimental results show that our proposed forest reranking algorithm achieves significant improvement when compared with conventional approaches.</p><p>5 0.57072115 <a title="58-lsi-5" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>Author: Jun Xie ; Haitao Mi ; Qun Liu</p><p>Abstract: Dependency structure, as a first step towards semantics, is believed to be helpful to improve translation quality. However, previous works on dependency structure based models typically resort to insertion operations to complete translations, which make it difficult to specify ordering information in translation rules. In our model of this paper, we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents relations and the target side as strings. The head-dependents rules require only substitution operation, thus our model requires no heuristics or separate ordering models of the previous works to control the word order of translations. Large-scale experiments show that our model performs well on long distance reordering, and outperforms the state- of-the-art constituency-to-string model (+1.47 BLEU on average) and hierarchical phrasebased model (+0.46 BLEU on average) on two Chinese-English NIST test sets without resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models.</p><p>6 0.56900501 <a title="58-lsi-6" href="./emnlp-2011-Computing_Logical_Form_on_Regulatory_Texts.html">32 emnlp-2011-Computing Logical Form on Regulatory Texts</a></p>
<p>7 0.54217535 <a title="58-lsi-7" href="./emnlp-2011-Efficient_retrieval_of_tree_translation_examples_for_Syntax-Based_Machine_Translation.html">47 emnlp-2011-Efficient retrieval of tree translation examples for Syntax-Based Machine Translation</a></p>
<p>8 0.50575906 <a title="58-lsi-8" href="./emnlp-2011-Heuristic_Search_for_Non-Bottom-Up_Tree_Structure_Prediction.html">65 emnlp-2011-Heuristic Search for Non-Bottom-Up Tree Structure Prediction</a></p>
<p>9 0.45986003 <a title="58-lsi-9" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>10 0.45280194 <a title="58-lsi-10" href="./emnlp-2011-Minimum_Imputed-Risk%3A_Unsupervised_Discriminative_Training_for_Machine_Translation.html">93 emnlp-2011-Minimum Imputed-Risk: Unsupervised Discriminative Training for Machine Translation</a></p>
<p>11 0.42916691 <a title="58-lsi-11" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>12 0.38602415 <a title="58-lsi-12" href="./emnlp-2011-Optimal_Search_for_Minimum_Error_Rate_Training.html">100 emnlp-2011-Optimal Search for Minimum Error Rate Training</a></p>
<p>13 0.35975114 <a title="58-lsi-13" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>14 0.3554889 <a title="58-lsi-14" href="./emnlp-2011-Syntactic_Decision_Tree_LMs%3A_Random_Selection_or_Intelligent_Design%3F.html">131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</a></p>
<p>15 0.34913382 <a title="58-lsi-15" href="./emnlp-2011-Accurate_Parsing_with_Compact_Tree-Substitution_Grammars%3A_Double-DOP.html">16 emnlp-2011-Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP</a></p>
<p>16 0.34734759 <a title="58-lsi-16" href="./emnlp-2011-A_Correction_Model_for_Word_Alignments.html">3 emnlp-2011-A Correction Model for Word Alignments</a></p>
<p>17 0.34295246 <a title="58-lsi-17" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>18 0.33682051 <a title="58-lsi-18" href="./emnlp-2011-Hierarchical_Phrase-based_Translation_Representations.html">66 emnlp-2011-Hierarchical Phrase-based Translation Representations</a></p>
<p>19 0.33418342 <a title="58-lsi-19" href="./emnlp-2011-Tuning_as_Ranking.html">138 emnlp-2011-Tuning as Ranking</a></p>
<p>20 0.32594797 <a title="58-lsi-20" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(15, 0.012), (23, 0.135), (36, 0.028), (37, 0.028), (45, 0.05), (46, 0.196), (53, 0.046), (54, 0.065), (57, 0.018), (62, 0.044), (64, 0.037), (65, 0.014), (66, 0.046), (69, 0.057), (79, 0.046), (82, 0.03), (90, 0.021), (96, 0.037), (98, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77885717 <a title="58-lda-1" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>Author: Xinyan Xiao ; Yang Liu ; Qun Liu ; Shouxun Lin</p><p>Abstract: Although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features, it is hard to scale up to large data due to decoding complexity. We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets.</p><p>2 0.71627825 <a title="58-lda-2" href="./emnlp-2011-Classifying_Sentences_as_Speech_Acts_in_Message_Board_Posts.html">27 emnlp-2011-Classifying Sentences as Speech Acts in Message Board Posts</a></p>
<p>Author: Ashequl Qadir ; Ellen Riloff</p><p>Abstract: This research studies the text genre of message board forums, which contain a mixture of expository sentences that present factual information and conversational sentences that include communicative acts between the writer and readers. Our goal is to create sentence classifiers that can identify whether a sentence contains a speech act, and can recognize sentences containing four different speech act classes: Commissives, Directives, Expressives, and Representatives. We conduct experiments using a wide variety of features, including lexical and syntactic features, speech act word lists from external resources, and domain-specific semantic class features. We evaluate our results on a collection of message board posts in the domain of veterinary medicine.</p><p>3 0.67198485 <a title="58-lda-3" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Noah A. Smith</p><p>Abstract: We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.</p><p>4 0.66681778 <a title="58-lda-4" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>Author: Yang Gao ; Philipp Koehn ; Alexandra Birch</p><p>Abstract: Long-distance reordering remains one of the biggest challenges facing machine translation. We derive soft constraints from the source dependency parsing to directly address the reordering problem for the hierarchical phrasebased model. Our approach significantly improves Chinese–English machine translation on a large-scale task by 0.84 BLEU points on average. Moreover, when we switch the tuning function from BLEU to the LRscore which promotes reordering, we observe total improvements of 1.21 BLEU, 1.30 LRscore and 3.36 TER over the baseline. On average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially effective for long-distance reodering.</p><p>5 0.66582364 <a title="58-lda-5" href="./emnlp-2011-Hierarchical_Phrase-based_Translation_Representations.html">66 emnlp-2011-Hierarchical Phrase-based Translation Representations</a></p>
<p>Author: Gonzalo Iglesias ; Cyril Allauzen ; William Byrne ; Adria de Gispert ; Michael Riley</p><p>Abstract: This paper compares several translation representations for a synchronous context-free grammar parse including CFGs/hypergraphs, finite-state automata (FSA), and pushdown automata (PDA). The representation choice is shown to determine the form and complexity of target LM intersection and shortest-path algorithms that follow. Intersection, shortest path, FSA expansion and RTN replacement algorithms are presented for PDAs. Chinese-toEnglish translation experiments using HiFST and HiPDT, FSA and PDA-based decoders, are presented using admissible (or exact) search, possible for HiFST with compact SCFG rulesets and HiPDT with compact LMs. For large rulesets with large LMs, we introduce a two-pass search strategy which we then analyze in terms of search errors and translation performance.</p><p>6 0.65314883 <a title="58-lda-6" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>7 0.65246779 <a title="58-lda-7" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>8 0.65094531 <a title="58-lda-8" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>9 0.65058613 <a title="58-lda-9" href="./emnlp-2011-Better_Evaluation_Metrics_Lead_to_Better_Machine_Translation.html">22 emnlp-2011-Better Evaluation Metrics Lead to Better Machine Translation</a></p>
<p>10 0.65038955 <a title="58-lda-10" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<p>11 0.64700395 <a title="58-lda-11" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>12 0.64640164 <a title="58-lda-12" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>13 0.64543039 <a title="58-lda-13" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>14 0.64493775 <a title="58-lda-14" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>15 0.64342362 <a title="58-lda-15" href="./emnlp-2011-Lateen_EM%3A_Unsupervised_Training_with_Multiple_Objectives%2C_Applied_to_Dependency_Grammar_Induction.html">79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</a></p>
<p>16 0.64093244 <a title="58-lda-16" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>17 0.64077377 <a title="58-lda-17" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>18 0.64048767 <a title="58-lda-18" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>19 0.64004964 <a title="58-lda-19" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>20 0.63901228 <a title="58-lda-20" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
