<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-20" href="#">emnlp2011-20</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</h1>
<br/><p>Source: <a title="emnlp-2011-20-pdf" href="http://aclweb.org/anthology//D/D11/D11-1019.pdf">pdf</a></p><p>Author: Jiajun Zhang ; Feifei Zhai ; Chengqing Zong</p><p>Abstract: Due to its explicit modeling of the grammaticality of the output via target-side syntax, the string-to-tree model has been shown to be one of the most successful syntax-based translation models. However, a major limitation of this model is that it does not utilize any useful syntactic information on the source side. In this paper, we analyze the difficulties of incorporating source syntax in a string-totree model. We then propose a new way to use the source syntax in a fuzzy manner, both in source syntactic annotation and in rule matching. We further explore three algorithms in rule matching: 0-1 matching, likelihood matching, and deep similarity matching. Our method not only guarantees grammatical output with an explicit target tree, but also enables the system to choose the proper translation rules via fuzzy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1</p><p>Reference: <a title="emnlp-2011-20-reference" href="../emnlp2011_reference/emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We then propose a new way to use the source syntax in a fuzzy manner, both in source syntactic annotation and in rule matching. [sent-7, score-1.472]
</p><p>2 We further explore three algorithms in rule matching: 0-1 matching, likelihood matching, and deep similarity matching. [sent-8, score-0.589]
</p><p>3 Our method not only guarantees grammatical output with an explicit target tree, but also enables the system to choose  the proper translation rules via fuzzy use of the source syntax. [sent-9, score-1.069]
</p><p>4 1  Introduction  In recent years, statistical translation models based upon linguistic syntax have shown promising progress in improving translation quality. [sent-11, score-0.498]
</p><p>5 It appears that encoding syntactic annotations on either side or both sides in translation rules can increase the expressiveness of rules and can produce more accurate translations with improved reordering. [sent-12, score-0.737]
</p><p>6 However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones. [sent-21, score-0.457]
</p><p>7 The source syntax is well-known to be helpful in improving translation accuracy, as shown especially by tree-to-string systems (Quirk et al. [sent-22, score-0.525]
</p><p>8 In this paper, we explore in the other direction: based on the strong string-to-tree model which  builds an explicit target syntactic tree during decoding rather than apply only a syntactic language model, we aim to find a useful way to incorporate the source-side syntax. [sent-35, score-0.479]
</p><p>9 Then we discuss the difficulties of integrating the source syntax into the string-to-tree model. [sent-39, score-0.468]
</p><p>10 Although the yield is grammatical, the translation is not correct since the system mistakenly applies rule r2, thus translating the Chinese preposition 和 (h é ) in the example sentence into the English conjunction and. [sent-43, score-0.588]
</p><p>11 If we are given this source syntactic clue, rule r3 will be preferred. [sent-50, score-0.64]
</p><p>12 We find that the difficulties lie in at least three problems: 1) For a string-to-tree rule such as r6 in figure 1, how should we syntactically annotate its source string? [sent-55, score-0.571]
</p><p>13 2) Given the source-annotated string-to-tree rules, how should we match these rules according to the test source tree during decoding? [sent-56, score-0.455]
</p><p>14 For the first problem, one may require the source side of a string-to-tree rule to be a  constituent. [sent-58, score-0.57]
</p><p>15 Inspired by Chiang (2010), we adopt a fuzzy way to label 205 every source string with the complex syntactic categories of SAMT (Zollmann and Venugopal, 2006). [sent-60, score-0.996]
</p><p>16 We will detail our fuzzy labeling method in Section 2. [sent-62, score-0.45]
</p><p>17 For the second problem, it appears simple and intuitive to match rules by requiring a rule’s source syntactic category to be the same as the category of the test string. [sent-63, score-0.901]
</p><p>18 Continuing to pursue the fuzzy methodology, we adopt a fuzzy matching procedure to enable matching of all the rules whose source strings match the test string, and then determine the degree of matching between the test source tree and each rule. [sent-65, score-2.606]
</p><p>19 We will discuss three fuzzy matching algorithms, from simple to complex, in Section 3. [sent-66, score-0.811]
</p><p>20 Our method not only guarantees the grammaticality of the output via the target tree structure, but also enables the system to choose appropriate translation rules during decoding through source syntactic fuzzy labeling and fuzzy matching. [sent-68, score-1.844]
</p><p>21 The main contributions of this paper are as follows: 1) We propose a fuzzy method for both source syntax annotation and rule matching for augmenting string-to-tree models. [sent-69, score-1.486]
</p><p>22 2) We design and investigate three fuzzy rule matching algorithms: 0-1 matching, likelihood matching, and deep similarity matching. [sent-70, score-1.372]
</p><p>23 We hope that this paper will demonstrate how to effectively incorporate both source and target syntax into a translation model with promising results. [sent-71, score-0.626]
</p><p>24 2  Rule Extraction  Since we annotate the source side of each string-totree rule with source parse tree information in a fuzzy way, we will henceforward denote the  source-syntax-decorated string-to-tree rule as a fuzzy-tree to exact-tree rule. [sent-72, score-1.632]
</p><p>25 We first briefly review issues of string-to-tree rule extraction; then we discuss how to augment the string-to-tree rules to yield fuzzy-tree to exact-tree rules. [sent-73, score-0.534]
</p><p>26 (2004) proposed the GHKM algorithm for extracting (minimal) string-to-tree translation rules from a triple (f, et, a), where f is the sourcelanguage sentence, et is a target-language parse tree whose yield e is the translation of f, and a is the set of word alignments between e and f. [sent-80, score-0.573]
</p><p>27 The basic idea of GHKM is to obtain the set of minimally-sized translation rules which can explain the mappings between source string and target parse tree. [sent-81, score-0.699]
</p><p>28 With fragmentation completed, the rules are extracted through a depth-first traversal of et : for each frontier being visited, a rule is extracted. [sent-87, score-0.577]
</p><p>29 To improve the rule coverage, SPMT models can be employed to obtain phrasal rules (Marcu et at. [sent-91, score-0.463]
</p><p>30 In addition, the minimal rules which share the adjacent tree fragments can be connected 206 together to form composed rules (Galley et al. [sent-93, score-0.467]
</p><p>31 Basically, the extraction algorithm includes two parts: (1) String-to-tree rule extraction (without considering the source parse tree); (2) Decoration of the source side of the string-totree rules with syntactic annotations. [sent-98, score-1.095]
</p><p>32 The simplest way to decorate, as mentioned in the Introduction, is to annotate the source-side of a string-to-tree rule with the syntactic tag that exactly covers the source string. [sent-101, score-0.728]
</p><p>33 The annotation of the source side of string-to-tree rules is processed in three steps: (1) If the source-side string corresponds to a syntactic category C in the source parse tree, we label the source string with C. [sent-121, score-1.427]
</p><p>34 (2) Otherwise, we check if there exists an extended category of the forms C1*C2, C1/C2 or C2\C11, indicating respectively that the source string spans two adjacent syntactic categories, a partial syntactic category C1 missing a C2 on the right, or a partial C1 missing a C2 on the left. [sent-122, score-1.04]
</p><p>35 C2, showing that the source string spans three adjacent syntactic categories or a partial category  with C1 and C2 on each side. [sent-125, score-0.765]
</p><p>36 Using the SAMT-style grammar, each source string can be associated with a syntactic category. [sent-129, score-0.462]
</p><p>37 For example, rule ro in Figure 2 uses the product category PP P* VP on the source side. [sent-133, score-0.688]
</p><p>38 For example, besides the rule rm in Figure 2, we encountered rules like 和 CC  IN with in the training data. [sent-135, score-0.463]
</p><p>39 Note that the final rule format used in translation depends on the specific fuzzy rule matching algorithm adopted. [sent-140, score-1.504]
</p><p>40 One way to apply the fuzzy-tree to exact-tree rules is to narrow the rule search space. [sent-142, score-0.463]
</p><p>41 They  are  used for illustration  with its parse tree, we can according to this strategy choose only the rules whose source syntax matches the test source tree. [sent-145, score-0.767]
</p><p>42 In this study, we keep the rule search space identical to that of the string-to-tree setting, and postpone the use of source-side syntax until the derivation stage. [sent-147, score-0.528]
</p><p>43 During derivation, a fuzzy matching algorithm will be adopted to compute a score to measure the compatibility between the rule and the test source syntax. [sent-148, score-1.321]
</p><p>44 In this section, three fuzzy matching algorithms, from simple to complex, are investigated in order. [sent-150, score-0.783]
</p><p>45 1  0-1 Matching  0-1 matching is a straightforward approach that rewards rules whose source syntactic category exactly matches the syntactic category of the test string and punishes mismatches. [sent-152, score-1.524]
</p><p>46 It has mainly been employed in hierarchical phrase-based models for integrating source or both-side syntax (Marton and Resnik, 2008; Chiang et al. [sent-153, score-0.462]
</p><p>47 In 0-1 matching, the rule’s source side must contain only one syntactic category, but a rule may have been decorated with more than one syntactic category on the source side. [sent-156, score-1.283]
</p><p>48 For example, the tag P in the rule 和P : 6, CC : 4  IN with appears more frequently, so the final rule used in 0-1 matching will be 和P   IN with . [sent-159, score-0.957]
</p><p>49 match_count calculates in a derivation the number of rules whose source-side syntactic category matches the syntactic category of the test string. [sent-161, score-0.898]
</p><p>50 2 Likelihood Matching It appears intuitively that the 0-1 matching algorithm does not make full use of the source-side syntax because it keeps only the most-frequent syntactic label and discards some potentially useful information. [sent-167, score-0.644]
</p><p>51 Therefore, it runs the risk of treating all the discarded source syntactic categories of the  rule as equally likely. [sent-168, score-0.724]
</p><p>52 For example, there is an extracted rule as follows: 的 DEC : 11233, DEG : 11073, DEV : 65  IN of 0-1 matching converts it into 的DEC  IN of . [sent-169, score-0.65]
</p><p>53 The use of this rule will be penalized if the syntactic category of the test string 的(dē) is parsed as DEG or DEV. [sent-170, score-0.728]
</p><p>54 Considering this situation, we propose the likelihood matching algorithm. [sent-174, score-0.426]
</p><p>55 First, we compute the likelihood of the rule’s source syntactic categories. [sent-175, score-0.444]
</p><p>56 Then, if we apply likelihood matching in the derivations in Figure 1 where the test string is 和 and its syntax is P(和), the matching score with the above rule will be 0. [sent-182, score-1.358]
</p><p>57 When the test Chinese word 和 is parsed as a category other than P or CC, the matching score with the above rule will be 7. [sent-184, score-0.81]
</p><p>58 Similar to 0-1 matching, likelihood matching will serve as an additional model feature representing the compatibility between categories and rules. [sent-186, score-0.548]
</p><p>59 3  Deep Similarity Matching  Considering the two algorithms above, we can see that the purpose of fuzzy matching is in fact to calculate a similarity. [sent-188, score-0.783]
</p><p>60 0-1 matching assigns similarity 1 for exact matches and 0 for mismatch, while likelihood matching directly utilizes the likelihood to measure the similarity. [sent-189, score-0.921]
</p><p>61 (2010) proposed this method to compute the similarity between two syntactic tag sequences, used to impose soft syntactic constraints in hierarchical phrase-based models. [sent-192, score-0.446]
</p><p>62 Analogously, we borrow this idea to calculate the similarity between two SAMT-style syntactic categories, and then apply it to calculate the degree of matching between a translation rule and the syntactic category of a test source string for purposes of fuzzy matching. [sent-193, score-2.143]
</p><p>63 Since we can now compute the similarity of any syntactic category pair, we are currently ready to compute the matching degree between the syntactic category of a test source string and a fuzzy-tree to exact-tree rule. [sent-209, score-1.421]
</p><p>64 To do this, we first convert the original fuzzy-tree to exact-tree rule to the rule of likelihood format without any smoothing. [sent-210, score-0.671]
</p><p>65 We then denote the syntax of a rule’s source-side RS  by weighting all the SAMT-style categories in RS  F RSc RSPRSc  F  c   (3)  where PRS c  is the likelihood of the category c. [sent-214, score-0.536]
</p><p>66 However, when the rules’ source string is decorated with syntax (fuzzy-tree to exact-tree rules), how should we binarize these rules? [sent-227, score-0.569]
</p><p>67 Analyzing the derivation process, we observe that a partial rule such as binary rule B2 never appears in the final derivation unless the rooted binary rule B1 also appears in the derivation. [sent-230, score-1.034]
</p><p>68 Based on this observation, we design a heuristic3  strategy: we simply attach the syntax PP*VP in the rooted binary rule B1, and do not decorate other binary rules with source syntax. [sent-231, score-0.884]
</p><p>69 Thus rule rn will be binarized as: 1  x2 x0*1 PP* VP  VP x0*1 : Vx0*x1 x2 : PP 2  5  x0 x1   Vx0*x1 x0 : VB  x1  : NP  Translation Model and Decoding  The proposed translation system is an augmentation of the string-to-tree model. [sent-232, score-0.562]
</p><p>70 The other two fuzzy matching algorithms are triggered in a similar way. [sent-241, score-0.783]
</p><p>71 In practice, since the derivation may include glue rules which are not scored by fuzzy  matching. [sent-246, score-0.692]
</p><p>72 We extracted minimal GHKM rules and the rules of SPMT Model 1 with source language phrases up to length L=4. [sent-258, score-0.608]
</p><p>73 The last three lines in Table 1 show that, for each fuzzy matching algorithm, our new system TF2ET performs significantly better than the baseline s2t system, with an improvement of more than 0. [sent-272, score-0.85]
</p><p>74 Specifically, the FT2ET system with deep similarity matching obtains the best translation quality in all tasks and surpasses the baseline s2t system by 0. [sent-284, score-0.745]
</p><p>75 By contrast, the system with likelihood matching does not perform as well as the other two algorithms, although it also significantly improves the baseline s2t in all tasks. [sent-289, score-0.457]
</p><p>76 This algorithm has several advantages: it is simple and easy to implement, and enhances the translation model by enabling its rules to take account of the source-side syntax to some degree. [sent-292, score-0.488]
</p><p>77 However, a major deficiency of this algorithm is that it does not make full use of the source side syntax, since it retains only the most frequent SAMT-style syntactic category to describe the rule’s source syntax. [sent-293, score-0.82]
</p><p>78 211  Instead of rewarding or penalizing, this algorithm uses the likelihood of the syntactic category to approximate the degree of matching between the test source syntactic category and the rule. [sent-296, score-1.334]
</p><p>79 For a category not in the rule’s source syntactic category set, the likelihood algorithm computes a smoothed likelihood. [sent-297, score-0.82]
</p><p>80 Compared with the above two matching algorithms, the deep similarity matching algorithm based on latent syntactic distribution is much more beautiful in theory. [sent-301, score-1.055]
</p><p>81 This algorithm can successfully measure the similarity between any two SAMTstyle syntactic categories (Table 2 gives some examples of similar and dissimilar category pairs). [sent-302, score-0.481]
</p><p>82 Then it can accurately compute the degree of matching between a test source syntactic category and a fuzzy-tree to exact-tree rule. [sent-303, score-0.913]
</p><p>83 However, the deep similarity matching algorithm  has two practical shortcomings. [sent-305, score-0.54]
</p><p>84 7  Related Work  Several studies have tried to incorporate source or target syntax into translation models in a fuzzy manner. [sent-318, score-1.035]
</p><p>85 They annotate the target side of each string-to-string rule using SAMT-style syntactic categories and aim to generate the output more syntactically. [sent-320, score-0.685]
</p><p>86 Thus they can be viewed as integrating target  syntax in a fuzzy manner. [sent-324, score-0.71]
</p><p>87 , 2010) apply fuzzy techniques for integrating source syntax into hierarchical phrasebased systems (Chiang, 2005, 2007). [sent-328, score-0.912]
</p><p>88 The first two studies employ 0-1 matching and the last tries deep similarity matching between two tag sequences. [sent-329, score-0.919]
</p><p>89 Furthermore, we apply fuzzy syntactic annotation on each rule’s source string and design three fuzzy rule matching algorithms. [sent-331, score-1.984]
</p><p>90 Chiang (2010) proposes a method for learning to translate with both source and target syntax in the framework of a hierarchical phrase-based system. [sent-332, score-0.493]
</p><p>91 This fuzzy use of source and target syntax enables the translation system to learn which tree labels are similar enough to be compatible, which ones are harmful to combine, and which ones can be ignored. [sent-335, score-1.136]
</p><p>92 8  Conclusion and Future Work  In this paper, we have proposed a new method for augmenting string-to-tree translation models with fuzzy use of the source syntax. [sent-337, score-0.836]
</p><p>93 We first applied a fuzzy annotation method which labels the source side of each string-to-tree rule with SAMT-style syntactic categories. [sent-338, score-1.16]
</p><p>94 Then we designed and explored three fuzzy rule matching algorithms: 0-1  matching, likelihood matching, and deep similarity matching. [sent-339, score-1.372]
</p><p>95 This substantial improvement verifies that our fuzzy use of source syntax is effective and can enhance the ability to choose proper translation rules during decoding while guaranteeing grammatical output with explicit target trees. [sent-341, score-1.308]
</p><p>96 213 Next, we plan to further study the likelihood fuzzy matching and deep similarity matching algorithms in order to fully exploit their potential. [sent-343, score-1.416]
</p><p>97 For example, we will combine the merits of 0-1 matching and likelihood matching so as to avoid the setting of parameter m in likelihood matching. [sent-344, score-0.852]
</p><p>98 We also plan to explore another direction: we will annotate the source side of each string-to-tree rule with subtrees or subtree sequences. [sent-345, score-0.612]
</p><p>99 We can then apply tree-kernel methods to compute a degree of matching between a rule and a test source subtree or subtree sequence. [sent-346, score-0.874]
</p><p>100 Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions. [sent-399, score-0.516]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fuzzy', 0.45), ('matching', 0.333), ('rule', 0.289), ('source', 0.211), ('vp', 0.197), ('category', 0.188), ('rules', 0.174), ('syntax', 0.171), ('chiang', 0.144), ('translation', 0.143), ('syntactic', 0.14), ('deep', 0.138), ('string', 0.111), ('pp', 0.102), ('likelihood', 0.093), ('vb', 0.092), ('deg', 0.091), ('cc', 0.084), ('categories', 0.084), ('np', 0.083), ('tree', 0.07), ('side', 0.07), ('similarity', 0.069), ('decoding', 0.069), ('derivation', 0.068), ('ghkm', 0.067), ('galley', 0.064), ('chinese', 0.061), ('target', 0.06), ('deepsim', 0.058), ('fragmentation', 0.058), ('lhs', 0.058), ('rhs', 0.058), ('spmt', 0.058), ('marton', 0.056), ('binarization', 0.056), ('frontier', 0.056), ('huang', 0.056), ('binarized', 0.053), ('pc', 0.053), ('venugopal', 0.053), ('dec', 0.053), ('hierarchical', 0.051), ('minimal', 0.049), ('zollmann', 0.048), ('dev', 0.048), ('preposition', 0.047), ('rn', 0.046), ('rs', 0.046), ('zhang', 0.046), ('tag', 0.046), ('terrorist', 0.046), ('hussein', 0.046), ('grammaticality', 0.046), ('liu', 0.045), ('knight', 0.044), ('yield', 0.043), ('kevin', 0.043), ('latent', 0.042), ('binarize', 0.042), ('happy', 0.042), ('annotate', 0.042), ('degree', 0.041), ('promising', 0.041), ('qun', 0.041), ('joshua', 0.04), ('marcu', 0.04), ('adjp', 0.039), ('dax', 0.039), ('decorate', 0.039), ('decoration', 0.039), ('punishes', 0.039), ('resnik', 0.038), ('compatibility', 0.038), ('lines', 0.036), ('sides', 0.036), ('conjunction', 0.035), ('element', 0.034), ('decorated', 0.034), ('italic', 0.034), ('fw', 0.033), ('augmenting', 0.032), ('partial', 0.031), ('system', 0.031), ('cowan', 0.03), ('verifies', 0.03), ('plex', 0.03), ('argd', 0.03), ('hui', 0.03), ('rh', 0.03), ('rj', 0.03), ('difficulties', 0.029), ('integrating', 0.029), ('pages', 0.029), ('jj', 0.028), ('discuss', 0.028), ('derivations', 0.028), ('converts', 0.028), ('borrow', 0.028), ('haizhou', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="20-tfidf-1" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>Author: Jiajun Zhang ; Feifei Zhai ; Chengqing Zong</p><p>Abstract: Due to its explicit modeling of the grammaticality of the output via target-side syntax, the string-to-tree model has been shown to be one of the most successful syntax-based translation models. However, a major limitation of this model is that it does not utilize any useful syntactic information on the source side. In this paper, we analyze the difficulties of incorporating source syntax in a string-totree model. We then propose a new way to use the source syntax in a fuzzy manner, both in source syntactic annotation and in rule matching. We further explore three algorithms in rule matching: 0-1 matching, likelihood matching, and deep similarity matching. Our method not only guarantees grammatical output with an explicit target tree, but also enables the system to choose the proper translation rules via fuzzy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1</p><p>2 0.22174098 <a title="20-tfidf-2" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>Author: Jun Xie ; Haitao Mi ; Qun Liu</p><p>Abstract: Dependency structure, as a first step towards semantics, is believed to be helpful to improve translation quality. However, previous works on dependency structure based models typically resort to insertion operations to complete translations, which make it difficult to specify ordering information in translation rules. In our model of this paper, we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents relations and the target side as strings. The head-dependents rules require only substitution operation, thus our model requires no heuristics or separate ordering models of the previous works to control the word order of translations. Large-scale experiments show that our model performs well on long distance reordering, and outperforms the state- of-the-art constituency-to-string model (+1.47 BLEU on average) and hierarchical phrasebased model (+0.46 BLEU on average) on two Chinese-English NIST test sets without resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models.</p><p>3 0.2012784 <a title="20-tfidf-3" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>Author: Xinyan Xiao ; Yang Liu ; Qun Liu ; Shouxun Lin</p><p>Abstract: Although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features, it is hard to scale up to large data due to decoding complexity. We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets.</p><p>4 0.18001924 <a title="20-tfidf-4" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>Author: Juri Ganitkevitch ; Chris Callison-Burch ; Courtney Napoles ; Benjamin Van Durme</p><p>Abstract: Previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora. However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems.</p><p>5 0.17179203 <a title="20-tfidf-5" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>Author: Yang Gao ; Philipp Koehn ; Alexandra Birch</p><p>Abstract: Long-distance reordering remains one of the biggest challenges facing machine translation. We derive soft constraints from the source dependency parsing to directly address the reordering problem for the hierarchical phrasebased model. Our approach significantly improves Chinese–English machine translation on a large-scale task by 0.84 BLEU points on average. Moreover, when we switch the tuning function from BLEU to the LRscore which promotes reordering, we observe total improvements of 1.21 BLEU, 1.30 LRscore and 3.36 TER over the baseline. On average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially effective for long-distance reodering.</p><p>6 0.14821976 <a title="20-tfidf-6" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>7 0.14754148 <a title="20-tfidf-7" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>8 0.11287665 <a title="20-tfidf-8" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<p>9 0.10554527 <a title="20-tfidf-9" href="./emnlp-2011-Simple_Effective_Decipherment_via_Combinatorial_Optimization.html">122 emnlp-2011-Simple Effective Decipherment via Combinatorial Optimization</a></p>
<p>10 0.10477787 <a title="20-tfidf-10" href="./emnlp-2011-Statistical_Machine_Translation_with_Local_Language_Models.html">125 emnlp-2011-Statistical Machine Translation with Local Language Models</a></p>
<p>11 0.10341901 <a title="20-tfidf-11" href="./emnlp-2011-Semi-supervised_CCG_Lexicon_Extension.html">121 emnlp-2011-Semi-supervised CCG Lexicon Extension</a></p>
<p>12 0.091848798 <a title="20-tfidf-12" href="./emnlp-2011-Minimum_Imputed-Risk%3A_Unsupervised_Discriminative_Training_for_Machine_Translation.html">93 emnlp-2011-Minimum Imputed-Risk: Unsupervised Discriminative Training for Machine Translation</a></p>
<p>13 0.09036389 <a title="20-tfidf-13" href="./emnlp-2011-Accurate_Parsing_with_Compact_Tree-Substitution_Grammars%3A_Double-DOP.html">16 emnlp-2011-Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP</a></p>
<p>14 0.090285487 <a title="20-tfidf-14" href="./emnlp-2011-A_Word_Reordering_Model_for_Improved_Machine_Translation.html">13 emnlp-2011-A Word Reordering Model for Improved Machine Translation</a></p>
<p>15 0.088683546 <a title="20-tfidf-15" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>16 0.087940894 <a title="20-tfidf-16" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>17 0.086953811 <a title="20-tfidf-17" href="./emnlp-2011-Heuristic_Search_for_Non-Bottom-Up_Tree_Structure_Prediction.html">65 emnlp-2011-Heuristic Search for Non-Bottom-Up Tree Structure Prediction</a></p>
<p>18 0.086751871 <a title="20-tfidf-18" href="./emnlp-2011-Syntax-Based_Grammaticality_Improvement_using_CCG_and_Guided_Search.html">132 emnlp-2011-Syntax-Based Grammaticality Improvement using CCG and Guided Search</a></p>
<p>19 0.08614596 <a title="20-tfidf-19" href="./emnlp-2011-Better_Evaluation_Metrics_Lead_to_Better_Machine_Translation.html">22 emnlp-2011-Better Evaluation Metrics Lead to Better Machine Translation</a></p>
<p>20 0.085843995 <a title="20-tfidf-20" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.302), (1, 0.189), (2, 0.076), (3, -0.121), (4, 0.002), (5, -0.125), (6, -0.064), (7, 0.051), (8, 0.114), (9, 0.005), (10, -0.004), (11, 0.215), (12, 0.012), (13, 0.029), (14, 0.064), (15, -0.144), (16, -0.06), (17, 0.048), (18, -0.093), (19, -0.018), (20, -0.008), (21, 0.168), (22, -0.071), (23, 0.074), (24, -0.157), (25, 0.057), (26, 0.026), (27, 0.052), (28, -0.115), (29, 0.106), (30, 0.005), (31, 0.118), (32, -0.053), (33, 0.015), (34, -0.016), (35, 0.034), (36, 0.016), (37, 0.004), (38, 0.114), (39, -0.128), (40, -0.046), (41, 0.043), (42, -0.077), (43, -0.106), (44, -0.06), (45, 0.002), (46, -0.048), (47, 0.022), (48, -0.028), (49, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98052067 <a title="20-lsi-1" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>Author: Jiajun Zhang ; Feifei Zhai ; Chengqing Zong</p><p>Abstract: Due to its explicit modeling of the grammaticality of the output via target-side syntax, the string-to-tree model has been shown to be one of the most successful syntax-based translation models. However, a major limitation of this model is that it does not utilize any useful syntactic information on the source side. In this paper, we analyze the difficulties of incorporating source syntax in a string-totree model. We then propose a new way to use the source syntax in a fuzzy manner, both in source syntactic annotation and in rule matching. We further explore three algorithms in rule matching: 0-1 matching, likelihood matching, and deep similarity matching. Our method not only guarantees grammatical output with an explicit target tree, but also enables the system to choose the proper translation rules via fuzzy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1</p><p>2 0.74504995 <a title="20-lsi-2" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>Author: Jun Xie ; Haitao Mi ; Qun Liu</p><p>Abstract: Dependency structure, as a first step towards semantics, is believed to be helpful to improve translation quality. However, previous works on dependency structure based models typically resort to insertion operations to complete translations, which make it difficult to specify ordering information in translation rules. In our model of this paper, we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents relations and the target side as strings. The head-dependents rules require only substitution operation, thus our model requires no heuristics or separate ordering models of the previous works to control the word order of translations. Large-scale experiments show that our model performs well on long distance reordering, and outperforms the state- of-the-art constituency-to-string model (+1.47 BLEU on average) and hierarchical phrasebased model (+0.46 BLEU on average) on two Chinese-English NIST test sets without resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models.</p><p>3 0.7135936 <a title="20-lsi-3" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>Author: Xinyan Xiao ; Yang Liu ; Qun Liu ; Shouxun Lin</p><p>Abstract: Although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features, it is hard to scale up to large data due to decoding complexity. We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets.</p><p>4 0.6210317 <a title="20-lsi-4" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>Author: Yang Gao ; Philipp Koehn ; Alexandra Birch</p><p>Abstract: Long-distance reordering remains one of the biggest challenges facing machine translation. We derive soft constraints from the source dependency parsing to directly address the reordering problem for the hierarchical phrasebased model. Our approach significantly improves Chinese–English machine translation on a large-scale task by 0.84 BLEU points on average. Moreover, when we switch the tuning function from BLEU to the LRscore which promotes reordering, we observe total improvements of 1.21 BLEU, 1.30 LRscore and 3.36 TER over the baseline. On average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially effective for long-distance reodering.</p><p>5 0.57055146 <a title="20-lsi-5" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>Author: Juri Ganitkevitch ; Chris Callison-Burch ; Courtney Napoles ; Benjamin Van Durme</p><p>Abstract: Previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora. However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems.</p><p>6 0.56743038 <a title="20-lsi-6" href="./emnlp-2011-Efficient_retrieval_of_tree_translation_examples_for_Syntax-Based_Machine_Translation.html">47 emnlp-2011-Efficient retrieval of tree translation examples for Syntax-Based Machine Translation</a></p>
<p>7 0.56081372 <a title="20-lsi-7" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>8 0.48792318 <a title="20-lsi-8" href="./emnlp-2011-Accurate_Parsing_with_Compact_Tree-Substitution_Grammars%3A_Double-DOP.html">16 emnlp-2011-Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP</a></p>
<p>9 0.48786259 <a title="20-lsi-9" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>10 0.46004337 <a title="20-lsi-10" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>11 0.4477554 <a title="20-lsi-11" href="./emnlp-2011-Heuristic_Search_for_Non-Bottom-Up_Tree_Structure_Prediction.html">65 emnlp-2011-Heuristic Search for Non-Bottom-Up Tree Structure Prediction</a></p>
<p>12 0.4297947 <a title="20-lsi-12" href="./emnlp-2011-Minimum_Imputed-Risk%3A_Unsupervised_Discriminative_Training_for_Machine_Translation.html">93 emnlp-2011-Minimum Imputed-Risk: Unsupervised Discriminative Training for Machine Translation</a></p>
<p>13 0.42696851 <a title="20-lsi-13" href="./emnlp-2011-Syntax-Based_Grammaticality_Improvement_using_CCG_and_Guided_Search.html">132 emnlp-2011-Syntax-Based Grammaticality Improvement using CCG and Guided Search</a></p>
<p>14 0.41130045 <a title="20-lsi-14" href="./emnlp-2011-Simple_Effective_Decipherment_via_Combinatorial_Optimization.html">122 emnlp-2011-Simple Effective Decipherment via Combinatorial Optimization</a></p>
<p>15 0.40155733 <a title="20-lsi-15" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<p>16 0.38617793 <a title="20-lsi-16" href="./emnlp-2011-Semi-supervised_CCG_Lexicon_Extension.html">121 emnlp-2011-Semi-supervised CCG Lexicon Extension</a></p>
<p>17 0.38294327 <a title="20-lsi-17" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<p>18 0.36343521 <a title="20-lsi-18" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>19 0.36175907 <a title="20-lsi-19" href="./emnlp-2011-Statistical_Machine_Translation_with_Local_Language_Models.html">125 emnlp-2011-Statistical Machine Translation with Local Language Models</a></p>
<p>20 0.3602429 <a title="20-lsi-20" href="./emnlp-2011-A_Word_Reordering_Model_for_Improved_Machine_Translation.html">13 emnlp-2011-A Word Reordering Model for Improved Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.133), (35, 0.242), (36, 0.028), (37, 0.029), (45, 0.041), (53, 0.03), (54, 0.043), (57, 0.012), (62, 0.022), (64, 0.043), (65, 0.024), (66, 0.053), (69, 0.024), (79, 0.079), (82, 0.018), (85, 0.015), (87, 0.016), (90, 0.019), (96, 0.043), (98, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8587895 <a title="20-lda-1" href="./emnlp-2011-Efficient_retrieval_of_tree_translation_examples_for_Syntax-Based_Machine_Translation.html">47 emnlp-2011-Efficient retrieval of tree translation examples for Syntax-Based Machine Translation</a></p>
<p>Author: Fabien Cromieres ; Sadao Kurohashi</p><p>Abstract: We propose an algorithm allowing to efficiently retrieve example treelets in a parsed tree database in order to allow on-the-fly extraction of syntactic translation rules. We also propose improvements of this algorithm allowing several kinds of flexible matchings.</p><p>same-paper 2 0.81359148 <a title="20-lda-2" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>Author: Jiajun Zhang ; Feifei Zhai ; Chengqing Zong</p><p>Abstract: Due to its explicit modeling of the grammaticality of the output via target-side syntax, the string-to-tree model has been shown to be one of the most successful syntax-based translation models. However, a major limitation of this model is that it does not utilize any useful syntactic information on the source side. In this paper, we analyze the difficulties of incorporating source syntax in a string-totree model. We then propose a new way to use the source syntax in a fuzzy manner, both in source syntactic annotation and in rule matching. We further explore three algorithms in rule matching: 0-1 matching, likelihood matching, and deep similarity matching. Our method not only guarantees grammatical output with an explicit target tree, but also enables the system to choose the proper translation rules via fuzzy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1</p><p>3 0.61797047 <a title="20-lda-3" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>Author: Juri Ganitkevitch ; Chris Callison-Burch ; Courtney Napoles ; Benjamin Van Durme</p><p>Abstract: Previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora. However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems.</p><p>4 0.59897423 <a title="20-lda-4" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Noah A. Smith</p><p>Abstract: We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.</p><p>5 0.5928095 <a title="20-lda-5" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>Author: Jun Xie ; Haitao Mi ; Qun Liu</p><p>Abstract: Dependency structure, as a first step towards semantics, is believed to be helpful to improve translation quality. However, previous works on dependency structure based models typically resort to insertion operations to complete translations, which make it difficult to specify ordering information in translation rules. In our model of this paper, we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents relations and the target side as strings. The head-dependents rules require only substitution operation, thus our model requires no heuristics or separate ordering models of the previous works to control the word order of translations. Large-scale experiments show that our model performs well on long distance reordering, and outperforms the state- of-the-art constituency-to-string model (+1.47 BLEU on average) and hierarchical phrasebased model (+0.46 BLEU on average) on two Chinese-English NIST test sets without resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models.</p><p>6 0.5923385 <a title="20-lda-6" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>7 0.5900842 <a title="20-lda-7" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>8 0.58319557 <a title="20-lda-8" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>9 0.58172798 <a title="20-lda-9" href="./emnlp-2011-Lexical_Generalization_in_CCG_Grammar_Induction_for_Semantic_Parsing.html">87 emnlp-2011-Lexical Generalization in CCG Grammar Induction for Semantic Parsing</a></p>
<p>10 0.58165616 <a title="20-lda-10" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>11 0.58093035 <a title="20-lda-11" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>12 0.58071399 <a title="20-lda-12" href="./emnlp-2011-Better_Evaluation_Metrics_Lead_to_Better_Machine_Translation.html">22 emnlp-2011-Better Evaluation Metrics Lead to Better Machine Translation</a></p>
<p>13 0.57913041 <a title="20-lda-13" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>14 0.57821691 <a title="20-lda-14" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<p>15 0.57689071 <a title="20-lda-15" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>16 0.57655847 <a title="20-lda-16" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>17 0.57516062 <a title="20-lda-17" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<p>18 0.57367158 <a title="20-lda-18" href="./emnlp-2011-Lateen_EM%3A_Unsupervised_Training_with_Multiple_Objectives%2C_Applied_to_Dependency_Grammar_Induction.html">79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</a></p>
<p>19 0.57204968 <a title="20-lda-19" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>20 0.56989449 <a title="20-lda-20" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
