<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-108" href="#">emnlp2011-108</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</h1>
<br/><p>Source: <a title="emnlp-2011-108-pdf" href="http://aclweb.org/anthology//D/D11/D11-1044.pdf">pdf</a></p><p>Author: Kevin Gimpel ; Noah A. Smith</p><p>Abstract: We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.</p><p>Reference: <a title="emnlp-2011-108-reference" href="../emnlp2011_reference/emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu ,na  Abstract We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). [sent-4, score-0.684]
</p><p>2 We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. [sent-6, score-0.692]
</p><p>3 For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. [sent-7, score-0.979]
</p><p>4 We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. [sent-9, score-0.357]
</p><p>5 We propose a model in which phrases are organized into a tree structure inspired by dependency 474 syntax. [sent-20, score-0.479]
</p><p>6 Instead of standard dependency trees in which words are vertices, our trees have phrases as vertices. [sent-21, score-0.487]
</p><p>7 We describe a simple heuristic to extract phrase dependencies from an aligned parallel cor-  pus parsed on the target side, and use them to compute target-side tree features. [sent-22, score-0.563]
</p><p>8 We define additional string-to-tree features and, if a source-side dependency parser is available, tree-to-tree features to capture properties of how phrase dependencies interact with reordering. [sent-23, score-0.832]
</p><p>9 The decoder involves generating a phrase lattice (Ueffing et al. [sent-26, score-0.613]
</p><p>10 , 2002) in a coarse pass using a phrase-based model, followed by lattice dependency parsing of the phrase lattice. [sent-27, score-1.129]
</p><p>11 This approach allows us to feasibly explore the combined search space of segmentations, phrase alignments, and target phrase dependency trees. [sent-28, score-0.877]
</p><p>12 We also describe experiments in which we replace supervised dependency parsers with unsupervised parsers, reporting promising results: using a supervised Chinese parser and a state-of-the-art unsupervised English parser provides our best results, giving an averaged gain of +0. [sent-32, score-0.58]
</p><p>13 2  Related Work  We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level. [sent-37, score-0.322]
</p><p>14 Aside from QG, there have been many efforts to use dependency syntax in machine translation. [sent-40, score-0.398]
</p><p>15 (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. [sent-42, score-0.975]
</p><p>16 (2008) presented an extension to Hiero (Chiang, 2005) in which rules have target-side dependency syntax and therefore enable the use of a dependency language model. [sent-44, score-0.655]
</p><p>17 More recently, researchers have sought the benefits of dependency syntax while preserving the advantages of phrase-based models, such as efficiency and coverage. [sent-45, score-0.351]
</p><p>18 Galley and Manning (2009) loosened standard assumptions about dependency parsing so that the efficient left-to-right decoding procedure of phrase-based translation could be retained while a dependency language model is incorporated. [sent-46, score-0.848]
</p><p>19 Carreras and Collins (2009) presented a string-todependency system that permits non-projective dependency trees (thereby allowing a larger space of translations) and use a rule extraction procedure that includes rules for every phrase in the phrase table. [sent-47, score-0.854]
</p><p>20 We take an additional step in this direction by  working with dependency grammars on the phrases themselves, thereby bringing together the structural components of phrase-based and dependency-based MT in a single model. [sent-48, score-0.445]
</p><p>21 Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al. [sent-57, score-0.499]
</p><p>22 We previously presented a word-based machine translation model based on a quasi-synchronous dependency  θ  grammar. [sent-59, score-0.421]
</p><p>23 Therefore, we use a dependency grammar in which the leaves are phrases rather than words. [sent-61, score-0.491]
</p><p>24 We define a phrase dependency grammar as a model p(φ, τφ |t) over the joint space of segmentations of a sen|tt)en ocvee rin ttohe phrases paancde dependency trees on the phrases. [sent-62, score-1.138]
</p><p>25 2 Phrase dependency grammars sis of the problem of intersecting phrase-based and hierarchical translation models, but do not provide experimental results. [sent-63, score-0.509]
</p><p>26 When used for translation modeling, they allow us to capture phenomena like local reordering and idiomatic translations within each phrase as well as long-distance re-  lationships among the phrases in a sentence. [sent-67, score-0.597]
</p><p>27 We then define a quasi-synchronous phrase dependency grammar (QPDG) as a conditional model p(t, γ, φ, τφ, a | s, τs) that induces a probabilistic monolingual phrase dependency grammar over sentences inspired by the source sentence and (lexical) dependency tree. [sent-68, score-1.664]
</p><p>28 The source and target sentences are segmented into phrases and the phrases are aligned in a one-to-one alignment. [sent-69, score-0.331]
</p><p>29 However, we never commit to a source phrase dependency tree, instead using a source lexical dependency tree output by a dependency parser, so our alignment variable a is a function from target tree nodes (phrases in φ) to source phrases in γ, which might not be source tree nodes. [sent-72, score-1.903]
</p><p>30 The features in our model may consider a large number of source phrase dependency trees as long as they are consistent with τs  . [sent-73, score-0.722]
</p><p>31 , 2007), including four phrase table probability features, a phrase penalty feature, an n-gram language model, a distortion cost, six lexicalized reordering features, and a word penalty feature. [sent-75, score-0.642]
</p><p>32 We now describe in detail the additional features 476  2 words in one of the phrases (dependencies in which one phrase is entirely punctuation are not shown). [sent-76, score-0.407]
</p><p>33 in our model that are used to score phrase dependency trees. [sent-78, score-0.558]
</p><p>34 , 2008; Galley and Manning, 2009), though unlike previous work our features model both the phrase segmentation and dependency structure. [sent-85, score-0.653]
</p><p>35 However, there do not currently exist treebanks with annotated phrase  “syn-  phrase “made up” for each direction, sorted by the conditional probability of the child phrase given the parent phrase and direction. [sent-87, score-1.09]
</p><p>36 Our solution is to use a standard supervised dependency parser and extract phrase dependencies using bilingual information. [sent-89, score-0.724]
</p><p>37 Given the set of extracted phrase pairs for a sentence, denote by W the set of unique target-side phrases among them. [sent-92, score-0.353]
</p><p>38 We parse the target sentence with a dependency parser and, for  each pair of phrases u, v ∈ W, we extract a phrase dependency (along sw uit,hv i t∈s direction) itrfa u a an dp v dsoe not overlap and there is at least one lexical dependency between a word in u and a word in v. [sent-93, score-1.463]
</p><p>39 If there are lexical dependencies in both directions, we extract a phrase dependency only for the single longest one. [sent-94, score-0.737]
</p><p>40 Since we use a projective dependency parser, the longest lexical dependency between two phrases is guaranteed to be unique. [sent-95, score-0.786]
</p><p>41 Table 2 shows a listing of the most frequent phrase dependencies extracted (lexical dependencies are omitted). [sent-96, score-0.454]
</p><p>42 We note that during training we never explicitly commit to any single phrase dependency tree for a target sentence. [sent-97, score-0.739]
</p><p>43 Rather, we extract phrase dependencies from all phrase dependency trees consistent with the word alignments and the lexical dependency tree. [sent-98, score-1.291]
</p><p>44 Thus we treat phrase dependency trees analogously to phrase segmentations in standard phrase extraction. [sent-99, score-1.155]
</p><p>45 (2009) used a shallow parser to convert lexical dependencies from a dependency parser into phrase dependencies. [sent-102, score-0.823]
</p><p>46 477 phrase dependencies of the form hu, v, di, where u risa tehe d hpeeandd phrase, v hise t fhoer mch hilud, phrase, haenrde d ∈ {left, right} is the direction, we then estimate dco ∈nd{i tlieonfta,lr probabilities p(v|u, d) using er enla etisvtiem fraetequency ensatilm praotiboanb. [sent-103, score-0.354]
</p><p>47 (1)  where d(i) = I[τφ(i) i> 0] is the direction of the dependency arc. [sent-109, score-0.304]
</p><p>48 4 The max expression protects unseen parent-child phrase dependencies from causing the score to be negative infinity. [sent-111, score-0.354]
</p><p>49 Our motivation is a desire for the features to be used to prefer one derivation over another but not to rule out a deriva−  tion completely if it merely happens to contain a dependency unobserved in the training data. [sent-112, score-0.398]
</p><p>50 Whenever we extract a phrase dependency, we extract the longest lexical dependency contained within it. [sent-115, score-0.637]
</p><p>51 For all hparent, child, directioni lexical dependency tuples hx, y, di, we desitriemctaioten ico lnedxii-tciaolna dle probabilities plex (y|x, d) ,f wrome e tshtiem parsed corpus using relative frequency de)st firmoamtio tnh. [sent-116, score-0.373]
</p><p>52 e Then, f coor a phrase dependency with longest lexical dependency hx, y, di, we add a feature for plex (y|x, d) to the model, using a fdor amu felaa suirmeil faorr t op Eq. [sent-117, score-1.015]
</p><p>53 dD)if tfoer tehnet instances of a phrase dependency may have different lexical dependencies extracted with them. [sent-119, score-0.691]
</p><p>54 We add the lexical weight for the most frequent, breaking ties by choosing the lexical dependency that maximizes p(y|x, d), as was also done by Koehn et al. [sent-120, score-0.37]
</p><p>55 In all, we include 4 target tree features: one for phrase dependencies, one for lexical dependencies, 4The reasoning here is that whenever we use a phrase dependency that we have observed in the training data, we want to boost the score of the translation. [sent-122, score-1.026]
</p><p>56 If we used log-probabilities, each observed dependency would incur a penalty. [sent-123, score-0.304]
</p><p>57 2 String-to-Tree Configurations We consider features that count instances of reordering configurations involving phrase dependencies. [sent-127, score-0.54]
</p><p>58 For example, when building a parent-child phrase dependency with the child to the left, one feature value is incremented if their aligned source-side phrases are in the same order. [sent-129, score-0.695]
</p><p>59 We begin with features for each of the quasi-synchronous configurations from Smith and Eisner (2006), adapted to phrase dependency grammars. [sent-134, score-0.752]
</p><p>60 6We actually include two versions of each configuration feature other than “root-root”: one for the source phrases being in the same order as the target phrases and one for them being swapped. [sent-140, score-0.476]
</p><p>61 Given a pair of source words, one with index j in source phrase a(τφ(i)) and the other with index k in source phrase a(i), we have a parentchild configuration if τs (k) = j; if τs (j) = k, a child-parent configuration is present. [sent-143, score-0.926]
</p><p>62 Therefore, only one configuration feature fires for each phrase dependency attachment. [sent-148, score-0.703]
</p><p>63 Finally, we include features that consider the dependency path distance between phrases in the source-side dependency tree that are aligned to parent-child pairs in τφ. [sent-149, score-0.912]
</p><p>64 We include a feature that sums, for each target phrase i, the inverse of the minimum undirected path length between each word in a(i) and each word in τφ(a(i)). [sent-150, score-0.507]
</p><p>65 The minimum undirected path length is defined as the number of dependency arcs that must be crossed to travel from one word to the other in τs. [sent-151, score-0.532]
</p><p>66 We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,τs and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-tofine strategy (Petrov, 2009) to speed up decoding. [sent-158, score-1.139]
</p><p>67 It has become common in recent years for MT researchers to exploit efficient data structures for encoding concise representations of the pruned search space of the model, such as phrase lattices for phrase-based MT (Ueffing et al. [sent-159, score-0.375]
</p><p>68 Each edge in a phrase lattice corresponds to a phrase pair and each path through the lattice corresponds to a tuple ht, γ, φ, ai for the input s. [sent-163, score-1.356]
</p><p>69 To also maximize over τφ, we perform lattice dependency parsing, which allows us to search over the space of tuples ht, γ, φ, a, τφi . [sent-166, score-0.663]
</p><p>70 Given the lattice and Gs,τs , tluapttilcees parsing ,isa a straightforward generalization of  the standard arc-factored dynamic programming algorithm from Eisner (1996). [sent-167, score-0.421]
</p><p>71 The lattice parsing algorithm requires O(E2V ) time and O(E2 + V E) space, where E is the number of edges in the lattice and V is the number of nodes. [sent-168, score-0.852]
</p><p>72 7 Typical phrase lattices might easily contain tens of thousands of nodes and edges, making exact search prohibitively expensive for all but the smallest lattices. [sent-169, score-0.375]
</p><p>73 Pass 1: Lattice Pruning After generating phrase lattices using a phrase-based MT system, we prune lattice edges using forward-backward pruning (Sixtus and Ortmanns, 1999), which has also been used in previous work using phrase lattices (Tromble et al. [sent-173, score-1.227]
</p><p>74 This pruning method computes the maxmarginal for each lattice edge, which is the score of the best full path that uses that edge. [sent-175, score-0.48]
</p><p>75 Max-marginals 7To prevent confusion, we use the term edge to refer to a phrase lattice edge and arc to refer to a parent-child dependency in the phrase dependency tree. [sent-176, score-1.585]
</p><p>76 479  offer the advantage that the best path in the lattice is preserved during pruning. [sent-177, score-0.434]
</p><p>77 Pass 2: Parent Ranking Given a pruned lattice, we then remove some candidate dependency arcs from consideration. [sent-181, score-0.382]
</p><p>78 It is common in dependency parsing to use a coarse model to rank the top k parents for each word, and to only consider these during parsing (Martins et al. [sent-182, score-0.531]
</p><p>79 Unlike string parsing, our phrase lattices impose several types of constraints on allowable arcs. [sent-184, score-0.375]
</p><p>80 For example, each node in the phrase lattice is annotated with a coverage vector—a bit vector indicating which words in the source sentence have been translated—which implies a topological ordering of the nodes. [sent-185, score-0.681]
</p><p>81 This algorithm also tells us whether each edge is reachable from each other edge, allowing us to immediately prune dependency arcs between edges that are unreachable from each other. [sent-187, score-0.509]
</p><p>82 In lattice parsing, however, most lattice edges will not be assigned any parent. [sent-190, score-0.79]
</p><p>83 Certain lattice edges are much more likely to be contained within paths, so we allow some edges to have more candidate parent edges than others. [sent-191, score-0.649]
</p><p>84 We introduce hyperparameters α, β, and to denote, respectively, the minimum, maximum, and average number of parent edges to be considered for each lattice edge (α ≤ ≤ β). [sent-192, score-0.56]
</p><p>85 scores (using the QPDG features and their weights ψ) and choose the top µE of these arcs while ensuring that each edge has at least α and at most β potential parent edges. [sent-194, score-0.315]
</p><p>86 We weight agenda items by the sum of their scores and the Floyd-Warshall best path scores both from the start node of the lattice to the beginning of the item and the end of the item to any final node. [sent-200, score-0.541]
</p><p>87 We first use MERT to train parameters for the coarse phrase-based model used to generate phrase lattices. [sent-212, score-0.357]
</p><p>88 We initialize λ to the default Moses feature weights and for ψ we initialize the two target phrase dependency weights to 0. [sent-215, score-0.769]
</p><p>89 We used this baseline Moses system to generate phrase lattices for our system, so our model includes all of the Moses features in addition to the  Q MP oD seG s(T )+S2T+ 2T)3 M43. [sent-235, score-0.429]
</p><p>90 We note that computing our features requires parsing the target (English) side of the parallel text, but not the  source side. [sent-255, score-0.321]
</p><p>91 An additional cluster was created for all other words; this allowed us to use phrase dependency cluster features even for out-ofvocabulary words. [sent-262, score-0.612]
</p><p>92 We used a max phrase length of 7 when extracting phrase dependencies to match the max phrase length used in phrase extraction. [sent-263, score-1.116]
</p><p>93 Approximately 87M unique phrase dependencies were extracted from the ZH-EN data and 7M from the UR-EN data. [sent-264, score-0.354]
</p><p>94 will enhance peace efforts after palestinian election us to boost peace efforts after palestinian elections : bush : us set to  boost  peace  : us to step up peace  Figure 2: (a) Moses translation output along with γ, φ, and a. [sent-300, score-1.034]
</p><p>95 An English gloss is shown above the Chinese sentence and above the gloss is shown the dependency parse from the Stanford parser. [sent-301, score-0.338]
</p><p>96 Fortunately, unsupervised dependency grammar induction has improved substantially in recent years due to a flurry of recent research. [sent-307, score-0.445]
</p><p>97 From the second set of features, we see that the model learns to favor producing dependency trees that are mostly isomorphic to the source tree, by favoring root-root and parent-child configurations at the expense of most others. [sent-352, score-0.554]
</p><p>98 Unsupervised induction of tree substitution grammars for dependency parsing. [sent-386, score-0.422]
</p><p>99 Concavity and initial-  ization for unsupervised dependency grammar induction. [sent-527, score-0.445]
</p><p>100 A new stringto-dependency machine translation algorithm with a target dependency language model. [sent-699, score-0.486]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lattice', 0.359), ('dependency', 0.304), ('qpdg', 0.276), ('phrase', 0.254), ('gimpel', 0.18), ('moses', 0.161), ('smith', 0.145), ('configurations', 0.14), ('qg', 0.126), ('chinese', 0.123), ('lattices', 0.121), ('translation', 0.117), ('bush', 0.116), ('configuration', 0.107), ('coarse', 0.103), ('dependencies', 0.1), ('bleu', 0.099), ('phrases', 0.099), ('reordering', 0.092), ('eisner', 0.09), ('palestinian', 0.09), ('grammar', 0.088), ('mert', 0.087), ('galley', 0.087), ('peace', 0.083), ('mt', 0.08), ('arcs', 0.078), ('tree', 0.076), ('path', 0.075), ('ht', 0.074), ('parent', 0.074), ('tuning', 0.073), ('koehn', 0.072), ('edges', 0.072), ('agenda', 0.072), ('czdeax', 0.069), ('election', 0.069), ('lfb', 0.069), ('source', 0.068), ('parser', 0.066), ('och', 0.065), ('target', 0.065), ('gs', 0.062), ('parsing', 0.062), ('decoding', 0.061), ('fine', 0.058), ('zollmann', 0.056), ('edge', 0.055), ('features', 0.054), ('weights', 0.054), ('ueffing', 0.054), ('unsupervised', 0.053), ('pass', 0.047), ('efforts', 0.047), ('deneefe', 0.047), ('segmentations', 0.047), ('dmv', 0.047), ('syntax', 0.047), ('longest', 0.046), ('pruning', 0.046), ('elatt', 0.046), ('elections', 0.046), ('greunneer', 0.046), ('imce', 0.046), ('intersecting', 0.046), ('sixtus', 0.046), ('stre', 0.046), ('turboparser', 0.046), ('martins', 0.044), ('tromble', 0.044), ('grammars', 0.042), ('distortion', 0.042), ('trees', 0.042), ('segmentation', 0.041), ('boost', 0.04), ('derivation', 0.04), ('urdu', 0.04), ('commit', 0.04), ('minimum', 0.039), ('prepositional', 0.039), ('side', 0.038), ('parsers', 0.038), ('shen', 0.038), ('feature', 0.038), ('english', 0.038), ('treebank', 0.037), ('brown', 0.036), ('undirected', 0.036), ('plex', 0.036), ('auli', 0.036), ('items', 0.035), ('quirk', 0.035), ('translations', 0.035), ('parse', 0.034), ('heuristic', 0.034), ('parallel', 0.034), ('lexical', 0.033), ('army', 0.033), ('bergsma', 0.033), ('levy', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="108-tfidf-1" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Noah A. Smith</p><p>Abstract: We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.</p><p>2 0.22144899 <a title="108-tfidf-2" href="./emnlp-2011-A_Fast_Re-scoring_Strategy_to_Capture_Long-Distance_Dependencies.html">5 emnlp-2011-A Fast Re-scoring Strategy to Capture Long-Distance Dependencies</a></p>
<p>Author: Anoop Deoras ; Tomas Mikolov ; Kenneth Church</p><p>Abstract: A re-scoring strategy is proposed that makes it feasible to capture more long-distance dependencies in the natural language. Two pass strategies have become popular in a number of recognition tasks such as ASR (automatic speech recognition), MT (machine translation) and OCR (optical character recognition). The first pass typically applies a weak language model (n-grams) to a lattice and the second pass applies a stronger language model to N best lists. The stronger language model is intended to capture more longdistance dependencies. The proposed method uses RNN-LM (recurrent neural network language model), which is a long span LM, to rescore word lattices in the second pass. A hill climbing method (iterative decoding) is proposed to search over islands of confusability in the word lattice. An evaluation based on Broadcast News shows speedups of 20 over basic N best re-scoring, and word error rate reduction of 8% (relative) on a highly competitive setup.</p><p>3 0.21592066 <a title="108-tfidf-3" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>Author: Jun Xie ; Haitao Mi ; Qun Liu</p><p>Abstract: Dependency structure, as a first step towards semantics, is believed to be helpful to improve translation quality. However, previous works on dependency structure based models typically resort to insertion operations to complete translations, which make it difficult to specify ordering information in translation rules. In our model of this paper, we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents relations and the target side as strings. The head-dependents rules require only substitution operation, thus our model requires no heuristics or separate ordering models of the previous works to control the word order of translations. Large-scale experiments show that our model performs well on long distance reordering, and outperforms the state- of-the-art constituency-to-string model (+1.47 BLEU on average) and hierarchical phrasebased model (+0.46 BLEU on average) on two Chinese-English NIST test sets without resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models.</p><p>4 0.20995505 <a title="108-tfidf-4" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>Author: Yang Gao ; Philipp Koehn ; Alexandra Birch</p><p>Abstract: Long-distance reordering remains one of the biggest challenges facing machine translation. We derive soft constraints from the source dependency parsing to directly address the reordering problem for the hierarchical phrasebased model. Our approach significantly improves Chinese–English machine translation on a large-scale task by 0.84 BLEU points on average. Moreover, when we switch the tuning function from BLEU to the LRscore which promotes reordering, we observe total improvements of 1.21 BLEU, 1.30 LRscore and 3.36 TER over the baseline. On average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially effective for long-distance reodering.</p><p>5 0.18012381 <a title="108-tfidf-5" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>Author: Jason Katz-Brown ; Slav Petrov ; Ryan McDonald ; Franz Och ; David Talbot ; Hiroshi Ichikawa ; Masakazu Seno ; Hideto Kazawa</p><p>Abstract: We propose a simple training regime that can improve the extrinsic performance of a parser, given only a corpus of sentences and a way to automatically evaluate the extrinsic quality of a candidate parse. We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system. We use a corpus of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress.</p><p>6 0.17179762 <a title="108-tfidf-6" href="./emnlp-2011-A_Fast%2C_Accurate%2C_Non-Projective%2C_Semantically-Enriched_Parser.html">4 emnlp-2011-A Fast, Accurate, Non-Projective, Semantically-Enriched Parser</a></p>
<p>7 0.16472979 <a title="108-tfidf-7" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<p>8 0.16286625 <a title="108-tfidf-8" href="./emnlp-2011-Multi-Source_Transfer_of_Delexicalized_Dependency_Parsers.html">95 emnlp-2011-Multi-Source Transfer of Delexicalized Dependency Parsers</a></p>
<p>9 0.16229418 <a title="108-tfidf-9" href="./emnlp-2011-Statistical_Machine_Translation_with_Local_Language_Models.html">125 emnlp-2011-Statistical Machine Translation with Local Language Models</a></p>
<p>10 0.15885092 <a title="108-tfidf-10" href="./emnlp-2011-A_Word_Reordering_Model_for_Improved_Machine_Translation.html">13 emnlp-2011-A Word Reordering Model for Improved Machine Translation</a></p>
<p>11 0.15211105 <a title="108-tfidf-11" href="./emnlp-2011-Joint_Models_for_Chinese_POS_Tagging_and_Dependency_Parsing.html">75 emnlp-2011-Joint Models for Chinese POS Tagging and Dependency Parsing</a></p>
<p>12 0.15108024 <a title="108-tfidf-12" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>13 0.14821976 <a title="108-tfidf-13" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>14 0.14731102 <a title="108-tfidf-14" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>15 0.14715932 <a title="108-tfidf-15" href="./emnlp-2011-Better_Evaluation_Metrics_Lead_to_Better_Machine_Translation.html">22 emnlp-2011-Better Evaluation Metrics Lead to Better Machine Translation</a></p>
<p>16 0.14509869 <a title="108-tfidf-16" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>17 0.14263982 <a title="108-tfidf-17" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>18 0.14058898 <a title="108-tfidf-18" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>19 0.13904485 <a title="108-tfidf-19" href="./emnlp-2011-Exact_Decoding_of_Phrase-Based_Translation_Models_through_Lagrangian_Relaxation.html">51 emnlp-2011-Exact Decoding of Phrase-Based Translation Models through Lagrangian Relaxation</a></p>
<p>20 0.13896364 <a title="108-tfidf-20" href="./emnlp-2011-Unsupervised_Structure_Prediction_with_Non-Parallel_Multilingual_Guidance.html">146 emnlp-2011-Unsupervised Structure Prediction with Non-Parallel Multilingual Guidance</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.41), (1, 0.299), (2, 0.09), (3, 0.042), (4, -0.022), (5, 0.01), (6, 0.075), (7, -0.076), (8, 0.033), (9, -0.021), (10, 0.037), (11, 0.116), (12, 0.091), (13, 0.024), (14, 0.047), (15, 0.081), (16, 0.054), (17, 0.004), (18, 0.127), (19, -0.002), (20, 0.069), (21, -0.023), (22, -0.077), (23, 0.005), (24, -0.081), (25, 0.068), (26, -0.052), (27, -0.052), (28, -0.044), (29, 0.028), (30, 0.044), (31, -0.077), (32, 0.097), (33, -0.014), (34, 0.117), (35, -0.077), (36, -0.018), (37, 0.007), (38, 0.107), (39, 0.013), (40, 0.041), (41, -0.007), (42, 0.138), (43, -0.006), (44, -0.085), (45, -0.184), (46, 0.058), (47, 0.077), (48, -0.048), (49, -0.08)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96303469 <a title="108-lsi-1" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Noah A. Smith</p><p>Abstract: We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.</p><p>2 0.73517299 <a title="108-lsi-2" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>Author: Jun Xie ; Haitao Mi ; Qun Liu</p><p>Abstract: Dependency structure, as a first step towards semantics, is believed to be helpful to improve translation quality. However, previous works on dependency structure based models typically resort to insertion operations to complete translations, which make it difficult to specify ordering information in translation rules. In our model of this paper, we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents relations and the target side as strings. The head-dependents rules require only substitution operation, thus our model requires no heuristics or separate ordering models of the previous works to control the word order of translations. Large-scale experiments show that our model performs well on long distance reordering, and outperforms the state- of-the-art constituency-to-string model (+1.47 BLEU on average) and hierarchical phrasebased model (+0.46 BLEU on average) on two Chinese-English NIST test sets without resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models.</p><p>3 0.67326474 <a title="108-lsi-3" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>Author: Yang Gao ; Philipp Koehn ; Alexandra Birch</p><p>Abstract: Long-distance reordering remains one of the biggest challenges facing machine translation. We derive soft constraints from the source dependency parsing to directly address the reordering problem for the hierarchical phrasebased model. Our approach significantly improves Chinese–English machine translation on a large-scale task by 0.84 BLEU points on average. Moreover, when we switch the tuning function from BLEU to the LRscore which promotes reordering, we observe total improvements of 1.21 BLEU, 1.30 LRscore and 3.36 TER over the baseline. On average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially effective for long-distance reodering.</p><p>4 0.63931292 <a title="108-lsi-4" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>Author: Kristian Woodsend ; Mirella Lapata</p><p>Abstract: Text simplification aims to rewrite text into simpler versions, and thus make information accessible to a broader audience. Most previous work simplifies sentences using handcrafted rules aimed at splitting long sentences, or substitutes difficult words using a predefined dictionary. This paper presents a datadriven model based on quasi-synchronous grammar, a formalism that can naturally capture structural mismatches and complex rewrite operations. We describe how such a grammar can be induced from Wikipedia and propose an integer linear programming model for selecting the most appropriate simplification from the space of possible rewrites generated by the grammar. We show experimentally that our method creates simplifications that significantly reduce the reading difficulty ofthe input, while maintaining grammaticality and preserving its meaning.</p><p>5 0.62413406 <a title="108-lsi-5" href="./emnlp-2011-A_Fast_Re-scoring_Strategy_to_Capture_Long-Distance_Dependencies.html">5 emnlp-2011-A Fast Re-scoring Strategy to Capture Long-Distance Dependencies</a></p>
<p>Author: Anoop Deoras ; Tomas Mikolov ; Kenneth Church</p><p>Abstract: A re-scoring strategy is proposed that makes it feasible to capture more long-distance dependencies in the natural language. Two pass strategies have become popular in a number of recognition tasks such as ASR (automatic speech recognition), MT (machine translation) and OCR (optical character recognition). The first pass typically applies a weak language model (n-grams) to a lattice and the second pass applies a stronger language model to N best lists. The stronger language model is intended to capture more longdistance dependencies. The proposed method uses RNN-LM (recurrent neural network language model), which is a long span LM, to rescore word lattices in the second pass. A hill climbing method (iterative decoding) is proposed to search over islands of confusability in the word lattice. An evaluation based on Broadcast News shows speedups of 20 over basic N best re-scoring, and word error rate reduction of 8% (relative) on a highly competitive setup.</p><p>6 0.6017561 <a title="108-lsi-6" href="./emnlp-2011-Heuristic_Search_for_Non-Bottom-Up_Tree_Structure_Prediction.html">65 emnlp-2011-Heuristic Search for Non-Bottom-Up Tree Structure Prediction</a></p>
<p>7 0.58903486 <a title="108-lsi-7" href="./emnlp-2011-Hierarchical_Phrase-based_Translation_Representations.html">66 emnlp-2011-Hierarchical Phrase-based Translation Representations</a></p>
<p>8 0.55738544 <a title="108-lsi-8" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>9 0.54258227 <a title="108-lsi-9" href="./emnlp-2011-A_Fast%2C_Accurate%2C_Non-Projective%2C_Semantically-Enriched_Parser.html">4 emnlp-2011-A Fast, Accurate, Non-Projective, Semantically-Enriched Parser</a></p>
<p>10 0.53851688 <a title="108-lsi-10" href="./emnlp-2011-Optimal_Search_for_Minimum_Error_Rate_Training.html">100 emnlp-2011-Optimal Search for Minimum Error Rate Training</a></p>
<p>11 0.53072244 <a title="108-lsi-11" href="./emnlp-2011-Multi-Source_Transfer_of_Delexicalized_Dependency_Parsers.html">95 emnlp-2011-Multi-Source Transfer of Delexicalized Dependency Parsers</a></p>
<p>12 0.52937895 <a title="108-lsi-12" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>13 0.51198637 <a title="108-lsi-13" href="./emnlp-2011-Exact_Decoding_of_Phrase-Based_Translation_Models_through_Lagrangian_Relaxation.html">51 emnlp-2011-Exact Decoding of Phrase-Based Translation Models through Lagrangian Relaxation</a></p>
<p>14 0.50924337 <a title="108-lsi-14" href="./emnlp-2011-Parse_Correction_with_Specialized_Models_for_Difficult_Attachment_Types.html">102 emnlp-2011-Parse Correction with Specialized Models for Difficult Attachment Types</a></p>
<p>15 0.5049454 <a title="108-lsi-15" href="./emnlp-2011-Inducing_Sentence_Structure_from_Parallel_Corpora_for_Reordering.html">74 emnlp-2011-Inducing Sentence Structure from Parallel Corpora for Reordering</a></p>
<p>16 0.50241041 <a title="108-lsi-16" href="./emnlp-2011-Minimum_Imputed-Risk%3A_Unsupervised_Discriminative_Training_for_Machine_Translation.html">93 emnlp-2011-Minimum Imputed-Risk: Unsupervised Discriminative Training for Machine Translation</a></p>
<p>17 0.49830171 <a title="108-lsi-17" href="./emnlp-2011-Joint_Models_for_Chinese_POS_Tagging_and_Dependency_Parsing.html">75 emnlp-2011-Joint Models for Chinese POS Tagging and Dependency Parsing</a></p>
<p>18 0.49277046 <a title="108-lsi-18" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>19 0.49059439 <a title="108-lsi-19" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>20 0.48428947 <a title="108-lsi-20" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(15, 0.011), (19, 0.125), (23, 0.12), (35, 0.013), (36, 0.031), (37, 0.056), (45, 0.051), (53, 0.04), (54, 0.036), (57, 0.013), (62, 0.033), (64, 0.067), (66, 0.04), (69, 0.04), (79, 0.053), (82, 0.03), (85, 0.02), (87, 0.015), (90, 0.021), (96, 0.074), (98, 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88371021 <a title="108-lda-1" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>Author: Marco Dinarelli ; Sophie Rosset</p><p>Abstract: Reranking models have been successfully applied to many tasks of Natural Language Processing. However, there are two aspects of this approach that need a deeper investigation: (i) Assessment of hypotheses generated for reranking at classification phase: baseline models generate a list of hypotheses and these are used for reranking without any assessment; (ii) Detection of cases where reranking models provide a worst result: the best hypothesis provided by the reranking model is assumed to be always the best result. In some cases the reranking model provides an incorrect hypothesis while the baseline best hypothesis is correct, especially when baseline models are accurate. In this paper we propose solutions for these two aspects: (i) a semantic inconsistency metric to select possibly more correct n-best hypotheses, from a large set generated by an SLU basiline model. The selected hypotheses are reranked applying a state-of-the-art model based on Partial Tree Kernels, which encode SLU hypotheses in Support Vector Machines with complex structured features; (ii) finally, we apply a decision strategy, based on confidence values, to select the final hypothesis between the first ranked hypothesis provided by the baseline SLU model and the first ranked hypothesis provided by the re-ranker. We show the effectiveness of these solutions presenting comparative results obtained reranking hypotheses generated by a very accurate Conditional Random Field model. We evaluate our approach on the French MEDIA corpus. The results show significant improvements with respect to current state-of-the-art and previous 1104 Sophie Rosset LIMSI-CNRS B.P. 133, 91403 Orsay Cedex France ro s set @ l ims i fr . re-ranking models.</p><p>same-paper 2 0.85067689 <a title="108-lda-2" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Noah A. Smith</p><p>Abstract: We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.</p><p>3 0.78370231 <a title="108-lda-3" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<p>Author: Sebastian Riedel ; Andrew McCallum</p><p>Abstract: Extracting biomedical events from literature has attracted much recent attention. The bestperforming systems so far have been pipelines of simple subtask-specific local classifiers. A natural drawback of such approaches are cascading errors introduced in early stages of the pipeline. We present three joint models of increasing complexity designed to overcome this problem. The first model performs joint trigger and argument extraction, and lends itself to a simple, efficient and exact inference algorithm. The second model captures correlations between events, while the third model ensures consistency between arguments of the same event. Inference in these models is kept tractable through dual decomposition. The first two models outperform the previous best joint approaches and are very competitive with respect to the current state-of-theart. The third model yields the best results reported so far on the BioNLP 2009 shared task, the BioNLP 2011 Genia task and the BioNLP 2011Infectious Diseases task.</p><p>4 0.77009493 <a title="108-lda-4" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>Author: Yang Gao ; Philipp Koehn ; Alexandra Birch</p><p>Abstract: Long-distance reordering remains one of the biggest challenges facing machine translation. We derive soft constraints from the source dependency parsing to directly address the reordering problem for the hierarchical phrasebased model. Our approach significantly improves Chinese–English machine translation on a large-scale task by 0.84 BLEU points on average. Moreover, when we switch the tuning function from BLEU to the LRscore which promotes reordering, we observe total improvements of 1.21 BLEU, 1.30 LRscore and 3.36 TER over the baseline. On average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially effective for long-distance reodering.</p><p>5 0.7681132 <a title="108-lda-5" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>Author: Jiajun Zhang ; Feifei Zhai ; Chengqing Zong</p><p>Abstract: Due to its explicit modeling of the grammaticality of the output via target-side syntax, the string-to-tree model has been shown to be one of the most successful syntax-based translation models. However, a major limitation of this model is that it does not utilize any useful syntactic information on the source side. In this paper, we analyze the difficulties of incorporating source syntax in a string-totree model. We then propose a new way to use the source syntax in a fuzzy manner, both in source syntactic annotation and in rule matching. We further explore three algorithms in rule matching: 0-1 matching, likelihood matching, and deep similarity matching. Our method not only guarantees grammatical output with an explicit target tree, but also enables the system to choose the proper translation rules via fuzzy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1</p><p>6 0.76395512 <a title="108-lda-6" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>7 0.76230407 <a title="108-lda-7" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>8 0.75200593 <a title="108-lda-8" href="./emnlp-2011-Hierarchical_Phrase-based_Translation_Representations.html">66 emnlp-2011-Hierarchical Phrase-based Translation Representations</a></p>
<p>9 0.74706984 <a title="108-lda-9" href="./emnlp-2011-Third-order_Variational_Reranking_on_Packed-Shared_Dependency_Forests.html">134 emnlp-2011-Third-order Variational Reranking on Packed-Shared Dependency Forests</a></p>
<p>10 0.74473685 <a title="108-lda-10" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>11 0.7431919 <a title="108-lda-11" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>12 0.74128246 <a title="108-lda-12" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>13 0.73958927 <a title="108-lda-13" href="./emnlp-2011-A_Word_Reordering_Model_for_Improved_Machine_Translation.html">13 emnlp-2011-A Word Reordering Model for Improved Machine Translation</a></p>
<p>14 0.73917377 <a title="108-lda-14" href="./emnlp-2011-Lateen_EM%3A_Unsupervised_Training_with_Multiple_Objectives%2C_Applied_to_Dependency_Grammar_Induction.html">79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</a></p>
<p>15 0.7385422 <a title="108-lda-15" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>16 0.73801708 <a title="108-lda-16" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>17 0.73652244 <a title="108-lda-17" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>18 0.73511243 <a title="108-lda-18" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>19 0.73482376 <a title="108-lda-19" href="./emnlp-2011-Joint_Models_for_Chinese_POS_Tagging_and_Dependency_Parsing.html">75 emnlp-2011-Joint Models for Chinese POS Tagging and Dependency Parsing</a></p>
<p>20 0.73181325 <a title="108-lda-20" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
