<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-15" href="#">emnlp2011-15</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</h1>
<br/><p>Source: <a title="emnlp-2011-15-pdf" href="http://aclweb.org/anthology//D/D11/D11-1020.pdf">pdf</a></p><p>Author: Jun Xie ; Haitao Mi ; Qun Liu</p><p>Abstract: Dependency structure, as a first step towards semantics, is believed to be helpful to improve translation quality. However, previous works on dependency structure based models typically resort to insertion operations to complete translations, which make it difficult to specify ordering information in translation rules. In our model of this paper, we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents relations and the target side as strings. The head-dependents rules require only substitution operation, thus our model requires no heuristics or separate ordering models of the previous works to control the word order of translations. Large-scale experiments show that our model performs well on long distance reordering, and outperforms the state- of-the-art constituency-to-string model (+1.47 BLEU on average) and hierarchical phrasebased model (+0.46 BLEU on average) on two Chinese-English NIST test sets without resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models.</p><p>Reference: <a title="emnlp-2011-15-reference" href="../emnlp2011_reference/emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 However, previous works on dependency structure based models typically resort to insertion operations to complete translations, which make it difficult to specify ordering information in translation rules. [sent-6, score-0.654]
</p><p>2 In our model of this paper, we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents relations and the target side as strings. [sent-7, score-0.671]
</p><p>3 The head-dependents rules require only substitution operation, thus our model requires no heuristics or separate ordering models of the previous works to control the word order of translations. [sent-8, score-0.307]
</p><p>4 For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models. [sent-12, score-0.54]
</p><p>5 Those attractive characteristics make it pos216 sible to improve translation quality by using dependency structures. [sent-15, score-0.347]
</p><p>6 Some researchers pay more attention to use dependency structure on the target side. [sent-16, score-0.397]
</p><p>7 , 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and em-  ploys a dependency language model to make the output more grammatically. [sent-18, score-0.911]
</p><p>8 Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. [sent-22, score-0.494]
</p><p>9 Conventional dependency structure based models (Lin, 2004; Quirk et al. [sent-25, score-0.305]
</p><p>10 , 2007) typically employ both substitution and insertion operation to complete translations, which make it difficult to specify ordering information directly in the translation rules. [sent-27, score-0.36]
</p><p>11 In this paper, we handle this problem by directly specifying the ordering information in headdependents rules that represent the source side as head-dependents relations and the target side as  string. [sent-31, score-0.834]
</p><p>12 For the first time, a source dependency tree based model catches up with and surpasses the stateof-the-art translation models. [sent-41, score-0.474]
</p><p>13 1 Dependency Sturcture A dependency structure for a sentence is a directed acyclic graph with words as nodes and modification relations as edges. [sent-43, score-0.541]
</p><p>14 Figure 1 (a) shows an example dependency structure of a Chinese sentence. [sent-45, score-0.305]
</p><p>15 For convenience, we use the lexicon dependency grammar (Hellwig, 2006) which adopts a bracket representation to express a projective dependency structure. [sent-47, score-0.525]
</p><p>16 The dependency structure of Figure 1 (a) can be expressed as: ((2010年) (FIFA) 世界杯) (在(南非)) (成功) 举行 where the lexicon in brackets represents the depen-  dents, while the lexicon out the brackets is the head. [sent-48, score-0.305]
</p><p>17 To construct the dependency structure of a sentence, the most important thing is to establish dependency relations and distinguish the head from the dependent. [sent-49, score-0.811]
</p><p>18 ssuucccceessssffuull yly  Figure 1: Examples of dependency structure (a), headdependents relation (b), head-dependents rule (r1 of Figure 2) and head rule (d). [sent-97, score-1.163]
</p><p>19 Hudson, 1990) for identifying a syntactic relation between a head and a dependent between a headdependent pair: 1. [sent-101, score-0.312]
</p><p>20 2 Head-Dependents Relation A head-dependents relation is composed of a head and all its dependents as shown in Figure 1(b). [sent-105, score-0.309]
</p><p>21 Since all the head-dependent pairs satisfy criteria 1 and 2, we can deduce that a head-dependents relation L holds the property that the head deter-  mines the syntactic and semantic categories of L, and can often replace L. [sent-106, score-0.347]
</p><p>22 Therefore, we can recursively replace the bottom level head-dependent relations of a dependency structure with their heads until the root. [sent-107, score-0.411]
</p><p>23 This implies an representation of the generation of a dependency structure on the basis of head-dependents relation. [sent-108, score-0.305]
</p><p>24 The former is an example of head-dependent rules that represent the source side as head-dependents relations and act as both translation rules and reordering rules. [sent-111, score-0.639]
</p><p>25 The latter is an example of head rules which are used for translating words. [sent-112, score-0.352]
</p><p>26 Formally, a dependency-to-string grammar is defined as a tuple ⟨Σ, N, ∆, R⟩, where Σ is a set of source language terminals, RN⟩, ,i sw a esreet oΣf categories for the terminals in Σ , ∆ is a set of target language terminals, and R is a set of translation rules. [sent-113, score-0.331]
</p><p>27 A rule  r in R is a tuple ⟨t, s, ϕ⟩, where: - t is a node labeled by terminal from Σ; or a head-dependents relation of the source dependency structures, with each node labeled by a terminal from Σ or a variable from a set X = {x1, x2 , . [sent-114, score-0.75]
</p><p>28 For example, the head-dependents rule shown in Figure 1 (c) can be formalized as:  t = ((x1:世界杯) (x2:在) (x3:AD) 举行)  s = x1 was held x3 ϕ = {x1:世界杯 ↔  x2  , x2:在 ↔  x1  x2,  x3:AD ↔ x3}  where the underline indicates a leaf node, and xi:letters indicates a pair of variable and its constraint. [sent-118, score-0.379]
</p><p>29 A derivation is informally defined as a sequence of steps converting a source dependency structure into a target language string, with each step apply-  ing one translation rule. [sent-119, score-0.629]
</p><p>30 / NNRR  (f)  22001100 F FIIFFAA WWoorrldld  CCuupp  wwaass hh eeldld s suucccceessssffuull yly i nn ? [sent-190, score-0.371]
</p><p>31 South Africa (g)  22001100 F FIIFFAA WWoorrldld CCuupp wwaass hh eeldld s suucccceessssffuull yly i nn [ SSoouutthh A Affrricicaa]]  Figure 2: An example derivation of dependency-to-string translation. [sent-195, score-0.411]
</p><p>32 CH  2010年  FIFA 世界杯  在 南非 成功 举行  EN 2010 FIFA World Cup was held successfully in  South Africa The Chinese sentence (a) is first parsed into a dependency structure (b), which is converted into an English string in five steps. [sent-198, score-0.475]
</p><p>33 First, at the root node, we apply head-dependents rule r1 shown in Figure 1(c) to translate the top level head-dependents relation and result in three unfinished substructures and target string in (c). [sent-199, score-0.38]
</p><p>34 Second, we use head rule r2 translating “成功” into “successfully” and reach situation (d). [sent-201, score-0.42]
</p><p>35 Third, we apply headdependents rule r3 translating the head-dependents relation rooted at “世界杯” and yield (e). [sent-202, score-0.52]
</p><p>36 Finally, we apply head rule r5 translating the residual node “南非” and obtain the final translation in (g). [sent-204, score-0.628]
</p><p>37 4  Rule Acquisition  The rule acquisition begins with a word-aligned cor-  pus: a set of triples ⟨T, S, A⟩, where T is a source dependency structure, ,SS ,isA a target si Tde i sentence, and A is an alignment relation between T and S. [sent-205, score-0.688]
</p><p>38 We extract from each triple ⟨T, S, A⟩ head rules that are ceoxntrsaicstte frnot mw eitahc thh ter wpleor ⟨dT alignments da rnudl hse tahda-t dependents rules that satisfy the intuition that syntactically close items tend to stay close across languages. [sent-206, score-0.456]
</p><p>39 We accomplish the rule acquisition through three steps: tree annotation, head-dependents fragments identification and rule induction. [sent-207, score-0.435]
</p><p>40 1 Tree Annotation Given a triple ⟨T, S, A⟩ as shown in Figure 3, we fGirivste annn ao ttraiptel eea ⟨cTh, nSo,Ade⟩ n so fs hTo wwnith in ntw Foig aurtterib 3,ut ewse: head span and dependency span, which are defined as follows. [sent-209, score-0.531]
</p><p>41 Given a node n, its head span hsp(n) is a set of index of the target words aligned to n. [sent-211, score-0.484]
</p><p>42 A head span hsp(n) is consistent if it satisfies the following property: ∀n′̸=nhsp(n′) ∩ hsp(n) = ∅. [sent-214, score-0.292]
</p><p>43 Each node is annotated with two spans, the former is head span and the latter dependency span. [sent-216, score-0.631]
</p><p>44 The nodes in acceptable head set are displayed in gray, and the nodes in acceptable dependent set are denoted by boxes. [sent-217, score-1.015]
</p><p>45 Given a head span hsp(n), its closure cloz(hsp(n)) is the smallest contiguous head span that is a superset of hsp(n). [sent-221, score-0.584]
</p><p>46 Given a subtree rooted at n, the dependency span dsp(n) of n is defined as:  T′  ∪ hsp(n′)). [sent-225, score-0.481]
</p><p>47 hsp(n′)n i′∪s∈ cTo′nsistent If the head spans of all the nodes of T′ is not consisdsp(n) = cloz(  tent, dsp(n) = ∅. [sent-226, score-0.377]
</p><p>48 The extraction of head rules from each node can be readily achieved with the same criteria as (Och and Ney, 2004). [sent-229, score-0.407]
</p><p>49 2 Head-Dependents Fragments Identification We then identify the head-dependents fragments that are suitable for rule induction from the annotated dependency structure. [sent-232, score-0.458]
</p><p>50 To facilitate the identification process, we first define two sets of dependency structure related to head spans and dependency spans. [sent-233, score-0.804]
</p><p>51 A acceptable head set ahs(T) of a dependency structure T is a set of nodes, each of which has a consistent head span. [sent-235, score-0.978]
</p><p>52 For example, the elements of the acceptable head set of the dependency structure in Figure 3 are displayed in gray. [sent-236, score-0.773]
</p><p>53 A acceptable dependent set adt(T) of a dependency structure T is a set of nodes, each of which satisfies: dep(n) ∅. [sent-238, score-0.618]
</p><p>54 For example, the elements of the acceptable dependent set of the dependency structure in Figure 3 are denoted by boxes. [sent-239, score-0.618]
</p><p>55 We say a head-dependents fragments is acceptable if it satisfies the following properties:  =  1. [sent-241, score-0.312]
</p><p>56 An acceptable head-dependents fragment holds the property that the head span ofthe root and the dependency spans ofthe sinks do not overlap with each other, which enables us to determine the reordering in the target side. [sent-244, score-1.234]
</p><p>57 The identification of acceptable head-dependents fragments can be achieved by a single preorder transversal of the annotated dependency structure. [sent-245, score-0.597]
</p><p>58 For each accessed internal node n, we check whether the head-dependents fragment f rooted at n is acceptable. [sent-246, score-0.367]
</p><p>59 Typically, each acceptable head-dependents fragment has three types of nodes: internal nodes, internal nodes of the dependency structure; leaf nodes, leaf nodes of the dependency structure; head node, a special internal node acting as the head of the related head-dependents relation. [sent-248, score-2.023]
</p><p>60 x1 held successfully x2 Figure 4: A lexicalized head-dependents rule (b) induced from the only acceptable head-dependents fragment (a)  of Figure 3. [sent-272, score-0.756]
</p><p>61 3 Rule Induction From each acceptable head-dependents fragment, we induce a set of lexicalized and unlexicalized head-dependents rules. [sent-274, score-0.448]
</p><p>62 1 Lexicalized Rule We induce a lexicalized head-dependents rule from an acceptable head-dependents fragment by the following procedure: 1. [sent-277, score-0.647]
</p><p>63 extract the head-dependents relation and mark the internal nodes as substitution sites. [sent-278, score-0.334]
</p><p>64 place the nodes in order according to the head span of the root and the dependency spans of the sinks, then replace the internal nodes with variables and the other nodes with the target words covered by their head spans. [sent-280, score-1.354]
</p><p>65 Figure 4 shows an acceptable head-dependents fragment and a lexicalized head-dependents rule induced from it. [sent-282, score-0.647]
</p><p>66 To alleviate this problem, we generalize the lexicalized headdependents rules and induce rules with unlexicalized nodes. [sent-286, score-0.617]
</p><p>67 As we know, the modification relation of a headdependents relation is determined by the edges. [sent-287, score-0.334]
</p><p>68 POS) and obtain new head-dependents relations with unlexicalized nodes holding the same modification relation. [sent-290, score-0.313]
</p><p>69 Here we call the lexicalized and unlexicalized head-dependents relations as instances of the modification relation. [sent-291, score-0.304]
</p><p>70 Based on this observation, from each lexicalized head-dependent rule, we generate new headdependents rules with unlexicalized nodes according  to the following principles: 1. [sent-295, score-0.567]
</p><p>71 change the aligned part of the target string into a new variable when turning a head node or a leaf node into its category; 2. [sent-296, score-0.66]
</p><p>72 keep the target side unchanged when turning a internal node into its category. [sent-297, score-0.373]
</p><p>73 3 Unaligned Words We handle the unaligned words of the target side by extending the head spans of the lexicalized head and leaf nodes on both left and right directions. [sent-350, score-1.041]
</p><p>74 Let d be a derivation that convert  a source dependency structure T into a target string e. [sent-358, score-0.582]
</p><p>75 In our experiments ofthis paper, we used seven features as follows: - translation probabilities P(t|s) and P(s|t); - lexical translation probabilities Plex(t|s) and  Plex(s|t); - rule penalty exp(− 1) ; 222 - language model Plm(e) ; - word penalty exp( |e| ). [sent-360, score-0.386]
</p><p>76 It finds the best derivation d∗ that convert the input dependency structure into a target string among all possible derivations D:  d∗ = argmaxd∈DP(D)  (2)  Given a source dependency structure T, the decoder transverses T in post-order. [sent-362, score-0.887]
</p><p>77 For each accessed internal node n, it enumerates all instances of the related modification relation of the head-dependents relation rooted at n, and checks the rule set for matched translation rules. [sent-363, score-0.71]
</p><p>78 For example, suppose that we can not find any translation rule about to “(2010年) (FIFA) 界杯”, we will construct a pseudo translation rule “(x1:2010年) (x2:FIFA) x3:世界杯 → x1 x2 x3”. [sent-365, score-0.556]
</p><p>79 A larger translation is generated by substituting the variables in the target side of a translation rule with the translations of the corresponding dependents. [sent-366, score-0.653]
</p><p>80 We parse the source sentences with Stanford Parser (Klein and Manning, 2003) into projective dependency structures, whose nodes are annotated  by POS tags and edges by typed dependencies. [sent-378, score-0.44]
</p><p>81 Figure 6 shows two translations of our test sets MT04 and MT05, which are selected because each holds a long distance dependency commonly used in Chinese. [sent-413, score-0.463]
</p><p>82 1373(2001) and help developing countries strength counter-terrorism capability building for the efforts,  Hiero-re: China appreciates Anti - Terrorism Committee to promote countries implement resolution No . [sent-437, score-0.388]
</p><p>83 In the second example, the Chinese input holds a long distance dependency “中 国 赞 赏 . [sent-481, score-0.393]
</p><p>84 Cons2str and hiero-re fail to capture this long distance dependency and provide monotonic translations which do not reflect the meaning of the source sentence. [sent-486, score-0.506]
</p><p>85 In contrast, dep2str successfully captures this long distance dependency and translates it into “China appreciates efforts of 224 . [sent-487, score-0.607]
</p><p>86 And the incapability of cons2str and hiero-re in handling long distance reordering of these sentences does not lie in the representation of translation rules but the compromises in rule extraction or decoding so as to bal-  ance the speed or grammar size and performance. [sent-499, score-0.616]
</p><p>87 Both strategies limit the ability of the models in processing long distance reordering of sentences with long and complex modification relations. [sent-504, score-0.304]
</p><p>88 Both models require projection of the source dependency structure to the target side via word alignment, and thus can not handle non-isomorphism between languages. [sent-513, score-0.586]
</p><p>89 , 2007) presents a dependency treelet string correspondence model which directly map a dependency structure to a target string. [sent-515, score-0.749]
</p><p>90 (Ding and Palmer, 2005) presents a translation model based on Synchronous Dependency Insertion Grammar(SDIG), which handles some of the non-isomorphism but requires both source and target dependency structures. [sent-516, score-0.523]
</p><p>91 Most important, all these works do not specify the ordering information directly in translation rules, and resort to either heuristics (Lin, 2004; Xiong et al. [sent-517, score-0.302]
</p><p>92 , 2008) exploits target dependency structures as dependency language models to ensure the grammaticality of the target string. [sent-522, score-0.706]
</p><p>93 , 225 2008) extends the hierarchical phrase-based model and present a string-to-dependency model, which employs string-to-dependency rules whose source side are string and the target as well-formed dependency structures. [sent-524, score-0.749]
</p><p>94 In contrast, our model exploits source dependency structures, as a tree-based system, it run much faster (linear time vs. [sent-525, score-0.323]
</p><p>95 9  Conclusions and future work  In this paper, we present a novel dependency-tostring model, which employs head-dependents rules that represent the source side as head-dependents relations and the target side as string. [sent-528, score-0.55]
</p><p>96 The headdependents rules specify the ordering information directly and require only substitution operation. [sent-529, score-0.47]
</p><p>97 Large scale experiments show that our model exhibits good performance in long distance reordering and outperforms the state-ofthe-art constituency-to-string model and hierarchical phrase-based model without resort to phrases and parse forest. [sent-531, score-0.328]
</p><p>98 In our future works, we will exploit the semantic information encoded in the dependency structures which is expected to further improve the translations, and replace 1-best dependency structures with dependency forests so as to alleviate the influence caused by parse errors. [sent-533, score-0.914]
</p><p>99 A new string-to-dependency machine translation al-  gorithm with a target dependency language model. [sent-622, score-0.439]
</p><p>100 A dependency treelet string correspondence model for statistical machine translation. [sent-631, score-0.352]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hsp', 0.325), ('acceptable', 0.263), ('dependency', 0.239), ('head', 0.205), ('rule', 0.17), ('headdependents', 0.163), ('fifa', 0.16), ('appreciates', 0.139), ('ad', 0.137), ('ordering', 0.121), ('nodes', 0.117), ('barnier', 0.116), ('xiong', 0.11), ('translation', 0.108), ('lexicalized', 0.108), ('fragment', 0.106), ('side', 0.105), ('rules', 0.102), ('leaf', 0.102), ('node', 0.1), ('africa', 0.093), ('dsp', 0.093), ('wwaass', 0.093), ('yly', 0.093), ('target', 0.092), ('powell', 0.091), ('span', 0.087), ('rooted', 0.085), ('countries', 0.084), ('substitution', 0.084), ('source', 0.084), ('quirk', 0.082), ('ding', 0.078), ('unlexicalized', 0.077), ('reordering', 0.076), ('internal', 0.076), ('resort', 0.073), ('committee', 0.073), ('subtree', 0.07), ('translations', 0.07), ('cloz', 0.07), ('eeldld', 0.07), ('fiiffaa', 0.07), ('sinks', 0.07), ('suucccceessssffuull', 0.07), ('china', 0.069), ('efforts', 0.067), ('hierarchical', 0.066), ('structure', 0.066), ('alleviate', 0.065), ('bleu', 0.064), ('relations', 0.062), ('string', 0.061), ('held', 0.06), ('said', 0.06), ('anti', 0.06), ('nnrr', 0.06), ('och', 0.059), ('long', 0.058), ('modification', 0.057), ('relation', 0.057), ('spans', 0.055), ('distance', 0.055), ('cubic', 0.054), ('unaligned', 0.052), ('ld', 0.052), ('treelet', 0.052), ('cup', 0.05), ('nr', 0.05), ('dependent', 0.05), ('fragments', 0.049), ('successfully', 0.049), ('chiang', 0.048), ('underline', 0.047), ('dependents', 0.047), ('vv', 0.047), ('grammar', 0.047), ('huang', 0.047), ('insertion', 0.047), ('nist', 0.047), ('palmer', 0.047), ('ccuupp', 0.046), ('talks', 0.046), ('terrorism', 0.046), ('transversal', 0.046), ('wwoorrldld', 0.046), ('acquisition', 0.046), ('translating', 0.045), ('chinese', 0.045), ('south', 0.045), ('hh', 0.045), ('structures', 0.044), ('replace', 0.044), ('dd', 0.043), ('surpasses', 0.043), ('holds', 0.041), ('promote', 0.041), ('derivation', 0.04), ('developing', 0.04), ('aug', 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="15-tfidf-1" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>Author: Jun Xie ; Haitao Mi ; Qun Liu</p><p>Abstract: Dependency structure, as a first step towards semantics, is believed to be helpful to improve translation quality. However, previous works on dependency structure based models typically resort to insertion operations to complete translations, which make it difficult to specify ordering information in translation rules. In our model of this paper, we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents relations and the target side as strings. The head-dependents rules require only substitution operation, thus our model requires no heuristics or separate ordering models of the previous works to control the word order of translations. Large-scale experiments show that our model performs well on long distance reordering, and outperforms the state- of-the-art constituency-to-string model (+1.47 BLEU on average) and hierarchical phrasebased model (+0.46 BLEU on average) on two Chinese-English NIST test sets without resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models.</p><p>2 0.2403426 <a title="15-tfidf-2" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>Author: Yang Gao ; Philipp Koehn ; Alexandra Birch</p><p>Abstract: Long-distance reordering remains one of the biggest challenges facing machine translation. We derive soft constraints from the source dependency parsing to directly address the reordering problem for the hierarchical phrasebased model. Our approach significantly improves Chinese–English machine translation on a large-scale task by 0.84 BLEU points on average. Moreover, when we switch the tuning function from BLEU to the LRscore which promotes reordering, we observe total improvements of 1.21 BLEU, 1.30 LRscore and 3.36 TER over the baseline. On average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially effective for long-distance reodering.</p><p>3 0.22174098 <a title="15-tfidf-3" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>Author: Jiajun Zhang ; Feifei Zhai ; Chengqing Zong</p><p>Abstract: Due to its explicit modeling of the grammaticality of the output via target-side syntax, the string-to-tree model has been shown to be one of the most successful syntax-based translation models. However, a major limitation of this model is that it does not utilize any useful syntactic information on the source side. In this paper, we analyze the difficulties of incorporating source syntax in a string-totree model. We then propose a new way to use the source syntax in a fuzzy manner, both in source syntactic annotation and in rule matching. We further explore three algorithms in rule matching: 0-1 matching, likelihood matching, and deep similarity matching. Our method not only guarantees grammatical output with an explicit target tree, but also enables the system to choose the proper translation rules via fuzzy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1</p><p>4 0.21592066 <a title="15-tfidf-4" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Noah A. Smith</p><p>Abstract: We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.</p><p>5 0.17263833 <a title="15-tfidf-5" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>Author: Xinyan Xiao ; Yang Liu ; Qun Liu ; Shouxun Lin</p><p>Abstract: Although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features, it is hard to scale up to large data due to decoding complexity. We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets.</p><p>6 0.14789766 <a title="15-tfidf-6" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>7 0.14337239 <a title="15-tfidf-7" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>8 0.12793621 <a title="15-tfidf-8" href="./emnlp-2011-Statistical_Machine_Translation_with_Local_Language_Models.html">125 emnlp-2011-Statistical Machine Translation with Local Language Models</a></p>
<p>9 0.11876018 <a title="15-tfidf-9" href="./emnlp-2011-A_Word_Reordering_Model_for_Improved_Machine_Translation.html">13 emnlp-2011-A Word Reordering Model for Improved Machine Translation</a></p>
<p>10 0.11569548 <a title="15-tfidf-10" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>11 0.11181772 <a title="15-tfidf-11" href="./emnlp-2011-Joint_Models_for_Chinese_POS_Tagging_and_Dependency_Parsing.html">75 emnlp-2011-Joint Models for Chinese POS Tagging and Dependency Parsing</a></p>
<p>12 0.10830866 <a title="15-tfidf-12" href="./emnlp-2011-Inducing_Sentence_Structure_from_Parallel_Corpora_for_Reordering.html">74 emnlp-2011-Inducing Sentence Structure from Parallel Corpora for Reordering</a></p>
<p>13 0.10615042 <a title="15-tfidf-13" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>14 0.10300668 <a title="15-tfidf-14" href="./emnlp-2011-A_Fast%2C_Accurate%2C_Non-Projective%2C_Semantically-Enriched_Parser.html">4 emnlp-2011-A Fast, Accurate, Non-Projective, Semantically-Enriched Parser</a></p>
<p>15 0.10167141 <a title="15-tfidf-15" href="./emnlp-2011-Heuristic_Search_for_Non-Bottom-Up_Tree_Structure_Prediction.html">65 emnlp-2011-Heuristic Search for Non-Bottom-Up Tree Structure Prediction</a></p>
<p>16 0.098850608 <a title="15-tfidf-16" href="./emnlp-2011-Accurate_Parsing_with_Compact_Tree-Substitution_Grammars%3A_Double-DOP.html">16 emnlp-2011-Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP</a></p>
<p>17 0.097477555 <a title="15-tfidf-17" href="./emnlp-2011-A_Cascaded_Classification_Approach_to_Semantic_Head_Recognition.html">2 emnlp-2011-A Cascaded Classification Approach to Semantic Head Recognition</a></p>
<p>18 0.097261265 <a title="15-tfidf-18" href="./emnlp-2011-Better_Evaluation_Metrics_Lead_to_Better_Machine_Translation.html">22 emnlp-2011-Better Evaluation Metrics Lead to Better Machine Translation</a></p>
<p>19 0.093858831 <a title="15-tfidf-19" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<p>20 0.090369016 <a title="15-tfidf-20" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.335), (1, 0.232), (2, 0.048), (3, -0.012), (4, -0.036), (5, -0.086), (6, 0.079), (7, 0.025), (8, 0.149), (9, -0.016), (10, 0.06), (11, 0.168), (12, 0.036), (13, 0.009), (14, 0.077), (15, -0.076), (16, -0.022), (17, 0.059), (18, -0.024), (19, 0.067), (20, 0.09), (21, 0.059), (22, -0.17), (23, 0.126), (24, -0.151), (25, 0.054), (26, 0.028), (27, 0.113), (28, -0.119), (29, -0.028), (30, 0.057), (31, 0.012), (32, -0.046), (33, -0.083), (34, 0.089), (35, -0.02), (36, -0.011), (37, 0.057), (38, 0.035), (39, 0.013), (40, -0.106), (41, 0.042), (42, -0.039), (43, -0.138), (44, -0.057), (45, -0.081), (46, 0.065), (47, 0.053), (48, -0.055), (49, 0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96901488 <a title="15-lsi-1" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>Author: Jun Xie ; Haitao Mi ; Qun Liu</p><p>Abstract: Dependency structure, as a first step towards semantics, is believed to be helpful to improve translation quality. However, previous works on dependency structure based models typically resort to insertion operations to complete translations, which make it difficult to specify ordering information in translation rules. In our model of this paper, we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents relations and the target side as strings. The head-dependents rules require only substitution operation, thus our model requires no heuristics or separate ordering models of the previous works to control the word order of translations. Large-scale experiments show that our model performs well on long distance reordering, and outperforms the state- of-the-art constituency-to-string model (+1.47 BLEU on average) and hierarchical phrasebased model (+0.46 BLEU on average) on two Chinese-English NIST test sets without resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models.</p><p>2 0.78762788 <a title="15-lsi-2" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>Author: Jiajun Zhang ; Feifei Zhai ; Chengqing Zong</p><p>Abstract: Due to its explicit modeling of the grammaticality of the output via target-side syntax, the string-to-tree model has been shown to be one of the most successful syntax-based translation models. However, a major limitation of this model is that it does not utilize any useful syntactic information on the source side. In this paper, we analyze the difficulties of incorporating source syntax in a string-totree model. We then propose a new way to use the source syntax in a fuzzy manner, both in source syntactic annotation and in rule matching. We further explore three algorithms in rule matching: 0-1 matching, likelihood matching, and deep similarity matching. Our method not only guarantees grammatical output with an explicit target tree, but also enables the system to choose the proper translation rules via fuzzy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1</p><p>3 0.77596575 <a title="15-lsi-3" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>Author: Yang Gao ; Philipp Koehn ; Alexandra Birch</p><p>Abstract: Long-distance reordering remains one of the biggest challenges facing machine translation. We derive soft constraints from the source dependency parsing to directly address the reordering problem for the hierarchical phrasebased model. Our approach significantly improves Chinese–English machine translation on a large-scale task by 0.84 BLEU points on average. Moreover, when we switch the tuning function from BLEU to the LRscore which promotes reordering, we observe total improvements of 1.21 BLEU, 1.30 LRscore and 3.36 TER over the baseline. On average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially effective for long-distance reodering.</p><p>4 0.69689262 <a title="15-lsi-4" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Noah A. Smith</p><p>Abstract: We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.</p><p>5 0.66179556 <a title="15-lsi-5" href="./emnlp-2011-Efficient_retrieval_of_tree_translation_examples_for_Syntax-Based_Machine_Translation.html">47 emnlp-2011-Efficient retrieval of tree translation examples for Syntax-Based Machine Translation</a></p>
<p>Author: Fabien Cromieres ; Sadao Kurohashi</p><p>Abstract: We propose an algorithm allowing to efficiently retrieve example treelets in a parsed tree database in order to allow on-the-fly extraction of syntactic translation rules. We also propose improvements of this algorithm allowing several kinds of flexible matchings.</p><p>6 0.64582598 <a title="15-lsi-6" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>7 0.59111422 <a title="15-lsi-7" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>8 0.51049942 <a title="15-lsi-8" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>9 0.49219957 <a title="15-lsi-9" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>10 0.47540653 <a title="15-lsi-10" href="./emnlp-2011-A_Cascaded_Classification_Approach_to_Semantic_Head_Recognition.html">2 emnlp-2011-A Cascaded Classification Approach to Semantic Head Recognition</a></p>
<p>11 0.45556787 <a title="15-lsi-11" href="./emnlp-2011-Heuristic_Search_for_Non-Bottom-Up_Tree_Structure_Prediction.html">65 emnlp-2011-Heuristic Search for Non-Bottom-Up Tree Structure Prediction</a></p>
<p>12 0.44017151 <a title="15-lsi-12" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>13 0.43272799 <a title="15-lsi-13" href="./emnlp-2011-A_Word_Reordering_Model_for_Improved_Machine_Translation.html">13 emnlp-2011-A Word Reordering Model for Improved Machine Translation</a></p>
<p>14 0.41886389 <a title="15-lsi-14" href="./emnlp-2011-Inducing_Sentence_Structure_from_Parallel_Corpora_for_Reordering.html">74 emnlp-2011-Inducing Sentence Structure from Parallel Corpora for Reordering</a></p>
<p>15 0.41154709 <a title="15-lsi-15" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>16 0.41084343 <a title="15-lsi-16" href="./emnlp-2011-Accurate_Parsing_with_Compact_Tree-Substitution_Grammars%3A_Double-DOP.html">16 emnlp-2011-Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP</a></p>
<p>17 0.39870274 <a title="15-lsi-17" href="./emnlp-2011-A_Fast%2C_Accurate%2C_Non-Projective%2C_Semantically-Enriched_Parser.html">4 emnlp-2011-A Fast, Accurate, Non-Projective, Semantically-Enriched Parser</a></p>
<p>18 0.38351986 <a title="15-lsi-18" href="./emnlp-2011-Statistical_Machine_Translation_with_Local_Language_Models.html">125 emnlp-2011-Statistical Machine Translation with Local Language Models</a></p>
<p>19 0.36218917 <a title="15-lsi-19" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<p>20 0.35844174 <a title="15-lsi-20" href="./emnlp-2011-Joint_Models_for_Chinese_POS_Tagging_and_Dependency_Parsing.html">75 emnlp-2011-Joint Models for Chinese POS Tagging and Dependency Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.102), (35, 0.014), (36, 0.029), (37, 0.039), (45, 0.066), (53, 0.034), (54, 0.04), (57, 0.017), (62, 0.03), (64, 0.029), (65, 0.293), (66, 0.045), (69, 0.02), (79, 0.057), (82, 0.022), (90, 0.034), (96, 0.034), (98, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79927105 <a title="15-lda-1" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>Author: Jun Xie ; Haitao Mi ; Qun Liu</p><p>Abstract: Dependency structure, as a first step towards semantics, is believed to be helpful to improve translation quality. However, previous works on dependency structure based models typically resort to insertion operations to complete translations, which make it difficult to specify ordering information in translation rules. In our model of this paper, we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents relations and the target side as strings. The head-dependents rules require only substitution operation, thus our model requires no heuristics or separate ordering models of the previous works to control the word order of translations. Large-scale experiments show that our model performs well on long distance reordering, and outperforms the state- of-the-art constituency-to-string model (+1.47 BLEU on average) and hierarchical phrasebased model (+0.46 BLEU on average) on two Chinese-English NIST test sets without resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models.</p><p>2 0.76625592 <a title="15-lda-2" href="./emnlp-2011-SMT_Helps_Bitext_Dependency_Parsing.html">118 emnlp-2011-SMT Helps Bitext Dependency Parsing</a></p>
<p>Author: Wenliang Chen ; Jun'ichi Kazama ; Min Zhang ; Yoshimasa Tsuruoka ; Yujie Zhang ; Yiou Wang ; Kentaro Torisawa ; Haizhou Li</p><p>Abstract: We propose a method to improve the accuracy of parsing bilingual texts (bitexts) with the help of statistical machine translation (SMT) systems. Previous bitext parsing methods use human-annotated bilingual treebanks that are hard to obtain. Instead, our approach uses an auto-generated bilingual treebank to produce bilingual constraints. However, because the auto-generated bilingual treebank contains errors, the bilingual constraints are noisy. To overcome this problem, we use large-scale unannotated data to verify the constraints and design a set of effective bilingual features for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT.</p><p>3 0.50326645 <a title="15-lda-3" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>Author: Yang Gao ; Philipp Koehn ; Alexandra Birch</p><p>Abstract: Long-distance reordering remains one of the biggest challenges facing machine translation. We derive soft constraints from the source dependency parsing to directly address the reordering problem for the hierarchical phrasebased model. Our approach significantly improves Chinese–English machine translation on a large-scale task by 0.84 BLEU points on average. Moreover, when we switch the tuning function from BLEU to the LRscore which promotes reordering, we observe total improvements of 1.21 BLEU, 1.30 LRscore and 3.36 TER over the baseline. On average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially effective for long-distance reodering.</p><p>4 0.50060171 <a title="15-lda-4" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>Author: Jiajun Zhang ; Feifei Zhai ; Chengqing Zong</p><p>Abstract: Due to its explicit modeling of the grammaticality of the output via target-side syntax, the string-to-tree model has been shown to be one of the most successful syntax-based translation models. However, a major limitation of this model is that it does not utilize any useful syntactic information on the source side. In this paper, we analyze the difficulties of incorporating source syntax in a string-totree model. We then propose a new way to use the source syntax in a fuzzy manner, both in source syntactic annotation and in rule matching. We further explore three algorithms in rule matching: 0-1 matching, likelihood matching, and deep similarity matching. Our method not only guarantees grammatical output with an explicit target tree, but also enables the system to choose the proper translation rules via fuzzy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1</p><p>5 0.48663497 <a title="15-lda-5" href="./emnlp-2011-Cache-based_Document-level_Statistical_Machine_Translation.html">25 emnlp-2011-Cache-based Document-level Statistical Machine Translation</a></p>
<p>Author: Zhengxian Gong ; Min Zhang ; Guodong Zhou</p><p>Abstract: Statistical machine translation systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time, ignoring document-level information. In this paper, we propose a cache-based approach to document-level translation. Since caches mainly depend on relevant data to supervise subsequent decisions, it is critical to fill the caches with highly-relevant data of a reasonable size. In this paper, we present three kinds of caches to store relevant document-level information: 1) a dynamic cache, which stores bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document; 2) a static cache, which stores relevant bilingual phrase pairs extracted from similar bilingual document pairs (i.e. source documents similar to the test document and their corresponding target documents) in the training parallel corpus; 3) a topic cache, which stores the target-side topic words related with the test document in the source-side. In particular, three new features are designed to explore various kinds of document-level information in above three kinds of caches. Evaluation shows the effectiveness of our cache-based approach to document-level translation with the performance improvement of 0.8 1 in BLUE score over Moses. Especially, detailed analysis and discussion are presented to give new insights to document-level translation. 1</p><p>6 0.48068544 <a title="15-lda-6" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>7 0.47746515 <a title="15-lda-7" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>8 0.47590166 <a title="15-lda-8" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>9 0.46977642 <a title="15-lda-9" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>10 0.46951461 <a title="15-lda-10" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>11 0.46850881 <a title="15-lda-11" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>12 0.46797809 <a title="15-lda-12" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>13 0.4656156 <a title="15-lda-13" href="./emnlp-2011-Hierarchical_Phrase-based_Translation_Representations.html">66 emnlp-2011-Hierarchical Phrase-based Translation Representations</a></p>
<p>14 0.46551368 <a title="15-lda-14" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<p>15 0.46507418 <a title="15-lda-15" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>16 0.46496186 <a title="15-lda-16" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>17 0.46482247 <a title="15-lda-17" href="./emnlp-2011-Better_Evaluation_Metrics_Lead_to_Better_Machine_Translation.html">22 emnlp-2011-Better Evaluation Metrics Lead to Better Machine Translation</a></p>
<p>18 0.46319801 <a title="15-lda-18" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>19 0.46285146 <a title="15-lda-19" href="./emnlp-2011-Multiword_Expression_Identification_with_Tree_Substitution_Grammars%3A_A_Parsing_tour_de_force_with_French.html">97 emnlp-2011-Multiword Expression Identification with Tree Substitution Grammars: A Parsing tour de force with French</a></p>
<p>20 0.46119875 <a title="15-lda-20" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
