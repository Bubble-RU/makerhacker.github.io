<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>43 emnlp-2011-Domain-Assisted Product Aspect Hierarchy Generation: Towards Hierarchical Organization of Unstructured Consumer Reviews</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-43" href="#">emnlp2011-43</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>43 emnlp-2011-Domain-Assisted Product Aspect Hierarchy Generation: Towards Hierarchical Organization of Unstructured Consumer Reviews</h1>
<br/><p>Source: <a title="emnlp-2011-43-pdf" href="http://aclweb.org/anthology//D/D11/D11-1013.pdf">pdf</a></p><p>Author: Jianxing Yu ; Zheng-Jun Zha ; Meng Wang ; Kai Wang ; Tat-Seng Chua</p><p>Abstract: This paper presents a domain-assisted approach to organize various aspects of a product into a hierarchy by integrating domain knowledge (e.g., the product specifications), as well as consumer reviews. Based on the derived hierarchy, we generate a hierarchical organization of consumer reviews on various product aspects and aggregate consumer opinions on these aspects. With such organization, user can easily grasp the overview of consumer reviews. Furthermore, we apply the hierarchy to the task of implicit aspect identification which aims to infer implicit aspects of the reviews that do not explicitly express those aspects but actually comment on them. The experimental results on 11popular products in four domains demonstrate the effectiveness of our approach.</p><p>Reference: <a title="emnlp-2011-43-reference" href="../emnlp2011_reference/emnlp-2011-Domain-Assisted_Product_Aspect_Hierarchy_Generation%3A_Towards_Hierarchical_Organization_of_Unstructured_Consumer_Reviews_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 sg  Abstract This paper presents a domain-assisted approach to organize various aspects of a product into a hierarchy by integrating domain knowledge (e. [sent-7, score-0.978]
</p><p>2 Based on the derived hierarchy, we generate a hierarchical organization of consumer reviews on various product aspects and aggregate consumer opinions on these aspects. [sent-10, score-2.199]
</p><p>3 With such organization, user can easily grasp the overview of consumer reviews. [sent-11, score-0.726]
</p><p>4 Furthermore, we apply the hierarchy to the task of implicit aspect identification which aims to infer implicit aspects of the reviews that do not explicitly express those aspects but actually comment on them. [sent-12, score-1.875]
</p><p>5 The experimental results on 11popular products in four domains demonstrate the effectiveness of our approach. [sent-13, score-0.032]
</p><p>6 1 Introduction  With the rapidly expanding e-commerce, most retail Web sites encourage consumers to write reviews to express their opinions on various aspects of products. [sent-14, score-0.773]
</p><p>7 Huge collections of consumer reviews are now available on the Web. [sent-15, score-0.928]
</p><p>8 These reviews have become an important resource for both consumers and firms. [sent-16, score-0.393]
</p><p>9 Consumers commonly seek quality information from online consumer reviews prior to purchasing a product, while many firms use online reviews as an important resource in their product development, marketing, and consumer relationship management. [sent-17, score-2.043]
</p><p>10 However, the reviews are disorganized, leading to the difficulty in information navigation and knowledge acquisition. [sent-18, score-0.333]
</p><p>11 It is impractical for user 140 to grasp the overview of consumer reviews and opinions on various aspects of a product from such enormous reviews. [sent-19, score-1.576]
</p><p>12 Among hundreds of product aspects, it is also inefficient for user to browse consumer reviews and opinions on a specific aspect. [sent-20, score-1.207]
</p><p>13 Thus, there is a compelling need to organize consumer reviews, so as to transform the reviews into a useful knowledge structure. [sent-21, score-1.069]
</p><p>14 Since the hierarchy can improve in-  formation representation and accessibility (Cimiano, 2006), we propose to organize the aspects of a product into a hierarchy and generate a hierarchical organization of consumer reviews accordingly. [sent-22, score-2.333]
</p><p>15 However, pattern-based methods usually suffer from inconsistency of parent-child relationships among the concepts, while clustering-based methods often result in low accuracy. [sent-25, score-0.055]
</p><p>16 Thus, by directly utilizing these methods to generate an aspect hierarchy from consumer reviews, the resulting hierarchy is usually inaccurate, leading to unsatisfactory review organization. [sent-26, score-1.751]
</p><p>17 On the other hand, domain knowledge of products is now available on the Web. [sent-27, score-0.08]
</p><p>18 For example, there are more than 248,474 product specifications in the product selling Web site CNet. [sent-28, score-0.465]
</p><p>19 These product specifications cover some product aspects and provide coarse-grained parent-  child relations among these aspects. [sent-30, score-0.761]
</p><p>20 Such domain knowledge is useful to help organize the product asProceeEddiningbsu orfg thh,e S 2c0o1tl1an Cdo,n UfeKr,en Jcuely on 27 E–m31p,ir 2ic0a1l1 M. [sent-31, score-0.324]
</p><p>21 ec th2o0d1s1 i Ans Nsoactuiartaioln La fonrg Cuaogmep Purtoatcieosnsainlg L,in pgaugies ti 1c4s0–150,  Figure 1: Sample hierarchical organization for iPhone 3G  pects into a hierarchy. [sent-33, score-0.116]
</p><p>22 However, the initial hierarchy obtained from domain knowledge usually cannot fit the review data well. [sent-34, score-0.488]
</p><p>23 For example, the initial hierarchy is usually too coarse and may not cover the specific aspects commented in the reviews, while some aspects in the hierarchy may not be of interests to users in the reviews. [sent-35, score-1.391]
</p><p>24 Motivated by the above observations, we propose in this paper to organize the product aspects into a hierarchy by simultaneously exploiting the domain knowledge (e. [sent-36, score-0.96]
</p><p>25 With derived aspect hierarchy, we generate a hierarchical organization of consumer reviews on various aspects and aggregate consumer opinions on these aspects. [sent-39, score-2.383]
</p><p>26 Figure 1illustrates a sample of hierarchical review organization for the product “iPhone 3G”. [sent-40, score-0.318]
</p><p>27 With such organization, users can easily grasp the overview of product aspects as well as conveniently navigate the consumer reviews and opinions on any aspect. [sent-41, score-1.6]
</p><p>28 For example, users can find that 623 reviews, out of 9,245 reviews, are about the aspect “price”, with 241 positive and 382 negative reviews. [sent-42, score-0.379]
</p><p>29 Given a collection of consumer reviews on a spe-  cific product, we first automatically acquire an initial aspect hierarchy from domain knowledge and identify the aspects from the reviews. [sent-43, score-2.064]
</p><p>30 Based on the 141 initial hierarchy, we develop a multi-criteria optimization approach to construct an aspect hierarchy to contain all the identified aspects. [sent-44, score-0.744]
</p><p>31 Our approach incrementally inserts the aspects into the initial hierarchy based on inter-aspect semantic distance, a metric used to measure the semantic relation among aspects. [sent-45, score-0.791]
</p><p>32 In order to derive reliable semantic distance, we propose to leverage external hierarchies, sampled from WordNet and Open Directory Project, to assist semantic distance learning. [sent-46, score-0.207]
</p><p>33 With resultant aspect hierarchy, the consumer reviews are then organized to their corresponding aspect nodes in the hierarchy. [sent-47, score-1.636]
</p><p>34 We then perform sentiment classification to determine consumer opinions on these aspects. [sent-48, score-0.681]
</p><p>35 Furthermore, we apply the hierarchy to the task of implicit aspect identification. [sent-49, score-0.788]
</p><p>36 This task aims to infer implicit aspects of the reviews that do not explicitly express those aspects but actually comment on them. [sent-50, score-1.052]
</p><p>37 For example, the implicit aspect of the review “It is so expensive” is “price. [sent-51, score-0.463]
</p><p>38 ” Most existing aspect  identification approaches rely on the appearance of aspect terms, and thus are not able to handle implicit aspect problem. [sent-52, score-1.192]
</p><p>39 Based on our aspect hierarchy, we can infer the implicit aspects by clustering the reviews into their corresponding aspect nodes in the hierarchy. [sent-53, score-1.405]
</p><p>40 We conduct experiments on 11 popular products in four domains. [sent-54, score-0.032]
</p><p>41 The main contributions of this work can be summarized as follows: 1) We propose to hierarchically organize consumer reviews according to an aspect hierarchy, so as to transfer the reviews into a useful knowledge structure. [sent-57, score-1.739]
</p><p>42 2) We develop a domain-assisted approach to generate an aspect hierarchy by integrating domain knowledge and consumer reviews. [sent-58, score-1.39]
</p><p>43 In order to derive reliable semantic distance between aspects, we propose to leverage external hierarchies to assist semantic distance learning. [sent-59, score-0.296]
</p><p>44 3) We apply the aspect hierarchy to the task of implicit aspect identification, and achieve satisfactory performance. [sent-60, score-1.142]
</p><p>45 Our approach is elaborated in Section 2 and applied to implicit aspect identification in Section 3. [sent-62, score-0.466]
</p><p>46 Section 4 presents the evaluations, while Section 5 reviews related work. [sent-63, score-0.315]
</p><p>47 2  Approach  Our approach consists of four components, including initial hierarchy acquisition, aspect identification, semantic distance learning, and aspect hierarchy generation. [sent-65, score-1.542]
</p><p>48 Next, we first define some preliminary and notations and then elaborate these components. [sent-66, score-0.053]
</p><p>49 An aspect hierarchy is defined as a tree that consists of a set of unique aspects A and a set of parent-child are slaetti oonfs u nRiq bueet wasepeenc tthse Ase aspects. [sent-69, score-1.031]
</p><p>50 Given the consumer reviews of a product, let A = {a1, · · · , ak} denotes the product aspects commAe =nte {da in, ·t·he· , reav}iew desn. [sent-70, score-1.377]
</p><p>51 oHte0s( tAh0e, p pRro0d)u dcet naospteesc tths ec oimni-tmiaeln hierarchy rdeevriievweds. [sent-71, score-0.357]
</p><p>52 It contains a set of aspects A0 and relations R0. [sent-73, score-0.311]
</p><p>53 Our  tcaosnkt aiisn tso aco senstt orufc ats an aspect hierarchy H(A, R), to cover sa tlol tchoen aspects nin a sApe acntd h itehreairrc parent-child relcaotvioenrs a R, so tshpaetc tthse i consumer reeirvi peawrse are hilidera rer-chically organized. [sent-74, score-1.69]
</p><p>54 2 Initial Hierarchy Acquisition As aforementioned, product specifications on product selling websites cover some product aspects and coarse-grained parent-child relations among these aspects. [sent-77, score-0.968]
</p><p>55 Such domain knowledge is useful to help organize aspects into a hierarchy. [sent-78, score-0.433]
</p><p>56 We here employ the approach proposed by Ye and Chua (2006) to automatically acquire an initial aspect hierarchy from the product specifications. [sent-79, score-0.939]
</p><p>57 The method first identifies the Web page region covering product descriptions and removes the irrelevant contents from the Web page. [sent-80, score-0.196]
</p><p>58 It then parses the region containing the product information to identify the aspects as well as their structure. [sent-81, score-0.495]
</p><p>59 Based on the aspects and their structure, it generates an aspect hierarchy. [sent-82, score-0.633]
</p><p>60 3 Aspect Identification To identify aspects in consumer reviews, we first parse each review using the Stanford parser 1. [sent-84, score-0.944]
</p><p>61 Since the aspects in consumer reviews are usually noun  1http://nlp. [sent-85, score-1.246]
</p><p>62 shtml 142  Figure 2: Sample Pros and Cons reviews or noun phrases (Liu, 2009), we extract the noun phrases (NP) from the parse tree as aspect candidates. [sent-88, score-0.711]
</p><p>63 While these candidates may contain much noise, we leverage Pros and Cons reviews (see Figure 2), which are prevalent in forum Web sites, to assist identify aspects from the candidates. [sent-89, score-0.702]
</p><p>64 It has been shown that simply extracting the frequent noun terms from the Pros and Cons reviews can get high accurate aspect terms (Liu el al. [sent-90, score-0.69]
</p><p>65 Thus, we extract the frequent noun terms from Pros and Cons reviews as features, then train a one-class SVM (Manevitz et al. [sent-92, score-0.336]
</p><p>66 While the obtained aspects may contain some synonym terms, such as “earphone” and “headphone”, we further perform synonym cluster-  ing to get unique aspects. [sent-94, score-0.395]
</p><p>67 Specifically, we first expand each aspect term with its synonym terms obtained from the synonym terms Web site 2, then cluster them to obtain unique aspects based on unigram feature. [sent-95, score-0.794]
</p><p>68 4 Semantic Distance Learning Our aspect hierarchy generation approach is essentially based on the semantic relations among aspects. [sent-97, score-0.795]
</p><p>69 We here define a metric, Semantic Distance, d(ax, ay), to quantitatively measure the semantic relation between aspects ax and ay. [sent-98, score-0.417]
</p><p>70 Next, we first introduce the linguistic features used in our work and then present the semantic distance learning algorithm that aims to find the optimal weights in Eq. [sent-100, score-0.112]
</p><p>71 1 Linguistic Features Given two aspects ax and ay, a feature is defined as a function generating a numeric score f(ax, ay) or a vector of scores. [sent-105, score-0.385]
</p><p>72 These features are generated based on auxiliary documents collected from Web. [sent-107, score-0.022]
</p><p>73 Specifically, we issue each aspect term and aspect term pair as queries to Google and Wikipedia, respectively, and collect the top 100 returned documents of each query. [sent-108, score-0.775]
</p><p>74 Based on these documents and sentences, the features are generated as follows. [sent-110, score-0.022]
</p><p>75 For each aspect, we collect the documents containing the aspect as context to build a unigram language model without smoothing. [sent-112, score-0.442]
</p><p>76 Similarly, we collect the left two and right two words surrounding each aspect as context and build a unigram language model. [sent-114, score-0.42]
</p><p>77 The  KL-divergence between the language models of two aspects is defined as the Local-Context feature. [sent-115, score-0.279]
</p><p>78 We measure the cooccurrence of two aspects by Pointwise Mutual Information (PMI): PMI(ax,ay)=log(Count(ax,ay)/ Count(ax) Count(ay)), where Count(·) stands for the number of documen)t)s, or sentences containing tthhee aspect(s), or the number of Google document hits for the aspect(s). [sent-117, score-0.279]
</p><p>79 Based on different definitions of Count(·), we define the features of Document PMI, SCeonutennt(c·e), PMI, eafnidne Google PMI, respectively. [sent-118, score-0.017]
</p><p>80 We parse the sentences that contain each aspect pair into syntactic trees via the Stanford Parser. [sent-120, score-0.354]
</p><p>81 The Syntactic-path feature is defined as the average length of the shortest syntactic path between the aspect pair in the tree. [sent-121, score-0.375]
</p><p>82 In addition, for each aspect, we collect a set of sentences containing it, and label the semantic role of the sentences via ASSERT parser 3. [sent-122, score-0.077]
</p><p>83 Given two aspects, the number of the Subject terms overlaps between their sentence sets is computed as the Subject Overlap feature. [sent-123, score-0.026]
</p><p>84 Similarly, for other semantic roles, such as objects, modifiers, and verbs, we define the features of Object Overlap, Modifier Overlap, and Verb  3http://cemantix. [sent-124, score-0.032]
</p><p>85 46 patterns are used in our work, including 6 patterns indicating the hypernym relations of two aspects (Hearst, 1992), and 40 patterns measuring the part-of relations of two aspects (Girju et al. [sent-128, score-0.703]
</p><p>86 These pattern features are asymmetric, and they take the parent-child relations among the aspects into consideration. [sent-130, score-0.351]
</p><p>87 All the patterns are listed in Appendix A (submitted as supplementary material). [sent-131, score-0.027]
</p><p>88 Based on these patterns, a 46dimensional score vector is obtained for each aspect pair. [sent-132, score-0.354]
</p><p>89 A score is 1if two aspects match a pattern, and 0 otherwise. [sent-133, score-0.279]
</p><p>90 In addition, we issue the query “define:aspect” to Google, and collect the definition of each aspect. [sent-136, score-0.045]
</p><p>91 We then count the word overlaps between the definitions of two aspects, as Definition Overlap feature. [sent-137, score-0.073]
</p><p>92 2 Semantic Distance Learning This section elaborates the learning algorithm that optimizes the semantic distance metric, i. [sent-140, score-0.107]
</p><p>93 Typically, we  can utilize the initial hierarchy as training data. [sent-144, score-0.39]
</p><p>94 The ground-truth distance between two aspects dG (ax, ay) is generated by summing up all the edge distances along the shortest path between ax and ay, where every edge weight is assumed as 1. [sent-145, score-0.461]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('consumer', 0.613), ('hierarchy', 0.357), ('aspect', 0.354), ('reviews', 0.315), ('aspects', 0.279), ('product', 0.17), ('ay', 0.129), ('organize', 0.106), ('ax', 0.106), ('organization', 0.082), ('implicit', 0.077), ('pros', 0.074), ('grasp', 0.071), ('cons', 0.068), ('opinions', 0.068), ('specifications', 0.064), ('pmi', 0.061), ('consumers', 0.061), ('synonym', 0.058), ('distance', 0.055), ('jianxing', 0.047), ('collect', 0.045), ('assist', 0.043), ('iphone', 0.041), ('tthse', 0.041), ('overlap', 0.038), ('selling', 0.037), ('identification', 0.035), ('hierarchies', 0.034), ('hierarchical', 0.034), ('initial', 0.033), ('products', 0.032), ('review', 0.032), ('semantic', 0.032), ('relations', 0.032), ('count', 0.03), ('domain', 0.03), ('google', 0.029), ('notations', 0.029), ('leverage', 0.028), ('patterns', 0.027), ('region', 0.026), ('overlaps', 0.026), ('comment', 0.026), ('infer', 0.026), ('concepts', 0.026), ('cover', 0.026), ('acquire', 0.025), ('sites', 0.025), ('users', 0.025), ('web', 0.025), ('aims', 0.025), ('express', 0.025), ('aggregate', 0.025), ('preliminary', 0.024), ('site', 0.024), ('documents', 0.022), ('noun', 0.021), ('shortest', 0.021), ('user', 0.021), ('unigram', 0.021), ('overview', 0.021), ('accessibility', 0.02), ('assert', 0.02), ('ats', 0.02), ('beckham', 0.02), ('browse', 0.02), ('chua', 0.02), ('cific', 0.02), ('clusteringbased', 0.02), ('ctoe', 0.02), ('elaborates', 0.02), ('inserts', 0.02), ('kai', 0.02), ('navigate', 0.02), ('unsatisfactory', 0.02), ('pattern', 0.02), ('among', 0.02), ('identify', 0.02), ('conveniently', 0.018), ('infocomm', 0.018), ('appearance', 0.018), ('dg', 0.018), ('marketing', 0.018), ('ase', 0.018), ('enormous', 0.018), ('hierarchically', 0.018), ('metric', 0.018), ('usually', 0.018), ('knowledge', 0.018), ('integrating', 0.018), ('singapore', 0.017), ('definitions', 0.017), ('derive', 0.017), ('commented', 0.017), ('price', 0.017), ('compelling', 0.017), ('inconsistency', 0.017), ('prevalent', 0.017), ('resource', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="43-tfidf-1" href="./emnlp-2011-Domain-Assisted_Product_Aspect_Hierarchy_Generation%3A_Towards_Hierarchical_Organization_of_Unstructured_Consumer_Reviews.html">43 emnlp-2011-Domain-Assisted Product Aspect Hierarchy Generation: Towards Hierarchical Organization of Unstructured Consumer Reviews</a></p>
<p>Author: Jianxing Yu ; Zheng-Jun Zha ; Meng Wang ; Kai Wang ; Tat-Seng Chua</p><p>Abstract: This paper presents a domain-assisted approach to organize various aspects of a product into a hierarchy by integrating domain knowledge (e.g., the product specifications), as well as consumer reviews. Based on the derived hierarchy, we generate a hierarchical organization of consumer reviews on various product aspects and aggregate consumer opinions on these aspects. With such organization, user can easily grasp the overview of consumer reviews. Furthermore, we apply the hierarchy to the task of implicit aspect identification which aims to infer implicit aspects of the reviews that do not explicitly express those aspects but actually comment on them. The experimental results on 11popular products in four domains demonstrate the effectiveness of our approach.</p><p>2 0.16832079 <a title="43-tfidf-2" href="./emnlp-2011-Generating_Aspect-oriented_Multi-Document_Summarization_with_Event-aspect_model.html">61 emnlp-2011-Generating Aspect-oriented Multi-Document Summarization with Event-aspect model</a></p>
<p>Author: Peng Li ; Yinglin Wang ; Wei Gao ; Jing Jiang</p><p>Abstract: In this paper, we propose a novel approach to automatic generation of aspect-oriented summaries from multiple documents. We first develop an event-aspect LDA model to cluster sentences into aspects. We then use extended LexRank algorithm to rank the sentences in each cluster. We use Integer Linear Programming for sentence selection. Key features of our method include automatic grouping of semantically related sentences and sentence ranking based on extension of random walk model. Also, we implement a new sentence compression algorithm which use dependency tree instead of parser tree. We compare our method with four baseline methods. Quantitative evaluation based on Rouge metric demonstrates the effectiveness and advantages of our method.</p><p>3 0.054396953 <a title="43-tfidf-3" href="./emnlp-2011-Personalized_Recommendation_of_User_Comments_via_Factor_Models.html">104 emnlp-2011-Personalized Recommendation of User Comments via Factor Models</a></p>
<p>Author: Deepak Agarwal ; Bee-Chung Chen ; Bo Pang</p><p>Abstract: In recent years, the amount of user-generated opinionated texts (e.g., reviews, user comments) continues to grow at a rapid speed: featured news stories on a major event easily attract thousands of user comments on a popular online News service. How to consume subjective information ofthis volume becomes an interesting and important research question. In contrast to previous work on review analysis that tried to filter or summarize information for a generic average user, we explore a different direction of enabling personalized recommendation of such information. For each user, our task is to rank the comments associated with a given article according to personalized user preference (i.e., whether the user is likely to like or dislike the comment). To this end, we propose a factor model that incorporates rater-comment and rater-author interactions simultaneously in a principled way. Our full model significantly outperforms strong baselines as well as related models that have been considered in previous work.</p><p>4 0.044731934 <a title="43-tfidf-4" href="./emnlp-2011-Semantic_Topic_Models%3A_Combining_Word_Distributional_Statistics_and_Dictionary_Definitions.html">119 emnlp-2011-Semantic Topic Models: Combining Word Distributional Statistics and Dictionary Definitions</a></p>
<p>Author: Weiwei Guo ; Mona Diab</p><p>Abstract: In this paper, we propose a novel topic model based on incorporating dictionary definitions. Traditional topic models treat words as surface strings without assuming predefined knowledge about word meaning. They infer topics only by observing surface word co-occurrence. However, the co-occurred words may not be semantically related in a manner that is relevant for topic coherence. Exploiting dictionary definitions explicitly in our model yields a better understanding of word semantics leading to better text modeling. We exploit WordNet as a lexical resource for sense definitions. We show that explicitly modeling word definitions helps improve performance significantly over the baseline for a text categorization task.</p><p>5 0.040723011 <a title="43-tfidf-5" href="./emnlp-2011-Semi-Supervised_Recursive_Autoencoders_for_Predicting_Sentiment_Distributions.html">120 emnlp-2011-Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</a></p>
<p>Author: Richard Socher ; Jeffrey Pennington ; Eric H. Huang ; Andrew Y. Ng ; Christopher D. Manning</p><p>Abstract: We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model’s ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.</p><p>6 0.038330298 <a title="43-tfidf-6" href="./emnlp-2011-Harnessing_WordNet_Senses_for_Supervised_Sentiment_Classification.html">63 emnlp-2011-Harnessing WordNet Senses for Supervised Sentiment Classification</a></p>
<p>7 0.037793394 <a title="43-tfidf-7" href="./emnlp-2011-Relation_Acquisition_using_Word_Classes_and_Partial_Patterns.html">113 emnlp-2011-Relation Acquisition using Word Classes and Partial Patterns</a></p>
<p>8 0.037741739 <a title="43-tfidf-8" href="./emnlp-2011-Cooooooooooooooollllllllllllll%21%21%21%21%21%21%21%21%21%21%21%21%21%21_Using_Word_Lengthening_to_Detect_Sentiment_in_Microblogs.html">33 emnlp-2011-Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using Word Lengthening to Detect Sentiment in Microblogs</a></p>
<p>9 0.037194658 <a title="43-tfidf-9" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>10 0.0371731 <a title="43-tfidf-10" href="./emnlp-2011-Bootstrapped_Named_Entity_Recognition_for_Product_Attribute_Extraction.html">23 emnlp-2011-Bootstrapped Named Entity Recognition for Product Attribute Extraction</a></p>
<p>11 0.0354953 <a title="43-tfidf-11" href="./emnlp-2011-Hierarchical_Verb_Clustering_Using_Graph_Factorization.html">67 emnlp-2011-Hierarchical Verb Clustering Using Graph Factorization</a></p>
<p>12 0.035051368 <a title="43-tfidf-12" href="./emnlp-2011-Structural_Opinion_Mining_for_Graph-based_Sentiment_Representation.html">126 emnlp-2011-Structural Opinion Mining for Graph-based Sentiment Representation</a></p>
<p>13 0.033062458 <a title="43-tfidf-13" href="./emnlp-2011-Relation_Extraction_with_Relation_Topics.html">114 emnlp-2011-Relation Extraction with Relation Topics</a></p>
<p>14 0.028611246 <a title="43-tfidf-14" href="./emnlp-2011-Large-Scale_Noun_Compound_Interpretation_Using_Bootstrapping_and_the_Web_as_a_Corpus.html">78 emnlp-2011-Large-Scale Noun Compound Interpretation Using Bootstrapping and the Web as a Corpus</a></p>
<p>15 0.028168656 <a title="43-tfidf-15" href="./emnlp-2011-Random_Walk_Inference_and_Learning_in_A_Large_Scale_Knowledge_Base.html">109 emnlp-2011-Random Walk Inference and Learning in A Large Scale Knowledge Base</a></p>
<p>16 0.027970081 <a title="43-tfidf-16" href="./emnlp-2011-Harnessing_different_knowledge_sources_to_measure_semantic_relatedness_under_a_uniform_model.html">64 emnlp-2011-Harnessing different knowledge sources to measure semantic relatedness under a uniform model</a></p>
<p>17 0.027695239 <a title="43-tfidf-17" href="./emnlp-2011-Extreme_Extraction_-_Machine_Reading_in_a_Week.html">57 emnlp-2011-Extreme Extraction - Machine Reading in a Week</a></p>
<p>18 0.024102733 <a title="43-tfidf-18" href="./emnlp-2011-Refining_the_Notions_of_Depth_and_Density_in_WordNet-based_Semantic_Similarity_Measures.html">112 emnlp-2011-Refining the Notions of Depth and Density in WordNet-based Semantic Similarity Measures</a></p>
<p>19 0.023666024 <a title="43-tfidf-19" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>20 0.023280108 <a title="43-tfidf-20" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.095), (1, -0.071), (2, -0.035), (3, -0.019), (4, 0.037), (5, 0.001), (6, 0.023), (7, -0.007), (8, 0.022), (9, 0.02), (10, 0.005), (11, -0.062), (12, -0.057), (13, 0.038), (14, 0.142), (15, -0.004), (16, -0.134), (17, -0.003), (18, 0.019), (19, 0.086), (20, 0.091), (21, -0.035), (22, 0.007), (23, -0.025), (24, -0.001), (25, 0.042), (26, 0.158), (27, 0.008), (28, -0.021), (29, -0.004), (30, -0.024), (31, 0.035), (32, -0.168), (33, -0.058), (34, -0.021), (35, -0.155), (36, 0.093), (37, -0.525), (38, 0.124), (39, 0.236), (40, -0.11), (41, -0.04), (42, 0.03), (43, 0.163), (44, 0.036), (45, -0.216), (46, -0.256), (47, 0.011), (48, 0.069), (49, -0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99007112 <a title="43-lsi-1" href="./emnlp-2011-Domain-Assisted_Product_Aspect_Hierarchy_Generation%3A_Towards_Hierarchical_Organization_of_Unstructured_Consumer_Reviews.html">43 emnlp-2011-Domain-Assisted Product Aspect Hierarchy Generation: Towards Hierarchical Organization of Unstructured Consumer Reviews</a></p>
<p>Author: Jianxing Yu ; Zheng-Jun Zha ; Meng Wang ; Kai Wang ; Tat-Seng Chua</p><p>Abstract: This paper presents a domain-assisted approach to organize various aspects of a product into a hierarchy by integrating domain knowledge (e.g., the product specifications), as well as consumer reviews. Based on the derived hierarchy, we generate a hierarchical organization of consumer reviews on various product aspects and aggregate consumer opinions on these aspects. With such organization, user can easily grasp the overview of consumer reviews. Furthermore, we apply the hierarchy to the task of implicit aspect identification which aims to infer implicit aspects of the reviews that do not explicitly express those aspects but actually comment on them. The experimental results on 11popular products in four domains demonstrate the effectiveness of our approach.</p><p>2 0.68113834 <a title="43-lsi-2" href="./emnlp-2011-Generating_Aspect-oriented_Multi-Document_Summarization_with_Event-aspect_model.html">61 emnlp-2011-Generating Aspect-oriented Multi-Document Summarization with Event-aspect model</a></p>
<p>Author: Peng Li ; Yinglin Wang ; Wei Gao ; Jing Jiang</p><p>Abstract: In this paper, we propose a novel approach to automatic generation of aspect-oriented summaries from multiple documents. We first develop an event-aspect LDA model to cluster sentences into aspects. We then use extended LexRank algorithm to rank the sentences in each cluster. We use Integer Linear Programming for sentence selection. Key features of our method include automatic grouping of semantically related sentences and sentence ranking based on extension of random walk model. Also, we implement a new sentence compression algorithm which use dependency tree instead of parser tree. We compare our method with four baseline methods. Quantitative evaluation based on Rouge metric demonstrates the effectiveness and advantages of our method.</p><p>3 0.22567901 <a title="43-lsi-3" href="./emnlp-2011-Hierarchical_Verb_Clustering_Using_Graph_Factorization.html">67 emnlp-2011-Hierarchical Verb Clustering Using Graph Factorization</a></p>
<p>Author: Lin Sun ; Anna Korhonen</p><p>Abstract: Most previous research on verb clustering has focussed on acquiring flat classifications from corpus data, although many manually built classifications are taxonomic in nature. Also Natural Language Processing (NLP) applications benefit from taxonomic classifications because they vary in terms of the granularity they require from a classification. We introduce a new clustering method called Hierarchical Graph Factorization Clustering (HGFC) and extend it so that it is optimal for the task. Our results show that HGFC outperforms the frequently used agglomerative clustering on a hierarchical test set extracted from VerbNet, and that it yields state-of-the-art performance also on a flat test set. We demonstrate how the method can be used to acquire novel classifications as well as to extend existing ones on the basis of some prior knowledge about the classification.</p><p>4 0.20192775 <a title="43-lsi-4" href="./emnlp-2011-Personalized_Recommendation_of_User_Comments_via_Factor_Models.html">104 emnlp-2011-Personalized Recommendation of User Comments via Factor Models</a></p>
<p>Author: Deepak Agarwal ; Bee-Chung Chen ; Bo Pang</p><p>Abstract: In recent years, the amount of user-generated opinionated texts (e.g., reviews, user comments) continues to grow at a rapid speed: featured news stories on a major event easily attract thousands of user comments on a popular online News service. How to consume subjective information ofthis volume becomes an interesting and important research question. In contrast to previous work on review analysis that tried to filter or summarize information for a generic average user, we explore a different direction of enabling personalized recommendation of such information. For each user, our task is to rank the comments associated with a given article according to personalized user preference (i.e., whether the user is likely to like or dislike the comment). To this end, we propose a factor model that incorporates rater-comment and rater-author interactions simultaneously in a principled way. Our full model significantly outperforms strong baselines as well as related models that have been considered in previous work.</p><p>5 0.19627041 <a title="43-lsi-5" href="./emnlp-2011-Harnessing_WordNet_Senses_for_Supervised_Sentiment_Classification.html">63 emnlp-2011-Harnessing WordNet Senses for Supervised Sentiment Classification</a></p>
<p>Author: Balamurali AR ; Aditya Joshi ; Pushpak Bhattacharyya</p><p>Abstract: Traditional approaches to sentiment classification rely on lexical features, syntax-based features or a combination of the two. We propose semantic features using word senses for a supervised document-level sentiment classifier. To highlight the benefit of sense-based features, we compare word-based representation of documents with a sense-based representation where WordNet senses of the words are used as features. In addition, we highlight the benefit of senses by presenting a part-ofspeech-wise effect on sentiment classification. Finally, we show that even if a WSD engine disambiguates between a limited set of words in a document, a sentiment classifier still performs better than what it does in absence of sense annotation. Since word senses used as features show promise, we also examine the possibility of using similarity metrics defined on WordNet to address the problem of not finding a sense in the training corpus. We per- form experiments using three popular similarity metrics to mitigate the effect of unknown synsets in a test corpus by replacing them with similar synsets from the training corpus. The results show promising improvement with respect to the baseline.</p><p>6 0.18339881 <a title="43-lsi-6" href="./emnlp-2011-Approximate_Scalable_Bounded_Space_Sketch_for_Large_Data_NLP.html">19 emnlp-2011-Approximate Scalable Bounded Space Sketch for Large Data NLP</a></p>
<p>7 0.16225758 <a title="43-lsi-7" href="./emnlp-2011-Relation_Acquisition_using_Word_Classes_and_Partial_Patterns.html">113 emnlp-2011-Relation Acquisition using Word Classes and Partial Patterns</a></p>
<p>8 0.15785433 <a title="43-lsi-8" href="./emnlp-2011-Computing_Logical_Form_on_Regulatory_Texts.html">32 emnlp-2011-Computing Logical Form on Regulatory Texts</a></p>
<p>9 0.15537091 <a title="43-lsi-9" href="./emnlp-2011-Bootstrapped_Named_Entity_Recognition_for_Product_Attribute_Extraction.html">23 emnlp-2011-Bootstrapped Named Entity Recognition for Product Attribute Extraction</a></p>
<p>10 0.15490845 <a title="43-lsi-10" href="./emnlp-2011-Semi-Supervised_Recursive_Autoencoders_for_Predicting_Sentiment_Distributions.html">120 emnlp-2011-Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</a></p>
<p>11 0.14760503 <a title="43-lsi-11" href="./emnlp-2011-Structural_Opinion_Mining_for_Graph-based_Sentiment_Representation.html">126 emnlp-2011-Structural Opinion Mining for Graph-based Sentiment Representation</a></p>
<p>12 0.14390393 <a title="43-lsi-12" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<p>13 0.13404012 <a title="43-lsi-13" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>14 0.13143916 <a title="43-lsi-14" href="./emnlp-2011-Literal_and_Metaphorical_Sense_Identification_through_Concrete_and_Abstract_Context.html">91 emnlp-2011-Literal and Metaphorical Sense Identification through Concrete and Abstract Context</a></p>
<p>15 0.12694749 <a title="43-lsi-15" href="./emnlp-2011-Large-Scale_Noun_Compound_Interpretation_Using_Bootstrapping_and_the_Web_as_a_Corpus.html">78 emnlp-2011-Large-Scale Noun Compound Interpretation Using Bootstrapping and the Web as a Corpus</a></p>
<p>16 0.12656286 <a title="43-lsi-16" href="./emnlp-2011-Corpus-Guided_Sentence_Generation_of_Natural_Images.html">34 emnlp-2011-Corpus-Guided Sentence Generation of Natural Images</a></p>
<p>17 0.11929027 <a title="43-lsi-17" href="./emnlp-2011-Enhancing_Chinese_Word_Segmentation_Using_Unlabeled_Data.html">48 emnlp-2011-Enhancing Chinese Word Segmentation Using Unlabeled Data</a></p>
<p>18 0.1164858 <a title="43-lsi-18" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>19 0.10825834 <a title="43-lsi-19" href="./emnlp-2011-Minimally_Supervised_Event_Causality_Identification.html">92 emnlp-2011-Minimally Supervised Event Causality Identification</a></p>
<p>20 0.10774303 <a title="43-lsi-20" href="./emnlp-2011-Structured_Sparsity_in_Structured_Prediction.html">129 emnlp-2011-Structured Sparsity in Structured Prediction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.1), (36, 0.014), (37, 0.018), (39, 0.413), (45, 0.078), (54, 0.016), (57, 0.032), (62, 0.039), (64, 0.017), (66, 0.024), (69, 0.017), (79, 0.046), (82, 0.014), (90, 0.012), (96, 0.024), (98, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80740023 <a title="43-lda-1" href="./emnlp-2011-Domain-Assisted_Product_Aspect_Hierarchy_Generation%3A_Towards_Hierarchical_Organization_of_Unstructured_Consumer_Reviews.html">43 emnlp-2011-Domain-Assisted Product Aspect Hierarchy Generation: Towards Hierarchical Organization of Unstructured Consumer Reviews</a></p>
<p>Author: Jianxing Yu ; Zheng-Jun Zha ; Meng Wang ; Kai Wang ; Tat-Seng Chua</p><p>Abstract: This paper presents a domain-assisted approach to organize various aspects of a product into a hierarchy by integrating domain knowledge (e.g., the product specifications), as well as consumer reviews. Based on the derived hierarchy, we generate a hierarchical organization of consumer reviews on various product aspects and aggregate consumer opinions on these aspects. With such organization, user can easily grasp the overview of consumer reviews. Furthermore, we apply the hierarchy to the task of implicit aspect identification which aims to infer implicit aspects of the reviews that do not explicitly express those aspects but actually comment on them. The experimental results on 11popular products in four domains demonstrate the effectiveness of our approach.</p><p>2 0.61475289 <a title="43-lda-2" href="./emnlp-2011-A_Non-negative_Matrix_Factorization_Based_Approach_for_Active_Dual_Supervision_from_Document_and_Word_Labels.html">9 emnlp-2011-A Non-negative Matrix Factorization Based Approach for Active Dual Supervision from Document and Word Labels</a></p>
<p>Author: Chao Shen ; Tao Li</p><p>Abstract: In active dual supervision, not only informative examples but also features are selected for labeling to build a high quality classifier with low cost. However, how to measure the informativeness for both examples and feature on the same scale has not been well solved. In this paper, we propose a non-negative matrix factorization based approach to address this issue. We first extend the matrix factorization framework to explicitly model the corresponding relationships between feature classes and examples classes. Then by making use of the reconstruction error, we propose a unified scheme to determine which feature or example a classifier is most likely to benefit from having labeled. Empirical results demonstrate the effectiveness of our proposed methods.</p><p>3 0.55281687 <a title="43-lda-3" href="./emnlp-2011-Cache-based_Document-level_Statistical_Machine_Translation.html">25 emnlp-2011-Cache-based Document-level Statistical Machine Translation</a></p>
<p>Author: Zhengxian Gong ; Min Zhang ; Guodong Zhou</p><p>Abstract: Statistical machine translation systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time, ignoring document-level information. In this paper, we propose a cache-based approach to document-level translation. Since caches mainly depend on relevant data to supervise subsequent decisions, it is critical to fill the caches with highly-relevant data of a reasonable size. In this paper, we present three kinds of caches to store relevant document-level information: 1) a dynamic cache, which stores bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document; 2) a static cache, which stores relevant bilingual phrase pairs extracted from similar bilingual document pairs (i.e. source documents similar to the test document and their corresponding target documents) in the training parallel corpus; 3) a topic cache, which stores the target-side topic words related with the test document in the source-side. In particular, three new features are designed to explore various kinds of document-level information in above three kinds of caches. Evaluation shows the effectiveness of our cache-based approach to document-level translation with the performance improvement of 0.8 1 in BLUE score over Moses. Especially, detailed analysis and discussion are presented to give new insights to document-level translation. 1</p><p>4 0.32987979 <a title="43-lda-4" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>Author: Limin Yao ; Aria Haghighi ; Sebastian Riedel ; Andrew McCallum</p><p>Abstract: We explore unsupervised approaches to relation extraction between two named entities; for instance, the semantic bornIn relation between a person and location entity. Concretely, we propose a series of generative probabilistic models, broadly similar to topic models, each which generates a corpus of observed triples of entity mention pairs and the surface syntactic dependency path between them. The output of each model is a clustering of observed relation tuples and their associated textual expressions to underlying semantic relation types. Our proposed models exploit entity type constraints within a relation as well as features on the dependency path between entity mentions. We examine effectiveness of our approach via multiple evaluations and demonstrate 12% error reduction in precision over a state-of-the-art weakly supervised baseline.</p><p>5 0.32495341 <a title="43-lda-5" href="./emnlp-2011-Relation_Extraction_with_Relation_Topics.html">114 emnlp-2011-Relation Extraction with Relation Topics</a></p>
<p>Author: Chang Wang ; James Fan ; Aditya Kalyanpur ; David Gondek</p><p>Abstract: This paper describes a novel approach to the semantic relation detection problem. Instead of relying only on the training instances for a new relation, we leverage the knowledge learned from previously trained relation detectors. Specifically, we detect a new semantic relation by projecting the new relation’s training instances onto a lower dimension topic space constructed from existing relation detectors through a three step process. First, we construct a large relation repository of more than 7,000 relations from Wikipedia. Second, we construct a set of non-redundant relation topics defined at multiple scales from the relation repository to characterize the existing relations. Similar to the topics defined over words, each relation topic is an interpretable multinomial distribution over the existing relations. Third, we integrate the relation topics in a kernel function, and use it together with SVM to construct detectors for new relations. The experimental results on Wikipedia and ACE data have confirmed that backgroundknowledge-based topics generated from the Wikipedia relation repository can significantly improve the performance over the state-of-theart relation detection approaches.</p><p>6 0.32454774 <a title="43-lda-6" href="./emnlp-2011-Extreme_Extraction_-_Machine_Reading_in_a_Week.html">57 emnlp-2011-Extreme Extraction - Machine Reading in a Week</a></p>
<p>7 0.32426643 <a title="43-lda-7" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>8 0.32086721 <a title="43-lda-8" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>9 0.32076985 <a title="43-lda-9" href="./emnlp-2011-Identifying_Relations_for_Open_Information_Extraction.html">70 emnlp-2011-Identifying Relations for Open Information Extraction</a></p>
<p>10 0.3203882 <a title="43-lda-10" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>11 0.32027659 <a title="43-lda-11" href="./emnlp-2011-Named_Entity_Recognition_in_Tweets%3A_An_Experimental_Study.html">98 emnlp-2011-Named Entity Recognition in Tweets: An Experimental Study</a></p>
<p>12 0.31778345 <a title="43-lda-12" href="./emnlp-2011-Syntax-Based_Grammaticality_Improvement_using_CCG_and_Guided_Search.html">132 emnlp-2011-Syntax-Based Grammaticality Improvement using CCG and Guided Search</a></p>
<p>13 0.31777126 <a title="43-lda-13" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>14 0.31740695 <a title="43-lda-14" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>15 0.31597254 <a title="43-lda-15" href="./emnlp-2011-Cooooooooooooooollllllllllllll%21%21%21%21%21%21%21%21%21%21%21%21%21%21_Using_Word_Lengthening_to_Detect_Sentiment_in_Microblogs.html">33 emnlp-2011-Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using Word Lengthening to Detect Sentiment in Microblogs</a></p>
<p>16 0.3159121 <a title="43-lda-16" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>17 0.31588644 <a title="43-lda-17" href="./emnlp-2011-A_Simple_Word_Trigger_Method_for_Social_Tag_Suggestion.html">11 emnlp-2011-A Simple Word Trigger Method for Social Tag Suggestion</a></p>
<p>18 0.31570733 <a title="43-lda-18" href="./emnlp-2011-Heuristic_Search_for_Non-Bottom-Up_Tree_Structure_Prediction.html">65 emnlp-2011-Heuristic Search for Non-Bottom-Up Tree Structure Prediction</a></p>
<p>19 0.31566632 <a title="43-lda-19" href="./emnlp-2011-Generating_Aspect-oriented_Multi-Document_Summarization_with_Event-aspect_model.html">61 emnlp-2011-Generating Aspect-oriented Multi-Document Summarization with Event-aspect model</a></p>
<p>20 0.31506744 <a title="43-lda-20" href="./emnlp-2011-Cross-Cutting_Models_of_Lexical_Semantics.html">37 emnlp-2011-Cross-Cutting Models of Lexical Semantics</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
