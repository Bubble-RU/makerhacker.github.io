<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>127 emnlp-2011-Structured Lexical Similarity via Convolution Kernels on Dependency Trees</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-127" href="#">emnlp2011-127</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>127 emnlp-2011-Structured Lexical Similarity via Convolution Kernels on Dependency Trees</h1>
<br/><p>Source: <a title="emnlp-2011-127-pdf" href="http://aclweb.org/anthology//D/D11/D11-1096.pdf">pdf</a></p><p>Author: Danilo Croce ; Alessandro Moschitti ; Roberto Basili</p><p>Abstract: Alessandro Moschitti DISI University of Trento 38123 Povo (TN), Italy mo s chitt i di s i @ .unit n . it Roberto Basili DII University of Tor Vergata 00133 Roma, Italy bas i i info .uni roma2 . it l@ over semantic networks, e.g. (Cowie et al., 1992; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, A central topic in natural language processing is the design of lexical and syntactic fea- tures suitable for the target application. In this paper, we study convolution dependency tree kernels for automatic engineering of syntactic and semantic patterns exploiting lexical similarities. We define efficient and powerful kernels for measuring the similarity between dependency structures, whose surface forms of the lexical nodes are in part or completely different. The experiments with such kernels for question classification show an unprecedented results, e.g. 41% of error reduction of the former state-of-the-art. Additionally, semantic role classification confirms the benefit of semantic smoothing for dependency kernels.</p><p>Reference: <a title="emnlp-2011-127-reference" href="../emnlp2011_reference/emnlp-2011-Structured_Lexical_Similarity_via_Convolution_Kernels_on_Dependency_Trees_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we study convolution dependency tree kernels for automatic engineering of syntactic and semantic patterns exploiting lexical similarities. [sent-11, score-0.708]
</p><p>2 We define efficient and powerful kernels for measuring the similarity between dependency structures, whose surface forms of the lexical nodes are in part or completely different. [sent-12, score-0.557]
</p><p>3 The experiments with such kernels for question classification show an unprecedented results, e. [sent-13, score-0.317]
</p><p>4 Additionally, semantic role classification confirms the benefit of semantic smoothing for dependency kernels. [sent-16, score-0.225]
</p><p>5 A semantic similarity can be defined at structural level over a graph, e. [sent-28, score-0.214]
</p><p>6 , 2009), as well as combining structural and lexical similarity 1034 1997; Schtze, 1998; Leacock and Chodorow, 1998; Pedersen et al. [sent-31, score-0.218]
</p><p>7 On the other hand, automatic feature engineering of syntactic or shallow semantic structures has been carried out by means of structural kernels, e. [sent-38, score-0.231]
</p><p>8 The main idea of structural kernels is to generate structures that in turn represent syntactic or shallow semantic features. [sent-48, score-0.514]
</p><p>9 Most notably, the work in (Bloehdorn and Moschitti, 2007b) encodes lexical similarity in such kernels. [sent-49, score-0.159]
</p><p>10 This is essentially the syntactic tree kernel (STK) proposed in (Collins and Duffy, 2002) in which syntactic fragments from constituency trees can be matched even if they only differ in the leaf nodes (i. [sent-50, score-0.646]
</p><p>11 This implies matching scores lower than 1, depending on the semantic similarity of the corresponding leaves in the syntactic fragments. [sent-53, score-0.243]
</p><p>12 Although this kernel achieves state-of-the-art performance in NLP tasks, such as Question ClassificaProce dEindgisnb oufr tgh e, 2 S0c1o1tl Canodn,f eUrKen,c Jeuol yn 2 E7m–3p1ir,ic 2a0l1 M1. [sent-54, score-0.175]
</p><p>13 , trivially STK  only matches the syntactic structure apple/orange when comparing the big beautiful apple to a nice large orange; and (ii) STK cannot be effectively applied to dependency structures, e. [sent-59, score-0.109]
</p><p>14 Additionally, to our knowledge, there is no previous study that clearly describes how dependency structures should be converted in trees to be fully and effectively exploitable by convolution kernels. [sent-62, score-0.34]
</p><p>15 Indeed, although the work in (Culotta and Sorensen, 2004) defines a dependency tree also using node similarity, it is not a convolution kernel: this results in a much poorer feature space. [sent-63, score-0.313]
</p><p>16 In this paper, we propose a study of convolution kernels for dependency structures aiming at jointly modeling syntactic and lexical semantic similarity. [sent-64, score-0.673]
</p><p>17 More precisely, we define several dependency trees exploitable by the Partial Tree Kernel (PTK) (Moschitti, 2006a) and compared them with STK over constituency trees. [sent-65, score-0.212]
</p><p>18 the Smoothed Partial Tree Kernels (SPTKs), which can measure the similarity of structural similar trees whose nodes are associated with different but related lexicals. [sent-68, score-0.275]
</p><p>19 Given the convolution nature of such  kernels any possible node path of lexicals provide a contribution smoothed by the similarity accounted by its nodes. [sent-69, score-0.704]
</p><p>20 In the reminder of this paper, Section 2 provides the background for structural and lexical similarity kernels. [sent-71, score-0.218]
</p><p>21 This in several cases can be efficiently and implicitly computed by kernel functions by exploiting the following dual formulation: Pi=1. [sent-77, score-0.175]
</p><p>22 l yiαiφ(oi)φ(o) + b = 0, where oi and o are Ptwo objects, φ is a mapping from the objects to featPure vectors x~ i and φ(oi)φ(o) = K(oi, o) is a kernel function implicitly defining such mapping. [sent-79, score-0.206]
</p><p>23 The most general kind of kernels used in NLP are string kernels, e. [sent-81, score-0.283]
</p><p>24 , f|F| } be a tree fragment space and  (n) f be an indicator} function, equal tto s 1a cief atnhed target fi is rooted at node n and equal to 0 otherwise. [sent-104, score-0.186]
</p><p>25 The ∆ function determines the richness of the kernel space and thus different tree kernels. [sent-107, score-0.286]
</p><p>26 O(|NT1 | + |NT2 |), for natural language syntactic trees (Moschitti, 2006a). [sent-114, score-0.102]
</p><p>27 I~1,~I2,l(XI~1)=l(I~2)  Yj=1  1To have a similarity score between 0 and 1, a normalization in the kernel space, i. [sent-119, score-0.283]
</p><p>28 This way, we penalize both larger trees and child subsequences with gaps. [sent-123, score-0.175]
</p><p>29 PTK is more general than the STK as if we only consider the contribution of shared subsequences containing all children of nodes, we implement the STK kernel. [sent-124, score-0.155]
</p><p>30 3 Lexical Semantic Kernel Given two text fragments d1 and d2 ∈ D (the text fragment set), a general lexical kernel∈ (Basili eet al. [sent-128, score-0.14]
</p><p>31 , 2005) defines their similarity as: K(d1,d2)  =  X w1∈dX1 X,w2 ∈d2  (ω1ω2)  σ(w1,w2)  (1)  where ω1 and ω2 are the weights of the words (features) w1 and w2 in the documents d1 and d2, respectively, and σ is a term similarity function, e. [sent-129, score-0.216]
</p><p>32 We determine the term similarity function through distributional analysis (Pado and Lapata, 2007), ac-  cording to the idea that the meaning of a word can be described by the set of textual contexts in which it appears (Distributional Hypothesis, (Harris, 1964)). [sent-135, score-0.108]
</p><p>33 Therefore, given two words w1 and w2, the term similarity function σ is estimated as the cosine similarity between the corresponding projections w~ 1 , w~ 2,  in. [sent-142, score-0.216]
</p><p>34 Another methods to design a valid kernel is to represent words as word vectors and compute σ as their scalar product between such vectors. [sent-146, score-0.175]
</p><p>35 We will refer to such similarity as WL (word list). [sent-149, score-0.108]
</p><p>36 3 Smoothing Partial Tree Kernel (SPTK) Combining lexical and structural kernels provides clear advantages on all-vs-all words similarity, which tends to semantically diverge. [sent-150, score-0.393]
</p><p>37 Following this idea, Bloedhorn & Moschitti (2007a) modified step (i) of ∆STK computation as follows: (i) if n1 and n2 are pre-terminal nodes with the same number of children, ∆STK (n1, n2) =  Qjn=c(1n1)  λ σ(lex(n1), lex(n2)), where lex returns theQ node label. [sent-152, score-0.124]
</p><p>38 This allows to match fragments having same structure but different leaves by assigning a score proportional to the product of the lexical sim-  ×  ilarities of each leaf pair. [sent-153, score-0.189]
</p><p>39 Although it is an interesting kernel, the fact that lexicals must belong to the leaf nodes of exactly the same structures limits its applications. [sent-154, score-0.312]
</p><p>40 Hereafter, we define a much more general smoothed tree kernel that can be applied to any tree and exploit any combination of lexical similarities, respecting the syntax enforced by the tree. [sent-156, score-0.488]
</p><p>41 ,  (2)  1037 where σ is any similarity between nodes, e. [sent-160, score-0.108]
</p><p>42 ubtrees rooted in subsequences of exactly p children (of n1 and n2) and m = min{l(cn1) , l(cn2)}. [sent-196, score-0.155]
</p><p>43 s2b = cn2 (a and b are the last children)  ∆p(s1a,s2b) = ∆(a,b) ×X|s1|X|s2|λ|s1|−i+|s2|−r× iX= X1  rX=  X1  ∆p−1 (s1[1 : i] , s2 [1 : r] ) where s1[1 : i] and s2 [1 : r] are the child subsequences from 1 to iand from 1 to r of s1 and s2. [sent-198, score-0.122]
</p><p>44 0; Note that Dp satisfies the recursive relation: Dp(k, l) = ∆p−1 (s1[1 : k], s2 [1 : l]) + λDp(k, l− 1) +λDp(k − 1, l) − λ2Dp(k − 1, l− 1) By means of the abo(vke − −re 1la,tli)o −n, we can compute )the child subsequences of two sequences s1 and s2 in O(p| s1| | s2 |). [sent-203, score-0.154]
</p><p>45 The latter is very small in natural language parse trees and we also avoid the computation of node pairs with non similar labels. [sent-208, score-0.122]
</p><p>46 We note that PTK generalizes both (i) SK, allowing the similarity between sequences (node children) structured in a tree and (ii) STK, allowing the computation of STK over any possible pair of subtrees extracted from the original tree. [sent-209, score-0.285]
</p><p>47 4 Innovative Features of SPTK The most similar kernel to SPTK is the Syntactic Semantic Tree Kernel (SSTK) proposed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b). [sent-212, score-0.175]
</p><p>48 However, the following aspects show the remarkable innovativeness of SPTK: •  •  SSTK can only work on constituency trees aSnSdT Knot c on dependency trees (see (Moschitti, 2006a)). [sent-213, score-0.221]
</p><p>49 The lexical similarity in SSTK is only applied tToh ele laefx ncaolde ssim i lna exactly StTheK same syntactic 1038 constituents. [sent-214, score-0.208]
</p><p>50 ”television system” and ”video system” (so also exploiting the meaningful similarity between “video” and “television”). [sent-220, score-0.108]
</p><p>51 •  •  The similarity in the PTK equation is added sTuhceh tihmaitl aSrPitTyK in st thilel corresponds to a ascddaleadr product in the semantic/structure space2. [sent-221, score-0.108]
</p><p>52 In case of PTK and SPTK different tree representations may lead to engineer more or less effective syntactic/semantic feature spaces. [sent-224, score-0.111]
</p><p>53 The next two sections provide our representation models for dependency trees and their discussion. [sent-225, score-0.113]
</p><p>54 1 Proposed Computational Structures Given the following sentence: (s1) What is the width of a football field? [sent-227, score-0.166]
</p><p>55 The representation tree for a phrase structure paradigm leaves little room for variations as shown  by the constituency tree (CT) in Figure 1. [sent-228, score-0.316]
</p><p>56 We apply lemmatization to the lexicals to improve generalization and, at the same time, we add a generalized PoS-tag, i. [sent-229, score-0.131]
</p><p>57 This is useful to measure similarity between lexicals belonging to the same grammatical category. [sent-232, score-0.239]
</p><p>58 In contrast, the conversion of dependency structures in computationally effective trees (for the above kernels) is not straightforward. [sent-233, score-0.189]
</p><p>59 ROOT VBZ PRD NN P  Figure 4: Lexical Centered Tree (LCT) to associate edges with dependencies but, since our kernels cannot process labels on the arcs, they must be associated with tree nodes. [sent-246, score-0.394]
</p><p>60 see Figure 3, where the PoS-Tags are children of GR nodes and fathers of their associated lexicals; and the Lexical Centered Tree (LCT), e. [sent-259, score-0.119]
</p><p>61 see Figure 6, which ignores the syntactic structure of the sentence being a simple sequence of PoS-Tag nodes, where lexicals are simply added as children; and the Lexical Sequence Tree (LST), where only lexical items are leaves of a single root node. [sent-270, score-0.302]
</p><p>62 paths only composed by similar lexical nodes constrained by syntactic dependencies. [sent-288, score-0.155]
</p><p>63 All the other trees produce fragments in which lexicals play the role of features of GR or PoS-Tag nodes. [sent-289, score-0.27]
</p><p>64 At the same time, we compare with the constituency trees and different kernels to derive the best syntactic paradigm for convolution  kernels. [sent-293, score-0.547]
</p><p>65 Most importantly, the role of lexical similarity embedded in syntactic structures will be investigated. [sent-294, score-0.321]
</p><p>66 htm 1040 cludes structural kernels in SVMLight (Joachims, 2000)) with the smooth match between tree nodes. [sent-302, score-0.453]
</p><p>67 For generating constituency trees, we used the Charniak parser (Charniak, 2000) whereas we applied LTH syntactic parser (described in (Johansson and Nugues, 2008a)) to generate dependency trees. [sent-303, score-0.164]
</p><p>68 The second approach uses the similarity based on word list (WL) as provided in (Li and Roth, 2002). [sent-317, score-0.108]
</p><p>69 Models: SVM-LightTK is applied to the different tree representations discussed in Section 4. [sent-318, score-0.111]
</p><p>70 LCTWL is the SPTK kernel applied to LCT structure, using WL similarity. [sent-324, score-0.175]
</p><p>71 The outcome of the several kernels applied to several structures for the coarse and fine grained QC is reported in Table 1. [sent-338, score-0.427]
</p><p>72 STK applied to a constituency tree and BOW, which is a linear ker-  4http://cogcomp. [sent-343, score-0.166]
</p><p>73 It is worth nothing that when no similarity is applied: (i) BOW produces high accuracy, i. [sent-351, score-0.108]
</p><p>74 , 2007)); (ii) PTK applied to the same tree of STK produces a slightly lower value (non-statistically significant difference); (iii) interestingly, when PTK is instead applied to dependency structures, it im-  art5  proves STK, i. [sent-355, score-0.171]
</p><p>75 80% since it is obviously subject to data sparseness (fragments only composed by lexicals are very sparse). [sent-361, score-0.131]
</p><p>76 The very important results can be noted when lexical similarity is used, i. [sent-362, score-0.159]
</p><p>77 3 Learning curves It is interesting to study the impact of syntactic/semantic kernels on the learning generalization. [sent-375, score-0.283]
</p><p>78 Number of Nodes  Figure 11: Micro-seconds for each kernel computation of the previous models without lexical similarity whereas Fig. [sent-378, score-0.368]
</p><p>79 We note that when no similarity is used the dependency trees better generalize than constituency trees or non-syntactic structures like LPST or BOW. [sent-380, score-0.405]
</p><p>80 When WL is activated, all models outperform the best kernel of the previous pool, i. [sent-381, score-0.175]
</p><p>81 Figure 11 shows the elapsed time in function of the number of nodes for different tree representations. [sent-389, score-0.166]
</p><p>82 We note that: (i) when the WL is not active, LCT and GRCT are very fast as they impose hierarchical matching of subtrees; (ii) when the similarity is activated, LCTWL and GRCTWL tend to match many more tree fragments thus their complexity increases. [sent-390, score-0.268]
</p><p>83 Only LPSTWL, which has no structure, matches a very large number of sequences of nodes, when the similarity is active. [sent-393, score-0.14]
</p><p>84 5 FrameNet Role Classification Experiments To verify that our findings are general and that our syntactic/semantic dependency kernels can be effectively exploited for diverse NLP tasks, we experimented with a completely different application, i. [sent-396, score-0.343]
</p><p>85 6  Final Remarks and Conclusion  In this paper, we have proposed a study on representation of dependency structures for the design of effective structural kernels. [sent-429, score-0.195]
</p><p>86 Most importantly, we  have defined a new class of kernel functions, i. [sent-430, score-0.175]
</p><p>87 These show that by exploiting the similarity between two sets of words carried out according to their dependency structure leads to an unprecedented result for QC, i. [sent-436, score-0.202]
</p><p>88 This is very valuable as previous work showed that tree kernels (TK) alone perform lower than models based on manually engineered features for SRL tasks, e. [sent-450, score-0.394]
</p><p>89 Thus for the first time in an SRL task, a general tree kernel reaches the same accuracy of heavy manual feature design. [sent-455, score-0.286]
</p><p>90 In word sense disambiguation tasks, SPTK can  generalize csoen dteisxatm according t aos syntactic acandn semantic constraints (selectional restrictions) making very effective distributional semantic approaches. [sent-461, score-0.143]
</p><p>91 Semantic kernels for text classification based on topological measures of feature similarity. [sent-487, score-0.283]
</p><p>92 A hybrid convolution tree kernel for semantic role labeling. [sent-525, score-0.477]
</p><p>93 The effect of syntactic representation on semantic role labeling. [sent-630, score-0.133]
</p><p>94 Exploiting syntactic 1045 and shallow semantic kernels for question/answer classification. [sent-663, score-0.379]
</p><p>95 A study on convolution kernels  for shallow semantic parsing. [sent-672, score-0.437]
</p><p>96 Efficient convolution kernels for dependency and constituent syntactic trees. [sent-676, score-0.499]
</p><p>97 Using information content to evaluate semantic similarity in a taxonomy. [sent-704, score-0.155]
</p><p>98 Support vector machines based on a semantic kernel for text categorization. [sent-727, score-0.222]
</p><p>99 Exploring Syntactic Features for Relation Extraction using a Convolution tree kernel. [sent-768, score-0.111]
</p><p>100 PRank: a comprehensive structural similarity measure over information networks. [sent-772, score-0.167]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sptk', 0.335), ('stk', 0.329), ('kernels', 0.283), ('moschitti', 0.271), ('ptk', 0.261), ('lct', 0.247), ('nmod', 0.204), ('kernel', 0.175), ('qc', 0.137), ('lexicals', 0.131), ('bloehdorn', 0.125), ('alessandro', 0.114), ('tree', 0.111), ('similarity', 0.108), ('convolution', 0.107), ('grct', 0.102), ('nn', 0.097), ('sk', 0.095), ('football', 0.095), ('subsequences', 0.091), ('centered', 0.082), ('wl', 0.08), ('sbj', 0.079), ('structures', 0.076), ('prd', 0.073), ('johansson', 0.071), ('width', 0.071), ('framenet', 0.069), ('wp', 0.069), ('children', 0.064), ('lsa', 0.062), ('dt', 0.061), ('dependency', 0.06), ('structural', 0.059), ('ux', 0.058), ('constituency', 0.055), ('srl', 0.055), ('nodes', 0.055), ('trees', 0.053), ('pmod', 0.053), ('lexical', 0.051), ('giuglea', 0.05), ('leaf', 0.05), ('basili', 0.049), ('fragments', 0.049), ('gr', 0.049), ('syntactic', 0.049), ('semantic', 0.047), ('roberto', 0.045), ('exploitable', 0.044), ('grctlsa', 0.044), ('lctlsa', 0.044), ('lpst', 0.044), ('sstk', 0.044), ('dp', 0.041), ('pedersen', 0.041), ('smoothed', 0.04), ('fragment', 0.04), ('cristianini', 0.039), ('grained', 0.039), ('productions', 0.039), ('leaves', 0.039), ('mehdad', 0.038), ('corley', 0.038), ('television', 0.038), ('pct', 0.038), ('role', 0.037), ('kudo', 0.036), ('node', 0.035), ('field', 0.034), ('smoothing', 0.034), ('ice', 0.034), ('video', 0.034), ('unprecedented', 0.034), ('computation', 0.034), ('root', 0.032), ('sequences', 0.032), ('copy', 0.032), ('vbp', 0.032), ('innovative', 0.032), ('oi', 0.031), ('child', 0.031), ('zhang', 0.03), ('mihalcea', 0.03), ('tk', 0.03), ('fine', 0.029), ('bunke', 0.029), ('cowie', 0.029), ('croce', 0.029), ('dii', 0.029), ('film', 0.029), ('golub', 0.029), ('lctwl', 0.029), ('lew', 0.029), ('loct', 0.029), ('qlj', 0.029), ('roma', 0.029), ('schtze', 0.029), ('sptks', 0.029), ('vnmodnnnmod', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="127-tfidf-1" href="./emnlp-2011-Structured_Lexical_Similarity_via_Convolution_Kernels_on_Dependency_Trees.html">127 emnlp-2011-Structured Lexical Similarity via Convolution Kernels on Dependency Trees</a></p>
<p>Author: Danilo Croce ; Alessandro Moschitti ; Roberto Basili</p><p>Abstract: Alessandro Moschitti DISI University of Trento 38123 Povo (TN), Italy mo s chitt i di s i @ .unit n . it Roberto Basili DII University of Tor Vergata 00133 Roma, Italy bas i i info .uni roma2 . it l@ over semantic networks, e.g. (Cowie et al., 1992; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, A central topic in natural language processing is the design of lexical and syntactic fea- tures suitable for the target application. In this paper, we study convolution dependency tree kernels for automatic engineering of syntactic and semantic patterns exploiting lexical similarities. We define efficient and powerful kernels for measuring the similarity between dependency structures, whose surface forms of the lexical nodes are in part or completely different. The experiments with such kernels for question classification show an unprecedented results, e.g. 41% of error reduction of the former state-of-the-art. Additionally, semantic role classification confirms the benefit of semantic smoothing for dependency kernels.</p><p>2 0.37716702 <a title="127-tfidf-2" href="./emnlp-2011-Using_Syntactic_and_Semantic_Structural_Kernels_for_Classifying_Definition_Questions_in_Jeopardy%21.html">147 emnlp-2011-Using Syntactic and Semantic Structural Kernels for Classifying Definition Questions in Jeopardy!</a></p>
<p>Author: Alessandro Moschitti ; Jennifer Chu-carroll ; Siddharth Patwardhan ; James Fan ; Giuseppe Riccardi</p><p>Abstract: The last decade has seen many interesting applications of Question Answering (QA) technology. The Jeopardy! quiz show is certainly one of the most fascinating, from the viewpoints of both its broad domain and the complexity of its language. In this paper, we study kernel methods applied to syntactic/semantic structures for accurate classification of Jeopardy! definition questions. Our extensive empirical analysis shows that our classification models largely improve on classifiers based on word-language models. Such classifiers are also used in the state-of-the-art QA pipeline constituting Watson, the IBM Jeopardy! system. Our experiments measuring their impact on Watson show enhancements in QA accuracy and a consequent increase in the amount of money earned in game-based evaluation.</p><p>3 0.13065556 <a title="127-tfidf-3" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>Author: Marco Dinarelli ; Sophie Rosset</p><p>Abstract: Reranking models have been successfully applied to many tasks of Natural Language Processing. However, there are two aspects of this approach that need a deeper investigation: (i) Assessment of hypotheses generated for reranking at classification phase: baseline models generate a list of hypotheses and these are used for reranking without any assessment; (ii) Detection of cases where reranking models provide a worst result: the best hypothesis provided by the reranking model is assumed to be always the best result. In some cases the reranking model provides an incorrect hypothesis while the baseline best hypothesis is correct, especially when baseline models are accurate. In this paper we propose solutions for these two aspects: (i) a semantic inconsistency metric to select possibly more correct n-best hypotheses, from a large set generated by an SLU basiline model. The selected hypotheses are reranked applying a state-of-the-art model based on Partial Tree Kernels, which encode SLU hypotheses in Support Vector Machines with complex structured features; (ii) finally, we apply a decision strategy, based on confidence values, to select the final hypothesis between the first ranked hypothesis provided by the baseline SLU model and the first ranked hypothesis provided by the re-ranker. We show the effectiveness of these solutions presenting comparative results obtained reranking hypotheses generated by a very accurate Conditional Random Field model. We evaluate our approach on the French MEDIA corpus. The results show significant improvements with respect to current state-of-the-art and previous 1104 Sophie Rosset LIMSI-CNRS B.P. 133, 91403 Orsay Cedex France ro s set @ l ims i fr . re-ranking models.</p><p>4 0.09249384 <a title="127-tfidf-4" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>Author: Reut Tsarfaty ; Joakim Nivre ; Evelina Andersson</p><p>Abstract: unkown-abstract</p><p>5 0.085957564 <a title="127-tfidf-5" href="./emnlp-2011-Accurate_Parsing_with_Compact_Tree-Substitution_Grammars%3A_Double-DOP.html">16 emnlp-2011-Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP</a></p>
<p>Author: Federico Sangati ; Willem Zuidema</p><p>Abstract: We present a novel approach to Data-Oriented Parsing (DOP). Like other DOP models, our parser utilizes syntactic fragments of arbitrary size from a treebank to analyze new sentences, but, crucially, it uses only those which are encountered at least twice. This criterion allows us to work with a relatively small but representative set of fragments, which can be employed as the symbolic backbone of several probabilistic generative models. For parsing we define a transform-backtransform approach that allows us to use standard PCFG technology, making our results easily replicable. According to standard Parseval metrics, our best model is on par with many state-ofthe-art parsers, while offering some complementary benefits: a simple generative probability model, and an explicit representation of the larger units of grammar.</p><p>6 0.080956504 <a title="127-tfidf-6" href="./emnlp-2011-A_Fast%2C_Accurate%2C_Non-Projective%2C_Semantically-Enriched_Parser.html">4 emnlp-2011-A Fast, Accurate, Non-Projective, Semantically-Enriched Parser</a></p>
<p>7 0.074780427 <a title="127-tfidf-7" href="./emnlp-2011-Linguistic_Redundancy_in_Twitter.html">89 emnlp-2011-Linguistic Redundancy in Twitter</a></p>
<p>8 0.070714228 <a title="127-tfidf-8" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>9 0.068588339 <a title="127-tfidf-9" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>10 0.065958582 <a title="127-tfidf-10" href="./emnlp-2011-A_Joint_Model_for_Extended_Semantic_Role_Labeling.html">7 emnlp-2011-A Joint Model for Extended Semantic Role Labeling</a></p>
<p>11 0.06498991 <a title="127-tfidf-11" href="./emnlp-2011-Unsupervised_Semantic_Role_Induction_with_Graph_Partitioning.html">145 emnlp-2011-Unsupervised Semantic Role Induction with Graph Partitioning</a></p>
<p>12 0.063902058 <a title="127-tfidf-12" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>13 0.063137263 <a title="127-tfidf-13" href="./emnlp-2011-Joint_Models_for_Chinese_POS_Tagging_and_Dependency_Parsing.html">75 emnlp-2011-Joint Models for Chinese POS Tagging and Dependency Parsing</a></p>
<p>14 0.06078371 <a title="127-tfidf-14" href="./emnlp-2011-Learning_the_Information_Status_of_Noun_Phrases_in_Spoken_Dialogues.html">84 emnlp-2011-Learning the Information Status of Noun Phrases in Spoken Dialogues</a></p>
<p>15 0.058317907 <a title="127-tfidf-15" href="./emnlp-2011-Relation_Extraction_with_Relation_Topics.html">114 emnlp-2011-Relation Extraction with Relation Topics</a></p>
<p>16 0.057242889 <a title="127-tfidf-16" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>17 0.055478521 <a title="127-tfidf-17" href="./emnlp-2011-Latent_Vector_Weighting_for_Word_Meaning_in_Context.html">80 emnlp-2011-Latent Vector Weighting for Word Meaning in Context</a></p>
<p>18 0.052499175 <a title="127-tfidf-18" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>19 0.052097891 <a title="127-tfidf-19" href="./emnlp-2011-Syntactic_Decision_Tree_LMs%3A_Random_Selection_or_Intelligent_Design%3F.html">131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</a></p>
<p>20 0.050355952 <a title="127-tfidf-20" href="./emnlp-2011-Timeline_Generation_through_Evolutionary_Trans-Temporal_Summarization.html">135 emnlp-2011-Timeline Generation through Evolutionary Trans-Temporal Summarization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.205), (1, -0.028), (2, -0.08), (3, 0.081), (4, -0.022), (5, -0.058), (6, -0.063), (7, -0.012), (8, 0.349), (9, -0.006), (10, 0.094), (11, -0.089), (12, -0.005), (13, 0.344), (14, -0.223), (15, -0.063), (16, -0.235), (17, 0.08), (18, -0.245), (19, -0.03), (20, -0.158), (21, -0.11), (22, -0.072), (23, 0.062), (24, -0.041), (25, 0.016), (26, -0.178), (27, -0.164), (28, 0.037), (29, -0.002), (30, -0.038), (31, -0.021), (32, 0.123), (33, -0.065), (34, 0.069), (35, 0.004), (36, -0.003), (37, -0.054), (38, 0.045), (39, 0.084), (40, -0.008), (41, 0.052), (42, -0.001), (43, -0.033), (44, -0.029), (45, 0.026), (46, -0.018), (47, 0.016), (48, 0.01), (49, -0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93688434 <a title="127-lsi-1" href="./emnlp-2011-Structured_Lexical_Similarity_via_Convolution_Kernels_on_Dependency_Trees.html">127 emnlp-2011-Structured Lexical Similarity via Convolution Kernels on Dependency Trees</a></p>
<p>Author: Danilo Croce ; Alessandro Moschitti ; Roberto Basili</p><p>Abstract: Alessandro Moschitti DISI University of Trento 38123 Povo (TN), Italy mo s chitt i di s i @ .unit n . it Roberto Basili DII University of Tor Vergata 00133 Roma, Italy bas i i info .uni roma2 . it l@ over semantic networks, e.g. (Cowie et al., 1992; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, A central topic in natural language processing is the design of lexical and syntactic fea- tures suitable for the target application. In this paper, we study convolution dependency tree kernels for automatic engineering of syntactic and semantic patterns exploiting lexical similarities. We define efficient and powerful kernels for measuring the similarity between dependency structures, whose surface forms of the lexical nodes are in part or completely different. The experiments with such kernels for question classification show an unprecedented results, e.g. 41% of error reduction of the former state-of-the-art. Additionally, semantic role classification confirms the benefit of semantic smoothing for dependency kernels.</p><p>2 0.92828709 <a title="127-lsi-2" href="./emnlp-2011-Using_Syntactic_and_Semantic_Structural_Kernels_for_Classifying_Definition_Questions_in_Jeopardy%21.html">147 emnlp-2011-Using Syntactic and Semantic Structural Kernels for Classifying Definition Questions in Jeopardy!</a></p>
<p>Author: Alessandro Moschitti ; Jennifer Chu-carroll ; Siddharth Patwardhan ; James Fan ; Giuseppe Riccardi</p><p>Abstract: The last decade has seen many interesting applications of Question Answering (QA) technology. The Jeopardy! quiz show is certainly one of the most fascinating, from the viewpoints of both its broad domain and the complexity of its language. In this paper, we study kernel methods applied to syntactic/semantic structures for accurate classification of Jeopardy! definition questions. Our extensive empirical analysis shows that our classification models largely improve on classifiers based on word-language models. Such classifiers are also used in the state-of-the-art QA pipeline constituting Watson, the IBM Jeopardy! system. Our experiments measuring their impact on Watson show enhancements in QA accuracy and a consequent increase in the amount of money earned in game-based evaluation.</p><p>3 0.33276296 <a title="127-lsi-3" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>Author: Marco Dinarelli ; Sophie Rosset</p><p>Abstract: Reranking models have been successfully applied to many tasks of Natural Language Processing. However, there are two aspects of this approach that need a deeper investigation: (i) Assessment of hypotheses generated for reranking at classification phase: baseline models generate a list of hypotheses and these are used for reranking without any assessment; (ii) Detection of cases where reranking models provide a worst result: the best hypothesis provided by the reranking model is assumed to be always the best result. In some cases the reranking model provides an incorrect hypothesis while the baseline best hypothesis is correct, especially when baseline models are accurate. In this paper we propose solutions for these two aspects: (i) a semantic inconsistency metric to select possibly more correct n-best hypotheses, from a large set generated by an SLU basiline model. The selected hypotheses are reranked applying a state-of-the-art model based on Partial Tree Kernels, which encode SLU hypotheses in Support Vector Machines with complex structured features; (ii) finally, we apply a decision strategy, based on confidence values, to select the final hypothesis between the first ranked hypothesis provided by the baseline SLU model and the first ranked hypothesis provided by the re-ranker. We show the effectiveness of these solutions presenting comparative results obtained reranking hypotheses generated by a very accurate Conditional Random Field model. We evaluate our approach on the French MEDIA corpus. The results show significant improvements with respect to current state-of-the-art and previous 1104 Sophie Rosset LIMSI-CNRS B.P. 133, 91403 Orsay Cedex France ro s set @ l ims i fr . re-ranking models.</p><p>4 0.29477662 <a title="127-lsi-4" href="./emnlp-2011-Accurate_Parsing_with_Compact_Tree-Substitution_Grammars%3A_Double-DOP.html">16 emnlp-2011-Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP</a></p>
<p>Author: Federico Sangati ; Willem Zuidema</p><p>Abstract: We present a novel approach to Data-Oriented Parsing (DOP). Like other DOP models, our parser utilizes syntactic fragments of arbitrary size from a treebank to analyze new sentences, but, crucially, it uses only those which are encountered at least twice. This criterion allows us to work with a relatively small but representative set of fragments, which can be employed as the symbolic backbone of several probabilistic generative models. For parsing we define a transform-backtransform approach that allows us to use standard PCFG technology, making our results easily replicable. According to standard Parseval metrics, our best model is on par with many state-ofthe-art parsers, while offering some complementary benefits: a simple generative probability model, and an explicit representation of the larger units of grammar.</p><p>5 0.27422205 <a title="127-lsi-5" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>Author: Reut Tsarfaty ; Joakim Nivre ; Evelina Andersson</p><p>Abstract: unkown-abstract</p><p>6 0.27263466 <a title="127-lsi-6" href="./emnlp-2011-Learning_the_Information_Status_of_Noun_Phrases_in_Spoken_Dialogues.html">84 emnlp-2011-Learning the Information Status of Noun Phrases in Spoken Dialogues</a></p>
<p>7 0.26012257 <a title="127-lsi-7" href="./emnlp-2011-Efficient_retrieval_of_tree_translation_examples_for_Syntax-Based_Machine_Translation.html">47 emnlp-2011-Efficient retrieval of tree translation examples for Syntax-Based Machine Translation</a></p>
<p>8 0.256782 <a title="127-lsi-8" href="./emnlp-2011-Syntactic_Decision_Tree_LMs%3A_Random_Selection_or_Intelligent_Design%3F.html">131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</a></p>
<p>9 0.25048873 <a title="127-lsi-9" href="./emnlp-2011-Linguistic_Redundancy_in_Twitter.html">89 emnlp-2011-Linguistic Redundancy in Twitter</a></p>
<p>10 0.21412471 <a title="127-lsi-10" href="./emnlp-2011-Refining_the_Notions_of_Depth_and_Density_in_WordNet-based_Semantic_Similarity_Measures.html">112 emnlp-2011-Refining the Notions of Depth and Density in WordNet-based Semantic Similarity Measures</a></p>
<p>11 0.20604947 <a title="127-lsi-11" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>12 0.20412411 <a title="127-lsi-12" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>13 0.20270388 <a title="127-lsi-13" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>14 0.19816174 <a title="127-lsi-14" href="./emnlp-2011-A_Fast%2C_Accurate%2C_Non-Projective%2C_Semantically-Enriched_Parser.html">4 emnlp-2011-A Fast, Accurate, Non-Projective, Semantically-Enriched Parser</a></p>
<p>15 0.19354704 <a title="127-lsi-15" href="./emnlp-2011-Unsupervised_Semantic_Role_Induction_with_Graph_Partitioning.html">145 emnlp-2011-Unsupervised Semantic Role Induction with Graph Partitioning</a></p>
<p>16 0.18981755 <a title="127-lsi-16" href="./emnlp-2011-Harnessing_WordNet_Senses_for_Supervised_Sentiment_Classification.html">63 emnlp-2011-Harnessing WordNet Senses for Supervised Sentiment Classification</a></p>
<p>17 0.18911871 <a title="127-lsi-17" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>18 0.18892723 <a title="127-lsi-18" href="./emnlp-2011-Parse_Correction_with_Specialized_Models_for_Difficult_Attachment_Types.html">102 emnlp-2011-Parse Correction with Specialized Models for Difficult Attachment Types</a></p>
<p>19 0.18610637 <a title="127-lsi-19" href="./emnlp-2011-Approximate_Scalable_Bounded_Space_Sketch_for_Large_Data_NLP.html">19 emnlp-2011-Approximate Scalable Bounded Space Sketch for Large Data NLP</a></p>
<p>20 0.18458386 <a title="127-lsi-20" href="./emnlp-2011-Latent_Vector_Weighting_for_Word_Meaning_in_Context.html">80 emnlp-2011-Latent Vector Weighting for Word Meaning in Context</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.307), (23, 0.099), (27, 0.019), (36, 0.026), (37, 0.033), (45, 0.058), (53, 0.021), (54, 0.028), (57, 0.017), (62, 0.015), (64, 0.018), (66, 0.056), (69, 0.015), (79, 0.047), (82, 0.02), (87, 0.011), (90, 0.04), (96, 0.066), (98, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78850484 <a title="127-lda-1" href="./emnlp-2011-Structured_Lexical_Similarity_via_Convolution_Kernels_on_Dependency_Trees.html">127 emnlp-2011-Structured Lexical Similarity via Convolution Kernels on Dependency Trees</a></p>
<p>Author: Danilo Croce ; Alessandro Moschitti ; Roberto Basili</p><p>Abstract: Alessandro Moschitti DISI University of Trento 38123 Povo (TN), Italy mo s chitt i di s i @ .unit n . it Roberto Basili DII University of Tor Vergata 00133 Roma, Italy bas i i info .uni roma2 . it l@ over semantic networks, e.g. (Cowie et al., 1992; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, A central topic in natural language processing is the design of lexical and syntactic fea- tures suitable for the target application. In this paper, we study convolution dependency tree kernels for automatic engineering of syntactic and semantic patterns exploiting lexical similarities. We define efficient and powerful kernels for measuring the similarity between dependency structures, whose surface forms of the lexical nodes are in part or completely different. The experiments with such kernels for question classification show an unprecedented results, e.g. 41% of error reduction of the former state-of-the-art. Additionally, semantic role classification confirms the benefit of semantic smoothing for dependency kernels.</p><p>2 0.77642488 <a title="127-lda-2" href="./emnlp-2011-Minimally_Supervised_Event_Causality_Identification.html">92 emnlp-2011-Minimally Supervised Event Causality Identification</a></p>
<p>Author: Quang Do ; Yee Seng Chan ; Dan Roth</p><p>Abstract: This paper develops a minimally supervised approach, based on focused distributional similarity methods and discourse connectives, for identifying of causality relations between events in context. While it has been shown that distributional similarity can help identifying causality, we observe that discourse connectives and the particular discourse relation they evoke in context provide additional information towards determining causality between events. We show that combining discourse relation predictions and distributional similarity methods in a global inference procedure provides additional improvements towards determining event causality.</p><p>3 0.5386557 <a title="127-lda-3" href="./emnlp-2011-Using_Syntactic_and_Semantic_Structural_Kernels_for_Classifying_Definition_Questions_in_Jeopardy%21.html">147 emnlp-2011-Using Syntactic and Semantic Structural Kernels for Classifying Definition Questions in Jeopardy!</a></p>
<p>Author: Alessandro Moschitti ; Jennifer Chu-carroll ; Siddharth Patwardhan ; James Fan ; Giuseppe Riccardi</p><p>Abstract: The last decade has seen many interesting applications of Question Answering (QA) technology. The Jeopardy! quiz show is certainly one of the most fascinating, from the viewpoints of both its broad domain and the complexity of its language. In this paper, we study kernel methods applied to syntactic/semantic structures for accurate classification of Jeopardy! definition questions. Our extensive empirical analysis shows that our classification models largely improve on classifiers based on word-language models. Such classifiers are also used in the state-of-the-art QA pipeline constituting Watson, the IBM Jeopardy! system. Our experiments measuring their impact on Watson show enhancements in QA accuracy and a consequent increase in the amount of money earned in game-based evaluation.</p><p>4 0.46008429 <a title="127-lda-4" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>Author: Marco Dinarelli ; Sophie Rosset</p><p>Abstract: Reranking models have been successfully applied to many tasks of Natural Language Processing. However, there are two aspects of this approach that need a deeper investigation: (i) Assessment of hypotheses generated for reranking at classification phase: baseline models generate a list of hypotheses and these are used for reranking without any assessment; (ii) Detection of cases where reranking models provide a worst result: the best hypothesis provided by the reranking model is assumed to be always the best result. In some cases the reranking model provides an incorrect hypothesis while the baseline best hypothesis is correct, especially when baseline models are accurate. In this paper we propose solutions for these two aspects: (i) a semantic inconsistency metric to select possibly more correct n-best hypotheses, from a large set generated by an SLU basiline model. The selected hypotheses are reranked applying a state-of-the-art model based on Partial Tree Kernels, which encode SLU hypotheses in Support Vector Machines with complex structured features; (ii) finally, we apply a decision strategy, based on confidence values, to select the final hypothesis between the first ranked hypothesis provided by the baseline SLU model and the first ranked hypothesis provided by the re-ranker. We show the effectiveness of these solutions presenting comparative results obtained reranking hypotheses generated by a very accurate Conditional Random Field model. We evaluate our approach on the French MEDIA corpus. The results show significant improvements with respect to current state-of-the-art and previous 1104 Sophie Rosset LIMSI-CNRS B.P. 133, 91403 Orsay Cedex France ro s set @ l ims i fr . re-ranking models.</p><p>5 0.4451229 <a title="127-lda-5" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Noah A. Smith</p><p>Abstract: We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.</p><p>6 0.44250634 <a title="127-lda-6" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>7 0.43688738 <a title="127-lda-7" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>8 0.43672889 <a title="127-lda-8" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>9 0.43468633 <a title="127-lda-9" href="./emnlp-2011-Unsupervised_Learning_of_Selectional_Restrictions_and_Detection_of_Argument_Coercions.html">144 emnlp-2011-Unsupervised Learning of Selectional Restrictions and Detection of Argument Coercions</a></p>
<p>10 0.43359378 <a title="127-lda-10" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>11 0.43108711 <a title="127-lda-11" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>12 0.42907435 <a title="127-lda-12" href="./emnlp-2011-Accurate_Parsing_with_Compact_Tree-Substitution_Grammars%3A_Double-DOP.html">16 emnlp-2011-Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP</a></p>
<p>13 0.42869267 <a title="127-lda-13" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>14 0.4286283 <a title="127-lda-14" href="./emnlp-2011-Lexical_Generalization_in_CCG_Grammar_Induction_for_Semantic_Parsing.html">87 emnlp-2011-Lexical Generalization in CCG Grammar Induction for Semantic Parsing</a></p>
<p>15 0.42805398 <a title="127-lda-15" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>16 0.42771566 <a title="127-lda-16" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>17 0.42755979 <a title="127-lda-17" href="./emnlp-2011-Syntax-Based_Grammaticality_Improvement_using_CCG_and_Guided_Search.html">132 emnlp-2011-Syntax-Based Grammaticality Improvement using CCG and Guided Search</a></p>
<p>18 0.42746165 <a title="127-lda-18" href="./emnlp-2011-Lateen_EM%3A_Unsupervised_Training_with_Multiple_Objectives%2C_Applied_to_Dependency_Grammar_Induction.html">79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</a></p>
<p>19 0.42695048 <a title="127-lda-19" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>20 0.4269062 <a title="127-lda-20" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
