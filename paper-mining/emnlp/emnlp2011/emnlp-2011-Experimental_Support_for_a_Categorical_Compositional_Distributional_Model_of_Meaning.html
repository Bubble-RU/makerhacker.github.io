<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-53" href="#">emnlp2011-53</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</h1>
<br/><p>Source: <a title="emnlp-2011-53-pdf" href="http://aclweb.org/anthology//D/D11/D11-1129.pdf">pdf</a></p><p>Author: Edward Grefenstette ; Mehrnoosh Sadrzadeh</p><p>Abstract: Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences. Our model matches the results of its competitors . in the first experiment, and betters them in the second. The general improvement in results with increase in syntactic complexity showcases the compositional power of our model.</p><p>Reference: <a title="emnlp-2011-53-reference" href="../emnlp2011_reference/emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. [sent-4, score-0.431]
</p><p>2 We implement the abstract categorical model of Coecke et al. [sent-5, score-0.197]
</p><p>3 The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. [sent-7, score-0.416]
</p><p>4 The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences. [sent-8, score-0.544]
</p><p>5 We are naturally good at understanding ambiguous words given a context, and forming the meaning of a sentence from the meaning of its parts. [sent-13, score-0.363]
</p><p>6 Discovering the process of meaning assignment in natural language is among the most challenging and foundational questions of linguistics and computer science. [sent-21, score-0.204]
</p><p>7 Compositional formal semantic models stem from classical ideas from mathematical logic, mainly Frege’s principle that the meaning of a sentence is a function of the meaning of its parts (Frege, 1892). [sent-24, score-0.498]
</p><p>8 The logical models relate to well known and robust logical formalisms, hence offering a scalable theory  of meaning which can be used to reason inferentially. [sent-26, score-0.427]
</p><p>9 They developed a unified mathematical framework whereby a sentence vector is by definition a function of the Kronecker product of its word vectors. [sent-35, score-0.198]
</p><p>10 The highlight of our implementation is that words  with relational types, such as verbs, adjectives, and adverbs are matrices that act on their arguments. [sent-39, score-0.218]
</p><p>11 The implementation is evaluated against the task provided by Mitchell and Lapata (2008) for disambiguating intransitive verbs, as well as a similar new experiment for transitive verbs. [sent-41, score-0.584]
</p><p>12 Our model improves on the best method evaluated in Mitchell and Lapata (2008) and offers promising results for the transitive case, demonstrating its scalability in comparison to that of other models. [sent-42, score-0.349]
</p><p>13 Our work shows that the categorical compositional distributional model of meaning permits a practical implementation and that this opens the way to the production of large scale compositional models. [sent-44, score-0.771]
</p><p>14 2  Two Orthogonal Semantic Models  Formal Semantics To compute the meaning of a sentence consisting of n words, meanings of these words must interact with one another. [sent-45, score-0.273]
</p><p>15 In formal se-  mantics, this further interaction is represented as a function derived from the grammatical structure of the sentence, but meanings of words are amorphous objects of the domain: no distinction is made between words that have the same type. [sent-46, score-0.204]
</p><p>16 The parse of a sentence such as “cats like milk” typically produces its semantic interpretation by substituting semantic representation for their grammatical constituents and applying β-reduction where needed. [sent-48, score-0.217]
</p><p>17 Word meaning is obtained empirically by examining the contexts1 in which a word appears, and equating the meaning of a word with the distribution of contexts it shares. [sent-67, score-0.32]
</p><p>18 Such context distributions can be encoded as vectors in a high dimensional space with contexts as basis vectors. [sent-74, score-0.308]
</p><p>19 For any word vector the scalar weight ciword associated with each context basis vector is a function of the number of times the word has appeared in that context. [sent-75, score-0.222]
</p><p>20 Semantic vectors  −w −o →rd,  n→i  (c1word, c2word, · · · , cnword) are also denoted by sums of such weight/basis vector pairs:  −w −o →rd  =  Xciwordn −→i Xi  Learning a semantic vector is just learning its basis weights from the corpus. [sent-76, score-0.463]
</p><p>21 The principal drawback of such models is their non-compositional nature: they ignore grammatical structure and logical words, and hence cannot compute the meanings of phrases and sentences in the same efficient way that they do for words. [sent-80, score-0.317]
</p><p>22 However, the dimensionality of sentence vectors produced in this manner differs for sentences of different length, barring all sentences from being compared in the same vector space, and growing exponentially with sentence length hence quickly becoming computationally intractable. [sent-89, score-0.391]
</p><p>23 3  A Hybrid Logico-Distributional Model  Whereas semantic compositional mechanisms for set-theoretic constructions are well understood, there are no obvious corresponding methods for vector spaces. [sent-90, score-0.242]
</p><p>24 1396 (2010) use the abstract setting of category theory to turn the grammatical structure of a sentence into a morphism compatible with the higher level logical structure of vector spaces. [sent-92, score-0.339]
</p><p>25 In distributional models, there is a meaning vector for each word, e. [sent-94, score-0.344]
</p><p>26 The logical recipe tells us to apply the mean-  ing of the verb to the meanings of subject and object. [sent-97, score-0.335]
</p><p>27 This is similar to logical models where verbs are relations and nouns are atomic sets. [sent-100, score-0.316]
</p><p>28 So verb vectors should be built differently from noun vectors, for instance as matrices. [sent-101, score-0.351]
</p><p>29 The general information as to which words should be matrices and which words atomic vectors is in fact encoded in the type-logical representation of the grammatical structure of the sentence. [sent-102, score-0.474]
</p><p>30 This is the linear map with word vectors as input and sentence vectors as output. [sent-103, score-0.439]
</p><p>31 Hence, at least theoretically, one should be able to build sentence vectors and compare their synonymity in exactly the same way as one measures word synonymity. [sent-104, score-0.329]
</p><p>32 Pregroup Grammars The aforementioned linear maps turn out to be the grammatical reductions of a type-logic called a Lambek pregroup grammar (Lambek, 2008)2. [sent-105, score-0.307]
</p><p>33 One consequence of this parity is that the grammatical reductions of a pregroup grammar can be directly transformed into linear maps that act on vectors. [sent-108, score-0.307]
</p><p>34 In a nutshell, pregroup types are either atomic or compound. [sent-109, score-0.296]
</p><p>35 and An example of a compound type is that of a verb The superscripted types express that the verb is a relation with two arguments of type n,  nr  nl). [sent-114, score-0.238]
</p><p>36 2The usage of pregroup types is not essential, the types of any other logic, for instance CCG can be used, but should be translated into the language of pregroups. [sent-116, score-0.219]
</p><p>37 A transitive sentence has types as shown in Figure 3. [sent-118, score-0.392]
</p><p>38 Each type n cancels out with its right adjoint from the right and its left adjoint nl from the left; mathematically speaking these mean3  nr  nln ≤  1  and  nnr  Here 1 is the unit of concatenation:  ≤ 1 1n = n1 =  n. [sent-119, score-0.263]
</p><p>39 The corresponding grammatical reduction of a transitive sentence is nnrsnl ≤ 1s1 = s. [sent-120, score-0.48]
</p><p>40 Tucheh diagram ofa transitive sentence is shown in Figure 3. [sent-122, score-0.392]
</p><p>41 nl n  Figure 3: The pregroup types and reduction diagram for a transitive sentence. [sent-124, score-0.611]
</p><p>42 (2010) and based on a general completeness theorem between compact categories, wire diagrams, and vector spaces, the meaning of sentences can be canonically reduced to linear algebraic formulae. [sent-126, score-0.216]
</p><p>43 The following is the meaning vector of our transitive sentence:  c−a −ts − − li −k −e − m −i →lk  = (f)  ? [sent-127, score-0.565]
</p><p>44 The categorical morphism correspond-  ing to it is denoted by the tensor product of 3 components: ? [sent-130, score-0.389]
</p><p>45 The cups stand for taking inner products, which when done with the basis vectors imitate substitution. [sent-135, score-0.392]
</p><p>46 iifn gn tlnhe = 1f = nthensre, then the pregroup collapses into a group where nl = nr. [sent-142, score-0.306]
</p><p>47 1397 lower dimensions, hence the dimensional explosion problem for Kronecker products is avoided:  Xcitjhc−a →ts|− →vii −→sth−w →j|−m −i →lki  ∈ S  (II)  Xitj  →vi, −w →j are basis vectors of V product hc−a →ts | →vii substitutes  and W. [sent-143, score-0.412]
</p><p>48 is a basis vector of the sentence space S in which meanings of sentences live, regardless of their grammatical structure. [sent-145, score-0.367]
</p><p>49 S is an abstract space: it needs to be instantiated to provide concrete meanings and synonymity measures. [sent-147, score-0.217]
</p><p>50 For instance, a truth-theoretic model is obtained by taking the sentence space S to be the 2dimensional space with basis vectors |1i (True) and |0i (False). [sent-148, score-0.391]
</p><p>51 4  Building Matrices for Relational Words  In this section we present a general scheme to build matrices for relational words. [sent-149, score-0.218]
</p><p>52 Recall that given a vector space A with basis { −n→i}i, the Kronecker Pproduct o sfp two vectors = Pi cian −→i and = Pi cibn −→i is defined as follows:  b→avs  →v ⊗ −→w  =  Xciacjb(n −→i⊗−n →j) Xij  →w  ××  where (n −→i ⊗ →nj) is just the pairing of the basis of A, i. [sent-150, score-0.474]
</p><p>53 The Kronecker product vectors belong in the tensor product of A with itself: A ⊗ A, hence iifn A th hea tse ndismore pnrsoidoun r, othfe Ase w wwitilhl bitse eolff: dimensionality r r. [sent-153, score-0.494]
</p><p>54 But in the distributional setting, this method will be too syntactic and dismissive of the actual meaning of ‘cat’ and ‘dog’ . [sent-166, score-0.288]
</p><p>55 If instead the corpus contains the sentence ‘the hound hunted the wild cat’ , cxy will be 0, restricting us to only assign meaning to sentences that have directly appeared in the corpus. [sent-167, score-0.26]
</p><p>56 We propose to, instead, use a level of abstraction by taking words such as verbs to be distributions over the semantic information in the vectors of their context words, rather than over the context words themselves. [sent-168, score-0.365]
</p><p>57 Start with an r-dimensional vector space N with basis in which meaning vectors of atomic words, suc}h as nouns, live. [sent-169, score-0.601]
</p><p>58 The basis vectors of N are in principle all the words from the corpus, however in practice and following Mitchell and Lapata (2008) we had to restrict these to a subset of the most occurring words. [sent-170, score-0.308]
</p><p>59 These basis vectors are not restricted to nouns: they can as well be verbs, adjectives, and adverbs, so that we can define the meaning of a noun in all possible contexts—as is usual in context-based models—and not only in the con-  { −→ni}i,  ×  text of other nouns. [sent-171, score-0.524]
</p><p>60 Note that basis words with relational types are treated as pure lexical items rather than as semantic objects represented as matrices. [sent-172, score-0.26]
</p><p>61 Each relational word P with grammatical type π and m adjoint types α1 , α2, · · · , αm is encoded as an (r . [sent-174, score-0.305]
</p><p>62 A transitive verb is represented as a 2 dimensional matrix since its type is nrsnl with two adjoint types and nl. [sent-185, score-0.618]
</p><p>63 The TF/IDFweighted values for vectors of the above four nouns (built from the BNC) are as shown in Table 1. [sent-192, score-0.245]
</p><p>64 5  Computing Sentence Vectors  Meaning of sentences are vectors computed by taking the variables of the categorical prescription of meaning (the linear map f obtained from the grammatical reduction of the sentence) to be determined by the matrices of the relational words. [sent-220, score-0.901]
</p><p>65 For instance the meaning of the transitive sentence ‘sub verb obj’ is:  s−u− b − − v −er −b − − o →bj  =  Xhs−u →b | →viih −→wj | −o →bjicitj −→st Xitj  We take V := W := N and S = N ⊗ N, then  −isP−. [sent-221, score-0.649]
</p><p>66 theHve nrcbe, Xhs−u →b | →niih −→nj | −o →bjicij( −→ni⊗ −→nj)  =  Xij  Xcisubcjobjcij( −→ni ⊗ −→nj) Xij This can be decomposed to point-wise multiplication of two vectors as follows:  ? [sent-224, score-0.254]
</p><p>67 The left argument is the Kronecker product of subject and object vectors and the right argument is the vector of the verb, so we obtain  ? [sent-230, score-0.517]
</p><p>68 ona isl cvoemrsimonu otaftitvhee, type-logical meaning aof dthisesentence: point-wise multiplication of the meaning of the verb to the Kronecker product of its subject and object:  s−u− b − − v −er −b − − o →bj  = −v −e →rb ? [sent-235, score-0.586]
</p><p>69 This mathematical operation can be informally described as a structured ‘mixing’ of the information of the subject and object, followed by it being ‘filtered’ through the information of the verb applied to them, in order to produce the information of the sentence. [sent-238, score-0.203]
</p><p>70 In the transitive case, S = N ⊗ N, hence →st = →niI ⊗ →ne tjr. [sent-239, score-0.4]
</p><p>71 a sMitiovree generally, tNhe ⊗ vector space corresponding to the abstract sentence space S is the concrete tensor space (N ⊗ . [sent-240, score-0.253]
</p><p>72 For instance the meaning of a transitive sentence with a modified subject  and a modified verb we have  a− d−j − s −u −b − v −e −rb − − o −b −j − a →dv ? [sent-249, score-0.709]
</p><p>73 =  After building vectors for sentences, we can compare their meaning and measure their degree of synonymy by taking their cosine measure. [sent-258, score-0.398]
</p><p>74 , 2011), it is suggested that the simplified model we presented and expanded here could be evaluated in the same way as lexical semantic models, measuring compositionally 1400 built sentence vectors against a benchmark dataset such as that provided by Mitchell and Lapata (2008). [sent-262, score-0.324]
</p><p>75 Following this, we present a new evaluation task extending the experimental methodology of Mitchell and Lapata (2008)  to transitive verb-centric sentences, and compare our model to those discussed by Mitchell and Lapata (2008) within this new experiment. [sent-264, score-0.389]
</p><p>76 Each entry of the dataset provides a noun, a target verb and landmark verb (both intransitive). [sent-266, score-0.234]
</p><p>77 The noun must be composed with both verbs to produce short phrase vectors the similarity of which is measured by the candidate. [sent-267, score-0.395]
</p><p>78 The additive and multiplicative models are simply applications of vector addition and component-wise multiplication. [sent-278, score-0.263]
</p><p>79 All vectors were built from a lemmatised version of the BNC. [sent-281, score-0.198]
</p><p>80 The noun basis was the 2000 most common context words, basis weights were the probability of context words given the target word divided by the overall probability ofthe context word. [sent-282, score-0.276]
</p><p>81 tTaihnes sco instrinaen measure of vectors was used as a similarity metric. [sent-287, score-0.255]
</p><p>82 Our categorical model performs significantly better than the existing second-place (Kintsch) and obtains a ρ quasiidentical to the multiplicative model, indicating sig-  nificant correlation with the annotator scores. [sent-289, score-0.34]
</p><p>83 19) model which is a combination of their multiplicative model and a weighted additive model. [sent-293, score-0.207]
</p><p>84 Each section was given to a group of evaluators, with a total of 25, who were asked to form simple transitive sentence pairs from the verbs, subject and object provided in each entry;  for instance ‘the table showed the result’ from ‘table show result’ . [sent-317, score-0.522]
</p><p>85 Sentence 1Sentence 2 mtaabple sh sohwow lo rceastuioltnmtaabple p eicxtuprreess lo creastiuolnt  mta bple sh sohwow lo rceastuioltnmta pbl e x p ircetus re lo rceastuioltn Table 4: Example entries from the transitive dataset without annotator score, second experiment. [sent-321, score-0.637]
</p><p>86 This is both because of the imprecise nature of the classification of verb pairs as HIGH or LOW; and since the objective similarity scores produced by a model that distinguishes sentences of different meaning from those of similar meaning can be renormalised in practice. [sent-324, score-0.474]
</p><p>87 Transitive verb vectors were trained as described in §4 with S = N⊗N. [sent-327, score-0.295]
</p><p>88 The additive model continues to make little distinction between senses of the verb during composition, and the multiplicative model’s alignment does not change, but becomes statistically indistinguishable from the non-compositional baseline model. [sent-352, score-0.304]
</p><p>89 7  Discussion  In this paper, we described an implementation of the categorical model of meaning (Coecke et al. [sent-354, score-0.357]
</p><p>90 , 2010), which combines the formal logical and the empirical distributional frameworks into a unified semantic model. [sent-355, score-0.325]
</p><p>91 The implementation is based on building matrices for words with relational types (adjectives, verbs), and vectors for words with atomic types (nouns), based on data from the BNC. [sent-356, score-0.493]
</p><p>92 We then show how to apply verbs to their subject/object, in order to compute the meaning of intransitive and transitive sentences. [sent-357, score-0.744]
</p><p>93 Other work uses matrices to model meaning (Baroni and Zamparelli, 2010; Guevara, 2010), but only for adjective-noun phrases. [sent-359, score-0.271]
</p><p>94 by regression from the composite adjective-noun context vectors, whereas our model is bottom-up: it learns sentence/phrase meaning compositionally from the vectors of the compartments of the composites. [sent-363, score-0.358]
</p><p>95 The matrix of the intransitive ‘break’ uses the corpusobserved information about the subject of break, including that of ‘Y’, similarly the matrix of the transitive ‘break’ uses information about its subject and object, including that of ‘X’ and ‘Y’ . [sent-365, score-0.744]
</p><p>96 We evaluated our model in two ways: first against the word disambiguation task of Mitchell and Lapata (2008) for intransitive verbs, and then against a similar new experiment for transitive verbs, which we developed. [sent-367, score-0.544]
</p><p>97 Our findings in the first experiment show that the categorical method performs on par with the  leading existing approaches. [sent-368, score-0.241]
</p><p>98 However, our approach is sensitive to grammatical structure, leading us to develop a second experiment taking this into account and differentiating it from models with commutative composition operations. [sent-370, score-0.267]
</p><p>99 The second experiment’s results deliver the expected qualitative difference between models, with our categorical model outperforming the others and showing an increase in alignment with humanjudgements in correlation with the increase in sentence complexity. [sent-371, score-0.24]
</p><p>100 1403 These results show that the high level categorical distributional model, uniting empirical data with logical form, can be implemented just like any other concrete model. [sent-373, score-0.492]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('transitive', 0.349), ('coecke', 0.246), ('pregroup', 0.219), ('vectors', 0.198), ('categorical', 0.197), ('lapata', 0.185), ('mitchell', 0.176), ('meaning', 0.16), ('intransitive', 0.151), ('compositional', 0.143), ('multiplicative', 0.143), ('distributional', 0.128), ('matrices', 0.111), ('kronecker', 0.111), ('adjoint', 0.11), ('basis', 0.11), ('logical', 0.108), ('relational', 0.107), ('verb', 0.097), ('tensor', 0.095), ('ni', 0.088), ('grammatical', 0.088), ('lambek', 0.088), ('synonymity', 0.088), ('cats', 0.086), ('milk', 0.086), ('xij', 0.084), ('verbs', 0.084), ('bj', 0.079), ('nj', 0.077), ('atomic', 0.077), ('sadrzadeh', 0.076), ('grefenstette', 0.074), ('rb', 0.071), ('meanings', 0.07), ('object', 0.07), ('judgements', 0.067), ('frege', 0.066), ('preller', 0.066), ('additive', 0.064), ('adjectives', 0.064), ('matrix', 0.062), ('subject', 0.06), ('logic', 0.059), ('concrete', 0.059), ('ts', 0.057), ('similarity', 0.057), ('physics', 0.057), ('cxy', 0.057), ('kintsch', 0.057), ('thereof', 0.057), ('vector', 0.056), ('multiplication', 0.056), ('noun', 0.056), ('product', 0.053), ('dog', 0.051), ('oxford', 0.051), ('cat', 0.051), ('hence', 0.051), ('commutative', 0.048), ('cij', 0.048), ('composition', 0.047), ('nouns', 0.047), ('mathematical', 0.046), ('formal', 0.046), ('experiment', 0.044), ('alcoholic', 0.044), ('cups', 0.044), ('cva', 0.044), ('elect', 0.044), ('equalities', 0.044), ('foundational', 0.044), ('iifn', 0.044), ('ipnrotod', 0.044), ('modelhighlow', 0.044), ('morphism', 0.044), ('parks', 0.044), ('qualified', 0.044), ('sherry', 0.044), ('sohwow', 0.044), ('superscripted', 0.044), ('widdows', 0.044), ('wittgenstein', 0.044), ('wolfson', 0.044), ('xcij', 0.044), ('xhs', 0.044), ('xitj', 0.044), ('sentence', 0.043), ('nl', 0.043), ('semantic', 0.043), ('methodology', 0.04), ('taking', 0.04), ('lo', 0.04), ('disambiguating', 0.04), ('dataset', 0.04), ('argument', 0.04), ('vii', 0.038), ('lk', 0.038), ('und', 0.038), ('algebraically', 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="53-tfidf-1" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>Author: Edward Grefenstette ; Mehrnoosh Sadrzadeh</p><p>Abstract: Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences. Our model matches the results of its competitors . in the first experiment, and betters them in the second. The general improvement in results with increase in syntactic complexity showcases the compositional power of our model.</p><p>2 0.20147058 <a title="53-tfidf-2" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>Author: Diarmuid O Seaghdha ; Anna Korhonen</p><p>Abstract: This paper investigates novel methods for incorporating syntactic information in probabilistic latent variable models of lexical choice and contextual similarity. The resulting models capture the effects of context on the interpretation of a word and in particular its effect on the appropriateness of replacing that word with a potentially related one. Evaluating our techniques on two datasets, we report performance above the prior state of the art for estimating sentence similarity and ranking lexical substitutes.</p><p>3 0.13382365 <a title="53-tfidf-3" href="./emnlp-2011-Latent_Vector_Weighting_for_Word_Meaning_in_Context.html">80 emnlp-2011-Latent Vector Weighting for Word Meaning in Context</a></p>
<p>Author: Tim Van de Cruys ; Thierry Poibeau ; Anna Korhonen</p><p>Abstract: This paper presents a novel method for the computation of word meaning in context. We make use of a factorization model in which words, together with their window-based context words and their dependency relations, are linked to latent dimensions. The factorization model allows us to determine which dimensions are important for a particular context, and adapt the dependency-based feature vector of the word accordingly. The evaluation on a lexical substitution task carried out for both English and French – indicates that our approach is able to reach better results than state-of-the-art methods in lexical substitution, while at the same time providing more accurate meaning representations. –</p><p>4 0.12651584 <a title="53-tfidf-4" href="./emnlp-2011-Compositional_Matrix-Space_Models_for_Sentiment_Analysis.html">30 emnlp-2011-Compositional Matrix-Space Models for Sentiment Analysis</a></p>
<p>Author: Ainur Yessenalina ; Claire Cardie</p><p>Abstract: We present a general learning-based approach for phrase-level sentiment analysis that adopts an ordinal sentiment scale and is explicitly compositional in nature. Thus, we can model the compositional effects required for accurate assignment of phrase-level sentiment. For example, combining an adverb (e.g., “very”) with a positive polar adjective (e.g., “good”) produces a phrase (“very good”) with increased polarity over the adjective alone. Inspired by recent work on distributional approaches to compositionality, we model each word as a matrix and combine words using iterated matrix multiplication, which allows for the modeling of both additive and multiplicative semantic effects. Although the multiplication-based matrix-space framework has been shown to be a theoretically elegant way to model composition (Rudolph and Giesbrecht, 2010), training such models has to be done carefully: the optimization is nonconvex and requires a good initial starting point. This paper presents the first such algorithm for learning a matrix-space model for semantic composition. In the context of the phrase-level sentiment analysis task, our experimental results show statistically significant improvements in performance over a bagof-words model.</p><p>5 0.09820085 <a title="53-tfidf-5" href="./emnlp-2011-Exploring_Supervised_LDA_Models_for_Assigning_Attributes_to_Adjective-Noun_Phrases.html">56 emnlp-2011-Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases</a></p>
<p>Author: Matthias Hartung ; Anette Frank</p><p>Abstract: This paper introduces an attribute selection task as a way to characterize the inherent meaning of property-denoting adjectives in adjective-noun phrases, such as e.g. hot in hot summer denoting the attribute TEMPERATURE, rather than TASTE. We formulate this task in a vector space model that represents adjectives and nouns as vectors in a semantic space defined over possible attributes. The vectors incorporate latent semantic information obtained from two variants of LDA topic models. Our LDA models outperform previous approaches on a small set of 10 attributes with considerable gains on sparse representations, which highlights the strong smoothing power of LDA models. For the first time, we extend the attribute selection task to a new data set with more than 200 classes. We observe that large-scale attribute selection is a hard problem, but a subset of attributes performs robustly on the large scale as well. Again, the LDA models outperform the VSM baseline.</p><p>6 0.097338334 <a title="53-tfidf-6" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>7 0.088159174 <a title="53-tfidf-7" href="./emnlp-2011-Unsupervised_Semantic_Role_Induction_with_Graph_Partitioning.html">145 emnlp-2011-Unsupervised Semantic Role Induction with Graph Partitioning</a></p>
<p>8 0.086001001 <a title="53-tfidf-8" href="./emnlp-2011-A_Cascaded_Classification_Approach_to_Semantic_Head_Recognition.html">2 emnlp-2011-A Cascaded Classification Approach to Semantic Head Recognition</a></p>
<p>9 0.071440808 <a title="53-tfidf-9" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>10 0.068532102 <a title="53-tfidf-10" href="./emnlp-2011-Lexical_Generalization_in_CCG_Grammar_Induction_for_Semantic_Parsing.html">87 emnlp-2011-Lexical Generalization in CCG Grammar Induction for Semantic Parsing</a></p>
<p>11 0.067818217 <a title="53-tfidf-11" href="./emnlp-2011-Bootstrapping_Semantic_Parsers_from_Conversations.html">24 emnlp-2011-Bootstrapping Semantic Parsers from Conversations</a></p>
<p>12 0.060371201 <a title="53-tfidf-12" href="./emnlp-2011-Unsupervised_Learning_of_Selectional_Restrictions_and_Detection_of_Argument_Coercions.html">144 emnlp-2011-Unsupervised Learning of Selectional Restrictions and Detection of Argument Coercions</a></p>
<p>13 0.060317039 <a title="53-tfidf-13" href="./emnlp-2011-Corpus-Guided_Sentence_Generation_of_Natural_Images.html">34 emnlp-2011-Corpus-Guided Sentence Generation of Natural Images</a></p>
<p>14 0.059034947 <a title="53-tfidf-14" href="./emnlp-2011-A_Joint_Model_for_Extended_Semantic_Role_Labeling.html">7 emnlp-2011-A Joint Model for Extended Semantic Role Labeling</a></p>
<p>15 0.057624768 <a title="53-tfidf-15" href="./emnlp-2011-Improving_Bilingual_Projections_via_Sparse_Covariance_Matrices.html">73 emnlp-2011-Improving Bilingual Projections via Sparse Covariance Matrices</a></p>
<p>16 0.054735098 <a title="53-tfidf-16" href="./emnlp-2011-Semi-Supervised_Recursive_Autoencoders_for_Predicting_Sentiment_Distributions.html">120 emnlp-2011-Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</a></p>
<p>17 0.053339634 <a title="53-tfidf-17" href="./emnlp-2011-A_generative_model_for_unsupervised_discovery_of_relations_and_argument_classes_from_clinical_texts.html">14 emnlp-2011-A generative model for unsupervised discovery of relations and argument classes from clinical texts</a></p>
<p>18 0.052849427 <a title="53-tfidf-18" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>19 0.052823957 <a title="53-tfidf-19" href="./emnlp-2011-Large-Scale_Noun_Compound_Interpretation_Using_Bootstrapping_and_the_Web_as_a_Corpus.html">78 emnlp-2011-Large-Scale Noun Compound Interpretation Using Bootstrapping and the Web as a Corpus</a></p>
<p>20 0.052450545 <a title="53-tfidf-20" href="./emnlp-2011-Analyzing_Methods_for_Improving_Precision_of_Pivot_Based_Bilingual_Dictionaries.html">18 emnlp-2011-Analyzing Methods for Improving Precision of Pivot Based Bilingual Dictionaries</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.213), (1, -0.078), (2, -0.126), (3, -0.051), (4, 0.107), (5, -0.003), (6, -0.081), (7, 0.135), (8, 0.12), (9, 0.051), (10, -0.089), (11, -0.039), (12, 0.064), (13, -0.193), (14, -0.177), (15, 0.042), (16, 0.101), (17, 0.201), (18, 0.016), (19, 0.035), (20, 0.018), (21, -0.216), (22, -0.011), (23, 0.004), (24, 0.042), (25, -0.063), (26, 0.048), (27, 0.07), (28, -0.073), (29, 0.075), (30, 0.058), (31, 0.029), (32, 0.04), (33, -0.082), (34, 0.051), (35, 0.123), (36, 0.003), (37, 0.006), (38, 0.094), (39, 0.027), (40, -0.093), (41, -0.055), (42, -0.036), (43, 0.145), (44, -0.05), (45, 0.029), (46, -0.031), (47, -0.121), (48, 0.015), (49, -0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96011192 <a title="53-lsi-1" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>Author: Edward Grefenstette ; Mehrnoosh Sadrzadeh</p><p>Abstract: Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences. Our model matches the results of its competitors . in the first experiment, and betters them in the second. The general improvement in results with increase in syntactic complexity showcases the compositional power of our model.</p><p>2 0.76162851 <a title="53-lsi-2" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>Author: Diarmuid O Seaghdha ; Anna Korhonen</p><p>Abstract: This paper investigates novel methods for incorporating syntactic information in probabilistic latent variable models of lexical choice and contextual similarity. The resulting models capture the effects of context on the interpretation of a word and in particular its effect on the appropriateness of replacing that word with a potentially related one. Evaluating our techniques on two datasets, we report performance above the prior state of the art for estimating sentence similarity and ranking lexical substitutes.</p><p>3 0.69675857 <a title="53-lsi-3" href="./emnlp-2011-Latent_Vector_Weighting_for_Word_Meaning_in_Context.html">80 emnlp-2011-Latent Vector Weighting for Word Meaning in Context</a></p>
<p>Author: Tim Van de Cruys ; Thierry Poibeau ; Anna Korhonen</p><p>Abstract: This paper presents a novel method for the computation of word meaning in context. We make use of a factorization model in which words, together with their window-based context words and their dependency relations, are linked to latent dimensions. The factorization model allows us to determine which dimensions are important for a particular context, and adapt the dependency-based feature vector of the word accordingly. The evaluation on a lexical substitution task carried out for both English and French – indicates that our approach is able to reach better results than state-of-the-art methods in lexical substitution, while at the same time providing more accurate meaning representations. –</p><p>4 0.52612025 <a title="53-lsi-4" href="./emnlp-2011-A_Cascaded_Classification_Approach_to_Semantic_Head_Recognition.html">2 emnlp-2011-A Cascaded Classification Approach to Semantic Head Recognition</a></p>
<p>Author: Lukas Michelbacher ; Alok Kothari ; Martin Forst ; Christina Lioma ; Hinrich Schutze</p><p>Abstract: Most NLP systems use tokenization as part of preprocessing. Generally, tokenizers are based on simple heuristics and do not recognize multi-word units (MWUs) like hot dog or black hole unless a precompiled list of MWUs is available. In this paper, we propose a new cascaded model for detecting MWUs of arbitrary length for tokenization, focusing on noun phrases in the physics domain. We adopt a classification approach because unlike other work on MWUs – tokenization requires a completely automatic approach. We achieve an accuracy of 68% for recognizing non-compositional MWUs and show that our MWU recognizer improves retrieval performance when used as part of an information retrieval system. – 1</p><p>5 0.45316902 <a title="53-lsi-5" href="./emnlp-2011-Corpus-Guided_Sentence_Generation_of_Natural_Images.html">34 emnlp-2011-Corpus-Guided Sentence Generation of Natural Images</a></p>
<p>Author: Yezhou Yang ; Ching Teo ; Hal Daume III ; Yiannis Aloimonos</p><p>Abstract: We propose a sentence generation strategy that describes images by predicting the most likely nouns, verbs, scenes and prepositions that make up the core sentence structure. The input are initial noisy estimates of the objects and scenes detected in the image using state of the art trained detectors. As predicting actions from still images directly is unreliable, we use a language model trained from the English Gigaword corpus to obtain their estimates; together with probabilities of co-located nouns, scenes and prepositions. We use these estimates as parameters on a HMM that models the sentence generation process, with hidden nodes as sentence components and image detections as the emissions. Experimental results show that our strategy of combining vision and language produces readable and de- , scriptive sentences compared to naive strategies that use vision alone.</p><p>6 0.42794228 <a title="53-lsi-6" href="./emnlp-2011-Improving_Bilingual_Projections_via_Sparse_Covariance_Matrices.html">73 emnlp-2011-Improving Bilingual Projections via Sparse Covariance Matrices</a></p>
<p>7 0.41419041 <a title="53-lsi-7" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>8 0.41265562 <a title="53-lsi-8" href="./emnlp-2011-Compositional_Matrix-Space_Models_for_Sentiment_Analysis.html">30 emnlp-2011-Compositional Matrix-Space Models for Sentiment Analysis</a></p>
<p>9 0.36666477 <a title="53-lsi-9" href="./emnlp-2011-Exploring_Supervised_LDA_Models_for_Assigning_Attributes_to_Adjective-Noun_Phrases.html">56 emnlp-2011-Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases</a></p>
<p>10 0.34560227 <a title="53-lsi-10" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>11 0.34382594 <a title="53-lsi-11" href="./emnlp-2011-Computing_Logical_Form_on_Regulatory_Texts.html">32 emnlp-2011-Computing Logical Form on Regulatory Texts</a></p>
<p>12 0.33855036 <a title="53-lsi-12" href="./emnlp-2011-Literal_and_Metaphorical_Sense_Identification_through_Concrete_and_Abstract_Context.html">91 emnlp-2011-Literal and Metaphorical Sense Identification through Concrete and Abstract Context</a></p>
<p>13 0.33586404 <a title="53-lsi-13" href="./emnlp-2011-Bootstrapping_Semantic_Parsers_from_Conversations.html">24 emnlp-2011-Bootstrapping Semantic Parsers from Conversations</a></p>
<p>14 0.33152586 <a title="53-lsi-14" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>15 0.32983971 <a title="53-lsi-15" href="./emnlp-2011-Approximate_Scalable_Bounded_Space_Sketch_for_Large_Data_NLP.html">19 emnlp-2011-Approximate Scalable Bounded Space Sketch for Large Data NLP</a></p>
<p>16 0.31777889 <a title="53-lsi-16" href="./emnlp-2011-Unsupervised_Semantic_Role_Induction_with_Graph_Partitioning.html">145 emnlp-2011-Unsupervised Semantic Role Induction with Graph Partitioning</a></p>
<p>17 0.31129766 <a title="53-lsi-17" href="./emnlp-2011-Lexical_Generalization_in_CCG_Grammar_Induction_for_Semantic_Parsing.html">87 emnlp-2011-Lexical Generalization in CCG Grammar Induction for Semantic Parsing</a></p>
<p>18 0.30901286 <a title="53-lsi-18" href="./emnlp-2011-Unsupervised_Learning_of_Selectional_Restrictions_and_Detection_of_Argument_Coercions.html">144 emnlp-2011-Unsupervised Learning of Selectional Restrictions and Detection of Argument Coercions</a></p>
<p>19 0.29798087 <a title="53-lsi-19" href="./emnlp-2011-Hierarchical_Verb_Clustering_Using_Graph_Factorization.html">67 emnlp-2011-Hierarchical Verb Clustering Using Graph Factorization</a></p>
<p>20 0.29762417 <a title="53-lsi-20" href="./emnlp-2011-Large-Scale_Noun_Compound_Interpretation_Using_Bootstrapping_and_the_Web_as_a_Corpus.html">78 emnlp-2011-Large-Scale Noun Compound Interpretation Using Bootstrapping and the Web as a Corpus</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.086), (36, 0.023), (37, 0.025), (45, 0.069), (53, 0.017), (54, 0.055), (57, 0.016), (62, 0.034), (64, 0.018), (66, 0.054), (69, 0.021), (79, 0.065), (82, 0.041), (87, 0.017), (90, 0.029), (92, 0.278), (96, 0.044), (98, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74659365 <a title="53-lda-1" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>Author: Edward Grefenstette ; Mehrnoosh Sadrzadeh</p><p>Abstract: Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences. Our model matches the results of its competitors . in the first experiment, and betters them in the second. The general improvement in results with increase in syntactic complexity showcases the compositional power of our model.</p><p>2 0.48613304 <a title="53-lda-2" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<p>Author: Diarmuid O Seaghdha ; Anna Korhonen</p><p>Abstract: This paper investigates novel methods for incorporating syntactic information in probabilistic latent variable models of lexical choice and contextual similarity. The resulting models capture the effects of context on the interpretation of a word and in particular its effect on the appropriateness of replacing that word with a potentially related one. Evaluating our techniques on two datasets, we report performance above the prior state of the art for estimating sentence similarity and ranking lexical substitutes.</p><p>3 0.47459304 <a title="53-lda-3" href="./emnlp-2011-Lexical_Generalization_in_CCG_Grammar_Induction_for_Semantic_Parsing.html">87 emnlp-2011-Lexical Generalization in CCG Grammar Induction for Semantic Parsing</a></p>
<p>Author: Tom Kwiatkowski ; Luke Zettlemoyer ; Sharon Goldwater ; Mark Steedman</p><p>Abstract: We consider the problem of learning factored probabilistic CCG grammars for semantic parsing from data containing sentences paired with logical-form meaning representations. Traditional CCG lexicons list lexical items that pair words and phrases with syntactic and semantic content. Such lexicons can be inefficient when words appear repeatedly with closely related lexical content. In this paper, we introduce factored lexicons, which include both lexemes to model word meaning and templates to model systematic variation in word usage. We also present an algorithm for learning factored CCG lexicons, along with a probabilistic parse-selection model. Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance greatly from the lexical factoring. benefits</p><p>4 0.47324482 <a title="53-lda-4" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Noah A. Smith</p><p>Abstract: We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.</p><p>5 0.46990985 <a title="53-lda-5" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>Author: Benjamin Borschinger ; Bevan K. Jones ; Mark Johnson</p><p>Abstract: It is often assumed that ‘grounded’ learning tasks are beyond the scope of grammatical inference techniques. In this paper, we show that the grounded task of learning a semantic parser from ambiguous training data as discussed in Kim and Mooney (2010) can be reduced to a Probabilistic Context-Free Grammar learning task in a way that gives state of the art results. We further show that additionally letting our model learn the language’s canonical word order improves its performance and leads to the highest semantic parsing f-scores previously reported in the literature.1</p><p>6 0.46974322 <a title="53-lda-6" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>7 0.46756366 <a title="53-lda-7" href="./emnlp-2011-A_Model_of_Discourse_Predictions_in_Human_Sentence_Processing.html">8 emnlp-2011-A Model of Discourse Predictions in Human Sentence Processing</a></p>
<p>8 0.46522614 <a title="53-lda-8" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>9 0.46500203 <a title="53-lda-9" href="./emnlp-2011-Multiword_Expression_Identification_with_Tree_Substitution_Grammars%3A_A_Parsing_tour_de_force_with_French.html">97 emnlp-2011-Multiword Expression Identification with Tree Substitution Grammars: A Parsing tour de force with French</a></p>
<p>10 0.46428481 <a title="53-lda-10" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>11 0.46400356 <a title="53-lda-11" href="./emnlp-2011-Better_Evaluation_Metrics_Lead_to_Better_Machine_Translation.html">22 emnlp-2011-Better Evaluation Metrics Lead to Better Machine Translation</a></p>
<p>12 0.46378082 <a title="53-lda-12" href="./emnlp-2011-Hierarchical_Phrase-based_Translation_Representations.html">66 emnlp-2011-Hierarchical Phrase-based Translation Representations</a></p>
<p>13 0.46220192 <a title="53-lda-13" href="./emnlp-2011-Exploring_Supervised_LDA_Models_for_Assigning_Attributes_to_Adjective-Noun_Phrases.html">56 emnlp-2011-Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases</a></p>
<p>14 0.46057689 <a title="53-lda-14" href="./emnlp-2011-Exploiting_Parse_Structures_for_Native_Language_Identification.html">54 emnlp-2011-Exploiting Parse Structures for Native Language Identification</a></p>
<p>15 0.45962033 <a title="53-lda-15" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>16 0.45934972 <a title="53-lda-16" href="./emnlp-2011-Hierarchical_Verb_Clustering_Using_Graph_Factorization.html">67 emnlp-2011-Hierarchical Verb Clustering Using Graph Factorization</a></p>
<p>17 0.45863587 <a title="53-lda-17" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>18 0.45777294 <a title="53-lda-18" href="./emnlp-2011-Syntax-Based_Grammaticality_Improvement_using_CCG_and_Guided_Search.html">132 emnlp-2011-Syntax-Based Grammaticality Improvement using CCG and Guided Search</a></p>
<p>19 0.45741919 <a title="53-lda-19" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>20 0.4573054 <a title="53-lda-20" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
