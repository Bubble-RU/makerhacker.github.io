<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>133 emnlp-2011-The Imagination of Crowds: Conversational AAC Language Modeling using Crowdsourcing and Large Data Sources</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-133" href="#">emnlp2011-133</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>133 emnlp-2011-The Imagination of Crowds: Conversational AAC Language Modeling using Crowdsourcing and Large Data Sources</h1>
<br/><p>Source: <a title="emnlp-2011-133-pdf" href="http://aclweb.org/anthology//D/D11/D11-1065.pdf">pdf</a></p><p>Author: Keith Vertanen ; Per Ola Kristensson</p><p>Abstract: Augmented and alternative communication (AAC) devices enable users with certain communication disabilities to participate in everyday conversations. Such devices often rely on statistical language models to improve text entry by offering word predictions. These predictions can be improved if the language model is trained on data that closely reflects the style of the users’ intended communications. Unfortunately, there is no large dataset consisting of genuine AAC messages. In this paper we demonstrate how we can crowdsource the creation of a large set of fictional AAC messages. We show that these messages model conversational AAC better than the currently used datasets based on telephone conversations or newswire text. We leverage our crowdsourced messages to intelligently select sentences from much larger sets of Twitter, blog and Usenet data. Compared to a model trained only on telephone transcripts, our best performing model reduced perplexity on three test sets of AAC-like communications by 60– 82% relative. This translated to a potential keystroke savings in a predictive keyboard interface of 5–1 1%.</p><p>Reference: <a title="emnlp-2011-133-reference" href="../emnlp2011_reference/emnlp-2011-The_Imagination_of_Crowds%3A_Conversational_AAC_Language_Modeling_using_Crowdsourcing_and_Large_Data_Sources_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 vert anen@princet on edu Abstract Augmented and alternative communication (AAC) devices enable users with certain communication disabilities to participate in everyday conversations. [sent-2, score-0.397]
</p><p>2 We show that these messages model conversational AAC better than the currently used datasets based on telephone conversations or newswire text. [sent-7, score-0.308]
</p><p>3 We leverage our  crowdsourced messages to intelligently select sentences from much larger sets of Twitter, blog and Usenet data. [sent-8, score-0.472]
</p><p>4 Compared to a model trained only on telephone transcripts, our best performing model reduced perplexity on three test sets of AAC-like communications by 60– 82% relative. [sent-9, score-0.385]
</p><p>5 This translated to a potential keystroke savings in a predictive keyboard interface of 5–1 1%. [sent-10, score-0.52]
</p><p>6 1 Introduction Users with certain communication disabilities rely on augmented and alternative communication (AAC) devices to take part in everyday conversations. [sent-11, score-0.335]
</p><p>7 Often these devices consist of a predictive text input method coupled with text-to-speech output. [sent-12, score-0.133]
</p><p>8 For AAC devices this means closely modeling everyday face-to-face communications. [sent-24, score-0.127]
</p><p>9 Therefore, previous research has used transcripts of telephone conversa-  tions or newswire text. [sent-27, score-0.177]
</p><p>10 In this paper we show that it is possible to significantly improve conversational AAC language modeling by first crowdsourcing the creation of a fictional collection of AAC messages on the Amazon Mechanical Turk microtask market. [sent-29, score-0.418]
</p><p>11 ec th2o0d1s1 i Ans Nsoactuiartaioln La fonrg Cuaogmep Purtoatcieosnsainlg L,in pgaugies ti 7c0s0–71 , fully designed microtask we collected 5890 messages from 298 unique workers. [sent-32, score-0.186]
</p><p>12 As we will see, word-for-word these fictional AAC messages are better at predicting AAC test sets than a wide-range of other text sources. [sent-33, score-0.157]
</p><p>13 Further, we demonstrate that Twitter, blog and Usenet data outperform telephone transcripts or newswire text. [sent-34, score-0.319]
</p><p>14 While our crowdsourced AAC data is better than other text sources, it is too small to train high-quality long-span language models. [sent-35, score-0.156]
</p><p>15 We therefore investigate how to use our crowdsourced collection to intelligently select AAC-like sentences from Twitter, blog and Usenet data. [sent-36, score-0.402]
</p><p>16 Using this method, we build a compact and well-performing mixture model from the Twitter, blog and Usenet sentences most similar to our crowdsourced data. [sent-39, score-0.404]
</p><p>17 On the three most AAC-like test sets, we found substantial reductions in not only perplexity but also in potential keystroke savings when used in a predictive keyboard interface. [sent-41, score-0.553]
</p><p>18 Finally, to aid other AAC researchers, we have publicly released our crowdsourced AAC collection, word lists and best-performing language models1 . [sent-42, score-0.156]
</p><p>19 2  Crowdsourcing AAC-like Messages  As we mentioned in the introduction, there are unfortunately no publicly available sources of genuine conversational AAC messages. [sent-43, score-0.114]
</p><p>20 We conjectured we could create surrogate data by asking workers on Amazon Mechanical Turk to imagine they were a user of an AAC device and having them invent things they might want to say. [sent-44, score-0.363]
</p><p>21 While crowdsourcing is commonly used for simple human computation  tasks, such as labeling images and transcribing audio, it is an open research question whether we can leverage workers’ creativity to invent plausible and useful AAC-like messages. [sent-45, score-0.195]
</p><p>22 In this section, we describe our carefully constructed microtask and compare how well our collected messages correspond to communications from actual AAC users. [sent-46, score-0.342]
</p><p>23 In type 1, the workers were told to imagine that due to an accident or medical condition they had to use a communication device to speak for them. [sent-53, score-0.417]
</p><p>24 In type 2, a worker first judged the plausibility of a communication written by a previous worker (figure 2). [sent-57, score-0.257]
</p><p>25 After judging, the worker was asked to “invent a completely new communication” as if the worker was the AAC user. [sent-58, score-0.126]
</p><p>26 The same communication was judged by three separate workers. [sent-60, score-0.107]
</p><p>27 2 Data Cleaning While most workers produced plausible and often creative communications, some workers entered obvious garbage. [sent-63, score-0.526]
</p><p>28 These workers were identified by a quick visual scan of the submitted communications. [sent-64, score-0.235]
</p><p>29 We rejected the work of 9% of the workers in type 1 and 4% of the workers in type 2. [sent-65, score-0.518]
</p><p>30 After removing these workers, we had 2481 communications from type 1 and 4440 communications from type 2. [sent-66, score-0.36]
</p><p>31 We first manually reviewed communications  sorted by worker. [sent-68, score-0.156]
</p><p>32 We removed workers whose text was non-fluent English or not plausible (e. [sent-69, score-0.268]
</p><p>33 We removed communications with an out-of-vocabulary (OOV) rate of over 20% with respect to a large word list of 330K words obtained from human-edited dictionaries2. [sent-73, score-0.156]
</p><p>34 We also removed communications that were all in upper case, contained common texting abbreviations (e. [sent-74, score-0.156]
</p><p>35 “plz”, “ru”, “2day”), communications over 80 characters, and communications with excessive letter repetitions (e. [sent-76, score-0.312]
</p><p>36 3 Results Tables 1and 2 show some example communications obtained in each HIT type. [sent-81, score-0.156]
</p><p>37 Sometimes, but not always, type 2 resulted in the worker writing a similar communication as the one judged. [sent-82, score-0.224]
</p><p>38 While it may reduce the diversity of communications, we found that workers were more eager to accept HITs of type 2. [sent-84, score-0.259]
</p><p>39 We also had to reject less work in type 2 and qualitatively found the communications to be more AAClike. [sent-89, score-0.18]
</p><p>40 Since workers had to imagine themselves in a 2We combined Wiktionary, Webster’s dictionary provided by Project Gutenberg, the CMU pronouncing dictionary and GNU aspell. [sent-90, score-0.286]
</p><p>41 3  Comparison of Training Sources  In this section, we compare the predictive performance of language models trained on our Turk AAC data with models trained on other text sources. [sent-109, score-0.114]
</p><p>42 TWITTER We collected Twitter messages via tThe streaming API between December 2010 and March 2011. [sent-127, score-0.133]
</p><p>43 Twitter may be particularly well suited for modeling AAC communications as tweets are short typed messages that are often informal person-to-person communications. [sent-129, score-0.289]
</p><p>44 TURKTRAIN Communications from 80% of the wTorkers in our crowdsourced collection. [sent-133, score-0.156]
</p><p>45 1 Test Sets We evaluated our models on the following test sets: •  •  •  •  COMM Sentences written in response to hypothetical communication situations collected by Venkatagiri (1999). [sent-139, score-0.136]
</p><p>46 TURKDEV Communications from 10% of the wTorkers in our crowdsourced collection (disjoint from TURKTRAIN and TURKTEST). [sent-146, score-0.19]
</p><p>47 TURKTEST Communications from 10% of the wTorkers in our crowdsourced collection (disjoint –  –  –  –  from TURKTRAIN and TURKDEV). [sent-149, score-0.19]
</p><p>48 The perplexity is the average of 20 models trained on random subsets of the training data (one standard deviation  error bars). [sent-183, score-0.17]
</p><p>49 We computed the mean and standard deviation of the per-word perplexity of the set of 20 models. [sent-192, score-0.146]
</p><p>50 As in the previous experiment, we computed the mean and standard deviation of the per-word perplexity of a set of 20 models. [sent-200, score-0.146]
</p><p>51 Increasing the amount of training data substantially reduced perplexity compared to 704 our small TURKTRAIN collection (figure 4). [sent-201, score-0.156]
</p><p>52 (1984) analyzed the communications made by five nonspeaking adults over 14 days. [sent-205, score-0.156]
</p><p>53 We believe this resulted from the situation we asked workers to imagine (i. [sent-214, score-0.316]
</p><p>54 ” Beukelman reports 33% of all communications could be made using only the top 500 words. [sent-224, score-0.156]
</p><p>55 Beukelman reports that 80% of words in the AAC users’ communications were in the top 500 words. [sent-234, score-0.156]
</p><p>56 81% of the words in our crowdsourced data were in this word list. [sent-235, score-0.156]
</p><p>57 4 Using Large Datasets Effectively In the previous section, we found our crowdsourced data was good at predicting AAC-like test sets. [sent-239, score-0.156]
</p><p>58 Therefore, we instead investigated how to use our crowdsourced data to intelligently select AAC-like data from other large datasets. [sent-242, score-0.202]
</p><p>59 This tends to find variants of exist-  ing communications in our Turk collection. [sent-256, score-0.156]
</p><p>60 We calculated the perplexity of each model on three test sets. [sent-272, score-0.122]
</p><p>61 entropy difference was the best on COMM, reducing perplexity by 10–20% relative compared to crossentropy selection. [sent-276, score-0.153]
</p><p>62 We show performance with 707  respect to usage in a typical AAC text entry interface based on word prediction. [sent-297, score-0.113]
</p><p>63 1 Predictive Text Entry Many AAC communication devices use word predictions. [sent-299, score-0.174]
</p><p>64 In a word prediction interface users type letters and the interface offers word completions based on the prefix of the current word and often the prior text. [sent-300, score-0.298]
</p><p>65 We assume a hypothetical predictive keyboard interface that displays five word predictions. [sent-302, score-0.275]
</p><p>66 Our keyboard makes predictions based on up to three words  of prior context. [sent-303, score-0.149]
</p><p>67 Our keyboard predicts words even before the first letter of a new word is typed. [sent-304, score-0.12]
</p><p>68 Ifthe system makes a correct prediction, we assume it takes only one keystroke to enter the word and any following space. [sent-306, score-0.146]
</p><p>69 We evaluate our predictive keyboard using the common metric of keystroke savings (KS): KS =  ? [sent-311, score-0.431]
</p><p>70 × 100%,  where kp is the number of keystrokes required with word predictions and ka is the number of keystrokes required without word prediction. [sent-315, score-0.135]
</p><p>71 2  Predictive Performance Experiment  We compared our mixture model using crossentropy difference selection with three baseline models trained on all of TWITTER, SWITCHBOARD and TURKTRAIN. [sent-317, score-0.171]
</p><p>72 9%  Table 4: Perplexity (PPL) and keystroke savings (KS) of different language models on four test sets. [sent-353, score-0.245]
</p><p>73 mixture model provided substantial increases in keystroke savings compared to a model trained solely on Switchboard. [sent-355, score-0.351]
</p><p>74 Our Switchboard model performed the best on SWITCHTEST with a keystroke savings of 58. [sent-358, score-0.245]
</p><p>75 For comparison, past work reported a keystroke savings of 55. [sent-360, score-0.245]
</p><p>76 3 Larger Mixture Model Experiment Our mixture language model used the best thresholds with respect to TURKDEV. [sent-366, score-0.134]
</p><p>77 This might be suboptimal in practice if an AAC user’s communications are somewhat different or more diverse than the language generated by the Turk workers. [sent-368, score-0.156]
</p><p>78 We trained a series of mixture models in which we varied the cross-entropy difference thresholds 708  Change from optimal thresholds  Figure 8: Keystroke savings on mixture models varying a constant added to the optimal thresholds with respect to TURKDEV. [sent-369, score-0.443]
</p><p>79 Using somewhat larger models did improve keystroke savings for all test sets except for TURKTEST (figure 8). [sent-372, score-0.245]
</p><p>80 6  Discussion  Given the ethical implications of collecting messages from actual AAC users, it is unlikely that a large corpus of genuine AAC messages will ever be available to researchers. [sent-381, score-0.273]
</p><p>81 This may have resulted in more messages about simple situations and perceived needs which could differ from true AAC usage. [sent-387, score-0.134]
</p><p>82 We asked workers to imagine they were using a scanning-style AAC device. [sent-390, score-0.286]
</p><p>83 We believe this led workers to presume they would require assistance in many routine physical tasks. [sent-391, score-0.235]
</p><p>84 Our workers were (presumably) without cognitive or language impairments. [sent-392, score-0.235]
</p><p>85 However, obtaining data representative of users with cognitive or language impairments via crowdsourcing would probably be difficult. [sent-395, score-0.155]
</p><p>86 We hope that by releasing our data and models it may be possible for those privy to real AAC communications to validate and report about the techniques described in this paper. [sent-398, score-0.156]
</p><p>87 We evaluated our models in terms of perplexity and keystrokes savings within the auspices of a predictive keyboard. [sent-399, score-0.34]
</p><p>88 Finally, while the predictive keyboard is a commonly studied interface, it is not appropriate for all AAC users. [sent-403, score-0.186]
</p><p>89 Eye-tracker users may prefer an interface such as Dasher (Ward and MacKay, 2002). [sent-404, score-0.151]
</p><p>90 Single-switch users may prefer an interface such as Nomon (Broderick and MacKay, 2009). [sent-405, score-0.151]
</p><p>91 Any AAC interface based on word- or letter-based predictions stands to benefit from the methods described in this paper. [sent-406, score-0.118]
</p><p>92 709  7  Conclusions  In this paper we have shown how workers’ creativity on a microtask crowdsourcing market can be used to create fictional but plausible AAC communications. [sent-407, score-0.255]
</p><p>93 We have demonstrated that these messages model conversational AAC better than the currently used datasets based on telephone conversations or newswire text. [sent-408, score-0.308]
</p><p>94 We used our new crowdsourced dataset to intelligently select sentences from Twitter, blog and Usenet data. [sent-409, score-0.368]
</p><p>95 Finally, compared to a model trained only on Switchboard, our best performing model reduced perplexity by 60-82% relative on three AAC-like test sets. [sent-412, score-0.146]
</p><p>96 This translated to a potential keystroke savings in a predictive keyboard interface of 5–1 1%. [sent-413, score-0.52]
</p><p>97 Our models significantly outperform models trained on the commonly used data sources  of telephone transcripts and newswire text. [sent-415, score-0.225]
</p><p>98 To aid other researchers, we have publicly released our crowdsourced AAC collection, word lists and bestperforming models. [sent-416, score-0.156]
</p><p>99 Frequency of word occurrence in communication samples produced by adult communication aid users. [sent-429, score-0.214]
</p><p>100 Efficient keyboard layouts for sequential access in augmentative and alternative communication. [sent-542, score-0.16]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('aac', 0.571), ('workers', 0.235), ('twitter', 0.221), ('usenet', 0.187), ('specialists', 0.173), ('switchboard', 0.161), ('switchtest', 0.16), ('crowdsourced', 0.156), ('communications', 0.156), ('comm', 0.149), ('keystroke', 0.146), ('turkdev', 0.146), ('turktrain', 0.146), ('blog', 0.142), ('trnka', 0.133), ('perplexity', 0.122), ('keyboard', 0.12), ('turktest', 0.12), ('communication', 0.107), ('beukelman', 0.107), ('turk', 0.104), ('messages', 0.104), ('savings', 0.099), ('crowdsourcing', 0.093), ('interface', 0.089), ('telephone', 0.083), ('mixture', 0.082), ('devices', 0.067), ('predictive', 0.066), ('worker', 0.063), ('users', 0.062), ('transcripts', 0.058), ('fictional', 0.053), ('keystrokes', 0.053), ('microtask', 0.053), ('conversational', 0.052), ('thresholds', 0.052), ('imagine', 0.051), ('intelligently', 0.046), ('wandmacher', 0.046), ('invent', 0.046), ('ks', 0.042), ('wer', 0.041), ('augmentative', 0.04), ('lesher', 0.04), ('wtorkers', 0.04), ('moore', 0.039), ('keith', 0.039), ('genuine', 0.038), ('newswire', 0.036), ('selection', 0.034), ('lewis', 0.034), ('collection', 0.034), ('prediction', 0.034), ('plausible', 0.033), ('intelligent', 0.033), ('conversations', 0.033), ('pruning', 0.032), ('user', 0.031), ('everyday', 0.031), ('crossentropy', 0.031), ('resulted', 0.03), ('vocabulary', 0.03), ('predictions', 0.029), ('collected', 0.029), ('modeling', 0.029), ('prevented', 0.029), ('graff', 0.027), ('aaclike', 0.027), ('antoine', 0.027), ('broderick', 0.027), ('bulyko', 0.027), ('burton', 0.027), ('csonversations', 0.027), ('debra', 0.027), ('ethical', 0.027), ('gonz', 0.027), ('horabail', 0.027), ('mackay', 0.027), ('nestor', 0.027), ('rinkus', 0.027), ('scanning', 0.027), ('shaoul', 0.027), ('slippers', 0.027), ('tonio', 0.027), ('venkatagiri', 0.027), ('yarrington', 0.027), ('sources', 0.024), ('deviation', 0.024), ('entry', 0.024), ('type', 0.024), ('sentences', 0.024), ('trained', 0.024), ('ppl', 0.023), ('typing', 0.023), ('julio', 0.023), ('entered', 0.023), ('creativity', 0.023), ('pasting', 0.023), ('disabilities', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000012 <a title="133-tfidf-1" href="./emnlp-2011-The_Imagination_of_Crowds%3A_Conversational_AAC_Language_Modeling_using_Crowdsourcing_and_Large_Data_Sources.html">133 emnlp-2011-The Imagination of Crowds: Conversational AAC Language Modeling using Crowdsourcing and Large Data Sources</a></p>
<p>Author: Keith Vertanen ; Per Ola Kristensson</p><p>Abstract: Augmented and alternative communication (AAC) devices enable users with certain communication disabilities to participate in everyday conversations. Such devices often rely on statistical language models to improve text entry by offering word predictions. These predictions can be improved if the language model is trained on data that closely reflects the style of the users’ intended communications. Unfortunately, there is no large dataset consisting of genuine AAC messages. In this paper we demonstrate how we can crowdsource the creation of a large set of fictional AAC messages. We show that these messages model conversational AAC better than the currently used datasets based on telephone conversations or newswire text. We leverage our crowdsourced messages to intelligently select sentences from much larger sets of Twitter, blog and Usenet data. Compared to a model trained only on telephone transcripts, our best performing model reduced perplexity on three test sets of AAC-like communications by 60– 82% relative. This translated to a potential keystroke savings in a predictive keyboard interface of 5–1 1%.</p><p>2 0.19212721 <a title="133-tfidf-2" href="./emnlp-2011-Discriminating_Gender_on_Twitter.html">41 emnlp-2011-Discriminating Gender on Twitter</a></p>
<p>Author: John D. Burger ; John Henderson ; George Kim ; Guido Zarrella</p><p>Abstract: Accurate prediction of demographic attributes from social media and other informal online content is valuable for marketing, personalization, and legal investigation. This paper describes the construction of a large, multilingual dataset labeled with gender, and investigates statistical models for determining the gender of uncharacterized Twitter users. We explore several different classifier types on this dataset. We show the degree to which classifier accuracy varies based on tweet volumes as well as when various kinds of profile metadata are included in the models. We also perform a large-scale human assessment using Amazon Mechanical Turk. Our methods significantly out-perform both baseline models and almost all humans on the same task.</p><p>3 0.13347371 <a title="133-tfidf-3" href="./emnlp-2011-Active_Learning_with_Amazon_Mechanical_Turk.html">17 emnlp-2011-Active Learning with Amazon Mechanical Turk</a></p>
<p>Author: Florian Laws ; Christian Scheible ; Hinrich Schutze</p><p>Abstract: Supervised classification needs large amounts of annotated training data that is expensive to create. Two approaches that reduce the cost of annotation are active learning and crowdsourcing. However, these two approaches have not been combined successfully to date. We evaluate the utility of active learning in crowdsourcing on two tasks, named entity recognition and sentiment detection, and show that active learning outperforms random selection of annotation examples in a noisy crowdsourcing scenario.</p><p>4 0.085442193 <a title="133-tfidf-4" href="./emnlp-2011-Named_Entity_Recognition_in_Tweets%3A_An_Experimental_Study.html">98 emnlp-2011-Named Entity Recognition in Tweets: An Experimental Study</a></p>
<p>Author: Alan Ritter ; Sam Clark ; Mausam ; Oren Etzioni</p><p>Abstract: People tweet more than 100 Million times daily, yielding a noisy, informal, but sometimes informative corpus of 140-character messages that mirrors the zeitgeist in an unprecedented manner. The performance of standard NLP tools is severely degraded on tweets. This paper addresses this issue by re-building the NLP pipeline beginning with part-of-speech tagging, through chunking, to named-entity recognition. Our novel T-NER system doubles F1 score compared with the Stanford NER system. T-NER leverages the redundancy inherent in tweets to achieve this performance, using LabeledLDA to exploit Freebase dictionaries as a source of distant supervision. LabeledLDA outperforms cotraining, increasing F1 by 25% over ten common entity types. Our NLP tools are available at: http : / / github .com/ aritt er /twitte r_nlp</p><p>5 0.081456408 <a title="133-tfidf-5" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<p>Author: Amittai Axelrod ; Xiaodong He ; Jianfeng Gao</p><p>Abstract: Xiaodong He Microsoft Research Redmond, WA 98052 xiaohe @mi cro s o ft . com Jianfeng Gao Microsoft Research Redmond, WA 98052 j fgao @mi cro s o ft . com have its own argot, vocabulary or stylistic preferences, such that the corpus characteristics will necWe explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain. These sentences may be selected with simple cross-entropy based methods, of which we present three. As these sentences are not themselves identical to the in-domain data, we call them pseudo in-domain subcorpora. These subcorpora 1% the size of the original can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus. Performance is further improved when we use these domain-adapted models in combination with a true in-domain model. The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining in- and general-domain systems during decoding. – –</p><p>6 0.078601345 <a title="133-tfidf-6" href="./emnlp-2011-Divide_and_Conquer%3A_Crowdsourcing_the_Creation_of_Cross-Lingual_Textual_Entailment_Corpora.html">42 emnlp-2011-Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora</a></p>
<p>7 0.077930741 <a title="133-tfidf-7" href="./emnlp-2011-Linguistic_Redundancy_in_Twitter.html">89 emnlp-2011-Linguistic Redundancy in Twitter</a></p>
<p>8 0.077238619 <a title="133-tfidf-8" href="./emnlp-2011-Rumor_has_it%3A_Identifying_Misinformation_in_Microblogs.html">117 emnlp-2011-Rumor has it: Identifying Misinformation in Microblogs</a></p>
<p>9 0.07399787 <a title="133-tfidf-9" href="./emnlp-2011-Data-Driven_Response_Generation_in_Social_Media.html">38 emnlp-2011-Data-Driven Response Generation in Social Media</a></p>
<p>10 0.071284257 <a title="133-tfidf-10" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>11 0.065051816 <a title="133-tfidf-11" href="./emnlp-2011-Cooooooooooooooollllllllllllll%21%21%21%21%21%21%21%21%21%21%21%21%21%21_Using_Word_Lengthening_to_Detect_Sentiment_in_Microblogs.html">33 emnlp-2011-Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using Word Lengthening to Detect Sentiment in Microblogs</a></p>
<p>12 0.057966519 <a title="133-tfidf-12" href="./emnlp-2011-Twitter_Catches_The_Flu%3A_Detecting_Influenza_Epidemics_using_Twitter.html">139 emnlp-2011-Twitter Catches The Flu: Detecting Influenza Epidemics using Twitter</a></p>
<p>13 0.048228934 <a title="133-tfidf-13" href="./emnlp-2011-Syntactic_Decision_Tree_LMs%3A_Random_Selection_or_Intelligent_Design%3F.html">131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</a></p>
<p>14 0.043818761 <a title="133-tfidf-14" href="./emnlp-2011-Identifying_and_Following_Expert_Investors_in_Stock_Microblogs.html">71 emnlp-2011-Identifying and Following Expert Investors in Stock Microblogs</a></p>
<p>15 0.043367837 <a title="133-tfidf-15" href="./emnlp-2011-Bootstrapping_Semantic_Parsers_from_Conversations.html">24 emnlp-2011-Bootstrapping Semantic Parsers from Conversations</a></p>
<p>16 0.040842902 <a title="133-tfidf-16" href="./emnlp-2011-Language_Models_for_Machine_Translation%3A_Original_vs._Translated_Texts.html">76 emnlp-2011-Language Models for Machine Translation: Original vs. Translated Texts</a></p>
<p>17 0.035434537 <a title="133-tfidf-17" href="./emnlp-2011-Personalized_Recommendation_of_User_Comments_via_Factor_Models.html">104 emnlp-2011-Personalized Recommendation of User Comments via Factor Models</a></p>
<p>18 0.035234228 <a title="133-tfidf-18" href="./emnlp-2011-A_Fast_Re-scoring_Strategy_to_Capture_Long-Distance_Dependencies.html">5 emnlp-2011-A Fast Re-scoring Strategy to Capture Long-Distance Dependencies</a></p>
<p>19 0.034952719 <a title="133-tfidf-19" href="./emnlp-2011-Bayesian_Checking_for_Topic_Models.html">21 emnlp-2011-Bayesian Checking for Topic Models</a></p>
<p>20 0.034229115 <a title="133-tfidf-20" href="./emnlp-2011-Unsupervised_Structure_Prediction_with_Non-Parallel_Multilingual_Guidance.html">146 emnlp-2011-Unsupervised Structure Prediction with Non-Parallel Multilingual Guidance</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.12), (1, -0.141), (2, 0.189), (3, -0.034), (4, -0.087), (5, 0.02), (6, -0.043), (7, -0.037), (8, -0.039), (9, -0.024), (10, -0.024), (11, -0.041), (12, -0.036), (13, -0.005), (14, -0.055), (15, 0.121), (16, 0.12), (17, -0.08), (18, -0.114), (19, 0.068), (20, 0.13), (21, 0.05), (22, 0.058), (23, -0.049), (24, -0.139), (25, -0.078), (26, -0.149), (27, 0.03), (28, 0.183), (29, -0.064), (30, 0.128), (31, 0.07), (32, -0.005), (33, -0.047), (34, 0.015), (35, 0.118), (36, -0.155), (37, -0.142), (38, -0.009), (39, -0.053), (40, -0.241), (41, 0.009), (42, -0.163), (43, 0.015), (44, -0.035), (45, -0.048), (46, 0.026), (47, 0.098), (48, 0.031), (49, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93544459 <a title="133-lsi-1" href="./emnlp-2011-The_Imagination_of_Crowds%3A_Conversational_AAC_Language_Modeling_using_Crowdsourcing_and_Large_Data_Sources.html">133 emnlp-2011-The Imagination of Crowds: Conversational AAC Language Modeling using Crowdsourcing and Large Data Sources</a></p>
<p>Author: Keith Vertanen ; Per Ola Kristensson</p><p>Abstract: Augmented and alternative communication (AAC) devices enable users with certain communication disabilities to participate in everyday conversations. Such devices often rely on statistical language models to improve text entry by offering word predictions. These predictions can be improved if the language model is trained on data that closely reflects the style of the users’ intended communications. Unfortunately, there is no large dataset consisting of genuine AAC messages. In this paper we demonstrate how we can crowdsource the creation of a large set of fictional AAC messages. We show that these messages model conversational AAC better than the currently used datasets based on telephone conversations or newswire text. We leverage our crowdsourced messages to intelligently select sentences from much larger sets of Twitter, blog and Usenet data. Compared to a model trained only on telephone transcripts, our best performing model reduced perplexity on three test sets of AAC-like communications by 60– 82% relative. This translated to a potential keystroke savings in a predictive keyboard interface of 5–1 1%.</p><p>2 0.63411653 <a title="133-lsi-2" href="./emnlp-2011-Divide_and_Conquer%3A_Crowdsourcing_the_Creation_of_Cross-Lingual_Textual_Entailment_Corpora.html">42 emnlp-2011-Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora</a></p>
<p>Author: Matteo Negri ; Luisa Bentivogli ; Yashar Mehdad ; Danilo Giampiccolo ; Alessandro Marchetti</p><p>Abstract: We address the creation of cross-lingual textual entailment corpora by means of crowdsourcing. Our goal is to define a cheap and replicable data collection methodology that minimizes the manual work done by expert annotators, without resorting to preprocessing tools or already annotated monolingual datasets. In line with recent works emphasizing the need of large-scale annotation efforts for textual entailment, our work aims to: i) tackle the scarcity of data available to train and evaluate systems, and ii) promote the recourse to crowdsourcing as an effective way to reduce the costs of data collection without sacrificing quality. We show that a complex data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators. The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German.</p><p>3 0.58629948 <a title="133-lsi-3" href="./emnlp-2011-Active_Learning_with_Amazon_Mechanical_Turk.html">17 emnlp-2011-Active Learning with Amazon Mechanical Turk</a></p>
<p>Author: Florian Laws ; Christian Scheible ; Hinrich Schutze</p><p>Abstract: Supervised classification needs large amounts of annotated training data that is expensive to create. Two approaches that reduce the cost of annotation are active learning and crowdsourcing. However, these two approaches have not been combined successfully to date. We evaluate the utility of active learning in crowdsourcing on two tasks, named entity recognition and sentiment detection, and show that active learning outperforms random selection of annotation examples in a noisy crowdsourcing scenario.</p><p>4 0.54089844 <a title="133-lsi-4" href="./emnlp-2011-Discriminating_Gender_on_Twitter.html">41 emnlp-2011-Discriminating Gender on Twitter</a></p>
<p>Author: John D. Burger ; John Henderson ; George Kim ; Guido Zarrella</p><p>Abstract: Accurate prediction of demographic attributes from social media and other informal online content is valuable for marketing, personalization, and legal investigation. This paper describes the construction of a large, multilingual dataset labeled with gender, and investigates statistical models for determining the gender of uncharacterized Twitter users. We explore several different classifier types on this dataset. We show the degree to which classifier accuracy varies based on tweet volumes as well as when various kinds of profile metadata are included in the models. We also perform a large-scale human assessment using Amazon Mechanical Turk. Our methods significantly out-perform both baseline models and almost all humans on the same task.</p><p>5 0.29811278 <a title="133-lsi-5" href="./emnlp-2011-Language_Models_for_Machine_Translation%3A_Original_vs._Translated_Texts.html">76 emnlp-2011-Language Models for Machine Translation: Original vs. Translated Texts</a></p>
<p>Author: Gennadi Lembersky ; Noam Ordan ; Shuly Wintner</p><p>Abstract: We investigate the differences between language models compiled from original target-language texts and those compiled from texts manually translated to the target language. Corroborating established observations of Translation Studies, we demonstrate that the latter are significantly better predictors of translated sentences than the former, and hence fit the reference set better. Furthermore, translated texts yield better language models for statistical machine translation than original texts.</p><p>6 0.28746426 <a title="133-lsi-6" href="./emnlp-2011-Twitter_Catches_The_Flu%3A_Detecting_Influenza_Epidemics_using_Twitter.html">139 emnlp-2011-Twitter Catches The Flu: Detecting Influenza Epidemics using Twitter</a></p>
<p>7 0.28491127 <a title="133-lsi-7" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<p>8 0.28196254 <a title="133-lsi-8" href="./emnlp-2011-Data-Driven_Response_Generation_in_Social_Media.html">38 emnlp-2011-Data-Driven Response Generation in Social Media</a></p>
<p>9 0.27069971 <a title="133-lsi-9" href="./emnlp-2011-Rumor_has_it%3A_Identifying_Misinformation_in_Microblogs.html">117 emnlp-2011-Rumor has it: Identifying Misinformation in Microblogs</a></p>
<p>10 0.26533699 <a title="133-lsi-10" href="./emnlp-2011-Personalized_Recommendation_of_User_Comments_via_Factor_Models.html">104 emnlp-2011-Personalized Recommendation of User Comments via Factor Models</a></p>
<p>11 0.26150241 <a title="133-lsi-11" href="./emnlp-2011-Linguistic_Redundancy_in_Twitter.html">89 emnlp-2011-Linguistic Redundancy in Twitter</a></p>
<p>12 0.2189364 <a title="133-lsi-12" href="./emnlp-2011-Syntactic_Decision_Tree_LMs%3A_Random_Selection_or_Intelligent_Design%3F.html">131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</a></p>
<p>13 0.21830869 <a title="133-lsi-13" href="./emnlp-2011-Efficient_Subsampling_for_Training_Complex_Language_Models.html">46 emnlp-2011-Efficient Subsampling for Training Complex Language Models</a></p>
<p>14 0.21616729 <a title="133-lsi-14" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>15 0.19236816 <a title="133-lsi-15" href="./emnlp-2011-Named_Entity_Recognition_in_Tweets%3A_An_Experimental_Study.html">98 emnlp-2011-Named Entity Recognition in Tweets: An Experimental Study</a></p>
<p>16 0.18366215 <a title="133-lsi-16" href="./emnlp-2011-Cooooooooooooooollllllllllllll%21%21%21%21%21%21%21%21%21%21%21%21%21%21_Using_Word_Lengthening_to_Detect_Sentiment_in_Microblogs.html">33 emnlp-2011-Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using Word Lengthening to Detect Sentiment in Microblogs</a></p>
<p>17 0.17823571 <a title="133-lsi-17" href="./emnlp-2011-Bootstrapping_Semantic_Parsers_from_Conversations.html">24 emnlp-2011-Bootstrapping Semantic Parsers from Conversations</a></p>
<p>18 0.17130689 <a title="133-lsi-18" href="./emnlp-2011-A_Simple_Word_Trigger_Method_for_Social_Tag_Suggestion.html">11 emnlp-2011-A Simple Word Trigger Method for Social Tag Suggestion</a></p>
<p>19 0.15413378 <a title="133-lsi-19" href="./emnlp-2011-A_Cascaded_Classification_Approach_to_Semantic_Head_Recognition.html">2 emnlp-2011-A Cascaded Classification Approach to Semantic Head Recognition</a></p>
<p>20 0.15219304 <a title="133-lsi-20" href="./emnlp-2011-Probabilistic_models_of_similarity_in_syntactic_context.html">107 emnlp-2011-Probabilistic models of similarity in syntactic context</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.1), (32, 0.023), (36, 0.017), (37, 0.031), (45, 0.088), (49, 0.38), (53, 0.032), (54, 0.02), (57, 0.023), (62, 0.01), (64, 0.018), (66, 0.024), (79, 0.034), (82, 0.017), (85, 0.027), (87, 0.018), (96, 0.031), (98, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70532036 <a title="133-lda-1" href="./emnlp-2011-The_Imagination_of_Crowds%3A_Conversational_AAC_Language_Modeling_using_Crowdsourcing_and_Large_Data_Sources.html">133 emnlp-2011-The Imagination of Crowds: Conversational AAC Language Modeling using Crowdsourcing and Large Data Sources</a></p>
<p>Author: Keith Vertanen ; Per Ola Kristensson</p><p>Abstract: Augmented and alternative communication (AAC) devices enable users with certain communication disabilities to participate in everyday conversations. Such devices often rely on statistical language models to improve text entry by offering word predictions. These predictions can be improved if the language model is trained on data that closely reflects the style of the users’ intended communications. Unfortunately, there is no large dataset consisting of genuine AAC messages. In this paper we demonstrate how we can crowdsource the creation of a large set of fictional AAC messages. We show that these messages model conversational AAC better than the currently used datasets based on telephone conversations or newswire text. We leverage our crowdsourced messages to intelligently select sentences from much larger sets of Twitter, blog and Usenet data. Compared to a model trained only on telephone transcripts, our best performing model reduced perplexity on three test sets of AAC-like communications by 60– 82% relative. This translated to a potential keystroke savings in a predictive keyboard interface of 5–1 1%.</p><p>2 0.61969948 <a title="133-lda-2" href="./emnlp-2011-Simple_Effective_Decipherment_via_Combinatorial_Optimization.html">122 emnlp-2011-Simple Effective Decipherment via Combinatorial Optimization</a></p>
<p>Author: Taylor Berg-Kirkpatrick ; Dan Klein</p><p>Abstract: We present a simple objective function that when optimized yields accurate solutions to both decipherment and cognate pair identification problems. The objective simultaneously scores a matching between two alphabets and a matching between two lexicons, each in a different language. We introduce a simple coordinate descent procedure that efficiently finds effective solutions to the resulting combinatorial optimization problem. Our system requires only a list of words in both languages as input, yet it competes with and surpasses several state-of-the-art systems that are both substantially more complex and make use of more information.</p><p>3 0.36040708 <a title="133-lda-3" href="./emnlp-2011-Active_Learning_with_Amazon_Mechanical_Turk.html">17 emnlp-2011-Active Learning with Amazon Mechanical Turk</a></p>
<p>Author: Florian Laws ; Christian Scheible ; Hinrich Schutze</p><p>Abstract: Supervised classification needs large amounts of annotated training data that is expensive to create. Two approaches that reduce the cost of annotation are active learning and crowdsourcing. However, these two approaches have not been combined successfully to date. We evaluate the utility of active learning in crowdsourcing on two tasks, named entity recognition and sentiment detection, and show that active learning outperforms random selection of annotation examples in a noisy crowdsourcing scenario.</p><p>4 0.35923469 <a title="133-lda-4" href="./emnlp-2011-A_Word_Reordering_Model_for_Improved_Machine_Translation.html">13 emnlp-2011-A Word Reordering Model for Improved Machine Translation</a></p>
<p>Author: Karthik Visweswariah ; Rajakrishnan Rajkumar ; Ankur Gandhe ; Ananthakrishnan Ramanathan ; Jiri Navratil</p><p>Abstract: Preordering of source side sentences has proved to be useful in improving statistical machine translation. Most work has used a parser in the source language along with rules to map the source language word order into the target language word order. The requirement to have a source language parser is a major drawback, which we seek to overcome in this paper. Instead of using a parser and then using rules to order the source side sentence we learn a model that can directly reorder source side sentences to match target word order using a small parallel corpus with highquality word alignments. Our model learns pairwise costs of a word immediately preced- ing another word. We use the Lin-Kernighan heuristic to find the best source reordering efficiently during training and testing and show that it suffices to provide good quality reordering. We show gains in translation performance based on our reordering model for translating from Hindi to English, Urdu to English (with a public dataset), and English to Hindi. For English to Hindi we show that our technique achieves better performance than a method that uses rules applied to the source side English parse.</p><p>5 0.357389 <a title="133-lda-5" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>Author: Yang Gao ; Philipp Koehn ; Alexandra Birch</p><p>Abstract: Long-distance reordering remains one of the biggest challenges facing machine translation. We derive soft constraints from the source dependency parsing to directly address the reordering problem for the hierarchical phrasebased model. Our approach significantly improves Chinese–English machine translation on a large-scale task by 0.84 BLEU points on average. Moreover, when we switch the tuning function from BLEU to the LRscore which promotes reordering, we observe total improvements of 1.21 BLEU, 1.30 LRscore and 3.36 TER over the baseline. On average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially effective for long-distance reodering.</p><p>6 0.3554934 <a title="133-lda-6" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>7 0.35511249 <a title="133-lda-7" href="./emnlp-2011-Named_Entity_Recognition_in_Tweets%3A_An_Experimental_Study.html">98 emnlp-2011-Named Entity Recognition in Tweets: An Experimental Study</a></p>
<p>8 0.35433912 <a title="133-lda-8" href="./emnlp-2011-Semantic_Topic_Models%3A_Combining_Word_Distributional_Statistics_and_Dictionary_Definitions.html">119 emnlp-2011-Semantic Topic Models: Combining Word Distributional Statistics and Dictionary Definitions</a></p>
<p>9 0.35374036 <a title="133-lda-9" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>10 0.35342222 <a title="133-lda-10" href="./emnlp-2011-Cross-Cutting_Models_of_Lexical_Semantics.html">37 emnlp-2011-Cross-Cutting Models of Lexical Semantics</a></p>
<p>11 0.35331976 <a title="133-lda-11" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>12 0.35306698 <a title="133-lda-12" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>13 0.35295188 <a title="133-lda-13" href="./emnlp-2011-Efficient_Subsampling_for_Training_Complex_Language_Models.html">46 emnlp-2011-Efficient Subsampling for Training Complex Language Models</a></p>
<p>14 0.35283077 <a title="133-lda-14" href="./emnlp-2011-Cooooooooooooooollllllllllllll%21%21%21%21%21%21%21%21%21%21%21%21%21%21_Using_Word_Lengthening_to_Detect_Sentiment_in_Microblogs.html">33 emnlp-2011-Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using Word Lengthening to Detect Sentiment in Microblogs</a></p>
<p>15 0.35199976 <a title="133-lda-15" href="./emnlp-2011-Better_Evaluation_Metrics_Lead_to_Better_Machine_Translation.html">22 emnlp-2011-Better Evaluation Metrics Lead to Better Machine Translation</a></p>
<p>16 0.35194746 <a title="133-lda-16" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>17 0.35180235 <a title="133-lda-17" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<p>18 0.35143256 <a title="133-lda-18" href="./emnlp-2011-Exploiting_Syntactic_and_Distributional_Information_for_Spelling_Correction_with_Web-Scale_N-gram_Models.html">55 emnlp-2011-Exploiting Syntactic and Distributional Information for Spelling Correction with Web-Scale N-gram Models</a></p>
<p>19 0.3506003 <a title="133-lda-19" href="./emnlp-2011-Exploring_Supervised_LDA_Models_for_Assigning_Attributes_to_Adjective-Noun_Phrases.html">56 emnlp-2011-Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases</a></p>
<p>20 0.35025692 <a title="133-lda-20" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
