<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>147 emnlp-2011-Using Syntactic and Semantic Structural Kernels for Classifying Definition Questions in Jeopardy!</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-147" href="#">emnlp2011-147</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>147 emnlp-2011-Using Syntactic and Semantic Structural Kernels for Classifying Definition Questions in Jeopardy!</h1>
<br/><p>Source: <a title="emnlp-2011-147-pdf" href="http://aclweb.org/anthology//D/D11/D11-1066.pdf">pdf</a></p><p>Author: Alessandro Moschitti ; Jennifer Chu-carroll ; Siddharth Patwardhan ; James Fan ; Giuseppe Riccardi</p><p>Abstract: The last decade has seen many interesting applications of Question Answering (QA) technology. The Jeopardy! quiz show is certainly one of the most fascinating, from the viewpoints of both its broad domain and the complexity of its language. In this paper, we study kernel methods applied to syntactic/semantic structures for accurate classification of Jeopardy! definition questions. Our extensive empirical analysis shows that our classification models largely improve on classifiers based on word-language models. Such classifiers are also used in the state-of-the-art QA pipeline constituting Watson, the IBM Jeopardy! system. Our experiments measuring their impact on Watson show enhancements in QA accuracy and a consequent increase in the amount of money earned in game-based evaluation.</p><p>Reference: <a title="emnlp-2011-147-reference" href="../emnlp2011_reference/emnlp-2011-Using_Syntactic_and_Semantic_Structural_Kernels_for_Classifying_Definition_Questions_in_Jeopardy%21_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we study kernel methods applied to syntactic/semantic structures for accurate classification of Jeopardy! [sent-18, score-0.3]
</p><p>2 In each game, three contestants compete for the opportunity to answer 60 questions in 12 categories of 5 questions each. [sent-34, score-0.651]
</p><p>3 questions cover an incredibly broad domain, from science, literature, history, to popular culture. [sent-36, score-0.279]
</p><p>4 questions are factoid questions, we find several other types of questions in the Jeopardy! [sent-40, score-0.632]
</p><p>5 The additional processing in these questions complements that of the factoid questions to achieve improved overall QA performance. [sent-42, score-0.632]
</p><p>6 Several characteristics of this class of questions warrant special processing: first, the clue (question)  1A Jeopardy! [sent-45, score-0.382]
</p><p>7 Second, these clues often do not indicate an answer type, which is an important feature for identifying correct answers in factoid questions (in the examples above, only (3) provided an answer type, “fund”). [sent-50, score-0.652]
</p><p>8 Third, definition questions are typically shorter in length than the average factoid question. [sent-51, score-0.472]
</p><p>9 These differences, namely the shorter clue length and the lack of answer types, make the use of a specialized machine learning model potentially promising for improving the overall system accuracy. [sent-52, score-0.204]
</p><p>10 clues are not phrased in interrogative form as questions typically are. [sent-57, score-0.325]
</p><p>11 This complicates the design of definition classifiers since we cannot directly use either typical structural patterns that characterize definition/description questions, or previous approaches, e. [sent-58, score-0.204]
</p><p>12 Given the complexity and the novelty of the task, we found it useful to exploit the kernel methods technology. [sent-63, score-0.189]
</p><p>13 In this paper, we apply SVMs and kernel methods to syntactic/semantic structures for modeling accurate classification of Jeopardy! [sent-69, score-0.3]
</p><p>14 For this purpose, we use several levels of linguistic information: word and POS tag sequences,  dependency, constituency and predicate argument structures and we combined them using state-ofthe-art structural kernels, e. [sent-71, score-0.231]
</p><p>15 To demonstrate the benefit of our question classifier, we integrated it into our Watson by coupling it with search and candidate generation against specialized dictionary resources. [sent-82, score-0.237]
</p><p>16 We show that in endto-end evaluations, Watson with kernel-based definition classification and specialized definition question processing achieves statistically significant im-  provement compared to our baseline systems. [sent-83, score-0.455]
</p><p>17 In the reminder of this paper, Section 2 describes Watson by focusing on the problem of definition question classification, Section 3 describes our models for such classifiers, Section 4 presents our experiments on QC, whereas Section 5 shows the final impact on Watson. [sent-84, score-0.261]
</p><p>18 System  This section gives a quick overview of Watson and the problem of classification of definition questions, which is the focus of this paper. [sent-87, score-0.199]
</p><p>19 In the rest of this section, we provide an overview of Watson, focusing on the task of answering  definitional questions. [sent-92, score-0.252]
</p><p>20 Question Analysis: The first stage of the pipeline, it applies several analytic components to identify key characteristics of the question (such as answer  Figure 1: Overview of Watson  type, question classes, etc. [sent-93, score-0.377]
</p><p>21 For instance, a question detected as an abbreviation question can invoke specialized candidate generators to produce possible expansions of the abbreviated term in the clue. [sent-98, score-0.407]
</p><p>22 Similarly, the question classes can impact the methods  for answer scoring and the machine learning models used for ranking candidate answers. [sent-99, score-0.271]
</p><p>23 Hypothesis Generation: Following question analysis, the Watson pipeline searches its document collection for relevant documents and passages that are likely to contain the correct answer to the question. [sent-101, score-0.264]
</p><p>24 The question classes detected during question analysis can focus the search towards specific subsets of the corpus. [sent-105, score-0.309]
</p><p>25 Similarly, during candidate generation, strategies used to generate the set 714 of candidate answers are selected based on the detected question classes. [sent-106, score-0.306]
</p><p>26 Hypothesis and Evidence Scoring: A wide variety  of answer scorers are then used to gather evidence supporting each candidate answer as the correct answer to the given question. [sent-107, score-0.358]
</p><p>27 The system can also choose to refrain from answering a question if it has low confidence in all candidates. [sent-111, score-0.228]
</p><p>28 These models are trained using selected feature sets based on question classes and candidate answers are “routed” to the appropriate model according to the question classes detected during question analysis. [sent-113, score-0.554]
</p><p>29 2 Answering Definition Questions Among the many question classes that Watson identifies and leverages for special processing, of particular interest for this paper is the class we refer to  as definition questions. [sent-115, score-0.286]
</p><p>30 These are questions whose clue texts contain one or more definitions of the correct answer. [sent-116, score-0.357]
</p><p>31 For instance, in example (3), the main clause in the question corresponds to a dictionary definition of the correct answer (annuity). [sent-117, score-0.38]
</p><p>32 Looking up this definition in dictionary resources could enable us to answer this question correctly and with high confidence. [sent-118, score-0.38]
</p><p>33 This suggests that special processing of such definition questions could allow us to hone in on the correct answer through processes different from those used for other types of questions. [sent-119, score-0.516]
</p><p>34 This paper explores strategies for definition question processing to improve overall question answering performance. [sent-120, score-0.489]
</p><p>35 Given an input question the Watson question analysis stage uses a definition question recognizer to detect this specific class of questions. [sent-122, score-0.545]
</p><p>36 Questions that are recognized as definition questions invoke search processes targeted towards dictionary-like sources in our system. [sent-124, score-0.427]
</p><p>37 The following sections present a description and evaluation of our approach for identifying and answering definition questions. [sent-127, score-0.205]
</p><p>38 Given the novelty of both the domain and the type of our classification items, we rely on kernel methods to study and design effective representations. [sent-132, score-0.231]
</p><p>39 Our approach consists of using SVMs and kernels for structured data applied to several types of structural lexical, syntactic and shallow semantic information. [sent-134, score-0.271]
</p><p>40 questions in our case) in the very high dimensional space of substructures, where each of the latter is a component of the implicit vectors associated with the examples. [sent-138, score-0.279]
</p><p>41 in  PMOD  NedNSb  NN  electrons  NMODform DT  Figure 3: Dependency Tree this  PASS  P A0  AM-TMP  phosphor  hit  P  PR  A1  A1  give energy phosphor  AM-MNR  PR  electromag. [sent-140, score-0.19]
</p><p>42 The symbols we used in the sequential descriptions of questions are words and part-of-speech tags (in two separate sequences). [sent-145, score-0.279]
</p><p>43 When applied to sequences and tree structures, the kernels discussed above produce many different kinds of features. [sent-156, score-0.274]
</p><p>44 2 Syntactic Semantic Structures We applied the above kernels to different structures. [sent-160, score-0.21]
</p><p>45 4  Experiments on Definition Question Classification  In these experiments, we study the role of kernel technology for the design of accurate classification of definition questions. [sent-173, score-0.35]
</p><p>46 We build several classifiers based on SVMs and kernel methods. [sent-174, score-0.216]
</p><p>47 These questions were manually annotated based on whether or not they are considered definitional. [sent-181, score-0.279]
</p><p>48 we use the following sequences:  PS: [wrb][vbn][in][nns][,][dt][nn][vbz][rp][jj][nn][in]  [dt][nn]  Additionally, we use constituency trees y 2From here the name syntactic tree kernels  cate-  (Joachims, 1999)5. [sent-188, score-0.36]
</p><p>49 Kernel Models: we apply the kernels described in Section 3 to the structures extracted from Jeopardy! [sent-206, score-0.279]
</p><p>50 linear kernel on bag-of-words from the clues; WSK, PSK and CSK, i. [sent-210, score-0.189]
</p><p>51 a linear kernel applied to the vector only constituted by the 1/0 output of RBC. [sent-220, score-0.189]
</p><p>52 2 Results and Discussion Table 1 shows the performance obtained using dif-  ferent kernels (feature spaces) with SVMs. [sent-226, score-0.21]
</p><p>53 Next, PSK and PASS provide a lower accuracy but they may be useful in kernel combinations as they can complement the information captured by the other models. [sent-250, score-0.189]
</p><p>54 sense as  already  Figure 5: Similarity according to PTK and STK lieve this is because definition questions are sometimes clustered into categories such as 4-LETTER WORDS or BEGINS WITH ”B”. [sent-253, score-0.398]
</p><p>55 Finally, the above points suggest that different kernels produce complementary information. [sent-259, score-0.21]
</p><p>56 The joint models can be simply built by summing kernel functions together. [sent-261, score-0.189]
</p><p>57 Finally, more complex kernels, especially the overall kernel summation, do not seem to improve the per718  Kernel SpacePrec. [sent-272, score-0.189]
</p><p>58 1 Experimental Setup We integrated the classifier into the question analysis module, and incorporated additional components to search against dictionary resources and extract candidate answers from these search results when a question is classified as definitional. [sent-282, score-0.471]
</p><p>59 In the final machine learning models, a separate model is trained for definition questions to enable scoring tailored to the specific characteristics of those questions. [sent-283, score-0.398]
</p><p>60 The first is definition-only evaluation, in which we apply our definition question classifier to identify a large 9Since we aim to compare a purely statistical approach to the rule-based approach, we did not experiment with the model that uses RBC as a feature in our end-to-end experiments. [sent-287, score-0.286]
</p><p>61 set of definition questions and evaluate the end-toend system’s performance on this large set of questions. [sent-288, score-0.398]
</p><p>62 The second type of evaluation is game-based evaluation, which assesses the impact of our definition question processing on Watson performance while preserving the natural distribution of these question types in Jeopardy! [sent-290, score-0.403]
</p><p>63 Game-based evaluations situate the system’s performance on definition questions relative to other types of questions, and enable us to gauge the component’s contributions in a game-based setting. [sent-292, score-0.398]
</p><p>64 For the definition-only evaluation, we selected all questions recognized as definitional by the statistical classifier from roughly 1000 unseen games (60000 questions), resulting in a test set of 1606 questions. [sent-294, score-0.484]
</p><p>65 Instead, we compare the StatDef system against the NoDef on these 1606 questions using two metrics: accuracy, defined as the percentage of questions correctly answered,  and p@70, the system’s Precision when answering only the top 70% most confident questions. [sent-296, score-0.644]
</p><p>66 game play as well as in real world applications where the system may refrain from answering a question when it is not confident about any of its answers. [sent-298, score-0.255]
</p><p>67 85871 5% Table 3: Definition-Only Evaluation Results  with an initial set of roughly 300 games, from which the RBC identified 1875 questions as definitional. [sent-303, score-0.279]
</p><p>68 We compared the RuleDef system’s performance on these questions against the NoDef baseline using the accuracy and p@70 metrics. [sent-304, score-0.279]
</p><p>69 games, consisting of 3546 questions after excluding audio/visual questions. [sent-306, score-0.279]
</p><p>70 2 Definition-Only Evaluation For the definition-only evaluation, we compared the StatDef system against the NoDef system on a set of 1606 questions that the StatDef system classified as definitional. [sent-309, score-0.312]
</p><p>71 To contrast the gain obtained by the StatDef system against that achieved by the RuleDef system, we ran the RuleDef system over the 1875 questions identified as definitional by the rule-based classifier. [sent-311, score-0.407]
</p><p>72 Of these 10Audio/visual questions are those accompanied by either an image or an audio clip. [sent-318, score-0.279]
</p><p>73 The text portions of these questions are often insufficient for identifying the correct answers. [sent-319, score-0.279]
</p><p>74 317 9 % %$ 2 4 5, 3819 0789 Table 4: Game-Based Evaluation Results  questions, the StatDef system classified 13 1 of them as definitional while the RuleDef system identified 480 definition questions. [sent-322, score-0.28]
</p><p>75 The lowered performance is due to the fact that the Precision of the RBC is much lower than that of the statistical classifier, and the special definition processing applied to questions that are erroneously classified as definitional was harmful. [sent-325, score-0.584]
</p><p>76 questions are stated as affirmative sentences, which are different from the typical QA questions. [sent-332, score-0.279]
</p><p>77 We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al. [sent-338, score-0.472]
</p><p>78 In contrast, we have shown that STK-CT is not effective  for our domain, as it presents very innovative elements: questions in affirmative and highly variable format. [sent-357, score-0.279]
</p><p>79 Thus, we employed new methods such as PTK, dependency structures, multiple sequence kernels including category information and many combinations. [sent-358, score-0.239]
</p><p>80 However, ours is the first study on the use of several combinations of kernels applied to several structures on very complex data from the Jeopardy! [sent-374, score-0.279]
</p><p>81 7 Final Remarks and Conclusion In this paper we have experimented with advanced structural kernels applied to several kinds of syntactic/semantic linguistic structures for the classification of questions in a new application domain, i. [sent-376, score-0.629]
</p><p>82 questions prevents us from achieving generalization with typical syntactic patterns even if they are derived by powerful methods such as STK. [sent-388, score-0.34]
</p><p>83 In particular, STK has been considered as the best kernel for exploiting syntactic information in con-  stituency trees, e. [sent-390, score-0.25]
</p><p>84 We have also provided an explanation of such behavior by means of error analysis: in contrast with traditional question classification, which focuses on basic syntactic patterns (e. [sent-403, score-0.203]
</p><p>85 Figure 5 shows that PTK captures partial patterns that are important for more complex questions like those in Jeopardy! [sent-406, score-0.308]
</p><p>86 Next, our kernel models improve up to 20 absolute percent points over n-grams based approaches, reaching a significant accuracy of about 70%. [sent-419, score-0.189]
</p><p>87 We focus on kernel methods for definition question for two reasons. [sent-423, score-0.45]
</p><p>88 First, their recognition relies heavily on parse structures and is therefore more amenable to the approach proposed in the paper than the recognition of other question types. [sent-424, score-0.211]
</p><p>89 Second, definition is by far the most frequent special question type in Jeopardy! [sent-425, score-0.286]
</p><p>90 In future work, we plan to extend the current research by investigating models capable of exploiting predicate argument structures for question classification and answer reranking. [sent-432, score-0.425]
</p><p>91 The use of syntactic/semantic kernels is a promising research direction (Basili et al. [sent-433, score-0.21]
</p><p>92 In this perspective kernel learning is a very interesting research line, considering the complexity of representation and classification problems in which our ker-  nels operate. [sent-435, score-0.231]
</p><p>93 A hybrid convolution tree kernel for seman-  tic role labeling. [sent-508, score-0.305]
</p><p>94 A hybrid convolution tree kernel for semantic role labeling. [sent-513, score-0.305]
</p><p>95 Speeding up training with tree kernels for node relation labeling. [sent-595, score-0.274]
</p><p>96 Exploiting syntactic and shallow semantic kernels for question/answer classification. [sent-636, score-0.242]
</p><p>97 Efficient convolution kernels for dependency and constituent syntactic trees. [sent-644, score-0.323]
</p><p>98 Convolution kernels on  constituent, dependency and sequential structures for relation extraction. [sent-653, score-0.308]
</p><p>99 Convolution kernels on 723 constituent, dependency and sequential structures for relation extraction. [sent-658, score-0.308]
</p><p>100 Overview of the NTCIR-6 cross-lingual question answering (CLQA) task. [sent-667, score-0.228]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('jeopardy', 0.412), ('questions', 0.279), ('rbc', 0.271), ('moschitti', 0.238), ('watson', 0.232), ('kernels', 0.21), ('kernel', 0.189), ('nodef', 0.185), ('ptk', 0.178), ('ruledef', 0.171), ('statdef', 0.157), ('qa', 0.145), ('question', 0.142), ('definitional', 0.128), ('definition', 0.119), ('stk', 0.111), ('wsk', 0.1), ('alessandro', 0.096), ('answer', 0.093), ('qc', 0.093), ('trec', 0.093), ('csk', 0.086), ('answering', 0.086), ('clue', 0.078), ('factoid', 0.074), ('sasaki', 0.071), ('structures', 0.069), ('answers', 0.067), ('tree', 0.064), ('phosphor', 0.057), ('voorhees', 0.057), ('constituency', 0.054), ('zhang', 0.052), ('convolution', 0.052), ('games', 0.052), ('bloehdorn', 0.049), ('suzuki', 0.048), ('clues', 0.046), ('predicate', 0.045), ('annuity', 0.043), ('bilotti', 0.043), ('cts', 0.043), ('ferrucci', 0.043), ('quiz', 0.043), ('scorers', 0.043), ('yearly', 0.043), ('classification', 0.042), ('svms', 0.042), ('energy', 0.041), ('overview', 0.038), ('vbn', 0.037), ('giuseppe', 0.037), ('candidate', 0.036), ('pass', 0.035), ('hit', 0.035), ('kudo', 0.035), ('argument', 0.034), ('nombank', 0.033), ('che', 0.033), ('duffy', 0.033), ('nguyen', 0.033), ('nn', 0.033), ('classified', 0.033), ('proceedings', 0.033), ('specialized', 0.033), ('syntactic', 0.032), ('fan', 0.031), ('roberto', 0.03), ('patterns', 0.029), ('sigir', 0.029), ('pipeline', 0.029), ('fund', 0.029), ('basili', 0.029), ('structural', 0.029), ('dependency', 0.029), ('clef', 0.029), ('dts', 0.029), ('earned', 0.029), ('invest', 0.029), ('invoke', 0.029), ('kaisser', 0.029), ('millionaire', 0.029), ('psk', 0.029), ('stituency', 0.029), ('wrb', 0.029), ('meyers', 0.027), ('game', 0.027), ('bow', 0.027), ('classifiers', 0.027), ('propbank', 0.026), ('jian', 0.026), ('min', 0.026), ('dictionary', 0.026), ('ibm', 0.025), ('surdeanu', 0.025), ('bunescu', 0.025), ('dt', 0.025), ('classifier', 0.025), ('detected', 0.025), ('special', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000015 <a title="147-tfidf-1" href="./emnlp-2011-Using_Syntactic_and_Semantic_Structural_Kernels_for_Classifying_Definition_Questions_in_Jeopardy%21.html">147 emnlp-2011-Using Syntactic and Semantic Structural Kernels for Classifying Definition Questions in Jeopardy!</a></p>
<p>Author: Alessandro Moschitti ; Jennifer Chu-carroll ; Siddharth Patwardhan ; James Fan ; Giuseppe Riccardi</p><p>Abstract: The last decade has seen many interesting applications of Question Answering (QA) technology. The Jeopardy! quiz show is certainly one of the most fascinating, from the viewpoints of both its broad domain and the complexity of its language. In this paper, we study kernel methods applied to syntactic/semantic structures for accurate classification of Jeopardy! definition questions. Our extensive empirical analysis shows that our classification models largely improve on classifiers based on word-language models. Such classifiers are also used in the state-of-the-art QA pipeline constituting Watson, the IBM Jeopardy! system. Our experiments measuring their impact on Watson show enhancements in QA accuracy and a consequent increase in the amount of money earned in game-based evaluation.</p><p>2 0.37716702 <a title="147-tfidf-2" href="./emnlp-2011-Structured_Lexical_Similarity_via_Convolution_Kernels_on_Dependency_Trees.html">127 emnlp-2011-Structured Lexical Similarity via Convolution Kernels on Dependency Trees</a></p>
<p>Author: Danilo Croce ; Alessandro Moschitti ; Roberto Basili</p><p>Abstract: Alessandro Moschitti DISI University of Trento 38123 Povo (TN), Italy mo s chitt i di s i @ .unit n . it Roberto Basili DII University of Tor Vergata 00133 Roma, Italy bas i i info .uni roma2 . it l@ over semantic networks, e.g. (Cowie et al., 1992; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, A central topic in natural language processing is the design of lexical and syntactic fea- tures suitable for the target application. In this paper, we study convolution dependency tree kernels for automatic engineering of syntactic and semantic patterns exploiting lexical similarities. We define efficient and powerful kernels for measuring the similarity between dependency structures, whose surface forms of the lexical nodes are in part or completely different. The experiments with such kernels for question classification show an unprecedented results, e.g. 41% of error reduction of the former state-of-the-art. Additionally, semantic role classification confirms the benefit of semantic smoothing for dependency kernels.</p><p>3 0.11952379 <a title="147-tfidf-3" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>Author: Marco Dinarelli ; Sophie Rosset</p><p>Abstract: Reranking models have been successfully applied to many tasks of Natural Language Processing. However, there are two aspects of this approach that need a deeper investigation: (i) Assessment of hypotheses generated for reranking at classification phase: baseline models generate a list of hypotheses and these are used for reranking without any assessment; (ii) Detection of cases where reranking models provide a worst result: the best hypothesis provided by the reranking model is assumed to be always the best result. In some cases the reranking model provides an incorrect hypothesis while the baseline best hypothesis is correct, especially when baseline models are accurate. In this paper we propose solutions for these two aspects: (i) a semantic inconsistency metric to select possibly more correct n-best hypotheses, from a large set generated by an SLU basiline model. The selected hypotheses are reranked applying a state-of-the-art model based on Partial Tree Kernels, which encode SLU hypotheses in Support Vector Machines with complex structured features; (ii) finally, we apply a decision strategy, based on confidence values, to select the final hypothesis between the first ranked hypothesis provided by the baseline SLU model and the first ranked hypothesis provided by the re-ranker. We show the effectiveness of these solutions presenting comparative results obtained reranking hypotheses generated by a very accurate Conditional Random Field model. We evaluate our approach on the French MEDIA corpus. The results show significant improvements with respect to current state-of-the-art and previous 1104 Sophie Rosset LIMSI-CNRS B.P. 133, 91403 Orsay Cedex France ro s set @ l ims i fr . re-ranking models.</p><p>4 0.1075611 <a title="147-tfidf-4" href="./emnlp-2011-Extreme_Extraction_-_Machine_Reading_in_a_Week.html">57 emnlp-2011-Extreme Extraction - Machine Reading in a Week</a></p>
<p>Author: Marjorie Freedman ; Lance Ramshaw ; Elizabeth Boschee ; Ryan Gabbard ; Gary Kratkiewicz ; Nicolas Ward ; Ralph Weischedel</p><p>Abstract: We report on empirical results in extreme extraction. It is extreme in that (1) from receipt of the ontology specifying the target concepts and relations, development is limited to one week and that (2) relatively little training data is assumed. We are able to surpass human recall and achieve an F1 of 0.5 1 on a question-answering task with less than 50 hours of effort using a hybrid approach that mixes active learning, bootstrapping, and limited (5 hours) manual rule writing. We compare the performance of three systems: extraction with handwritten rules, bootstrapped extraction, and a combination. We show that while the recall of the handwritten rules surpasses that of the learned system, the learned system is able to improve the overall recall and F1.</p><p>5 0.068804413 <a title="147-tfidf-5" href="./emnlp-2011-Relation_Extraction_with_Relation_Topics.html">114 emnlp-2011-Relation Extraction with Relation Topics</a></p>
<p>Author: Chang Wang ; James Fan ; Aditya Kalyanpur ; David Gondek</p><p>Abstract: This paper describes a novel approach to the semantic relation detection problem. Instead of relying only on the training instances for a new relation, we leverage the knowledge learned from previously trained relation detectors. Specifically, we detect a new semantic relation by projecting the new relation’s training instances onto a lower dimension topic space constructed from existing relation detectors through a three step process. First, we construct a large relation repository of more than 7,000 relations from Wikipedia. Second, we construct a set of non-redundant relation topics defined at multiple scales from the relation repository to characterize the existing relations. Similar to the topics defined over words, each relation topic is an interpretable multinomial distribution over the existing relations. Third, we integrate the relation topics in a kernel function, and use it together with SVM to construct detectors for new relations. The experimental results on Wikipedia and ACE data have confirmed that backgroundknowledge-based topics generated from the Wikipedia relation repository can significantly improve the performance over the state-of-theart relation detection approaches.</p><p>6 0.065606371 <a title="147-tfidf-6" href="./emnlp-2011-Learning_the_Information_Status_of_Noun_Phrases_in_Spoken_Dialogues.html">84 emnlp-2011-Learning the Information Status of Noun Phrases in Spoken Dialogues</a></p>
<p>7 0.060993571 <a title="147-tfidf-7" href="./emnlp-2011-A_Joint_Model_for_Extended_Semantic_Role_Labeling.html">7 emnlp-2011-A Joint Model for Extended Semantic Role Labeling</a></p>
<p>8 0.060500249 <a title="147-tfidf-8" href="./emnlp-2011-A_Fast%2C_Accurate%2C_Non-Projective%2C_Semantically-Enriched_Parser.html">4 emnlp-2011-A Fast, Accurate, Non-Projective, Semantically-Enriched Parser</a></p>
<p>9 0.057275571 <a title="147-tfidf-9" href="./emnlp-2011-Relation_Acquisition_using_Word_Classes_and_Partial_Patterns.html">113 emnlp-2011-Relation Acquisition using Word Classes and Partial Patterns</a></p>
<p>10 0.055505626 <a title="147-tfidf-10" href="./emnlp-2011-Syntactic_Decision_Tree_LMs%3A_Random_Selection_or_Intelligent_Design%3F.html">131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</a></p>
<p>11 0.055166941 <a title="147-tfidf-11" href="./emnlp-2011-Joint_Models_for_Chinese_POS_Tagging_and_Dependency_Parsing.html">75 emnlp-2011-Joint Models for Chinese POS Tagging and Dependency Parsing</a></p>
<p>12 0.054225564 <a title="147-tfidf-12" href="./emnlp-2011-Linguistic_Redundancy_in_Twitter.html">89 emnlp-2011-Linguistic Redundancy in Twitter</a></p>
<p>13 0.054064874 <a title="147-tfidf-13" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>14 0.051357135 <a title="147-tfidf-14" href="./emnlp-2011-Timeline_Generation_through_Evolutionary_Trans-Temporal_Summarization.html">135 emnlp-2011-Timeline Generation through Evolutionary Trans-Temporal Summarization</a></p>
<p>15 0.05069793 <a title="147-tfidf-15" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>16 0.050523948 <a title="147-tfidf-16" href="./emnlp-2011-Unsupervised_Semantic_Role_Induction_with_Graph_Partitioning.html">145 emnlp-2011-Unsupervised Semantic Role Induction with Graph Partitioning</a></p>
<p>17 0.05030388 <a title="147-tfidf-17" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>18 0.049722213 <a title="147-tfidf-18" href="./emnlp-2011-A_novel_dependency-to-string_model_for_statistical_machine_translation.html">15 emnlp-2011-A novel dependency-to-string model for statistical machine translation</a></p>
<p>19 0.049306259 <a title="147-tfidf-19" href="./emnlp-2011-Augmenting_String-to-Tree_Translation_Models_with_Fuzzy_Use_of_Source-side_Syntax.html">20 emnlp-2011-Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax</a></p>
<p>20 0.048714761 <a title="147-tfidf-20" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.192), (1, -0.041), (2, -0.082), (3, 0.069), (4, -0.052), (5, -0.101), (6, -0.036), (7, -0.006), (8, 0.305), (9, -0.002), (10, 0.078), (11, -0.101), (12, -0.049), (13, 0.332), (14, -0.224), (15, -0.039), (16, -0.226), (17, 0.009), (18, -0.277), (19, -0.018), (20, -0.155), (21, -0.093), (22, -0.073), (23, 0.053), (24, 0.006), (25, 0.021), (26, -0.169), (27, -0.241), (28, 0.035), (29, -0.029), (30, -0.087), (31, -0.071), (32, 0.147), (33, -0.036), (34, 0.06), (35, 0.017), (36, 0.013), (37, -0.012), (38, 0.073), (39, 0.073), (40, -0.025), (41, 0.043), (42, 0.03), (43, -0.006), (44, -0.043), (45, 0.034), (46, -0.032), (47, 0.019), (48, 0.002), (49, -0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95131093 <a title="147-lsi-1" href="./emnlp-2011-Using_Syntactic_and_Semantic_Structural_Kernels_for_Classifying_Definition_Questions_in_Jeopardy%21.html">147 emnlp-2011-Using Syntactic and Semantic Structural Kernels for Classifying Definition Questions in Jeopardy!</a></p>
<p>Author: Alessandro Moschitti ; Jennifer Chu-carroll ; Siddharth Patwardhan ; James Fan ; Giuseppe Riccardi</p><p>Abstract: The last decade has seen many interesting applications of Question Answering (QA) technology. The Jeopardy! quiz show is certainly one of the most fascinating, from the viewpoints of both its broad domain and the complexity of its language. In this paper, we study kernel methods applied to syntactic/semantic structures for accurate classification of Jeopardy! definition questions. Our extensive empirical analysis shows that our classification models largely improve on classifiers based on word-language models. Such classifiers are also used in the state-of-the-art QA pipeline constituting Watson, the IBM Jeopardy! system. Our experiments measuring their impact on Watson show enhancements in QA accuracy and a consequent increase in the amount of money earned in game-based evaluation.</p><p>2 0.88417053 <a title="147-lsi-2" href="./emnlp-2011-Structured_Lexical_Similarity_via_Convolution_Kernels_on_Dependency_Trees.html">127 emnlp-2011-Structured Lexical Similarity via Convolution Kernels on Dependency Trees</a></p>
<p>Author: Danilo Croce ; Alessandro Moschitti ; Roberto Basili</p><p>Abstract: Alessandro Moschitti DISI University of Trento 38123 Povo (TN), Italy mo s chitt i di s i @ .unit n . it Roberto Basili DII University of Tor Vergata 00133 Roma, Italy bas i i info .uni roma2 . it l@ over semantic networks, e.g. (Cowie et al., 1992; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, A central topic in natural language processing is the design of lexical and syntactic fea- tures suitable for the target application. In this paper, we study convolution dependency tree kernels for automatic engineering of syntactic and semantic patterns exploiting lexical similarities. We define efficient and powerful kernels for measuring the similarity between dependency structures, whose surface forms of the lexical nodes are in part or completely different. The experiments with such kernels for question classification show an unprecedented results, e.g. 41% of error reduction of the former state-of-the-art. Additionally, semantic role classification confirms the benefit of semantic smoothing for dependency kernels.</p><p>3 0.34044981 <a title="147-lsi-3" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>Author: Marco Dinarelli ; Sophie Rosset</p><p>Abstract: Reranking models have been successfully applied to many tasks of Natural Language Processing. However, there are two aspects of this approach that need a deeper investigation: (i) Assessment of hypotheses generated for reranking at classification phase: baseline models generate a list of hypotheses and these are used for reranking without any assessment; (ii) Detection of cases where reranking models provide a worst result: the best hypothesis provided by the reranking model is assumed to be always the best result. In some cases the reranking model provides an incorrect hypothesis while the baseline best hypothesis is correct, especially when baseline models are accurate. In this paper we propose solutions for these two aspects: (i) a semantic inconsistency metric to select possibly more correct n-best hypotheses, from a large set generated by an SLU basiline model. The selected hypotheses are reranked applying a state-of-the-art model based on Partial Tree Kernels, which encode SLU hypotheses in Support Vector Machines with complex structured features; (ii) finally, we apply a decision strategy, based on confidence values, to select the final hypothesis between the first ranked hypothesis provided by the baseline SLU model and the first ranked hypothesis provided by the re-ranker. We show the effectiveness of these solutions presenting comparative results obtained reranking hypotheses generated by a very accurate Conditional Random Field model. We evaluate our approach on the French MEDIA corpus. The results show significant improvements with respect to current state-of-the-art and previous 1104 Sophie Rosset LIMSI-CNRS B.P. 133, 91403 Orsay Cedex France ro s set @ l ims i fr . re-ranking models.</p><p>4 0.2838923 <a title="147-lsi-4" href="./emnlp-2011-Extreme_Extraction_-_Machine_Reading_in_a_Week.html">57 emnlp-2011-Extreme Extraction - Machine Reading in a Week</a></p>
<p>Author: Marjorie Freedman ; Lance Ramshaw ; Elizabeth Boschee ; Ryan Gabbard ; Gary Kratkiewicz ; Nicolas Ward ; Ralph Weischedel</p><p>Abstract: We report on empirical results in extreme extraction. It is extreme in that (1) from receipt of the ontology specifying the target concepts and relations, development is limited to one week and that (2) relatively little training data is assumed. We are able to surpass human recall and achieve an F1 of 0.5 1 on a question-answering task with less than 50 hours of effort using a hybrid approach that mixes active learning, bootstrapping, and limited (5 hours) manual rule writing. We compare the performance of three systems: extraction with handwritten rules, bootstrapped extraction, and a combination. We show that while the recall of the handwritten rules surpasses that of the learned system, the learned system is able to improve the overall recall and F1.</p><p>5 0.27549943 <a title="147-lsi-5" href="./emnlp-2011-Learning_the_Information_Status_of_Noun_Phrases_in_Spoken_Dialogues.html">84 emnlp-2011-Learning the Information Status of Noun Phrases in Spoken Dialogues</a></p>
<p>Author: Altaf Rahman ; Vincent Ng</p><p>Abstract: An entity in a dialogue may be old, new, or mediated/inferrable with respect to the hearer’s beliefs. Knowing the information status of the entities participating in a dialogue can therefore facilitate its interpretation. We address the under-investigated problem of automatically determining the information status of discourse entities. Specifically, we extend Nissim’s (2006) machine learning approach to information-status determination with lexical and structured features, and exploit learned knowledge of the information status of each discourse entity for coreference resolution. Experimental results on a set of Switchboard dialogues reveal that (1) incorporating our proposed features into Nissim’s feature set enables our system to achieve stateof-the-art performance on information-status classification, and (2) the resulting information can be used to improve the performance of learning-based coreference resolvers.</p><p>6 0.23668644 <a title="147-lsi-6" href="./emnlp-2011-Linguistic_Redundancy_in_Twitter.html">89 emnlp-2011-Linguistic Redundancy in Twitter</a></p>
<p>7 0.2248628 <a title="147-lsi-7" href="./emnlp-2011-Accurate_Parsing_with_Compact_Tree-Substitution_Grammars%3A_Double-DOP.html">16 emnlp-2011-Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP</a></p>
<p>8 0.22456996 <a title="147-lsi-8" href="./emnlp-2011-Syntactic_Decision_Tree_LMs%3A_Random_Selection_or_Intelligent_Design%3F.html">131 emnlp-2011-Syntactic Decision Tree LMs: Random Selection or Intelligent Design?</a></p>
<p>9 0.20454386 <a title="147-lsi-9" href="./emnlp-2011-Relation_Extraction_with_Relation_Topics.html">114 emnlp-2011-Relation Extraction with Relation Topics</a></p>
<p>10 0.19997758 <a title="147-lsi-10" href="./emnlp-2011-Relation_Acquisition_using_Word_Classes_and_Partial_Patterns.html">113 emnlp-2011-Relation Acquisition using Word Classes and Partial Patterns</a></p>
<p>11 0.19370183 <a title="147-lsi-11" href="./emnlp-2011-Efficient_retrieval_of_tree_translation_examples_for_Syntax-Based_Machine_Translation.html">47 emnlp-2011-Efficient retrieval of tree translation examples for Syntax-Based Machine Translation</a></p>
<p>12 0.18736689 <a title="147-lsi-12" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>13 0.18540977 <a title="147-lsi-13" href="./emnlp-2011-Timeline_Generation_through_Evolutionary_Trans-Temporal_Summarization.html">135 emnlp-2011-Timeline Generation through Evolutionary Trans-Temporal Summarization</a></p>
<p>14 0.16534542 <a title="147-lsi-14" href="./emnlp-2011-Predicting_Thread_Discourse_Structure_over_Technical_Web_Forums.html">105 emnlp-2011-Predicting Thread Discourse Structure over Technical Web Forums</a></p>
<p>15 0.15917338 <a title="147-lsi-15" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>16 0.15860675 <a title="147-lsi-16" href="./emnlp-2011-Harnessing_WordNet_Senses_for_Supervised_Sentiment_Classification.html">63 emnlp-2011-Harnessing WordNet Senses for Supervised Sentiment Classification</a></p>
<p>17 0.15785623 <a title="147-lsi-17" href="./emnlp-2011-Parse_Correction_with_Specialized_Models_for_Difficult_Attachment_Types.html">102 emnlp-2011-Parse Correction with Specialized Models for Difficult Attachment Types</a></p>
<p>18 0.15735117 <a title="147-lsi-18" href="./emnlp-2011-A_Weakly-supervised_Approach_to_Argumentative_Zoning_of_Scientific_Documents.html">12 emnlp-2011-A Weakly-supervised Approach to Argumentative Zoning of Scientific Documents</a></p>
<p>19 0.15686923 <a title="147-lsi-19" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>20 0.1568269 <a title="147-lsi-20" href="./emnlp-2011-A_Joint_Model_for_Extended_Semantic_Role_Labeling.html">7 emnlp-2011-A Joint Model for Extended Semantic Role Labeling</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.077), (23, 0.103), (27, 0.296), (36, 0.031), (37, 0.026), (45, 0.054), (53, 0.014), (54, 0.029), (57, 0.027), (62, 0.02), (64, 0.013), (66, 0.031), (69, 0.013), (79, 0.048), (82, 0.01), (87, 0.015), (90, 0.021), (96, 0.067), (98, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.78300744 <a title="147-lda-1" href="./emnlp-2011-Unsupervised_Learning_of_Selectional_Restrictions_and_Detection_of_Argument_Coercions.html">144 emnlp-2011-Unsupervised Learning of Selectional Restrictions and Detection of Argument Coercions</a></p>
<p>Author: Kirk Roberts ; Sanda Harabagiu</p><p>Abstract: Metonymic language is a pervasive phenomenon. Metonymic type shifting, or argument type coercion, results in a selectional restriction violation where the argument’s semantic class differs from the class the predicate expects. In this paper we present an unsupervised method that learns the selectional restriction of arguments and enables the detection of argument coercion. This method also generates an enhanced probabilistic resolution of logical metonymies. The experimental results indicate substantial improvements the detection of coercions and the ranking of metonymic interpretations.</p><p>same-paper 2 0.76539248 <a title="147-lda-2" href="./emnlp-2011-Using_Syntactic_and_Semantic_Structural_Kernels_for_Classifying_Definition_Questions_in_Jeopardy%21.html">147 emnlp-2011-Using Syntactic and Semantic Structural Kernels for Classifying Definition Questions in Jeopardy!</a></p>
<p>Author: Alessandro Moschitti ; Jennifer Chu-carroll ; Siddharth Patwardhan ; James Fan ; Giuseppe Riccardi</p><p>Abstract: The last decade has seen many interesting applications of Question Answering (QA) technology. The Jeopardy! quiz show is certainly one of the most fascinating, from the viewpoints of both its broad domain and the complexity of its language. In this paper, we study kernel methods applied to syntactic/semantic structures for accurate classification of Jeopardy! definition questions. Our extensive empirical analysis shows that our classification models largely improve on classifiers based on word-language models. Such classifiers are also used in the state-of-the-art QA pipeline constituting Watson, the IBM Jeopardy! system. Our experiments measuring their impact on Watson show enhancements in QA accuracy and a consequent increase in the amount of money earned in game-based evaluation.</p><p>3 0.72378021 <a title="147-lda-3" href="./emnlp-2011-Unsupervised_Information_Extraction_with_Distributional_Prior_Knowledge.html">143 emnlp-2011-Unsupervised Information Extraction with Distributional Prior Knowledge</a></p>
<p>Author: Cane Wing-ki Leung ; Jing Jiang ; Kian Ming A. Chai ; Hai Leong Chieu ; Loo-Nin Teow</p><p>Abstract: We address the task of automatic discovery of information extraction template from a given text collection. Our approach clusters candidate slot fillers to identify meaningful template slots. We propose a generative model that incorporates distributional prior knowledge to help distribute candidates in a document into appropriate slots. Empirical results suggest that the proposed prior can bring substantial improvements to our task as compared to a K-means baseline and a Gaussian mixture model baseline. Specifically, the proposed prior has shown to be effective when coupled with discriminative features of the candidates.</p><p>4 0.51084709 <a title="147-lda-4" href="./emnlp-2011-Structured_Lexical_Similarity_via_Convolution_Kernels_on_Dependency_Trees.html">127 emnlp-2011-Structured Lexical Similarity via Convolution Kernels on Dependency Trees</a></p>
<p>Author: Danilo Croce ; Alessandro Moschitti ; Roberto Basili</p><p>Abstract: Alessandro Moschitti DISI University of Trento 38123 Povo (TN), Italy mo s chitt i di s i @ .unit n . it Roberto Basili DII University of Tor Vergata 00133 Roma, Italy bas i i info .uni roma2 . it l@ over semantic networks, e.g. (Cowie et al., 1992; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, A central topic in natural language processing is the design of lexical and syntactic fea- tures suitable for the target application. In this paper, we study convolution dependency tree kernels for automatic engineering of syntactic and semantic patterns exploiting lexical similarities. We define efficient and powerful kernels for measuring the similarity between dependency structures, whose surface forms of the lexical nodes are in part or completely different. The experiments with such kernels for question classification show an unprecedented results, e.g. 41% of error reduction of the former state-of-the-art. Additionally, semantic role classification confirms the benefit of semantic smoothing for dependency kernels.</p><p>5 0.48306265 <a title="147-lda-5" href="./emnlp-2011-Minimally_Supervised_Event_Causality_Identification.html">92 emnlp-2011-Minimally Supervised Event Causality Identification</a></p>
<p>Author: Quang Do ; Yee Seng Chan ; Dan Roth</p><p>Abstract: This paper develops a minimally supervised approach, based on focused distributional similarity methods and discourse connectives, for identifying of causality relations between events in context. While it has been shown that distributional similarity can help identifying causality, we observe that discourse connectives and the particular discourse relation they evoke in context provide additional information towards determining causality between events. We show that combining discourse relation predictions and distributional similarity methods in a global inference procedure provides additional improvements towards determining event causality.</p><p>6 0.44353339 <a title="147-lda-6" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>7 0.43666914 <a title="147-lda-7" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>8 0.43522254 <a title="147-lda-8" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>9 0.43115869 <a title="147-lda-9" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>10 0.43064222 <a title="147-lda-10" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>11 0.43038467 <a title="147-lda-11" href="./emnlp-2011-Syntax-Based_Grammaticality_Improvement_using_CCG_and_Guided_Search.html">132 emnlp-2011-Syntax-Based Grammaticality Improvement using CCG and Guided Search</a></p>
<p>12 0.42933756 <a title="147-lda-12" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>13 0.42902836 <a title="147-lda-13" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>14 0.42792431 <a title="147-lda-14" href="./emnlp-2011-Lexical_Generalization_in_CCG_Grammar_Induction_for_Semantic_Parsing.html">87 emnlp-2011-Lexical Generalization in CCG Grammar Induction for Semantic Parsing</a></p>
<p>15 0.426873 <a title="147-lda-15" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<p>16 0.42598158 <a title="147-lda-16" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>17 0.42507574 <a title="147-lda-17" href="./emnlp-2011-Soft_Dependency_Constraints_for_Reordering_in_Hierarchical_Phrase-Based_Translation.html">123 emnlp-2011-Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation</a></p>
<p>18 0.42469481 <a title="147-lda-18" href="./emnlp-2011-Correcting_Semantic_Collocation_Errors_with_L1-induced_Paraphrases.html">35 emnlp-2011-Correcting Semantic Collocation Errors with L1-induced Paraphrases</a></p>
<p>19 0.42419457 <a title="147-lda-19" href="./emnlp-2011-Reducing_Grounded_Learning_Tasks_To_Grammatical_Inference.html">111 emnlp-2011-Reducing Grounded Learning Tasks To Grammatical Inference</a></p>
<p>20 0.42417845 <a title="147-lda-20" href="./emnlp-2011-Named_Entity_Recognition_in_Tweets%3A_An_Experimental_Study.html">98 emnlp-2011-Named Entity Recognition in Tweets: An Experimental Study</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
