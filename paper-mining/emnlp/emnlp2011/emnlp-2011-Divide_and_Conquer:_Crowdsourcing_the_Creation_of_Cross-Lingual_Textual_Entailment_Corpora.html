<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>42 emnlp-2011-Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-42" href="#">emnlp2011-42</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>42 emnlp-2011-Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora</h1>
<br/><p>Source: <a title="emnlp-2011-42-pdf" href="http://aclweb.org/anthology//D/D11/D11-1062.pdf">pdf</a></p><p>Author: Matteo Negri ; Luisa Bentivogli ; Yashar Mehdad ; Danilo Giampiccolo ; Alessandro Marchetti</p><p>Abstract: We address the creation of cross-lingual textual entailment corpora by means of crowdsourcing. Our goal is to define a cheap and replicable data collection methodology that minimizes the manual work done by expert annotators, without resorting to preprocessing tools or already annotated monolingual datasets. In line with recent works emphasizing the need of large-scale annotation efforts for textual entailment, our work aims to: i) tackle the scarcity of data available to train and evaluate systems, and ii) promote the recourse to crowdsourcing as an effective way to reduce the costs of data collection without sacrificing quality. We show that a complex data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators. The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German.</p><p>Reference: <a title="emnlp-2011-42-reference" href="../emnlp2011_reference/emnlp-2011-Divide_and_Conquer%3A_Crowdsourcing_the_Creation_of_Cross-Lingual_Textual_Entailment_Corpora_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora Matteo Negri FBK-irst Trento, Italy negri @ fbk . [sent-1, score-0.181]
</p><p>2 it  Abstract We address the creation of cross-lingual textual entailment corpora by means of crowdsourcing. [sent-5, score-0.765]
</p><p>3 Our goal is to define a cheap and replicable data collection methodology that minimizes the manual work done by expert annotators, without resorting to preprocessing tools or already annotated monolingual datasets. [sent-6, score-0.276]
</p><p>4 We show that a complex  data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators. [sent-8, score-0.313]
</p><p>5 The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German. [sent-9, score-0.277]
</p><p>6 In the last few years, monolingual TE corpora for English and other European languages have been created and distributed in the framework of several evaluation campaigns, including the RTE Challenge1, the Answer Validation Exercise at CLEF2, and the Textual Entailment task at EVALITA3. [sent-17, score-0.141]
</p><p>7 Moreover, in the data creation process, large amounts of hand-crafted T-H pairs often have to be discarded in order to retain only those featuring full agreement, in terms of the assigned entailmentjudgements, among multiple annotators. [sent-19, score-0.37]
</p><p>8 The amount of discarded pairs is usually high, contributing to increase the costs of creating  textual entailment datasets4. [sent-20, score-0.826]
</p><p>9 The issues related to the shortage of datasets and the high costs for their creation are more evident 1http://www. [sent-21, score-0.217]
</p><p>10 it/2009/tasks/te 4For instance, in the first five RTE Challenges, the average effort needed to create 1,000 pairs featuring full agreement among 3 annotators was around 2. [sent-27, score-0.239]
</p><p>11 Typically, around 25% of the original pairs had to be discarded during the process, due to low inter-annotator agreement (Bentivogli et al. [sent-29, score-0.168]
</p><p>12 To address these issues, in this paper we devise a cost-effective methodology to create cross-lingual textual entailment corpora. [sent-34, score-0.667]
</p><p>13 In particular, we focus on the following problems: (1) Is it possible to collect T-H pairs minimizing  the intervention of expert annotators? [sent-35, score-0.144]
</p><p>14 To address this question, we explore the feasibility of crowdsourcing the corpus creation process. [sent-36, score-0.311]
</p><p>15 Complex tasks are usually hard to explain in a simple way understandable to non-experts, difficult to accomplish, and not suitable for the application of the quality-check mechanisms provided by current crowdsourcing services. [sent-42, score-0.223]
</p><p>16 Our “divide and conquer” solution represents the first attempt to address a complex task involving content generation and labelling through the definition of a cheap and reliable pipeline of simple tasks which are easy to define, accomplish, and control. [sent-43, score-0.197]
</p><p>17 We tackle this question by separating the problem of creating and annotating TE pairs from the issues related to the multilingual dimension. [sent-45, score-0.172]
</p><p>18 Our solution builds on the assumption that entailment annotations can be projected across aligned T-H pairs in different languages. [sent-46, score-0.637]
</p><p>19 In this case, a complex multilingual task is reduced to a sequence of simpler subtasks where the most difficult one, the generation of entailment pairs, is entirely monolingual. [sent-47, score-0.668]
</p><p>20 Moreover, since the core monolingual tasks of the process are carried out by manipulating English texts, we are able to address the very large community of English speaking workers, with a considerable reduction of costs and execution time. [sent-49, score-0.209]
</p><p>21 We believe that, in the same spirit of recent works promoting large-scale annotation efforts around entailment corpora (Sammons et al. [sent-51, score-0.594]
</p><p>22 As regards textual entailment, the first work exploring the use of crowdsourcing services for data annotation is described in (Snow et al. [sent-56, score-0.317]
</p><p>23 , 2008), which shows high agreement between non-expert annotations ofthe RTE-1 dataset and existing gold standard  labels assigned by expert labellers. [sent-57, score-0.141]
</p><p>24 Focusing on the actual generation of monolingual entailment pairs, (Wang and Callison-Burch, 2010) experiments the use of MTurk to collect facts and counter facts related to texts extracted from an existing RTE corpus annotated with named entities. [sent-58, score-0.753]
</p><p>25 First, they still use available RTE data to obtain a monolingual TE corpus, whereas we pursue the more ambitious goal of generating from scratch aligned CLTE corpora for different language combinations. [sent-68, score-0.174]
</p><p>26 In contrast, our approach integrates quality control mechanisms at all stages of the data collection/annotation process, thus minimizing the recourse to experts to check the quality of the collected material. [sent-71, score-0.343]
</p><p>27 Related research in the CLTE direction is reported in (Negri and Mehdad, 2010), which describes the creation of an English-Spanish corpus obtained from the RTE-3 dataset by translating the English hypotheses into Spanish. [sent-72, score-0.152]
</p><p>28 3  Quality Control of Crowdsourced Data  The design of data acquisition HITs has to take into  account several factors, each having a considerable impact on the difficulty of instructing the workers, the quality and quantity of the collected data, the time and overall costs of the acquisition. [sent-75, score-0.183]
</p><p>29 For annotation jobs, quality control mechanisms can be easily set up by calculating Turkers’ agreement, by applying voting schemes, or by adding hidden gold units to the data to be annotated8. [sent-84, score-0.263]
</p><p>30 In contrast, the quality of the results of content generation jobs is harder to assess, due to the fact that multiple valid results are accept-  able (e. [sent-85, score-0.228]
</p><p>31 In such situations the standard quality control mechanisms are not directly applicable, and the detection of errors requires either costly manual verification at the end of the acquisition process, or more complex and creative solutions integrating HITs for quality check. [sent-88, score-0.227]
</p><p>32 The objective is to collect aligned T-H pairs for different language combinations, reproducing an RTE-like annotation style. [sent-99, score-0.238]
</p><p>33 However, our annotation is not limited to the standard RTE framework, where only unidirectional entailment from T to H is considered. [sent-100, score-0.818]
</p><p>34 As a useful extension, we annotate any pos-  sible entailment relation between the two text fragments, including: i) bidirectional entailment (i. [sent-101, score-1.09]
</p><p>35 semantic equivalence between T and H), ii) unidirectional entailment from T to H, and iii) unidirectional entailment from H to T. [sent-103, score-1.494]
</p><p>36 The resulting pairs can be easily used to generate not only standard RTE datasets9, but also general-purpose collections featuring multi-directional entailment relations. [sent-104, score-0.642]
</p><p>37 1 Data Acquisition and Annotation We collect large amounts of CLTE pairs carrying out the most difficult part of the process (the creation of entailment-annotated pairs) at a monolingual level. [sent-106, score-0.354]
</p><p>38 L1, L2, L3), n entailment corpora are created: one monolingual (L1/L1), and n-1 crosslingual (L1/L2, and L1/L3). [sent-109, score-0.622]
</p><p>39 Original and modified sentences are then paired and annotated to form an entailment dataset for L1. [sent-111, score-0.603]
</p><p>40 The CLTE corpora are obtained by combining the modified sentences in L1 with the original sentences in L2 and L3, and projecting to the multilingual pairs the annotations assigned to the monolingual pairs. [sent-112, score-0.483]
</p><p>41 In principle, only two stages of the process require crowdsourcing multilingual tasks, but do not concern entailment annotations. [sent-113, score-0.642]
</p><p>42 L2) in order to extend the corpus to cover new language combina9With the positive examples drawn from bidirectional and unidirectional entailments from T to H, and the negative ones drawn from unidirectional entailments from H to T. [sent-119, score-0.832]
</p><p>43 GER ENG ITA STEP1: S (emnotenonlcine g muoald)if ica2on  Figure 1: Corpus creation process. [sent-126, score-0.152]
</p><p>44 The main steps of our corpus creation process, depicted in Figure 1, can be summarized as follows:  Step1: Sentence modification. [sent-127, score-0.152]
</p><p>45 The original English sentences (ENG) are modified through (monolingual) generation HITs asking Turkers to: i) preserve the meaning of the original sentences using different surface forms, or ii) slightly change their meaning by adding or removing content. [sent-128, score-0.231]
</p><p>46 com/ another way to think about entailment is to consider whether one text T1 adds new information to the content of another text T: if so, then T is entailed by T1. [sent-134, score-0.515]
</p><p>47 Paraphrases of the original ENG texts, that will be used to create bidirectional entailment pairs (ENG↔ENG1); 2. [sent-136, score-0.71]
</p><p>48 More  specific  sentences  (the  outcome  of  content addition operations), used to create ENG←ENG1 unidirectional entailment pairs; 3. [sent-137, score-0.852]
</p><p>49 More general sentences (the outcome of content removal operations), used to create ENG→ENG1 unidirectional entailment pairs. [sent-138, score-0.852]
</p><p>50 Entailment pairs composed of the original sentences (ENG) and the modified ones (ENG1) are used as input of (monolingual) annotation HITs asking Turkers to decide which of the two texts contains more information. [sent-140, score-0.297]
</p><p>51 As a result, each ENG/ENG1 pair is annotated as an example of uni-/bidirectional entailment, and stored in the monolingual English corpus. [sent-141, score-0.168]
</p><p>52 Since the original ENG texts are aligned with the ITA and GER texts, the entailment annotations of ENG/ENG1 pairs can be projected to the other language pairs and the ITA/ENG1 and GER/ENG1 pairs are stored in the CLTE corpus. [sent-142, score-0.843]
</p><p>53 lexical) there might be slight semantic variations which, however, are very unlikely to play a crucial role in determining entailment relations. [sent-149, score-0.481]
</p><p>54 The modified sentences (ENG1) are translated into Italian (ITA1) through (multilingual) generation HITs reproducing the approach described in (Negri and Mehdad, 2010). [sent-151, score-0.191]
</p><p>55 As a result, three new datasets are produced by automatically projecting annotations: the monolingual ITA/ITA1, and the cross-lingual ENG/ITA1 and GER/ITA1. [sent-152, score-0.152]
</p><p>56 2  Crowdsourcing Sentence Modification and TE Annotation Sentence modification and TE annotation have been decomposed into a pipeline of simpler monolingual  English sub-tasks. [sent-156, score-0.266]
</p><p>57 To cope with the quality control issues discussed in Section 3, such tests are realized using gold standard units, either hidden in the data to be annotated (annotation HITs) or defined as test questions that workers must correctly answer (generation HITs). [sent-163, score-0.258]
</p><p>58 As a further quality check, all the annotation HITs consider Turkers’ agreement as a way to filter out low quality results (only annotations featuring agreement among 4 out of 5 workers are retained). [sent-165, score-0.523]
</p><p>59 As a reliability test, before creating the paraphrase workers are asked to judge if two English sentences contain the same in-  formation. [sent-169, score-0.278]
</p><p>60 This validation HIT represents a quality check of the output of each generation task (i. [sent-173, score-0.174]
</p><p>61 As a reliability test, before generating the  Figure 2: Sentence modification and TE annotation pipeline. [sent-184, score-0.153]
</p><p>62 new sentence workers are asked to judge which of two given English sentences is more detailed. [sent-185, score-0.175]
</p><p>63 As a reliability test, before generating the new sentence workers are asked tojudge which oftwo given English sentences is less detailed. [sent-189, score-0.212]
</p><p>64 These HITs are combined in an iterative process that alternates text generation, grammaticality check, and entailment annotation steps. [sent-193, score-0.598]
</p><p>65 In HIT-1 (Paraphrase) 1,414 paraphrases were collected asking three different meaning-preserving modifications of each of the 467 original sentences12. [sent-201, score-0.171]
</p><p>66 From a theoretical point of view, 12Often, crowdsourced jobs return a number of output items that is slightly larger than required, due to the labour distribution mechanism internal to MTurk. [sent-203, score-0.173]
</p><p>67 collecting many variants of a small pool of original sentences aims to create pairs featuring different entailment relations with similar superficial forms. [sent-204, score-0.713]
</p><p>68 After this validation HIT, the number of acceptable paraphrases was reduced to 1,326 (with 88 discarded sentences, corresponding to 6. [sent-207, score-0.146]
</p><p>69 The pairs marked as bidirectional entailments (1,205) were divided in three groups: 25% of the pairs (301) were directly stored in the final corpus, while the ENG1 paraphrases of the remaining 75% (904) were equally distributed to the next modification steps. [sent-210, score-0.487]
</p><p>70 In both HIT-4a (Add Information) and HIT-4b (Remove information) two new modified sentences were asked for each of the 452 paraphrases received as input. [sent-211, score-0.147]
</p><p>71 The sentences collected in these generation tasks were respectively 916 and 923. [sent-212, score-0.187]
</p><p>72 As a result 1,438 new pairs were created; out of these, 148 resulted to be bidirectional entailments and were stored in the corpus. [sent-214, score-0.318]
</p><p>73 Finally, the 1,298 entailment pairs judged as non-  bidirectional in the two previously completed HIT3 (8+1,290) were given as input to HIT-5 (Unidi676 rectional Entailment). [sent-215, score-0.709]
</p><p>74 The pairs which passed the agreement threshold were classified according to the judgement received, and stored in the corpus as unidirectional entailment pairs. [sent-216, score-0.895]
</p><p>75 First, the percentage of discarded items confirms the effectiveness of decomposing complex generation tasks into simpler subtasks that integrate validation HITs and quality checks based on non-experts’ agreement. [sent-218, score-0.313]
</p><p>76 Looking at costs and execution time, it is hard to draw definite conclusions due to several factors  that influence the progress of the crowdsourced jobs (e. [sent-225, score-0.245]
</p><p>77 one  week to prepare gold units, and to handle the I/O of each HIT), these figures show that our methodology provides a cheaper and faster way to collect entailment data in comparison with the RTE average costs reported in Section 1. [sent-237, score-0.644]
</p><p>78 n It must be noted that our methodology does not lead to the creation of pairs where some information is provided in one text and not in the other, and viceversa, as Example 1 shows: Example 1. [sent-239, score-0.281]
</p><p>79 These negative examples in both directions represent a natural extension of the dataset, relevant also for specific application-oriented scenarios, and their creation will be addressed in future work. [sent-243, score-0.152]
</p><p>80 First, the generated corpora are perfectly suitable to produce entailment datasets similar to those used in the traditional RTE evaluation framework. [sent-245, score-0.523]
</p><p>81 In particular, considering any possible entailment relation between two text fragments, our annotation subsumes the one proposed in RTE campaigns. [sent-246, score-0.552]
</p><p>82 Moreover, by swapping GE1NG as sa nndeg EatNivGe1 eixna tmheunidirectional entailment pairs, 491 additional negative examples and 680 positive examples can be easily obtained. [sent-249, score-0.481]
</p><p>83 The notion of containing more/less information, used in the “Unidirectional Entailment” HIT, can mostly be applied straightforwardly to the entailment definition. [sent-269, score-0.481]
</p><p>84 This is not always corresponding to the actual entailment relation between the sentences. [sent-272, score-0.481]
</p><p>85 As a consequence, 43 pairs featuring wrong entailment annotations were encountered. [sent-273, score-0.698]
</p><p>86 In these cases, the modified sentence was judged more/less specific than the original one, leading to unidirectional entailment annotation. [sent-275, score-0.833]
</p><p>87 On the contrary, in terms of the standard entailment definition, the correct annotation is “no entailment” (as in Example 4, which was annotated as ENG→ENG1): Example 4. [sent-276, score-0.584]
</p><p>88 These pairs were labelled as unidirectional entailments (in the example above ENG→ENG1), under mthee assumption tahmatp a proper name →is more specific and informative than a pronoun. [sent-282, score-0.419]
</p><p>89 However, adhering to the TE definition, co-referring expressions are equivalent, and their realization does not play any role in the entailment decision. [sent-283, score-0.481]
</p><p>90 This implies that the correct entailment annotation is “bidirectional”. [sent-284, score-0.552]
</p><p>91 In these cases, Turkers judged the sentence containing the explicit mention as more specific, and thus the pair was annotated as unidirectional entailment. [sent-286, score-0.331]
</p><p>92 In Example 6, the expression “the trigger” in ENG1 implicitly means “the click of the trigger”, making the two sentences equivalent, and the entailment bidirectional (instead of ENG→ENG1). [sent-290, score-0.646]
</p><p>93 In these cases, the modified sentence was judged less/more specific than the original one (and thus considered as unidirectional entailment), even though the correct judgement is “bidirectional”, as in: Example 7. [sent-292, score-0.352]
</p><p>94 Considering that the most expensive phase in the creation of a TE dataset is the generation of the pairs, this is a significant achievement. [sent-300, score-0.222]
</p><p>95 Differently, the entailment assessment phase appears to be more problematic, accounting for the majority of errors. [sent-301, score-0.481]
</p><p>96 As a final remark, considering that in the creation of a TE dataset the manual check of the annotated pairs represents a minor cost, even the involvement of experts to filter out wrong annotations would not decrease the cost-effectiveness of the proposed methodology. [sent-306, score-0.38]
</p><p>97 Following the recent trends promoting annotation efforts that go beyond the established RTE Challenge framework (unidirectional entailment between monolingual T-H pairs), in this paper we addressed the multilingual dimension of the problem. [sent-308, score-0.69]
</p><p>98 Our primary goal was the creation of large-scale collections of entailment pairs for different language combinations. [sent-309, score-0.7]
</p><p>99 These include the decomposition of a complex content generation task in a pipeline of simpler subtasks accessible to a large crowd of non-experts, and the integration of quality control mechanisms at each stage of the process. [sent-313, score-0.385]
</p><p>100 The authors would like to thank Emanuele Pianta for the helpful discussions, and Giovanni Moretti for the valuable support in the creation of the CLTE dataset. [sent-317, score-0.152]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('entailment', 0.481), ('eng', 0.297), ('clte', 0.266), ('unidirectional', 0.266), ('hits', 0.187), ('creation', 0.152), ('turkers', 0.147), ('negri', 0.141), ('workers', 0.138), ('mturk', 0.135), ('mehdad', 0.135), ('te', 0.128), ('bidirectional', 0.128), ('crowdsourcing', 0.122), ('rte', 0.115), ('monolingual', 0.099), ('featuring', 0.094), ('textual', 0.09), ('entailments', 0.086), ('jobs', 0.086), ('collected', 0.08), ('amazons', 0.078), ('bentivogli', 0.078), ('subtasks', 0.078), ('annotation', 0.071), ('generation', 0.07), ('pairs', 0.067), ('hit', 0.067), ('creating', 0.066), ('costs', 0.065), ('mechanical', 0.065), ('mechanisms', 0.064), ('conquer', 0.063), ('crowdflower', 0.063), ('matteo', 0.063), ('methodology', 0.062), ('discarded', 0.057), ('paraphrases', 0.057), ('annotations', 0.056), ('trento', 0.055), ('yashar', 0.054), ('modified', 0.053), ('projecting', 0.053), ('pipeline', 0.051), ('control', 0.05), ('crowdsourced', 0.049), ('luisa', 0.047), ('grammaticality', 0.046), ('execution', 0.045), ('modification', 0.045), ('agreement', 0.044), ('italian', 0.044), ('italy', 0.044), ('cheap', 0.042), ('corpora', 0.042), ('expert', 0.041), ('fbk', 0.04), ('routed', 0.04), ('units', 0.04), ('experts', 0.039), ('multilingual', 0.039), ('items', 0.038), ('quality', 0.038), ('stored', 0.037), ('verification', 0.037), ('danilo', 0.037), ('understandable', 0.037), ('dagan', 0.037), ('feasibility', 0.037), ('reliability', 0.037), ('sentences', 0.037), ('ii', 0.036), ('collect', 0.036), ('trigger', 0.035), ('texts', 0.035), ('create', 0.034), ('content', 0.034), ('regards', 0.034), ('couples', 0.034), ('asking', 0.034), ('check', 0.034), ('judged', 0.033), ('aligned', 0.033), ('turk', 0.033), ('validation', 0.032), ('annotated', 0.032), ('iii', 0.032), ('difficulties', 0.031), ('akp', 0.031), ('balat', 0.031), ('bullets', 0.031), ('celct', 0.031), ('evalita', 0.031), ('kosovo', 0.031), ('mrozinski', 0.031), ('multicultural', 0.031), ('qualifications', 0.031), ('reproducing', 0.031), ('sammons', 0.031), ('struggling', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="42-tfidf-1" href="./emnlp-2011-Divide_and_Conquer%3A_Crowdsourcing_the_Creation_of_Cross-Lingual_Textual_Entailment_Corpora.html">42 emnlp-2011-Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora</a></p>
<p>Author: Matteo Negri ; Luisa Bentivogli ; Yashar Mehdad ; Danilo Giampiccolo ; Alessandro Marchetti</p><p>Abstract: We address the creation of cross-lingual textual entailment corpora by means of crowdsourcing. Our goal is to define a cheap and replicable data collection methodology that minimizes the manual work done by expert annotators, without resorting to preprocessing tools or already annotated monolingual datasets. In line with recent works emphasizing the need of large-scale annotation efforts for textual entailment, our work aims to: i) tackle the scarcity of data available to train and evaluate systems, and ii) promote the recourse to crowdsourcing as an effective way to reduce the costs of data collection without sacrificing quality. We show that a complex data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators. The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German.</p><p>2 0.18384168 <a title="42-tfidf-2" href="./emnlp-2011-Active_Learning_with_Amazon_Mechanical_Turk.html">17 emnlp-2011-Active Learning with Amazon Mechanical Turk</a></p>
<p>Author: Florian Laws ; Christian Scheible ; Hinrich Schutze</p><p>Abstract: Supervised classification needs large amounts of annotated training data that is expensive to create. Two approaches that reduce the cost of annotation are active learning and crowdsourcing. However, these two approaches have not been combined successfully to date. We evaluate the utility of active learning in crowdsourcing on two tasks, named entity recognition and sentiment detection, and show that active learning outperforms random selection of annotation examples in a noisy crowdsourcing scenario.</p><p>3 0.10470638 <a title="42-tfidf-3" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>Author: Prodromos Malakasiotis ; Ion Androutsopoulos</p><p>Abstract: We present a method that paraphrases a given sentence by first generating candidate paraphrases and then ranking (or classifying) them. The candidates are generated by applying existing paraphrasing rules extracted from parallel corpora. The ranking component considers not only the overall quality of the rules that produced each candidate, but also the extent to which they preserve grammaticality and meaning in the particular context of the input sentence, as well as the degree to which the candidate differs from the input. We experimented with both a Maximum Entropy classifier and an SVR ranker. Experimental results show that incorporating features from an existing paraphrase recognizer in the ranking component improves performance, and that our overall method compares well against a state of the art paraphrase generator, when paraphrasing rules apply to the input sentences. We also propose a new methodology to evaluate the ranking components of generate-and-rank paraphrase generators, which evaluates them across different combinations of weights for grammaticality, meaning preservation, and diversity. The paper is accompanied by a paraphrasing dataset we constructed for evaluations of this kind.</p><p>4 0.097861618 <a title="42-tfidf-4" href="./emnlp-2011-Linguistic_Redundancy_in_Twitter.html">89 emnlp-2011-Linguistic Redundancy in Twitter</a></p>
<p>Author: Fabio Massimo Zanzotto ; Marco Pennaccchiotti ; Kostas Tsioutsiouliklis</p><p>Abstract: In the last few years, the interest of the research community in micro-blogs and social media services, such as Twitter, is growing exponentially. Yet, so far not much attention has been paid on a key characteristic of microblogs: the high level of information redundancy. The aim of this paper is to systematically approach this problem by providing an operational definition of redundancy. We cast redundancy in the framework of Textual Entailment Recognition. We also provide quantitative evidence on the pervasiveness of redundancy in Twitter, and describe a dataset of redundancy-annotated tweets. Finally, we present a general purpose system for identifying redundant tweets. An extensive quantitative evaluation shows that our system successfully solves the redundancy detection task, improving over baseline systems with statistical significance.</p><p>5 0.078601345 <a title="42-tfidf-5" href="./emnlp-2011-The_Imagination_of_Crowds%3A_Conversational_AAC_Language_Modeling_using_Crowdsourcing_and_Large_Data_Sources.html">133 emnlp-2011-The Imagination of Crowds: Conversational AAC Language Modeling using Crowdsourcing and Large Data Sources</a></p>
<p>Author: Keith Vertanen ; Per Ola Kristensson</p><p>Abstract: Augmented and alternative communication (AAC) devices enable users with certain communication disabilities to participate in everyday conversations. Such devices often rely on statistical language models to improve text entry by offering word predictions. These predictions can be improved if the language model is trained on data that closely reflects the style of the users’ intended communications. Unfortunately, there is no large dataset consisting of genuine AAC messages. In this paper we demonstrate how we can crowdsource the creation of a large set of fictional AAC messages. We show that these messages model conversational AAC better than the currently used datasets based on telephone conversations or newswire text. We leverage our crowdsourced messages to intelligently select sentences from much larger sets of Twitter, blog and Usenet data. Compared to a model trained only on telephone transcripts, our best performing model reduced perplexity on three test sets of AAC-like communications by 60– 82% relative. This translated to a potential keystroke savings in a predictive keyboard interface of 5–1 1%.</p><p>6 0.075290598 <a title="42-tfidf-6" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>7 0.071406975 <a title="42-tfidf-7" href="./emnlp-2011-Data-Driven_Response_Generation_in_Social_Media.html">38 emnlp-2011-Data-Driven Response Generation in Social Media</a></p>
<p>8 0.048843272 <a title="42-tfidf-8" href="./emnlp-2011-Corroborating_Text_Evaluation_Results_with_Heterogeneous_Measures.html">36 emnlp-2011-Corroborating Text Evaluation Results with Heterogeneous Measures</a></p>
<p>9 0.047208201 <a title="42-tfidf-9" href="./emnlp-2011-Discriminating_Gender_on_Twitter.html">41 emnlp-2011-Discriminating Gender on Twitter</a></p>
<p>10 0.04690079 <a title="42-tfidf-10" href="./emnlp-2011-A_Probabilistic_Forest-to-String_Model_for_Language_Generation_from_Typed_Lambda_Calculus_Expressions.html">10 emnlp-2011-A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions</a></p>
<p>11 0.044956144 <a title="42-tfidf-11" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>12 0.03907454 <a title="42-tfidf-12" href="./emnlp-2011-Language_Models_for_Machine_Translation%3A_Original_vs._Translated_Texts.html">76 emnlp-2011-Language Models for Machine Translation: Original vs. Translated Texts</a></p>
<p>13 0.038670518 <a title="42-tfidf-13" href="./emnlp-2011-Minimum_Imputed-Risk%3A_Unsupervised_Discriminative_Training_for_Machine_Translation.html">93 emnlp-2011-Minimum Imputed-Risk: Unsupervised Discriminative Training for Machine Translation</a></p>
<p>14 0.038215823 <a title="42-tfidf-14" href="./emnlp-2011-Syntax-Based_Grammaticality_Improvement_using_CCG_and_Guided_Search.html">132 emnlp-2011-Syntax-Based Grammaticality Improvement using CCG and Guided Search</a></p>
<p>15 0.037327349 <a title="42-tfidf-15" href="./emnlp-2011-Structured_Lexical_Similarity_via_Convolution_Kernels_on_Dependency_Trees.html">127 emnlp-2011-Structured Lexical Similarity via Convolution Kernels on Dependency Trees</a></p>
<p>16 0.037111554 <a title="42-tfidf-16" href="./emnlp-2011-Multi-Source_Transfer_of_Delexicalized_Dependency_Parsers.html">95 emnlp-2011-Multi-Source Transfer of Delexicalized Dependency Parsers</a></p>
<p>17 0.036905244 <a title="42-tfidf-17" href="./emnlp-2011-Evaluating_Dependency_Parsing%3A_Robust_and_Heuristics-Free_Cross-Annotation_Evaluation.html">50 emnlp-2011-Evaluating Dependency Parsing: Robust and Heuristics-Free Cross-Annotation Evaluation</a></p>
<p>18 0.036760122 <a title="42-tfidf-18" href="./emnlp-2011-Universal_Morphological_Analysis_using_Structured_Nearest_Neighbor_Prediction.html">140 emnlp-2011-Universal Morphological Analysis using Structured Nearest Neighbor Prediction</a></p>
<p>19 0.03636181 <a title="42-tfidf-19" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>20 0.035972632 <a title="42-tfidf-20" href="./emnlp-2011-Better_Evaluation_Metrics_Lead_to_Better_Machine_Translation.html">22 emnlp-2011-Better Evaluation Metrics Lead to Better Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.158), (1, -0.048), (2, 0.069), (3, -0.051), (4, -0.004), (5, -0.05), (6, -0.053), (7, 0.071), (8, -0.01), (9, 0.066), (10, -0.037), (11, -0.083), (12, 0.043), (13, 0.03), (14, 0.024), (15, 0.043), (16, 0.062), (17, -0.134), (18, -0.187), (19, 0.048), (20, 0.058), (21, 0.093), (22, 0.136), (23, -0.096), (24, -0.075), (25, -0.114), (26, -0.222), (27, 0.057), (28, 0.193), (29, -0.248), (30, 0.134), (31, 0.164), (32, 0.033), (33, 0.006), (34, 0.191), (35, 0.039), (36, -0.051), (37, -0.03), (38, -0.081), (39, -0.051), (40, -0.179), (41, -0.004), (42, -0.035), (43, 0.046), (44, -0.081), (45, -0.025), (46, -0.039), (47, -0.037), (48, -0.119), (49, -0.114)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96350121 <a title="42-lsi-1" href="./emnlp-2011-Divide_and_Conquer%3A_Crowdsourcing_the_Creation_of_Cross-Lingual_Textual_Entailment_Corpora.html">42 emnlp-2011-Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora</a></p>
<p>Author: Matteo Negri ; Luisa Bentivogli ; Yashar Mehdad ; Danilo Giampiccolo ; Alessandro Marchetti</p><p>Abstract: We address the creation of cross-lingual textual entailment corpora by means of crowdsourcing. Our goal is to define a cheap and replicable data collection methodology that minimizes the manual work done by expert annotators, without resorting to preprocessing tools or already annotated monolingual datasets. In line with recent works emphasizing the need of large-scale annotation efforts for textual entailment, our work aims to: i) tackle the scarcity of data available to train and evaluate systems, and ii) promote the recourse to crowdsourcing as an effective way to reduce the costs of data collection without sacrificing quality. We show that a complex data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators. The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German.</p><p>2 0.65111637 <a title="42-lsi-2" href="./emnlp-2011-Active_Learning_with_Amazon_Mechanical_Turk.html">17 emnlp-2011-Active Learning with Amazon Mechanical Turk</a></p>
<p>Author: Florian Laws ; Christian Scheible ; Hinrich Schutze</p><p>Abstract: Supervised classification needs large amounts of annotated training data that is expensive to create. Two approaches that reduce the cost of annotation are active learning and crowdsourcing. However, these two approaches have not been combined successfully to date. We evaluate the utility of active learning in crowdsourcing on two tasks, named entity recognition and sentiment detection, and show that active learning outperforms random selection of annotation examples in a noisy crowdsourcing scenario.</p><p>3 0.60925627 <a title="42-lsi-3" href="./emnlp-2011-The_Imagination_of_Crowds%3A_Conversational_AAC_Language_Modeling_using_Crowdsourcing_and_Large_Data_Sources.html">133 emnlp-2011-The Imagination of Crowds: Conversational AAC Language Modeling using Crowdsourcing and Large Data Sources</a></p>
<p>Author: Keith Vertanen ; Per Ola Kristensson</p><p>Abstract: Augmented and alternative communication (AAC) devices enable users with certain communication disabilities to participate in everyday conversations. Such devices often rely on statistical language models to improve text entry by offering word predictions. These predictions can be improved if the language model is trained on data that closely reflects the style of the users’ intended communications. Unfortunately, there is no large dataset consisting of genuine AAC messages. In this paper we demonstrate how we can crowdsource the creation of a large set of fictional AAC messages. We show that these messages model conversational AAC better than the currently used datasets based on telephone conversations or newswire text. We leverage our crowdsourced messages to intelligently select sentences from much larger sets of Twitter, blog and Usenet data. Compared to a model trained only on telephone transcripts, our best performing model reduced perplexity on three test sets of AAC-like communications by 60– 82% relative. This translated to a potential keystroke savings in a predictive keyboard interface of 5–1 1%.</p><p>4 0.34560707 <a title="42-lsi-4" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>Author: Prodromos Malakasiotis ; Ion Androutsopoulos</p><p>Abstract: We present a method that paraphrases a given sentence by first generating candidate paraphrases and then ranking (or classifying) them. The candidates are generated by applying existing paraphrasing rules extracted from parallel corpora. The ranking component considers not only the overall quality of the rules that produced each candidate, but also the extent to which they preserve grammaticality and meaning in the particular context of the input sentence, as well as the degree to which the candidate differs from the input. We experimented with both a Maximum Entropy classifier and an SVR ranker. Experimental results show that incorporating features from an existing paraphrase recognizer in the ranking component improves performance, and that our overall method compares well against a state of the art paraphrase generator, when paraphrasing rules apply to the input sentences. We also propose a new methodology to evaluate the ranking components of generate-and-rank paraphrase generators, which evaluates them across different combinations of weights for grammaticality, meaning preservation, and diversity. The paper is accompanied by a paraphrasing dataset we constructed for evaluations of this kind.</p><p>5 0.28265482 <a title="42-lsi-5" href="./emnlp-2011-Learning_Sentential_Paraphrases_from_Bilingual_Parallel_Corpora_for_Text-to-Text_Generation.html">83 emnlp-2011-Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation</a></p>
<p>Author: Juri Ganitkevitch ; Chris Callison-Burch ; Courtney Napoles ; Benjamin Van Durme</p><p>Abstract: Previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora. However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems.</p><p>6 0.27200216 <a title="42-lsi-6" href="./emnlp-2011-Data-Driven_Response_Generation_in_Social_Media.html">38 emnlp-2011-Data-Driven Response Generation in Social Media</a></p>
<p>7 0.24373333 <a title="42-lsi-7" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>8 0.22950946 <a title="42-lsi-8" href="./emnlp-2011-Parser_Evaluation_over_Local_and_Non-Local_Deep_Dependencies_in_a_Large_Corpus.html">103 emnlp-2011-Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</a></p>
<p>9 0.22187433 <a title="42-lsi-9" href="./emnlp-2011-Corroborating_Text_Evaluation_Results_with_Heterogeneous_Measures.html">36 emnlp-2011-Corroborating Text Evaluation Results with Heterogeneous Measures</a></p>
<p>10 0.22124515 <a title="42-lsi-10" href="./emnlp-2011-Ranking_Human_and_Machine_Summarization_Systems.html">110 emnlp-2011-Ranking Human and Machine Summarization Systems</a></p>
<p>11 0.22023824 <a title="42-lsi-11" href="./emnlp-2011-Corpus-Guided_Sentence_Generation_of_Natural_Images.html">34 emnlp-2011-Corpus-Guided Sentence Generation of Natural Images</a></p>
<p>12 0.21632031 <a title="42-lsi-12" href="./emnlp-2011-Improving_Bilingual_Projections_via_Sparse_Covariance_Matrices.html">73 emnlp-2011-Improving Bilingual Projections via Sparse Covariance Matrices</a></p>
<p>13 0.21115132 <a title="42-lsi-13" href="./emnlp-2011-Discriminating_Gender_on_Twitter.html">41 emnlp-2011-Discriminating Gender on Twitter</a></p>
<p>14 0.20115572 <a title="42-lsi-14" href="./emnlp-2011-Linguistic_Redundancy_in_Twitter.html">89 emnlp-2011-Linguistic Redundancy in Twitter</a></p>
<p>15 0.18926242 <a title="42-lsi-15" href="./emnlp-2011-Heuristic_Search_for_Non-Bottom-Up_Tree_Structure_Prediction.html">65 emnlp-2011-Heuristic Search for Non-Bottom-Up Tree Structure Prediction</a></p>
<p>16 0.17340714 <a title="42-lsi-16" href="./emnlp-2011-Computing_Logical_Form_on_Regulatory_Texts.html">32 emnlp-2011-Computing Logical Form on Regulatory Texts</a></p>
<p>17 0.16949594 <a title="42-lsi-17" href="./emnlp-2011-Watermarking_the_Outputs_of_Structured_Prediction_with_an_application_in_Statistical_Machine_Translation..html">148 emnlp-2011-Watermarking the Outputs of Structured Prediction with an application in Statistical Machine Translation.</a></p>
<p>18 0.15709284 <a title="42-lsi-18" href="./emnlp-2011-Generating_Aspect-oriented_Multi-Document_Summarization_with_Event-aspect_model.html">61 emnlp-2011-Generating Aspect-oriented Multi-Document Summarization with Event-aspect model</a></p>
<p>19 0.14908852 <a title="42-lsi-19" href="./emnlp-2011-Approximate_Scalable_Bounded_Space_Sketch_for_Large_Data_NLP.html">19 emnlp-2011-Approximate Scalable Bounded Space Sketch for Large Data NLP</a></p>
<p>20 0.14750737 <a title="42-lsi-20" href="./emnlp-2011-Syntax-Based_Grammaticality_Improvement_using_CCG_and_Guided_Search.html">132 emnlp-2011-Syntax-Based Grammaticality Improvement using CCG and Guided Search</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(23, 0.629), (32, 0.02), (36, 0.021), (37, 0.01), (45, 0.046), (53, 0.013), (54, 0.022), (57, 0.012), (62, 0.019), (64, 0.022), (66, 0.022), (79, 0.03), (82, 0.012), (90, 0.011), (96, 0.023), (98, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99625093 <a title="42-lda-1" href="./emnlp-2011-Divide_and_Conquer%3A_Crowdsourcing_the_Creation_of_Cross-Lingual_Textual_Entailment_Corpora.html">42 emnlp-2011-Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora</a></p>
<p>Author: Matteo Negri ; Luisa Bentivogli ; Yashar Mehdad ; Danilo Giampiccolo ; Alessandro Marchetti</p><p>Abstract: We address the creation of cross-lingual textual entailment corpora by means of crowdsourcing. Our goal is to define a cheap and replicable data collection methodology that minimizes the manual work done by expert annotators, without resorting to preprocessing tools or already annotated monolingual datasets. In line with recent works emphasizing the need of large-scale annotation efforts for textual entailment, our work aims to: i) tackle the scarcity of data available to train and evaluate systems, and ii) promote the recourse to crowdsourcing as an effective way to reduce the costs of data collection without sacrificing quality. We show that a complex data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators. The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German.</p><p>2 0.99420583 <a title="42-lda-2" href="./emnlp-2011-Minimum_Imputed-Risk%3A_Unsupervised_Discriminative_Training_for_Machine_Translation.html">93 emnlp-2011-Minimum Imputed-Risk: Unsupervised Discriminative Training for Machine Translation</a></p>
<p>Author: Zhifei Li ; Ziyuan Wang ; Jason Eisner ; Sanjeev Khudanpur ; Brian Roark</p><p>Abstract: Discriminative training for machine translation has been well studied in the recent past. A limitation of the work to date is that it relies on the availability of high-quality in-domain bilingual text for supervised training. We present an unsupervised discriminative training framework to incorporate the usually plentiful target-language monolingual data by using a rough “reverse” translation system. Intuitively, our method strives to ensure that probabilistic “round-trip” translation from a target- language sentence to the source-language and back will have low expected loss. Theoretically, this may be justified as (discriminatively) minimizing an imputed empirical risk. Empirically, we demonstrate that augmenting supervised training with unsupervised data improves translation performance over the supervised case for both IWSLT and NIST tasks.</p><p>3 0.99313307 <a title="42-lda-3" href="./emnlp-2011-A_Joint_Model_for_Extended_Semantic_Role_Labeling.html">7 emnlp-2011-A Joint Model for Extended Semantic Role Labeling</a></p>
<p>Author: Vivek Srikumar ; Dan Roth</p><p>Abstract: This paper presents a model that extends semantic role labeling. Existing approaches independently analyze relations expressed by verb predicates or those expressed as nominalizations. However, sentences express relations via other linguistic phenomena as well. Furthermore, these phenomena interact with each other, thus restricting the structures they articulate. In this paper, we use this intuition to define a joint inference model that captures the inter-dependencies between verb semantic role labeling and relations expressed using prepositions. The scarcity of jointly labeled data presents a crucial technical challenge for learning a joint model. The key strength of our model is that we use existing structure predictors as black boxes. By enforcing consistency constraints between their predictions, we show improvements in the performance of both tasks without retraining the individual models.</p><p>4 0.99049515 <a title="42-lda-4" href="./emnlp-2011-Enhancing_Chinese_Word_Segmentation_Using_Unlabeled_Data.html">48 emnlp-2011-Enhancing Chinese Word Segmentation Using Unlabeled Data</a></p>
<p>Author: Weiwei Sun ; Jia Xu</p><p>Abstract: This paper investigates improving supervised word segmentation accuracy with unlabeled data. Both large-scale in-domain data and small-scale document text are considered. We present a unified solution to include features derived from unlabeled data to a discriminative learning model. For the large-scale data, we derive string statistics from Gigaword to assist a character-based segmenter. In addition, we introduce the idea about transductive, document-level segmentation, which is designed to improve the system recall for out-ofvocabulary (OOV) words which appear more than once inside a document. Novel features1 result in relative error reductions of 13.8% and 15.4% in terms of F-score and the recall of OOV words respectively.</p><p>5 0.98742419 <a title="42-lda-5" href="./emnlp-2011-Timeline_Generation_through_Evolutionary_Trans-Temporal_Summarization.html">135 emnlp-2011-Timeline Generation through Evolutionary Trans-Temporal Summarization</a></p>
<p>Author: Rui Yan ; Liang Kong ; Congrui Huang ; Xiaojun Wan ; Xiaoming Li ; Yan Zhang</p><p>Abstract: We investigate an important and challenging problem in summary generation, i.e., Evolutionary Trans-Temporal Summarization (ETTS), which generates news timelines from massive data on the Internet. ETTS greatly facilitates fast news browsing and knowledge comprehension, and hence is a necessity. Given the collection oftime-stamped web documents related to the evolving news, ETTS aims to return news evolution along the timeline, consisting of individual but correlated summaries on each date. Existing summarization algorithms fail to utilize trans-temporal characteristics among these component summaries. We propose to model trans-temporal correlations among component summaries for timelines, using inter-date and intra-date sen- tence dependencies, and present a novel combination. We develop experimental systems to compare 5 rival algorithms on 6 instinctively different datasets which amount to 10251 documents. Evaluation results in ROUGE metrics indicate the effectiveness of the proposed approach based on trans-temporal information. 1</p><p>6 0.90404105 <a title="42-lda-6" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>7 0.90332901 <a title="42-lda-7" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>8 0.90088034 <a title="42-lda-8" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>9 0.87655216 <a title="42-lda-9" href="./emnlp-2011-Lateen_EM%3A_Unsupervised_Training_with_Multiple_Objectives%2C_Applied_to_Dependency_Grammar_Induction.html">79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</a></p>
<p>10 0.86360502 <a title="42-lda-10" href="./emnlp-2011-Active_Learning_with_Amazon_Mechanical_Turk.html">17 emnlp-2011-Active Learning with Amazon Mechanical Turk</a></p>
<p>11 0.85924941 <a title="42-lda-11" href="./emnlp-2011-Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection.html">44 emnlp-2011-Domain Adaptation via Pseudo In-Domain Data Selection</a></p>
<p>12 0.85849744 <a title="42-lda-12" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>13 0.85715789 <a title="42-lda-13" href="./emnlp-2011-Generating_Aspect-oriented_Multi-Document_Summarization_with_Event-aspect_model.html">61 emnlp-2011-Generating Aspect-oriented Multi-Document Summarization with Event-aspect model</a></p>
<p>14 0.85548741 <a title="42-lda-14" href="./emnlp-2011-Splitting_Noun_Compounds_via_Monolingual_and_Bilingual_Paraphrasing%3A_A_Study_on_Japanese_Katakana_Words.html">124 emnlp-2011-Splitting Noun Compounds via Monolingual and Bilingual Paraphrasing: A Study on Japanese Katakana Words</a></p>
<p>15 0.85325021 <a title="42-lda-15" href="./emnlp-2011-Structural_Opinion_Mining_for_Graph-based_Sentiment_Representation.html">126 emnlp-2011-Structural Opinion Mining for Graph-based Sentiment Representation</a></p>
<p>16 0.85156101 <a title="42-lda-16" href="./emnlp-2011-Bootstrapped_Named_Entity_Recognition_for_Product_Attribute_Extraction.html">23 emnlp-2011-Bootstrapped Named Entity Recognition for Product Attribute Extraction</a></p>
<p>17 0.84846544 <a title="42-lda-17" href="./emnlp-2011-Cache-based_Document-level_Statistical_Machine_Translation.html">25 emnlp-2011-Cache-based Document-level Statistical Machine Translation</a></p>
<p>18 0.84804744 <a title="42-lda-18" href="./emnlp-2011-Linguistic_Redundancy_in_Twitter.html">89 emnlp-2011-Linguistic Redundancy in Twitter</a></p>
<p>19 0.84546882 <a title="42-lda-19" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>20 0.84498417 <a title="42-lda-20" href="./emnlp-2011-Learning_Local_Content_Shift_Detectors_from_Document-level_Information.html">82 emnlp-2011-Learning Local Content Shift Detectors from Document-level Information</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
