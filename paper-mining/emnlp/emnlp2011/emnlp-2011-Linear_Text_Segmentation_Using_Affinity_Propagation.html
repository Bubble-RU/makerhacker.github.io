<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>88 emnlp-2011-Linear Text Segmentation Using Affinity Propagation</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2011" href="../home/emnlp2011_home.html">emnlp2011</a> <a title="emnlp-2011-88" href="#">emnlp2011-88</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>88 emnlp-2011-Linear Text Segmentation Using Affinity Propagation</h1>
<br/><p>Source: <a title="emnlp-2011-88-pdf" href="http://aclweb.org/anthology//D/D11/D11-1026.pdf">pdf</a></p><p>Author: Anna Kazantseva ; Stan Szpakowicz</p><p>Abstract: This paper presents a new algorithm for linear text segmentation. It is an adaptation of Affinity Propagation, a state-of-the-art clustering algorithm in the framework of factor graphs. Affinity Propagation for Segmentation, or APS, receives a set of pairwise similarities between data points and produces segment boundaries and segment centres data points which best describe all other data points within the segment. APS iteratively passes messages in a cyclic factor graph, until convergence. Each iteration works with information on all available similarities, resulting in highquality results. APS scales linearly for realistic segmentation tasks. We derive the algorithm from the original Affinity Propagation formu– lation, and evaluate its performance on topical text segmentation in comparison with two state-of-the art segmenters. The results suggest that APS performs on par with or outperforms these two very competitive baselines.</p><p>Reference: <a title="emnlp-2011-88-reference" href="../emnlp2011_reference/emnlp-2011-Linear_Text_Segmentation_Using_Affinity_Propagation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 It is an adaptation of Affinity Propagation, a state-of-the-art clustering algorithm in the framework of factor graphs. [sent-4, score-0.206]
</p><p>2 Affinity Propagation for Segmentation, or APS, receives a set of pairwise similarities between data points and produces segment boundaries and segment centres data points which best describe all other data points within the segment. [sent-5, score-1.052]
</p><p>3 APS iteratively passes messages in a cyclic factor graph, until convergence. [sent-6, score-0.361]
</p><p>4 We derive the algorithm from the original Affinity Propagation formu–  lation, and evaluate its performance on topical text segmentation in comparison with two state-of-the art segmenters. [sent-9, score-0.288]
</p><p>5 Topical text segmentation identifies the more noticeable topic shifts. [sent-13, score-0.226]
</p><p>6 A topical segmenter’s output is a very simple picture of the document’s structure. [sent-14, score-0.101]
</p><p>7 That is why improved quality of text segmentation can benefit other language-processing tasks. [sent-16, score-0.19]
</p><p>8 ca We present Affinity Propagation for Segmentation (APS), an adaptation of a state-of-the-art clustering algorithm, Affinity Propagation (Frey and Dueck, 2007; Givoni and Frey, 2009). [sent-19, score-0.082]
</p><p>9 1 The original AP algorithm considerably improved exemplarbased clustering both in terms of speed and the quality of solutions. [sent-20, score-0.106]
</p><p>10 At its core, APS is suitable for segmenting any sequences of data, but we present it in the context of segmenting documents. [sent-22, score-0.05]
</p><p>11 APS takes as input a matrix of pairwise similarities between sentences and, for each sentence, a preference value which indicates an a priori belief in how likely a sentence is to be chosen as a segment centre. [sent-23, score-0.541]
</p><p>12 APS outputs segment assignments and segment centres data points which best explain all other points in a segment. [sent-24, score-0.793]
</p><p>13 The algorithm attempts to maximize net similarity the sum of similarities between all data points and their respective segment centres. [sent-25, score-0.707]
</p><p>14 APS operates by iteratively passing messages in –  –  a factor graph (Kschischang, Frey, and Loeliger, 2001) until a good set of segments emerges. [sent-26, score-0.467]
</p><p>15 Each iteration considers all similarities takes into account all available information. [sent-27, score-0.142]
</p><p>16 For the majority of realistic segmentation tasks, however, the upper bound is O(MN) messages, where M is a constant. [sent-29, score-0.214]
</p><p>17 This is more computationally expensive than the requirements of locally informed segmentation algorithms such as those based on HMM or CRF (see Section 2), but for a globallyinformed algorithm the requirements are very reasonable. [sent-30, score-0.367]
</p><p>18 APS is an instance of loopy-belief propagation (belief propagation on cyclic graphs) which has –  1An implementation of APS in Java, and the data sets, can be downloaded at hwww . [sent-31, score-0.375]
</p><p>19 Theoretically, such algorithms are not guaranteed to converge or to maximize the objective function. [sent-38, score-0.119]
</p><p>20 The desired number of segments can be set by adjusting preferences. [sent-41, score-0.038]
</p><p>21 We evaluate the performance of APS on three tasks: finding topical boundaries in transcripts of course lectures (Malioutov and Barzilay, 2006), identifying sections in medical textbooks (Eisenstein and Barzilay, 2008) and identifying chapter breaks in novels. [sent-42, score-0.118]
</p><p>22 We compare APS with two recent systems: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). [sent-43, score-0.104]
</p><p>23 Section 2 of the paper outlines relevant research on topical text segmentation. [sent-46, score-0.109]
</p><p>24 Section 3 briefly covers the framework of factor graphs and outlines the original Affinity Propagation algorithm for clustering. [sent-47, score-0.251]
</p><p>25 Section 4 contains the derivation of the new update messages for APSeg. [sent-48, score-0.198]
</p><p>26 2  Related Work  This sections discusses selected text segmentation methods and positions the proposed APS algorithm in that context. [sent-50, score-0.237]
</p><p>27 Most research on automatic text segmentation revolves around a simple idea: when the topic shifts, so does the vocabulary (Youmans, 1991). [sent-51, score-0.226]
</p><p>28 We can roughly subdivide existing approaches into two categories: locally informed and globally informed. [sent-52, score-0.143]
</p><p>29 Locally informed segmenters attempt to identify topic shifts by considering only a small portion of complete document. [sent-53, score-0.193]
</p><p>30 Other examples include text 285 segmentation using Hidden Markov Models (Blei and Moreno, 2001) or Conditional Random Fields (Lafferty, McCallum, and Pereira, 2001). [sent-57, score-0.19]
</p><p>31 Locally informed methods are often very efficient because of lean memory and CPU time requirements. [sent-58, score-0.066]
</p><p>32 Globally informed methods consider “the big picture” when determining the most likely location of segment boundaries. [sent-60, score-0.356]
</p><p>33 Malioutov and Barzilay (2006) show that the knowledge about long-range similarities between sentences improves segmentation quality. [sent-62, score-0.299]
</p><p>34 The document is represented as a graph: nodes are sentences and edges are weighted using a measure of lexical similarity. [sent-64, score-0.075]
</p><p>35 The graph is cut in a way which maximizes the net edge weight within each segment and minimizes the net weight of severed edges. [sent-65, score-0.526]
</p><p>36 Such Minimum Cut segmentation resembles APS the most among others mentioned in this paper. [sent-66, score-0.19]
</p><p>37 Another notable direction in text segmentation uses generative models to find segment boundaries. [sent-68, score-0.48]
</p><p>38 Segment boundaries are assigned so as to maximize the likelihood of observing the complete sequence. [sent-70, score-0.109]
</p><p>39 (2009) use a Latent Dirichlet allocation topic model (Blei, Ng, and Jordan, 2003) to find coherent segment boundaries. [sent-72, score-0.326]
</p><p>40 Such methods output segment boundaries and suggest lexical distribution associated with each segment. [sent-73, score-0.334]
</p><p>41 Globally informed models generally perform better, especially on more challenging datasets such as speech recordings, but they have unsurprisingly higher memory and CPU time requirements. [sent-75, score-0.066]
</p><p>42 It is unsupervised and, unlike most other segmenters, does not require specifying the desired number of segments as an input parameter. [sent-77, score-0.038]
</p><p>43 Because APS operates on a precompiled matrix of pair-wise sentence similarities, it is easy to incorporate new kinds ofinformation, such as synonymy or adjacency. [sent-79, score-0.065]
</p><p>44 It also provides some information as to what the segment is about, because each segment is associated with a segment centre. [sent-80, score-0.87]
</p><p>45 –  –  3  Factor graphs and affinity propagation for clustering 3. [sent-81, score-0.575]
</p><p>46 1 Factor graphs and the max-sum algorithm The APS algorithm is an instance of belief propagation on a cyclic factor graph. [sent-82, score-0.492]
</p><p>47 In order to explain the derivation of the algorithm, we will first briefly introduce factor graphs as a framework. [sent-83, score-0.192]
</p><p>48 Many computational problems can be reduced to maximizing the value of a multi-variate function F(x1, . [sent-84, score-0.064]
</p><p>49 In Equation 1, H is a set of discrete indices and fh is a local function with arguments Xh ⊂ {x1, . [sent-88, score-0.087]
</p><p>50 ,xn) =  X fh(Xh)  (1)  hX∈H  Factor graphs offer a concise graphical representation for such problems. [sent-94, score-0.092]
</p><p>51 A global function F which can be decomposed into a sum of M local function fh can be represented as a bi-partite graph with M function nodes and N variable nodes (M = |H|). [sent-95, score-0.471]
</p><p>52 Figure n1 sohdoewss a an example loef a dfaescto (Mr graph f|o)r. [sent-96, score-0.059]
</p><p>53 The factor (or function) nodes are dark squares, the  variable nodes are light circles. [sent-98, score-0.323]
</p><p>54 The well-known max-sum algorithm (Bishop, 2006) seeks a configuration of variables which maximizes the objective function. [sent-99, score-0.13]
</p><p>55 It finds the maximum in acyclic factor graphs, but in graphs with cycles neither convergence nor optimality are guaranteed (Pearl, 1982). [sent-100, score-0.215]
</p><p>56 The max-sum algorithm amounts to propagating messages from function nodes to variable nodes and from variable nodes to function nodes. [sent-102, score-0.635]
</p><p>57 x1fx12fx32x4  N(x) is the set of all function nodes which are x’s neighbours. [sent-104, score-0.107]
</p><p>58 The message reflects the evidence about the distribution of x from all functions which have x as an argument, except for the function corresponding to the receiving node f. [sent-105, score-0.34]
</p><p>59 A message µf→x from function f to variable x is computed as follows:  µf→x=N m(fa)\xx(f(x1,. [sent-106, score-0.283]
</p><p>60 ,xm) +x0∈XN(f)\xµx0→f) (3) N(f) is the set of all variable nodes which are f’s neighbours. [sent-109, score-0.148]
</p><p>61 The message reflects the evidence about the distribution of x from function f and its neighbours other than x. [sent-110, score-0.324]
</p><p>62 A common message-passing schedule on cyclic factor graphs is flooding: iteratively passing all variable-to-function messages, then all function-tovariable messages. [sent-111, score-0.31]
</p><p>63 Upon convergence, the summary message reflecting final beliefs about the maximizing configuration of variables is computed as a sum of all incoming function-to-variable messages. [sent-112, score-0.368]
</p><p>64 2  Affinity Propagation  The APS algorithm described in this paper is a modification of the original Affinity Propagation algorithm intended for exemplar-based clustering (Frey and Dueck, 2007; Givoni and Frey, 2009). [sent-114, score-0.13]
</p><p>65 This section describes the binary variable formulation proposed by Givoni and Frey, and lays the groundwork for deriving the new update messages (Section 4). [sent-115, score-0.271]
</p><p>66 Affinity Propagation for exemplar-based clustering is formulated as follows: to cluster N data points, one must specify a matrix of pairwise similarities {SIM(i, j)}i,j∈{1,. [sent-116, score-0.27]
</p><p>67 ,N},i6=j and a set of islealrfi-tsiiemsil {aSriItieMs (so-called preferences) SIM (j, j) swehlfic-shi mreilflaercitti a priori ablel eidef ps rienf ehroewnc likely IeMach( jd,ajta) point is to be selected as an exemplar. [sent-119, score-0.073]
</p><p>68 The algorithm then assigns each data point to an exemplar so as to maximize net similarity the sum of Figure 2: Factor graph for affinity propagation. [sent-121, score-0.734]
</p><p>69 –  E1  Ej  EN  similarities between all points and their respective exemplars; this is expressed by Equation 7. [sent-122, score-0.19]
</p><p>70 Figure 2 shows a schematic factor graph for this problem, with N2 binary variables. [sent-123, score-0.159]
</p><p>71 Function nodes Ej enforce a coherence constraint: a data point cannot exemplify another point unless it is an exemplar for itself:  Ej(c1j,. [sent-125, score-0.368]
</p><p>72 ,cNj) =0−∞ifo th crje srjow=miese 0 i6 = ∧ j cij=( 14) An I node encodes a single-cluster constraint: each data point must belong to exactly one exemplar and therefore to one cluster:  –  Ii(ci1,. [sent-126, score-0.295]
</p><p>73 ,cNj) Xj  According to Equation 3, the computation of a single factor-to-variable message involves maximizing over 2n configurations. [sent-139, score-0.21]
</p><p>74 Tinthsis a drastically ere tdou −c∞es th foer n muomsbte cro nofconfigurations which can maximize the message values. [sent-141, score-0.243]
</p><p>75 Given this simple fact, Givoni and Frey (2009) show how to reduce the necessary update messages to only two types of scalar ones: availabilities (α) and responsibilities (ρ). [sent-142, score-0.293]
</p><p>76 Instead of sending two-valued messages (corresponding etoa dth oef ft sweon possible values of the binary variables), we can send the difference for the two possible configurations: γij = γij (1) − γij (0) effectively, a log-likelihood ratio. [sent-144, score-0.242]
</p><p>77 –  2Normally, each iteration of the algorithm sends five types of two-valued messages: to and from functions E and I and a message from functions S. [sent-145, score-0.235]
</p><p>78 Fortunately, the messages sent to and from E factors to the variable nodes subsume the three other message types and it is not necessary to compute them explicitly. [sent-146, score-0.573]
</p><p>79 Figure 3: Examples of valid configuration of hidden  variables {cij} for clustering and segmentation. [sent-149, score-0.184]
</p><p>80 (a) Clustering  (b) Segmentation  The algorithm converges when the set of points labelled as exemplars remains unchanged for a predetermined number of iterations. [sent-150, score-0.224]
</p><p>81 When the algorithm terminates, messages to each variable are added together. [sent-151, score-0.273]
</p><p>82 A positive final message indicates that the most likely value of a variable cij is 1 (point j is an exemplar for i), a negative message indicates that it is 0 (j is not an exemplar for i). [sent-152, score-1.005]
</p><p>83 4  Affinity Propagation for Segmentation  This section explains how we adapt the Affinity Propagation clustering algorithm to segmentation. [sent-153, score-0.106]
</p><p>84 In this setting, sentences are data points and we refer to exemplars as segment centres. [sent-154, score-0.49]
</p><p>85 Given a doc-  ument, we want to assign each sentence to a segment centre so as to maximize net similarity. [sent-155, score-0.508]
</p><p>86 The new formulation relies on the same underlying factor graph (Figure 2). [sent-156, score-0.159]
</p><p>87 A binary variable node cij is set to 1iff sentence j is the segment centre for sentence i. [sent-157, score-0.77]
</p><p>88 When clustering is the objective, a cluster may consist of points coming from anywhere in the data sequence. [sent-158, score-0.185]
</p><p>89 When segmentation is the objective, a segment must consist of a solid block of points around the segment centre. [sent-159, score-0.851]
</p><p>90 Figure 3 shows, for a toy problem with 5 data points, possible valid configurations of variables {cij } for clustering (3a) acondn figoru segmentation (3b). [sent-160, score-0.375]
</p><p>91 Case 1 is the original coherence ∞co ninst trhairneet. [sent-163, score-0.04]
</p><p>92 2 C astsaete 1s sth tahte no point kc may be in the segment with a centre is j, if k lies before the start of the segment (the sequence c(s−1)j = 0, csj = 1 necessarily corresponds to the start of the segment). [sent-165, score-0.709]
</p><p>93 288  The  E function nodes are the only changed part of the factor graph, so we only must re-derive α messages (availabilities) sent from factors E to variable nodes. [sent-167, score-0.527]
</p><p>94 A function-to-variable message is computed as shown in Equation 11 (elaborated Equation 3), and the only incoming messages to E nodes are responsibilities (ρ messages):  µf→x=N m(fa)\xx(f(x1,. [sent-168, score-0.518]
</p><p>95 ,cNj) +cijX, i6=jρij(cij))) We need to compute the message values for the two possible settings of binary variables denoted –  as  (1) and αij (0) and propagate the difference αij = αij(1) - αij(0). [sent-173, score-0.21]
</p><p>96 Consider the case of factor Ej sending an α message to the variable node cjj (i. [sent-174, score-0.535]
</p><p>97 If cjj = 0 then point j is not its own segment centre and the only valid configuration is to set all other cij to 0: αij  –  αjj(0) =ci mj,ai6=xj(Ej(c1j,. [sent-177, score-0.798]
</p><p>98 ,cNj) +ciXj,i6=jρij(cij)) = Xρij(0)  (12)  Xi6=j  To compute αij (1) (point j is its own segment centre), we only must maximize over configurations which will not correspond to cases 2 and 3 in Equation 10 (other assignments are trivially non-optimal because they would evaluate Ej to −∞). [sent-179, score-0.399]
</p><p>99 Let the start of a segment be s, 1 ≤ s < j −an∞d )th. [sent-180, score-0.29]
</p><p>100 e Leentd t hoef tshtaer segment gbme e, j + s1, < e ≤ N <. [sent-181, score-0.29]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('aps', 0.461), ('segment', 0.29), ('cij', 0.258), ('ij', 0.253), ('affinity', 0.242), ('ej', 0.199), ('segmentation', 0.19), ('message', 0.178), ('messages', 0.176), ('frey', 0.176), ('propagation', 0.159), ('exemplar', 0.159), ('givoni', 0.127), ('exemplars', 0.119), ('similarities', 0.109), ('factor', 0.1), ('graphs', 0.092), ('centre', 0.082), ('clustering', 0.082), ('points', 0.081), ('malioutov', 0.076), ('nodes', 0.075), ('topical', 0.074), ('variable', 0.073), ('sent', 0.071), ('net', 0.071), ('sim', 0.069), ('barzilay', 0.068), ('node', 0.067), ('informed', 0.066), ('sending', 0.066), ('maximize', 0.065), ('graph', 0.059), ('cyclic', 0.057), ('awa', 0.055), ('fh', 0.055), ('equation', 0.053), ('segmenter', 0.052), ('anka', 0.051), ('availabilities', 0.051), ('centres', 0.051), ('cjj', 0.051), ('dueck', 0.051), ('igiven', 0.051), ('neighbours', 0.051), ('point', 0.047), ('shifts', 0.047), ('incoming', 0.045), ('boundaries', 0.044), ('configurations', 0.044), ('electrical', 0.044), ('segmenters', 0.044), ('responsibilities', 0.044), ('configuration', 0.043), ('eisenstein', 0.043), ('reflects', 0.041), ('locally', 0.041), ('coherence', 0.04), ('xn', 0.039), ('segments', 0.038), ('xj', 0.038), ('sum', 0.038), ('topic', 0.036), ('belief', 0.036), ('globally', 0.036), ('cut', 0.035), ('outlines', 0.035), ('cpu', 0.035), ('xh', 0.035), ('ottawa', 0.035), ('iteration', 0.033), ('operates', 0.033), ('passing', 0.033), ('function', 0.032), ('maximizing', 0.032), ('variables', 0.032), ('matrix', 0.032), ('objective', 0.031), ('hearst', 0.031), ('similarity', 0.029), ('iteratively', 0.028), ('fa', 0.027), ('picture', 0.027), ('valid', 0.027), ('priori', 0.026), ('pairwise', 0.025), ('competitive', 0.025), ('segmenting', 0.025), ('algorithm', 0.024), ('realistic', 0.024), ('discusses', 0.023), ('guaranteed', 0.023), ('requirements', 0.023), ('blei', 0.023), ('preference', 0.023), ('cluster', 0.022), ('update', 0.022), ('evidence', 0.022), ('xx', 0.022), ('encodes', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="88-tfidf-1" href="./emnlp-2011-Linear_Text_Segmentation_Using_Affinity_Propagation.html">88 emnlp-2011-Linear Text Segmentation Using Affinity Propagation</a></p>
<p>Author: Anna Kazantseva ; Stan Szpakowicz</p><p>Abstract: This paper presents a new algorithm for linear text segmentation. It is an adaptation of Affinity Propagation, a state-of-the-art clustering algorithm in the framework of factor graphs. Affinity Propagation for Segmentation, or APS, receives a set of pairwise similarities between data points and produces segment boundaries and segment centres data points which best describe all other data points within the segment. APS iteratively passes messages in a cyclic factor graph, until convergence. Each iteration works with information on all available similarities, resulting in highquality results. APS scales linearly for realistic segmentation tasks. We derive the algorithm from the original Affinity Propagation formu– lation, and evaluate its performance on topical text segmentation in comparison with two state-of-the art segmenters. The results suggest that APS performs on par with or outperforms these two very competitive baselines.</p><p>2 0.1145964 <a title="88-tfidf-2" href="./emnlp-2011-Non-parametric_Bayesian_Segmentation_of_Japanese_Noun_Phrases.html">99 emnlp-2011-Non-parametric Bayesian Segmentation of Japanese Noun Phrases</a></p>
<p>Author: Yugo Murawaki ; Sadao Kurohashi</p><p>Abstract: A key factor of high quality word segmentation for Japanese is a high-coverage dictionary, but it is costly to manually build such a lexical resource. Although external lexical resources for human readers are potentially good knowledge sources, they have not been utilized due to differences in segmentation criteria. To supplement a morphological dictionary with these resources, we propose a new task of Japanese noun phrase segmentation. We apply non-parametric Bayesian language models to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text. For inference, we propose a novel block sampling procedure named hybrid type-based sampling, which has the ability to directly escape a local optimum that is not too distant from the global optimum. Experiments show that the proposed method efficiently corrects the initial segmentation given by a morphological ana- lyzer.</p><p>3 0.084008865 <a title="88-tfidf-3" href="./emnlp-2011-Enhancing_Chinese_Word_Segmentation_Using_Unlabeled_Data.html">48 emnlp-2011-Enhancing Chinese Word Segmentation Using Unlabeled Data</a></p>
<p>Author: Weiwei Sun ; Jia Xu</p><p>Abstract: This paper investigates improving supervised word segmentation accuracy with unlabeled data. Both large-scale in-domain data and small-scale document text are considered. We present a unified solution to include features derived from unlabeled data to a discriminative learning model. For the large-scale data, we derive string statistics from Gigaword to assist a character-based segmenter. In addition, we introduce the idea about transductive, document-level segmentation, which is designed to improve the system recall for out-ofvocabulary (OOV) words which appear more than once inside a document. Novel features1 result in relative error reductions of 13.8% and 15.4% in terms of F-score and the recall of OOV words respectively.</p><p>4 0.071548074 <a title="88-tfidf-4" href="./emnlp-2011-Personalized_Recommendation_of_User_Comments_via_Factor_Models.html">104 emnlp-2011-Personalized Recommendation of User Comments via Factor Models</a></p>
<p>Author: Deepak Agarwal ; Bee-Chung Chen ; Bo Pang</p><p>Abstract: In recent years, the amount of user-generated opinionated texts (e.g., reviews, user comments) continues to grow at a rapid speed: featured news stories on a major event easily attract thousands of user comments on a popular online News service. How to consume subjective information ofthis volume becomes an interesting and important research question. In contrast to previous work on review analysis that tried to filter or summarize information for a generic average user, we explore a different direction of enabling personalized recommendation of such information. For each user, our task is to rank the comments associated with a given article according to personalized user preference (i.e., whether the user is likely to like or dislike the comment). To this end, we propose a factor model that incorporates rater-comment and rater-author interactions simultaneously in a principled way. Our full model significantly outperforms strong baselines as well as related models that have been considered in previous work.</p><p>5 0.071212068 <a title="88-tfidf-5" href="./emnlp-2011-A_Correction_Model_for_Word_Alignments.html">3 emnlp-2011-A Correction Model for Word Alignments</a></p>
<p>Author: J. Scott McCarley ; Abraham Ittycheriah ; Salim Roukos ; Bing Xiang ; Jian-ming Xu</p><p>Abstract: Models of word alignment built as sequences of links have limited expressive power, but are easy to decode. Word aligners that model the alignment matrix can express arbitrary alignments, but are difficult to decode. We propose an alignment matrix model as a correction algorithm to an underlying sequencebased aligner. Then a greedy decoding algorithm enables the full expressive power of the alignment matrix formulation. Improved alignment performance is shown for all nine language pairs tested. The improved alignments also improved translation quality from Chinese to English and English to Italian.</p><p>6 0.065933205 <a title="88-tfidf-6" href="./emnlp-2011-Minimally_Supervised_Event_Causality_Identification.html">92 emnlp-2011-Minimally Supervised Event Causality Identification</a></p>
<p>7 0.062813625 <a title="88-tfidf-7" href="./emnlp-2011-Unsupervised_Semantic_Role_Induction_with_Graph_Partitioning.html">145 emnlp-2011-Unsupervised Semantic Role Induction with Graph Partitioning</a></p>
<p>8 0.055772685 <a title="88-tfidf-8" href="./emnlp-2011-Multilayer_Sequence_Labeling.html">96 emnlp-2011-Multilayer Sequence Labeling</a></p>
<p>9 0.04712452 <a title="88-tfidf-9" href="./emnlp-2011-Class_Label_Enhancement_via_Related_Instances.html">26 emnlp-2011-Class Label Enhancement via Related Instances</a></p>
<p>10 0.046196789 <a title="88-tfidf-10" href="./emnlp-2011-Hierarchical_Verb_Clustering_Using_Graph_Factorization.html">67 emnlp-2011-Hierarchical Verb Clustering Using Graph Factorization</a></p>
<p>11 0.045414537 <a title="88-tfidf-11" href="./emnlp-2011-Classifying_Sentences_as_Speech_Acts_in_Message_Board_Posts.html">27 emnlp-2011-Classifying Sentences as Speech Acts in Message Board Posts</a></p>
<p>12 0.044824108 <a title="88-tfidf-12" href="./emnlp-2011-Optimizing_Semantic_Coherence_in_Topic_Models.html">101 emnlp-2011-Optimizing Semantic Coherence in Topic Models</a></p>
<p>13 0.044725668 <a title="88-tfidf-13" href="./emnlp-2011-Timeline_Generation_through_Evolutionary_Trans-Temporal_Summarization.html">135 emnlp-2011-Timeline Generation through Evolutionary Trans-Temporal Summarization</a></p>
<p>14 0.043208316 <a title="88-tfidf-14" href="./emnlp-2011-Robust_Disambiguation_of_Named_Entities_in_Text.html">116 emnlp-2011-Robust Disambiguation of Named Entities in Text</a></p>
<p>15 0.040550958 <a title="88-tfidf-15" href="./emnlp-2011-Semantic_Topic_Models%3A_Combining_Word_Distributional_Statistics_and_Dictionary_Definitions.html">119 emnlp-2011-Semantic Topic Models: Combining Word Distributional Statistics and Dictionary Definitions</a></p>
<p>16 0.039884329 <a title="88-tfidf-16" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>17 0.038228963 <a title="88-tfidf-17" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>18 0.037882015 <a title="88-tfidf-18" href="./emnlp-2011-Generating_Aspect-oriented_Multi-Document_Summarization_with_Event-aspect_model.html">61 emnlp-2011-Generating Aspect-oriented Multi-Document Summarization with Event-aspect model</a></p>
<p>19 0.035169415 <a title="88-tfidf-19" href="./emnlp-2011-Experimental_Support_for_a_Categorical_Compositional_Distributional_Model_of_Meaning.html">53 emnlp-2011-Experimental Support for a Categorical Compositional Distributional Model of Meaning</a></p>
<p>20 0.034000404 <a title="88-tfidf-20" href="./emnlp-2011-Named_Entity_Recognition_in_Tweets%3A_An_Experimental_Study.html">98 emnlp-2011-Named Entity Recognition in Tweets: An Experimental Study</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/emnlp2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.133), (1, -0.055), (2, -0.045), (3, -0.036), (4, 0.011), (5, 0.013), (6, -0.03), (7, -0.06), (8, -0.105), (9, -0.021), (10, -0.031), (11, -0.011), (12, -0.015), (13, 0.028), (14, -0.06), (15, -0.047), (16, -0.153), (17, 0.091), (18, 0.201), (19, 0.115), (20, 0.059), (21, 0.014), (22, -0.166), (23, 0.029), (24, 0.021), (25, -0.05), (26, -0.127), (27, 0.044), (28, 0.076), (29, 0.144), (30, 0.123), (31, 0.043), (32, 0.073), (33, 0.008), (34, 0.04), (35, 0.016), (36, -0.09), (37, -0.085), (38, -0.031), (39, -0.206), (40, -0.146), (41, 0.122), (42, 0.158), (43, 0.116), (44, 0.236), (45, 0.226), (46, 0.005), (47, 0.055), (48, -0.232), (49, -0.104)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9806754 <a title="88-lsi-1" href="./emnlp-2011-Linear_Text_Segmentation_Using_Affinity_Propagation.html">88 emnlp-2011-Linear Text Segmentation Using Affinity Propagation</a></p>
<p>Author: Anna Kazantseva ; Stan Szpakowicz</p><p>Abstract: This paper presents a new algorithm for linear text segmentation. It is an adaptation of Affinity Propagation, a state-of-the-art clustering algorithm in the framework of factor graphs. Affinity Propagation for Segmentation, or APS, receives a set of pairwise similarities between data points and produces segment boundaries and segment centres data points which best describe all other data points within the segment. APS iteratively passes messages in a cyclic factor graph, until convergence. Each iteration works with information on all available similarities, resulting in highquality results. APS scales linearly for realistic segmentation tasks. We derive the algorithm from the original Affinity Propagation formu– lation, and evaluate its performance on topical text segmentation in comparison with two state-of-the art segmenters. The results suggest that APS performs on par with or outperforms these two very competitive baselines.</p><p>2 0.4530305 <a title="88-lsi-2" href="./emnlp-2011-Non-parametric_Bayesian_Segmentation_of_Japanese_Noun_Phrases.html">99 emnlp-2011-Non-parametric Bayesian Segmentation of Japanese Noun Phrases</a></p>
<p>Author: Yugo Murawaki ; Sadao Kurohashi</p><p>Abstract: A key factor of high quality word segmentation for Japanese is a high-coverage dictionary, but it is costly to manually build such a lexical resource. Although external lexical resources for human readers are potentially good knowledge sources, they have not been utilized due to differences in segmentation criteria. To supplement a morphological dictionary with these resources, we propose a new task of Japanese noun phrase segmentation. We apply non-parametric Bayesian language models to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text. For inference, we propose a novel block sampling procedure named hybrid type-based sampling, which has the ability to directly escape a local optimum that is not too distant from the global optimum. Experiments show that the proposed method efficiently corrects the initial segmentation given by a morphological ana- lyzer.</p><p>3 0.42873776 <a title="88-lsi-3" href="./emnlp-2011-Enhancing_Chinese_Word_Segmentation_Using_Unlabeled_Data.html">48 emnlp-2011-Enhancing Chinese Word Segmentation Using Unlabeled Data</a></p>
<p>Author: Weiwei Sun ; Jia Xu</p><p>Abstract: This paper investigates improving supervised word segmentation accuracy with unlabeled data. Both large-scale in-domain data and small-scale document text are considered. We present a unified solution to include features derived from unlabeled data to a discriminative learning model. For the large-scale data, we derive string statistics from Gigaword to assist a character-based segmenter. In addition, we introduce the idea about transductive, document-level segmentation, which is designed to improve the system recall for out-ofvocabulary (OOV) words which appear more than once inside a document. Novel features1 result in relative error reductions of 13.8% and 15.4% in terms of F-score and the recall of OOV words respectively.</p><p>4 0.3633644 <a title="88-lsi-4" href="./emnlp-2011-Class_Label_Enhancement_via_Related_Instances.html">26 emnlp-2011-Class Label Enhancement via Related Instances</a></p>
<p>Author: Zornitsa Kozareva ; Konstantin Voevodski ; Shanghua Teng</p><p>Abstract: Class-instance label propagation algorithms have been successfully used to fuse information from multiple sources in order to enrich a set of unlabeled instances with class labels. Yet, nobody has explored the relationships between the instances themselves to enhance an initial set of class-instance pairs. We propose two graph-theoretic methods (centrality and regularization), which start with a small set of labeled class-instance pairs and use the instance-instance network to extend the class labels to all instances in the network. We carry out a comparative study with state-of-the-art knowledge harvesting algorithm and show that our approach can learn additional class labels while maintaining high accuracy. We conduct a comparative study between class-instance and instance-instance graphs used to propagate the class labels and show that the latter one achieves higher accuracy.</p><p>5 0.29663649 <a title="88-lsi-5" href="./emnlp-2011-A_Correction_Model_for_Word_Alignments.html">3 emnlp-2011-A Correction Model for Word Alignments</a></p>
<p>Author: J. Scott McCarley ; Abraham Ittycheriah ; Salim Roukos ; Bing Xiang ; Jian-ming Xu</p><p>Abstract: Models of word alignment built as sequences of links have limited expressive power, but are easy to decode. Word aligners that model the alignment matrix can express arbitrary alignments, but are difficult to decode. We propose an alignment matrix model as a correction algorithm to an underlying sequencebased aligner. Then a greedy decoding algorithm enables the full expressive power of the alignment matrix formulation. Improved alignment performance is shown for all nine language pairs tested. The improved alignments also improved translation quality from Chinese to English and English to Italian.</p><p>6 0.28031626 <a title="88-lsi-6" href="./emnlp-2011-Hierarchical_Verb_Clustering_Using_Graph_Factorization.html">67 emnlp-2011-Hierarchical Verb Clustering Using Graph Factorization</a></p>
<p>7 0.2662783 <a title="88-lsi-7" href="./emnlp-2011-Multilayer_Sequence_Labeling.html">96 emnlp-2011-Multilayer Sequence Labeling</a></p>
<p>8 0.24150378 <a title="88-lsi-8" href="./emnlp-2011-Personalized_Recommendation_of_User_Comments_via_Factor_Models.html">104 emnlp-2011-Personalized Recommendation of User Comments via Factor Models</a></p>
<p>9 0.21776578 <a title="88-lsi-9" href="./emnlp-2011-Unsupervised_Semantic_Role_Induction_with_Graph_Partitioning.html">145 emnlp-2011-Unsupervised Semantic Role Induction with Graph Partitioning</a></p>
<p>10 0.20871013 <a title="88-lsi-10" href="./emnlp-2011-Learning_to_Simplify_Sentences_with_Quasi-Synchronous_Grammar_and_Integer_Programming.html">85 emnlp-2011-Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming</a></p>
<p>11 0.19085398 <a title="88-lsi-11" href="./emnlp-2011-Minimally_Supervised_Event_Causality_Identification.html">92 emnlp-2011-Minimally Supervised Event Causality Identification</a></p>
<p>12 0.18564026 <a title="88-lsi-12" href="./emnlp-2011-Unsupervised_Information_Extraction_with_Distributional_Prior_Knowledge.html">143 emnlp-2011-Unsupervised Information Extraction with Distributional Prior Knowledge</a></p>
<p>13 0.18077224 <a title="88-lsi-13" href="./emnlp-2011-Harnessing_different_knowledge_sources_to_measure_semantic_relatedness_under_a_uniform_model.html">64 emnlp-2011-Harnessing different knowledge sources to measure semantic relatedness under a uniform model</a></p>
<p>14 0.17560858 <a title="88-lsi-14" href="./emnlp-2011-Classifying_Sentences_as_Speech_Acts_in_Message_Board_Posts.html">27 emnlp-2011-Classifying Sentences as Speech Acts in Message Board Posts</a></p>
<p>15 0.17055595 <a title="88-lsi-15" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>16 0.16952439 <a title="88-lsi-16" href="./emnlp-2011-Refining_the_Notions_of_Depth_and_Density_in_WordNet-based_Semantic_Similarity_Measures.html">112 emnlp-2011-Refining the Notions of Depth and Density in WordNet-based Semantic Similarity Measures</a></p>
<p>17 0.16938265 <a title="88-lsi-17" href="./emnlp-2011-Efficient_Subsampling_for_Training_Complex_Language_Models.html">46 emnlp-2011-Efficient Subsampling for Training Complex Language Models</a></p>
<p>18 0.16902597 <a title="88-lsi-18" href="./emnlp-2011-Lateen_EM%3A_Unsupervised_Training_with_Multiple_Objectives%2C_Applied_to_Dependency_Grammar_Induction.html">79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</a></p>
<p>19 0.16065159 <a title="88-lsi-19" href="./emnlp-2011-Timeline_Generation_through_Evolutionary_Trans-Temporal_Summarization.html">135 emnlp-2011-Timeline Generation through Evolutionary Trans-Temporal Summarization</a></p>
<p>20 0.1592636 <a title="88-lsi-20" href="./emnlp-2011-The_Imagination_of_Crowds%3A_Conversational_AAC_Language_Modeling_using_Crowdsourcing_and_Large_Data_Sources.html">133 emnlp-2011-The Imagination of Crowds: Conversational AAC Language Modeling using Crowdsourcing and Large Data Sources</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/emnlp2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(15, 0.011), (23, 0.115), (32, 0.348), (36, 0.059), (37, 0.026), (45, 0.045), (54, 0.014), (57, 0.01), (62, 0.032), (64, 0.024), (66, 0.036), (69, 0.014), (79, 0.03), (82, 0.029), (90, 0.02), (96, 0.055), (98, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80669975 <a title="88-lda-1" href="./emnlp-2011-Linear_Text_Segmentation_Using_Affinity_Propagation.html">88 emnlp-2011-Linear Text Segmentation Using Affinity Propagation</a></p>
<p>Author: Anna Kazantseva ; Stan Szpakowicz</p><p>Abstract: This paper presents a new algorithm for linear text segmentation. It is an adaptation of Affinity Propagation, a state-of-the-art clustering algorithm in the framework of factor graphs. Affinity Propagation for Segmentation, or APS, receives a set of pairwise similarities between data points and produces segment boundaries and segment centres data points which best describe all other data points within the segment. APS iteratively passes messages in a cyclic factor graph, until convergence. Each iteration works with information on all available similarities, resulting in highquality results. APS scales linearly for realistic segmentation tasks. We derive the algorithm from the original Affinity Propagation formu– lation, and evaluate its performance on topical text segmentation in comparison with two state-of-the art segmenters. The results suggest that APS performs on par with or outperforms these two very competitive baselines.</p><p>2 0.62998462 <a title="88-lda-2" href="./emnlp-2011-Active_Learning_with_Amazon_Mechanical_Turk.html">17 emnlp-2011-Active Learning with Amazon Mechanical Turk</a></p>
<p>Author: Florian Laws ; Christian Scheible ; Hinrich Schutze</p><p>Abstract: Supervised classification needs large amounts of annotated training data that is expensive to create. Two approaches that reduce the cost of annotation are active learning and crowdsourcing. However, these two approaches have not been combined successfully to date. We evaluate the utility of active learning in crowdsourcing on two tasks, named entity recognition and sentiment detection, and show that active learning outperforms random selection of annotation examples in a noisy crowdsourcing scenario.</p><p>3 0.40932634 <a title="88-lda-3" href="./emnlp-2011-Quasi-Synchronous_Phrase_Dependency_Grammars_for_Machine_Translation.html">108 emnlp-2011-Quasi-Synchronous Phrase Dependency Grammars for Machine Translation</a></p>
<p>Author: Kevin Gimpel ; Noah A. Smith</p><p>Abstract: We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.</p><p>4 0.40272132 <a title="88-lda-4" href="./emnlp-2011-Training_dependency_parsers_by_jointly_optimizing_multiple_objectives.html">137 emnlp-2011-Training dependency parsers by jointly optimizing multiple objectives</a></p>
<p>Author: Keith Hall ; Ryan McDonald ; Jason Katz-Brown ; Michael Ringgaard</p><p>Abstract: We present an online learning algorithm for training parsers which allows for the inclusion of multiple objective functions. The primary example is the extension of a standard supervised parsing objective function with additional loss-functions, either based on intrinsic parsing quality or task-specific extrinsic measures of quality. Our empirical results show how this approach performs for two dependency parsing algorithms (graph-based and transition-based parsing) and how it achieves increased performance on multiple target tasks including reordering for machine translation and parser adaptation.</p><p>5 0.40083483 <a title="88-lda-5" href="./emnlp-2011-Tuning_as_Ranking.html">138 emnlp-2011-Tuning as Ranking</a></p>
<p>Author: Mark Hopkins ; Jonathan May</p><p>Abstract: We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chiang et al., 2008b), PRO is easy to implement. It uses off-the-shelf linear binary classifier software and can be built on top of an existing MERT framework in a matter of hours. We establish PRO’s scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.</p><p>6 0.3992725 <a title="88-lda-6" href="./emnlp-2011-Fast_and_Robust_Joint_Models_for_Biomedical_Event_Extraction.html">59 emnlp-2011-Fast and Robust Joint Models for Biomedical Event Extraction</a></p>
<p>7 0.39919645 <a title="88-lda-7" href="./emnlp-2011-Structural_Opinion_Mining_for_Graph-based_Sentiment_Representation.html">126 emnlp-2011-Structural Opinion Mining for Graph-based Sentiment Representation</a></p>
<p>8 0.39874992 <a title="88-lda-8" href="./emnlp-2011-Training_a_Parser_for_Machine_Translation_Reordering.html">136 emnlp-2011-Training a Parser for Machine Translation Reordering</a></p>
<p>9 0.39743862 <a title="88-lda-9" href="./emnlp-2011-Identification_of_Multi-word_Expressions_by_Combining_Multiple_Linguistic_Information_Sources.html">69 emnlp-2011-Identification of Multi-word Expressions by Combining Multiple Linguistic Information Sources</a></p>
<p>10 0.39524943 <a title="88-lda-10" href="./emnlp-2011-A_Bayesian_Mixture_Model_for_PoS_Induction_Using_Multiple_Features.html">1 emnlp-2011-A Bayesian Mixture Model for PoS Induction Using Multiple Features</a></p>
<p>11 0.3928934 <a title="88-lda-11" href="./emnlp-2011-Closing_the_Loop%3A_Fast%2C_Interactive_Semi-Supervised_Annotation_With_Queries_on_Features_and_Instances.html">28 emnlp-2011-Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances</a></p>
<p>12 0.39227584 <a title="88-lda-12" href="./emnlp-2011-Lateen_EM%3A_Unsupervised_Training_with_Multiple_Objectives%2C_Applied_to_Dependency_Grammar_Induction.html">79 emnlp-2011-Lateen EM: Unsupervised Training with Multiple Objectives, Applied to Dependency Grammar Induction</a></p>
<p>13 0.3915855 <a title="88-lda-13" href="./emnlp-2011-Hypotheses_Selection_Criteria_in_a_Reranking_Framework_for_Spoken_Language_Understanding.html">68 emnlp-2011-Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding</a></p>
<p>14 0.3907097 <a title="88-lda-14" href="./emnlp-2011-A_Generate_and_Rank_Approach_to_Sentence_Paraphrasing.html">6 emnlp-2011-A Generate and Rank Approach to Sentence Paraphrasing</a></p>
<p>15 0.38990593 <a title="88-lda-15" href="./emnlp-2011-Third-order_Variational_Reranking_on_Packed-Shared_Dependency_Forests.html">134 emnlp-2011-Third-order Variational Reranking on Packed-Shared Dependency Forests</a></p>
<p>16 0.38892379 <a title="88-lda-16" href="./emnlp-2011-Syntax-Based_Grammaticality_Improvement_using_CCG_and_Guided_Search.html">132 emnlp-2011-Syntax-Based Grammaticality Improvement using CCG and Guided Search</a></p>
<p>17 0.38742146 <a title="88-lda-17" href="./emnlp-2011-Heuristic_Search_for_Non-Bottom-Up_Tree_Structure_Prediction.html">65 emnlp-2011-Heuristic Search for Non-Bottom-Up Tree Structure Prediction</a></p>
<p>18 0.38715726 <a title="88-lda-18" href="./emnlp-2011-Structured_Relation_Discovery_using_Generative_Models.html">128 emnlp-2011-Structured Relation Discovery using Generative Models</a></p>
<p>19 0.38632584 <a title="88-lda-19" href="./emnlp-2011-Fast_Generation_of_Translation_Forest_for_Large-Scale_SMT_Discriminative_Training.html">58 emnlp-2011-Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training</a></p>
<p>20 0.38584334 <a title="88-lda-20" href="./emnlp-2011-Identifying_Relations_for_Open_Information_Extraction.html">70 emnlp-2011-Identifying Relations for Open Information Extraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
