<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>78 emnlp-2013-Exploiting Language Models for Visual Recognition</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-78" href="../emnlp2013/emnlp-2013-Exploiting_Language_Models_for_Visual_Recognition.html">emnlp2013-78</a> <a title="emnlp-2013-78-reference" href="#">emnlp2013-78-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>78 emnlp-2013-Exploiting Language Models for Visual Recognition</h1>
<br/><p>Source: <a title="emnlp-2013-78-pdf" href="http://aclweb.org/anthology//D/D13/D13-1072.pdf">pdf</a></p><p>Author: Dieu-Thu Le ; Jasper Uijlings ; Raffaella Bernardi</p><p>Abstract: The problem of learning language models from large text corpora has been widely studied within the computational linguistic community. However, little is known about the performance of these language models when applied to the computer vision domain. In this work, we compare representative models: a window-based model, a topic model, a distributional memory and a commonsense knowledge database, ConceptNet, in two visual recognition scenarios: human action recognition and object prediction. We examine whether the knowledge extracted from texts through these models are compatible to the knowledge represented in images. We determine the usefulness of different language models in aiding the two visual recognition tasks. The study shows that the language models built from general text corpora can be used instead of expensive annotated images and even outperform the image model when testing on a big general dataset.</p><br/>
<h2>reference text</h2><br/>
<br/><br/><br/></body>
</html>
