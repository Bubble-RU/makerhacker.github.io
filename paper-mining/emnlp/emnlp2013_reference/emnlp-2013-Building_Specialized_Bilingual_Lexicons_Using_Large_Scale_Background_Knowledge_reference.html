<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>42 emnlp-2013-Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-42" href="../emnlp2013/emnlp-2013-Building_Specialized_Bilingual_Lexicons_Using_Large_Scale_Background_Knowledge.html">emnlp2013-42</a> <a title="emnlp-2013-42-reference" href="#">emnlp2013-42-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>42 emnlp-2013-Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge</h1>
<br/><p>Source: <a title="emnlp-2013-42-pdf" href="http://aclweb.org/anthology//D/D13/D13-1046.pdf">pdf</a></p><p>Author: Dhouha Bouamor ; Adrian Popescu ; Nasredine Semmar ; Pierre Zweigenbaum</p><p>Abstract: Bilingual lexicons are central components of machine translation and cross-lingual information retrieval systems. Their manual construction requires strong expertise in both languages involved and is a costly process. Several automatic methods were proposed as an alternative but they often rely on resources available in a limited number of languages and their performances are still far behind the quality of manual translations. We introduce a novel approach to the creation of specific domain bilingual lexicon that relies on Wikipedia. This massively multilingual encyclopedia makes it possible to create lexicons for a large number of language pairs. Wikipedia is used to extract domains in each language, to link domains between languages and to create generic translation dictionaries. The approach is tested on four specialized domains and is compared to three state of the art approaches using two language pairs: FrenchEnglish and Romanian-English. The newly introduced method compares favorably to existing methods in all configurations tested.</p><br/>
<h2>reference text</h2><p>Dhouha Bouamor, Nasredine Semmar, and Pierre Zweigenbaum. 2013. Context vector disambiguation for bilingual lexicon extraction. In Proceedings of the 51st Association for Computational Linguistics (ACLHLT), Sofia, Bulgaria, August. Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.  Looking for candidate translational equivalents in specialized, comparable corpora. In Proceedings of the 19th international conference on Computational linguistics - Volume 2, COLING ’02, pages 1–5. Association for Computational Linguistics. Yun-Chuang Chiao and Pierre Zweigenbaum. 2003. The effect of a general lexicon in corpus-based identification of french-english medical word translations. In Proceedings Medical Informatics Europe, volume 95 of Studies in Health Technology and Informatics, pages 397–402, Amsterdam. Pascale Fung. 1998. A statistical view on bilingual lexicon extraction: From parallel corpora to non-parallel corpora. In Parallel Text Processing, pages 1–17. Springer. Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipedia-based explicit semantic analysis. In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI’07, pages 1606–161 1, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc. Alexander Halavais and Derek Lackaff. 2008. An Analysis of Topical Coverage of Wikipedia. Journal of Computer-Mediated Communication, 13(2):429–440. Z.S. Harris. 1954. Distributional structure. Word. Samer Hassan and Rada Mihalcea. 2011. Semantic relatedness using salient semantic analysis. In AAAI.  Amir Hazem and Emmanuel Morin. 2012. Adaptive dictionary for bilingual lexicon extraction from comparable corpora. In Proceedings, 8th international conference on Language Resources and Evaluation (LREC),  Istanbul, Turkey, May. Azniah Ismail and Suresh Manandhar. 2010. Bilingual lexicon extraction from comparable corpora using in-domain terms. In Proceedings of the 23rd International Conference on Computational Linguistics:  ’ 10, pages Linguistics.  Posters, COLING Computational  481–489.  Audrey Laroche and Philippe Langlais. 2010. Revisiting context-based projection methods for term-translation spotting in comparable corpora. In 23rd International Conference on Computational Linguistics (Coling 2010), pages 617–625, Beijing, China, Aug. Emmanuel Morin and B ´eatrice Daille. 2006. Comparabilit ´e de corpus et fouille terminologique multilingue. In Traitement Automatique des Langues (TAL). Emmanuel Morin and Emmanuel Prochasson. 2011. Bilingual lexicon extraction from comparable corpora enhanced with parallel corpora. In Proceedings, 4th Workshop on Building and Using Comparable Corpora (BUCC), page 27–34, Portland, Oregon, USA. Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Comput. Linguist., 29(1): 19–5 1, March. Emmanuel Prochasson, Emmanuel Morin, and Kyo Kageura. 2009. Anchor points for bilingual lexicon extraction from small comparable corpora. In Proceedings, 12th Conference on Machine Translation Summit (MT Summit XII), page 284–291, Ottawa, Ontario, Canada. Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and Shaul Markovitch. 2011. A word at a time: computing word relatedness using temporal semantic analysis. In Proceedings of the 20th international conference on World wide web, WWW ’ 11, pages 337–346, New York, NY, USA. ACM. Reinhard Rapp. 1995. Identifying word translations in non-parallel texts. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, ACL ’95, pages 320–322. Association for Computational Linguistics. Reinhard Rapp. 1999. Automatic identification of word translations from unrelated english and german corpora. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99, pages 5 19–526. Association for Computational Linguistics.  Association for  P. Sorg and P. Cimiano. 2012. Exploiting wikipedia for cross-lingual and multilingual information retrieval. Data Knowl. Eng. , 74:26–45, April. 489</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
