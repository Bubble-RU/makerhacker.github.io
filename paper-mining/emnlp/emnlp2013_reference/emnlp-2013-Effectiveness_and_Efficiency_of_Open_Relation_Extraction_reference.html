<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>68 emnlp-2013-Effectiveness and Efficiency of Open Relation Extraction</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-68" href="../emnlp2013/emnlp-2013-Effectiveness_and_Efficiency_of_Open_Relation_Extraction.html">emnlp2013-68</a> <a title="emnlp-2013-68-reference" href="#">emnlp2013-68-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>68 emnlp-2013-Effectiveness and Efficiency of Open Relation Extraction</h1>
<br/><p>Source: <a title="emnlp-2013-68-pdf" href="http://aclweb.org/anthology//D/D13/D13-1043.pdf">pdf</a></p><p>Author: Filipe Mesquita ; Jordan Schmidek ; Denilson Barbosa</p><p>Abstract: A large number of Open Relation Extraction approaches have been proposed recently, covering a wide range of NLP machinery, from “shallow” (e.g., part-of-speech tagging) to “deep” (e.g., semantic role labeling–SRL). A natural question then is what is the tradeoff between NLP depth (and associated computational cost) versus effectiveness. This paper presents a fair and objective experimental comparison of 8 state-of-the-art approaches over 5 different datasets, and sheds some light on the issue. The paper also describes a novel method, EXEMPLAR, which adapts ideas from SRL to less costly NLP machinery, resulting in substantial gains both in efficiency and effectiveness, over binary and n-ary relation extraction tasks.</p><br/>
<h2>reference text</h2><br/>
<br/><br/><br/></body>
</html>
