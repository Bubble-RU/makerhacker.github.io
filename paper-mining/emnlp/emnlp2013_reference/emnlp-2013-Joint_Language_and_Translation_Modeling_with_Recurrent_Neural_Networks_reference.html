<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>113 emnlp-2013-Joint Language and Translation Modeling with Recurrent Neural Networks</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-113" href="../emnlp2013/emnlp-2013-Joint_Language_and_Translation_Modeling_with_Recurrent_Neural_Networks.html">emnlp2013-113</a> <a title="emnlp-2013-113-reference" href="#">emnlp2013-113-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>113 emnlp-2013-Joint Language and Translation Modeling with Recurrent Neural Networks</h1>
<br/><p>Source: <a title="emnlp-2013-113-pdf" href="http://aclweb.org/anthology//D/D13/D13-1106.pdf">pdf</a></p><p>Author: Michael Auli ; Michel Galley ; Chris Quirk ; Geoffrey Zweig</p><p>Abstract: We present a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words. The weaker independence assumptions of this model result in a vastly larger search space compared to related feedforward-based language or translation models. We tackle this issue with a new lattice rescoring algorithm and demonstrate its effectiveness empirically. Our joint model builds on a well known recurrent neural network language model (Mikolov, 2012) augmented by a layer of additional inputs from the source language. We show competitive accuracy compared to the traditional channel model features. Our best results improve the output of a system trained on WMT 2012 French-English data by up to 1.5 BLEU, and by 1.1BLEU on average across several test sets.</p><br/>
<h2>reference text</h2><br/>
<br/><br/><br/></body>
</html>
