<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>52 emnlp-2013-Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-52" href="../emnlp2013/emnlp-2013-Converting_Continuous-Space_Language_Models_into_N-Gram_Language_Models_for_Statistical_Machine_Translation.html">emnlp2013-52</a> <a title="emnlp-2013-52-reference" href="#">emnlp2013-52-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>52 emnlp-2013-Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation</h1>
<br/><p>Source: <a title="emnlp-2013-52-pdf" href="http://aclweb.org/anthology//D/D13/D13-1082.pdf">pdf</a></p><p>Author: Rui Wang ; Masao Utiyama ; Isao Goto ; Eiichro Sumita ; Hai Zhao ; Bao-Liang Lu</p><p>Abstract: Neural network language models, or continuous-space language models (CSLMs), have been shown to improve the performance of statistical machine translation (SMT) when they are used for reranking n-best translations. However, CSLMs have not been used in the first pass decoding of SMT, because using CSLMs in decoding takes a lot of time. In contrast, we propose a method for converting CSLMs into back-off n-gram language models (BNLMs) so that we can use converted CSLMs in decoding. We show that they outperform the original BNLMs and are comparable with the traditional use of CSLMs in reranking.</p><br/>
<h2>reference text</h2><p>Ebru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran, and Abhinav Sethy. 2013. Converting neural network language models into back-off language models for efficient decoding in automatic speech recognition. In Proc. of IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP 2013), Vancouver, Canada, May. IEEE. Yoshua Bengio, R ´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research (JMLR), 3: 1137–1 155, March. Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, ACL ’96, pages 3 10–318, Santa Cruz, California, June. Association for Computational Linguistics. Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical report, Computer Science Group, Harvard Univ. A. Deoras, T. Mikolov, S. Kombrink, M. Karafiat, and Sanjeev Khudanpur. 2011. Variational approximation of long-span language models for lvcsr. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 5532– 5535, Prague, Czech Republic, May. IEEE.  Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and Takehito Utsuro. 2010. Overview of the patent translation task at the ntcir-8 workshop. In In Proceedings of the 8th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, QuestionAnswering and Crosslingual Information Access, pages 293–302, Tokyo, Japan, June. 849 Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and Benjamin K. Tsou. 2011. Overview of the patent machine translation task at the NTCIR-9 workshop. In Proceedings of NTCIR-9 Workshop Meeting, pages 559–578, Tokyo, Japan, December. Zhongqiang Huang, Jacob Devlin, and Spyros Matsoukas. 2013. Bbn’s systems for the chineseenglish sub-task of the ntcir-10 patentmt evaluation. In NTCIR-10, Tokyo, Japan, June. Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 48–54, Edmonton, Canada. Association for Computational Linguistics.  Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 388–395, Barcelona, Spain, July. Association for Computational Linguistics. Hai-Son Le, I. Oparin, A. Allauzen, J. Gauvain, and F. Yvon. 2011. Structured output layer neural network language model. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 5524–5527, Prague, Czech Republic, May. IEEE. Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget, and Jan Cernock. 2011. Strategies for training large scale neural network language models. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 196– 201, Prague, Czech Republic, May. IEEE. Jan Niehues and Alex Waibel. 2012. Continuous space language models using restricted boltzmann machines. In Proceedings of the International Workshop for Spoken Language Translation, IWSLT 2012, pages 3 11–3 18, Hong Kong. Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1): 19–5 1, March. Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In  Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan, July. Association for Computational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 3 11– 3 18, Philadelphia, Pennsylvania, June. Association for Computational Linguistics. Holger Schwenk, Daniel Dchelotte, and Jean-Luc Gauvain. 2006. Continuous space language models for statistical machine translation. In Proceedings of the COLING/ACL on Main conference poster sessions, COLING-ACL ’06, pages 723–730, Sydney, Australia, July. Association for Computational Linguistics. Holger Schwenk, Anthony Rousseau, and Mohammed Attik. 2012. Large, pruned or continuous space language models on a gpu for statistical machine translation. In Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future ofLanguage ModelingforHLT, WLM ’ 12, pages 11–19, Montreal, Canada, June. Association for Computational Linguistics. Holger Schwenk. 2007. Continuous space language models. Computer Speech and Language, 21(3):492– 518. Holger Schwenk. 2010. Continuous-space language models for statistical machine translation. The Prague Bulletin of Mathematical Linguistics, pages 137–146. Le Hai Son, Alexandre Allauzen, Guillaume Wisniewski, and Fran ¸cois Yvon. 2010. Training continuous space language models: some practical issues. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’ 10, pages 778–788, Cambridge, Massachusetts, October. Association for Computational Linguistics. Le Hai Son, Alexandre Allauzen, and Fran ¸cois Yvon. 2012. Continuous space translation models with neural networks. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’ 12, pages 39–48, Montreal, Canada, June. Association for Computational Linguistics. Andreas Stolcke. 2002. Srilm-an extensible language modeling toolkit. In Proceedings International Conference on Spoken Language Processing, pages 257–286, November. Omar F. Zaidan. 2009. Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems. The Prague Bulletin of  Mathematical Linguistics, 91:79–88. 850</p>
<br/>
<br/><br/><br/></body>
</html>
