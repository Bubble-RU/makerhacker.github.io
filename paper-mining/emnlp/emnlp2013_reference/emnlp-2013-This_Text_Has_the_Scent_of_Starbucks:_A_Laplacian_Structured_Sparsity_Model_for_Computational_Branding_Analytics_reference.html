<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>184 emnlp-2013-This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-184" href="../emnlp2013/emnlp-2013-This_Text_Has_the_Scent_of_Starbucks%3A_A_Laplacian_Structured_Sparsity_Model_for_Computational_Branding_Analytics.html">emnlp2013-184</a> <a title="emnlp-2013-184-reference" href="#">emnlp2013-184-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>184 emnlp-2013-This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</h1>
<br/><p>Source: <a title="emnlp-2013-184-pdf" href="http://aclweb.org/anthology//D/D13/D13-1131.pdf">pdf</a></p><p>Author: William Yang Wang ; Edward Lin ; John Kominek</p><p>Abstract: We propose a Laplacian structured sparsity model to study computational branding analytics. To do this, we collected customer reviews from Starbucks, Dunkin’ Donuts, and other coffee shops across 38 major cities in the Midwest and Northeastern regions of USA. We study the brand related language use through these reviews, with focuses on the brand satisfaction and gender factors. In particular, we perform three tasks: automatic brand identification from raw text, joint brand-satisfaction prediction, and joint brandgender-satisfaction prediction. This work extends previous studies in text classification by incorporating the dependency and interaction among local features in the form of structured sparsity in a log-linear model. Our quantitative evaluation shows that our approach which combines the advantages of graphical modeling and sparsity modeling techniques significantly outperforms various standard and stateof-the-art text classification algorithms. In addition, qualitative analysis of our model reveals important features of the language uses associated with the specific brands.</p><br/>
<h2>reference text</h2><p>M. Belkin and P. Niyogi. 2001 . Laplacian eigenmaps and spectral techniques for embedding and clustering. Advances in neural information processing systems (NIPS). M. Belkin, P. Niyogi, and V. Sindhwani. 2006. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. The Journal of Machine Learning Research (JMLR). P. N. Bennett and N. Nguyen. 2009. Refined experts: improving classification in large taxonomies. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval. K. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft. 1999. When is nearest neighbor meaningful? Proceedings of the International Conference on Database Theory (ICDT). F. Biadsy, W.Y. Wang, A. Rosenberg, and J. Hirschberg. 2011. Intoxication detection using phonetic, phonotactic and prosodic cues. In Proceedings of the 12th Annual Conference of the International Speech Communication Association (Interspeech 2011). V. Chahuneau, K. Gimpel, B.R. Routledge, L. Scherlis,  and N.A. Smith. 2012. Word salad: Relating food prices and descriptions. C.C. Chang and C.J. Lin. 2011. Libsvm: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST). W. Y. Chen, Y. Song, H. Bai, C. J. Lin, and E. Y. Chang. 2011. Parallel spectral clustering in distributed systems. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). Y. N. Chen, W. Y. Wang, and A. I. Rudnicky. 2013. An empirical investigation of sparse log-linear models for improved dialogue act classification. In Proceedings of the 38th International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2013). W. W. Cohen. 2012. Learning similarity measures based on random walks. In Procceedings of the 21nd ACM International Conference on Information and Knowledge Management (CIKM). L. DiCarlo. 2004. Dunkin’ donuts vs. starbucks. In Forbes.com - Monday Matchup. J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. 2008. Efficient projections onto the l1-ball for learning in high dimensions. In Proceedings of the 25th international conference on Machine learning (ICML). J. Eisenstein, A. Ahmed, and E. Xing. 2011a. Sparse additive generative models of text. Proceedings of the 28th International Conference on Machine Learning (ICML 2011).  1335 J. Eisenstein, N. A. Smith, and E. P. Xing. 2011b. Discovering sociolinguistic associations with structured sparsity. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT). S. Gao, I. W. Tsang, L. T. Chia, and P. Zhao. 2010. Local features are not lonely–laplacian sparse coding for image classification. In 2010 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). S. Gao, I. Tsang, and L. Chia. 2012. Laplacian sparse coding, hypergraph laplacian sparse coding, and applications. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). P. M. Guadagni and J. D. C. Little. 1983. A logit model of brand choice calibrated on scanner data. Marketing science. M. K. Kim, K. Lopetcharat, and M. A. Drake. 2013. Influence of packaging information on consumer liking of chocolate milk. Journal of dairy science. A.A. Kuehn. 1962. Consumer brand choice as a learning process. S. Le Cessie and JC Van Houwelingen. 1992. Ridge estimators in logistic regression. Applied statistics. Chris Lederer and Sam Hill. 2001. See your brands through your customers eyes. Harvard Business Re-  view, 79(6): 125–133. S.I. Lee, H. Lee, P. Abbeel, and A.Y. Ng. 2006. Efficient l1regularized logistic regression. In Proceedings of the National Conference on Artificial Intelligence (AAAI). D.C. Liu and J. Nocedal. 1989. On the limited memory bfgs method for large scale optimization. Mathematical programming. D. Luo, H.G. Hosseini, and J.R. Stewart. 2004. Application of ann with extracted parameters from an electronic nose in cigarette brand identification. Sensors and Actuators B: Chemical. J. C. Makens. 1965. Effect of brand preference upon consumers perceived taste of turkey meat. Journal of Applied Psychology. A. F. T. Martins, N. A. Smith, P. M. Q. Aguiar, and M. A. T. Figueiredo. 2011. Structured sparsity in structured prediction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). Y. Moon and J. Quelch. 2006. Starbucks: delivering customer service. Harvard Business School. M. OConnell and G. Gooding. 2006. The use of first names to evaluate reports of gender and its effect on the distribution of married and unmarried couple households. In Proceedings of the Annual Meetings of the Population Association of America. S. Ovide. 2011. Face off! dunkin’ donuts vs. starbucks.  In  Deal Journal - Wall Street Journal Blogs. M. Paul and M. Dredze. 2012. Factorial lda: Sparse multi-dimensional text models. In Advances in Neural Information Processing Systems (NIPS). F. Pelisson, D. Hall, O. Riff, and J. Crowley. 2003. Brand identification using gaussian derivative histograms. Computer Vision Systems. R. Rifkin and A. Klautau. 2004. In defense of one-vsall classification. The Journal of Machine Learning Research (JMLR). M. Schmidt, G. Fung, and R. Rosales. 2007. Fast optimization methods for l1regularization: A comparative study and two new approaches. Machine Learning. M. Schmidt, E. Van Den Berg, M. Friedlander, and K. Murphy. 2009. Optimizing costly functions with simple constraints: A limited-memory projected quasi-newton algorithm. In Proceedings of Conference on Artificial Intelligence and Statistics (AIStats). R. Tibshirani. 1996. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological). W. Y. Wang and J. Hirschberg. 2011. Detecting levels of interest from spoken dialog with multistream prediction feedback and similarity based hierarchical fusion learning. In Proceedings of the 12th annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL 2011). W. Y. Wang, S. Finkelstein, A. Ogan, A. W. Black, and J. Cassell. 2012a. “love ya, jerkface”: using sparse log-linear models to build positive (and impolite) relationships with teens. In Proceedings of the 13th annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL 2012). W. Y. Wang, E. Mayfield, S. Naidu, and J. Dittmar. 2012b. Historical analysis of legal opinions with a sparse mixed-effects latent variable model. In Proceedings ofthe 50thAnnual Meeting ofthe Association for Computational Linguistics (ACL 2012). K. Q. Weinberger, F. Sha, Q. Zhu, and L. K. Saul. 2007. Graph laplacian regularization for large-scale semidefinite programming. Advances in neural information processing systems (NIPS). H. Zou and T. Hastie. 2005. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology). 1336</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
