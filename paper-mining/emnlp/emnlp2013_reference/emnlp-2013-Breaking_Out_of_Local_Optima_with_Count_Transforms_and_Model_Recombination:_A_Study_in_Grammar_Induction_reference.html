<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-40" href="../emnlp2013/emnlp-2013-Breaking_Out_of_Local_Optima_with_Count_Transforms_and_Model_Recombination%3A_A_Study_in_Grammar_Induction.html">emnlp2013-40</a> <a title="emnlp-2013-40-reference" href="#">emnlp2013-40-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>40 emnlp-2013-Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</h1>
<br/><p>Source: <a title="emnlp-2013-40-pdf" href="http://aclweb.org/anthology//D/D13/D13-1204.pdf">pdf</a></p><p>Author: Valentin I. Spitkovsky ; Hiyan Alshawi ; Daniel Jurafsky</p><p>Abstract: Many statistical learning problems in NLP call for local model search methods. But accuracy tends to suffer with current techniques, which often explore either too narrowly or too broadly: hill-climbers can get stuck in local optima, whereas samplers may be inefficient. We propose to arrange individual local optimizers into organized networks. Our building blocks are operators of two types: (i) transform, which suggests new places to search, via non-random restarts from already-found local optima; and (ii) join, which merges candidate solutions to find better optima. Experiments on grammar induction show that pursuing different transforms (e.g., discarding parts of a learned model or ignoring portions of training data) results in improvements. Groups of locally-optimal solutions can be further perturbed jointly, by constructing mixtures. Using these tools, we designed several modular dependency grammar induction networks of increasing complexity. Our complete sys- tem achieves 48.6% accuracy (directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data) more than 5% higher than the previous state-of-the-art. —</p><br/>
<h2>reference text</h2><p>H. Alshawi. 1996. Head automata for speech translation. In ICSLP. F. Attneave. 1953. Psychological probability as a function of experienced frequency. Experimental Psychology, 46. M. Bar-Hillel. 1980. The base-rate fallacy in probability judgments. Acta Psychologica, 44. A. Belz. 1998. Discovering phonotactic finite-state automata by genetic search. In COLING-ACL. Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009. Curriculum learning. In ICML. A. Bhargava and G. Kondrak. 2009. Multiple word alignment with profile hidden Markov models. In NAACL-HLT: Student Research and Doctoral Consortium.  Y. Bisk and J. Hockenmaier. 2012. Simple robust grammar induction with combinatory categorial grammars. In AAAI. A. Blum and T. Mitchell. 1998. Combining labeled and unlabeled data with co-training. In COLT. P. Blunsom and T. Cohn. 2010. Unsupervised induction of tree substitution grammars for dependency parsing. In EMNLP. S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In CoNLL. X. Carreras and L. M `arquez. 2005. Introduction to the CoNLL2005 shared task: Semantic role labeling. In CoNLL. L. J. Chapman. 1967. Illusory correlation in observational report. Verbal Learning and Verbal Behavior, 6. A. Clark. 2000. Inducing syntactic categories by context distribution clustering. In CoNLL-LLL. A. Clark. 2003. Combining distributional and morphological information for part of speech induction. In EACL. S. B. Cohen and N. A. Smith. 2010. Viterbi training for PCFGs: Hardness results and competitiveness of uniform initialization. In ACL. T. Cohn, P. Blunsom, and S. Goldwater. 2011. Inducing treesubstitution grammars. JMLR. M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In EMNLP. 1993 M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania. E. Corlett and G. Penn. 2010. An exact A∗ method for deciphering letter-substitution ciphers. In ACL.  F. I. M. Craik and E. Bialystok. 2006. Cognition through the lifespan: mechanisms of change. TRENDS in Cognitive Sciences, 10. C. de Marcken. 1995. Lexical heads, phrase structure and the induction of grammar. In WVLC. K. Duh and K. Kirchhoff. 2004. Automatic learning of language model structure. In COLING. J. Eisner. 2012. Grammar induction: Beyond local search. In ICGI. G. Elidan, M. Ninio, N. Friedman, and D. Schuurmans. 2002. Data perturbation for escaping local maxima in learning. In AAAI. J. L. Elman. 1993. Learning and development in neural networks: The importance of starting small. Cognition, 48. M. Elsner and W. Schudy. 2009. Bounding and comparing methods for correlation clustering beyond ILP. In NAACLHLT: Integer Linear Programming for NLP. C. M. Fonseca and P. J. Fleming. 1993. Genetic algorithms for multiobjective optimization: Formulation, discussion and generalization. In ICGA. C. J. Geyer. 1991. Markov chain Monte Carlo maximum likelihood. In Interface Symposium. J. Gillenwater, K. Ganchev, J. Gra ¸ca, F. Pereira, and B. Taskar. 2010. Posterior sparsity in unsupervised dependency parsing. Technical report, University of Pennsylvania. K. Gimpel and N. A. Smith. 2012. Concavity and initialization for unsupervised dependency parsing. In NAACL-HLT. F. Glover and M. Laguna. 1993. Tabu search. In C. R. Reeves, editor, Modern Heuristic Techniques for Combina-  torial Problems. Blackwell Scientific Publications. F. Glover. 1989. Tabu search Part I. ORSA Journal on Computing, 1. D. E. Goldberg. 1989. Genetic Algorithms in Search, Optimization & Machine Learning. Addison-Wesley. D. Golland, J. DeNero, and J. Uszkoreit. 2012. A featurerich constituent context model for grammar induction. In EMNLP-CoNLL. M. R. Gormley and J. Eisner. 2013. Nonconvex global optimization for latent-variable models. In ACL. W. P. Headden, III, M. Johnson, and D. McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In NAACL-HLT. G. Hinton and S. Roweis. 2003. Stochastic neighbor embedding. In NIPS. G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. In ArXiv. J. H. Holland. 1975. Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence. University of Michigan Press. H. H. Hoos and T. St¨ utzle. 2004. Stochastic Local Search: Foundations and Applications. Morgan Kaufmann. C. R. Houck, J. A. Joines, and M. G. Kay. 1996. Comparison of genetic algorithms, random restart, and two-opt switching for solving large location-allocation problems. Computers & Operations Research, 23. X. Hu, R. Shonkwiler, and M. C. Spruill. 1994. Random restarts in global optimization. Technical report, GT. —  Y. Huang, M. Zhang, and C. L. Tan. 2012. Improved constituent context model with features. In PACLIC. F. Jelinek and R. L. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse data. In Pattern Recognition in Practice. D. S. Johnson, C. H. Papadimitriou, and M. Yannakakis. 1988. How easy is local search? Journal of Computer and System Sciences, 37. D. Kahneman and A. Tversky. 1972. Subjective probability: A judgment of representativeness. Cognitive Psychology, 3. D. Kahneman and A. Tversky. 1973. On the psychology of prediction. Psychological Review, 80. D. Kahneman and A. Tversky. 1982. Evidential impact of base rates. In D. Kahneman, P. Slovic, and A. Tversky, editors, Judgment under uncertainty: Heuristics and biases. Cambridge University Press. J. Ke, M. Ogura, and W. S.-Y. Wang. 2003. Optimization models of sound systems using genetic algorithms. Computational Linguistics, 29. J. Kim and R. J. Mooney. 2010. Generative alignment and semantic parsing for learning from ambiguous supervision. In COLING. S. Kirkpatrick, C. D. Gelatt, Jr., and M. P. Vecchi. 1983. Optimization by simulated annealing. Science, 220. D. Klein and C. D. Manning. 2002. A generative constituentcontext model for improved grammar induction. In ACL. D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In ACL. D. Koller and N. Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques. MIT Press.  K. A. Krueger and P. Dayan. 2009. Flexible shaping: How learning in small steps helps. Cognition, 110. D. Lashkari and P. Golland. 2008. Convex clustering with exemplar-based models. In NIPS. P. Liang and D. Klein. 2009. Online EM for unsupervised models. In NAACL-HLT. L. K. Low and H.-J. Cheng. 2006. Axon pruning: an essential step underlying the developmental plasticity of neuronal connections. Royal Society of London Philosophical Transactions Series B, 361. M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19. D. Mare cˇek and M. Straka. 2013. Stop-probability estimates computed on a large corpus improve unsupervised dependency parsing. In ACL. D. Mare cˇek and Z. Zˇabokrtsk y´. 2011. Gibbs sampling with treeness constraint in unsupervised dependency parsing. In ROBUS. D. Mare cˇek and Z. 2012. Exploiting reducibility in unsupervised dependency parsing. In EMNLP-CoNLL.  Zˇabokrtsk y´.  1994 R. Martin-Brualla, E. Alfonseca, M. Pasca, K. Hall, E. RobledoArnuncio, and M. Ciaramita. 2010. Instance sense induction from attribute sets. In COLING. N. McIntyre and M. Lapata. 2010. Plot induction and evolutionary search for story generation. In ACL. X.-d. Mei, S.-h. Sun, J.-s. Pan, and T.-Y. Chen. 2001. Optimization of HMM by the tabu search algorithm. In RO-  CLING. C. Mellish, A. Knott, J. Oberlander, and M. O’Donnell. 1998. Experiments using stochastic search for text planning. In INLG. R. C. Moore and C. Quirk. 2008. Random restarts in minimum error rate training for statistical machine translation. In COLING. T. Naseem and R. Barzilay. 2011. Using semantic cues to learn syntax. In AAAI. R. M. Neal and G. E. Hinton. 1999. A view of the EM algorithm that justifies incremental, sparse, and other variants. In M. I. Jordan, editor, Learning in Graphical Models. MIT Press. J. Nivre, J. Hall, S. K ¨ubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In EMNLP-CoNLL. M. A. Paskin. 2001a. Cubic-time parsing and learning algorithms for grammatical bigram models. Technical report, UCB. M. A. Paskin. 2001b. Grammatical bigrams. In NIPS. F. Pereira and Y. Schabes. 1992. Inside-outside reestimation from partially bracketed corpora. In ACL. S. Petrov. 2010. Products of random latent variable grammars. In NAACL-HLT. E. Ponvert, J. Baldridge, and K. Erk. 2011. Simple unsupervised grammar induction from raw text with cascaded finite state models. In ACL-HLT. K. V. Price, R. M. Storn, and J. A. Lampinen. 2005. Differential Evolution: A Practical Approach to Global Optimiza-  tion. Springer.  Ravi and K. Knight. 2009. Minimized models for unsupervised part-of-speech tagging. In ACL-IJCNLP. K. Rose. 1998. Deterministic annealing for clustering, compression, classification, regression and related optmization problems. Proceedings of the IEEE, 86. Y. Seginer. 2007. Fast unsupervised incremental parsing. In S.  ACL.  and D. Mitchell. 1992. A new method for solving hard satisfiability problems. In AAAI. B. Selman, H. A. Kautz, and B. Cohen. 1994. Noise strategies for improving local search. In AAAI. F. J. Solis and R. J.-B. Wets. 1981. Minimization by random search techniques. Mathematics of Operations Research, 6. V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009. Baby Steps: How “Less is More” in unsupervised dependency parsing. In GRLL. V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Manning. 2010. Viterbi training improves unsupervised dependency parsing. In CoNLL. B. Selman, H. Levesque,  V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 201 1a. Lateen EM: Unsupervised training with multiple objectives, applied to dependency grammar induction. In EMNLP. V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b. Punctuation: Making a point in unsupervised dependency parsing. In CoNLL. V. I. Spitkovsky, A. X. Chang, H. Alshawi, and D. Jurafsky. 2011c. Unsupervised dependency parsing without gold partof-speech tags. In EMNLP. V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012a. Bootstrapping dependency grammar inducers from incomplete sentence fragments via austere models. In ICGI. V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012b. Three dependency-and-boundary models for grammar induction. In EMNLP-CoNLL. W. Stadler, editor. 1988. Multicriteria Optimization in Engineering and in the Sciences. Plenum Press. A. Sun, R. Grishman, and S. Sekine. 2011. Semi-supervised relation extraction with large-scale word clustering. In ACL. M. Surdeanu and C. D. Manning. 2010. Ensemble models for dependency parsing: Cheap and good? In NAACL-HLT. K. Tu and V. Honavar. 2011. On the utility of curricula in  unsupervised learning of probabilistic grammars. In IJCAI. K. Tu and V. Honavar. 2012. Unambiguity regularization for unsupervised learning of probabilistic grammars. In EMNLP-CoNLL. A. Tversky and D. Kahneman. 1973. Availability: A heuristic for judging frequency and probability. Cognitive Psychology, 5. S. I. Wang and C. D. Manning. 2013. Fast dropout training. In ICML. Q. I. Wang, D. Schuurmans, and D. Lin. 2008. Semisupervised convex training for dependency parsing. In HLTACL. T. Xiao, J. Zhu, M. Zhu, and H. Wang. 2010. Boosting-based system combination for machine translation. In ACL. D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In ACL. 1995</p>
<br/>
<br/><br/><br/></body>
</html>
