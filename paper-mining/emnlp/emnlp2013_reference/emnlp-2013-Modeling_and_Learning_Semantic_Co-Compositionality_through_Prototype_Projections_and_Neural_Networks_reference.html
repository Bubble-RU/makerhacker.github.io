<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</title>
</head>

<body>
<p><a title="emnlp" href="../emnlp_home.html">emnlp</a> <a title="emnlp-2013" href="../home/emnlp2013_home.html">emnlp2013</a> <a title="emnlp-2013-134" href="../emnlp2013/emnlp-2013-Modeling_and_Learning_Semantic_Co-Compositionality_through_Prototype_Projections_and_Neural_Networks.html">emnlp2013-134</a> <a title="emnlp-2013-134-reference" href="#">emnlp2013-134-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>134 emnlp-2013-Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</h1>
<br/><p>Source: <a title="emnlp-2013-134-pdf" href="http://aclweb.org/anthology//D/D13/D13-1014.pdf">pdf</a></p><p>Author: Masashi Tsubaki ; Kevin Duh ; Masashi Shimbo ; Yuji Matsumoto</p><p>Abstract: We present a novel vector space model for semantic co-compositionality. Inspired by Generative Lexicon Theory (Pustejovsky, 1995), our goal is a compositional model where both predicate and argument are allowed to modify each others’ meaning representations while generating the overall semantics. This readily addresses some major challenges with current vector space models, notably the polysemy issue and the use of one representation per word type. We implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations. We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality. The model achieves the best result to date (ρ = 0.47) on the semantic similarity task of transitive verbs (Grefenstette and Sadrzadeh, 2011).</p><br/>
<h2>reference text</h2><p>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: A collection of very large linguistically processed webcrawled corpora. Language resources and evaluation, 43(3):209–226. Marco Baroni, Raffaella Bernardi, and Roberto Zamparelli. 2013. Frege in space: A program for compositional distributional semantics. Linguistic Issues in Language Technologies. Yoshua Bengio, R ´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3: 1137–1 155. William Blacoe and Mirella Lapata. 2012. A comparison of vector-based representations for semantic composition. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL). Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.  2010. Mathematical foundations for a compositional distributional model of meaning. CoRR, abs/1003.4394. Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the International Conference on Machine Learning (ICML). Ronan Collobert, Jason Weston, L e´on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493– 2537. Georgiana Dinu, Nghia The Pham, and Marco Barori. 2013. General estimation and evaluation of compositional distributional semantic models. In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality. Katrin Erk and Sebastian Pad o´. 2008. A structured vector space model for word meaning in context. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). Katrin Erk and Sebastian Pad o´. 2009. Paraphrase assessment in structured vector space: Exploring parameters and datasets. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics. Katrin Erk. 2012. Vector space models of word meaning and phrase meaning: A survey. Language and Lin-  guistics Compass, 6(10):635–653. G Frege. 1892. U¨ber sinn und bedeutung. In Zeitschfrift f u¨r Philosophie und philosophische Kritik, 100. Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). Emiliano Guevara. 2010. A regression model of adjective-noun compositionality in distributional semantics. In Proceedings of the Workshop on GEometrical Models of Natural Language Semantics. Eric Huang, Richard Socher, Christopher Manning, and Andrew Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representations. In Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics (NAACLHLT). Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). Jeff Mitchell and Mirella Lapata. 2010. Composition in  distributional models of semantics. Cognitive Science, 34(8): 1388–1439. Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for dependency parsing. In Proceedings of the International Conference on Language Resources and Evaluation (LREC). James Pustejovsky. 1995. The Generative Lexicon. MIT Press, Cambridge, MA. 140 Joseph Reisinger and Raymond J Mooney. 2010. Multiprototype vector-space models of word meaning. In Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT). Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). Stefan Thater, Hagen F ¨urstenau, and Manfred Pinkal.  2010. Contextualizing semantic representations using syntactically enriched vector models. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). Stefan Thater, Hagen F ¨urstenau, and Manfred Pinkal. 2011. Word meaning in context: A simple and effective vector model. In Asian Federation of Natural Language Processing (IJCNLP). Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). Peter D Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of artificial intelligence research, 37(1): 141– 188. Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen. 2013. A tensor-based factorization model of semantic compositionality. In Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics (NAACLHLT). Fabio Massimo Zanzotto, Ioannis Korkontzelos, Francesca Fallucchi, and Suresh Manandhar. 2010. Estimating linear models for compositional distributional semantics. In Proceedings of  the International Conference Linguistics (COLING).  on  Computational</p>
<br/>
<br/><br/><br/></body>
</html>
