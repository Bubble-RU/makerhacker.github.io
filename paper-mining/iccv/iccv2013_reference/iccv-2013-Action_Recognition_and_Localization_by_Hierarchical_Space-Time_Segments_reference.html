<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-37" href="../iccv2013/iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">iccv2013-37</a> <a title="iccv-2013-37-reference" href="#">iccv2013-37-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</h1>
<br/><p>Source: <a title="iccv-2013-37-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Ma_Action_Recognition_and_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Shugao Ma, Jianming Zhang, Nazli Ikizler-Cinbis, Stan Sclaroff</p><p>Abstract: We propose Hierarchical Space-Time Segments as a new representation for action recognition and localization. This representation has a two-level hierarchy. The first level comprises the root space-time segments that may contain a human body. The second level comprises multi-grained space-time segments that contain parts of the root. We present an unsupervised method to generate this representation from video, which extracts both static and non-static relevant space-time segments, and also preserves their hierarchical and temporal relationships. Using simple linear SVM on the resultant bag of hierarchical space-time segments representation, we attain better than, or comparable to, state-of-the-art action recognition performance on two challenging benchmark datasets and at the same time produce good action localization results.</p><br/>
<h2>reference text</h2><p>[1] P. Arbelaez, M. Maire, C. C. Fowlkes, and J. Malik. From contours to regions: An empirical evaluation. In CVPR, 2009.</p>
<p>[2] A. F. Bobick and J. W. Davis. The recognition of human movement using temporal templates. TPAMI, 23(3):257– 267, 2001.</p>
<p>[3] M. Bregonzio, S. Gong, and T. Xiang. Recognising action as clouds of space-time interest points. In CVPR, 2009.</p>
<p>[4] W. Brendel and S. Todorovic. Learning spatiotemporal graphs of human activities. In ICCV, 2011.</p>
<p>[5] C. X. Chenliang Xu and J. J. Corso. Streaming hierarchical video segmentation. In ECCV, 2012.</p>
<p>[6] A. Gaidon, Z. Harchaoui, and C. Schmid. Recognizing activities with cluster-trees of tracklets. In BMVC, 2012.</p>
<p>[7] A. Gilbert, J. Illingworth, and R. Bowden. Fast realistic multi-action recognition using mined dense spatio-temporal features. In ICCV, 2009.</p>
<p>[8] L. Gorelick, M. Blank, E. Shechtman, M. Irani, and R. Basri. Actions as space-time shapes. TPAMI, 29(12):2247–2253, 2007.</p>
<p>[9] M. Grundmann, V. Kwatra, M. Han, and I. A. Essa. Effi-</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]  cient hierarchical graph-based video segmentation. In CVPR, 2010. N. Ikizler-Cinbis and S. Sclaroff. Object, scene and actions: Combining multiple features for human action recognition. In ECCV, 2010. A. Kovashka and K. Grauman. Learning a hierarchy of discriminative space-time neighborhood features for human action recognition. In CVPR, 2010. T. Lan, Y. Wang, and G. Mori. Discriminative figure-centric models for joint action localization and recognition. In ICCV, 2011. I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld. Learning realistic human actions from movies. In CVPR, 2008. 2750  video. The yellow boxes outlines the extracted  segments.  The inclusion of one box in another indicates the parent-children relationships  covered by red mask is our action localization  output.  First five rows  are  from UCF-Sports, and last  two rows are  from HighFive.</p>
<p>[14] Y. J. Lee, J. Kim, and K. Grauman. Key-segments for video object segmentation. In ICCV, 2011.</p>
<p>[15] M. Leordeanu, R. Sukthankar, and C. Sminchisescu. Efficient closed-form solution to generalized boundary detection. In ECCV, 2012.</p>
<p>[16] T. Ma and L. J. Latecki. Maximum weight cliques with mutex constraints for video object segmentation. In CVPR, 2012.</p>
<p>[17] A. Patron-Perez, M. Marszalek, A. Zisserman, and I. D. Reid. High five: Recognising human interactions in tv shows. In BMVC, 2010.</p>
<p>[18] M. Raptis, I. Kokkinos, and S. Soatto. Discovering discriminative action parts from mid-level video representations. In CVPR, 2012.</p>
<p>[19] M. D. Rodriguez, J. Ahmed, and M. Shah. Action mach a spatio-temporal maximum average correlation height filter for action recognition. In CVPR, 2008.</p>
<p>[20] D. Tran and J. Yuan. Optimal spatio-temporal path discovery for video event detection. In CVPR, 2011.</p>
<p>[21] D. Tran and J. Yuan. Max-margin structured output regression for spatio-temporal action localization. In NIPS, 2012.</p>
<p>[22] H. Wang, A. Kl¨ aser, C. Schmid, and C.-L. Liu. Action recognition by dense trajectories. In CVPR, 2011.</p>
<p>[23] Y. Wang, K. Huang, and T. Tan. Human activity recognition based on r transform. In CVPR, 2007.</p>
<p>[24] X. Wu, D. Xu, L. Duan, and J. Luo. Action recognition using context and appearance distribution features. In CVPR, 2011.</p>
<p>[25] Y. Xie, H. Chang, Z. Li, L. Liang, X. Chen, and D. Zhao. A unified framework for locating and recognizing human actions. In CVPR, 2011.</p>
<p>[26] Z. L. Yang Wang, Duan Tran and D. Forsyth. Discriminative hierarchical part-based models for human parsing and action recognition. JMLR, 13:30753102, 2012. 275 1</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
