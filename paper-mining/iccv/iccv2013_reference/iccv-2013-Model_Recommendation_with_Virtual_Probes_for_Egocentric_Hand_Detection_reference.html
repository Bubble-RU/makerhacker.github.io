<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>267 iccv-2013-Model Recommendation with Virtual Probes for Egocentric Hand Detection</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-267" href="../iccv2013/iccv-2013-Model_Recommendation_with_Virtual_Probes_for_Egocentric_Hand_Detection.html">iccv2013-267</a> <a title="iccv-2013-267-reference" href="#">iccv2013-267-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>267 iccv-2013-Model Recommendation with Virtual Probes for Egocentric Hand Detection</h1>
<br/><p>Source: <a title="iccv-2013-267-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Li_Model_Recommendation_with_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Cheng Li, Kris M. Kitani</p><p>Abstract: Egocentric cameras can be used to benefit such tasks as analyzing fine motor skills, recognizing gestures and learning about hand-object manipulation. To enable such technology, we believe that the hands must detected on thepixellevel to gain important information about the shape of the hands and fingers. We show that the problem of pixel-wise hand detection can be effectively solved, by posing the problem as a model recommendation task. As such, the goal of a recommendation system is to recommend the n-best hand detectors based on the probe set a small amount of labeled data from the test distribution. This requirement of a probe set is a serious limitation in many applications, such as ego-centric hand detection, where the test distribution may be continually changing. To address this limitation, we propose the use of virtual probes which can be automatically extracted from the test distribution. The key idea is – that many features, such as the color distribution or relative performance between two detectors, can be used as a proxy to the probe set. In our experiments we show that the recommendation paradigm is well-equipped to handle complex changes in the appearance of the hands in firstperson vision. In particular, we show how our system is able to generalize to new scenarios by testing our model across multiple users.</p><br/>
<h2>reference text</h2><p>[1] Y. Boykov and V. Kolmogorov. An experimental comparison of mincut/max-flow algorithms for energy minimization in vision. PAMI, 26(9):1 124–1 137, 2004. 5</p>
<p>[2] R. Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997. 2</p>
<p>[3] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005. 3, 6</p>
<p>[4] D. Dey, T. Liu, M. Hebert, and J. A. Bagnell. Contextual sequence prediction via submodular function optimization. In Robotics Science and Systems, 2012. 2</p>
<p>[5] A. Fathi, X. Ren, and J. Rehg. Learning to recognize objects in ego-</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]</p>
<p>[19]  centric activities. In CVPR, 2011. 2, 5, 6 T. Joachims. Transductive inference for text classification using support vector machines. In ICML, 1999. 2 M. Jones and J. Rehg. Statistical color models with application to skin detection. In CVPR, 1999. 2 J. Kim and H. Park. Toward faster nonnegative matrix factorization: A new algorithm and comparisons. In International Conference on Data Mining, 2008. 4 C. Li and K. M. Kitani. Pixel-level hand detection for ego-centric videos. In CVPR, 2013. 2, 3, 4, 5, 6 P. Matikainen, R. Sukthankar, and M. Hebert. Model recommendation for action recognition. In CVPR, 2012. 2, 3, 4 M. Pandey and S. Lazebnik. Scene recognition and weakly supervised object localization with deformable part-based models. In ICCV, 2011. 3 H. Pirsiavash and D. Ramanan. Detecting activities of daily living in first-person camera views. In CVPR, 2012. 5 X. Ren and C. Gu. Figure-ground segmentation improves handled object recognition in egocentric video. In CVPR, 2010. 2, 5 K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In ECCV. 2010. 2 Y. Sheikh, O. Javed, and T. Kanade. Background subtraction for freely moving cameras. In ICCV, 2009. 2, 5 A. Shrivastava, T. Malisiewicz, A. Gupta, and A. A. Efros. Datadriven visual similarity for cross-domain image matching. SIGGRAPH ASIA, 30(6), 2011. 3 L. Sigal, S. Sclaroff, and V. Athitsos. Skin color-based video segmentation under time-varying illumination. PAMI, 26(7):862–877, 2004. 2 M. Sugiyama, T. Kanamori, T. Suzuki, S. Hido, J. Sese, I. Takeuchi, and L. Wang. A density-ratio framework for statistical data processing. IPSJ Transactions on Computer Vision and Applications, 1:183– 208, 2009. 2 A. Vazquez-Reina, S. Avidan, H. Pfister, and E. Miller. Multiple hypothesis video segmentation from superpixel flows. In ECCV. 2010. 5  263 1</p>
<br/>
<br/><br/><br/></body>
</html>
