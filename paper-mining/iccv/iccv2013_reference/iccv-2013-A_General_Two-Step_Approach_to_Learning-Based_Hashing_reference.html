<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>13 iccv-2013-A General Two-Step Approach to Learning-Based Hashing</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-13" href="../iccv2013/iccv-2013-A_General_Two-Step_Approach_to_Learning-Based_Hashing.html">iccv2013-13</a> <a title="iccv-2013-13-reference" href="#">iccv2013-13-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>13 iccv-2013-A General Two-Step Approach to Learning-Based Hashing</h1>
<br/><p>Source: <a title="iccv-2013-13-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Lin_A_General_Two-Step_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Guosheng Lin, Chunhua Shen, David Suter, Anton van_den_Hengel</p><p>Abstract: Most existing approaches to hashing apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to respond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. This framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problemspecific hashing methods. Our framework decomposes the hashing learning problem into two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically be formulated as binary quadratic problems, and the second step can be accomplished by training standard binary classifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate that the proposed framework is effective, flexible and outperforms the state-of-the-art.</p><br/>
<h2>reference text</h2><p>[1] M. A´. Carreira-Perpi˜ n a´n. The elastic embedding algorithm for dimensionality reduction. In Proc. Int. Conf. Mach. Learn., 2010. 4</p>
<p>[2] A. Gionis, P. Indyk, and R. Motwani. Similarity search in high dimensions via hashing. In Proc. Int. Conf. Very Large Data Bases, 1999. 1, 4</p>
<p>[3] Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin. Iterative quantization: a procrustean approach to learning binary codes for large-scale image retrieval. IEEE Trans. Pattern Analysis Mach. Intelli., 2012. 1, 4, 5</p>
<p>[4] J.-P. Heo, Y. Lee, J. He, S.-F. Chang, and S.-E. Yoon. Spherical</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]  hashing. In Proc. IEEE Conf. Comp. Vis. Pattern Recogn., 2012. 4 B. Kulis and T. Darrell. Learning to hash with binary reconstructive embeddings. In Proc. Adv. Neural Info. Process. Syst., 2009. 1, 2, 4, 5 B. Kulis and K. Grauman. Kernelized locality-sensitive hashing. IEEE Trans. Pattern Analysis Mach. Intelli., 2012. 4 X. Li, G. Lin, C. Shen, A. van den Hengel, and A. R. Dick. Learning hash functions using column generation. In Proc. Int. Conf. Mach. Learn. , 2013. 1 W. Liu, J. Wang, R. Ji, Y. Jiang, and S. Chang. Supervised hashing with kernels. In Proc. IEEE Conf. Comp. Vis. Pattern Recogn., 2012. 1, 2, 4, 5 W. Liu, J. Wang, S. Kumar, and S. F. Chang. Hashing with graphs. In Proc. Int. Conf. Mach. Learn. , 2011. 4 M. Norouzi and D. Fleet. Minimal loss hashing for compact binary codes. In Proc. Int. Conf. Mach. Learn., 2011. 1, 2, 4, 5 F. Shen, C. Shen, Q. Shi, A. van den Hengel, and Z. Tang. Inductive hashing on manifolds. In Proc. IEEE Conf. Comp. Vis. Pattern 22555588 LABLEME  LABLEME  LABLEME  achieves best performances. FLICKR1 M  FLICKR1 M  TINY580K  TINY580K  nPrsiceo0 72.0534618.02Reca3.l(024bits).5ITSBKMRLSPT0Q.EH 6−sCA07.ocPeinr0 .19786543.210NumB2ISKTM0LRPSTbQEHeLr−sH3Co0frAe4t0iv5ds6a0mpl7es(3802bi9ts)10inoscerP0 58.764321.0 R3.ecal04(2bi5.ts)06ITKBMS RLTPQ0HEL7.−sHCA08.noiscerP0 9.8765432.1N0um2bero30f t4iev5d0sa6mple7s0(38ITBMS2K0RTPLQbHEL−isHt9C0)A1  unsupervised methods. Our method TSH achieves on par result with KSH. TSH and KSH significantly outperform other methods. Recogn. , 2013. 1</p>
<p>[12] A. Torralba, R. Fergus, and W. Freeman. 80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE Trans. Pattern Analysis Mach. Intelli., 2008. 1</p>
<p>[13] J. Wang, S. Kumar, and S. Chang. Semi-supervised hashing for large scale search. IEEE Trans. Pattern Analysis Mach. Intelli., 2012. 1, 4, 5</p>
<p>[14] Y. Weiss, R. Fergus, and A. Torralba. Multidimensional spectral hashing. In Proc. Eur. Conf. Comp. Vis., 2012. 1, 2, 4</p>
<p>[15] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In Proc. Adv. Neural Info. Process. Syst., 2008. 1, 4</p>
<p>[16] D. Zhang, J. Wang, D. Cai, and J. Lu. Self-taught hashing for fast similarity search. In Proc. Annual ACM SIGIR Conf., 2010. 1, 2, 4</p>
<p>[17] C. Zhu, R. H. Byrd, P. Lu, and J. Nocedal. Algorithm 778: L-BFGSB: Fortran subroutines for large-scale bound-constrained optimization. ACM T. Math. Softw., 1997. 4 22555599</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
