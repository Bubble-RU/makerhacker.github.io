<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>352 iccv-2013-Revisiting Example Dependent Cost-Sensitive Learning with Decision Trees</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-352" href="../iccv2013/iccv-2013-Revisiting_Example_Dependent_Cost-Sensitive_Learning_with_Decision_Trees.html">iccv2013-352</a> <a title="iccv-2013-352-reference" href="#">iccv2013-352-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>352 iccv-2013-Revisiting Example Dependent Cost-Sensitive Learning with Decision Trees</h1>
<br/><p>Source: <a title="iccv-2013-352-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Aodha_Revisiting_Example_Dependent_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Oisin Mac Aodha, Gabriel J. Brostow</p><p>Abstract: Typical approaches to classification treat class labels as disjoint. For each training example, it is assumed that there is only one class label that correctly describes it, and that all other labels are equally bad. We know however, that good and bad labels are too simplistic in many scenarios, hurting accuracy. In the realm of example dependent costsensitive learning, each label is instead a vector representing a data point’s affinity for each of the classes. At test time, our goal is not to minimize the misclassification rate, but to maximize that affinity. We propose a novel example dependent cost-sensitive impurity measure for decision trees. Our experiments show that this new impurity measure improves test performance while still retaining the fast test times of standard classification trees. We compare our approach to classification trees and other cost-sensitive methods on three computer vision problems, tracking, descriptor matching, and optical flow, and show improvements in all three domains.</p><br/>
<h2>reference text</h2><p>[1] UCI KDD Archive. http : / / kdd . i s . uc i c .edu / . 2</p>
<p>[2] N. Abe, B. Zadrozny, and J. Langford. An iterative method for multi-class cost-sensitive learning. In KDD, 2004. 2</p>
<p>[3] S. Baker, D. Scharstein, J. Lewis, S. Roth, M. J. Black, and R. Szeliski. A database and evaluation methodology for optical flow. IJCV, 2011. 6</p>
<p>[4] U. Brefeld, P. Geibel, and F. Wysotzki. Support vector machines with example dependent costs. ECML, 2003. 2</p>
<p>[5] L. Breiman. Random forests. Machine Learning, 45(1), 2001. 2</p>
<p>[6] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classification and Regression Trees. 1984. 1, 2, 3</p>
<p>[7] T. Brox and J. Malik. Large Displacement Optical Flow: Descriptor Matching in Variational Motion Estimation. PAMI, 2010. 6, 7</p>
<p>[8] A. Criminisi and J. Shotton. Decision Forests for Computer Vision and Medical Image Analysis. 2013. 2, 4</p>
<p>[9] P. Domingos. Metacost: a general method for making classifiers cost-sensitive. In KDD, 1999. 1, 2</p>
<p>[10] C. Elkan. The foundations of cost-sensitive learning. In Int. Joint Conference on Artificial Intelligence, 2001 . 2</p>
<p>[11] I. Everts, J. van Gemert, and T. Gevers. Per-patch descriptor selection using surface and scene properties. In ECCV, 2012. 2, 4, 5, 6</p>
<p>[12] J. Gall, A. Yao, N. Razavi, L. Van Gool, and V. Lempitsky. Hough forests for object detection, tracking, and action recognition. PAMI, 2011. 2</p>
<p>[13] C. Garc ı´a Cifuentes, M. Sturzel, F. Jurie, and G. J. Brostow. Motion models that only work sometimes. In BMVC, 2012. 2, 4, 5</p>
<p>[14] J. Geusebroek, G. Burghouts, and A. Smeulders. The amsterdam library of object images. IJCV, 2005. 5</p>
<p>[15] T.-K. Jan, D.-W. Wang, C.-H. Lin, and H.-T. Lin. A simple methodology for soft cost-sensitive classification. In KDD,</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]</p>
<p>[19]</p>
<p>[20]</p>
<p>[21]</p>
<p>[22]</p>
<p>[23]</p>
<p>[24]</p>
<p>[25]</p>
<p>[26]</p>
<p>[27]</p>
<p>[28]  2012. 2 S. Ji and L. Carin. Cost-sensitive feature acquisition and classification. Pattern Recognition, 2007. 2 P. Kontschieder, S. R. Bul o´, H. Bischof, and M. Pelillo. Structured class-labels in random forests for semantic image labelling. In ICCV, 2011. 2 O. Mac Aodha, A. Humayun, M. Pollefeys, and G. J. Brostow. Learning a confidence measure for optical flow. PAMI, 2012. 2, 4, 6 S. Nowozin. Improved information gain estimates for decision tree induction. In ICML, 2012. 2 J. Quinlan. C4.5: Programs for Machine Learning. 1993. 3 D. Sun, S. Roth, and M. Black. Secrets of optical flow estimation and their principles. In CVPR, 2010. 6, 7 M. Tan. Cost-sensitive learning of classification knowledge and its applications in robotics. Machine Learning, 1993. 2 K. M. Ting. An instance-weighting method to induce costsensitive trees. Knowledge and Data Engineering, IEEE Transactions on, 2002. 2, 3 H. Tu and H. Lin. One-sided support vector regression for multiclass cost-sensitive classification. In ICML, 2010. 2, 4, 5, 6 S. Vijayanarasimhan and K. Grauman. Cost-sensitive active visual category learning. IJCV, 2011. 2 H. Wang, A. Kl¨ aser, C. Schmid, and L. Cheng-Lin. Action Recognition by Dense Trajectories. In CVPR, 2011. 5 M. Werlberger, W. Trobin, T. Pock, A. Wedel, D. Cremers, and H. Bischof. Anisotropic Huber-L1 Optical Flow. In BMVC, 2009. 6, 7 P. Yin, A. Criminisi, J. Winn, and I. A. Essa. Bilayer segmentation of webcam videos using tree-based classifiers. PAMI,</p>
<p>[29]</p>
<p>[30]  [3 1]</p>
<p>[32]  2011. 2 C. Zach, T. Pock, and H. Bischof. A Duality Based Approach for Realtime TV-L1 Optical Flow. In DAGM, 2007. 6, 7 B. Zadrozny and C. Elkan. Learning and making decisions when costs and probabilities are both unknown. In KDD, 2001. 1, 2 B. Zadrozny, J. Langford, and N. Abe. Cost-sensitive learning by cost-proportionate example weighting. In ICDM, 2003. 2, 5 Z.-H. Zhou and X.-Y. Liu. On multi-class cost-sensitive learning. In AAAI, 2006. 2 200</p>
<br/>
<br/><br/><br/></body>
</html>
