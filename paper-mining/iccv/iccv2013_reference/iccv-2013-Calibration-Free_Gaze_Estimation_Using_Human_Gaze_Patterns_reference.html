<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>67 iccv-2013-Calibration-Free Gaze Estimation Using Human Gaze Patterns</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-67" href="../iccv2013/iccv-2013-Calibration-Free_Gaze_Estimation_Using_Human_Gaze_Patterns.html">iccv2013-67</a> <a title="iccv-2013-67-reference" href="#">iccv2013-67-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>67 iccv-2013-Calibration-Free Gaze Estimation Using Human Gaze Patterns</h1>
<br/><p>Source: <a title="iccv-2013-67-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Alnajar_Calibration-Free_Gaze_Estimation_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Fares Alnajar, Theo Gevers, Roberto Valenti, Sennay Ghebreab</p><p>Abstract: We present a novel method to auto-calibrate gaze estimators based on gaze patterns obtained from other viewers. Our method is based on the observation that the gaze patterns of humans are indicative of where a new viewer will look at [12]. When a new viewer is looking at a stimulus, we first estimate a topology of gaze points (initial gaze points). Next, these points are transformed so that they match the gaze patterns of other humans to find the correct gaze points. In a flexible uncalibrated setup with a web camera and no chin rest, the proposed method was tested on ten subjects and ten images. The method estimates the gaze points after looking at a stimulus for a few seconds with an average accuracy of 4.3â—¦. Although the reported performance is lower than what could be achieved with dedicated hardware or calibrated setup, the proposed method still provides a sufficient accuracy to trace the viewer attention. This is promising considering the fact that auto-calibration is done in a flexible setup , without the use of a chin rest, and based only on a few seconds of gaze initialization data. To the best of our knowledge, this is the first work to use human gaze patterns in order to auto-calibrate gaze estimators.</p><br/>
<h2>reference text</h2><p>[1] K. Smith, S.O. Ba, J. Odobez, and D. Gatica-Perez. Tracking the visual focus of attention for a varying number of wandering people. IEEE Transactions on Pattern Analysis and Machine Intelligence, Volume 30 Issue 7, p. 1212-1229, 2008.</p>
<p>[2] P. Majaranta and K.-J. Rih. Twenty years of eye typing: systems and design issues. Symposium on Eye Tracking Research and Applications (ETRA), p. 15-22 , 2002.</p>
<p>[3] Y. Sugano, Y. Matsushita, and Y. Sato. Appearance-based gaze estimation using visual saliency. IEEE Transactions on 143  Pattern Analysis and Machine Intelligence, Volume 35 Issue 2, p. 329-341, 2013.</p>
<p>[4] Y. Sugano, Y. Matsushita, and Y.Sato. Calibration-free gaze</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]  sensing using saliency maps. IEEE Conference on Computer Vision andPattern Recognition (CVPR), p. 2667-2674, 2010. D.W. Hansen, J.P. Hansen, M. Nielsen, A.S. Johansen, and M.B. Stegmann. Eye typing using Markov and active appearance models. Sixth IEEE Workshop on Applications of Computer Vision, p. 132-136 , 2002. L. Feng, Y. Sugano, T. Okabe, and Y. Sato. Inferring human gaze from appearance via adaptive linear regression. IEEE International Conference on Computer Vision (ICCV), p. 153-160, 2011. R. Valenti and T. Gevers. Accurate eye center location through invariant isocentric patterns. IEEE Transactions on Pattern Analysis and Machine Intelligence, Volume 34 Issue 9, p. 1785-1798, 2012. A. Villanueva, R. Cabeza, and S. Porta. Eye tracking: pupil orientation geometrical modeling. Image and Vision Computing, Volume 24 Issue 7, p. 663679, 2006. E.D. Guestrin and M. Eizenman. General theory of remote gaze estimation using the pupil center and corneal reflections. IEEE Transactions on Biomedical Engineering, Volume 53 Issue 6, p. 1124-1 133, 2006. E. D. Guestrin and M. Eizenman. Remote point-of-gaze estimation requiring a single-point calibration for applications with infants. Symposium on Eye Tracking Research and Applications (ETRA), p. 267-274, 2008. D.W Hansen and Q Ji. In the eye of the beholder: a survey of models for eyes and gaze. IEEE Transactions on Pattern Analysis and Machine Intelligence, Volume 32 Issue 3, p. 478-500, 2010. T. Judd, K. Ehinger, F. Durand, and A. Torralba. Learning to predict where humans look. IEEE International Conference</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]  on Computer Vision (ICCV), p. 2106-21 13, 2009. K. Tan, D. Kriegman, and N. Ahuja. Appearance-based eye gaze estimation. Applications of Computer Vision, p. 191195, 2002. J. Chen and Q. Ji. Probabilistic gaze estimation without active personal calibration. IEEE International Conference on Computer Vision (ICCV), p. 609-616, 2011. S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, Volume 229, p. 2323-2326, 2000. Tobii Technology: http://www.tobii.com/. X. Zhu and D. Ramanan. Face detection, pose estimation, and landmark localization in the wild. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), p. 28792886, 2012. B. Russell, A. Torralba, K. Murphy, and W. Freeman. Labelme: a database and web-based tool for image annotation.. MIT AI Lab Memo AIM-2005-025, MIT CSAIL, 2005.  Figure 5. Gaze estimation results for the first four images with subject 3. The red traces represent the estimated gaze points while the blue traces represent the ground truth obtained from the Tobii gaze estimator. The results are achieved using 2D-manifold and K-closest points. 144</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
