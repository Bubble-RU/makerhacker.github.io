<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-9" href="../iccv2013/iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">iccv2013-9</a> <a title="iccv-2013-9-reference" href="#">iccv2013-9-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</h1>
<br/><p>Source: <a title="iccv-2013-9-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Thomas_A_Flexible_Scene_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Diego Thomas, Akihiro Sugimoto</p><p>Abstract: Updating a global 3D model with live RGB-D measurements has proven to be successful for 3D reconstruction of indoor scenes. Recently, a Truncated Signed Distance Function (TSDF) volumetric model and a fusion algorithm have been introduced (KinectFusion), showing significant advantages such as computational speed and accuracy of the reconstructed scene. This algorithm, however, is expensive in memory when constructing and updating the global model. As a consequence, the method is not well scalable to large scenes. We propose a new flexible 3D scene representation using a set of planes that is cheap in memory use and, nevertheless, achieves accurate reconstruction of indoor scenes from RGB-D image sequences. Projecting the scene onto different planes reduces significantly the size of the scene representation and thus it allows us to generate a global textured 3D model with lower memory requirement while keeping accuracy and easiness to update with live RGB-D measurements. Experimental results demonstrate that our proposed flexible 3D scene representation achieves accurate reconstruction, while keeping the scalability for large indoor scenes.</p><br/>
<h2>reference text</h2><p>[1] KinFu Large Scene code: http : / /po int c l ouds . o rg / document at i / tut ori al s /us ing_kin fu_ on l arge_s cal e .php. 3, 6</p>
<p>[2] SIFT-GPU code: http : / /www . c s .unc . edu / ˜ccwu / s i gpu / . 5, 6 ft</p>
<p>[3] P. Besl and N. McKay. A method for registration of 3-d</p>
<p>[4]</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]  shapes. IEEE Trans on PAMI, 14(2):239–256, 1992. 2 D. Borrmann, J. Elseberg, K. Lingermann, and A. Nuhter. The 3d hough transform for plane detection in point clouds: A review and new accumulator design. 3D Research 02, pages 1330–1332, 2011. 3 E. Candes, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? Journal of the ACM, 58(3): 11:1–1 1:37, 2011. 3 J. Chen, D. Bautembach, and S. Izadi. Scalable real-time volumetric surface reconstruction. ACM Transaction on Graphics - SIGGRAPH’13, 32(4): 1132: 1–1 13:8, 2013. 3 T. Cormen, C. Leiserson, R. Rivest, and C. Stein. Introduction to algorithms, sec. 26.2, ”the floyd-warshall algorithm”. MIT Press and McGraw-Hill, pages 558–565, 2001. 3, 4 A. Davison, I. Reid, N. Molton, and O. Stasse. Monoslam: Real-time single camera slam. IEEE Trans. on PAMI, pages 1052–1067, 2007. 1 P. Henry, D. Fox, A. Bhowmik, and R. Mongia. Patch volumes: Segmentation-based consistent mapping with rgb-d cameras. Proc. of International Conference on 3D Vision (3DV’13), 2013. 3 P. Henry, M. Krainin, E. Herbst, X. Ren, and D. Fox. Rgbd mapping: Using depth cameras for dense 2d modeling of indoor environments. Proc. of International Symposium on Experimental Robotics, 2010. 1, 5 T. Jaeggli, T. Konenckx, and L. Gool. Online 3d acquisition and model integration. Proc. of Procam’03, 2003. 1 M. Kazhdan, M. Bolitho, and H. Hoppe. Poisson surface reconstruction. Proc. of Eurographics Symposium on Geometry, 2006. 1 R. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim,</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]</p>
<p>[19]</p>
<p>[20]</p>
<p>[21]</p>
<p>[22]</p>
<p>[23]  A. Davison, P. Kohli, J. Shotton, S. Hodges, and A. Fitzgibbon. Kinectfusion: Real-time dense surface mapping and tracking. Proc. of ISMAR ’11, pages 127–136, 2011. 1, 2, 5 C. Nguyen, S. Izadi, and D. Lovell. Modeling kinect sensor noise for improved 3d reconstruction and tracking. Proc. of 3DIM/PVT’12, pages 524–530, 2012. 1, 2 H. Pfister, M. Zwicker, J. Baar, and M. Gross. Surfels: Surface elements as rendering primitives. In ACM Transactions on Graphics (Proc. of SIGGRAPH’00), 2000. 1, 2 H. Roth and M. Vona. Moving volume kinectfusion. Proc. of BMVC, 2012. 1, 3 A. Segal, D. Haehnel, and S. Thrun. Generalized-ICP. Robotics: Science and Systems, 2009. 2, 5 R. Shnabel, R. Wahl, and R. Klein. Efficient ransac for point-cloud shape detection. Computer Graphics Forum, 26(2):214–226, 2007. 3 R. Vidal, Y. Ma, and S. Sastry. Generalized principal component analysis (gpa). Proc. of CVPR’03, pages 621–628, 2003. 3 T. Weise, T. Wismer, B. Leibe, and L. Gool. In-hand scanning with online loop closure. Proc. of ICCV Workshops’09, pages 1630–1637, 2009. 2 T. Whelan, J. McDonald, M. Kaess, M. Fallon, H. Johansson, and J. Leonard. Kintinuous: Spatially extended kinectfusion. Proc. of RSS Workshop on RGB-D: Advanced Reasoning with Depth Camera, 2012. 1, 3 M. Zeng, F. Zhao, J. Zheng, and X. Liu. Octree-based fusion for realtime 3d reconstruction. Transaction of Graphical Models, 75(3): 126–136, 2013. 3 Q.-Y. Zhou and V. Koltun. Dense scene reconstruction with points of interest. ACM Transaction on Graphics - SIG-  GRAPH’13, 32(4): 112: 1–1 12:8, 2013. 3 2807</p>
<br/>
<br/><br/><br/></body>
</html>
