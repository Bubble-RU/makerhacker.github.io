<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>452 iccv-2013-YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-452" href="../iccv2013/iccv-2013-YouTube2Text%3A_Recognizing_and_Describing_Arbitrary_Activities_Using_Semantic_Hierarchies_and_Zero-Shot_Recognition.html">iccv2013-452</a> <a title="iccv-2013-452-reference" href="#">iccv2013-452-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>452 iccv-2013-YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition</h1>
<br/><p>Source: <a title="iccv-2013-452-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Guadarrama_YouTube2Text_Recognizing_and_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Sergio Guadarrama, Niveda Krishnamoorthy, Girish Malkarnenkar, Subhashini Venugopalan, Raymond Mooney, Trevor Darrell, Kate Saenko</p><p>Abstract: Despite a recent push towards large-scale object recognition, activity recognition remains limited to narrow domains and small vocabularies of actions. In this paper, we tackle the challenge of recognizing and describing activities “in-the-wild”. We present a solution that takes a short video clip and outputs a brief sentence that sums up the main activity in the video, such as the actor, the action and its object. Unlike previous work, our approach works on out-of-domain actions: it does not require training videos of the exact activity. If it cannot find an accurate prediction for a pre-trained model, it finds a less specific answer that is also plausible from a pragmatic standpoint. We use semantic hierarchies learned from the data to help to choose an appropriate level of generalization, and priors learned from web-scale natural language corpora to penalize unlikely combinations of actors/actions/objects; we also use a web-scale language model to “fill in ” novel verbs, i.e. when the verb does not appear in the training set. We evaluate our method on a large YouTube corpus and demonstrate it is able to generate short sentence descriptions of video clips better than baseline approaches.</p><br/>
<h2>reference text</h2><p>[1]  H. Aradhye, G. Toderici, and J. Yagnik. Video2text: Learning to annotate video  content. In Data Mining Workshops, 2009. ICDMW ’09. IEEE International Conference on, pages 144–151, 2009. 2</p>
<p>[2]  A. Barbu,  A. Bridge,  Z. Burchill,  D. Coroian,  S. Dickinson,  S. Fidler,  A. Michaux, S. Mussman, S. Narayanaswamy, D. Salvi, et al. Video in sen-  tences out. In Proceedings of the 28th Conference on Uncertainty in Artificial Intelligence (UAI), pages 102–12, 2012. 1</p>
<p>[3]  C. Chang and C. Lin. Libsvm:  a library for support vector machines.  ACM  Trans. on Intelligent Systems and Technology (TIST), 2(3):27, 2011. 4, 5</p>
<p>[4]  D. L. Chen and W. B. Dolan.  Collecting highly parallel data for paraphrase  evaluation. In Proceddings of ACL, 2013, pages 190–200, Portland, Oregon, 2011. 3</p>
<p>[5] P. Das, C. Xu, R. F. Doell, and J. J. Corso. A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object  stitching. In Computer Vision and Pattern Recognition (CVPR), 2013., pages  2634–2641. IEEE Computer Society, 2013. 1, 2</p>
<p>[6] J. Deng, J. Krause, A. C. Berg, and L. Fei-Fei. Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition. In Computer  Vision and Pattern Recognition (CVPR), 2012., pages 3450–3457. IEEE, 2012.  2, 3, 4, 5, 8</p>
<p>[7] J. Deng, K. Li, M. Do, H. Su, and L. Fei-Fei. Construction and Analysis of a  Large Scale Image Ontology. In Vision Sciences Society, 2009. 1</p>
<p>[8] D. Ding, F. Metze, S. Rawat, P. Schulam, S. Burger, E. Younessian, L. Bao, M. Christel, and A. Hauptmann. Beyond audio and video retrieval: towards  multimedia summarization. In Proceedings of the 2nd ACM International Conference on Multimedia Retrieval, page 2. ACM, 2012. 2</p>
<p>[9] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The  pascal visual object classes (voc) challenge. International Journal of Computer Vision, 88(2):303–338, June 2010. 3  Vision,</p>
<p>[10] A. Farhadi, M. Hejrati, M. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier, and D. Forsyth. Every picture tells a story: Generating sentences from images.  Computer Vision–ECCV 2010, pages 15–29, 2010. 2</p>
<p>[11] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part-based models. IEEE Transactions  on Pattern Analysis and Machine Intelligence., 32(9):1627–1645, 2010. 4, 5</p>
<p>[12] M. Khan and Y. Gotoh.  Describing video contents in natural language.  Proceedings of the EACL Workshop on Innovative Hybrid Approaches to the Processing of Textual Data, pages 27–35, 2012. 1, 2</p>
<p>[13] A. Kojima, T. Tamura, and K. Fukunaga. Natural language description of human activities from video images based on concept hierarchy of actions.  International Journal of Computer Vision, 50(2):171–184, 2002. 2</p>
<p>[14] N. Krishnamoorthy, G. Malkarnenkar, R. J. Mooney, K. Saenko, and S. Guadarrama. Generating natural-language video descriptions using text-mined knowl-  edge. In Procedings of AAAI, 2013, 2013. 2, 3, 4</p>
<p>[15]  G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. Berg, and T. Berg. Baby talk: Understanding and generating simple image descriptions. In Computer  Vision and Pattern Recognition (CVPR), 2011., pages 1601–1608. IEEE, 2011. 2</p>
<p>[16] I. Laptev and P. Perez.  Retrieving  actions in movies.  In International  Conference on Computer Vision, 2007. ICCV 2007., pages 1–8. IEEE, 2007. 3</p>
<p>[17] M. Lee, A. Hakeem, N. Haering, and S. Zhu. Save: A framework for seman-  tic annotation of visual events. In Computer Vision and Pattern Recognition Workshops, 2008. CVPRW’08., pages 1–8. IEEE, 2008. 2</p>
<p>[18] L. Li, H. Su, E. Xing, and L. Fei-Fei. Object bank: A high-level image representation for scene classification and semantic feature sparsification. Advances  in Neural Information Processing Systems, 24, 2010. 4, 5</p>
<p>[19]  S. Li, G. Kulkarni, T. Berg, A. Berg, and Y. Choi. Composing simple image de-  scriptions using web-scale n-grams. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 220–228. Association for  Computational Linguistics, 2011. 1, 2</p>
<p>[20] T. Motwani and R. Mooney. Improving video activity recognition using object  recognition and text mining. In European Conference on Artificial Intelligence.  ECAI, 2012. 2, 3</p>
<p>[21] T. Pedersen, S. Patwardhan, and J. Michelizzi. Wordnet:: Similarity: measuring  the relatedness of concepts. In Demonstration Papers at HLT-NAACL 2004,  pages 38–41 . Association for Computational Linguistics, 2004. 3</p>
<p>[22] F. Pereira, N. Tishby, and L. Lee. Distributional clustering of english words.  In Proceedings of the 31st annual meeting on Association for Computational Linguistics, pages 183–190. Association for Computational Linguistics, 1993. 3</p>
<p>[23] J. C. Platt.  Probabilistic outputs for support vector machines and compar-  isons to regularized likelihood methods. In ADVANCES IN LARGE MARGIN CLASSIFIERS, pages 61–74. MIT Press, 1999. 4</p>
<p>[24]  K. Reddy and M. Shah. Recognizing 50 human action categories of web videos.  Machine Vision and Applications, pages 1–11, 2012. 1, 3</p>
<p>[25] B. Russell, A. Torralba, K. Murphy, and W. T. Freeman. Labelme: a database  and web-based tool for image annotation. In International Journal of Computer  Vision, 2007. 1 Vision,</p>
<p>[26]  C. Schuldt, I. Laptev, and B. Caputo. Recognizing human actions: A local svm  approach. In Pattern Recognition, 2004. ICPR 2004., volume 3, pages 32–36.  IEEE, 2004. 3</p>
<p>[27] H. Wang, A. Klaser, C. Schmid, and C.-L. Liu. Action recognition by dense  trajectories. In Computer Vision and Pattern Recognition (CVPR), 2011., pages 3169–3176. IEEE, 201 1. 1, 4, 5</p>
<p>[28] Z. Wu and M. Palmer. Verbs semantics and lexical selection. In Proceedings of  the 32nd annual meeting on Association for Computational Linguistics, pages  133–138. Association for Computational Linguistics, 1994. 3</p>
<p>[29] Y. Yang, C. L. Teo, H. Daum e´, III, and Y. Aloimonos. Corpus-guided sentence  generation of natural images. In Proc. of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 444–454, 2011. 2</p>
<p>[30] B. Yao, X. Yang, L. Lin, M. Lee, and S. Zhu. I2t: Image parsing to text de-  scription. Proceedings of the IEEE, 98(8):1485–1508, 2010. 2 [3 1] J. Zhang, M. Marszałek, S. Lazebnik, and C. Schmid. Local features and kernels for classification of texture and object categories: A comprehensive study.  International Journal of Computer Vision, 73(2):213–238, 2007. 4, 5 2718  hierarchical method; (HE) the method in [6]. The top examples are ones where our model does better than FL and HE, and the bottom three are examples where it does similar or worse. 2719</p>
<br/>
<br/><br/><br/></body>
</html>
