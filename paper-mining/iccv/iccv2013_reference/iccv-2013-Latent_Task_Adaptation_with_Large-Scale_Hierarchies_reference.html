<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-233" href="../iccv2013/iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">iccv2013-233</a> <a title="iccv-2013-233-reference" href="#">iccv2013-233-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</h1>
<br/><p>Source: <a title="iccv-2013-233-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Jia_Latent_Task_Adaptation_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Yangqing Jia, Trevor Darrell</p><p>Abstract: Recent years have witnessed the success of large-scale image classification systems that are able to identify objects among thousands of possible labels. However, it is yet unclear how general classifiers such as ones trained on ImageNet can be optimally adapted to specific tasks, each of which only covers a semantically related subset of all the objects in the world. It is inefficient and suboptimal to retrain classifiers whenever a new task is given, and is inapplicable when tasks are not given explicitly, but implicitly specified as a set of image queries. In this paper we propose a novel probabilistic model that jointly identifies the underlying task and performs prediction with a lineartime probabilistic inference algorithm, given a set of query images from a latent task. We present efficient ways to estimate parameters for the model, and an open-source toolbox to train classifiers distributedly at a large scale. Empirical results based on the ImageNet data showed significant performance increase over several baseline algorithms.</p><br/>
<h2>reference text</h2><p>[1] J. T. Abbott, J. L. Austerweil, and T. L. Griffiths. Constructing a hypothesis space from the web for large-scale bayesian word learning. In Annu. Conf. Cog. Sci. Soc., 2012. 2, 3</p>
<p>[2] A. Berg, J. Deng, and L. Fei-Fei. ILSVRC 2010. http : / /www . image-net . o rg/ cha l enge s / l LSVRC/2 0 10 /, 2008. 1, 3, 5</p>
<p>[3] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Ng. Large scale distributed deep networks. In NIPS, 2012. 5</p>
<p>[4] J. Deng, J. Krause, A. C. Berg, and L. Fei-Fei. Hedging your bets: Optimizing accuracy-specificity trade-offs in large</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]  scale visual recognition. In CVPR, 2012. 2, 6 J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12:2121–2159, 2010. 2, 4, 5 R. Farrell, O. Oza, N. Zhang, V. I. Morariu, T. Darrell, and L. S. Davis. Birdlets: Subordinate categorization using volumetric primitives and pose-normalized appearance. In ICCV, 2011. 1 C. Fellbaum. Wordnet. Theory and Applications of Ontology: Computer Applications, pages 23 1–243, 2010. 3 T. Gao and D. Koller. Discriminative learning of relaxed hierarchy for large-scale visual recognition. In ICCV. IEEE, 2011. 2 L. K. Hansen and J. Larsen. Linear unlearning for cross-validation. Advances in Computational Mathematics, 5(1):269–280, 1996. 4 Z. Harchaoui, M. Douze, M. Paulin, M. Dudik, and J. Malick. Large-scale image classification with trace-norm regularization. In CVPR, 2012. 2 D. Jurafsky and J. H. Martin. Speech & Language Processing. Pearson Prentice Hall, 2000. 4, 5 A. Khosla, N. Jayadevaprakash, B. Yao, and L. Fei-Fei. Novel dataset for fine-grained image categorization. In CVPR FGVC workshop, 2011. 1 A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012. 1, 5 B. Kulis, K. Saenko, and T. Darrell. What you saw is not what you get: Domain adaptation using asymmetric kernel transforms. In CVPR, 2011. 2 L.-J. Li, R. Socher, and L. Fei-Fei. Towards total scene un-</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]</p>
<p>[19]</p>
<p>[20]</p>
<p>[21]</p>
<p>[22]</p>
<p>[23]</p>
<p>[24]</p>
<p>[25]</p>
<p>[26]  derstanding: Classification, annotation and segmentation in an automatic framework. In CVPR, 2009. 2 Y. Lin, F. Lv, S. Zhu, M. Yang, T. Cour, K. Yu, L. Cao, and T. Huang. Large-scale image classification: fast feature extraction and svm training. In CVPR, 2011. 1, 5 E. M. Markman. Categorization and naming in children: Problems of induction. MIT Press, 1991. 3 R. Raina, A. Battle, H. Lee, B. Packer, and A. Y. Ng. Selftaught learning: transfer learning from unlabeled data. In ICML, 2007. 1 K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In ECCV, 2010. 2 R. Salakhutdinov, A. Torralba, and J. Tenenbaum. Learning to share visual appearance for multiclass object detection. In CVPR, 2011. 2 J. S ´anchez and F. Perronnin. High-dimensional signature compression for large-scale image classification. In CVPR, 2011. 1, 5 R. N. Shepard et al. Toward a universal law of generalization for psychological science. Science, 237(4820): 13 17–1323, 1987. 3 J. B. Tenenbaum, T. L. Griffiths, et al. Generalization, similarity, and bayesian inference. Behavioral and brain sciences, 24(4):629–640, 2001. 2, 3 J. B. Tenenbaum, T. L. Griffiths, C. Kemp, et al. Theorybased bayesian models of inductive learning and reasoning. Trends in cognitive sciences, 10(7):309–3 18, 2006. 3 A. Torralba. Contextual priming for object detection. IJCV, 53(2): 169–191, 2003. 2 F. Xu, J. B. Tenenbaum, et al. Word learning as bayesian inference. Psychological review, 114(2):245, 2007. 2  2087</p>
<br/>
<br/><br/><br/></body>
</html>
