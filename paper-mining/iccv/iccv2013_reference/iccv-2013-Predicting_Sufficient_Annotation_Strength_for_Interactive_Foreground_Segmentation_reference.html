<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-326" href="../iccv2013/iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">iccv2013-326</a> <a title="iccv-2013-326-reference" href="#">iccv2013-326-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</h1>
<br/><p>Source: <a title="iccv-2013-326-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Jain_Predicting_Sufficient_Annotation_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Suyog Dutt Jain, Kristen Grauman</p><p>Abstract: The mode of manual annotation used in an interactive segmentation algorithm affects both its accuracy and easeof-use. For example, bounding boxes are fast to supply, yet may be too coarse to get good results on difficult images; freehand outlines are slower to supply and more specific, yet they may be overkill for simple images. Whereas existing methods assume a fixed form of input no matter the image, we propose to predict the tradeoff between accuracy and effort. Our approach learns whether a graph cuts segmentation will succeed if initialized with a given annotation mode, based on the image ’s visual separability and foreground uncertainty. Using these predictions, we optimize the mode of input requested on new images a user wants segmented. Whether given a single image that should be segmented as quickly as possible, or a batch of images that must be segmented within a specified time budget, we show how to select the easiest modality that will be sufficiently strong to yield high quality segmentations. Extensive results with real users and three datasets demonstrate the impact.</p><br/>
<h2>reference text</h2><p>[1] D. Batra, A. Kowdle, D. Parikh, J. Luo, and T. Chen. iCoseg: Interactive cosegmentation with intelligent scribble guidance. In CVPR, 2010. 1, 2, 5, 6</p>
<p>[2] Y. Boykov and M. Jolly. Interactive graph cuts for optimal boundary and region segmentation of objects in N-D images. In CVPR, 2001 . 1, 2, 3  ag.e%Alp()Ovr89 824605T1Uos0taelrSn15tu0odayiR2n0etsiOmuGERlaet2Tuf5so(n0r−uibdsIrnoBatmpl3uPFidrn0teguadet.s)3[5r20es]  Figure 6: Left: Annotation choices under a budget with real  user data. Right: Example user annotations  for bounding box  (top), sloppy contour (middle), and tight polygon (bottom).</p>
<p>[3] Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy minimization via graph cuts. PAMI, 23(1 1): 1222–1239, Nov. 2001. 3</p>
<p>[4] J. Carreira and C. Sminchisescu. CPMC: Automatic object segmentation using constrained parametric min-cuts. PAMI, 34(7): 1312–1328, 2012. 3</p>
<p>[5] I. Endres and D. Hoiem. Category independent object proposals. In ECCV, 2010. 3</p>
<p>[6] P. F. Felzenszwalb and D. P. Huttenlocher. Efficient graph-based image segmentation. IJCV, 59(2):167–181, Sept. 2004. 4</p>
<p>[7] V. Gulshan, C. Rother, A. Criminisi, A. Blake, and A. Zisserman. Geodesic star convexity for interactive image segmentation. In CVPR, 2010. 1, 2, 5</p>
<p>[8] M. Kass, A. Witkin, and D. Terzopoulos. Snakes: Active contour models. IJCV, pages 321–331, 1988. 1, 2</p>
<p>[9] T. Kohlberger, V. Singh, C. Alvino, C. Bahlmann, and L. Grady. Evaluating segmentation error without ground truth. In MICCAI, 2012. 3</p>
<p>[10] P. Kohli, H. Nickisch, C. Rother, and C. Rhemann. User-centric learning and evaluation of interactive segmentation systems. IJCV, 100(3):261–274, Dec. 2012. 2</p>
<p>[11] P. Kohli and P. H. S. Torr. Measuring uncertainty in graph cut solutions. CVIU, 112(1):30–38, 2008. 4</p>
<p>[12] V. S. Lempitsky, P. Kohli, C. Rother, and T. Sharp. Image segmentation with a bounding box prior. In ICCV, 2009. 1, 2</p>
<p>[13] D. Liu, Y. Xiong, K. Pulli, and L. Shapiro. Estimating image segmentation difficulty. In Machine learning and data mining in pattern recognition, 2011. 3</p>
<p>[14] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y. Shum. Learning to detect a salient object. PAMI, 33(2):353–367, Feb. 201 1. 4</p>
<p>[15] T. Malisiewicz and A. A. Efros. Improving spatial support for objects via multiple segmentations. In BMVC, 2007. 5</p>
<p>[16] E. Mortensen and W. Barrett. Intelligent scissors for image composition. In SIGGRAPH, 1995. 1, 2</p>
<p>[17] N. Otsu. A Threshold Selection Method from Gray-level Histograms. IEEE Trans on Sys, Man and Cybernetics, 9(1):62–66, Jan. 1979. 5</p>
<p>[18] X. Ren and J. Malik. Learning a classification model for segmentation. In ICCV, 2003. 3</p>
<p>[19] C. Rother, V. Kolmogorov, and A. Blake. “grabcut”: interactive foreground extraction using iterated graph cuts. 2004. 1, 2, 3</p>
<p>[20] B. Siddiquie and A. Gupta. Beyond Active Noun Tagging: Modeling Contextual Interactions for Multi-Class Active Learning. In CVPR, 2010. 2</p>
<p>[21] A. Vezhnevets, J. Buhmann, and V. Ferrari. Active learning for semantic segmentation with expected change. In CVPR, 2012. 2</p>
<p>[22] S. Vijayanarasimhan and K. Grauman. What’s it going to cost you?: Predicting effort vs. informativeness for multi-label image annotations. In CVPR, 2009. 2, 3, 5, 6</p>
<p>[23] S. Vijayanarasimhan and K. Grauman. Active frame selection for label propagation in videos. In ECCV, 2012. 2</p>
<p>[24] S. Vijayanarasimhan, P. Jain, and K. Grauman. Far-sighted active learning on a budget for image and video recognition. In CVPR, 2010. 2</p>
<p>[25] C. Vondrick and D. Ramanan. Video annotation and tracking with active learn-  ing. In NIPS, 2011. 2</p>
<p>[26] C. Vondrick, D. Ramanan, and D. Patterson. Efficiently scaling up video annotation with crowdsourced marketplaces. In ECCV, 2010. 2</p>
<p>[27] C. Wah, S. Branson, P. Perona, and S. Belongie. Multiclass recognition and part localization with humans in the loop. In ICCV, 2011. 2</p>
<p>[28] D. Wang, C. Yan, S. Shan, and X. Chen. Active learning for interactive segmentation with expected confidence change. In ACCV, 2012. 2, 6 1320</p>
<br/>
<br/><br/><br/></body>
</html>
