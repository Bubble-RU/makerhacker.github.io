<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-41" href="../iccv2013/iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">iccv2013-41</a> <a title="iccv-2013-41-reference" href="#">iccv2013-41-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</h1>
<br/><p>Source: <a title="iccv-2013-41-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Bandla_Active_Learning_of_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Sunil Bandla, Kristen Grauman</p><p>Abstract: Collecting and annotating videos of realistic human actions is tedious, yet critical for training action recognition systems. We propose a method to actively request the most useful video annotations among a large set of unlabeled videos. Predicting the utility of annotating unlabeled video is not trivial, since any given clip may contain multiple actions of interest, and it need not be trimmed to temporal regions of interest. To deal with this problem, we propose a detection-based active learner to train action category models. We develop a voting-based framework to localize likely intervals of interest in an unlabeled clip, and use them to estimate the total reduction in uncertainty that annotating that clip would yield. On three datasets, we show our approach can learn accurate action detectors more efficiently than alternative active learning strategies that fail to accommodate the “untrimmed” nature of real video data.</p><br/>
<h2>reference text</h2><p>[1] mmlab.disi.unitn.it/wiki/index.php/Shot Boundary Detection.</p>
<p>[2] M. Blank, L. Gorelick, E. Shechtman, M. Irani, and R. Basri. Actions as Space-Time Shapes. In ICCV, 2005.</p>
<p>[3] D. Cohn, Z. Ghahramani, and M. Jordan. Active Learning with Statistical Models. JAIR, 4: 129–145, 1996.</p>
<p>[4] O. Duchenne, I. Laptev, J. Sivic, F. Bach, and J. Ponce. Automatic Annotation of Human Actions in Video. In CVPR, 2009.</p>
<p>[5] A. Fathi, M. Balcan, X. Ren, and J. Rehg. Combining Self Training and Active Learning for Video Segmentation. In BMVC, 2011.</p>
<p>[6] Y. Guo and R. Greiner. Optimistic Active Learning using Mutual Information. In IJCAI, 2007.</p>
<p>[7] A. Kl¨ aser, M. Marszałek, C. Schmid, and A. Zisserman. Human Focused Action Localization in Video. In SGA Wkshp, 2010.</p>
<p>[8] A. Kovashka, S. Vijayanarasimhan, and K. Grauman. Actively Selecting Annotations among Objects and Attributes. In ICCV, 2011.</p>
<p>[9] I. Laptev and T. Lindeberg. Space-time Interest Points. In ICCV, 2003.</p>
<p>[10] I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld. Learning Realistic Human Actions From Movies. In CVPR, 2008.</p>
<p>[11] M. Marszalek, I. Laptev, and C. Schmid. Actions in Context. In CVPR, 2009.</p>
<p>[12] A. Oikonomopoulos, I. Patras, and M. Pantic. An Implicit Spatiotemporal Shape Model For Human Activity Localization And Recognition. In CVPR, 2008.</p>
<p>[13] B. Price, B. Morse, and S. Cohen. Livecut: Learning-based Interac-</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]</p>
<p>[19]</p>
<p>[20]</p>
<p>[21]</p>
<p>[22]</p>
<p>[23]</p>
<p>[24]</p>
<p>[25]</p>
<p>[26]</p>
<p>[27]</p>
<p>[28]</p>
<p>[29]</p>
<p>[30] [3 1]  tive Video Segmentation by Evaluation of Multiple Propagated Cues. In ICCV, 2009. G. Qi, X. Hua, Y. Rui, J. Tang, and H. Zhang. Two-dimensional Active Learning for Image Classification. In CVPR, 2008. D. Ramanan and D. Forsyth. Automatic Annotation of Everyday Movements. In NIPS, 2003. M. Raptis, I. Kokkinos, and S. Soatto. Discovering Discriminative Action Parts from Mid-level Video Representations. In CVPR, 2012. N. Roy and A. McCallum. Toward Optimal Active Learning through Sampling Estimation of Error Reduction. In ICML, 2001. M. Ryoo and J. K. Aggarwal. UT-Interaction Dataset, 2010. S. Satkin and M. Hebert. Modeling the Temporal Extent of Actions. In ECCV, 2010. B. Siddiquie and A. Gupta. Beyond Active Noun Tagging: Modeling Contextual Interactions for Active Learning. In CVPR, 2010. S. Tong and E. Chang. Support Vector Machine Active Learning for Image Retrieval. In ACM Multimedia, 2001. S. Vijayanarasimhan and K. Grauman. Multi-level Active Prediction Of Useful Image Annotations For Recognition. In NIPS, 2009. S. Vijayanarasimhan and K. Grauman. Large-Scale Live Active Learning: Training Object Detectors With Crawled Data and Crowds. In CVPR, 2011. S. Vijayanarasimhan and K. Grauman. Active Frame Selection for Label Propagation in Videos. In ECCV, 2012. S. Vijayanarasimhan, P. Jain, and K. Grauman. Far-sighted Active Learning on a Budget for Image and Video Recognition. In CVPR, 2010. C. Vondrick, D. Patterson, and D. Ramanan. Efficiently Scaling Up Crowdsourced Video Annotation. IJCV, 2012. C. Vondrick and D. Ramanan. Video Annotation and Tracking with Active Learning. In NIPS, 2011. G. Willems, J. H. Becker, T. Tuytelaars, and L. V. Gool. Exemplarbased Action Recognition In Video. In ECCV, 2009. R. Yan, J. Yang, and A. Hauptmann. Automatically Labeling Data Using Multi-Class Active Learning. In ICCV, 2003. A. Yao, J. Gall, and L. V. Gool. A Hough Transform-based Voting Framework for Action Recognition. In CVPR, 2010. J. Yuan, Z. Liu, and Y. Wu. Discriminative Subvolume Search for  Efficient Action Detection. In CVPR, 2009. 11884400</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
