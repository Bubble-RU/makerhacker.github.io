<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-240" href="../iccv2013/iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">iccv2013-240</a> <a title="iccv-2013-240-reference" href="#">iccv2013-240-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</h1>
<br/><p>Source: <a title="iccv-2013-240-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Wang_Learning_Maximum_Margin_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Jiang Wang, Ying Wu</p><p>Abstract: Temporal misalignment and duration variation in video actions largely influence the performance of action recognition, but it is very difficult to specify effective temporal alignment on action sequences. To address this challenge, this paper proposes a novel discriminative learning-based temporal alignment method, called maximum margin temporal warping (MMTW), to align two action sequences and measure their matching score. Based on the latent structure SVM formulation, the proposed MMTW method is able to learn a phantom action template to represent an action class for maximum discrimination against other classes. The recognition of this action class is based on the associated learned alignment of the input action. Extensive experiments on five benchmark datasets have demonstrated that this MMTW model is able to significantly promote the accuracy and robustness of action recognition under temporal misalignment and variations.</p><br/>
<h2>reference text</h2><p>[1] R. Arandjelovic and A. Zisserman. Three things everyone should know to improve object retrieval. In CVPR, 2012.</p>
<p>[2] W. Brendel and S. Todorovic. Learning spatiotemporal graphs of human activities. In ICCV. Ieee, Nov. 201 1.</p>
<p>[3] N. Dalal and B. Triggs. Histograms of Oriented Gradients for Human Detection. In CVPR, pages 886–893. IEEE, 2005.</p>
<p>[4] H. Ding, G. Trajcevski, and P. Scheuermann. Querying and mining of time series data: experimental comparison of representations and distance measures. In PVLDB, volume 1, 2008.</p>
<p>[5] A. Gaidon, C. Schmid, and L. I. Grenoble. Actom Sequence Models for Efficient Action Detection. In CVPR, 2011.</p>
<p>[6] M. Hoai and F. De la Torre. Max-margin early event detectors. In CVPR, pages 2863–2870. Ieee, June 2012.</p>
<p>[7] A. Kovashka and K. Grauman. Learning a hierarchy of discriminative spacetime neighborhood features for human action recognition. In CVPR, pages 2046–2053, 2010.</p>
<p>[8] I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld. Learning realistic human actions from movies. In CVPR, volume 1, pages 1–8, 2008.</p>
<p>[9] Q. Le, W. Zou, S. Yeung, and A. Ng. Learning hierarchical invariant spatiotemporal features for action recognition with independent subspace analysis. In CVPR, 201 1.</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]</p>
<p>[19]</p>
<p>[20]</p>
<p>[21]</p>
<p>[22]</p>
<p>[23]</p>
<p>[24]</p>
<p>[25]</p>
<p>[26]  K. Li, J. Hu, and Y. Fu. Modeling Complex Temporal Composition of Actionlets for Activity Prediction. In ECCV, pages 286–299, 2012. W. Li, Z. Zhang, and Z. Liu. Action recognition based on a bag of 3d points. In Human Communicative Behavior Analysis Workshop (in conjunction with CVPR), 2010. F. Lv and R. Nevatia. Recognition and Segmentation of 3-D Human Action Using HMM and Multi-class AdaBoost. In ECCV, pages 359–372, 2006. Minh Hoai and F. D. Torre. Maximum Margin Temporal Clustering. In ICML, volume XX, 2012. M. Muller and T. R ¨oder. Motion templates for automatic classification and retrieval of motion capture data. In Proceedings of the 2006 ACM SIGGRAPH/Eurographics symposium on Computer animation, pages 137–146. Eurographics Association, 2006. J. C. Niebles, C.-w. Chen, and L. Fei-fei. Modeling Temporal Structure of Decomposable Motion Segments for Activity Classification. In ECCV, pages 1–14, 2010. O. Oreifej and Z. Liu. HON4D : Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences. In CVPR, 2013. M. Pei, Y. Jia, and S.-c. Zhu. Parsing Video Events with Goal inference and Intent Prediction. In ICCV, 201 1. M. D. Rodriguez, J. Ahmed, and M. Shah. Action MACH:a spatio-temporal Maximum Average Correlation Height filter for action recognition. In CVPR, pages 1–8. Ieee, June 2008. S. Sadanand and J. Corso. Action bank: A high-level representation of activity in video. In CVPR, number May, 2012. J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake. Real-time human pose recognition in parts from single depth images. In CVPR, 2011. T. Simon, M. H. Nguyen, F. D. La, and J. F. Cohn. Action Unit Detection with Segment-based SVMs. In CVPR, 2010. K. Tang and D. Koller. Learning latent temporal structure for complex event detection. In CVPR, pages 1250–1257. Ieee, June 2012. D. Tran and J. Yuan. Max-Margin Structured Output Regression for SpatioTemporal Action Localization. In NIPS, 2012. H. Wang, M. M. Ullah, A. Klaser, I. Laptev, and C. Schmid. Evaluation of local spatio-temporal features for action recognition. In BMVC. British Machine Vision Association, 2009. J. Wang, Z. Liu, J. Chorowski, Z. Chen, and Y. Wu. Robust 3D Action Recognition with Random Occupancy Patterns. In ECCV, pages 1–14, 2012. J. Wang, Z. Liu, Y. Wu, and J. Yuan. Mining Actionlet Ensemble for Action Recognition with Depth Cameras. In CVPR, 2012.</p>
<p>[27] X. Wu, D. Xu, L. Duan, and J. Luo. Action recognition using context and appearance distribution features. In CVPR. Ieee, June 201 1.</p>
<p>[28] L. Xia, C.-c. Chen, and J. K. Aggarwal. View Invariant Human Action Recognition Using Histograms of 3D Joints The University of Texas at Austin. In CVPR 2012 HAU3D Workshop.</p>
<p>[29] T. Xiang and S. Gong. Beyond Tracking: Modelling Activity and Understanding Behaviour. International Journal of Computer Vision, 67(1):21–51, Apr. 2006.</p>
<p>[30] X. Yang and Y. Tian. EigenJoints-based Action Recognition Using Na¨ ıveBayes-Nearest-Neighbor. In CVPR 2012 HAU3D Workshop, 2012. [3 1] X. Yang, C. Zhang, and Y. Tian. Recognizing Actions Using Depth Motion Maps-based Histograms of Oriented Gradients. In ACM Multimedia, 2012.</p>
<p>[32] B. Yao and S.-C. Zhu. Learning deformable action templates from cluttered videos. In ICCV, pages 1507–1514. IEEE, Sept. 2009.</p>
<p>[33] C. Yu and T. Joachims. Learning structural SVMs with latent variables. In Proceedings of the 26th Annual International Conference on Machine Learning, New York, New York, USA, 2009. ACM. 2695</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
