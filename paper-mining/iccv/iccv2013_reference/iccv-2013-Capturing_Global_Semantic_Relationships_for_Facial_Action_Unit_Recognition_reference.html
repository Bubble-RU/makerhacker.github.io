<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>69 iccv-2013-Capturing Global Semantic Relationships for Facial Action Unit Recognition</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-69" href="../iccv2013/iccv-2013-Capturing_Global_Semantic_Relationships_for_Facial_Action_Unit_Recognition.html">iccv2013-69</a> <a title="iccv-2013-69-reference" href="#">iccv2013-69-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>69 iccv-2013-Capturing Global Semantic Relationships for Facial Action Unit Recognition</h1>
<br/><p>Source: <a title="iccv-2013-69-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Wang_Capturing_Global_Semantic_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Ziheng Wang, Yongqiang Li, Shangfei Wang, Qiang Ji</p><p>Abstract: In this paper we tackle the problem of facial action unit (AU) recognition by exploiting the complex semantic relationships among AUs, which carry crucial top-down information yet have not been thoroughly exploited. Towards this goal, we build a hierarchical model that combines the bottom-level image features and the top-level AU relationships to jointly recognize AUs in a principled manner. The proposed model has two major advantages over existing methods. 1) Unlike methods that can only capture local pair-wise AU dependencies, our model is developed upon the restricted Boltzmann machine and therefore can exploit the global relationships among AUs. 2) Although AU relationships are influenced by many related factors such as facial expressions, these factors are generally ignored by the current methods. Our model, however, can successfully capture them to more accurately characterize the AU relationships. Efficient learning and inference algorithms of the proposed model are also developed. Experimental results on benchmark databases demonstrate the effectiveness of the proposed approach in modelling complex AU relationships as well as its superior AU recognition performance over existing approaches.</p><br/>
<h2>reference text</h2><p>[1] M. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel, and J. Movellan. Recognizing facial expression: machine learning and application to spontaneous behavior. In CVPR, pages 568–573, 2005.</p>
<p>[2]</p>
<p>[3]</p>
<p>[4]</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]  2 J. Bazzo and M. Lamar. Recognizing facial actions using gabor wavelets with neutral face average difference. In Automatic Face and Gesture Recognition, IEEE International Conference on, pages 505–510, 2004. 2 T. Cootes, G. Edwards, and C. Taylor. Active appearance models. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 23(6):681–685, Jun. 6 P. Ekman and W. Friesen. Facial Action Coding System: A Technique for the Measurement of Facial Movement. Consulting Psychologists Press, Palo Alto, 1978. 1 G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Comput., 14(8): 1771–1800, Aug. 2002. 3 G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural Comput., 18(7): 1527–1554, July 2006. 2 B. Jiang, M. Valstar, and M. Pantic. Action unit detection using sparse appearance descriptors in space-time video volumes. In Automatic Face Gesture Recognition and Workshops, IEEE International Conference on, pages 3 14–321 , 2011. 2, 6, 7 T. Kanade, J. Cohn, and Y. Tian. Comprehensive database for facial expression analysis. In Automatic Face and Gesture Recognition, 2000. Proceedings. Fourth IEEE International Conference on, pages 46 –53, 2000. 6 Y. Li, Y. Zhao, S. Wang, and Q. Ji. Simultaneous facial feature tracking and facial expression recognition. Image Processing, IEEE Transactions on, 2013. 7 J.-J. J. Lien, T. Kanade, J. Cohn, and C. Li. Detection, tracking, and classification of action units in facial expression. Journal of Robotics and Autonomous Systems, July 1999. 2 P. Lucey, J. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews. The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression. In CVPR Workshop, 2010. 2, 6, 7 M. H. Mahoor, M. Zhou, K. L. Veon, S. M. Mavadati, and J. F. Cohn. Facial action unit recognition with sparse representation. In In Au-</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]</p>
<p>[19]</p>
<p>[20]</p>
<p>[21]</p>
<p>[22]  tomatic Face & Gesture Recognition and Workshops, IEEE International Conference on, pages 336–342. IEEE, 2011. 2 G. Mckeown, M. F. Valstar, R. Cowie, M. Pantic, and M. Schroeder. The semaine database: Annotated multimodal records ofemotionally coloured conversations between a person and a limited agent. IEEE Transactions on Affective Computing, 3:5–17, April 2012. Issue 1. 6 V. Nair and G. E. Hinton. 3d object recognition with deep belief nets. In NIPS, pages 1339–1347, 2009. 2, 5 M. Pantic and I. Patras. Dynamics of facial expression: recognition of facial actions and their temporal segments from face profile image sequences. Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on, 36(2):433–449, 2006. 2 M. Pantic and L. Rothkrantz. Facial action recognition for facial expression analysis from static face images. Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on, 34(3): 1449 –1461, june 2004. 2 M. Pantic, M. Valstar, R. Rademaker, and L. Maat. Web-based database for facial expression analysis. In Multimedia and Expo, IEEE International Conference on. IEEE, 2005. J. M. Susskind, G. E. Hinton, J. R. Movellan, and A. K. Anderson. Generating facial expressions with deep belief nets. Affective Computing, Emotion Modelling, Synthesis and Recognition, pages 421– 440, 2008. 2 Y.-L. Tian, T. Kanade, and J. Cohn. Recognizing action units for facial expression analysis. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 23(2):97–1 15, 2001. 2 Y. Tong and Q. Ji. Learning bayesian networks with qualitative constraints. In CVPR, pages 1–8. IEEE, 2008. 2, 5, 7 Y. Tong, W. Liao, and Q. Ji. Facial action unit recognition by exploiting their dynamic and semantic relationships. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(10): 1683– 1699, Oct. 2007. 2, 7 M. F. Valstar, B. Jiang, M. Mehu, M. Pantic, and K. Scherer. The first facial expression recognition and analysis challenge. In Automatic Face & Gesture Recognition and Workshops, IEEE International Conference on, pages 921–926. IEEE, 2011. 7</p>
<p>[23] M. F. Valstar and M. Pantic. Fully automatic recognition of the temporal phases of facial actions. IEEE Transactions on Systems, Man, and Cybernetics, Part B, pages 28–43, 2012. 2</p>
<p>[24] V. Vapnik and A. Vashist. A new learning paradigm: Learning using privileged information. Neural Networks, 22:544–557, July 2009. 4</p>
<p>[25] J. Whitehill and C. Omlin. Haar features for facs au recognition. In Automatic Face and Gesture Recognition, 7th International Conference on, pages 97–101, 2006. 2 3333 10 14</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
