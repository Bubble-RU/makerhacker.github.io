<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-265" href="../iccv2013/iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">iccv2013-265</a> <a title="iccv-2013-265-reference" href="#">iccv2013-265-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</h1>
<br/><p>Source: <a title="iccv-2013-265-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Wang_Mining_Motion_Atoms_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Limin Wang, Yu Qiao, Xiaoou Tang</p><p>Abstract: This paper proposes motion atom and phrase as a midlevel temporal “part” for representing and classifying complex action. Motion atom is defined as an atomic part of action, and captures the motion information of action video in a short temporal scale. Motion phrase is a temporal composite of multiple motion atoms with an AND/OR structure, which further enhances the discriminative ability of motion atoms by incorporating temporal constraints in a longer scale. Specifically, given a set of weakly labeled action videos, we firstly design a discriminative clustering method to automatically discovera set ofrepresentative motion atoms. Then, based on these motion atoms, we mine effective motion phrases with high discriminative and representativepower. We introduce a bottom-upphrase construction algorithm and a greedy selection method for this mining task. We examine the classification performance of the motion atom and phrase based representation on two complex action datasets: Olympic Sports and UCF50. Experimental results show that our method achieves superior performance over recent published methods on both datasets.</p><br/>
<h2>reference text</h2><p>[1] J. K. Aggarwal and M. S. Ryoo. Human activity analysis: A review. ACM Comput. Surv., 43(3): 16, 2011.</p>
<p>[2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules in large databases. In VLDB, 1994.</p>
<p>[3] M. R. Amer, D. Xie, M. Zhao, S. Todorovic, and S. C. Zhu. Cost-sensitive top-down/bottom-up inference for multiscale activity</p>
<p>[4]</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]  recognition. In ECCV, pages 187–200, 2012. T. L. Berg, A. C. Berg, and J. Shih. Automatic attribute discovery and characterization from noisy web data. In ECCV, 2010. C.-C. Chang and C.-J. Lin. Libsvm: A library for support vector machines. ACM TIST, 2(3):27, 2011. P. F. Felzenszwalb, R. B. Girshick, D. A. McAllester, and D. Ramanan. Object detection with discriminatively trained part-based models. TPAMI, 32(9): 1627–1645, 2010. B. J. Frey and D. Dueck. Clustering by passing messages between data points. Science, 315:972–976, 2007. A. Gaidon, Z. Harchaoui, and C. Schmid. Actom sequence models for efficient action detection. In CVPR, 2011. L. Gorelick, M. Blank, E. Shechtman, M. Irani, and R. Basri. Actions as space-time shapes. TPAMI, 29(12):2247–2253, 2007. C. Gu, P. A. Arbel ´aez, Y. Lin, K. Yu, and J. Malik. Multi-component models for object detection. In ECCV, 2012. O. Kliper-Gross, Y. Gurovich, T. Hassner, and L. Wolf. Motion interchange patterns for action recognition in unconstrained videos. In ECCV, 2012. I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld. Learning realistic human actions from movies. In CVPR, 2008. B. Laxton, J. Lim, and D. J. Kriegman. Leveraging temporal, contextual and ordering constraints for recognizing complex activities in video. In CVPR, 2007. J. Liu, B. Kuipers, and S. Savarese. Recognizing human actions by attributes. In CVPR, 2011. J. C. Niebles, C.-W. Chen, and F.-F. Li. Modeling temporal structure of decomposable motion segments for activity classification. In ECCV, 2010. N. Oliver, B. Rosario, and A. Pentland. A bayesian computer vision system for modeling human interactions. TPAMI, 2000. X. Peng, Y. Qiao, Q. Peng, and X. Qi. Exploring motion boundary based sampling and spatial-temporal context descriptors for action recognition. In BMVC, 2013. K. K. Reddy and M. Shah. Recognizing 50 human action categories of web videos. MVAP, 2012.</p>
<p>[19] M. Rohrbach, M. Regneri, M. Andriluka, S. Amin, M. Pinkal, and B. Schiele. Script data for attribute-based recognition of composite activities. In ECCV, 2012.</p>
<p>[20] S. Sadanand and J. J. Corso. Action bank: A high-level representation of activity in video. In CVPR, 2012.</p>
<p>[21] C. Sch u¨ldt, I. Laptev, and B. Caputo. Recognizing human actions: A local svm approach. In ICPR, 2004.</p>
<p>[22] S. Singh, A. Gupta, and A. A. Efros. Unsupervised discovery of mid-level discriminative patches. In ECCV, 2012.</p>
<p>[23] K. Tang, F.-F. Li, and D. Koller. Learning latent temporal structure for complex event detection. In CVPR, 2012.</p>
<p>[24] H. Wang, A. Kl¨ aser, C. Schmid, and C.-L. Liu. Dense trajectories and motion boundary descriptors for action recognition. International Journal of Computer Vision, 103(1):60–79, 2013.</p>
<p>[25] L. Wang, Y. Qiao, and X. Tang. Motionlets: Mid-level 3d parts for human motion recognition. In CVPR, pages 2674–2681, 2013.</p>
<p>[26] L. Wang, Y. Qiao, and X. Tang. Latent hierarchical model of temporal structure for complex activity classification. TIP, to appear.</p>
<p>[27] S. B. Wang, A. Quattoni, L.-P. Morency, D. Demirdjian, and T. Darrell. Hidden conditional random fields for gesture recognition. In CVPR, 2006.</p>
<p>[28] X. Wang, L. Wang, and Y. Qiao. A comparative study of encoding, pooling and normalization methods for action recognition. In ACCV, pages 572–585, 2012.</p>
<p>[29] B. Yao and F.-F. Li. Grouplet: A structured image representation for recognizing human and object interactions. In CVPR, 2010.</p>
<p>[30] J. Zhang, M. Marszalek, S. Lazebnik, and C. Schmid. Local features and kernels for classification of texture and object categories: A comprehensive study. IJCV, 73(2):213–238, 2007. 2687</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
