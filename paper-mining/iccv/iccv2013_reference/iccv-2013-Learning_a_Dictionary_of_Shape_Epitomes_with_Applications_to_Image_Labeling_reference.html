<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>245 iccv-2013-Learning a Dictionary of Shape Epitomes with Applications to Image Labeling</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-245" href="../iccv2013/iccv-2013-Learning_a_Dictionary_of_Shape_Epitomes_with_Applications_to_Image_Labeling.html">iccv2013-245</a> <a title="iccv-2013-245-reference" href="#">iccv2013-245-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>245 iccv-2013-Learning a Dictionary of Shape Epitomes with Applications to Image Labeling</h1>
<br/><p>Source: <a title="iccv-2013-245-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Chen_Learning_a_Dictionary_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Liang-Chieh Chen, George Papandreou, Alan L. Yuille</p><p>Abstract: The first main contribution of this paper is a novel method for representing images based on a dictionary of shape epitomes. These shape epitomes represent the local edge structure of the image and include hidden variables to encode shift and rotations. They are learnt in an unsupervised manner from groundtruth edges. This dictionary is compact but is also able to capture the typical shapes of edges in natural images. In this paper, we illustrate the shape epitomes by applying them to the image labeling task. In other work, described in the supplementary material, we apply them to edge detection and image modeling. We apply shape epitomes to image labeling by using Conditional Random Field (CRF) Models. They are alternatives to the superpixel or pixel representations used in most CRFs. In our approach, the shape of an image patch is encoded by a shape epitome from the dictionary. Unlike the superpixel representation, our method avoids making early decisions which cannot be reversed. Our resulting hierarchical CRFs efficiently capture both local and global class co-occurrence properties. We demonstrate its quanti- tative and qualitativeproperties ofour approach with image labeling experiments on two standard datasets: MSRC-21 and Stanford Background.</p><br/>
<h2>reference text</h2><p>N00014-12NIH Grant</p>
<p>[1] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik. Contour detection and hierarchical image segmentation. PAMI, 2011.</p>
<p>[2] K. Chatfield, V. Lempitsky, A. Vedaldi, and A. Zisserman. The devil is in the details: an evaluation of recent feature encoding methods. BMVC, 2011.</p>
<p>[3] M. Collins. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. ACL, 2002.</p>
<p>[4] A. Delong, A. Osokin, H. N. Isack, and Y. Boykov. Fast approximate energy minimization with label costs. International Journal of Computer Vision, 2012.</p>
<p>[5] B. Frey and D. Dueck. Clustering by passing messages between data points. Science, 2007.</p>
<p>[6] B. Fulkerson, A. Vedaldi, and S. Soatto. Class segmentation and object localization with superpixel neighborhoods. ICCV, 2009.</p>
<p>[7] C. Galleguillos, A. Rabinovich, and S. Belongie. Object categorization using co-occurrence, location and appearance. CVPR, 2008.</p>
<p>[8] J. Gonfaus, X. Boix, J. Van De Weijer, A. Bagdanov, J. Serrat, and J. Gonzalez. Harmony potentials for joint classification and segmentation. CVPR, 2010.</p>
<p>[9] S. Gould. Multiclass pixel labeling with non-local matching constraints. CVPR, 2012.</p>
<p>[10] S. Gould, R. Fulton, and D. Koller. Decomposing a scene into geometric and semantically consistent regions. ICCV, 2009.</p>
<p>[11] X. He, R. Zemel, and D. Ray. Learning and incorporating top-down cues in image segmentation. ECCV, 2006.</p>
<p>[12] N. Jojic, B. Frey, and A. Kannan. Epitomic analysis of ap-</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]</p>
<p>[19]</p>
<p>[20]</p>
<p>[21]</p>
<p>[22]</p>
<p>[23]</p>
<p>[24]</p>
<p>[25]  pearance and shape. ICCV, 2003. F. Jurie and B. Triggs. Creating efficient codebooks for visual recognition. ICCV, 2005. P. Kohli, L. Ladicky, and P. Torr. Robust higher order potentials for enforcing label consistency. IJCV, 2009. N. Komodakis and N. Paragios. Beyond pairwise energies: Efficient optimization for higher-order mrfs. CVPR, 2009. P. Kontschieder, S. Bul o´, H. Bischof, and M. Pelillo. Structured class-labels in random forests for semantic image labelling. ICCV, 2011. P. Kr ¨ahenb u¨hl and V. Koltun. Efficient inference in fully connected CRFs with gaussian edge potentials. NIPS, 2011. S. Kumar and M. Hebert. Discriminative random fields. IJCV, 2006. L. Ladicky, C. Russell, P. Kohli, and P. Torr. Associative hierarchical CRFs for object class image segmentation. ICCV, 2009. J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. ICML, 2001 . V. Lempitsky, A. Vedaldi, and A. Zisserman. A pylon model for semantic segmentation. NIPS, 2011. A. Lucchi, Y. Li, X. Boix, K. Smith, and P. Fua. Are spatial and global constraints really necessary for segmentation? ICCV, 2011. D. Munoz, J. Bagnell, and M. Hebert. Stacked hierarchical labeling. ECCV, 2010. S. Roth and M. Black. Fields of experts. IJCV, 2009. J. Shotton, M. Johnson, and R. Cipolla. Semantic texton forests for image categorization and segmentation. CVPR, 2008.</p>
<p>[26] J. Shotton, J. Winn, C. Rother, and A. Criminisi. Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context. IJCV, 2009.</p>
<p>[27] L. Zhu, Y. Chen, Y. Lin, C. Lin, and A. Yuille. Recursive segmentation and recognition templates for image parsing. PAMI, 2012.</p>
<p>[28] D. Zoran and Y. Weiss. From learning models of natural image patches to whole image restoration. ICCV, 2011. 343  classification rate averaged over the whole dataset, and Average accuracy refers to the Pxl-Cls model is the pixel-wise classifier, whose output is integrated in our models.  (excerpted from [27]). Note that  our  models  capture  object shapes  more  mean  of all object class classification  accurately than the HIM.  rates.  The  able to capture object shaFpiegsu,r ees8p e.c Qiaulalylit athtiev eco awnd sh qaupaen itnita thtieve fo reusruthlts co olnum thne. Stanford Background dcalatassseifti.cation rate averaged over dataset. 344</p>
<br/>
<br/><br/><br/></body>
</html>
