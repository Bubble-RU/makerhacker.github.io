<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>341 iccv-2013-Real-Time Body Tracking with One Depth Camera and Inertial Sensors</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-341" href="../iccv2013/iccv-2013-Real-Time_Body_Tracking_with_One_Depth_Camera_and_Inertial_Sensors.html">iccv2013-341</a> <a title="iccv-2013-341-reference" href="#">iccv2013-341-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>341 iccv-2013-Real-Time Body Tracking with One Depth Camera and Inertial Sensors</h1>
<br/><p>Source: <a title="iccv-2013-341-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Helten_Real-Time_Body_Tracking_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Thomas Helten, Meinard Müller, Hans-Peter Seidel, Christian Theobalt</p><p>Abstract: In recent years, the availability of inexpensive depth cameras, such as the Microsoft Kinect, has boosted the research in monocular full body skeletal pose tracking. Unfortunately, existing trackers often fail to capture poses where a single camera provides insufficient data, such as non-frontal poses, and all other poses with body part occlusions. In this paper, we present a novel sensor fusion approach for real-time full body tracking that succeeds in such difficult situations. It takes inspiration from previous tracking solutions, and combines a generative tracker and a discriminative tracker retrieving closest poses in a database. In contrast to previous work, both trackers employ data from a low number of inexpensive body-worn inertial sensors. These sensors provide reliable and complementary information when the monocular depth information alone is not sufficient. We also contribute by new algorithmic solutions to best fuse depth and inertial data in both trackers. One is a new visibility model to determine global body pose, occlusions and usable depth correspondences and to decide what data modality to use for discriminative tracking. We also contribute with a new inertial-basedpose retrieval, and an adapted late fusion step to calculate the final body pose.</p><br/>
<h2>reference text</h2><p>[1] A. Baak, M. M ¨uller, G. Bharaj, H.-P. Seidel, and C. Theobalt. A data-driven approach for real-time full body pose reconstruction from a depth camera. In ICCV, 2011.</p>
<p>[2] V. Ganapathi, C. Plagemann, D. Koller, and S. Thrun. Realtime human pose tracking from range data. In ECCV, 2012.</p>
<p>[3] V. Ganapathi, C. Plagemann, S. Thrun, and D. Koller. Real time motion capture using a single time-of-flight camera. In CVPR, 2010.</p>
<p>[4] R. Girshick, J. Shotton, P. Kohli, A. Criminisi, and A. Fitzgibbon. Efficient regression of general-activity human poses from depth images. In ICCV, pages 415–422, 2011.</p>
<p>[5] S. Knoop, S. Vacek, and R. Dillmann. Fusion of 2D and 3D sensor data for articulated body tracking. Robotics and Autonomous Systems, 57(3):321–329, 2009.</p>
<p>[6] A. Kolb, E. Barth, R. Koch, and R. Larsen. Time-of-flight sensors in computer graphics. CGF, 29(1): 141–159, 2010.</p>
<p>[7] H. Liu, X. Wei, J. Chai, I. Ha, and T. Rhee. Realtime human motion control with a small number of inertial sensors. In I3D, pages 133–140, 2011.</p>
<p>[8] Y. Pekelny and C. Gotsman. Articulated object reconstruction and markerless motion capture from depth video. CGF, 27(2):399–408, 2008.</p>
<p>[9] C. Plagemann, V. Ganapathi, D. Koller, and S. Thrun. Realtime identification and localization of body parts from depth images. In ICRA, Anchorage, Alaska, USA, 2010.</p>
<p>[10] G. Pons-Moll, A. Baak, T. Helten, M. M ¨uller, H.-P. Seidel, and B. Rosenhahn. Multisensor-fusion for 3d full-body hu-</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]</p>
<p>[19]</p>
<p>[20]  man motion capture. In CVPR, pages 663–670, 2010. R. Poppe. A survey on vision-based human action recognition. Image and Vision Computing, 28(6):976–990, 2010. M. Salzmann and R. Urtasun. Combining discriminative and generative methods for 3D deformable surface and articulated pose reconstruction. In CVPR, 2010. J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake. Real-time human pose recognition in parts from a single depth image. In CVPR, 2011. C. Stoll, N. Hasler, J. Gall, H.-P. Seidel, and C. Theobalt. Fast articulated motion tracking using a sums of gaussians body model. In ICCV, pages 951–958, 2011. J. Tautges, A. Zinke, B. Kr¨ uger, J. Baumann, A. Weber, T. Helten, M. M ¨uller, H.-P. Seidel, and B. Eberhardt. Motion reconstruction using sparse accelerometer data. TOG, 30(3): 18: 1–18: 12, 2011. J. Taylor, J. Shotton, T. Sharp, and A. W. Fitzgibbon. The Vitruvian manifold: Inferring dense correspondences for one-shot human pose estimation. In CVPR, 2012. X. Wei, P. Zhang, and J. Chai. Accurate realtime fullbody motion capture using a single depth camera. TOG, 31(6): 188: 1–188: 12, 2012. A. Weiss, D. Hirshberg, and M. Black. Home 3D body scans from noisy image and range data. In ICCV, 2011. G. Ye, Y. Liu, N. Hasler, X. Ji, Q. Dai, and C. Theobalt. Performance capture of interacting characters with handheld kinects. In Proc. ECCV, pages 828–841, 2012. M. Ye, X. Wang, R. Yang, L. Ren, and M. Pollefeys. Accurate 3d pose estimation from a single depth image. In ICCV, pages 731–738, 2011.</p>
<p>[21] L. Zhang, B. Curless, and S. M. Seitz. Spacetime stereo: Shape recovery for dynamic scenes. In CVPR, 2003.</p>
<p>[22] Y. Zhu, B. Dariush, and K. Fujimura. Kinematic self retargeting: A framework for human pose estimation. CVIU, 114(12): 1362–1375, 2010.</p>
<p>[23] J. Ziegler, H. Kretzschmar, C. Stachniss, G. Grisetti, and W. Burgard. Accurate human motion capture in large areas by combining IMU- and laser-based people tracking. In IROS, pages 86–91, 2011. 11 11 1122</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
