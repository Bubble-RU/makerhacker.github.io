<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>268 iccv-2013-Modeling 4D Human-Object Interactions for Event and Object Recognition</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-268" href="../iccv2013/iccv-2013-Modeling_4D_Human-Object_Interactions_for_Event_and_Object_Recognition.html">iccv2013-268</a> <a title="iccv-2013-268-reference" href="#">iccv2013-268-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>268 iccv-2013-Modeling 4D Human-Object Interactions for Event and Object Recognition</h1>
<br/><p>Source: <a title="iccv-2013-268-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Wei_Modeling_4D_Human-Object_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Ping Wei, Yibiao Zhao, Nanning Zheng, Song-Chun Zhu</p><p>Abstract: Recognizing the events and objects in the video sequence are two challenging tasks due to the complex temporal structures and the large appearance variations. In this paper, we propose a 4D human-object interaction model, where the two tasks jointly boost each other. Our human-object interaction is defined in 4D space: i) the cooccurrence and geometric constraints of human pose and object in 3D space; ii) the sub-events transition and objects coherence in 1D temporal dimension. We represent the structure of events, sub-events and objects in a hierarchical graph. For an input RGB-depth video, we design a dynamic programming beam search algorithm to: i) segment the video, ii) recognize the events, and iii) detect the objects simultaneously. For evaluation, we built a large-scale multiview 3D event dataset which contains 3815 video sequences and 383,036 RGBD frames captured by the Kinect cameras. The experiment results on this dataset show the effectiveness of our method.</p><br/>
<h2>reference text</h2><p>[1] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. 2005.</p>
<p>[2] J. Gall, A. Fossati, and L. van Gool. Functional categorization of objects using real-time markerless motion capture. In CVPR, 2011.</p>
<p>[3] J. J. Gibson. The Theory of Affordances. Lawrence Erlbaum, 1977.</p>
<p>[4] H. Grabner, J. Gall, and L. J. V. Gool. What makes a chair a chair? In CVPR, 2011.</p>
<p>[5] A. Gupta, A. Kembhavi, and L. Davis. Observing humanobject interactions: Using spatial and functional compatibility for recognition. IEEE TPAMI, 31(10), 2009.</p>
<p>[6] X. R. Kevin Lai, Liefeng Bo and D. Fox. Detection-based object labeling in 3d scenes. In ICRA, 2012.</p>
<p>[7] H. S. Koppula, R. Gupta, and A. Saxena. Learning human activities and object affordances from rgb-d videos. The International Journal of Robotics Research, 32(8), 2013.</p>
<p>[8] F. Lv and R. Nevatia. Recognition and segmentation of 3-d human action using hmm and multi-class adaboost. In ECCV, 2006.</p>
<p>[9] M. Marszałek, I. Laptev, and C. Schmid. Actions in context. In CVPR, 2009.</p>
<p>[10] M. M¨ uller and T. R ¨oder. Motion templates for automatic classification and retrieval of motion capture data. In ACM SIGGRAPH/Eurographics symposium on Computer animation, 2006.</p>
<p>[11] B. Packer, K. Saenko, and D. Koller. A combined pose, object, and feature model for action understanding. In CVPR, 2012.</p>
<p>[12] M. Pei, Y. Jia, and S.-C. Zhu. Parsing video events with goal inference and intent prediction. In ICCV, 2011.</p>
<p>[13] J. C. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in large margin classifiers, 1999.</p>
<p>[14] A. Prest, V. Ferrari, and C. Schmid. Explicit modeling of human-object interactions in realistic videos. Technical report, INRIA, 2011.</p>
<p>[15] A. Prest, C. Schmid, and V. Ferrari. Weakly supervised learning of interactions between humans and objects. TPAMI, 34(3), 2012.</p>
<p>[16] L. R. Rabiner. A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2), 1989.</p>
<p>[17] S. Sadanand and J. J. Corso. Action bank: A high-level representation of activity in video. In CVPR, 2012.</p>
<p>[18] J. Shotton, A. W. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake. Real-time human pose recognition in parts from single depth images. In CVPR, 2011.</p>
<p>[19] J. Sung, C. Ponce, B. Selman, and A. Saxena. Unstructured human activity detection from rgbd images. In ICRA, 2012.</p>
<p>[20] K. Tang, L. Fei-Fei, and D. Koller. Learning latent temporal structure for complex event detection. In CVPR, 2012.</p>
<p>[21] C. Tillmann and H. Ney. Word reordering and a dynamic programming beam search algorithm for statistical machine translation. Computational Linguistics, 29, 2003.</p>
<p>[22] J. Wang, Z. Liu, Y. Wu, and J. Yuan. Mining actionlet ensemble for action recognition with depth cameras. In CVPR, 2012.</p>
<p>[23] J. X. Wu, A. Osuntogun, T. Choudhury, M. Philipose, and J. M. Rehg. A scalable approach to activity recognition based on object use. In ICCV, 2007.</p>
<p>[24] B. Yao and L. Fei-Fei. Recognizing human-object interactions in still images by modeling the mutual context of objects and human poses. TPAMI, 34(9), 2012.</p>
<p>[25] Y. Zhao and S.-C. Zhu. Scene parsing by integrating function, geometry and appearance models. In CVPR, 2013. 3279</p>
<br/>
<br/><br/><br/></body>
</html>
