<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>108 iccv-2013-Depth from Combining Defocus and Correspondence Using Light-Field Cameras</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-108" href="../iccv2013/iccv-2013-Depth_from_Combining_Defocus_and_Correspondence_Using_Light-Field_Cameras.html">iccv2013-108</a> <a title="iccv-2013-108-reference" href="#">iccv2013-108-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>108 iccv-2013-Depth from Combining Defocus and Correspondence Using Light-Field Cameras</h1>
<br/><p>Source: <a title="iccv-2013-108-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Tao_Depth_from_Combining_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Michael W. Tao, Sunil Hadap, Jitendra Malik, Ravi Ramamoorthi</p><p>Abstract: Light-field cameras have recently become available to the consumer market. An array of micro-lenses captures enough information that one can refocus images after acquisition, as well as shift one â€™s viewpoint within the subapertures of the main lens, effectively obtaining multiple views. Thus, depth cues from both defocus and correspondence are available simultaneously in a single capture. Previously, defocus could be achieved only through multiple image exposures focused at different depths, while correspondence cues needed multiple exposures at different viewpoints or multiple cameras; moreover, both cues could not easily be obtained together. In this paper, we present a novel simple and principled algorithm that computes dense depth estimation by combining both defocus and correspondence depth cues. We analyze the x-u 2D epipolar image (EPI), where by convention we assume the spatial x coordinate is horizontal and the angular u coordinate is vertical (our final algorithm uses the full 4D EPI). We show that defocus depth cues are obtained by computing the horizontal (spatial) variance after vertical (angular) integration, and correspondence depth cues by computing the vertical (angular) variance. We then show how to combine the two cues into a high quality depth map, suitable for computer vision applications such as matting, full control of depth-of-field, and surface reconstruction.</p><br/>
<h2>reference text</h2><p>[1] E. Adelson and J. Wang. Single lens stereo with a plenoptic camera. PAMI, 1992. 1, 3</p>
<p>[2] S. Baker, D. Scharstein, J. Lewis, S. Roth, M. Black, and R. Szeliski. A database and evaluation methodology for optical flow. ICCV, 2007. 5</p>
<p>[3] R. Bolles, H. Baker, and D. Marimont. Epipolar-plane image analysis: an approach to determining structure from motion. IJCV, 1997. 1</p>
<p>[4] A. Criminisi, S. Kang, R. Swaminathan, R. Szeliski, and P. Anandan. Extracting layers and analyzing their specular</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]  properties using epipolar-plane-image analysis. CVIU, 2005. 1 A. Criminisi, T. Sharp, and C. Rother. Geodesic image and video editing. ACM Transactions on Graphics, 2010. 2 S. Gortler, R. Grzeszczuk, R. Szeliski, and M. Cohen. The lumigraph. In ACM SIGGRAPH, 1996. 1 H. Hirschmuller, P. Innocent, and J. Garibaldi. Real-time correlation-based stereo vision with reduced border errors. IJCV, 2002. 4 B. Horn and B. Schunck. Determining optical flow. Artificial Intelligence, 1981 . 3 http://www.lytro.com. Lytro redefines photography with light field cameras. Press Release, June 2011. 1 A. Janoch, S. Karayev, Y. Jia, J. Barron, M. Fritz, K. Saenko, and T. Darrell. A catergory-level 3D object dataset: putting the kinect to work. In ICCV, 2011. 2, 5 C. Kim, H. Zimmer, Y. Pritch, A. Sorkine-Hornung, and M. Gross. Scene reconstruction from high spatio-angular resolution light fields. In SIGGRAPH, 2013. 3 W. Klarquist, W. Geisler, and A. Brovic. Maximumlikelihood depth-from-defocus for active vision. In Inter. Conf. Intell. Robots and Systems, 1995. 3 T. J. Kosloff, M. W. Tao, and B. A. Barsky. Depth of field postprocessing for layered scenes using constant-time rectangle spreading. In Graphics Interface, 2009. 2 A. Levin. Analyzing depth form coded aperture sets. In ECCV, 2010. 3 M. Levoy and P. Hanrahan. Light field rendering. In ACM SIGGRAPH, 1996. 1, 3 J. Li, E. Li, Y. Chen, L. Xu, and Y. Zhang. Bundled depthmap merging for multi-view stereo. In CVPR, 2010. 3</p>
<p>[17] C. Liang, T. Lin, B. Wong, C. Liu, and H. Chen. Programmable aperture photography: multiplexed light field acquisition. In ACM SIGGRAPH, 2008. 3</p>
<p>[18] B. Lucas and T. Kanade. An iterative image registration technique with an application to stereo vision. In Imaging Understanding Workshop, 1981 . 3</p>
<p>[19] M. Matousek, T. Werner, and V. Hlavac. Accurate correspondences from epipolar plane images. In Computer Vision Winter Workshop, 2001. 4</p>
<p>[20] D. Min, J. Lu, and M. Do. Joint histogram based cost aggregation for stereo matching. PAMI, 2013. 3</p>
<p>[21] R. Ng, M. Levoy, M. Bredif, G. Duval, M. Horowitz, and P. Hanrahan. Light field photographhy with a hand-held plenoptic camera. CSTR 2005-02, 2005. 1, 3, 5</p>
<p>[22] M. Okutomi and T. Kanade. A multiple-baseline stereo. PAMI, 1993. 3</p>
<p>[23] C. Perwass and P. Wietzke. Single lens 3D-camera with extended depth-of-field. In SPIE Elect. Imaging, 2012. 1, 3</p>
<p>[24] D. Scharstein and R. Szeliski. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. IJCV, 2002. 3</p>
<p>[25] Y. Schechner and N. Kiryati. Depth from defocus vs. stereo: how different really are they? IJCV, 2000. 2</p>
<p>[26] J. Shotton, R. Girshick, F. A., T. Sharp, M. Cook, M. Finocchio, M. Richard, P. Kohli, A. Criminsi, A. Kipman, and A. Blake. Efficient human pose estimation from single depth images. PAMI, 2012. 2</p>
<p>[27] S. Sinha, D. Steedly, R. Szeliski, M. Agrawala, and M. Pollefeys. Interactive 3D architectural modeling from unordered photo collections. In ACM SIGGRAPH Asia, 2008. 2</p>
<p>[28] A. Stein and M. Hebert. Occlusion boundaries from motion:</p>
<p>[29]</p>
<p>[30] [3 1]</p>
<p>[32]</p>
<p>[33]</p>
<p>[34]  low-level detection and mid-level reasoning. IJCV, 2009. 6 M. Subbarao, T. Yuan, and J. Tyan. Integration of defocus and focus analysis with stereo for 3D shape recovery. SPIE Three Dimensional Imaging and Laser-Based Systems for Metrology and Inspection III, 1998. 3 D. Sun, S. Roth, and M. Black. Secrets of optical flow estimation and their principles. In CVPR, 2010. 5, 6 P. Sundberg, J. Malik, M. Maire, P. Arbelaez, and T. Brox. Occlusion boundary detection and figure/ground assignment from optical flow. In CVPR, 2011. 6 V. Vaish, R. Szeliski, C. Zitnick, S. Kang, and M. Levoy. Reconstructing occluded surfaces using synthetic apertures: stereo, focus and robust measures. In CVPR, 2006. 2, 3 S. Wanner and B. Goldluecke. Globally consistent depth labeling of 4D light fields. In CVPR, 2012. 3, 5, 6 M. Wantanabe and S. Nayar. Rational filters for passive depth from defocus. IJCV, 1998. 2 680</p>
<br/>
<br/><br/><br/></body>
</html>
