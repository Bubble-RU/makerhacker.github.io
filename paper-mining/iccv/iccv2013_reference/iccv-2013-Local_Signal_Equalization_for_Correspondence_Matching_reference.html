<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>255 iccv-2013-Local Signal Equalization for Correspondence Matching</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-255" href="../iccv2013/iccv-2013-Local_Signal_Equalization_for_Correspondence_Matching.html">iccv2013-255</a> <a title="iccv-2013-255-reference" href="#">iccv2013-255-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>255 iccv-2013-Local Signal Equalization for Correspondence Matching</h1>
<br/><p>Source: <a title="iccv-2013-255-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Bradley_Local_Signal_Equalization_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Derek Bradley, Thabo Beeler</p><p>Abstract: Correspondence matching is one of the most common problems in computer vision, and it is often solved using photo-consistency of local regions. These approaches typically assume that the frequency content in the local region is consistent in the image pair, such that matching is performed on similar signals. However, in many practical situations this is not the case, for example with low depth of field cameras a scene point may be out of focus in one view and in-focus in the other, causing a mismatch of frequency signals. Furthermore, this mismatch can vary spatially over the entire image. In this paper we propose a local signal equalization approach for correspondence matching. Using a measure of local image frequency, we equalize local signals using an efficient scale-space image representation such that their frequency contents are optimally suited for matching. Our approach allows better correspondence matching, which we demonstrate with a number of stereo reconstruction examples on synthetic and real datasets.</p><br/>
<h2>reference text</h2><p>[1] S. Baker, D. Scharstein, J. P. Lewis, S. Roth, M. J. Black, and R. Szeliski. A database and evaluation methodology for optical flow. IJCV, 92:1–31, 2011.</p>
<p>[2] T. Beeler, B. Bickel, R. Sumner, P. Beardsley, and M. Gross. High-quality single-shot capture of facial geometry. ACM Trans. Graphics (Proc. SIGGRAPH), 2010.</p>
<p>[3] D. Bradley, T. Boubekeur, and W. Heidrich. Accurate multiview reconstruction using robust binocular stereo and surface meshing. In CVPR, 2008.</p>
<p>[4] P. Brigger, F. M ¨uller, K. Illgner, and M. Unser. Centered pyramids. IEEE Trans. Image Proc., 8(9), 1999.</p>
<p>[5] B. Cyganek. Adaptive window growing technique for efficient image matching. LNCS, 3522:308–3 15, 2005.</p>
<p>[6] J. Ens and P. Lawrence. An investigation of methods for determining depth from focus. IEEE PAMI, 15:97–108, 1993. 11888866  Camera  C1  Camera  C2  Camera Focus  Naïve  Ours  Center: Color-coded difference of local frequencies. Right: 3D point cloud results. The na¨ ıve algorithm successfully reconstructs areas where the signals are similar but quickly degrades with increasing signal mismatch. Signal equalization greatly improves accuracy and robustness in areas of signal mismatch. Naïve  Ours  Center: Color-coded difference of local frequencies. Right: 3D point cloud results from two different views.</p>
<p>[7] Y. Furukawa and J. Ponce. Accurate, dense, and robust multiview stereopsis. IEEE Trans. PAMI, 32(8):1362–1376, 2010.</p>
<p>[8] M. Goesele, N. Snavely, B. Curless, H. Hoppe, and S. M. Seitz. Multi-view stereo for community photo collections. In ICCV, 2007.</p>
<p>[9] M. Habbecke and L. Kobbelt. A surface-growing approach to multi-view stereo reconstruction. In CVPR, 2007.</p>
<p>[10] X. Hu and P. Mordohai. A quantitative evaluation of condence measures for stereo vision. IEEE PAMI, 34(1 1), 2012.</p>
<p>[11] W. Jakob. Mitsuba, 2012. www.mitsuba-renderer.org.</p>
<p>[12] H. Jin and P. Favaro. A variational approach to shape from defocus. In ECCV, 2002.</p>
<p>[13] T. Kanade and M. Okutomi. A stereo matching algorithm with an adaptive window: Theory and experiment. IEEE Trans. PAMI, 16(9), 1994.</p>
<p>[14] R. Klowsky, A. Kuijper, and M. Goesele. Modulation trans-</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]  fer function of patch-based stereo systems. In CVPR, 2012. R. Klowsky, A. Kuijper, and M. Goesele. Weighted patchbased reconstruction: linking (multi-view) stereo to scale space. LNCS, 7893, 2013. H.-S. Koo and C.-S. Jeong. An area-based stereo matching using adaptive search range and window size. LNCS, 2074:44–53, 2001. S. Li, J. T. Kwok, and Y. Wang. Combination of images with diverse focuses using the spatial frequency. Information Fusion, 2(3): 169 176, 2001. C. Menard and W. Kropatsch. Adaptive stereo matching in correlation scale-space. LNCS, 13 10:677–684, 1997. A. S. Ogale and Y. Aloimonos. Shape and the stereo correspondence problem. IJCV, 65(3): 147–162, 2005. D. Scharstein and R. Szeliski. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. IJCV, 47(1/2/3):7–42, 2002. S. M. Seitz, B. Curless, J. Diebel, D. Scharstein, and R. Szeliski. A comparison and evaluation of multi-view stereo reconstruction algorithms. In CVPR, 2006. C. Strecha, A. M. Bronstein, M. M. Bronstein, and P. Fua. Ldahash: improved matching with smaller descriptors. IEEE Trans. PAMI, 34(1):66–78, 2012. M. Subbarao and G. Surya. Depth from defocus: A spatial domain approach. IJCV, 13(3):271–294, 1994. S. Yoon, D. Min, and K. Sohn. Fast dense stereo matching using adaptive window in hierarchical framework. LNCS, 4292:316–325, 2006. –</p>
<p>[18]</p>
<p>[19]</p>
<p>[20]</p>
<p>[21]</p>
<p>[22]</p>
<p>[23]</p>
<p>[24]  11888877</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
