<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-188" href="../iccv2013/iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">iccv2013-188</a> <a title="iccv-2013-188-reference" href="#">iccv2013-188-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</h1>
<br/><p>Source: <a title="iccv-2013-188-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Luo_Group_Sparsity_and_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Jiajia Luo, Wei Wang, Hairong Qi</p><p>Abstract: Human action recognition based on the depth information provided by commodity depth sensors is an important yet challenging task. The noisy depth maps, different lengths of action sequences, and free styles in performing actions, may cause large intra-class variations. In this paper, a new framework based on sparse coding and temporal pyramid matching (TPM) is proposed for depthbased human action recognition. Especially, a discriminative class-specific dictionary learning algorithm isproposed for sparse coding. By adding the group sparsity and geometry constraints, features can be well reconstructed by the sub-dictionary belonging to the same class, and the geometry relationships among features are also kept in the calculated coefficients. The proposed approach is evaluated on two benchmark datasets captured by depth cameras. Experimental results show that the proposed algorithm repeatedly hqi } @ ut k . edu GB ImagesR epth ImagesD setkonlSy0 896.5170d4ept.3h021 .x02y 19.876504.dep3th02.1 x02. achieves superior performance to the state of the art algorithms. Moreover, the proposed dictionary learning method also outperforms classic dictionary learning approaches.</p><br/>
<h2>reference text</h2><p>[1] M. Aharon, M. Elad, and A. M. Bruckstein. K-svd: An algorithm for designing overcomplete dicitonaries for sparse representation. IEEE Trans. Signal Processing, 54:43 11–4322, 2006. 6</p>
<p>[2] H. Bondell and B. Reich. Simultaneous regression shrinkage, variable selection and supervised clustering of predictors with oscar. Biometrics, 64(1): 115–123, 2008. 3</p>
<p>[3] P. Dollar, V. Rabaud, G. Cottrell, and S. Belongie. Behavior recognition via sparse spatio-temporal features. VS-PETS, 2005. 7</p>
<p>[4] Y. Fang, R. Wang, and B. Dai. Graph-oriented learning via</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]  automatic group sparsity for data analysis. IEEE 12th International Conference on Data Mining, 2012. 2, 3 S. Gao, I. Tshang, L. Chia, and P. Zhao. Local features are not lonely - laplacian sparse coding for image classification. CVPR, 2010. 3 Z. Jiang, Z. Lin, and L. S. Davis. Learning a discriminative dictionary for sparse coding via label consistent k-svd. CVPR, 2011. 2 S. Kong and D. Wang. A dictionary learning approach for classification: separating the particularity and the commonality. ECCV, 2012. 2, 3 I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld. Learning realistic human actions from movies. CVPR, 2008. 7</p>
<p>[9] H. Lee, A. Battle, R. Raina, and A. Y. Ng. Efficient sparse code algorithms. NIPS, 2007. 4, 5</p>
<p>[10] W. Li, Z. Zhang, and Z. Liu. Action recognition based on a bag of 3d points. In Human communicative behavior analysis workshop (in conjunction with CVPR), 2010. 5, 6, 7</p>
<p>[11] F. Lv and R. Nevatia. Recognition and segmentation of 3-d human action using hmm and multi-class adaboost. ECCV, pages 359–372, 2006. 2, 6</p>
<p>[12] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online dictionary learning for sparse coding. ICML, 2009. 5</p>
<p>[13] J. Martens and I. Sutskever. Learning recurrent neural networks with hessian-free optimzation, 2011. 6</p>
<p>[14] M. Muller and T. Roder. Motion templates for automatic classification and retrieval of motion capture data. In proceedings of the 2006 ACM SIGGRAPH/Eurographics symposium on compute animation, pages 137–146, 2006. 2, 6, 7</p>
<p>[15] I. Ramirez, P. Sprechmann, and G. Sapiro.  Classification</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]</p>
<p>[19]</p>
<p>[20]</p>
<p>[21]</p>
<p>[22]</p>
<p>[23]</p>
<p>[24]  and clustering via dictionary learning with structured incoherence and shared features. CVPR, 2010. 2, 3, 6 J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake. Real-time human pose recognition in parts from single depth cameras. CVPR, 2011. 1, 3 A. W. Vieira, E. R. Nascimento, G. Oliveira, Z. Liu, and M. Campos. Stop: space-time occupancy patterns for 3d action recognition from depth map sequences. 1 Iberoamer7th ican congress on pattern recognition. 6 J. Wang, Z. Liu, J. Chorowski, Z. Chen, and Y. Wu. Robust 3d action recognition with random occupancy patterns. ECCV, 2012. 6 J. Wang, Z. Liu, Y. Wu, and J. Yuan. Mining actionlet ensemble for action recognition with depth cameras. CVPR, 2012. 1, 2, 3, 5, 6, 7 J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong. Locality-constrained linear coding for image classification. CVPR, 2010. 5 L. Xia, C. C. Chen, and J. K. Aggarwal. View invariant human action recognition using histograms of 3d joints. CVPR Workshop, 2012. 2, 6, 7 J. Yang, K. Yu, Y. Gong, and T. Huang. Linear spatial pyramid matching using sparse coding for image classification. CVPR, 2009. 3, 5, 6 M. Yang, L. Zhang, X. Feng, and D. Zhang. Fisher discrimination dictionary learning for sparse representation. ICCV, 2011. 2 X. Yang and Y. Tian. Eigenjoints-based action recognition using naive bayes nearest neighbor. CVPR 2012 HAU3D Workshop, 2012. 2, 6, 7</p>
<p>[25] Q. Zhang and B. Li. Discriminative k-svd for dicitonary learning in face recognition. CVPR, 2010. 2</p>
<p>[26] H. Zou and H. Hastie. Regression and variable selection via the elastic net. Journal of the Royal Statistical Society, Series B, 67(2):301–320, 2005. 2, 3 11881166</p>
<br/>
<br/><br/><br/></body>
</html>
