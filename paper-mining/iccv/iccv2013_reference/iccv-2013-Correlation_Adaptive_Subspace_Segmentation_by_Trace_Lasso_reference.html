<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>93 iccv-2013-Correlation Adaptive Subspace Segmentation by Trace Lasso</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-93" href="../iccv2013/iccv-2013-Correlation_Adaptive_Subspace_Segmentation_by_Trace_Lasso.html">iccv2013-93</a> <a title="iccv-2013-93-reference" href="#">iccv2013-93-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>93 iccv-2013-Correlation Adaptive Subspace Segmentation by Trace Lasso</h1>
<br/><p>Source: <a title="iccv-2013-93-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Lu_Correlation_Adaptive_Subspace_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Canyi Lu, Jiashi Feng, Zhouchen Lin, Shuicheng Yan</p><p>Abstract: This paper studies the subspace segmentation problem. Given a set of data points drawn from a union of subspaces, the goal is to partition them into their underlying subspaces they were drawn from. The spectral clustering method is used as the framework. It requires to find an affinity matrix which is close to block diagonal, with nonzero entries corresponding to the data point pairs from the same subspace. In this work, we argue that both sparsity and the grouping effect are important for subspace segmentation. A sparse affinity matrix tends to be block diagonal, with less connections between data points from different subspaces. The grouping effect ensures that the highly corrected data which are usually from the same subspace can be grouped together. Sparse Subspace Clustering (SSC), by using ?1-minimization, encourages sparsity for data selection, but it lacks of the grouping effect. On the contrary, Low-RankRepresentation (LRR), by rank minimization, and Least Squares Regression (LSR), by ?2-regularization, exhibit strong grouping effect, but they are short in subset selection. Thus the obtained affinity matrix is usually very sparse by SSC, yet very dense by LRR and LSR. In this work, we propose the Correlation Adaptive Subspace Segmentation (CASS) method by using trace Lasso. CASS is a data correlation dependent method which simultaneously performs automatic data selection and groups correlated data together. It can be regarded as a method which adaptively balances SSC and LSR. Both theoretical and experimental results show the effectiveness of CASS.</p><br/>
<h2>reference text</h2><p>[1] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1): 1–122, 2011.</p>
<p>[2] B. Cheng, J. Yang, S. Yan, Y. Fu, and T. S. Huang. Learning with ?1-graph for image analysis. TIP, 19(Compendex):858– 866, 2010.</p>
<p>[3] E. Elhamifar and R. Vidal. Sparse subspace clustering. In CVPR, pages 2790–2797, 2009.</p>
<p>[4] E. Elhamifar and R. Vidal. Sparse subspace clustering: Algorithm, theory, and applications. arXiv preprint arXiv:1203.1005, 2012.</p>
<p>[5] Y. Fang, R. Wang, and B. Dai. Graph-oriented learning via automatic group sparsity for data analysis. In ICDM, pages 251–259. IEEE, 2012.</p>
<p>[6] A. S. Georghiades, P. N. Belhumeur, and D. J. Kriegman. From few to many: Illumination cone models for face recognition under variable lighting and pose. TPAMI, 23(6):643– 660, 2001.</p>
<p>[7] E. Grave, G. Obozinski, and F. Bach. Trace Lasso: a trace norm regularization for correlated designs. In NIPS, 2011.</p>
<p>[8] J. Gui, Z. Sun, W. Jia, R. Hu, Y. Lei, and S. Ji. Discriminant sparse neighborhood preserving embedding for face recognition. Pattern Recognition, 45(8):2884–2893, 2012.</p>
<p>[9] J. Ho, M.-H. Yang, J. Lim, K.-C. Lee, and D. Kriegman. Clustering appearances of objects under varying illumination conditions. In CVPR, volume 1.</p>
<p>[10] A. Hoerl and R. Kennard. Ridge regression: biased estimation for nonorthogonal problems. Technometrics, 12(1):55– 67, 1970.</p>
<p>[11] H. Huang, C. Ding, D. Luo, and T. Li. Simultaneous tensor subspace selection and clustering: the equivalence of high order SVD and k-means clustering. In KDD, pages 327–335, 2008.</p>
<p>[12] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma. Robust recovery of subspace structures by low-rank representation. TPAMI, PP(99): 1, 2012.</p>
<p>[13] G. Liu, Z. Lin, and Y. Yu. Robust subspace segmentation by low-rank representation. In ICML, pages 663–670, 2010.</p>
<p>[14] G. Liu and S. Yan. Latent low-rank representation for subspace segmentation and feature extraction. In ICCV, pages 1615–1622, 2011.</p>
<p>[15] R. Liu, Z. Lin, and Z. Su. Linearized alternating direction method with parallel splitting and adaptive penalty for separable convex programs in machine learning. In ACML, 2013.</p>
<p>[16] C.-Y. Lu, H. Min, Z.-Q. Zhao, L. Zhu, D.-S. Huang, and S. Yan. Robust and efficient subspace segmentation via least squares regression. In ECCV, 2012.</p>
<p>[17] D. Luo, F. Nie, C. Ding, and H. Huang. Multi-subspace representation and discovery. In ECML PKDD, volume 6912</p>
<p>[18]</p>
<p>[19]</p>
<p>[20]</p>
<p>[21]</p>
<p>[22]</p>
<p>[23]</p>
<p>[24]  LNAI, pages 405–420, 2011. Y. Ni, J. Sun, X. Yuan, S. Yan, and L.-F. Cheong. Robust low-rank subspace segmentation with semidefinite guarantees. In ICDM Workshops, pages 1179–1 188, 2010. S. Rao, R. Tron, R. Vidal, and Y. Ma. Motion segmentation in the presence of outlying, incomplete, or corrupted trajectories. TPAMI, 32(10): 1832–1845, 2010. J. B. Shi and J. Malik. Normalized cuts and image segmentation. TPAMI, 22(8):888–905, 2000. M. Szummer and T. Jaakkola. Partially labeled classification with Markov random walks. In NIPS, pages 945–952, 2001 . R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society. Series B, 50(1):267–288, 1996. R. Vidal, Y. Ma, and S. Sastry. Generalized principal component analysis (GPCA). TPAMI, 27(12): 1945–1959, 2005. S. Wang, X. Yuan, T. Yao, S. Yan, and J. Shen. Efficient subspace segmentation via quadratic programming. In AAAI, volume 1, pages 5 19–524, 2011. 1352</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
