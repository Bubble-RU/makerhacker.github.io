<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>222 iccv-2013-Joint Learning of Discriminative Prototypes and Large Margin Nearest Neighbor Classifiers</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-222" href="../iccv2013/iccv-2013-Joint_Learning_of_Discriminative_Prototypes_and_Large_Margin_Nearest_Neighbor_Classifiers.html">iccv2013-222</a> <a title="iccv-2013-222-reference" href="#">iccv2013-222-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>222 iccv-2013-Joint Learning of Discriminative Prototypes and Large Margin Nearest Neighbor Classifiers</h1>
<br/><p>Source: <a title="iccv-2013-222-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Kostinger_Joint_Learning_of_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Martin Köstinger, Paul Wohlhart, Peter M. Roth, Horst Bischof</p><p>Abstract: In this paper, we raise important issues concerning the evaluation complexity of existing Mahalanobis metric learning methods. The complexity scales linearly with the size of the dataset. This is especially cumbersome on large scale or for real-time applications with limited time budget. To alleviate this problem we propose to represent the dataset by a fixed number of discriminative prototypes. In particular, we introduce a new method that jointly chooses the positioning of prototypes and also optimizes the Mahalanobis distance metric with respect to these. We show that choosing the positioning of the prototypes and learning the metric in parallel leads to a drastically reduced evaluation effort while maintaining the discriminative essence of the original dataset. Moreover, for most problems our method performing k-nearest prototype (k-NP) classification on the condensed dataset leads to even better generalization compared to k-NN classification using all data. Results on a variety of challenging benchmarks demonstrate the power of our method. These include standard machine learning datasets as well as the challenging Public Fig- ures Face Database. On the competitive machine learning benchmarks we are comparable to the state-of-the-art while being more efficient. On the face benchmark we clearly outperform the state-of-the-art in Mahalanobis metric learning with drastically reduced evaluation effort.</p><br/>
<h2>reference text</h2><p>[1] E. Bonilla and A. Robles-Kelly. Discriminative probabilistic prototype learning. In Proc. ICML, 2012. 3</p>
<p>[2] A. Bordes, L. Bottou, P. Gallinari, and J. Weston. Solving multiclass support vector machines with LaRank. In Proc. ICML, 2007. 6</p>
<p>[3] T. E. d. Campo, B. R. Babu, and M. Varma. Character Recognition in Natural Images. In Proc. VISAPP, 2009. 5</p>
<p>[4] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. Trans. on Intelligent Systems and Technology, 2011. 6</p>
<p>[5] M. S. Charikar. Similarity estimation techniques from round-</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]  ing algorithms. In Proc. ACM Symposium on Theory of Computing, 2002. 5 K. Crammer, R. Gilad-bachrach, A. Navot, and N. Tishby. Margin analysis of the LVQ algorithm. In Advances NIPS, 2002. 2 K. Crammer, Y. Singer, N. Cristianini, J. Shawe-taylor, and B. Williamson. On the algorithmic implementation of multiclass kernel-based vector machines. JMLR, 2001 . 6 J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon. Information-theoretic metric learning. In Proc. ICML, 2007. 1, 2, 7 R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classification. JMLR, 2008. 6 A. Frank and A. Asuncion. UCI machine learning repository, 2010. University of California, Irvine, School of Information and Computer Sciences. Available online at http://archive.ics.uci.edu/ml. 5, 6 M. Guillaumin, J. Verbeek, and C. Schmid. Is that you? Metric learning approaches for face identification. In Proc. ICCV, 2009. 1, 2, 7 J.-P. Heo, Y. Lee, J. He, S.-F. Chang, and S.-E. Yoon. Spherical hashing. In Proc. CVPR, 2012. 6 N. J. Higham. Computing a nearest symmetric positive semidefinite matrix. Linear Algebra and its Applications, 103: 103–1 18, 1988. 4 M. Hirzer, P. M. Roth, M. K ¨ostinger, and H. Bischof. Relaxed pairwise learned metric for person re-identification. In Proc. ECCV, 2012. 1 J. J. Hull. A database for handwritten text recognition research. Trans. PAMI, 1994. 5</p>
<p>[16] P. Jain, B. Kulis, and K. Grauman. Fast image search for learned metrics. In Proc. CVPR, 2008. 1, 2, 5, 6</p>
<p>[17] T. Kohonen. Self-organization and associative memory. Springer-Verlag New York, Inc., 1989. 2</p>
<p>[18] M. K ¨ostinger, M. Hirzer, P. Wohlhart, P. M. Roth, and H. Bischof. Large scale metric learning from equivalence constraints. In Proc. CVPR, 2012. 2</p>
<p>[19] B. Kulis and K. Grauman. Kernelized locality-sensitive hashing. PAMI, 2012. 2, 5, 6</p>
<p>[20] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar. Attribute and Simile Classifiers for Face Verification. In Proc. ICCV, 2009. 2, 7</p>
<p>[21] L. Ladicky and P. H. S. Torr. Locally linear support vector machines. In Proc. ICML, 2011. 6, 7</p>
<p>[22] Y. Lin, T. Zhang, S. Zhu, and K. Yu. Deep coding networks. In Advances NIPS, 2010. 6</p>
<p>[23] S. Seo and K. Obermayer. Soft learning vector quantization. Neural Computation, 2002. 2</p>
<p>[24] C. Shen. Non-sparse linear representations for visual tracking with online reservoir metric learning. In Proc. CVPR, 2012. 1</p>
<p>[25] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and interdependent output variables. JMLR, 2005. 6</p>
<p>[26] A. Vedaldi and A. Zisserman. Efficient additive kernels via explicit feature maps. Pattern Analysis and Machine Intellingence, 34(3), 2011. 7</p>
<p>[27] J. Wang, J. Yang, K. Yu, F. Lv, T. S. Huang, and Y. Gong. Locality-constrained linear coding for image classification. In Proc. CVPR, 2010. 6</p>
<p>[28] K. Q. Weinberger, J. Blitzer, and L. K. Saul. Distance metric learning for large margin nearest neighbor classification. In Advances NIPS, 2006. 1, 2, 3, 6, 7</p>
<p>[29] K. Q. Weinberger and L. K. Saul. Fast solvers and efficient implementations for distance metric learning. In Proc. ICML, 2008. 1, 2, 5, 6</p>
<p>[30] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In Advances NIPS, 2008. 6 [3 1] T. Yang and V. Kecman. Adaptive local hyperplane classification. Neurocomputing, 2008. 6</p>
<p>[32] J. Ye, Z. Zhao, and H. Liu. Adaptive distance metric learning for clustering. In Proc. CVPR, 2007. 1</p>
<p>[33] K. Yu and T. Zhang. Improved local coordinate coding using local tangents. In Proc. ICML, 2010. 6</p>
<p>[34] K. Yu, T. Zhang, and Y. Gong. Nonlinear learning using local coordinate coding. In Advances NIPS, 2009. 6</p>
<p>[35] Z. Zhang, L. Ladicky, P. Torr, and A. Saffari. Learning anchor planes for classification. In Advances NIPS, 2011. 6</p>
<p>[36] Z. Zhang, P. Sturgess, S. Sengupta, N. Crook, and P. H. S. Torr. Efficient discriminative learning of parametric nearest neighbor classifiers. In Proc. CVPR, 2012. 3, 4, 5, 6, 7 33 11 1192</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
