<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-161" href="../iccv2013/iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">iccv2013-161</a> <a title="iccv-2013-161-reference" href="#">iccv2013-161-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</h1>
<br/><p>Source: <a title="iccv-2013-161-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Bao_Fast_Sparsity-Based_Orthogonal_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Chenglong Bao, Jian-Feng Cai, Hui Ji</p><p>Abstract: In recent years, how to learn a dictionary from input images for sparse modelling has been one very active topic in image processing and recognition. Most existing dictionary learning methods consider an over-complete dictionary, e.g. the K-SVD method. Often they require solving some minimization problem that is very challenging in terms of computational feasibility and efficiency. However, if the correlations among dictionary atoms are not well constrained, the redundancy of the dictionary does not necessarily improve the performance of sparse coding. This paper proposed a fast orthogonal dictionary learning method for sparse image representation. With comparable performance on several image restoration tasks, the proposed method is much more computationally efficient than the over-complete dictionary based learning methods.</p><br/>
<h2>reference text</h2><p>[1] M. Aharon and M. E. Bruckstein. K-svd: An algorithm for designing of overcomplete dictionaries for sparse representation. IEEE Trans. Signal Processing, 54(11), 2006.</p>
<p>[2] M. Bertalmio, G. Sapiro, V. Caselles, and C. Ballester. Image  inpainting. In ACM SIGGRAPH, 2000.</p>
<p>[3] J. Cai, R. Chan, and Z. Shen. A framelet-based image inpainting algorith. Appl. Comp. Harm. Anal., 24(2), 2008. 33338892  abcde  Figure 3. Comparison of  text  removal. (a) image with overlapped  texts;  (b-e) correspond  to  the results from [2],  two  over-complete  dictionary learning method with ?1 norm sparsity penalty and MC penalty ([27]), and Algorithm 3.  Figure 4. Image inpainting with 50% random missing pixels. (a) Original image; (b) corrupted image; (c-e) the results from from over-complete dictionary learning method with ?1 norm sparsity penalty and MC penalty ([27]), and Algorithm 3.  two</p>
<p>[4] J. Cai, S. Huang, H. Ji, Z. Shen, and G. Ye. Data-driven tight frame construction and image denoising. CAM report 12-40, UCLA, 2013.</p>
<p>[5] J. Cai, H. Ji, C. Liu, and Z. Shen. Framelet based blind image deblurring from a single image. IEEE Trans. Image Processing, 21(2):565–572, 2012.</p>
<p>[6] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image denoising by sparse 3-d transform-domain collaborative filtering. IEEE Trans. Image Processing, 16(8):2080–2095, 2007.</p>
<p>[7] I. Daubechies. Ten Lectures on Wavelets. SIAM, 1992.</p>
<p>[8] I. Daubechies, B. Han, A. Ron, and Z. Shen. Framelets: Mrabased constructions of wavelet frames. Appl. Comp. Harm. Anal., 14(1): 1–46, 2003.</p>
<p>[9] C.-A. Deledalle, J. Salmon, and A. S. Dalalyan. Image denoising with patch based pca: local versus global. In BMVC, 2011.</p>
<p>[10] W. Dong, X. Li, L. Zhang, and G. Shi. Sparsity-based image denoising via dictionary learning and structured clustering. In CVPR, 2011.</p>
<p>[11] D. Donoho and M. Elad. Optimally sparse representation in general (non-orthogonal) dictionary via ?1 minimization. PNAS, 100:2197–2202, 2002.</p>
<p>[12] M. Elad and M. Aharon. Image denoising via sparse and redundant representations over learned dictionaries. IEEE Trans. Image Processing, 15(12):3736–3745, 2006.</p>
<p>[13] G. Golub and C. V. Loan. Matrix Computations. JHU Press, 1996.</p>
<p>[14] Z. Jiang, Z. Lin, and L. Davis. Learning a dicscriminative dictionary for sparse coding via label consistent K-SVD. In CVPR, 2011.</p>
<p>[15] M. S. Lewicki and T. J. Sejnowski. Learning overcomplete representations. Neural computation, 12(2):337–365, 2000.</p>
<p>[16] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factorization and sparse coding. JMLR, 11, 2010.</p>
<p>[17] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Non-local sparse models for image restoration. In ICCV, pages 2272–2279. IEEE, 2009.</p>
<p>[18] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Supervised dictionary learning. In NIPS, 2009.</p>
<p>[19] J. Mairal, M. Elad, and G. Sapiro. Sparse representation for color image restoration. Image Processing, IEEE Transactions on, 17(1):53–69, 2008.</p>
<p>[20] J. Mairal, G. Sapiro, and M. Elad. Learning multiscale sparse</p>
<p>[21]</p>
<p>[22]</p>
<p>[23]</p>
<p>[24]</p>
<p>[25]</p>
<p>[26]  representations for image and video restoration. Technical report, DTIC Document, 2007. S. Mallat. A Wavelet Tour of Signal Processing: The Sparse Way. Academic Press, third edition, 2008. R. Mazhar and P. D. Gader. EK-SVD: Optimized dictionary design for sparse representations. In ICPR, 2008. B. A. Olshausen, D. J. Field, et al. Sparse coding with an overcomplete basis set: A strategy employed by vi? Vision research, 37(23):331 1–3326, 1997. B. Ophir, M. Lustig, and M. Elad. Multi-scale dictionary learning using wavelets. IEEE J. Selected Topics in Signal Processing, 5(5):1014–1024, 2011. K. Rao and P. Yip. Discrete Cosine Transform: Algorithms, Advantages and Applications. Academic Press, 1990. R. Rubinstein, M. Zibulevsky, and M. Elad. Efficient implementation of the k-svd algorithm using batch orthogonal matching pursuit. CS Technion, 2008. 33338903</p>
<p>[27] J. Shi, X. Ren, G. Dai, J. Wang, and Z. Zhang. A non-convex relaxation approach to sparse dictionary learning. In CVPR,</p>
<p>[28]</p>
<p>[29]</p>
<p>[30] [3 1]  2011. A. Tropp. Greed is good: algorithmic results for sparse approximation. IEEE Trans. Inf. Theory, 50(10), 2004. L. Zhang, W. Dong, D. Zhang, and G. Shi. Two-stage image denoising by principal component analysis with local pixel grouping. Pattern Recognition, 43(4): 153 1–1549, 2010. Q. Zhang and B. Li. Discriminative k-SVD for dictionary learning in face recognition. In CVPR, 2010. H. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis. Journal of computational and graphical statistics, 15(2):265–286, 2006.  Appendix A: Proof of Proposition  2.1.  Dˆ?Dˆ  By the fact that = In, the minimization (6) is the equivalent to the following minimization mVin  ?Dˆ?G − V ?2F + λ2?V ?0,  (10)  which can rewritten as  {mVii,nj}?ij(Vi,j− (Dˆ?G)i,j)2+ λ|Vi,j| or equivalently the summation of multiple independent univariate minimization problems  ?i,j{mVii,nj}(Vi,j− (Dˆ?G)i,j)2+ λ|Vi,j|. 2 +  Recall that minimization problem minx∈R ?x − y? ?x?0 has a unique solution x∗ = Tλ (y). Th?usx, t h−e unique  λ2  minimizer for (6) is Tλ(Dˆ?G). Appendix B: Proof of Proposition 2.2. The objective function in (7) is equal to ?G − AVA − DVD?2F =?G − AVA?2F + ?DVD?2F − Tr((G − AVA)?DVD). (11) If D?D = I and A?D = 0, then the first two terms in (11) are constant and Tr((AVA)?DVD) = 0. Therefore, the minimization (7) is equivalent to mDaxTr(D?GVD?),s.t. D?D = Ir,A?D = 0.  (12)  Considering the following SVD: (In − PA)GVD?  = PΣQ?.  From the Theorem 4 in [3 1], D = PQ? is the minimizer of the following minimization problem D∈mRanx×rTr(D?(I − PA)GVD?),  s.t. D?D = Ir.  (13)  Notice that the space spanned by the columns P is equal to the one spanned by the columns of (I−PA)GVD? which is orthogonal nton ethde b space spanned by IA−. TPherefore, A?D = A?PQ? = 0. Put all together, we have D = PQ? is the minimizer to the following minimization problem.  Dm∈Rarx×pTr(D?(I − PA)GVD?), s.t. D?D = Ip, A?D = 0.  (14)  Together with the fact  D?GVD?== DD??(PIA −G PVAD?)G+V DD??(I − PA)GVD?  (15)  The last equality in (15) holds when the constraint A?D = 0 is satisfied. The proof is complete. 33338914</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
