<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-29" href="../iccv2013/iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">iccv2013-29</a> <a title="iccv-2013-29-reference" href="#">iccv2013-29-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</h1>
<br/><p>Source: <a title="iccv-2013-29-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Liu_A_Scalable_Unsupervised_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Lingqiao Liu, Lei Wang</p><p>Abstract: To achieve a good trade-off between recognition accuracy and computational efficiency, it is often needed to reduce high-dimensional visual data to medium-dimensional ones. For this task, even applying a simple full-matrixbased linear projection causes significant computation and memory use. When the number of visual data is large, how to efficiently learn such a projection could even become a problem. The recent feature merging approach offers an efficient way to reduce the dimensionality, which only requires a single scan of features to perform reduction. However, existing merging algorithms do not scale well with highdimensional data, especially in the unsupervised case. To address this problem, we formulate unsupervised feature merging as a PCA problem imposed with a special structure constraint. By exploiting its connection with kmeans, we transform this constrained PCA problem into a feature clustering problem. Moreover, we employ the hashing technique to improve its scalability. These produce a scalable feature merging algorithm for our dimensional- ity reduction task. In addition, we develop an extension of this method by leveraging the neighborhood structure in the data to further improve dimensionality reduction performance. In further, we explore the incorporation of bipolar merging a variant of merging function which allows the subtraction operation into our algorithms. Through three applications in visual recognition, we demonstrate that our methods can not only achieve good dimensionality reduction performance with little computational cost but also help to create more powerful representation at both image level and local feature level. – –</p><br/>
<h2>reference text</h2><p>[1] E. Bingham and H. Mannila. Random projection in dimensionality reduction: applications to image and text data. In KDD, 2001. 2, 3</p>
<p>[2] K. Chatfield, V. Lempitsky, A. Vedaldi, and A. Zisserman. The devil is in the details: an evaluation of recent feature encoding methods. In BMVC, 2011. 1</p>
<p>[3] C. Cortes, M. Mohri, and A. Rostamizadeh. Algorithms for learning kernels based on centered alignment. JMLR, 2012. 4</p>
<p>[4] C. H. Q. Ding and X. He. K-means clustering via principal compo-</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]  nent analysis. In ICML, 2004. 3 T. T. Do, L. Gan, Y. Chen, N. Nguyen, and T. D. Tran. Fast and efficient dimensionality reduction using structurally random matrices. In ICASSP, pages 1821–1824, 2009. 2 B. Fulkerson, A. Vedaldi, and S. Soatto. Localizing objects with smart dictionaries. In ECCV, pages 179–192, 2008. 1, 2 S. Hussain and B. Triggs. Visual recognition using local quantized patterns. In ECCV (2), pages 716–729, 2012. 1, 6 H. Jegou, M. Douze, and C. Schmid. Hamming embedding and weak geometric consistency for large scale image search. In ECCV, 2008. 7</p>
<p>[9] H. Jegou, M. Douze, C. Schmid, and P. P ´erez. Aggregating local descriptors into a compact image representation. In CVPR, 2010. 7</p>
<p>[10] J. Liu and M. Shah. Learning human actions via information maximization. CVPR, 2008. 2, 5</p>
<p>[11] J. Liu, Y. Yang, and M. Shah. Learning semantic visual vocabularies using diffusion distance. In CVPR, pages 461–468, 2009. 2</p>
<p>[12] L. Liu, L. Wang, and X. Liu. In defence of soft-assignment coding. In CVPR, 2012. 5, 7</p>
<p>[13] L. Liu, L. Wang, and C. Shen. A generalized probabilistic framework for compact codebook creation. In CVPR, 2011. 1, 2</p>
<p>[14] T. Ojala, M. Pietik¨ ainen, and D. Harwood. A comparative study of texture measures with classification based on featured distributions. Pattern Recognition, 29(1):51–59, Jan. 1996. 6</p>
<p>[15] F. Perronnin, J. S ´anchez, and T. Mensink. Improving the fisher kernel for large-scale image classification. In ECCV, 2010. 1</p>
<p>[16] Q. Shi, H. Li, and C. Shen. Rapid face recognition using hashing. In CVPR, pages 2753–2760, 2010. 2, 3, 4, 5</p>
<p>[17] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong. Localityconstrained linear coding for image classification. In CVPR 2010. 5</p>
<p>[18] L. Wang, L. Zhou, and C. Shen. A fast algorithm for creating a compact and discriminative visual codebook. In ECCV, 2008. 1, 2</p>
<p>[19] J. M. Winn, A. Criminisi, and T. P. Minka. Object categorization by learned universal visual dictionary. In ICCV, 2005. 2</p>
<p>[20] J. Wu and J. M. Rehg. Centrist: A visual descriptor for scene cate-  gorization. IEEE TPAMI., 33(8): 1489–1501, 2011. 6</p>
<p>[21] J. Yang, K. Yu, Y. Gong, and T. S. Huang. Linear spatial pyramid matching using sparse coding for image classification. In CVPR, 2009. 7</p>
<p>[22] B. Yao, G. R. Bradski, and F.-F. Li. A codebook-free and annotationfree approach for fine-grained image categorization. In CVPR, pages 3466–3473, 2012. 7</p>
<p>[23] H. Zha, X. He, C. H. Q. Ding, M. Gu, and H. D. Simon. Spectral relaxation for k-means clustering. In NIPS, pages 1057–1064, 2001. 3 333000111555</p>
<br/>
<br/><br/><br/></body>
</html>
