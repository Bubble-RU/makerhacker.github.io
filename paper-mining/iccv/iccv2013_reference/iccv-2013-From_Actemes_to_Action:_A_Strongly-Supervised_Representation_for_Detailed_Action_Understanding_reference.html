<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>175 iccv-2013-From Actemes to Action: A Strongly-Supervised Representation for Detailed Action Understanding</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-175" href="../iccv2013/iccv-2013-From_Actemes_to_Action%3A_A_Strongly-Supervised_Representation_for_Detailed_Action_Understanding.html">iccv2013-175</a> <a title="iccv-2013-175-reference" href="#">iccv2013-175-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>175 iccv-2013-From Actemes to Action: A Strongly-Supervised Representation for Detailed Action Understanding</h1>
<br/><p>Source: <a title="iccv-2013-175-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Zhang_From_Actemes_to_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Weiyu Zhang, Menglong Zhu, Konstantinos G. Derpanis</p><p>Abstract: This paper presents a novel approach for analyzing human actions in non-scripted, unconstrained video settings based on volumetric, x-y-t, patch classifiers, termed actemes. Unlike previous action-related work, the discovery of patch classifiers is posed as a strongly-supervised process. Specifically, keypoint labels (e.g., position) across spacetime are used in a data-driven training process to discover patches that are highly clustered in the spacetime keypoint configuration space. To support this process, a new human action dataset consisting of challenging consumer videos is introduced, where notably the action label, the 2D position of a set of keypoints and their visibilities are provided for each video frame. On a novel input video, each acteme is used in a sliding volume scheme to yield a set of sparse, non-overlapping detections. These detections provide the intermediate substrate for segmenting out the action. For action classification, the proposed representation shows significant improvement over state-of-the-art low-level features, while providing spatiotemporal localiza- tion as additional output. This output sheds further light into detailed action understanding.</p><br/>
<h2>reference text</h2><p>[1] L. Bourdev, S. Maji, T. Brox, and J. Malik. Detecting people using mutually consistent poselet activations. In ECCV, 2010.</p>
<p>[2] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High accuracy optical flow estimation based on a theory for warping.</p>
<p>[3]</p>
<p>[4]</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]  In ECCV, 2004. T. Davies and D. Hoffman. Facial attention and spacetime fragments. Axiomathes, 13(3-4):303–327, 2003. K. Derpanis, M. Sizintsev, K. Cannons, and R. Wildes. Action spotting and recognition based on a spatiotemporal orientation analysis. PAMI, 35(3), 2013. P. Doll a´r, V. Rabaud, G. Cottrell, and S. Belongie. Behavior recognition via sparse spatio-temporal features. In VS-PETS, October 2005. A. Efros, A. Berg, G. Mori, and J. Malik. Recognizing action at a distance. In ICCV, 2003. A. Fathi and G. Mori. Action recognition by learning midlevel motion features. In CVPR, 2008. P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained partbased models. PAMI, 32(9): 1627–1645, 2010. A. Gaidon, Z. Harchaoui, and C. Schmid. Actom sequence models for efficient action detection. In CVPR, 2011. J. Gall, A. Yao, N. Razavi, L. Van Gool, and V. Lempitsky. Hough forests for object detection, tracking, and action recognition. PAMI, 33(1 1):2188–2202, Nov. 2011. W. Geisler. Motion streaks provide a spatial code for motion direction. Nature, 400(6739):65–69, 1999. 22225544  (red) for each action.  Bottom two rows  show example  true (green)  and false (red) positives.</p>
<p>[12] H. Jhuang, T. Serre, L. Wolf, and T. Poggio. A biologically inspired system for action recognition. In ICCV, 2007.</p>
<p>[13] Y. Ke, R. Sukthankar, and M. Hebert. Event detection in crowded videos. In ICCV, 2007.</p>
<p>[14] A. Klaser, M. Marszalek, and C. Schmid. A spatio-temporal descriptor based on 3D-gradients. In BMVC, 2008.</p>
<p>[15] O. Kliper-Gross, Y. Gurovich, T. Hassner, and L. Wolf. Motion interchange patterns for action recognition in unconstrained videos. In ECCV, 2012.</p>
<p>[16] O. Kliper-Gross, T. Hassner, and L. Wolf. The action similarity labeling challenge. PAMI, 34(3):615–621, 2012.</p>
<p>[17] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. HMDB: A large video database for human motion recognition. In ICCV, 2011.</p>
<p>[18] I. Laptev. On space-time interest points. IJCV, 64(2-3): 107– 123, 2005.</p>
<p>[19] S. Maji, L. Bourdev, and J. Malik. Action recognition from a</p>
<p>[20]</p>
<p>[21]</p>
<p>[22]</p>
<p>[23]</p>
<p>[24]</p>
<p>[25]</p>
<p>[26]</p>
<p>[27]</p>
<p>[28]</p>
<p>[29]</p>
<p>[30]  distributed representation of pose and appearance. In CVPR, 2011. T. B. Moeslund, A. Hilton, V. Kr¨ uger, and L. Sigal, editors. Visual Analysis of Humans - Looking at People. Springer, 2011. J. Niebles, C. Chen, and L. Fei Fei. Modeling temporal structure of decomposable motion segments for activity classification. In ECCV, 2010. M. Raptis, I. Kokkinos, and S. Soatto. Discovering discriminative action parts from mid-level video representations. In CVPR, 2012. K. K. Reddy and M. Shah. Recognizing 50 human action categories of web videos. Mach. Vis. Appl., 24(5):971–981, 2013. B. Russell, W. Freeman, A. Efros, J. Sivic, and A. Zisserman. Using multiple segmentations to discover objects and their extent in image collections. In CVPR, 2006. S. Sadanand and J. Corso. Action bank: A high-level representation of activity in video. In CVPR, 2012. K. Schindler and L. Van Gool. Action snippets: How many frames does human action recognition require? In CVPR, 2008. C. Schuldt, I. Laptev, and B. Caputo. Recognizing human actions: A local SVM approach. In ICPR, pages III: 32–36, 2004. C. Vondrick, D. Patterson, and D. Ramanan. Efficiently scaling up crowdsourced video annotation. IJCV, 2012. H. Wang, A. Klaser, C. Schmid, and C. Liu. Action recognition by dense trajectories. In CVPR, 2011. H. Wang, M. Ullah, A. Klaser, I. Laptev, and C. Schmid. Evaluation of local spatio-temporal features for action recog-  nition. In BMVC, 2009. [3 1] Y. Yang and D. Ramanan. Articulated pose estimation with flexible mixtures-of-parts. In CVPR, 2011. 22225555</p>
<br/>
<br/><br/><br/></body>
</html>
