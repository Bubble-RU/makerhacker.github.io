<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>428 iccv-2013-Translating Video Content to Natural Language Descriptions</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-428" href="../iccv2013/iccv-2013-Translating_Video_Content_to_Natural_Language_Descriptions.html">iccv2013-428</a> <a title="iccv-2013-428-reference" href="#">iccv2013-428-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>428 iccv-2013-Translating Video Content to Natural Language Descriptions</h1>
<br/><p>Source: <a title="iccv-2013-428-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Rohrbach_Translating_Video_Content_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Marcus Rohrbach, Wei Qiu, Ivan Titov, Stefan Thater, Manfred Pinkal, Bernt Schiele</p><p>Abstract: Humans use rich natural language to describe and communicate visual perceptions. In order to provide natural language descriptions for visual content, this paper combines two important ingredients. First, we generate a rich semantic representation of the visual content including e.g. object and activity labels. To predict the semantic representation we learn a CRF to model the relationships between different components of the visual input. And second, we propose to formulate the generation of natural language as a machine translation problem using the semantic representation as source language and the generated sentences as target language. For this we exploit the power of a parallel corpus of videos and textual descriptions and adapt statistical machine translation to translate between our two languages. We evaluate our video descriptions on the TACoS dataset [23], which contains video snippets aligned with sentence descriptions. Using automatic evaluation and human judgments we show significant improvements over several baseline approaches, motivated by prior work. Our translation approach also shows improvements over related work on an image description task.</p><br/>
<h2>reference text</h2><p>[1] A. Aker and R. J. Gaizauskas. Generating image descriptions using dependency relational patterns. In ACL, 2010. 2</p>
<p>[2] A. Barbu, A. Bridge, Z. Burchill, D. Coroian, S. Dickinson, S. Fidler, A. Michaux, S. Mussman, S. Narayanaswamy, D. Salvi, L. Schmidt, J. Shangguan, J. M. Siskind, J. Waggoner, S. Wang, J. Wei, Y. Yin, and Z. Zhang. Video in sentences out. In UAI, 2012. 1, 2</p>
<p>[3] J. Corso, C. Xu, P. Das, R. F. Doell, and P. Rosebrough. Thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching. In CVPR, 2013. 1, 2</p>
<p>[4] P. Duygulu, K. Barnard, N. de Freitas, and D. A. Forsyth. Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary. In ECCV, 2002. 2</p>
<p>[5] A. Farhadi, M. Hejrati, M. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier, and D. Forsyth. Every picture tells a story: Generating sentences from images. In ECCV, 2010. 1, 2, 3, 4, 5, 6, 7</p>
<p>[6] M. Federico, N. Bertoldi, and M. Cettolo. IRSTLM: an open source toolkit for handling large scale language models. In Interspeech. ISCA, 2008. 4</p>
<p>[7] Y. Feng and M. Lapata. How many words is a picture worth? Automatic caption generation for news images. ACL’ 10. 2 439  Table 4: Example output of our system (blue) compared to baseline approaches and human descriptions, errors in red. (1, 2) our system provides the best output; (2, 3) our system partially recovers from a wrong SR; (4) failure case.</p>
<p>[8] S. Guadarrama, N. Krishnamoorthy, G. Malkarnenkar, R. Mooney, T. Darrell, and K. Saenko. Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shoot recognition. In ICCV, 2013. 1, 2</p>
<p>[9] A. Gupta, P. Srinivasan, J. B. Shi, and L. Davis. Understanding videos, constructing plots learning a visually grounded storyline model from annotated videos. In CVPR, 2009. 1, 2</p>
<p>[10] P. Hanckmann, K. Schutte, and G. J. Burghouts. Automated textual descriptions for a wide range of video events with 48 human actions. In ECCV Workshops, 2012. 1, 2</p>
<p>[11] M. U. G. Khan, L. Zhang, and Y. Gotoh. Human focused video description. In ICCV Workshops, 2011. 1, 2</p>
<p>[12] P. Koehn. Statistical Machine Translation. Cambridge University Press, 2010. 2</p>
<p>[13] P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. Moses: Open source toolkit for statistical machine translation. In ACL demo, 2007. 2, 4</p>
<p>[14] A. Kojima, T. Tamura, and K. Fukunaga. Natural language description of human activities from video images based on concept hierarchy of actions. IJCV, 2002. 1, 2</p>
<p>[15] G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg, and T. L. Berg. Baby talk: Understanding and generating simple image descriptions. In CVPR, 2011. 1, 2, 3, 4, 5, 6, 7</p>
<p>[16] P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and Y. Choi. Collective generation of natural image descriptions. In ACL, 2012. 1, 3, 5</p>
<p>[17] A. Lopez. Statistical machine translation. ACM, 2008. 2</p>
<p>[18] M. Mitchell, J. Dodge, A. Goyal, K. Yamaguchi, K. Stratos, X. Han, A. Mensch, A. C. Berg, T. L. Berg, and H. D. III. Midge: Generating image descriptions from computer vision detections. In EACL, 2012. 1, 3, 6</p>
<p>[19] F. J. Och and H. Ney. A systematic comparison of various statistical alignment models. CL, 2003. 4</p>
<p>[20] V. Ordonez, G. Kulkarni, and T. L. Berg. Im2text: Describing images using 1 million captioned photographs. In NIPS, 2011. 3</p>
<p>[21] K. Papineni, S. Roukos, T. Ward, and W. jing Zhu. BLEU: a method for automatic evaluation of machine translation. In ACL, 2002. 5</p>
<p>[22] V. Ramanathan, P. Liang, and L. Fei-Fei. Video event understanding using natural language descriptions. In ICCV, 2013. 7</p>
<p>[23] M. Regneri, M. Rohrbach, D. Wetzel, S. Thater, B. Schiele, and M. Pinkal. Grounding action descriptions in videos.</p>
<p>[24]</p>
<p>[25]</p>
<p>[26]</p>
<p>[27]  TACL, 2013. 1, 2, 4, 5 M. Rohrbach, M. Regneri, M. Andriluka, S. Amin, M. Pinkal, and B. Schiele. Script data for attribute-based recognition of composite activities. In ECCV, 2012. 2, 5 M. Schmidt. UGM: Matlab code for undirected graphical models. di.ens.fr/∼mschmidt/Software/UGM.html, 2013. 3 mC.o dCe. Tan, eYn.s-G.fr. Jiang, amndid tC/S.-oWftw. Ngo. GTMow.ahrtdmsl textually describing complex video contents with audio-visual concept classifiers. In ACM Multimedia, 2011. 1, 2 H. Wang, A. Kl¨ aser, C. Schmid, and C. Liu. Dense trajectories and motion boundary descriptors for action recognition. IJCV, 2013. 2, 4, 5 440</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
