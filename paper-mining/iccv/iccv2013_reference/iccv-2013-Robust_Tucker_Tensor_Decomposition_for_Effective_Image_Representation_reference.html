<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>362 iccv-2013-Robust Tucker Tensor Decomposition for Effective Image Representation</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-362" href="../iccv2013/iccv-2013-Robust_Tucker_Tensor_Decomposition_for_Effective_Image_Representation.html">iccv2013-362</a> <a title="iccv-2013-362-reference" href="#">iccv2013-362-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>362 iccv-2013-Robust Tucker Tensor Decomposition for Effective Image Representation</h1>
<br/><p>Source: <a title="iccv-2013-362-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Zhang_Robust_Tucker_Tensor_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Miao Zhang, Chris Ding</p><p>Abstract: Many tensor based algorithms have been proposed for the study of high dimensional data in a large variety ofcomputer vision and machine learning applications. However, most of the existing tensor analysis approaches are based on Frobenius norm, which makes them sensitive to outliers, because they minimize the sum of squared errors and enlarge the influence of both outliers and large feature noises. In this paper, we propose a robust Tucker tensor decomposition model (RTD) to suppress the influence of outliers, which uses L1-norm loss function. Yet, the optimization on L1-norm based tensor analysis is much harder than standard tensor decomposition. In this paper, we propose a simple and efficient algorithm to solve our RTD model. Moreover, tensor factorization-based image storage needs much less space than PCA based methods. We carry out extensive experiments to evaluate the proposed algorithm, and verify the robustness against image occlusions. Both numerical and visual results show that our RTD model is consistently better against the existence of outliers than previous tensor and PCA methods.</p><br/>
<h2>reference text</h2><p>[1] A. Baccini, P. Besse, and A. D. Falguerolles. An L1-norm PCA and a heuristic approach. Ordinal and Symbolic Data Analysis, edited by E. Diday, Y. Lechevalier and O. Opitz, Springer, 1996.</p>
<p>[2] P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman. Eigenfaces vs. fisherfaces: Recognition using class specific linear projection. IEEE Trans. Pattern Anal. Mach. Intell. , 19(7):71 1–720, 1997.</p>
<p>[3] D. P. Bertsekas. Nonlinear Programming, 2nd Ed. MIT Press, 1998.</p>
<p>[4] J. Bolton and W. J. Krzanowski. A characterization of principal components for projection pursuit, Mar. 26 2001.</p>
<p>[5] E. J. Candes, X. Li, Y. Ma, and J. Wright. Robust principal component analysis, Dec. 18 2009.</p>
<p>[6] C. H. Q. Ding and J. Ye. 2-dimensional singular value decomposition for 2d maps and images. In SDM, 2005.</p>
<p>[7] Y. Fu, Z. Li, J. Yuan, Y. Wu, and T. S. Huang. Locality versus globality: Query-driven localized linear models for facial image computing. IEEE Trans. Circuits Syst. Video Techn., 18(12): 1741–1752, 2008.</p>
<p>[8] J. Galpin and D. Hawkins. Methods of l1 estimation of a covariance matrix. Computational Statistics and Data Analysis, 5:305–319, 1987.</p>
<p>[9] J. Gao. Robust L1 principal component analysis and its bayesian variational inference. Neural Computation, 20(2), 2008.</p>
<p>[10] X. He, S. Yan, Y. Hu, and H. jiang Zhang. Learning a locality preserving subspace for visual recognition. In in Proc. IEEE International Conference on Computer Vision, pages 385–</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]</p>
<p>[19]</p>
<p>[20]</p>
<p>[21]  393, 2003. K. Inoue and K. Urahama. Equivalence of non-iterative algorithms for simultaneous low rank approximations of matrices. In CVPR (1), pages 154–159, 2006. Q. Ke and T. Kanade. Robust l1 norm factorization in the presence of outliers and missing data by alternative convex programming. In IEEE Conf. Computer Vision and Pattern Recognition, pages 592–599, 2004. N. Kwak. Principal component analysis based on L1-norm maximization. IEEE Trans. Pattern Anal. Mach. Intell, 30(9): 1672–1680, 2008. L. D. Lathauwer, B. D. Moor, and J. Vandewalle. A multilinear singular value decomposition. SIAM J. Matrix Anal. Appl, 21: 1253–1278, 2000. D. Luo, C. Ding, and H. Huang. Are tensor decomposition solutions unique? on the global convergence of hosvd and parafac algorithms. 2008. A. Shashua and A. Levin. Linear image coding for regression and classification using the tensor-rank principle. In CVPR, pages 42–49, 2001 . F. D. Torre and M. J. Black. A framework for robust subspace learning. Int’l J. Computer Vision, pages 117–142, 2003. L. R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31:279–3 11, 1966c. M. Turk and A. Pentland. Eigen faces for recognition. Journal of Cognitive Neuroscience, 3:71–86, 1991. M. A. O. Vasilescu and D. Terzopoulos. Multilinear analysis of image ensembles: Tensorfaces. In ECCV (1), pages 447– 460, 2002. A. Y. Yang, A. Ganesh, Z. Zhou, S. Sastry, and Y. Ma. A review of fast l1-minimization algorithms for robust face  recognition. CoRR, abs/1007.3753, 2010.</p>
<p>[22] J. Yang, D. Zhang, A. F. Frangi, and J.-Y. Yang. Twodimensional pca: A new approach to appearance-based face representation and recognition. IEEE Trans. Pattern Anal. Mach. Intell., 26(1): 131–137, 2004.</p>
<p>[23] J. Ye. Generalized low rank approximations of matrices. In ICML, 2004.</p>
<p>[24] H. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis. J. Computational and Graphical Statistics, 15:265–286, 2006. 22445555</p>
<br/>
<br/><br/><br/></body>
</html>
