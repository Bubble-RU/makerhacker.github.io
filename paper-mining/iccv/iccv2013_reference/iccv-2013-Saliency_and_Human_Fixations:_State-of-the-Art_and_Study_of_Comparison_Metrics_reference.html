<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>373 iccv-2013-Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-373" href="../iccv2013/iccv-2013-Saliency_and_Human_Fixations%3A_State-of-the-Art_and_Study_of_Comparison_Metrics.html">iccv2013-373</a> <a title="iccv-2013-373-reference" href="#">iccv2013-373-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>373 iccv-2013-Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics</h1>
<br/><p>Source: <a title="iccv-2013-373-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Riche_Saliency_and_Human_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Nicolas Riche, Matthieu Duvinage, Matei Mancas, Bernard Gosselin, Thierry Dutoit</p><p>Abstract: Visual saliency has been an increasingly active research area in the last ten years with dozens of saliency models recently published. Nowadays, one of the big challenges in the field is to find a way to fairly evaluate all of these models. In this paper, on human eye fixations ,we compare the ranking of 12 state-of-the art saliency models using 12 similarity metrics. The comparison is done on Jian Li ’s database containing several hundreds of natural images. Based on Kendall concordance coefficient, it is shown that some of the metrics are strongly correlated leading to a redundancy in the performance metrics reported in the available benchmarks. On the other hand, other metrics provide a more diverse picture of models ’ overall performance. As a recommendation, three similarity metrics should be used to obtain a complete point of view of saliency model performance.</p><br/>
<h2>reference text</h2><p>[1] R. Achanta, S. Hemami, F. Estrada, and S. Susstrunk. Frequency-tuned salient region detection. In 2009, IEEE Conference on CVPR, pages 1597–1604. IEEE, 2009.</p>
<p>[2] U. Attention Group. Computational Attention Website. http : / /t ct s . fpms .ac .be / att ent i on.</p>
<p>[3] A. Borji. Evaluation measures for saliency maps. http s : / / s it e s . google . com/ s it e / s al iencyevaluat i /home. on</p>
<p>[4] A. Borji and L. Itti. State-of-the-art in visual attention modeling. IEEE Transactions on PAMI, 2012.</p>
<p>[5] A. Borji, D. Sihite, and L. Itti. Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study. IEEE TIP, 2012.</p>
<p>[6] N. Bruce and J. Tsotsos. Saliency based on information maximization. Advances in neural information processing systems, 18: 155, 2006.</p>
<p>[7] A. Garcia-Diaz, X. Fdez-Vidal, X. Pardo, and R. Dosil. Decorrelation and distinctiveness provide with human-like saliency. In Advanced Concepts for Intelligent Vision Systems, pages 343–354. Springer, 2009.</p>
<p>[8] C. Guo, Q. Ma, and L. Zhang. Spatio-temporal saliency detection using phase spectrum of quaternion fourier transform. In 2008, IEEE Conference on CVPR, pages 1–8. IEEE, 2008.</p>
<p>[9] X. Hou and L. Zhang. Saliency detection: A spectral residual approach. In 2007, IEEE Conference on CVPR, pages 1–8. IEEE, 2007.</p>
<p>[10] X. Hou and L. Zhang. Dynamic visual attention: Searching</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]</p>
<p>[19]</p>
<p>[20]  for coding length increments. Advances in neural information processing systems, 21:681–688, 2008. D. C. Howell. Statistical methods for psychology. Wadsworth Publishing Company, 2012. L. Itti, C. Koch, and E. Niebur. A model of saliency-based visual attention for rapid scene analysis. IEEE Transactions on PAMI, 20(1 1): 1254–1259, 1998. T. Judd. saliency benchmark. http : / /people . cs ail . mit .edu /t j udd/ S aliencyBenchmark / . T. Judd, F. Durand, and A. Torralba. A benchmark of computational models of saliency to predict human fixations. MIT tech report, 2012. C. Koch and S. Ullman. Shifts in selective visual attention: towards the underlying neural circuitry. Hum Neurobiol, 4(4):219–27, 1985. O. Le Meur and T. Baccino. Methods for comparing scanpaths and saliency maps: strengths and weaknesses. Behavior Research Methods, pages 1–16, 2011. O. Le Meur, P. Le Callet, D. Barba, et al. Predicting visual fixations on video based on low-level visual features. Vision research, 47(19):2483–2498, 2007. J. Li, M. D. Levine, X. An, X. Xu, and H. He. Visual saliency based on scale-space analysis in the frequency domain. IEEE Transactions on PAMI, 35:996–1010, 2012. N. Ouerhani, R. Von Wartburg, H. Hugli, and R. Muri. Empirical validation of the saliency-based model of visual attention. Electronic letters on computer vision and image analysis, 3(1): 13–24, 2004. O. Pele and M. Werman. A linear time histogram metric for improved sift matching. In Computer Vision–ECCV 2008, pages 495–508. Springer, 2008.</p>
<p>[21] O. Pele and M. Werman. Fast and robust earth mover’s distances. In 2009 IEEE 12th ICCV, pages 460–467. IEEE, 2009.</p>
<p>[22] R. J. Peters and L. Itti. Applying computational tools to predict gaze direction in interactive visual environments. ACM Transactions on Applied Perception (TAP), 5(2):9, 2008.</p>
<p>[23] R. J. Peters, A. Iyer, L. Itti, and C. Koch. Components of bottom-up gaze allocation in natural images. Vision research, 45(18):2397–2416, 2005.</p>
<p>[24] U. Rajashekar, L. K. Cormack, and A. C. Bovik. Point-ofgaze analysis reveals visual search strategies. In Proceedings of SPIE, volume 5292, pages 296–306, 2004.</p>
<p>[25] N. Riche, M. Mancas, M. Duvinage, M. Mibulumukini, B. Gosselin, and T. Dutoit. Rare2012: A multi-scale raritybased saliency detection with its comparative statistical analysis. Signal Processing: Image Communication, 28(6):642 658, 2013.</p>
<p>[26] B. W. Tatler, R. J. Baddeley, I. D. Gilchrist, et al. Visual correlates of fixation selection: Effects of scale and time. Vision research, 45(5):643–659, 2005.</p>
<p>[27] A. Toet. Computational versus psychophysical bottom-up image saliency: A comparative evaluation study. IEEE Transactions on PAMI, 33(1 1):213 1–2146, 2011.</p>
<p>[28] A. Torralba. Modeling global scene factors in attention. JOSA A, 20(7): 1407–1418, 2003.</p>
<p>[29] A. Torralba, A. Oliva, M. S. Castelhano, and J. M. Henderson. Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search. Psychological review, 113(4):766, 2006.</p>
<p>[30] J. K. Tsotsos, S. M. Culhane, W. Y. Kei Wai, Y. Lai, N. Davis, and F. Nuflo. Modeling visual attention via selective tuning. –  Artificial intelligence, 78(1):507–545, 1995.</p>
<p>[31] L. Zhang, M. H. Tong, T. K. Marks, H. Shan, and G. W. Cottrell. Sun: A bayesian framework for saliency using natural statistics. Journal of Vision, 8(7), 2008.</p>
<p>[32] Q. Zhao and C. Koch. Learning a saliency map using fixated locations in natural scenes. Journal of vision, 11(3), 2011. 11 116600</p>
<br/>
<br/><br/><br/></body>
</html>
