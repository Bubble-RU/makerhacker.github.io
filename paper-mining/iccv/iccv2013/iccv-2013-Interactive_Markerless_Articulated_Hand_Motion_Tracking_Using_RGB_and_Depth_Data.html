<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>218 iccv-2013-Interactive Markerless Articulated Hand Motion Tracking Using RGB and Depth Data</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-218" href="#">iccv2013-218</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>218 iccv-2013-Interactive Markerless Articulated Hand Motion Tracking Using RGB and Depth Data</h1>
<br/><p>Source: <a title="iccv-2013-218-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Sridhar_Interactive_Markerless_Articulated_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Srinath Sridhar, Antti Oulasvirta, Christian Theobalt</p><p>Abstract: Tracking the articulated 3D motion of the hand has important applications, for example, in human–computer interaction and teleoperation. We present a novel method that can capture a broad range of articulated hand motions at interactive rates. Our hybrid approach combines, in a voting scheme, a discriminative, part-based pose retrieval method with a generative pose estimation method based on local optimization. Color information from a multiview RGB camera setup along with a person-specific hand model are used by the generative method to find the pose that best explains the observed images. In parallel, our discriminative pose estimation method uses fingertips detected on depth data to estimate a complete or partial pose of the hand by adopting a part-based pose retrieval strategy. This part-based strategy helps reduce the search space drastically in comparison to a global pose retrieval strategy. Quantitative results show that our method achieves state-of-the-art accuracy on challenging sequences and a near-realtime performance of 10 fps on a desktop computer.</p><p>Reference: <a title="iccv-2013-218-reference" href="../iccv2013_reference/iccv-2013-Interactive_Markerless_Articulated_Hand_Motion_Tracking_Using_RGB_and_Depth_Data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de ,  Abstract Tracking the articulated 3D motion of the hand has important applications, for example, in human–computer interaction and teleoperation. [sent-3, score-0.378]
</p><p>2 We present a novel method that can capture a broad range of articulated hand motions at interactive rates. [sent-4, score-0.53]
</p><p>3 Our hybrid approach combines, in a voting scheme, a discriminative, part-based pose retrieval method with a generative pose estimation method based on local optimization. [sent-5, score-1.001]
</p><p>4 Color information from a multiview RGB camera setup along with a person-specific hand model are used by the generative method to find the pose that best explains the observed images. [sent-6, score-0.719]
</p><p>5 In parallel, our discriminative pose estimation method uses fingertips detected on depth data to estimate a complete or partial pose of the hand by adopting a part-based pose retrieval strategy. [sent-7, score-1.828]
</p><p>6 This part-based strategy helps reduce the search space drastically in comparison to a global pose retrieval strategy. [sent-8, score-0.456]
</p><p>7 Introduction Interactive markerless tracking of articulated hand motion has many applications in human–computer interaction, teleoperation, sign language recognition, and virtual character control among others. [sent-11, score-0.556]
</p><p>8 Marker or glove-based solutions exist for tracking the articulations of the hand [25], but they constrain natural hand movement and require extra user effort. [sent-12, score-0.719]
</p><p>9 Recently, many commercial sensors have been developed that detect 3D fingertip locations without using markers but these sensors do not recover a semantically meaningful skeleton model of the hand. [sent-13, score-0.443]
</p><p>10 In this paper we describe a novel markerless hand motion tracking method that captures a broad range of articulations in the form of a kinematic skeleton at near-realtime frame rates. [sent-14, score-0.912]
</p><p>11 Our approach combines two methods (a) Generative pose estimation on multiple RGB images using local optimization (bottom row and top left) (b) Part-based pose retrieval on five finger databases indexed using detected fingertips (top right). [sent-16, score-1.402]
</p><p>12 Most previous realtime markerless approaches (see Section 2) capture slow and simple articulated hand motion since reconstruction of a broader range of complex motions requires offline computation. [sent-18, score-0.653]
</p><p>13 Our algorithm follows a hybrid approach that combines a generative pose estimator with a discriminative one (Figure 1). [sent-19, score-0.61]
</p><p>14 The input to our method are RGB images from five calibrated cameras, depth data from a monocular time-of-flight (ToF) sensor and a user-specific hand model (Section 3). [sent-20, score-0.498]
</p><p>15 The output of our method are the global pose and joint angles of the hand represented using 26 parameters. [sent-21, score-0.661]
</p><p>16 However, using the same strategy for hand tracking is chal-  lenging because of the absence of sufficiently discriminating image features, self-occlusions caused by fingers, and the large number of possible hand poses. [sent-23, score-0.605]
</p><p>17 Similar to previous work in full-body motion tracking [2, 26, 29] we instantiate two pose estimators in parallel. [sent-25, score-0.491]
</p><p>18 SoG stands for Sum of Gaussians  ative pose estimator uses local optimization and a similarity metric based on the Sum of Gaussians (SoG) model [23] to find the pose that best explains the input RGB images (Section 4). [sent-28, score-0.712]
</p><p>19 Second, the discriminative pose estimator, the key technical contribution of this paper, is a part-based retrieval technique that allows us to recover poses spanning a large hand articulation space while dealing with selfocclusions. [sent-29, score-0.81]
</p><p>20 Our discriminative pose estimation method first detects fingertips on depth data using a linear SVM classifier (Section 5. [sent-30, score-0.746]
</p><p>21 The detected fingertips are then used in a hypothesize-and-test framework along with five finger pose databases to obtain multiple pose hypotheses, each of which is tested using two criteria (Section 5. [sent-32, score-1.254]
</p><p>22 The final (complete or partial) hand pose is the pose that has the least er-  ror between the estimated and observed fingertip positions. [sent-34, score-1.235]
</p><p>23 This is then used as initialization for local optimization in the generative pose estimator. [sent-35, score-0.47]
</p><p>24 This part-based approach reduces the database size dramatically as only the articulations of each finger need to be indexed. [sent-36, score-0.432]
</p><p>25 The evidence from both pose estimators are fused using an error metric to obtain a final hand pose (Section 6). [sent-37, score-0.891]
</p><p>26 While there are numerous benchmark datasets for full-body pose estimation, we know of none for hand motion tracking. [sent-39, score-0.633]
</p><p>27 To sum up, the primary contributions of this paper are: •  •  •  A hybrid approach that combines a generative pose estAim haytborri d ba aspepdr on lho tchaalt optimization ewniethr a vnoev peols partbased pose retrieval strategy. [sent-44, score-0.959]
</p><p>28 A near-realtime framework that captures hand motions wAi nthe a l-erevaellt iomf precision arnkd t speed necessary df moro ointiotenr-s active applications. [sent-45, score-0.363]
</p><p>29 Related Work One of the first kinematics-based hand motion tracking methods was presented by Rehg and Kanade [18]. [sent-48, score-0.419]
</p><p>30 The first study of size of the motion space of hand articulations when using kinematic skeletons was done by Lin et al. [sent-49, score-0.534]
</p><p>31 Subsequent surveys of vision-based hand tracking methods [5] have divided methods into two categories—generative methods based on local or global optimization and discriminative methods based on learning from exemplars or exemplar pose retrieval. [sent-52, score-0.746]
</p><p>32 [14] presented a method based on particle swarm optimization for full DoF hand tracking using a depth sensor. [sent-54, score-0.484]
</p><p>33 Other generative approaches have been proposed that use objects being manipulated by the hand as constraints [7, 8, 15, 19]. [sent-56, score-0.398]
</p><p>34 Discriminative Methods: A method for 3D hand pose estimation framed as a database indexing problem was pro-  posed by Athitsos and Sclaroff [1]. [sent-61, score-0.668]
</p><p>35 The idea of using a global pose retrieval from a database of hand poses was explored by Wang et al. [sent-63, score-0.81]
</p><p>36 However, in order to cover the whole range of hand motions the size of the database required would be large. [sent-65, score-0.447]
</p><p>37 [10] proposed a method for hand pose estimation by hand part labeling but not as a kinematic skeleton. [sent-67, score-0.969]
</p><p>38 Our approach takes inspiration from hybrid approaches to full-body pose estimation, such as Ye et al. [sent-73, score-0.383]
</p><p>39 However, our discriminative pose estimator uses a part-based pose retrieval technique as opposed to global pose retrieval. [sent-77, score-1.176]
</p><p>40 The complementary single-view depth data helps us to retrieve poses effectively, as we can resolve depth ambiguities and detect fingertip features in the 2. [sent-81, score-0.649]
</p><p>41 We position nk cameras in an approximate hemisphere such that typical hand motions within this hemispherical space would be visible in multiple cameras. [sent-85, score-0.441]
</p><p>42 The ToF camera is placed such that the hand motion space is within its range and is extrinsically calibrated along with the RGB cameras. [sent-91, score-0.396]
</p><p>43 Hand Modeling: In order to capture the articulations  of the hand we model it as a kinematic chain consisting of 32 joints (see Figure 4). [sent-94, score-0.505]
</p><p>44 Sinc∈e we use a SoG model based generative tracking approach we also augment the kinematic skeleton with 30 uniform 3D Gaussians with a fixed mean, variance,  (a)(b)(c)  Figure 3. [sent-97, score-0.463]
</p><p>45 Generative Hand Pose Estimation  Generative tracking estimates the hand pose parameters ΘG that best match a given set of nk input RGB images according to a consistency energy. [sent-109, score-0.677]
</p><p>46 Using the above defined similarity measure, we can find how similar a particular pose of the 3G SoG hand model is to the observed RGB images. [sent-143, score-0.602]
</p><p>47 We captured four static hand  poses in which joints were clearly visible, and manually positioned our default hand skeleton to fit the poses. [sent-154, score-0.646]
</p><p>48 which enables realtime estimation of the pose parameters at every time instant t, as analytical gradients can be computed for our energy function. [sent-161, score-0.425]
</p><p>49 In Section 5, we describe how our part-based pose retrieval strategy can be used to initialize the optimization. [sent-164, score-0.427]
</p><p>50 Even though the generative pose optimization method is fast and proven to be reliable for full-body tracking, it quickly reaches its limits during hand tracking and fails by converging to local pose optima from which it cannot recover. [sent-165, score-1.173]
</p><p>51 This is because the hand exhibits a higher articulation complexity than the body (thus allowing for a much wider range of poses in a small space), faster motions, and  homogeneous color. [sent-166, score-0.371]
</p><p>52 The consequences are frequent selfocclusions and large visible displacements of the hand between two frames which challenge a local pose optimizer. [sent-167, score-0.603]
</p><p>53 We therefore complement our generative tracker with an efficient discriminative hand pose estimation algorithm described in the following sections. [sent-169, score-0.801]
</p><p>54 It generates hand pose hypotheses in parallel to the generative method and is able to re-initialize it in case of convergence to a wrong pose. [sent-170, score-0.719]
</p><p>55 Part-based Pose Retrieval The goal of our discriminative pose estimation method is to estimate a complete or partial pose, Θ? [sent-172, score-0.443]
</p><p>56 recover the full hand pose, we separately recover the pose of each finger ΘfD. [sent-178, score-0.832]
</p><p>57 This is achieved by extracting fingertips on the depth image using a linear SVM classifier, and by using the detected positions to find the closest match in multiple exemplar finger pose databases. [sent-179, score-0.971]
</p><p>58 First, for combinatorial reasons, the articulation space that we are able to represent in a pose database of necessarily limited size is much larger than when using a single pose database with exemplars for the entire hand (Section 5. [sent-181, score-1.048]
</p><p>59 Second, our approach has 22445599  the advantage of being able to recover a partial hand pose (i. [sent-183, score-0.61]
</p><p>60 The recovered finger poses are then assembled using a hypothesize-and-test framework to form a complete or partial pose Θ? [sent-186, score-0.672]
</p><p>61 ose Database Generation We briefly motivate the need for using multiple finger databases as opposed to a single global pose database. [sent-191, score-0.671]
</p><p>62 The global pose retrieval method of Wang and Popovi´ c [25] uses 18, 000 poses sampled from real hand motion. [sent-192, score-0.754]
</p><p>63 If each joint angle were discretized into 3, then for global pose retrieval the size of the database would be of the order of 1010. [sent-196, score-0.538]
</p><p>64 On the other hand, part-based pose retrieval would need five databases, each with a size of 81. [sent-197, score-0.458]
</p><p>65 Thus, part-based pose retrieval results in much smaller databases for the hand than global pose retrieval. [sent-198, score-1.085]
</p><p>66 Previous approaches [2, 25] that use global pose retrieval capture real data using motion capture systems for generat-  ing a pose database. [sent-200, score-0.84]
</p><p>67 However, complex hand motions are difficult to capture using mocap systems because of selfocclusions and glove constraints. [sent-201, score-0.396]
</p><p>68 We therefore obtain our finger pose database by synthetically generating the poses over discretizations of all joint angles for each finger. [sent-202, score-0.793]
</p><p>69 1) For each synthetic pose generated per finger, we compute the end effector position xsf with respect to a local skeleton coordinate system (see Section 5. [sent-204, score-0.504]
</p><p>70 We use the computed 3D end effector position as our database indexing feature since it uniquely identifies a pose of the finger and can be detected comparatively easily on depth data. [sent-206, score-0.896]
</p><p>71 Palm and Hand Orientation Estimation Since our finger pose databases are indexed based on features relative to the hand model, we need to normalize the detected query features so that they lie in the same frame of reference. [sent-211, score-0.98]
</p><p>72 We first apply a box filter on the depth image Id to extract the depth image, Ib, and unprojected point cloud, Cb, corresponding to the hand only. [sent-213, score-0.548]
</p><p>73 The detected palm center and orientations serve to stabilize the results of the finger pose database look up (see Figure 4). [sent-225, score-0.836]
</p><p>74 Fingertip Detection For our part-based pose retrieval strategy, we need to reliably detect the end effector positions in the depth data. [sent-228, score-0.612]
</p><p>75 Previous work in full-body pose estimation has used features such as Geodesic extrema [2, 16] which do not work well for the hand and result in spurious extrema which are difficult to disambiguate from the real extrema. [sent-229, score-0.678]
</p><p>76 After elimination, we obtain five or less fingertip candidate points xcf. [sent-239, score-0.375]
</p><p>77 Figure 1 shows one depth frame with detected fingertips overlaid and Figure 4 shows the filtered fingertips on the point cloud. [sent-240, score-0.647]
</p><p>78 Finger Pose Estimation  The final step of discriminative pose estimation is to find the complete or partial pose of the hand, Θ? [sent-243, score-0.764]
</p><p>79 However, in order to query the finger pose databases w? [sent-245, score-0.642]
</p><p>80 hard problem since there is tremendous variation in fingertip appearance in depth or RGB images. [sent-248, score-0.472]
</p><p>81 First, for each permutation σi ∈ Σ we reject a hypothesized pose early based on the dist∈anc Σe o wfe eea rechje cdett aec hteydp fingertip t op otshee nearest neighbor in the finger pose database corresponding to the current labeling for that fingertip. [sent-250, score-1.304]
</p><p>82 of detected fingertips, xi is the position of a fingertip corresponding to a candidate fingertip xcf and is the current hypothesis pose. [sent-256, score-0.798]
</p><p>83 Pose Candidate Fusion  At this stage, we have two hand pose candidates, ΘG and Θ? [sent-266, score-0.57]
</p><p>84 With our unoptimized C++ code, the most time consuming components were the local optimization for generative pose estimation (53 ms) and multiscale fingertip detection (40 ms). [sent-285, score-0.856]
</p><p>85 All sequences were manually annotated to mark fingertip and palm center positions in the depth data. [sent-289, score-0.657]
</p><p>86 Overall quantitative results from our experiments show that our approach ofcombining a generative pose estimation method with a discriminative part-base pose retrieval technique (SoG + PBPFingertip) performs better than other alternatives in most cases. [sent-298, score-0.979]
</p><p>87 The Euclidean distance between the estimated and ground truth fingertip positions  and palm center positions are computed for each frame for all datasets. [sent-304, score-0.54]
</p><p>88 This can be attributed to the fact that each time generative pose estimation fails, the discriminative part-base pose retrieval strategy re-initializes it appropriately. [sent-311, score-0.979]
</p><p>89 Conclusion and Future Work In this paper, we presented a novel method for tracking the articulated 3D motion of the human hand using a hybrid method. [sent-331, score-0.547]
</p><p>90 Our main contribution was the use of a new method for part-based pose retrieval in conjunction with image-based pose optimization. [sent-333, score-0.748]
</p><p>91 Part-based pose retrieval enables recovery and stable tracking of poses with self-occlusions that are characteristic of hand motion, and enables a dramatic reduction of the pose database size. [sent-334, score-1.209]
</p><p>92 We would therefore like to explore reducing the number of cameras and incorporating depth data into generative pose optimization. [sent-339, score-0.649]
</p><p>93 Finally, we would like to achieve faster frame rates by using the parallel structure of generative pose optimization. [sent-340, score-0.514]
</p><p>94 A data-driven approach for real-time full body pose reconstruction from a depth camera. [sent-360, score-0.449]
</p><p>95 An objectdependent hand pose prior from sparse training data. [sent-405, score-0.57]
</p><p>96 Hand pose estimation and hand shape classification using multi-layered randomized decision forests. [sent-438, score-0.612]
</p><p>97 Efficient model-based 3D tracking of hand articulations using kinect. [sent-469, score-0.47]
</p><p>98 Visual tracking of high DOF articulated structures: An application to human hand tracking. [sent-497, score-0.422]
</p><p>99 Real-time human pose recognition in parts from single depth images. [sent-517, score-0.449]
</p><p>100 Accurate 3D pose estimation from a single depth image. [sent-585, score-0.491]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sog', 0.401), ('fingertip', 0.344), ('pose', 0.321), ('finger', 0.262), ('hand', 0.249), ('fingertips', 0.215), ('palm', 0.152), ('generative', 0.149), ('depth', 0.128), ('tof', 0.117), ('rgb', 0.115), ('motions', 0.114), ('articulations', 0.114), ('kinematic', 0.108), ('tracking', 0.107), ('retrieval', 0.106), ('skeleton', 0.099), ('stoll', 0.095), ('gaussians', 0.094), ('fingerwave', 0.086), ('oikonomidis', 0.083), ('fingers', 0.071), ('markerless', 0.071), ('articulated', 0.066), ('gpimage', 0.065), ('motion', 0.063), ('realtime', 0.062), ('hybrid', 0.062), ('databases', 0.059), ('effector', 0.057), ('database', 0.056), ('pages', 0.056), ('dof', 0.056), ('calibrated', 0.056), ('berlin', 0.055), ('heidelberg', 0.051), ('cameras', 0.051), ('poses', 0.049), ('forth', 0.046), ('km', 0.045), ('articulation', 0.045), ('detected', 0.045), ('cloud', 0.044), ('interactive', 0.044), ('frame', 0.044), ('abduction', 0.043), ('adduction', 0.043), ('discretizations', 0.043), ('pbpfingertip', 0.043), ('pbpfingertips', 0.043), ('quadtree', 0.043), ('sogs', 0.043), ('unprojected', 0.043), ('wlelim', 0.043), ('popovi', 0.043), ('hands', 0.042), ('estimation', 0.042), ('kb', 0.04), ('discriminative', 0.04), ('partial', 0.04), ('fps', 0.04), ('eij', 0.038), ('estimator', 0.038), ('oversampling', 0.038), ('xcf', 0.038), ('flexion', 0.038), ('keskin', 0.038), ('baak', 0.038), ('ganapathi', 0.038), ('hamer', 0.038), ('kyriazis', 0.038), ('plagemann', 0.038), ('ka', 0.037), ('angles', 0.036), ('athitsos', 0.035), ('monocular', 0.034), ('consisting', 0.034), ('bj', 0.033), ('permutations', 0.033), ('extrema', 0.033), ('selfocclusions', 0.033), ('sequences', 0.033), ('similarity', 0.032), ('theobalt', 0.032), ('five', 0.031), ('ballan', 0.03), ('gall', 0.029), ('helping', 0.029), ('rehg', 0.029), ('seidel', 0.029), ('global', 0.029), ('broad', 0.029), ('cj', 0.029), ('seven', 0.028), ('range', 0.028), ('koller', 0.028), ('volume', 0.027), ('position', 0.027), ('optima', 0.026), ('joint', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999875 <a title="218-tfidf-1" href="./iccv-2013-Interactive_Markerless_Articulated_Hand_Motion_Tracking_Using_RGB_and_Depth_Data.html">218 iccv-2013-Interactive Markerless Articulated Hand Motion Tracking Using RGB and Depth Data</a></p>
<p>Author: Srinath Sridhar, Antti Oulasvirta, Christian Theobalt</p><p>Abstract: Tracking the articulated 3D motion of the hand has important applications, for example, in human–computer interaction and teleoperation. We present a novel method that can capture a broad range of articulated hand motions at interactive rates. Our hybrid approach combines, in a voting scheme, a discriminative, part-based pose retrieval method with a generative pose estimation method based on local optimization. Color information from a multiview RGB camera setup along with a person-specific hand model are used by the generative method to find the pose that best explains the observed images. In parallel, our discriminative pose estimation method uses fingertips detected on depth data to estimate a complete or partial pose of the hand by adopting a part-based pose retrieval strategy. This part-based strategy helps reduce the search space drastically in comparison to a global pose retrieval strategy. Quantitative results show that our method achieves state-of-the-art accuracy on challenging sequences and a near-realtime performance of 10 fps on a desktop computer.</p><p>2 0.37091675 <a title="218-tfidf-2" href="./iccv-2013-Efficient_Hand_Pose_Estimation_from_a_Single_Depth_Image.html">133 iccv-2013-Efficient Hand Pose Estimation from a Single Depth Image</a></p>
<p>Author: Chi Xu, Li Cheng</p><p>Abstract: We tackle the practical problem of hand pose estimation from a single noisy depth image. A dedicated three-step pipeline is proposed: Initial estimation step provides an initial estimation of the hand in-plane orientation and 3D location; Candidate generation step produces a set of 3D pose candidate from the Hough voting space with the help of the rotational invariant depth features; Verification step delivers the final 3D hand pose as the solution to an optimization problem. We analyze the depth noises, and suggest tips to minimize their negative impacts on the overall performance. Our approach is able to work with Kinecttype noisy depth images, and reliably produces pose estimations of general motions efficiently (12 frames per second). Extensive experiments are conducted to qualitatively and quantitatively evaluate the performance with respect to the state-of-the-art methods that have access to additional RGB images. Our approach is shown to deliver on par or even better results.</p><p>3 0.24132149 <a title="218-tfidf-3" href="./iccv-2013-Real-Time_Body_Tracking_with_One_Depth_Camera_and_Inertial_Sensors.html">341 iccv-2013-Real-Time Body Tracking with One Depth Camera and Inertial Sensors</a></p>
<p>Author: Thomas Helten, Meinard Müller, Hans-Peter Seidel, Christian Theobalt</p><p>Abstract: In recent years, the availability of inexpensive depth cameras, such as the Microsoft Kinect, has boosted the research in monocular full body skeletal pose tracking. Unfortunately, existing trackers often fail to capture poses where a single camera provides insufficient data, such as non-frontal poses, and all other poses with body part occlusions. In this paper, we present a novel sensor fusion approach for real-time full body tracking that succeeds in such difficult situations. It takes inspiration from previous tracking solutions, and combines a generative tracker and a discriminative tracker retrieving closest poses in a database. In contrast to previous work, both trackers employ data from a low number of inexpensive body-worn inertial sensors. These sensors provide reliable and complementary information when the monocular depth information alone is not sufficient. We also contribute by new algorithmic solutions to best fuse depth and inertial data in both trackers. One is a new visibility model to determine global body pose, occlusions and usable depth correspondences and to decide what data modality to use for discriminative tracking. We also contribute with a new inertial-basedpose retrieval, and an adapted late fusion step to calculate the final body pose.</p><p>4 0.21566197 <a title="218-tfidf-4" href="./iccv-2013-Real-Time_Articulated_Hand_Pose_Estimation_Using_Semi-supervised_Transductive_Regression_Forests.html">340 iccv-2013-Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests</a></p>
<p>Author: Danhang Tang, Tsz-Ho Yu, Tae-Kyun Kim</p><p>Abstract: This paper presents the first semi-supervised transductive algorithm for real-time articulated hand pose estimation. Noisy data and occlusions are the major challenges of articulated hand pose estimation. In addition, the discrepancies among realistic and synthetic pose data undermine the performances of existing approaches that use synthetic data extensively in training. We therefore propose the Semi-supervised Transductive Regression (STR) forest which learns the relationship between a small, sparsely labelled realistic dataset and a large synthetic dataset. We also design a novel data-driven, pseudo-kinematic technique to refine noisy or occluded joints. Our contributions include: (i) capturing the benefits of both realistic and synthetic data via transductive learning; (ii) showing accuracies can be improved by considering unlabelled data; and (iii) introducing a pseudo-kinematic technique to refine articulations efficiently. Experimental results show not only the promising performance of our method with respect to noise and occlusions, but also its superiority over state-of- the-arts in accuracy, robustness and speed.</p><p>5 0.18403222 <a title="218-tfidf-5" href="./iccv-2013-Monocular_Image_3D_Human_Pose_Estimation_under_Self-Occlusion.html">273 iccv-2013-Monocular Image 3D Human Pose Estimation under Self-Occlusion</a></p>
<p>Author: Ibrahim Radwan, Abhinav Dhall, Roland Goecke</p><p>Abstract: In this paper, an automatic approach for 3D pose reconstruction from a single image is proposed. The presence of human body articulation, hallucinated parts and cluttered background leads to ambiguity during the pose inference, which makes the problem non-trivial. Researchers have explored various methods based on motion and shading in order to reduce the ambiguity and reconstruct the 3D pose. The key idea of our algorithm is to impose both kinematic and orientation constraints. The former is imposed by projecting a 3D model onto the input image and pruning the parts, which are incompatible with the anthropomorphism. The latter is applied by creating synthetic views via regressing the input view to multiple oriented views. After applying the constraints, the 3D model is projected onto the initial and synthetic views, which further reduces the ambiguity. Finally, we borrow the direction of the unambiguous parts from the synthetic views to the initial one, which results in the 3D pose. Quantitative experiments are performed on the HumanEva-I dataset and qualitatively on unconstrained images from the Image Parse dataset. The results show the robustness of the proposed approach to accurately reconstruct the 3D pose form a single image.</p><p>6 0.16852987 <a title="218-tfidf-6" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>7 0.16835667 <a title="218-tfidf-7" href="./iccv-2013-A_Non-parametric_Bayesian_Network_Prior_of_Human_Pose.html">24 iccv-2013-A Non-parametric Bayesian Network Prior of Human Pose</a></p>
<p>8 0.16542986 <a title="218-tfidf-8" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>9 0.14842536 <a title="218-tfidf-9" href="./iccv-2013-Pictorial_Human_Spaces%3A_How_Well_Do_Humans_Perceive_a_3D_Articulated_Pose%3F.html">316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</a></p>
<p>10 0.13365111 <a title="218-tfidf-10" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>11 0.1305327 <a title="218-tfidf-11" href="./iccv-2013-Allocentric_Pose_Estimation.html">46 iccv-2013-Allocentric Pose Estimation</a></p>
<p>12 0.13019873 <a title="218-tfidf-12" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>13 0.12600961 <a title="218-tfidf-13" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>14 0.12286147 <a title="218-tfidf-14" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>15 0.12126336 <a title="218-tfidf-15" href="./iccv-2013-Image_Guided_Depth_Upsampling_Using_Anisotropic_Total_Generalized_Variation.html">209 iccv-2013-Image Guided Depth Upsampling Using Anisotropic Total Generalized Variation</a></p>
<p>16 0.11664697 <a title="218-tfidf-16" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>17 0.11424338 <a title="218-tfidf-17" href="./iccv-2013-No_Matter_Where_You_Are%3A_Flexible_Graph-Guided_Multi-task_Learning_for_Multi-view_Head_Pose_Classification_under_Target_Motion.html">291 iccv-2013-No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion</a></p>
<p>18 0.1136585 <a title="218-tfidf-18" href="./iccv-2013-Parsing_IKEA_Objects%3A_Fine_Pose_Estimation.html">308 iccv-2013-Parsing IKEA Objects: Fine Pose Estimation</a></p>
<p>19 0.11192905 <a title="218-tfidf-19" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>20 0.11121662 <a title="218-tfidf-20" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.223), (1, -0.128), (2, -0.018), (3, 0.064), (4, 0.062), (5, -0.093), (6, 0.035), (7, -0.016), (8, -0.114), (9, 0.196), (10, 0.05), (11, -0.062), (12, -0.141), (13, -0.052), (14, -0.045), (15, 0.121), (16, 0.012), (17, -0.224), (18, 0.001), (19, 0.1), (20, 0.087), (21, 0.039), (22, 0.072), (23, 0.035), (24, -0.026), (25, -0.019), (26, 0.032), (27, 0.126), (28, 0.014), (29, 0.027), (30, -0.014), (31, 0.051), (32, -0.041), (33, 0.041), (34, -0.026), (35, -0.051), (36, -0.003), (37, -0.01), (38, -0.022), (39, 0.04), (40, 0.032), (41, 0.009), (42, -0.013), (43, 0.052), (44, -0.025), (45, 0.067), (46, 0.084), (47, -0.032), (48, -0.039), (49, 0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97993141 <a title="218-lsi-1" href="./iccv-2013-Interactive_Markerless_Articulated_Hand_Motion_Tracking_Using_RGB_and_Depth_Data.html">218 iccv-2013-Interactive Markerless Articulated Hand Motion Tracking Using RGB and Depth Data</a></p>
<p>Author: Srinath Sridhar, Antti Oulasvirta, Christian Theobalt</p><p>Abstract: Tracking the articulated 3D motion of the hand has important applications, for example, in human–computer interaction and teleoperation. We present a novel method that can capture a broad range of articulated hand motions at interactive rates. Our hybrid approach combines, in a voting scheme, a discriminative, part-based pose retrieval method with a generative pose estimation method based on local optimization. Color information from a multiview RGB camera setup along with a person-specific hand model are used by the generative method to find the pose that best explains the observed images. In parallel, our discriminative pose estimation method uses fingertips detected on depth data to estimate a complete or partial pose of the hand by adopting a part-based pose retrieval strategy. This part-based strategy helps reduce the search space drastically in comparison to a global pose retrieval strategy. Quantitative results show that our method achieves state-of-the-art accuracy on challenging sequences and a near-realtime performance of 10 fps on a desktop computer.</p><p>2 0.93466711 <a title="218-lsi-2" href="./iccv-2013-Real-Time_Body_Tracking_with_One_Depth_Camera_and_Inertial_Sensors.html">341 iccv-2013-Real-Time Body Tracking with One Depth Camera and Inertial Sensors</a></p>
<p>Author: Thomas Helten, Meinard Müller, Hans-Peter Seidel, Christian Theobalt</p><p>Abstract: In recent years, the availability of inexpensive depth cameras, such as the Microsoft Kinect, has boosted the research in monocular full body skeletal pose tracking. Unfortunately, existing trackers often fail to capture poses where a single camera provides insufficient data, such as non-frontal poses, and all other poses with body part occlusions. In this paper, we present a novel sensor fusion approach for real-time full body tracking that succeeds in such difficult situations. It takes inspiration from previous tracking solutions, and combines a generative tracker and a discriminative tracker retrieving closest poses in a database. In contrast to previous work, both trackers employ data from a low number of inexpensive body-worn inertial sensors. These sensors provide reliable and complementary information when the monocular depth information alone is not sufficient. We also contribute by new algorithmic solutions to best fuse depth and inertial data in both trackers. One is a new visibility model to determine global body pose, occlusions and usable depth correspondences and to decide what data modality to use for discriminative tracking. We also contribute with a new inertial-basedpose retrieval, and an adapted late fusion step to calculate the final body pose.</p><p>3 0.90622234 <a title="218-lsi-3" href="./iccv-2013-Efficient_Hand_Pose_Estimation_from_a_Single_Depth_Image.html">133 iccv-2013-Efficient Hand Pose Estimation from a Single Depth Image</a></p>
<p>Author: Chi Xu, Li Cheng</p><p>Abstract: We tackle the practical problem of hand pose estimation from a single noisy depth image. A dedicated three-step pipeline is proposed: Initial estimation step provides an initial estimation of the hand in-plane orientation and 3D location; Candidate generation step produces a set of 3D pose candidate from the Hough voting space with the help of the rotational invariant depth features; Verification step delivers the final 3D hand pose as the solution to an optimization problem. We analyze the depth noises, and suggest tips to minimize their negative impacts on the overall performance. Our approach is able to work with Kinecttype noisy depth images, and reliably produces pose estimations of general motions efficiently (12 frames per second). Extensive experiments are conducted to qualitatively and quantitatively evaluate the performance with respect to the state-of-the-art methods that have access to additional RGB images. Our approach is shown to deliver on par or even better results.</p><p>4 0.81509614 <a title="218-lsi-4" href="./iccv-2013-Real-Time_Articulated_Hand_Pose_Estimation_Using_Semi-supervised_Transductive_Regression_Forests.html">340 iccv-2013-Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests</a></p>
<p>Author: Danhang Tang, Tsz-Ho Yu, Tae-Kyun Kim</p><p>Abstract: This paper presents the first semi-supervised transductive algorithm for real-time articulated hand pose estimation. Noisy data and occlusions are the major challenges of articulated hand pose estimation. In addition, the discrepancies among realistic and synthetic pose data undermine the performances of existing approaches that use synthetic data extensively in training. We therefore propose the Semi-supervised Transductive Regression (STR) forest which learns the relationship between a small, sparsely labelled realistic dataset and a large synthetic dataset. We also design a novel data-driven, pseudo-kinematic technique to refine noisy or occluded joints. Our contributions include: (i) capturing the benefits of both realistic and synthetic data via transductive learning; (ii) showing accuracies can be improved by considering unlabelled data; and (iii) introducing a pseudo-kinematic technique to refine articulations efficiently. Experimental results show not only the promising performance of our method with respect to noise and occlusions, but also its superiority over state-of- the-arts in accuracy, robustness and speed.</p><p>5 0.80656087 <a title="218-lsi-5" href="./iccv-2013-Monocular_Image_3D_Human_Pose_Estimation_under_Self-Occlusion.html">273 iccv-2013-Monocular Image 3D Human Pose Estimation under Self-Occlusion</a></p>
<p>Author: Ibrahim Radwan, Abhinav Dhall, Roland Goecke</p><p>Abstract: In this paper, an automatic approach for 3D pose reconstruction from a single image is proposed. The presence of human body articulation, hallucinated parts and cluttered background leads to ambiguity during the pose inference, which makes the problem non-trivial. Researchers have explored various methods based on motion and shading in order to reduce the ambiguity and reconstruct the 3D pose. The key idea of our algorithm is to impose both kinematic and orientation constraints. The former is imposed by projecting a 3D model onto the input image and pruning the parts, which are incompatible with the anthropomorphism. The latter is applied by creating synthetic views via regressing the input view to multiple oriented views. After applying the constraints, the 3D model is projected onto the initial and synthetic views, which further reduces the ambiguity. Finally, we borrow the direction of the unambiguous parts from the synthetic views to the initial one, which results in the 3D pose. Quantitative experiments are performed on the HumanEva-I dataset and qualitatively on unconstrained images from the Image Parse dataset. The results show the robustness of the proposed approach to accurately reconstruct the 3D pose form a single image.</p><p>6 0.79369432 <a title="218-lsi-6" href="./iccv-2013-Pictorial_Human_Spaces%3A_How_Well_Do_Humans_Perceive_a_3D_Articulated_Pose%3F.html">316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</a></p>
<p>7 0.71879005 <a title="218-lsi-7" href="./iccv-2013-A_Non-parametric_Bayesian_Network_Prior_of_Human_Pose.html">24 iccv-2013-A Non-parametric Bayesian Network Prior of Human Pose</a></p>
<p>8 0.70677495 <a title="218-lsi-8" href="./iccv-2013-Multi-scale_Topological_Features_for_Hand_Posture_Representation_and_Analysis.html">278 iccv-2013-Multi-scale Topological Features for Hand Posture Representation and Analysis</a></p>
<p>9 0.66947788 <a title="218-lsi-9" href="./iccv-2013-Live_Metric_3D_Reconstruction_on_Mobile_Phones.html">254 iccv-2013-Live Metric 3D Reconstruction on Mobile Phones</a></p>
<p>10 0.66798598 <a title="218-lsi-10" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>11 0.64732367 <a title="218-lsi-11" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>12 0.64366335 <a title="218-lsi-12" href="./iccv-2013-No_Matter_Where_You_Are%3A_Flexible_Graph-Guided_Multi-task_Learning_for_Multi-view_Head_Pose_Classification_under_Target_Motion.html">291 iccv-2013-No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion</a></p>
<p>13 0.64113683 <a title="218-lsi-13" href="./iccv-2013-Allocentric_Pose_Estimation.html">46 iccv-2013-Allocentric Pose Estimation</a></p>
<p>14 0.63705689 <a title="218-lsi-14" href="./iccv-2013-Parsing_IKEA_Objects%3A_Fine_Pose_Estimation.html">308 iccv-2013-Parsing IKEA Objects: Fine Pose Estimation</a></p>
<p>15 0.63678908 <a title="218-lsi-15" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>16 0.62164694 <a title="218-lsi-16" href="./iccv-2013-Discovering_Object_Functionality.html">118 iccv-2013-Discovering Object Functionality</a></p>
<p>17 0.5804438 <a title="218-lsi-17" href="./iccv-2013-Strong_Appearance_and_Expressive_Spatial_Models_for_Human_Pose_Estimation.html">403 iccv-2013-Strong Appearance and Expressive Spatial Models for Human Pose Estimation</a></p>
<p>18 0.56267655 <a title="218-lsi-18" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>19 0.53492713 <a title="218-lsi-19" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>20 0.52891469 <a title="218-lsi-20" href="./iccv-2013-Recognising_Human-Object_Interaction_via_Exemplar_Based_Modelling.html">344 iccv-2013-Recognising Human-Object Interaction via Exemplar Based Modelling</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.056), (7, 0.017), (12, 0.019), (26, 0.057), (31, 0.04), (34, 0.011), (35, 0.018), (40, 0.018), (42, 0.122), (43, 0.17), (64, 0.067), (73, 0.042), (78, 0.011), (84, 0.068), (89, 0.179), (90, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83938074 <a title="218-lda-1" href="./iccv-2013-Robust_Face_Landmark_Estimation_under_Occlusion.html">355 iccv-2013-Robust Face Landmark Estimation under Occlusion</a></p>
<p>Author: Xavier P. Burgos-Artizzu, Pietro Perona, Piotr Dollár</p><p>Abstract: Human faces captured in real-world conditions present large variations in shape and occlusions due to differences in pose, expression, use of accessories such as sunglasses and hats and interactions with objects (e.g. food). Current face landmark estimation approaches struggle under such conditions since theyfail toprovide aprincipled way ofhandling outliers. We propose a novel method, called Robust Cascaded Pose Regression (RCPR) which reduces exposure to outliers by detecting occlusions explicitly and using robust shape-indexed features. We show that RCPR improves on previous landmark estimation methods on three popular face datasets (LFPW, LFW and HELEN). We further explore RCPR ’s performance by introducing a novel face dataset focused on occlusion, composed of 1,007 faces presenting a wide range of occlusion patterns. RCPR reduces failure cases by half on all four datasets, at the same time as it detects face occlusions with a 80/40% precision/recall.</p><p>same-paper 2 0.83929497 <a title="218-lda-2" href="./iccv-2013-Interactive_Markerless_Articulated_Hand_Motion_Tracking_Using_RGB_and_Depth_Data.html">218 iccv-2013-Interactive Markerless Articulated Hand Motion Tracking Using RGB and Depth Data</a></p>
<p>Author: Srinath Sridhar, Antti Oulasvirta, Christian Theobalt</p><p>Abstract: Tracking the articulated 3D motion of the hand has important applications, for example, in human–computer interaction and teleoperation. We present a novel method that can capture a broad range of articulated hand motions at interactive rates. Our hybrid approach combines, in a voting scheme, a discriminative, part-based pose retrieval method with a generative pose estimation method based on local optimization. Color information from a multiview RGB camera setup along with a person-specific hand model are used by the generative method to find the pose that best explains the observed images. In parallel, our discriminative pose estimation method uses fingertips detected on depth data to estimate a complete or partial pose of the hand by adopting a part-based pose retrieval strategy. This part-based strategy helps reduce the search space drastically in comparison to a global pose retrieval strategy. Quantitative results show that our method achieves state-of-the-art accuracy on challenging sequences and a near-realtime performance of 10 fps on a desktop computer.</p><p>3 0.83889264 <a title="218-lda-3" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>Author: Alon Faktor, Michal Irani</p><p>Abstract: Given a set of images which share an object from the same semantic category, we would like to co-segment the shared object. We define ‘good’ co-segments to be ones which can be easily composed (like a puzzle) from large pieces of other co-segments, yet are difficult to compose from remaining image parts. These pieces must not only match well but also be statistically significant (hard to compose at random). This gives rise to co-segmentation of objects in very challenging scenarios with large variations in appearance, shape and large amounts of clutter. We further show how multiple images can collaborate and “score each others ’ co-segments to improve the overall fidelity and accuracy of the co-segmentation. Our co-segmentation can be applied both to large image collections, as well as to very few images (where there is too little data for unsupervised learning). At the extreme, it can be applied even to a single image, to extract its co-occurring objects. Our approach obtains state-of-the-art results on benchmark datasets. We further show very encouraging co-segmentation results on the challenging PASCAL-VOC dataset. ”</p><p>4 0.81042844 <a title="218-lda-4" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>Author: Christoph Straehle, Ullrich Koethe, Fred A. Hamprecht</p><p>Abstract: We propose a scheme that allows to partition an image into a previously unknown number of segments, using only minimal supervision in terms of a few must-link and cannotlink annotations. We make no use of regional data terms, learning instead what constitutes a likely boundary between segments. Since boundaries are only implicitly specified through cannot-link constraints, this is a hard and nonconvex latent variable problem. We address this problem in a greedy fashion using a randomized decision tree on features associated with interpixel edges. We use a structured purity criterion during tree construction and also show how a backtracking strategy can be used to prevent the greedy search from ending up in poor local optima. The proposed strategy is compared with prior art on natural images.</p><p>5 0.80514109 <a title="218-lda-5" href="./iccv-2013-Learning_Near-Optimal_Cost-Sensitive_Decision_Policy_for_Object_Detection.html">241 iccv-2013-Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection</a></p>
<p>Author: Tianfu Wu, Song-Chun Zhu</p><p>Abstract: Many object detectors, such as AdaBoost, SVM and deformable part-based models (DPM), compute additive scoring functions at a large number of windows scanned over image pyramid, thus computational efficiency is an important consideration beside accuracy performance. In this paper, we present a framework of learning cost-sensitive decision policy which is a sequence of two-sided thresholds to execute early rejection or early acceptance based on the accumulative scores at each step. A decision policy is said to be optimal if it minimizes an empirical global risk function that sums over the loss of false negatives (FN) and false positives (FP), and the cost of computation. While the risk function is very complex due to high-order connections among the two-sided thresholds, we find its upper bound can be optimized by dynamic programming (DP) efficiently and thus say the learned policy is near-optimal. Given the loss of FN and FP and the cost in three numbers, our method can produce a policy on-the-fly for Adaboost, SVM and DPM. In experiments, we show that our decision policy outperforms state-of-the-art cascade methods significantly in terms of speed with similar accuracy performance.</p><p>6 0.80323952 <a title="218-lda-6" href="./iccv-2013-Finding_the_Best_from_the_Second_Bests_-_Inhibiting_Subjective_Bias_in_Evaluation_of_Visual_Tracking_Algorithms.html">168 iccv-2013-Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms</a></p>
<p>7 0.79583466 <a title="218-lda-7" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>8 0.79573178 <a title="218-lda-8" href="./iccv-2013-Semantically-Based_Human_Scanpath_Estimation_with_HMMs.html">381 iccv-2013-Semantically-Based Human Scanpath Estimation with HMMs</a></p>
<p>9 0.79478419 <a title="218-lda-9" href="./iccv-2013-Bayesian_Robust_Matrix_Factorization_for_Image_and_Video_Processing.html">60 iccv-2013-Bayesian Robust Matrix Factorization for Image and Video Processing</a></p>
<p>10 0.79190844 <a title="218-lda-10" href="./iccv-2013-Stacked_Predictive_Sparse_Coding_for_Classification_of_Distinct_Regions_in_Tumor_Histopathology.html">401 iccv-2013-Stacked Predictive Sparse Coding for Classification of Distinct Regions in Tumor Histopathology</a></p>
<p>11 0.78862625 <a title="218-lda-11" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>12 0.78856921 <a title="218-lda-12" href="./iccv-2013-Efficient_Hand_Pose_Estimation_from_a_Single_Depth_Image.html">133 iccv-2013-Efficient Hand Pose Estimation from a Single Depth Image</a></p>
<p>13 0.78809345 <a title="218-lda-13" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>14 0.7875908 <a title="218-lda-14" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>15 0.78725171 <a title="218-lda-15" href="./iccv-2013-Internet_Based_Morphable_Model.html">219 iccv-2013-Internet Based Morphable Model</a></p>
<p>16 0.78713322 <a title="218-lda-16" href="./iccv-2013-Bayesian_Joint_Topic_Modelling_for_Weakly_Supervised_Object_Localisation.html">59 iccv-2013-Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation</a></p>
<p>17 0.78682655 <a title="218-lda-17" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>18 0.78669387 <a title="218-lda-18" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>19 0.7862097 <a title="218-lda-19" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>20 0.78585768 <a title="218-lda-20" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
