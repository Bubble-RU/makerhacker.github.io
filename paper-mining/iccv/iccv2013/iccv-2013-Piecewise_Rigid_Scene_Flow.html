<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>317 iccv-2013-Piecewise Rigid Scene Flow</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-317" href="#">iccv2013-317</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>317 iccv-2013-Piecewise Rigid Scene Flow</h1>
<br/><p>Source: <a title="iccv-2013-317-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Vogel_Piecewise_Rigid_Scene_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Christoph Vogel, Konrad Schindler, Stefan Roth</p><p>Abstract: Estimating dense 3D scene flow from stereo sequences remains a challenging task, despite much progress in both classical disparity and 2D optical flow estimation. To overcome the limitations of existing techniques, we introduce a novel model that represents the dynamic 3D scene by a collection of planar, rigidly moving, local segments. Scene flow estimation then amounts to jointly estimating the pixelto-segment assignment, and the 3D position, normal vector, and rigid motion parameters of a plane for each segment. The proposed energy combines an occlusion-sensitive data term with appropriate shape, motion, and segmentation regularizers. Optimization proceeds in two stages: Starting from an initial superpixelization, we estimate the shape and motion parameters of all segments by assigning a proposal from a set of moving planes. Then the pixel-to-segment assignment is updated, while holding the shape and motion parameters of the moving planes fixed. We demonstrate the benefits of our model on different real-world image sets, including the challenging KITTI benchmark. We achieve leading performance levels, exceeding competing 3D scene flow methods, and even yielding better 2D motion estimates than all tested dedicated optical flow techniques.</p><p>Reference: <a title="iccv-2013-317-reference" href="../iccv2013_reference/iccv-2013-Piecewise_Rigid_Scene_Flow_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Piecewise Rigid Scene Flow  Christoph Vogel  Konrad Schindler  Photogrammetry & Remote Sensing, ETH Zurich Abstract Estimating dense 3D scene flow from stereo sequences remains a challenging task, despite much progress in both classical disparity and 2D optical flow estimation. [sent-1, score-1.707]
</p><p>2 Scene flow estimation then amounts to jointly estimating the pixelto-segment assignment, and the 3D position, normal vector, and rigid motion parameters of a plane for each segment. [sent-3, score-0.96]
</p><p>3 Optimization proceeds in two stages: Starting from an initial superpixelization, we estimate the shape and motion parameters of all segments by assigning a proposal from a set of moving planes. [sent-5, score-0.585]
</p><p>4 Then the pixel-to-segment assignment is updated, while holding the shape and motion parameters of the moving planes fixed. [sent-6, score-0.504]
</p><p>5 We achieve  leading performance levels, exceeding competing 3D scene flow methods, and even yielding better 2D motion estimates than all tested dedicated optical flow techniques. [sent-8, score-1.584]
</p><p>6 Introduction Scene flow estimation is the task of estimating dense 3D surface shape as well as a dense 3D motion field from two (or more) views of a scene taken at two (or more) time steps [20]. [sent-10, score-1.017]
</p><p>7 The 3D scene flow generalizes two classical problems of computer vision, dense stereo matching and dense optical flow estimation. [sent-12, score-1.647]
</p><p>8 Yet, despite significant progress in both stereo [4, 9, 26] and 2D optical flow estimation [5, 16, 17], existing 3D scene flow techniques [e. [sent-13, score-1.613]
</p><p>9 Perhaps surprisingly, the additional information available in stereo motion sequences has not been leveraged to the extent that 3D scene flow outperforms dedicated stereo or 2D optical flow techniques at their respective task. [sent-16, score-2.164]
</p><p>10 Much like stereo or 2D motion estimation, scene flow Stefan Roth Department of Computer Science, TU Darmstadt estimation is ill-posed due to the 3D equivalent of the aperture problem, and thus requires prior assumptions on geom-  etry and motion. [sent-17, score-1.165]
</p><p>11 We posit that existing 3D scene flow techniques have been limited by the underlying representation, and propose to model the scene as a collection of planar regions, each undergoing a rigid motion. [sent-24, score-0.978]
</p><p>12 Following prior work in stereo [4], we argue that most scenes of interest consist of regions with a consistent motion pattern, into which they can be segmented at least implicitly during scene flow estimation. [sent-25, score-1.125]
</p><p>13 Since a larger support is required to fit a plane and its rigid motion (9 unknowns) reliably, we base the initial estimation not on individual pixels, but on a superpixel segmentation of the reference image. [sent-26, score-0.647]
</p><p>14 Scene flow estimation is then cast as a labeling problem, which assigns each pixel to a segment and each segment to a rigidly moving 3D plane. [sent-28, score-1.297]
</p><p>15 Although the superpixels significantly simplify and stabilize the inference, they lead to inaccuracies at flow boundaries, since the initial segmentation does not take into account depth or motion discontinuities. [sent-29, score-0.844]
</p><p>16 We also show how to explicitly include occlusion reasoning both at the segment and pixel level. [sent-31, score-0.486]
</p><p>17 In experiments on challenging, realistic data the proposed approach substantially outperforms three state-of-the-art 3D scene flow methods. [sent-33, score-0.623]
</p><p>18 (right) Processing steps and final result of piecewise rigid scene flow estimation. [sent-35, score-0.855]
</p><p>19 Estimated depth, the lateral 3D motion component, and the re-projected 2D flow are shown. [sent-36, score-0.675]
</p><p>20 outperforms recent dedicated stereo and optical flow algorithms in challenging settings on their respective task. [sent-38, score-1.039]
</p><p>21 Estimation proceeds in two independent  steps: First, 2D optical flow fields are estimated for all views (without requiring that they must be projections of the same 3D flow). [sent-42, score-0.694]
</p><p>22 Stereo disparity is precomputed for each time step; then the optical flow for a reference view and the disparity differences for the other view are estimated. [sent-46, score-1.019]
</p><p>23 [13] integrate a Kalman filter into this approach to yield smooth flow fields over multiple frames. [sent-48, score-0.509]
</p><p>24 Huguet and Devernay [10] were possibly the first to estimate geometry and flow in an integrated manner with a variational formulation. [sent-50, score-0.593]
</p><p>25 [3] parameterize the scene flow by depth and a 3D motion vector w. [sent-52, score-0.885]
</p><p>26 a reference view, and estimate all parameters jointly with a 3D extension of the widely used optical flow method of Brox et al. [sent-55, score-0.743]
</p><p>27 The local rigidity assumption, which for sparse motion estimation dates back to at least Adiv [1], has also been used in 3D motion capture with explicit surface models [e. [sent-59, score-0.452]
</p><p>28 Also related is the optical flow approach of Nir et al. [sent-62, score-0.654]
</p><p>29 [12], in which the flow field is (over-) parameterized by explicitly searching for rigid motion parameters, and then encouraging their smoothness. [sent-63, score-0.841]
</p><p>30 In the presence of a dominant rigid motion (“background motion”) they alternatingly estimate both the relative camera pose and the scene flow. [sent-66, score-0.492]
</p><p>31 Common to these previous approaches to 3D scene flow is that they penalize deviations from spatial smoothness, typically in a robust way. [sent-67, score-0.702]
</p><p>32 In the context of stereo disparity and optical flow, explicit modeling of discontinuities by means of segmentation or layer-based formulations has a long history [23] and has recently gained renewed attention: Bleyer et al. [sent-68, score-0.637]
</p><p>33 [4] estimate disparity by assuming the scene to be segmented into planar superpixels and parameterizing their geometry. [sent-69, score-0.432]
</p><p>34 optical flow that enforces epipolar motion as hard constraint [27]. [sent-74, score-0.922]
</p><p>35 [17] compute optical flow by parameterizing the motion per segment with 2D affine transformations, and also perform occlusion handling. [sent-78, score-1.263]
</p><p>36 Discrete optimization based on fusion of proposals has been applied before to 2D optical flow estimation by Lempitsky et al. [sent-80, score-0.753]
</p><p>37 Piecewise Rigid Model for 3D Scene Flow In contrast to typical approaches to 3D scene flow, our novel model parameterizes the scene as a collection of piecewise planar regions, each of which moves rigidly over time. [sent-84, score-0.625]
</p><p>38 To that end we define an energy function that assigns each pixel to a segment and each segment to the 3D geometry and motion of a plane. [sent-86, score-0.831]
</p><p>39 This allows us to estimate the 3D scene flow and depth for every pixel of a reference view. [sent-87, score-0.856]
</p><p>40 Parameterizing the scene with moving planes also conveniently allows the pixel locations assigned to each plane to be transformed easily between images or mapped into 3D space using the corresponding homographies. [sent-106, score-0.63]
</p><p>41 Over-segmentation is deliberately accepted, both to ensure correct depth and flow estimation for nonplanar and articulated objects, and to maximize boundary recall, even at the cost of spurious segment boundaries that do not correspond to depth or motion discontinuities. [sent-110, score-1.127]
</p><p>42 Model overview Our aim is to estimate depth and a 3D scene flow vector for each pixel ofthe reference frame Il0. [sent-113, score-0.856]
</p><p>43 For now we assume that we have a finite set of possible rigidly moving proposal planes Π = {πj } in 3D. [sent-114, score-0.564]
</p><p>44 , W Wweh itchhen assigns efoacrh tw pixel p i∈n gIsl0: tAo a segment s ∈ I S→; an Sd, a mapping gPn : aSc →h p iΠxe tlo p assign teoac ah segment sto ∈ a rigidly moving n3Dg plane π →∈ ΠΠ . [sent-116, score-0.871]
</p><p>45 t We thus formulate scene flow estimation as minimizing E(P, S) = ED(P, S) + λER(P, S) + μES(S). [sent-117, score-0.663]
</p><p>46 This amounts to a total of 4 constraints per pixel (two stereo constraints at time steps 0 and 1, and two optical flow constraints for the two left, respectively right images; see Fig. [sent-124, score-1.038]
</p><p>47 Our representation with rigidly moving 3D planes induces homographies, which map pixels from the reference view Il0 to the remaining views: Hr0(π) = (M − mnt)K−1  (2a)  Hl1(π) = K(R − tnt)K−1  (2b)  Hr1(π) = (MR −? [sent-126, score-0.6]
</p><p>48 Shape and motion regularization The regularization terms shall encourage piecewise smooth 3D shape, as well as a piecewise smooth 3D motion field. [sent-149, score-0.899]
</p><p>49 Since each segment is assigned to one rigidly moving plane, smoothness within a segment is always satisfied; we thus only need to consider the segment boundaries. [sent-150, score-0.91]
</p><p>50 If 2D regularization in the image plane is desired instead, one can mimic the regularizer of [18] by replacing the 3D distances with disparity differences, differences between 2D optical flow vectors, and changes of the disparity difference over time. [sent-200, score-1.066]
</p><p>51 We employ a segment regularization term similar in spirit to the approach of [21], which encourages smooth segments whose boundaries coincide with image edges. [sent-204, score-0.57]
</p><p>52 The motivation is twofold: First, this prevents segments from getting too large, such that the scene is not overly simplified by the assumed piecewise planarity and piecewise rigid motion. [sent-225, score-0.586]
</p><p>53 The set of seed points E(si) for a segment contains the center pixel of tsheee segment si in the initial superpixelization, as well as all pixels from a regularly-spaced grid (with spacing NS) that fell into the respective initial superpixel. [sent-227, score-0.631]
</p><p>54 term ED and the shape and motion regularization Recall that these regularizers can only incur penalties at segment boundaries, since neighboring pixels within a segment will be assigned to the same moving plane, thus incur no regularization penalty. [sent-247, score-1.14]
</p><p>55 The optimization is performed over a set of proposal planes with their associated motion (see Sec. [sent-253, score-0.502]
</p><p>56 Proposal generation To perform inference over the depth and motion of each segment, we require a comprehensive proposal set of 3D planes along with their rigid motions. [sent-266, score-0.69]
</p><p>57 We can generate these from the output of other scene flow algorithms or by combining the results of stereo and optical flow algorithms (see Sec. [sent-267, score-1.573]
</p><p>58 To that end we fit a 3D plane to each superpixel segment, and estimate its rigid motion from the flow field(s). [sent-270, score-0.928]
</p><p>59 In either case, fitting must be robust to a potentially large amount of outliers, caused both by inaccurate depth and motion estimates and by superpixels not being aligned with surface or motion boundaries. [sent-271, score-0.522]
</p><p>60 We address this by robustly minimizing the transfer error: We first generate an initial solution by minimizing the quadratic transfer error using efficient algebraic methods, and then refine the rigidly moving proposal planes by gradient descent on the robust transfer error (Lorentzian penalty). [sent-272, score-0.564]
</p><p>61 We apply the well-known principle ofusing a fixed occlusion penalty θ, if a certain pixel p (in the reference view) is occluded in at least one view of a pair. [sent-286, score-0.482]
</p><p>62 We now consider whether a pixel p is occluded or not, which depends both on its binary segment assignment xp, and on whether there is any other pixel q (or possibly multiple pixels) that occludes p. [sent-305, score-0.529]
</p><p>63 The summand becomes ˆ u0p, if xp = 0 and the product equals 1, which is the case if there is no pixel that could possibly occlude p, or if all possibly occluding pixels q are assigned a segment xq in which they do not lead to an occlusion. [sent-320, score-0.622]
</p><p>64 1 shows the estimated 3D scene flow (left), and the results after various processing stages (right). [sent-337, score-0.623]
</p><p>65 The segment-based scene reconstruction (Segment) without occlusion handling already gives fairly plausible results in unoccluded areas, but assigns incorrect depth and motion to regions not visible in the reference image (best seen immediately left of the  ×  pedestrian). [sent-338, score-0.731]
</p><p>66 By adding the segment-based occlusion model (Segment & Occlusion), the occlusion regions are properly detected and their motion is extrapolated in a more realistic manner. [sent-339, score-0.5]
</p><p>67 Comparison with 2D scene flow For comparison with other scene flow methods from the literature [10, 18, 24] we consider the synthetic scene of [10], which consists of two independently rotating hemispheres in front of a plane. [sent-351, score-1.4]
</p><p>68 The dataset provides images (1240 376 pixels) recorded with a calibprroatveidd esste irmeoa rig on a car, f3o7r6 benchmarking eofd optical fclaolwiand stereo algorithms in the context of automotive applica-  ×  tions. [sent-356, score-0.551]
</p><p>69 Having stereo pairs from consecutive video frames, the dataset also fulfills the requirements for scene flow estimation (see Fig. [sent-365, score-0.959]
</p><p>70 The KITTI dataset provides a very challenging testbed for today’s stereo, optical flow and scene flow algorithms: First, pixel displacements in the data set are large in general, exceeding 150 pixels for stereo and 250 pixels for optical flow. [sent-368, score-2.026]
</p><p>71 not visible in all four images, we let the stereo and 2D flow algorithms from the proposal generator predict which pixels are out-of-bounds and encourage the scene flow estimate to stay near that prediction. [sent-380, score-1.659]
</p><p>72 reference frame; (right) flow reprojected  to the reference image. [sent-424, score-0.647]
</p><p>73 Nonetheless, we believe that this dataset is better suited for scene flow evaluation than the very limited, synthetic datasets used before [e. [sent-428, score-0.623]
</p><p>74 At time of writing the results of the 2D baseline rank 13th in both stereo and optical flow among published methods in the official KITTI ranking. [sent-433, score-0.95]
</p><p>75 Finally, we distinguish the use of various proposal sets: All experiments default to proposals from the stereo and flowderived 2D baseline technique (S+F). [sent-442, score-0.514]
</p><p>76 The suffix (+R) denotes that a proposal set composed from locally rigid scene flow (Rig, [22]) is additionally used. [sent-443, score-0.914]
</p><p>77 Finally, we optionally make use of egomotion proposals (+E), by estimating the dominant 3D motion using our proposal fitting technique on the 2D baseline output. [sent-444, score-0.424]
</p><p>78 Rather, epipolar motion is one of several proposal solutions, which are used to minimize an energy that can cope with general, non-epipolar motion. [sent-446, score-0.528]
</p><p>79 2D and 3D regularization lead to rather similar results, possibly explained by the fact that the evaluation does not have ground truth for 3D scene flow, but only for disparity and 2D optical flow. [sent-448, score-0.556]
</p><p>80 We thus mostly rely on the 2D regularizer, and note that better 3D benchmarks are needed for quantitative evaluation of 3D scene flow methods. [sent-449, score-0.623]
</p><p>81 Additional occlusion reasoning (-O) improves the results, especially for motion estimates in occluded areas, but performance in the stereo case slightly decreases. [sent-450, score-0.737]
</p><p>82 Still, already the 2D proposal set from S+F alone is sufficient to surpass all our baselines by a large margin, including two recent 3D scene flow techniques [3, 22], on average by 33%. [sent-456, score-0.782]
</p><p>83 f rA pte rt-ipmixe ol fo writing our method (PRSPix-2D+R+E) ranks 1st out of 28 published approaches for optical flow in all measures, and 3rd out of25 published methods in stereo, while (PRSPix-2D) ranked 3th and 5th respectively. [sent-461, score-0.654]
</p><p>84 Moreover, it strongly outperforms another 3D scene flow technique [7], even on its semi-dense output. [sent-465, score-0.655]
</p><p>85 Finally, even the best general 2D optical flow method is surpassed. [sent-466, score-0.654]
</p><p>86 To the best of our knowledge, this is the first scene flow method to outperform optical flow algorithms w. [sent-467, score-1.277]
</p><p>87 Conclusion We have shown that modeling a dynamic scene with local regions corresponding to rigidly moving planes can lead to compelling results for the task of joint geometry and 3D 1383  threshold of Z pixels) in non-occluded areas (Noc) and over the full image (All). [sent-472, score-0.636]
</p><p>88 The proposed model achieves accurate geometry and motion boundaries by refining an initial oversegmentation of the scene, and allows for occlusion reasoning. [sent-474, score-0.449]
</p><p>89 We show that our method substantially outperforms previous dense scene flow approaches on a challenging data set, and even surpasses dedicated state-of-the-art stereo and optical flow techniques at their respective task. [sent-475, score-1.699]
</p><p>90 Its main limitation are scenes with strongly non-rigid motion or extreme curvature, where the piecewise planar and rigid approximation does not hold. [sent-476, score-0.539]
</p><p>91 Determining three-dimensional motion and structure from optical flow generated by several moving objects. [sent-482, score-0.98]
</p><p>92 Multi-view scene flow estimation: A view centered variational approach. [sent-497, score-0.707]
</p><p>93 High accuracy optical flow estimation based on a theory for warping. [sent-513, score-0.694]
</p><p>94 A variational method for scene flow estimation from stereo sequences. [sent-542, score-1.002]
</p><p>95 Dense, robust, and accurate motion field estimation from stereo image sequences in real-time. [sent-561, score-0.576]
</p><p>96 Pushing the limits of stereo using variational stereo estimation. [sent-568, score-0.635]
</p><p>97 Joint motion estimation and segmentation of complex scenes with label costs and occlusion modeling. [sent-590, score-0.452]
</p><p>98 Differences between stereo and motion behaviour on synthetic and realworld stereo sequences. [sent-606, score-0.798]
</p><p>99 3D scene flow estimation with a rigid motion prior. [sent-626, score-1.001]
</p><p>100 Efficient dense scene flow from sparse or dense  [25] [26]  [27] [28]  stereo data. [sent-640, score-0.993]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('flow', 0.469), ('stereo', 0.296), ('motion', 0.206), ('segment', 0.198), ('optical', 0.185), ('kitti', 0.161), ('proposal', 0.159), ('scene', 0.154), ('rigidly', 0.148), ('occlusion', 0.147), ('planes', 0.137), ('rigid', 0.132), ('moving', 0.12), ('piecewise', 0.1), ('segments', 0.1), ('disparity', 0.097), ('reference', 0.089), ('pixel', 0.088), ('endpoints', 0.086), ('superpixelization', 0.084), ('rabe', 0.084), ('plane', 0.083), ('xp', 0.082), ('penalty', 0.082), ('regularization', 0.08), ('census', 0.073), ('opi', 0.071), ('rig', 0.07), ('planar', 0.069), ('pixels', 0.065), ('energy', 0.064), ('epipolar', 0.062), ('xq', 0.061), ('proposals', 0.059), ('segmentation', 0.059), ('parameterizing', 0.058), ('depth', 0.056), ('boundaries', 0.055), ('vogel', 0.055), ('regularizer', 0.055), ('coincide', 0.054), ('superpixels', 0.054), ('reasoning', 0.053), ('dedicated', 0.051), ('wedel', 0.05), ('exceeding', 0.05), ('assigned', 0.048), ('encourage', 0.047), ('boundary', 0.047), ('huguet', 0.047), ('prspix', 0.047), ('uip', 0.047), ('vedula', 0.047), ('yamaguchi', 0.047), ('ns', 0.046), ('seed', 0.044), ('unoccluded', 0.043), ('term', 0.043), ('variational', 0.043), ('endpoint', 0.042), ('noc', 0.042), ('qpbo', 0.042), ('valgaerts', 0.042), ('basha', 0.042), ('vaudrey', 0.042), ('view', 0.041), ('geometry', 0.041), ('thresholds', 0.041), ('assignment', 0.041), ('possibly', 0.04), ('estimation', 0.04), ('smooth', 0.04), ('deviations', 0.04), ('views', 0.04), ('penalties', 0.04), ('penalize', 0.039), ('dnm', 0.039), ('occludes', 0.039), ('nir', 0.039), ('superpixel', 0.038), ('respective', 0.038), ('cope', 0.037), ('dense', 0.037), ('unger', 0.037), ('assigns', 0.036), ('areas', 0.036), ('nq', 0.035), ('aggressive', 0.035), ('occluded', 0.035), ('field', 0.034), ('bleyer', 0.033), ('brightness', 0.032), ('bruhn', 0.032), ('lempitsky', 0.032), ('strongly', 0.032), ('incur', 0.031), ('boolean', 0.031), ('intrinsics', 0.031), ('np', 0.031), ('normal', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="317-tfidf-1" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>Author: Christoph Vogel, Konrad Schindler, Stefan Roth</p><p>Abstract: Estimating dense 3D scene flow from stereo sequences remains a challenging task, despite much progress in both classical disparity and 2D optical flow estimation. To overcome the limitations of existing techniques, we introduce a novel model that represents the dynamic 3D scene by a collection of planar, rigidly moving, local segments. Scene flow estimation then amounts to jointly estimating the pixelto-segment assignment, and the 3D position, normal vector, and rigid motion parameters of a plane for each segment. The proposed energy combines an occlusion-sensitive data term with appropriate shape, motion, and segmentation regularizers. Optimization proceeds in two stages: Starting from an initial superpixelization, we estimate the shape and motion parameters of all segments by assigning a proposal from a set of moving planes. Then the pixel-to-segment assignment is updated, while holding the shape and motion parameters of the moving planes fixed. We demonstrate the benefits of our model on different real-world image sets, including the challenging KITTI benchmark. We achieve leading performance levels, exceeding competing 3D scene flow methods, and even yielding better 2D motion estimates than all tested dedicated optical flow techniques.</p><p>2 0.36964157 <a title="317-tfidf-2" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<p>Author: Jim Braux-Zin, Romain Dupont, Adrien Bartoli</p><p>Abstract: Dense motion field estimation (typically Romain Dupont1 romain . dupont @ cea . fr Adrien Bartoli2 adrien . bart o l @ gmai l com i . 2 ISIT, Universit e´ d’Auvergne/CNRS, France sions are explicitly modeled [32, 13]. Coarse-to-fine warping improves global convergence by making the assumption that optical flow, the motion of smaller structures is similar to the motion of stereo disparity and surface registration) is a key computer vision problem. Many solutions have been proposed to compute small or large displacements, narrow or wide baseline stereo disparity, but a unified methodology is still lacking. We here introduce a general framework that robustly combines direct and feature-based matching. The feature-based cost is built around a novel robust distance function that handles keypoints and “weak” features such as segments. It allows us to use putative feature matches which may contain mismatches to guide dense motion estimation out of local minima. Our framework uses a robust direct data term (AD-Census). It is implemented with a powerful second order Total Generalized Variation regularization with external and self-occlusion reasoning. Our framework achieves state of the art performance in several cases (standard optical flow benchmarks, wide-baseline stereo and non-rigid surface registration). Our framework has a modular design that customizes to specific application needs.</p><p>3 0.3398594 <a title="317-tfidf-3" href="./iccv-2013-DeepFlow%3A_Large_Displacement_Optical_Flow_with_Deep_Matching.html">105 iccv-2013-DeepFlow: Large Displacement Optical Flow with Deep Matching</a></p>
<p>Author: Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, Cordelia Schmid</p><p>Abstract: Optical flow computation is a key component in many computer vision systems designed for tasks such as action detection or activity recognition. However, despite several major advances over the last decade, handling large displacement in optical flow remains an open problem. Inspired by the large displacement optical flow of Brox & Malik [6], our approach, termed DeepFlow, blends a matching algorithm with a variational approach for optical flow. We propose a descriptor matching algorithm, tailored to the optical flow problem, that allows to boost performance on fast motions. The matching algorithm builds upon a multi-stage architecture with 6 layers, interleaving convolutions and max-pooling, a construction akin to deep convolutional nets. Using dense sampling, it allows to efficiently retrieve quasi-dense correspondences, and enjoys a built-in smoothing effect on descriptors matches, a valuable assetfor integration into an energy minimizationframework for optical flow estimation. DeepFlow efficiently handles large displacements occurring in realistic videos, and shows competitive performance on optical flow benchmarks. Furthermore, it sets a new state-of-the-art on the MPI-Sintel dataset [8].</p><p>4 0.30311975 <a title="317-tfidf-4" href="./iccv-2013-Optical_Flow_via_Locally_Adaptive_Fusion_of_Complementary_Data_Costs.html">300 iccv-2013-Optical Flow via Locally Adaptive Fusion of Complementary Data Costs</a></p>
<p>Author: Tae Hyun Kim, Hee Seok Lee, Kyoung Mu Lee</p><p>Abstract: Many state-of-the-art optical flow estimation algorithms optimize the data and regularization terms to solve ill-posed problems. In this paper, in contrast to the conventional optical flow framework that uses a single or fixed data model, we study a novel framework that employs locally varying data term that adaptively combines different multiple types of data models. The locally adaptive data term greatly reduces the matching ambiguity due to the complementary nature of the multiple data models. The optimal number of complementary data models is learnt by minimizing the redundancy among them under the minimum description length constraint (MDL). From these chosen data models, a new optical flow estimation energy model is designed with the weighted sum of the multiple data models, and a convex optimization-based highly effective and practical solution thatfinds the opticalflow, as well as the weights isproposed. Comparative experimental results on the Middlebury optical flow benchmark show that the proposed method using the complementary data models outperforms the state-ofthe art methods.</p><p>5 0.28223455 <a title="317-tfidf-5" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>Author: Manjunath Narayana, Allen Hanson, Erik Learned-Miller</p><p>Abstract: In moving camera videos, motion segmentation is commonly performed using the image plane motion of pixels, or optical flow. However, objects that are at different depths from the camera can exhibit different optical flows even if they share the same real-world motion. This can cause a depth-dependent segmentation of the scene. Our goal is to develop a segmentation algorithm that clusters pixels that have similar real-world motion irrespective of their depth in the scene. Our solution uses optical flow orientations instead of the complete vectors and exploits the well-known property that under camera translation, optical flow orientations are independent of object depth. We introduce a probabilistic model that automatically estimates the number of observed independent motions and results in a labeling that is consistent with real-world motion in the scene. The result of our system is that static objects are correctly identified as one segment, even if they are at different depths. Color features and information from previous frames in the video sequence are used to correct occasional errors due to the orientation-based segmentation. We present results on more than thirty videos from different benchmarks. The system is particularly robust on complex background scenes containing objects at significantly different depths.</p><p>6 0.26807988 <a title="317-tfidf-6" href="./iccv-2013-Locally_Affine_Sparse-to-Dense_Matching_for_Motion_and_Occlusion_Estimation.html">256 iccv-2013-Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation</a></p>
<p>7 0.2613121 <a title="317-tfidf-7" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>8 0.226467 <a title="317-tfidf-8" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>9 0.21583751 <a title="317-tfidf-9" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>10 0.21400218 <a title="317-tfidf-10" href="./iccv-2013-PM-Huber%3A_PatchMatch_with_Huber_Regularization_for_Stereo_Matching.html">304 iccv-2013-PM-Huber: PatchMatch with Huber Regularization for Stereo Matching</a></p>
<p>11 0.20247003 <a title="317-tfidf-11" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>12 0.19421275 <a title="317-tfidf-12" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>13 0.18864723 <a title="317-tfidf-13" href="./iccv-2013-Measuring_Flow_Complexity_in_Videos.html">263 iccv-2013-Measuring Flow Complexity in Videos</a></p>
<p>14 0.18175651 <a title="317-tfidf-14" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>15 0.16768508 <a title="317-tfidf-15" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>16 0.16698483 <a title="317-tfidf-16" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>17 0.16048056 <a title="317-tfidf-17" href="./iccv-2013-Topology-Constrained_Layered_Tracking_with_Latent_Flow.html">420 iccv-2013-Topology-Constrained Layered Tracking with Latent Flow</a></p>
<p>18 0.1505637 <a title="317-tfidf-18" href="./iccv-2013-Internet_Based_Morphable_Model.html">219 iccv-2013-Internet Based Morphable Model</a></p>
<p>19 0.14998682 <a title="317-tfidf-19" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>20 0.14336273 <a title="317-tfidf-20" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.307), (1, -0.284), (2, 0.016), (3, 0.185), (4, 0.045), (5, 0.051), (6, -0.065), (7, -0.015), (8, 0.083), (9, 0.005), (10, -0.022), (11, 0.117), (12, 0.296), (13, -0.013), (14, -0.052), (15, -0.104), (16, -0.178), (17, 0.006), (18, 0.179), (19, 0.085), (20, 0.133), (21, -0.02), (22, -0.011), (23, -0.068), (24, 0.127), (25, -0.129), (26, 0.081), (27, -0.056), (28, -0.024), (29, 0.001), (30, -0.088), (31, -0.007), (32, -0.112), (33, -0.069), (34, -0.076), (35, 0.046), (36, -0.001), (37, 0.03), (38, -0.037), (39, -0.084), (40, -0.007), (41, -0.06), (42, -0.021), (43, 0.005), (44, 0.014), (45, 0.007), (46, -0.078), (47, -0.028), (48, 0.004), (49, -0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98150539 <a title="317-lsi-1" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>Author: Christoph Vogel, Konrad Schindler, Stefan Roth</p><p>Abstract: Estimating dense 3D scene flow from stereo sequences remains a challenging task, despite much progress in both classical disparity and 2D optical flow estimation. To overcome the limitations of existing techniques, we introduce a novel model that represents the dynamic 3D scene by a collection of planar, rigidly moving, local segments. Scene flow estimation then amounts to jointly estimating the pixelto-segment assignment, and the 3D position, normal vector, and rigid motion parameters of a plane for each segment. The proposed energy combines an occlusion-sensitive data term with appropriate shape, motion, and segmentation regularizers. Optimization proceeds in two stages: Starting from an initial superpixelization, we estimate the shape and motion parameters of all segments by assigning a proposal from a set of moving planes. Then the pixel-to-segment assignment is updated, while holding the shape and motion parameters of the moving planes fixed. We demonstrate the benefits of our model on different real-world image sets, including the challenging KITTI benchmark. We achieve leading performance levels, exceeding competing 3D scene flow methods, and even yielding better 2D motion estimates than all tested dedicated optical flow techniques.</p><p>2 0.86062008 <a title="317-lsi-2" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<p>Author: Jim Braux-Zin, Romain Dupont, Adrien Bartoli</p><p>Abstract: Dense motion field estimation (typically Romain Dupont1 romain . dupont @ cea . fr Adrien Bartoli2 adrien . bart o l @ gmai l com i . 2 ISIT, Universit e´ d’Auvergne/CNRS, France sions are explicitly modeled [32, 13]. Coarse-to-fine warping improves global convergence by making the assumption that optical flow, the motion of smaller structures is similar to the motion of stereo disparity and surface registration) is a key computer vision problem. Many solutions have been proposed to compute small or large displacements, narrow or wide baseline stereo disparity, but a unified methodology is still lacking. We here introduce a general framework that robustly combines direct and feature-based matching. The feature-based cost is built around a novel robust distance function that handles keypoints and “weak” features such as segments. It allows us to use putative feature matches which may contain mismatches to guide dense motion estimation out of local minima. Our framework uses a robust direct data term (AD-Census). It is implemented with a powerful second order Total Generalized Variation regularization with external and self-occlusion reasoning. Our framework achieves state of the art performance in several cases (standard optical flow benchmarks, wide-baseline stereo and non-rigid surface registration). Our framework has a modular design that customizes to specific application needs.</p><p>3 0.81828761 <a title="317-lsi-3" href="./iccv-2013-Optical_Flow_via_Locally_Adaptive_Fusion_of_Complementary_Data_Costs.html">300 iccv-2013-Optical Flow via Locally Adaptive Fusion of Complementary Data Costs</a></p>
<p>Author: Tae Hyun Kim, Hee Seok Lee, Kyoung Mu Lee</p><p>Abstract: Many state-of-the-art optical flow estimation algorithms optimize the data and regularization terms to solve ill-posed problems. In this paper, in contrast to the conventional optical flow framework that uses a single or fixed data model, we study a novel framework that employs locally varying data term that adaptively combines different multiple types of data models. The locally adaptive data term greatly reduces the matching ambiguity due to the complementary nature of the multiple data models. The optimal number of complementary data models is learnt by minimizing the redundancy among them under the minimum description length constraint (MDL). From these chosen data models, a new optical flow estimation energy model is designed with the weighted sum of the multiple data models, and a convex optimization-based highly effective and practical solution thatfinds the opticalflow, as well as the weights isproposed. Comparative experimental results on the Middlebury optical flow benchmark show that the proposed method using the complementary data models outperforms the state-ofthe art methods.</p><p>4 0.79551148 <a title="317-lsi-4" href="./iccv-2013-DeepFlow%3A_Large_Displacement_Optical_Flow_with_Deep_Matching.html">105 iccv-2013-DeepFlow: Large Displacement Optical Flow with Deep Matching</a></p>
<p>Author: Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, Cordelia Schmid</p><p>Abstract: Optical flow computation is a key component in many computer vision systems designed for tasks such as action detection or activity recognition. However, despite several major advances over the last decade, handling large displacement in optical flow remains an open problem. Inspired by the large displacement optical flow of Brox & Malik [6], our approach, termed DeepFlow, blends a matching algorithm with a variational approach for optical flow. We propose a descriptor matching algorithm, tailored to the optical flow problem, that allows to boost performance on fast motions. The matching algorithm builds upon a multi-stage architecture with 6 layers, interleaving convolutions and max-pooling, a construction akin to deep convolutional nets. Using dense sampling, it allows to efficiently retrieve quasi-dense correspondences, and enjoys a built-in smoothing effect on descriptors matches, a valuable assetfor integration into an energy minimizationframework for optical flow estimation. DeepFlow efficiently handles large displacements occurring in realistic videos, and shows competitive performance on optical flow benchmarks. Furthermore, it sets a new state-of-the-art on the MPI-Sintel dataset [8].</p><p>5 0.77767086 <a title="317-lsi-5" href="./iccv-2013-Locally_Affine_Sparse-to-Dense_Matching_for_Motion_and_Occlusion_Estimation.html">256 iccv-2013-Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation</a></p>
<p>Author: Marius Leordeanu, Andrei Zanfir, Cristian Sminchisescu</p><p>Abstract: Estimating a dense correspondence field between successive video frames, under large displacement, is important in many visual learning and recognition tasks. We propose a novel sparse-to-dense matching method for motion field estimation and occlusion detection. As an alternative to the current coarse-to-fine approaches from the optical flow literature, we start from the higher level of sparse matching with rich appearance and geometric constraints collected over extended neighborhoods, using an occlusion aware, locally affine model. Then, we move towards the simpler, but denser classic flow field model, with an interpolation procedure that offers a natural transition between the sparse and the dense correspondence fields. We experimentally demonstrate that our appearance features and our complex geometric constraintspermit the correct motion estimation even in difficult cases of large displacements and significant appearance changes. We also propose a novel classification method for occlusion detection that works in conjunction with the sparse-to-dense matching model. We validate our approach on the newly released Sintel dataset and obtain state-of-the-art results.</p><p>6 0.73273897 <a title="317-lsi-6" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>7 0.69790661 <a title="317-lsi-7" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>8 0.66383415 <a title="317-lsi-8" href="./iccv-2013-Two-Point_Gait%3A_Decoupling_Gait_from_Body_Shape.html">430 iccv-2013-Two-Point Gait: Decoupling Gait from Body Shape</a></p>
<p>9 0.64973813 <a title="317-lsi-9" href="./iccv-2013-Topology-Constrained_Layered_Tracking_with_Latent_Flow.html">420 iccv-2013-Topology-Constrained Layered Tracking with Latent Flow</a></p>
<p>10 0.62976509 <a title="317-lsi-10" href="./iccv-2013-Measuring_Flow_Complexity_in_Videos.html">263 iccv-2013-Measuring Flow Complexity in Videos</a></p>
<p>11 0.59720874 <a title="317-lsi-11" href="./iccv-2013-PM-Huber%3A_PatchMatch_with_Huber_Regularization_for_Stereo_Matching.html">304 iccv-2013-PM-Huber: PatchMatch with Huber Regularization for Stereo Matching</a></p>
<p>12 0.59615248 <a title="317-lsi-12" href="./iccv-2013-Optimal_Orthogonal_Basis_and_Image_Assimilation%3A_Motion_Modeling.html">301 iccv-2013-Optimal Orthogonal Basis and Image Assimilation: Motion Modeling</a></p>
<p>13 0.56114513 <a title="317-lsi-13" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>14 0.55988115 <a title="317-lsi-14" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>15 0.54826623 <a title="317-lsi-15" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>16 0.53508377 <a title="317-lsi-16" href="./iccv-2013-Modeling_Self-Occlusions_in_Dynamic_Shape_and_Appearance_Tracking.html">270 iccv-2013-Modeling Self-Occlusions in Dynamic Shape and Appearance Tracking</a></p>
<p>17 0.52940857 <a title="317-lsi-17" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>18 0.52639085 <a title="317-lsi-18" href="./iccv-2013-Estimating_the_Material_Properties_of_Fabric_from_Video.html">145 iccv-2013-Estimating the Material Properties of Fabric from Video</a></p>
<p>19 0.52159166 <a title="317-lsi-19" href="./iccv-2013-Motion-Aware_KNN_Laplacian_for_Video_Matting.html">275 iccv-2013-Motion-Aware KNN Laplacian for Video Matting</a></p>
<p>20 0.50863266 <a title="317-lsi-20" href="./iccv-2013-Fix_Structured_Learning_of_2013_ICCV_paper_k2opt.pdf.html">171 iccv-2013-Fix Structured Learning of 2013 ICCV paper k2opt.pdf</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.074), (7, 0.012), (12, 0.012), (18, 0.113), (26, 0.096), (31, 0.044), (42, 0.099), (48, 0.013), (64, 0.075), (73, 0.062), (78, 0.013), (89, 0.253), (95, 0.013), (98, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96097761 <a title="317-lda-1" href="./iccv-2013-Conservation_Tracking.html">87 iccv-2013-Conservation Tracking</a></p>
<p>Author: Martin Schiegg, Philipp Hanslovsky, Bernhard X. Kausler, Lars Hufnagel, Fred A. Hamprecht</p><p>Abstract: The quality of any tracking-by-assignment hinges on the accuracy of the foregoing target detection / segmentation step. In many kinds of images, errors in this first stage are unavoidable. These errors then propagate to, and corrupt, the tracking result. Our main contribution is the first probabilistic graphical model that can explicitly account for over- and undersegmentation errors even when the number of tracking targets is unknown and when they may divide, as in cell cultures. The tracking model we present implements global consistency constraints for the number of targets comprised by each detection and is solved to global optimality on reasonably large 2D+t and 3D+t datasets. In addition, we empirically demonstrate the effectiveness of a postprocessing that allows to establish target identity even across occlusion / undersegmentation. The usefulness and efficiency of this new tracking method is demonstrated on three different and challenging 2D+t and 3D+t datasets from developmental biology.</p><p>same-paper 2 0.93570691 <a title="317-lda-2" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>Author: Christoph Vogel, Konrad Schindler, Stefan Roth</p><p>Abstract: Estimating dense 3D scene flow from stereo sequences remains a challenging task, despite much progress in both classical disparity and 2D optical flow estimation. To overcome the limitations of existing techniques, we introduce a novel model that represents the dynamic 3D scene by a collection of planar, rigidly moving, local segments. Scene flow estimation then amounts to jointly estimating the pixelto-segment assignment, and the 3D position, normal vector, and rigid motion parameters of a plane for each segment. The proposed energy combines an occlusion-sensitive data term with appropriate shape, motion, and segmentation regularizers. Optimization proceeds in two stages: Starting from an initial superpixelization, we estimate the shape and motion parameters of all segments by assigning a proposal from a set of moving planes. Then the pixel-to-segment assignment is updated, while holding the shape and motion parameters of the moving planes fixed. We demonstrate the benefits of our model on different real-world image sets, including the challenging KITTI benchmark. We achieve leading performance levels, exceeding competing 3D scene flow methods, and even yielding better 2D motion estimates than all tested dedicated optical flow techniques.</p><p>3 0.93380624 <a title="317-lda-3" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>Author: Jimei Yang, Yi-Hsuan Tsai, Ming-Hsuan Yang</p><p>Abstract: We present a hybrid parametric and nonparametric algorithm, exemplar cut, for generating class-specific object segmentation hypotheses. For the parametric part, we train a pylon model on a hierarchical region tree as the energy function for segmentation. For the nonparametric part, we match the input image with each exemplar by using regions to obtain a score which augments the energy function from the pylon model. Our method thus generates a set of highly plausible segmentation hypotheses by solving a series of exemplar augmented graph cuts. Experimental results on the Graz and PASCAL datasets show that the proposed algorithm achievesfavorable segmentationperformance against the state-of-the-art methods in terms of visual quality and accuracy.</p><p>4 0.92791927 <a title="317-lda-4" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>Author: Yuandong Tian, Srinivasa G. Narasimhan</p><p>Abstract: Real-world surfaces such as clothing, water and human body deform in complex ways. The image distortions observed are high-dimensional and non-linear, making it hard to estimate these deformations accurately. The recent datadriven descent approach [17] applies Nearest Neighbor estimators iteratively on a particular distribution of training samples to obtain a globally optimal and dense deformation field between a template and a distorted image. In this work, we develop a hierarchical structure for the Nearest Neighbor estimators, each of which can have only a local image support. We demonstrate in both theory and practice that this algorithm has several advantages over the nonhierarchical version: it guarantees global optimality with significantly fewer training samples, is several orders faster, provides a metric to decide whether a given image is “hard” (or “easy ”) requiring more (or less) samples, and can handle more complex scenes that include both global motion and local deformation. The proposed algorithm successfully tracks a broad range of non-rigid scenes including water, clothing, and medical images, and compares favorably against several other deformation estimation and tracking approaches that do not provide optimality guarantees.</p><p>5 0.92786646 <a title="317-lda-5" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>Author: Diego Thomas, Akihiro Sugimoto</p><p>Abstract: Updating a global 3D model with live RGB-D measurements has proven to be successful for 3D reconstruction of indoor scenes. Recently, a Truncated Signed Distance Function (TSDF) volumetric model and a fusion algorithm have been introduced (KinectFusion), showing significant advantages such as computational speed and accuracy of the reconstructed scene. This algorithm, however, is expensive in memory when constructing and updating the global model. As a consequence, the method is not well scalable to large scenes. We propose a new flexible 3D scene representation using a set of planes that is cheap in memory use and, nevertheless, achieves accurate reconstruction of indoor scenes from RGB-D image sequences. Projecting the scene onto different planes reduces significantly the size of the scene representation and thus it allows us to generate a global textured 3D model with lower memory requirement while keeping accuracy and easiness to update with live RGB-D measurements. Experimental results demonstrate that our proposed flexible 3D scene representation achieves accurate reconstruction, while keeping the scalability for large indoor scenes.</p><p>6 0.92444223 <a title="317-lda-6" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>7 0.92430472 <a title="317-lda-7" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>8 0.92427039 <a title="317-lda-8" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>9 0.92341602 <a title="317-lda-9" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>10 0.92330855 <a title="317-lda-10" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>11 0.92319232 <a title="317-lda-11" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>12 0.9231286 <a title="317-lda-12" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>13 0.92311859 <a title="317-lda-13" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>14 0.92266548 <a title="317-lda-14" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>15 0.92234075 <a title="317-lda-15" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>16 0.92208946 <a title="317-lda-16" href="./iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain.html">351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</a></p>
<p>17 0.92199111 <a title="317-lda-17" href="./iccv-2013-Higher_Order_Matching_for_Consistent_Multiple_Target_Tracking.html">200 iccv-2013-Higher Order Matching for Consistent Multiple Target Tracking</a></p>
<p>18 0.92179078 <a title="317-lda-18" href="./iccv-2013-Topology-Constrained_Layered_Tracking_with_Latent_Flow.html">420 iccv-2013-Topology-Constrained Layered Tracking with Latent Flow</a></p>
<p>19 0.92121881 <a title="317-lda-19" href="./iccv-2013-GrabCut_in_One_Cut.html">186 iccv-2013-GrabCut in One Cut</a></p>
<p>20 0.92109942 <a title="317-lda-20" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
