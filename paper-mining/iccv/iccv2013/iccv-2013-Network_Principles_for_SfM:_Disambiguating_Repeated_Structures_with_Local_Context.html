<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>289 iccv-2013-Network Principles for SfM: Disambiguating Repeated Structures with Local Context</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-289" href="#">iccv2013-289</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>289 iccv-2013-Network Principles for SfM: Disambiguating Repeated Structures with Local Context</h1>
<br/><p>Source: <a title="iccv-2013-289-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Wilson_Network_Principles_for_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Kyle Wilson, Noah Snavely</p><p>Abstract: Repeated features are common in urban scenes. Many objects, such as clock towers with nearly identical sides, or domes with strong radial symmetries, pose challenges for structure from motion. When similar but distinct features are mistakenly equated, the resulting 3D reconstructions can have errors ranging from phantom walls and superimposed structures to a complete failure to reconstruct. We present a new approach to solving such problems by considering the local visibility structure of such repeated features. Drawing upon network theory, we present a new way of scoring features using a measure of local clustering. Our model leads to a simple, fast, and highly scalable technique for disambiguating repeated features based on an analysis of an underlying visibility graph, without relying on explicit geometric reasoning. We demonstrate our method on several very large datasets drawn from Internet photo collections, and compare it to a more traditional geometry-based disambiguation technique.</p><p>Reference: <a title="iccv-2013-289-reference" href="../iccv2013_reference/iccv-2013-Network_Principles_for_SfM%3A_Disambiguating_Repeated_Structures_with_Local_Context_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Many objects, such as clock towers with nearly identical sides, or domes with strong radial symmetries, pose challenges for structure from motion. [sent-3, score-0.155]
</p><p>2 When similar but distinct features are mistakenly equated, the resulting 3D reconstructions can have errors ranging from phantom walls and superimposed structures to a complete failure to reconstruct. [sent-4, score-0.249]
</p><p>3 We present a new approach to solving such problems by considering the local visibility structure of such repeated features. [sent-5, score-0.355]
</p><p>4 Drawing upon network theory, we present a new way of scoring features using a measure of local clustering. [sent-6, score-0.107]
</p><p>5 Our model leads to a simple, fast, and highly scalable technique for disambiguating repeated features based on an analysis of an underlying visibility graph, without relying on explicit geometric reasoning. [sent-7, score-0.521]
</p><p>6 We demonstrate our method on several very large datasets drawn from Internet photo collections, and compare it to a more traditional geometry-based disambiguation technique. [sent-8, score-0.275]
</p><p>7 While great strides have been made in automatic SfM methods, one continuing challenge is to handle scenes containing distinct objects that look similar. [sent-11, score-0.218]
</p><p>8 For example, consider a symmetric four-sided tower with a similar appearance from each side (e. [sent-12, score-0.196]
</p><p>9 Incorrectly inferring correspondence between, say, the north and south faces can cause major problems for SfM methods. [sent-15, score-0.136]
</p><p>10 For instance, two or more distinct faces may be “glued” together, causing other objects in the surrounding scene to be incorrectly superimposed on top of each other in the reconstruction. [sent-16, score-0.2]
</p><p>11 In such cases, the resulting geometry can be hopelessly ensnarled, with no easy way to disentangle it. [sent-17, score-0.062]
</p><p>12 These problems commonly occur when local feature correspondence methods incorrectly associate distinct 3D points. [sent-18, score-0.148]
</p><p>13 Prominent errors in the reconstruction, including repeated and phantom structures, are highlighted. [sent-21, score-0.143]
</p><p>14 (b) The same model, correctly disambiguated using our proposed method. [sent-22, score-0.045]
</p><p>15 The radially symmetric main dome and four nearly-identical surrounding spires form a complicated ambiguity structure. [sent-25, score-0.216]
</p><p>16 As parts of the model get reconstructed in the wrong position around the dome, sparse “phantom” domes and towers ap-  pear, as highlighted in blue. [sent-26, score-0.124]
</p><p>17 Furthermore, the four spires themselves look similar on each side, and the scalloped north and south sides of the rear apse also look the same, causing multiple copies of several more key features to appear. [sent-27, score-0.442]
</p><p>18 The SfM disambiguation problem seeks to produce a correct reconstruction in the presence of similar looking objects [12, 13, 8, 4]. [sent-29, score-0.281]
</p><p>19 In scenes with relatively few occlusions, reasoning about unseen features can reveal differences between similar objects [4]. [sent-32, score-0.103]
</p><p>20 In this paper, we consider the problem of disambiguating large, unstructured Internet photo collections, which pose significant challenges. [sent-33, score-0.217]
</p><p>21 These collections rarely come in sequence and often capture scenes with a complicated occlusion structure. [sent-34, score-0.121]
</p><p>22 They also represent very uneven distributions of views—popular viewpoints will be heavily represented, while side streets may only be captured a few times. [sent-35, score-0.074]
</p><p>23 We address this problem through the intuition that incorrect feature correspondences result in anomalous structures in a visibility graph representing images and the features they see. [sent-37, score-0.472]
</p><p>24 We describe a model of an incorrect, ambiguous feature track, based on the local structure of a visibility graph around such a track. [sent-38, score-0.381]
</p><p>25 Based on this model, we propose a simple, local, measure of track “goodness” inspired by local clustering coefficients used in social networks analysis. [sent-39, score-0.219]
</p><p>26 This  measure is a graph-topological one distinct from geometric measures used in prior work, and is very efficient to compute. [sent-40, score-0.105]
</p><p>27 This new measure gives us a signal that we then use in scene disambiguation technique that is scalable and which works surprisingly well in practice. [sent-41, score-0.258]
</p><p>28 We demonstrate our technique on several large-scale Internet photo collections, and compare to an existing geometric method. [sent-42, score-0.094]
</p><p>29 In summary, our contributions are (1) a simple, efficient new measure of anomalous behavior in feature correspondence, and (2) a scalable method for disambiguating and reconstructing scenes based on this measure. [sent-43, score-0.262]
</p><p>30 Related Work Much of the recent work in scene disambiguation has focused on analysis of geometry. [sent-49, score-0.215]
</p><p>31 [12] use a Bayesian belief network based on positive belief for feature  Figure 2. [sent-51, score-0.153]
</p><p>32 Tracks A and C, in green and blue respectively, each correctly correspond to a single 3D point, while track B (in red) mistakenly refers to both the front and back faces of a bell tower (making it a bad track). [sent-53, score-0.769]
</p><p>33 matches, negative belief for missing features, and consistency of triplets of epipolar geometry (EG) constraints between images. [sent-54, score-0.105]
</p><p>34 In [13], reasoning over triplets of images [5] is expanded to looking for consistent EGs over larger loops. [sent-55, score-0.135]
</p><p>35 [8] find local reasoning insufficient, instead detecting bad EGs with global expectation maximization. [sent-58, score-0.393]
</p><p>36 [2] detect symmetries while reconstructing, exploiting them to improve reconstruction accuracy. [sent-64, score-0.079]
</p><p>37 In contrast, our method is designed for much larger, more unstructured collections of photos, such as those downloaded from the Internet. [sent-66, score-0.121]
</p><p>38 As with our method, many previous approaches are based on analysis of an underlying graph, including graphs encoding matches or EGs between images, performing explicit geometric reasoning over these graphs. [sent-68, score-0.135]
</p><p>39 Our work reasons about a different graph, a bipartite visibility graph encoding relations between cameras and points, and performs a purely topological, local analysis over this graph. [sent-69, score-0.55]
</p><p>40 Following [9], we refer to a set of matched features across several images— often found through transitive closure of pairwise feature matches—as a track. [sent-75, score-0.062]
</p><p>41 The set of tracks can be naturally described in terms of a bipartite graph, G = (I, T, E), where nodes are images I tracks T, and edges (i, t) ∈ E exist and when a feature in image iis a member of tra(cki, tt. [sent-76, score-0.997]
</p><p>42 Ideally, each track t ∈ T represents a single 3D point, in which case G has a tn ∈atu Tral interpretation: the visibility graph encodes which 3D points are visible in which images. [sent-79, score-0.54]
</p><p>43 However, in the presence of structural ambiguity, some of these tracks (which we call bad tracks) refer to more than one 3D point. [sent-80, score-0.712]
</p><p>44 A and C correctly correspond to a single 3D point, but B is a bad track comprised of points on both the north and south sides of a tower. [sent-82, score-0.691]
</p><p>45 While a single bad track may not break a subsequent SfM algorithm, an ambiguous scene can contain many such tracks that form geometrically consistent sets. [sent-83, score-0.901]
</p><p>46 We address the disambiguation problem by finding a new set of tracks that is as correct as possible; we realize this goal by attempting to identify and remove bad tracks. [sent-84, score-0.927]
</p><p>47 Our method has two main elements: (1) a model and corresponding score function for bad tracks based on a local  visibility analysis, and (2) an SfM procedure that uses this model and score to produce a disambiguated reconstruction. [sent-85, score-1.143]
</p><p>48 Modeling Bad Tracks using Visibility Our model for a bad track is based on background context. [sent-88, score-0.487]
</p><p>49 Whether or not images see the same background objects is useful information for detecting bad tracks. [sent-89, score-0.298]
</p><p>50 In previous work, such as [4], such reasoning is geometric. [sent-90, score-0.065]
</p><p>51 In our case, we consider the weaker condition of visibility, as encoded topologically in the visibility graph G. [sent-91, score-0.351]
</p><p>52 We reason that even if objects look the same, they will often have different backgrounds. [sent-92, score-0.049]
</p><p>53 Figure 3(a) shows a simplified geometric view of the Seville scene in Figure 2, depicting cameras and the tracks they observe. [sent-93, score-0.448]
</p><p>54 Here B represents a feature on a window of the bell tower. [sent-94, score-0.062]
</p><p>55 Because the tower is symmetric, the windows on the left and right sides of the tower look the same, and so there are two distinct 3D points represented by B. [sent-95, score-0.458]
</p><p>56 Tracks A and C represent neighbors of B from the left or right views—other points in the world that are seen with B. [sent-96, score-0.102]
</p><p>57 , they cannot both be seen in a single photograph). [sent-100, score-0.049]
</p><p>58 In Figure 3(b) we see the visibility graph for this scene. [sent-101, score-0.351]
</p><p>59 In network terms, if A and C aren’t seen together, then B is a bridge between them. [sent-102, score-0.173]
</p><p>60 This bridging property provides evidence that B is a bad track, as one would expect visibility to be, roughly speaking, “transitive” (e. [sent-103, score-0.566]
</p><p>61 , if A is seen with B, and B with C, it would  be surprising to never see A and C together). [sent-105, score-0.049]
</p><p>62 This suspicion is heightened in Figure 3 (c), a more complex visibility graph where additional context is present. [sent-106, score-0.422]
</p><p>63 , other tracks seen in images that see B) form two clusters, and B bridges these two clusters. [sent-109, score-0.463]
</p><p>64 This leads to an intuition about local neighborhoods of tracks: bad tracks are those that are most like a bridge between two or more clusters of other “context” tracks in the visibility graph. [sent-110, score-1.577]
</p><p>65 To make this intuition quantifiable, we turn to network theory, which provides a useful measurement: the bipartite local clustering coefficient. [sent-111, score-0.335]
</p><p>66 For a node v, this is defined as  lcc(v) =## tr 2i-apnagthless c ceennteterreedd a att v v,  (1)  where a 2-path is any choice of two distinct neighbors of v. [sent-114, score-0.124]
</p><p>67 The lcc score measures local transitivity: in social network parlance, it is the fraction of my pairs of friends (pairs of neighbors of v) who are themselves friends with each other. [sent-116, score-0.442]
</p><p>68 This can be computed per node; when a network divides roughly into clusters, this score will be low for vertices that bridge clusters. [sent-117, score-0.168]
</p><p>69 In our case, G is a bipartite graph, over which the lcc score is identically zero, because there are no triangles by definition. [sent-118, score-0.354]
</p><p>70 However, Opsahl [7] proposes a natural extension  of the lcc score to bipartite graphs, using four-paths instead of triangles. [sent-119, score-0.316]
</p><p>71 This is the bipartite local clustering coefficient:  blcc(t) =# cl#os 4e-dpa 4t-hpsa cthesnt ceernetder aetd t at t  (2)  This function is exactly the fraction of the time that local transitivity holds at track t. [sent-120, score-0.605]
</p><p>72 In the language of the visibility graph, it answers the question, “if I’m seen with neighbor tracks A and C, how often are they seen together, over all such neighbors? [sent-121, score-0.78]
</p><p>73 ” A typical 4-path rooted at B is shown in bold red in Figure 3(c). [sent-122, score-0.07]
</p><p>74 , the path is not closed), since tracks A and C are never both seen in the same image. [sent-125, score-0.463]
</p><p>75 As a measure of local transitivity, we argue that the blcc tends to identify bad tracks that look like track B in Figure 3. [sent-126, score-1.328]
</p><p>76 By modeling bad tracks in this way, we assume that the different parts of a bad track have context and are not covisible. [sent-127, score-1.239]
</p><p>77 Certainly scenes can be imagined where images tend to see all 3D points which comprise a bad track. [sent-128, score-0.365]
</p><p>78 For example, consider a sign with repeated letters and shapes; 5 15  Tracks  1  1  1  1  0. [sent-129, score-0.057]
</p><p>79 The grey square represents the tower, while the blue and green objects are 3D points seen as tracks  A and  C. [sent-135, score-0.463]
</p><p>80 A sample 4-path rooted at bad track B, for which local transitivity fails, is shown in bold red. [sent-138, score-0.71]
</p><p>81 In practice, these bad tracks are not the sort of structural ambiguity which breaks reconstructions. [sent-141, score-0.747]
</p><p>82 The many correct neighbors are enough context for the bad track to be rejected on epipolar constraints earlier in a SfM pipeline. [sent-142, score-0.61]
</p><p>83 However, we acknowledge that this score is not a universal solution to SfM disambiguation; it relies on the separated context often present in natural scenes, which we find to be a very useful cue in practice. [sent-143, score-0.084]
</p><p>84 Algorithm for Disambiguating a Model We now have a score (the blcc) that captures our intuition about what makes a track good or bad. [sent-145, score-0.292]
</p><p>85 However, in real visibility graphs, an additional issue arises. [sent-146, score-0.268]
</p><p>86 Looking more closely at a bad track tb that has n neighbor images in k clusters, blcc(tb) will be lowest when each cluster has kn images. [sent-147, score-0.543]
</p><p>87 However, if at our bad track some of these clusters are much larger than others, blcc(tb) will be inflated and thus less sensitive. [sent-148, score-0.534]
</p><p>88 Unfortunately, Internet photo collections often represent very non-uniform samplings of viewpoint, with a large disparity in the number of images that capture different parts of a scene. [sent-149, score-0.172]
</p><p>89 As a result, on realworld datasets the blcc computed on the full visibility graph G can be skewed depending on local density. [sent-152, score-0.729]
</p><p>90 We instead compute the blcc on a subgraph of G that spans G but is more uniform. [sent-153, score-0.454]
</p><p>91 Our SfM disambiguation procedure thus  has two main steps: (1) compute a more uniform subgraph, then (2) remove bad tracks from that subgraph based on an analysis of blcc scores. [sent-154, score-1.381]
</p><p>92 To address the issue of uneven sampling of views in Internet datasets, we propose to find a subgraph G? [sent-156, score-0.152]
</p><p>93 s⊂hou Tld have the following properties: Uniformity: Tracks should be seen similarly often. [sent-164, score-0.049]
</p><p>94 Reconstructability: Most tracks should be seen often enough to be reconstructed. [sent-165, score-0.463]
</p><p>95 should be made up of images with wide field of view, since these often see more context than telephoto pictures. [sent-167, score-0.071]
</p><p>96 such that each track is covered a certain minimum number of times by images in I? [sent-169, score-0.189]
</p><p>97 We begin by restricting the subset of tracks we consider to long tracks (i. [sent-171, score-0.828]
</p><p>98 , tracks visible in many images), based on the intuition that long tracks are the most important for connecting up the graph, and because the blcc score is less stable for tracks with few neighbors. [sent-173, score-1.693]
</p><p>99 To formalize, let δG (t) denote the degree of a track node t in G. [sent-174, score-0.189]
</p><p>100 We define long tracks as the set TL = {t ∈ T : δG(t) ≥ Nlong}. [sent-175, score-0.414]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tracks', 0.414), ('blcc', 0.348), ('bad', 0.298), ('visibility', 0.268), ('sfm', 0.227), ('disambiguation', 0.215), ('track', 0.189), ('bipartite', 0.169), ('tower', 0.135), ('transitivity', 0.123), ('disambiguating', 0.119), ('egs', 0.114), ('subgraph', 0.106), ('coeur', 0.104), ('sacre', 0.104), ('seville', 0.104), ('lcc', 0.103), ('internet', 0.089), ('phantom', 0.086), ('collections', 0.083), ('graph', 0.083), ('network', 0.077), ('south', 0.074), ('distinct', 0.071), ('domes', 0.07), ('spires', 0.07), ('sides', 0.068), ('reasoning', 0.065), ('tl', 0.065), ('north', 0.062), ('bell', 0.062), ('basilica', 0.062), ('anomalous', 0.062), ('transitive', 0.062), ('photo', 0.06), ('intuition', 0.059), ('repeated', 0.057), ('tb', 0.056), ('towers', 0.054), ('neighbors', 0.053), ('friends', 0.051), ('seen', 0.049), ('mistakenly', 0.049), ('look', 0.049), ('incorrectly', 0.047), ('bridge', 0.047), ('dome', 0.047), ('clusters', 0.047), ('symmetries', 0.046), ('uneven', 0.046), ('disambiguated', 0.045), ('score', 0.044), ('superimposed', 0.043), ('scalable', 0.043), ('rooted', 0.042), ('context', 0.04), ('causing', 0.039), ('triangles', 0.038), ('unstructured', 0.038), ('belief', 0.038), ('scenes', 0.038), ('triplets', 0.037), ('front', 0.036), ('graphs', 0.036), ('ambiguity', 0.035), ('geometric', 0.034), ('looking', 0.033), ('fraction', 0.033), ('symmetric', 0.033), ('reconstruction', 0.033), ('suspicion', 0.031), ('apse', 0.031), ('kyle', 0.031), ('aren', 0.031), ('hopelessly', 0.031), ('telephoto', 0.031), ('continuing', 0.031), ('clock', 0.031), ('aetd', 0.031), ('radially', 0.031), ('disentangle', 0.031), ('nost', 0.031), ('parlance', 0.031), ('tral', 0.031), ('local', 0.03), ('covering', 0.03), ('epipolar', 0.03), ('paris', 0.029), ('pear', 0.029), ('imagined', 0.029), ('phrased', 0.029), ('uniformity', 0.029), ('strides', 0.029), ('corne', 0.029), ('samplings', 0.029), ('noah', 0.029), ('quantifiable', 0.029), ('wilson', 0.029), ('bold', 0.028), ('side', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="289-tfidf-1" href="./iccv-2013-Network_Principles_for_SfM%3A_Disambiguating_Repeated_Structures_with_Local_Context.html">289 iccv-2013-Network Principles for SfM: Disambiguating Repeated Structures with Local Context</a></p>
<p>Author: Kyle Wilson, Noah Snavely</p><p>Abstract: Repeated features are common in urban scenes. Many objects, such as clock towers with nearly identical sides, or domes with strong radial symmetries, pose challenges for structure from motion. When similar but distinct features are mistakenly equated, the resulting 3D reconstructions can have errors ranging from phantom walls and superimposed structures to a complete failure to reconstruct. We present a new approach to solving such problems by considering the local visibility structure of such repeated features. Drawing upon network theory, we present a new way of scoring features using a measure of local clustering. Our model leads to a simple, fast, and highly scalable technique for disambiguating repeated features based on an analysis of an underlying visibility graph, without relying on explicit geometric reasoning. We demonstrate our method on several very large datasets drawn from Internet photo collections, and compare it to a more traditional geometry-based disambiguation technique.</p><p>2 0.17076124 <a title="289-tfidf-2" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>Author: Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, James M. Rehg</p><p>Abstract: We propose an unsupervised video segmentation approach by simultaneously tracking multiple holistic figureground segments. Segment tracks are initialized from a pool of segment proposals generated from a figure-ground segmentation algorithm. Then, online non-local appearance models are trained incrementally for each track using a multi-output regularized least squares formulation. By using the same set of training examples for all segment tracks, a computational trick allows us to track hundreds of segment tracks efficiently, as well as perform optimal online updates in closed-form. Besides, a new composite statistical inference approach is proposed for refining the obtained segment tracks, which breaks down the initial segment proposals and recombines for better ones by utilizing highorder statistic estimates from the appearance model and enforcing temporal consistency. For evaluating the algorithm, a dataset, SegTrack v2, is collected with about 1,000 frames with pixel-level annotations. The proposed framework outperforms state-of-the-art approaches in the dataset, show- ing its efficiency and robustness to challenges in different video sequences.</p><p>3 0.13584669 <a title="289-tfidf-3" href="./iccv-2013-Video_Motion_for_Every_Visible_Point.html">441 iccv-2013-Video Motion for Every Visible Point</a></p>
<p>Author: Susanna Ricco, Carlo Tomasi</p><p>Abstract: Dense motion of image points over many video frames can provide important information about the world. However, occlusions and drift make it impossible to compute long motionpaths by merely concatenating opticalflow vectors between consecutive frames. Instead, we solve for entire paths directly, and flag the frames in which each is visible. As in previous work, we anchor each path to a unique pixel which guarantees an even spatial distribution of paths. Unlike earlier methods, we allow paths to be anchored in any frame. By explicitly requiring that at least one visible path passes within a small neighborhood of every pixel, we guarantee complete coverage of all visible points in all frames. We achieve state-of-the-art results on real sequences including both rigid and non-rigid motions with significant occlusions.</p><p>4 0.13077052 <a title="289-tfidf-4" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<p>Author: Ernesto Brau, Jinyan Guan, Kyle Simek, Luca Del Pero, Colin Reimer Dawson, Kobus Barnard</p><p>Abstract: Jinyan Guan† j guan1 @ emai l ari z ona . edu . Kyle Simek† ks imek@ emai l ari z ona . edu . Colin Reimer Dawson‡ cdaws on@ emai l ari z ona . edu . ‡School of Information University of Arizona Kobus Barnard‡ kobus @ s i sta . ari z ona . edu ∗School of Informatics University of Edinburgh for tracking an unknown and changing number of people in a scene using video taken from a single, fixed viewpoint. We develop a Bayesian modeling approach for tracking people in 3D from monocular video with unknown cameras. Modeling in 3D provides natural explanations for occlusions and smoothness discontinuities that result from projection, and allows priors on velocity and smoothness to be grounded in physical quantities: meters and seconds vs. pixels and frames. We pose the problem in the context of data association, in which observations are assigned to tracks. A correct application of Bayesian inference to multitarget tracking must address the fact that the model’s dimension changes as tracks are added or removed, and thus, posterior densities of different hypotheses are not comparable. We address this by marginalizing out the trajectory parameters so the resulting posterior over data associations has constant dimension. This is made tractable by using (a) Gaussian process priors for smooth trajectories and (b) approximately Gaussian likelihood functions. Our approach provides a principled method for incorporating multiple sources of evidence; we present results using both optical flow and object detector outputs. Results are comparable to recent work on 3D tracking and, unlike others, our method requires no pre-calibrated cameras.</p><p>5 0.12468198 <a title="289-tfidf-5" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>Author: Aleksandr V. Segal, Ian Reid</p><p>Abstract: We propose a novel parametrization of the data association problem for multi-target tracking. In our formulation, the number of targets is implicitly inferred together with the data association, effectively solving data association and model selection as a single inference problem. The novel formulation allows us to interpret data association and tracking as a single Switching Linear Dynamical System (SLDS). We compute an approximate posterior solution to this problem using a dynamic programming/message passing technique. This inference-based approach allows us to incorporate richer probabilistic models into the tracking system. In particular, we incorporate inference over inliers/outliers and track termination times into the system. We evaluate our approach on publicly available datasets and demonstrate results competitive with, and in some cases exceeding the state of the art.</p><p>6 0.12281433 <a title="289-tfidf-6" href="./iccv-2013-Finding_Actors_and_Actions_in_Movies.html">166 iccv-2013-Finding Actors and Actions in Movies</a></p>
<p>7 0.10401605 <a title="289-tfidf-7" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>8 0.10069276 <a title="289-tfidf-8" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>9 0.10008679 <a title="289-tfidf-9" href="./iccv-2013-NYC3DCars%3A_A_Dataset_of_3D_Vehicles_in_Geographic_Context.html">286 iccv-2013-NYC3DCars: A Dataset of 3D Vehicles in Geographic Context</a></p>
<p>10 0.098901831 <a title="289-tfidf-10" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>11 0.087713048 <a title="289-tfidf-11" href="./iccv-2013-A_Global_Linear_Method_for_Camera_Pose_Registration.html">17 iccv-2013-A Global Linear Method for Camera Pose Registration</a></p>
<p>12 0.085568279 <a title="289-tfidf-12" href="./iccv-2013-Global_Fusion_of_Relative_Motions_for_Robust%2C_Accurate_and_Scalable_Structure_from_Motion.html">184 iccv-2013-Global Fusion of Relative Motions for Robust, Accurate and Scalable Structure from Motion</a></p>
<p>13 0.08402063 <a title="289-tfidf-13" href="./iccv-2013-Pyramid_Coding_for_Functional_Scene_Element_Recognition_in_Video_Scenes.html">331 iccv-2013-Pyramid Coding for Functional Scene Element Recognition in Video Scenes</a></p>
<p>14 0.078700893 <a title="289-tfidf-14" href="./iccv-2013-Event_Recognition_in_Photo_Collections_with_a_Stopwatch_HMM.html">147 iccv-2013-Event Recognition in Photo Collections with a Stopwatch HMM</a></p>
<p>15 0.072634593 <a title="289-tfidf-15" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>16 0.067120649 <a title="289-tfidf-16" href="./iccv-2013-Joint_Deep_Learning_for_Pedestrian_Detection.html">220 iccv-2013-Joint Deep Learning for Pedestrian Detection</a></p>
<p>17 0.067062348 <a title="289-tfidf-17" href="./iccv-2013-Learning_Graph_Matching%3A_Oriented_to_Category_Modeling_from_Cluttered_Scenes.html">237 iccv-2013-Learning Graph Matching: Oriented to Category Modeling from Cluttered Scenes</a></p>
<p>18 0.064584665 <a title="289-tfidf-18" href="./iccv-2013-Improving_Graph_Matching_via_Density_Maximization.html">214 iccv-2013-Improving Graph Matching via Density Maximization</a></p>
<p>19 0.063789621 <a title="289-tfidf-19" href="./iccv-2013-An_Enhanced_Structure-from-Motion_Paradigm_Based_on_the_Absolute_Dual_Quadric_and_Images_of_Circular_Points.html">49 iccv-2013-An Enhanced Structure-from-Motion Paradigm Based on the Absolute Dual Quadric and Images of Circular Points</a></p>
<p>20 0.062879197 <a title="289-tfidf-20" href="./iccv-2013-A_Non-parametric_Bayesian_Network_Prior_of_Human_Pose.html">24 iccv-2013-A Non-parametric Bayesian Network Prior of Human Pose</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.145), (1, -0.049), (2, -0.004), (3, 0.029), (4, 0.041), (5, 0.015), (6, -0.003), (7, 0.004), (8, 0.011), (9, 0.014), (10, -0.014), (11, -0.017), (12, -0.016), (13, 0.061), (14, 0.011), (15, 0.059), (16, 0.035), (17, 0.031), (18, -0.01), (19, 0.022), (20, -0.07), (21, -0.095), (22, 0.018), (23, -0.037), (24, 0.04), (25, 0.0), (26, -0.048), (27, -0.093), (28, -0.069), (29, 0.044), (30, 0.023), (31, 0.002), (32, 0.076), (33, -0.092), (34, -0.08), (35, 0.154), (36, 0.187), (37, 0.021), (38, 0.057), (39, -0.037), (40, -0.096), (41, 0.02), (42, 0.051), (43, 0.02), (44, -0.096), (45, -0.135), (46, 0.023), (47, 0.02), (48, -0.063), (49, 0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94169402 <a title="289-lsi-1" href="./iccv-2013-Network_Principles_for_SfM%3A_Disambiguating_Repeated_Structures_with_Local_Context.html">289 iccv-2013-Network Principles for SfM: Disambiguating Repeated Structures with Local Context</a></p>
<p>Author: Kyle Wilson, Noah Snavely</p><p>Abstract: Repeated features are common in urban scenes. Many objects, such as clock towers with nearly identical sides, or domes with strong radial symmetries, pose challenges for structure from motion. When similar but distinct features are mistakenly equated, the resulting 3D reconstructions can have errors ranging from phantom walls and superimposed structures to a complete failure to reconstruct. We present a new approach to solving such problems by considering the local visibility structure of such repeated features. Drawing upon network theory, we present a new way of scoring features using a measure of local clustering. Our model leads to a simple, fast, and highly scalable technique for disambiguating repeated features based on an analysis of an underlying visibility graph, without relying on explicit geometric reasoning. We demonstrate our method on several very large datasets drawn from Internet photo collections, and compare it to a more traditional geometry-based disambiguation technique.</p><p>2 0.53474069 <a title="289-lsi-2" href="./iccv-2013-Video_Motion_for_Every_Visible_Point.html">441 iccv-2013-Video Motion for Every Visible Point</a></p>
<p>Author: Susanna Ricco, Carlo Tomasi</p><p>Abstract: Dense motion of image points over many video frames can provide important information about the world. However, occlusions and drift make it impossible to compute long motionpaths by merely concatenating opticalflow vectors between consecutive frames. Instead, we solve for entire paths directly, and flag the frames in which each is visible. As in previous work, we anchor each path to a unique pixel which guarantees an even spatial distribution of paths. Unlike earlier methods, we allow paths to be anchored in any frame. By explicitly requiring that at least one visible path passes within a small neighborhood of every pixel, we guarantee complete coverage of all visible points in all frames. We achieve state-of-the-art results on real sequences including both rigid and non-rigid motions with significant occlusions.</p><p>3 0.53009957 <a title="289-lsi-3" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<p>Author: Ernesto Brau, Jinyan Guan, Kyle Simek, Luca Del Pero, Colin Reimer Dawson, Kobus Barnard</p><p>Abstract: Jinyan Guan† j guan1 @ emai l ari z ona . edu . Kyle Simek† ks imek@ emai l ari z ona . edu . Colin Reimer Dawson‡ cdaws on@ emai l ari z ona . edu . ‡School of Information University of Arizona Kobus Barnard‡ kobus @ s i sta . ari z ona . edu ∗School of Informatics University of Edinburgh for tracking an unknown and changing number of people in a scene using video taken from a single, fixed viewpoint. We develop a Bayesian modeling approach for tracking people in 3D from monocular video with unknown cameras. Modeling in 3D provides natural explanations for occlusions and smoothness discontinuities that result from projection, and allows priors on velocity and smoothness to be grounded in physical quantities: meters and seconds vs. pixels and frames. We pose the problem in the context of data association, in which observations are assigned to tracks. A correct application of Bayesian inference to multitarget tracking must address the fact that the model’s dimension changes as tracks are added or removed, and thus, posterior densities of different hypotheses are not comparable. We address this by marginalizing out the trajectory parameters so the resulting posterior over data associations has constant dimension. This is made tractable by using (a) Gaussian process priors for smooth trajectories and (b) approximately Gaussian likelihood functions. Our approach provides a principled method for incorporating multiple sources of evidence; we present results using both optical flow and object detector outputs. Results are comparable to recent work on 3D tracking and, unlike others, our method requires no pre-calibrated cameras.</p><p>4 0.52037275 <a title="289-lsi-4" href="./iccv-2013-NYC3DCars%3A_A_Dataset_of_3D_Vehicles_in_Geographic_Context.html">286 iccv-2013-NYC3DCars: A Dataset of 3D Vehicles in Geographic Context</a></p>
<p>Author: Kevin Matzen, Noah Snavely</p><p>Abstract: Geometry and geography can play an important role in recognition tasks in computer vision. To aid in studying connections between geometry and recognition, we introduce NYC3DCars, a rich dataset for vehicle detection in urban scenes built from Internet photos drawn from the wild, focused on densely trafficked areas of New York City. Our dataset is augmented with detailed geometric and geographic information, including full camera poses derived from structure from motion, 3D vehicle annotations, and geographic information from open resources, including road segmentations and directions of travel. NYC3DCars can be used to study new questions about using geometric information in detection tasks, and to explore applications of Internet photos in understanding cities. To demonstrate the utility of our data, we evaluate the use of the geographic information in our dataset to enhance a parts-based detection method, and suggest other avenues for future exploration.</p><p>5 0.49237761 <a title="289-lsi-5" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>Author: Aleksandr V. Segal, Ian Reid</p><p>Abstract: We propose a novel parametrization of the data association problem for multi-target tracking. In our formulation, the number of targets is implicitly inferred together with the data association, effectively solving data association and model selection as a single inference problem. The novel formulation allows us to interpret data association and tracking as a single Switching Linear Dynamical System (SLDS). We compute an approximate posterior solution to this problem using a dynamic programming/message passing technique. This inference-based approach allows us to incorporate richer probabilistic models into the tracking system. In particular, we incorporate inference over inliers/outliers and track termination times into the system. We evaluate our approach on publicly available datasets and demonstrate results competitive with, and in some cases exceeding the state of the art.</p><p>6 0.48426512 <a title="289-lsi-6" href="./iccv-2013-Simultaneous_Clustering_and_Tracklet_Linking_for_Multi-face_Tracking_in_Videos.html">393 iccv-2013-Simultaneous Clustering and Tracklet Linking for Multi-face Tracking in Videos</a></p>
<p>7 0.47819173 <a title="289-lsi-7" href="./iccv-2013-Global_Fusion_of_Relative_Motions_for_Robust%2C_Accurate_and_Scalable_Structure_from_Motion.html">184 iccv-2013-Global Fusion of Relative Motions for Robust, Accurate and Scalable Structure from Motion</a></p>
<p>8 0.47651374 <a title="289-lsi-8" href="./iccv-2013-Conservation_Tracking.html">87 iccv-2013-Conservation Tracking</a></p>
<p>9 0.47151592 <a title="289-lsi-9" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>10 0.45765534 <a title="289-lsi-10" href="./iccv-2013-A_Global_Linear_Method_for_Camera_Pose_Registration.html">17 iccv-2013-A Global Linear Method for Camera Pose Registration</a></p>
<p>11 0.45752123 <a title="289-lsi-11" href="./iccv-2013-Discovering_Details_and_Scene_Structure_with_Hierarchical_Iconoid_Shift.html">117 iccv-2013-Discovering Details and Scene Structure with Hierarchical Iconoid Shift</a></p>
<p>12 0.44864759 <a title="289-lsi-12" href="./iccv-2013-Efficient_and_Robust_Large-Scale_Rotation_Averaging.html">138 iccv-2013-Efficient and Robust Large-Scale Rotation Averaging</a></p>
<p>13 0.44114509 <a title="289-lsi-13" href="./iccv-2013-Pyramid_Coding_for_Functional_Scene_Element_Recognition_in_Video_Scenes.html">331 iccv-2013-Pyramid Coding for Functional Scene Element Recognition in Video Scenes</a></p>
<p>14 0.44043636 <a title="289-lsi-14" href="./iccv-2013-Discriminative_Label_Propagation_for_Multi-object_Tracking_with_Sporadic_Appearance_Features.html">120 iccv-2013-Discriminative Label Propagation for Multi-object Tracking with Sporadic Appearance Features</a></p>
<p>15 0.4240922 <a title="289-lsi-15" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>16 0.42002055 <a title="289-lsi-16" href="./iccv-2013-An_Enhanced_Structure-from-Motion_Paradigm_Based_on_the_Absolute_Dual_Quadric_and_Images_of_Circular_Points.html">49 iccv-2013-An Enhanced Structure-from-Motion Paradigm Based on the Absolute Dual Quadric and Images of Circular Points</a></p>
<p>17 0.41292378 <a title="289-lsi-17" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>18 0.40402889 <a title="289-lsi-18" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>19 0.40059754 <a title="289-lsi-19" href="./iccv-2013-Higher_Order_Matching_for_Consistent_Multiple_Target_Tracking.html">200 iccv-2013-Higher Order Matching for Consistent Multiple Target Tracking</a></p>
<p>20 0.39875489 <a title="289-lsi-20" href="./iccv-2013-The_Way_They_Move%3A_Tracking_Multiple_Targets_with_Similar_Appearance.html">418 iccv-2013-The Way They Move: Tracking Multiple Targets with Similar Appearance</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.103), (7, 0.013), (12, 0.015), (23, 0.27), (26, 0.062), (31, 0.031), (34, 0.013), (42, 0.094), (64, 0.093), (73, 0.019), (89, 0.183)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78638804 <a title="289-lda-1" href="./iccv-2013-Network_Principles_for_SfM%3A_Disambiguating_Repeated_Structures_with_Local_Context.html">289 iccv-2013-Network Principles for SfM: Disambiguating Repeated Structures with Local Context</a></p>
<p>Author: Kyle Wilson, Noah Snavely</p><p>Abstract: Repeated features are common in urban scenes. Many objects, such as clock towers with nearly identical sides, or domes with strong radial symmetries, pose challenges for structure from motion. When similar but distinct features are mistakenly equated, the resulting 3D reconstructions can have errors ranging from phantom walls and superimposed structures to a complete failure to reconstruct. We present a new approach to solving such problems by considering the local visibility structure of such repeated features. Drawing upon network theory, we present a new way of scoring features using a measure of local clustering. Our model leads to a simple, fast, and highly scalable technique for disambiguating repeated features based on an analysis of an underlying visibility graph, without relying on explicit geometric reasoning. We demonstrate our method on several very large datasets drawn from Internet photo collections, and compare it to a more traditional geometry-based disambiguation technique.</p><p>2 0.77467084 <a title="289-lda-2" href="./iccv-2013-SGTD%3A_Structure_Gradient_and_Texture_Decorrelating_Regularization_for_Image_Decomposition.html">364 iccv-2013-SGTD: Structure Gradient and Texture Decorrelating Regularization for Image Decomposition</a></p>
<p>Author: Qiegen Liu, Jianbo Liu, Pei Dong, Dong Liang</p><p>Abstract: This paper presents a novel structure gradient and texture decorrelating regularization (SGTD) for image decomposition. The motivation of the idea is under the assumption that the structure gradient and texture components should be properly decorrelated for a successful decomposition. The proposed model consists of the data fidelity term, total variation regularization and the SGTD regularization. An augmented Lagrangian method is proposed to address this optimization issue, by first transforming the unconstrained problem to an equivalent constrained problem and then applying an alternating direction method to iteratively solve the subproblems. Experimental results demonstrate that the proposed method presents better or comparable performance as state-of-the-art methods do.</p><p>3 0.73075813 <a title="289-lda-3" href="./iccv-2013-Pictorial_Human_Spaces%3A_How_Well_Do_Humans_Perceive_a_3D_Articulated_Pose%3F.html">316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</a></p>
<p>Author: Elisabeta Marinoiu, Dragos Papava, Cristian Sminchisescu</p><p>Abstract: Human motion analysis in images and video is a central computer vision problem. Yet, there are no studies that reveal how humans perceive other people in images and how accurate they are. In this paper we aim to unveil some of the processing–as well as the levels of accuracy–involved in the 3D perception of people from images by assessing the human performance. Our contributions are: (1) the construction of an experimental apparatus that relates perception and measurement, in particular the visual and kinematic performance with respect to 3D ground truth when the human subject is presented an image of a person in a given pose; (2) the creation of a dataset containing images, articulated 2D and 3D pose ground truth, as well as synchronized eye movement recordings of human subjects, shown a variety of human body configurations, both easy and difficult, as well as their ‘re-enacted’ 3D poses; (3) quantitative analysis revealing the human performance in 3D pose reenactment tasks, the degree of stability in the visual fixation patterns of human subjects, and the way it correlates with different poses. We also discuss the implications of our find- ings for the construction of visual human sensing systems.</p><p>4 0.71532905 <a title="289-lda-4" href="./iccv-2013-Low-Rank_Sparse_Coding_for_Image_Classification.html">258 iccv-2013-Low-Rank Sparse Coding for Image Classification</a></p>
<p>Author: Tianzhu Zhang, Bernard Ghanem, Si Liu, Changsheng Xu, Narendra Ahuja</p><p>Abstract: In this paper, we propose a low-rank sparse coding (LRSC) method that exploits local structure information among features in an image for the purpose of image-level classification. LRSC represents densely sampled SIFT descriptors, in a spatial neighborhood, collectively as lowrank, sparse linear combinations of codewords. As such, it casts the feature coding problem as a low-rank matrix learning problem, which is different from previous methods that encode features independently. This LRSC has a number of attractive properties. (1) It encourages sparsity in feature codes, locality in codebook construction, and low-rankness for spatial consistency. (2) LRSC encodes local features jointly by considering their low-rank structure information, and is computationally attractive. We evaluate the LRSC by comparing its performance on a set of challenging benchmarks with that of 7 popular coding and other state-of-theart methods. Our experiments show that by representing local features jointly, LRSC not only outperforms the state-ofthe-art in classification accuracy but also improves the time complexity of methods that use a similar sparse linear repre- sentation model for feature coding [36].</p><p>5 0.69741035 <a title="289-lda-5" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>Author: Heng Yang, Ioannis Patras</p><p>Abstract: In this paper we propose a method for the localization of multiple facial features on challenging face images. In the regression forests (RF) framework, observations (patches) that are extracted at several image locations cast votes for the localization of several facial features. In order to filter out votes that are not relevant, we pass them through two types of sieves, that are organised in a cascade, and which enforce geometric constraints. The first sieve filters out votes that are not consistent with a hypothesis for the location of the face center. Several sieves of the second type, one associated with each individual facial point, filter out distant votes. We propose a method that adjusts onthe-fly the proximity threshold of each second type sieve by applying a classifier which, based on middle-level features extracted from voting maps for the facial feature in question, makes a sequence of decisions on whether the threshold should be reduced or not. We validate our proposed method on two challenging datasets with images collected from the Internet in which we obtain state of the art results without resorting to explicit facial shape models. We also show the benefits of our method for proximity threshold adjustment especially on ’difficult’ face images.</p><p>6 0.67920065 <a title="289-lda-6" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>7 0.67761374 <a title="289-lda-7" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>8 0.67564952 <a title="289-lda-8" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>9 0.67485785 <a title="289-lda-9" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>10 0.6728189 <a title="289-lda-10" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>11 0.67270458 <a title="289-lda-11" href="./iccv-2013-Linear_Sequence_Discriminant_Analysis%3A_A_Model-Based_Dimensionality_Reduction_Method_for_Vector_Sequences.html">253 iccv-2013-Linear Sequence Discriminant Analysis: A Model-Based Dimensionality Reduction Method for Vector Sequences</a></p>
<p>12 0.6726535 <a title="289-lda-12" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>13 0.67258179 <a title="289-lda-13" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>14 0.67155725 <a title="289-lda-14" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>15 0.67128301 <a title="289-lda-15" href="./iccv-2013-Simultaneous_Clustering_and_Tracklet_Linking_for_Multi-face_Tracking_in_Videos.html">393 iccv-2013-Simultaneous Clustering and Tracklet Linking for Multi-face Tracking in Videos</a></p>
<p>16 0.67004347 <a title="289-lda-16" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>17 0.66969359 <a title="289-lda-17" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>18 0.66955769 <a title="289-lda-18" href="./iccv-2013-Real-Time_Articulated_Hand_Pose_Estimation_Using_Semi-supervised_Transductive_Regression_Forests.html">340 iccv-2013-Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests</a></p>
<p>19 0.66882443 <a title="289-lda-19" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>20 0.66858208 <a title="289-lda-20" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
