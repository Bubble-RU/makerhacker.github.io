<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-417" href="#">iccv2013-417</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</h1>
<br/><p>Source: <a title="iccv-2013-417-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Zanfir_The_Moving_Pose_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Mihai Zanfir, Marius Leordeanu, Cristian Sminchisescu</p><p>Abstract: Human action recognition under low observational latency is receiving a growing interest in computer vision due to rapidly developing technologies in human-robot interaction, computer gaming and surveillance. In this paper we propose a fast, simple, yet powerful non-parametric Moving Pose (MP)frameworkfor low-latency human action and activity recognition. Central to our methodology is a moving pose descriptor that considers both pose information as well as differential quantities (speed and acceleration) of the human body joints within a short time window around the current frame. The proposed descriptor is used in conjunction with a modified kNN classifier that considers both the temporal location of a particular frame within the action sequence as well as the discrimination power of its moving pose descriptor compared to other frames in the training set. The resulting method is non-parametric and enables low-latency recognition, one-shot learning, and action detection in difficult unsegmented sequences. Moreover, the framework is real-time, scalable, and outperforms more sophisticated approaches on challenging benchmarks like MSR-Action3D or MSR-DailyActivities3D.</p><p>Reference: <a title="iccv-2013-417-reference" href="../iccv2013_reference/iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper we propose a fast, simple, yet powerful non-parametric Moving Pose (MP)frameworkfor low-latency human action and activity recognition. [sent-2, score-0.643]
</p><p>2 Central to our methodology is a moving pose descriptor that considers both pose information as well as differential quantities (speed and acceleration) of the human body joints within a short time window around the current frame. [sent-3, score-0.852]
</p><p>3 The proposed descriptor is used in conjunction with a modified kNN classifier that considers both the temporal location of a particular frame within the action sequence as well as the discrimination power of its moving pose descriptor compared to other frames in the training set. [sent-4, score-1.339]
</p><p>4 The resulting method is non-parametric and enables low-latency recognition, one-shot learning, and action detection in difficult unsegmented sequences. [sent-5, score-0.843]
</p><p>5 Introduction Automatic action and activity recognition are important computer vision research problems of broad practical applicability. [sent-8, score-0.612]
</p><p>6 The new technologies being developed in computer games using RGB-D cameras, or for environmental awareness, require flexible methodologies that are both accurate and have low observational latency, such that actions can be reliably recognized long before the entire observation stream is presented. [sent-9, score-0.512]
</p><p>7 The task of early action detection was addressed only recently but gains momentum [7, 9] due to the fast development of new technologies in human-robot interaction or surveillance. [sent-10, score-0.654]
</p><p>8 The accurate real-time tracking of 3D skeletons [21], made possible with the introduction of low-cost RGB-D cameras, led to the development of efficient methods for classification of dance moves [17] and other arbitrary actions [7, 24]. [sent-11, score-0.375]
</p><p>9 Most published methods, however, require an entire action sequence in order to perform classification. [sent-12, score-0.616]
</p><p>10 Only very few techniques offer lowlatency responses that would allow the rapid identification of an action long before it ends [7, 9], and not all methods are are able to cope with unsegmented test sequences. [sent-13, score-0.804]
</p><p>11 First, we propose the Moving Pose descriptor–a novel frame-based dynamic representation that captures not only  the 3D body pose but also differential properties like the speed and acceleration of the human body joints within a short time window around the current frame. [sent-15, score-0.805]
</p><p>12 We argue that due to physical constraints like inertia, or latency in muscle actuation, the body movements associated with an action can often be well approximated by a quadratic function, expressed in terms of the first and second derivatives of the body pose with respect to time. [sent-16, score-1.29]
</p><p>13 Our second contribution is a modified non-parametric kNN classifier that considers not only the global temporal location of a particular frame within the action sequence, but also the different discriminative power associated with moving pose descriptors computed in each frame. [sent-17, score-1.039]
</p><p>14 This makes our method capable to perform not only state of the art low-latency action recognition, but also accurate action detection in natural, unsegmented sequences captured under weakly controlled conditions. [sent-18, score-1.507]
</p><p>15 We also show that our method is well suited for the one-shot learning of actions, unlike any of the recent, early action recognition approaches. [sent-19, score-0.62]
</p><p>16 Related Work: Traditional research on general action recognition focuses mainly on recognition accuracy using hidden Markov models, and more recently conditional random fields [14, 22], and less on reducing observational latency [1, 10, 20]. [sent-20, score-1.094]
</p><p>17 Thus, the action has to be recognized as soon as enough evidence becomes available, ideally long before the whole  sequence is observed. [sent-22, score-0.583]
</p><p>18 2752  Many authors have observed that primitive actions can be well represented by a few key poses [5, 7, 19] for action classification. [sent-23, score-0.981]
</p><p>19 Variations on using key poses include matching shape information to prototypes [3] and using discriminative frames to weight features in a bag-of-words classifier [27]. [sent-24, score-0.325]
</p><p>20 Distinctive key poses from contours are learned by [4], while [11] matches silhouettes between the test frames and learned key frames. [sent-25, score-0.323]
</p><p>21 In this paper we first assume that actions are already segmented, then show that our approach can be extended to automatically process unsegmented sequences. [sent-30, score-0.563]
</p><p>22 Other techniques  [16] focus on reducing the computational latency of decoding hidden state sequences, rather than on the observational latency linked with classifying early partial events. [sent-32, score-0.863]
</p><p>23 Representing Human Actions and Activities Human actions are often considered to be primitive activities consisting of one or a few atomic body movements or poses such as: walking, sitting, standing, kicking or jumping. [sent-34, score-0.712]
</p><p>24 In this paper we focus on recognizing actions or activities that are well described by the 3D body pose and movement only. [sent-36, score-0.653]
</p><p>25 We represent an action as a sequence of frame descriptors ordered in time. [sent-38, score-0.678]
</p><p>26 Consequently, we need local frame descriptors that capture as much information as possible about the action in the neighborhood of a given frame, in terms of both pose and the kinematics of body joints. [sent-41, score-0.889]
</p><p>27 At a given moment in time, a certain pose together with specific movements ofthe 3D bodyjoints  could be highly predictive of the intentions of the human subject and the action performed. [sent-44, score-0.812]
</p><p>28 Despite the natural skeleton size and proportions variation in the human population, people tend to perform the same action in the same qualitative way, by moving similarly. [sent-46, score-0.777]
</p><p>29 Thus, for a given action, differential quantities like the relative speed and acceleration of these joints could be more stable across subjects than their actual 3D locations. [sent-47, score-0.47]
</p><p>30 It is clear that a good descriptor should capture both the static pose as well as the joint kinematics at a given moment in time. [sent-48, score-0.376]
</p><p>31 It is important to differentiate between actions spanning similar poses but different directions of movement, such as jumping vs. [sent-51, score-0.436]
</p><p>32 Changes in directions as well as in speed produce nonzero acceleration, which is useful to differentiate between actions involving circular motions such as drawing a circle or waving, versus drawing a line or standing up. [sent-55, score-0.49]
</p><p>33 (1)  Effectively, the instantaneous pose and its first and second order derivatives at a given time t0 contain information about the pose function over a time segment around the current t0. [sent-57, score-0.436]
</p><p>34 Therefore, this type of local information could be seen as an approximation of an action snippet [18] and can be used to reliably classify actions. [sent-59, score-0.551]
</p><p>35 In experiments we actually show that introducing kinematic information, encoded as proposed, induces a significant improvement over the use of poses-only, even when all poses needed to compute derivatives are used within local frame descriptors and fed to classifiers (Table 4). [sent-60, score-0.387]
</p><p>36 The Moving Pose Descriptor The skeleton representation of a pose in each frame of a video captures the 3D positions of the skeleton joints pi = (px ,py, pz), where i ∈ {1, . [sent-62, score-0.645]
</p><p>37 Note that Gaussian smoothing produces a lag of two frames which does not significantly impact the overall latency in practice. [sent-76, score-0.389]
</p><p>38 In order to suppress noise in the estimated input pose and to compensate for skeleton variations across different subjects, we normalize the poses as described in the following paragraph. [sent-77, score-0.422]
</p><p>39 The final frame descriptor Xt for frame at time t is obtained by concatenating the pose and its derivatives over time: Xt = [Pt, αδPt, βδ2Pt] . [sent-79, score-0.442]
</p><p>40 n]  Pose Normalization (Algorithm 1): Human subjects have variations in body and limb sizes which are not relevant for the action performed. [sent-94, score-0.718]
</p><p>41 We learn average skeleton segment lengths from training data (a segment is defined by any two linked joints). [sent-96, score-0.336]
</p><p>42 Action Classification The proposed MP descriptor encodes pose and kinematic information to describe action segments. [sent-112, score-0.82]
</p><p>43 In order to emphasize its discriminative power and for training flexibility (including one-shot learning) we use a non-parametric action classification scheme based on k-nearest-neighbors (kNN). [sent-113, score-0.671]
</p><p>44 Learning Discriminative Frames One of the main difficulties in analyzing action sequences is that not all frames are representative for an action. [sent-121, score-0.71]
</p><p>45 When performing certain tasks a person often has to go through poses that are common to many other actions or activities. [sent-122, score-0.403]
</p><p>46 Our simple solution to this problem is to classify, at training time, each frame in the training set using a kNN classifier (consider each training descriptor as an unknown one and use the others to classify it). [sent-125, score-0.35]
</p><p>47 The poses  with low confidence score were usually the common, shared ones, such as standing poses from the start and the end of each sequence. [sent-137, score-0.386]
</p><p>48 some of the most discriminative frames, automatically discovered by the algorithm, for a few action classes. [sent-138, score-0.553]
</p><p>49 Global Temporal Information for Classification A limitation of classical kNN classification applied to action recognition is its lack of account of global temporal ordering. [sent-142, score-0.691]
</p><p>50 The descriptor X captures only local temporal variations but does not include information that would allow one to identify when during the action sequence a particular short action segment takes place. [sent-143, score-1.309]
</p><p>51 By combining discriminative local moving pose descriptors like MP with a temporal aware classification scheme,  we can now account for two important aspects in action classification: the discriminative power of key poses as well as their local dynamics, and the global temporal course of an action. [sent-148, score-1.252]
</p><p>52 This turns out to be important for more complex activities that consist of many atomic action units. [sent-149, score-0.636]
</p><p>53 Experiments We test our method on both simple and complex actions, present in both segmented as well as unsegmented sequences. [sent-151, score-0.338]
</p><p>54 sb(cs)(b)  c∗  as our own unsegmented action dataset1 . [sent-162, score-0.804]
</p><p>55 We also demonstrate the effectiveness of the proposed moving pose framework and the usefulness of our confidence and global temporal constraints, over different baselines. [sent-164, score-0.357]
</p><p>56 Our results clearly show that differential quantities like local motion and acceleration are powerful cues for action recognition and in conjunction with static 3D poses they can capture the specific dynamics inherently linked to different types of actions, far beyond the static poses themselves. [sent-165, score-1.164]
</p><p>57 Our method is able to accurately recognize actions with low observational latency at real time rates of 200−500 FwPitSh (depending on ntahel ldataetancseyt)a on a desktop computer. [sent-166, score-0.741]
</p><p>58 Action Recognition The MSR Action3D dataset consists of temporally segmented action sequences captured by a RGB-D camera. [sent-169, score-0.737]
</p><p>59 There are 20 actions in the dataset, performed 2 3 times −  There  are  20  actions  1Publicly available  in  at www  . [sent-170, score-0.546]
</p><p>60 In the context of action detection (discussed later in 5. [sent-209, score-0.553]
</p><p>61 not immediately applicable for action detection in realistic unsegmented scenarios, where it would require no-action states. [sent-213, score-0.843]
</p><p>62 This dataset contains 1, 280 temporally segmented action sequences gathered from 16 different human subjects. [sent-218, score-0.783]
</p><p>63 It is interesting to notice that even though poses from a time window implicitly contain movement information, the explicit estimation of speed and acceleration makes a significant difference in the overall performance. [sent-257, score-0.42]
</p><p>64 This empirical result confirms the intuition that the actual body movement is highly predictive of the action class. [sent-258, score-0.654]
</p><p>65 For example on MSR-Action 3D dataset, adding the speed of the joints (δP) improves the accuracy by a substantial 25% margin over 3D pose (P) alone, whereas the acceleration (δ2P) adds another significant 6%. [sent-259, score-0.472]
</p><p>66 The acceleration component is very important for actions with curved movements, such as drawing a circle. [sent-260, score-0.451]
</p><p>67 A similar trend was observed on the actions dataset from [7], where the accuracy dropped from 98. [sent-262, score-0.358]
</p><p>68 Latency Analysis Our method is well suited for action recognition with low observational latency, since at any point in time we could output the action that accumulated the largest number of votes, at a given rejection threshold. [sent-299, score-1.337]
</p><p>69 The intuition is that the higher this value relative to the votes from the other classes, the more confident 2757  we are that our winning action is indeed correct. [sent-303, score-0.639]
</p><p>70 3 we show that by varying a threshold θ on the function output we can confidently classify actions long before the end of the sequence. [sent-305, score-0.342]
</p><p>71 One-shot Learning Another advantage of our non-parametric approach over  current methods is the ability to recognize actions using a very limited amount of training data. [sent-310, score-0.344]
</p><p>72 Since we use a nonparametric classification approach we can apply our method in the case of one-shot learning, when only a single training sequence for each action is available. [sent-311, score-0.673]
</p><p>73 We tested the MP approach in the one-shot learning scenario on both MSRActions3D and DailyActivities: in the previous experiments (Tables 1 and 3) there are about 13 training sequences per action and 10 per activity. [sent-312, score-0.665]
</p><p>74 For the results presented in this section (Table 5) we performed 100 different experiments where a single training sequence was chosen randomly for each action with decision performed based on such ‘early’ learners. [sent-313, score-0.628]
</p><p>75 [7] uses multiple instance learning of canonical action poses based on many training sequences. [sent-337, score-0.689]
</p><p>76 Neural networks [13], motion templates [15], HMMs [12] and action graphs [10] need sufficient data to learn models. [sent-338, score-0.514]
</p><p>77 Action Detection in Unsegmented Sequences We test the efficiency of our method on a significantly more difficult scenario–that of action recognition and detection in unsegmented sequences. [sent-341, score-0.885]
</p><p>78 Our method can be easily modified to handle low-latency action detection in unsegmented sequences. [sent-342, score-0.874]
</p><p>79 The action classification responses per frame are obtained as before. [sent-343, score-0.627]
</p><p>80 Since in the unsegmented case, we do not know the temporal length of the action performed, we take a sliding window approach and learn a window of size W by cross-validation. [sent-344, score-0.996]
</p><p>81 Given votes accumulated over this window, we estimate the probability of observing action class c at a given moment t as:  R(c,t) =? [sent-345, score-0.752]
</p><p>82 e classification response of action c computed exactly as in the unsegmented case. [sent-351, score-0.878]
</p><p>83 After computing responses for each action, the winning action produces the final frame label c∗ (t) = argmax(R(c, t)). [sent-352, score-0.628]
</p><p>84 We performed action detection, following precision-recall experimental protocols widely used in object detection from images, such as Pascal VOC Challenge [8] (our overlapping threshold is 0. [sent-357, score-0.553]
</p><p>85 Note that our method performs well even in the unsegmented case, when all test sequences are concatenated into a single input sequence. [sent-360, score-0.396]
</p><p>86 Unsegmented action detection performance Scoring method Detection (AP) 0. [sent-363, score-0.553]
</p><p>87 Each subject was in-  structed to freely perform 3 sequences of multiple actions, 7 different actions per set, chosen randomly from the pool of actions present in MSR-Actions3D. [sent-373, score-0.652]
</p><p>88 The subjects were asked to perform them in any order and number of times, with any intermediate actions in between the fixed ones. [sent-374, score-0.353]
</p><p>89 We present results 2758  (see Table 7) for both unsegmented action detection average precision as well as action classification for comparison (when the ground truth segmentations were available). [sent-377, score-1.402]
</p><p>90 The experiments indicate that our system is able to efficiently detect actions in difficult unsegmented sequences, in the presence of different subjects (than in training) and in uncontrolled environments. [sent-378, score-0.643]
</p><p>91 Conclusions We have presented  a novel moving pose descriptor  framework for action recognition which uses pose and kinematic information encoded as differential 3D quantities. [sent-386, score-1.129]
</p><p>92 We demonstrated the power of our methodology by obtaining state of the art results on recent, challenging benchmarks for action and activity recognition. [sent-388, score-0.642]
</p><p>93 Moreover, we show that our techniques are well suited for action detection in difficult unsegmented sequences and for low latency recognition, unlike most previous approaches. [sent-389, score-1.276]
</p><p>94 Human action recognition in videos using kinematic features and multiple instance learning. [sent-396, score-0.637]
</p><p>95 Exploring the trade-off between accuracy and observational latency in action recognition. [sent-437, score-1.01]
</p><p>96 Single view human action recognition using key pose matching and viterbi path searching. [sent-466, score-0.783]
</p><p>97 Recognition and segmentation of 3d human action using hmm and multi-class adaboost. [sent-471, score-0.56]
</p><p>98 Action snippets: How many frames does human action recognition require ? [sent-509, score-0.692]
</p><p>99 A discriminative key pose sequence model for recognizing human interactions. [sent-543, score-0.335]
</p><p>100 Mining actionlet ensemble for action recognition with depth cameras. [sent-550, score-0.622]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('action', 0.514), ('latency', 0.299), ('unsegmented', 0.29), ('actions', 0.273), ('observational', 0.169), ('mp', 0.161), ('skeleton', 0.148), ('acceleration', 0.145), ('pose', 0.144), ('knn', 0.137), ('poses', 0.13), ('joints', 0.109), ('sequences', 0.106), ('activities', 0.096), ('body', 0.092), ('frames', 0.09), ('temporal', 0.09), ('descriptor', 0.081), ('derivatives', 0.081), ('kinematic', 0.081), ('subjects', 0.08), ('votes', 0.079), ('ldcrf', 0.072), ('phip', 0.072), ('standing', 0.072), ('moving', 0.069), ('sequence', 0.069), ('msr', 0.068), ('frame', 0.068), ('movements', 0.068), ('actionlet', 0.066), ('hip', 0.064), ('activity', 0.056), ('differential', 0.054), ('confidence', 0.054), ('window', 0.051), ('ellis', 0.051), ('movement', 0.048), ('dailyactivities', 0.048), ('segmented', 0.048), ('human', 0.046), ('winning', 0.046), ('speed', 0.046), ('classification', 0.045), ('training', 0.045), ('accumulated', 0.044), ('kinematics', 0.044), ('art', 0.044), ('recognition', 0.042), ('segment', 0.041), ('limbs', 0.041), ('observing', 0.04), ('moment', 0.04), ('joint', 0.04), ('temporally', 0.039), ('discriminative', 0.039), ('detection', 0.039), ('classify', 0.037), ('iconic', 0.037), ('recurrent', 0.037), ('key', 0.037), ('technologies', 0.037), ('early', 0.036), ('quantities', 0.036), ('class', 0.035), ('differentiate', 0.033), ('entire', 0.033), ('drawing', 0.033), ('cj', 0.032), ('linked', 0.032), ('confidently', 0.032), ('limb', 0.032), ('xt', 0.031), ('modified', 0.031), ('velocity', 0.031), ('skeletons', 0.03), ('dataset', 0.03), ('response', 0.029), ('classifier', 0.029), ('silhouettes', 0.029), ('lengths', 0.029), ('normalization', 0.028), ('positions', 0.028), ('decoding', 0.028), ('accuracy', 0.028), ('interaction', 0.028), ('suited', 0.028), ('power', 0.028), ('primitive', 0.027), ('dance', 0.027), ('descriptors', 0.027), ('dropped', 0.027), ('powerful', 0.027), ('static', 0.027), ('atomic', 0.026), ('sitting', 0.026), ('involve', 0.026), ('rejection', 0.026), ('current', 0.026), ('derivative', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="417-tfidf-1" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>Author: Mihai Zanfir, Marius Leordeanu, Cristian Sminchisescu</p><p>Abstract: Human action recognition under low observational latency is receiving a growing interest in computer vision due to rapidly developing technologies in human-robot interaction, computer gaming and surveillance. In this paper we propose a fast, simple, yet powerful non-parametric Moving Pose (MP)frameworkfor low-latency human action and activity recognition. Central to our methodology is a moving pose descriptor that considers both pose information as well as differential quantities (speed and acceleration) of the human body joints within a short time window around the current frame. The proposed descriptor is used in conjunction with a modified kNN classifier that considers both the temporal location of a particular frame within the action sequence as well as the discrimination power of its moving pose descriptor compared to other frames in the training set. The resulting method is non-parametric and enables low-latency recognition, one-shot learning, and action detection in difficult unsegmented sequences. Moreover, the framework is real-time, scalable, and outperforms more sophisticated approaches on challenging benchmarks like MSR-Action3D or MSR-DailyActivities3D.</p><p>2 0.44316918 <a title="417-tfidf-2" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>Author: Ping Wei, Nanning Zheng, Yibiao Zhao, Song-Chun Zhu</p><p>Abstract: Action recognition has often been posed as a classification problem, which assumes that a video sequence only have one action class label and different actions are independent. However, a single human body can perform multiple concurrent actions at the same time, and different actions interact with each other. This paper proposes a concurrent action detection model where the action detection is formulated as a structural prediction problem. In this model, an interval in a video sequence can be described by multiple action labels. An detected action interval is determined both by the unary local detector and the relations with other actions. We use a wavelet feature to represent the action sequence, and design a composite temporal logic descriptor to describe the action relations. The model parameters are trained by structural SVM learning. Given a long video sequence, a sequential decision window search algorithm is designed to detect the actions. Experiments on our new collected concurrent action dataset demonstrate the strength of our method.</p><p>3 0.39433706 <a title="417-tfidf-3" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>Author: Jiang Wang, Ying Wu</p><p>Abstract: Temporal misalignment and duration variation in video actions largely influence the performance of action recognition, but it is very difficult to specify effective temporal alignment on action sequences. To address this challenge, this paper proposes a novel discriminative learning-based temporal alignment method, called maximum margin temporal warping (MMTW), to align two action sequences and measure their matching score. Based on the latent structure SVM formulation, the proposed MMTW method is able to learn a phantom action template to represent an action class for maximum discrimination against other classes. The recognition of this action class is based on the associated learned alignment of the input action. Extensive experiments on five benchmark datasets have demonstrated that this MMTW model is able to significantly promote the accuracy and robustness of action recognition under temporal misalignment and variations.</p><p>4 0.33310953 <a title="417-tfidf-4" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>Author: Sunil Bandla, Kristen Grauman</p><p>Abstract: Collecting and annotating videos of realistic human actions is tedious, yet critical for training action recognition systems. We propose a method to actively request the most useful video annotations among a large set of unlabeled videos. Predicting the utility of annotating unlabeled video is not trivial, since any given clip may contain multiple actions of interest, and it need not be trimmed to temporal regions of interest. To deal with this problem, we propose a detection-based active learner to train action category models. We develop a voting-based framework to localize likely intervals of interest in an unlabeled clip, and use them to estimate the total reduction in uncertainty that annotating that clip would yield. On three datasets, we show our approach can learn accurate action detectors more efficiently than alternative active learning strategies that fail to accommodate the “untrimmed” nature of real video data.</p><p>5 0.30644834 <a title="417-tfidf-5" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>Author: Shugao Ma, Jianming Zhang, Nazli Ikizler-Cinbis, Stan Sclaroff</p><p>Abstract: We propose Hierarchical Space-Time Segments as a new representation for action recognition and localization. This representation has a two-level hierarchy. The first level comprises the root space-time segments that may contain a human body. The second level comprises multi-grained space-time segments that contain parts of the root. We present an unsupervised method to generate this representation from video, which extracts both static and non-static relevant space-time segments, and also preserves their hierarchical and temporal relationships. Using simple linear SVM on the resultant bag of hierarchical space-time segments representation, we attain better than, or comparable to, state-of-the-art action recognition performance on two challenging benchmark datasets and at the same time produce good action localization results.</p><p>6 0.28902408 <a title="417-tfidf-6" href="./iccv-2013-Video_Event_Understanding_Using_Natural_Language_Descriptions.html">440 iccv-2013-Video Event Understanding Using Natural Language Descriptions</a></p>
<p>7 0.27484488 <a title="417-tfidf-7" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>8 0.26818511 <a title="417-tfidf-8" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>9 0.26712334 <a title="417-tfidf-9" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>10 0.25578159 <a title="417-tfidf-10" href="./iccv-2013-From_Actemes_to_Action%3A_A_Strongly-Supervised_Representation_for_Detailed_Action_Understanding.html">175 iccv-2013-From Actemes to Action: A Strongly-Supervised Representation for Detailed Action Understanding</a></p>
<p>11 0.24408884 <a title="417-tfidf-11" href="./iccv-2013-Learning_to_Share_Latent_Tasks_for_Action_Recognition.html">249 iccv-2013-Learning to Share Latent Tasks for Action Recognition</a></p>
<p>12 0.23760855 <a title="417-tfidf-12" href="./iccv-2013-Finding_Actors_and_Actions_in_Movies.html">166 iccv-2013-Finding Actors and Actions in Movies</a></p>
<p>13 0.23147744 <a title="417-tfidf-13" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>14 0.23147455 <a title="417-tfidf-14" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>15 0.20151368 <a title="417-tfidf-15" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>16 0.19725524 <a title="417-tfidf-16" href="./iccv-2013-Pictorial_Human_Spaces%3A_How_Well_Do_Humans_Perceive_a_3D_Articulated_Pose%3F.html">316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</a></p>
<p>17 0.19598435 <a title="417-tfidf-17" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>18 0.19082628 <a title="417-tfidf-18" href="./iccv-2013-Latent_Multitask_Learning_for_View-Invariant_Action_Recognition.html">231 iccv-2013-Latent Multitask Learning for View-Invariant Action Recognition</a></p>
<p>19 0.18580703 <a title="417-tfidf-19" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>20 0.16727665 <a title="417-tfidf-20" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.293), (1, 0.252), (2, 0.142), (3, 0.388), (4, 0.019), (5, -0.082), (6, 0.159), (7, -0.118), (8, -0.071), (9, 0.101), (10, 0.059), (11, 0.07), (12, -0.079), (13, -0.108), (14, 0.167), (15, 0.075), (16, -0.006), (17, -0.122), (18, -0.04), (19, -0.002), (20, 0.071), (21, 0.02), (22, -0.053), (23, -0.154), (24, -0.064), (25, -0.029), (26, -0.022), (27, 0.036), (28, -0.006), (29, 0.006), (30, 0.004), (31, 0.027), (32, 0.03), (33, 0.045), (34, 0.014), (35, -0.002), (36, 0.016), (37, -0.021), (38, 0.008), (39, -0.017), (40, -0.014), (41, 0.012), (42, -0.043), (43, 0.033), (44, -0.002), (45, -0.054), (46, 0.032), (47, -0.034), (48, 0.022), (49, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9788124 <a title="417-lsi-1" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>Author: Mihai Zanfir, Marius Leordeanu, Cristian Sminchisescu</p><p>Abstract: Human action recognition under low observational latency is receiving a growing interest in computer vision due to rapidly developing technologies in human-robot interaction, computer gaming and surveillance. In this paper we propose a fast, simple, yet powerful non-parametric Moving Pose (MP)frameworkfor low-latency human action and activity recognition. Central to our methodology is a moving pose descriptor that considers both pose information as well as differential quantities (speed and acceleration) of the human body joints within a short time window around the current frame. The proposed descriptor is used in conjunction with a modified kNN classifier that considers both the temporal location of a particular frame within the action sequence as well as the discrimination power of its moving pose descriptor compared to other frames in the training set. The resulting method is non-parametric and enables low-latency recognition, one-shot learning, and action detection in difficult unsegmented sequences. Moreover, the framework is real-time, scalable, and outperforms more sophisticated approaches on challenging benchmarks like MSR-Action3D or MSR-DailyActivities3D.</p><p>2 0.95856637 <a title="417-lsi-2" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>Author: Ping Wei, Nanning Zheng, Yibiao Zhao, Song-Chun Zhu</p><p>Abstract: Action recognition has often been posed as a classification problem, which assumes that a video sequence only have one action class label and different actions are independent. However, a single human body can perform multiple concurrent actions at the same time, and different actions interact with each other. This paper proposes a concurrent action detection model where the action detection is formulated as a structural prediction problem. In this model, an interval in a video sequence can be described by multiple action labels. An detected action interval is determined both by the unary local detector and the relations with other actions. We use a wavelet feature to represent the action sequence, and design a composite temporal logic descriptor to describe the action relations. The model parameters are trained by structural SVM learning. Given a long video sequence, a sequential decision window search algorithm is designed to detect the actions. Experiments on our new collected concurrent action dataset demonstrate the strength of our method.</p><p>3 0.93419707 <a title="417-lsi-3" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>Author: Jiang Wang, Ying Wu</p><p>Abstract: Temporal misalignment and duration variation in video actions largely influence the performance of action recognition, but it is very difficult to specify effective temporal alignment on action sequences. To address this challenge, this paper proposes a novel discriminative learning-based temporal alignment method, called maximum margin temporal warping (MMTW), to align two action sequences and measure their matching score. Based on the latent structure SVM formulation, the proposed MMTW method is able to learn a phantom action template to represent an action class for maximum discrimination against other classes. The recognition of this action class is based on the associated learned alignment of the input action. Extensive experiments on five benchmark datasets have demonstrated that this MMTW model is able to significantly promote the accuracy and robustness of action recognition under temporal misalignment and variations.</p><p>4 0.87630272 <a title="417-lsi-4" href="./iccv-2013-From_Actemes_to_Action%3A_A_Strongly-Supervised_Representation_for_Detailed_Action_Understanding.html">175 iccv-2013-From Actemes to Action: A Strongly-Supervised Representation for Detailed Action Understanding</a></p>
<p>Author: Weiyu Zhang, Menglong Zhu, Konstantinos G. Derpanis</p><p>Abstract: This paper presents a novel approach for analyzing human actions in non-scripted, unconstrained video settings based on volumetric, x-y-t, patch classifiers, termed actemes. Unlike previous action-related work, the discovery of patch classifiers is posed as a strongly-supervised process. Specifically, keypoint labels (e.g., position) across spacetime are used in a data-driven training process to discover patches that are highly clustered in the spacetime keypoint configuration space. To support this process, a new human action dataset consisting of challenging consumer videos is introduced, where notably the action label, the 2D position of a set of keypoints and their visibilities are provided for each video frame. On a novel input video, each acteme is used in a sliding volume scheme to yield a set of sparse, non-overlapping detections. These detections provide the intermediate substrate for segmenting out the action. For action classification, the proposed representation shows significant improvement over state-of-the-art low-level features, while providing spatiotemporal localiza- tion as additional output. This output sheds further light into detailed action understanding.</p><p>5 0.82064646 <a title="417-lsi-5" href="./iccv-2013-Latent_Multitask_Learning_for_View-Invariant_Action_Recognition.html">231 iccv-2013-Latent Multitask Learning for View-Invariant Action Recognition</a></p>
<p>Author: Behrooz Mahasseni, Sinisa Todorovic</p><p>Abstract: This paper presents an approach to view-invariant action recognition, where human poses and motions exhibit large variations across different camera viewpoints. When each viewpoint of a given set of action classes is specified as a learning task then multitask learning appears suitable for achieving view invariance in recognition. We extend the standard multitask learning to allow identifying: (1) latent groupings of action views (i.e., tasks), and (2) discriminative action parts, along with joint learning of all tasks. This is because it seems reasonable to expect that certain distinct views are more correlated than some others, and thus identifying correlated views could improve recognition. Also, part-based modeling is expected to improve robustness against self-occlusion when actors are imaged from different views. Results on the benchmark datasets show that we outperform standard multitask learning by 21.9%, and the state-of-the-art alternatives by 4.5–6%.</p><p>6 0.79586625 <a title="417-lsi-6" href="./iccv-2013-Action_Recognition_with_Actons.html">38 iccv-2013-Action Recognition with Actons</a></p>
<p>7 0.78749293 <a title="417-lsi-7" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>8 0.76872909 <a title="417-lsi-8" href="./iccv-2013-Video_Event_Understanding_Using_Natural_Language_Descriptions.html">440 iccv-2013-Video Event Understanding Using Natural Language Descriptions</a></p>
<p>9 0.72949719 <a title="417-lsi-9" href="./iccv-2013-Finding_Actors_and_Actions_in_Movies.html">166 iccv-2013-Finding Actors and Actions in Movies</a></p>
<p>10 0.71976143 <a title="417-lsi-10" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>11 0.70823073 <a title="417-lsi-11" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>12 0.69304883 <a title="417-lsi-12" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>13 0.66989827 <a title="417-lsi-13" href="./iccv-2013-Manipulation_Pattern_Discovery%3A_A_Nonparametric_Bayesian_Approach.html">260 iccv-2013-Manipulation Pattern Discovery: A Nonparametric Bayesian Approach</a></p>
<p>14 0.65760285 <a title="417-lsi-14" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>15 0.65305471 <a title="417-lsi-15" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>16 0.63462156 <a title="417-lsi-16" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>17 0.62396687 <a title="417-lsi-17" href="./iccv-2013-Learning_to_Share_Latent_Tasks_for_Action_Recognition.html">249 iccv-2013-Learning to Share Latent Tasks for Action Recognition</a></p>
<p>18 0.59366506 <a title="417-lsi-18" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>19 0.57724506 <a title="417-lsi-19" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>20 0.56166917 <a title="417-lsi-20" href="./iccv-2013-Capturing_Global_Semantic_Relationships_for_Facial_Action_Unit_Recognition.html">69 iccv-2013-Capturing Global Semantic Relationships for Facial Action Unit Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.058), (7, 0.013), (12, 0.22), (13, 0.015), (26, 0.076), (31, 0.045), (35, 0.013), (40, 0.016), (42, 0.108), (64, 0.132), (73, 0.023), (84, 0.011), (89, 0.158), (95, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9041996 <a title="417-lda-1" href="./iccv-2013-Target-Driven_Moire_Pattern_Synthesis_by_Phase_Modulation.html">413 iccv-2013-Target-Driven Moire Pattern Synthesis by Phase Modulation</a></p>
<p>Author: Pei-Hen Tsai, Yung-Yu Chuang</p><p>Abstract: This paper investigates an approach for generating two grating images so that the moir e´ pattern of their superposition resembles the target image. Our method is grounded on the fundamental moir e´ theorem. By focusing on the visually most dominant (1, −1)-moir e´ component, we obtain the phase smto ddoumlaintiaonnt c (o1n,s−tr1a)in-mt on the phase shifts bee otwbteaeinn the two grating images. For improving visual appearance of the grating images and hiding capability the embedded image, a smoothness term is added to spread information between the two grating images and an appearance phase function is used to add irregular structures into grating images. The grating images can be printed on transparencies and the hidden image decoding can be performed optically by overlaying them together. The proposed method enables the creation of moir e´ art and allows visual decoding without computers.</p><p>2 0.88623881 <a title="417-lda-2" href="./iccv-2013-POP%3A_Person_Re-identification_Post-rank_Optimisation.html">305 iccv-2013-POP: Person Re-identification Post-rank Optimisation</a></p>
<p>Author: Chunxiao Liu, Chen Change Loy, Shaogang Gong, Guijin Wang</p><p>Abstract: Owing to visual ambiguities and disparities, person reidentification methods inevitably produce suboptimal ranklist, which still requires exhaustive human eyeballing to identify the correct target from hundreds of different likelycandidates. Existing re-identification studies focus on improving the ranking performance, but rarely look into the critical problem of optimising the time-consuming and error-prone post-rank visual search at the user end. In this study, we present a novel one-shot Post-rank OPtimisation (POP) method, which allows a user to quickly refine their search by either “one-shot” or a couple of sparse negative selections during a re-identification process. We conduct systematic behavioural studies to understand user’s searching behaviour and show that the proposed method allows correct re-identification to converge 2.6 times faster than the conventional exhaustive search. Importantly, through extensive evaluations we demonstrate that the method is capable of achieving significant improvement over the stateof-the-art distance metric learning based ranking models, even with just “one shot” feedback optimisation, by as much as over 30% performance improvement for rank 1reidentification on the VIPeR and i-LIDS datasets.</p><p>3 0.85580951 <a title="417-lda-3" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>Author: Mohamed Elhoseiny, Babak Saleh, Ahmed Elgammal</p><p>Abstract: The main question we address in this paper is how to use purely textual description of categories with no training images to learn visual classifiers for these categories. We propose an approach for zero-shot learning of object categories where the description of unseen categories comes in the form of typical text such as an encyclopedia entry, without the need to explicitly defined attributes. We propose and investigate two baseline formulations, based on regression and domain adaptation. Then, we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to predict the classifier parameters for new classes. We applied the proposed approach on two fine-grained categorization datasets, and the results indicate successful classifier prediction.</p><p>same-paper 4 0.85160184 <a title="417-lda-4" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>Author: Mihai Zanfir, Marius Leordeanu, Cristian Sminchisescu</p><p>Abstract: Human action recognition under low observational latency is receiving a growing interest in computer vision due to rapidly developing technologies in human-robot interaction, computer gaming and surveillance. In this paper we propose a fast, simple, yet powerful non-parametric Moving Pose (MP)frameworkfor low-latency human action and activity recognition. Central to our methodology is a moving pose descriptor that considers both pose information as well as differential quantities (speed and acceleration) of the human body joints within a short time window around the current frame. The proposed descriptor is used in conjunction with a modified kNN classifier that considers both the temporal location of a particular frame within the action sequence as well as the discrimination power of its moving pose descriptor compared to other frames in the training set. The resulting method is non-parametric and enables low-latency recognition, one-shot learning, and action detection in difficult unsegmented sequences. Moreover, the framework is real-time, scalable, and outperforms more sophisticated approaches on challenging benchmarks like MSR-Action3D or MSR-DailyActivities3D.</p><p>5 0.8340109 <a title="417-lda-5" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>Author: Michael Van_Den_Bergh, Gemma Roig, Xavier Boix, Santiago Manen, Luc Van_Gool</p><p>Abstract: Superpixel and objectness algorithms are broadly used as a pre-processing step to generate support regions and to speed-up further computations. Recently, many algorithms have been extended to video in order to exploit the temporal consistency between frames. However, most methods are computationally too expensive for real-time applications. We introduce an online, real-time video superpixel algorithm based on the recently proposed SEEDS superpixels. A new capability is incorporated which delivers multiple diverse samples (hypotheses) of superpixels in the same image or video sequence. The multiple samples are shown to provide a strong cue to efficiently measure the objectness of image windows, and we introduce the novel concept of objectness in temporal windows. Experiments show that the video superpixels achieve comparable performance to state-of-the-art offline methods while running at 30 fps on a single 2.8 GHz i7 CPU. State-of-the-art performance on objectness is also demonstrated, yet orders of magnitude faster and extended to temporal windows in video.</p><p>6 0.80727047 <a title="417-lda-6" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>7 0.78824031 <a title="417-lda-7" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>8 0.77797788 <a title="417-lda-8" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>9 0.76026481 <a title="417-lda-9" href="./iccv-2013-Video_Event_Understanding_Using_Natural_Language_Descriptions.html">440 iccv-2013-Video Event Understanding Using Natural Language Descriptions</a></p>
<p>10 0.75990427 <a title="417-lda-10" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>11 0.75948441 <a title="417-lda-11" href="./iccv-2013-Learning_Maximum_Margin_Temporal_Warping_for_Action_Recognition.html">240 iccv-2013-Learning Maximum Margin Temporal Warping for Action Recognition</a></p>
<p>12 0.75663453 <a title="417-lda-12" href="./iccv-2013-Concurrent_Action_Detection_with_Structural_Prediction.html">86 iccv-2013-Concurrent Action Detection with Structural Prediction</a></p>
<p>13 0.75416541 <a title="417-lda-13" href="./iccv-2013-Finding_Actors_and_Actions_in_Movies.html">166 iccv-2013-Finding Actors and Actions in Movies</a></p>
<p>14 0.75123632 <a title="417-lda-14" href="./iccv-2013-Efficient_Pedestrian_Detection_by_Directly_Optimizing_the_Partial_Area_under_the_ROC_Curve.html">136 iccv-2013-Efficient Pedestrian Detection by Directly Optimizing the Partial Area under the ROC Curve</a></p>
<p>15 0.74858868 <a title="417-lda-15" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>16 0.74712515 <a title="417-lda-16" href="./iccv-2013-Incorporating_Cloud_Distribution_in_Sky_Representation.html">215 iccv-2013-Incorporating Cloud Distribution in Sky Representation</a></p>
<p>17 0.74241638 <a title="417-lda-17" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>18 0.73904252 <a title="417-lda-18" href="./iccv-2013-Finding_the_Best_from_the_Second_Bests_-_Inhibiting_Subjective_Bias_in_Evaluation_of_Visual_Tracking_Algorithms.html">168 iccv-2013-Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms</a></p>
<p>19 0.73900181 <a title="417-lda-19" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>20 0.73802513 <a title="417-lda-20" href="./iccv-2013-Pictorial_Human_Spaces%3A_How_Well_Do_Humans_Perceive_a_3D_Articulated_Pose%3F.html">316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
