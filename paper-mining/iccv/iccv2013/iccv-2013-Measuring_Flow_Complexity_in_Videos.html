<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>263 iccv-2013-Measuring Flow Complexity in Videos</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-263" href="#">iccv2013-263</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>263 iccv-2013-Measuring Flow Complexity in Videos</h1>
<br/><p>Source: <a title="iccv-2013-263-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Ali_Measuring_Flow_Complexity_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Saad Ali</p><p>Abstract: In this paper a notion of flow complexity that measures the amount of interaction among objects is introduced and an approach to compute it directly from a video sequence is proposed. The approach employs particle trajectories as the input representation of motion and maps it into a ‘braid’ based representation. The mapping is based on the observation that 2D trajectories of particles take the form of a braid in space-time due to the intermingling among particles over time. As a result of this mapping, the problem of estimating the flow complexity from particle trajectories becomes the problem of estimating braid complexity, which in turn can be computed by measuring the topological entropy of a braid. For this purpose recently developed mathematical tools from braid theory are employed which allow rapid computation of topological entropy of braids. The approach is evaluated on a dataset consisting of open source videos depicting variations in terms of types of moving objects, scene layout, camera view angle, motion patterns, and object densities. The results show that the proposed approach is able to quantify the complexity of the flow, and at the same time provides useful insights about the sources of the complexity.</p><p>Reference: <a title="iccv-2013-263-reference" href="../iccv2013_reference/iccv-2013-Measuring_Flow_Complexity_in_Videos_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com @  Abstract In this paper a notion of flow complexity that measures the amount of interaction among objects is introduced and an approach to compute it directly from a video sequence is proposed. [sent-4, score-0.414]
</p><p>2 The approach employs particle trajectories as the input representation of motion and maps it into a ‘braid’ based representation. [sent-5, score-0.511]
</p><p>3 The mapping is based on the observation that 2D trajectories of particles take the form of a braid in space-time due to the intermingling among particles over time. [sent-6, score-1.256]
</p><p>4 As a result of this mapping, the problem of estimating the flow complexity from particle trajectories becomes the problem of estimating braid complexity, which in turn can be computed by measuring the topological entropy of a braid. [sent-7, score-1.671]
</p><p>5 For this purpose recently developed mathematical tools from braid theory are employed which allow rapid computation of topological entropy of braids. [sent-8, score-0.982]
</p><p>6 The question arises, is there an objective way to measure this complexity directly from a video  (a)(b)(c)  (d)(e)(f) Figure 1: Frames from videos depicting movements with different level of flow complexity. [sent-16, score-0.475]
</p><p>7 sequence, thus making the notion of motion or flow complexity explicit? [sent-18, score-0.493]
</p><p>8 In this paper we attempt to address this question by proposing a measure that quantifies the flow or motion complexity. [sent-19, score-0.381]
</p><p>9 For this purpose, the flow complexity is considered as a measure that captures the amount of collective interaction among objects resulting due to the kinematic and mixing properties of the underlying optical flow field. [sent-20, score-0.699]
</p><p>10 While many current methods can compute optical flow, segment motion patterns, and track objects in these videos, there is a lack of approaches that can objectively answer the question: what is the complexity of flow in these sequences? [sent-32, score-0.536]
</p><p>11 Is the flow resulting from a large number of athletes in a marathon more complex than the motion of a small group of people crossing the street? [sent-33, score-0.665]
</p><p>12 First, particle trajectories are used as the representation of the flow in a video. [sent-41, score-0.626]
</p><p>13 Second, we make a key observation that 2D trajectories of particles take the form of a braid in space-time due to intermingling (or interaction) among the objects observed in a  video. [sent-46, score-1.092]
</p><p>14 A braid is defined as any complex structure or pattern formed by intertwining three or more strands. [sent-47, score-0.624]
</p><p>15 The common notion of intertwining in braids and intermingling in trajectories is what allows us to make the connection between the two. [sent-48, score-0.495]
</p><p>16 As a result, the problem of estimating the flow complexity is mapped into the problem of estimating the braid complexity, which in turn can be computed by measuring the topological entropy of the braid. [sent-49, score-1.293]
</p><p>17 The topological entropy can be used as a measure of flow complexity because the degree of entanglement of trajectories over time, which signifies the complexity of the underlying flow, is directly proportional to the topological entropy of the corresponding braid. [sent-50, score-1.507]
</p><p>18 Third, we compute the topological entropy of a braid using several mathematical tools from braid theory [11, 8]. [sent-51, score-1.539]
</p><p>19 These tools allow rapid computation of topological entropy even with a large number of trajectories [11, 8]. [sent-52, score-0.678]
</p><p>20 The terms braid entropy, flow entropy and topological entropy will be used interchangeably in the text. [sent-53, score-1.477]
</p><p>21 [1] developed a motion entropy measure to distinguish high motion intensity frames from low motion intensity frames. [sent-58, score-0.742]
</p><p>22 Statistical distributions of both direction and magnitude are  employed for this purpose and the idea is that a set of regular motion vectors, which will appear more frequently in a low motion intensity frames, would generate a lower entropy (i. [sent-59, score-0.614]
</p><p>23 low motion complexity) while a set of irregular motion vectors, which will appear more frequently in a high motion intensity frames, would generate a higher entropy (i. [sent-61, score-0.691]
</p><p>24 Similar notion of motion entropy has been used for video watermarking in [3]. [sent-64, score-0.487]
</p><p>25 [2] employed an entropy measure defined on angle distributions of motion vectors while Liu et al. [sent-66, score-0.402]
</p><p>26 [6] used magnitude between successive frames along the dominant motion direction for characterization of motion in a video sequence. [sent-67, score-0.447]
</p><p>27 The descriptors are again based on optical flow vectors and are designed to capture the mean, variance, and difference of motion vector magnitudes. [sent-72, score-0.451]
</p><p>28 The motion activity is also one of the motion features included in the visual part of the MPEG-7 standard and has been used to describe the level or intensity of activity, action, or motion in a video sequence [5] . [sent-73, score-0.479]
</p><p>29 First, our approach is based on a trajectory based representation as opposed to the instantaneous optical flow based representation. [sent-75, score-0.449]
</p><p>30 It has been shown by the body ofwork  on motion pattern analysis ([24][15][23]) that particle trajectories integrate motion information over longer durations of time, and therefore capture the dynamics of the underlying flow in a more reliable and robust way. [sent-76, score-0.967]
</p><p>31 As interactions among objects emerge overtime, statistics derived from instantaneous optical flow are not suitable to characterize interaction driven motion complexity. [sent-80, score-0.505]
</p><p>32 The advection process generates a dense collection of trajectories which are sampled to obtain a subset of N trajectories (Step 3). [sent-93, score-0.692]
</p><p>33 Next, N sampled trajectories are mapped into a braid and a generator sequence is computed for this braid by locating the precise times at which particles exchange their positions along the chosen projection axis (Step 4). [sent-94, score-1.967]
</p><p>34 a Tlihnee slope of the line is used as a measure of topological entropy or flow complexity (Step 8). [sent-104, score-0.779]
</p><p>35 From preliminary experiments, it is observed that the exact choice of optical flow algorithm is not critical, however, it is important to obtain both small and large scale motion structures. [sent-109, score-0.451]
</p><p>36 due to explicit flow smoothness constraints small scale motion structures are not preserved during optical flow computation [18]. [sent-112, score-0.721]
</p><p>37 For this purpose, particle advection is used to obtain a dense  ×  collection of trajectories [24, 25]. [sent-122, score-0.493]
</p><p>38 The trajectories for the interval (t, t+T) are obtained by overlaying a grid of particles on the first flow field, U(t). [sent-124, score-0.687]
</p><p>39 me 76 o8f0 0the tsreaj trajectories originate f3r2o0m r areas dioenpicting dominant flows (e. [sent-139, score-0.4]
</p><p>40 The trajectories are sampled from the dense collection of trajectories in Fig. [sent-143, score-0.6]
</p><p>41 To reduce the computational complexity of subsequent steps, the number of trajectories are decreased through a sampling step. [sent-147, score-0.382]
</p><p>42 A physical braid is defined as a collection of three or more strands that are intertwined. [sent-163, score-0.603]
</p><p>43 Since trajectories are spacetime constructs and their intermingling results in intertwining as well, each trajectory can be considered as a strand of a braid. [sent-164, score-0.493]
</p><p>44 Using this similarity, the geometric representation of a braid is obtained by projecting each trajectory into the xt−plane [8]. [sent-165, score-0.678]
</p><p>45 Note that exchange in positions along x-axis in the geometric representation of the braid is called a crossing. [sent-173, score-0.63]
</p><p>46 Another important thing to note is that any braid is uniquely defined by how the trajectories intertwine or cross in the projected view. [sent-174, score-0.905]
</p><p>47 The trajectories can be perturbed but the braid will be invariant to this perturbation as long as the direction, number, or order of crossing is not changed [9, 8]. [sent-175, score-1.013]
</p><p>48 (b) Shows how index  locations of particles are updated and generator sequence is created from the crossing time information. [sent-191, score-0.46]
</p><p>49 projection starts by sorting the sampled trajectories in terms of their position along the projection axis (e. [sent-192, score-0.536]
</p><p>50 During the next time step, t to t + 1, pairwise crossings between all trajectories are computed using line intersection computation. [sent-198, score-0.41]
</p><p>51 6a), and the order of the trajectories before crossing (which is [2, 3] in Fig. [sent-205, score-0.434]
</p><p>52 Generator and Crossing Time Computation Generators are operators that act on a braid by taking a pair of trajectories (or strands) and crossing them. [sent-210, score-1.013]
</p><p>53 They allow conversion of the geometric representation of the braid into an algebraic representation, as shown in Fig. [sent-211, score-0.579]
</p><p>54 Here 11 110000  two generators act in a sequence where the first generator causes the green trajectory to pass in front of the black trajectory, while the second generator causes the black trajectory to pass in front of the red trajectory. [sent-213, score-0.591]
</p><p>55 Each generator is stored along with its crossing time as this information is needed for evolving the loops in Section 3. [sent-220, score-0.427]
</p><p>56 Loop Generation Encoding  and Dynnikov  Coordinate  Now that trajectories are converted into a geometric representation of braid and a sequence of generators is com-  puted, the next step is to utilize this information for computing the entropy of the braid. [sent-225, score-1.242]
</p><p>57 This is achieved by using the concept of a loop where loop is a non-self intersecting closed curve or a region that passes around or encapsulates a set of particles. [sent-226, score-0.522]
</p><p>58 The intuition behind using loops is that the length of a loop surrounding a set of particles involved in complex motion will grow exponentially over time, while a loop surrounding particles that exhibit simple motion e. [sent-229, score-1.285]
</p><p>59 Symbolic Encoding of Loops: The symbolic encoding of a loop surrounding a set of particles is achieved by introducing a coordinate system which uniquely defines a loop. [sent-234, score-0.566]
</p><p>60 Vertical reference lines are used to encode the loop in Dynnikov coordinates [12] while the horizontal reference line is used for computing length of the loop (see Eq. [sent-253, score-0.64]
</p><p>61 domly initialized loop around particles proceeds as follows: Let μi counts the number of intersections of boundary of the loop with the reference line above and below the particles, and νi counts the number of intersections of the boundary of loop with the reference lines in between the particles. [sent-255, score-1.093]
</p><p>62 The number of crossings above and below the first and the last particle (along the projection line) is not required as that can be deduced from the other crossing information [9, 12]. [sent-256, score-0.416]
</p><p>63 7 provides a visualization of this initial loop encoding step where the above and below crossings and their magnitudes are μ1 = 1, μ2 = 3, μ3 = 3 and μ4 = 1, while the midpoint crossings and their magnitudes are ν1 = 4, ν2 = 4 and ν3 = 2. [sent-258, score-0.484]
</p><p>64 The Dynnikov coordinates are essentially a comparison of number of times a loop passes above or below the (i + 1)th particle (captured by ai), and the number of times loop passes to the left and to the right of (i 1)th particle (captured by bi). [sent-266, score-0.792]
</p><p>65 The idea is that as generators are applied to the trajectories, the loop surrounding the trajectories also moves. [sent-278, score-0.675]
</p><p>66 These rules allow rapid computation of change in the position of the  loop due to a generator and prevent the need for a computationally intensive loop advection process. [sent-280, score-0.743]
</p><p>67 It is pertinent to mention that initialization of a loop is done in a random fashion, and since a loop is represented by 2N 4 coordinates, a large innucmeb aer lo oofp possible loops can Nb e generated ifnoart a ,N a trajectory sample. [sent-283, score-0.699]
</p><p>68 This result demonstrates that the length of a loop is proportional to the number of times the loop crosses (intersects) a line connecting all the particles (green horizontal line in Fig. [sent-289, score-0.751]
</p><p>69 If the rate of growth is slow or remains approximately the same, the loop is surrounding particles exhibiting simple motion. [sent-303, score-0.536]
</p><p>70 However, instead of directly computing the topological entropy we compute the braid entropy which is a lower bound on the topological entropy. [sent-310, score-1.341]
</p><p>71 As more trajectories are included it converges to the topological entropy of the underlying the flow (or dynamical system) [13, 10]. [sent-311, score-0.93]
</p><p>72 In general the topological entropy of a dynamical system measures the loss of information under the dynamics, and this loss happens usually in the presence of chaotic (or complex) motion. [sent-312, score-0.421]
</p><p>73 Given the growth rate of a loop, the braid entropy, Sb, is computed as [10]:  Sb = lim  ddtlogL(t). [sent-313, score-0.67]
</p><p>74 (3)  Here Sb is an approximation to braid entropy and captures  the the asymptotic growth rate of the logarithm of the length of the loop L(t). [sent-314, score-1.238]
</p><p>75 To obtain a single value, L(t) is averaged over all M sampling runs, and slope of a line fitted to the logarithm of the averaged L(t) is used as the magnitude of the braid entropy. [sent-316, score-0.773]
</p><p>76 The value also represents the topological entropy or motion complexity of the underlying flow. [sent-317, score-0.625]
</p><p>77 In terms of motion patterns, the dataset contains dominant flows of various shapes and orientations, and flows resulting from intermingling of objects, chaotic motion, and multiple independently moving objects. [sent-325, score-0.485]
</p><p>78 Trajectories are obtained by advecting a dense grid of particles through the flow and N = 50 trajectories are sampled for further analysis. [sent-333, score-0.767]
</p><p>79 The braid entropy for a particular projection axis is computed by fitting a line to the logarithm of averaged L(t) as described in Section 3. [sent-339, score-1.095]
</p><p>80 The entropy computation is repeated for 12 projection axis oriented from θ = 0◦ to 165◦ with a jump of 15◦ in between (see Fig. [sent-341, score-0.446]
</p><p>81 Finally, the braid entropy averaged over all projection axis is used as a measure of flow complexity in a video. [sent-343, score-1.359]
</p><p>82 43051 52053054050 T (time)  (b)  Video  (c)  Figure 8: (a) The 12 projection axis ranging from θ = 0◦ to 165◦ ; (b) The growth rate, logL(t), of all videos in the dataset; (c) The computed braid entropy of each video (sorted in descending order). [sent-348, score-1.203]
</p><p>83 Based on the slope of the line fitted to each of these curves, videos are categorized into three types of flow complexity based on high, medium and low growth rates of the logL(t). [sent-354, score-0.603]
</p><p>84 It is observed that the videos representing high flow complexity contain dense motion at high speeds and are also captured by a zoomed-in camera (i. [sent-355, score-0.566]
</p><p>85 This is intuitively understandable as fast moving trajectories are able to perform many more crossings per unit time thereby increasing the braiding factor. [sent-361, score-0.403]
</p><p>86 Similarly flow at a street crossing, when observed by a zoomed-in camera, becomes highly complex due to people walking in opposite directions and articulated motion of arms and legs. [sent-362, score-0.449]
</p><p>87 The medium complexity flows correspond to videos containing zoomed-out views of crowds, usually from a high angle view, where trajectories intertwine primarily due to high density of objects. [sent-363, score-0.617]
</p><p>88 Finally, videos with low complexity flows contain motion in a dominant direction (i. [sent-368, score-0.447]
</p><p>89 people running along a path, traffic on a road, motion of synthetic characters in one direction), or flows at extremely high density (almost packing density) where it becomes hard for particles to exchange their positions (see Fig. [sent-370, score-0.49]
</p><p>90 When there is a dominant motion, high braid entropy is observed only along few projection axis. [sent-373, score-0.969]
</p><p>91 8c summarizes the braid entropy, which is an average of braid entropy over 12 projection axis, for all videos in the dataset. [sent-378, score-1.56]
</p><p>92 Next we discuss the effect of projection axis on flow complexity computation. [sent-389, score-0.488]
</p><p>93 For this purpose, for each projection axis percentage change in the entropy with respect to the highest entropy value along any other axis is computed. [sent-390, score-0.844]
</p><p>94 The idea is that if the underlying motion is truly  complex, the percentage change in entropy should be small (i. [sent-392, score-0.457]
</p><p>95 On the other hand if the percentage change in entropy is large, it signals the existence of a motion which is complex only along certain axis. [sent-395, score-0.454]
</p><p>96 On the other hand, 11 110033  videos depicting flows with highest amount of percentage change in braid entropy contain flows in a dominant direction (e. [sent-402, score-1.222]
</p><p>97 10c has one of the highest percentage change in the braid entropy, with the highest entropy of 0. [sent-406, score-0.877]
</p><p>98 , axis slightly slanted with respect to the dominant direction of motion, and the lowest entropy of 0. [sent-410, score-0.437]
</p><p>99 961)Lgo(1 2086420 50(dTt)mie10 5 Figure 10: Videos having low (top row) and high mean percentage change in braid entropy with respect to the projection axis. [sent-416, score-0.933]
</p><p>100 11 shows the flow complexity based ordering of videos displayed in Fig. [sent-418, score-0.41]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('braid', 0.579), ('trajectories', 0.275), ('entropy', 0.269), ('flow', 0.248), ('loop', 0.246), ('dynnikov', 0.178), ('particles', 0.164), ('crossing', 0.159), ('generator', 0.137), ('motion', 0.133), ('generators', 0.119), ('topological', 0.112), ('loops', 0.108), ('particle', 0.103), ('trajectory', 0.099), ('axis', 0.099), ('crossings', 0.098), ('advection', 0.092), ('growth', 0.091), ('complexity', 0.085), ('flows', 0.083), ('videos', 0.077), ('braids', 0.074), ('enrtopy', 0.074), ('intermingling', 0.074), ('optical', 0.07), ('logl', 0.059), ('marathon', 0.059), ('projection', 0.056), ('colony', 0.053), ('dynamics', 0.049), ('intersections', 0.049), ('crowd', 0.048), ('ants', 0.045), ('intertwining', 0.045), ('nortpy', 0.045), ('fish', 0.044), ('encoding', 0.042), ('dominant', 0.042), ('chaotic', 0.04), ('saad', 0.04), ('street', 0.039), ('line', 0.037), ('medium', 0.037), ('athletes', 0.037), ('surrounding', 0.035), ('coordinates', 0.034), ('depicting', 0.033), ('symbolic', 0.033), ('integers', 0.033), ('video', 0.032), ('logarithm', 0.032), ('instantaneous', 0.032), ('clockwise', 0.032), ('keyframes', 0.032), ('ai', 0.031), ('ali', 0.03), ('passes', 0.03), ('density', 0.03), ('advecting', 0.03), ('intertwine', 0.03), ('iyengar', 0.03), ('lippman', 0.03), ('moussafir', 0.03), ('peker', 0.03), ('thiffeault', 0.03), ('moving', 0.03), ('school', 0.029), ('magnitude', 0.029), ('people', 0.029), ('percentage', 0.029), ('reference', 0.028), ('slope', 0.028), ('exchange', 0.028), ('frames', 0.028), ('sb', 0.027), ('notion', 0.027), ('direction', 0.027), ('sampled', 0.027), ('mubarak', 0.026), ('entanglement', 0.026), ('watermarking', 0.026), ('underlying', 0.026), ('bi', 0.026), ('coordinate', 0.025), ('activity', 0.025), ('entangled', 0.024), ('strands', 0.024), ('pieces', 0.024), ('averaged', 0.023), ('intensity', 0.023), ('dense', 0.023), ('along', 0.023), ('patterns', 0.023), ('computation', 0.022), ('sampling', 0.022), ('interaction', 0.022), ('length', 0.021), ('divakaran', 0.021), ('uniquely', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="263-tfidf-1" href="./iccv-2013-Measuring_Flow_Complexity_in_Videos.html">263 iccv-2013-Measuring Flow Complexity in Videos</a></p>
<p>Author: Saad Ali</p><p>Abstract: In this paper a notion of flow complexity that measures the amount of interaction among objects is introduced and an approach to compute it directly from a video sequence is proposed. The approach employs particle trajectories as the input representation of motion and maps it into a ‘braid’ based representation. The mapping is based on the observation that 2D trajectories of particles take the form of a braid in space-time due to the intermingling among particles over time. As a result of this mapping, the problem of estimating the flow complexity from particle trajectories becomes the problem of estimating braid complexity, which in turn can be computed by measuring the topological entropy of a braid. For this purpose recently developed mathematical tools from braid theory are employed which allow rapid computation of topological entropy of braids. The approach is evaluated on a dataset consisting of open source videos depicting variations in terms of types of moving objects, scene layout, camera view angle, motion patterns, and object densities. The results show that the proposed approach is able to quantify the complexity of the flow, and at the same time provides useful insights about the sources of the complexity.</p><p>2 0.22590689 <a title="263-tfidf-2" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<p>Author: Ali Elqursh, Ahmed Elgammal</p><p>Abstract: The vast majority of work on motion segmentation adopts the affine camera model due to its simplicity. Under the affine model, the motion segmentation problem becomes that of subspace separation. Due to this assumption, such methods are mainly offline and exhibit poor performance when the assumption is not satisfied. This is made evident in state-of-the-art methods that relax this assumption by using piecewise affine spaces and spectral clustering techniques to achieve better results. In this paper, we formulate the problem of motion segmentation as that of manifold separation. We then show how label propagation can be used in an online framework to achieve manifold separation. The performance of our framework is evaluated on a benchmark dataset and achieves competitive performance while being online.</p><p>3 0.22415094 <a title="263-tfidf-3" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>Author: Heng Wang, Cordelia Schmid</p><p>Abstract: Recently dense trajectories were shown to be an efficient video representation for action recognition and achieved state-of-the-art results on a variety of datasets. This paper improves their performance by taking into account camera motion to correct them. To estimate camera motion, we match feature points between frames using SURF descriptors and dense optical flow, which are shown to be complementary. These matches are, then, used to robustly estimate a homography with RANSAC. Human motion is in general different from camera motion and generates inconsistent matches. To improve the estimation, a human detector is employed to remove these matches. Given the estimated camera motion, we remove trajectories consistent with it. We also use this estimation to cancel out camera motion from the optical flow. This significantly improves motion-based descriptors, such as HOF and MBH. Experimental results onfour challenging action datasets (i.e., Hollywood2, HMDB51, Olympic Sports and UCF50) significantly outperform the current state of the art.</p><p>4 0.2154191 <a title="263-tfidf-4" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>Author: Feng Shi, Zhong Zhou, Jiangjian Xiao, Wei Wu</p><p>Abstract: Due to occlusions and objects ’ non-rigid deformation in the scene, the obtained motion trajectories from common trackers may contain a number of missing or mis-associated entries. To cluster such corrupted point based trajectories into multiple motions is still a hard problem. In this paper, we present an approach that exploits temporal and spatial characteristics from tracked points to facilitate segmentation of incomplete and corrupted trajectories, thereby obtain highly robust results against severe data missing and noises. Our method first uses the Discrete Cosine Transform (DCT) bases as a temporal smoothness constraint on trajectory projection to ensure the validity of resulting components to repair pathological trajectories. Then, based on an observation that the trajectories of foreground and background in a scene may have different spatial distributions, we propose a two-stage clustering strategy that first performs foreground-background separation then segments remaining foreground trajectories. We show that, with this new clustering strategy, sequences with complex motions can be accurately segmented by even using a simple trans- lational model. Finally, a series of experiments on Hopkins 155 dataset andBerkeley motion segmentation dataset show the advantage of our method over other state-of-the-art motion segmentation algorithms in terms of both effectiveness and robustness.</p><p>5 0.19269672 <a title="263-tfidf-5" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>Author: Jiaming Guo, Zhuwen Li, Loong-Fah Cheong, Steven Zhiying Zhou</p><p>Abstract: Given a pair of videos having a common action, our goal is to simultaneously segment this pair of videos to extract this common action. As a preprocessing step, we first remove background trajectories by a motion-based figureground segmentation. To remove the remaining background and those extraneous actions, we propose the trajectory cosaliency measure, which captures the notion that trajectories recurring in all the videos should have their mutual saliency boosted. This requires a trajectory matching process which can compare trajectories with different lengths and not necessarily spatiotemporally aligned, and yet be discriminative enough despite significant intra-class variation in the common action. We further leverage the graph matching to enforce geometric coherence between regions so as to reduce feature ambiguity and matching errors. Finally, to classify the trajectories into common action and action outliers, we formulate the problem as a binary labeling of a Markov Random Field, in which the data term is measured by the trajectory co-saliency and the smooth- ness term is measured by the spatiotemporal consistency between trajectories. To evaluate the performance of our framework, we introduce a dataset containing clips that have animal actions as well as human actions. Experimental results show that the proposed method performs well in common action extraction.</p><p>6 0.18864723 <a title="263-tfidf-6" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>7 0.18424946 <a title="263-tfidf-7" href="./iccv-2013-Camera_Alignment_Using_Trajectory_Intersections_in_Unsynchronized_Videos.html">68 iccv-2013-Camera Alignment Using Trajectory Intersections in Unsynchronized Videos</a></p>
<p>8 0.16609253 <a title="263-tfidf-8" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>9 0.14875732 <a title="263-tfidf-9" href="./iccv-2013-DeepFlow%3A_Large_Displacement_Optical_Flow_with_Deep_Matching.html">105 iccv-2013-DeepFlow: Large Displacement Optical Flow with Deep Matching</a></p>
<p>10 0.14048827 <a title="263-tfidf-10" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>11 0.13431178 <a title="263-tfidf-11" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>12 0.1249155 <a title="263-tfidf-12" href="./iccv-2013-Optical_Flow_via_Locally_Adaptive_Fusion_of_Complementary_Data_Costs.html">300 iccv-2013-Optical Flow via Locally Adaptive Fusion of Complementary Data Costs</a></p>
<p>13 0.1180689 <a title="263-tfidf-13" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>14 0.11064313 <a title="263-tfidf-14" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<p>15 0.10316709 <a title="263-tfidf-15" href="./iccv-2013-Online_Robust_Non-negative_Dictionary_Learning_for_Visual_Tracking.html">298 iccv-2013-Online Robust Non-negative Dictionary Learning for Visual Tracking</a></p>
<p>16 0.10089276 <a title="263-tfidf-16" href="./iccv-2013-Topology-Constrained_Layered_Tracking_with_Latent_Flow.html">420 iccv-2013-Topology-Constrained Layered Tracking with Latent Flow</a></p>
<p>17 0.099002346 <a title="263-tfidf-17" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<p>18 0.097097993 <a title="263-tfidf-18" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>19 0.096709155 <a title="263-tfidf-19" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>20 0.096069165 <a title="263-tfidf-20" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.171), (1, -0.078), (2, 0.039), (3, 0.189), (4, -0.016), (5, 0.089), (6, -0.012), (7, 0.077), (8, 0.143), (9, 0.127), (10, 0.012), (11, 0.028), (12, 0.153), (13, 0.003), (14, 0.007), (15, 0.008), (16, -0.04), (17, 0.075), (18, 0.064), (19, 0.046), (20, -0.044), (21, -0.067), (22, 0.159), (23, 0.149), (24, 0.016), (25, 0.064), (26, 0.124), (27, 0.04), (28, 0.054), (29, -0.005), (30, -0.01), (31, 0.021), (32, -0.014), (33, -0.005), (34, 0.017), (35, -0.105), (36, -0.031), (37, -0.0), (38, -0.003), (39, -0.036), (40, 0.03), (41, -0.083), (42, 0.046), (43, -0.016), (44, 0.024), (45, -0.028), (46, -0.095), (47, 0.025), (48, -0.058), (49, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96514368 <a title="263-lsi-1" href="./iccv-2013-Measuring_Flow_Complexity_in_Videos.html">263 iccv-2013-Measuring Flow Complexity in Videos</a></p>
<p>Author: Saad Ali</p><p>Abstract: In this paper a notion of flow complexity that measures the amount of interaction among objects is introduced and an approach to compute it directly from a video sequence is proposed. The approach employs particle trajectories as the input representation of motion and maps it into a ‘braid’ based representation. The mapping is based on the observation that 2D trajectories of particles take the form of a braid in space-time due to the intermingling among particles over time. As a result of this mapping, the problem of estimating the flow complexity from particle trajectories becomes the problem of estimating braid complexity, which in turn can be computed by measuring the topological entropy of a braid. For this purpose recently developed mathematical tools from braid theory are employed which allow rapid computation of topological entropy of braids. The approach is evaluated on a dataset consisting of open source videos depicting variations in terms of types of moving objects, scene layout, camera view angle, motion patterns, and object densities. The results show that the proposed approach is able to quantify the complexity of the flow, and at the same time provides useful insights about the sources of the complexity.</p><p>2 0.75893843 <a title="263-lsi-2" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>Author: Heng Wang, Cordelia Schmid</p><p>Abstract: Recently dense trajectories were shown to be an efficient video representation for action recognition and achieved state-of-the-art results on a variety of datasets. This paper improves their performance by taking into account camera motion to correct them. To estimate camera motion, we match feature points between frames using SURF descriptors and dense optical flow, which are shown to be complementary. These matches are, then, used to robustly estimate a homography with RANSAC. Human motion is in general different from camera motion and generates inconsistent matches. To improve the estimation, a human detector is employed to remove these matches. Given the estimated camera motion, we remove trajectories consistent with it. We also use this estimation to cancel out camera motion from the optical flow. This significantly improves motion-based descriptors, such as HOF and MBH. Experimental results onfour challenging action datasets (i.e., Hollywood2, HMDB51, Olympic Sports and UCF50) significantly outperform the current state of the art.</p><p>3 0.72914672 <a title="263-lsi-3" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>Author: Feng Shi, Zhong Zhou, Jiangjian Xiao, Wei Wu</p><p>Abstract: Due to occlusions and objects ’ non-rigid deformation in the scene, the obtained motion trajectories from common trackers may contain a number of missing or mis-associated entries. To cluster such corrupted point based trajectories into multiple motions is still a hard problem. In this paper, we present an approach that exploits temporal and spatial characteristics from tracked points to facilitate segmentation of incomplete and corrupted trajectories, thereby obtain highly robust results against severe data missing and noises. Our method first uses the Discrete Cosine Transform (DCT) bases as a temporal smoothness constraint on trajectory projection to ensure the validity of resulting components to repair pathological trajectories. Then, based on an observation that the trajectories of foreground and background in a scene may have different spatial distributions, we propose a two-stage clustering strategy that first performs foreground-background separation then segments remaining foreground trajectories. We show that, with this new clustering strategy, sequences with complex motions can be accurately segmented by even using a simple trans- lational model. Finally, a series of experiments on Hopkins 155 dataset andBerkeley motion segmentation dataset show the advantage of our method over other state-of-the-art motion segmentation algorithms in terms of both effectiveness and robustness.</p><p>4 0.72523808 <a title="263-lsi-4" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<p>Author: Ali Elqursh, Ahmed Elgammal</p><p>Abstract: The vast majority of work on motion segmentation adopts the affine camera model due to its simplicity. Under the affine model, the motion segmentation problem becomes that of subspace separation. Due to this assumption, such methods are mainly offline and exhibit poor performance when the assumption is not satisfied. This is made evident in state-of-the-art methods that relax this assumption by using piecewise affine spaces and spectral clustering techniques to achieve better results. In this paper, we formulate the problem of motion segmentation as that of manifold separation. We then show how label propagation can be used in an online framework to achieve manifold separation. The performance of our framework is evaluated on a benchmark dataset and achieves competitive performance while being online.</p><p>5 0.72194242 <a title="263-lsi-5" href="./iccv-2013-Camera_Alignment_Using_Trajectory_Intersections_in_Unsynchronized_Videos.html">68 iccv-2013-Camera Alignment Using Trajectory Intersections in Unsynchronized Videos</a></p>
<p>Author: Thomas Kuo, Santhoshkumar Sunderrajan, B.S. Manjunath</p><p>Abstract: This paper addresses the novel and challenging problem of aligning camera views that are unsynchronized by low and/or variable frame rates using object trajectories. Unlike existing trajectory-based alignment methods, our method does not require frame-to-frame synchronization. Instead, we propose using the intersections of corresponding object trajectories to match views. To find these intersections, we introduce a novel trajectory matching algorithm based on matching Spatio-Temporal Context Graphs (STCGs). These graphs represent the distances between trajectories in time and space within a view, and are matched to an STCG from another view to find the corresponding trajectories. To the best of our knowledge, this is one of the first attempts to align views that are unsynchronized with variable frame rates. The results on simulated and real-world datasets show trajectory intersections area viablefeatureforcamera alignment, and that the trajectory matching method performs well in real-world scenarios.</p><p>6 0.69474894 <a title="263-lsi-6" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>7 0.63585311 <a title="263-lsi-7" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>8 0.61868608 <a title="263-lsi-8" href="./iccv-2013-Optimal_Orthogonal_Basis_and_Image_Assimilation%3A_Motion_Modeling.html">301 iccv-2013-Optimal Orthogonal Basis and Image Assimilation: Motion Modeling</a></p>
<p>9 0.61629093 <a title="263-lsi-9" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>10 0.58758456 <a title="263-lsi-10" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>11 0.55204147 <a title="263-lsi-11" href="./iccv-2013-Estimating_the_Material_Properties_of_Fabric_from_Video.html">145 iccv-2013-Estimating the Material Properties of Fabric from Video</a></p>
<p>12 0.54164112 <a title="263-lsi-12" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>13 0.50119674 <a title="263-lsi-13" href="./iccv-2013-Optical_Flow_via_Locally_Adaptive_Fusion_of_Complementary_Data_Costs.html">300 iccv-2013-Optical Flow via Locally Adaptive Fusion of Complementary Data Costs</a></p>
<p>14 0.50061142 <a title="263-lsi-14" href="./iccv-2013-DeepFlow%3A_Large_Displacement_Optical_Flow_with_Deep_Matching.html">105 iccv-2013-DeepFlow: Large Displacement Optical Flow with Deep Matching</a></p>
<p>15 0.49657276 <a title="263-lsi-15" href="./iccv-2013-Topology-Constrained_Layered_Tracking_with_Latent_Flow.html">420 iccv-2013-Topology-Constrained Layered Tracking with Latent Flow</a></p>
<p>16 0.49012232 <a title="263-lsi-16" href="./iccv-2013-Manipulation_Pattern_Discovery%3A_A_Nonparametric_Bayesian_Approach.html">260 iccv-2013-Manipulation Pattern Discovery: A Nonparametric Bayesian Approach</a></p>
<p>17 0.48813421 <a title="263-lsi-17" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<p>18 0.4790498 <a title="263-lsi-18" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>19 0.46533942 <a title="263-lsi-19" href="./iccv-2013-Two-Point_Gait%3A_Decoupling_Gait_from_Body_Shape.html">430 iccv-2013-Two-Point Gait: Decoupling Gait from Body Shape</a></p>
<p>20 0.45134303 <a title="263-lsi-20" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.045), (7, 0.024), (12, 0.021), (26, 0.067), (31, 0.036), (34, 0.011), (40, 0.016), (42, 0.084), (48, 0.016), (64, 0.047), (73, 0.025), (89, 0.21), (95, 0.286), (98, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86742365 <a title="263-lda-1" href="./iccv-2013-A_Novel_Earth_Mover%27s_Distance_Methodology_for_Image_Matching_with_Gaussian_Mixture_Models.html">25 iccv-2013-A Novel Earth Mover's Distance Methodology for Image Matching with Gaussian Mixture Models</a></p>
<p>Author: Peihua Li, Qilong Wang, Lei Zhang</p><p>Abstract: The similarity or distance measure between Gaussian mixture models (GMMs) plays a crucial role in contentbased image matching. Though the Earth Mover’s Distance (EMD) has shown its advantages in matching histogram features, its potentials in matching GMMs remain unclear and are not fully explored. To address this problem, we propose a novel EMD methodology for GMM matching. We ?rst present a sparse representation based EMD called SR-EMD by exploiting the sparse property of the underlying problem. SR-EMD is more ef?cient and robust than the conventional EMD. Second, we present two novel ground distances between component Gaussians based on the information geometry. The perspective from the Riemannian geometry distinguishes the proposed ground distances from the classical entropy- or divergence-based ones. Furthermore, motivated by the success of distance metric learning of vector data, we make the ?rst attempt to learn the EMD distance metrics between GMMs by using a simple yet effective supervised pair-wise based method. It can adapt the distance metrics between GMMs to speci?c classi?ca- tion tasks. The proposed method is evaluated on both simulated data and benchmark real databases and achieves very promising performance.</p><p>2 0.82949704 <a title="263-lda-2" href="./iccv-2013-Learning_Graph_Matching%3A_Oriented_to_Category_Modeling_from_Cluttered_Scenes.html">237 iccv-2013-Learning Graph Matching: Oriented to Category Modeling from Cluttered Scenes</a></p>
<p>Author: Quanshi Zhang, Xuan Song, Xiaowei Shao, Huijing Zhao, Ryosuke Shibasaki</p><p>Abstract: Although graph matching is a fundamental problem in pattern recognition, and has drawn broad interest from many fields, the problem of learning graph matching has not received much attention. In this paper, we redefine the learning ofgraph matching as a model learningproblem. In addition to conventional training of matching parameters, our approach modifies the graph structure and attributes to generate a graphical model. In this way, the model learning is oriented toward both matching and recognition performance, and can proceed in an unsupervised1 fashion. Experiments demonstrate that our approach outperforms conventional methods for learning graph matching.</p><p>3 0.80981338 <a title="263-lda-3" href="./iccv-2013-A_Generic_Deformation_Model_for_Dense_Non-rigid_Surface_Registration%3A_A_Higher-Order_MRF-Based_Approach.html">16 iccv-2013-A Generic Deformation Model for Dense Non-rigid Surface Registration: A Higher-Order MRF-Based Approach</a></p>
<p>Author: Yun Zeng, Chaohui Wang, Xianfeng Gu, Dimitris Samaras, Nikos Paragios</p><p>Abstract: We propose a novel approach for dense non-rigid 3D surface registration, which brings together Riemannian geometry and graphical models. To this end, we first introduce a generic deformation model, called Canonical Distortion Coefficients (CDCs), by characterizing the deformation of every point on a surface using the distortions along its two principle directions. This model subsumes the deformation groups commonly used in surface registration such as isometry and conformality, and is able to handle more complex deformations. We also derive its discrete counterpart which can be computed very efficiently in a closed form. Based on these, we introduce a higher-order Markov Random Field (MRF) model which seamlessly integrates our deformation model and a geometry/texture similarity metric. Then we jointly establish the optimal correspondences for all the points via maximum a posteriori (MAP) inference. Moreover, we develop a parallel optimization algorithm to efficiently perform the inference for the proposed higher-order MRF model. The resulting registration algorithm outperforms state-of-the-art methods in both dense non-rigid 3D surface registration and tracking.</p><p>same-paper 4 0.80763239 <a title="263-lda-4" href="./iccv-2013-Measuring_Flow_Complexity_in_Videos.html">263 iccv-2013-Measuring Flow Complexity in Videos</a></p>
<p>Author: Saad Ali</p><p>Abstract: In this paper a notion of flow complexity that measures the amount of interaction among objects is introduced and an approach to compute it directly from a video sequence is proposed. The approach employs particle trajectories as the input representation of motion and maps it into a ‘braid’ based representation. The mapping is based on the observation that 2D trajectories of particles take the form of a braid in space-time due to the intermingling among particles over time. As a result of this mapping, the problem of estimating the flow complexity from particle trajectories becomes the problem of estimating braid complexity, which in turn can be computed by measuring the topological entropy of a braid. For this purpose recently developed mathematical tools from braid theory are employed which allow rapid computation of topological entropy of braids. The approach is evaluated on a dataset consisting of open source videos depicting variations in terms of types of moving objects, scene layout, camera view angle, motion patterns, and object densities. The results show that the proposed approach is able to quantify the complexity of the flow, and at the same time provides useful insights about the sources of the complexity.</p><p>5 0.77911955 <a title="263-lda-5" href="./iccv-2013-GOSUS%3A_Grassmannian_Online_Subspace_Updates_with_Structured-Sparsity.html">182 iccv-2013-GOSUS: Grassmannian Online Subspace Updates with Structured-Sparsity</a></p>
<p>Author: Jia Xu, Vamsi K. Ithapu, Lopamudra Mukherjee, James M. Rehg, Vikas Singh</p><p>Abstract: We study the problem of online subspace learning in the context of sequential observations involving structured perturbations. In online subspace learning, the observations are an unknown mixture of two components presented to the model sequentially the main effect which pertains to the subspace and a residual/error term. If no additional requirement is imposed on the residual, it often corresponds to noise terms in the signal which were unaccounted for by the main effect. To remedy this, one may impose ‘structural’ contiguity, which has the intended effect of leveraging the secondary terms as a covariate that helps the estimation of the subspace itself, instead of merely serving as a noise residual. We show that the corresponding online estimation procedure can be written as an approximate optimization process on a Grassmannian. We propose an efficient numerical solution, GOSUS, Grassmannian Online Subspace Updates with Structured-sparsity, for this problem. GOSUS is expressive enough in modeling both homogeneous perturbations of the subspace and structural contiguities of outliers, and after certain manipulations, solvable — via an alternating direction method of multipliers (ADMM). We evaluate the empirical performance of this algorithm on two problems of interest: online background subtraction and online multiple face tracking, and demonstrate that it achieves competitive performance with the state-of-the-art in near real time.</p><p>6 0.75725824 <a title="263-lda-6" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>7 0.74990684 <a title="263-lda-7" href="./iccv-2013-Corrected-Moment_Illuminant_Estimation.html">92 iccv-2013-Corrected-Moment Illuminant Estimation</a></p>
<p>8 0.68137467 <a title="263-lda-8" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>9 0.67709202 <a title="263-lda-9" href="./iccv-2013-Geometric_Registration_Based_on_Distortion_Estimation.html">183 iccv-2013-Geometric Registration Based on Distortion Estimation</a></p>
<p>10 0.67642152 <a title="263-lda-10" href="./iccv-2013-Coherent_Object_Detection_with_3D_Geometric_Context_from_a_Single_Image.html">79 iccv-2013-Coherent Object Detection with 3D Geometric Context from a Single Image</a></p>
<p>11 0.67491573 <a title="263-lda-11" href="./iccv-2013-Dynamic_Structured_Model_Selection.html">130 iccv-2013-Dynamic Structured Model Selection</a></p>
<p>12 0.67484641 <a title="263-lda-12" href="./iccv-2013-Log-Euclidean_Kernels_for_Sparse_Representation_and_Dictionary_Learning.html">257 iccv-2013-Log-Euclidean Kernels for Sparse Representation and Dictionary Learning</a></p>
<p>13 0.67246944 <a title="263-lda-13" href="./iccv-2013-Space-Time_Tradeoffs_in_Photo_Sequencing.html">397 iccv-2013-Space-Time Tradeoffs in Photo Sequencing</a></p>
<p>14 0.67050385 <a title="263-lda-14" href="./iccv-2013-Flattening_Supervoxel_Hierarchies_by_the_Uniform_Entropy_Slice.html">172 iccv-2013-Flattening Supervoxel Hierarchies by the Uniform Entropy Slice</a></p>
<p>15 0.66944981 <a title="263-lda-15" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>16 0.66896844 <a title="263-lda-16" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>17 0.66477609 <a title="263-lda-17" href="./iccv-2013-Discriminative_Label_Propagation_for_Multi-object_Tracking_with_Sporadic_Appearance_Features.html">120 iccv-2013-Discriminative Label Propagation for Multi-object Tracking with Sporadic Appearance Features</a></p>
<p>18 0.66425103 <a title="263-lda-18" href="./iccv-2013-Modeling_4D_Human-Object_Interactions_for_Event_and_Object_Recognition.html">268 iccv-2013-Modeling 4D Human-Object Interactions for Event and Object Recognition</a></p>
<p>19 0.66388535 <a title="263-lda-19" href="./iccv-2013-Efficient_Higher-Order_Clustering_on_the_Grassmann_Manifold.html">134 iccv-2013-Efficient Higher-Order Clustering on the Grassmann Manifold</a></p>
<p>20 0.66222417 <a title="263-lda-20" href="./iccv-2013-Curvature-Aware_Regularization_on_Riemannian_Submanifolds.html">100 iccv-2013-Curvature-Aware Regularization on Riemannian Submanifolds</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
