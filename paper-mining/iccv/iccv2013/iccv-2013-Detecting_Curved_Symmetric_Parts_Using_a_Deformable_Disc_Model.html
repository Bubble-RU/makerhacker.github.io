<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>110 iccv-2013-Detecting Curved Symmetric Parts Using a Deformable Disc Model</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-110" href="#">iccv2013-110</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>110 iccv-2013-Detecting Curved Symmetric Parts Using a Deformable Disc Model</h1>
<br/><p>Source: <a title="iccv-2013-110-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Lee_Detecting_Curved_Symmetric_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Tom Sie Ho Lee, Sanja Fidler, Sven Dickinson</p><p>Abstract: Symmetry is a powerful shape regularity that’s been exploited by perceptual grouping researchers in both human and computer vision to recover part structure from an image without a priori knowledge of scene content. Drawing on the concept of a medial axis, defined as the locus of centers of maximal inscribed discs that sweep out a symmetric part, we model part recovery as the search for a sequence of deformable maximal inscribed disc hypotheses generated from a multiscale superpixel segmentation, a framework proposed by [13]. However, we learn affinities between adjacent superpixels in a space that’s invariant to bending and tapering along the symmetry axis, enabling us to capture a wider class of symmetric parts. Moreover, we introduce a global cost that perceptually integrates the hypothesis space by combining a pairwise and a higher-level smoothing term, which we minimize globally using dynamic programming. The new framework is demonstrated on two datasets, and is shown to significantly outperform the baseline [13].</p><p>Reference: <a title="iccv-2013-110-reference" href="../iccv2013_reference/iccv-2013-Detecting_Curved_Symmetric_Parts_Using_a_Deformable_Disc_Model_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 However, we learn affinities between adjacent superpixels in a space that’s invariant to bending and tapering along the symmetry axis, enabling us to capture a wider class of symmetric parts. [sent-3, score-1.061]
</p><p>2 It’s clear that as object databases grow, the role of perceptual grouping to extract a discriminative, domain-independent indexing structure that can prune a large database down to a small number of promising candidates will increase dramatically. [sent-13, score-0.289]
</p><p>3 One of the most powerful indexing structures is a configuration of parts, in which a set of parts (and their relations) belonging to the same object is recovered without any a priori knowledge of scene content, i. [sent-14, score-0.206]
</p><p>4 The bottom-up recovery of a set of generic parts can be traced back to the earliest days of computer vision, and includes Blum’s medial axis transform (MAT) [3], Binford’s generalized cylinders [2], Pentland’s superquadrics [25], and Biederman’s geons [1], to name just a few examples. [sent-17, score-0.417]
</p><p>5 [13] introduced a bottom-up approach which first detects symmetric parts and then groups them nonaccidentally to form indexing structures. [sent-21, score-0.396]
</p><p>6 The key contribution is the modeling of a “deformable” maximal inscribed disc as a superpixel. [sent-22, score-0.442]
</p><p>7 The image is segmented into superpixels at multiple scales, where each scale yields a graph in which nodes are superpixels. [sent-23, score-0.261]
</p><p>8 Adjacent superpixels are linked by an edge, to which a learned affinity function assigns a measure of how likely two superpixels represent adjacent maximal discs belonging to the same part. [sent-24, score-1.151]
</p><p>9 A graph clustering algorithm is then applied to the superpixel graph to yield a set of connected components representing symmetric object parts. [sent-25, score-0.433]
</p><p>10 While the framework outperformed previous approaches to symmetric part detection, it suffered from a number of serious limitations, as illustrated in Figure 1. [sent-26, score-0.364]
</p><p>11 To begin with, the symmetry model was restrictive in assuming that parts had straight axes and constant width. [sent-27, score-0.493]
</p><p>12 This prevented the correct detection of significantly curved and/or tapered parts, as illustrated in Figure 1(a). [sent-28, score-0.226]
</p><p>13 Second, superpixel grouping was restricted to a single scale, rather than integrated across multiple scales, preventing the detection of tapered parts whose component superpixels span multiple scales (Figure 1(c)). [sent-29, score-0.814]
</p><p>14 Finally, the superpixel grouping algorithm did not enforce a notion of good continuation, which led to incorrect detections when faced with ambigu-  ous grouping possibilities, as reflected in the undersegmentation of the two symmetric parts (leaves) into a single part in Figure 1(e). [sent-30, score-0.85]
</p><p>15 Like [13], superpixels are generated at multiple scales and represent hypotheses of maximal inscribed discs. [sent-32, score-0.53]
</p><p>16 Like [13], adjacent hypotheses are assigned an affinity by a learned affinity function trained on manually detected symmetric parts. [sent-33, score-0.609]
</p><p>17 And like [13], hypotheses must ultimately be selected and grouped to form symmetric parts. [sent-34, score-0.419]
</p><p>18 But the model we use to assign the affinities, the nature of the search space of hypotheses which are selected and grouped to become parts, and the grouping algorithm that selects and groups hypotheses are different from [13], and represent the three main contributions of this paper. [sent-35, score-0.415]
</p><p>19 In our first contribution, we relax the assumption that a symmetric part is straight with constant width, and extend the model to allow a part to bend and taper. [sent-36, score-0.471]
</p><p>20 Given two adjacent hypotheses, we fit a deformable ellipse to their union, from which an estimate of bending and tapering can be recovered. [sent-37, score-0.571]
</p><p>21 We then warp the image of the union, effectively “undoing” the bending and tapering, and yielding an invariant model of a symmetric part whose axis is straight and whose width is constant. [sent-38, score-0.664]
</p><p>22 By factoring out these deformations, we reduce the variability of the symmetry data used to train the classifier, and allow ourselves to adopt the learn-  ing framework of [13] while accommodating much greater within-class variation. [sent-39, score-0.286]
</p><p>23 In Figure 1(b), we see how this new model can detect curved symmetric parts. [sent-40, score-0.344]
</p><p>24 In our second contribution, we relax the assumption that maximal disc hypotheses can only be grouped within a given scale, and extend the grouping process to integrate the hypotheses from multiple scales. [sent-41, score-0.775]
</p><p>25 In our multiscale approach, we make no assumptions about how the scales relate to each other, nor do we assume that adjacent maximal discs comprising a part must be drawn from adjacent scales. [sent-42, score-0.951]
</p><p>26 We construct a single, integrated search space of deformable discs and assign an affinity to any adjacent or overlapping discs, regardless of which scale they come from. [sent-43, score-0.864]
</p><p>27 This allows superpixels from different scales to be grouped into the same part, an essential requirement for detecting significantly tapered parts, as illustrated in Figure 1(d). [sent-44, score-0.416]
</p><p>28 In our third contribution, we remove the global part symmetry constraint of [13] and relax it with smoothed local symmetry. [sent-45, score-0.372]
</p><p>29 Moreover, we reformulate the problem from a graph segmentation problem to a sequence finding problem, replacing a simple agglomerative clustering algorithm with an optimal grouping algorithm that captures the perceptual grouping principle of good continuation. [sent-46, score-0.519]
</p><p>30 Specifically, we adapt the salient curve detection framework of Felzenszwalb & McAllester [11], in which edgels are grouped to form salient, continuous curves, to group superpixels (hypothesized maximal discs) into salient, continuous, symmetric parts. [sent-47, score-0.821]
</p><p>31 Other classes of approaches have taken a less restrictive approach that first attempts to detect local symmetries, in the form of parts, and then finds nonaccidental groupings of the detected parts to form indexing structures. [sent-56, score-0.233]
</p><p>32 Unfortunately, these filterbased approaches yield many false positive and false negative symmetric part detections, and the lack of explicit part boundary extraction makes part attachment detection unreliable. [sent-60, score-0.554]
</p><p>33 A more powerful filter-based approach was recently proposed by Tsogkas and Kokkinos [36], in which integral images are applied to an edge map to efficiently compute 13 features, including a novel spectral symmetry feature, at each pixel at each of 13 scales. [sent-61, score-0.378]
</p><p>34 But the method shows promise in recovering an approximation to a medial axis transform of an image. [sent-64, score-0.289]
</p><p>35 Some require smooth contours or initialization, while others were designed to detect symmetric objects and cannot detect and group the symmetric parts that make up an asymmetric object. [sent-70, score-0.644]
</p><p>36 A more recent line of methods extract interest point features, such as SIFT [17], and group them across an unknown symmetry axis [18, 12]. [sent-71, score-0.368]
</p><p>37 A recent approach by Narayanan and Kimia [24] proposes an elegant framework for grouping medial fragments into meaningful groups. [sent-73, score-0.391]
</p><p>38 –  –  ground segmentation, the approach computes a shock graph over the entire image of a cluttered scene, and then applies a sequence of medial transforms to the medial fragments, maintaining a large space of grouping hypotheses. [sent-76, score-0.769]
</p><p>39 A Representation for Symmetric Parts Adopting the framework of [13], we define a symmetric part as a sequence of deformable discs, where each deformable disc d is a compact image region (a pixel mask) that roughly corresponds to a maximal inscribed disc. [sent-81, score-1.233]
</p><p>40 Unlike the classical medial axis transform in which maximal inscribed discs are bitangent to the part’s boundary, as shown in Figure 2(a), our deformable discs are not constrained to be circular, and are allowed to deform to the shape of the boundary while maintaining high compactness. [sent-82, score-1.849]
</p><p>41 As a result, the number of deformable discs required to capture the shape of the part is far less than the number required using maximal discs, as shown in Figure 2(b). [sent-83, score-0.897]
</p><p>42 11775555  Superpixels, being compact and having the tendency to  deform to image boundaries, are ideally suited as a model of a deformable disc. [sent-84, score-0.228]
</p><p>43 Therefore, a superpixel segmentation can be seen as a set of deformable disc hypotheses. [sent-85, score-0.486]
</p><p>44 But since we have no a priori knowledge of part scale, and since a tapered part may be captured by deformable discs of different sizes, as shown in Figure 2(c), we generate superpixels (deformable disc hypotheses) at different scales. [sent-86, score-1.408]
</p><p>45 In contrast to [13], whose framework restricted grouping to superpixels at the same scale and thus could not handle significant taper, we group superpixels from a single hypothesis set that combines superpixels from all scales. [sent-87, score-0.789]
</p><p>46 Given a set D of deformable disc hypotheses, our goal is tGo perceptually group rdmefaoblrem adbislce hdyipscost hthesaets belong atol the same part. [sent-89, score-0.46]
</p><p>47 Since the vast majority of superpixels will not correspond to true deformable discs, we must manage the complexity of the search space. [sent-90, score-0.386]
</p><p>48 We adopt a proximity constraint between any two deformable discs and consider grouping together only deformable discs whose underlying superpixels are adjacent or overlapping. [sent-91, score-1.92]
</p><p>49 We thus capture the set D in a graph G, whose nodes di represent dceafpoturrmeab thlee sdeistcs D a innd wah goraspeh edges (hdois, dj) span pairs of deformable discs whose underlying superpixels are adjacent or overlapping. [sent-92, score-1.155]
</p><p>50 Pairs of superpixels in which one superpixel is entirely contained by the other are redundant groupings and are not included as edges in G. [sent-93, score-0.305]
</p><p>51 Each edge is assigned  a symmetry-based affinity gwehsic inh, as d Eeascchri ebeddg ein i sS aecstsiiognn e4d, reflects the degree to which the pair of deformable discs is believed to belong to the same symmetric part. [sent-94, score-1.085]
</p><p>52 Defining a Deformable Disc Affinity A restricted model of symmetry was used in [13], in which the axis was straight and the width was constant along the axis. [sent-96, score-0.459]
</p><p>53 An ellipse was fit to the region defined by two adjacent superpixels, defining a scale- and orientationinvariant coordinate system into which a grid was placed; edgels in the vicinity of the region boundary were then populated into the grid. [sent-97, score-0.503]
</p><p>54 Connected components with high edge affinity yielded symmetric parts. [sent-99, score-0.373]
</p><p>55 To handle curvature and taper in symmetric parts, we relax the model of symmetry by replacing the ellipse with a deformable ellipse that accommodates bending and tapering [26]. [sent-100, score-1.256]
</p><p>56 A deformable ellipse is fit to the boundary of the region, as shown in Figure 3(a). [sent-102, score-0.354]
</p><p>57 The axis is allowed to curve and the width to taper along the axis. [sent-103, score-0.195]
</p><p>58 Figure 3(b) shows edgels in the vicinity of the region boundary to which the model was  (a)  (b)  (c)(d) Figure 3. [sent-104, score-0.249]
</p><p>59 The boundary of the deformed ellipse along with its two axes are shown in red. [sent-106, score-0.193]
</p><p>60 The boundary edgels shown in (b) are warped into W resulting in (c), where the medial axis, shown bold, has been straightened ai nnd ( any taper t rehmeo mveeddia. [sent-107, score-0.506]
</p><p>61 Next, we “undo” the fitted bending and tapering deformations using the warp W that maps the boundary into the invariant coordinate system W as shown in Figure 3(c). [sent-110, score-0.295]
</p><p>62 Finally, boundary edgels are populated ionwton a grid on W 3( ct)o. [sent-111, score-0.253]
</p><p>63 11775566  We compute the shape feature in the invariant coordinate  system W defined by the fitted deformable ellipse. [sent-129, score-0.255]
</p><p>64 The edgels are then warped by W(·; wˆ ) into W, thereby reducing shape variability, i bny particular straightening bthye r symmetry aapxeis aanridremoving taper. [sent-132, score-0.49]
</p><p>65 We train on a set of images containing symmetric parts that are annotated with pixel masks. [sent-139, score-0.367]
</p><p>66 Positive masks are adjacent deformable discs spanning the width of a part, and negative masks oversegment across the width, span over the part boundary, or undersegment the part. [sent-140, score-1.038]
</p><p>67 At each iteration a candidate node was added to a cluster provided that the union of the node and cluster satisfied a global symmetry constraint. [sent-145, score-0.286]
</p><p>68 First, while the global symmetry constraint offered a useful abstraction mechanism, it was enforced by fitting an ellipse to the region corresponding to the entire cluster, thus restricting detectable parts to straight axes. [sent-147, score-0.564]
</p><p>69 The second limitation concerned the grouping algorithm, which was designed for graph clustering rather than for sequence clustering. [sent-148, score-0.259]
</p><p>70 We define ternary otenrdms to o{t b(idnia−r1y , dteir , mdis+ 1{)s}(din=−11 to cover slightly larger subsequences over which a no)ti}on of smoothed local symmetry can be computed. [sent-156, score-0.353]
</p><p>71 This is in contrast to the restrictive global scope over which symmetry was computed in  Figure 4. [sent-157, score-0.317]
</p><p>72 (a) Grouping is formulated as finding a sequence P of adjacent deformable disc hypotheses. [sent-158, score-0.56]
</p><p>73 (b) In the dynamic programming algorithm, a candidate sequence (yellow) is dequeued and possible extending discs (green) are considered. [sent-159, score-0.627]
</p><p>74 h oavveer adapted tthhee method of Felzenszwalb & McAllester [11], which grouped sequences of edgels into salient curves, to our setting in order to group sequences ofdeformable discs into salient symmetric parts. [sent-170, score-1.15]
</p><p>75 r N isot dee tfhinate we yha tvhee assumed an equal contribution to the total cost from each disc regardless of its size. [sent-177, score-0.232]
</p><p>76 At each iteration, the most promising sequence P∗ is dequeued from Q, and new candidate sequences are proposed by extending the end of P∗ with adjacent discs, as shown in Figure 4(b). [sent-182, score-0.243]
</p><p>77 3 over remaining disc hypotheses, and stopping when a maximum cost is reached. [sent-187, score-0.232]
</p><p>78 It is possible for a symmetric part to have a long spiral axis, however we do not expect the length of a sequence to be on the order of the number of superpixels. [sent-189, score-0.375]
</p><p>79 Results Following the criteria in [13] for symmetric part detection, we evaluate the ability of the algorithm to find detection masks corresponding to object parts in a cluttered scene. [sent-193, score-0.517]
</p><p>80 We denote as BSDS-Parts a set of 36 images which are annotated with ground-truth masks corresponding to the symmetric parts of prominent objects (e. [sent-195, score-0.406]
</p><p>81 We also evaluate the contribution of computing symmetry over a 3-disc subsequence as compared to a 2-disc subsequence (“ours w/o smoothing” in Figure 6), and find that the larger scope is beneficial. [sent-212, score-0.286]
</p><p>82 Example (a) demonstrates the successful recovery of a snake along with a second symmetric part representing its shadow (left side of snake). [sent-214, score-0.394]
</p><p>83 The image is over-  laid with detection masks, over which the linear structure of each part is indicated by green line segments that join adjacent deformable discs. [sent-215, score-0.367]
</p><p>84 Most of the plane parts are correctly segmented, with the left wing overpartitioned due to the engine, while the bird’s parts are correctly segmented; configurations of such detected parts can provide a powerful index into a database of part-based shape models. [sent-217, score-0.426]
</p><p>85 In example (d), many of the symmetric parts (both straight and curved) comprising the boat are correctly recovered. [sent-218, score-0.42]
</p><p>86 Examples (e) and (h) show symmetric parts detected in a variety of scenes of intermediate complexity. [sent-219, score-0.397]
</p><p>87 One could imagine a higher-level grouping module that could group the symmetric parts produced by our framework according to principles of collinearity/cocurvilinearity. [sent-223, score-0.551]
</p><p>88 We also note that part recall depends on the ability of the superpixel segmentation algorithm to yield deformable maximal discs that make up a symmetric part. [sent-224, score-1.233]
</p><p>89 Conclusions Symmetry is a powerful regularity in our world that projects to a powerful regularity in the image. [sent-228, score-0.252]
</p><p>90 In the absence of an object prior, symmetry is a powerful cue for detecting parts whose configurations, in turn, can help manage search in a large-scale recognition task. [sent-229, score-0.462]
</p><p>91 The symmetric part detection framework of Levinshtein et al. [sent-230, score-0.359]
</p><p>92 [13] draws on the power of the medial axis while avoiding its pitfalls. [sent-231, score-0.289]
</p><p>93 However, it suffers from some serious limitations that limit its ability to detect more general classes of symmetric parts. [sent-232, score-0.338]
</p><p>94 11775599  ber of extensions to [13], including both a richer yet more flexible model for symmetry, a multiscale framework, and an optimal grouping strategy based on the regularity ofgood continuation. [sent-240, score-0.313]
</p><p>95 The resulting framework significantly outperforms that of [13], offering an improved perceptual grouping framework for recovering symmetric parts without a priori knowledge of scene content. [sent-241, score-0.657]
</p><p>96 In future work, we will address the problem of part grouping to yield part configurations whose relational information offers the discriminative power to prune a large database down to a small number of promising candidates. [sent-242, score-0.355]
</p><p>97 Geometric saliency of curve correspondences and grouping of symmetric contours. [sent-278, score-0.461]
</p><p>98 Computational symmetry in computer vision and computer graphics: A survey. [sent-333, score-0.286]
</p><p>99 Training pdms on models: the case of deformable superellipses. [sent-393, score-0.194]
</p><p>100 Globally optimal grouping for symmetric closed boundaries by combining boundary and region information. [sent-450, score-0.542]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('discs', 0.518), ('symmetry', 0.286), ('symmetric', 0.277), ('disc', 0.232), ('medial', 0.207), ('deformable', 0.194), ('superpixels', 0.192), ('grouping', 0.184), ('edgels', 0.134), ('tapered', 0.132), ('inscribed', 0.113), ('tapering', 0.113), ('ellipse', 0.107), ('maximal', 0.097), ('shokoufandeh', 0.094), ('adjacent', 0.091), ('parts', 0.09), ('hypotheses', 0.089), ('axis', 0.082), ('levinshtein', 0.08), ('perceptual', 0.076), ('taper', 0.075), ('regularity', 0.069), ('curved', 0.067), ('ternary', 0.067), ('shock', 0.067), ('bending', 0.066), ('snake', 0.062), ('affinity', 0.061), ('superpixel', 0.06), ('multiscale', 0.06), ('powerful', 0.057), ('mdet', 0.057), ('mgt', 0.057), ('whd', 0.057), ('part', 0.055), ('straight', 0.053), ('groupings', 0.053), ('grouped', 0.053), ('boundary', 0.053), ('brady', 0.05), ('cham', 0.044), ('sequences', 0.043), ('sequence', 0.043), ('salient', 0.041), ('di', 0.039), ('masks', 0.039), ('scales', 0.039), ('bretzner', 0.038), ('connell', 0.038), ('crowley', 0.038), ('dequeued', 0.038), ('ionwton', 0.038), ('narayanan', 0.038), ('plagues', 0.038), ('ski', 0.038), ('stahl', 0.038), ('superquadrics', 0.038), ('tsogkas', 0.038), ('width', 0.038), ('segmented', 0.037), ('warped', 0.037), ('continuation', 0.036), ('wing', 0.036), ('affinities', 0.036), ('edge', 0.035), ('warp', 0.035), ('deform', 0.034), ('perceptually', 0.034), ('vicinity', 0.034), ('siddiqi', 0.033), ('undersegment', 0.033), ('shape', 0.033), ('axes', 0.033), ('ending', 0.033), ('yield', 0.032), ('graph', 0.032), ('serious', 0.032), ('relax', 0.031), ('span', 0.031), ('cvgip', 0.031), ('lindeberg', 0.031), ('restrictive', 0.031), ('preventing', 0.03), ('leaves', 0.03), ('detected', 0.03), ('priori', 0.03), ('cluttered', 0.029), ('limitations', 0.029), ('blum', 0.029), ('bone', 0.029), ('horse', 0.029), ('whose', 0.029), ('indexing', 0.029), ('region', 0.028), ('fitted', 0.028), ('extending', 0.028), ('editor', 0.028), ('populated', 0.028), ('detection', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="110-tfidf-1" href="./iccv-2013-Detecting_Curved_Symmetric_Parts_Using_a_Deformable_Disc_Model.html">110 iccv-2013-Detecting Curved Symmetric Parts Using a Deformable Disc Model</a></p>
<p>Author: Tom Sie Ho Lee, Sanja Fidler, Sven Dickinson</p><p>Abstract: Symmetry is a powerful shape regularity that’s been exploited by perceptual grouping researchers in both human and computer vision to recover part structure from an image without a priori knowledge of scene content. Drawing on the concept of a medial axis, defined as the locus of centers of maximal inscribed discs that sweep out a symmetric part, we model part recovery as the search for a sequence of deformable maximal inscribed disc hypotheses generated from a multiscale superpixel segmentation, a framework proposed by [13]. However, we learn affinities between adjacent superpixels in a space that’s invariant to bending and tapering along the symmetry axis, enabling us to capture a wider class of symmetric parts. Moreover, we introduce a global cost that perceptually integrates the hypothesis space by combining a pairwise and a higher-level smoothing term, which we minimize globally using dynamic programming. The new framework is demonstrated on two datasets, and is shown to significantly outperform the baseline [13].</p><p>2 0.2008058 <a title="110-tfidf-2" href="./iccv-2013-SYM-FISH%3A_A_Symmetry-Aware_Flip_Invariant_Sketch_Histogram_Shape_Descriptor.html">368 iccv-2013-SYM-FISH: A Symmetry-Aware Flip Invariant Sketch Histogram Shape Descriptor</a></p>
<p>Author: Xiaochun Cao, Hua Zhang, Si Liu, Xiaojie Guo, Liang Lin</p><p>Abstract: Recently, studies on sketch, such as sketch retrieval and sketch classification, have received more attention in the computer vision community. One of its most fundamental and essential problems is how to more effectively describe a sketch image. Many existing descriptors, such as shape context, have achieved great success. In this paper, we propose a new descriptor, namely Symmetric-aware Flip Invariant Sketch Histogram (SYM-FISH) to refine the shape context feature. Its extraction process includes three steps. First the Flip Invariant Sketch Histogram (FISH) descriptor is extracted on the input image, which is a flip-invariant version of the shape context feature. Then we explore the symmetry character of the image by calculating the kurtosis coefficient. Finally, the SYM-FISH is generated by constructing a symmetry table. The new SYM-FISH descriptor supplements the original shape context by encoding the symmetric information, which is a pervasive characteristic of natural scene and objects. We evaluate the efficacy of the novel descriptor in two applications, i.e., sketch retrieval and sketch classification. Extensive experiments on three datasets well demonstrate the effectiveness and robustness of the proposed SYM-FISH descriptor.</p><p>3 0.12981583 <a title="110-tfidf-3" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>Author: Matthias Reso, Jörn Jachalsky, Bodo Rosenhahn, Jörn Ostermann</p><p>Abstract: Superpixel algorithms represent a very useful and increasingly popular preprocessing step for a wide range of computer vision applications, as they offer the potential to boost efficiency and effectiveness. In this regards, this paper presents a highly competitive approach for temporally consistent superpixelsfor video content. The approach is based on energy-minimizing clustering utilizing a novel hybrid clustering strategy for a multi-dimensional feature space working in a global color subspace and local spatial subspaces. Moreover, a new contour evolution based strategy is introduced to ensure spatial coherency of the generated superpixels. For a thorough evaluation the proposed approach is compared to state of the art supervoxel algorithms using established benchmarks and shows a superior performance.</p><p>4 0.1148451 <a title="110-tfidf-4" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>Author: Michael Van_Den_Bergh, Gemma Roig, Xavier Boix, Santiago Manen, Luc Van_Gool</p><p>Abstract: Superpixel and objectness algorithms are broadly used as a pre-processing step to generate support regions and to speed-up further computations. Recently, many algorithms have been extended to video in order to exploit the temporal consistency between frames. However, most methods are computationally too expensive for real-time applications. We introduce an online, real-time video superpixel algorithm based on the recently proposed SEEDS superpixels. A new capability is incorporated which delivers multiple diverse samples (hypotheses) of superpixels in the same image or video sequence. The multiple samples are shown to provide a strong cue to efficiently measure the objectness of image windows, and we introduce the novel concept of objectness in temporal windows. Experiments show that the video superpixels achieve comparable performance to state-of-the-art offline methods while running at 30 fps on a single 2.8 GHz i7 CPU. State-of-the-art performance on objectness is also demonstrated, yet orders of magnitude faster and extended to temporal windows in video.</p><p>5 0.10713009 <a title="110-tfidf-5" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>Author: Abdelaziz Djelouah, Jean-Sébastien Franco, Edmond Boyer, François Le_Clerc, Patrick Pérez</p><p>Abstract: In this paper, we address the problem of object segmentation in multiple views or videos when two or more viewpoints of the same scene are available. We propose a new approach that propagates segmentation coherence information in both space and time, hence allowing evidences in one image to be shared over the complete set. To this aim the segmentation is cast as a single efficient labeling problem over space and time with graph cuts. In contrast to most existing multi-view segmentation methods that rely on some form of dense reconstruction, ours only requires a sparse 3D sampling to propagate information between viewpoints. The approach is thoroughly evaluated on standard multiview datasets, as well as on videos. With static views, results compete with state of the art methods but they are achieved with significantly fewer viewpoints. With multiple videos, we report results that demonstrate the benefit of segmentation propagation through temporal cues.</p><p>6 0.083255462 <a title="110-tfidf-6" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<p>7 0.077782959 <a title="110-tfidf-7" href="./iccv-2013-A_Method_of_Perceptual-Based_Shape_Decomposition.html">21 iccv-2013-A Method of Perceptual-Based Shape Decomposition</a></p>
<p>8 0.072552666 <a title="110-tfidf-8" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>9 0.07099025 <a title="110-tfidf-9" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>10 0.070018366 <a title="110-tfidf-10" href="./iccv-2013-Pose-Configurable_Generic_Tracking_of_Elongated_Objects.html">320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</a></p>
<p>11 0.068728507 <a title="110-tfidf-11" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>12 0.066967197 <a title="110-tfidf-12" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>13 0.065827288 <a title="110-tfidf-13" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>14 0.064083457 <a title="110-tfidf-14" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>15 0.063703544 <a title="110-tfidf-15" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>16 0.063670307 <a title="110-tfidf-16" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>17 0.062383175 <a title="110-tfidf-17" href="./iccv-2013-Revisiting_the_PnP_Problem%3A_A_Fast%2C_General_and_Optimal_Solution.html">353 iccv-2013-Revisiting the PnP Problem: A Fast, General and Optimal Solution</a></p>
<p>18 0.062370423 <a title="110-tfidf-18" href="./iccv-2013-CoDeL%3A_A_Human_Co-detection_and_Labeling_Framework.html">75 iccv-2013-CoDeL: A Human Co-detection and Labeling Framework</a></p>
<p>19 0.062211409 <a title="110-tfidf-19" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>20 0.060425214 <a title="110-tfidf-20" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.154), (1, -0.04), (2, 0.046), (3, -0.019), (4, 0.033), (5, 0.019), (6, -0.037), (7, 0.042), (8, 0.004), (9, -0.073), (10, 0.031), (11, 0.076), (12, -0.029), (13, -0.017), (14, -0.02), (15, 0.039), (16, 0.049), (17, -0.006), (18, -0.02), (19, -0.039), (20, 0.049), (21, -0.005), (22, -0.017), (23, -0.02), (24, -0.014), (25, 0.003), (26, -0.038), (27, 0.01), (28, -0.036), (29, -0.032), (30, -0.003), (31, 0.02), (32, 0.018), (33, 0.047), (34, 0.092), (35, -0.029), (36, 0.082), (37, -0.048), (38, -0.039), (39, -0.112), (40, 0.022), (41, -0.042), (42, -0.032), (43, -0.011), (44, -0.024), (45, -0.04), (46, 0.055), (47, 0.095), (48, -0.044), (49, 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91019064 <a title="110-lsi-1" href="./iccv-2013-Detecting_Curved_Symmetric_Parts_Using_a_Deformable_Disc_Model.html">110 iccv-2013-Detecting Curved Symmetric Parts Using a Deformable Disc Model</a></p>
<p>Author: Tom Sie Ho Lee, Sanja Fidler, Sven Dickinson</p><p>Abstract: Symmetry is a powerful shape regularity that’s been exploited by perceptual grouping researchers in both human and computer vision to recover part structure from an image without a priori knowledge of scene content. Drawing on the concept of a medial axis, defined as the locus of centers of maximal inscribed discs that sweep out a symmetric part, we model part recovery as the search for a sequence of deformable maximal inscribed disc hypotheses generated from a multiscale superpixel segmentation, a framework proposed by [13]. However, we learn affinities between adjacent superpixels in a space that’s invariant to bending and tapering along the symmetry axis, enabling us to capture a wider class of symmetric parts. Moreover, we introduce a global cost that perceptually integrates the hypothesis space by combining a pairwise and a higher-level smoothing term, which we minimize globally using dynamic programming. The new framework is demonstrated on two datasets, and is shown to significantly outperform the baseline [13].</p><p>2 0.67829996 <a title="110-lsi-2" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>Author: Matthias Reso, Jörn Jachalsky, Bodo Rosenhahn, Jörn Ostermann</p><p>Abstract: Superpixel algorithms represent a very useful and increasingly popular preprocessing step for a wide range of computer vision applications, as they offer the potential to boost efficiency and effectiveness. In this regards, this paper presents a highly competitive approach for temporally consistent superpixelsfor video content. The approach is based on energy-minimizing clustering utilizing a novel hybrid clustering strategy for a multi-dimensional feature space working in a global color subspace and local spatial subspaces. Moreover, a new contour evolution based strategy is introduced to ensure spatial coherency of the generated superpixels. For a thorough evaluation the proposed approach is compared to state of the art supervoxel algorithms using established benchmarks and shows a superior performance.</p><p>3 0.64874452 <a title="110-lsi-3" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>Author: Michael Van_Den_Bergh, Gemma Roig, Xavier Boix, Santiago Manen, Luc Van_Gool</p><p>Abstract: Superpixel and objectness algorithms are broadly used as a pre-processing step to generate support regions and to speed-up further computations. Recently, many algorithms have been extended to video in order to exploit the temporal consistency between frames. However, most methods are computationally too expensive for real-time applications. We introduce an online, real-time video superpixel algorithm based on the recently proposed SEEDS superpixels. A new capability is incorporated which delivers multiple diverse samples (hypotheses) of superpixels in the same image or video sequence. The multiple samples are shown to provide a strong cue to efficiently measure the objectness of image windows, and we introduce the novel concept of objectness in temporal windows. Experiments show that the video superpixels achieve comparable performance to state-of-the-art offline methods while running at 30 fps on a single 2.8 GHz i7 CPU. State-of-the-art performance on objectness is also demonstrated, yet orders of magnitude faster and extended to temporal windows in video.</p><p>4 0.6072365 <a title="110-lsi-4" href="./iccv-2013-Semi-supervised_Learning_for_Large_Scale_Image_Cosegmentation.html">383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</a></p>
<p>Author: Zhengxiang Wang, Rujie Liu</p><p>Abstract: This paper introduces to use semi-supervised learning for large scale image cosegmentation. Different from traditional unsupervised cosegmentation that does not use any segmentation groundtruth, semi-supervised cosegmentation exploits the similarity from both the very limited training image foregrounds, as well as the common object shared between the large number of unsegmented images. This would be a much practical way to effectively cosegment a large number of related images simultaneously, where previous unsupervised cosegmentation work poorly due to the large variances in appearance between different images and the lack ofsegmentation groundtruthfor guidance in cosegmentation. For semi-supervised cosegmentation in large scale, we propose an effective method by minimizing an energy function, which consists of the inter-image distance, the intraimage distance and the balance term. We also propose an iterative updating algorithm to efficiently solve this energy function, which decomposes the original energy minimization problem into sub-problems, and updates each image alternatively to reduce the number of variables in each subproblem for computation efficiency. Experiment results on iCoseg and Pascal VOC datasets show that the proposed cosegmentation method can effectively cosegment hundreds of images in less than one minute. And our semi-supervised cosegmentation is able to outperform both unsupervised cosegmentation as well asfully supervised single image segmentation, especially when the training data is limited.</p><p>5 0.59799051 <a title="110-lsi-5" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<p>Author: Abdelaziz Djelouah, Jean-Sébastien Franco, Edmond Boyer, François Le_Clerc, Patrick Pérez</p><p>Abstract: In this paper, we address the problem of object segmentation in multiple views or videos when two or more viewpoints of the same scene are available. We propose a new approach that propagates segmentation coherence information in both space and time, hence allowing evidences in one image to be shared over the complete set. To this aim the segmentation is cast as a single efficient labeling problem over space and time with graph cuts. In contrast to most existing multi-view segmentation methods that rely on some form of dense reconstruction, ours only requires a sparse 3D sampling to propagate information between viewpoints. The approach is thoroughly evaluated on standard multiview datasets, as well as on videos. With static views, results compete with state of the art methods but they are achieved with significantly fewer viewpoints. With multiple videos, we report results that demonstrate the benefit of segmentation propagation through temporal cues.</p><p>6 0.5905897 <a title="110-lsi-6" href="./iccv-2013-A_Method_of_Perceptual-Based_Shape_Decomposition.html">21 iccv-2013-A Method of Perceptual-Based Shape Decomposition</a></p>
<p>7 0.57528895 <a title="110-lsi-7" href="./iccv-2013-SYM-FISH%3A_A_Symmetry-Aware_Flip_Invariant_Sketch_Histogram_Shape_Descriptor.html">368 iccv-2013-SYM-FISH: A Symmetry-Aware Flip Invariant Sketch Histogram Shape Descriptor</a></p>
<p>8 0.56528121 <a title="110-lsi-8" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>9 0.55105424 <a title="110-lsi-9" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>10 0.52751082 <a title="110-lsi-10" href="./iccv-2013-Content-Aware_Rotation.html">90 iccv-2013-Content-Aware Rotation</a></p>
<p>11 0.52491611 <a title="110-lsi-11" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>12 0.50876796 <a title="110-lsi-12" href="./iccv-2013-Discovering_Details_and_Scene_Structure_with_Hierarchical_Iconoid_Shift.html">117 iccv-2013-Discovering Details and Scene Structure with Hierarchical Iconoid Shift</a></p>
<p>13 0.50611126 <a title="110-lsi-13" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>14 0.50607842 <a title="110-lsi-14" href="./iccv-2013-Human_Re-identification_by_Matching_Compositional_Template_with_Cluster_Sampling.html">205 iccv-2013-Human Re-identification by Matching Compositional Template with Cluster Sampling</a></p>
<p>15 0.49837121 <a title="110-lsi-15" href="./iccv-2013-Discriminatively_Trained_Templates_for_3D_Object_Detection%3A_A_Real_Time_Scalable_Approach.html">121 iccv-2013-Discriminatively Trained Templates for 3D Object Detection: A Real Time Scalable Approach</a></p>
<p>16 0.47995302 <a title="110-lsi-16" href="./iccv-2013-Parsing_IKEA_Objects%3A_Fine_Pose_Estimation.html">308 iccv-2013-Parsing IKEA Objects: Fine Pose Estimation</a></p>
<p>17 0.4705022 <a title="110-lsi-17" href="./iccv-2013-Example-Based_Facade_Texture_Synthesis.html">148 iccv-2013-Example-Based Facade Texture Synthesis</a></p>
<p>18 0.47001189 <a title="110-lsi-18" href="./iccv-2013-Progressive_Multigrid_Eigensolvers_for_Multiscale_Spectral_Segmentation.html">329 iccv-2013-Progressive Multigrid Eigensolvers for Multiscale Spectral Segmentation</a></p>
<p>19 0.46493271 <a title="110-lsi-19" href="./iccv-2013-GrabCut_in_One_Cut.html">186 iccv-2013-GrabCut in One Cut</a></p>
<p>20 0.46399629 <a title="110-lsi-20" href="./iccv-2013-BOLD_Features_to_Detect_Texture-less_Objects.html">57 iccv-2013-BOLD Features to Detect Texture-less Objects</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.086), (7, 0.018), (12, 0.017), (13, 0.013), (26, 0.09), (31, 0.041), (34, 0.028), (37, 0.234), (40, 0.015), (42, 0.107), (48, 0.01), (64, 0.037), (73, 0.027), (89, 0.148), (95, 0.013), (98, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.81099641 <a title="110-lda-1" href="./iccv-2013-On_the_Mean_Curvature_Flow_on_Graphs_with_Applications_in_Image_and_Manifold_Processing.html">296 iccv-2013-On the Mean Curvature Flow on Graphs with Applications in Image and Manifold Processing</a></p>
<p>Author: Abdallah El_Chakik, Abderrahim Elmoataz, Ahcene Sadi</p><p>Abstract: In this paper, we propose an adaptation and transcription of the mean curvature level set equation on a general discrete domain (weighted graphs with arbitrary topology). We introduce the perimeters on graph using difference operators and define the curvature as the first variation of these perimeters. Our proposed approach of mean curvature unifies both local and non local notions of mean curvature on Euclidean domains. Furthermore, it allows the extension to the processing of manifolds and data which can be represented by graphs.</p><p>same-paper 2 0.79868174 <a title="110-lda-2" href="./iccv-2013-Detecting_Curved_Symmetric_Parts_Using_a_Deformable_Disc_Model.html">110 iccv-2013-Detecting Curved Symmetric Parts Using a Deformable Disc Model</a></p>
<p>Author: Tom Sie Ho Lee, Sanja Fidler, Sven Dickinson</p><p>Abstract: Symmetry is a powerful shape regularity that’s been exploited by perceptual grouping researchers in both human and computer vision to recover part structure from an image without a priori knowledge of scene content. Drawing on the concept of a medial axis, defined as the locus of centers of maximal inscribed discs that sweep out a symmetric part, we model part recovery as the search for a sequence of deformable maximal inscribed disc hypotheses generated from a multiscale superpixel segmentation, a framework proposed by [13]. However, we learn affinities between adjacent superpixels in a space that’s invariant to bending and tapering along the symmetry axis, enabling us to capture a wider class of symmetric parts. Moreover, we introduce a global cost that perceptually integrates the hypothesis space by combining a pairwise and a higher-level smoothing term, which we minimize globally using dynamic programming. The new framework is demonstrated on two datasets, and is shown to significantly outperform the baseline [13].</p><p>3 0.78646559 <a title="110-lda-3" href="./iccv-2013-A_Rotational_Stereo_Model_Based_on_XSlit_Imaging.html">28 iccv-2013-A Rotational Stereo Model Based on XSlit Imaging</a></p>
<p>Author: Jinwei Ye, Yu Ji, Jingyi Yu</p><p>Abstract: Traditional stereo matching assumes perspective viewing cameras under a translational motion: the second camera is translated away from the first one to create parallax. In this paper, we investigate a different, rotational stereo model on a special multi-perspective camera, the XSlit camera [9, 24]. We show that rotational XSlit (R-XSlit) stereo can be effectively created by fixing the sensor and slit locations but switching the two slits’ directions. We first derive the epipolar geometry of R-XSlit in the 4D light field ray space. Our derivation leads to a simple but effective scheme for locating corresponding epipolar “curves ”. To conduct stereo matching, we further derive a new disparity term in our model and develop a patch-based graph-cut solution. To validate our theory, we assemble an XSlit lens by using a pair of cylindrical lenses coupled with slit-shaped apertures. The XSlit lens can be mounted on commodity cameras where the slit directions are adjustable to form desirable R-XSlit pairs. We show through experiments that R-XSlitprovides apotentially advantageous imaging system for conducting fixed-location, dynamic baseline stereo.</p><p>4 0.69402242 <a title="110-lda-4" href="./iccv-2013-Large-Scale_Multi-resolution_Surface_Reconstruction_from_RGB-D_Sequences.html">228 iccv-2013-Large-Scale Multi-resolution Surface Reconstruction from RGB-D Sequences</a></p>
<p>Author: Frank Steinbrücker, Christian Kerl, Daniel Cremers</p><p>Abstract: We propose a method to generate highly detailed, textured 3D models of large environments from RGB-D sequences. Our system runs in real-time on a standard desktop PC with a state-of-the-art graphics card. To reduce the memory consumption, we fuse the acquired depth maps and colors in a multi-scale octree representation of a signed distance function. To estimate the camera poses, we construct a pose graph and use dense image alignment to determine the relative pose between pairs of frames. We add edges between nodes when we detect loop-closures and optimize the pose graph to correct for long-term drift. Our implementation is highly parallelized on graphics hardware to achieve real-time performance. More specifically, we can reconstruct, store, and continuously update a colored 3D model of an entire corridor of nine rooms at high levels of detail in real-time on a single GPU with 2.5GB.</p><p>5 0.68864644 <a title="110-lda-5" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>Author: S. Karthikeyan, Vignesh Jagadeesh, Renuka Shenoy, Miguel Ecksteinz, B.S. Manjunath</p><p>Abstract: Eye movement studies have confirmed that overt attention is highly biased towards faces and text regions in images. In this paper we explore a novel problem of predicting face and text regions in images using eye tracking data from multiple subjects. The problem is challenging as we aim to predict the semantics (face/text/background) only from eye tracking data without utilizing any image information. The proposed algorithm spatially clusters eye tracking data obtained in an image into different coherent groups and subsequently models the likelihood of the clusters containing faces and text using afully connectedMarkov Random Field (MRF). Given the eye tracking datafrom a test image, itpredicts potential face/head (humans, dogs and cats) and text locations reliably. Furthermore, the approach can be used to select regions of interest for further analysis by object detectors for faces and text. The hybrid eye position/object detector approach achieves better detection performance and reduced computation time compared to using only the object detection algorithm. We also present a new eye tracking dataset on 300 images selected from ICDAR, Street-view, Flickr and Oxford-IIIT Pet Dataset from 15 subjects.</p><p>6 0.68618006 <a title="110-lda-6" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>7 0.68450624 <a title="110-lda-7" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>8 0.68271691 <a title="110-lda-8" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>9 0.68221694 <a title="110-lda-9" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>10 0.68213457 <a title="110-lda-10" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>11 0.68183184 <a title="110-lda-11" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>12 0.68068254 <a title="110-lda-12" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>13 0.68067551 <a title="110-lda-13" href="./iccv-2013-A_Convex_Optimization_Framework_for_Active_Learning.html">6 iccv-2013-A Convex Optimization Framework for Active Learning</a></p>
<p>14 0.68064737 <a title="110-lda-14" href="./iccv-2013-Learning_a_Dictionary_of_Shape_Epitomes_with_Applications_to_Image_Labeling.html">245 iccv-2013-Learning a Dictionary of Shape Epitomes with Applications to Image Labeling</a></p>
<p>15 0.67982024 <a title="110-lda-15" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>16 0.67962492 <a title="110-lda-16" href="./iccv-2013-Learning_Near-Optimal_Cost-Sensitive_Decision_Policy_for_Object_Detection.html">241 iccv-2013-Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection</a></p>
<p>17 0.67961371 <a title="110-lda-17" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>18 0.6794641 <a title="110-lda-18" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>19 0.67924035 <a title="110-lda-19" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>20 0.6790961 <a title="110-lda-20" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
