<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-448" href="#">iccv2013-448</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</h1>
<br/><p>Source: <a title="iccv-2013-448-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Straehle_Weakly_Supervised_Learning_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Christoph Straehle, Ullrich Koethe, Fred A. Hamprecht</p><p>Abstract: We propose a scheme that allows to partition an image into a previously unknown number of segments, using only minimal supervision in terms of a few must-link and cannotlink annotations. We make no use of regional data terms, learning instead what constitutes a likely boundary between segments. Since boundaries are only implicitly specified through cannot-link constraints, this is a hard and nonconvex latent variable problem. We address this problem in a greedy fashion using a randomized decision tree on features associated with interpixel edges. We use a structured purity criterion during tree construction and also show how a backtracking strategy can be used to prevent the greedy search from ending up in poor local optima. The proposed strategy is compared with prior art on natural images.</p><p>Reference: <a title="iccv-2013-448-reference" href="../iccv2013_reference/iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Weakly supervised learning of image partitioning using decision trees with structured split criteria  chri st oph  Christoph Straehle . [sent-1, score-0.819]
</p><p>2 We address this problem in a greedy fashion using a randomized decision tree on features associated with interpixel edges. [sent-14, score-0.845]
</p><p>3 We use a structured purity criterion during tree construction and also show how a backtracking strategy can be used to prevent the greedy search from ending up in poor local optima. [sent-15, score-0.513]
</p><p>4 Introduction This paper describes a new method to learn edge mod-  els from sparse user scribbles marking regions. [sent-18, score-0.537]
</p><p>5 Normally, scribbles as shown on the left would be used to learn region appearance models that can then serve as potential functions in energy-minimizing segmentation methods such as graph cuts. [sent-20, score-0.488]
</p><p>6 Another popular approach would use the scribbles as seeds for a suitable region growing algorithm such as a seeded watershed or random walker. [sent-22, score-0.348]
</p><p>7 When region appearance alone is not informative, segmentation must be based on an edge model. [sent-24, score-0.372]
</p><p>8 Our ambition is to train such a model with minimal labeling effort on the  the user scribbles is treated as an individual label and the decision tree is trained using must-link constraints inside each component and cannot-link constraints between label components. [sent-25, score-1.202]
</p><p>9 The resulting tree learns an edge model consistent with the user-provided constraints and successfully generalizes to the unlabeled part of the image, where it segments many objects successfully. [sent-26, score-0.612]
</p><p>10 Traditional learning methods require the user to place edge scribbles exactly on the desired edges. [sent-28, score-0.537]
</p><p>11 In contrast, we strive to use cheap region scribbles like in Figure 1to train edge models instead of the usual region models. [sent-32, score-0.558]
</p><p>12 Clearly, region scribbles cannot be used for edge learn-  ing directly because they are typically located far away from edges. [sent-33, score-0.514]
</p><p>13 It turns out that these constraints contain sufficient information for successful training of an edge model, and we propose a structured decision tree-type algorithm to solve this problem. [sent-41, score-0.72]
</p><p>14 However, such a simplistic criterion is unsuitable for segmentation quality assessment anyway [30, 13]: It cannot penalize the global consequences (big changes in the resulting connected components) that may be caused by local errors such as a fine gap in an object contour. [sent-43, score-0.343]
</p><p>15 The proposed algorithm recursively builds a decision tree that predicts the state of each edge of the image graph. [sent-47, score-1.012]
</p><p>16 During tree construction we use a non-local split criterion which takes into account the global connectivity consequences of local edge predictions. [sent-48, score-0.916]
</p><p>17 g  To the best of our knowledge, this is the first time a decision tree is trained using a non-local structured split criterion. [sent-50, score-0.929]
</p><p>18 Related Work Previous work on edge learning includes many ap-  proaches which are based on training data with exact boundary localization, e. [sent-53, score-0.346]
</p><p>19 The author learns an edge model from inaccurate boundary annotations. [sent-59, score-0.346]
</p><p>20 The decision tree based edge learning algorithm that we propose is related to [25]. [sent-66, score-0.908]
</p><p>21 The authors learn edge on/off probabilities using a decision tree, but the method requires a dense labeling as input and does not take the effect on the connected components of the graph into account it acts locally. [sent-67, score-1.013]
</p><p>22 Our special split criterion is inspired for example by [15] where a special loss function in the split nodes of a decision tree is optimized. [sent-68, score-1.23]
</p><p>23 The authors of [20] have introduced a decision tree algorithm that works on locally structured labels. [sent-70, score-0.722]
</p><p>24 In [21] a decision tree is proposed whose intermediate learning state is used as a feature for further tree growing. [sent-72, score-1.051]
</p><p>25 This is the basis for our proposed algorithm which evaluates a structured split criterion with regard to the intermediate tree state. [sent-73, score-0.615]
</p><p>26 These algorithms partition an image graph into connected components using said constraints. [sent-75, score-0.378]
</p><p>27 Both partitioning algorithm use a single scalar value associated with each edge whereas our method can take a multitude of edge features into account and learns an edge model from the given constraints. [sent-76, score-0.84]
</p><p>28 Problem Definition and Objective function We consider a segmentation problem defined on a graph  ××  G(E, V) in which the nodes ni ∈ V correspond to the pixGels( Eo,fV an image, ahn tdh eth neo edges (i∈, j V) c∈o rEr correspond teo p tihxepixel neighborhood do tfh teh eed image. [sent-81, score-0.474]
</p><p>29 jW)e ∈ assume a spounitadb tloe tsheet of edge features wijf (such as color gradients or structure 11885500  tensor eigenvalues on different scales) is available and can be attributed to each edge (i, j). [sent-82, score-0.519]
</p><p>30 The decision variables xij ∈ {0, 1} determine whe=the −r an edge (i, j) is removed from (xij ,=1 0}), d or rrmeminaiens w (xij e=r a1n) in the graph G. [sent-84, score-0.972]
</p><p>31 The objective function F(c, x) which we seek to maximize depends on the set of constraints c and the connected components or partitions π(x) implied by the binary edge indicator variables xij : F(c, π(x)) = TP  TN  −  FP  FN  ? [sent-85, score-0.81]
</p><p>32 Thus, in the optimum ˆx = argmaxx F(c, π(x)), the binary indicator vector ˆx corresponds to a partitioning of the graph into a set of connected components π(x) that satisfy the constraint matrix. [sent-89, score-0.472]
</p><p>33 Note that the number of connected components defined by π(x) can be larger than the number of components defined by the constraints c: the unconstrained pixels of the image can be partitioned in many different ways. [sent-90, score-0.375]
</p><p>34 Overview: trees  global optimization using decision  The objective function F(c, π(x)) defines a global objective which depends on the structure of the graph G. [sent-93, score-0.688]
</p><p>35 Maximizing F may be a simple task when the constraint set is very small: many different possible edge assignments x may partition the graph correctly. [sent-94, score-0.461]
</p><p>36 The objective function is highly non-smooth and non-convex: switching a single binary indicator variable xij for example on the boundary between two objects may have a huge influence on the value of F. [sent-96, score-0.389]
</p><p>37 We have chosen to optimize Equation 1 using a greedy method inspired by traditional decision trees. [sent-98, score-0.41]
</p><p>38 Decision trees are constructed in the following fashion: Given a set of examples Si associated with a decision tree node i, a tree is built starting at the root node 0 by partitioning the set of examples into two subsets SL and SR. [sent-99, score-1.678]
</p><p>39 The decision how the set Si is partitioned depends on the parameters θi for node i and the features of the samples. [sent-100, score-0.642]
</p><p>40 Important is the purely local dependency of the split functions that are usually used the function which is optimized only depends on the split parameters θi of the node ithat is optimized and the set of training examples Si over which this node optimizes and their associated features. [sent-103, score-1.055]
</p><p>41 Intuitively speaking, in each node we optimize the split parameters θi of that node, given all split decisions of all other nodes at the current state of the tree. [sent-109, score-0.854]
</p><p>42 , θi−1)  In our case we seek to optimize the objective function F over the connected components of a graph G obeying constraints on pairs of nodes. [sent-113, score-0.445]
</p><p>43 The samples and features of the decision tree consist of edges and their associated features on this graph. [sent-114, score-0.903]
</p><p>44 Tree construction starts by trying to satisfy as many of the given constraints as possible by thresholding a single feature and thereby splitting the edges of the graph into one set that is removed from and another one that remains in the graph. [sent-118, score-0.486]
</p><p>45 The decision on the split parameters θi of node iis optimized by sorting the edges Si associated with decision tree node ion mtry different feature values. [sent-119, score-2.048]
</p><p>46 For each of the mtry features all possible split points are evaluated by first removing all edges Si associated with the decision tree node from the graph G. [sent-120, score-1.517]
</p><p>47 It is important to realize that only the edges of the currently considered split node are removed, the presence and absence of all other edges, as defined by the current state of the decision tree, remains unchanged. [sent-121, score-1.063]
</p><p>48 In a second step, the edges associated with split node iare re-inserted into the graph one after the other in the order of increasing feature weight. [sent-122, score-0.792]
</p><p>49 After each edge insertion, its effect on the connected com-  ponents of the graph is efficiently evaluated using a union 118855 11  Figure 2. [sent-123, score-0.465]
</p><p>50 Starting at the root node (1) the edges are partitioned into two sets, one of which is assigned xij = 1 (thick, edges stays in the graph) while the edges of the other partition are assigned xij = 0 (thin, edges are removed from the graph). [sent-125, score-1.613]
</p><p>51 In the next steps this initial decision on the edge states is revised by partitioning the two edge sets recursively further into an on  and off  components  set  such that the objective function F(x, c) is maximized. [sent-126, score-1.098]
</p><p>52 The value of the objective function depends on the connected G that are induced by the edge state defined in the leaf nodes. [sent-127, score-0.636]
</p><p>53 The connected components are distinguished by  of the graph  the node colors. [sent-128, score-0.55]
</p><p>54 Using the TP, FP, TN and FN counts we compute the value of the objective function F and remember the best split position that we encountered while inserting the edges into the graph. [sent-131, score-0.517]
</p><p>55 After determining the feature and split position that yield the highest objective function value, two child nodes are added to the currently considered node iand the split parameters θi of the node are set accordingly. [sent-133, score-1.086]
</p><p>56 These two child nodes determine the new state of their associated edges until they are further refined in a recursive fashion. [sent-134, score-0.459]
</p><p>57 Splitting a node and the associated edge set further thus re-optimizes the state ofthe edges associated with that node: the final leaf node with which an edge is associated defines  the edge state within the tree. [sent-136, score-1.808]
</p><p>58 It is important to see that during this recursive partitioning the optimization in each leaf node involves only the edges associated with that particular node. [sent-138, score-0.744]
</p><p>59 The state of the other edges is determined by the already existing leaf nodes and is assumed fixed. [sent-139, score-0.485]
</p><p>60 Thus each leaf node is optimized conditioned on the graph state given by the current complete decision tree. [sent-140, score-0.978]
</p><p>61 While the insertion of edges and its effect on the connected components of the graph can be computed very efficiently, handling edge removal is more difficult. [sent-141, score-0.827]
</p><p>62 Handling edge removal requires either extremely intricate algorithms [27] or a linear scan over possibly all edges in G even though the removed edge set is very small. [sent-142, score-0.691]
</p><p>63 For this reason we compute the connected components of the graph a single time once all edges associated with a decision tree node are removed. [sent-143, score-1.453]
</p><p>64 We then trace all changes to the union find data structure and to the TP, FP, TN and FN pair counts caused by inserting an edge into the graph when we test for a split position. [sent-144, score-0.622]
</p><p>65 This allows us to unwind all changes once the objective function has been evaluated for all split points along one feature and to efficiently begin testing for better splits using the next feature without recomputing the connected component state from scratch. [sent-145, score-0.496]
</p><p>66 For these reasons, we propose a novel backtracking scheme during decision tree building: we allow for split nodes to be inserted at arbitrary positions in the tree, not only at the leaves of the tree. [sent-151, score-1.137]
</p><p>67 Since we allow these nodes to be inserted at any tree level, these split nodes can optimize over a larger set of edges com–  –  11885522  Figure 3. [sent-152, score-0.974]
</p><p>68 First a random subtree of the decision tree is selected (green overlay). [sent-154, score-0.82]
</p><p>69 Then a split node is inserted above this subtree whose split function is optimized under the assumption that the left partition below the inserted node is passed onto the existing subtree, while the right partition is passed to a new leaf node and is assigned either to 0 or 1. [sent-155, score-1.997]
</p><p>70 The insertion of a split at a higher tree level effectively optimizes over a larger set of samples compared to adding a split ad a leaf node. [sent-156, score-1.011]
</p><p>71 The insertion split takes away some samples from an existing part of a decision tree and overrides the existing subtree partially. [sent-157, score-1.135]
</p><p>72 In the extreme case of inserting such a node above the current root of the tree, the new node can reoptimize over all edges of the graph. [sent-159, score-0.728]
</p><p>73 This novel insertion split partitions the edge set arriving at an inserted node into two parts, such that the left partition is passed to the already existing subtree below the inserted split node as before while the right partition is either assigned to xij = 0 or xij = 1. [sent-160, score-2.303]
</p><p>74 Thus an existing learned combination of rules, defined by the subtree below the inserted node, is partially reused and the decision for a subset of the edges below/above a feature threshold is reconsid-  ered. [sent-161, score-0.795]
</p><p>75 The optimization of an inner node of the decision tree is executed in the same manner as already described for a leaf node. [sent-162, score-1.065]
</p><p>76 The only difference is that when scanning over the edges in the partition in increasing/decreasing feature weight order, not all edges are re-inserted into the graph. [sent-163, score-0.424]
</p><p>77 Instead, the current state of the edge xij which is determined by the subtree of the node currently under consideration is used as a mask. [sent-164, score-0.903]
</p><p>78 In a first trial only edges with current state xij = 1are re-inserted. [sent-165, score-0.458]
</p><p>79 This corresponds to overriding the subtree for the edges right (increasing sort order) / left (decreasing sort order) of the split position with a xij = 0 assignment. [sent-166, score-0.858]
</p><p>80 In a second trial, only the edges with xij = 0 are removed from the graph before inserting all edges in increasing/decreasing order. [sent-167, score-0.792]
</p><p>81 This corresponds to overriding the subtree for some edges with a xij = 1assignment left or right of the split position. [sent-168, score-0.792]
</p><p>82 Decision tree prediction algorithm Once a decision tree has been built in the described manner, it can be used to determine a segmentation of the graph by predicting the binary indicator xij for all edges. [sent-171, score-1.4]
</p><p>83 Prediction proceeds as in any normal decision tree: samples (in our case edges (i, j)) are passed down the tree, starting from the root node 0 by comparing the value wijf of edge feature f with the split value that is stored in a tree node. [sent-172, score-1.65]
</p><p>84 Once a leaf node is  reached, the x label of this leaf node is assigned to the edge. [sent-174, score-0.828]
</p><p>85 Now all edges with xij = 1 are switched on in the graph and its connected components are determined. [sent-175, score-0.696]
</p><p>86 In an unsupervised segmentation setting, the resulting connected components of the graph are the final output. [sent-176, score-0.402]
</p><p>87 The examples with homogenous boundary appearance (cells, neural tissue) do not profit from more decision tree levels - the tree starts to 11885533  segmented with the same parameter settings. [sent-197, score-1.207]
</p><p>88 This ovefitting behaviour of single decision trees is well known. [sent-204, score-0.442]
</p><p>89 The plots show the training and testing score over the decision tree depth. [sent-208, score-0.674]
</p><p>90 The examples with homogenous boundary appearance (cells, neural tissue) do not profit from more decision tree levels - the tree starts to overfit after the first level. [sent-210, score-1.207]
</p><p>91 ow that our purely edge based decision tree achieves a segmentation quality that surpasses many established methods, without learning local class probabilities (unary potentials describing region appearance) from the user labels. [sent-222, score-1.17]
</p><p>92 Our method exclusively relies on the edge probabilities learned from sparse user scribbles. [sent-226, score-0.358]
</p><p>93 to learn an edge model from sparse region scribbles interpreted as  must-link and cannot-link constraints. [sent-230, score-0.514]
</p><p>94 To solve this problem, a novel global structured learning scheme based on decision trees is introduced. [sent-231, score-0.49]
</p><p>95 We explain how decision trees can be trained using a global structured loss criterion and show how they can be used to learn an edge model on an image graph. [sent-232, score-0.777]
</p><p>96 In addition, we show how local minima during tree construction can be overcome by a split node insertion that reuses the already learned decision structure. [sent-233, score-1.231]
</p><p>97 Interactive graph cuts for optimal boundary & region segmentation of objects in nd images. [sent-272, score-0.364]
</p><p>98 Regression tree fieldsan efficient, non-parametric approach to image labeling problems. [sent-358, score-0.354]
</p><p>99 Our decision tree iteratively partitions the image graph into connected components (indicated by same color). [sent-401, score-1.016]
</p><p>100 In each tree level (1,2,3) the decisions are refined such that a global objective function over the image graph is optimized that enforces the pairwise connectivity and exclusion constraints that are implicitly defined by the labels. [sent-402, score-0.669]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('decision', 0.367), ('tree', 0.307), ('node', 0.242), ('scribbles', 0.236), ('edge', 0.234), ('xij', 0.211), ('split', 0.207), ('edges', 0.177), ('leaf', 0.149), ('subtree', 0.146), ('connected', 0.117), ('graph', 0.114), ('boundary', 0.112), ('insertion', 0.108), ('inserted', 0.105), ('segmentation', 0.094), ('nodes', 0.089), ('partitioning', 0.086), ('profit', 0.079), ('consequences', 0.079), ('fn', 0.078), ('tp', 0.077), ('components', 0.077), ('cannotlink', 0.076), ('interpixel', 0.076), ('trees', 0.075), ('nowozin', 0.074), ('constraints', 0.071), ('partition', 0.07), ('state', 0.07), ('iwr', 0.068), ('seeded', 0.068), ('cfi', 0.068), ('user', 0.067), ('inserting', 0.067), ('fp', 0.066), ('objective', 0.066), ('passed', 0.065), ('lbe', 0.062), ('backtracking', 0.062), ('interactive', 0.059), ('probabilities', 0.057), ('tn', 0.056), ('criterion', 0.053), ('associated', 0.052), ('closedness', 0.051), ('helmstaedter', 0.051), ('inhomogenous', 0.051), ('kiwis', 0.051), ('mtry', 0.051), ('overriding', 0.051), ('preibisch', 0.051), ('saalfeld', 0.051), ('wijf', 0.051), ('structured', 0.048), ('labeling', 0.047), ('removed', 0.046), ('assigned', 0.046), ('fred', 0.045), ('tomancak', 0.045), ('cardona', 0.045), ('apples', 0.045), ('briggman', 0.045), ('denk', 0.045), ('region', 0.044), ('constraint', 0.043), ('rg', 0.043), ('greedy', 0.043), ('splitting', 0.043), ('ide', 0.042), ('plos', 0.042), ('linking', 0.041), ('decisions', 0.039), ('boundaries', 0.039), ('lhi', 0.039), ('jancsary', 0.039), ('electron', 0.039), ('supervision', 0.039), ('si', 0.039), ('sr', 0.039), ('recursive', 0.038), ('path', 0.038), ('microscopy', 0.037), ('cut', 0.036), ('supervised', 0.036), ('isolated', 0.036), ('connectivity', 0.036), ('optimized', 0.036), ('tissue', 0.036), ('component', 0.036), ('satisfy', 0.035), ('homogenous', 0.035), ('kontschieder', 0.035), ('recursively', 0.034), ('partitions', 0.034), ('sort', 0.033), ('child', 0.033), ('optimizes', 0.033), ('partitioned', 0.033), ('sl', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000011 <a title="448-tfidf-1" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>Author: Christoph Straehle, Ullrich Koethe, Fred A. Hamprecht</p><p>Abstract: We propose a scheme that allows to partition an image into a previously unknown number of segments, using only minimal supervision in terms of a few must-link and cannotlink annotations. We make no use of regional data terms, learning instead what constitutes a likely boundary between segments. Since boundaries are only implicitly specified through cannot-link constraints, this is a hard and nonconvex latent variable problem. We address this problem in a greedy fashion using a randomized decision tree on features associated with interpixel edges. We use a structured purity criterion during tree construction and also show how a backtracking strategy can be used to prevent the greedy search from ending up in poor local optima. The proposed strategy is compared with prior art on natural images.</p><p>2 0.30579615 <a title="448-tfidf-2" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>Author: Piotr Dollár, C. Lawrence Zitnick</p><p>Abstract: Edge detection is a critical component of many vision systems, including object detectors and image segmentation algorithms. Patches of edges exhibit well-known forms of local structure, such as straight lines or T-junctions. In this paper we take advantage of the structure present in local image patches to learn both an accurate and computationally efficient edge detector. We formulate the problem of predicting local edge masks in a structured learning framework applied to random decision forests. Our novel approach to learning decision trees robustly maps the structured labels to a discrete space on which standard information gain measures may be evaluated. The result is an approach that obtains realtime performance that is orders of magnitude faster than many competing state-of-the-art approaches, while also achieving state-of-the-art edge detection results on the BSDS500 Segmentation dataset and NYU Depth dataset. Finally, we show the potential of our approach as a general purpose edge detector by showing our learned edge models generalize well across datasets.</p><p>3 0.18215375 <a title="448-tfidf-3" href="./iccv-2013-Find_the_Best_Path%3A_An_Efficient_and_Accurate_Classifier_for_Image_Hierarchies.html">165 iccv-2013-Find the Best Path: An Efficient and Accurate Classifier for Image Hierarchies</a></p>
<p>Author: Min Sun, Wan Huang, Silvio Savarese</p><p>Abstract: Many methods have been proposed to solve the image classification problem for a large number of categories. Among them, methods based on tree-based representations achieve good trade-off between accuracy and test time efficiency. While focusing on learning a tree-shaped hierarchy and the corresponding set of classifiers, most of them [11, 2, 14] use a greedy prediction algorithm for test time efficiency. We argue that the dramatic decrease in accuracy at high efficiency is caused by the specific design choice of the learning and greedy prediction algorithms. In this work, we propose a classifier which achieves a better trade-off between efficiency and accuracy with a given tree-shaped hierarchy. First, we convert the classification problem as finding the best path in the hierarchy, and a novel branchand-bound-like algorithm is introduced to efficiently search for the best path. Second, we jointly train the classifiers using a novel Structured SVM (SSVM) formulation with additional bound constraints. As a result, our method achieves a significant 4.65%, 5.43%, and 4.07% (relative 24.82%, 41.64%, and 109.79%) improvement in accuracy at high efficiency compared to state-of-the-art greedy “tree-based” methods [14] on Caltech-256 [15], SUN [32] and ImageNet 1K [9] dataset, respectively. Finally, we show that our branch-and-bound-like algorithm naturally ranks the paths in the hierarchy (Fig. 8) so that users can further process them.</p><p>4 0.17451163 <a title="448-tfidf-4" href="./iccv-2013-Tree_Shape_Priors_with_Connectivity_Constraints_Using_Convex_Relaxation_on_General_Graphs.html">429 iccv-2013-Tree Shape Priors with Connectivity Constraints Using Convex Relaxation on General Graphs</a></p>
<p>Author: Jan Stühmer, Peter Schröder, Daniel Cremers</p><p>Abstract: We propose a novel method to include a connectivity prior into image segmentation that is based on a binary labeling of a directed graph, in this case a geodesic shortest path tree. Specifically we make two contributions: First, we construct a geodesic shortest path tree with a distance measure that is related to the image data and the bending energy of each path in the tree. Second, we include a connectivity prior in our segmentation model, that allows to segment not only a single elongated structure, but instead a whole connected branching tree. Because both our segmentation model and the connectivity constraint are convex, a global optimal solution can be found. To this end, we generalize a recent primal-dual algorithm for continuous convex optimization to an arbitrary graph structure. To validate our method we present results on data from medical imaging in angiography and retinal blood vessel segmentation.</p><p>5 0.17002298 <a title="448-tfidf-5" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<p>Author: Kevin Tang, Bangpeng Yao, Li Fei-Fei, Daphne Koller</p><p>Abstract: In this paper, we tackle the problem of combining features extracted from video for complex event recognition. Feature combination is an especially relevant task in video data, as there are many features we can extract, ranging from image features computed from individual frames to video features that take temporal information into account. To combine features effectively, we propose a method that is able to be selective of different subsets of features, as some features or feature combinations may be uninformative for certain classes. We introduce a hierarchical method for combining features based on the AND/OR graph structure, where nodes in the graph represent combinations of different sets of features. Our method automatically learns the structure of the AND/OR graph using score-based structure learning, and we introduce an inference procedure that is able to efficiently compute structure scores. We present promising results and analysis on the difficult and large-scale 2011 TRECVID Multimedia Event Detection dataset [17].</p><p>6 0.16868412 <a title="448-tfidf-6" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>7 0.16526553 <a title="448-tfidf-7" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>8 0.15416652 <a title="448-tfidf-8" href="./iccv-2013-Random_Forests_of_Local_Experts_for_Pedestrian_Detection.html">336 iccv-2013-Random Forests of Local Experts for Pedestrian Detection</a></p>
<p>9 0.12848043 <a title="448-tfidf-9" href="./iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps.html">208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</a></p>
<p>10 0.12543465 <a title="448-tfidf-10" href="./iccv-2013-Learning_Near-Optimal_Cost-Sensitive_Decision_Policy_for_Object_Detection.html">241 iccv-2013-Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection</a></p>
<p>11 0.1230097 <a title="448-tfidf-11" href="./iccv-2013-A_Fully_Hierarchical_Approach_for_Finding_Correspondences_in_Non-rigid_Shapes.html">11 iccv-2013-A Fully Hierarchical Approach for Finding Correspondences in Non-rigid Shapes</a></p>
<p>12 0.12049212 <a title="448-tfidf-12" href="./iccv-2013-Improving_Graph_Matching_via_Density_Maximization.html">214 iccv-2013-Improving Graph Matching via Density Maximization</a></p>
<p>13 0.11893793 <a title="448-tfidf-13" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>14 0.11821365 <a title="448-tfidf-14" href="./iccv-2013-Alternating_Regression_Forests_for_Object_Detection_and_Pose_Estimation.html">47 iccv-2013-Alternating Regression Forests for Object Detection and Pose Estimation</a></p>
<p>15 0.11756765 <a title="448-tfidf-15" href="./iccv-2013-Learning_Graph_Matching%3A_Oriented_to_Category_Modeling_from_Cluttered_Scenes.html">237 iccv-2013-Learning Graph Matching: Oriented to Category Modeling from Cluttered Scenes</a></p>
<p>16 0.11637737 <a title="448-tfidf-16" href="./iccv-2013-Revisiting_Example_Dependent_Cost-Sensitive_Learning_with_Decision_Trees.html">352 iccv-2013-Revisiting Example Dependent Cost-Sensitive Learning with Decision Trees</a></p>
<p>17 0.11613818 <a title="448-tfidf-17" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>18 0.11370052 <a title="448-tfidf-18" href="./iccv-2013-From_Large_Scale_Image_Categorization_to_Entry-Level_Categories.html">176 iccv-2013-From Large Scale Image Categorization to Entry-Level Categories</a></p>
<p>19 0.11215331 <a title="448-tfidf-19" href="./iccv-2013-Discriminative_Label_Propagation_for_Multi-object_Tracking_with_Sporadic_Appearance_Features.html">120 iccv-2013-Discriminative Label Propagation for Multi-object Tracking with Sporadic Appearance Features</a></p>
<p>20 0.10746416 <a title="448-tfidf-20" href="./iccv-2013-Potts_Model%2C_Parametric_Maxflow_and_K-Submodular_Functions.html">324 iccv-2013-Potts Model, Parametric Maxflow and K-Submodular Functions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.231), (1, 0.022), (2, -0.002), (3, -0.05), (4, 0.082), (5, 0.049), (6, -0.1), (7, 0.056), (8, 0.061), (9, -0.152), (10, -0.106), (11, 0.041), (12, 0.002), (13, 0.1), (14, 0.129), (15, 0.204), (16, -0.013), (17, -0.167), (18, -0.066), (19, 0.099), (20, -0.073), (21, 0.021), (22, -0.061), (23, 0.082), (24, -0.056), (25, -0.012), (26, -0.093), (27, 0.005), (28, 0.039), (29, -0.126), (30, 0.125), (31, 0.045), (32, -0.083), (33, 0.006), (34, -0.118), (35, 0.129), (36, -0.079), (37, 0.01), (38, 0.015), (39, -0.066), (40, -0.049), (41, -0.015), (42, 0.003), (43, 0.021), (44, -0.014), (45, -0.02), (46, -0.087), (47, -0.036), (48, -0.016), (49, -0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97692651 <a title="448-lsi-1" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>Author: Christoph Straehle, Ullrich Koethe, Fred A. Hamprecht</p><p>Abstract: We propose a scheme that allows to partition an image into a previously unknown number of segments, using only minimal supervision in terms of a few must-link and cannotlink annotations. We make no use of regional data terms, learning instead what constitutes a likely boundary between segments. Since boundaries are only implicitly specified through cannot-link constraints, this is a hard and nonconvex latent variable problem. We address this problem in a greedy fashion using a randomized decision tree on features associated with interpixel edges. We use a structured purity criterion during tree construction and also show how a backtracking strategy can be used to prevent the greedy search from ending up in poor local optima. The proposed strategy is compared with prior art on natural images.</p><p>2 0.80757427 <a title="448-lsi-2" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>Author: Piotr Dollár, C. Lawrence Zitnick</p><p>Abstract: Edge detection is a critical component of many vision systems, including object detectors and image segmentation algorithms. Patches of edges exhibit well-known forms of local structure, such as straight lines or T-junctions. In this paper we take advantage of the structure present in local image patches to learn both an accurate and computationally efficient edge detector. We formulate the problem of predicting local edge masks in a structured learning framework applied to random decision forests. Our novel approach to learning decision trees robustly maps the structured labels to a discrete space on which standard information gain measures may be evaluated. The result is an approach that obtains realtime performance that is orders of magnitude faster than many competing state-of-the-art approaches, while also achieving state-of-the-art edge detection results on the BSDS500 Segmentation dataset and NYU Depth dataset. Finally, we show the potential of our approach as a general purpose edge detector by showing our learned edge models generalize well across datasets.</p><p>3 0.70475489 <a title="448-lsi-3" href="./iccv-2013-Find_the_Best_Path%3A_An_Efficient_and_Accurate_Classifier_for_Image_Hierarchies.html">165 iccv-2013-Find the Best Path: An Efficient and Accurate Classifier for Image Hierarchies</a></p>
<p>Author: Min Sun, Wan Huang, Silvio Savarese</p><p>Abstract: Many methods have been proposed to solve the image classification problem for a large number of categories. Among them, methods based on tree-based representations achieve good trade-off between accuracy and test time efficiency. While focusing on learning a tree-shaped hierarchy and the corresponding set of classifiers, most of them [11, 2, 14] use a greedy prediction algorithm for test time efficiency. We argue that the dramatic decrease in accuracy at high efficiency is caused by the specific design choice of the learning and greedy prediction algorithms. In this work, we propose a classifier which achieves a better trade-off between efficiency and accuracy with a given tree-shaped hierarchy. First, we convert the classification problem as finding the best path in the hierarchy, and a novel branchand-bound-like algorithm is introduced to efficiently search for the best path. Second, we jointly train the classifiers using a novel Structured SVM (SSVM) formulation with additional bound constraints. As a result, our method achieves a significant 4.65%, 5.43%, and 4.07% (relative 24.82%, 41.64%, and 109.79%) improvement in accuracy at high efficiency compared to state-of-the-art greedy “tree-based” methods [14] on Caltech-256 [15], SUN [32] and ImageNet 1K [9] dataset, respectively. Finally, we show that our branch-and-bound-like algorithm naturally ranks the paths in the hierarchy (Fig. 8) so that users can further process them.</p><p>4 0.68493736 <a title="448-lsi-4" href="./iccv-2013-Alternating_Regression_Forests_for_Object_Detection_and_Pose_Estimation.html">47 iccv-2013-Alternating Regression Forests for Object Detection and Pose Estimation</a></p>
<p>Author: Samuel Schulter, Christian Leistner, Paul Wohlhart, Peter M. Roth, Horst Bischof</p><p>Abstract: We present Alternating Regression Forests (ARFs), a novel regression algorithm that learns a Random Forest by optimizing a global loss function over all trees. This interrelates the information of single trees during the training phase and results in more accurate predictions. ARFs can minimize any differentiable regression loss without sacrificing the appealing properties of Random Forests, like low computational complexity during both, training and testing. Inspired by recent developments for classification [19], we derive a new algorithm capable of dealing with different regression loss functions, discuss its properties and investigate the relations to other methods like Boosted Trees. We evaluate ARFs on standard machine learning benchmarks, where we observe better generalization power compared to both standard Random Forests and Boosted Trees. Moreover, we apply the proposed regressor to two computer vision applications: object detection and head pose estimation from depth images. ARFs outperform the Random Forest baselines in both tasks, illustrating the importance of optimizing a common loss function for all trees.</p><p>5 0.67799407 <a title="448-lsi-5" href="./iccv-2013-Tree_Shape_Priors_with_Connectivity_Constraints_Using_Convex_Relaxation_on_General_Graphs.html">429 iccv-2013-Tree Shape Priors with Connectivity Constraints Using Convex Relaxation on General Graphs</a></p>
<p>Author: Jan Stühmer, Peter Schröder, Daniel Cremers</p><p>Abstract: We propose a novel method to include a connectivity prior into image segmentation that is based on a binary labeling of a directed graph, in this case a geodesic shortest path tree. Specifically we make two contributions: First, we construct a geodesic shortest path tree with a distance measure that is related to the image data and the bending energy of each path in the tree. Second, we include a connectivity prior in our segmentation model, that allows to segment not only a single elongated structure, but instead a whole connected branching tree. Because both our segmentation model and the connectivity constraint are convex, a global optimal solution can be found. To this end, we generalize a recent primal-dual algorithm for continuous convex optimization to an arbitrary graph structure. To validate our method we present results on data from medical imaging in angiography and retinal blood vessel segmentation.</p><p>6 0.67475295 <a title="448-lsi-6" href="./iccv-2013-Revisiting_Example_Dependent_Cost-Sensitive_Learning_with_Decision_Trees.html">352 iccv-2013-Revisiting Example Dependent Cost-Sensitive Learning with Decision Trees</a></p>
<p>7 0.63752145 <a title="448-lsi-7" href="./iccv-2013-Potts_Model%2C_Parametric_Maxflow_and_K-Submodular_Functions.html">324 iccv-2013-Potts Model, Parametric Maxflow and K-Submodular Functions</a></p>
<p>8 0.63129765 <a title="448-lsi-8" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<p>9 0.6122027 <a title="448-lsi-9" href="./iccv-2013-Random_Forests_of_Local_Experts_for_Pedestrian_Detection.html">336 iccv-2013-Random Forests of Local Experts for Pedestrian Detection</a></p>
<p>10 0.59882379 <a title="448-lsi-10" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>11 0.58445823 <a title="448-lsi-11" href="./iccv-2013-A_Fully_Hierarchical_Approach_for_Finding_Correspondences_in_Non-rigid_Shapes.html">11 iccv-2013-A Fully Hierarchical Approach for Finding Correspondences in Non-rigid Shapes</a></p>
<p>12 0.58085656 <a title="448-lsi-12" href="./iccv-2013-From_Large_Scale_Image_Categorization_to_Entry-Level_Categories.html">176 iccv-2013-From Large Scale Image Categorization to Entry-Level Categories</a></p>
<p>13 0.56892449 <a title="448-lsi-13" href="./iccv-2013-Unsupervised_Random_Forest_Manifold_Alignment_for_Lipreading.html">437 iccv-2013-Unsupervised Random Forest Manifold Alignment for Lipreading</a></p>
<p>14 0.54759091 <a title="448-lsi-14" href="./iccv-2013-A_Deformable_Mixture_Parsing_Model_with_Parselets.html">8 iccv-2013-A Deformable Mixture Parsing Model with Parselets</a></p>
<p>15 0.54251873 <a title="448-lsi-15" href="./iccv-2013-Image_Segmentation_with_Cascaded_Hierarchical_Models_and_Logistic_Disjunctive_Normal_Networks.html">211 iccv-2013-Image Segmentation with Cascaded Hierarchical Models and Logistic Disjunctive Normal Networks</a></p>
<p>16 0.53830487 <a title="448-lsi-16" href="./iccv-2013-Fix_Structured_Learning_of_2013_ICCV_paper_k2opt.pdf.html">171 iccv-2013-Fix Structured Learning of 2013 ICCV paper k2opt.pdf</a></p>
<p>17 0.53735006 <a title="448-lsi-17" href="./iccv-2013-Discriminative_Label_Propagation_for_Multi-object_Tracking_with_Sporadic_Appearance_Features.html">120 iccv-2013-Discriminative Label Propagation for Multi-object Tracking with Sporadic Appearance Features</a></p>
<p>18 0.53165686 <a title="448-lsi-18" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>19 0.52331781 <a title="448-lsi-19" href="./iccv-2013-Improving_Graph_Matching_via_Density_Maximization.html">214 iccv-2013-Improving Graph Matching via Density Maximization</a></p>
<p>20 0.50396961 <a title="448-lsi-20" href="./iccv-2013-New_Graph_Structured_Sparsity_Model_for_Multi-label_Image_Annotations.html">290 iccv-2013-New Graph Structured Sparsity Model for Multi-label Image Annotations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.12), (7, 0.021), (12, 0.011), (26, 0.078), (27, 0.022), (31, 0.067), (34, 0.02), (40, 0.027), (42, 0.101), (43, 0.132), (44, 0.011), (64, 0.045), (73, 0.027), (89, 0.186), (95, 0.01), (98, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90908593 <a title="448-lda-1" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>Author: Christoph Straehle, Ullrich Koethe, Fred A. Hamprecht</p><p>Abstract: We propose a scheme that allows to partition an image into a previously unknown number of segments, using only minimal supervision in terms of a few must-link and cannotlink annotations. We make no use of regional data terms, learning instead what constitutes a likely boundary between segments. Since boundaries are only implicitly specified through cannot-link constraints, this is a hard and nonconvex latent variable problem. We address this problem in a greedy fashion using a randomized decision tree on features associated with interpixel edges. We use a structured purity criterion during tree construction and also show how a backtracking strategy can be used to prevent the greedy search from ending up in poor local optima. The proposed strategy is compared with prior art on natural images.</p><p>2 0.90271604 <a title="448-lda-2" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>Author: Alon Faktor, Michal Irani</p><p>Abstract: Given a set of images which share an object from the same semantic category, we would like to co-segment the shared object. We define ‘good’ co-segments to be ones which can be easily composed (like a puzzle) from large pieces of other co-segments, yet are difficult to compose from remaining image parts. These pieces must not only match well but also be statistically significant (hard to compose at random). This gives rise to co-segmentation of objects in very challenging scenarios with large variations in appearance, shape and large amounts of clutter. We further show how multiple images can collaborate and “score each others ’ co-segments to improve the overall fidelity and accuracy of the co-segmentation. Our co-segmentation can be applied both to large image collections, as well as to very few images (where there is too little data for unsupervised learning). At the extreme, it can be applied even to a single image, to extract its co-occurring objects. Our approach obtains state-of-the-art results on benchmark datasets. We further show very encouraging co-segmentation results on the challenging PASCAL-VOC dataset. ”</p><p>3 0.87689137 <a title="448-lda-3" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>Author: Ming-Ming Cheng, Jonathan Warrell, Wen-Yan Lin, Shuai Zheng, Vibhav Vineet, Nigel Crook</p><p>Abstract: Detecting visually salient regions in images is one of the fundamental problems in computer vision. We propose a novel method to decompose an image into large scale perceptually homogeneous elements for efficient salient region detection, using a soft image abstraction representation. By considering both appearance similarity and spatial distribution of image pixels, the proposed representation abstracts out unnecessary image details, allowing the assignment of comparable saliency values across similar regions, and producing perceptually accurate salient region detection. We evaluate our salient region detection approach on the largest publicly available dataset with pixel accurate annotations. The experimental results show that the proposed method outperforms 18 alternate methods, reducing the mean absolute error by 25.2% compared to the previous best result, while being computationally more efficient.</p><p>4 0.87548006 <a title="448-lda-4" href="./iccv-2013-Robust_Face_Landmark_Estimation_under_Occlusion.html">355 iccv-2013-Robust Face Landmark Estimation under Occlusion</a></p>
<p>Author: Xavier P. Burgos-Artizzu, Pietro Perona, Piotr Dollár</p><p>Abstract: Human faces captured in real-world conditions present large variations in shape and occlusions due to differences in pose, expression, use of accessories such as sunglasses and hats and interactions with objects (e.g. food). Current face landmark estimation approaches struggle under such conditions since theyfail toprovide aprincipled way ofhandling outliers. We propose a novel method, called Robust Cascaded Pose Regression (RCPR) which reduces exposure to outliers by detecting occlusions explicitly and using robust shape-indexed features. We show that RCPR improves on previous landmark estimation methods on three popular face datasets (LFPW, LFW and HELEN). We further explore RCPR ’s performance by introducing a novel face dataset focused on occlusion, composed of 1,007 faces presenting a wide range of occlusion patterns. RCPR reduces failure cases by half on all four datasets, at the same time as it detects face occlusions with a 80/40% precision/recall.</p><p>5 0.87262738 <a title="448-lda-5" href="./iccv-2013-Class-Specific_Simplex-Latent_Dirichlet_Allocation_for_Image_Classification.html">73 iccv-2013-Class-Specific Simplex-Latent Dirichlet Allocation for Image Classification</a></p>
<p>Author: Mandar Dixit, Nikhil Rasiwasia, Nuno Vasconcelos</p><p>Abstract: An extension of the latent Dirichlet allocation (LDA), denoted class-specific-simplex LDA (css-LDA), is proposed for image classification. An analysis of the supervised LDA models currently used for this task shows that the impact of class information on the topics discovered by these models is very weak in general. This implies that the discovered topics are driven by general image regularities, rather than the semantic regularities of interest for classification. To address this, we introduce a model that induces supervision in topic discovery, while retaining the original flexibility of LDA to account for unanticipated structures of interest. The proposed css-LDA is an LDA model with class supervision at the level of image features. In css-LDA topics are discovered per class, i.e. a single set of topics shared across classes is replaced by multiple class-specific topic sets. This model can be used for generative classification using the Bayes decision rule or even extended to discriminative classification with support vector machines (SVMs). A css-LDA model can endow an image with a vector of class and topic specific count statistics that are similar to the Bag-of-words (BoW) histogram. SVM-based discriminants can be learned for classes in the space of these histograms. The effectiveness of css-LDA model in both generative and discriminative classification frameworks is demonstrated through an extensive experimental evaluation, involving multiple benchmark datasets, where it is shown to outperform all existing LDA based image classification approaches.</p><p>6 0.87128252 <a title="448-lda-6" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>7 0.87074476 <a title="448-lda-7" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>8 0.87035614 <a title="448-lda-8" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>9 0.86921191 <a title="448-lda-9" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>10 0.86735618 <a title="448-lda-10" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>11 0.86728942 <a title="448-lda-11" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>12 0.86687833 <a title="448-lda-12" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>13 0.86466956 <a title="448-lda-13" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>14 0.86425912 <a title="448-lda-14" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>15 0.86417437 <a title="448-lda-15" href="./iccv-2013-Video_Synopsis_by_Heterogeneous_Multi-source_Correlation.html">443 iccv-2013-Video Synopsis by Heterogeneous Multi-source Correlation</a></p>
<p>16 0.86394083 <a title="448-lda-16" href="./iccv-2013-Learning_to_Rank_Using_Privileged_Information.html">248 iccv-2013-Learning to Rank Using Privileged Information</a></p>
<p>17 0.86355901 <a title="448-lda-17" href="./iccv-2013-Interactive_Markerless_Articulated_Hand_Motion_Tracking_Using_RGB_and_Depth_Data.html">218 iccv-2013-Interactive Markerless Articulated Hand Motion Tracking Using RGB and Depth Data</a></p>
<p>18 0.86330444 <a title="448-lda-18" href="./iccv-2013-PhotoOCR%3A_Reading_Text_in_Uncontrolled_Conditions.html">315 iccv-2013-PhotoOCR: Reading Text in Uncontrolled Conditions</a></p>
<p>19 0.86220914 <a title="448-lda-19" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>20 0.86167538 <a title="448-lda-20" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
