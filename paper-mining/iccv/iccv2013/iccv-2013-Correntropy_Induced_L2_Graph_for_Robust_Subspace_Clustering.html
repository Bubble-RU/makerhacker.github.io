<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>94 iccv-2013-Correntropy Induced L2 Graph for Robust Subspace Clustering</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-94" href="#">iccv2013-94</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>94 iccv-2013-Correntropy Induced L2 Graph for Robust Subspace Clustering</h1>
<br/><p>Source: <a title="iccv-2013-94-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Lu_Correntropy_Induced_L2_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Canyi Lu, Jinhui Tang, Min Lin, Liang Lin, Shuicheng Yan, Zhouchen Lin</p><p>Abstract: In this paper, we study the robust subspace clustering problem, which aims to cluster the given possibly noisy data points into their underlying subspaces. A large pool of previous subspace clustering methods focus on the graph construction by different regularization of the representation coefficient. We instead focus on the robustness of the model to non-Gaussian noises. We propose a new robust clustering method by using the correntropy induced metric, which is robust for handling the non-Gaussian and impulsive noises. Also we further extend the method for handling the data with outlier rows/features. The multiplicative form of half-quadratic optimization is used to optimize the nonconvex correntropy objective function of the proposed models. Extensive experiments on face datasets well demonstrate that the proposed methods are more robust to corruptions and occlusions.</p><p>Reference: <a title="iccv-2013-94-reference" href="../iccv2013_reference/iccv-2013-Correntropy_Induced_L2_Graph_for_Robust_Subspace_Clustering_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn  Abstract In this paper, we study the robust subspace clustering problem, which aims to cluster the given possibly noisy data points into their underlying subspaces. [sent-11, score-0.433]
</p><p>2 A large pool of previous subspace clustering methods focus on the graph construction by different regularization of the representation coefficient. [sent-12, score-0.422]
</p><p>3 We propose a new robust clustering method by using the correntropy induced metric, which is robust for handling the non-Gaussian and impulsive noises. [sent-14, score-0.843]
</p><p>4 The multiplicative form of half-quadratic optimization is used to optimize the nonconvex correntropy objective function of the proposed models. [sent-16, score-0.479]
</p><p>5 Extensive experiments on face datasets well demonstrate that the proposed methods are more robust to corruptions and occlusions. [sent-17, score-0.214]
</p><p>6 The subspaces are possibly the most widely used data model, since many real-world data, such as face images and motions, can be well characterized by subspaces. [sent-20, score-0.172]
</p><p>7 Subspace clustering is a fundamental problem and has numerous applications in the machine learning and computer vision literature, e. [sent-22, score-0.169]
</p><p>8 Figure 1 illustrates some face images from three subjects. [sent-29, score-0.132]
</p><p>9 The face images with pixel corruption, sunglasses and/or scarf, deviate from their underlying subspaces. [sent-30, score-0.321]
</p><p>10 This paper aims to address the robust subspace clustering problem with various noises, such as the non-Gaussian noises. [sent-32, score-0.409]
</p><p>11 11880011  | |v| |2 and | |v| |∞ denote the L2 norm and infinity norm  of v||vec|t|ora v, respectively. [sent-45, score-0.312]
</p><p>12 L1 norm, L21 norm and nuclear norm of matrix M are defined as | |M? [sent-46, score-0.389]
</p><p>13 Related Work Many subspace clustering methods have been proposed [21, 11, 6, 7]. [sent-56, score-0.377]
</p><p>14 In this work, we focus on the recent graph based subspace clustering methods [3, 4, 11, 10, 13]. [sent-57, score-0.422]
</p><p>15 These methods are based on the spectral clustering, and its first step aims to construct an affinity (or graph) matrix which is close to be block diagonal, with zero elements corresponding to data pair from different subspaces. [sent-58, score-0.13]
</p><p>16 The LSR method uses the Frobenius norm to model both the reconstruction error and the representation matrix, L(X − XZ) = | |X − XZ| |2F and R(Z) = | |Z| |2F. [sent-70, score-0.166]
</p><p>17 LSR hLa(sX a −clo XseZd )fo =rm | s|Xolu −tio Xn Zw|h|icha nmda Rke(sZ Zit) )e =ffic |i|Zen|t|, and the grouping effect makes it effective for subspace clustering. [sent-71, score-0.301]
</p><p>18 The Frobenius norm and L21 norm are used as the loss function while the L1 norm, nuclear norm and Frobenius are used to control the affinity matrix. [sent-73, score-0.556]
</p><p>19 In this work, we show that the L1 norm, L21 norm and nuclear norm all satisfy certain conditions, and thus the previous subspace clustering methods, including SSC, LRR and MSR, can be unified within a general framework from the perspective of half-quadratic optimization [17]. [sent-75, score-0.767]
</p><p>20 Different from the previous methods which focus on a regularization term R(Z), this work focuses on the conrsetrguucltairoinz error t teerrmm L R((ZZ) )f,o rt h roisbu wsot subspace learning. [sent-77, score-0.231]
</p><p>21 cPorne-svtirouucst wono rekrsro use rthme L LF(roZb)e fnoirus ro norm utob measure trhnien quality of approximation, which is optimal for the case of independent and identically distributed (i. [sent-78, score-0.143]
</p><p>22 LRR by using the L21 norm is able to remove the outlier samples, but it is sensitive to the outlier features. [sent-82, score-0.255]
</p><p>23 To overcome the weakness of mean squared error, we propose a new robust subspace clustering method which uses the correntropy induced metric as the loss function. [sent-83, score-1.03]
</p><p>24 The Frobenius norm is used to control the affinity matrix to preserve the grouping effect as in LSR. [sent-84, score-0.306]
</p><p>25 Then we minimize the non-convex correntropy objective of the proposed method by alternate minimization. [sent-85, score-0.479]
</p><p>26 Contributions and Organization We summarize the contributions of this work as follows: •  •  We propose a new robust subspace clustering method by Correntropy Iewnd urocebdus tL s2u (bCspILa2c)e graph. [sent-88, score-0.409]
</p><p>27 We apply the correntropy induced L2 graph for face clustering tuhnede cor vrraerniotruosp types cofe corruptions faonrd f oc-  clusions. [sent-91, score-0.956]
</p><p>28 Section 2 gives a brief review of the half-quadratic analysis and presents a general half-quadratic framework for robust subspace clustering. [sent-94, score-0.24]
</p><p>29 Section 3 elaborates the proposed CIL2 graph for robust subspace clustering. [sent-95, score-0.285]
</p><p>30 Section 4 provides experimental results on face clustering under different settings. [sent-96, score-0.28]
</p><p>31 f φ(x) that satisfies the following conditions [17] (a) x → φ(x) is convex on R, (b) x → φ(√x) is concave on R+ , (c) φ(x) = φ(−x) , x ∈ R,  (3)  (d) φ(x) is C1 on R, (e) φ? [sent-109, score-0.12]
</p><p>32 If φ(·) satisfies all the conditions in (3), there exists a duaIlf fuφ n(c·)ti soant isψf [e1s7 ]a slu tchhe th coatn φ(x) =  s i∈nRf{21sx2  + ψ(s)},  (5)  where s is determined by the minimizer function δ(·) with respect st ios φ d(e·t)e. [sent-113, score-0.096]
</p><p>33 We roughly say the L1 norm satisfies all the conditions in (3) in this sense. [sent-140, score-0.239]
</p><p>34 The work [16] for solving low rank minimization by iteratively reweighted least squares minimization can be interpreted as the half-quadratic minimization. [sent-159, score-0.171]
</p><p>35 If both two functions satisfy all the conditions in (3), the sum of them also satisfies these conditions. [sent-160, score-0.12]
</p><p>36 Based on the above analysis, previous subspace clustering methods by using the L1 norm, L21 norm and nuclear norm can be optimized by the half-quadratic analysis on (9)(10) by slightly relaxing the objective function. [sent-162, score-0.743]
</p><p>37 shown in Table 1, previous subspace clustering methods, including SSC, LRR, MSR and LSR, can be regarded as special cases of the problem (2) from the view of half-quadratic analysis. [sent-168, score-0.425]
</p><p>38 Note that the Frobenius norm | | · | |2F does not need half-quadratic representation biuesca nuosrem mi t| |i·s| already quadratic. [sent-169, score-0.173]
</p><p>39 The work in [5] extended the concept of mean squared error adaptation to information theoretic learning (ITL) to include the information theoretic criteria. [sent-176, score-0.13]
</p><p>40 Then they further proposed the concept of correntropy to process non-Gaussian and impulsive noises [12]. [sent-177, score-0.596]
</p><p>41 The correntropy is a generalized similarity measure between two arbitrary scalar random variables u and v defined by Vσ(u, v) = E[kσ(e)] , (11) where e = u − v, E[·] is the expectation operator, and wkσh (e·)re eis e eth =e ke ur −nel vfu,n Ec[t·i]on is. [sent-178, score-0.479]
</p><p>42 [12] extended the concept of correntropy criterion for a general similarity measurement between any two vectors, which is called the Correntropy Induced Metric (CIM). [sent-184, score-0.5]
</p><p>43 The effectiveness and robustness of correntropy have been verified in face recognition [9], feature selection [8] and signal processing [12]. [sent-194, score-0.611]
</p><p>44 This paper uses this concept for robust subspace clustering. [sent-195, score-0.261]
</p><p>45 Correntropy Induced L2 Graph For robust subspace clustering, we use the correntropy to replace the Frobenius norm in the LSR model to model the reconstruction error, leading to the Correntropy Induced L2 (CIL2) graph as follows:  mZin? [sent-198, score-0.907]
</p><p>46 (19)  11880044  From (15) or problem (18), we can see that the correntropy based LSR model can be regarded as a weighted LSR,  where each weight Sij corresponding to Eij is used to control the effect of Eij . [sent-214, score-0.562]
</p><p>47 For example, some rows of the face images with sunglasses and scarf are outliers, which are not discriminative for classification and clustering. [sent-218, score-0.445]
</p><p>48 The mechanism of correntropy and the Proposition 1ensure that both CIL2 and rCIL2 are not only robust to noises but also preserve the grouping effect. [sent-243, score-0.615]
</p><p>49 Datasets and Settings Our experiments are performed on three face datasets: Yale, ORL and AR. [sent-250, score-0.111]
</p><p>50 The Yale face dataset [1] contains 165 grayscale images of 15 individuals. [sent-252, score-0.163]
</p><p>51 652103456780LkCrmN1R2I9ELe0ans1 Percent of pixels corrupted in each image  Percent of pixels corrupted in each image  Figure 4. [sent-256, score-0.216]
</p><p>52 Clustering accuracy and NMI on the Yale dataset with pixel corruption for different algorithms. [sent-257, score-0.129]
</p><p>53 The images were taken with a tolerance for some tilting and rotation of the face up to 20 degrees. [sent-263, score-0.132]
</p><p>54 These images suffer different facial variations, including various facial expressions (neutral, smile, anger, and scream), illumination variations (left light on, right light on, and all side lights on), and occlusion by sunglasses or scarf. [sent-269, score-0.367]
</p><p>55 Evaluation Metrics The clustering result is evaluated by the accuracy and normalized mutual information (NMI) metric as in[22]. [sent-274, score-0.243]
</p><p>56 8765432190 56708Lk CrmN1R2IELe90ans1 Percent of pixels corrupted in each image  Percent of pixels corrupted in each image  Figure 5. [sent-290, score-0.216]
</p><p>57 Clustering accuracy and NMI on the ORL dataset with pixel corruption for different algorithms. [sent-291, score-0.129]
</p><p>58 Algorithm Settings We compare our rCIL2 and CIL2 graphs with several graph construction methods for subspace clustering, includ-  ing the L1-graph [3] (or SSC [4]), L2-graph (LSR) [13], and LRR-graph [10]. [sent-306, score-0.283]
</p><p>59 kNN and LLE [18] are also applied to construct graphs for subspace clustering. [sent-307, score-0.238]
</p><p>60 Results under Random Pixel Corruption In some practical scenarios, the face images may be partially corrupted. [sent-312, score-0.132]
</p><p>61 We evaluate the algorithmic robustness on the Yale and ORL face datasets. [sent-313, score-0.132]
</p><p>62 Each image is corrupted by replacing a percentage of randomly chosen pixels with i. [sent-314, score-0.144]
</p><p>63 The corrupted pixels are randomly chosen for each image, and the locations are unknown. [sent-318, score-0.108]
</p><p>64 We vary the percentage r of corrupted pixels from 10% to 100%. [sent-319, score-0.144]
</p><p>65 To the human eyes, beyond 50% corruption, the corrupted images are barely recognizable as face images. [sent-321, score-0.216]
</p><p>66 Figures 4 and 5 show the means of clustering accuracy and NMI of different methods as functions of the corruption level. [sent-323, score-0.298]
</p><p>67 P876549 e10rcn2tofi3mag4s0w5hrn6dom7blc8k0LCrN1R2Ic9mLl0Eeuasnio1 Percent of images with random block occlusion  Percent of images with random block occlusion  Figure 6. [sent-328, score-0.306]
</p><p>68 Clustering accuracy and NMI on the Yale dataset with block occlusion for different algorithms. [sent-329, score-0.155]
</p><p>69 Clustering accuracy and NMI on the ORL dataset with block occlusion for different algorithms. [sent-331, score-0.155]
</p><p>70 In particular, the CIL2 usually performs better than rCIL2 when the percentage of the corrupted pixels is no more than 50% on the Yale dataset and 70% on the ORL dataset. [sent-334, score-0.144]
</p><p>71 This is because each row of  ×  images may not be regarded as outliers when the level of the random pixel corruption is low. [sent-335, score-0.175]
</p><p>72 LRR and L2-graph perform competitively on both datasets, which also verifies the effectiveness of the grouping effect of these two methods for subspace clustering. [sent-336, score-0.301]
</p><p>73 Results under Contiguous Occlusion In this subsection we simulate various types of contiguous occlusions by replacing a randomly selected local region in some randomly selected images with a black-white square and an unrelated monkey image. [sent-340, score-0.217]
</p><p>74 Figure 3 (b) shows some face images with such black-white occlusions, in size of 8 8 pixels. [sent-342, score-0.132]
</p><p>75 Figures 6 and 7 show the means of clustering accuracy and NMI of each method on different percentages of corrupted images. [sent-346, score-0.276]
</p><p>76 Compared with previous subspace clustering methods, the im-  ×  Figure 8. [sent-348, score-0.377]
</p><p>77 Clustering accuracy and NMI on the AR dataset with an unrelated image occlusion for different algorithms. [sent-349, score-0.155]
</p><p>78 The phenomenon is similar to the random pixel corruption scenario, since the images with block occlusion will not lead to outlier rows. [sent-351, score-0.315]
</p><p>79 Notice that in this experiment, r percentage of the images in each dataset is selected to be occluded with a size of 8 8 block, adnatda sthetus is tsheele decreasing curves do wf it hhe a clustering accuracy and NMI are flatter than those in Figures 4 and 5. [sent-353, score-0.249]
</p><p>80 For each image, we randomly select a local region to be replaced by an unrelated monkey image. [sent-357, score-0.14]
</p><p>81 Figure 8 shows the clustering accuracy and NMI of each method on the AR dataset with unrelated monkey image occlusion. [sent-364, score-0.332]
</p><p>82 Results on Real-World Malicious Occlusion In real-world face recognition systems, people may wear sunglasses or scarfs which make the classification or clustering more challenging. [sent-369, score-0.446]
</p><p>83 In this subsection, we evaluate the robustness of the proposed method on the AR dataset with sunglasses and scarf occlusions. [sent-370, score-0.334]
</p><p>84 In each session, each subject has 7 face images with different facial variations, 3 face images with sunglasses occlusion and 3 face images with scarf occlusion. [sent-372, score-0.835]
</p><p>85 Figure 3 (d) shows some face images with such an occlusion. [sent-373, score-0.132]
</p><p>86 In each session, we conduct two experiments corresponding to the sunglasses and scarf occlusions. [sent-374, score-0.313]
</p><p>87 For sunglasses occlusion, we use the first 2 normal face images 11880077  Table 2. [sent-375, score-0.298]
</p><p>88 The clustering accuracy (%) and NMI (%) of different algorithms on the AR dataset. [sent-376, score-0.192]
</p><p>89 5  and 3 face images with sunglasses of each subject. [sent-421, score-0.298]
</p><p>90 For scarf occlusion, we use the first 2 normal face images and 3 face images with scarf of each subject. [sent-422, score-0.558]
</p><p>91 Table 2 shows the clustering results on the AR dataset for the images with sunglasses and scarf occlusions. [sent-423, score-0.503]
</p><p>92 Different from the above experiments, rCIL2 achieves the best clustering accuracy and NMI in all cases. [sent-424, score-0.192]
</p><p>93 That is because the face images with sunglasses and scarf occlusions contain many outlier rows/features, and rCIL2 is designed for such a task. [sent-425, score-0.523]
</p><p>94 Conclusions In this paper, we study the robust subspace clustering problem, and present a general framework from the viewpoint of half-quadratic optimization to unify the L1 norm, Frobenius norm, L21 norm and nuclear norm based subspace clustering methods. [sent-428, score-1.152]
</p><p>95 Previous iteratively reweighted least squares optimization methods for the sparse and low rank minimization can be regarded as the half-quadratic optimization. [sent-429, score-0.189]
</p><p>96 As a new special case, we use the correntropy as the loss function for robust subspace clustering to handle the non-Gaussian and impulsive noises. [sent-430, score-0.938]
</p><p>97 An alternate  minimization algorithm is used to optimize the non-convex correntropy  objective. [sent-431, score-0.509]
</p><p>98 Extensive experiments  on the face  clustering with various types of corruptions and occlusions well demonstrate the effectiveness proposed methods by comparing  and robustness  of the  with the state-of-the-art  subspace clustering methods. [sent-432, score-0.771]
</p><p>99 Robust and efficient subspace segmentation via least squares regression. [sent-538, score-0.241]
</p><p>100 Parameterisation of a stochastic model for human face identification. [sent-575, score-0.111]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('correntropy', 0.479), ('nmi', 0.403), ('xz', 0.26), ('subspace', 0.208), ('lsr', 0.195), ('orl', 0.173), ('clustering', 0.169), ('sunglasses', 0.166), ('lrr', 0.166), ('scarf', 0.147), ('norm', 0.143), ('cim', 0.115), ('face', 0.111), ('corruption', 0.106), ('yale', 0.105), ('eij', 0.09), ('corrupted', 0.084), ('ssc', 0.083), ('induced', 0.081), ('nuclear', 0.08), ('monkey', 0.08), ('ar', 0.074), ('percent', 0.072), ('occlusion', 0.072), ('corruptions', 0.071), ('frobenius', 0.07), ('xiz', 0.065), ('sij', 0.063), ('msr', 0.061), ('unrelated', 0.06), ('block', 0.06), ('grouping', 0.058), ('reweighted', 0.057), ('outlier', 0.056), ('satisfies', 0.056), ('facial', 0.054), ('xtx', 0.05), ('impulsive', 0.05), ('regarded', 0.048), ('mzin', 0.048), ('affinity', 0.047), ('noises', 0.046), ('graph', 0.045), ('mzinj', 0.043), ('subspaces', 0.04), ('conditions', 0.04), ('cj', 0.038), ('canyi', 0.038), ('ij', 0.036), ('percentage', 0.036), ('effect', 0.035), ('contiguous', 0.034), ('squares', 0.033), ('proposition', 0.033), ('robust', 0.032), ('zi', 0.032), ('ci', 0.032), ('squared', 0.032), ('resized', 0.031), ('grayscale', 0.031), ('tsp', 0.031), ('ui', 0.03), ('mi', 0.03), ('minimization', 0.03), ('graphs', 0.03), ('clusters', 0.029), ('metric', 0.029), ('singapore', 0.029), ('session', 0.029), ('mij', 0.029), ('pages', 0.028), ('theoretic', 0.027), ('ite', 0.026), ('mj', 0.026), ('school', 0.026), ('infinity', 0.026), ('diag', 0.025), ('lim', 0.025), ('pi', 0.025), ('pixels', 0.024), ('check', 0.024), ('cluster', 0.024), ('satisfy', 0.024), ('concave', 0.024), ('glasses', 0.024), ('deviate', 0.023), ('diagonal', 0.023), ('matrix', 0.023), ('error', 0.023), ('accuracy', 0.023), ('occlusions', 0.022), ('ang', 0.022), ('tpami', 0.022), ('mutual', 0.022), ('concept', 0.021), ('ei', 0.021), ('images', 0.021), ('arbitrarily', 0.021), ('robustness', 0.021), ('rank', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="94-tfidf-1" href="./iccv-2013-Correntropy_Induced_L2_Graph_for_Robust_Subspace_Clustering.html">94 iccv-2013-Correntropy Induced L2 Graph for Robust Subspace Clustering</a></p>
<p>Author: Canyi Lu, Jinhui Tang, Min Lin, Liang Lin, Shuicheng Yan, Zhouchen Lin</p><p>Abstract: In this paper, we study the robust subspace clustering problem, which aims to cluster the given possibly noisy data points into their underlying subspaces. A large pool of previous subspace clustering methods focus on the graph construction by different regularization of the representation coefficient. We instead focus on the robustness of the model to non-Gaussian noises. We propose a new robust clustering method by using the correntropy induced metric, which is robust for handling the non-Gaussian and impulsive noises. Also we further extend the method for handling the data with outlier rows/features. The multiplicative form of half-quadratic optimization is used to optimize the nonconvex correntropy objective function of the proposed models. Extensive experiments on face datasets well demonstrate that the proposed methods are more robust to corruptions and occlusions.</p><p>2 0.40745902 <a title="94-tfidf-2" href="./iccv-2013-Robust_Subspace_Clustering_via_Half-Quadratic_Minimization.html">360 iccv-2013-Robust Subspace Clustering via Half-Quadratic Minimization</a></p>
<p>Author: Yingya Zhang, Zhenan Sun, Ran He, Tieniu Tan</p><p>Abstract: Subspace clustering has important and wide applications in computer vision and pattern recognition. It is a challenging task to learn low-dimensional subspace structures due to the possible errors (e.g., noise and corruptions) existing in high-dimensional data. Recent subspace clustering methods usually assume a sparse representation of corrupted errors and correct the errors iteratively. However large corruptions in real-world applications can not be well addressed by these methods. A novel optimization model for robust subspace clustering is proposed in this paper. The objective function of our model mainly includes two parts. The first part aims to achieve a sparse representation of each high-dimensional data point with other data points. The second part aims to maximize the correntropy between a given data point and its low-dimensional representation with other points. Correntropy is a robust measure so that the influence of large corruptions on subspace clustering can be greatly suppressed. An extension of our method with explicit introduction of representation error terms into the model is also proposed. Half-quadratic minimization is provided as an efficient solution to the proposed robust subspace clustering formulations. Experimental results on Hopkins 155 dataset and Extended Yale Database B demonstrate that our method outperforms state-of-the-art subspace clustering methods.</p><p>3 0.24336135 <a title="94-tfidf-3" href="./iccv-2013-Distributed_Low-Rank_Subspace_Segmentation.html">122 iccv-2013-Distributed Low-Rank Subspace Segmentation</a></p>
<p>Author: Ameet Talwalkar, Lester Mackey, Yadong Mu, Shih-Fu Chang, Michael I. Jordan</p><p>Abstract: Vision problems ranging from image clustering to motion segmentation to semi-supervised learning can naturally be framed as subspace segmentation problems, in which one aims to recover multiple low-dimensional subspaces from noisy and corrupted input data. Low-Rank Representation (LRR), a convex formulation of the subspace segmentation problem, is provably and empirically accurate on small problems but does not scale to the massive sizes of modern vision datasets. Moreover, past work aimed at scaling up low-rank matrix factorization is not applicable to LRR given its non-decomposable constraints. In this work, we propose a novel divide-and-conquer algorithm for large-scale subspace segmentation that can cope with LRR ’s non-decomposable constraints and maintains LRR ’s strong recovery guarantees. This has immediate implications for the scalability of subspace segmentation, which we demonstrate on a benchmark face recognition dataset and in simulations. We then introduce novel applications of LRR-based subspace segmentation to large-scale semisupervised learning for multimedia event detection, concept detection, and image tagging. In each case, we obtain stateof-the-art results and order-of-magnitude speed ups.</p><p>4 0.23962815 <a title="94-tfidf-4" href="./iccv-2013-Correlation_Adaptive_Subspace_Segmentation_by_Trace_Lasso.html">93 iccv-2013-Correlation Adaptive Subspace Segmentation by Trace Lasso</a></p>
<p>Author: Canyi Lu, Jiashi Feng, Zhouchen Lin, Shuicheng Yan</p><p>Abstract: This paper studies the subspace segmentation problem. Given a set of data points drawn from a union of subspaces, the goal is to partition them into their underlying subspaces they were drawn from. The spectral clustering method is used as the framework. It requires to find an affinity matrix which is close to block diagonal, with nonzero entries corresponding to the data point pairs from the same subspace. In this work, we argue that both sparsity and the grouping effect are important for subspace segmentation. A sparse affinity matrix tends to be block diagonal, with less connections between data points from different subspaces. The grouping effect ensures that the highly corrected data which are usually from the same subspace can be grouped together. Sparse Subspace Clustering (SSC), by using ?1-minimization, encourages sparsity for data selection, but it lacks of the grouping effect. On the contrary, Low-RankRepresentation (LRR), by rank minimization, and Least Squares Regression (LSR), by ?2-regularization, exhibit strong grouping effect, but they are short in subset selection. Thus the obtained affinity matrix is usually very sparse by SSC, yet very dense by LRR and LSR. In this work, we propose the Correlation Adaptive Subspace Segmentation (CASS) method by using trace Lasso. CASS is a data correlation dependent method which simultaneously performs automatic data selection and groups correlated data together. It can be regarded as a method which adaptively balances SSC and LSR. Both theoretical and experimental results show the effectiveness of CASS.</p><p>5 0.23575053 <a title="94-tfidf-5" href="./iccv-2013-Latent_Space_Sparse_Subspace_Clustering.html">232 iccv-2013-Latent Space Sparse Subspace Clustering</a></p>
<p>Author: Vishal M. Patel, Hien Van Nguyen, René Vidal</p><p>Abstract: We propose a novel algorithm called Latent Space Sparse Subspace Clustering for simultaneous dimensionality reduction and clustering of data lying in a union of subspaces. Specifically, we describe a method that learns the projection of data and finds the sparse coefficients in the low-dimensional latent space. Cluster labels are then assigned by applying spectral clustering to a similarity matrix built from these sparse coefficients. An efficient optimization method is proposed and its non-linear extensions based on the kernel methods are presented. One of the main advantages of our method is that it is computationally efficient as the sparse coefficients are found in the low-dimensional latent space. Various experiments show that the proposed method performs better than the competitive state-of-theart subspace clustering methods.</p><p>6 0.16657963 <a title="94-tfidf-6" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>7 0.12453104 <a title="94-tfidf-7" href="./iccv-2013-GOSUS%3A_Grassmannian_Online_Subspace_Updates_with_Structured-Sparsity.html">182 iccv-2013-GOSUS: Grassmannian Online Subspace Updates with Structured-Sparsity</a></p>
<p>8 0.11675752 <a title="94-tfidf-8" href="./iccv-2013-Minimal_Basis_Facility_Location_for_Subspace_Segmentation.html">264 iccv-2013-Minimal Basis Facility Location for Subspace Segmentation</a></p>
<p>9 0.11036768 <a title="94-tfidf-9" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>10 0.1056831 <a title="94-tfidf-10" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>11 0.099202588 <a title="94-tfidf-11" href="./iccv-2013-Efficient_Higher-Order_Clustering_on_the_Grassmann_Manifold.html">134 iccv-2013-Efficient Higher-Order Clustering on the Grassmann Manifold</a></p>
<p>12 0.099034019 <a title="94-tfidf-12" href="./iccv-2013-Unifying_Nuclear_Norm_and_Bilinear_Factorization_Approaches_for_Low-Rank_Matrix_Decomposition.html">434 iccv-2013-Unifying Nuclear Norm and Bilinear Factorization Approaches for Low-Rank Matrix Decomposition</a></p>
<p>13 0.096408695 <a title="94-tfidf-13" href="./iccv-2013-Learning_Coupled_Feature_Spaces_for_Cross-Modal_Matching.html">235 iccv-2013-Learning Coupled Feature Spaces for Cross-Modal Matching</a></p>
<p>14 0.095705479 <a title="94-tfidf-14" href="./iccv-2013-Synergistic_Clustering_of_Image_and_Segment_Descriptors_for_Unsupervised_Scene_Understanding.html">412 iccv-2013-Synergistic Clustering of Image and Segment Descriptors for Unsupervised Scene Understanding</a></p>
<p>15 0.093431756 <a title="94-tfidf-15" href="./iccv-2013-Non-convex_P-Norm_Projection_for_Robust_Sparsity.html">292 iccv-2013-Non-convex P-Norm Projection for Robust Sparsity</a></p>
<p>16 0.090931818 <a title="94-tfidf-16" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>17 0.088155389 <a title="94-tfidf-17" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>18 0.086812563 <a title="94-tfidf-18" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>19 0.080190867 <a title="94-tfidf-19" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>20 0.076562464 <a title="94-tfidf-20" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.167), (1, 0.004), (2, -0.094), (3, -0.045), (4, -0.177), (5, 0.079), (6, 0.083), (7, 0.19), (8, 0.184), (9, 0.009), (10, 0.083), (11, 0.031), (12, -0.143), (13, 0.034), (14, -0.101), (15, -0.063), (16, 0.014), (17, -0.038), (18, -0.001), (19, 0.054), (20, -0.054), (21, 0.173), (22, -0.1), (23, -0.174), (24, 0.021), (25, -0.13), (26, -0.061), (27, 0.003), (28, -0.05), (29, -0.038), (30, 0.002), (31, -0.031), (32, 0.055), (33, -0.005), (34, -0.075), (35, 0.028), (36, 0.006), (37, -0.028), (38, -0.037), (39, 0.004), (40, -0.003), (41, -0.033), (42, -0.032), (43, -0.025), (44, 0.04), (45, -0.032), (46, -0.013), (47, -0.051), (48, 0.02), (49, -0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9479636 <a title="94-lsi-1" href="./iccv-2013-Correntropy_Induced_L2_Graph_for_Robust_Subspace_Clustering.html">94 iccv-2013-Correntropy Induced L2 Graph for Robust Subspace Clustering</a></p>
<p>Author: Canyi Lu, Jinhui Tang, Min Lin, Liang Lin, Shuicheng Yan, Zhouchen Lin</p><p>Abstract: In this paper, we study the robust subspace clustering problem, which aims to cluster the given possibly noisy data points into their underlying subspaces. A large pool of previous subspace clustering methods focus on the graph construction by different regularization of the representation coefficient. We instead focus on the robustness of the model to non-Gaussian noises. We propose a new robust clustering method by using the correntropy induced metric, which is robust for handling the non-Gaussian and impulsive noises. Also we further extend the method for handling the data with outlier rows/features. The multiplicative form of half-quadratic optimization is used to optimize the nonconvex correntropy objective function of the proposed models. Extensive experiments on face datasets well demonstrate that the proposed methods are more robust to corruptions and occlusions.</p><p>2 0.93732035 <a title="94-lsi-2" href="./iccv-2013-Robust_Subspace_Clustering_via_Half-Quadratic_Minimization.html">360 iccv-2013-Robust Subspace Clustering via Half-Quadratic Minimization</a></p>
<p>Author: Yingya Zhang, Zhenan Sun, Ran He, Tieniu Tan</p><p>Abstract: Subspace clustering has important and wide applications in computer vision and pattern recognition. It is a challenging task to learn low-dimensional subspace structures due to the possible errors (e.g., noise and corruptions) existing in high-dimensional data. Recent subspace clustering methods usually assume a sparse representation of corrupted errors and correct the errors iteratively. However large corruptions in real-world applications can not be well addressed by these methods. A novel optimization model for robust subspace clustering is proposed in this paper. The objective function of our model mainly includes two parts. The first part aims to achieve a sparse representation of each high-dimensional data point with other data points. The second part aims to maximize the correntropy between a given data point and its low-dimensional representation with other points. Correntropy is a robust measure so that the influence of large corruptions on subspace clustering can be greatly suppressed. An extension of our method with explicit introduction of representation error terms into the model is also proposed. Half-quadratic minimization is provided as an efficient solution to the proposed robust subspace clustering formulations. Experimental results on Hopkins 155 dataset and Extended Yale Database B demonstrate that our method outperforms state-of-the-art subspace clustering methods.</p><p>3 0.93139601 <a title="94-lsi-3" href="./iccv-2013-Correlation_Adaptive_Subspace_Segmentation_by_Trace_Lasso.html">93 iccv-2013-Correlation Adaptive Subspace Segmentation by Trace Lasso</a></p>
<p>Author: Canyi Lu, Jiashi Feng, Zhouchen Lin, Shuicheng Yan</p><p>Abstract: This paper studies the subspace segmentation problem. Given a set of data points drawn from a union of subspaces, the goal is to partition them into their underlying subspaces they were drawn from. The spectral clustering method is used as the framework. It requires to find an affinity matrix which is close to block diagonal, with nonzero entries corresponding to the data point pairs from the same subspace. In this work, we argue that both sparsity and the grouping effect are important for subspace segmentation. A sparse affinity matrix tends to be block diagonal, with less connections between data points from different subspaces. The grouping effect ensures that the highly corrected data which are usually from the same subspace can be grouped together. Sparse Subspace Clustering (SSC), by using ?1-minimization, encourages sparsity for data selection, but it lacks of the grouping effect. On the contrary, Low-RankRepresentation (LRR), by rank minimization, and Least Squares Regression (LSR), by ?2-regularization, exhibit strong grouping effect, but they are short in subset selection. Thus the obtained affinity matrix is usually very sparse by SSC, yet very dense by LRR and LSR. In this work, we propose the Correlation Adaptive Subspace Segmentation (CASS) method by using trace Lasso. CASS is a data correlation dependent method which simultaneously performs automatic data selection and groups correlated data together. It can be regarded as a method which adaptively balances SSC and LSR. Both theoretical and experimental results show the effectiveness of CASS.</p><p>4 0.87191021 <a title="94-lsi-4" href="./iccv-2013-Distributed_Low-Rank_Subspace_Segmentation.html">122 iccv-2013-Distributed Low-Rank Subspace Segmentation</a></p>
<p>Author: Ameet Talwalkar, Lester Mackey, Yadong Mu, Shih-Fu Chang, Michael I. Jordan</p><p>Abstract: Vision problems ranging from image clustering to motion segmentation to semi-supervised learning can naturally be framed as subspace segmentation problems, in which one aims to recover multiple low-dimensional subspaces from noisy and corrupted input data. Low-Rank Representation (LRR), a convex formulation of the subspace segmentation problem, is provably and empirically accurate on small problems but does not scale to the massive sizes of modern vision datasets. Moreover, past work aimed at scaling up low-rank matrix factorization is not applicable to LRR given its non-decomposable constraints. In this work, we propose a novel divide-and-conquer algorithm for large-scale subspace segmentation that can cope with LRR ’s non-decomposable constraints and maintains LRR ’s strong recovery guarantees. This has immediate implications for the scalability of subspace segmentation, which we demonstrate on a benchmark face recognition dataset and in simulations. We then introduce novel applications of LRR-based subspace segmentation to large-scale semisupervised learning for multimedia event detection, concept detection, and image tagging. In each case, we obtain stateof-the-art results and order-of-magnitude speed ups.</p><p>5 0.86241806 <a title="94-lsi-5" href="./iccv-2013-Latent_Space_Sparse_Subspace_Clustering.html">232 iccv-2013-Latent Space Sparse Subspace Clustering</a></p>
<p>Author: Vishal M. Patel, Hien Van Nguyen, René Vidal</p><p>Abstract: We propose a novel algorithm called Latent Space Sparse Subspace Clustering for simultaneous dimensionality reduction and clustering of data lying in a union of subspaces. Specifically, we describe a method that learns the projection of data and finds the sparse coefficients in the low-dimensional latent space. Cluster labels are then assigned by applying spectral clustering to a similarity matrix built from these sparse coefficients. An efficient optimization method is proposed and its non-linear extensions based on the kernel methods are presented. One of the main advantages of our method is that it is computationally efficient as the sparse coefficients are found in the low-dimensional latent space. Various experiments show that the proposed method performs better than the competitive state-of-theart subspace clustering methods.</p><p>6 0.77748424 <a title="94-lsi-6" href="./iccv-2013-Minimal_Basis_Facility_Location_for_Subspace_Segmentation.html">264 iccv-2013-Minimal Basis Facility Location for Subspace Segmentation</a></p>
<p>7 0.77219355 <a title="94-lsi-7" href="./iccv-2013-GOSUS%3A_Grassmannian_Online_Subspace_Updates_with_Structured-Sparsity.html">182 iccv-2013-GOSUS: Grassmannian Online Subspace Updates with Structured-Sparsity</a></p>
<p>8 0.70586717 <a title="94-lsi-8" href="./iccv-2013-Efficient_Higher-Order_Clustering_on_the_Grassmann_Manifold.html">134 iccv-2013-Efficient Higher-Order Clustering on the Grassmann Manifold</a></p>
<p>9 0.67073834 <a title="94-lsi-9" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>10 0.53576076 <a title="94-lsi-10" href="./iccv-2013-Progressive_Multigrid_Eigensolvers_for_Multiscale_Spectral_Segmentation.html">329 iccv-2013-Progressive Multigrid Eigensolvers for Multiscale Spectral Segmentation</a></p>
<p>11 0.49574322 <a title="94-lsi-11" href="./iccv-2013-Learning_Coupled_Feature_Spaces_for_Cross-Modal_Matching.html">235 iccv-2013-Learning Coupled Feature Spaces for Cross-Modal Matching</a></p>
<p>12 0.49458316 <a title="94-lsi-12" href="./iccv-2013-Robust_Matrix_Factorization_with_Unknown_Noise.html">357 iccv-2013-Robust Matrix Factorization with Unknown Noise</a></p>
<p>13 0.42606169 <a title="94-lsi-13" href="./iccv-2013-Finding_Causal_Interactions_in_Video_Sequences.html">167 iccv-2013-Finding Causal Interactions in Video Sequences</a></p>
<p>14 0.41821381 <a title="94-lsi-14" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>15 0.41372618 <a title="94-lsi-15" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>16 0.40528154 <a title="94-lsi-16" href="./iccv-2013-Simultaneous_Clustering_and_Tracklet_Linking_for_Multi-face_Tracking_in_Videos.html">393 iccv-2013-Simultaneous Clustering and Tracklet Linking for Multi-face Tracking in Videos</a></p>
<p>17 0.40045762 <a title="94-lsi-17" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>18 0.39331079 <a title="94-lsi-18" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>19 0.39321208 <a title="94-lsi-19" href="./iccv-2013-Automatic_Kronecker_Product_Model_Based_Detection_of_Repeated_Patterns_in_2D_Urban_Images.html">55 iccv-2013-Automatic Kronecker Product Model Based Detection of Repeated Patterns in 2D Urban Images</a></p>
<p>20 0.38889369 <a title="94-lsi-20" href="./iccv-2013-A_Generalized_Iterated_Shrinkage_Algorithm_for_Non-convex_Sparse_Coding.html">14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.065), (6, 0.011), (7, 0.024), (12, 0.015), (26, 0.063), (27, 0.018), (31, 0.04), (40, 0.012), (42, 0.126), (48, 0.024), (64, 0.032), (73, 0.042), (78, 0.028), (89, 0.116), (92, 0.237), (95, 0.018), (96, 0.013), (98, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.76476324 <a title="94-lda-1" href="./iccv-2013-Progressive_Multigrid_Eigensolvers_for_Multiscale_Spectral_Segmentation.html">329 iccv-2013-Progressive Multigrid Eigensolvers for Multiscale Spectral Segmentation</a></p>
<p>Author: Michael Maire, Stella X. Yu</p><p>Abstract: We reexamine the role of multiscale cues in image segmentation using an architecture that constructs a globally coherent scale-space output representation. This characteristic is in contrast to many existing works on bottom-up segmentation, whichprematurely compress information into a single scale. The architecture is a standard extension of Normalized Cuts from an image plane to an image pyramid, with cross-scale constraints enforcing consistency in the solution while allowing emergence of coarse-to-fine detail. We observe that multiscale processing, in addition to improving segmentation quality, offers a route by which to speed computation. We make a significant algorithmic advance in the form of a custom multigrid eigensolver for constrained Angular Embedding problems possessing coarseto-fine structure. Multiscale Normalized Cuts is a special case. Our solver builds atop recent results on randomized matrix approximation, using a novel interpolation operation to mold its computational strategy according to crossscale constraints in the problem definition. Applying our solver to multiscale segmentation problems demonstrates speedup by more than an order of magnitude. This speedup is at the algorithmic level and carries over to any implementation target.</p><p>same-paper 2 0.75919056 <a title="94-lda-2" href="./iccv-2013-Correntropy_Induced_L2_Graph_for_Robust_Subspace_Clustering.html">94 iccv-2013-Correntropy Induced L2 Graph for Robust Subspace Clustering</a></p>
<p>Author: Canyi Lu, Jinhui Tang, Min Lin, Liang Lin, Shuicheng Yan, Zhouchen Lin</p><p>Abstract: In this paper, we study the robust subspace clustering problem, which aims to cluster the given possibly noisy data points into their underlying subspaces. A large pool of previous subspace clustering methods focus on the graph construction by different regularization of the representation coefficient. We instead focus on the robustness of the model to non-Gaussian noises. We propose a new robust clustering method by using the correntropy induced metric, which is robust for handling the non-Gaussian and impulsive noises. Also we further extend the method for handling the data with outlier rows/features. The multiplicative form of half-quadratic optimization is used to optimize the nonconvex correntropy objective function of the proposed models. Extensive experiments on face datasets well demonstrate that the proposed methods are more robust to corruptions and occlusions.</p><p>3 0.66605163 <a title="94-lda-3" href="./iccv-2013-Dynamic_Probabilistic_Volumetric_Models.html">128 iccv-2013-Dynamic Probabilistic Volumetric Models</a></p>
<p>Author: Ali Osman Ulusoy, Octavian Biris, Joseph L. Mundy</p><p>Abstract: This paper presents a probabilistic volumetric framework for image based modeling of general dynamic 3-d scenes. The framework is targeted towards high quality modeling of complex scenes evolving over thousands of frames. Extensive storage and computational resources are required in processing large scale space-time (4-d) data. Existing methods typically store separate 3-d models at each time step and do not address such limitations. A novel 4-d representation is proposed that adaptively subdivides in space and time to explain the appearance of 3-d dynamic surfaces. This representation is shown to achieve compression of 4-d data and provide efficient spatio-temporal processing. The advances oftheproposedframework is demonstrated on standard datasets using free-viewpoint video and 3-d tracking applications.</p><p>4 0.64507931 <a title="94-lda-4" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>Author: Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, Philip S. Yu</p><p>Abstract: Transfer learning is established as an effective technology in computer visionfor leveraging rich labeled data in the source domain to build an accurate classifier for the target domain. However, most prior methods have not simultaneously reduced the difference in both the marginal distribution and conditional distribution between domains. In this paper, we put forward a novel transfer learning approach, referred to as Joint Distribution Adaptation (JDA). Specifically, JDA aims to jointly adapt both the marginal distribution and conditional distribution in a principled dimensionality reduction procedure, and construct new feature representation that is effective and robustfor substantial distribution difference. Extensive experiments verify that JDA can significantly outperform several state-of-the-art methods on four types of cross-domain image classification problems.</p><p>5 0.63710588 <a title="94-lda-5" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>Author: Vidit Jain, Sachin Sudhakar Farfade</p><p>Abstract: Classification cascades have been very effective for object detection. Such a cascade fails to perform well in data domains with variations in appearances that may not be captured in the training examples. This limited generalization severely restricts the domains for which they can be used effectively. A common approach to address this limitation is to train a new cascade of classifiers from scratch for each of the new domains. Building separate detectors for each of the different domains requires huge annotation and computational effort, making it not scalable to a large number of data domains. Here we present an algorithm for quickly adapting a pre-trained cascade of classifiers using a small number oflabeledpositive instancesfrom a different yet similar data domain. In our experiments with images of human babies and human-like characters from movies, we demonstrate that the adapted cascade significantly outperforms both of the original cascade and the one trained from scratch using the given training examples. –</p><p>6 0.63503075 <a title="94-lda-6" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>7 0.63442862 <a title="94-lda-7" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>8 0.63441312 <a title="94-lda-8" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>9 0.63392448 <a title="94-lda-9" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>10 0.63383174 <a title="94-lda-10" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>11 0.63173586 <a title="94-lda-11" href="./iccv-2013-Correlation_Adaptive_Subspace_Segmentation_by_Trace_Lasso.html">93 iccv-2013-Correlation Adaptive Subspace Segmentation by Trace Lasso</a></p>
<p>12 0.6316269 <a title="94-lda-12" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>13 0.63120288 <a title="94-lda-13" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>14 0.63079923 <a title="94-lda-14" href="./iccv-2013-Robust_Tucker_Tensor_Decomposition_for_Effective_Image_Representation.html">362 iccv-2013-Robust Tucker Tensor Decomposition for Effective Image Representation</a></p>
<p>15 0.63047522 <a title="94-lda-15" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>16 0.62868488 <a title="94-lda-16" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>17 0.62853837 <a title="94-lda-17" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>18 0.6281957 <a title="94-lda-18" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>19 0.6280992 <a title="94-lda-19" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>20 0.62792397 <a title="94-lda-20" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
