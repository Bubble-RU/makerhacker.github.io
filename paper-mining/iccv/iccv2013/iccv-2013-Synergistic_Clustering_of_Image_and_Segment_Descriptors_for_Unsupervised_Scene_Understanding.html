<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>412 iccv-2013-Synergistic Clustering of Image and Segment Descriptors for Unsupervised Scene Understanding</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-412" href="#">iccv2013-412</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>412 iccv-2013-Synergistic Clustering of Image and Segment Descriptors for Unsupervised Scene Understanding</h1>
<br/><p>Source: <a title="iccv-2013-412-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Steinberg_Synergistic_Clustering_of_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Daniel M. Steinberg, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: With the advent of cheap, high fidelity, digital imaging systems, the quantity and rate of generation of visual data can dramatically outpace a humans ability to label or annotate it. In these situations there is scope for the use of unsupervised approaches that can model these datasets and automatically summarise their content. To this end, we present a totally unsupervised, and annotation-less, model for scene understanding. This model can simultaneously cluster whole-image and segment descriptors, therebyforming an unsupervised model of scenes and objects. We show that this model outperforms other unsupervised models that can only cluster one source of information (image or segment) at once. We are able to compare unsupervised and supervised techniques using standard measures derived from confusion matrices and contingency tables. This shows that our unsupervised model is competitive with current supervised and weakly-supervised models for scene understanding on standard datasets. We also demonstrate our model operating on a dataset with more than 100,000 images col- lected by an autonomous underwater vehicle.</p><p>Reference: <a title="iccv-2013-412-reference" href="../iccv2013_reference/iccv-2013-Synergistic_Clustering_of_Image_and_Segment_Descriptors_for_Unsupervised_Scene_Understanding_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In these situations there is scope for the use of unsupervised approaches that can model these datasets and automatically summarise their content. [sent-6, score-0.123]
</p><p>2 This model can simultaneously cluster whole-image and segment descriptors, therebyforming an unsupervised model of scenes and objects. [sent-8, score-0.377]
</p><p>3 We show that this model outperforms other unsupervised models that can only cluster one source of information (image or segment) at once. [sent-9, score-0.191]
</p><p>4 We are able to compare unsupervised and supervised techniques using standard measures derived from confusion matrices and contingency tables. [sent-10, score-0.183]
</p><p>5 This shows that our unsupervised model is competitive with current supervised and weakly-supervised models for scene understanding on standard datasets. [sent-11, score-0.166]
</p><p>6 We also demonstrate our model operating on a dataset with more than 100,000 images col-  lected by an autonomous underwater vehicle. [sent-12, score-0.105]
</p><p>7 One example of this is the collection of scientific visual data by autonomous agents such as planetary rovers, unmanned air vehicles (UAVs), or autonomous underwater vehicles (AUVs). [sent-15, score-0.159]
</p><p>8 (a) A random selection of images from 8 of the 15 image clusters found by our proposed model on the MSRC dataset and (b) some of the (28) corresponding segment clusters. [sent-23, score-0.345]
</p><p>9 The image clusters have a normalised mutual information (NMI) score of 0. [sent-24, score-0.147]
</p><p>10 These models can be supervised at the scene level, object level, or both. [sent-31, score-0.07]
</p><p>11 Some of the aforementioned models can also be used in a fully unsupervised setting, where no annotation or label data is available. [sent-36, score-0.093]
</p><p>12 Generally unsupervised methods for understanding visual data have been given less focus than supervised and weakly-supervised methods. [sent-39, score-0.13]
</p><p>13 There has been research into unsupervised methods for object discovery [17, 19, 24, 11] and scene discovery [8, 14], which are related but distinct problems to scene understanding. [sent-41, score-0.147]
</p><p>14 In this paper, we present a Bayesian graphical model specialised for truly unsupervised scene understanding applications. [sent-42, score-0.132]
</p><p>15 It is able to model multiple albums of images at both scene and object levels without human supervi-  sion. [sent-44, score-0.081]
</p><p>16 Rather than relying on human-generated scene labels, it infers scene-types by clustering images. [sent-45, score-0.098]
</p><p>17 These object-types are formed by simultaneously clustering image-segment descriptors hence multiple-source. [sent-47, score-0.117]
</p><p>18 The primary contribution of this work is in showing that the MCM can synergistically cluster both image and segment descriptors. [sent-54, score-0.302]
</p><p>19 It outperforms unsupervised models that only consider one source of information. [sent-55, score-0.075]
</p><p>20 There are T image clusters and K segment clusters. [sent-62, score-0.308]
</p><p>21 The number of clusters is inferred from the data. [sent-63, score-0.122]
</p><p>22 Each of these albums has Ij images, indexed by i, which in turn contain Nji non-overlapping segments or superpixels, indexed by n. [sent-65, score-0.118]
</p><p>23 Each segment in an image has an associated descriptor, xjin ∈ RDx . [sent-66, score-0.33]
</p><p>24 mixture models, we have an indicator variable zjin |(yji = t) ∼ Categ(βt), which assigns segment observatio|n(ys to= object-types (segment clusters) by taking a value in {1, . [sent-80, score-0.385]
</p><p>25 ll S segment indicators in an image, a distribution of objects, βt, ctoa tboer sd irnaw ann ifmroamge. [sent-88, score-0.223]
</p><p>26 E∼acWh image also has an associated descriptor, wji ∈ RDw , which is distributed according to an album-specific mixture of Gaussians;  {zjin}nN=ji1,  (γΛk)-1  ? [sent-95, score-0.178]
</p><p>27 We  33445647  re-use the indicators yji to assign each image descriptor to an associated image cluster, or scene-type, t. [sent-104, score-0.253]
</p><p>28 These indicators are drawn from a Categorical distribution, yji ∼ Categ(πj), where πj is the set of all t mixture weights i∼n album j. [sent-105, score-0.313]
</p><p>29 In this way the learned scene-type cluster is serving a similar role to a scene label as used by supervised algorithms. [sent-121, score-0.186]
</p><p>30 All of this information is transferred through the shared yji indicator. [sent-123, score-0.186]
</p><p>31 Draw an image observation ∼from Ca ttheeg (chπosen image cluster wji | (yji = t) ∼ N(ηt, Ψt). [sent-151, score-0.26]
</p><p>32 Choose a segment cluster zjin | (yji = t) ∼ Categ(βt). [sent-157, score-0.446]
</p><p>33 Draw a segment observation from the segment cluster xjin | (zjin = k) ∼ N(μk, Λk). [sent-159, score-0.632]
</p><p>34 We chose to model spatial layout at the descriptor level since it requires less computation complexity than modelling segment spatial layout directly in the MCM. [sent-164, score-0.282]
</p><p>35 This allows the MCM to scale to larger datasets, but does not allow spatial information to directly influence segment clustering. [sent-165, score-0.21]
</p><p>36 More details of the image and segment descriptors are given in section 4. [sent-166, score-0.241]
</p><p>37 are expectations with respect to the variational posterior ]di asrteri ebux-tions, all of which are given in the supplementary material. [sent-195, score-0.076]
</p><p>38 We can see in (5) that the image labels, yji are calculated as an exponential sum of Gaussian and Multinomial loglikelihoods, weighted by the jth album’s mixture weights. [sent-196, score-0.22]
</p><p>39 Similarly, the variational posterior over the segment labels, zjin, is,  q(zjin= k) =Zz1jinexp? [sent-198, score-0.262]
</p><p>40 bilities, q(yjin = t), assigns more or less likelihood to the current 33445658  segment cluster, k, based on the probability of the image belonging to a scene-type, t. [sent-211, score-0.207]
</p><p>41 The sum in (8) must be performed in descending cluster size order, as per [10]. [sent-236, score-0.116]
</p><p>42 This value is not used for the segment descriptors since they are whitened, see section 4. [sent-242, score-0.241]
</p><p>43 Cwidth,i and Cwidth,s (i for image, s for segment) are tunable parameters that encode the  a-priori “width” of the mixtures (diagonal magnitude of the prior cluster covariances), and influence the number of clusters found. [sent-243, score-0.279]
</p><p>44 If the number of clusters, T and K, is known or set to some large value, the indicator and posterior hyperparameter updates can be iterated until F converges to a lpoacraalm metaexrim upudmat. [sent-245, score-0.106]
</p><p>45 We have found that bettpeern clustering r neastuulrtsa can r bisee eo ibnta Fin. [sent-247, score-0.082]
</p><p>46 ed W ief we guide tdh teh satea brecthfor the segment clusters. [sent-248, score-0.186]
</p><p>47 The segment-cluster search heuristic we use is a much faster, greedy version of the exhaustive heuristic presented in [10]. [sent-249, score-0.1]
</p><p>48 The MCM starts with K = 1 segment clusters, and iterates until convergence. [sent-250, score-0.186]
</p><p>49 Then the segment cluster is split in a direction perpendicular to its principal axis. [sent-251, score-0.337]
</p><p>50 These two clusters are then refined by running variational Bayes over them for a limited number of iterations. [sent-252, score-0.167]
</p><p>51 as F Finc isre eassteimd iante vdal wueit,h hth teh split wisl accepted adn sdp tlhite, whole model is again iterated until convergence. [sent-254, score-0.076]
</p><p>52 The exhaustive heuristic proceeds by trialling every possible cluster split between each model convergence stage, and only accepts the split that maximises F. [sent-256, score-0.236]
</p><p>53 In our “split-tally” heuristic, we greedily guess which  cluster to split first by ranking all clusters’ approximate contribution to F (details in the supplementary material). [sent-259, score-0.151]
</p><p>54 Also, a tally ins kept (odfe thaoiwls many tuipmpelse a ecnltuasrteyr m mhaatse previously failed a split trial. [sent-260, score-0.089]
</p><p>55 The first cluster split to increase F is accepted, and the tally for the original cluster is resFet. [sent-262, score-0.321]
</p><p>56 We have found this split-tally heuristic greatly reduces run-time, without much impact to performance, mostly because of the tally. [sent-264, score-0.087]
</p><p>57 Image Representation Being an unsupervised algorithm, the MCM relies heavily on highly discriminative visual descriptors. [sent-268, score-0.075]
</p><p>58 We have chosen unsupervised feature learning algorithms for this task. [sent-269, score-0.075]
</p><p>59 They keep with the unsupervised theme of this work, and have lead to excellent performance in a number of classification tasks, e. [sent-270, score-0.075]
</p><p>60 Images wji For the image descriptors, wji, we use a modified sparse coding spatial pyramid matching (ScSPM) descriptor [27]. [sent-275, score-0.174]
</p><p>61 We have found lpitattlceh etos no 6 r×ed1u6ct pioinxe ilsn, c wlaisthsi afic sattriiodne aonfd 8 clustering performance doing this. [sent-277, score-0.082]
</p><p>62 This is far too large to use with a Gaussian model, but we have found these codes are highly compressible with (randomised) PCA – to the point that we can compress them to Dw = 20 while still achieving excellent image clustering performance. [sent-280, score-0.082]
</p><p>63 Segments xjin Out of the many segment descriptors tried, it was found that pooling dense independent component analysis (ICA) codes within segments gave the best results. [sent-283, score-0.434]
</p><p>64 fL as tis mean tshalift n algorithm [ix5]is used to segment the original images into sets of pixels, Sjin. [sent-289, score-0.186]
</p><p>65 The ICA responses are transformed and mean-pooled within each segment in the following manner:  x? [sent-291, score-0.186]
</p><p>66 Sjinlog|rl|  (10)  These transformations greatly improved segment clustering performance. [sent-293, score-0.265]
</p><p>67 We conjecture that the absolute value makes the descriptors invariant to 90 degree phase shifts in rl . [sent-294, score-0.083]
</p><p>68 iTnhge fhiena l o segment descriptors, xjin, are boabctkain toed ( by P,C∞A)-. [sent-296, score-0.186]
</p><p>69 ScSPM descriptors encode the spatial layout and structural information of an image (the “gist”), whereas the ICA features encode fine-grained colour and texture information. [sent-302, score-0.141]
</p><p>70 We use three standard datasets (single al-  bum) and a large novel dataset consisting of twelve surveys (albums) from an autonomous underwater vehicle (AUV). [sent-306, score-0.105]
</p><p>71 Normalised mutual information (NMI) [20] is used to compare the clustering results to the ground truth image and segment labels. [sent-307, score-0.248]
</p><p>72 This is a fairly common measure in the clustering literature as it permits performance to be compared in situations where the number of ground truth classes and clusters are different. [sent-308, score-0.226]
</p><p>73 We also estimate the mean accuracy for the clustering results when benchmarking against supervised and weaklysupervised algorithms. [sent-311, score-0.096]
</p><p>74 This is done using the contingency table used to calculate NMI, which is just a table with the number of rows equalling the number of truth classes, and the number of columns equalling the number of clusters. [sent-312, score-0.108]
</p><p>75 Each cell in the table is a count of the number of observations assigned to the corresponding class and cluster labels. [sent-313, score-0.116]
</p><p>76 Some classes will have zero counts, and multiple clusters may be merged into one class. [sent-315, score-0.144]
</p><p>77 We believe this is entirely unbiased, but may heavily penalise the clustering results in situations where no clusters map to a class. [sent-316, score-0.204]
</p><p>78 , when many clusters are found  there is a greater chance they will be merged into the correct classes. [sent-319, score-0.142]
</p><p>79 We also compare the MCM to the unsupervised variational Dirichlet process (VDP) of [10], latent Dirichlet allocation [3] with Gaussian clusters (G-LDA), and self-tuning spectral clustering (SC) [28]. [sent-325, score-0.325]
</p><p>80 The VDP, G-LDA, and SC can only cluster one descriptor source (image or segment) at once. [sent-326, score-0.146]
</p><p>81 We have modified the VDP and G-LDA to use the same cluster splitting heuristic discussed in section 3, and the same priors as the MCM. [sent-327, score-0.166]
</p><p>82 We use SC for clustering the image ScSPM descriptors only, and we run it with 10 random starts of Kmeans given the true number of classes. [sent-329, score-0.117]
</p><p>83 Microsoft Research Classes  ×  The first dataset considered is Microsoft’s MSRC v2 dataset, which has both scene and object labels and is used by [7, 12]. [sent-335, score-0.075]
</p><p>84 s Wt feo fro tuhned dI tChAat descriptors (with a dictionary of 50 bases). [sent-340, score-0.075]
</p><p>85 The results for image clustering/classification are given in Table 1, with the line separating the unsupervised from the weakly- and supervised algorithms. [sent-341, score-0.109]
</p><p>86 However, the MCM still manages to achieve visually consistent image and segment clusters, see Figure 1. [sent-403, score-0.186]
</p><p>87 In order to assign a segment a ground-truth label, the mode of the pixels in the segment had to be of the label type. [sent-405, score-0.372]
</p><p>88 To quantify the MCM’s segment clustering performance we ran it for an array of Cwidth,s values and compared it against the unsupervised VDP and G-LDA algorithms. [sent-406, score-0.323]
</p><p>89 The VDP clusters all segments without any notion of context, and G-LDA can model each image as having its own proportions of segment clusters (image context). [sent-407, score-0.459]
</p><p>90 glcSu 231000 VGMD−CLMPDA (emis) T11055000000 2  4  6  8  10  Segment cluster prior, Cwidth,s  Figure 4. [sent-415, score-0.116]
</p><p>91 a gTeh ep segment lrakbeedls b feosrt this dataset were unconstrained in their categories, and so using the LabelMe Matlab toolbox, we combined all of the labels with 5 or more instances into 22 classes (given in the supplementary material). [sent-455, score-0.247]
</p><p>92 A random selection of images from 5 of the 10 image clusters found by the MCM on the UIUC dataset (a), with some of the (30) corresponding segment clusters in (b). [sent-461, score-0.467]
</p><p>93 In this case it seems the segment clusters may be confounding the MCM image clustering somewhat. [sent-510, score-0.37]
</p><p>94 From Figure 4 we can see the MCM again far outperforms the other unsupervised algorithms for segment clustering, demonstrating the importance of scene-type context for object recognition. [sent-511, score-0.261]
</p><p>95 This dataset depicts 8 types of sporting events and has 1579 images (maximum dimension of 320 pixels), unfortunately it has no segment labels. [sent-515, score-0.224]
</p><p>96 We use the same segment descriptor settings as the MSRC dataset. [sent-516, score-0.216]
</p><p>97 Note that the algorithm from [7] is also fully unsupervised for this dataset. [sent-518, score-0.075]
</p><p>98 Image classification in this dataset is more difficult than  ×  Image cluster prior, Cwidth,i  Figure 6. [sent-519, score-0.133]
</p><p>99 Robotic Dataset  The last dataset we use is a novel dataset containing images of various underwater habitats obtained by an AUV from J = 12 deployments off of the east coast of Tasmania, Australia [26]. [sent-528, score-0.087]
</p><p>100 The monochrome image of the pair is used for the ScSPM descriptors, and the colour for the ICA segment descriptors. [sent-530, score-0.205]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mcm', 0.672), ('vdp', 0.343), ('scspm', 0.226), ('yji', 0.186), ('segment', 0.186), ('nmi', 0.176), ('wji', 0.144), ('xjin', 0.144), ('zjin', 0.144), ('clusters', 0.122), ('cluster', 0.116), ('ica', 0.102), ('unsupervised', 0.075), ('categ', 0.072), ('nji', 0.072), ('clustering', 0.062), ('dirichlet', 0.058), ('sc', 0.058), ('msrc', 0.058), ('slda', 0.056), ('descriptors', 0.055), ('auv', 0.054), ('gdir', 0.054), ('lsbp', 0.054), ('tally', 0.054), ('eq', 0.054), ('underwater', 0.053), ('uiuc', 0.051), ('heuristic', 0.05), ('jt', 0.049), ('vjt', 0.048), ('labelme', 0.046), ('variational', 0.045), ('albums', 0.045), ('generalised', 0.044), ('tk', 0.043), ('confusion', 0.038), ('indicators', 0.037), ('cmoavx', 0.036), ('contingency', 0.036), ('equalling', 0.036), ('outpace', 0.036), ('reef', 0.036), ('rubble', 0.036), ('shell', 0.036), ('ttrunc', 0.036), ('wholeimage', 0.036), ('scene', 0.036), ('album', 0.036), ('du', 0.036), ('split', 0.035), ('autonomous', 0.035), ('supervised', 0.034), ('mixture', 0.034), ('layout', 0.033), ('logn', 0.032), ('disclda', 0.032), ('screw', 0.032), ('posterior', 0.031), ('descriptor', 0.03), ('recognise', 0.03), ('maximising', 0.03), ('segments', 0.029), ('summarise', 0.028), ('rl', 0.028), ('hyperparameter', 0.027), ('updates', 0.026), ('normalised', 0.025), ('sand', 0.025), ('influence', 0.024), ('draw', 0.024), ('initialisation', 0.024), ('whitening', 0.024), ('patches', 0.024), ('dw', 0.023), ('indexed', 0.022), ('iterated', 0.022), ('multinomial', 0.022), ('ste', 0.022), ('sydney', 0.022), ('labels', 0.022), ('classes', 0.022), ('allocation', 0.021), ('understanding', 0.021), ('events', 0.021), ('assigns', 0.021), ('sport', 0.021), ('situations', 0.02), ('dictionary', 0.02), ('gc', 0.02), ('found', 0.02), ('ij', 0.02), ('weights', 0.02), ('colour', 0.019), ('accepted', 0.019), ('annotation', 0.018), ('vehicles', 0.018), ('dataset', 0.017), ('encode', 0.017), ('greatly', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="412-tfidf-1" href="./iccv-2013-Synergistic_Clustering_of_Image_and_Segment_Descriptors_for_Unsupervised_Scene_Understanding.html">412 iccv-2013-Synergistic Clustering of Image and Segment Descriptors for Unsupervised Scene Understanding</a></p>
<p>Author: Daniel M. Steinberg, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: With the advent of cheap, high fidelity, digital imaging systems, the quantity and rate of generation of visual data can dramatically outpace a humans ability to label or annotate it. In these situations there is scope for the use of unsupervised approaches that can model these datasets and automatically summarise their content. To this end, we present a totally unsupervised, and annotation-less, model for scene understanding. This model can simultaneously cluster whole-image and segment descriptors, therebyforming an unsupervised model of scenes and objects. We show that this model outperforms other unsupervised models that can only cluster one source of information (image or segment) at once. We are able to compare unsupervised and supervised techniques using standard measures derived from confusion matrices and contingency tables. This shows that our unsupervised model is competitive with current supervised and weakly-supervised models for scene understanding on standard datasets. We also demonstrate our model operating on a dataset with more than 100,000 images col- lected by an autonomous underwater vehicle.</p><p>2 0.11777014 <a title="412-tfidf-2" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>Author: Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, James M. Rehg</p><p>Abstract: We propose an unsupervised video segmentation approach by simultaneously tracking multiple holistic figureground segments. Segment tracks are initialized from a pool of segment proposals generated from a figure-ground segmentation algorithm. Then, online non-local appearance models are trained incrementally for each track using a multi-output regularized least squares formulation. By using the same set of training examples for all segment tracks, a computational trick allows us to track hundreds of segment tracks efficiently, as well as perform optimal online updates in closed-form. Besides, a new composite statistical inference approach is proposed for refining the obtained segment tracks, which breaks down the initial segment proposals and recombines for better ones by utilizing highorder statistic estimates from the appearance model and enforcing temporal consistency. For evaluating the algorithm, a dataset, SegTrack v2, is collected with about 1,000 frames with pixel-level annotations. The proposed framework outperforms state-of-the-art approaches in the dataset, show- ing its efficiency and robustness to challenges in different video sequences.</p><p>3 0.095705479 <a title="412-tfidf-3" href="./iccv-2013-Correntropy_Induced_L2_Graph_for_Robust_Subspace_Clustering.html">94 iccv-2013-Correntropy Induced L2 Graph for Robust Subspace Clustering</a></p>
<p>Author: Canyi Lu, Jinhui Tang, Min Lin, Liang Lin, Shuicheng Yan, Zhouchen Lin</p><p>Abstract: In this paper, we study the robust subspace clustering problem, which aims to cluster the given possibly noisy data points into their underlying subspaces. A large pool of previous subspace clustering methods focus on the graph construction by different regularization of the representation coefficient. We instead focus on the robustness of the model to non-Gaussian noises. We propose a new robust clustering method by using the correntropy induced metric, which is robust for handling the non-Gaussian and impulsive noises. Also we further extend the method for handling the data with outlier rows/features. The multiplicative form of half-quadratic optimization is used to optimize the nonconvex correntropy objective function of the proposed models. Extensive experiments on face datasets well demonstrate that the proposed methods are more robust to corruptions and occlusions.</p><p>4 0.074234165 <a title="412-tfidf-4" href="./iccv-2013-Low-Rank_Sparse_Coding_for_Image_Classification.html">258 iccv-2013-Low-Rank Sparse Coding for Image Classification</a></p>
<p>Author: Tianzhu Zhang, Bernard Ghanem, Si Liu, Changsheng Xu, Narendra Ahuja</p><p>Abstract: In this paper, we propose a low-rank sparse coding (LRSC) method that exploits local structure information among features in an image for the purpose of image-level classification. LRSC represents densely sampled SIFT descriptors, in a spatial neighborhood, collectively as lowrank, sparse linear combinations of codewords. As such, it casts the feature coding problem as a low-rank matrix learning problem, which is different from previous methods that encode features independently. This LRSC has a number of attractive properties. (1) It encourages sparsity in feature codes, locality in codebook construction, and low-rankness for spatial consistency. (2) LRSC encodes local features jointly by considering their low-rank structure information, and is computationally attractive. We evaluate the LRSC by comparing its performance on a set of challenging benchmarks with that of 7 popular coding and other state-of-theart methods. Our experiments show that by representing local features jointly, LRSC not only outperforms the state-ofthe-art in classification accuracy but also improves the time complexity of methods that use a similar sparse linear repre- sentation model for feature coding [36].</p><p>5 0.069663957 <a title="412-tfidf-5" href="./iccv-2013-Pyramid_Coding_for_Functional_Scene_Element_Recognition_in_Video_Scenes.html">331 iccv-2013-Pyramid Coding for Functional Scene Element Recognition in Video Scenes</a></p>
<p>Author: Eran Swears, Anthony Hoogs, Kim Boyer</p><p>Abstract: Recognizing functional scene elemeents in video scenes based on the behaviors of moving objects that interact with them is an emerging problem ooff interest. Existing approaches have a limited ability to chharacterize elements such as cross-walks, intersections, andd buildings that have low activity, are multi-modal, or havee indirect evidence. Our approach recognizes the low activvity and multi-model elements (crosswalks/intersections) by introducing a hierarchy of descriptive clusters to fform a pyramid of codebooks that is sparse in the numbber of clusters and dense in content. The incorporation oof local behavioral context such as person-enter-building aand vehicle-parking nearby enables the detection of elemennts that do not have direct motion-based evidence, e.g. buuildings. These two contributions significantly improvee scene element recognition when compared against thhree state-of-the-art approaches. Results are shown on tyypical ground level surveillance video and for the first time on the more complex Wide Area Motion Imagery.</p><p>6 0.066829853 <a title="412-tfidf-6" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>7 0.064060688 <a title="412-tfidf-7" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>8 0.06159823 <a title="412-tfidf-8" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>9 0.055201758 <a title="412-tfidf-9" href="./iccv-2013-BOLD_Features_to_Detect_Texture-less_Objects.html">57 iccv-2013-BOLD Features to Detect Texture-less Objects</a></p>
<p>10 0.051750924 <a title="412-tfidf-10" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>11 0.050876036 <a title="412-tfidf-11" href="./iccv-2013-Characterizing_Layouts_of_Outdoor_Scenes_Using_Spatial_Topic_Processes.html">72 iccv-2013-Characterizing Layouts of Outdoor Scenes Using Spatial Topic Processes</a></p>
<p>12 0.050612044 <a title="412-tfidf-12" href="./iccv-2013-Class-Specific_Simplex-Latent_Dirichlet_Allocation_for_Image_Classification.html">73 iccv-2013-Class-Specific Simplex-Latent Dirichlet Allocation for Image Classification</a></p>
<p>13 0.049831729 <a title="412-tfidf-13" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<p>14 0.04980484 <a title="412-tfidf-14" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>15 0.047579512 <a title="412-tfidf-15" href="./iccv-2013-Video_Synopsis_by_Heterogeneous_Multi-source_Correlation.html">443 iccv-2013-Video Synopsis by Heterogeneous Multi-source Correlation</a></p>
<p>16 0.047551997 <a title="412-tfidf-16" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>17 0.045299225 <a title="412-tfidf-17" href="./iccv-2013-Stacked_Predictive_Sparse_Coding_for_Classification_of_Distinct_Regions_in_Tumor_Histopathology.html">401 iccv-2013-Stacked Predictive Sparse Coding for Classification of Distinct Regions in Tumor Histopathology</a></p>
<p>18 0.045040723 <a title="412-tfidf-18" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>19 0.044475567 <a title="412-tfidf-19" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>20 0.044279017 <a title="412-tfidf-20" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.113), (1, 0.016), (2, 0.008), (3, 0.006), (4, -0.004), (5, 0.023), (6, -0.037), (7, 0.034), (8, 0.007), (9, -0.041), (10, 0.024), (11, 0.021), (12, -0.013), (13, 0.012), (14, -0.045), (15, -0.041), (16, -0.016), (17, -0.004), (18, 0.004), (19, -0.007), (20, -0.003), (21, 0.005), (22, 0.018), (23, -0.036), (24, 0.045), (25, -0.034), (26, 0.022), (27, -0.025), (28, -0.006), (29, 0.02), (30, 0.018), (31, 0.014), (32, -0.044), (33, -0.01), (34, -0.102), (35, 0.06), (36, 0.016), (37, -0.063), (38, 0.017), (39, -0.057), (40, 0.029), (41, 0.115), (42, -0.081), (43, -0.059), (44, -0.044), (45, -0.049), (46, -0.128), (47, 0.033), (48, -0.056), (49, 0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91941267 <a title="412-lsi-1" href="./iccv-2013-Synergistic_Clustering_of_Image_and_Segment_Descriptors_for_Unsupervised_Scene_Understanding.html">412 iccv-2013-Synergistic Clustering of Image and Segment Descriptors for Unsupervised Scene Understanding</a></p>
<p>Author: Daniel M. Steinberg, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: With the advent of cheap, high fidelity, digital imaging systems, the quantity and rate of generation of visual data can dramatically outpace a humans ability to label or annotate it. In these situations there is scope for the use of unsupervised approaches that can model these datasets and automatically summarise their content. To this end, we present a totally unsupervised, and annotation-less, model for scene understanding. This model can simultaneously cluster whole-image and segment descriptors, therebyforming an unsupervised model of scenes and objects. We show that this model outperforms other unsupervised models that can only cluster one source of information (image or segment) at once. We are able to compare unsupervised and supervised techniques using standard measures derived from confusion matrices and contingency tables. This shows that our unsupervised model is competitive with current supervised and weakly-supervised models for scene understanding on standard datasets. We also demonstrate our model operating on a dataset with more than 100,000 images col- lected by an autonomous underwater vehicle.</p><p>2 0.62746215 <a title="412-lsi-2" href="./iccv-2013-Pyramid_Coding_for_Functional_Scene_Element_Recognition_in_Video_Scenes.html">331 iccv-2013-Pyramid Coding for Functional Scene Element Recognition in Video Scenes</a></p>
<p>Author: Eran Swears, Anthony Hoogs, Kim Boyer</p><p>Abstract: Recognizing functional scene elemeents in video scenes based on the behaviors of moving objects that interact with them is an emerging problem ooff interest. Existing approaches have a limited ability to chharacterize elements such as cross-walks, intersections, andd buildings that have low activity, are multi-modal, or havee indirect evidence. Our approach recognizes the low activvity and multi-model elements (crosswalks/intersections) by introducing a hierarchy of descriptive clusters to fform a pyramid of codebooks that is sparse in the numbber of clusters and dense in content. The incorporation oof local behavioral context such as person-enter-building aand vehicle-parking nearby enables the detection of elemennts that do not have direct motion-based evidence, e.g. buuildings. These two contributions significantly improvee scene element recognition when compared against thhree state-of-the-art approaches. Results are shown on tyypical ground level surveillance video and for the first time on the more complex Wide Area Motion Imagery.</p><p>3 0.60944617 <a title="412-lsi-3" href="./iccv-2013-BOLD_Features_to_Detect_Texture-less_Objects.html">57 iccv-2013-BOLD Features to Detect Texture-less Objects</a></p>
<p>Author: Federico Tombari, Alessandro Franchi, Luigi Di_Stefano</p><p>Abstract: Object detection in images withstanding significant clutter and occlusion is still a challenging task whenever the object surface is characterized by poor informative content. We propose to tackle this problem by a compact and distinctive representation of groups of neighboring line segments aggregated over limited spatial supports and invariant to rotation, translation and scale changes. Peculiarly, our proposal allows for leveraging on the inherent strengths of descriptor-based approaches, i.e. robustness to occlusion and clutter and scalability with respect to the size of the model library, also when dealing with scarcely textured objects.</p><p>4 0.56856596 <a title="412-lsi-4" href="./iccv-2013-Video_Segmentation_by_Tracking_Many_Figure-Ground_Segments.html">442 iccv-2013-Video Segmentation by Tracking Many Figure-Ground Segments</a></p>
<p>Author: Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, James M. Rehg</p><p>Abstract: We propose an unsupervised video segmentation approach by simultaneously tracking multiple holistic figureground segments. Segment tracks are initialized from a pool of segment proposals generated from a figure-ground segmentation algorithm. Then, online non-local appearance models are trained incrementally for each track using a multi-output regularized least squares formulation. By using the same set of training examples for all segment tracks, a computational trick allows us to track hundreds of segment tracks efficiently, as well as perform optimal online updates in closed-form. Besides, a new composite statistical inference approach is proposed for refining the obtained segment tracks, which breaks down the initial segment proposals and recombines for better ones by utilizing highorder statistic estimates from the appearance model and enforcing temporal consistency. For evaluating the algorithm, a dataset, SegTrack v2, is collected with about 1,000 frames with pixel-level annotations. The proposed framework outperforms state-of-the-art approaches in the dataset, show- ing its efficiency and robustness to challenges in different video sequences.</p><p>5 0.55873114 <a title="412-lsi-5" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>Author: Alon Faktor, Michal Irani</p><p>Abstract: Given a set of images which share an object from the same semantic category, we would like to co-segment the shared object. We define ‘good’ co-segments to be ones which can be easily composed (like a puzzle) from large pieces of other co-segments, yet are difficult to compose from remaining image parts. These pieces must not only match well but also be statistically significant (hard to compose at random). This gives rise to co-segmentation of objects in very challenging scenarios with large variations in appearance, shape and large amounts of clutter. We further show how multiple images can collaborate and “score each others ’ co-segments to improve the overall fidelity and accuracy of the co-segmentation. Our co-segmentation can be applied both to large image collections, as well as to very few images (where there is too little data for unsupervised learning). At the extreme, it can be applied even to a single image, to extract its co-occurring objects. Our approach obtains state-of-the-art results on benchmark datasets. We further show very encouraging co-segmentation results on the challenging PASCAL-VOC dataset. ”</p><p>6 0.51055425 <a title="412-lsi-6" href="./iccv-2013-Efficient_3D_Scene_Labeling_Using_Fields_of_Trees.html">132 iccv-2013-Efficient 3D Scene Labeling Using Fields of Trees</a></p>
<p>7 0.50419796 <a title="412-lsi-7" href="./iccv-2013-Scene_Collaging%3A_Analysis_and_Synthesis_of_Natural_Images_with_Semantic_Layers.html">375 iccv-2013-Scene Collaging: Analysis and Synthesis of Natural Images with Semantic Layers</a></p>
<p>8 0.47891158 <a title="412-lsi-8" href="./iccv-2013-A_New_Adaptive_Segmental_Matching_Measure_for_Human_Activity_Recognition.html">22 iccv-2013-A New Adaptive Segmental Matching Measure for Human Activity Recognition</a></p>
<p>9 0.46179754 <a title="412-lsi-9" href="./iccv-2013-Data-Driven_3D_Primitives_for_Single_Image_Understanding.html">102 iccv-2013-Data-Driven 3D Primitives for Single Image Understanding</a></p>
<p>10 0.46162972 <a title="412-lsi-10" href="./iccv-2013-Drosophila_Embryo_Stage_Annotation_Using_Label_Propagation.html">125 iccv-2013-Drosophila Embryo Stage Annotation Using Label Propagation</a></p>
<p>11 0.45340872 <a title="412-lsi-11" href="./iccv-2013-SIFTpack%3A_A_Compact_Representation_for_Efficient_SIFT_Matching.html">365 iccv-2013-SIFTpack: A Compact Representation for Efficient SIFT Matching</a></p>
<p>12 0.44946009 <a title="412-lsi-12" href="./iccv-2013-Low-Rank_Sparse_Coding_for_Image_Classification.html">258 iccv-2013-Low-Rank Sparse Coding for Image Classification</a></p>
<p>13 0.44772789 <a title="412-lsi-13" href="./iccv-2013-Detecting_Irregular_Curvilinear_Structures_in_Gray_Scale_and_Color_Imagery_Using_Multi-directional_Oriented_Flux.html">112 iccv-2013-Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using Multi-directional Oriented Flux</a></p>
<p>14 0.43512812 <a title="412-lsi-14" href="./iccv-2013-Characterizing_Layouts_of_Outdoor_Scenes_Using_Spatial_Topic_Processes.html">72 iccv-2013-Characterizing Layouts of Outdoor Scenes Using Spatial Topic Processes</a></p>
<p>15 0.43107086 <a title="412-lsi-15" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>16 0.42987347 <a title="412-lsi-16" href="./iccv-2013-Pose-Configurable_Generic_Tracking_of_Elongated_Objects.html">320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</a></p>
<p>17 0.42723772 <a title="412-lsi-17" href="./iccv-2013-Class-Specific_Simplex-Latent_Dirichlet_Allocation_for_Image_Classification.html">73 iccv-2013-Class-Specific Simplex-Latent Dirichlet Allocation for Image Classification</a></p>
<p>18 0.42598444 <a title="412-lsi-18" href="./iccv-2013-Video_Synopsis_by_Heterogeneous_Multi-source_Correlation.html">443 iccv-2013-Video Synopsis by Heterogeneous Multi-source Correlation</a></p>
<p>19 0.42220107 <a title="412-lsi-19" href="./iccv-2013-Action_Recognition_and_Localization_by_Hierarchical_Space-Time_Segments.html">37 iccv-2013-Action Recognition and Localization by Hierarchical Space-Time Segments</a></p>
<p>20 0.42106435 <a title="412-lsi-20" href="./iccv-2013-Fingerspelling_Recognition_with_Semi-Markov_Conditional_Random_Fields.html">170 iccv-2013-Fingerspelling Recognition with Semi-Markov Conditional Random Fields</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.08), (7, 0.019), (12, 0.02), (13, 0.012), (26, 0.055), (31, 0.059), (40, 0.017), (42, 0.069), (48, 0.015), (64, 0.045), (73, 0.037), (89, 0.127), (97, 0.291), (98, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83735305 <a title="412-lda-1" href="./iccv-2013-Saliency_and_Human_Fixations%3A_State-of-the-Art_and_Study_of_Comparison_Metrics.html">373 iccv-2013-Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics</a></p>
<p>Author: Nicolas Riche, Matthieu Duvinage, Matei Mancas, Bernard Gosselin, Thierry Dutoit</p><p>Abstract: Visual saliency has been an increasingly active research area in the last ten years with dozens of saliency models recently published. Nowadays, one of the big challenges in the field is to find a way to fairly evaluate all of these models. In this paper, on human eye fixations ,we compare the ranking of 12 state-of-the art saliency models using 12 similarity metrics. The comparison is done on Jian Li ’s database containing several hundreds of natural images. Based on Kendall concordance coefficient, it is shown that some of the metrics are strongly correlated leading to a redundancy in the performance metrics reported in the available benchmarks. On the other hand, other metrics provide a more diverse picture of models ’ overall performance. As a recommendation, three similarity metrics should be used to obtain a complete point of view of saliency model performance.</p><p>2 0.75521111 <a title="412-lda-2" href="./iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications.html">347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</a></p>
<p>Author: Hesamoddin Salehian, Guang Cheng, Baba C. Vemuri, Jeffrey Ho</p><p>Abstract: Symmetric positive-definite (SPD) matrices are ubiquitous in Computer Vision, Machine Learning and Medical Image Analysis. Finding the center/average of a population of such matrices is a common theme in many algorithms such as clustering, segmentation, principal geodesic analysis, etc. The center of a population of such matrices can be defined using a variety of distance/divergence measures as the minimizer of the sum of squared distances/divergences from the unknown center to the members of the population. It is well known that the computation of the Karcher mean for the space of SPD matrices which is a negativelycurved Riemannian manifold is computationally expensive. Recently, the LogDet divergence-based center was shown to be a computationally attractive alternative. However, the LogDet-based mean of more than two matrices can not be computed in closed form, which makes it computationally less attractive for large populations. In this paper we present a novel recursive estimator for center based on the Stein distance which is the square root of the LogDet di– vergence that is significantly faster than the batch mode computation of this center. The key theoretical contribution is a closed-form solution for the weighted Stein center of two SPD matrices, which is used in the recursive computation of the Stein center for a population of SPD matrices. Additionally, we show experimental evidence of the convergence of our recursive Stein center estimator to the batch mode Stein center. We present applications of our recursive estimator to K-means clustering and image indexing depicting significant time gains over corresponding algorithms that use the batch mode computations. For the latter application, we develop novel hashing functions using the Stein distance and apply it to publicly available data sets, and experimental results have shown favorable com– ∗This research was funded in part by the NIH grant NS066340 to BCV. †Corresponding author parisons to other competing methods.</p><p>same-paper 3 0.71776515 <a title="412-lda-3" href="./iccv-2013-Synergistic_Clustering_of_Image_and_Segment_Descriptors_for_Unsupervised_Scene_Understanding.html">412 iccv-2013-Synergistic Clustering of Image and Segment Descriptors for Unsupervised Scene Understanding</a></p>
<p>Author: Daniel M. Steinberg, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: With the advent of cheap, high fidelity, digital imaging systems, the quantity and rate of generation of visual data can dramatically outpace a humans ability to label or annotate it. In these situations there is scope for the use of unsupervised approaches that can model these datasets and automatically summarise their content. To this end, we present a totally unsupervised, and annotation-less, model for scene understanding. This model can simultaneously cluster whole-image and segment descriptors, therebyforming an unsupervised model of scenes and objects. We show that this model outperforms other unsupervised models that can only cluster one source of information (image or segment) at once. We are able to compare unsupervised and supervised techniques using standard measures derived from confusion matrices and contingency tables. This shows that our unsupervised model is competitive with current supervised and weakly-supervised models for scene understanding on standard datasets. We also demonstrate our model operating on a dataset with more than 100,000 images col- lected by an autonomous underwater vehicle.</p><p>4 0.68797553 <a title="412-lda-4" href="./iccv-2013-Large-Scale_Image_Annotation_by_Efficient_and_Robust_Kernel_Metric_Learning.html">227 iccv-2013-Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning</a></p>
<p>Author: Zheyun Feng, Rong Jin, Anil Jain</p><p>Abstract: One of the key challenges in search-based image annotation models is to define an appropriate similarity measure between images. Many kernel distance metric learning (KML) algorithms have been developed in order to capture the nonlinear relationships between visual features and semantics ofthe images. Onefundamental limitation in applying KML to image annotation is that it requires converting image annotations into binary constraints, leading to a significant information loss. In addition, most KML algorithms suffer from high computational cost due to the requirement that the learned matrix has to be positive semi-definitive (PSD). In this paper, we propose a robust kernel metric learning (RKML) algorithm based on the regression technique that is able to directly utilize image annotations. The proposed method is also computationally more efficient because PSD property is automatically ensured by regression. We provide the theoretical guarantee for the proposed algorithm, and verify its efficiency and effectiveness for image annotation by comparing it to state-of-the-art approaches for both distance metric learning and image annotation. ,</p><p>5 0.66936606 <a title="412-lda-5" href="./iccv-2013-Analysis_of_Scores%2C_Datasets%2C_and_Models_in_Visual_Saliency_Prediction.html">50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</a></p>
<p>Author: Ali Borji, Hamed R. Tavakoli, Dicky N. Sihite, Laurent Itti</p><p>Abstract: Significant recent progress has been made in developing high-quality saliency models. However, less effort has been undertaken on fair assessment of these models, over large standardized datasets and correctly addressing confounding factors. In this study, we pursue a critical and quantitative look at challenges (e.g., center-bias, map smoothing) in saliency modeling and the way they affect model accuracy. We quantitatively compare 32 state-of-the-art models (using the shuffled AUC score to discount center-bias) on 4 benchmark eye movement datasets, for prediction of human fixation locations and scanpath sequence. We also account for the role of map smoothing. We find that, although model rankings vary, some (e.g., AWS, LG, AIM, and HouNIPS) consistently outperform other models over all datasets. Some models work well for prediction of both fixation locations and scanpath sequence (e.g., Judd, GBVS). Our results show low prediction accuracy for models over emotional stimuli from the NUSEF dataset. Our last benchmark, for the first time, gauges the ability of models to decode the stimulus category from statistics of fixations, saccades, and model saliency values at fixated locations. In this test, ITTI and AIM models win over other models. Our benchmark provides a comprehensive high-level picture of the strengths and weaknesses of many popular models, and suggests future research directions in saliency modeling.</p><p>6 0.66737092 <a title="412-lda-6" href="./iccv-2013-Saliency_Detection_via_Dense_and_Sparse_Reconstruction.html">372 iccv-2013-Saliency Detection via Dense and Sparse Reconstruction</a></p>
<p>7 0.66732305 <a title="412-lda-7" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>8 0.65717947 <a title="412-lda-8" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>9 0.64044499 <a title="412-lda-9" href="./iccv-2013-Saliency_Detection%3A_A_Boolean_Map_Approach.html">369 iccv-2013-Saliency Detection: A Boolean Map Approach</a></p>
<p>10 0.60919285 <a title="412-lda-10" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>11 0.60438347 <a title="412-lda-11" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>12 0.60339391 <a title="412-lda-12" href="./iccv-2013-Saliency_Detection_via_Absorbing_Markov_Chain.html">371 iccv-2013-Saliency Detection via Absorbing Markov Chain</a></p>
<p>13 0.57227689 <a title="412-lda-13" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>14 0.57127053 <a title="412-lda-14" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>15 0.56663543 <a title="412-lda-15" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>16 0.55774653 <a title="412-lda-16" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>17 0.55562121 <a title="412-lda-17" href="./iccv-2013-Semantically-Based_Human_Scanpath_Estimation_with_HMMs.html">381 iccv-2013-Semantically-Based Human Scanpath Estimation with HMMs</a></p>
<p>18 0.55528051 <a title="412-lda-18" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>19 0.53983659 <a title="412-lda-19" href="./iccv-2013-Log-Euclidean_Kernels_for_Sparse_Representation_and_Dictionary_Learning.html">257 iccv-2013-Log-Euclidean Kernels for Sparse Representation and Dictionary Learning</a></p>
<p>20 0.53922772 <a title="412-lda-20" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
