<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>229 iccv-2013-Large-Scale Video Hashing via Structure Learning</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-229" href="#">iccv2013-229</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>229 iccv-2013-Large-Scale Video Hashing via Structure Learning</h1>
<br/><p>Source: <a title="iccv-2013-229-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Ye_Large-Scale_Video_Hashing_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Guangnan Ye, Dong Liu, Jun Wang, Shih-Fu Chang</p><p>Abstract: Recently, learning based hashing methods have become popular for indexing large-scale media data. Hashing methods map high-dimensional features to compact binary codes that are efficient to match and robust in preserving original similarity. However, most of the existing hashing methods treat videos as a simple aggregation of independent frames and index each video through combining the indexes of frames. The structure information of videos, e.g., discriminative local visual commonality and temporal consistency, is often neglected in the design of hash functions. In this paper, we propose a supervised method that explores the structure learning techniques to design efficient hash functions. The proposed video hashing method formulates a minimization problem over a structure-regularized empirical loss. In particular, the structure regularization exploits the common local visual patterns occurring in video frames that are associated with the same semantic class, and simultaneously preserves the temporal consistency over successive frames from the same video. We show that the minimization objective can be efficiently solved by an Acceler- ated Proximal Gradient (APG) method. Extensive experiments on two large video benchmark datasets (up to around 150K video clips with over 12 million frames) show that the proposed method significantly outperforms the state-ofthe-art hashing methods.</p><p>Reference: <a title="iccv-2013-229-reference" href="../iccv2013_reference/iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Recently, learning based hashing methods have become popular for indexing large-scale media data. [sent-8, score-0.581]
</p><p>2 However, most of the existing hashing methods treat videos as a simple aggregation of independent frames and index each video through combining the indexes of frames. [sent-10, score-0.994]
</p><p>3 , discriminative local visual commonality and temporal consistency, is often neglected in the design of hash functions. [sent-13, score-0.728]
</p><p>4 In this paper, we propose a supervised method that explores the structure learning techniques to design efficient hash functions. [sent-14, score-0.674]
</p><p>5 The proposed video hashing method formulates a minimization problem over a structure-regularized empirical loss. [sent-15, score-0.714]
</p><p>6 In particular, the structure regularization exploits the common local visual patterns occurring in video frames that are associated with the same semantic class, and simultaneously preserves the temporal consistency over successive frames from the same video. [sent-16, score-0.671]
</p><p>7 Extensive experiments on two large video benchmark datasets (up to around 150K video clips with over 12 million frames) show that the proposed method significantly outperforms the state-ofthe-art hashing methods. [sent-18, score-0.895]
</p><p>8 Besides the well-known issue of semantic gap, the computational cost is another bottleneck for content-based video search since exhaustive comparisons of low-level visual features are practically prohibitive, when handling a large collection of video clips. [sent-21, score-0.364]
</p><p>9 An illustration of the proposed hash code generation for videos within the event category “feeding an animal”. [sent-24, score-0.827]
</p><p>10 Temporal consistency is preserved, and successive frames are grouped and put into the same hash bucket. [sent-28, score-0.76]
</p><p>11 work of learning to hash has been well studied, and many new hashing methods have been developed through incorporating various machine learning techniques, ranging from unsupervised to semi-supervised to supervised learning [22, 21, 11, 7, 12, 8, 6]. [sent-29, score-1.172]
</p><p>12 The key idea for learning-based hashing is to leverage data properties or human supervision to derive compact yet accurate hash codes. [sent-30, score-1.078]
</p><p>13 Most of the existing hashing methods can be directly applied to index video data, such as the recent multiple feature based video  hashing [19] and submodular video hashing [3]. [sent-31, score-2.094]
</p><p>14 Despite of the promising results reported in the literature, the existing video hashing methods cannot explicitly encode the specific structure information in video clips, e. [sent-32, score-0.88]
</p><p>15 , the commonly shared local visual patterns by videos associated with the same semantic labels and the temporal consistency between successive frames. [sent-34, score-0.476]
</p><p>16 To address the above problems, we propose to explore the structure information to design novel video hashing methods. [sent-35, score-0.76]
</p><p>17 Notably, although each video clip contains fruitful local visual patterns, only a limited number of discriminative local patterns are shared by videos within the same semantic category. [sent-38, score-0.476]
</p><p>18 Therefore, the hashing method should ensure that the hash codes for successive frames to be as similar as possible (Figure 1). [sent-45, score-1.361]
</p><p>19 To incorporate these two types of structure characteris-  tics of videos, we propose a supervised framework with structure learning to design efficient linear hash functions for video indexing. [sent-46, score-0.902]
</p><p>20 To capture the common local patterns across all the video frames from the same category, the first regularization term imposes a ‘2,1-norm over the hash functions so that only a small number of informative feature dimensions are selected. [sent-48, score-0.879]
</p><p>21 In this way, we obtain consistent patterns across different videos, and improve the discrimination ability of the learned hash functions. [sent-52, score-0.568]
</p><p>22 The second regularization term uses a ‘∞-norm on the hash codes of successive frames, which enforces successive frames to receive similar hash codes and essentially preserves the temporal consistency in Hamming space. [sent-53, score-1.677]
</p><p>23 Extensive experiments over two large video benchmark datasets and comparisons with representative learning-based hashing methods demonstrate the superiority of the proposed video hashing method. [sent-55, score-1.391]
</p><p>24 Recently, learning to hash framework has been extensively investigated and various machine learning algorithms are incorporated for designing efficient hash functions. [sent-59, score-1.096]
</p><p>25 In the following, we briefly introduce several representative learning-based hashing methods and the recent applications on video indexing and retrieval. [sent-60, score-0.713]
</p><p>26 Unsupervised hashing methods often utilize the data properties such as distribution or manifold structure to design effective indexing schemes. [sent-61, score-0.635]
</p><p>27 For example, spectral hashing assumes that the data are sampled from a uniform distribution and partitions the data along their principal directions with the consideration of spatial frequencies [22]. [sent-62, score-0.555]
</p><p>28 Graph hashing explores the low-dimensional manifold structure of data to design compact hash codes [13]. [sent-63, score-1.242]
</p><p>29 Supervised hashing learning can be mainly categorized as pointwise and pairwise methods. [sent-64, score-0.598]
</p><p>30 Pointwise methods, such as boosted similarity sensitive coding [18] and deep neural network-based method [20], often treat the design of hash functions as a special classification problem and use samples’ labels in the training procedure. [sent-65, score-0.616]
</p><p>31 Pairwise methods take into account the pairwise relationship of samples in the hash function learning. [sent-66, score-0.572]
</p><p>32 As a popular formulation, many recent works, including binary reconstructive embedding [11], complementary hashing [23], supervised hashing with kernels [14], and iterative quantization [7] fall into the category of pairwise methods. [sent-67, score-1.23]
</p><p>33 Finally, semi-supervised  hashing method plays a tradeoff between supervised information and data properties to design robust hash functions, which aims at alleviating the defects from overfitting or insufficient training [21]. [sent-68, score-1.14]
</p><p>34 To our best knowledge, there are very limited studies on developing specific hash functions to index structured data like videos. [sent-70, score-0.558]
</p><p>35 [3] proposed a submodular hashing framework to index videos. [sent-74, score-0.572]
</p><p>36 [19] proposed a multiple feature based hashing for video near-duplicate detection. [sent-76, score-0.684]
</p><p>37 However, in those video hashing methods, conventional hashing methods like locality sensitive hashing [6] and spectral hashing [22] are often applied to generate binary codes. [sent-77, score-2.332]
</p><p>38 None of the existing methods really consider the special structure information like visual commonality and temporal consistency of videos to design structure-specific hash functions for video indexing. [sent-78, score-1.157]
</p><p>39 In contrast, our proposed video hashing method leverages video structure information in a supervised learning paradigm to derive optimal binary codes for large-scale video retrieval. [sent-79, score-1.249]
</p><p>40 , xi,ni] is a video consisting of 22227733  successive frames, yi ∈ {0, 1} is the label of video Xi1. [sent-91, score-0.408]
</p><p>41 Given a video frame x, we want to learn K-bit binary codes c ∈ {0, 1}K, which needs to design K binary hash fcuondcetsio cns ∈. [sent-94, score-0.917]
</p><p>42 {In0 t,1hi}s work, we consider the linear hash functions for their simplicity and efficiency. [sent-95, score-0.558]
</p><p>43 , K) can be defined as: ni  hk(x) = sgn(wk>x + bk), (1) where wk ∈ Rd is a hash hyperplane and bk is the intercept. [sent-99, score-0.618]
</p><p>44 The resulting hk (x) ∈ {−1, 1}, and the corresponding binary hash bit can x be) simply c,a1l}cu, alantedd t as ck o(rxr)e p=o n(1d +ing gh kb (inxa)r)y/2 h. [sent-101, score-0.617]
</p><p>45 a The Hamming distance between the hash codes of two frames xia and xjb from two videos Xi and Xj can be defined as: XK  d(xia,xjb) =  X(ck(xia)  − ck(xjb))2  =k41=kX1K=1(hk(xia) − hk(xjb) 2. [sent-102, score-1.163]
</p><p>46 (2)  Based on the above definition, a straightforward way to define the Hamming distance between videos Xi and Xj is:  D(Xi,Xj) =ni1njaXn=i1bXn=j1d(xia,xjb),  (3)  which means that the Hamming distance between two videos equals to the average Hamming distance of each pair of frames. [sent-103, score-0.382]
</p><p>47 The basic assumption behind this definition is that most of frames within a video should be useful for measuring the distance between videos, and similar pairwise metric methodology has been proven to be effective in [3]. [sent-104, score-0.297]
</p><p>48 However, the above function is not tractable due to the discrete nature of sgn function in each hash function. [sent-105, score-0.56]
</p><p>49 We formulate the following objective which learns the hash functions by minimizing a structure-regularized cost function: XN  mWin  i,Xj=1‘? [sent-114, score-0.584]
</p><p>50 Since each row of W will be multiplied with one specific dimension  of the features, making it zero will discard the influence of the feature dimension from hash function learning. [sent-123, score-0.526]
</p><p>51 The selected features correspond to the local common patterns in the video frames related to a certain video category, which convey discriminative information. [sent-125, score-0.474]
</p><p>52 The minimization of kW>xi,t − W>xi,t+1 k∞ ensures the hash vectors toiof tnw oof s kuWccessive− −fra Wmes as simkilar as possible [2], i. [sent-126, score-0.556]
</p><p>53 , encouraging the maximum absolute value of the entry-wise differences between two hash vectors to be zero. [sent-128, score-0.526]
</p><p>54 This accounts for the preservation of the temporal structure in the hash codes generated for all frames of a video. [sent-129, score-0.844]
</p><p>55 Notably, we apply ‘∞-norm here instead of ‘1-norm or ‘2-norm since ‘∞-norm will ensure stronger constraint that the two hash vectors of successive frames are close to each other. [sent-130, score-0.721]
</p><p>56 ˆy ij = 1 leads to D(Xi, Xj) ≤ while ˆy ij = −1 leads to D(Xi, Xj) > In th)is way, iwt heinlefo rˆ yces t=hat − −v1ide loeasd wsi ttho the same category label should be close to each other while videos with different category labels should be far apart. [sent-139, score-0.359]
</p><p>57 δ  22227744  With this loss term, we incorporate the supervision information into the hash function learning, leading to discriminative binary codes. [sent-141, score-0.622]
</p><p>58 To demonstrate the strength of using both spatial and temporal structure information of videos, we test several variants of the proposed structure learning based hashing methods, and also compare with several representative hashing methods in our experiments. [sent-225, score-1.237]
</p><p>59 Note that both SH and SPLH treat each video as a composite of independent frames and index the video by combining the  Figure 2. [sent-230, score-0.427]
</p><p>60 hash codes of frames, similar as that described in [3]. [sent-234, score-0.636]
</p><p>61 Moreover, SH and SPLH are the representative hashing methods with publicly available codes, and hence are chosen for fair study. [sent-235, score-0.53]
</p><p>62 Although there are some recent methods for video hashing [3, 19], they are built on standard hashing techniques like LSH, and require specific settings, like submodular or multiple-feature representations, hence not serving as standard comparable methods in our experiments. [sent-236, score-1.256]
</p><p>63 Evaluations and Settings We follow the evaluation protocols used in [21] and adopt the following two criteria: (1) Hamming ranking: All the video clips are ranked according to their Hamming distance to the query video. [sent-239, score-0.287]
</p><p>64 (2) Hash lookup: A hash lookup table is constructed and all the samples fall within a Hamming ball with radius r (r = 2 in our setting) to the query sample are returned. [sent-240, score-0.797]
</p><p>65 Since each query video is represented as a set of binary codes corresponding to the frames in the video, here we adopt the following query strategy to return the nearest neighbor (NN) video clips. [sent-241, score-0.698]
</p><p>66 For Hamming ranking based evaluation, it is fairly straightforward to return the nearest video clips by exhaustively computing and ranking the video Hamming distance between the query and the database samples, as defined in Eq. [sent-242, score-0.523]
</p><p>67 For hash lookup based evaluation, we first retrieve the nearest neighbor frames within Hamming radius 2 for each individual hash code vector of the query video frames. [sent-244, score-1.547]
</p><p>68 Then all the videos whose composite frames are successfully hit by any query frame will be regarded as the candidate NN videos. [sent-245, score-0.423]
</p><p>69 An intuitive way to rank all these candidate NN videos is by counting the hit frequency of each video (normalized by the number of total frames in that video). [sent-246, score-0.474]
</p><p>70 Note that these two evaluations focus on different aspects of hashing techniques. [sent-247, score-0.53]
</p><p>71 Hash lookup emphasizes the search speed since the query complexity is often constant time, but the search quality could be unjustified when using very long hash codes, resulting in failed queries due to empty return within Hamming radius r. [sent-251, score-0.802]
</p><p>72 For hash lookup criterion we compute retrieval precision which measures the percentage of true neighbors within Hamming radius r [21]. [sent-254, score-0.722]
</p><p>73 This indicates that our method is able to achieve comparable time complexity as the existing hashing method. [sent-268, score-0.53]
</p><p>74 In our experiments, we randomly select 5 videos from each semantic category as labeled data for training, and choose another 25 videos in each category as the query videos for testing hashing performance. [sent-273, score-1.294]
</p><p>75 This results in 100 training videos and 500 query videos. [sent-274, score-0.267]
</p><p>76 The key frames are evenly sampled every 2 seconds and each video has at least 30 key frames. [sent-276, score-0.314]
</p><p>77 In the experiments, we evaluate the performance using hash codes with different length, ranging from 12 to 64 bits. [sent-282, score-0.636]
</p><p>78 This is due to the fact that the former methods take advantages of structure information of video data (either discriminative local patterns or temporal consistency 22227777  (a) MAP  (b) Precision within Hamming radius 2 Figure 5. [sent-285, score-0.49]
</p><p>79 across successive frames) while the latter one only blindly generates hash codes without accounting any structure information; (3) Our proposed VHDT clearly beats the conventional hashing methods like SH and SPLH. [sent-287, score-1.308]
</p><p>80 The reason is that these methods only try to learn hash functions for simple samples such as images and hence are not appropriate for video data; (4) Our VHDT method performs better than VHD and VHT, since the latter ones merely consider one aspect of the structure information in videos. [sent-288, score-0.779]
</p><p>81 However, the precisions of all methods begin to drop when using longer hash codes. [sent-291, score-0.526]
</p><p>82 This is because that, with the increasing of the number of hash bits, the number of samples falling in a bucket decreases exponentially, resulting in empty returns within the Hamming radius 2. [sent-292, score-0.685]
</p><p>83 The  entire dataset has around 150K videos falling into 25 semantic event categories. [sent-299, score-0.287]
</p><p>84 For each video, we extract key frames every 2 seconds and obtain the final set containing over 12 million video key frames. [sent-301, score-0.314]
</p><p>85 The first is to evaluate on the 10K videos with ground truth labels while the second is to evaluate on the entire 150K videos based on the top returned videos labeled by ourselves. [sent-304, score-0.604]
</p><p>86 In this scenario, 25 labeled videos in each category are chosen as the query video clips for testing hashing performance. [sent-308, score-1.049]
</p><p>87 The performance improvements are consistent as the number of hash bits varies. [sent-311, score-0.554]
</p><p>88 In contrast, our proposed hashing method focuses on real time retrieval on large-scale video data. [sent-314, score-0.684]
</p><p>89 Therefore, we only report the precision within the top 100 returned videos for each method. [sent-319, score-0.275]
</p><p>90 We randomly select 5 videos with ground truth from each of the 25 categories, and consider the 125 videos as queries. [sent-320, score-0.382]
</p><p>91 64-bit binary code is generated to search videos within the 150K video dataset. [sent-321, score-0.405]
</p><p>92 Accuracy of top 100 retrieval videos using 64 bits over the 150K video dataset. [sent-332, score-0.373]
</p><p>93 retrieve the NN videos within Hamming radius 2 and then pick up the top 100 videos ranked by the normalized frame hit frequencies. [sent-333, score-0.556]
</p><p>94 Finally, the category label ofeach unlabeled video within the top 100 videos is manually annotated. [sent-334, score-0.413]
</p><p>95 We calculate the accuracy of top 100 videos of each query and report the average performance across 125 queries in Table 1. [sent-335, score-0.297]
</p><p>96 Figure 6 shows the key frames of some exemplar query videos as well as the top 6 returned key frames, which shows that our structure learning based video hashing consistently generates better visual retrieval results. [sent-337, score-1.185]
</p><p>97 The proposed method works in a supervised setting with a ‘1-norm based empirical loss regularized by the video structure related terms. [sent-340, score-0.28]
</p><p>98 Specifically, we use a ‘2,1-norm to select certain feature dimensions in the training videos to capture the discriminative local visual patterns, and employ a ‘∞-norm in the binary codes of successive frames to preserve the temporal consistency in the learned Hamming space. [sent-341, score-0.698]
</p><p>99 Submodular video hashing: A unified framework towards video pooling and indexing. [sent-362, score-0.308]
</p><p>100 Multiple feature hashing for real-time large scale near-duplicate video retrieval. [sent-473, score-0.684]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hashing', 0.53), ('hash', 0.526), ('hamming', 0.206), ('videos', 0.191), ('vhdt', 0.173), ('video', 0.154), ('xjb', 0.154), ('ccv', 0.142), ('trecvid', 0.126), ('codes', 0.11), ('med', 0.105), ('splh', 0.102), ('successive', 0.1), ('apg', 0.099), ('frames', 0.095), ('xia', 0.087), ('radius', 0.086), ('query', 0.076), ('temporal', 0.071), ('commonality', 0.068), ('lipschitz', 0.068), ('wk', 0.063), ('vhd', 0.058), ('vht', 0.058), ('lookup', 0.057), ('clips', 0.057), ('columbia', 0.055), ('ix', 0.052), ('supervised', 0.05), ('xj', 0.043), ('ij', 0.043), ('submodular', 0.042), ('patterns', 0.042), ('event', 0.042), ('structure', 0.042), ('ranking', 0.041), ('category', 0.041), ('consistency', 0.039), ('consumer', 0.039), ('rd', 0.037), ('sh', 0.037), ('wi', 0.036), ('kw', 0.035), ('design', 0.034), ('loss', 0.034), ('sgn', 0.034), ('hit', 0.034), ('hk', 0.033), ('xi', 0.033), ('semantic', 0.033), ('binary', 0.033), ('functions', 0.032), ('returned', 0.031), ('differentiable', 0.03), ('dimensions', 0.03), ('minimization', 0.03), ('queries', 0.03), ('lf', 0.03), ('indexing', 0.029), ('discriminative', 0.029), ('bk', 0.029), ('gi', 0.029), ('hi', 0.028), ('bits', 0.028), ('within', 0.027), ('ellis', 0.027), ('pthe', 0.027), ('frame', 0.027), ('xn', 0.027), ('precision', 0.026), ('objective', 0.026), ('bow', 0.026), ('ye', 0.026), ('samples', 0.025), ('pointwise', 0.025), ('spectral', 0.025), ('xt', 0.025), ('calculated', 0.025), ('ck', 0.025), ('reconstructive', 0.025), ('copy', 0.025), ('ibm', 0.025), ('smoothing', 0.024), ('treat', 0.024), ('mwin', 0.023), ('realized', 0.023), ('comparisons', 0.023), ('nn', 0.023), ('chang', 0.023), ('gradient', 0.022), ('approximation', 0.022), ('learning', 0.022), ('feeding', 0.022), ('minimizer', 0.022), ('key', 0.022), ('pairwise', 0.021), ('falling', 0.021), ('ap', 0.021), ('seconds', 0.021), ('herein', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="229-tfidf-1" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>Author: Guangnan Ye, Dong Liu, Jun Wang, Shih-Fu Chang</p><p>Abstract: Recently, learning based hashing methods have become popular for indexing large-scale media data. Hashing methods map high-dimensional features to compact binary codes that are efficient to match and robust in preserving original similarity. However, most of the existing hashing methods treat videos as a simple aggregation of independent frames and index each video through combining the indexes of frames. The structure information of videos, e.g., discriminative local visual commonality and temporal consistency, is often neglected in the design of hash functions. In this paper, we propose a supervised method that explores the structure learning techniques to design efficient hash functions. The proposed video hashing method formulates a minimization problem over a structure-regularized empirical loss. In particular, the structure regularization exploits the common local visual patterns occurring in video frames that are associated with the same semantic class, and simultaneously preserves the temporal consistency over successive frames from the same video. We show that the minimization objective can be efficiently solved by an Acceler- ated Proximal Gradient (APG) method. Extensive experiments on two large video benchmark datasets (up to around 150K video clips with over 12 million frames) show that the proposed method significantly outperforms the state-ofthe-art hashing methods.</p><p>2 0.65272909 <a title="229-tfidf-2" href="./iccv-2013-A_General_Two-Step_Approach_to_Learning-Based_Hashing.html">13 iccv-2013-A General Two-Step Approach to Learning-Based Hashing</a></p>
<p>Author: Guosheng Lin, Chunhua Shen, David Suter, Anton van_den_Hengel</p><p>Abstract: Most existing approaches to hashing apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to respond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. This framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problemspecific hashing methods. Our framework decomposes the hashing learning problem into two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically be formulated as binary quadratic problems, and the second step can be accomplished by training standard binary classifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate that the proposed framework is effective, flexible and outperforms the state-of-the-art.</p><p>3 0.59320039 <a title="229-tfidf-3" href="./iccv-2013-Complementary_Projection_Hashing.html">83 iccv-2013-Complementary Projection Hashing</a></p>
<p>Author: Zhongming Jin, Yao Hu, Yue Lin, Debing Zhang, Shiding Lin, Deng Cai, Xuelong Li</p><p>Abstract: Recently, hashing techniques have been widely applied to solve the approximate nearest neighbors search problem in many vision applications. Generally, these hashing approaches generate 2c buckets, where c is the length of the hash code. A good hashing method should satisfy the following two requirements: 1) mapping the nearby data points into the same bucket or nearby (measured by xue long l i opt . ac . cn @ a(a)b(b) the Hamming distance) buckets. 2) all the data points are evenly distributed among all the buckets. In this paper, we propose a novel algorithm named Complementary Projection Hashing (CPH) to find the optimal hashing functions which explicitly considers the above two requirements. Specifically, CPHaims at sequentiallyfinding a series ofhyperplanes (hashing functions) which cross the sparse region of the data. At the same time, the data points are evenly distributed in the hypercubes generated by these hyperplanes. The experiments comparing with the state-of-the-art hashing methods demonstrate the effectiveness of the proposed method.</p><p>4 0.54854745 <a title="229-tfidf-4" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>Author: Jun Wang, Wei Liu, Andy X. Sun, Yu-Gang Jiang</p><p>Abstract: Hashing techniques have been intensively investigated in the design of highly efficient search engines for largescale computer vision applications. Compared with prior approximate nearest neighbor search approaches like treebased indexing, hashing-based search schemes have prominent advantages in terms of both storage and computational efficiencies. Moreover, the procedure of devising hash functions can be easily incorporated into sophisticated machine learning tools, leading to data-dependent and task-specific compact hash codes. Therefore, a number of learning paradigms, ranging from unsupervised to supervised, have been applied to compose appropriate hash functions. How- ever, most of the existing hash function learning methods either treat hash function design as a classification problem or generate binary codes to satisfy pairwise supervision, and have not yet directly optimized the search accuracy. In this paper, we propose to leverage listwise supervision into a principled hash function learning framework. In particular, the ranking information is represented by a set of rank triplets that can be used to assess the quality of ranking. Simple linear projection-based hash functions are solved efficiently through maximizing the ranking quality over the training data. We carry out experiments on large image datasets with size up to one million and compare with the state-of-the-art hashing techniques. The extensive results corroborate that our learned hash codes via listwise supervision can provide superior search accuracy without incurring heavy computational overhead.</p><p>5 0.42523131 <a title="229-tfidf-5" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>Author: Lixin Fan</p><p>Abstract: This paper proposes to learn binary hash codes within a statistical learning framework, in which an upper bound of the probability of Bayes decision errors is derived for different forms of hash functions and a rigorous proof of the convergence of the upper bound is presented. Consequently, minimizing such an upper bound leads to consistent performance improvements of existing hash code learning algorithms, regardless of whether original algorithms are unsupervised or supervised. This paper also illustrates a fast hash coding method that exploits simple binary tests to achieve orders of magnitude improvement in coding speed as compared to projection based methods.</p><p>6 0.23328507 <a title="229-tfidf-6" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>7 0.17858618 <a title="229-tfidf-7" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>8 0.17023939 <a title="229-tfidf-8" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>9 0.16594146 <a title="229-tfidf-9" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>10 0.16209348 <a title="229-tfidf-10" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>11 0.14549822 <a title="229-tfidf-11" href="./iccv-2013-Feature_Weighting_via_Optimal_Thresholding_for_Video_Analysis.html">163 iccv-2013-Feature Weighting via Optimal Thresholding for Video Analysis</a></p>
<p>12 0.11972616 <a title="229-tfidf-12" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>13 0.10150602 <a title="229-tfidf-13" href="./iccv-2013-Fast_Neighborhood_Graph_Search_Using_Cartesian_Concatenation.html">159 iccv-2013-Fast Neighborhood Graph Search Using Cartesian Concatenation</a></p>
<p>14 0.097855963 <a title="229-tfidf-14" href="./iccv-2013-How_Related_Exemplars_Help_Complex_Event_Detection_in_Web_Videos%3F.html">203 iccv-2013-How Related Exemplars Help Complex Event Detection in Web Videos?</a></p>
<p>15 0.086047284 <a title="229-tfidf-15" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>16 0.085074998 <a title="229-tfidf-16" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<p>17 0.084725067 <a title="229-tfidf-17" href="./iccv-2013-Dynamic_Pooling_for_Complex_Event_Recognition.html">127 iccv-2013-Dynamic Pooling for Complex Event Recognition</a></p>
<p>18 0.081335127 <a title="229-tfidf-18" href="./iccv-2013-Joint_Inverted_Indexing.html">221 iccv-2013-Joint Inverted Indexing</a></p>
<p>19 0.080505058 <a title="229-tfidf-19" href="./iccv-2013-Mining_Multiple_Queries_for_Image_Retrieval%3A_On-the-Fly_Learning_of_an_Object-Specific_Mid-level_Representation.html">266 iccv-2013-Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation</a></p>
<p>20 0.080026098 <a title="229-tfidf-20" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.188), (1, 0.133), (2, -0.062), (3, -0.051), (4, -0.056), (5, 0.479), (6, 0.039), (7, -0.011), (8, -0.356), (9, 0.211), (10, -0.321), (11, 0.14), (12, -0.01), (13, -0.141), (14, -0.073), (15, 0.023), (16, -0.149), (17, 0.182), (18, -0.17), (19, 0.075), (20, 0.012), (21, 0.047), (22, 0.019), (23, 0.017), (24, 0.024), (25, -0.002), (26, -0.033), (27, -0.006), (28, -0.002), (29, -0.011), (30, 0.005), (31, -0.008), (32, 0.018), (33, -0.011), (34, -0.016), (35, -0.016), (36, -0.003), (37, 0.015), (38, -0.035), (39, 0.013), (40, -0.013), (41, -0.026), (42, -0.006), (43, -0.005), (44, 0.01), (45, 0.037), (46, 0.022), (47, 0.02), (48, -0.025), (49, -0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95312887 <a title="229-lsi-1" href="./iccv-2013-Complementary_Projection_Hashing.html">83 iccv-2013-Complementary Projection Hashing</a></p>
<p>Author: Zhongming Jin, Yao Hu, Yue Lin, Debing Zhang, Shiding Lin, Deng Cai, Xuelong Li</p><p>Abstract: Recently, hashing techniques have been widely applied to solve the approximate nearest neighbors search problem in many vision applications. Generally, these hashing approaches generate 2c buckets, where c is the length of the hash code. A good hashing method should satisfy the following two requirements: 1) mapping the nearby data points into the same bucket or nearby (measured by xue long l i opt . ac . cn @ a(a)b(b) the Hamming distance) buckets. 2) all the data points are evenly distributed among all the buckets. In this paper, we propose a novel algorithm named Complementary Projection Hashing (CPH) to find the optimal hashing functions which explicitly considers the above two requirements. Specifically, CPHaims at sequentiallyfinding a series ofhyperplanes (hashing functions) which cross the sparse region of the data. At the same time, the data points are evenly distributed in the hypercubes generated by these hyperplanes. The experiments comparing with the state-of-the-art hashing methods demonstrate the effectiveness of the proposed method.</p><p>same-paper 2 0.94274759 <a title="229-lsi-2" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>Author: Guangnan Ye, Dong Liu, Jun Wang, Shih-Fu Chang</p><p>Abstract: Recently, learning based hashing methods have become popular for indexing large-scale media data. Hashing methods map high-dimensional features to compact binary codes that are efficient to match and robust in preserving original similarity. However, most of the existing hashing methods treat videos as a simple aggregation of independent frames and index each video through combining the indexes of frames. The structure information of videos, e.g., discriminative local visual commonality and temporal consistency, is often neglected in the design of hash functions. In this paper, we propose a supervised method that explores the structure learning techniques to design efficient hash functions. The proposed video hashing method formulates a minimization problem over a structure-regularized empirical loss. In particular, the structure regularization exploits the common local visual patterns occurring in video frames that are associated with the same semantic class, and simultaneously preserves the temporal consistency over successive frames from the same video. We show that the minimization objective can be efficiently solved by an Acceler- ated Proximal Gradient (APG) method. Extensive experiments on two large video benchmark datasets (up to around 150K video clips with over 12 million frames) show that the proposed method significantly outperforms the state-ofthe-art hashing methods.</p><p>3 0.94111508 <a title="229-lsi-3" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>Author: Jun Wang, Wei Liu, Andy X. Sun, Yu-Gang Jiang</p><p>Abstract: Hashing techniques have been intensively investigated in the design of highly efficient search engines for largescale computer vision applications. Compared with prior approximate nearest neighbor search approaches like treebased indexing, hashing-based search schemes have prominent advantages in terms of both storage and computational efficiencies. Moreover, the procedure of devising hash functions can be easily incorporated into sophisticated machine learning tools, leading to data-dependent and task-specific compact hash codes. Therefore, a number of learning paradigms, ranging from unsupervised to supervised, have been applied to compose appropriate hash functions. How- ever, most of the existing hash function learning methods either treat hash function design as a classification problem or generate binary codes to satisfy pairwise supervision, and have not yet directly optimized the search accuracy. In this paper, we propose to leverage listwise supervision into a principled hash function learning framework. In particular, the ranking information is represented by a set of rank triplets that can be used to assess the quality of ranking. Simple linear projection-based hash functions are solved efficiently through maximizing the ranking quality over the training data. We carry out experiments on large image datasets with size up to one million and compare with the state-of-the-art hashing techniques. The extensive results corroborate that our learned hash codes via listwise supervision can provide superior search accuracy without incurring heavy computational overhead.</p><p>4 0.93975586 <a title="229-lsi-4" href="./iccv-2013-A_General_Two-Step_Approach_to_Learning-Based_Hashing.html">13 iccv-2013-A General Two-Step Approach to Learning-Based Hashing</a></p>
<p>Author: Guosheng Lin, Chunhua Shen, David Suter, Anton van_den_Hengel</p><p>Abstract: Most existing approaches to hashing apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to respond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. This framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problemspecific hashing methods. Our framework decomposes the hashing learning problem into two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically be formulated as binary quadratic problems, and the second step can be accomplished by training standard binary classifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate that the proposed framework is effective, flexible and outperforms the state-of-the-art.</p><p>5 0.88604629 <a title="229-lsi-5" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>Author: Lixin Fan</p><p>Abstract: This paper proposes to learn binary hash codes within a statistical learning framework, in which an upper bound of the probability of Bayes decision errors is derived for different forms of hash functions and a rigorous proof of the convergence of the upper bound is presented. Consequently, minimizing such an upper bound leads to consistent performance improvements of existing hash code learning algorithms, regardless of whether original algorithms are unsupervised or supervised. This paper also illustrates a fast hash coding method that exploits simple binary tests to achieve orders of magnitude improvement in coding speed as compared to projection based methods.</p><p>6 0.45545843 <a title="229-lsi-6" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>7 0.44614467 <a title="229-lsi-7" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>8 0.44564745 <a title="229-lsi-8" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>9 0.40562871 <a title="229-lsi-9" href="./iccv-2013-Joint_Inverted_Indexing.html">221 iccv-2013-Joint Inverted Indexing</a></p>
<p>10 0.33965883 <a title="229-lsi-10" href="./iccv-2013-Random_Grids%3A_Fast_Approximate_Nearest_Neighbors_and_Range_Searching_for_Image_Search.html">337 iccv-2013-Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search</a></p>
<p>11 0.30978587 <a title="229-lsi-11" href="./iccv-2013-Fast_Neighborhood_Graph_Search_Using_Cartesian_Concatenation.html">159 iccv-2013-Fast Neighborhood Graph Search Using Cartesian Concatenation</a></p>
<p>12 0.27243781 <a title="229-lsi-12" href="./iccv-2013-Feature_Weighting_via_Optimal_Thresholding_for_Video_Analysis.html">163 iccv-2013-Feature Weighting via Optimal Thresholding for Video Analysis</a></p>
<p>13 0.26623753 <a title="229-lsi-13" href="./iccv-2013-Fast_Subspace_Search_via_Grassmannian_Based_Hashing.html">162 iccv-2013-Fast Subspace Search via Grassmannian Based Hashing</a></p>
<p>14 0.26342386 <a title="229-lsi-14" href="./iccv-2013-Visual_Semantic_Complex_Network_for_Web_Images.html">446 iccv-2013-Visual Semantic Complex Network for Web Images</a></p>
<p>15 0.25551027 <a title="229-lsi-15" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>16 0.24871999 <a title="229-lsi-16" href="./iccv-2013-Abnormal_Event_Detection_at_150_FPS_in_MATLAB.html">34 iccv-2013-Abnormal Event Detection at 150 FPS in MATLAB</a></p>
<p>17 0.24771269 <a title="229-lsi-17" href="./iccv-2013-Video_Synopsis_by_Heterogeneous_Multi-source_Correlation.html">443 iccv-2013-Video Synopsis by Heterogeneous Multi-source Correlation</a></p>
<p>18 0.24605381 <a title="229-lsi-18" href="./iccv-2013-Neighbor-to-Neighbor_Search_for_Fast_Coding_of_Feature_Vectors.html">287 iccv-2013-Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors</a></p>
<p>19 0.24185333 <a title="229-lsi-19" href="./iccv-2013-Fix_Structured_Learning_of_2013_ICCV_paper_k2opt.pdf.html">171 iccv-2013-Fix Structured Learning of 2013 ICCV paper k2opt.pdf</a></p>
<p>20 0.23955572 <a title="229-lsi-20" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.195), (4, 0.042), (7, 0.171), (12, 0.016), (26, 0.056), (31, 0.043), (42, 0.091), (48, 0.011), (64, 0.049), (73, 0.03), (78, 0.011), (89, 0.168)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93150914 <a title="229-lda-1" href="./iccv-2013-Supervised_Binary_Hash_Code_Learning_with_Jensen_Shannon_Divergence.html">409 iccv-2013-Supervised Binary Hash Code Learning with Jensen Shannon Divergence</a></p>
<p>Author: Lixin Fan</p><p>Abstract: This paper proposes to learn binary hash codes within a statistical learning framework, in which an upper bound of the probability of Bayes decision errors is derived for different forms of hash functions and a rigorous proof of the convergence of the upper bound is presented. Consequently, minimizing such an upper bound leads to consistent performance improvements of existing hash code learning algorithms, regardless of whether original algorithms are unsupervised or supervised. This paper also illustrates a fast hash coding method that exploits simple binary tests to achieve orders of magnitude improvement in coding speed as compared to projection based methods.</p><p>2 0.901788 <a title="229-lda-2" href="./iccv-2013-Text_Localization_in_Natural_Images_Using_Stroke_Feature_Transform_and_Text_Covariance_Descriptors.html">415 iccv-2013-Text Localization in Natural Images Using Stroke Feature Transform and Text Covariance Descriptors</a></p>
<p>Author: Weilin Huang, Zhe Lin, Jianchao Yang, Jue Wang</p><p>Abstract: In this paper, we present a new approach for text localization in natural images, by discriminating text and non-text regions at three levels: pixel, component and textline levels. Firstly, a powerful low-level filter called the Stroke Feature Transform (SFT) is proposed, which extends the widely-used Stroke Width Transform (SWT) by incorporating color cues of text pixels, leading to significantly enhanced performance on inter-component separation and intra-component connection. Secondly, based on the output of SFT, we apply two classifiers, a text component classifier and a text-line classifier, sequentially to extract text regions, eliminating the heuristic procedures that are commonly used in previous approaches. The two classifiers are built upon two novel Text Covariance Descriptors (TCDs) that encode both the heuristic properties and the statistical characteristics of text stokes. Finally, text regions are located by simply thresholding the text-line confident map. Our method was evaluated on two benchmark datasets: ICDAR 2005 and ICDAR 2011, and the corresponding F- , measure values are 0. 72 and 0. 73, respectively, surpassing previous methods in accuracy by a large margin.</p><p>3 0.88667929 <a title="229-lda-3" href="./iccv-2013-Image_Set_Classification_Using_Holistic_Multiple_Order_Statistics_Features_and_Localized_Multi-kernel_Metric_Learning.html">212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</a></p>
<p>Author: Jiwen Lu, Gang Wang, Pierre Moulin</p><p>Abstract: This paper presents a new approach for image set classification, where each training and testing example contains a set of image instances of an object captured from varying viewpoints or under varying illuminations. While a number of image set classification methods have been proposed in recent years, most of them model each image set as a single linear subspace or mixture of linear subspaces, which may lose some discriminative information for classification. To address this, we propose exploring multiple order statistics as features of image sets, and develop a localized multikernel metric learning (LMKML) algorithm to effectively combine different order statistics information for classification. Our method achieves the state-of-the-art performance on four widely used databases including the Honda/UCSD, CMU Mobo, and Youtube face datasets, and the ETH-80 object dataset.</p><p>same-paper 4 0.8861109 <a title="229-lda-4" href="./iccv-2013-Large-Scale_Video_Hashing_via_Structure_Learning.html">229 iccv-2013-Large-Scale Video Hashing via Structure Learning</a></p>
<p>Author: Guangnan Ye, Dong Liu, Jun Wang, Shih-Fu Chang</p><p>Abstract: Recently, learning based hashing methods have become popular for indexing large-scale media data. Hashing methods map high-dimensional features to compact binary codes that are efficient to match and robust in preserving original similarity. However, most of the existing hashing methods treat videos as a simple aggregation of independent frames and index each video through combining the indexes of frames. The structure information of videos, e.g., discriminative local visual commonality and temporal consistency, is often neglected in the design of hash functions. In this paper, we propose a supervised method that explores the structure learning techniques to design efficient hash functions. The proposed video hashing method formulates a minimization problem over a structure-regularized empirical loss. In particular, the structure regularization exploits the common local visual patterns occurring in video frames that are associated with the same semantic class, and simultaneously preserves the temporal consistency over successive frames from the same video. We show that the minimization objective can be efficiently solved by an Acceler- ated Proximal Gradient (APG) method. Extensive experiments on two large video benchmark datasets (up to around 150K video clips with over 12 million frames) show that the proposed method significantly outperforms the state-ofthe-art hashing methods.</p><p>5 0.88359725 <a title="229-lda-5" href="./iccv-2013-From_Semi-supervised_to_Transfer_Counting_of_Crowds.html">178 iccv-2013-From Semi-supervised to Transfer Counting of Crowds</a></p>
<p>Author: Chen Change Loy, Shaogang Gong, Tao Xiang</p><p>Abstract: Regression-based techniques have shown promising results for people counting in crowded scenes. However, most existing techniques require expensive and laborious data annotation for model training. In this study, we propose to address this problem from three perspectives: (1) Instead of exhaustively annotating every single frame, the most informative frames are selected for annotation automatically and actively. (2) Rather than learning from only labelled data, the abundant unlabelled data are exploited. (3) Labelled data from other scenes are employed to further alleviate the burden for data annotation. All three ideas are implemented in a unified active and semi-supervised regression framework with ability to perform transfer learning, by exploiting the underlying geometric structure of crowd patterns via manifold analysis. Extensive experiments validate the effectiveness of our approach.</p><p>6 0.85939944 <a title="229-lda-6" href="./iccv-2013-No_Matter_Where_You_Are%3A_Flexible_Graph-Guided_Multi-task_Learning_for_Multi-view_Head_Pose_Classification_under_Target_Motion.html">291 iccv-2013-No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion</a></p>
<p>7 0.85565954 <a title="229-lda-7" href="./iccv-2013-Visual_Semantic_Complex_Network_for_Web_Images.html">446 iccv-2013-Visual Semantic Complex Network for Web Images</a></p>
<p>8 0.85450506 <a title="229-lda-8" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>9 0.85012251 <a title="229-lda-9" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>10 0.84915769 <a title="229-lda-10" href="./iccv-2013-A_General_Two-Step_Approach_to_Learning-Based_Hashing.html">13 iccv-2013-A General Two-Step Approach to Learning-Based Hashing</a></p>
<p>11 0.84784025 <a title="229-lda-11" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<p>12 0.84525287 <a title="229-lda-12" href="./iccv-2013-Improving_Graph_Matching_via_Density_Maximization.html">214 iccv-2013-Improving Graph Matching via Density Maximization</a></p>
<p>13 0.84130394 <a title="229-lda-13" href="./iccv-2013-Complementary_Projection_Hashing.html">83 iccv-2013-Complementary Projection Hashing</a></p>
<p>14 0.84113562 <a title="229-lda-14" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>15 0.83786166 <a title="229-lda-15" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>16 0.83753872 <a title="229-lda-16" href="./iccv-2013-Offline_Mobile_Instance_Retrieval_with_a_Small_Memory_Footprint.html">294 iccv-2013-Offline Mobile Instance Retrieval with a Small Memory Footprint</a></p>
<p>17 0.83581841 <a title="229-lda-17" href="./iccv-2013-Revisiting_Example_Dependent_Cost-Sensitive_Learning_with_Decision_Trees.html">352 iccv-2013-Revisiting Example Dependent Cost-Sensitive Learning with Decision Trees</a></p>
<p>18 0.82658684 <a title="229-lda-18" href="./iccv-2013-Pose_Estimation_and_Segmentation_of_People_in_3D_Movies.html">322 iccv-2013-Pose Estimation and Segmentation of People in 3D Movies</a></p>
<p>19 0.82479405 <a title="229-lda-19" href="./iccv-2013-Pose_Estimation_with_Unknown_Focal_Length_Using_Points%2C_Directions_and_Lines.html">323 iccv-2013-Pose Estimation with Unknown Focal Length Using Points, Directions and Lines</a></p>
<p>20 0.82277524 <a title="229-lda-20" href="./iccv-2013-Linear_Sequence_Discriminant_Analysis%3A_A_Model-Based_Dimensionality_Reduction_Method_for_Vector_Sequences.html">253 iccv-2013-Linear Sequence Discriminant Analysis: A Model-Based Dimensionality Reduction Method for Vector Sequences</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
