<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-399" href="#">iccv2013-399</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</h1>
<br/><p>Source: <a title="iccv-2013-399-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Sadovnik_Spoken_Attributes_Mixing_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Amir Sadovnik, Andrew Gallagher, Devi Parikh, Tsuhan Chen</p><p>Abstract: In recent years, there has been a great deal of progress in describing objects with attributes. Attributes have proven useful for object recognition, image search, face verification, image description, and zero-shot learning. Typically, attributes are either binary or relative: they describe either the presence or absence of a descriptive characteristic, or the relative magnitude of the characteristic when comparing two exemplars. However, prior work fails to model the actual way in which humans use these attributes in descriptive statements of images. Specifically, it does not address the important interactions between the binary and relative aspects of an attribute. In this work we propose a spoken attribute classifier which models a more natural way of using an attribute in a description. For each attribute we train a classifier which captures the specific way this attribute should be used. We show that as a result of using this model, we produce descriptions about images of people that are more natural and specific than past systems.</p><p>Reference: <a title="iccv-2013-399-reference" href="../iccv2013_reference/iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Typically, attributes are either binary or relative: they describe either the presence or absence of a descriptive characteristic, or the relative magnitude of the characteristic when comparing two exemplars. [sent-5, score-0.621]
</p><p>2 However, prior work fails to model the actual way in which humans use these attributes in descriptive statements of images. [sent-6, score-0.615]
</p><p>3 In this work we propose a spoken attribute classifier which models a more natural way of using an attribute in a description. [sent-8, score-1.485]
</p><p>4 For each attribute we train a classifier which captures the specific way this attribute should be used. [sent-9, score-0.931]
</p><p>5 The goal in this paper is to produce better, more natural, attribute-based image descriptions that we learn from human statements and refer to as spoken attributes. [sent-14, score-0.95]
</p><p>6 ” Of course, this direct statement communicates the fact that Tom’s level of the “smiling” attribute is greater than Susan’s, but that is not the end of the story. [sent-19, score-0.75]
</p><p>7 However, in image (b), where neither face is smiling, using such a relative smiling statement does not make sense. [sent-40, score-0.731]
</p><p>8 For the glasses attribute it never makes sense to use a relative description even though a ranker output for image (d) would suggest that the person on the left has “more glasses” than the person on the right. [sent-41, score-0.888]
</p><p>9 In this paper we propose a unified spoken  attribute classifier that learns this mapping for each attribute and can “say the right thing. [sent-42, score-1.485]
</p><p>10 Therefore, even a simple relative statement about an attribute can convey a great deal of meaning and informa2160  tion about the image content. [sent-46, score-0.952]
</p><p>11 For one thing, humans fluidly move between using binary (“Tom and Susan are smiling”) and relative (“Tom is smiling more than Susan”) attribute statements. [sent-48, score-0.934]
</p><p>12 In this work, our goal is to “say the right thing” by choosing between relative and binary attribute statements in the same way that a human describer would. [sent-50, score-1.022]
</p><p>13 We believe we are the first work to jointly consider both binary and relative attributes, and the first to model spoken attributes i. [sent-51, score-1.144]
</p><p>14 model ways in which humans mix binary and relative statements in their spoken language. [sent-53, score-1.184]
</p><p>15 Many attributes have utility in both binary and relative form (e. [sent-55, score-0.576]
</p><p>16 In past work, either the binary or the relative forms of an attribute were exclusively explored. [sent-58, score-0.714]
</p><p>17 This is a limitation that humans do not share, and can result in learning attribute relationships that lose correspondence with spoken language. [sent-59, score-1.042]
</p><p>18 When the relative attribute “more masculine” is learned from male-female training pairs, the resulting model does not relate directly to the phrase humans would actually use. [sent-61, score-0.662]
</p><p>19 Not all attributes are equally useful in both binary and relative forms. [sent-62, score-0.557]
</p><p>20 For example, the “wears glasses” attribute is exclusively binary; either a person is wearing glasses or she isn’t. [sent-63, score-0.615]
</p><p>21 Although it is possible to use relative attributes when talking about glasses such as “larger glasses” or “stylish glasses”, these are in effect attributes of glasses and could be learned separately. [sent-65, score-0.999]
</p><p>22 Our work characterizes the usage patterns of six different attributes, and shows interesting differences and commonalities between the usage of these spoken attributes. [sent-66, score-0.644]
</p><p>23 In summary, we learn the relationship between attributes (both binary and relative) and the statements that humans use to describe images, allowing us to produce more natural image descriptions and better understand human-generated attribute-based statements. [sent-67, score-0.818]
</p><p>24 Finally, we believe this work represents a first example of joining computer vision attributes with actual spoken attributes used in language to improve computer-generated image descriptions, and to interpret image captions. [sent-70, score-1.143]
</p><p>25 [23] use facial attributes for search in both binary and relative form. [sent-78, score-0.574]
</p><p>26 To the best of our knowledge, we are the first to propose a model that automatically determines whether a categorical or relative statement about an attribute is appropriate for a given image. [sent-81, score-0.923]
</p><p>27 Instead  of selecting a true attribute statement, we select a spoken attribute statement: a binary or relative attribute statement that a human would actually use to describe the image. [sent-89, score-2.51]
</p><p>28 In this work, rather than focusing on which aspects of an image a person describes, we address the orthogonal task of modeling how a person is likely to describe a certain attribute in an image. [sent-95, score-0.535]
</p><p>29 Understanding Spoken Attributes  ×  To motivate our work and explore our hypothesis that limiting attributes to either binary or relative cannot faithfully capture how people actually use the attribute in descriptions, we collect a new dataset of spoken attributes. [sent-100, score-1.667]
</p><p>30 With this dataset, we can examine the contrast between conventional methods of collecting ground truth (for building either binary classifiers or relative rankers) and what people truly believe makes a good, descriptive statement for an image. [sent-101, score-0.753]
</p><p>31 We work with facial attributes because they have been extensively studied and have shown utility for exploring ideas related to attribute research. [sent-102, score-0.71]
</p><p>32 Second, for each pair of faces we collect the relative magnitude of the attribute between the two faces. [sent-111, score-0.689]
</p><p>33 These two questions provide the traditional binary and relative ranking annotations that have been used in previous attribute studies. [sent-115, score-0.785]
</p><p>34 For the third question, we collect spoken attribute ground truth by asking the worker to select one of six statements, spanning the space of both binary and relative attributes: • Both x and y have attribute a. [sent-116, score-1.827]
</p><p>35 Our statement domain is still somewhat limited because it has only six statements instead of being completely free-form. [sent-123, score-0.644]
</p><p>36 We conducted a pilot experiment to explore the effects of the exact question posed to the worker on the selected spoken attribute answers. [sent-125, score-1.061]
</p><p>37 This figure provides a visualization of the relationship between conventional binary and relative attributes, and the workers’ spoken attributes. [sent-139, score-0.86]
</p><p>38 For each attribute (rows), and for each binary and relative attribute ground truth label (columns) we show the distribution of the type of selected spoken attribute. [sent-140, score-1.708]
</p><p>39 For example, for the teeth visible attribute, when both faces have visible teeth (the left-most cell in the  teeth visible row), about 40% of the images were described 2162  indicates the ground truth relative and binary label that is true for an image. [sent-141, score-0.66]
</p><p>40 Each pie chart represents the distribution of the statement selected by subjects as the spoken attribute. [sent-142, score-0.914]
</p><p>41 For example, the  glasses attribute row has no yellow, meaning that glasses was never used in the relative form in the spoken attribute experiment. [sent-144, score-1.918]
</p><p>42 In contrast, the male attribute was only used in relative form (“more masculine”) when both people were male. [sent-145, score-0.739]
</p><p>43 by the spoken attribute “x’s teeth are more visible that y’s”, while for about 55% of images the spoken attribute “both faces have visible teeth” was selected. [sent-148, score-2.17]
</p><p>44 Clearly, when not forced to make either a binary or a relative statement, our workers select different statements – sometimes relative and sometimes binary – as the spoken attribute depending on the face pair they are describing. [sent-149, score-1.974]
</p><p>45 This chart clearly shows the differences between simply using either relative or binary statements and the spoken attribute statements that people actually prefer to say. [sent-150, score-1.99]
</p><p>46 For example, the glasses attribute never receives a relative statement as the spoken attribute (no yellow in that row), indicating that people simply do not say “x has more glasses than y. [sent-151, score-2.351]
</p><p>47 ” However, about 20% of the images receive a relative statement for glasses when using the traditional method to collect relative attribute ranking labels. [sent-152, score-1.315]
</p><p>48 This occurs in situations where one person has glasses and the other does not, and the workers are forced to select the only ranking  statement that seems logically consistent. [sent-153, score-0.6]
</p><p>49 Another example that shows limitations of using solely binary descriptions is the portion of relative statements selected when both people have a certain attribute. [sent-157, score-0.744]
</p><p>50 Relative statements were chosen for about two-thirds of both the teeth-visible and the smiling attributes when both people in the image possess that attribute. [sent-158, score-0.82]
</p><p>51 For none of the attributes was a relative statement selected as the spoken attribute when neither of the faces had the attribute. [sent-160, score-1.818]
</p><p>52 This supports our hypothesis that relative statements cannot be used throughout the entire attribute space (as shown in Fig. [sent-161, score-0.888]
</p><p>53 For example, glasses is never used in relative form, male is used in relative form about a third of the time both people are male, while smiling and teeth-visible are used in relative form much more often. [sent-178, score-1.014]
</p><p>54 , “more masculine” is rarely used as the spoken attribute unless both faces are male). [sent-184, score-1.052]
</p><p>55 Spoken Attributes Classifier To predict the spoken attribute statement, we formalize the problem as a multiclass classification problem. [sent-186, score-1.031]
</p><p>56 The outputs (SVM scores) of these classifiers are fed as inputs (features) to the second layer classifier, whose role is to predict the correct spoken attribute statement from among the binary and relative attribute statement choices. [sent-197, score-2.458]
</p><p>57 Although we can directly feed the low-level face pyramid features to an alternatively trained spoken attribute classifier (we compare to this method in Sec. [sent-198, score-1.091]
</p><p>58 By projecting each face’s features onto it and taking the difference, the relative strength of the attribute  between the pair is found. [sent-204, score-0.638]
</p><p>59 -one) linear SVM with the six spoken attribute statements described in Sec. [sent-207, score-1.312]
</p><p>60 We extract a 5 dimensional vector from the first stage classifiers that includes each face’s scores from the binary classifier, each face’s projection from the relative classifier, and the difference between the relative classifier scores for the pair. [sent-209, score-0.589]
</p><p>61 The binary scores indicate the belief of the presence or absence of that attribute for each specific face. [sent-210, score-0.595]
</p><p>62 The ranking scores indicate the uncalibrated absolute strength of the presence of the attribute and the difference indicates the relative attribute strength between the two faces. [sent-211, score-1.102]
</p><p>63 We tried using as features a 30 dimensional vector that stores these 5 features across all 6 attributes to train the spoken attribute classifier for each attribute, but no improvement in performance was observed, so we did not include them in our final model. [sent-213, score-1.328]
</p><p>64 Experiments and Results In this Section, we present the results from a number of experiments that explore our model’s ability to “say the right thing” by predicting correct spoken attributes, and to  0%  50%  100%  Figure 4. [sent-215, score-0.605]
</p><p>65 3 (4 binary statements followed by 2 relative statements). [sent-219, score-0.581]
</p><p>66 interpret spoken attributes to better understand the binary attributes that the images possess. [sent-220, score-1.221]
</p><p>67 In other words, we predict the spoken attribute description for the image. [sent-224, score-1.052]
</p><p>68 e t Ahes a otutrrib suptoek setant aetmtreibnut tthea pt rise dmicotisot nc,om wemo sinmlyused as the spoken attribute for the specific attribute at hand. [sent-228, score-1.434]
</p><p>69 rTaihnee output oadfi tthioen SaVl bMion a pair of faces corresponds to one of the four binary statements and is used as the spoken attribute prediction. [sent-233, score-1.469]
</p><p>70 , nearly equal attribute levels), we output one of the binary statements that indicate agreement (either “both” or “neither” faces have the attribute, choosing the statement with the larger prior for the attribute). [sent-240, score-1.218]
</p><p>71 This method can not produce binary statements where one face has the attribute but the other doesn’t. [sent-242, score-0.88]
</p><p>72 Finally, we test the model on the test image using both layers to get our predicted spoken attribute statement. [sent-254, score-0.994]
</p><p>73 This is because many of the spoken attributes are mostly binary (for example glasses and male) and so good performance at selecting the correct binary statement provides the correct spoken attribute in many cases. [sent-263, score-2.588]
</p><p>74 However, this obviously fails to produce any relative attribute statements as the spoken attribute, missing a significant portion of the vocabulary that humans actually use. [sent-264, score-1.525]
</p><p>75 Our approach produces both relative and binary attribute statements as the spoken attribute and has the best overall performance. [sent-265, score-1.999]
</p><p>76 ROC curves for our experiment on the smiling attribute as presented in Sec. [sent-290, score-0.611]
</p><p>77 Since we allow relative statements (which are not very common) we end up using it inappropriately at times as compared to the binary method. [sent-296, score-0.581]
</p><p>78 Therefore, the results of our spoken attribute classifier are useful in producing relevant images to these types of search queries. [sent-302, score-1.078]
</p><p>79 We show the ROC curves for the smiling attribute (other attributes produce similar results) in Figure 5. [sent-309, score-0.896]
</p><p>80 These results show that our unified framework can handle both relative and binary statements well. [sent-310, score-0.581]
</p><p>81 In addition, our spoken attribute classifier can better capture additional meaning which is not explicit in the statement, but is expected by the user. [sent-311, score-1.076]
</p><p>82 Binary Classifier Prediction by Reading Between the Lines We show that the spoken attribute statements can carry more information than what is explicitly said. [sent-316, score-1.285]
</p><p>83 For example, when the spoken attribute statement is a relative statement, it is possible to “read between the lines” and gain information about the binary presence or absence of attributes in the image. [sent-317, score-1.896]
</p><p>84 We examine the subset of the images that have a relative statement as the spoken attribute. [sent-318, score-1.069]
</p><p>85 However, we train it solely on the subset of images with relative spoken attribute statements. [sent-323, score-1.184]
</p><p>86 This represents the information we gain by knowing the spoken attribute. [sent-324, score-0.57]
</p><p>87 Finally we try an additional method which simply selects the most common binary statement for that specific relative statement. [sent-325, score-0.647]
</p><p>88 We show results for attributes that have enough (> 40) images where the relative  statement was the ground truth spoken attribute. [sent-328, score-1.336]
</p><p>89 First, it appears that adding the spoken attribute statement in some cases can significantly help the binary classifier (as is shown for teeth-visible and beard). [sent-330, score-1.504]
</p><p>90 which were predicted correctly only after adding the spoken attribute statement in Fig. [sent-341, score-1.32]
</p><p>91 The bald binary prediction does not improve when adding the spoken attribute statement. [sent-343, score-1.199]
</p><p>92 Although for most attributes, using a relative statement already implies that both people have that attribute (as in bearded and smiling), this is not true for the bald attribute. [sent-345, score-1.13]
</p><p>93 A relative “more bald” 2166  tribute statement contains implicit binary information and can help binary attribute classification. [sent-346, score-1.157]
</p><p>94 The binary classifier misclassifies these images as “B has the attribute and A does not”. [sent-348, score-0.608]
</p><p>95 However, since for these attributes the relative expression is almost only used when both people have the attribute, we correct the binary classification. [sent-349, score-0.65]
</p><p>96 Therefore, the fact that a relative statement was selected does not contain any information about the binary results, strengthening our claim that for each attribute, the model that is learned is unique to that attribute. [sent-351, score-0.616]
</p><p>97 Conclusion In this paper we show that the traditional way of modeling attributes as either binary or relative is not sufficient to fully exploit the expressiveness of the attributes, and can lead to unnatural descriptions. [sent-353, score-0.594]
</p><p>98 We construct a spoken attribute classifier that is able to capture the interactions between the relative and binary aspects of an attribute and better model the way this attribute would be used in an actual description. [sent-355, score-2.199]
</p><p>99 For example, we might not wish to talk about the beard attribute if we are presented we an image of a pair of females (unless one of them is bearded). [sent-360, score-0.538]
</p><p>100 We plan to investigate this problem of selecting which attributes to use, and using these correlations between attributes to read between the lines. [sent-362, score-0.571]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spoken', 0.57), ('attribute', 0.424), ('statement', 0.326), ('statements', 0.291), ('attributes', 0.267), ('smiling', 0.187), ('relative', 0.173), ('glasses', 0.146), ('binary', 0.117), ('bald', 0.088), ('susan', 0.08), ('teeth', 0.079), ('people', 0.075), ('descriptions', 0.071), ('classifier', 0.067), ('male', 0.067), ('worker', 0.067), ('tom', 0.062), ('reading', 0.052), ('beard', 0.052), ('sadovnik', 0.047), ('masculine', 0.047), ('person', 0.045), ('bearded', 0.044), ('faces', 0.043), ('workers', 0.041), ('hhaass', 0.04), ('layer', 0.036), ('description', 0.035), ('thing', 0.035), ('corne', 0.033), ('humans', 0.033), ('say', 0.032), ('textual', 0.031), ('face', 0.03), ('visible', 0.03), ('cornell', 0.028), ('ranking', 0.028), ('six', 0.027), ('attttrriibbuuttee', 0.027), ('parikh', 0.026), ('sentences', 0.026), ('gallagher', 0.025), ('lines', 0.025), ('collect', 0.025), ('descriptive', 0.024), ('pair', 0.024), ('predict', 0.023), ('berg', 0.023), ('questions', 0.023), ('confusion', 0.022), ('language', 0.022), ('read', 0.022), ('describing', 0.021), ('describe', 0.021), ('comment', 0.021), ('ranksvm', 0.021), ('females', 0.021), ('classifiers', 0.021), ('traditional', 0.02), ('never', 0.02), ('absence', 0.019), ('scores', 0.019), ('desire', 0.019), ('utility', 0.019), ('correct', 0.018), ('referring', 0.018), ('scheirer', 0.018), ('chart', 0.018), ('roc', 0.018), ('produce', 0.018), ('choosing', 0.017), ('strength', 0.017), ('believe', 0.017), ('generating', 0.017), ('unnatural', 0.017), ('talk', 0.017), ('search', 0.017), ('svm', 0.017), ('solely', 0.017), ('predicting', 0.017), ('kulkarni', 0.017), ('specific', 0.016), ('usage', 0.016), ('phrase', 0.016), ('actually', 0.016), ('interactively', 0.016), ('proxy', 0.015), ('relationships', 0.015), ('neither', 0.015), ('unless', 0.015), ('interesting', 0.015), ('selecting', 0.015), ('meaning', 0.015), ('simply', 0.015), ('examined', 0.014), ('forced', 0.014), ('multiclass', 0.014), ('convey', 0.014), ('belhumeur', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="399-tfidf-1" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Devi Parikh, Tsuhan Chen</p><p>Abstract: In recent years, there has been a great deal of progress in describing objects with attributes. Attributes have proven useful for object recognition, image search, face verification, image description, and zero-shot learning. Typically, attributes are either binary or relative: they describe either the presence or absence of a descriptive characteristic, or the relative magnitude of the characteristic when comparing two exemplars. However, prior work fails to model the actual way in which humans use these attributes in descriptive statements of images. Specifically, it does not address the important interactions between the binary and relative aspects of an attribute. In this work we propose a spoken attribute classifier which models a more natural way of using an attribute in a description. For each attribute we train a classifier which captures the specific way this attribute should be used. We show that as a result of using this model, we produce descriptions about images of people that are more natural and specific than past systems.</p><p>2 0.43619028 <a title="399-tfidf-2" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>Author: Xiaoyang Wang, Qiang Ji</p><p>Abstract: This paper proposes a unified probabilistic model to model the relationships between attributes and objects for attribute prediction and object recognition. As a list of semantically meaningful properties of objects, attributes generally relate to each other statistically. In this paper, we propose a unified probabilistic model to automatically discover and capture both the object-dependent and objectindependent attribute relationships. The model utilizes the captured relationships to benefit both attribute prediction and object recognition. Experiments on four benchmark attribute datasets demonstrate the effectiveness of the proposed unified model for improving attribute prediction as well as object recognition in both standard and zero-shot learning cases.</p><p>3 0.36760411 <a title="399-tfidf-3" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>Author: Adriana Kovashka, Kristen Grauman</p><p>Abstract: Current methods learn monolithic attribute predictors, with the assumption that a single model is sufficient to reflect human understanding of a visual attribute. However, in reality, humans vary in how they perceive the association between a named property and image content. For example, two people may have slightly different internal models for what makes a shoe look “formal”, or they may disagree on which of two scenes looks “more cluttered”. Rather than discount these differences as noise, we propose to learn user-specific attribute models. We adapt a generic model trained with annotations from multiple users, tailoring it to satisfy user-specific labels. Furthermore, we propose novel techniques to infer user-specific labels based on transitivity and contradictions in the user’s search history. We demonstrate that adapted attributes improve accuracy over both existing monolithic models as well as models that learn from scratch with user-specific data alone. In addition, we show how adapted attributes are useful to personalize image search, whether with binary or relative attributes.</p><p>4 0.30802521 <a title="399-tfidf-4" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>Author: Naman Turakhia, Devi Parikh</p><p>Abstract: When we look at an image, some properties or attributes of the image stand out more than others. When describing an image, people are likely to describe these dominant attributes first. Attribute dominance is a result of a complex interplay between the various properties present or absent in the image. Which attributes in an image are more dominant than others reveals rich information about the content of the image. In this paper we tap into this information by modeling attribute dominance. We show that this helps improve the performance of vision systems on a variety of human-centric applications such as zero-shot learning, image search and generating textual descriptions of images.</p><p>5 0.26720944 <a title="399-tfidf-5" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>Author: Jungseock Joo, Shuo Wang, Song-Chun Zhu</p><p>Abstract: We present a part-based approach to the problem of human attribute recognition from a single image of a human body. To recognize the attributes of human from the body parts, it is important to reliably detect the parts. This is a challenging task due to the geometric variation such as articulation and view-point changes as well as the appearance variation of the parts arisen from versatile clothing types. The prior works have primarily focused on handling . edu . cn ???????????? geometric variation by relying on pre-trained part detectors or pose estimators, which require manual part annotation, but the appearance variation has been relatively neglected in these works. This paper explores the importance of the appearance variation, which is directly related to the main task, attribute recognition. To this end, we propose to learn a rich appearance part dictionary of human with significantly less supervision by decomposing image lattice into overlapping windows at multiscale and iteratively refining local appearance templates. We also present quantitative results in which our proposed method outperforms the existing approaches.</p><p>6 0.26583987 <a title="399-tfidf-6" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>7 0.25530031 <a title="399-tfidf-7" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>8 0.19958851 <a title="399-tfidf-8" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>9 0.16801246 <a title="399-tfidf-9" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>10 0.14711757 <a title="399-tfidf-10" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>11 0.14208165 <a title="399-tfidf-11" href="./iccv-2013-Implied_Feedback%3A_Learning_Nuances_of_User_Behavior_in_Image_Search.html">213 iccv-2013-Implied Feedback: Learning Nuances of User Behavior in Image Search</a></p>
<p>12 0.13718458 <a title="399-tfidf-12" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>13 0.12226079 <a title="399-tfidf-13" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>14 0.12225998 <a title="399-tfidf-14" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>15 0.11894163 <a title="399-tfidf-15" href="./iccv-2013-Learning_Graph_Matching%3A_Oriented_to_Category_Modeling_from_Cluttered_Scenes.html">237 iccv-2013-Learning Graph Matching: Oriented to Category Modeling from Cluttered Scenes</a></p>
<p>16 0.11736801 <a title="399-tfidf-16" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<p>17 0.11161952 <a title="399-tfidf-17" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>18 0.10850823 <a title="399-tfidf-18" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>19 0.10373382 <a title="399-tfidf-19" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>20 0.10135375 <a title="399-tfidf-20" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.138), (1, 0.182), (2, -0.079), (3, -0.197), (4, 0.185), (5, -0.073), (6, -0.086), (7, -0.214), (8, 0.297), (9, 0.23), (10, -0.07), (11, 0.055), (12, 0.063), (13, 0.005), (14, -0.08), (15, -0.008), (16, -0.004), (17, 0.049), (18, -0.062), (19, 0.015), (20, -0.005), (21, 0.063), (22, 0.03), (23, -0.05), (24, -0.078), (25, -0.025), (26, 0.035), (27, -0.021), (28, -0.039), (29, 0.039), (30, -0.045), (31, 0.002), (32, -0.032), (33, -0.007), (34, 0.002), (35, -0.038), (36, 0.001), (37, -0.041), (38, -0.027), (39, -0.027), (40, 0.005), (41, 0.063), (42, -0.015), (43, 0.048), (44, -0.018), (45, 0.019), (46, 0.008), (47, -0.006), (48, -0.002), (49, -0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98605663 <a title="399-lsi-1" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Devi Parikh, Tsuhan Chen</p><p>Abstract: In recent years, there has been a great deal of progress in describing objects with attributes. Attributes have proven useful for object recognition, image search, face verification, image description, and zero-shot learning. Typically, attributes are either binary or relative: they describe either the presence or absence of a descriptive characteristic, or the relative magnitude of the characteristic when comparing two exemplars. However, prior work fails to model the actual way in which humans use these attributes in descriptive statements of images. Specifically, it does not address the important interactions between the binary and relative aspects of an attribute. In this work we propose a spoken attribute classifier which models a more natural way of using an attribute in a description. For each attribute we train a classifier which captures the specific way this attribute should be used. We show that as a result of using this model, we produce descriptions about images of people that are more natural and specific than past systems.</p><p>2 0.97207183 <a title="399-lsi-2" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>Author: Naman Turakhia, Devi Parikh</p><p>Abstract: When we look at an image, some properties or attributes of the image stand out more than others. When describing an image, people are likely to describe these dominant attributes first. Attribute dominance is a result of a complex interplay between the various properties present or absent in the image. Which attributes in an image are more dominant than others reveals rich information about the content of the image. In this paper we tap into this information by modeling attribute dominance. We show that this helps improve the performance of vision systems on a variety of human-centric applications such as zero-shot learning, image search and generating textual descriptions of images.</p><p>3 0.94221544 <a title="399-lsi-3" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>Author: Xiaoyang Wang, Qiang Ji</p><p>Abstract: This paper proposes a unified probabilistic model to model the relationships between attributes and objects for attribute prediction and object recognition. As a list of semantically meaningful properties of objects, attributes generally relate to each other statistically. In this paper, we propose a unified probabilistic model to automatically discover and capture both the object-dependent and objectindependent attribute relationships. The model utilizes the captured relationships to benefit both attribute prediction and object recognition. Experiments on four benchmark attribute datasets demonstrate the effectiveness of the proposed unified model for improving attribute prediction as well as object recognition in both standard and zero-shot learning cases.</p><p>4 0.8895548 <a title="399-lsi-4" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>Author: Adriana Kovashka, Kristen Grauman</p><p>Abstract: Current methods learn monolithic attribute predictors, with the assumption that a single model is sufficient to reflect human understanding of a visual attribute. However, in reality, humans vary in how they perceive the association between a named property and image content. For example, two people may have slightly different internal models for what makes a shoe look “formal”, or they may disagree on which of two scenes looks “more cluttered”. Rather than discount these differences as noise, we propose to learn user-specific attribute models. We adapt a generic model trained with annotations from multiple users, tailoring it to satisfy user-specific labels. Furthermore, we propose novel techniques to infer user-specific labels based on transitivity and contradictions in the user’s search history. We demonstrate that adapted attributes improve accuracy over both existing monolithic models as well as models that learn from scratch with user-specific data alone. In addition, we show how adapted attributes are useful to personalize image search, whether with binary or relative attributes.</p><p>5 0.80933201 <a title="399-lsi-5" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>Author: Sukrit Shankar, Joan Lasenby, Roberto Cipolla</p><p>Abstract: Relative (comparative) attributes are promising for thematic ranking of visual entities, which also aids in recognition tasks [19, 23]. However, attribute rank learning often requires a substantial amount of relational supervision, which is highly tedious, and apparently impracticalfor realworld applications. In this paper, we introduce the Semantic Transform, which under minimal supervision, adaptively finds a semantic feature space along with a class ordering that is related in the best possible way. Such a semantic space is found for every attribute category. To relate the classes under weak supervision, the class ordering needs to be refined according to a cost function in an iterative procedure. This problem is ideally NP-hard, and we thus propose a constrained search tree formulation for the same. Driven by the adaptive semantic feature space representation, our model achieves the best results to date for all of the tasks of relative, absolute and zero-shot classification on two popular datasets.</p><p>6 0.76584339 <a title="399-lsi-6" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>7 0.73442006 <a title="399-lsi-7" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>8 0.71281385 <a title="399-lsi-8" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>9 0.60431325 <a title="399-lsi-9" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>10 0.59440863 <a title="399-lsi-10" href="./iccv-2013-Relative_Attributes_for_Large-Scale_Abandoned_Object_Detection.html">350 iccv-2013-Relative Attributes for Large-Scale Abandoned Object Detection</a></p>
<p>11 0.59161371 <a title="399-lsi-11" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<p>12 0.48573327 <a title="399-lsi-12" href="./iccv-2013-Learning_the_Visual_Interpretation_of_Sentences.html">246 iccv-2013-Learning the Visual Interpretation of Sentences</a></p>
<p>13 0.47702006 <a title="399-lsi-13" href="./iccv-2013-NEIL%3A_Extracting_Visual_Knowledge_from_Web_Data.html">285 iccv-2013-NEIL: Extracting Visual Knowledge from Web Data</a></p>
<p>14 0.46765786 <a title="399-lsi-14" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>15 0.46106559 <a title="399-lsi-15" href="./iccv-2013-Implied_Feedback%3A_Learning_Nuances_of_User_Behavior_in_Image_Search.html">213 iccv-2013-Implied Feedback: Learning Nuances of User Behavior in Image Search</a></p>
<p>16 0.41680881 <a title="399-lsi-16" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>17 0.41627473 <a title="399-lsi-17" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>18 0.39369535 <a title="399-lsi-18" href="./iccv-2013-Modifying_the_Memorability_of_Face_Photographs.html">272 iccv-2013-Modifying the Memorability of Face Photographs</a></p>
<p>19 0.38046274 <a title="399-lsi-19" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>20 0.37061417 <a title="399-lsi-20" href="./iccv-2013-Quadruplet-Wise_Image_Similarity_Learning.html">332 iccv-2013-Quadruplet-Wise Image Similarity Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.065), (7, 0.022), (12, 0.039), (26, 0.047), (31, 0.033), (34, 0.114), (42, 0.118), (64, 0.031), (73, 0.255), (89, 0.127), (98, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86755443 <a title="399-lda-1" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>Author: Qiong Yan, Xiaoyong Shen, Li Xu, Shaojie Zhuo, Xiaopeng Zhang, Liang Shen, Jiaya Jia</p><p>Abstract: Color, infrared, and flash images captured in different fields can be employed to effectively eliminate noise and other visual artifacts. We propose a two-image restoration framework considering input images in different fields, for example, one noisy color image and one dark-flashed nearinfrared image. The major issue in such a framework is to handle structure divergence and find commonly usable edges and smooth transition for visually compelling image reconstruction. We introduce a scale map as a competent representation to explicitly model derivative-level confidence and propose new functions and a numerical solver to effectively infer it following new structural observations. Our method is general and shows a principled way for cross-field restoration.</p><p>2 0.86744595 <a title="399-lda-2" href="./iccv-2013-Single-Patch_Low-Rank_Prior_for_Non-pointwise_Impulse_Noise_Removal.html">394 iccv-2013-Single-Patch Low-Rank Prior for Non-pointwise Impulse Noise Removal</a></p>
<p>Author: Ruixuan Wang, Emanuele Trucco</p><p>Abstract: This paper introduces a ‘low-rank prior’ for small oriented noise-free image patches: considering an oriented patch as a matrix, a low-rank matrix approximation is enough to preserve the texture details in the properly oriented patch. Based on this prior, we propose a single-patch method within a generalized joint low-rank and sparse matrix recovery framework to simultaneously detect and remove non-pointwise random-valued impulse noise (e.g., very small blobs). A weighting matrix is incorporated in the framework to encode an initial estimate of the spatial noise distribution. An accelerated proximal gradient method is adapted to estimate the optimal noise-free image patches. Experiments show the effectiveness of our framework in removing non-pointwise random-valued impulse noise.</p><p>3 0.81625968 <a title="399-lda-3" href="./iccv-2013-Bayesian_3D_Tracking_from_Monocular_Video.html">58 iccv-2013-Bayesian 3D Tracking from Monocular Video</a></p>
<p>Author: Ernesto Brau, Jinyan Guan, Kyle Simek, Luca Del Pero, Colin Reimer Dawson, Kobus Barnard</p><p>Abstract: Jinyan Guan† j guan1 @ emai l ari z ona . edu . Kyle Simek† ks imek@ emai l ari z ona . edu . Colin Reimer Dawson‡ cdaws on@ emai l ari z ona . edu . ‡School of Information University of Arizona Kobus Barnard‡ kobus @ s i sta . ari z ona . edu ∗School of Informatics University of Edinburgh for tracking an unknown and changing number of people in a scene using video taken from a single, fixed viewpoint. We develop a Bayesian modeling approach for tracking people in 3D from monocular video with unknown cameras. Modeling in 3D provides natural explanations for occlusions and smoothness discontinuities that result from projection, and allows priors on velocity and smoothness to be grounded in physical quantities: meters and seconds vs. pixels and frames. We pose the problem in the context of data association, in which observations are assigned to tracks. A correct application of Bayesian inference to multitarget tracking must address the fact that the model’s dimension changes as tracks are added or removed, and thus, posterior densities of different hypotheses are not comparable. We address this by marginalizing out the trajectory parameters so the resulting posterior over data associations has constant dimension. This is made tractable by using (a) Gaussian process priors for smooth trajectories and (b) approximately Gaussian likelihood functions. Our approach provides a principled method for incorporating multiple sources of evidence; we present results using both optical flow and object detector outputs. Results are comparable to recent work on 3D tracking and, unlike others, our method requires no pre-calibrated cameras.</p><p>same-paper 4 0.80255014 <a title="399-lda-4" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Devi Parikh, Tsuhan Chen</p><p>Abstract: In recent years, there has been a great deal of progress in describing objects with attributes. Attributes have proven useful for object recognition, image search, face verification, image description, and zero-shot learning. Typically, attributes are either binary or relative: they describe either the presence or absence of a descriptive characteristic, or the relative magnitude of the characteristic when comparing two exemplars. However, prior work fails to model the actual way in which humans use these attributes in descriptive statements of images. Specifically, it does not address the important interactions between the binary and relative aspects of an attribute. In this work we propose a spoken attribute classifier which models a more natural way of using an attribute in a description. For each attribute we train a classifier which captures the specific way this attribute should be used. We show that as a result of using this model, we produce descriptions about images of people that are more natural and specific than past systems.</p><p>5 0.77989495 <a title="399-lda-5" href="./iccv-2013-Multiple_Non-rigid_Surface_Detection_and_Registration.html">283 iccv-2013-Multiple Non-rigid Surface Detection and Registration</a></p>
<p>Author: Yi Wu, Yoshihisa Ijiri, Ming-Hsuan Yang</p><p>Abstract: Detecting and registering nonrigid surfaces are two important research problems for computer vision. Much work has been done with the assumption that there exists only one instance in the image. In this work, we propose an algorithm that detects and registers multiple nonrigid instances of given objects in a cluttered image. Specifically, after we use low level feature points to obtain the initial matches between templates and the input image, a novel high-order affinity graph is constructed to model the consistency of local topology. A hierarchical clustering approach is then used to locate the nonrigid surfaces. To remove the outliers in the cluster, we propose a deterministic annealing approach based on the Thin Plate Spline (TPS) model. The proposed method achieves high accuracy even when the number of outliers is nineteen times larger than the inliers. As the matches may appear sparsely in each instance, we propose a TPS based match growing approach to propagate the matches. Finally, an approach that fuses feature and appearance information is proposed to register each nonrigid surface. Extensive experiments and evaluations demonstrate that the proposed algorithm achieves promis- ing results in detecting and registering multiple non-rigid surfaces in a cluttered scene.</p><p>6 0.76632118 <a title="399-lda-6" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<p>7 0.71950626 <a title="399-lda-7" href="./iccv-2013-Joint_Noise_Level_Estimation_from_Personal_Photo_Collections.html">223 iccv-2013-Joint Noise Level Estimation from Personal Photo Collections</a></p>
<p>8 0.71552765 <a title="399-lda-8" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>9 0.71216816 <a title="399-lda-9" href="./iccv-2013-A_New_Image_Quality_Metric_for_Image_Auto-denoising.html">23 iccv-2013-A New Image Quality Metric for Image Auto-denoising</a></p>
<p>10 0.71177256 <a title="399-lda-10" href="./iccv-2013-How_Do_You_Tell_a_Blackbird_from_a_Crow%3F.html">202 iccv-2013-How Do You Tell a Blackbird from a Crow?</a></p>
<p>11 0.68351978 <a title="399-lda-11" href="./iccv-2013-Bayesian_Robust_Matrix_Factorization_for_Image_and_Video_Processing.html">60 iccv-2013-Bayesian Robust Matrix Factorization for Image and Video Processing</a></p>
<p>12 0.67766929 <a title="399-lda-12" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>13 0.67745233 <a title="399-lda-13" href="./iccv-2013-Multi-scale_Topological_Features_for_Hand_Posture_Representation_and_Analysis.html">278 iccv-2013-Multi-scale Topological Features for Hand Posture Representation and Analysis</a></p>
<p>14 0.66768098 <a title="399-lda-14" href="./iccv-2013-Robust_Non-parametric_Data_Fitting_for_Correspondence_Modeling.html">358 iccv-2013-Robust Non-parametric Data Fitting for Correspondence Modeling</a></p>
<p>15 0.66302896 <a title="399-lda-15" href="./iccv-2013-A_Robust_Analytical_Solution_to_Isometric_Shape-from-Template_with_Focal_Length_Calibration.html">27 iccv-2013-A Robust Analytical Solution to Isometric Shape-from-Template with Focal Length Calibration</a></p>
<p>16 0.66153294 <a title="399-lda-16" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>17 0.65593857 <a title="399-lda-17" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>18 0.65534514 <a title="399-lda-18" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>19 0.65470433 <a title="399-lda-19" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>20 0.65364504 <a title="399-lda-20" href="./iccv-2013-Optical_Flow_via_Locally_Adaptive_Fusion_of_Complementary_Data_Costs.html">300 iccv-2013-Optical Flow via Locally Adaptive Fusion of Complementary Data Costs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
