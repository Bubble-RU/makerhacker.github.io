<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-335" href="#">iccv2013-335</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</h1>
<br/><p>Source: <a title="iccv-2013-335-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Zhang_Random_Faces_Guided_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Yizhe Zhang, Ming Shao, Edward K. Wong, Yun Fu</p><p>Abstract: One of the most challenging task in face recognition is to identify people with varied poses. Namely, the test faces have significantly different poses compared with the registered faces. In this paper, we propose a high-level feature learning scheme to extract pose-invariant identity feature for face recognition. First, we build a single-hiddenlayer neural network with sparse constraint, to extractposeinvariant feature in a supervised fashion. Second, we further enhance the discriminative capability of the proposed feature by using multiple random faces as the target values for multiple encoders. By enforcing the target values to be uniquefor inputfaces over differentposes, the learned highlevel feature that is represented by the neurons in the hidden layer is pose free and only relevant to the identity information. Finally, we conduct face identification on CMU MultiPIE, and verification on Labeled Faces in the Wild (LFW) databases, where identification rank-1 accuracy and face verification accuracy with ROC curve are reported. These experiments demonstrate that our model is superior to oth- er state-of-the-art approaches on handling pose variations.</p><p>Reference: <a title="iccv-2013-335-reference" href="../iccv2013_reference/iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract One of the most challenging task in face recognition is to identify people with varied poses. [sent-4, score-0.373]
</p><p>2 Namely, the test faces have significantly different poses compared with the registered faces. [sent-5, score-0.528]
</p><p>3 In this paper, we propose a high-level feature learning scheme to extract pose-invariant identity feature for face recognition. [sent-6, score-0.689]
</p><p>4 Second, we further enhance the discriminative capability of the proposed feature by using multiple random faces as the target values for multiple encoders. [sent-8, score-0.595]
</p><p>5 By enforcing the target values to be uniquefor inputfaces over differentposes, the learned highlevel feature that is represented by the neurons in the hidden layer is pose free and only relevant to the identity information. [sent-9, score-0.909]
</p><p>6 Finally, we conduct face identification on CMU MultiPIE, and verification on Labeled Faces in the Wild (LFW) databases, where identification rank-1 accuracy and face verification accuracy with ROC curve are reported. [sent-10, score-1.026]
</p><p>7 , the encoder, and set the target values to be random faces (RF). [sent-21, score-0.444]
</p><p>8 We design D encoders and therefore have D random faces for each ID. [sent-22, score-0.47]
</p><p>9 For most of the state-of-the-art face recognition algoHuman facial images play important roles in security issues and social media analytics, where many real-world applications have been successfully developed during the past decades, e. [sent-25, score-0.582]
</p><p>10 , face identification and verification, facial expression recognition, facial illumination simulation and removing, aging simulation and age estimation, under either controlled lab environment, or unrestricted environment. [sent-27, score-0.88]
</p><p>11 However, in both environments, pose is one of the most critical problems since faces in 2D images with different poses are significantly different from each other even ∗indicates equal contributions. [sent-28, score-0.613]
</p><p>12 rithms, finding correspondence or face alignment is the first yet the most essential step because all experiments based on comparisons between registered and test faces need either pixel-wise or semantic level alignment. [sent-29, score-0.82]
</p><p>13 First, this high-level pose free feature reduces the impact of diverse poses in the feature space. [sent-36, score-0.525]
</p><p>14 For example, we can project side-view facial feature to frontview facial feature, by a transform function. [sent-39, score-0.49]
</p><p>15 Therefore, good facial feature should keep its common attributes as well as private ones. [sent-41, score-0.363]
</p><p>16 For example, we use “1” to label the first subject, but its identity feature could be either vector x1 or x2, or concatenated vector [x1; x2] as long as they are not identical with other subjects’ identity feature. [sent-44, score-0.386]
</p><p>17 , frontal face, which guides the supervised feature learning in the hidden layer. [sent-48, score-0.491]
</p><p>18 Since the output of this S-NN only relies on the value in the hidden layer, neurons in the hidden layer are potentially good representations for pose free identity feature. [sent-49, score-0.906]
</p><p>19 Second, we enhance the discriminative power of the proposed identity feature by assigning random faces to the target values of S-NN. [sent-50, score-0.746]
</p><p>20 Introducing multiple random faces allows us to learn multiple encoders which randomly encode private or common attributes to the identity feature. [sent-52, score-0.697]
</p><p>21 Third, we demonstrate the effectiveness of the proposed method by facial images over different poses captured in the controlled environment (Multi-PIE) and facial images in the real-world (LFW) over different poses, mixed with other impact factors, such as illuminations, expressions. [sent-53, score-0.549]
</p><p>22 Related Work There are two lines in the related work: (1) face feature representation, (2) pose-invariant face recognition, which are highly related to the proposed model in this paper. [sent-56, score-0.72]
</p><p>23 In general, face feature representation contains two categories, namely, holistic feature, and local descriptor. [sent-57, score-0.431]
</p><p>24 Holistic feature uses the entire face region as the input, followed by certain operations, e. [sent-58, score-0.402]
</p><p>25 Other than general face feature representation, there are also a group of pose specified face recognition algorithms. [sent-65, score-0.917]
</p><p>26 The core idea of this method is to compute identity feature regardless of poses through a group of angle specified linear functions. [sent-67, score-0.416]
</p><p>27 Recently, Coupled Latent Space Discriminative Analysis (CLSDA) [26] has been proposed to tackle the multiple pose face recognition. [sent-69, score-0.486]
</p><p>28 Different from theirs, our approach generates the identity feature directly  through a non-linear mapping and this identity feature can be expanded for the purpose of discriminant. [sent-71, score-0.47]
</p><p>29 In [1], authors present an alignment strategy called “stack flow” that discovers viewpoint induced spatial deformities undergone by a face on the local patch level. [sent-73, score-0.426]
</p><p>30 They learn the relationship of face images between every two adjacent angle bin to form an incremental wrapping knowledge. [sent-74, score-0.405]
</p><p>31 By this knowledge, virtual frontal faces can be generated from non-frontal faces through one or multiple times of face wrapping, and recognition can be done on the same frontal pose images by off-the-shelf approaches. [sent-75, score-1.76]
</p><p>32 3D face model has been proposed for pose-invariant face recognition [2, 22, 16]. [sent-78, score-0.665]
</p><p>33 Pose Normalization [2] creates a novel match scheme that for each gallery and probe image, it generates a virtual frontal face, and the similarity between probe and gallery images could be evaluated on the same frontal pose condition. [sent-79, score-1.009]
</p><p>34 3D Generic Elastic Mod2417  el [22] learns a 3D generic elastic model from 3D face images. [sent-80, score-0.359]
</p><p>35 With 3D models, they synthesize a group of virtual face images in different poses for each gallery image in frontal pose. [sent-81, score-0.829]
</p><p>36 The recognition process first estimates the  pose angle of the probe face image, and then performs face matching with virtual gallery face images of the same pose. [sent-82, score-1.41]
</p><p>37 Morphable Displacement Field (MDF) [16] also considers generating virtual faces to match the gallery. [sent-83, score-0.428]
</p><p>38 In brief, above methods heavily rely on automatically and robustly fitting a 3D face model to a 2D input image, which is easily affected by factors such as illumination and expression. [sent-85, score-0.347]
</p><p>39 Sparse manyto-one encoder takes responsibility for mapping different poses to the frontal face, therefore yielding a high-level pose free feature in the hidden layer contained in the SNN. [sent-88, score-1.407]
</p><p>40 On the other hand, random faces provide many options for the output of S-NN, and artificially produce many random shared structures between two identities. [sent-89, score-0.449]
</p><p>41 Specifically, in our problem, the input of the SME is training facial images over different poses (many), while the target values are facial images of the same identity as the input but with frontal pose (one). [sent-96, score-1.232]
</p><p>42 The basic idea of this encoder is that regardless of the input pose, we encourage the output of this singlehidden-layer neural network to be close to the frontal pose facial image of the same identity. [sent-97, score-1.285]
</p><p>43 W∈e Rfirst centralize each feature by the mean feature of a specific pose over all subjects, namely,  xij= xji− xj,  where xj=I1i? [sent-101, score-0.384]
</p><p>44 (1)  In the feed-forward neural network, the element in the hidden layer is essentially the output of a weight function followed by an activation. [sent-103, score-0.393]
</p><p>45 However, in our model, we intentionally set the target values as the frontal pose facial images, i. [sent-114, score-0.68]
</p><p>46 Since the neurons in the hidden layer are basis for the output layer, our configuration of the target values enforces that the hidden layer has to be a pose-invariant high-level representation for the input. [sent-117, score-0.775]
</p><p>47 h(xij)  ×  We formulate objective function of the proposed encoder as:  W1,bm1,iWn2,b221N? [sent-118, score-0.458]
</p><p>48 First, not all features are equally important, especially for faces that have 2418  Encoder  Encoder 1  Encoder 2  Encoder  D  feature RF1feature1RF2feature. [sent-129, score-0.423]
</p><p>49 Compared with using a single frontal face as the target value in (A), random faces in (B) simulate the overlap facial parts between different individuals by randomness. [sent-133, score-1.325]
</p><p>50 The feature generated by hidden layer may contain more discriminative identity information. [sent-134, score-0.578]
</p><p>51 After learning model parameters W1, W2, b1, and b2, we obtain the hidden layer output τi for each test as a poseinvariant high-level feature, and any classifier can be used to do the recognition task. [sent-155, score-0.513]
</p><p>52 Random Faces In the previous model, we set the target value as the frontal facial image of each subject, and encourage = This produces output that approximates the frontal face regardless of input. [sent-158, score-1.151]
</p><p>53 Therefore, the hidden layer output can represent the pose-invariant high-level feature. [sent-159, score-0.346]
</p><p>54 On abstract level, the frontal face for each subject in the proposed encoder model is only a representation. [sent-160, score-1.055]
</p><p>55 Therefore, any unique matrix can work as this representative during the training phase, not necessarily the frontal face of the input image. [sent-161, score-0.62]
</p><p>56 Left: using full-aligned faces for model-1 by learning a single W1; Right: using non-aligned faces for model-2 by learning multiple W1s. [sent-168, score-0.73]
</p><p>57 In fact, faces are not totally different, because they share similar structures. [sent-170, score-0.339]
</p><p>58 For each subject i, we generate D random faces yid ∈ Rn, 1 ≤ d ≤ D, where each single pixel is i. [sent-173, score-0.414]
</p><p>59 te Armpsp aoref appearance, banutthey play the same roles of frontal faces as the representatives in training the encoder. [sent-180, score-0.644]
</p><p>60 For each input xi (we omit pose index for simplicity), we train D different encoders and consequently, there are D outputs from the hidden layers, i. [sent-181, score-0.435]
</p><p>61 Non-Aligned Face In this part, we introduce two models corresponding to two different face alignment strategies, which are shown in Figure 3. [sent-192, score-0.398]
</p><p>62 As mentioned before, face alignment is the most important pre-processing step before feature extrac-  tion. [sent-193, score-0.482]
</p><p>63 If for each input face with arbitrary pose, we select dense correspondences (facial landmarks), and extract features from local patches defined by these correspondences, then the feature has already been aligned. [sent-194, score-0.457]
</p><p>64 Still, we need frontal faces to guide the hypothesis outputs. [sent-195, score-0.616]
</p><p>65 For any test input with pose j, we do not need to align it to the frontal pose, rather we find its pose-invariant feature 2419  -30? [sent-199, score-0.551]
</p><p>66 Face Identification Configuration In this section, we use Multi-PIE [9] database to test the proposed models on face identification. [sent-217, score-0.378]
</p><p>67 For full-aligned experiments, we use the state-of-the-art face alignment model in [30] to do landmark localization, as Figure 4 shown. [sent-218, score-0.432]
</p><p>68 For non-aligned experiments, faces are manually cropped and resized to 128 128, based on the boundary of the face, rraesthizeer dth toan 1 2la8nd ×m 1a2rk8s, on ethde ofanc teh. [sent-219, score-0.407]
</p><p>69 From Figure 4 we can see that when the pose angle goes beyond 45◦, some face landmarks will disappear. [sent-221, score-0.589]
</p><p>70 Different form theirs, in this paper, we use pose estimation model proposed in [30] to infer the pose for input parameter pair {W1j , b1j }. [sent-232, score-0.365]
</p><p>71 , size of the hidden layer to be approximately half of the number of individuals in the training set. [sent-236, score-0.461]
</p><p>72 For the sparse many-to-one encoder, we set the output to be the input’s corresponding frontal face feature. [sent-237, score-0.628]
</p><p>73 Face Identification Results  In face identification, we predict each probe image’s identity by nearest-neighbor classifier. [sent-253, score-0.529]
</p><p>74 Sett ing-1 registers each individual’s frontal face (0◦) as the gallery. [sent-255, score-0.557]
</p><p>75 So the dimension of the feature for each face is 20 20 52. [sent-264, score-0.402]
</p><p>76 In addition, we also report the virtual frontal faces generated by model-1 (without random faces). [sent-267, score-0.702]
</p><p>77 “Glasses” means the face recognition rate on the original testing set (249 individuals) which includes eyeglasses, while “No-Glasses” means the results on a subset (158 individuals) of the original testing set where there is no eyeglasses. [sent-277, score-0.347]
</p><p>78 We believe this is mainly due to the accurate face alignment by [30] in the preprocessing step. [sent-295, score-0.398]
</p><p>79 In this experiment, faces are manually cropped based on the boundary of faces, which do not rely on any landmarks, and resized to 128 128. [sent-302, score-0.407]
</p><p>80 t Soi implement non-aligned face identification where we learn separated , } for different poses. [sent-304, score-0.437]
</p><p>81 Apparently, this task is very challenging, ra dndif fthereernetfo proes we expand nthtley training kse ist and use the last 237 individuals’ facial images in Multi-PIE as the training set, and the first 100 individuals’ facial images as the test set. [sent-305, score-0.505]
</p><p>82 Odd rows: test faces; Even rows: virtual front faces by model-1. [sent-309, score-0.459]
</p><p>83 Face Verification in the Wild “Labeled Faces in the Wild” (LFW) [12] is a benchmark database for evaluating face verification algorithm on “wild” real-world images. [sent-336, score-0.423]
</p><p>84 This dataset contains 13,000 images of faces collected from the Internet, and 1680 individuals with at least two face images. [sent-337, score-0.778]
</p><p>85 Since our feature learning scheme relies on the identity of the training set, we follow the unrestricted setting of the LFW. [sent-338, score-0.357]
</p><p>86 In this verification experiment, we run model-1 with 100 random faces, and the size of hidden layer is 10. [sent-341, score-0.417]
</p><p>87 We followed the method used in multi-one-shot [27] to centralize faces according to their poses, which is formulated in Eq. [sent-345, score-0.387]
</p><p>88 At last, we use face feature pairs in the test fold for face verification. [sent-352, score-0.751]
</p><p>89 From results we can see that LFW is very challenging since all the faces are from real-world with arbitrary poses, expressions as well as illuminations, as shown in Figure 5. [sent-355, score-0.339]
</p><p>90 Conclusion In this paper, we presented a novel many-to-one highlevel face feature learning approach for extracting poseinvariant and discriminative identity feature from 2D facial 2422  Left: l1norm; Middle: l2 norm; Right: Impact of the coder size. [sent-372, score-1.015]
</p><p>91 First, we designed an l1 norm regularized manyto-one encoder to remove the impact introduced by diverse poses from feature learning process. [sent-374, score-0.741]
</p><p>92 Second, we enhanced the discriminant of the pose free feature by setting multiple random faces as the target values of our encoders. [sent-375, score-0.813]
</p><p>93 Learning patch correspondences for improved viewpoint invariant face recognition. [sent-390, score-0.346]
</p><p>94 Fully automatic pose-invariant face recognition via 3d pose normalization. [sent-402, score-0.515]
</p><p>95 Labeled faces in the wild: A database for studying face recognition in unconstrained environments. [sent-480, score-0.715]
</p><p>96 Maximizing intra-individual correlations for face recognition across pose differences. [sent-496, score-0.515]
</p><p>97 Morphable displacement field based image matching for face recognition across pose. [sent-516, score-0.375]
</p><p>98 Unconstrained pose-invariant face recognition using 3d generic elastic models. [sent-552, score-0.388]
</p><p>99 Tied factor analysis for face recognition across large pose differences. [sent-561, score-0.515]
</p><p>100 Robust pose invariant face recognition using coupled latent space discriminant analysis. [sent-594, score-0.561]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('encoder', 0.458), ('faces', 0.339), ('face', 0.318), ('frontal', 0.239), ('facial', 0.203), ('pose', 0.168), ('layer', 0.164), ('identity', 0.151), ('hidden', 0.142), ('individuals', 0.121), ('identification', 0.119), ('lfw', 0.112), ('poses', 0.106), ('encoders', 0.096), ('virtual', 0.089), ('unconstraint', 0.088), ('xij', 0.084), ('feature', 0.084), ('eyeglasses', 0.081), ('poseinvariant', 0.081), ('sme', 0.081), ('alignment', 0.08), ('gallery', 0.077), ('verification', 0.076), ('xji', 0.076), ('private', 0.076), ('landmarks', 0.07), ('target', 0.07), ('probe', 0.06), ('subfigure', 0.06), ('network', 0.059), ('apparently', 0.058), ('wild', 0.057), ('mvda', 0.054), ('sett', 0.054), ('wrapping', 0.054), ('lbp', 0.053), ('neurons', 0.053), ('registered', 0.052), ('centralize', 0.048), ('mdf', 0.048), ('neural', 0.047), ('free', 0.046), ('discriminant', 0.046), ('pages', 0.046), ('regardless', 0.042), ('northeastern', 0.042), ('tpami', 0.041), ('elastic', 0.041), ('regularization', 0.04), ('subject', 0.04), ('elder', 0.04), ('output', 0.04), ('office', 0.039), ('resized', 0.039), ('hypothesis', 0.038), ('lda', 0.037), ('discriminative', 0.037), ('impact', 0.037), ('unrestricted', 0.037), ('morphable', 0.037), ('lao', 0.037), ('rf', 0.036), ('session', 0.036), ('award', 0.035), ('random', 0.035), ('tied', 0.035), ('landmark', 0.034), ('training', 0.034), ('extrapolation', 0.034), ('angle', 0.033), ('yij', 0.032), ('roles', 0.032), ('sparse', 0.031), ('highlevel', 0.031), ('test', 0.031), ('ieee', 0.031), ('namely', 0.031), ('enhance', 0.03), ('norm', 0.03), ('input', 0.029), ('cropped', 0.029), ('database', 0.029), ('holistic', 0.029), ('recognition', 0.029), ('patch', 0.028), ('illuminations', 0.028), ('displacement', 0.028), ('subjects', 0.027), ('springer', 0.027), ('people', 0.026), ('profile', 0.026), ('folds', 0.026), ('learning', 0.026), ('extract', 0.026), ('rm', 0.025), ('maximizing', 0.025), ('superior', 0.025), ('shan', 0.025), ('setting', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="335-tfidf-1" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>Author: Yizhe Zhang, Ming Shao, Edward K. Wong, Yun Fu</p><p>Abstract: One of the most challenging task in face recognition is to identify people with varied poses. Namely, the test faces have significantly different poses compared with the registered faces. In this paper, we propose a high-level feature learning scheme to extract pose-invariant identity feature for face recognition. First, we build a single-hiddenlayer neural network with sparse constraint, to extractposeinvariant feature in a supervised fashion. Second, we further enhance the discriminative capability of the proposed feature by using multiple random faces as the target values for multiple encoders. By enforcing the target values to be uniquefor inputfaces over differentposes, the learned highlevel feature that is represented by the neurons in the hidden layer is pose free and only relevant to the identity information. Finally, we conduct face identification on CMU MultiPIE, and verification on Labeled Faces in the Wild (LFW) databases, where identification rank-1 accuracy and face verification accuracy with ROC curve are reported. These experiments demonstrate that our model is superior to oth- er state-of-the-art approaches on handling pose variations.</p><p>2 0.32367161 <a title="335-tfidf-2" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>Author: Kristina Scherbaum, James Petterson, Rogerio S. Feris, Volker Blanz, Hans-Peter Seidel</p><p>Abstract: Face detection is an important task in computer vision and often serves as the first step for a variety of applications. State-of-the-art approaches use efficient learning algorithms and train on large amounts of manually labeled imagery. Acquiring appropriate training images, however, is very time-consuming and does not guarantee that the collected training data is representative in terms of data variability. Moreover, available data sets are often acquired under controlled settings, restricting, for example, scene illumination or 3D head pose to a narrow range. This paper takes a look into the automated generation of adaptive training samples from a 3D morphable face model. Using statistical insights, the tailored training data guarantees full data variability and is enriched by arbitrary facial attributes such as age or body weight. Moreover, it can automatically adapt to environmental constraints, such as illumination or viewing angle of recorded video footage from surveillance cameras. We use the tailored imagery to train a new many-core imple- mentation of Viola Jones ’ AdaBoost object detection framework. The new implementation is not only faster but also enables the use of multiple feature channels such as color features at training time. In our experiments we trained seven view-dependent face detectors and evaluate these on the Face Detection Data Set and Benchmark (FDDB). Our experiments show that the use of tailored training imagery outperforms state-of-the-art approaches on this challenging dataset.</p><p>3 0.29905197 <a title="335-tfidf-3" href="./iccv-2013-Coupling_Alignments_with_Recognition_for_Still-to-Video_Face_Recognition.html">97 iccv-2013-Coupling Alignments with Recognition for Still-to-Video Face Recognition</a></p>
<p>Author: Zhiwu Huang, Xiaowei Zhao, Shiguang Shan, Ruiping Wang, Xilin Chen</p><p>Abstract: The Still-to-Video (S2V) face recognition systems typically need to match faces in low-quality videos captured under unconstrained conditions against high quality still face images, which is very challenging because of noise, image blur, lowface resolutions, varying headpose, complex lighting, and alignment difficulty. To address the problem, one solution is to select the frames of ‘best quality ’ from videos (hereinafter called quality alignment in this paper). Meanwhile, the faces in the selected frames should also be geometrically aligned to the still faces offline well-aligned in the gallery. In this paper, we discover that the interactions among the three tasks–quality alignment, geometric alignment and face recognition–can benefit from each other, thus should be performed jointly. With this in mind, we propose a Coupling Alignments with Recognition (CAR) method to tightly couple these tasks via low-rank regularized sparse representation in a unified framework. Our method makes the three tasks promote mutually by a joint optimization in an Augmented Lagrange Multiplier routine. Extensive , experiments on two challenging S2V datasets demonstrate that our method outperforms the state-of-the-art methods impressively.</p><p>4 0.24188341 <a title="335-tfidf-4" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>Author: Renliang Weng, Jiwen Lu, Junlin Hu, Gao Yang, Yap-Peng Tan</p><p>Abstract: Over the past two decades, a number of face recognition methods have been proposed in the literature. Most of them use holistic face images to recognize people. However, human faces are easily occluded by other objects in many real-world scenarios and we have to recognize the person of interest from his/her partial faces. In this paper, we propose a new partial face recognition approach by using feature set matching, which is able to align partial face patches to holistic gallery faces automatically and is robust to occlusions and illumination changes. Given each gallery image and probe face patch, we first detect keypoints and extract their local features. Then, we propose a Metric Learned ExtendedRobust PointMatching (MLERPM) method to discriminatively match local feature sets of a pair of gallery and probe samples. Lastly, the similarity of two faces is converted as the distance between two feature sets. Experimental results on three public face databases are presented to show the effectiveness of the proposed approach.</p><p>5 0.23264253 <a title="335-tfidf-5" href="./iccv-2013-Cascaded_Shape_Space_Pruning_for_Robust_Facial_Landmark_Detection.html">70 iccv-2013-Cascaded Shape Space Pruning for Robust Facial Landmark Detection</a></p>
<p>Author: Xiaowei Zhao, Shiguang Shan, Xiujuan Chai, Xilin Chen</p><p>Abstract: In this paper, we propose a novel cascaded face shape space pruning algorithm for robust facial landmark detection. Through progressively excluding the incorrect candidate shapes, our algorithm can accurately and efficiently achieve the globally optimal shape configuration. Specifically, individual landmark detectors are firstly applied to eliminate wrong candidates for each landmark. Then, the candidate shape space is further pruned by jointly removing incorrect shape configurations. To achieve this purpose, a discriminative structure classifier is designed to assess the candidate shape configurations. Based on the learned discriminative structure classifier, an efficient shape space pruning strategy is proposed to quickly reject most incorrect candidate shapes while preserve the true shape. The proposed algorithm is carefully evaluated on a large set of real world face images. In addition, comparison results on the publicly available BioID and LFW face databases demonstrate that our algorithm outperforms some state-of-the-art algorithms.</p><p>6 0.22919078 <a title="335-tfidf-6" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>7 0.2163768 <a title="335-tfidf-7" href="./iccv-2013-Pose-Free_Facial_Landmark_Fitting_via_Optimized_Part_Mixtures_and_Cascaded_Deformable_Shape_Model.html">321 iccv-2013-Pose-Free Facial Landmark Fitting via Optimized Part Mixtures and Cascaded Deformable Shape Model</a></p>
<p>8 0.21548401 <a title="335-tfidf-8" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>9 0.2066817 <a title="335-tfidf-9" href="./iccv-2013-Deep_Learning_Identity-Preserving_Face_Space.html">106 iccv-2013-Deep Learning Identity-Preserving Face Space</a></p>
<p>10 0.19602165 <a title="335-tfidf-10" href="./iccv-2013-Hidden_Factor_Analysis_for_Age_Invariant_Face_Recognition.html">195 iccv-2013-Hidden Factor Analysis for Age Invariant Face Recognition</a></p>
<p>11 0.19563575 <a title="335-tfidf-11" href="./iccv-2013-Hybrid_Deep_Learning_for_Face_Verification.html">206 iccv-2013-Hybrid Deep Learning for Face Verification</a></p>
<p>12 0.19369748 <a title="335-tfidf-12" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>13 0.19340776 <a title="335-tfidf-13" href="./iccv-2013-Accurate_and_Robust_3D_Facial_Capture_Using_a_Single_RGBD_Camera.html">36 iccv-2013-Accurate and Robust 3D Facial Capture Using a Single RGBD Camera</a></p>
<p>14 0.19146252 <a title="335-tfidf-14" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>15 0.1817036 <a title="335-tfidf-15" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>16 0.16538937 <a title="335-tfidf-16" href="./iccv-2013-Internet_Based_Morphable_Model.html">219 iccv-2013-Internet Based Morphable Model</a></p>
<p>17 0.16075143 <a title="335-tfidf-17" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>18 0.14829561 <a title="335-tfidf-18" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>19 0.13247132 <a title="335-tfidf-19" href="./iccv-2013-Exemplar-Based_Graph_Matching_for_Robust_Facial_Landmark_Localization.html">149 iccv-2013-Exemplar-Based Graph Matching for Robust Facial Landmark Localization</a></p>
<p>20 0.13117549 <a title="335-tfidf-20" href="./iccv-2013-No_Matter_Where_You_Are%3A_Flexible_Graph-Guided_Multi-task_Learning_for_Multi-view_Head_Pose_Classification_under_Target_Motion.html">291 iccv-2013-No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.248), (1, 0.037), (2, -0.143), (3, -0.15), (4, -0.028), (5, -0.225), (6, 0.385), (7, 0.135), (8, 0.006), (9, 0.04), (10, -0.041), (11, 0.091), (12, 0.062), (13, -0.005), (14, -0.064), (15, 0.045), (16, -0.047), (17, -0.038), (18, -0.058), (19, 0.041), (20, -0.023), (21, -0.139), (22, 0.064), (23, -0.082), (24, 0.025), (25, 0.059), (26, -0.041), (27, 0.025), (28, -0.006), (29, 0.098), (30, 0.021), (31, -0.024), (32, 0.035), (33, -0.036), (34, -0.069), (35, -0.046), (36, -0.047), (37, -0.03), (38, -0.001), (39, 0.037), (40, 0.015), (41, -0.019), (42, -0.027), (43, -0.028), (44, -0.013), (45, 0.028), (46, -0.017), (47, -0.018), (48, -0.004), (49, 0.005)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97558218 <a title="335-lsi-1" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>Author: Yizhe Zhang, Ming Shao, Edward K. Wong, Yun Fu</p><p>Abstract: One of the most challenging task in face recognition is to identify people with varied poses. Namely, the test faces have significantly different poses compared with the registered faces. In this paper, we propose a high-level feature learning scheme to extract pose-invariant identity feature for face recognition. First, we build a single-hiddenlayer neural network with sparse constraint, to extractposeinvariant feature in a supervised fashion. Second, we further enhance the discriminative capability of the proposed feature by using multiple random faces as the target values for multiple encoders. By enforcing the target values to be uniquefor inputfaces over differentposes, the learned highlevel feature that is represented by the neurons in the hidden layer is pose free and only relevant to the identity information. Finally, we conduct face identification on CMU MultiPIE, and verification on Labeled Faces in the Wild (LFW) databases, where identification rank-1 accuracy and face verification accuracy with ROC curve are reported. These experiments demonstrate that our model is superior to oth- er state-of-the-art approaches on handling pose variations.</p><p>2 0.90480465 <a title="335-lsi-2" href="./iccv-2013-Deep_Learning_Identity-Preserving_Face_Space.html">106 iccv-2013-Deep Learning Identity-Preserving Face Space</a></p>
<p>Author: Zhenyao Zhu, Ping Luo, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: Face recognition with large pose and illumination variations is a challenging problem in computer vision. This paper addresses this challenge by proposing a new learningbased face representation: the face identity-preserving (FIP) features. Unlike conventional face descriptors, the FIP features can significantly reduce intra-identity variances, while maintaining discriminativeness between identities. Moreover, the FIP features extracted from an image under any pose and illumination can be used to reconstruct its face image in the canonical view. This property makes it possible to improve the performance of traditional descriptors, such as LBP [2] and Gabor [31], which can be extracted from our reconstructed images in the canonical view to eliminate variations. In order to learn the FIP features, we carefully design a deep network that combines the feature extraction layers and the reconstruction layer. The former encodes a face image into the FIP features, while the latter transforms them to an image in the canonical view. Extensive experiments on the large MultiPIE face database [7] demonstrate that it significantly outperforms the state-of-the-art face recognition methods.</p><p>3 0.89600348 <a title="335-lsi-3" href="./iccv-2013-Hidden_Factor_Analysis_for_Age_Invariant_Face_Recognition.html">195 iccv-2013-Hidden Factor Analysis for Age Invariant Face Recognition</a></p>
<p>Author: Dihong Gong, Zhifeng Li, Dahua Lin, Jianzhuang Liu, Xiaoou Tang</p><p>Abstract: Age invariant face recognition has received increasing attention due to its great potential in real world applications. In spite of the great progress in face recognition techniques, reliably recognizingfaces across ages remains a difficult task. The facial appearance of a person changes substantially over time, resulting in significant intra-class variations. Hence, the key to tackle this problem is to separate the variation caused by aging from the person-specific features that are stable. Specifically, we propose a new method, calledHidden FactorAnalysis (HFA). This methodcaptures the intuition above through a probabilistic model with two latent factors: an identity factor that is age-invariant and an age factor affected by the aging process. Then, the observed appearance can be modeled as a combination of the components generated based on these factors. We also develop a learning algorithm that jointly estimates the latent factors and the model parameters using an EM procedure. Extensive experiments on two well-known public domain face aging datasets: MORPH (the largest public face aging database) and FGNET, clearly show that the proposed method achieves notable improvement over state-of-the-art algorithms.</p><p>4 0.86479056 <a title="335-lsi-4" href="./iccv-2013-Face_Recognition_via_Archetype_Hull_Ranking.html">154 iccv-2013-Face Recognition via Archetype Hull Ranking</a></p>
<p>Author: Yuanjun Xiong, Wei Liu, Deli Zhao, Xiaoou Tang</p><p>Abstract: The archetype hull model is playing an important role in large-scale data analytics and mining, but rarely applied to vision problems. In this paper, we migrate such a geometric model to address face recognition and verification together through proposing a unified archetype hull ranking framework. Upon a scalable graph characterized by a compact set of archetype exemplars whose convex hull encompasses most of the training images, the proposed framework explicitly captures the relevance between any query and the stored archetypes, yielding a rank vector over the archetype hull. The archetype hull ranking is then executed on every block of face images to generate a blockwise similarity measure that is achieved by comparing two different rank vectors with respect to the same archetype hull. After integrating blockwise similarity measurements with learned importance weights, we accomplish a sensible face similarity measure which can support robust and effective face recognition and verification. We evaluate the face similarity measure in terms of experiments performed on three benchmark face databases Multi-PIE, Pubfig83, and LFW, demonstrat- ing its performance superior to the state-of-the-arts.</p><p>5 0.85744518 <a title="335-lsi-5" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>Author: Kristina Scherbaum, James Petterson, Rogerio S. Feris, Volker Blanz, Hans-Peter Seidel</p><p>Abstract: Face detection is an important task in computer vision and often serves as the first step for a variety of applications. State-of-the-art approaches use efficient learning algorithms and train on large amounts of manually labeled imagery. Acquiring appropriate training images, however, is very time-consuming and does not guarantee that the collected training data is representative in terms of data variability. Moreover, available data sets are often acquired under controlled settings, restricting, for example, scene illumination or 3D head pose to a narrow range. This paper takes a look into the automated generation of adaptive training samples from a 3D morphable face model. Using statistical insights, the tailored training data guarantees full data variability and is enriched by arbitrary facial attributes such as age or body weight. Moreover, it can automatically adapt to environmental constraints, such as illumination or viewing angle of recorded video footage from surveillance cameras. We use the tailored imagery to train a new many-core imple- mentation of Viola Jones ’ AdaBoost object detection framework. The new implementation is not only faster but also enables the use of multiple feature channels such as color features at training time. In our experiments we trained seven view-dependent face detectors and evaluate these on the Face Detection Data Set and Benchmark (FDDB). Our experiments show that the use of tailored training imagery outperforms state-of-the-art approaches on this challenging dataset.</p><p>6 0.84476179 <a title="335-lsi-6" href="./iccv-2013-Hybrid_Deep_Learning_for_Face_Verification.html">206 iccv-2013-Hybrid Deep Learning for Face Verification</a></p>
<p>7 0.83516258 <a title="335-lsi-7" href="./iccv-2013-Modifying_the_Memorability_of_Face_Photographs.html">272 iccv-2013-Modifying the Memorability of Face Photographs</a></p>
<p>8 0.8117153 <a title="335-lsi-8" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>9 0.81056184 <a title="335-lsi-9" href="./iccv-2013-Sieving_Regression_Forest_Votes_for_Facial_Feature_Detection_in_the_Wild.html">391 iccv-2013-Sieving Regression Forest Votes for Facial Feature Detection in the Wild</a></p>
<p>10 0.80019605 <a title="335-lsi-10" href="./iccv-2013-Coupling_Alignments_with_Recognition_for_Still-to-Video_Face_Recognition.html">97 iccv-2013-Coupling Alignments with Recognition for Still-to-Video Face Recognition</a></p>
<p>11 0.75031936 <a title="335-lsi-11" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>12 0.73080546 <a title="335-lsi-12" href="./iccv-2013-Fast_High_Dimensional_Vector_Multiplication_Face_Recognition.html">158 iccv-2013-Fast High Dimensional Vector Multiplication Face Recognition</a></p>
<p>13 0.71075267 <a title="335-lsi-13" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>14 0.70299721 <a title="335-lsi-14" href="./iccv-2013-Markov_Network-Based_Unified_Classifier_for_Face_Identification.html">261 iccv-2013-Markov Network-Based Unified Classifier for Face Identification</a></p>
<p>15 0.70270073 <a title="335-lsi-15" href="./iccv-2013-Pose-Free_Facial_Landmark_Fitting_via_Optimized_Part_Mixtures_and_Cascaded_Deformable_Shape_Model.html">321 iccv-2013-Pose-Free Facial Landmark Fitting via Optimized Part Mixtures and Cascaded Deformable Shape Model</a></p>
<p>16 0.70257133 <a title="335-lsi-16" href="./iccv-2013-Like_Father%2C_Like_Son%3A_Facial_Expression_Dynamics_for_Kinship_Verification.html">251 iccv-2013-Like Father, Like Son: Facial Expression Dynamics for Kinship Verification</a></p>
<p>17 0.68816066 <a title="335-lsi-17" href="./iccv-2013-Robust_Face_Landmark_Estimation_under_Occlusion.html">355 iccv-2013-Robust Face Landmark Estimation under Occlusion</a></p>
<p>18 0.68685621 <a title="335-lsi-18" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>19 0.67355555 <a title="335-lsi-19" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>20 0.64989376 <a title="335-lsi-20" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.047), (4, 0.015), (7, 0.017), (12, 0.018), (26, 0.073), (31, 0.064), (34, 0.254), (42, 0.19), (64, 0.035), (73, 0.019), (89, 0.155), (98, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89197683 <a title="335-lda-1" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>Author: Naman Turakhia, Devi Parikh</p><p>Abstract: When we look at an image, some properties or attributes of the image stand out more than others. When describing an image, people are likely to describe these dominant attributes first. Attribute dominance is a result of a complex interplay between the various properties present or absent in the image. Which attributes in an image are more dominant than others reveals rich information about the content of the image. In this paper we tap into this information by modeling attribute dominance. We show that this helps improve the performance of vision systems on a variety of human-centric applications such as zero-shot learning, image search and generating textual descriptions of images.</p><p>2 0.85888529 <a title="335-lda-2" href="./iccv-2013-How_Do_You_Tell_a_Blackbird_from_a_Crow%3F.html">202 iccv-2013-How Do You Tell a Blackbird from a Crow?</a></p>
<p>Author: Thomas Berg, Peter N. Belhumeur</p><p>Abstract: How do you tell a blackbirdfrom a crow? There has been great progress toward automatic methods for visual recognition, including fine-grained visual categorization in which the classes to be distinguished are very similar. In a task such as bird species recognition, automatic recognition systems can now exceed the performance of non-experts – most people are challenged to name a couple dozen bird species, let alone identify them. This leads us to the question, “Can a recognition system show humans what to look for when identifying classes (in this case birds)? ” In the context of fine-grained visual categorization, we show that we can automatically determine which classes are most visually similar, discover what visual features distinguish very similar classes, and illustrate the key features in a way meaningful to humans. Running these methods on a dataset of bird images, we can generate a visual field guide to birds which includes a tree of similarity that displays the similarity relations between all species, pages for each species showing the most similar other species, and pages for each pair of similar species illustrating their differences.</p><p>same-paper 3 0.83503318 <a title="335-lda-3" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>Author: Yizhe Zhang, Ming Shao, Edward K. Wong, Yun Fu</p><p>Abstract: One of the most challenging task in face recognition is to identify people with varied poses. Namely, the test faces have significantly different poses compared with the registered faces. In this paper, we propose a high-level feature learning scheme to extract pose-invariant identity feature for face recognition. First, we build a single-hiddenlayer neural network with sparse constraint, to extractposeinvariant feature in a supervised fashion. Second, we further enhance the discriminative capability of the proposed feature by using multiple random faces as the target values for multiple encoders. By enforcing the target values to be uniquefor inputfaces over differentposes, the learned highlevel feature that is represented by the neurons in the hidden layer is pose free and only relevant to the identity information. Finally, we conduct face identification on CMU MultiPIE, and verification on Labeled Faces in the Wild (LFW) databases, where identification rank-1 accuracy and face verification accuracy with ROC curve are reported. These experiments demonstrate that our model is superior to oth- er state-of-the-art approaches on handling pose variations.</p><p>4 0.82409179 <a title="335-lda-4" href="./iccv-2013-Multi-scale_Topological_Features_for_Hand_Posture_Representation_and_Analysis.html">278 iccv-2013-Multi-scale Topological Features for Hand Posture Representation and Analysis</a></p>
<p>Author: Kaoning Hu, Lijun Yin</p><p>Abstract: In this paper, we propose a multi-scale topological feature representation for automatic analysis of hand posture. Such topological features have the advantage of being posture-dependent while being preserved under certain variations of illumination, rotation, personal dependency, etc. Our method studies the topology of the holes between the hand region and its convex hull. Inspired by the principle of Persistent Homology, which is the theory of computational topology for topological feature analysis over multiple scales, we construct the multi-scale Betti Numbers matrix (MSBNM) for the topological feature representation. In our experiments, we used 12 different hand postures and compared our features with three popular features (HOG, MCT, and Shape Context) on different data sets. In addition to hand postures, we also extend the feature representations to arm postures. The results demonstrate the feasibility and reliability of the proposed method.</p><p>5 0.80401754 <a title="335-lda-5" href="./iccv-2013-Box_in_the_Box%3A_Joint_3D_Layout_and_Object_Reasoning_from_Single_Images.html">64 iccv-2013-Box in the Box: Joint 3D Layout and Object Reasoning from Single Images</a></p>
<p>Author: Alexander G. Schwing, Sanja Fidler, Marc Pollefeys, Raquel Urtasun</p><p>Abstract: In this paper we propose an approach to jointly infer the room layout as well as the objects present in the scene. Towards this goal, we propose a branch and bound algorithm which is guaranteed to retrieve the global optimum of the joint problem. The main difficulty resides in taking into account occlusion in order to not over-count the evidence. We introduce a new decomposition method, which generalizes integral geometry to triangular shapes, and allows us to bound the different terms in constant time. We exploit both geometric cues and object detectors as image features and show large improvements in 2D and 3D object detection over state-of-the-art deformable part-based models.</p><p>6 0.80177224 <a title="335-lda-6" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>7 0.79878962 <a title="335-lda-7" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>8 0.79258472 <a title="335-lda-8" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>9 0.78827399 <a title="335-lda-9" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>10 0.76753378 <a title="335-lda-10" href="./iccv-2013-Efficient_and_Robust_Large-Scale_Rotation_Averaging.html">138 iccv-2013-Efficient and Robust Large-Scale Rotation Averaging</a></p>
<p>11 0.75789487 <a title="335-lda-11" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>12 0.73405802 <a title="335-lda-12" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>13 0.72462153 <a title="335-lda-13" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>14 0.72419369 <a title="335-lda-14" href="./iccv-2013-Deep_Learning_Identity-Preserving_Face_Space.html">106 iccv-2013-Deep Learning Identity-Preserving Face Space</a></p>
<p>15 0.72212166 <a title="335-lda-15" href="./iccv-2013-Coherent_Object_Detection_with_3D_Geometric_Context_from_a_Single_Image.html">79 iccv-2013-Coherent Object Detection with 3D Geometric Context from a Single Image</a></p>
<p>16 0.72122532 <a title="335-lda-16" href="./iccv-2013-Implied_Feedback%3A_Learning_Nuances_of_User_Behavior_in_Image_Search.html">213 iccv-2013-Implied Feedback: Learning Nuances of User Behavior in Image Search</a></p>
<p>17 0.71965748 <a title="335-lda-17" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>18 0.71914399 <a title="335-lda-18" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>19 0.71792322 <a title="335-lda-19" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>20 0.71611702 <a title="335-lda-20" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
