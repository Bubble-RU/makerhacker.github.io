<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>48 iccv-2013-An Adaptive Descriptor Design for Object Recognition in the Wild</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-48" href="#">iccv2013-48</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>48 iccv-2013-An Adaptive Descriptor Design for Object Recognition in the Wild</h1>
<br/><p>Source: <a title="iccv-2013-48-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Guo_An_Adaptive_Descriptor_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Zhenyu Guo, Z. Jane Wang</p><p>Abstract: Digital images nowadays show large appearance variabilities on picture styles, in terms of color tone, contrast, vignetting, and etc. These ‘picture styles’ are directly related to the scene radiance, image pipeline of the camera, and post processing functions (e.g., photography effect filters). Due to the complexity and nonlinearity of these factors, popular gradient-based image descriptors generally are not invariant to different picture styles, which could degrade the performance for object recognition. Given that images shared online or created by individual users are taken with a wide range of devices and may be processed by various post processing functions, to find a robust object recognition system is useful and challenging. In this paper, we investigate the influence of picture styles on object recognition by making a connection between image descriptors and a pixel mapping function g, and accordingly propose an adaptive approach based on a g-incorporated kernel descriptor and multiple kernel learning, without estimating or specifying the image styles used in training and testing. We conduct experiments on the Domain Adaptation data set, the Oxford Flower data set, and several variants of the Flower data set by introducing popular photography effects through post-processing. The results demonstrate that theproposedmethod consistently yields recognition improvements over standard descriptors in all studied cases.</p><p>Reference: <a title="iccv-2013-48-reference" href="../iccv2013_reference/iccv-2013-An_Adaptive_Descriptor_Design_for_Object_Recognition_in_the_Wild_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ca  Abstract Digital images nowadays show large appearance variabilities on picture styles, in terms of color tone, contrast, vignetting, and etc. [sent-5, score-0.373]
</p><p>2 Due to the complexity and nonlinearity of these factors, popular gradient-based image descriptors generally are not invariant to different picture styles, which could degrade the performance for object recognition. [sent-9, score-0.506]
</p><p>3 Given that images shared online or created by individual users are taken with a wide range of devices and may be processed by various post processing functions, to find a robust object recognition system is useful and challenging. [sent-10, score-0.375]
</p><p>4 We conduct experiments on the Domain Adaptation data set, the Oxford Flower data set, and several variants of the Flower data set by introducing popular photography effects through post-processing. [sent-12, score-0.242]
</p><p>5 The results demonstrate that theproposedmethod consistently yields recognition improvements over standard descriptors in all studied cases. [sent-13, score-0.257]
</p><p>6 Here we refer such characteristics of digital images as picture styles. [sent-16, score-0.411]
</p><p>7 With the popularity of photo editing and sharing services such as Instagram, Facebook and Flickr that are available on mobile devices, many digital images generated by users nowadays are captured by a wide range of devices (e. [sent-17, score-0.25]
</p><p>8 , smart phones and digital slrs) and processed using different photography effect filters (e. [sent-19, score-0.353]
</p><p>9 We show 3 pairs of images about the same objects with different picture styles. [sent-22, score-0.344]
</p><p>10 (c) and (d) are of the same object and taken under the same condition by a digital SLR and a webcam respectively, representing two different image pipelines. [sent-24, score-0.18]
</p><p>11 (f) is an image obtained by applying InstaramTMlomo-fi effect filter as a post-processing step to image (e), representing one specific photography effect. [sent-25, score-0.206]
</p><p>12 Instagram) to get distinct picture styles with strong personal artistic expressions. [sent-26, score-0.697]
</p><p>13 We select two regions at the same location from the two images (indicated by red boxes), and show the pixel patch, gradient, and SIFT descriptor for each of them. [sent-30, score-0.212]
</p><p>14 We then plot the difference between two descriptors in the right. [sent-31, score-0.17]
</p><p>15 tion is to recognize natural scenes [14], daily objects [5], or fine-grained species [18, 23] based on digital images, it is natural to extend the scope of object recognition from standard laboratory images to photos in the wild for daily use. [sent-32, score-0.27]
</p><p>16 Although there are a large number of picture styles, their contributing factors can be separated into 3 major categories: (1) scene radiance, (2) image pipeline, and (3) post processing. [sent-33, score-0.353]
</p><p>17 1, we show three pairs of images of the same objects to illustrate different picture styles. [sent-35, score-0.344]
</p><p>18 To illustrate the connection between image descriptors with picture styles, we take an image from the Oxford Flower data set and process it with a popular Instagram effect filter: lomo-fi. [sent-36, score-0.557]
</p><p>19 We select two patches at the same locations for these two images respectively, and compute the gradients and SIFT descriptors of the patches, which are shown in Fig. [sent-37, score-0.247]
</p><p>20 Although these two image patches are almost the same except the color tones, we note that the resulting SIFT descriptors differ with each other about 33% in terms of l2 norm, which probably will make them be quantized into two dictionary words in the bag-of-word model. [sent-39, score-0.238]
</p><p>21 Since the difference is significant for two images that are almost identical in content, it is reasonable to assume that the difference could be more significant for two contentdifferent images with different picture styles within one object class. [sent-40, score-0.797]
</p><p>22 Therefore, when images used for training and testing don’t have similar picture styles, the accuracy of object recognition will degrade. [sent-41, score-0.475]
</p><p>23 Although the DA touches the picture style issue by considering two sets of images from different devices as two domains, in their algorithms the domain lable of an image has to be specified. [sent-46, score-0.567]
</p><p>24 However, in real world applications, images collected from Internet have no “domain labels”, and the training /testing sets are always mixtures of images with various picture styles. [sent-47, score-0.406]
</p><p>25 Furthermore, more picture styles can be created by users through post-processing (e. [sent-48, score-0.768]
</p><p>26 Therefore, with a more general setting than DA, developing robust object recognition algorithms becomes useful and challenging, which should overcome the difficulties introduced by different picture styles without knowing the style information. [sent-51, score-0.826]
</p><p>27 In this paper, we study this general object recognition problem with a focus on picture-style-considered descriptor design. [sent-52, score-0.197]
</p><p>28 Existing approaches usually ignore the differences of picture styles when computing the standard descriptors, and then try to reduce the influences of picture styles in the corresponding feature spaces. [sent-53, score-1.436]
</p><p>29 a tFioonr, c we assume t whaet dtheneo pixel mapping fFuronmction g would influence the object recognition accuracy when the images used in training and testing are processed by g (which is confirmed later by experimental results in Section 4. [sent-64, score-0.312]
</p><p>30 However, by defining g based on a convex combination of several basis functions, in this paper, we incorporate the pixel mapping function g into image descriptors, and we propose an adaptive descriptor design based on kernel learning. [sent-69, score-0.565]
</p><p>31 Though we derive the method based on kernel descriptors [2], it is worth mentioning that the proposed approach can be extended to existing standard descriptors as a general framework. [sent-70, score-0.548]
</p><p>32 Then we revisit the kernel descriptors in Section 2. [sent-73, score-0.365]
</p><p>33 In the data set introduced in [19], images from dslr and webcam are different in picture styles, which is similar to the focus of this paper. [sent-78, score-0.45]
</p><p>34 Works in [9, 12,26] estimate the model ofimage pipelines, but such estimations are difficult and have no clear relationships with the descriptor and recognition accuracy. [sent-81, score-0.169]
</p><p>35 In the area of key point matching, several robust descriptors were proposed, such as DAISY [21], GIH [15] and DaLI [17]. [sent-82, score-0.17]
</p><p>36 Descriptor learning methods [20, 24, 25] were also developed to determine the parameters of the descriptors through optimization. [sent-83, score-0.17]
</p><p>37 The different goal leads to descriptors that are not suitable for object recognition, since they are too discriminative to tolerate the within-class variances of object categories. [sent-85, score-0.226]
</p><p>38 Kernel Descriptor Revisit The kernel descriptor (KDES) is proposed by Bo et. [sent-87, score-0.29]
</p><p>39 The gradient match kernel between two image patches P and Q can be described as kgrad(P,Q) = ? [sent-92, score-0.268]
</p><p>40 | |2) is a Gaussian position kernel and ko(θ˜(z), θ˜(z? [sent-103, score-0.166]
</p><p>41 ) | |2 is a Gaussian kernel over gradient −oγrie|n|θta(tzio)n −s. [sent-105, score-0.198]
</p><p>42 [2] also showed that gradient based descriptor like SIFT [16], SURF [1], and HoG [4] are special cases under this kernel view framework. [sent-117, score-0.322]
</p><p>43 For the image-level descriptors, Bo and Sminchisescu [3] presented Efficient Match Kernels (EMK) which provide a general kernel view of matching between two images as two sets of local descriptors. [sent-118, score-0.202]
</p><p>44 Let X and Y denote the set of local descriptors for images Ix and Iy respectively. [sent-120, score-0.206]
</p><p>45 iWs hae dne applying EeMctoKr on top otefd kgrad, we can have the image-level kernel as  Kemk(Ix,Iy) =|X1||Y |x? [sent-122, score-0.193]
</p><p>46 v [e3c]to pr souvcihd tsha at Kemk (Ix , Iy) = Φ(Ix)TΦ(Iy), which makes the match kernel can be used in real applications with efficient computation and storage. [sent-126, score-0.195]
</p><p>47 g-incorporated Kernel Descriptor As stated in Introduction, we want to apply a pixel mapping function g to images used for object recognition. [sent-130, score-0.192]
</p><p>48 In this section, we will give the relationship between pixels and descriptors under the function g. [sent-131, score-0.17]
</p><p>49 Notice that the magnitudes used in kgrad are normalized based on local patches, which is important to make the contextual information comparable for different patches. [sent-159, score-0.196]
</p><p>50 Now given two image patches g(P) and g(Q), which are obtained by applying the function g to patches P and Q respectively, we derive the gradient match kernel between them as following  ˆkgrad(g(P),g(Q)) = ? [sent-167, score-0.336]
</p><p>51 And it is worth noting that kˆgrad above is different from the standard kgrad in Eq. [sent-221, score-0.238]
</p><p>52 In addition, Km’s are positive definite (PD) kernels, which makes Kemk here a convex combination of PD kernels and can be used in standard multiple kernel learning. [sent-247, score-0.328]
</p><p>53 Therefore, we successfully transfer the problem of searching optimal g∗ into the problem of learning the optimal kernel weights through Eq. [sent-248, score-0.166]
</p><p>54 , photography effect filters) used for photography, we note that Gamma correction and the “S” curve are two major categories of photography effects. [sent-262, score-0.379]
</p><p>55 (8), the image-level kernel can be decomposed as a convex combination of several base kernels. [sent-285, score-0.23]
</p><p>56 We adopt General Multiple Kernel Learning (GMKL) [22] and put non-negative constraints on the kernel coefficients. [sent-286, score-0.166]
</p><p>57 Adaptive Descriptor Design  In this section, we summarize the major steps of the proposed adaptive descriptor design as follows: Step-1 Process image I from the data set with {gi}i3=1 shown Pinr Eq. [sent-293, score-0.217]
</p><p>58 Step-2 Compute gradient-based descriptors {gi(I)}i3=1 to get 4 descriptors. [sent-295, score-0.17]
</p><p>59 for  I and  Step-3 Build a codebook using K-means by sampling from all training images and all 4 descriptors of each image. [sent-296, score-0.311]
</p><p>60 Step-6 Train GMKL on 16 base kernels to obtain optimal kernel weights. [sent-300, score-0.29]
</p><p>61 Our proposed method does not require prior knowledge on picture styles oftraining or testing images, and the Adaptive Descriptor Design (ADD) can work as a general framework. [sent-301, score-0.729]
</p><p>62 According to the analysis by [2], most gradient-based descriptors, such as SIFT [16], SURF [1] and HoG [4], are special cases of the kernel descriptor, which all can be used in Step-2 to compute descriptors from image patches. [sent-303, score-0.336]
</p><p>63 In other words, our proposed algorithm can be used widely to improve previous methods which are based on gradient descriptors and SVMs. [sent-305, score-0.202]
</p><p>64 For an image I from the data set, it is equivalent to extracting standard descriptors of g(I) and using a single kernel SVM based on these descriptors for classification. [sent-307, score-0.548]
</p><p>65 We also process the images from the Oxford  Flower data set using several popular photography effect filters in InstagramTM1 . [sent-320, score-0.276]
</p><p>66 As we stated in Introduction, the two domains dslr and webcam only differ in picture styles which are due to different image pipelines. [sent-324, score-0.888]
</p><p>67 Applying the proposed ADD algorithm, we adopt KDES + EMK and SURF+BoW two sets of features to demonstrate that ADD can work as a general framework to improve the performances of gradient-based descriptors in general. [sent-325, score-0.17]
</p><p>68 It is worth noting that we don’t use any domain-label information to specify the picture styles of images, our proposed method could figure out an optimal descriptor automatically based on the training set. [sent-327, score-0.847]
</p><p>69 1  ADD based on KDES and EMK  We extract KDES descriptors of all the images in three domains and create a 1,500-word codebook by applying Kmeans clustering on a subset of all 4 types (original + 3 variants for each images) of descriptors from amazon domain. [sent-330, score-0.612]
</p><p>70 And then this codebook is used to quantize 4 types of descriptors of all 3 domains of images using EMK. [sent-331, score-0.341]
</p><p>71 Af-  ter obtaining the 16 linear kernels by computing the inner product of every two types of descriptors between two given images, we conduct object recognition experiments using SVMs for: the standard KDES, the average kernel of these 16 kernels (AK), and the GMKL based on 16 kernels. [sent-332, score-0.662]
</p><p>72 the standard KDES in all cases for both the average kernel and an optimal kernel learned by GMKL. [sent-343, score-0.374]
</p><p>73 Particularly, the ADD GMKL method improves about 6% from the standard KDES in all cases, which is close to the improvements obtained by domain adaptation methods [7, 11, 13, 19] where domain-label information is used. [sent-344, score-0.235]
</p><p>74 2  ADD based on SURF and BoW  To show the general applicability of the proposed ADD,  we follow previous methods [7, 11, 13, 19] to extract standard SURF descriptors from the original and 3 variants of each image, then a 800-word codebook is created from amazon domain. [sent-347, score-0.393]
</p><p>75 All images in 3 domains are quantized by this codebook using Vector-quantization to get Bag-ofWord features. [sent-348, score-0.198]
</p><p>76 After obtaining 16 linear kernels, we also conduct experiments using the standard KDES, the average kernel, and an optimal kernel learned by GMKL. [sent-349, score-0.239]
</p><p>77 The proposed ADD methods also outperform the standard SURF descriptor in all cases. [sent-351, score-0.166]
</p><p>78 However, in this experiment, the average kernel approach gives better results than that of the GMKL learned kernel in some cases. [sent-352, score-0.332]
</p><p>79 We think the worse performance of the GMKL based ADD is due to the lack of training, since the SURF descriptors are sparsely extracted from images and only 11 (8 from the source domain and 3 from the target domain) training images per category are used. [sent-353, score-0.36]
</p><p>80 But the results of ADD AK and ADD GMKL are sufficient to show that the proposed Adaptive Descriptor Design can be applied on top of gradient-based descriptors widely, for different tasks. [sent-354, score-0.17]
</p><p>81 Oxford Flower Data Set Oxford Flower data set [18] contains 1360 images for 17 flower species. [sent-357, score-0.227]
</p><p>82 , they were taken by different devices under different conditions), the factors of scene radiance and image pipelines are already 22557733  Figure 5. [sent-363, score-0.178]
</p><p>83 We show an example image and its variants processed by 3 photography effect filters in Fig. [sent-366, score-0.325]
</p><p>84 For convenience of expression, we refer the data sets obtained by applying effect filters by picture styles. [sent-369, score-0.435]
</p><p>85 1, we extract KDES for all images from all 4 styles (original, lomo-fi, lord-kelvin, and Nashville). [sent-372, score-0.425]
</p><p>86 We construct a 2,000-word codebook by sampling 4 types of descriptors from the original style. [sent-373, score-0.249]
</p><p>87 To simulate the real image collections as mixtures of images with different picture styles, we first generate an ex-  perimental data set from the 4 styles, then split this data set into training and testing sets. [sent-375, score-0.402]
</p><p>88 1) Experimental data set generation: M styles are chosen first, from which we want to sample images. [sent-376, score-0.389]
</p><p>89 For a given ID, only one image is uniformly randomly selected from M styles (i. [sent-377, score-0.389]
</p><p>90 Different from domain adaptation, images used here in training or testing are not separated according to domain labels, which is more similar to real-world applications where no information of picture styles are available. [sent-384, score-0.975]
</p><p>91 We perform object recognition using SVMs for the standard KDES, average kernel, and the optimal kernel by GMKL. [sent-385, score-0.281]
</p><p>92 From the top 4 rows of Table 3, we note that the recognition accuracy decreases when images with different picture styles are used, which confirms the motivation we described in Section 1. [sent-389, score-0.778]
</p><p>93 Further, when images are uniformly sampled from all 4 styles, the standard KDES descriptor yields worst performance, which is reasonable since the higher diversity in appearances of images leads to larger differences between descriptors of similar image patches of the same objects. [sent-400, score-0.449]
</p><p>94 After demonstrating that picture styles can affect the recognition accuracy, the improved performance of ADD GMKL shows that our proposed algorithm is an efficient solution. [sent-401, score-0.742]
</p><p>95 Therefore, the Adaptive Descriptor Design can be used widely on top of gradient-based descriptors to further improve the recognition accuracy. [sent-403, score-0.215]
</p><p>96 Since 4 types of descriptors are extracted from one image on a dense grid and there are 1360 images in total, this codebook introduces large distortion in quantization, which decreases the performance of the average kernel approach. [sent-406, score-0.451]
</p><p>97 Conclusion In this paper, we focus on the effects of different picture styles of images on object recognition. [sent-408, score-0.793]
</p><p>98 After show22557744  ing the connection between pixel mapping functions and gradient-based image descriptors, we incorporate the pixel mapping function g into the image descriptor and propose an Adaptive Descriptor Design (ADD) framework for object recognition in the wild. [sent-409, score-0.498]
</p><p>99 Efficient match kernel between sets of features for visual recognition. [sent-428, score-0.195]
</p><p>100 What you saw is not what you get: Domain adaptation using asymmetric kernel transforms. [sent-498, score-0.267]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('styles', 0.389), ('gmkl', 0.343), ('picture', 0.308), ('kdes', 0.302), ('kgrad', 0.196), ('flower', 0.191), ('descriptors', 0.17), ('kernel', 0.166), ('photography', 0.14), ('oxford', 0.131), ('kp', 0.126), ('descriptor', 0.124), ('ko', 0.114), ('surf', 0.108), ('adaptation', 0.101), ('instagram', 0.098), ('domain', 0.092), ('conference', 0.09), ('kernels', 0.09), ('emk', 0.087), ('add', 0.087), ('codebook', 0.079), ('gamma', 0.079), ('devices', 0.075), ('akadd', 0.074), ('brightening', 0.074), ('kemk', 0.074), ('gi', 0.073), ('ieee', 0.071), ('digital', 0.067), ('darkening', 0.065), ('mg', 0.065), ('functions', 0.063), ('filters', 0.061), ('correction', 0.06), ('style', 0.056), ('iy', 0.056), ('webcam', 0.056), ('domains', 0.056), ('da', 0.055), ('pages', 0.054), ('adaptive', 0.054), ('basis', 0.053), ('pixel', 0.052), ('dslr', 0.05), ('adwmselrbacz', 0.049), ('fgrad', 0.049), ('kdesadd', 0.049), ('nashville', 0.049), ('omnwd', 0.049), ('oncomputer', 0.049), ('sourcetargetstandard', 0.049), ('tones', 0.049), ('mapping', 0.047), ('processed', 0.046), ('recognition', 0.045), ('bo', 0.045), ('post', 0.045), ('radiance', 0.043), ('users', 0.043), ('standard', 0.042), ('patches', 0.041), ('vignetting', 0.04), ('winder', 0.04), ('connection', 0.04), ('effect', 0.039), ('design', 0.039), ('pattern', 0.039), ('variants', 0.039), ('ix', 0.038), ('saenko', 0.036), ('images', 0.036), ('amazon', 0.035), ('slr', 0.035), ('daisy', 0.035), ('base', 0.034), ('ak', 0.033), ('international', 0.033), ('bow', 0.033), ('gradient', 0.032), ('effects', 0.032), ('testing', 0.032), ('pipelines', 0.031), ('sen', 0.031), ('conduct', 0.031), ('convex', 0.03), ('nowadays', 0.029), ('revisit', 0.029), ('stated', 0.029), ('taken', 0.029), ('match', 0.029), ('created', 0.028), ('object', 0.028), ('quantized', 0.027), ('applying', 0.027), ('daily', 0.026), ('ece', 0.026), ('training', 0.026), ('km', 0.026), ('patch', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="48-tfidf-1" href="./iccv-2013-An_Adaptive_Descriptor_Design_for_Object_Recognition_in_the_Wild.html">48 iccv-2013-An Adaptive Descriptor Design for Object Recognition in the Wild</a></p>
<p>Author: Zhenyu Guo, Z. Jane Wang</p><p>Abstract: Digital images nowadays show large appearance variabilities on picture styles, in terms of color tone, contrast, vignetting, and etc. These ‘picture styles’ are directly related to the scene radiance, image pipeline of the camera, and post processing functions (e.g., photography effect filters). Due to the complexity and nonlinearity of these factors, popular gradient-based image descriptors generally are not invariant to different picture styles, which could degrade the performance for object recognition. Given that images shared online or created by individual users are taken with a wide range of devices and may be processed by various post processing functions, to find a robust object recognition system is useful and challenging. In this paper, we investigate the influence of picture styles on object recognition by making a connection between image descriptors and a pixel mapping function g, and accordingly propose an adaptive approach based on a g-incorporated kernel descriptor and multiple kernel learning, without estimating or specifying the image styles used in training and testing. We conduct experiments on the Domain Adaptation data set, the Oxford Flower data set, and several variants of the Flower data set by introducing popular photography effects through post-processing. The results demonstrate that theproposedmethod consistently yields recognition improvements over standard descriptors in all studied cases.</p><p>2 0.14056166 <a title="48-tfidf-2" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>Author: Fatemeh Mirrashed, Mohammad Rastegari</p><p>Abstract: We propose an unsupervised domain adaptation method that exploits intrinsic compact structures of categories across different domains using binary attributes. Our method directly optimizes for classification in the target domain. The key insight is finding attributes that are discriminative across categories and predictable across domains. We achieve a performance that significantly exceeds the state-of-the-art results on standard benchmarks. In fact, in many cases, our method reaches the same-domain performance, the upper bound, in unsupervised domain adaptation scenarios.</p><p>3 0.1261394 <a title="48-tfidf-3" href="./iccv-2013-Unsupervised_Domain_Adaptation_by_Domain_Invariant_Projection.html">435 iccv-2013-Unsupervised Domain Adaptation by Domain Invariant Projection</a></p>
<p>Author: Mahsa Baktashmotlagh, Mehrtash T. Harandi, Brian C. Lovell, Mathieu Salzmann</p><p>Abstract: Domain-invariant representations are key to addressing the domain shift problem where the training and test examples follow different distributions. Existing techniques that have attempted to match the distributions of the source and target domains typically compare these distributions in the original feature space. This space, however, may not be directly suitable for such a comparison, since some of the features may have been distorted by the domain shift, or may be domain specific. In this paper, we introduce a Domain Invariant Projection approach: An unsupervised domain adaptation method that overcomes this issue by extracting the information that is invariant across the source and target domains. More specifically, we learn a projection of the data to a low-dimensional latent space where the distance between the empirical distributions of the source and target examples is minimized. We demonstrate the effectiveness of our approach on the task of visual object recognition and show that it outperforms state-of-the-art methods on a standard domain adaptation benchmark dataset.</p><p>4 0.1167491 <a title="48-tfidf-4" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<p>Author: Sadeep Jayasumana, Mathieu Salzmann, Hongdong Li, Mehrtash Harandi</p><p>Abstract: We propose a framework for 2D shape analysis using positive definite kernels defined on Kendall’s shape manifold. Different representations of 2D shapes are known to generate different nonlinear spaces. Due to the nonlinearity of these spaces, most existing shape classification algorithms resort to nearest neighbor methods and to learning distances on shape spaces. Here, we propose to map shapes on Kendall’s shape manifold to a high dimensional Hilbert space where Euclidean geometry applies. To this end, we introduce a kernel on this manifold that permits such a mapping, and prove its positive definiteness. This kernel lets us extend kernel-based algorithms developed for Euclidean spaces, such as SVM, MKL and kernel PCA, to the shape manifold. We demonstrate the benefits of our approach over the state-of-the-art methods on shape classification, clustering and retrieval.</p><p>5 0.11303271 <a title="48-tfidf-5" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>Author: Stefanos Zafeiriou, Irene Kotsia</p><p>Abstract: Kernels have been a common tool of machine learning and computer vision applications for modeling nonlinearities and/or the design of robust1 similarity measures between objects. Arguably, the class of positive semidefinite (psd) kernels, widely known as Mercer’s Kernels, constitutes one of the most well-studied cases. For every psd kernel there exists an associated feature map to an arbitrary dimensional Hilbert space H, the so-called feature space. Tdihme mnsaiionn reason ebreth sipnadc ep s Hd ,ke threne slos’-c c aplolpedul aferiattyu rise the fact that classification/regression techniques (such as Support Vector Machines (SVMs)) and component analysis algorithms (such as Kernel Principal Component Analysis (KPCA)) can be devised in H, without an explicit defisnisiti (oKnP of t)h)e c feature map, only by using athne xkperlniceitl (dtehfeso-called kernel trick). Recently, due to the development of very efficient solutions for large scale linear SVMs and for incremental linear component analysis, the research to- wards finding feature map approximations for classes of kernels has attracted significant interest. In this paper, we attempt the derivation of explicit feature maps of a recently proposed class of kernels, the so-called one-shot similarity kernels. We show that for this class of kernels either there exists an explicit representation in feature space or the kernel can be expressed in such a form that allows for exact incremental learning. We theoretically explore the properties of these kernels and show how these kernels can be used for the development of robust visual tracking, recognition and deformable fitting algorithms. 1Robustness may refer to either the presence of outliers and noise the robustness to a class of transformations (e.g., translation). or to ∗ Irene Kotsia ,†,? ∗Electronics Laboratory, Department of Physics, University of Patras, Greece ?School of Science and Technology, Middlesex University, London i .kot s i @mdx . ac .uk a</p><p>6 0.10326704 <a title="48-tfidf-6" href="./iccv-2013-Unsupervised_Visual_Domain_Adaptation_Using_Subspace_Alignment.html">438 iccv-2013-Unsupervised Visual Domain Adaptation Using Subspace Alignment</a></p>
<p>7 0.097108349 <a title="48-tfidf-7" href="./iccv-2013-Frustratingly_Easy_NBNN_Domain_Adaptation.html">181 iccv-2013-Frustratingly Easy NBNN Domain Adaptation</a></p>
<p>8 0.092417113 <a title="48-tfidf-8" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>9 0.09030357 <a title="48-tfidf-9" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>10 0.081198044 <a title="48-tfidf-10" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<p>11 0.079295963 <a title="48-tfidf-11" href="./iccv-2013-Nonparametric_Blind_Super-resolution.html">293 iccv-2013-Nonparametric Blind Super-resolution</a></p>
<p>12 0.078967579 <a title="48-tfidf-12" href="./iccv-2013-Quantize_and_Conquer%3A_A_Dimensionality-Recursive_Solution_to_Clustering%2C_Vector_Quantization%2C_and_Image_Retrieval.html">333 iccv-2013-Quantize and Conquer: A Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval</a></p>
<p>13 0.077087671 <a title="48-tfidf-13" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>14 0.068431295 <a title="48-tfidf-14" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>15 0.068195611 <a title="48-tfidf-15" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>16 0.067393772 <a title="48-tfidf-16" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>17 0.06675671 <a title="48-tfidf-17" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>18 0.064661659 <a title="48-tfidf-18" href="./iccv-2013-Fine-Grained_Categorization_by_Alignments.html">169 iccv-2013-Fine-Grained Categorization by Alignments</a></p>
<p>19 0.064436585 <a title="48-tfidf-19" href="./iccv-2013-EVSAC%3A_Accelerating_Hypotheses_Generation_by_Modeling_Matching_Scores_with_Extreme_Value_Theory.html">131 iccv-2013-EVSAC: Accelerating Hypotheses Generation by Modeling Matching Scores with Extreme Value Theory</a></p>
<p>20 0.062109716 <a title="48-tfidf-20" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.164), (1, 0.031), (2, -0.04), (3, -0.05), (4, -0.025), (5, 0.039), (6, 0.003), (7, -0.005), (8, 0.01), (9, -0.064), (10, -0.002), (11, -0.103), (12, 0.044), (13, -0.087), (14, 0.032), (15, -0.069), (16, 0.024), (17, -0.011), (18, 0.053), (19, -0.055), (20, 0.076), (21, -0.02), (22, 0.045), (23, 0.051), (24, 0.018), (25, 0.022), (26, -0.03), (27, 0.02), (28, -0.033), (29, 0.043), (30, 0.054), (31, 0.001), (32, -0.002), (33, -0.017), (34, -0.038), (35, 0.014), (36, 0.045), (37, -0.101), (38, 0.019), (39, 0.019), (40, -0.094), (41, 0.016), (42, 0.003), (43, -0.028), (44, -0.015), (45, -0.02), (46, 0.008), (47, -0.024), (48, -0.067), (49, -0.001)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93985891 <a title="48-lsi-1" href="./iccv-2013-An_Adaptive_Descriptor_Design_for_Object_Recognition_in_the_Wild.html">48 iccv-2013-An Adaptive Descriptor Design for Object Recognition in the Wild</a></p>
<p>Author: Zhenyu Guo, Z. Jane Wang</p><p>Abstract: Digital images nowadays show large appearance variabilities on picture styles, in terms of color tone, contrast, vignetting, and etc. These ‘picture styles’ are directly related to the scene radiance, image pipeline of the camera, and post processing functions (e.g., photography effect filters). Due to the complexity and nonlinearity of these factors, popular gradient-based image descriptors generally are not invariant to different picture styles, which could degrade the performance for object recognition. Given that images shared online or created by individual users are taken with a wide range of devices and may be processed by various post processing functions, to find a robust object recognition system is useful and challenging. In this paper, we investigate the influence of picture styles on object recognition by making a connection between image descriptors and a pixel mapping function g, and accordingly propose an adaptive approach based on a g-incorporated kernel descriptor and multiple kernel learning, without estimating or specifying the image styles used in training and testing. We conduct experiments on the Domain Adaptation data set, the Oxford Flower data set, and several variants of the Flower data set by introducing popular photography effects through post-processing. The results demonstrate that theproposedmethod consistently yields recognition improvements over standard descriptors in all studied cases.</p><p>2 0.72597438 <a title="48-lsi-2" href="./iccv-2013-Frustratingly_Easy_NBNN_Domain_Adaptation.html">181 iccv-2013-Frustratingly Easy NBNN Domain Adaptation</a></p>
<p>Author: Tatiana Tommasi, Barbara Caputo</p><p>Abstract: Over the last years, several authors have signaled that state of the art categorization methods fail to perform well when trained and tested on data from different databases. The general consensus in the literature is that this issue, known as domain adaptation and/or dataset bias, is due to a distribution mismatch between data collections. Methods addressing it go from max-margin classifiers to learning how to modify the features and obtain a more robust representation. The large majority of these works use BOW feature descriptors, and learning methods based on imageto-image distance functions. Following the seminal work of [6], in this paper we challenge these two assumptions. We experimentally show that using the NBNN classifier over existing domain adaptation databases achieves always very strong performances. We build on this result, and present an NBNN-based domain adaptation algorithm that learns iteratively a class metric while inducing, for each sample, a large margin separation among classes. To the best of our knowledge, this is the first work casting the domain adaptation problem within the NBNN framework. Experiments show that our method achieves the state of the art, both in the unsupervised and semi-supervised settings.</p><p>3 0.6979472 <a title="48-lsi-3" href="./iccv-2013-Unsupervised_Domain_Adaptation_by_Domain_Invariant_Projection.html">435 iccv-2013-Unsupervised Domain Adaptation by Domain Invariant Projection</a></p>
<p>Author: Mahsa Baktashmotlagh, Mehrtash T. Harandi, Brian C. Lovell, Mathieu Salzmann</p><p>Abstract: Domain-invariant representations are key to addressing the domain shift problem where the training and test examples follow different distributions. Existing techniques that have attempted to match the distributions of the source and target domains typically compare these distributions in the original feature space. This space, however, may not be directly suitable for such a comparison, since some of the features may have been distorted by the domain shift, or may be domain specific. In this paper, we introduce a Domain Invariant Projection approach: An unsupervised domain adaptation method that overcomes this issue by extracting the information that is invariant across the source and target domains. More specifically, we learn a projection of the data to a low-dimensional latent space where the distance between the empirical distributions of the source and target examples is minimized. We demonstrate the effectiveness of our approach on the task of visual object recognition and show that it outperforms state-of-the-art methods on a standard domain adaptation benchmark dataset.</p><p>4 0.6623224 <a title="48-lsi-4" href="./iccv-2013-Nested_Shape_Descriptors.html">288 iccv-2013-Nested Shape Descriptors</a></p>
<p>Author: Jeffrey Byrne, Jianbo Shi</p><p>Abstract: In this paper, we propose a new family of binary local feature descriptors called nested shape descriptors. These descriptors are constructed by pooling oriented gradients over a large geometric structure called the Hawaiian earring, which is constructed with a nested correlation structure that enables a new robust local distance function called the nesting distance. This distance function is unique to the nested descriptor and provides robustness to outliers from order statistics. In this paper, we define the nested shape descriptor family and introduce a specific member called the seed-of-life descriptor. We perform a trade study to determine optimal descriptor parameters for the task of image matching. Finally, we evaluate performance compared to state-of-the-art local feature descriptors on the VGGAffine image matching benchmark, showing significant performance gains. Our descriptor is thefirst binary descriptor to outperform SIFT on this benchmark.</p><p>5 0.64887977 <a title="48-lsi-5" href="./iccv-2013-Shape_Index_Descriptors_Applied_to_Texture-Based_Galaxy_Analysis.html">388 iccv-2013-Shape Index Descriptors Applied to Texture-Based Galaxy Analysis</a></p>
<p>Author: Kim Steenstrup Pedersen, Kristoffer Stensbo-Smidt, Andrew Zirm, Christian Igel</p><p>Abstract: A texture descriptor based on the shape index and the accompanying curvedness measure is proposed, and it is evaluated for the automated analysis of astronomical image data. A representative sample of images of low-redshift galaxies from the Sloan Digital Sky Survey (SDSS) serves as a testbed. The goal of applying texture descriptors to these data is to extract novel information about galaxies; information which is often lost in more traditional analysis. In this study, we build a regression model for predicting a spectroscopic quantity, the specific star-formation rate (sSFR). As texture features we consider multi-scale gradient orientation histograms as well as multi-scale shape index histograms, which lead to a new descriptor. Our results show that we can successfully predict spectroscopic quantities from the texture in optical multi-band images. We successfully recover the observed bi-modal distribution of galaxies into quiescent and star-forming. The state-ofthe-art for predicting the sSFR is a color-based physical model. We significantly improve its accuracy by augmenting the model with texture information. This study is thefirst step towards enabling the quantification of physical galaxy properties from imaging data alone.</p><p>6 0.64567429 <a title="48-lsi-6" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>7 0.64073056 <a title="48-lsi-7" href="./iccv-2013-SIFTpack%3A_A_Compact_Representation_for_Efficient_SIFT_Matching.html">365 iccv-2013-SIFTpack: A Compact Representation for Efficient SIFT Matching</a></p>
<p>8 0.63706642 <a title="48-lsi-8" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>9 0.63444763 <a title="48-lsi-9" href="./iccv-2013-Codemaps_-_Segment%2C_Classify_and_Search_Objects_Locally.html">77 iccv-2013-Codemaps - Segment, Classify and Search Objects Locally</a></p>
<p>10 0.63402641 <a title="48-lsi-10" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>11 0.63377345 <a title="48-lsi-11" href="./iccv-2013-Discovering_Details_and_Scene_Structure_with_Hierarchical_Iconoid_Shift.html">117 iccv-2013-Discovering Details and Scene Structure with Hierarchical Iconoid Shift</a></p>
<p>12 0.63176179 <a title="48-lsi-12" href="./iccv-2013-To_Aggregate_or_Not_to_aggregate%3A_Selective_Match_Kernels_for_Image_Search.html">419 iccv-2013-To Aggregate or Not to aggregate: Selective Match Kernels for Image Search</a></p>
<p>13 0.62502569 <a title="48-lsi-13" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<p>14 0.61849374 <a title="48-lsi-14" href="./iccv-2013-Unsupervised_Visual_Domain_Adaptation_Using_Subspace_Alignment.html">438 iccv-2013-Unsupervised Visual Domain Adaptation Using Subspace Alignment</a></p>
<p>15 0.61350542 <a title="48-lsi-15" href="./iccv-2013-Stacked_Predictive_Sparse_Coding_for_Classification_of_Distinct_Regions_in_Tumor_Histopathology.html">401 iccv-2013-Stacked Predictive Sparse Coding for Classification of Distinct Regions in Tumor Histopathology</a></p>
<p>16 0.60866076 <a title="48-lsi-16" href="./iccv-2013-Image_Set_Classification_Using_Holistic_Multiple_Order_Statistics_Features_and_Localized_Multi-kernel_Metric_Learning.html">212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</a></p>
<p>17 0.59708953 <a title="48-lsi-17" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>18 0.58755779 <a title="48-lsi-18" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>19 0.58751273 <a title="48-lsi-19" href="./iccv-2013-Large-Scale_Image_Annotation_by_Efficient_and_Robust_Kernel_Metric_Learning.html">227 iccv-2013-Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning</a></p>
<p>20 0.57392246 <a title="48-lsi-20" href="./iccv-2013-Heterogeneous_Auto-similarities_of_Characteristics_%28HASC%29%3A_Exploiting_Relational_Information_for_Classification.html">193 iccv-2013-Heterogeneous Auto-similarities of Characteristics (HASC): Exploiting Relational Information for Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.06), (7, 0.025), (13, 0.014), (26, 0.069), (27, 0.254), (31, 0.045), (40, 0.025), (42, 0.09), (48, 0.016), (64, 0.031), (73, 0.044), (89, 0.196), (98, 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86765027 <a title="48-lda-1" href="./iccv-2013-Non-convex_P-Norm_Projection_for_Robust_Sparsity.html">292 iccv-2013-Non-convex P-Norm Projection for Robust Sparsity</a></p>
<p>Author: Mithun Das Gupta, Sanjeev Kumar</p><p>Abstract: In this paper, we investigate the properties of Lp norm (p ≤ 1) within a projection framework. We start with the (KpK T≤ equations of the neoctni-olnin efraarm optimization problem a thnde then use its key properties to arrive at an algorithm for Lp norm projection on the non-negative simplex. We compare with L1projection which needs prior knowledge of the true norm, as well as hard thresholding based sparsificationproposed in recent compressed sensing literature. We show performance improvements compared to these techniques across different vision applications.</p><p>2 0.85399222 <a title="48-lda-2" href="./iccv-2013-Real-Time_Solution_to_the_Absolute_Pose_Problem_with_Unknown_Radial_Distortion_and_Focal_Length.html">342 iccv-2013-Real-Time Solution to the Absolute Pose Problem with Unknown Radial Distortion and Focal Length</a></p>
<p>Author: Zuzana Kukelova, Martin Bujnak, Tomas Pajdla</p><p>Abstract: Theproblem ofdetermining the absoluteposition andorientation of a camera from a set of 2D-to-3D point correspondences is one of the most important problems in computer vision with a broad range of applications. In this paper we present a new solution to the absolute pose problem for camera with unknown radial distortion and unknown focal length from five 2D-to-3D point correspondences. Our new solver is numerically more stable, more accurate, and significantly faster than the existing state-of-the-art minimal fourpoint absolutepose solvers for this problem. Moreover, our solver results in less solutions and can handle larger radial distortions. The new solver is straightforward and uses only simple concepts from linear algebra. Therefore it is simpler than the state-of-the-art Gr¨ obner basis solvers. We compare our new solver with the existing state-of-theart solvers and show its usefulness on synthetic and real datasets. 1</p><p>3 0.81509495 <a title="48-lda-3" href="./iccv-2013-Partial_Sum_Minimization_of_Singular_Values_in_RPCA_for_Low-Level_Vision.html">310 iccv-2013-Partial Sum Minimization of Singular Values in RPCA for Low-Level Vision</a></p>
<p>Author: Tae-Hyun Oh, Hyeongwoo Kim, Yu-Wing Tai, Jean-Charles Bazin, In So Kweon</p><p>Abstract: Robust Principal Component Analysis (RPCA) via rank minimization is a powerful tool for recovering underlying low-rank structure of clean data corrupted with sparse noise/outliers. In many low-level vision problems, not only it is known that the underlying structure of clean data is low-rank, but the exact rank of clean data is also known. Yet, when applying conventional rank minimization for those problems, the objective function is formulated in a way that does not fully utilize a priori target rank information about the problems. This observation motivates us to investigate whether there is a better alternative solution when using rank minimization. In this paper, instead of minimizing the nuclear norm, we propose to minimize the partial sum of singular values. The proposed objective function implicitly encourages the target rank constraint in rank minimization. Our experimental analyses show that our approach performs better than conventional rank minimization when the number of samples is deficient, while the solutions obtained by the two approaches are almost identical when the number of samples is more than sufficient. We apply our approach to various low-level vision problems, e.g. high dynamic range imaging, photometric stereo and image alignment, and show that our results outperform those obtained by the conventional nuclear norm rank minimization method.</p><p>same-paper 4 0.79544848 <a title="48-lda-4" href="./iccv-2013-An_Adaptive_Descriptor_Design_for_Object_Recognition_in_the_Wild.html">48 iccv-2013-An Adaptive Descriptor Design for Object Recognition in the Wild</a></p>
<p>Author: Zhenyu Guo, Z. Jane Wang</p><p>Abstract: Digital images nowadays show large appearance variabilities on picture styles, in terms of color tone, contrast, vignetting, and etc. These ‘picture styles’ are directly related to the scene radiance, image pipeline of the camera, and post processing functions (e.g., photography effect filters). Due to the complexity and nonlinearity of these factors, popular gradient-based image descriptors generally are not invariant to different picture styles, which could degrade the performance for object recognition. Given that images shared online or created by individual users are taken with a wide range of devices and may be processed by various post processing functions, to find a robust object recognition system is useful and challenging. In this paper, we investigate the influence of picture styles on object recognition by making a connection between image descriptors and a pixel mapping function g, and accordingly propose an adaptive approach based on a g-incorporated kernel descriptor and multiple kernel learning, without estimating or specifying the image styles used in training and testing. We conduct experiments on the Domain Adaptation data set, the Oxford Flower data set, and several variants of the Flower data set by introducing popular photography effects through post-processing. The results demonstrate that theproposedmethod consistently yields recognition improvements over standard descriptors in all studied cases.</p><p>5 0.79185379 <a title="48-lda-5" href="./iccv-2013-Semantic-Aware_Co-indexing_for_Image_Retrieval.html">378 iccv-2013-Semantic-Aware Co-indexing for Image Retrieval</a></p>
<p>Author: Shiliang Zhang, Ming Yang, Xiaoyu Wang, Yuanqing Lin, Qi Tian</p><p>Abstract: Inverted indexes in image retrieval not only allow fast access to database images but also summarize all knowledge about the database, so that their discriminative capacity largely determines the retrieval performance. In this paper, for vocabulary tree based image retrieval, we propose a semantic-aware co-indexing algorithm to jointly San Antonio, TX 78249 . j dl@gmai l com qit ian@cs .ut sa . edu . The query embed two strong cues into the inverted indexes: 1) local invariant features that are robust to delineate low-level image contents, and 2) semantic attributes from large-scale object recognition that may reveal image semantic meanings. For an initial set of inverted indexes of local features, we utilize 1000 semantic attributes to filter out isolated images and insert semantically similar images to the initial set. Encoding these two distinct cues together effectively enhances the discriminative capability of inverted indexes. Such co-indexing operations are totally off-line and introduce small computation overhead to online query cause only local features but no semantic attributes are used for query. Experiments and comparisons with recent retrieval methods on 3 datasets, i.e., UKbench, Holidays, Oxford5K, and 1.3 million images from Flickr as distractors, manifest the competitive performance of our method 1.</p><p>6 0.76745629 <a title="48-lda-6" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>7 0.71808654 <a title="48-lda-7" href="./iccv-2013-Multi-view_3D_Reconstruction_from_Uncalibrated_Radially-Symmetric_Cameras.html">280 iccv-2013-Multi-view 3D Reconstruction from Uncalibrated Radially-Symmetric Cameras</a></p>
<p>8 0.71069872 <a title="48-lda-8" href="./iccv-2013-Potts_Model%2C_Parametric_Maxflow_and_K-Submodular_Functions.html">324 iccv-2013-Potts Model, Parametric Maxflow and K-Submodular Functions</a></p>
<p>9 0.7045002 <a title="48-lda-9" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>10 0.70417243 <a title="48-lda-10" href="./iccv-2013-Unsupervised_Intrinsic_Calibration_from_a_Single_Frame_Using_a_%22Plumb-Line%22_Approach.html">436 iccv-2013-Unsupervised Intrinsic Calibration from a Single Frame Using a "Plumb-Line" Approach</a></p>
<p>11 0.70345277 <a title="48-lda-11" href="./iccv-2013-A_Robust_Analytical_Solution_to_Isometric_Shape-from-Template_with_Focal_Length_Calibration.html">27 iccv-2013-A Robust Analytical Solution to Isometric Shape-from-Template with Focal Length Calibration</a></p>
<p>12 0.70104712 <a title="48-lda-12" href="./iccv-2013-Dynamic_Probabilistic_Volumetric_Models.html">128 iccv-2013-Dynamic Probabilistic Volumetric Models</a></p>
<p>13 0.69837981 <a title="48-lda-13" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>14 0.69817746 <a title="48-lda-14" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>15 0.69739062 <a title="48-lda-15" href="./iccv-2013-Fast_Neighborhood_Graph_Search_Using_Cartesian_Concatenation.html">159 iccv-2013-Fast Neighborhood Graph Search Using Cartesian Concatenation</a></p>
<p>16 0.69650543 <a title="48-lda-16" href="./iccv-2013-Revisiting_the_PnP_Problem%3A_A_Fast%2C_General_and_Optimal_Solution.html">353 iccv-2013-Revisiting the PnP Problem: A Fast, General and Optimal Solution</a></p>
<p>17 0.69588971 <a title="48-lda-17" href="./iccv-2013-What_is_the_Most_EfficientWay_to_Select_Nearest_Neighbor_Candidates_for_Fast_Approximate_Nearest_Neighbor_Search%3F.html">450 iccv-2013-What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?</a></p>
<p>18 0.69580114 <a title="48-lda-18" href="./iccv-2013-Lifting_3D_Manhattan_Lines_from_a_Single_Image.html">250 iccv-2013-Lifting 3D Manhattan Lines from a Single Image</a></p>
<p>19 0.69405735 <a title="48-lda-19" href="./iccv-2013-Unifying_Nuclear_Norm_and_Bilinear_Factorization_Approaches_for_Low-Rank_Matrix_Decomposition.html">434 iccv-2013-Unifying Nuclear Norm and Bilinear Factorization Approaches for Low-Rank Matrix Decomposition</a></p>
<p>20 0.69296414 <a title="48-lda-20" href="./iccv-2013-An_Enhanced_Structure-from-Motion_Paradigm_Based_on_the_Absolute_Dual_Quadric_and_Images_of_Circular_Points.html">49 iccv-2013-An Enhanced Structure-from-Motion Paradigm Based on the Absolute Dual Quadric and Images of Circular Points</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
