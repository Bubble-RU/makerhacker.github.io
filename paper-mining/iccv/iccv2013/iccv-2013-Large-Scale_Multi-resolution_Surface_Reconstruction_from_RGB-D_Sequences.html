<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>228 iccv-2013-Large-Scale Multi-resolution Surface Reconstruction from RGB-D Sequences</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-228" href="#">iccv2013-228</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>228 iccv-2013-Large-Scale Multi-resolution Surface Reconstruction from RGB-D Sequences</h1>
<br/><p>Source: <a title="iccv-2013-228-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Steinbrucker_Large-Scale_Multi-resolution_Surface_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Frank Steinbrücker, Christian Kerl, Daniel Cremers</p><p>Abstract: We propose a method to generate highly detailed, textured 3D models of large environments from RGB-D sequences. Our system runs in real-time on a standard desktop PC with a state-of-the-art graphics card. To reduce the memory consumption, we fuse the acquired depth maps and colors in a multi-scale octree representation of a signed distance function. To estimate the camera poses, we construct a pose graph and use dense image alignment to determine the relative pose between pairs of frames. We add edges between nodes when we detect loop-closures and optimize the pose graph to correct for long-term drift. Our implementation is highly parallelized on graphics hardware to achieve real-time performance. More specifically, we can reconstruct, store, and continuously update a colored 3D model of an entire corridor of nine rooms at high levels of detail in real-time on a single GPU with 2.5GB.</p><p>Reference: <a title="iccv-2013-228-reference" href="../iccv2013_reference/iccv-2013-Large-Scale_Multi-resolution_Surface_Reconstruction_from_RGB-D_Sequences_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Our system runs in real-time on a standard desktop PC with a state-of-the-art graphics card. [sent-4, score-0.092]
</p><p>2 To reduce the memory consumption, we fuse the acquired depth maps and colors in a multi-scale octree representation of a signed distance function. [sent-5, score-0.876]
</p><p>3 To estimate the camera poses, we construct a pose graph and use dense image alignment to determine the relative pose between pairs of frames. [sent-6, score-0.433]
</p><p>4 We add edges between nodes when we detect loop-closures and optimize the pose graph to correct for long-term drift. [sent-7, score-0.113]
</p><p>5 Our implementation is highly parallelized on graphics hardware to achieve real-time performance. [sent-8, score-0.138]
</p><p>6 More specifically, we can reconstruct, store, and continuously update a colored 3D model of an entire corridor of nine rooms at high levels of detail in real-time on a single GPU with 2. [sent-9, score-0.265]
</p><p>7 Introduction Reconstructing the geometry and texture of the world  from a sequence of images is among the fascinating challenges in computer vision. [sent-12, score-0.201]
</p><p>8 Going beyond the classical problem known as Simultaneous Localization and Mapping (SLAM) or Structure-from-Motion (SFM), we want to estimate the camera poses, the scene geometry and the scenetexture. [sent-13, score-0.254]
</p><p>9 While impressive progress in this domain has been achieved over the last decade [2, 1, 9, 5], many of these approaches are based on visual keypoints that are reconstructed in 3D, which typically leads to sparse reconstructions in form of 3D point clouds. [sent-14, score-0.301]
</p><p>10 More recent methods based on depth images such as KinectFusion [12] aim at dense reconstruction using 3D voxel grids, which however requires a multiple in terms of memory and computational complexity. [sent-15, score-0.765]
</p><p>11 contract  Input ImageReconstructed model  Reconstructed viewOctre Structure Figure 1: Reconstruction of an office floor consisting of 9 rooms over an area of 45m× 12m× 3. [sent-17, score-0.337]
</p><p>12 rolling reconstruction volumes [14, 19] or octree data structures [7, 21]. [sent-19, score-0.501]
</p><p>13 However, in contrast to our approach, all of the above works are either not real-time [7], lack texture estimation [21], or do not support revisiting already tesselated volumes [14, 20]. [sent-20, score-0.206]
</p><p>14 Our approach integrates all of these features in a single system running at more than 15 Hz. [sent-21, score-0.039]
</p><p>15 To estimate the camera poses, classical structure-frommotion approaches match visual features across multiple images and optimize the camera poses to minimize the reprojection errors. [sent-22, score-0.477]
</p><p>16 In contrast, recently upcoming dense methods aim at aligning the full image by minimizing the photometric error over all pixels [3, 13, 15, 18], thereby exploiting the available image information better than featurebased methods. [sent-23, score-0.283]
</p><p>17 It is important to note that approaches that track the camera pose with respect to the reconstructed model (such as all existing KinectFusion-based methods [12, 21, 20]) are inherently prone to drift. [sent-24, score-0.383]
</p><p>18 In  contrast, we integrate dense image alignment in a SLAM framework to effectively reduce drift while keeping the advantages of dense image registration. [sent-25, score-0.393]
</p><p>19 a fast, sparse, multi-resolution tree structure for geometry and texture reconstruction of large-scale scenes, 2. [sent-27, score-0.256]
</p><p>20 a SLAM system based on a dense image alignment to estimate a drift-free camera trajectory, with superior performance to several state-of-the-art methods. [sent-28, score-0.285]
</p><p>21 An example of a reconstructed model of a large office floor is given in Figure 1: As it can be seen, the resulting model is globally consistent and contains fine details, while it still fits completely in the limited memory of a state-ofthe-art GPU. [sent-29, score-0.512]
</p><p>22 This paper is organized as follows: In Section 2, we describe how we achieve memory-efficient surface reconstruction using octrees on the GPU. [sent-30, score-0.468]
</p><p>23 In Section 3, we present how we extend dense tracking to compensate for drift by the detection of loop closures. [sent-31, score-0.181]
</p><p>24 Multi-Resolution Surface Reconstruction In this section, we describe our approach to memory-  efficient surface reconstruction. [sent-34, score-0.259]
</p><p>25 First, we provide a brief introduction to implicit surface representations based on signed distance functions. [sent-35, score-0.695]
</p><p>26 Subsequently, we replace the regular grid by an adaptive octree and a narrow-band technique to significantly reduce the memory consumption. [sent-36, score-0.358]
</p><p>27 Signed Distance Volume Following the works of [4, 12], we store our surface implicitly as a signed distance function in a 3D volume. [sent-39, score-0.769]
</p><p>28 The volume is approximated by a finite number of voxels. [sent-40, score-0.094]
</p><p>29 At every point in the volume the function indicates how far away the closest surface is. [sent-41, score-0.353]
</p><p>30 Points in front of an object have a negative sign and points inside a positive one. [sent-42, score-0.063]
</p><p>31 The zero crossing indicates the location of the surface. [sent-43, score-0.054]
</p><p>32 When considering a geometry that is updated and changed over time, the signed distance representation has the benefit of being able to handle arbitrary changes in the surface topology, in constrast to e. [sent-44, score-0.85]
</p><p>33 The signed distance function is incrementally constructed from a sequence of RGB-D images and associated camera poses. [sent-47, score-0.605]
</p><p>34 Given an RGB-D image at time t with a color valued image Ict and depth map Zt, the camera pose Tt, and tvhaelu uiendtri inmsaicg camera parameters, we integrate it into the volume using the following procedure. [sent-48, score-0.695]
</p><p>35 For every voxel in the volume, we compute its center point in the camera frame pc pc = Tt p. [sent-49, score-0.926]
</p><p>36 (1)  Afterwards, we determine the pixel location x of the voxel center pc in the depth map Zt using the projection function of the stanidnar thde pinhole camera model, i. [sent-50, score-0.928]
</p><p>37 The measured point is reconstructed using the inverse projection function π−1 (x, Z) : pobs  = π−1 (x, Zt (x)) . [sent-53, score-0.321]
</p><p>38 (2)  The value of the signed distance function at the voxel center is determind by ΔD = max{min{Φ, |pc | − |pobs | }, −Φ}. [sent-54, score-0.831]
</p><p>39 (3)  The choice of the truncation threshold Φ depends on the expected uncertainty and noise of the depth sensor. [sent-55, score-0.138]
</p><p>40 The lower Φ is, the more fine scale structures and concavities of the surface can be reconstructed. [sent-56, score-0.389]
</p><p>41 If Φ is too low however, objects might appear several times in the reconstruction due to sensor noise or pose estimation errors. [sent-57, score-0.274]
</p><p>42 We chose it to be twice the voxel scale of the grid resolution for the experiments in this paper. [sent-58, score-0.433]
</p><p>43 Furthermore, we compute a weight for the measurement ΔD using the weight function w(ΔD), that expresses our confidence in the distance observation according to our sensor model. [sent-59, score-0.339]
</p><p>44 Figure 2 visualizes the truncated signed distance and the weight function used in our experiments. [sent-60, score-0.593]
</p><p>45 The distance D(p, t) and the weight W(p, t) stored in the voxel are updated using the following equations:  W(p, t) = w(ΔD) + W(p, t − 1),  D(p,t) =D(p,t −w( 1Δ)WD)(p +,t W −(1p ), +t − Δ 1D)w(ΔD). [sent-61, score-0.554]
</p><p>46 (4) (5)  Similar to the distance update we compute the new voxel color C(p, t) as  C(p,t) =C(p,t −w 1()ΔWD()p ,+tW − ( 1p) +,t I −tc 1(x))w(ΔD). [sent-62, score-0.437]
</p><p>47 (6)  We use a weighting function that assigns a weight of 1 to all pixels in front of the observed surface, and a linearly decreasing weight behind the surface (cf. [sent-63, score-0.464]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('voxel', 0.353), ('signed', 0.352), ('surface', 0.259), ('octree', 0.21), ('pc', 0.201), ('pobs', 0.181), ('slam', 0.179), ('zt', 0.157), ('kerl', 0.148), ('reconstructed', 0.14), ('reconstruction', 0.135), ('camera', 0.129), ('rooms', 0.113), ('memory', 0.106), ('wd', 0.101), ('cremers', 0.095), ('volume', 0.094), ('volumes', 0.094), ('drift', 0.092), ('dense', 0.089), ('office', 0.088), ('floor', 0.084), ('distance', 0.084), ('depth', 0.082), ('tvhaelu', 0.08), ('ddd', 0.08), ('fascinating', 0.08), ('rgen', 0.08), ('upcoming', 0.08), ('poses', 0.075), ('featurebased', 0.074), ('octrees', 0.074), ('concavities', 0.074), ('store', 0.074), ('pose', 0.074), ('weight', 0.071), ('corridor', 0.07), ('cker', 0.07), ('steinbr', 0.07), ('geometry', 0.069), ('alignment', 0.067), ('dfg', 0.067), ('thde', 0.067), ('sensor', 0.065), ('munich', 0.064), ('front', 0.063), ('kinectfusion', 0.062), ('mod', 0.062), ('rolling', 0.062), ('revisiting', 0.06), ('frank', 0.06), ('consumption', 0.058), ('christian', 0.058), ('fine', 0.056), ('integrate', 0.056), ('truncation', 0.056), ('classical', 0.056), ('tt', 0.055), ('demand', 0.055), ('daniel', 0.054), ('pinhole', 0.054), ('crossing', 0.054), ('contract', 0.052), ('ict', 0.052), ('texture', 0.052), ('valued', 0.051), ('sturm', 0.05), ('reprojection', 0.049), ('expresses', 0.048), ('afterwards', 0.047), ('graphics', 0.046), ('updated', 0.046), ('hardware', 0.046), ('parallelized', 0.046), ('desktop', 0.046), ('sfm', 0.045), ('visualizes', 0.043), ('truncated', 0.043), ('topology', 0.042), ('decade', 0.042), ('center', 0.042), ('grid', 0.042), ('fuse', 0.042), ('grids', 0.042), ('continuously', 0.041), ('tc', 0.041), ('keypoints', 0.041), ('reconstructions', 0.041), ('nine', 0.041), ('photometric', 0.04), ('gpu', 0.04), ('prone', 0.04), ('incrementally', 0.04), ('changed', 0.04), ('optimize', 0.039), ('integrates', 0.039), ('fits', 0.038), ('going', 0.038), ('twice', 0.038), ('impressive', 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="228-tfidf-1" href="./iccv-2013-Large-Scale_Multi-resolution_Surface_Reconstruction_from_RGB-D_Sequences.html">228 iccv-2013-Large-Scale Multi-resolution Surface Reconstruction from RGB-D Sequences</a></p>
<p>Author: Frank Steinbrücker, Christian Kerl, Daniel Cremers</p><p>Abstract: We propose a method to generate highly detailed, textured 3D models of large environments from RGB-D sequences. Our system runs in real-time on a standard desktop PC with a state-of-the-art graphics card. To reduce the memory consumption, we fuse the acquired depth maps and colors in a multi-scale octree representation of a signed distance function. To estimate the camera poses, we construct a pose graph and use dense image alignment to determine the relative pose between pairs of frames. We add edges between nodes when we detect loop-closures and optimize the pose graph to correct for long-term drift. Our implementation is highly parallelized on graphics hardware to achieve real-time performance. More specifically, we can reconstruct, store, and continuously update a colored 3D model of an entire corridor of nine rooms at high levels of detail in real-time on a single GPU with 2.5GB.</p><p>2 0.28513855 <a title="228-tfidf-2" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>Author: Carl Yuheng Ren, Victor Prisacariu, David Murray, Ian Reid</p><p>Abstract: We introduce a probabilistic framework for simultaneous tracking and reconstruction of 3D rigid objects using an RGB-D camera. The tracking problem is handled using a bag-of-pixels representation and a back-projection scheme. Surface and background appearance models are learned online, leading to robust tracking in the presence of heavy occlusion and outliers. In both our tracking and reconstruction modules, the 3D object is implicitly embedded using a 3D level-set function. The framework is initialized with a simple shape primitive model (e.g. a sphere or a cube), and the real 3D object shape is tracked and reconstructed online. Unlike existing depth-based 3D reconstruction works, which either rely on calibrated/fixed camera set up or use the observed world map to track the depth camera, our framework can simultaneously track and reconstruct small moving objects. We use both qualitative and quantitative results to demonstrate the superior performance of both tracking and reconstruction of our method.</p><p>3 0.21669169 <a title="228-tfidf-3" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>Author: Michael Weinmann, Aljosa Osep, Roland Ruiters, Reinhard Klein</p><p>Abstract: In this paper, we present a novel, robust multi-view normal field integration technique for reconstructing the full 3D shape of mirroring objects. We employ a turntablebased setup with several cameras and displays. These are used to display illumination patterns which are reflected by the object surface. The pattern information observed in the cameras enables the calculation of individual volumetric normal fields for each combination of camera, display and turntable angle. As the pattern information might be blurred depending on the surface curvature or due to nonperfect mirroring surface characteristics, we locally adapt the decoding to the finest still resolvable pattern resolution. In complex real-world scenarios, the normal fields contain regions without observations due to occlusions and outliers due to interreflections and noise. Therefore, a robust reconstruction using only normal information is challenging. Via a non-parametric clustering of normal hypotheses derived for each point in the scene, we obtain both the most likely local surface normal and a local surface consistency estimate. This information is utilized in an iterative mincut based variational approach to reconstruct the surface geometry.</p><p>4 0.1977405 <a title="228-tfidf-4" href="./iccv-2013-3D_Scene_Understanding_by_Voxel-CRF.html">2 iccv-2013-3D Scene Understanding by Voxel-CRF</a></p>
<p>Author: Byung-Soo Kim, Pushmeet Kohli, Silvio Savarese</p><p>Abstract: Scene understanding is an important yet very challenging problem in computer vision. In the past few years, researchers have taken advantage of the recent diffusion of depth-RGB (RGB-D) cameras to help simplify the problem of inferring scene semantics. However, while the added 3D geometry is certainly useful to segment out objects with different depth values, it also adds complications in that the 3D geometry is often incorrect because of noisy depth measurements and the actual 3D extent of the objects is usually unknown because of occlusions. In this paper we propose a new method that allows us to jointly refine the 3D reconstruction of the scene (raw depth values) while accurately segmenting out the objects or scene elements from the 3D reconstruction. This is achieved by introducing a new model which we called Voxel-CRF. The Voxel-CRF model is based on the idea of constructing a conditional random field over a 3D volume of interest which captures the semantic and 3D geometric relationships among different elements (voxels) of the scene. Such model allows to jointly estimate (1) a dense voxel-based 3D reconstruction and (2) the semantic labels associated with each voxel even in presence of par- tial occlusions using an approximate yet efficient inference strategy. We evaluated our method on the challenging NYU Depth dataset (Version 1and 2). Experimental results show that our method achieves competitive accuracy in inferring scene semantics and visually appealing results in improving the quality of the 3D reconstruction. We also demonstrate an interesting application of object removal and scene completion from RGB-D images.</p><p>5 0.17452133 <a title="228-tfidf-5" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>Author: Benjamin Ummenhofer, Thomas Brox</p><p>Abstract: 3D reconstruction deals with the problem of finding the shape of an object from a set of images. Thin objects that have virtually no volumepose a special challengefor reconstruction with respect to shape representation and fusion of depth information. In this paper we present a dense pointbased reconstruction method that can deal with this special class of objects. We seek to jointly optimize a set of depth maps by treating each pixel as a point in space. Points are pulled towards a common surface by pairwise forces in an iterative scheme. The method also handles the problem of opposed surfaces by means of penalty forces. Efficient optimization is achieved by grouping points to superpixels and a spatial hashing approach for fast neighborhood queries. We show that the approach is on a par with state-of-the-art methods for standard multi view stereo settings and gives superior results for thin objects.</p><p>6 0.15532285 <a title="228-tfidf-6" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<p>7 0.15078416 <a title="228-tfidf-7" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>8 0.14202765 <a title="228-tfidf-8" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>9 0.14098005 <a title="228-tfidf-9" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>10 0.13574 <a title="228-tfidf-10" href="./iccv-2013-Dynamic_Probabilistic_Volumetric_Models.html">128 iccv-2013-Dynamic Probabilistic Volumetric Models</a></p>
<p>11 0.1148733 <a title="228-tfidf-11" href="./iccv-2013-Volumetric_Semantic_Segmentation_Using_Pyramid_Context_Features.html">447 iccv-2013-Volumetric Semantic Segmentation Using Pyramid Context Features</a></p>
<p>12 0.10652144 <a title="228-tfidf-12" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>13 0.10606496 <a title="228-tfidf-13" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>14 0.10061514 <a title="228-tfidf-14" href="./iccv-2013-Shape_Anchors_for_Data-Driven_Multi-view_Reconstruction.html">387 iccv-2013-Shape Anchors for Data-Driven Multi-view Reconstruction</a></p>
<p>15 0.094356336 <a title="228-tfidf-15" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<p>16 0.091254026 <a title="228-tfidf-16" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>17 0.090638638 <a title="228-tfidf-17" href="./iccv-2013-Image_Guided_Depth_Upsampling_Using_Anisotropic_Total_Generalized_Variation.html">209 iccv-2013-Image Guided Depth Upsampling Using Anisotropic Total Generalized Variation</a></p>
<p>18 0.089373901 <a title="228-tfidf-18" href="./iccv-2013-Live_Metric_3D_Reconstruction_on_Mobile_Phones.html">254 iccv-2013-Live Metric 3D Reconstruction on Mobile Phones</a></p>
<p>19 0.08926741 <a title="228-tfidf-19" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>20 0.088147148 <a title="228-tfidf-20" href="./iccv-2013-A_Global_Linear_Method_for_Camera_Pose_Registration.html">17 iccv-2013-A Global Linear Method for Camera Pose Registration</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.185), (1, -0.203), (2, -0.04), (3, 0.039), (4, 0.008), (5, -0.027), (6, 0.018), (7, -0.146), (8, -0.052), (9, 0.039), (10, 0.005), (11, 0.013), (12, -0.103), (13, 0.057), (14, 0.016), (15, -0.059), (16, 0.012), (17, -0.005), (18, -0.029), (19, -0.034), (20, -0.055), (21, 0.011), (22, 0.088), (23, 0.025), (24, -0.076), (25, -0.004), (26, -0.06), (27, 0.095), (28, 0.074), (29, 0.076), (30, 0.054), (31, -0.095), (32, 0.035), (33, -0.085), (34, -0.09), (35, 0.037), (36, 0.065), (37, 0.068), (38, 0.118), (39, 0.061), (40, 0.025), (41, -0.118), (42, -0.121), (43, 0.067), (44, -0.012), (45, 0.002), (46, -0.058), (47, -0.039), (48, -0.02), (49, -0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95280665 <a title="228-lsi-1" href="./iccv-2013-Large-Scale_Multi-resolution_Surface_Reconstruction_from_RGB-D_Sequences.html">228 iccv-2013-Large-Scale Multi-resolution Surface Reconstruction from RGB-D Sequences</a></p>
<p>Author: Frank Steinbrücker, Christian Kerl, Daniel Cremers</p><p>Abstract: We propose a method to generate highly detailed, textured 3D models of large environments from RGB-D sequences. Our system runs in real-time on a standard desktop PC with a state-of-the-art graphics card. To reduce the memory consumption, we fuse the acquired depth maps and colors in a multi-scale octree representation of a signed distance function. To estimate the camera poses, we construct a pose graph and use dense image alignment to determine the relative pose between pairs of frames. We add edges between nodes when we detect loop-closures and optimize the pose graph to correct for long-term drift. Our implementation is highly parallelized on graphics hardware to achieve real-time performance. More specifically, we can reconstruct, store, and continuously update a colored 3D model of an entire corridor of nine rooms at high levels of detail in real-time on a single GPU with 2.5GB.</p><p>2 0.78884441 <a title="228-lsi-2" href="./iccv-2013-Elastic_Fragments_for_Dense_Scene_Reconstruction.html">139 iccv-2013-Elastic Fragments for Dense Scene Reconstruction</a></p>
<p>Author: Qian-Yi Zhou, Stephen Miller, Vladlen Koltun</p><p>Abstract: We present an approach to reconstruction of detailed scene geometry from range video. Range data produced by commodity handheld cameras suffers from high-frequency errors and low-frequency distortion. Our approach deals with both sources of error by reconstructing locally smooth scene fragments and letting these fragments deform in order to align to each other. We develop a volumetric registration formulation that leverages the smoothness of the deformation to make optimization practical for large scenes. Experimental results demonstrate that our approach substantially increases the fidelity of complex scene geometry reconstructed with commodity handheld cameras.</p><p>3 0.7740618 <a title="228-lsi-3" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>Author: Diego Thomas, Akihiro Sugimoto</p><p>Abstract: Updating a global 3D model with live RGB-D measurements has proven to be successful for 3D reconstruction of indoor scenes. Recently, a Truncated Signed Distance Function (TSDF) volumetric model and a fusion algorithm have been introduced (KinectFusion), showing significant advantages such as computational speed and accuracy of the reconstructed scene. This algorithm, however, is expensive in memory when constructing and updating the global model. As a consequence, the method is not well scalable to large scenes. We propose a new flexible 3D scene representation using a set of planes that is cheap in memory use and, nevertheless, achieves accurate reconstruction of indoor scenes from RGB-D image sequences. Projecting the scene onto different planes reduces significantly the size of the scene representation and thus it allows us to generate a global textured 3D model with lower memory requirement while keeping accuracy and easiness to update with live RGB-D measurements. Experimental results demonstrate that our proposed flexible 3D scene representation achieves accurate reconstruction, while keeping the scalability for large indoor scenes.</p><p>4 0.76832002 <a title="228-lsi-4" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>Author: Benjamin Ummenhofer, Thomas Brox</p><p>Abstract: 3D reconstruction deals with the problem of finding the shape of an object from a set of images. Thin objects that have virtually no volumepose a special challengefor reconstruction with respect to shape representation and fusion of depth information. In this paper we present a dense pointbased reconstruction method that can deal with this special class of objects. We seek to jointly optimize a set of depth maps by treating each pixel as a point in space. Points are pulled towards a common surface by pairwise forces in an iterative scheme. The method also handles the problem of opposed surfaces by means of penalty forces. Efficient optimization is achieved by grouping points to superpixels and a spatial hashing approach for fast neighborhood queries. We show that the approach is on a par with state-of-the-art methods for standard multi view stereo settings and gives superior results for thin objects.</p><p>5 0.72134495 <a title="228-lsi-5" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<p>Author: Carl Yuheng Ren, Victor Prisacariu, David Murray, Ian Reid</p><p>Abstract: We introduce a probabilistic framework for simultaneous tracking and reconstruction of 3D rigid objects using an RGB-D camera. The tracking problem is handled using a bag-of-pixels representation and a back-projection scheme. Surface and background appearance models are learned online, leading to robust tracking in the presence of heavy occlusion and outliers. In both our tracking and reconstruction modules, the 3D object is implicitly embedded using a 3D level-set function. The framework is initialized with a simple shape primitive model (e.g. a sphere or a cube), and the real 3D object shape is tracked and reconstructed online. Unlike existing depth-based 3D reconstruction works, which either rely on calibrated/fixed camera set up or use the observed world map to track the depth camera, our framework can simultaneously track and reconstruct small moving objects. We use both qualitative and quantitative results to demonstrate the superior performance of both tracking and reconstruction of our method.</p><p>6 0.68427044 <a title="228-lsi-6" href="./iccv-2013-3D_Scene_Understanding_by_Voxel-CRF.html">2 iccv-2013-3D Scene Understanding by Voxel-CRF</a></p>
<p>7 0.6625756 <a title="228-lsi-7" href="./iccv-2013-Multi-view_Normal_Field_Integration_for_3D_Reconstruction_of_Mirroring_Objects.html">281 iccv-2013-Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects</a></p>
<p>8 0.63144726 <a title="228-lsi-8" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>9 0.62175161 <a title="228-lsi-9" href="./iccv-2013-Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization.html">284 iccv-2013-Multiview Photometric Stereo Using Planar Mesh Parameterization</a></p>
<p>10 0.60832155 <a title="228-lsi-10" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>11 0.57560927 <a title="228-lsi-11" href="./iccv-2013-Live_Metric_3D_Reconstruction_on_Mobile_Phones.html">254 iccv-2013-Live Metric 3D Reconstruction on Mobile Phones</a></p>
<p>12 0.57491273 <a title="228-lsi-12" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>13 0.56481916 <a title="228-lsi-13" href="./iccv-2013-Dynamic_Probabilistic_Volumetric_Models.html">128 iccv-2013-Dynamic Probabilistic Volumetric Models</a></p>
<p>14 0.56239748 <a title="228-lsi-14" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>15 0.52634317 <a title="228-lsi-15" href="./iccv-2013-Shape_Anchors_for_Data-Driven_Multi-view_Reconstruction.html">387 iccv-2013-Shape Anchors for Data-Driven Multi-view Reconstruction</a></p>
<p>16 0.51022804 <a title="228-lsi-16" href="./iccv-2013-Incorporating_Cloud_Distribution_in_Sky_Representation.html">215 iccv-2013-Incorporating Cloud Distribution in Sky Representation</a></p>
<p>17 0.5020774 <a title="228-lsi-17" href="./iccv-2013-Multi-view_3D_Reconstruction_from_Uncalibrated_Radially-Symmetric_Cameras.html">280 iccv-2013-Multi-view 3D Reconstruction from Uncalibrated Radially-Symmetric Cameras</a></p>
<p>18 0.49741942 <a title="228-lsi-18" href="./iccv-2013-Refractive_Structure-from-Motion_on_Underwater_Images.html">348 iccv-2013-Refractive Structure-from-Motion on Underwater Images</a></p>
<p>19 0.49595258 <a title="228-lsi-19" href="./iccv-2013-Volumetric_Semantic_Segmentation_Using_Pyramid_Context_Features.html">447 iccv-2013-Volumetric Semantic Segmentation Using Pyramid Context Features</a></p>
<p>20 0.49344867 <a title="228-lsi-20" href="./iccv-2013-Data-Driven_3D_Primitives_for_Single_Image_Understanding.html">102 iccv-2013-Data-Driven 3D Primitives for Single Image Understanding</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.059), (26, 0.039), (31, 0.04), (37, 0.232), (42, 0.109), (64, 0.043), (73, 0.033), (89, 0.328), (98, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92963111 <a title="228-lda-1" href="./iccv-2013-On_the_Mean_Curvature_Flow_on_Graphs_with_Applications_in_Image_and_Manifold_Processing.html">296 iccv-2013-On the Mean Curvature Flow on Graphs with Applications in Image and Manifold Processing</a></p>
<p>Author: Abdallah El_Chakik, Abderrahim Elmoataz, Ahcene Sadi</p><p>Abstract: In this paper, we propose an adaptation and transcription of the mean curvature level set equation on a general discrete domain (weighted graphs with arbitrary topology). We introduce the perimeters on graph using difference operators and define the curvature as the first variation of these perimeters. Our proposed approach of mean curvature unifies both local and non local notions of mean curvature on Euclidean domains. Furthermore, it allows the extension to the processing of manifolds and data which can be represented by graphs.</p><p>2 0.89563376 <a title="228-lda-2" href="./iccv-2013-A_Rotational_Stereo_Model_Based_on_XSlit_Imaging.html">28 iccv-2013-A Rotational Stereo Model Based on XSlit Imaging</a></p>
<p>Author: Jinwei Ye, Yu Ji, Jingyi Yu</p><p>Abstract: Traditional stereo matching assumes perspective viewing cameras under a translational motion: the second camera is translated away from the first one to create parallax. In this paper, we investigate a different, rotational stereo model on a special multi-perspective camera, the XSlit camera [9, 24]. We show that rotational XSlit (R-XSlit) stereo can be effectively created by fixing the sensor and slit locations but switching the two slits’ directions. We first derive the epipolar geometry of R-XSlit in the 4D light field ray space. Our derivation leads to a simple but effective scheme for locating corresponding epipolar “curves ”. To conduct stereo matching, we further derive a new disparity term in our model and develop a patch-based graph-cut solution. To validate our theory, we assemble an XSlit lens by using a pair of cylindrical lenses coupled with slit-shaped apertures. The XSlit lens can be mounted on commodity cameras where the slit directions are adjustable to form desirable R-XSlit pairs. We show through experiments that R-XSlitprovides apotentially advantageous imaging system for conducting fixed-location, dynamic baseline stereo.</p><p>same-paper 3 0.89561129 <a title="228-lda-3" href="./iccv-2013-Large-Scale_Multi-resolution_Surface_Reconstruction_from_RGB-D_Sequences.html">228 iccv-2013-Large-Scale Multi-resolution Surface Reconstruction from RGB-D Sequences</a></p>
<p>Author: Frank Steinbrücker, Christian Kerl, Daniel Cremers</p><p>Abstract: We propose a method to generate highly detailed, textured 3D models of large environments from RGB-D sequences. Our system runs in real-time on a standard desktop PC with a state-of-the-art graphics card. To reduce the memory consumption, we fuse the acquired depth maps and colors in a multi-scale octree representation of a signed distance function. To estimate the camera poses, we construct a pose graph and use dense image alignment to determine the relative pose between pairs of frames. We add edges between nodes when we detect loop-closures and optimize the pose graph to correct for long-term drift. Our implementation is highly parallelized on graphics hardware to achieve real-time performance. More specifically, we can reconstruct, store, and continuously update a colored 3D model of an entire corridor of nine rooms at high levels of detail in real-time on a single GPU with 2.5GB.</p><p>4 0.88082689 <a title="228-lda-4" href="./iccv-2013-Detecting_Curved_Symmetric_Parts_Using_a_Deformable_Disc_Model.html">110 iccv-2013-Detecting Curved Symmetric Parts Using a Deformable Disc Model</a></p>
<p>Author: Tom Sie Ho Lee, Sanja Fidler, Sven Dickinson</p><p>Abstract: Symmetry is a powerful shape regularity that’s been exploited by perceptual grouping researchers in both human and computer vision to recover part structure from an image without a priori knowledge of scene content. Drawing on the concept of a medial axis, defined as the locus of centers of maximal inscribed discs that sweep out a symmetric part, we model part recovery as the search for a sequence of deformable maximal inscribed disc hypotheses generated from a multiscale superpixel segmentation, a framework proposed by [13]. However, we learn affinities between adjacent superpixels in a space that’s invariant to bending and tapering along the symmetry axis, enabling us to capture a wider class of symmetric parts. Moreover, we introduce a global cost that perceptually integrates the hypothesis space by combining a pairwise and a higher-level smoothing term, which we minimize globally using dynamic programming. The new framework is demonstrated on two datasets, and is shown to significantly outperform the baseline [13].</p><p>5 0.83439994 <a title="228-lda-5" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>Author: Tae Hyun Kim, Byeongjoo Ahn, Kyoung Mu Lee</p><p>Abstract: Most conventional single image deblurring methods assume that the underlying scene is static and the blur is caused by only camera shake. In this paper, in contrast to this restrictive assumption, we address the deblurring problem of general dynamic scenes which contain multiple moving objects as well as camera shake. In case of dynamic scenes, moving objects and background have different blur motions, so the segmentation of the motion blur is required for deblurring each distinct blur motion accurately. Thus, we propose a novel energy model designed with the weighted sum of multiple blur data models, which estimates different motion blurs and their associated pixelwise weights, and resulting sharp image. In this framework, the local weights are determined adaptively and get high values when the corresponding data models have high data fidelity. And, the weight information is used for the segmentation of the motion blur. Non-local regularization of weights are also incorporated to produce more reliable segmentation results. A convex optimization-based method is used for the solution of the proposed energy model. Exper- imental results demonstrate that our method outperforms conventional approaches in deblurring both dynamic scenes and static scenes.</p><p>6 0.83380079 <a title="228-lda-6" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>7 0.8330577 <a title="228-lda-7" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<p>8 0.83286929 <a title="228-lda-8" href="./iccv-2013-3D_Scene_Understanding_by_Voxel-CRF.html">2 iccv-2013-3D Scene Understanding by Voxel-CRF</a></p>
<p>9 0.83270168 <a title="228-lda-9" href="./iccv-2013-Locally_Affine_Sparse-to-Dense_Matching_for_Motion_and_Occlusion_Estimation.html">256 iccv-2013-Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation</a></p>
<p>10 0.83262497 <a title="228-lda-10" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<p>11 0.83117372 <a title="228-lda-11" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>12 0.83090711 <a title="228-lda-12" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>13 0.83069682 <a title="228-lda-13" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>14 0.83061129 <a title="228-lda-14" href="./iccv-2013-EVSAC%3A_Accelerating_Hypotheses_Generation_by_Modeling_Matching_Scores_with_Extreme_Value_Theory.html">131 iccv-2013-EVSAC: Accelerating Hypotheses Generation by Modeling Matching Scores with Extreme Value Theory</a></p>
<p>15 0.82973731 <a title="228-lda-15" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>16 0.82858348 <a title="228-lda-16" href="./iccv-2013-Shufflets%3A_Shared_Mid-level_Parts_for_Fast_Object_Detection.html">390 iccv-2013-Shufflets: Shared Mid-level Parts for Fast Object Detection</a></p>
<p>17 0.82839 <a title="228-lda-17" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>18 0.82816869 <a title="228-lda-18" href="./iccv-2013-DeepFlow%3A_Large_Displacement_Optical_Flow_with_Deep_Matching.html">105 iccv-2013-DeepFlow: Large Displacement Optical Flow with Deep Matching</a></p>
<p>19 0.82811934 <a title="228-lda-19" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>20 0.82805586 <a title="228-lda-20" href="./iccv-2013-STAR3D%3A_Simultaneous_Tracking_and_Reconstruction_of_3D_Objects_Using_RGB-D_Data.html">366 iccv-2013-STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
