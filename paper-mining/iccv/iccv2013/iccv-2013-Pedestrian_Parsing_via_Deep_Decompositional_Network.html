<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>311 iccv-2013-Pedestrian Parsing via Deep Decompositional Network</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-311" href="#">iccv2013-311</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>311 iccv-2013-Pedestrian Parsing via Deep Decompositional Network</h1>
<br/><p>Source: <a title="iccv-2013-311-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Luo_Pedestrian_Parsing_via_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Ping Luo, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: We propose a new Deep Decompositional Network (DDN) for parsing pedestrian images into semantic regions, such as hair, head, body, arms, and legs, where the pedestrians can be heavily occluded. Unlike existing methods based on template matching or Bayesian inference, our approach directly maps low-level visual features to the label maps of body parts with DDN, which is able to accurately estimate complex pose variations with good robustness to occlusions and background clutters. DDN jointly estimates occluded regions and segments body parts by stacking three types of hidden layers: occlusion estimation layers, completion layers, and decomposition layers. The occlusion estimation layers estimate a binary mask, indicating which part of a pedestrian is invisible. The completion layers synthesize low-level features of the invisible part from the original features and the occlusion mask. The decomposition layers directly transform the synthesized visual features to label maps. We devise a new strategy to pre-train these hidden layers, and then fine-tune the entire network using the stochastic gradient descent. Experimental results show that our approach achieves better segmentation accuracy than the state-of-the-art methods on pedestrian images with or without occlusions. Another important contribution of this paper is that it provides a large scale benchmark human parsing dataset1 that includes 3, 673 annotated samples collected from 171 surveillance videos. It is 20 times larger than existing public datasets.</p><p>Reference: <a title="iccv-2013-311-reference" href="../iccv2013_reference/iccv-2013-Pedestrian_Parsing_via_Deep_Decompositional_Network_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 hk  Abstract We propose a new Deep Decompositional Network (DDN) for parsing pedestrian images into semantic regions, such as hair, head, body, arms, and legs, where the pedestrians can be heavily occluded. [sent-9, score-0.311]
</p><p>2 Unlike existing methods based on template matching or Bayesian inference, our approach directly maps low-level visual features to the label maps of body parts with DDN, which is able to accurately estimate complex pose variations with good robustness to occlusions and background clutters. [sent-10, score-0.292]
</p><p>3 DDN jointly estimates occluded regions and segments body parts by stacking three types of hidden layers: occlusion estimation layers, completion layers, and decomposition layers. [sent-11, score-0.56]
</p><p>4 The occlusion estimation layers estimate a binary mask, indicating which part of a pedestrian is invisible. [sent-12, score-0.593]
</p><p>5 The completion layers synthesize low-level features of the invisible part from the original features and the occlusion mask. [sent-13, score-0.606]
</p><p>6 The decomposition layers directly transform the synthesized visual features to label maps. [sent-14, score-0.39]
</p><p>7 We devise a new strategy to pre-train these hidden layers, and then fine-tune the entire network using the  stochastic gradient descent. [sent-15, score-0.214]
</p><p>8 Experimental results show that our approach achieves better segmentation accuracy than the state-of-the-art methods on pedestrian images with or without occlusions. [sent-16, score-0.164]
</p><p>9 Another important contribution of this paper is that it provides a large scale benchmark human parsing dataset1 that includes 3, 673 annotated samples collected from 171 surveillance videos. [sent-17, score-0.168]
</p><p>10 Introduction Pedestrian analysis is an important topic in computer vision, including pedestrian detection, pose estimation, ∗This work is supported by the General Research Fund sponsored by the Research Grants Council of Hong Kong (Project No. [sent-20, score-0.164]
</p><p>11 This paper focuses on parsing a pedestrian figure into different semantic parts, such as hair, head, body, arms, and legs. [sent-33, score-0.264]
</p><p>12 Existing studies of pedestrian parsing [2, 1, 6, 20] generally fall into two categories: template matching and Bayesian inference. [sent-37, score-0.285]
</p><p>13 The pixel-level segmentation of body parts was first proposed in [2], which searches for templates of body parts (poselets) in the training set by incorporating the 3D skeletons of humans. [sent-38, score-0.22]
</p><p>14 The identified templates are directly used as segmentation results and cannot accurately fit body boundaries of pedestrians in tests. [sent-39, score-0.184]
</p><p>15 [1] (SBP) provided the ground truth annotations of the PennFudan pedestrian database [27] and used it to evaluate the 2648  segmentation accuracy of their algorithm. [sent-42, score-0.164]
</p><p>16 Their method segments an image into superpixels and then merges the superpixels into candidate body parts by comparing their shapes and positions with templates in the training set. [sent-43, score-0.115]
</p><p>17 [6] (PbOS) treated human parsing as a Bayesian inference problem. [sent-49, score-0.144]
</p><p>18 They model the appearance prior as Gaussian mixture of pixel colors, and the body shape prior is modeled by the pose skeleton in [20] and the multinomial shape boltzmann machine in [6]. [sent-51, score-0.224]
</p><p>19 This paper addresses the aforementioned limitations by proposing a new deep model, the Deep Decompositional Network (DDN), which utilizes HOG features [3] as input and outputs the segmentation label maps. [sent-59, score-0.24]
</p><p>20 HOG features can effectively characterize the boundaries of body parts and estimate human poses. [sent-60, score-0.105]
</p><p>21 In order to explicitly handle the occlusion problem, DDN stacks three types of hidden layers, including occlusion estimation layers, completion layers, and decomposition layers (see Fig. [sent-61, score-0.862]
</p><p>22 Specifically, the occlusion estimation layers infer a binary mask, indicating which part of the features is occluded. [sent-63, score-0.451]
</p><p>23 Finally, the decomposition layers decompose the synthesized features to the label maps by learning a mapping (transformation) from the feature space to the space of label maps (see an example in Fig. [sent-65, score-0.493]
</p><p>24 Unlike CNN [11], whose weights are shared and locally connected, we find fully connecting adjacent layers in DDN can capture the global structures of humans and can improve the parsing results. [sent-67, score-0.387]
</p><p>25 At the training stage, we devise a new strategy based on least squares dictionary learning to pre-train the occlusion estimation layers and the decomposition layers, while the completion layers are pre-trained with a modified denoising autoencoder [26]. [sent-68, score-1.089]
</p><p>26 The entire network is then fine-tuned by the stochastic gradient descent. [sent-69, score-0.16]
</p><p>27 At the testing stage, our network can efficiently transform an image into label maps without template matching or MCMC sampling. [sent-70, score-0.259]
</p><p>28 (1) This is the first time that deep learning is studied specifically for pedestrian parsing. [sent-72, score-0.321]
</p><p>29 We propose a novel deep network, where the models for occlusion estimation, data completion, and data transformation are incorporated into a unified deep architecture and jointly trained. [sent-74, score-0.624]
</p><p>30 (4) We provide a largescale benchmark human parsing dataset (refer to footnote 1) which includes 3, 673 annotated samples collected from 171 surveillance videos, making it 20 times larger than existing public datasets. [sent-77, score-0.168]
</p><p>31 Related Work  We review some related works on occlusion estimation [28, 4, 7, 24, 17], data completion [5, 21, 8], and crossmodality data transformation [16, 14, 10]. [sent-80, score-0.368]
</p><p>32 Our DDN with deep structures are more powerful than SVM, which is a flat model [8]. [sent-83, score-0.179]
</p><p>33 In their network, occlusion patterns are sampled from the models and then verified with input images. [sent-86, score-0.157]
</p><p>34 In contrast, our model directly maps the input features to occlusion masks. [sent-89, score-0.2]
</p><p>35 The deep belief network (DBN) [8] and the deep Boltzmann machine (DBM) [21] both consist of multiple layers of RBMs, and complete the corrupted data using probabilistic inference. [sent-92, score-0.788]
</p><p>36 The denoising autoencoder (DAE) [26] has shown excellent performance at recovering corrupted data, and we have integrated it as a module in our DDN. [sent-94, score-0.156]
</p><p>37 [13] marginalized missing data with proposed deep sum product network for facial attribute recognition. [sent-96, score-0.313]
</p><p>38 [16] proposed a multimodel deep network that concatenates data across modalities as input and reconstructs them by learning a shared representation. [sent-100, score-0.313]
</p><p>39 DDN architecture, which combines occlusion estimation, data completion, and data transformation in an unified deep network. [sent-103, score-0.377]
</p><p>40 the joint representation of images and label maps for face parsing. [sent-104, score-0.106]
</p><p>41 [29] proposed a deep network to transform a face image under arbitrary pose and lighting to a canonical view. [sent-106, score-0.381]
</p><p>42 [10] used convolutional neural networks (CNN), which consider data of one modality as input and the corresponding data of the other modality as output. [sent-109, score-0.106]
</p><p>43 The decomposition layers in DDN are similar to CNN, but with fully-connected layers that capture the global structures of the pedestrians. [sent-110, score-0.594]
</p><p>44 2 (a) shows the architecture of DDN, the input of  which is a feature vector x, and the output is a set of label maps {y1, . [sent-113, score-0.15]
</p><p>45 Each layer is fully lcaobnenle mcteadps w {yith the ne}xt o upper layer, aEnadc hth laeyree are one down-sampling layer, two occlusion estimation layers, two completion layers, and two decomposition layers. [sent-117, score-0.53]
</p><p>46 More layers can be added for more complex problems. [sent-119, score-0.265]
</p><p>47 x is also mapped to a binary occlusion mask ∈ [0, 1]n through two weight matrices Wo1 , Wo2 , and biase∈s bo1 , bo2 . [sent-121, score-0.247]
</p><p>48 xio = 0 if the i-th element of the feature is occluded, and xio = 1 otherwise. [sent-123, score-0.106]
</p><p>49 is computed as  xo  xo  xo xo = τ(Wo2ρ(Wo1x  + bo1 ) + bo2 ),  (1)  where τ(x) = 1/(1 + exp(−x)) and ρ(x) = max(0, x). [sent-124, score-0.664]
</p><p>50 Twhhee efirs τt( xo)cc =lus 1i/on(1 1es +tim exapti(o−nx layer employs t hme rxec(0ti,fixe)d. [sent-125, score-0.12]
</p><p>51 The second layer models binary data with the sigmoid function. [sent-127, score-0.12]
</p><p>52 is reconstructed from and xd as follows,  xc  xo  z  =  ρ(Wc2ρ(Wc1 (xo ? [sent-136, score-0.237]
</p><p>53 On the top of DDN, the completed feature xc is decomposed (transformed) into several label maps {y1, . [sent-141, score-0.158]
</p><p>54 Each label map yi ∈ [0, 1]n is estimated by yi  = τ(Wit2ρ(Wt1xc  + bt1) + bti2),  (4)  where yij = 0 indicates the pixel belongs to the background and yij = 1indicates the pixel is on the corresponding body part. [sent-151, score-0.164]
</p><p>55 Pre-training Occlusion Estimation Layer The occlusion estimation layers infer a binary mask xo from an input feature x. [sent-160, score-0.649]
</p><p>56 2F,  (5)  where Ho1 = ρ(Wo1 X) is the output of the first layer as shown in Fig. [sent-166, score-0.12]
</p><p>57 We pre-train each layer with two steps: parameters initialization and reconstruction error minimization. [sent-210, score-0.12]
</p><p>58 In the first step, for each completion layer, let v? [sent-211, score-0.16]
</p><p>59 For each clean sample, we generate 40 corrupted samples by computing the element-wise product between the feature and the 40 templates in Fig. [sent-232, score-0.109]
</p><p>60 Pre-training Decomposition Layers The first decomposition layer transforms the output of the previous layer to a different space through the weight matrix Wt1. [sent-239, score-0.334]
</p><p>61 The second layer projects the output of the first layer to several subspaces through a set of weight matrices {Wit2 }. [sent-240, score-0.298]
</p><p>62 Both decomposition layers can be pre-trained using the strategy introduced in Sec. [sent-245, score-0.329]
</p><p>63 , L} and iare the indices of layers and iterations. [sent-268, score-0.265]
</p><p>64 For the output layer of DDN,  eL = diag(y  − y)diag(y)(1  − y),  (13)  where diag(·) is the diagonal matrix. [sent-287, score-0.12]
</p><p>65 -th lower layer ew ditiah gth(·e) sigmoid iafugnocntiaoln m, tahtrei backpropagation error is denoted as e? [sent-289, score-0.151]
</p><p>66 For a lower layer with the rectified linear function, the backpropagation error is computed as  ei? [sent-301, score-0.151]
</p><p>67 The occlusion estimation layers are pre-trained with 600 images selected from the CUHK occlusion dataset [17], where the ground truth of occlusion masks was obtained as the overlapping regions of the bounding boxes of neighboring pedestrians, e. [sent-319, score-0.765]
</p><p>68 Both the completion and decomposition layers are pre-trained with the HumanEva dataset [22], which contains 937 clean pedestrians with the ground truth of label maps annotated by [1]. [sent-323, score-0.664]
</p><p>69 Pretraining the decomposition layers requires clean images and their label maps. [sent-324, score-0.414]
</p><p>70 Pre-training the completion layers requires clean images, the corrupted data of which can be obtained by element-wise multiplication with the 40 occlusion templates shown in Fig. [sent-325, score-0.691]
</p><p>71 2 shows the results of pedestrian parsing on two datasets: the Penn-Fudan dataset [27] and a new dataset constructed by us. [sent-329, score-0.264]
</p><p>72 Our pedestrian parsing dataset contains 3, 673 images from 171 videos of differentsurveillancescenes (PPSS), where 2, 064 images are occluded and 1, 609 are not. [sent-331, score-0.301]
</p><p>73 7, which shows that large pose, illumination, and occlusion variations are present. [sent-334, score-0.157]
</p><p>74 Compared with Penn-Fudan, PPSS is much larger and more diversified on scene coverage, and is therefore suitable to evaluate the performance of pedestrian parsing algorithms in practical applications. [sent-335, score-0.264]
</p><p>75 We compare with structured SVM [7] and RoBM [24] for occlusion estimation on  the CUHK dataset [17]. [sent-347, score-0.186]
</p><p>76 The two completion layers in DDN have neurons 104 and 3, 000, respectively. [sent-366, score-0.425]
</p><p>77 CNN has three convolutional layers, and each layer has 32 filters. [sent-389, score-0.146]
</p><p>78 However, the major advantage of DDN is that all the three modules can be well integrated into an unified deep architecture and jointly fine-tuned for the ultimate goal of human parsing. [sent-392, score-0.288]
</p><p>79 2 shows that the performance of human parsing is significantly improved after fine tuning. [sent-395, score-0.144]
</p><p>80 It takes two hours to train our network on one NVIDIA GTX 670 GPU, and takes less than 0. [sent-402, score-0.134]
</p><p>81 We also show the results by using only the decomposition layers (DL), which are trained with the HumanEva dataset. [sent-404, score-0.329]
</p><p>82 Table 4 (a) reports the human parsing accuracy of DDN compared with SBP [1], P&S; [20], and PbOS [6]. [sent-405, score-0.178]
</p><p>83 DDN adopts HOG features and the fully-connected network architecture. [sent-414, score-0.134]
</p><p>84 For our methods, using DL alone achieves better results than DDN, since this dataset has no occlusion, which means that the occlusion estimation and completion layers may slightly induce noise to the DL in the DDN. [sent-420, score-0.611]
</p><p>85 1, and fine-tune the network with the training data of PPSS. [sent-426, score-0.134]
</p><p>86 These three methods have the best performance among state-of-the-art methods on occlusion estimation, data completion, and data transformation. [sent-433, score-0.157]
</p><p>87 First, the performance of DL drops significantly when occlusion is present. [sent-436, score-0.157]
</p><p>88 DL essentially transform the occluded HOG features to the label maps, which is difficult since the feature space of occlusion is extremely large. [sent-437, score-0.255]
</p><p>89 8 presents some segmentation examples of DDN compared to DL, and shows that DDN can effectively capture the pose of the human when large occlusion is present, because our network is carefully designed to handle occlusion. [sent-439, score-0.357]
</p><p>90 Note that some small body parts can still be estimated, such as “shoes” and “arms”, since the correlations between pixels are implicitly maintained by our network structure. [sent-443, score-0.217]
</p><p>91 Conclusions We present a new Deep Decompositional Network (DDN) for pedestrian parsing. [sent-450, score-0.142]
</p><p>92 DDN combines the occlusion estimation layers, completion layers, and the decomposition layers in an unified network, which can handle large occlusions. [sent-451, score-0.694]
</p><p>93 We construct a large benchmark parsing dataset that is larger and more difficult than the existing dataset. [sent-452, score-0.122]
</p><p>94 Our method outperforms the state-of-the-art on pedestrian parsing, both with and without occlusions. [sent-453, score-0.142]
</p><p>95 The shape boltzmann machine: a strong model of object shape. [sent-482, score-0.097]
</p><p>96 A deep sum-product architecture for robust facial attributes analysis. [sent-538, score-0.247]
</p><p>97 A discriminative deep model for pedestrian detection with occlusion handling. [sent-565, score-0.478]
</p><p>98 Modeling mutual visibility relationship with a deep model in pedestrian detection. [sent-570, score-0.321]
</p><p>99 A generative model for simultaneous estimation of human body shape and pixellevel segmentation. [sent-580, score-0.134]
</p><p>100 Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. [sent-618, score-0.397]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ddn', 0.724), ('layers', 0.265), ('deep', 0.179), ('xo', 0.166), ('completion', 0.16), ('occlusion', 0.157), ('pedestrian', 0.142), ('network', 0.134), ('sbp', 0.124), ('parsing', 0.122), ('layer', 0.12), ('cnn', 0.102), ('boltzmann', 0.097), ('pbos', 0.089), ('ppss', 0.089), ('humaneva', 0.087), ('body', 0.083), ('wc', 0.076), ('robm', 0.071), ('cuhk', 0.07), ('architecture', 0.068), ('dl', 0.066), ('decomposition', 0.064), ('autoencoder', 0.063), ('decompositional', 0.058), ('bmae', 0.053), ('eslami', 0.053), ('msbm', 0.053), ('xio', 0.053), ('arms', 0.051), ('dae', 0.048), ('pedestrians', 0.047), ('dbm', 0.047), ('dbn', 0.047), ('clean', 0.046), ('vc', 0.046), ('maps', 0.043), ('denoising', 0.042), ('occlusions', 0.041), ('completed', 0.041), ('luo', 0.04), ('rbm', 0.039), ('label', 0.039), ('occluded', 0.037), ('ouyang', 0.036), ('xd', 0.036), ('rauschert', 0.036), ('xc', 0.035), ('hog', 0.035), ('diag', 0.035), ('reports', 0.034), ('templates', 0.032), ('mask', 0.032), ('backpropagation', 0.031), ('pretraining', 0.031), ('corrupted', 0.031), ('weight', 0.03), ('hidden', 0.03), ('mnih', 0.029), ('ngiam', 0.029), ('modality', 0.029), ('estimation', 0.029), ('matrices', 0.028), ('noises', 0.028), ('biases', 0.028), ('rbms', 0.027), ('convolutional', 0.026), ('hair', 0.026), ('stochastic', 0.026), ('clutters', 0.025), ('devise', 0.024), ('face', 0.024), ('surveillance', 0.024), ('synthesize', 0.024), ('cloth', 0.023), ('clothes', 0.023), ('accuracies', 0.023), ('chinese', 0.022), ('human', 0.022), ('transformation', 0.022), ('networks', 0.022), ('segmentation', 0.022), ('transform', 0.022), ('pose', 0.022), ('mcmc', 0.022), ('multinomial', 0.022), ('yij', 0.021), ('hc', 0.021), ('template', 0.021), ('hong', 0.021), ('salakhutdinov', 0.021), ('kong', 0.02), ('dictionary', 0.02), ('hinton', 0.02), ('bc', 0.02), ('module', 0.02), ('ym', 0.019), ('unified', 0.019), ('poselets', 0.019), ('bengio', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="311-tfidf-1" href="./iccv-2013-Pedestrian_Parsing_via_Deep_Decompositional_Network.html">311 iccv-2013-Pedestrian Parsing via Deep Decompositional Network</a></p>
<p>Author: Ping Luo, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: We propose a new Deep Decompositional Network (DDN) for parsing pedestrian images into semantic regions, such as hair, head, body, arms, and legs, where the pedestrians can be heavily occluded. Unlike existing methods based on template matching or Bayesian inference, our approach directly maps low-level visual features to the label maps of body parts with DDN, which is able to accurately estimate complex pose variations with good robustness to occlusions and background clutters. DDN jointly estimates occluded regions and segments body parts by stacking three types of hidden layers: occlusion estimation layers, completion layers, and decomposition layers. The occlusion estimation layers estimate a binary mask, indicating which part of a pedestrian is invisible. The completion layers synthesize low-level features of the invisible part from the original features and the occlusion mask. The decomposition layers directly transform the synthesized visual features to label maps. We devise a new strategy to pre-train these hidden layers, and then fine-tune the entire network using the stochastic gradient descent. Experimental results show that our approach achieves better segmentation accuracy than the state-of-the-art methods on pedestrian images with or without occlusions. Another important contribution of this paper is that it provides a large scale benchmark human parsing dataset1 that includes 3, 673 annotated samples collected from 171 surveillance videos. It is 20 times larger than existing public datasets.</p><p>2 0.22509682 <a title="311-tfidf-2" href="./iccv-2013-Multi-stage_Contextual_Deep_Learning_for_Pedestrian_Detection.html">279 iccv-2013-Multi-stage Contextual Deep Learning for Pedestrian Detection</a></p>
<p>Author: Xingyu Zeng, Wanli Ouyang, Xiaogang Wang</p><p>Abstract: Cascaded classifiers1 have been widely used in pedestrian detection and achieved great success. These classifiers are trained sequentially without joint optimization. In this paper, we propose a new deep model that can jointly train multi-stage classifiers through several stages of backpropagation. It keeps the score map output by a classifier within a local region and uses it as contextual information to support the decision at the next stage. Through a specific design of the training strategy, this deep architecture is able to simulate the cascaded classifiers by mining hard samples to train the network stage-by-stage. Each classifier handles samples at a different difficulty level. Unsupervised pre-training and specifically designed stage-wise supervised training are used to regularize the optimization problem. Both theoretical analysis and experimental results show that the training strategy helps to avoid overfitting. Experimental results on three datasets (Caltech, ETH and TUD-Brussels) show that our approach outperforms the state-of-the-art approaches.</p><p>3 0.20694886 <a title="311-tfidf-3" href="./iccv-2013-Joint_Deep_Learning_for_Pedestrian_Detection.html">220 iccv-2013-Joint Deep Learning for Pedestrian Detection</a></p>
<p>Author: Wanli Ouyang, Xiaogang Wang</p><p>Abstract: Feature extraction, deformation handling, occlusion handling, and classi?cation are four important components in pedestrian detection. Existing methods learn or design these components either individually or sequentially. The interaction among these components is not yet well explored. This paper proposes that they should be jointly learned in order to maximize their strengths through cooperation. We formulate these four components into a joint deep learning framework and propose a new deep network architecture1. By establishing automatic, mutual interaction among components, the deep model achieves a 9% reduction in the average miss rate compared with the current best-performing pedestrian detection approaches on the largest Caltech benchmark dataset.</p><p>4 0.13509218 <a title="311-tfidf-4" href="./iccv-2013-Deep_Learning_Identity-Preserving_Face_Space.html">106 iccv-2013-Deep Learning Identity-Preserving Face Space</a></p>
<p>Author: Zhenyao Zhu, Ping Luo, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: Face recognition with large pose and illumination variations is a challenging problem in computer vision. This paper addresses this challenge by proposing a new learningbased face representation: the face identity-preserving (FIP) features. Unlike conventional face descriptors, the FIP features can significantly reduce intra-identity variances, while maintaining discriminativeness between identities. Moreover, the FIP features extracted from an image under any pose and illumination can be used to reconstruct its face image in the canonical view. This property makes it possible to improve the performance of traditional descriptors, such as LBP [2] and Gabor [31], which can be extracted from our reconstructed images in the canonical view to eliminate variations. In order to learn the FIP features, we carefully design a deep network that combines the feature extraction layers and the reconstruction layer. The former encodes a face image into the FIP features, while the latter transforms them to an image in the canonical view. Extensive experiments on the large MultiPIE face database [7] demonstrate that it significantly outperforms the state-of-the-art face recognition methods.</p><p>5 0.13461831 <a title="311-tfidf-5" href="./iccv-2013-Hybrid_Deep_Learning_for_Face_Verification.html">206 iccv-2013-Hybrid Deep Learning for Face Verification</a></p>
<p>Author: Yi Sun, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: This paper proposes a hybrid convolutional network (ConvNet)-Restricted Boltzmann Machine (RBM) model for face verification in wild conditions. A key contribution of this work is to directly learn relational visual features, which indicate identity similarities, from raw pixels of face pairs with a hybrid deep network. The deep ConvNets in our model mimic the primary visual cortex to jointly extract local relational visual features from two face images compared with the learned filter pairs. These relational features are further processed through multiple layers to extract high-level and global features. Multiple groups of ConvNets are constructed in order to achieve robustness and characterize face similarities from different aspects. The top-layerRBMperforms inferencefrom complementary high-level features extracted from different ConvNet groups with a two-level average pooling hierarchy. The entire hybrid deep network is jointly fine-tuned to optimize for the task of face verification. Our model achieves competitive face verification performance on the LFW dataset.</p><p>6 0.12713803 <a title="311-tfidf-6" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>7 0.11107802 <a title="311-tfidf-7" href="./iccv-2013-Handling_Occlusions_with_Franken-Classifiers.html">190 iccv-2013-Handling Occlusions with Franken-Classifiers</a></p>
<p>8 0.10859267 <a title="311-tfidf-8" href="./iccv-2013-Exploiting_Reflection_Change_for_Automatic_Reflection_Removal.html">151 iccv-2013-Exploiting Reflection Change for Automatic Reflection Removal</a></p>
<p>9 0.10160063 <a title="311-tfidf-9" href="./iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain.html">351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</a></p>
<p>10 0.10091063 <a title="311-tfidf-10" href="./iccv-2013-Modeling_Occlusion_by_Discriminative_AND-OR_Structures.html">269 iccv-2013-Modeling Occlusion by Discriminative AND-OR Structures</a></p>
<p>11 0.0856914 <a title="311-tfidf-11" href="./iccv-2013-Learning_People_Detectors_for_Tracking_in_Crowded_Scenes.html">242 iccv-2013-Learning People Detectors for Tracking in Crowded Scenes</a></p>
<p>12 0.083039209 <a title="311-tfidf-12" href="./iccv-2013-Topology-Constrained_Layered_Tracking_with_Latent_Flow.html">420 iccv-2013-Topology-Constrained Layered Tracking with Latent Flow</a></p>
<p>13 0.075367071 <a title="311-tfidf-13" href="./iccv-2013-Strong_Appearance_and_Expressive_Spatial_Models_for_Human_Pose_Estimation.html">403 iccv-2013-Strong Appearance and Expressive Spatial Models for Human Pose Estimation</a></p>
<p>14 0.075018719 <a title="311-tfidf-14" href="./iccv-2013-Locally_Affine_Sparse-to-Dense_Matching_for_Motion_and_Occlusion_Estimation.html">256 iccv-2013-Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation</a></p>
<p>15 0.074665189 <a title="311-tfidf-15" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>16 0.074158303 <a title="311-tfidf-16" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>17 0.074052654 <a title="311-tfidf-17" href="./iccv-2013-DeepFlow%3A_Large_Displacement_Optical_Flow_with_Deep_Matching.html">105 iccv-2013-DeepFlow: Large Displacement Optical Flow with Deep Matching</a></p>
<p>18 0.072565146 <a title="311-tfidf-18" href="./iccv-2013-A_Non-parametric_Bayesian_Network_Prior_of_Human_Pose.html">24 iccv-2013-A Non-parametric Bayesian Network Prior of Human Pose</a></p>
<p>19 0.071690209 <a title="311-tfidf-19" href="./iccv-2013-Scene_Collaging%3A_Analysis_and_Synthesis_of_Natural_Images_with_Semantic_Layers.html">375 iccv-2013-Scene Collaging: Analysis and Synthesis of Natural Images with Semantic Layers</a></p>
<p>20 0.069027402 <a title="311-tfidf-20" href="./iccv-2013-Random_Forests_of_Local_Experts_for_Pedestrian_Detection.html">336 iccv-2013-Random Forests of Local Experts for Pedestrian Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.146), (1, 0.007), (2, -0.023), (3, -0.034), (4, 0.029), (5, -0.08), (6, 0.013), (7, 0.079), (8, -0.033), (9, -0.018), (10, -0.024), (11, 0.009), (12, 0.038), (13, -0.059), (14, 0.004), (15, 0.037), (16, -0.017), (17, 0.044), (18, 0.141), (19, 0.183), (20, -0.047), (21, -0.001), (22, 0.028), (23, -0.074), (24, -0.18), (25, -0.028), (26, -0.019), (27, 0.077), (28, -0.086), (29, 0.114), (30, -0.052), (31, 0.105), (32, 0.134), (33, -0.032), (34, 0.036), (35, 0.064), (36, -0.077), (37, 0.105), (38, -0.005), (39, 0.053), (40, -0.074), (41, -0.067), (42, -0.04), (43, -0.019), (44, 0.028), (45, 0.141), (46, 0.089), (47, 0.028), (48, -0.051), (49, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9357332 <a title="311-lsi-1" href="./iccv-2013-Pedestrian_Parsing_via_Deep_Decompositional_Network.html">311 iccv-2013-Pedestrian Parsing via Deep Decompositional Network</a></p>
<p>Author: Ping Luo, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: We propose a new Deep Decompositional Network (DDN) for parsing pedestrian images into semantic regions, such as hair, head, body, arms, and legs, where the pedestrians can be heavily occluded. Unlike existing methods based on template matching or Bayesian inference, our approach directly maps low-level visual features to the label maps of body parts with DDN, which is able to accurately estimate complex pose variations with good robustness to occlusions and background clutters. DDN jointly estimates occluded regions and segments body parts by stacking three types of hidden layers: occlusion estimation layers, completion layers, and decomposition layers. The occlusion estimation layers estimate a binary mask, indicating which part of a pedestrian is invisible. The completion layers synthesize low-level features of the invisible part from the original features and the occlusion mask. The decomposition layers directly transform the synthesized visual features to label maps. We devise a new strategy to pre-train these hidden layers, and then fine-tune the entire network using the stochastic gradient descent. Experimental results show that our approach achieves better segmentation accuracy than the state-of-the-art methods on pedestrian images with or without occlusions. Another important contribution of this paper is that it provides a large scale benchmark human parsing dataset1 that includes 3, 673 annotated samples collected from 171 surveillance videos. It is 20 times larger than existing public datasets.</p><p>2 0.80124587 <a title="311-lsi-2" href="./iccv-2013-Joint_Deep_Learning_for_Pedestrian_Detection.html">220 iccv-2013-Joint Deep Learning for Pedestrian Detection</a></p>
<p>Author: Wanli Ouyang, Xiaogang Wang</p><p>Abstract: Feature extraction, deformation handling, occlusion handling, and classi?cation are four important components in pedestrian detection. Existing methods learn or design these components either individually or sequentially. The interaction among these components is not yet well explored. This paper proposes that they should be jointly learned in order to maximize their strengths through cooperation. We formulate these four components into a joint deep learning framework and propose a new deep network architecture1. By establishing automatic, mutual interaction among components, the deep model achieves a 9% reduction in the average miss rate compared with the current best-performing pedestrian detection approaches on the largest Caltech benchmark dataset.</p><p>3 0.74192387 <a title="311-lsi-3" href="./iccv-2013-Multi-stage_Contextual_Deep_Learning_for_Pedestrian_Detection.html">279 iccv-2013-Multi-stage Contextual Deep Learning for Pedestrian Detection</a></p>
<p>Author: Xingyu Zeng, Wanli Ouyang, Xiaogang Wang</p><p>Abstract: Cascaded classifiers1 have been widely used in pedestrian detection and achieved great success. These classifiers are trained sequentially without joint optimization. In this paper, we propose a new deep model that can jointly train multi-stage classifiers through several stages of backpropagation. It keeps the score map output by a classifier within a local region and uses it as contextual information to support the decision at the next stage. Through a specific design of the training strategy, this deep architecture is able to simulate the cascaded classifiers by mining hard samples to train the network stage-by-stage. Each classifier handles samples at a different difficulty level. Unsupervised pre-training and specifically designed stage-wise supervised training are used to regularize the optimization problem. Both theoretical analysis and experimental results show that the training strategy helps to avoid overfitting. Experimental results on three datasets (Caltech, ETH and TUD-Brussels) show that our approach outperforms the state-of-the-art approaches.</p><p>4 0.59277481 <a title="311-lsi-4" href="./iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain.html">351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</a></p>
<p>Author: David Eigen, Dilip Krishnan, Rob Fergus</p><p>Abstract: Photographs taken through a window are often compromised by dirt or rain present on the window surface. Common cases of this include pictures taken from inside a vehicle, or outdoor security cameras mounted inside a protective enclosure. At capture time, defocus can be used to remove the artifacts, but this relies on achieving a shallow depth-of-field and placement of the camera close to the window. Instead, we present a post-capture image processing solution that can remove localized rain and dirt artifacts from a single image. We collect a dataset of clean/corrupted image pairs which are then used to train a specialized form of convolutional neural network. This learns how to map corrupted image patches to clean ones, implicitly capturing the characteristic appearance of dirt and water droplets in natural images. Our models demonstrate effective removal of dirt and rain in outdoor test conditions.</p><p>5 0.58617759 <a title="311-lsi-5" href="./iccv-2013-Hybrid_Deep_Learning_for_Face_Verification.html">206 iccv-2013-Hybrid Deep Learning for Face Verification</a></p>
<p>Author: Yi Sun, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: This paper proposes a hybrid convolutional network (ConvNet)-Restricted Boltzmann Machine (RBM) model for face verification in wild conditions. A key contribution of this work is to directly learn relational visual features, which indicate identity similarities, from raw pixels of face pairs with a hybrid deep network. The deep ConvNets in our model mimic the primary visual cortex to jointly extract local relational visual features from two face images compared with the learned filter pairs. These relational features are further processed through multiple layers to extract high-level and global features. Multiple groups of ConvNets are constructed in order to achieve robustness and characterize face similarities from different aspects. The top-layerRBMperforms inferencefrom complementary high-level features extracted from different ConvNet groups with a two-level average pooling hierarchy. The entire hybrid deep network is jointly fine-tuned to optimize for the task of face verification. Our model achieves competitive face verification performance on the LFW dataset.</p><p>6 0.56381667 <a title="311-lsi-6" href="./iccv-2013-Exploiting_Reflection_Change_for_Automatic_Reflection_Removal.html">151 iccv-2013-Exploiting Reflection Change for Automatic Reflection Removal</a></p>
<p>7 0.53248143 <a title="311-lsi-7" href="./iccv-2013-Deep_Learning_Identity-Preserving_Face_Space.html">106 iccv-2013-Deep Learning Identity-Preserving Face Space</a></p>
<p>8 0.51148188 <a title="311-lsi-8" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>9 0.47267151 <a title="311-lsi-9" href="./iccv-2013-Image_Segmentation_with_Cascaded_Hierarchical_Models_and_Logistic_Disjunctive_Normal_Networks.html">211 iccv-2013-Image Segmentation with Cascaded Hierarchical Models and Logistic Disjunctive Normal Networks</a></p>
<p>10 0.46703029 <a title="311-lsi-10" href="./iccv-2013-Topology-Constrained_Layered_Tracking_with_Latent_Flow.html">420 iccv-2013-Topology-Constrained Layered Tracking with Latent Flow</a></p>
<p>11 0.43849954 <a title="311-lsi-11" href="./iccv-2013-Paper_Doll_Parsing%3A_Retrieving_Similar_Styles_to_Parse_Clothing_Items.html">306 iccv-2013-Paper Doll Parsing: Retrieving Similar Styles to Parse Clothing Items</a></p>
<p>12 0.40966725 <a title="311-lsi-12" href="./iccv-2013-Modeling_Occlusion_by_Discriminative_AND-OR_Structures.html">269 iccv-2013-Modeling Occlusion by Discriminative AND-OR Structures</a></p>
<p>13 0.40681297 <a title="311-lsi-13" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>14 0.39873341 <a title="311-lsi-14" href="./iccv-2013-Handling_Occlusions_with_Franken-Classifiers.html">190 iccv-2013-Handling Occlusions with Franken-Classifiers</a></p>
<p>15 0.38869485 <a title="311-lsi-15" href="./iccv-2013-A_Generalized_Low-Rank_Appearance_Model_for_Spatio-temporally_Correlated_Rain_Streaks.html">15 iccv-2013-A Generalized Low-Rank Appearance Model for Spatio-temporally Correlated Rain Streaks</a></p>
<p>16 0.37485117 <a title="311-lsi-16" href="./iccv-2013-A_Deformable_Mixture_Parsing_Model_with_Parselets.html">8 iccv-2013-A Deformable Mixture Parsing Model with Parselets</a></p>
<p>17 0.357914 <a title="311-lsi-17" href="./iccv-2013-Heterogeneous_Auto-similarities_of_Characteristics_%28HASC%29%3A_Exploiting_Relational_Information_for_Classification.html">193 iccv-2013-Heterogeneous Auto-similarities of Characteristics (HASC): Exploiting Relational Information for Classification</a></p>
<p>18 0.35282665 <a title="311-lsi-18" href="./iccv-2013-CoDeL%3A_A_Human_Co-detection_and_Labeling_Framework.html">75 iccv-2013-CoDeL: A Human Co-detection and Labeling Framework</a></p>
<p>19 0.34433156 <a title="311-lsi-19" href="./iccv-2013-Random_Forests_of_Local_Experts_for_Pedestrian_Detection.html">336 iccv-2013-Random Forests of Local Experts for Pedestrian Detection</a></p>
<p>20 0.34242278 <a title="311-lsi-20" href="./iccv-2013-Modeling_Self-Occlusions_in_Dynamic_Shape_and_Appearance_Tracking.html">270 iccv-2013-Modeling Self-Occlusions in Dynamic Shape and Appearance Tracking</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.054), (7, 0.016), (26, 0.077), (31, 0.029), (34, 0.011), (35, 0.012), (42, 0.068), (48, 0.376), (64, 0.06), (67, 0.014), (73, 0.025), (78, 0.015), (89, 0.129)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.81544399 <a title="311-lda-1" href="./iccv-2013-Pyramid_Coding_for_Functional_Scene_Element_Recognition_in_Video_Scenes.html">331 iccv-2013-Pyramid Coding for Functional Scene Element Recognition in Video Scenes</a></p>
<p>Author: Eran Swears, Anthony Hoogs, Kim Boyer</p><p>Abstract: Recognizing functional scene elemeents in video scenes based on the behaviors of moving objects that interact with them is an emerging problem ooff interest. Existing approaches have a limited ability to chharacterize elements such as cross-walks, intersections, andd buildings that have low activity, are multi-modal, or havee indirect evidence. Our approach recognizes the low activvity and multi-model elements (crosswalks/intersections) by introducing a hierarchy of descriptive clusters to fform a pyramid of codebooks that is sparse in the numbber of clusters and dense in content. The incorporation oof local behavioral context such as person-enter-building aand vehicle-parking nearby enables the detection of elemennts that do not have direct motion-based evidence, e.g. buuildings. These two contributions significantly improvee scene element recognition when compared against thhree state-of-the-art approaches. Results are shown on tyypical ground level surveillance video and for the first time on the more complex Wide Area Motion Imagery.</p><p>same-paper 2 0.77938521 <a title="311-lda-2" href="./iccv-2013-Pedestrian_Parsing_via_Deep_Decompositional_Network.html">311 iccv-2013-Pedestrian Parsing via Deep Decompositional Network</a></p>
<p>Author: Ping Luo, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: We propose a new Deep Decompositional Network (DDN) for parsing pedestrian images into semantic regions, such as hair, head, body, arms, and legs, where the pedestrians can be heavily occluded. Unlike existing methods based on template matching or Bayesian inference, our approach directly maps low-level visual features to the label maps of body parts with DDN, which is able to accurately estimate complex pose variations with good robustness to occlusions and background clutters. DDN jointly estimates occluded regions and segments body parts by stacking three types of hidden layers: occlusion estimation layers, completion layers, and decomposition layers. The occlusion estimation layers estimate a binary mask, indicating which part of a pedestrian is invisible. The completion layers synthesize low-level features of the invisible part from the original features and the occlusion mask. The decomposition layers directly transform the synthesized visual features to label maps. We devise a new strategy to pre-train these hidden layers, and then fine-tune the entire network using the stochastic gradient descent. Experimental results show that our approach achieves better segmentation accuracy than the state-of-the-art methods on pedestrian images with or without occlusions. Another important contribution of this paper is that it provides a large scale benchmark human parsing dataset1 that includes 3, 673 annotated samples collected from 171 surveillance videos. It is 20 times larger than existing public datasets.</p><p>3 0.74120981 <a title="311-lda-3" href="./iccv-2013-Bounded_Labeling_Function_for_Global_Segmentation_of_Multi-part_Objects_with_Geometric_Constraints.html">63 iccv-2013-Bounded Labeling Function for Global Segmentation of Multi-part Objects with Geometric Constraints</a></p>
<p>Author: Masoud S. Nosrati, Shawn Andrews, Ghassan Hamarneh</p><p>Abstract: The inclusion of shape and appearance priors have proven useful for obtaining more accurate and plausible segmentations, especially for complex objects with multiple parts. In this paper, we augment the popular MumfordShah model to incorporate two important geometrical constraints, termed containment and detachment, between different regions with a specified minimum distance between their boundaries. Our method is able to handle multiple instances of multi-part objects defined by these geometrical hamarneh} @ s fu . ca (a)Standar laΩb ehlingΩfuhnctionseting(Ωb)hΩOuirseΩtijng Figure 1: The inside vs. outside ambiguity in (a) is resolved by our containment constraint in (b). constraints using a single labeling function while maintaining global optimality. We demonstrate the utility and advantages of these two constraints and show that the proposed convex continuous method is superior to other state-of-theart methods, including its discrete counterpart, in terms of memory usage, and metrication errors.</p><p>4 0.67900956 <a title="311-lda-4" href="./iccv-2013-Pose-Configurable_Generic_Tracking_of_Elongated_Objects.html">320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</a></p>
<p>Author: Daniel Wesierski, Patrick Horain</p><p>Abstract: Elongated objects have various shapes and can shift, rotate, change scale, and be rigid or deform by flexing, articulating, and vibrating, with examples as varied as a glass bottle, a robotic arm, a surgical suture, a finger pair, a tram, and a guitar string. This generally makes tracking of poses of elongated objects very challenging. We describe a unified, configurable framework for tracking the pose of elongated objects, which move in the image plane and extend over the image region. Our method strives for simplicity, versatility, and efficiency. The object is decomposed into a chained assembly of segments of multiple parts that are arranged under a hierarchy of tailored spatio-temporal constraints. In this hierarchy, segments can rescale independently while their elasticity is controlled with global orientations and local distances. While the trend in tracking is to design complex, structure-free algorithms that update object appearance on- line, we show that our tracker, with the novel but remarkably simple, structured organization of parts with constant appearance, reaches or improves state-of-the-art performance. Most importantly, our model can be easily configured to track exact pose of arbitrary, elongated objects in the image plane. The tracker can run up to 100 fps on a desktop PC, yet the computation time scales linearly with the number of object parts. To our knowledge, this is the first approach to generic tracking of elongated objects.</p><p>5 0.65197355 <a title="311-lda-5" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>Author: Zhuoyuan Chen, Ying Wu</p><p>Abstract: Sparsity models have recently shown great promise in many vision tasks. Using a learned dictionary in sparsity models can in general outperform predefined bases in clean data. In practice, both training and testing data may be corrupted and contain noises and outliers. Although recent studies attempted to cope with corrupted data and achieved encouraging results in testing phase, how to handle corruption in training phase still remains a very difficult problem. In contrast to most existing methods that learn the dictionaryfrom clean data, this paper is targeted at handling corruptions and outliers in training data for dictionary learning. We propose a general method to decompose the reconstructive residual into two components: a non-sparse component for small universal noises and a sparse component for large outliers, respectively. In addition, , further analysis reveals the connection between our approach and the “partial” dictionary learning approach, updating only part of the prototypes (or informative codewords) with remaining (or noisy codewords) fixed. Experiments on synthetic data as well as real applications have shown satisfactory per- formance of this new robust dictionary learning approach.</p><p>6 0.63861853 <a title="311-lda-6" href="./iccv-2013-Illuminant_Chromaticity_from_Image_Sequences.html">207 iccv-2013-Illuminant Chromaticity from Image Sequences</a></p>
<p>7 0.54429764 <a title="311-lda-7" href="./iccv-2013-Joint_Deep_Learning_for_Pedestrian_Detection.html">220 iccv-2013-Joint Deep Learning for Pedestrian Detection</a></p>
<p>8 0.52599764 <a title="311-lda-8" href="./iccv-2013-Multi-stage_Contextual_Deep_Learning_for_Pedestrian_Detection.html">279 iccv-2013-Multi-stage Contextual Deep Learning for Pedestrian Detection</a></p>
<p>9 0.51442993 <a title="311-lda-9" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>10 0.50036985 <a title="311-lda-10" href="./iccv-2013-Hybrid_Deep_Learning_for_Face_Verification.html">206 iccv-2013-Hybrid Deep Learning for Face Verification</a></p>
<p>11 0.49007297 <a title="311-lda-11" href="./iccv-2013-Deep_Learning_Identity-Preserving_Face_Space.html">106 iccv-2013-Deep Learning Identity-Preserving Face Space</a></p>
<p>12 0.48264325 <a title="311-lda-12" href="./iccv-2013-A_Color_Constancy_Model_with_Double-Opponency_Mechanisms.html">5 iccv-2013-A Color Constancy Model with Double-Opponency Mechanisms</a></p>
<p>13 0.47871238 <a title="311-lda-13" href="./iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps.html">208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</a></p>
<p>14 0.47521156 <a title="311-lda-14" href="./iccv-2013-Person_Re-identification_by_Salience_Matching.html">313 iccv-2013-Person Re-identification by Salience Matching</a></p>
<p>15 0.47423577 <a title="311-lda-15" href="./iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain.html">351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</a></p>
<p>16 0.47171026 <a title="311-lda-16" href="./iccv-2013-Perceptual_Fidelity_Aware_Mean_Squared_Error.html">312 iccv-2013-Perceptual Fidelity Aware Mean Squared Error</a></p>
<p>17 0.47055298 <a title="311-lda-17" href="./iccv-2013-Beyond_Hard_Negative_Mining%3A_Efficient_Detector_Learning_via_Block-Circulant_Decomposition.html">61 iccv-2013-Beyond Hard Negative Mining: Efficient Detector Learning via Block-Circulant Decomposition</a></p>
<p>18 0.46804774 <a title="311-lda-18" href="./iccv-2013-Exploiting_Reflection_Change_for_Automatic_Reflection_Removal.html">151 iccv-2013-Exploiting Reflection Change for Automatic Reflection Removal</a></p>
<p>19 0.4645592 <a title="311-lda-19" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>20 0.46253228 <a title="311-lda-20" href="./iccv-2013-Modeling_Self-Occlusions_in_Dynamic_Shape_and_Appearance_Tracking.html">270 iccv-2013-Modeling Self-Occlusions in Dynamic Shape and Appearance Tracking</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
