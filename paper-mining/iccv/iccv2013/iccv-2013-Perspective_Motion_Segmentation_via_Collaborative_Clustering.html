<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-314" href="#">iccv2013-314</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</h1>
<br/><p>Source: <a title="iccv-2013-314-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Li_Perspective_Motion_Segmentation_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Zhuwen Li, Jiaming Guo, Loong-Fah Cheong, Steven Zhiying Zhou</p><p>Abstract: This paper addresses real-world challenges in the motion segmentation problem, including perspective effects, missing data, and unknown number of motions. It first formulates the 3-D motion segmentation from two perspective views as a subspace clustering problem, utilizing the epipolar constraint of an image pair. It then combines the point correspondence information across multiple image frames via a collaborative clustering step, in which tight integration is achieved via a mixed norm optimization scheme. For model selection, wepropose an over-segment and merge approach, where the merging step is based on the property of the ?1-norm ofthe mutual sparse representation oftwo oversegmented groups. The resulting algorithm can deal with incomplete trajectories and perspective effects substantially better than state-of-the-art two-frame and multi-frame methods. Experiments on a 62-clip dataset show the significant superiority of the proposed idea in both segmentation accuracy and model selection.</p><p>Reference: <a title="iccv-2013-314-reference" href="../iccv2013_reference/iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 j iaming  Abstract This paper addresses real-world challenges in the motion segmentation problem, including perspective effects, missing data, and unknown number of motions. [sent-3, score-0.689]
</p><p>2 It first formulates the 3-D motion segmentation from two perspective views as a subspace clustering problem, utilizing the epipolar constraint of an image pair. [sent-4, score-0.83]
</p><p>3 It then combines the point correspondence information across multiple image frames via a collaborative clustering step, in which tight integration is achieved via a mixed norm optimization scheme. [sent-5, score-0.316]
</p><p>4 The resulting algorithm can deal with incomplete trajectories and perspective effects substantially better than state-of-the-art two-frame and multi-frame methods. [sent-8, score-0.438]
</p><p>5 Introduction  ,  Previous approaches to the 3D motion segmentation problem can be roughly separated into the multi-frame and the two-frame methods. [sent-11, score-0.328]
</p><p>6 Multi-frame methods have been studied mostly under the affine assumption, because under this assumption the trajectories of a rigid motion across multiple frames lie in an affine subspace with a dimension of no more than 3, or a linear subspace with a dimension of at most 4. [sent-12, score-0.937]
</p><p>7 One can then solve the problem using either a factorization or a subspace separation framework [2, 7, 9, 10, 11, 14, 19, 22, 27, 28, 3 1, 34]. [sent-13, score-0.285]
</p><p>8 Two-view methods are usually based on the epipolar geometry, and are thus capable of handling perspective effects. [sent-14, score-0.247]
</p><p>9 The motion model fitting and selection are carried out by either statistical methods [13, 16, 24, 29] or algebraic methods [23, 32, 33]. [sent-15, score-0.27]
</p><p>10 The multi-frame methods have been better developed, partly due to the elegance of its formulation and partly due to the release of the Hopkins155 database [30], which contains largely clips with little perspective effects. [sent-16, score-0.464]
</p><p>11 Motion segmentation results of two sequences with strong perspective effects using SSC. [sent-20, score-0.457]
</p><p>12 Firstly, multi-frame affine methods suffer from their inability to deal with perspective effects, while this presents no problem in the two-frame method; it becomes a significant consideration when using shorter lenses for shooting outdoor sequences. [sent-24, score-0.279]
</p><p>13 Figure 1 shows the results of two sequences with perspective effects from Hopkins155; these results are produced by the state-of-the-art clustering algorithm sparse subspace clustering (SSC) [9]. [sent-25, score-0.715]
</p><p>14 Secondly, multi-frame affine methods generally require the trajectories to have full-length. [sent-27, score-0.216]
</p><p>15 If one simply filters out the trajectories which are absent in some frames, the density of the trajectories is likely to be significantly decreased, resulting in lack of coverage of many parts of the sequence. [sent-28, score-0.266]
</p><p>16 Clearly, two-frame methods suffer to a much lesser extent from the missing entry issue. [sent-32, score-0.231]
</p><p>17 (a) 60 trajectories obtained with the full-length requirement, and (b) 524 trajectories without the full-length requirement. [sent-34, score-0.266]
</p><p>18 argue that matrix completion techniques can help to fill in the missing entries [5]. [sent-36, score-0.434]
</p><p>19 Figure 2(c) shows the data matrix of the “delivery van” data, which has about 50% missing entries and is non-uniformly distributed. [sent-39, score-0.342]
</p><p>20 Thirdly, the number of motion groups is usually assumed  to be known a priori for multi-frame affine methods. [sent-41, score-0.429]
</p><p>21 It is indeed a strong indication that model selection is actually difficult for motion segmentation. [sent-42, score-0.27]
</p><p>22 Related to this issue is the fact that the number of motion groups in each clip of the Hopkins155 dataset remains unchanged throughout the frames, which makes it easy to indulge in the aforementioned assumption. [sent-43, score-0.416]
</p><p>23 In real videos, the number of motion groups may change throughout a clip as moving objects enter or leave the scene. [sent-44, score-0.452]
</p><p>24 On the other hand, there are clearly scenes where an observation period as short as two gle image pair, we revisit the epipolar constraint of twoperspective-view (TPV), leading to a subspace segmentation problem formulation that segments the null spaces of  the appropriate equations. [sent-50, score-0.383]
</p><p>25 Thus, the idea of subspace separation applies and one can follow the SSC approach in converting the motion segmentation problem into a graph partitioning problem based on an affinity matrix. [sent-51, score-0.627]
</p><p>26 A more powerful formulation that integrates multiple frames then follows, in which we derive an aggregated affinity matrix from multiple image pairs and seek a joint sparse coefficient recovery across multiple image pairs, i. [sent-53, score-0.406]
</p><p>27 , the sparse affinity coefficients of a particular trajectory should be consistently distributed across multiple image pairs in the sense that this trajectory should use the same set of other trajectories to express itself across allimage pairs. [sent-55, score-0.511]
</p><p>28 We first make a rough model estimation by analyzing the Laplacian matrix of the affinity matrix and over-segment the data into groups. [sent-58, score-0.236]
</p><p>29 Model selection remains very much an open problem in motion segmentation. [sent-84, score-0.27]
</p><p>30 While the number of zero eigenvalues of the Laplacian matrix can be related to the number of connected components of the affinity matrix, the challenge lies in determining the number of eigenvalues close to zero in a robust manner [19, 26]. [sent-85, score-0.268]
</p><p>31 This in turn allows us to perform merging of two over-segmented groups in a very robust way. [sent-96, score-0.235]
</p><p>32 Our work pays the price of a lower trajectory density for a more accurate motion model and a higher quality data input. [sent-101, score-0.29]
</p><p>33 ⎣⎡ff 132111 f 2312 f f231333⎦⎤∈ R3×3is the fundamental matrix, w⎣hich connects corre⎦spondences under the same rigid motion in two views. [sent-115, score-0.239]
</p><p>34 xp yp 1 )f = 0, (2)  =  )T  where f ( f11 f12 f13 f21 f22 f23 f31 f32 f33 is the 9 1 vector made up of the entries of F in row-major oisr tdheer. [sent-122, score-0.234]
</p><p>35 Clearly, those wp under the same rigid motion k form a hyperplane perpendicular to fk, which we refer to as the TPV motion subspace. [sent-124, score-0.68]
</p><p>36 Thus, in general the set of  ×  wp for points undergoing the same rigid motion k forms a unique hyperplane perpendicular to fk. [sent-128, score-0.526]
</p><p>37 From equation (3), it can be shown that  wp is related to a 9  1371  3 matrix H? [sent-136, score-0.274]
</p><p>38 −h23  −h13 0 It can be observed from −(4h) tha−t hthose− wp under the aforementioned degenerate configurations fall on the intersection of three hyperplanes, each of which is perpendicular to one  column of H? [sent-140, score-0.378]
</p><p>39 Thus, wp under these degenerate configurations live in a lower dimensional subspace with dimension no more than 6. [sent-145, score-0.438]
</p><p>40 Fortunately, there are various subspace separation algorithms [9, 19] that can handle subspaces with different dimensions and the above situation should pose no special problem. [sent-146, score-0.281]
</p><p>41 Sparse subspace clustering The preceding section has reduced the motion segmentation task to that of clustering subspaces of dimension at most 8 in R9 in general. [sent-150, score-0.741]
</p><p>42 The SSC algorithm can be used directly to perform subspace clustering for the case of single image pair; the case of multiple image pairs requires joint sparsity and will be discussed in Section 3. [sent-152, score-0.312]
</p><p>43 1  Single image pair  We briefly review the SSC algorithm in the context of the TPV motion subspace: each column wp can be represented as a linear combination of the other columns wq ? [sent-157, score-0.519]
</p><p>44 An alternative way is to accumulate the individual affinity matrices or adopt the multi-view spectral clustering method [36]. [sent-193, score-0.227]
</p><p>45 In other words, the nonzero entries of C(l) should be sparse and those columns corresponding to the same trajectory across the different C(l) should share the same support set. [sent-198, score-0.246]
</p><p>46 Notice that the correspondences can be missing in some image pairs, here “missing” means a trajectory is invisible in either one or both of the image pair. [sent-229, score-0.336]
</p><p>47 In this case, we fill in with a 09× 1 column vector for the missing data so as to ensure that 9a×ll1 W(l) have the same dimension. [sent-230, score-0.233]
</p><p>48 More specifically, if a trajectory p is missing in the image pair l, then in the l-th correspondence matrix W(l) , the p-th column = 09× 1. [sent-231, score-0.447]
</p><p>49 Our rationales for filling in with 09× 1 are t-  wp(l)  ×  wofold: 1) when we want to obtain the sparse coding for the p-th point, the optimal solution for the missing data in the l-th image pair is 0P−1 1, not incurring any cost in equation (8), nor biasing the solution for other C(l) in any way. [sent-232, score-0.342]
</p><p>50 q, the missing data will not be chosen to represent the point q in the l-th image pair since it contributes nothing to the representation of q. [sent-235, score-0.247]
</p><p>51 This allows us to treat a trajectory with missing data in a uniform manner, without affecting the joint optimization scheme. [sent-236, score-0.29]
</p><p>52 If a corrupted match is detected in E∗(l) , we will delete it from image pair lbut preserve the correct matches of that trajectory in other image pairs unless all matches of that trajectory are corrupted. [sent-262, score-0.339]
</p><p>53 Merging via coefficient analysis As the number of motion groups is usually not known a priori in reality, we have to come to grips with the model selection problem. [sent-265, score-0.515]
</p><p>54 In view of the difficulty of cluster detection, we propose to first over-segment the data based on the number of zero eigenvalues of the Laplacian matrix of the affinity matrix, and then attempt to merge the clusters later via the following model selection scheme. [sent-266, score-0.317]
</p><p>55 Given a data point q ∈ RD and a group of points {piG}iMi=ve1n nst aac dkaetda as tihnet c qolu ∈mn Rs of the matrix P ∈ RD×M {anpd spanning tehde a subspace mS,n isf o we use Patr itxo represent q, ia. [sent-267, score-0.323]
</p><p>56 Now consider two groups of points obtained from the over-segmentation step, P ∈ RD×M and Q ∈ RD×N,  wovheors-ese cgmoleumntnatsi {np si}teiMp=,1 Pand ∈ { Rqi}iN=1 are e Qxtra ∈cte Rd from subspaces uSmu nasnd { pSv} respectively. [sent-282, score-0.264]
</p><p>57 =e case for the multiple image pairs in a manner analogous to the collaborative clustering algorithm in (8). [sent-303, score-0.229]
</p><p>58 Ppu0t ←: S Cetu rorfe mnto stieot no fg groups Pfor k← = C 1u →ren (tK se t− of 1) g rdouo rfo kr =ea 1ch → group pair d doo Compute relationship matrix R according to (13). [sent-326, score-0.358]
</p><p>59 return Pk enrde tifu end for return Pk return  P  One might question what if some of the groups are too small or degenerate such that they do not adequately represent the underlying subspace S. [sent-332, score-0.418]
</p><p>60 Ctruleea trhlya,t sthucehre invariably exist some other groups whose points fully span the subspace S. [sent-334, score-0.361]
</p><p>61 We choose the first and the last frames of all sequences as the image pair for the testing, which ensures that all correspondences in the scene have sufficient displacements in the image plane. [sent-344, score-0.241]
</p><p>62 For the sake of comparison, we assume the number of motion groups is known in this experiment, like what many algorithms did. [sent-345, score-0.346]
</p><p>63 We also list the classification er-  rors when applying ALC[22], GPCA[3 1], LSA[34], SSC[9] and LRR[19] to the affine motion subspace for comparison. [sent-346, score-0.489]
</p><p>64 1Even if a motion group consists of say, just two walls, the degenerate case of the over-segmentation yielding two walls cleanly (and thus not mergeable) seldom arises; instead, the points of the two walls are usually segmented non-exactly by our over-segmentation step. [sent-348, score-0.437]
</p><p>65 34 The results indicate that segmentation from two properly chosen views is almost as good as segmentation from the multiple views. [sent-368, score-0.26]
</p><p>66 We believe that this is due to a combination of factors such as the better modeling of perspective effect and the choice of better clustering methods. [sent-370, score-0.249]
</p><p>67 Results on multiple image pairs We now evaluate the complete algorithm using multiple  image pairs without knowing the number of motion groups and with challenges like missing data and perspective effects. [sent-373, score-0.821]
</p><p>68 Since Hopkins155 has a very unbalanced number of 2-motion and 3motion clips (120 and 35 respectively), we retain only the 50 original seed videos (the other 105 2-motion clips are created by splitting off from the 3-motion clips). [sent-375, score-0.47]
</p><p>69 More importantly, to evaluate the performance under missing data and perspective effects, we added 12 clips with incomplete trajectories, of which 4 are from [25] and the other 8 are captured by us using a handheld camera with a wide angle lens. [sent-376, score-0.659]
</p><p>70 The newly captured sequences contain about 100 frames each, some of which experience heavy occlusions, posing significant challenge to the matrix completion task, as we shall see later. [sent-377, score-0.311]
</p><p>71 Of the resultant 62 motion clips, 26 contain two motions, 36 contain three motions, 12 suffer from missing data, and 9 have strong perspective effects (some of these categories are not mutually exclusive). [sent-378, score-0.671]
</p><p>72 Classification results on 62-clip dataset  MethodALCGPCALBFLRRMSMCORKSSCM-TPV Classification error (%) - clips with perspective effect: 9 clips Mean16. [sent-385, score-0.675]
</p><p>73 46) Classification (%) clips with missing data: 12 clips Mean25. [sent-401, score-0.668]
</p><p>74 37) Group number estimation all 62 clips # correct2133293525373346 error  -  error  -  error  -  -  second order difference (SOD) method as in LBF. [sent-449, score-0.361]
</p><p>75 For those algorithms which do not explicitly handle missing data, such as LBF, LRR, ORK and SSC, we recover the data matrix using Chen’s matrix completion approach[5], which in our experience has the best performance among various competing algorithms (such as OptSpace[20], GROUSE[1] and etc. [sent-451, score-0.436]
</p><p>76 Since the estimated number of motion groups may not be the same as the ground truth number, we exhaustively test all the cluster pairings to obtain the best  error rates. [sent-455, score-0.388]
</p><p>77 Furthermore, to investigate if good model selection results in good segmentation, the error rates obtained by only considering sequences where the number of motions is correctly estimated are shown in the bracket. [sent-456, score-0.291]
</p><p>78 We also show some qualitative results obtained with the newly captured clips in Figure 4. [sent-457, score-0.235]
</p><p>79 In the first part, the classification error rates of the 9 clips with strong perspective effects are presented. [sent-459, score-0.558]
</p><p>80 Although ALC and MSMC also reported good results when the number of motion groups is correctly estimated, perspective effects have a significant detrimental impact on their model selection steps, resulting in substantially higher error rates of ALC and MSMC. [sent-461, score-0.702]
</p><p>81 GPCA broke down mainly due to the instability of the Power Factorization method used for filling in missing data. [sent-464, score-0.246]
</p><p>82 Of the only sequence whose motion number is correctly estimated (the “Van” clip, last row of Figure 4), LRR has a very poor classification error rate. [sent-467, score-0.279]
</p><p>83 MSMC failed in those sequences with complicated objects and backgrounds due to its simple motion model based on homography. [sent-468, score-0.283]
</p><p>84 These clips are relatively easy, because they have complete trajectories. [sent-471, score-0.235]
</p><p>85 The average classification error of our method on all 50 clips is 7. [sent-472, score-0.316]
</p><p>86 56% is clearly the best compared to other stateof-the-art motion segmentation algorithms. [sent-476, score-0.328]
</p><p>87 These figures also demonstrate that model selection remains a recalcitrant problem, and to achieve real progress in motion segmentation, we must meet this challenge heads-on. [sent-477, score-0.301]
</p><p>88 It has 46 correct motion number estimation out of 62 clips (next best is 37), and the average classification error of all clips is 7. [sent-479, score-0.749]
</p><p>89 These overall performances demonstrate that our method is capable of handling the var-  ious real challenges in the motion segmentation problem. [sent-482, score-0.359]
</p><p>90 Conclusions We solve the 3D motion segmentation problem of multiple frames rooted in the epipolar geometry of two perspective views via a collaborative clustering algorithm. [sent-484, score-0.773]
</p><p>91 Qualitative  GPCA  LBF  LRR  results of the real data with missing entries. [sent-487, score-0.229]
</p><p>92 MSMC  ORK  SSC  M-TPV  The segmentation results of the 50-th frames of the sequences  are  presented. [sent-488, score-0.276]
</p><p>93 Leveraging on this, we first over-segment the motion groups, and then merge them according to the relationships. [sent-491, score-0.245]
</p><p>94 Multibody factorization with uncertainty and missing data using the em algorithm. [sent-556, score-0.274]
</p><p>95 Two-view motion segmentation with model selection and outlier removal by ransac-enhanced dirichlet process mixture models. [sent-566, score-0.454]
</p><p>96 Track to the future: Spatiotemporal video segmentation with long-range motions cues. [sent-577, score-0.222]
</p><p>97 Robust algebraic segmentation of mixed rigid-body and planar motions from two views. [sent-629, score-0.279]
</p><p>98 A benchmark for the comparison of 3-d motion segmentation algorithms. [sent-665, score-0.328]
</p><p>99 Motion segmentation with missing data by power factorization and generalized pca. [sent-670, score-0.404]
</p><p>100 A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and non-degenerate. [sent-687, score-0.266]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ssc', 0.338), ('clips', 0.235), ('tpv', 0.222), ('wp', 0.201), ('motion', 0.198), ('missing', 0.198), ('subspace', 0.169), ('perspective', 0.163), ('lrr', 0.16), ('groups', 0.148), ('alc', 0.136), ('trajectories', 0.133), ('segmentation', 0.13), ('msmc', 0.123), ('yp', 0.115), ('multibody', 0.113), ('lbf', 0.111), ('rpq', 0.111), ('ork', 0.102), ('gpca', 0.095), ('trajectory', 0.092), ('completion', 0.092), ('motions', 0.092), ('affinity', 0.09), ('merging', 0.087), ('clustering', 0.086), ('sequences', 0.085), ('epipolar', 0.084), ('affine', 0.083), ('effects', 0.079), ('factorization', 0.076), ('ci', 0.074), ('matrix', 0.073), ('selection', 0.072), ('subspaces', 0.072), ('entries', 0.071), ('cci', 0.068), ('degenerate', 0.068), ('cp', 0.067), ('cand', 0.066), ('lsa', 0.064), ('incomplete', 0.063), ('norm', 0.061), ('frames', 0.061), ('mixed', 0.057), ('pairs', 0.057), ('delivery', 0.055), ('grips', 0.055), ('medianim', 0.055), ('outlier', 0.054), ('collaborative', 0.051), ('relationship', 0.051), ('spectral', 0.051), ('singapore', 0.05), ('suzhou', 0.049), ('zhuwen', 0.049), ('corrupted', 0.049), ('pair', 0.049), ('filling', 0.048), ('xp', 0.048), ('merge', 0.047), ('sparse', 0.047), ('correspondences', 0.046), ('rd', 0.045), ('walls', 0.045), ('vidal', 0.045), ('points', 0.044), ('soltanolkotabi', 0.043), ('perpendicular', 0.042), ('error', 0.042), ('coefficient', 0.042), ('rigid', 0.041), ('lj', 0.041), ('separation', 0.04), ('sod', 0.039), ('pk', 0.039), ('classification', 0.039), ('clip', 0.038), ('sparsest', 0.038), ('group', 0.037), ('enter', 0.037), ('columns', 0.036), ('integrates', 0.036), ('column', 0.035), ('eigenvalues', 0.035), ('manner', 0.035), ('merged', 0.034), ('denser', 0.034), ('rao', 0.034), ('partly', 0.033), ('suffer', 0.033), ('adequately', 0.033), ('tron', 0.033), ('jl', 0.033), ('diag', 0.033), ('laplacian', 0.032), ('aforementioned', 0.032), ('real', 0.031), ('relaxed', 0.031), ('ijcv', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="314-tfidf-1" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>Author: Zhuwen Li, Jiaming Guo, Loong-Fah Cheong, Steven Zhiying Zhou</p><p>Abstract: This paper addresses real-world challenges in the motion segmentation problem, including perspective effects, missing data, and unknown number of motions. It first formulates the 3-D motion segmentation from two perspective views as a subspace clustering problem, utilizing the epipolar constraint of an image pair. It then combines the point correspondence information across multiple image frames via a collaborative clustering step, in which tight integration is achieved via a mixed norm optimization scheme. For model selection, wepropose an over-segment and merge approach, where the merging step is based on the property of the ?1-norm ofthe mutual sparse representation oftwo oversegmented groups. The resulting algorithm can deal with incomplete trajectories and perspective effects substantially better than state-of-the-art two-frame and multi-frame methods. Experiments on a 62-clip dataset show the significant superiority of the proposed idea in both segmentation accuracy and model selection.</p><p>2 0.39075002 <a title="314-tfidf-2" href="./iccv-2013-Latent_Space_Sparse_Subspace_Clustering.html">232 iccv-2013-Latent Space Sparse Subspace Clustering</a></p>
<p>Author: Vishal M. Patel, Hien Van Nguyen, René Vidal</p><p>Abstract: We propose a novel algorithm called Latent Space Sparse Subspace Clustering for simultaneous dimensionality reduction and clustering of data lying in a union of subspaces. Specifically, we describe a method that learns the projection of data and finds the sparse coefficients in the low-dimensional latent space. Cluster labels are then assigned by applying spectral clustering to a similarity matrix built from these sparse coefficients. An efficient optimization method is proposed and its non-linear extensions based on the kernel methods are presented. One of the main advantages of our method is that it is computationally efficient as the sparse coefficients are found in the low-dimensional latent space. Various experiments show that the proposed method performs better than the competitive state-of-theart subspace clustering methods.</p><p>3 0.34031677 <a title="314-tfidf-3" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>Author: Feng Shi, Zhong Zhou, Jiangjian Xiao, Wei Wu</p><p>Abstract: Due to occlusions and objects ’ non-rigid deformation in the scene, the obtained motion trajectories from common trackers may contain a number of missing or mis-associated entries. To cluster such corrupted point based trajectories into multiple motions is still a hard problem. In this paper, we present an approach that exploits temporal and spatial characteristics from tracked points to facilitate segmentation of incomplete and corrupted trajectories, thereby obtain highly robust results against severe data missing and noises. Our method first uses the Discrete Cosine Transform (DCT) bases as a temporal smoothness constraint on trajectory projection to ensure the validity of resulting components to repair pathological trajectories. Then, based on an observation that the trajectories of foreground and background in a scene may have different spatial distributions, we propose a two-stage clustering strategy that first performs foreground-background separation then segments remaining foreground trajectories. We show that, with this new clustering strategy, sequences with complex motions can be accurately segmented by even using a simple trans- lational model. Finally, a series of experiments on Hopkins 155 dataset andBerkeley motion segmentation dataset show the advantage of our method over other state-of-the-art motion segmentation algorithms in terms of both effectiveness and robustness.</p><p>4 0.29784995 <a title="314-tfidf-4" href="./iccv-2013-Robust_Subspace_Clustering_via_Half-Quadratic_Minimization.html">360 iccv-2013-Robust Subspace Clustering via Half-Quadratic Minimization</a></p>
<p>Author: Yingya Zhang, Zhenan Sun, Ran He, Tieniu Tan</p><p>Abstract: Subspace clustering has important and wide applications in computer vision and pattern recognition. It is a challenging task to learn low-dimensional subspace structures due to the possible errors (e.g., noise and corruptions) existing in high-dimensional data. Recent subspace clustering methods usually assume a sparse representation of corrupted errors and correct the errors iteratively. However large corruptions in real-world applications can not be well addressed by these methods. A novel optimization model for robust subspace clustering is proposed in this paper. The objective function of our model mainly includes two parts. The first part aims to achieve a sparse representation of each high-dimensional data point with other data points. The second part aims to maximize the correntropy between a given data point and its low-dimensional representation with other points. Correntropy is a robust measure so that the influence of large corruptions on subspace clustering can be greatly suppressed. An extension of our method with explicit introduction of representation error terms into the model is also proposed. Half-quadratic minimization is provided as an efficient solution to the proposed robust subspace clustering formulations. Experimental results on Hopkins 155 dataset and Extended Yale Database B demonstrate that our method outperforms state-of-the-art subspace clustering methods.</p><p>5 0.28490415 <a title="314-tfidf-5" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<p>Author: Ali Elqursh, Ahmed Elgammal</p><p>Abstract: The vast majority of work on motion segmentation adopts the affine camera model due to its simplicity. Under the affine model, the motion segmentation problem becomes that of subspace separation. Due to this assumption, such methods are mainly offline and exhibit poor performance when the assumption is not satisfied. This is made evident in state-of-the-art methods that relax this assumption by using piecewise affine spaces and spectral clustering techniques to achieve better results. In this paper, we formulate the problem of motion segmentation as that of manifold separation. We then show how label propagation can be used in an online framework to achieve manifold separation. The performance of our framework is evaluated on a benchmark dataset and achieves competitive performance while being online.</p><p>6 0.25648779 <a title="314-tfidf-6" href="./iccv-2013-Distributed_Low-Rank_Subspace_Segmentation.html">122 iccv-2013-Distributed Low-Rank Subspace Segmentation</a></p>
<p>7 0.23467964 <a title="314-tfidf-7" href="./iccv-2013-Correlation_Adaptive_Subspace_Segmentation_by_Trace_Lasso.html">93 iccv-2013-Correlation Adaptive Subspace Segmentation by Trace Lasso</a></p>
<p>8 0.18308905 <a title="314-tfidf-8" href="./iccv-2013-Minimal_Basis_Facility_Location_for_Subspace_Segmentation.html">264 iccv-2013-Minimal Basis Facility Location for Subspace Segmentation</a></p>
<p>9 0.17443666 <a title="314-tfidf-9" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>10 0.16657963 <a title="314-tfidf-10" href="./iccv-2013-Correntropy_Induced_L2_Graph_for_Robust_Subspace_Clustering.html">94 iccv-2013-Correntropy Induced L2 Graph for Robust Subspace Clustering</a></p>
<p>11 0.16399513 <a title="314-tfidf-11" href="./iccv-2013-Efficient_Higher-Order_Clustering_on_the_Grassmann_Manifold.html">134 iccv-2013-Efficient Higher-Order Clustering on the Grassmann Manifold</a></p>
<p>12 0.15586744 <a title="314-tfidf-12" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>13 0.13989253 <a title="314-tfidf-13" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>14 0.13952102 <a title="314-tfidf-14" href="./iccv-2013-GOSUS%3A_Grassmannian_Online_Subspace_Updates_with_Structured-Sparsity.html">182 iccv-2013-GOSUS: Grassmannian Online Subspace Updates with Structured-Sparsity</a></p>
<p>15 0.13658161 <a title="314-tfidf-15" href="./iccv-2013-Unifying_Nuclear_Norm_and_Bilinear_Factorization_Approaches_for_Low-Rank_Matrix_Decomposition.html">434 iccv-2013-Unifying Nuclear Norm and Bilinear Factorization Approaches for Low-Rank Matrix Decomposition</a></p>
<p>16 0.12685767 <a title="314-tfidf-16" href="./iccv-2013-Stable_Hyper-pooling_and_Query_Expansion_for_Event_Detection.html">400 iccv-2013-Stable Hyper-pooling and Query Expansion for Event Detection</a></p>
<p>17 0.12611754 <a title="314-tfidf-17" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>18 0.12013207 <a title="314-tfidf-18" href="./iccv-2013-Camera_Alignment_Using_Trajectory_Intersections_in_Unsynchronized_Videos.html">68 iccv-2013-Camera Alignment Using Trajectory Intersections in Unsynchronized Videos</a></p>
<p>19 0.11982746 <a title="314-tfidf-19" href="./iccv-2013-Video_Synopsis_by_Heterogeneous_Multi-source_Correlation.html">443 iccv-2013-Video Synopsis by Heterogeneous Multi-source Correlation</a></p>
<p>20 0.10515301 <a title="314-tfidf-20" href="./iccv-2013-A_Unified_Video_Segmentation_Benchmark%3A_Annotation%2C_Metrics_and_Analysis.html">33 iccv-2013-A Unified Video Segmentation Benchmark: Annotation, Metrics and Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.265), (1, -0.06), (2, -0.043), (3, 0.112), (4, -0.178), (5, 0.204), (6, -0.004), (7, 0.219), (8, 0.27), (9, 0.08), (10, 0.139), (11, 0.046), (12, -0.147), (13, -0.005), (14, -0.128), (15, -0.023), (16, 0.005), (17, 0.029), (18, 0.018), (19, 0.062), (20, -0.097), (21, 0.128), (22, -0.032), (23, 0.029), (24, -0.027), (25, -0.068), (26, -0.001), (27, 0.045), (28, -0.023), (29, -0.052), (30, 0.001), (31, 0.007), (32, 0.036), (33, 0.004), (34, -0.023), (35, -0.04), (36, -0.03), (37, 0.022), (38, 0.029), (39, 0.048), (40, -0.044), (41, 0.007), (42, 0.005), (43, 0.027), (44, 0.023), (45, -0.017), (46, 0.02), (47, -0.007), (48, 0.006), (49, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95722091 <a title="314-lsi-1" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>Author: Zhuwen Li, Jiaming Guo, Loong-Fah Cheong, Steven Zhiying Zhou</p><p>Abstract: This paper addresses real-world challenges in the motion segmentation problem, including perspective effects, missing data, and unknown number of motions. It first formulates the 3-D motion segmentation from two perspective views as a subspace clustering problem, utilizing the epipolar constraint of an image pair. It then combines the point correspondence information across multiple image frames via a collaborative clustering step, in which tight integration is achieved via a mixed norm optimization scheme. For model selection, wepropose an over-segment and merge approach, where the merging step is based on the property of the ?1-norm ofthe mutual sparse representation oftwo oversegmented groups. The resulting algorithm can deal with incomplete trajectories and perspective effects substantially better than state-of-the-art two-frame and multi-frame methods. Experiments on a 62-clip dataset show the significant superiority of the proposed idea in both segmentation accuracy and model selection.</p><p>2 0.92971981 <a title="314-lsi-2" href="./iccv-2013-Minimal_Basis_Facility_Location_for_Subspace_Segmentation.html">264 iccv-2013-Minimal Basis Facility Location for Subspace Segmentation</a></p>
<p>Author: Choon-Meng Lee, Loong-Fah Cheong</p><p>Abstract: In contrast to the current motion segmentation paradigm that assumes independence between the motion subspaces, we approach the motion segmentation problem by seeking the parsimonious basis set that can represent the data. Our formulation explicitly looks for the overlap between subspaces in order to achieve a minimal basis representation. This parsimonious basis set is important for the performance of our model selection scheme because the sharing of basis results in savings of model complexity cost. We propose the use of affinity propagation based method to determine the number of motion. The key lies in the incorporation of a global cost model into the factor graph, serving the role of model complexity. The introduction of this global cost model requires additional message update in the factor graph. We derive an efficient update for the new messages associated with this global cost model. An important step in the use of affinity propagation is the subspace hypotheses generation. We use the row-sparse convex proxy solution as an initialization strategy. We further encourage the selection of subspace hypotheses with shared basis by integrat- ing a discount scheme that lowers the factor graph facility cost based on shared basis. We verified the model selection and classification performance of our proposed method on both the original Hopkins 155 dataset and the more balanced Hopkins 380 dataset.</p><p>3 0.8588106 <a title="314-lsi-3" href="./iccv-2013-Latent_Space_Sparse_Subspace_Clustering.html">232 iccv-2013-Latent Space Sparse Subspace Clustering</a></p>
<p>Author: Vishal M. Patel, Hien Van Nguyen, René Vidal</p><p>Abstract: We propose a novel algorithm called Latent Space Sparse Subspace Clustering for simultaneous dimensionality reduction and clustering of data lying in a union of subspaces. Specifically, we describe a method that learns the projection of data and finds the sparse coefficients in the low-dimensional latent space. Cluster labels are then assigned by applying spectral clustering to a similarity matrix built from these sparse coefficients. An efficient optimization method is proposed and its non-linear extensions based on the kernel methods are presented. One of the main advantages of our method is that it is computationally efficient as the sparse coefficients are found in the low-dimensional latent space. Various experiments show that the proposed method performs better than the competitive state-of-theart subspace clustering methods.</p><p>4 0.84099913 <a title="314-lsi-4" href="./iccv-2013-Robust_Subspace_Clustering_via_Half-Quadratic_Minimization.html">360 iccv-2013-Robust Subspace Clustering via Half-Quadratic Minimization</a></p>
<p>Author: Yingya Zhang, Zhenan Sun, Ran He, Tieniu Tan</p><p>Abstract: Subspace clustering has important and wide applications in computer vision and pattern recognition. It is a challenging task to learn low-dimensional subspace structures due to the possible errors (e.g., noise and corruptions) existing in high-dimensional data. Recent subspace clustering methods usually assume a sparse representation of corrupted errors and correct the errors iteratively. However large corruptions in real-world applications can not be well addressed by these methods. A novel optimization model for robust subspace clustering is proposed in this paper. The objective function of our model mainly includes two parts. The first part aims to achieve a sparse representation of each high-dimensional data point with other data points. The second part aims to maximize the correntropy between a given data point and its low-dimensional representation with other points. Correntropy is a robust measure so that the influence of large corruptions on subspace clustering can be greatly suppressed. An extension of our method with explicit introduction of representation error terms into the model is also proposed. Half-quadratic minimization is provided as an efficient solution to the proposed robust subspace clustering formulations. Experimental results on Hopkins 155 dataset and Extended Yale Database B demonstrate that our method outperforms state-of-the-art subspace clustering methods.</p><p>5 0.83311099 <a title="314-lsi-5" href="./iccv-2013-Correlation_Adaptive_Subspace_Segmentation_by_Trace_Lasso.html">93 iccv-2013-Correlation Adaptive Subspace Segmentation by Trace Lasso</a></p>
<p>Author: Canyi Lu, Jiashi Feng, Zhouchen Lin, Shuicheng Yan</p><p>Abstract: This paper studies the subspace segmentation problem. Given a set of data points drawn from a union of subspaces, the goal is to partition them into their underlying subspaces they were drawn from. The spectral clustering method is used as the framework. It requires to find an affinity matrix which is close to block diagonal, with nonzero entries corresponding to the data point pairs from the same subspace. In this work, we argue that both sparsity and the grouping effect are important for subspace segmentation. A sparse affinity matrix tends to be block diagonal, with less connections between data points from different subspaces. The grouping effect ensures that the highly corrected data which are usually from the same subspace can be grouped together. Sparse Subspace Clustering (SSC), by using ?1-minimization, encourages sparsity for data selection, but it lacks of the grouping effect. On the contrary, Low-RankRepresentation (LRR), by rank minimization, and Least Squares Regression (LSR), by ?2-regularization, exhibit strong grouping effect, but they are short in subset selection. Thus the obtained affinity matrix is usually very sparse by SSC, yet very dense by LRR and LSR. In this work, we propose the Correlation Adaptive Subspace Segmentation (CASS) method by using trace Lasso. CASS is a data correlation dependent method which simultaneously performs automatic data selection and groups correlated data together. It can be regarded as a method which adaptively balances SSC and LSR. Both theoretical and experimental results show the effectiveness of CASS.</p><p>6 0.82740062 <a title="314-lsi-6" href="./iccv-2013-Distributed_Low-Rank_Subspace_Segmentation.html">122 iccv-2013-Distributed Low-Rank Subspace Segmentation</a></p>
<p>7 0.78704083 <a title="314-lsi-7" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>8 0.75719601 <a title="314-lsi-8" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>9 0.7537818 <a title="314-lsi-9" href="./iccv-2013-Correntropy_Induced_L2_Graph_for_Robust_Subspace_Clustering.html">94 iccv-2013-Correntropy Induced L2 Graph for Robust Subspace Clustering</a></p>
<p>10 0.74607199 <a title="314-lsi-10" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<p>11 0.7380482 <a title="314-lsi-11" href="./iccv-2013-Efficient_Higher-Order_Clustering_on_the_Grassmann_Manifold.html">134 iccv-2013-Efficient Higher-Order Clustering on the Grassmann Manifold</a></p>
<p>12 0.71460867 <a title="314-lsi-12" href="./iccv-2013-GOSUS%3A_Grassmannian_Online_Subspace_Updates_with_Structured-Sparsity.html">182 iccv-2013-GOSUS: Grassmannian Online Subspace Updates with Structured-Sparsity</a></p>
<p>13 0.62609375 <a title="314-lsi-13" href="./iccv-2013-Progressive_Multigrid_Eigensolvers_for_Multiscale_Spectral_Segmentation.html">329 iccv-2013-Progressive Multigrid Eigensolvers for Multiscale Spectral Segmentation</a></p>
<p>14 0.6064949 <a title="314-lsi-14" href="./iccv-2013-Optimal_Orthogonal_Basis_and_Image_Assimilation%3A_Motion_Modeling.html">301 iccv-2013-Optimal Orthogonal Basis and Image Assimilation: Motion Modeling</a></p>
<p>15 0.52215576 <a title="314-lsi-15" href="./iccv-2013-Robust_Matrix_Factorization_with_Unknown_Noise.html">357 iccv-2013-Robust Matrix Factorization with Unknown Noise</a></p>
<p>16 0.51394093 <a title="314-lsi-16" href="./iccv-2013-Finding_Causal_Interactions_in_Video_Sequences.html">167 iccv-2013-Finding Causal Interactions in Video Sequences</a></p>
<p>17 0.4932138 <a title="314-lsi-17" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>18 0.48879114 <a title="314-lsi-18" href="./iccv-2013-Camera_Alignment_Using_Trajectory_Intersections_in_Unsynchronized_Videos.html">68 iccv-2013-Camera Alignment Using Trajectory Intersections in Unsynchronized Videos</a></p>
<p>19 0.47229436 <a title="314-lsi-19" href="./iccv-2013-Measuring_Flow_Complexity_in_Videos.html">263 iccv-2013-Measuring Flow Complexity in Videos</a></p>
<p>20 0.46963075 <a title="314-lsi-20" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.07), (7, 0.019), (12, 0.014), (19, 0.158), (26, 0.073), (27, 0.01), (31, 0.074), (42, 0.144), (48, 0.021), (64, 0.044), (73, 0.049), (74, 0.013), (78, 0.011), (89, 0.183), (95, 0.015), (98, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9331125 <a title="314-lda-1" href="./iccv-2013-Learning_Slow_Features_for_Behaviour_Analysis.html">243 iccv-2013-Learning Slow Features for Behaviour Analysis</a></p>
<p>Author: Lazaros Zafeiriou, Mihalis A. Nicolaou, Stefanos Zafeiriou, Symeon Nikitidis, Maja Pantic</p><p>Abstract: A recently introduced latent feature learning technique for time varying dynamic phenomena analysis is the socalled Slow Feature Analysis (SFA). SFA is a deterministic component analysis technique for multi-dimensional sequences that by minimizing the variance of the first order time derivative approximation of the input signal finds uncorrelated projections that extract slowly-varying features ordered by their temporal consistency and constancy. In this paper, we propose a number of extensions in both the deterministic and the probabilistic SFA optimization frameworks. In particular, we derive a novel deterministic SFA algorithm that is able to identify linear projections that extract the common slowest varying features of two or more sequences. In addition, we propose an Expectation Maximization (EM) algorithm to perform inference in a probabilistic formulation of SFA and similarly extend it in order to handle two and more time varying data sequences. Moreover, we demonstrate that the probabilistic SFA (EMSFA) algorithm that discovers the common slowest varying latent space of multiple sequences can be combined with dynamic time warping techniques for robust sequence timealignment. The proposed SFA algorithms were applied for facial behavior analysis demonstrating their usefulness and appropriateness for this task.</p><p>2 0.90067637 <a title="314-lda-2" href="./iccv-2013-The_Way_They_Move%3A_Tracking_Multiple_Targets_with_Similar_Appearance.html">418 iccv-2013-The Way They Move: Tracking Multiple Targets with Similar Appearance</a></p>
<p>Author: Caglayan Dicle, Octavia I. Camps, Mario Sznaier</p><p>Abstract: We introduce a computationally efficient algorithm for multi-object tracking by detection that addresses four main challenges: appearance similarity among targets, missing data due to targets being out of the field of view or occluded behind other objects, crossing trajectories, and camera motion. The proposed method uses motion dynamics as a cue to distinguish targets with similar appearance, minimize target mis-identification and recover missing data. Computational efficiency is achieved by using a Generalized Linear Assignment (GLA) coupled with efficient procedures to recover missing data and estimate the complexity of the underlying dynamics. The proposed approach works with tracklets of arbitrary length and does not assume a dynamical model a priori, yet it captures the overall motion dynamics of the targets. Experiments using challenging videos show that this framework can handle complex target motions, non-stationary cameras and long occlusions, on scenarios where appearance cues are not available or poor.</p><p>same-paper 3 0.8893733 <a title="314-lda-3" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>Author: Zhuwen Li, Jiaming Guo, Loong-Fah Cheong, Steven Zhiying Zhou</p><p>Abstract: This paper addresses real-world challenges in the motion segmentation problem, including perspective effects, missing data, and unknown number of motions. It first formulates the 3-D motion segmentation from two perspective views as a subspace clustering problem, utilizing the epipolar constraint of an image pair. It then combines the point correspondence information across multiple image frames via a collaborative clustering step, in which tight integration is achieved via a mixed norm optimization scheme. For model selection, wepropose an over-segment and merge approach, where the merging step is based on the property of the ?1-norm ofthe mutual sparse representation oftwo oversegmented groups. The resulting algorithm can deal with incomplete trajectories and perspective effects substantially better than state-of-the-art two-frame and multi-frame methods. Experiments on a 62-clip dataset show the significant superiority of the proposed idea in both segmentation accuracy and model selection.</p><p>4 0.88499367 <a title="314-lda-4" href="./iccv-2013-Semi-supervised_Learning_for_Large_Scale_Image_Cosegmentation.html">383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</a></p>
<p>Author: Zhengxiang Wang, Rujie Liu</p><p>Abstract: This paper introduces to use semi-supervised learning for large scale image cosegmentation. Different from traditional unsupervised cosegmentation that does not use any segmentation groundtruth, semi-supervised cosegmentation exploits the similarity from both the very limited training image foregrounds, as well as the common object shared between the large number of unsegmented images. This would be a much practical way to effectively cosegment a large number of related images simultaneously, where previous unsupervised cosegmentation work poorly due to the large variances in appearance between different images and the lack ofsegmentation groundtruthfor guidance in cosegmentation. For semi-supervised cosegmentation in large scale, we propose an effective method by minimizing an energy function, which consists of the inter-image distance, the intraimage distance and the balance term. We also propose an iterative updating algorithm to efficiently solve this energy function, which decomposes the original energy minimization problem into sub-problems, and updates each image alternatively to reduce the number of variables in each subproblem for computation efficiency. Experiment results on iCoseg and Pascal VOC datasets show that the proposed cosegmentation method can effectively cosegment hundreds of images in less than one minute. And our semi-supervised cosegmentation is able to outperform both unsupervised cosegmentation as well asfully supervised single image segmentation, especially when the training data is limited.</p><p>5 0.87758178 <a title="314-lda-5" href="./iccv-2013-Saliency_Detection_via_Absorbing_Markov_Chain.html">371 iccv-2013-Saliency Detection via Absorbing Markov Chain</a></p>
<p>Author: Bowen Jiang, Lihe Zhang, Huchuan Lu, Chuan Yang, Ming-Hsuan Yang</p><p>Abstract: In this paper, we formulate saliency detection via absorbing Markov chain on an image graph model. We jointly consider the appearance divergence and spatial distribution of salient objects and the background. The virtual boundary nodes are chosen as the absorbing nodes in a Markov chain and the absorbed time from each transient node to boundary absorbing nodes is computed. The absorbed time of transient node measures its global similarity with all absorbing nodes, and thus salient objects can be consistently separated from the background when the absorbed time is used as a metric. Since the time from transient node to absorbing nodes relies on the weights on the path and their spatial distance, the background region on the center of image may be salient. We further exploit the equilibrium distribution in an ergodic Markov chain to reduce the absorbed time in the long-range smooth background regions. Extensive experiments on four benchmark datasets demonstrate robustness and efficiency of the proposed method against the state-of-the-art methods.</p><p>6 0.85091603 <a title="314-lda-6" href="./iccv-2013-Probabilistic_Elastic_Part_Model_for_Unsupervised_Face_Detector_Adaptation.html">328 iccv-2013-Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation</a></p>
<p>7 0.8481043 <a title="314-lda-7" href="./iccv-2013-Handling_Occlusions_with_Franken-Classifiers.html">190 iccv-2013-Handling Occlusions with Franken-Classifiers</a></p>
<p>8 0.84491003 <a title="314-lda-8" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>9 0.84386551 <a title="314-lda-9" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>10 0.84379828 <a title="314-lda-10" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>11 0.84324837 <a title="314-lda-11" href="./iccv-2013-Bayesian_Joint_Topic_Modelling_for_Weakly_Supervised_Object_Localisation.html">59 iccv-2013-Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation</a></p>
<p>12 0.84290445 <a title="314-lda-12" href="./iccv-2013-Distributed_Low-Rank_Subspace_Segmentation.html">122 iccv-2013-Distributed Low-Rank Subspace Segmentation</a></p>
<p>13 0.84284842 <a title="314-lda-13" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>14 0.8427546 <a title="314-lda-14" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>15 0.84224808 <a title="314-lda-15" href="./iccv-2013-Coherent_Object_Detection_with_3D_Geometric_Context_from_a_Single_Image.html">79 iccv-2013-Coherent Object Detection with 3D Geometric Context from a Single Image</a></p>
<p>16 0.84220713 <a title="314-lda-16" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>17 0.84203875 <a title="314-lda-17" href="./iccv-2013-Group_Norm_for_Learning_Structured_SVMs_with_Unstructured_Latent_Variables.html">187 iccv-2013-Group Norm for Learning Structured SVMs with Unstructured Latent Variables</a></p>
<p>18 0.84199387 <a title="314-lda-18" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>19 0.84181696 <a title="314-lda-19" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>20 0.84147644 <a title="314-lda-20" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
