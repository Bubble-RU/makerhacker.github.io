<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-361" href="#">iccv2013-361</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</h1>
<br/><p>Source: <a title="iccv-2013-361-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Shi_Robust_Trajectory_Clustering_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Feng Shi, Zhong Zhou, Jiangjian Xiao, Wei Wu</p><p>Abstract: Due to occlusions and objects ’ non-rigid deformation in the scene, the obtained motion trajectories from common trackers may contain a number of missing or mis-associated entries. To cluster such corrupted point based trajectories into multiple motions is still a hard problem. In this paper, we present an approach that exploits temporal and spatial characteristics from tracked points to facilitate segmentation of incomplete and corrupted trajectories, thereby obtain highly robust results against severe data missing and noises. Our method first uses the Discrete Cosine Transform (DCT) bases as a temporal smoothness constraint on trajectory projection to ensure the validity of resulting components to repair pathological trajectories. Then, based on an observation that the trajectories of foreground and background in a scene may have different spatial distributions, we propose a two-stage clustering strategy that first performs foreground-background separation then segments remaining foreground trajectories. We show that, with this new clustering strategy, sequences with complex motions can be accurately segmented by even using a simple trans- lational model. Finally, a series of experiments on Hopkins 155 dataset andBerkeley motion segmentation dataset show the advantage of our method over other state-of-the-art motion segmentation algorithms in terms of both effectiveness and robustness.</p><p>Reference: <a title="iccv-2013-361-reference" href="../iccv2013_reference/iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 To cluster such corrupted point based trajectories into multiple motions is still a hard problem. [sent-2, score-0.976]
</p><p>2 In this paper, we present an approach that exploits temporal and spatial characteristics from tracked points to facilitate segmentation of incomplete and corrupted trajectories, thereby obtain highly robust results against severe data missing and noises. [sent-3, score-0.672]
</p><p>3 Our method first uses the Discrete Cosine Transform (DCT) bases as a temporal smoothness constraint on trajectory projection to ensure the validity of resulting components to repair pathological trajectories. [sent-4, score-0.5]
</p><p>4 Then, based on an observation that the trajectories of foreground and background in a scene may have different spatial distributions, we propose a two-stage clustering strategy that first performs foreground-background separation then segments remaining foreground trajectories. [sent-5, score-1.152]
</p><p>5 We show that, with this new clustering strategy, sequences with complex motions can be accurately segmented by even using a simple trans-  lational model. [sent-6, score-0.372]
</p><p>6 Finally, a series of experiments on Hopkins 155 dataset andBerkeley motion segmentation dataset show the advantage of our method over other state-of-the-art motion segmentation algorithms in terms of both effectiveness and robustness. [sent-7, score-0.55]
</p><p>7 That is, given a set of feature point trajectories tracked from a video, one seeks to cluster the trajectories according to their corresponding motions. [sent-11, score-1.206]
</p><p>8 Due to containing long-term motion cues, feature trajectories are very suitable to be used to partition video into ∗Corresponding author. [sent-12, score-0.748]
</p><p>9 Therefore, the input trajectories to motion segmentation algorithm are often contaminated by missing and corrupted entries. [sent-19, score-1.23]
</p><p>10 For solving this problem, some methods [7, 10, 13] are proposed to repair the pathological trajectories by computing their sparse or low-rank representation  with respect to a dictionary formed by all other trajectories. [sent-21, score-0.745]
</p><p>11 However, this kind of algorithms has an inherent drawback that requires a sufficiently large set of complete and uncorrupted trajectories, thus it becomes unsuitable for the real sequences which may have a large number of missing and corrupted data. [sent-22, score-0.65]
</p><p>12 Another kind of motion segmentation algorithms [8, 4, 3, 9, 12, 11, 6] which does not require any completion of trajectories is recently proposed, where certain motion models are used for trajectory clustering. [sent-23, score-1.187]
</p><p>13 Thus, although it has significant advantage in handling incomplete trajectories, they may still fail on the sequences when the objects motion deviates the employed motion models in the algorithms. [sent-24, score-0.556]
</p><p>14 Obviously, there is a need to develop a motion segmentation algorithm that can handle not only the missing and corrupted data but also complex motions both typically of real tracking process. [sent-25, score-0.8]
</p><p>15 We notice that trajectories of feature points from most nature deforming objects are smooth and continuous. [sent-28, score-0.615]
</p><p>16 Therefore, it implies that such temporal smoothness of feature trajectories can be compactly represented by predefined basis vectors. [sent-29, score-0.754]
</p><p>17 Based on this analysis, we select DCT as predefined bases to approximate feature trajectories, and then design a non-linear optimization  scheme that can effectively decompose the input trajectories into a set of DCT basis vectors and corresponding coefficients. [sent-30, score-0.78]
</p><p>18 We also notice that trajectories of foreground and background may have different characteristics in spatial distribution, that is, background trajectories (i. [sent-32, score-1.508]
</p><p>19 static parts of a scene) are usually dispersed all over the scene while foreground trajectories (i. [sent-34, score-0.747]
</p><p>20 Based on this observation, we present a two-stage clustering strategy which first separates foreground trajectories from background trajectories based on motion subspaces constraints and then divides the foreground trajectories into different partitions using spectral clustering. [sent-37, score-2.588]
</p><p>21 By this way, our method can even use a simple translational model to obtain highly robust segmentation results for those sequences with complicated rigid or nonrigid object motions. [sent-38, score-0.421]
</p><p>22 Finally, we evaluate our method and state-of-the-art trajectory clustering algorithms on both the Hopkins 155 dataset [16] and the Berkeley motion segmentation dataset [3]. [sent-39, score-0.56]
</p><p>23 Related Work During the past two decades, numerous trajectory clustering algorithms have been proposed for motion segmentation. [sent-43, score-0.424]
</p><p>24 Most early subspace-based methods, such as [5, 17], assume that all input trajectories are complete and do not contain gross errors. [sent-48, score-0.611]
</p><p>25 However, the tracking failure of feature points is very common in real-world automatic tracking, causing trajectories to have missing entries (incomplete trajectories) or some entries with gross errors (corrupted trajectories). [sent-49, score-0.945]
</p><p>26 [13] proposed Agglomerative Lossy Compression (ALC) that repairs a trajectory with missing or corrupted entries prior to subspace separation by  computing its sparse representation with respect to all other trajectories. [sent-51, score-0.699]
</p><p>27 The method proposed by Elhamifar and Vidal [7], known as sparse subspace clustering (SSC), uses a similar strategy as ALC to handle pathological trajectories, and uses the sparse coefficients of trajectories to build the affinity matrix for spectral clustering. [sent-52, score-0.971]
</p><p>28 The three methods, although have been successfully used to segment sequences with a certain amount of missing and corrupted data, have an inherent drawback, which requires an assumption that each motion should have a sufficiently large subset of complete and uncorrupted trajectories. [sent-55, score-0.786]
</p><p>29 In recent years, a few trajectory clustering methods [8, 4, 3, 9, 12, 11, 6] which do not rely on motion subspaces constraints are proposed for motion segmentation. [sent-57, score-0.63]
</p><p>30 These methods usually utilize a motion model to measure similarities between all trajectories, and then perform a common clustering technique to segment trajectories. [sent-58, score-0.348]
</p><p>31 Among them, [4, 3, 9, 11] use the velocity information to represent trajectories similarities, thus are translational model-based methods. [sent-59, score-0.761]
</p><p>32 [8, 12, 6] use higher order motion models to measure trajectories similarities. [sent-60, score-0.748]
</p><p>33 Compared to multi-body factorization methods, the motion model-based methods have signif-  icant advantage in handling incomplete trajectories because they do not require any completion of the input trajectories. [sent-61, score-0.959]
</p><p>34 In [8, 3, 9, 12, 6], similarities between trajectories are computed only using their available entries. [sent-62, score-0.615]
</p><p>35 In [4, 11], an iterative optimization algorithm is proposed to decompose a velocity matrix computed from the incomplete trajectories, and the resulting components are used as representations of trajectories for clustering. [sent-63, score-0.763]
</p><p>36 A limitation of this kind of method is that it can only compute the similarity of trajectories based on the underlying motion model. [sent-64, score-0.778]
</p><p>37 As a result, it will often lead to poor performance when they were applied to segment sequences containing motions that deviate from their motion models. [sent-65, score-0.432]
</p><p>38 Proposed Algorithm In this paper, we suppose that trajectories of P feature points have been obtained by running some existing trackers, e. [sent-67, score-0.589]
</p><p>39 Our goal is to partition the P trajectories into different groups according to their corresponding motions. [sent-74, score-0.589]
</p><p>40 Generally speaking, the difficulty of obtaining desirable results by factorizing a matrix depends heavily on the quality of this matrix, which can be measured by the amount of its missing and corrupted data. [sent-113, score-0.413]
</p><p>41 Specific to our method, in the presence of considerable quantities of incomplete and corrupted trajectories from automatic feature tracking, it is challenging to decompose W into components to capture the inherent similarity of trajectories. [sent-114, score-1.032]
</p><p>42 There are a number of predefined bases which can approximate smooth signals compactly, and among them, the DCT bases have been proved to be particularly suitable for motion trajectories [1, 2]. [sent-120, score-0.999]
</p><p>43 (a)&(c) Original trajectories & trajectories with 50% missing and 100% corrupted entries (adding Gaussian noise with zero mean and variance 0. [sent-195, score-1.638]
</p><p>44 (b)&(d) Visualization of two-dimensional Laplacian eigenmaps projection of C(p) of the trajectories in (a)&(c) respectively. [sent-200, score-0.648]
</p><p>45 From C =  VT∈,  DR,r w×2eP d,e  (D−1UT)S,  it can be deduced that C(p) = ((c2p−1)T, (c2p)T)T is the weighted sum of spatial and translational information of T(p), thus can be used to distinguish trajectories belonging to different translational motions. [sent-210, score-0.967]
</p><p>46 , P, the three translational motions can be separated from each other clearly even with severe data missing and noises. [sent-219, score-0.437]
</p><p>47 , P to distinguish trajectories belonging to different non-translational motions? [sent-223, score-0.623]
</p><p>48 To answer this question, we need to classify trajectories as foreground and background, which are respectively induced by the motions of camera and objects in the scene. [sent-224, score-0.866]
</p><p>49 Note that foreground and background trajectories have different spatial characteristics, that is, background trajectories are usually distributed over the scene while foreground trajectories belonging to the same motion are usually spatially close to each other. [sent-225, score-2.416]
</p><p>50 To show the differences of foreground and background trajectories resulted from their spatial characteristics, we plot the computed S and C(p) ,p = 1, . [sent-226, score-0.817]
</p><p>51 (a) Input trajectories and ground-truth segmentation, red and green denote foreground clusters while blue denote background cluster. [sent-241, score-0.863]
</p><p>52 , sdT)T for foreground trajectories & foreground trajectories and background trajectories in rectangle ‘A’, ‘B’, and ‘C’ . [sent-246, score-2.153]
</p><p>53 It can be seen from Figure 3(b) and 3(c) that the spatial and translational information both exhibit tight and well-separated clusters for foreground trajectories. [sent-252, score-0.376]
</p><p>54 , P can easily distinguish trajectories belonging to different non-translational foreground motions. [sent-256, score-0.781]
</p><p>55 However, this argument is unsuitable for background trajectories since they are usually distributed in space. [sent-259, score-0.684]
</p><p>56 From Figure 3(e), we can see that the translational information of background trajectories in rectangle ‘A’ , ‘B’, and ‘C’ approaches that of foreground trajectories. [sent-260, score-0.989]
</p><p>57 , P to cluster trajectories in a video sequence with non-translational background motion. [sent-264, score-0.744]
</p><p>58 To circumvent this problem, we have developed the following two-stage clustering, which first separates foreground from background based on motion subspaces constraints then segments foreground using spectral clustering, to segment C(p) ,p = 1, . [sent-265, score-0.703]
</p><p>59 Apply spectral clustering to A to segment all trajectories into 2 clusters, and choose the one with lower dimension as background2. [sent-275, score-0.796]
</p><p>60 Iterate the following two steps until convergence: (a) Compute the bases of background subspace by performing SVD on the matrix formed by C(p) ,p ∈ background : N = (μ1 , μ2 , μ3, μ4). [sent-277, score-0.362]
</p><p>61 (b) Compute projection error of all trajectories to background subspace: ? [sent-278, score-0.709]
</p><p>62 Compute projection error of foreground trajectories to foreground subspace, and reject the trajectories with projection error greater than a threshold λ as outliers. [sent-286, score-1.594]
</p><p>63 In our two-stage clustering, step 2 provides a good initialization for the following iteration, thus leads to a superior convergence performance of the foreground background segmentation (usually needs less than 10 iterations to con-  ×  verge). [sent-290, score-0.324]
</p><p>64 Experiments  In this section, we evaluate our method on both the Hopkins 155 dataset [16] and the Berkeley motion segmentation dataset [3] by comparing with state-of-the-art trajectory clustering algorithms. [sent-293, score-0.56]
</p><p>65 t,dra}jectory to its motion subspace, which is computed by SVD on all trajectories in the cluster it belonged, to measure segmentation quality and choose one with the fewest error as the best. [sent-306, score-0.894]
</p><p>66 (a) Input trajectories and ground-truth segmentation, red and green denote foreground clusters while blue denote background cluster. [sent-310, score-0.863]
</p><p>67 The dataset consists of 120 sequences with 2 motions and 35 sequences with 3 motions which can be divided into three categories: checkerboard, traffic, and articulated. [sent-318, score-0.502]
</p><p>68 For each sequence, feature trajectories are obtained using an automatic tracker, and errors in tracking are manually corrected. [sent-319, score-0.629]
</p><p>69 Therefore, the motion sequences in this dataset can be considered as clean data without any corruption or missing entries. [sent-320, score-0.506]
</p><p>70 We run our method on all sequences of Hopkins 155 dataset and compute the average and median misclassification rates for each category of sequences. [sent-321, score-0.341]
</p><p>71 se-  mentation results for checkerboard sequences with two and three motions and traffic sequences with three motions. [sent-329, score-0.503]
</p><p>72 We further divide the checkerboard sequences into two parts: 5 1 sequences without or with only translational background motions (e. [sent-334, score-0.7]
</p><p>73 ‘ 1RT2TC’)3; 53 sequences with nontranslational background motions (e. [sent-336, score-0.353]
</p><p>74 From Table 3, it can be observed that nontranslational background motion leads to significant deterioration of the performance of translational model-based NNMF. [sent-340, score-0.443]
</p><p>75 In contrast, our method, although also based on the translational motion model, can achieve low misclassification rates for the sequences with non-translational background motion as well, showing the effectiveness of foreground-background separation in our clustering algorithm. [sent-342, score-1.058]
</p><p>76 Missing and Corrupted Data In this subsection, two experiments are conducted to test the robustness of our method to missing and corrupted data in motion sequences. [sent-348, score-0.545]
</p><p>77 The first experiment is designed to examine how the segmentation accuracy of our method changes with increasing percentage of missing or corrupted entries in a sequence. [sent-349, score-0.585]
</p><p>78 In this experiment, we compare the performance of our method with that of ALC and NNMF, which both can deal with incomplete and corrupted trajectories. [sent-350, score-0.356]
</p><p>79 We refer to ALC designing to handle incomplete and corrupted trajectories as ALC-miss and ALCcorrupted, respectively. [sent-351, score-0.945]
</p><p>80 Then, in Figure 5(Top), the misclassification rates of our method, ALC, and NNMF on each sequence are plotted as a function of percentage of missing entries. [sent-354, score-0.431]
</p><p>81 rWiaen tchee nλ run our method,  ALC, and NNMF on the resulting sequences, and plot their misclassification rates as a function of percentage of corrupted entries in Figure 5(Bottom). [sent-360, score-0.542]
</p><p>82 From Figure 5, it can be seen that for clean sequences without any corruption or missing entries, all of the three algorithms can give near-perfect segmentation results. [sent-361, score-0.423]
</p><p>83 However, when increasing amounts of missing or corrupted data are introduced to a given sequence, the performance of ALC and NNMF degrades much faster than ours. [sent-362, score-0.386]
</p><p>84 Especially in the case of more than half of trajectories being abandoned or corrupted, our method significantly outperforms other methods. [sent-363, score-0.589]
</p><p>85 The different performances of the three algorithms are mainly due to the different ways of dealing with incomplete and corrupted trajectories. [sent-364, score-0.356]
</p><p>86 To ALC, the requirement of enough complete and uncorrupted trajectories for repairing the pathological trajectories limits its ability to robustly segment the sequences with a large number of missing or corrupted entries. [sent-365, score-1.868]
</p><p>87 In the second experiment, we use Berkeley motion segmentation dataset to test the performance of our method on motion sequences with real incomplete and corrupted trajectories. [sent-367, score-0.912]
</p><p>88 each sequence, feature trajectories are obtained using tracker in [15], and no manual effort is made to correct or remove incorrect tracks. [sent-377, score-0.614]
</p><p>89 Therefore, the sequences in this dataset contain considerable quantities of missing and corrupted entries. [sent-378, score-0.549]
</p><p>90 Another feature of each sequence in this dataset is that the background area either remains static or undergoes mainly translational motion. [sent-379, score-0.319]
</p><p>91 The superior performance on Berkeley motion segmentation dataset  again demonstrates the robustness of our method to missing and corrupted data. [sent-391, score-0.661]
</p><p>92 Conclusion This paper proposes a new trajectory clustering algorithm for motion segmentation which is highly robust to the missing and corrupted data typical of real-world tracking process. [sent-393, score-0.946]
</p><p>93 Differing from previous works, we propose using the DCT basis as temporal smoothness constraint to facilitate segmentation of incomplete and corrupted trajectories. [sent-394, score-0.563]
</p><p>94 Due to the optimality of DCT basis for representing motion trajectories, our method can estimate reliable low-dimensional representation of trajectories even with a large amount of missing data and corrupted noises. [sent-395, score-1.159]
</p><p>95 The advantage of the proposed two-stage clustering is that we can use a simple translational model to effectively handle sequences containing complex motions. [sent-397, score-0.425]
</p><p>96 Top: The misclassification rates of our method, ALC, and NNMF on ‘1RT2TC’ (a), ‘arm’ (b), and ‘cars2 06’ (c) when percentage of missing entries in the sequence increases from 0% to 80%. [sent-399, score-0.505]
</p><p>97 Bottom: The misclassification rates of our method, ALC, and NNMF on ‘ 1RT2TC’ (d), ‘arm’ (e), and ‘cars2 06’ (f) when percentage of corrupted entries in the sequence increases from 0% to 100%. [sent-400, score-0.599]
</p><p>98 Semi-nonnegative matrix factorization for motion segmentation with missing data. [sent-484, score-0.503]
</p><p>99 Motion segmentation via robust subspace separation in the presence of outlying, incomplete, or corrupted trajectories. [sent-500, score-0.441]
</p><p>100 Dense point trajectories by gpu-accelerated large displacement optical flow. [sent-515, score-0.589]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('trajectories', 0.589), ('alc', 0.243), ('corrupted', 0.24), ('hopkins', 0.237), ('nnmf', 0.233), ('dct', 0.18), ('translational', 0.172), ('motion', 0.159), ('foreground', 0.158), ('missing', 0.146), ('misclassification', 0.14), ('trajectory', 0.134), ('clustering', 0.131), ('sequences', 0.122), ('motions', 0.119), ('bases', 0.116), ('incomplete', 0.116), ('pathological', 0.098), ('segmentation', 0.096), ('checkerboard', 0.095), ('msmc', 0.075), ('factorization', 0.075), ('entries', 0.074), ('berkeley', 0.072), ('background', 0.07), ('rates', 0.059), ('subspace', 0.059), ('sequence', 0.057), ('uncorrupted', 0.052), ('subspaces', 0.047), ('separation', 0.046), ('clusters', 0.046), ('traffic', 0.045), ('spectral', 0.044), ('smoothness', 0.044), ('nontranslational', 0.042), ('xfp', 0.042), ('yfp', 0.042), ('temporal', 0.042), ('ssc', 0.041), ('tracking', 0.04), ('repair', 0.038), ('akhter', 0.038), ('jp', 0.037), ('compactly', 0.035), ('corruption', 0.035), ('pth', 0.035), ('separates', 0.035), ('inherent', 0.035), ('belonging', 0.034), ('pages', 0.033), ('segment', 0.032), ('characteristics', 0.032), ('nonrigid', 0.031), ('eigenmaps', 0.031), ('sdt', 0.031), ('decompose', 0.031), ('kind', 0.03), ('percentage', 0.029), ('arm', 0.029), ('cluster', 0.028), ('projection', 0.028), ('subsection', 0.028), ('matrix', 0.027), ('china', 0.027), ('brox', 0.026), ('similarities', 0.026), ('deforming', 0.026), ('rao', 0.026), ('tables', 0.026), ('tron', 0.025), ('vec', 0.025), ('basis', 0.025), ('tracker', 0.025), ('elhamifar', 0.025), ('unsuitable', 0.025), ('clean', 0.024), ('khan', 0.023), ('affinity', 0.023), ('vidal', 0.023), ('unknowns', 0.022), ('error', 0.022), ('gross', 0.022), ('simon', 0.022), ('articulated', 0.022), ('quantities', 0.021), ('svd', 0.021), ('dataset', 0.02), ('cp', 0.02), ('visualization', 0.02), ('formed', 0.02), ('completion', 0.02), ('trackers', 0.02), ('rf', 0.019), ('partitions', 0.019), ('predefined', 0.019), ('snmf', 0.019), ('jectory', 0.019), ('robert', 0.019), ('arranging', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999881 <a title="361-tfidf-1" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>Author: Feng Shi, Zhong Zhou, Jiangjian Xiao, Wei Wu</p><p>Abstract: Due to occlusions and objects ’ non-rigid deformation in the scene, the obtained motion trajectories from common trackers may contain a number of missing or mis-associated entries. To cluster such corrupted point based trajectories into multiple motions is still a hard problem. In this paper, we present an approach that exploits temporal and spatial characteristics from tracked points to facilitate segmentation of incomplete and corrupted trajectories, thereby obtain highly robust results against severe data missing and noises. Our method first uses the Discrete Cosine Transform (DCT) bases as a temporal smoothness constraint on trajectory projection to ensure the validity of resulting components to repair pathological trajectories. Then, based on an observation that the trajectories of foreground and background in a scene may have different spatial distributions, we propose a two-stage clustering strategy that first performs foreground-background separation then segments remaining foreground trajectories. We show that, with this new clustering strategy, sequences with complex motions can be accurately segmented by even using a simple trans- lational model. Finally, a series of experiments on Hopkins 155 dataset andBerkeley motion segmentation dataset show the advantage of our method over other state-of-the-art motion segmentation algorithms in terms of both effectiveness and robustness.</p><p>2 0.48789868 <a title="361-tfidf-2" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<p>Author: Ali Elqursh, Ahmed Elgammal</p><p>Abstract: The vast majority of work on motion segmentation adopts the affine camera model due to its simplicity. Under the affine model, the motion segmentation problem becomes that of subspace separation. Due to this assumption, such methods are mainly offline and exhibit poor performance when the assumption is not satisfied. This is made evident in state-of-the-art methods that relax this assumption by using piecewise affine spaces and spectral clustering techniques to achieve better results. In this paper, we formulate the problem of motion segmentation as that of manifold separation. We then show how label propagation can be used in an online framework to achieve manifold separation. The performance of our framework is evaluated on a benchmark dataset and achieves competitive performance while being online.</p><p>3 0.34031677 <a title="361-tfidf-3" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>Author: Zhuwen Li, Jiaming Guo, Loong-Fah Cheong, Steven Zhiying Zhou</p><p>Abstract: This paper addresses real-world challenges in the motion segmentation problem, including perspective effects, missing data, and unknown number of motions. It first formulates the 3-D motion segmentation from two perspective views as a subspace clustering problem, utilizing the epipolar constraint of an image pair. It then combines the point correspondence information across multiple image frames via a collaborative clustering step, in which tight integration is achieved via a mixed norm optimization scheme. For model selection, wepropose an over-segment and merge approach, where the merging step is based on the property of the ?1-norm ofthe mutual sparse representation oftwo oversegmented groups. The resulting algorithm can deal with incomplete trajectories and perspective effects substantially better than state-of-the-art two-frame and multi-frame methods. Experiments on a 62-clip dataset show the significant superiority of the proposed idea in both segmentation accuracy and model selection.</p><p>4 0.3382884 <a title="361-tfidf-4" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>Author: Jiaming Guo, Zhuwen Li, Loong-Fah Cheong, Steven Zhiying Zhou</p><p>Abstract: Given a pair of videos having a common action, our goal is to simultaneously segment this pair of videos to extract this common action. As a preprocessing step, we first remove background trajectories by a motion-based figureground segmentation. To remove the remaining background and those extraneous actions, we propose the trajectory cosaliency measure, which captures the notion that trajectories recurring in all the videos should have their mutual saliency boosted. This requires a trajectory matching process which can compare trajectories with different lengths and not necessarily spatiotemporally aligned, and yet be discriminative enough despite significant intra-class variation in the common action. We further leverage the graph matching to enforce geometric coherence between regions so as to reduce feature ambiguity and matching errors. Finally, to classify the trajectories into common action and action outliers, we formulate the problem as a binary labeling of a Markov Random Field, in which the data term is measured by the trajectory co-saliency and the smooth- ness term is measured by the spatiotemporal consistency between trajectories. To evaluate the performance of our framework, we introduce a dataset containing clips that have animal actions as well as human actions. Experimental results show that the proposed method performs well in common action extraction.</p><p>5 0.32490593 <a title="361-tfidf-5" href="./iccv-2013-Camera_Alignment_Using_Trajectory_Intersections_in_Unsynchronized_Videos.html">68 iccv-2013-Camera Alignment Using Trajectory Intersections in Unsynchronized Videos</a></p>
<p>Author: Thomas Kuo, Santhoshkumar Sunderrajan, B.S. Manjunath</p><p>Abstract: This paper addresses the novel and challenging problem of aligning camera views that are unsynchronized by low and/or variable frame rates using object trajectories. Unlike existing trajectory-based alignment methods, our method does not require frame-to-frame synchronization. Instead, we propose using the intersections of corresponding object trajectories to match views. To find these intersections, we introduce a novel trajectory matching algorithm based on matching Spatio-Temporal Context Graphs (STCGs). These graphs represent the distances between trajectories in time and space within a view, and are matched to an STCG from another view to find the corresponding trajectories. To the best of our knowledge, this is one of the first attempts to align views that are unsynchronized with variable frame rates. The results on simulated and real-world datasets show trajectory intersections area viablefeatureforcamera alignment, and that the trajectory matching method performs well in real-world scenarios.</p><p>6 0.27121779 <a title="361-tfidf-6" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>7 0.2154191 <a title="361-tfidf-7" href="./iccv-2013-Measuring_Flow_Complexity_in_Videos.html">263 iccv-2013-Measuring Flow Complexity in Videos</a></p>
<p>8 0.20848542 <a title="361-tfidf-8" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>9 0.19106951 <a title="361-tfidf-9" href="./iccv-2013-Latent_Space_Sparse_Subspace_Clustering.html">232 iccv-2013-Latent Space Sparse Subspace Clustering</a></p>
<p>10 0.17259824 <a title="361-tfidf-10" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>11 0.1648258 <a title="361-tfidf-11" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>12 0.16451164 <a title="361-tfidf-12" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>13 0.16396289 <a title="361-tfidf-13" href="./iccv-2013-Minimal_Basis_Facility_Location_for_Subspace_Segmentation.html">264 iccv-2013-Minimal Basis Facility Location for Subspace Segmentation</a></p>
<p>14 0.142425 <a title="361-tfidf-14" href="./iccv-2013-Robust_Subspace_Clustering_via_Half-Quadratic_Minimization.html">360 iccv-2013-Robust Subspace Clustering via Half-Quadratic Minimization</a></p>
<p>15 0.13042527 <a title="361-tfidf-15" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<p>16 0.12196623 <a title="361-tfidf-16" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>17 0.1163691 <a title="361-tfidf-17" href="./iccv-2013-The_Way_They_Move%3A_Tracking_Multiple_Targets_with_Similar_Appearance.html">418 iccv-2013-The Way They Move: Tracking Multiple Targets with Similar Appearance</a></p>
<p>18 0.10669126 <a title="361-tfidf-18" href="./iccv-2013-Manipulation_Pattern_Discovery%3A_A_Nonparametric_Bayesian_Approach.html">260 iccv-2013-Manipulation Pattern Discovery: A Nonparametric Bayesian Approach</a></p>
<p>19 0.10386028 <a title="361-tfidf-19" href="./iccv-2013-Efficient_Higher-Order_Clustering_on_the_Grassmann_Manifold.html">134 iccv-2013-Efficient Higher-Order Clustering on the Grassmann Manifold</a></p>
<p>20 0.097062498 <a title="361-tfidf-20" href="./iccv-2013-Unifying_Nuclear_Norm_and_Bilinear_Factorization_Approaches_for_Low-Rank_Matrix_Decomposition.html">434 iccv-2013-Unifying Nuclear Norm and Bilinear Factorization Approaches for Low-Rank Matrix Decomposition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.203), (1, -0.047), (2, 0.057), (3, 0.223), (4, -0.115), (5, 0.164), (6, -0.034), (7, 0.23), (8, 0.283), (9, 0.155), (10, 0.125), (11, 0.076), (12, 0.003), (13, -0.02), (14, -0.102), (15, 0.011), (16, 0.022), (17, 0.074), (18, -0.005), (19, 0.035), (20, -0.181), (21, 0.036), (22, 0.133), (23, 0.245), (24, -0.025), (25, 0.158), (26, 0.029), (27, 0.022), (28, 0.063), (29, 0.003), (30, 0.017), (31, 0.065), (32, 0.023), (33, 0.033), (34, 0.041), (35, -0.088), (36, -0.073), (37, 0.047), (38, 0.045), (39, 0.012), (40, 0.014), (41, 0.051), (42, 0.035), (43, 0.057), (44, 0.008), (45, 0.056), (46, -0.002), (47, 0.03), (48, 0.011), (49, 0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97781676 <a title="361-lsi-1" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>Author: Feng Shi, Zhong Zhou, Jiangjian Xiao, Wei Wu</p><p>Abstract: Due to occlusions and objects ’ non-rigid deformation in the scene, the obtained motion trajectories from common trackers may contain a number of missing or mis-associated entries. To cluster such corrupted point based trajectories into multiple motions is still a hard problem. In this paper, we present an approach that exploits temporal and spatial characteristics from tracked points to facilitate segmentation of incomplete and corrupted trajectories, thereby obtain highly robust results against severe data missing and noises. Our method first uses the Discrete Cosine Transform (DCT) bases as a temporal smoothness constraint on trajectory projection to ensure the validity of resulting components to repair pathological trajectories. Then, based on an observation that the trajectories of foreground and background in a scene may have different spatial distributions, we propose a two-stage clustering strategy that first performs foreground-background separation then segments remaining foreground trajectories. We show that, with this new clustering strategy, sequences with complex motions can be accurately segmented by even using a simple trans- lational model. Finally, a series of experiments on Hopkins 155 dataset andBerkeley motion segmentation dataset show the advantage of our method over other state-of-the-art motion segmentation algorithms in terms of both effectiveness and robustness.</p><p>2 0.91596127 <a title="361-lsi-2" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<p>Author: Ali Elqursh, Ahmed Elgammal</p><p>Abstract: The vast majority of work on motion segmentation adopts the affine camera model due to its simplicity. Under the affine model, the motion segmentation problem becomes that of subspace separation. Due to this assumption, such methods are mainly offline and exhibit poor performance when the assumption is not satisfied. This is made evident in state-of-the-art methods that relax this assumption by using piecewise affine spaces and spectral clustering techniques to achieve better results. In this paper, we formulate the problem of motion segmentation as that of manifold separation. We then show how label propagation can be used in an online framework to achieve manifold separation. The performance of our framework is evaluated on a benchmark dataset and achieves competitive performance while being online.</p><p>3 0.80759293 <a title="361-lsi-3" href="./iccv-2013-Camera_Alignment_Using_Trajectory_Intersections_in_Unsynchronized_Videos.html">68 iccv-2013-Camera Alignment Using Trajectory Intersections in Unsynchronized Videos</a></p>
<p>Author: Thomas Kuo, Santhoshkumar Sunderrajan, B.S. Manjunath</p><p>Abstract: This paper addresses the novel and challenging problem of aligning camera views that are unsynchronized by low and/or variable frame rates using object trajectories. Unlike existing trajectory-based alignment methods, our method does not require frame-to-frame synchronization. Instead, we propose using the intersections of corresponding object trajectories to match views. To find these intersections, we introduce a novel trajectory matching algorithm based on matching Spatio-Temporal Context Graphs (STCGs). These graphs represent the distances between trajectories in time and space within a view, and are matched to an STCG from another view to find the corresponding trajectories. To the best of our knowledge, this is one of the first attempts to align views that are unsynchronized with variable frame rates. The results on simulated and real-world datasets show trajectory intersections area viablefeatureforcamera alignment, and that the trajectory matching method performs well in real-world scenarios.</p><p>4 0.73239535 <a title="361-lsi-4" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>Author: Zhuwen Li, Jiaming Guo, Loong-Fah Cheong, Steven Zhiying Zhou</p><p>Abstract: This paper addresses real-world challenges in the motion segmentation problem, including perspective effects, missing data, and unknown number of motions. It first formulates the 3-D motion segmentation from two perspective views as a subspace clustering problem, utilizing the epipolar constraint of an image pair. It then combines the point correspondence information across multiple image frames via a collaborative clustering step, in which tight integration is achieved via a mixed norm optimization scheme. For model selection, wepropose an over-segment and merge approach, where the merging step is based on the property of the ?1-norm ofthe mutual sparse representation oftwo oversegmented groups. The resulting algorithm can deal with incomplete trajectories and perspective effects substantially better than state-of-the-art two-frame and multi-frame methods. Experiments on a 62-clip dataset show the significant superiority of the proposed idea in both segmentation accuracy and model selection.</p><p>5 0.73050803 <a title="361-lsi-5" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>Author: Feng Liu, Yuzhen Niu, Hailin Jin</p><p>Abstract: Shaky stereoscopic video is not only unpleasant to watch but may also cause 3D fatigue. Stabilizing the left and right view of a stereoscopic video separately using a monocular stabilization method tends to both introduce undesirable vertical disparities and damage horizontal disparities, which may destroy the stereoscopic viewing experience. In this paper, we present a joint subspace stabilization method for stereoscopic video. We prove that the low-rank subspace constraint for monocular video [10] also holds for stereoscopic video. Particularly, the feature trajectories from the left and right video share the same subspace. Based on this proof, we develop a stereo subspace stabilization method that jointly computes a common subspace from the left and right video and uses it to stabilize the two videos simultaneously. Our method meets the stereoscopic constraints without 3D reconstruction or explicit left-right correspondence. We test our method on a variety of stereoscopic videos with different scene content and camera motion. The experiments show that our method achieves high-quality stabilization for stereoscopic video in a robust and efficient way.</p><p>6 0.71431398 <a title="361-lsi-6" href="./iccv-2013-Measuring_Flow_Complexity_in_Videos.html">263 iccv-2013-Measuring Flow Complexity in Videos</a></p>
<p>7 0.69025171 <a title="361-lsi-7" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>8 0.66720313 <a title="361-lsi-8" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>9 0.62122864 <a title="361-lsi-9" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>10 0.61675799 <a title="361-lsi-10" href="./iccv-2013-Minimal_Basis_Facility_Location_for_Subspace_Segmentation.html">264 iccv-2013-Minimal Basis Facility Location for Subspace Segmentation</a></p>
<p>11 0.54957104 <a title="361-lsi-11" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>12 0.5144136 <a title="361-lsi-12" href="./iccv-2013-Manipulation_Pattern_Discovery%3A_A_Nonparametric_Bayesian_Approach.html">260 iccv-2013-Manipulation Pattern Discovery: A Nonparametric Bayesian Approach</a></p>
<p>13 0.4951129 <a title="361-lsi-13" href="./iccv-2013-The_Way_They_Move%3A_Tracking_Multiple_Targets_with_Similar_Appearance.html">418 iccv-2013-The Way They Move: Tracking Multiple Targets with Similar Appearance</a></p>
<p>14 0.49381164 <a title="361-lsi-14" href="./iccv-2013-Optimal_Orthogonal_Basis_and_Image_Assimilation%3A_Motion_Modeling.html">301 iccv-2013-Optimal Orthogonal Basis and Image Assimilation: Motion Modeling</a></p>
<p>15 0.4802652 <a title="361-lsi-15" href="./iccv-2013-A_Unified_Video_Segmentation_Benchmark%3A_Annotation%2C_Metrics_and_Analysis.html">33 iccv-2013-A Unified Video Segmentation Benchmark: Annotation, Metrics and Analysis</a></p>
<p>16 0.44098374 <a title="361-lsi-16" href="./iccv-2013-Latent_Space_Sparse_Subspace_Clustering.html">232 iccv-2013-Latent Space Sparse Subspace Clustering</a></p>
<p>17 0.42070061 <a title="361-lsi-17" href="./iccv-2013-Mining_Motion_Atoms_and_Phrases_for_Complex_Action_Recognition.html">265 iccv-2013-Mining Motion Atoms and Phrases for Complex Action Recognition</a></p>
<p>18 0.42013982 <a title="361-lsi-18" href="./iccv-2013-Estimating_the_Material_Properties_of_Fabric_from_Video.html">145 iccv-2013-Estimating the Material Properties of Fabric from Video</a></p>
<p>19 0.41498956 <a title="361-lsi-19" href="./iccv-2013-Unsupervised_Random_Forest_Manifold_Alignment_for_Lipreading.html">437 iccv-2013-Unsupervised Random Forest Manifold Alignment for Lipreading</a></p>
<p>20 0.40125939 <a title="361-lsi-20" href="./iccv-2013-Efficient_Higher-Order_Clustering_on_the_Grassmann_Manifold.html">134 iccv-2013-Efficient Higher-Order Clustering on the Grassmann Manifold</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.088), (19, 0.017), (26, 0.099), (27, 0.011), (31, 0.085), (35, 0.022), (42, 0.093), (48, 0.018), (50, 0.164), (64, 0.064), (73, 0.034), (89, 0.189), (98, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90644574 <a title="361-lda-1" href="./iccv-2013-Example-Based_Facade_Texture_Synthesis.html">148 iccv-2013-Example-Based Facade Texture Synthesis</a></p>
<p>Author: Dengxin Dai, Hayko Riemenschneider, Gerhard Schmitt, Luc Van_Gool</p><p>Abstract: There is an increased interest in the efficient creation of city models, be it virtual or as-built. We present a method for synthesizing complex, photo-realistic facade images, from a single example. After parsing the example image into its semantic components, a tiling for it is generated. Novel tilings can then be created, yielding facade textures with different dimensions or with occluded parts inpainted. A genetic algorithm guides the novel facades as well as inpainted parts to be consistent with the example, both in terms of their overall structure and their detailed textures. Promising results for multiple standard datasets in particular for the different building styles they contain demonstrate the potential of the method. – –</p><p>2 0.90434551 <a title="361-lda-2" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<p>Author: Salil Tambe, Ashok Veeraraghavan, Amit Agrawal</p><p>Abstract: Current Light Field (LF) cameras offer fixed resolution in space, time and angle which is decided a-priori and is independent of the scene. These cameras either trade-off spatial resolution to capture single-shot LF [20, 27, 12] or tradeoff temporal resolution by assuming a static scene to capture high spatial resolution LF [18, 3]. Thus, capturing high spatial resolution LF video for dynamic scenes remains an open and challenging problem. We present the concept, design and implementation of a LF video camera that allows capturing high resolution LF video. The spatial, angular and temporal resolution are not fixed a-priori and we exploit the scene-specific redundancy in space, time and angle. Our reconstruction is motion-aware and offers a continuum of resolution tradeoff with increasing motion in the scene. The key idea is (a) to design efficient multiplexing matrices that allow resolution tradeoffs, (b) use dictionary learning and sparse repre- sentations for robust reconstruction, and (c) perform local motion-aware adaptive reconstruction. We perform extensive analysis and characterize the performance of our motion-aware reconstruction algorithm. We show realistic simulations using a graphics simulator as well as real results using a LCoS based programmable camera. We demonstrate novel results such as high resolution digital refocusing for dynamic moving objects.</p><p>same-paper 3 0.88178515 <a title="361-lda-3" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>Author: Feng Shi, Zhong Zhou, Jiangjian Xiao, Wei Wu</p><p>Abstract: Due to occlusions and objects ’ non-rigid deformation in the scene, the obtained motion trajectories from common trackers may contain a number of missing or mis-associated entries. To cluster such corrupted point based trajectories into multiple motions is still a hard problem. In this paper, we present an approach that exploits temporal and spatial characteristics from tracked points to facilitate segmentation of incomplete and corrupted trajectories, thereby obtain highly robust results against severe data missing and noises. Our method first uses the Discrete Cosine Transform (DCT) bases as a temporal smoothness constraint on trajectory projection to ensure the validity of resulting components to repair pathological trajectories. Then, based on an observation that the trajectories of foreground and background in a scene may have different spatial distributions, we propose a two-stage clustering strategy that first performs foreground-background separation then segments remaining foreground trajectories. We show that, with this new clustering strategy, sequences with complex motions can be accurately segmented by even using a simple trans- lational model. Finally, a series of experiments on Hopkins 155 dataset andBerkeley motion segmentation dataset show the advantage of our method over other state-of-the-art motion segmentation algorithms in terms of both effectiveness and robustness.</p><p>4 0.8684988 <a title="361-lda-4" href="./iccv-2013-Facial_Action_Unit_Event_Detection_by_Cascade_of_Tasks.html">155 iccv-2013-Facial Action Unit Event Detection by Cascade of Tasks</a></p>
<p>Author: Xiaoyu Ding, Wen-Sheng Chu, Fernando De_La_Torre, Jeffery F. Cohn, Qiao Wang</p><p>Abstract: Automatic facial Action Unit (AU) detection from video is a long-standing problem in facial expression analysis. AU detection is typically posed as a classification problem between frames or segments of positive examples and negative ones, where existing work emphasizes the use of different features or classifiers. In this paper, we propose a method called Cascade of Tasks (CoT) that combines the use ofdifferent tasks (i.e., , frame, segment and transition)for AU event detection. We train CoT in a sequential manner embracing diversity, which ensures robustness and generalization to unseen data. In addition to conventional framebased metrics that evaluate frames independently, we propose a new event-based metric to evaluate detection performance at event-level. We show how the CoT method consistently outperforms state-of-the-art approaches in both frame-based and event-based metrics, across three public datasets that differ in complexity: CK+, FERA and RUFACS.</p><p>5 0.83475327 <a title="361-lda-5" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>Author: Ming-Ming Cheng, Jonathan Warrell, Wen-Yan Lin, Shuai Zheng, Vibhav Vineet, Nigel Crook</p><p>Abstract: Detecting visually salient regions in images is one of the fundamental problems in computer vision. We propose a novel method to decompose an image into large scale perceptually homogeneous elements for efficient salient region detection, using a soft image abstraction representation. By considering both appearance similarity and spatial distribution of image pixels, the proposed representation abstracts out unnecessary image details, allowing the assignment of comparable saliency values across similar regions, and producing perceptually accurate salient region detection. We evaluate our salient region detection approach on the largest publicly available dataset with pixel accurate annotations. The experimental results show that the proposed method outperforms 18 alternate methods, reducing the mean absolute error by 25.2% compared to the previous best result, while being computationally more efficient.</p><p>6 0.83216226 <a title="361-lda-6" href="./iccv-2013-Class-Specific_Simplex-Latent_Dirichlet_Allocation_for_Image_Classification.html">73 iccv-2013-Class-Specific Simplex-Latent Dirichlet Allocation for Image Classification</a></p>
<p>7 0.83169752 <a title="361-lda-7" href="./iccv-2013-Find_the_Best_Path%3A_An_Efficient_and_Accurate_Classifier_for_Image_Hierarchies.html">165 iccv-2013-Find the Best Path: An Efficient and Accurate Classifier for Image Hierarchies</a></p>
<p>8 0.8292836 <a title="361-lda-8" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>9 0.82617557 <a title="361-lda-9" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>10 0.82461637 <a title="361-lda-10" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>11 0.82461172 <a title="361-lda-11" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>12 0.82426536 <a title="361-lda-12" href="./iccv-2013-PhotoOCR%3A_Reading_Text_in_Uncontrolled_Conditions.html">315 iccv-2013-PhotoOCR: Reading Text in Uncontrolled Conditions</a></p>
<p>13 0.82047331 <a title="361-lda-13" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>14 0.82045555 <a title="361-lda-14" href="./iccv-2013-Motion-Aware_KNN_Laplacian_for_Video_Matting.html">275 iccv-2013-Motion-Aware KNN Laplacian for Video Matting</a></p>
<p>15 0.82016873 <a title="361-lda-15" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>16 0.820153 <a title="361-lda-16" href="./iccv-2013-Topology-Constrained_Layered_Tracking_with_Latent_Flow.html">420 iccv-2013-Topology-Constrained Layered Tracking with Latent Flow</a></p>
<p>17 0.81978214 <a title="361-lda-17" href="./iccv-2013-Regionlets_for_Generic_Object_Detection.html">349 iccv-2013-Regionlets for Generic Object Detection</a></p>
<p>18 0.81923962 <a title="361-lda-18" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>19 0.8189826 <a title="361-lda-19" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>20 0.81885523 <a title="361-lda-20" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
