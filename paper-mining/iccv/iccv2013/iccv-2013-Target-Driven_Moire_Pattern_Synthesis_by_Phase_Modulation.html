<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>413 iccv-2013-Target-Driven Moire Pattern Synthesis by Phase Modulation</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-413" href="#">iccv2013-413</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>413 iccv-2013-Target-Driven Moire Pattern Synthesis by Phase Modulation</h1>
<br/><p>Source: <a title="iccv-2013-413-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Tsai_Target-Driven_Moire_Pattern_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Pei-Hen Tsai, Yung-Yu Chuang</p><p>Abstract: This paper investigates an approach for generating two grating images so that the moir e´ pattern of their superposition resembles the target image. Our method is grounded on the fundamental moir e´ theorem. By focusing on the visually most dominant (1, −1)-moir e´ component, we obtain the phase smto ddoumlaintiaonnt c (o1n,s−tr1a)in-mt on the phase shifts bee otwbteaeinn the two grating images. For improving visual appearance of the grating images and hiding capability the embedded image, a smoothness term is added to spread information between the two grating images and an appearance phase function is used to add irregular structures into grating images. The grating images can be printed on transparencies and the hidden image decoding can be performed optically by overlaying them together. The proposed method enables the creation of moir e´ art and allows visual decoding without computers.</p><p>Reference: <a title="iccv-2013-413-reference" href="../iccv2013_reference/iccv-2013-Target-Driven_Moire_Pattern_Synthesis_by_Phase_Modulation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Target-Driven Moir e´ Pattern Synthesis by Phase Modulation Pei-Hen Tsai Yung-Yu Chuang∗ National Taiwan University  Abstract This paper investigates an approach for generating two grating images so that the moir e´ pattern of their superposition resembles the target image. [sent-1, score-1.53]
</p><p>2 Our method is grounded on the fundamental moir e´ theorem. [sent-2, score-0.815]
</p><p>3 By focusing on the visually most dominant (1, −1)-moir e´ component, we obtain the phase smto ddoumlaintiaonnt c (o1n,s−tr1a)in-mt on the phase shifts bee otwbteaeinn the two grating images. [sent-3, score-0.765]
</p><p>4 For improving visual appearance of the grating images and hiding capability the embedded image, a smoothness term is added to spread information between the two grating images and an appearance phase function is used to add irregular structures into grating images. [sent-4, score-1.745]
</p><p>5 The grating images can be printed on transparencies and the hidden image decoding can be performed optically by overlaying them together. [sent-5, score-0.651]
</p><p>6 The proposed method enables the creation of moir e´ art and allows visual decoding without computers. [sent-6, score-0.83]
</p><p>7 One property that makes  moir e´ patterns mysterious and interesting is that they consist of new patterns which are clearly visible in the superposition but never appear in any of the original structures [1]. [sent-9, score-1.03]
</p><p>8 For computer graphics and computer vision, moir e´ patterns are often unwanted artifacts produced by rendering programs or digital cameras, due to undersampling or unexpected interaction between overlaid structures. [sent-10, score-0.842]
</p><p>9 In this paper, we study an opposite problem where moir e´ phenomenon is not abandoned but utilized for synthesizing the desired patterns. [sent-12, score-0.867]
</p><p>10 More formally, given a target image, we want to find two grating images so that their superposition resembles the target image through moir e´ effects but none of each reveals the target image individually. [sent-13, score-1.624]
</p><p>11 The moir e´ phenomenon is intriguing because it is mysterious and unexpected. [sent-14, score-0.845]
</p><p>12 Fortunately, for years, scientists have developed techniques for analyzing moir e´ patterns with the given structures, such as frequency-domain analysis, geometrical approaches and interferometric methods. [sent-17, score-0.817]
</p><p>13 Our method is grounded on the fundamental moir e´ theorem which, through frequency-domain analysis, provides a mathematical formulation of the moir e´ components between curvilinear grating images, one kind of repetitive non-periodic grating images (such as L1 and L2 in  Figure 1). [sent-18, score-2.611]
</p><p>14 By focusing on the visually most dominating (1, −1)-moir e´ component, we obtain the phase modulation c(1o,n−str1a)-inmt owirh ´eic cho gives ethnte, c woend oibttioanin o thf eth peh phase oshduifltas iboentween the two grating images for synthesizing the required target image. [sent-19, score-0.921]
</p><p>15 Direct application of the phase modulation constraint generates two grating images whose superposition resembles the desired target image. [sent-20, score-0.924]
</p><p>16 Unfortunately, the grating image alone could also reveal the target image although only obscurely. [sent-21, score-0.521]
</p><p>17 To make the grating images more uncorrelated to the target image, a smoothness term is used to enforce information spread between the two grating images. [sent-22, score-1.03]
</p><p>18 Furthermore, an appearance phase function is added for imposing unrelated structures into the grating images and controlling their appearances, making the target image more invisible in each of them. [sent-23, score-0.704]
</p><p>19 Our method can be used for several applications such as creating moir e´ art, in which two seemingly unrelated grating images are superposed to reveal an unexpected target image. [sent-24, score-1.379]
</p><p>20 Given “The Starry Night” and “The Scream” as the target images, our method generates two grating images L1 and L2. [sent-26, score-0.519]
</p><p>21 When overlaying L1 over L2 with their top edges aligned, the occurred moir e´ pattern resembles “The Starry Night”. [sent-27, score-0.899]
</p><p>22 In addition to inciting sense of wonder by moir e´ art, due to its information hiding nature, our method can also be used for steganography in which the message image is hidden and embedded, and can only be revealed with the key image. [sent-29, score-0.936]
</p><p>23 Our grating images can be  printed on separated transparencies and the decoding can be simply performed by overlaying them together. [sent-30, score-0.614]
</p><p>24 11991122  targetimageI1targetimageI2gratingimageL1gratingimageL2superpositon#1superpositon#2  Figure 1: An example of moir e´ art created by our method. [sent-33, score-0.814]
</p><p>25 Given two target images I1 and I2, we generate two grating images L1 and L2. [sent-34, score-0.532]
</p><p>26 When overlaying L1 over L2 with their top edges aligned, the interference pattern incurred by the moir e´ phenomenon resembles I1. [sent-35, score-0.946]
</p><p>27 By moving L1 downwards to align its bottom edge with L2’s, the moir e´ pattern looks like I2. [sent-36, score-0.865]
</p><p>28 Related work Amidror’s book “The Theory ofthe Moir e´ Phenomenon” provides a comprehensive and thorough treatment of the theory behind moir e´ phenomenon [1]. [sent-38, score-0.829]
</p><p>29 It applies Fourier domain approaches to give a detailed analysis for moir e´ phenomenon caused by periodic patterns and repetitive nonperiodic patterns. [sent-39, score-1.028]
</p><p>30 Our method is built upon the fundamental moir e´ theorem described in this book. [sent-40, score-0.837]
</p><p>31 Lebanon and Bruckstein [6] pioneered in studying synthesis of the desired moir e´ pattern caused by superposing two generated images. [sent-41, score-0.904]
</p><p>32 Although both their method and our method build upon the fundamental moir e´ theorem, with the proposed smoothness term and appearance phase function, the results of our method are visually more promising. [sent-42, score-1.066]
</p><p>33 Hersch and Chosson [3] proposed a moir e´ synthesis method for creating dynamic moving moir e´ patterns when shifting the base band stripe-like layer slowly. [sent-44, score-1.634]
</p><p>34 Some focused on image hiding in time-averaged moir e´ [9, 10], the moir e´ pattern appeared on a fast oscillating image due to persistence of vision. [sent-45, score-1.725]
</p><p>35 However, optical watermarking uses occlusion while our method uses moir e´ phenomenon. [sent-50, score-0.845]
</p><p>36 Fundamental moir e´ theorem  This paper takes the spectral approach for analysing the moir e´ phenomenon [1]. [sent-56, score-1.647]
</p><p>37 In this paper, we consider the moir e´ pattern between repetitive non-periodic grating layers, called curvilinear gratings. [sent-66, score-1.322]
</p><p>38 Here, the periodic function p determines the intensity behaviour while the bending function φ determines the geometric layout of the grating r. [sent-68, score-0.631]
</p><p>39 In the superposition (c), some low-frequency, nearly horizontal moir e´ patterns are noticeable. [sent-76, score-0.975]
</p><p>40 The two components close to the origin (f1−f2 and f2 −f1) correspond to the visible moir e´ patterns as they are o−f low-frequency and significant enough. [sent-80, score-0.86]
</p><p>41 The fundamental moir e´ theorem [1] states that the periodic profile and the geometric layout of the moir e´ are completely independent to each other. [sent-81, score-1.856]
</p><p>42 Mathematically, it says that the (k1, k2)-moir e´ component mk1 of the superposition r (here, k1, k2 are integers indicating the different moir e´ components, called moir e´ index) is given by ,k2  mk1,k2 (x, y)  = pk1,k2 (φk1,k2 (x, y)). [sent-82, score-1.75]
</p><p>43 The bending function φk1 brings moir e´ appearance and is given by ,k2  φk1,k2 (x, y)  = k1φ1(x,  y)  The 1-D periodic profile function moir e´ is given by  pk1 ,k2  into the (k1, k2)-  + k2φ2(x, pk1 ,k2  (4)  y). [sent-83, score-1.863]
</p><p>44 With the fundamenFtal m{oiFr e{´ pthe}o(rkemu,) we can decompose the moir e´ pattern into several (k1, k2)-moir e´ components whose periodic profiles and bending phase functions can be treated independently. [sent-93, score-1.279]
</p><p>45 Moir e´ pattern synthesis We attempt to solve the following problem: Given a target image I, find two curvilinear grating images L1 and L2 such that the moir e´ pattern of their superposition is close to I. [sent-96, score-1.587]
</p><p>46 We will impose some desired properties of the gratings for different applications in Section 5, but focus on the basic moir e´ pattern synthesis problem in this section. [sent-97, score-0.931]
</p><p>47 This indicates that we only need to consider four moir e´ components, (1, 1), (−1, −1), (1, −1) and (−1, 1). [sent-105, score-0.796]
</p><p>48 mNootier e´ eth caotm (p1,o n1)e natnsd, ((1−,11), −, (1−) 1m,−oir1 e´) are equivalent as they just swap (th1,e1 r)o alensd o (f− L11, −a1nd) mL2o ; esim ariela erq fuoirv (le1,n −t a1s) ahendy (−1, 1) moir e´ components. [sent-106, score-0.796]
</p><p>49 Therefore, we only manipulate the (1, −1) moir e´ component and leave owtehe orn components efo trh efr (e1e, as they are loemssp visually ndo lteiacveeable because of high frequency or low magnitude. [sent-109, score-0.872]
</p><p>50 )l iacnidty ,φ wme e( dx,e yfin) e≡ m φ1,−1 ()x ≡, y m) as the moir e´ component, intensity profile )a n≡d phase function for the (1, −1)nmeonitr, e´ ontfe ethnsei superposition S p respectively. [sent-115, score-1.183]
</p><p>51 11991144  ( ca) L I1( db) LIˆ2(e) superposit on Figure 4: A simple example for moir e´ pattern synthesis. [sent-116, score-0.826]
</p><p>52 When superposing L1 and L2 together, the generated moir e´ pattern (e) looks similar to the target image Iˆ. [sent-122, score-0.954]
</p><p>53 t Hthaotwe thvee ar, p bpeecaaruasnec eth oef range 1o)f- fmuonicrt ´eio mn pm kiss usually smaller than [0, 1], the synthesized moir e´ cannot match the intensity range of I. [sent-129, score-0.89]
</p><p>54 (10)  Thanks to the fundamental moir e´ theorem, the periodic profile pm can be pre-determined by the Fourier coefficients of p1 and p2 using Equation 9. [sent-135, score-1.106]
</p><p>55 Note that the target image is slightly visible (although very obscurely) in one of the grating images. [sent-149, score-0.524]
</p><p>56 The most important factor is probably the range of the resulting periodic profile because it directly affects the range of the intensity of the hidden moir e´ pattern. [sent-155, score-1.094]
</p><p>57 The periodic profile pm with a larger range potentially gives a moir e´ image with better contrast. [sent-156, score-1.088]
</p><p>58 mIfo tihr´e e periodic profiles p1 fan hdig p2 contain high-frequency components, they will have more significant high-frequency Fourier coefficients and also significant amplitudes of pk1,k2 for high-order moir e´ (where |k1| > 1and |k2 | > 1). [sent-162, score-1.04]
</p><p>59 Since we ignore high-order moir e´ |ink our fr 1am anedw o|krk|, significant high-order em hoigir eh´ components could inference the target moir e´ pattern and make it less noticeable. [sent-163, score-1.724]
</p><p>60 When using square waves as periodic profiles, the amplitude of high-order moir e´ components are not negligible. [sent-164, score-1.007]
</p><p>61 Thus, the superposition could contain undesired high-frequency moir e´ patterns. [sent-165, score-0.981]
</p><p>62 On the other hand, cosine  waves do not have high-frequency components and there is no high-order moir e´ components when using as profiles. [sent-166, score-0.892]
</p><p>63 The periodic profiles also directly affects the appearance of the resultant grating images. [sent-168, score-0.72]
</p><p>64 For example, some applications might require binary grating images and  the square waves become more suitable. [sent-178, score-0.496]
</p><p>65 We used two cosine functions f(x) = 12+21 cosx as the profiles for most cases because they contain less artifacts due to high-order moir e´ components and have an acceptable range to represent the hidden image with good contrast. [sent-179, score-1.008]
</p><p>66 Applications and results The previous section introduces the basic method for synthesizing the moir e´ pattern so that it looks similar to the target image. [sent-181, score-0.935]
</p><p>67 Note that we have the luxury to control grating images’ appearances by adjusting the phase functions as long as their phase difference satisfies the phase modulation constraint (Equation 12). [sent-182, score-1.001]
</p><p>68 Figure 6(d) an+d E(e,) wshheowre that the smoothness term improves the visual appearance of the grating and superposed images. [sent-252, score-0.578]
</p><p>69 (17) This property allows use to design the appearance of the grating images by adding an appearance phase function φA. [sent-255, score-0.689]
</p><p>70 However, when the frequency of φA is too high, undesired moir e´ will become noticeable on the grating images due to gridded sampling pattern of image pixels. [sent-257, score-1.357]
</p><p>71 Adding the appearance phase function could also distort the appearance of the grating images, making the target images even less noticeable. [sent-258, score-0.751]
</p><p>72 Figure 7 shows an example for controlling the appearance of the grating images and better image hiding with a diagonal  stripe pattern as the appearance phase function. [sent-259, score-0.819]
</p><p>73 11991166  Figure 6: The vertical smoothness term ES ensures appearance smoothness of the grating and the superposed images. [sent-260, score-0.63]
</p><p>74 Given the portraits of Newton and Beethoven as targets (a), without ES (λ=0), both the grating images (b) and the superposed images (c) contain visually disturbing discontinuity. [sent-261, score-0.536]
</p><p>75 Adding the smoothness term with λ=1 helps reducing the discontinuity as shown in the grating images (d) and superposed images (e). [sent-262, score-0.572]
</p><p>76 However, when the frequency is too high, moir e´ effects could be observed in the grating images due to insufficient sampling limited by the image resolution. [sent-265, score-1.304]
</p><p>77 The proposed method can also be used for hiding a secret image within two grating images. [sent-271, score-0.587]
</p><p>78 For this application, it is important to hide the image well so that the secret image cannot be visible with only one of the two grating images. [sent-273, score-0.516]
</p><p>79 For achieving this goal, in addition to using the smoothness term to spread the information into two gratings, we have also added noise appearance phase functions to make the grating images more difficult to decrypt. [sent-274, score-0.736]
</p><p>80 We would like to generate a key image K and a set of information images L1 to LN such that, when superposing K on Li, the moir e´ pattern looks like the target image Ii. [sent-275, score-0.967]
</p><p>81 In order to spread information to both K and Li so that the hidden information is not obvious in either, similar to moir e´ art, we add a smoothness term and have the following energy function: E = EA + λES + E, where the appearance term EA is given by: ? [sent-277, score-0.944]
</p><p>82 2  A larger λ in the energy function helps hiding the target images better, but resulting in more blur moir e´ pattern as well. [sent-350, score-1.004]
</p><p>83 Even with the smoothness term, the target image could still be slightly visible in the grating images as shown in Figure 8(b) especially when the secret image contains discontinuities. [sent-351, score-0.631]
</p><p>84 Unlike moir e´ art, φA does not have the requirement in Equation 17 and can be an arbitrary phase function. [sent-353, score-0.949]
</p><p>85 Optical superposition One advantage ofthe proposed approach is that the moir e´ superposition can be performed without computers. [sent-365, score-1.112]
</p><p>86 Although in principle, grating images need to be perfectly aligned, in practice, we found that the hidden images are still recognizable with imperfect alignments. [sent-372, score-0.52]
</p><p>87 For example, the sampling pattern of the color filter array (CFA) in a digital camera serves as a grating pattern G due to sampling. [sent-374, score-0.516]
</p><p>88 Given a target image I and a known grating pattern G, we can find the image L = pL (φL (x, y)) by obtaining φL with φL (x, y) = φG (x, y) + Ψ(x, y) ,  (21)  (Iˆ). [sent-375, score-0.536]
</p><p>89 However, because one of the grating image is given, we do not have the freedom to add appearance phase functions. [sent-378, score-0.629]
</p><p>90 We printed the two grating images ofFigure 6 on a transparency and a paper respectively. [sent-383, score-0.544]
</p><p>91 (a)gratingmage(b)scre ncapturedbythecamera  Figure 11: With the known Bayer pattern G of the RICOH  R7 digital camera, we can obtain the grating image (a). [sent-386, score-0.486]
</p><p>92 Note that the hidden image seems recognizable in the grating image because in this scenario we cannot add a noise phase function. [sent-388, score-0.647]
</p><p>93 Because of the limited range of pm, our method can only generate moir e´ images with lower contrast. [sent-393, score-0.822]
</p><p>94 However, in the cases when high-order moir e´ components become significant, they could disturb the appearance of the target images and make them less recognizable. [sent-399, score-0.955]
</p><p>95 Finally, the resolution and frequency content of the hidden images are limited by the process of moir e´ phenomenon. [sent-400, score-0.882]
</p><p>96 Conclusions This paper addresses the problem of designing moir e´ patterns. [sent-402, score-0.796]
</p><p>97 In addition, we introduce the phase appearance function to further decorrelate the grating images and the target image, making target image invisible in the grating images. [sent-404, score-1.224]
</p><p>98 Our optimization function is customized for each problem and the phase function can be controlled to decorrelate the target images and grating images. [sent-405, score-0.686]
</p><p>99 While successful in many ways, the effective resolution of the synthesized moir e´ image is less than the resolution of the target image. [sent-407, score-0.858]
</p><p>100 In addition, the moir e´ image has lower contrast and brightness. [sent-408, score-0.796]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('moir', 0.796), ('grating', 0.444), ('periodic', 0.159), ('superposition', 0.158), ('phase', 0.153), ('hiding', 0.103), ('profiles', 0.073), ('gratings', 0.072), ('profile', 0.064), ('target', 0.062), ('printed', 0.059), ('pm', 0.056), ('modulation', 0.055), ('superposed', 0.051), ('overlaying', 0.046), ('superposing', 0.045), ('wave', 0.041), ('secret', 0.04), ('smoothness', 0.039), ('hidden', 0.037), ('frequency', 0.036), ('cryptography', 0.036), ('transparencies', 0.036), ('curvilinear', 0.033), ('phenomenon', 0.033), ('appearance', 0.032), ('watermarking', 0.032), ('pattern', 0.03), ('transparency', 0.028), ('resembles', 0.027), ('functions', 0.027), ('waves', 0.027), ('synthesizing', 0.026), ('fourier', 0.025), ('components', 0.025), ('perlin', 0.024), ('cos', 0.022), ('theorem', 0.022), ('synthesis', 0.021), ('patterns', 0.021), ('looks', 0.021), ('spectra', 0.02), ('equation', 0.019), ('fundamental', 0.019), ('repetitive', 0.019), ('cosine', 0.019), ('art', 0.018), ('visible', 0.018), ('beethoven', 0.018), ('carrier', 0.018), ('cfa', 0.018), ('cosx', 0.018), ('downwards', 0.018), ('hersch', 0.018), ('starry', 0.018), ('period', 0.018), ('tone', 0.017), ('optical', 0.017), ('es', 0.017), ('spread', 0.016), ('scream', 0.016), ('hides', 0.016), ('ricoh', 0.016), ('luxury', 0.016), ('mysterious', 0.016), ('ea', 0.016), ('bending', 0.016), ('decoding', 0.016), ('could', 0.015), ('visually', 0.015), ('adding', 0.015), ('lebanon', 0.015), ('security', 0.015), ('interference', 0.014), ('hide', 0.014), ('decorrelate', 0.014), ('noticeable', 0.014), ('spectrum', 0.013), ('vertical', 0.013), ('images', 0.013), ('magnitude', 0.013), ('grayscale', 0.013), ('communications', 0.013), ('recognizable', 0.013), ('unexpected', 0.013), ('displaying', 0.013), ('range', 0.013), ('term', 0.012), ('resultant', 0.012), ('digital', 0.012), ('undesired', 0.012), ('bend', 0.012), ('coefficients', 0.012), ('become', 0.012), ('stripe', 0.012), ('night', 0.012), ('desired', 0.012), ('intensity', 0.012), ('optics', 0.011), ('impacts', 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="413-tfidf-1" href="./iccv-2013-Target-Driven_Moire_Pattern_Synthesis_by_Phase_Modulation.html">413 iccv-2013-Target-Driven Moire Pattern Synthesis by Phase Modulation</a></p>
<p>Author: Pei-Hen Tsai, Yung-Yu Chuang</p><p>Abstract: This paper investigates an approach for generating two grating images so that the moir e´ pattern of their superposition resembles the target image. Our method is grounded on the fundamental moir e´ theorem. By focusing on the visually most dominant (1, −1)-moir e´ component, we obtain the phase smto ddoumlaintiaonnt c (o1n,s−tr1a)in-mt on the phase shifts bee otwbteaeinn the two grating images. For improving visual appearance of the grating images and hiding capability the embedded image, a smoothness term is added to spread information between the two grating images and an appearance phase function is used to add irregular structures into grating images. The grating images can be printed on transparencies and the hidden image decoding can be performed optically by overlaying them together. The proposed method enables the creation of moir e´ art and allows visual decoding without computers.</p><p>2 0.044155274 <a title="413-tfidf-2" href="./iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length.html">407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</a></p>
<p>Author: Nicolas Martin, Vincent Couture, Sébastien Roy</p><p>Abstract: We present a scanning method that recovers dense subpixel camera-projector correspondence without requiring any photometric calibration nor preliminary knowledge of their relative geometry. Subpixel accuracy is achieved by considering several zero-crossings defined by the difference between pairs of unstructured patterns. We use gray-level band-pass white noise patterns that increase robustness to indirect lighting and scene discontinuities. Simulated and experimental results show that our method recovers scene geometry with high subpixel precision, and that it can handle many challenges of active reconstruction systems. We compare our results to state of the art methods such as micro phase shifting and modulated phase shifting.</p><p>3 0.027901536 <a title="413-tfidf-3" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>Author: Hongteng Xu, Hongyuan Zha</p><p>Abstract: Data sparsity has been a thorny issuefor manifold-based image synthesis, and in this paper we address this critical problem by leveraging ideas from transfer learning. Specifically, we propose methods based on generating auxiliary data in the form of synthetic samples using transformations of the original sparse samples. To incorporate the auxiliary data, we propose a weighted data synthesis method, which adaptively selects from the generated samples for inclusion during the manifold learning process via a weighted iterative algorithm. To demonstrate the feasibility of the proposed method, we apply it to the problem of face image synthesis from sparse samples. Compared with existing methods, the proposed method shows encouraging results with good performance improvements.</p><p>4 0.027009096 <a title="413-tfidf-4" href="./iccv-2013-Image_Set_Classification_Using_Holistic_Multiple_Order_Statistics_Features_and_Localized_Multi-kernel_Metric_Learning.html">212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</a></p>
<p>Author: Jiwen Lu, Gang Wang, Pierre Moulin</p><p>Abstract: This paper presents a new approach for image set classification, where each training and testing example contains a set of image instances of an object captured from varying viewpoints or under varying illuminations. While a number of image set classification methods have been proposed in recent years, most of them model each image set as a single linear subspace or mixture of linear subspaces, which may lose some discriminative information for classification. To address this, we propose exploring multiple order statistics as features of image sets, and develop a localized multikernel metric learning (LMKML) algorithm to effectively combine different order statistics information for classification. Our method achieves the state-of-the-art performance on four widely used databases including the Honda/UCSD, CMU Mobo, and Youtube face datasets, and the ETH-80 object dataset.</p><p>5 0.026321013 <a title="413-tfidf-5" href="./iccv-2013-Unsupervised_Visual_Domain_Adaptation_Using_Subspace_Alignment.html">438 iccv-2013-Unsupervised Visual Domain Adaptation Using Subspace Alignment</a></p>
<p>Author: Basura Fernando, Amaury Habrard, Marc Sebban, Tinne Tuytelaars</p><p>Abstract: In this paper, we introduce a new domain adaptation (DA) algorithm where the source and target domains are represented by subspaces described by eigenvectors. In this context, our method seeks a domain adaptation solution by learning a mapping function which aligns the source subspace with the target one. We show that the solution of the corresponding optimization problem can be obtained in a simple closed form, leading to an extremely fast algorithm. We use a theoretical result to tune the unique hyperparameter corresponding to the size of the subspaces. We run our method on various datasets and show that, despite its intrinsic simplicity, it outperforms state of the art DA methods.</p><p>6 0.02558646 <a title="413-tfidf-6" href="./iccv-2013-Local_Signal_Equalization_for_Correspondence_Matching.html">255 iccv-2013-Local Signal Equalization for Correspondence Matching</a></p>
<p>7 0.025401624 <a title="413-tfidf-7" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>8 0.023573691 <a title="413-tfidf-8" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>9 0.023480127 <a title="413-tfidf-9" href="./iccv-2013-Detecting_Irregular_Curvilinear_Structures_in_Gray_Scale_and_Color_Imagery_Using_Multi-directional_Oriented_Flux.html">112 iccv-2013-Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using Multi-directional Oriented Flux</a></p>
<p>10 0.023221582 <a title="413-tfidf-10" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>11 0.021265306 <a title="413-tfidf-11" href="./iccv-2013-Locally_Affine_Sparse-to-Dense_Matching_for_Motion_and_Occlusion_Estimation.html">256 iccv-2013-Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation</a></p>
<p>12 0.020784587 <a title="413-tfidf-12" href="./iccv-2013-Unsupervised_Domain_Adaptation_by_Domain_Invariant_Projection.html">435 iccv-2013-Unsupervised Domain Adaptation by Domain Invariant Projection</a></p>
<p>13 0.020583542 <a title="413-tfidf-13" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>14 0.020527659 <a title="413-tfidf-14" href="./iccv-2013-Tracking_Revisited_Using_RGBD_Camera%3A_Unified_Benchmark_and_Baselines.html">424 iccv-2013-Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines</a></p>
<p>15 0.019740727 <a title="413-tfidf-15" href="./iccv-2013-Active_MAP_Inference_in_CRFs_for_Efficient_Semantic_Segmentation.html">42 iccv-2013-Active MAP Inference in CRFs for Efficient Semantic Segmentation</a></p>
<p>16 0.019186864 <a title="413-tfidf-16" href="./iccv-2013-Optical_Flow_via_Locally_Adaptive_Fusion_of_Complementary_Data_Costs.html">300 iccv-2013-Optical Flow via Locally Adaptive Fusion of Complementary Data Costs</a></p>
<p>17 0.018452678 <a title="413-tfidf-17" href="./iccv-2013-Automatic_Kronecker_Product_Model_Based_Detection_of_Repeated_Patterns_in_2D_Urban_Images.html">55 iccv-2013-Automatic Kronecker Product Model Based Detection of Repeated Patterns in 2D Urban Images</a></p>
<p>18 0.018431466 <a title="413-tfidf-18" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>19 0.018325418 <a title="413-tfidf-19" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>20 0.018180052 <a title="413-tfidf-20" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.047), (1, -0.017), (2, -0.013), (3, -0.007), (4, -0.009), (5, 0.004), (6, 0.003), (7, -0.009), (8, 0.004), (9, -0.005), (10, -0.008), (11, -0.017), (12, 0.009), (13, 0.004), (14, 0.029), (15, -0.007), (16, -0.018), (17, 0.005), (18, 0.005), (19, -0.003), (20, 0.016), (21, -0.003), (22, 0.023), (23, -0.012), (24, 0.02), (25, 0.011), (26, 0.006), (27, -0.04), (28, -0.025), (29, -0.011), (30, 0.018), (31, 0.024), (32, 0.005), (33, -0.017), (34, 0.028), (35, -0.006), (36, -0.0), (37, 0.018), (38, -0.042), (39, 0.035), (40, -0.011), (41, -0.04), (42, -0.003), (43, -0.012), (44, -0.025), (45, 0.001), (46, -0.012), (47, 0.04), (48, 0.004), (49, 0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8148458 <a title="413-lsi-1" href="./iccv-2013-Target-Driven_Moire_Pattern_Synthesis_by_Phase_Modulation.html">413 iccv-2013-Target-Driven Moire Pattern Synthesis by Phase Modulation</a></p>
<p>Author: Pei-Hen Tsai, Yung-Yu Chuang</p><p>Abstract: This paper investigates an approach for generating two grating images so that the moir e´ pattern of their superposition resembles the target image. Our method is grounded on the fundamental moir e´ theorem. By focusing on the visually most dominant (1, −1)-moir e´ component, we obtain the phase smto ddoumlaintiaonnt c (o1n,s−tr1a)in-mt on the phase shifts bee otwbteaeinn the two grating images. For improving visual appearance of the grating images and hiding capability the embedded image, a smoothness term is added to spread information between the two grating images and an appearance phase function is used to add irregular structures into grating images. The grating images can be printed on transparencies and the hidden image decoding can be performed optically by overlaying them together. The proposed method enables the creation of moir e´ art and allows visual decoding without computers.</p><p>2 0.48588723 <a title="413-lsi-2" href="./iccv-2013-Incorporating_Cloud_Distribution_in_Sky_Representation.html">215 iccv-2013-Incorporating Cloud Distribution in Sky Representation</a></p>
<p>Author: Kuan-Chuan Peng, Tsuhan Chen</p><p>Abstract: Most sky models only describe the cloudiness ofthe overall sky by a single category or parameter such as sky index, which does not account for the distribution of the clouds across the sky. To capture variable cloudiness, we extend the concept of sky index to a random field indicating the level of cloudiness of each sky pixel in our proposed sky representation based on the Igawa sky model. We formulate the problem of solving the sky index of every sky pixel as a labeling problem, where an approximate solution can be efficiently found. Experimental results show that our proposed sky model has better expressiveness, stability with respect to variation in camera parameters, and geo-location estimation in outdoor images compared to the uniform sky index model. Potential applications of our proposed sky model include sky image rendering, where sky images can be generated with an arbitrary cloud distribution at any time and any location, previously impossible with traditional sky models.</p><p>3 0.47795323 <a title="413-lsi-3" href="./iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length.html">407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</a></p>
<p>Author: Nicolas Martin, Vincent Couture, Sébastien Roy</p><p>Abstract: We present a scanning method that recovers dense subpixel camera-projector correspondence without requiring any photometric calibration nor preliminary knowledge of their relative geometry. Subpixel accuracy is achieved by considering several zero-crossings defined by the difference between pairs of unstructured patterns. We use gray-level band-pass white noise patterns that increase robustness to indirect lighting and scene discontinuities. Simulated and experimental results show that our method recovers scene geometry with high subpixel precision, and that it can handle many challenges of active reconstruction systems. We compare our results to state of the art methods such as micro phase shifting and modulated phase shifting.</p><p>4 0.47065043 <a title="413-lsi-4" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>Author: Andy J. Ma, Pong C. Yuen, Jiawei Li</p><p>Abstract: This paper addresses a new person re-identification problem without the label information of persons under non-overlapping target cameras. Given the matched (positive) and unmatched (negative) image pairs from source domain cameras, as well as unmatched (negative) image pairs which can be easily generated from target domain cameras, we propose a Domain Transfer Ranked Support Vector Machines (DTRSVM) method for re-identification under target domain cameras. To overcome the problems introduced due to the absence of matched (positive) image pairs in target domain, we relax the discriminative constraint to a necessary condition only relying on the positive mean in target domain. By estimating the target positive mean using source and target domain data, a new discriminative model with high confidence in target positive mean and low confidence in target negative image pairs is developed. Since the necessary condition may not truly preserve the discriminability, multi-task support vector ranking is proposed to incorporate the training data from source domain with label information. Experimental results show that the proposed DTRSVM outperforms existing methods without using label information in target cameras. And the top 30 rank accuracy can be improved by the proposed method upto 9.40% on publicly available person re-identification datasets.</p><p>5 0.440806 <a title="413-lsi-5" href="./iccv-2013-Automatic_Kronecker_Product_Model_Based_Detection_of_Repeated_Patterns_in_2D_Urban_Images.html">55 iccv-2013-Automatic Kronecker Product Model Based Detection of Repeated Patterns in 2D Urban Images</a></p>
<p>Author: Juan Liu, Emmanouil Psarakis, Ioannis Stamos</p><p>Abstract: Repeated patterns (such as windows, tiles, balconies and doors) are prominent and significant features in urban scenes. Therefore, detection of these repeated patterns becomes very important for city scene analysis. This paper attacks the problem of repeated patterns detection in a precise, efficient and automatic way, by combining traditional feature extraction followed by a Kronecker product lowrank modeling approach. Our method is tailored for 2D images of building fac ¸ades. We have developed algorithms for automatic selection ofa representative texture withinfa ¸cade images using vanishing points and Harris corners. After rectifying the input images, we describe novel algorithms that extract repeated patterns by using Kronecker product based modeling that is based on a solid theoretical foundation. Our approach is unique and has not ever been used for fac ¸ade analysis. We have tested our algorithms in a large set of images.</p><p>6 0.43966755 <a title="413-lsi-6" href="./iccv-2013-Global_Fusion_of_Relative_Motions_for_Robust%2C_Accurate_and_Scalable_Structure_from_Motion.html">184 iccv-2013-Global Fusion of Relative Motions for Robust, Accurate and Scalable Structure from Motion</a></p>
<p>7 0.42923525 <a title="413-lsi-7" href="./iccv-2013-Enhanced_Continuous_Tabu_Search_for_Parameter_Estimation_in_Multiview_Geometry.html">141 iccv-2013-Enhanced Continuous Tabu Search for Parameter Estimation in Multiview Geometry</a></p>
<p>8 0.4246251 <a title="413-lsi-8" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>9 0.42408636 <a title="413-lsi-9" href="./iccv-2013-Cross-View_Action_Recognition_over_Heterogeneous_Feature_Spaces.html">99 iccv-2013-Cross-View Action Recognition over Heterogeneous Feature Spaces</a></p>
<p>10 0.42122364 <a title="413-lsi-10" href="./iccv-2013-Unsupervised_Domain_Adaptation_by_Domain_Invariant_Projection.html">435 iccv-2013-Unsupervised Domain Adaptation by Domain Invariant Projection</a></p>
<p>11 0.40904588 <a title="413-lsi-11" href="./iccv-2013-Efficient_and_Robust_Large-Scale_Rotation_Averaging.html">138 iccv-2013-Efficient and Robust Large-Scale Rotation Averaging</a></p>
<p>12 0.40620524 <a title="413-lsi-12" href="./iccv-2013-The_Way_They_Move%3A_Tracking_Multiple_Targets_with_Similar_Appearance.html">418 iccv-2013-The Way They Move: Tracking Multiple Targets with Similar Appearance</a></p>
<p>13 0.39999649 <a title="413-lsi-13" href="./iccv-2013-Unsupervised_Visual_Domain_Adaptation_Using_Subspace_Alignment.html">438 iccv-2013-Unsupervised Visual Domain Adaptation Using Subspace Alignment</a></p>
<p>14 0.39965576 <a title="413-lsi-14" href="./iccv-2013-Structured_Light_in_Sunlight.html">405 iccv-2013-Structured Light in Sunlight</a></p>
<p>15 0.39257628 <a title="413-lsi-15" href="./iccv-2013-Separating_Reflective_and_Fluorescent_Components_Using_High_Frequency_Illumination_in_the_Spectral_Domain.html">385 iccv-2013-Separating Reflective and Fluorescent Components Using High Frequency Illumination in the Spectral Domain</a></p>
<p>16 0.3912558 <a title="413-lsi-16" href="./iccv-2013-Finding_Causal_Interactions_in_Video_Sequences.html">167 iccv-2013-Finding Causal Interactions in Video Sequences</a></p>
<p>17 0.38711947 <a title="413-lsi-17" href="./iccv-2013-Modeling_the_Calibration_Pipeline_of_the_Lytro_Camera_for_High_Quality_Light-Field_Image_Reconstruction.html">271 iccv-2013-Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction</a></p>
<p>18 0.386866 <a title="413-lsi-18" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>19 0.38363504 <a title="413-lsi-19" href="./iccv-2013-A_Global_Linear_Method_for_Camera_Pose_Registration.html">17 iccv-2013-A Global Linear Method for Camera Pose Registration</a></p>
<p>20 0.38269517 <a title="413-lsi-20" href="./iccv-2013-Extrinsic_Camera_Calibration_without_a_Direct_View_Using_Spherical_Mirror.html">152 iccv-2013-Extrinsic Camera Calibration without a Direct View Using Spherical Mirror</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.047), (7, 0.031), (12, 0.429), (26, 0.06), (31, 0.037), (40, 0.012), (42, 0.058), (64, 0.02), (73, 0.026), (89, 0.099)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74480158 <a title="413-lda-1" href="./iccv-2013-Target-Driven_Moire_Pattern_Synthesis_by_Phase_Modulation.html">413 iccv-2013-Target-Driven Moire Pattern Synthesis by Phase Modulation</a></p>
<p>Author: Pei-Hen Tsai, Yung-Yu Chuang</p><p>Abstract: This paper investigates an approach for generating two grating images so that the moir e´ pattern of their superposition resembles the target image. Our method is grounded on the fundamental moir e´ theorem. By focusing on the visually most dominant (1, −1)-moir e´ component, we obtain the phase smto ddoumlaintiaonnt c (o1n,s−tr1a)in-mt on the phase shifts bee otwbteaeinn the two grating images. For improving visual appearance of the grating images and hiding capability the embedded image, a smoothness term is added to spread information between the two grating images and an appearance phase function is used to add irregular structures into grating images. The grating images can be printed on transparencies and the hidden image decoding can be performed optically by overlaying them together. The proposed method enables the creation of moir e´ art and allows visual decoding without computers.</p><p>2 0.68670332 <a title="413-lda-2" href="./iccv-2013-POP%3A_Person_Re-identification_Post-rank_Optimisation.html">305 iccv-2013-POP: Person Re-identification Post-rank Optimisation</a></p>
<p>Author: Chunxiao Liu, Chen Change Loy, Shaogang Gong, Guijin Wang</p><p>Abstract: Owing to visual ambiguities and disparities, person reidentification methods inevitably produce suboptimal ranklist, which still requires exhaustive human eyeballing to identify the correct target from hundreds of different likelycandidates. Existing re-identification studies focus on improving the ranking performance, but rarely look into the critical problem of optimising the time-consuming and error-prone post-rank visual search at the user end. In this study, we present a novel one-shot Post-rank OPtimisation (POP) method, which allows a user to quickly refine their search by either “one-shot” or a couple of sparse negative selections during a re-identification process. We conduct systematic behavioural studies to understand user’s searching behaviour and show that the proposed method allows correct re-identification to converge 2.6 times faster than the conventional exhaustive search. Importantly, through extensive evaluations we demonstrate that the method is capable of achieving significant improvement over the stateof-the-art distance metric learning based ranking models, even with just “one shot” feedback optimisation, by as much as over 30% performance improvement for rank 1reidentification on the VIPeR and i-LIDS datasets.</p><p>3 0.58617532 <a title="413-lda-3" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>Author: Mohamed Elhoseiny, Babak Saleh, Ahmed Elgammal</p><p>Abstract: The main question we address in this paper is how to use purely textual description of categories with no training images to learn visual classifiers for these categories. We propose an approach for zero-shot learning of object categories where the description of unseen categories comes in the form of typical text such as an encyclopedia entry, without the need to explicitly defined attributes. We propose and investigate two baseline formulations, based on regression and domain adaptation. Then, we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to predict the classifier parameters for new classes. We applied the proposed approach on two fine-grained categorization datasets, and the results indicate successful classifier prediction.</p><p>4 0.52130032 <a title="413-lda-4" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>Author: Michael Van_Den_Bergh, Gemma Roig, Xavier Boix, Santiago Manen, Luc Van_Gool</p><p>Abstract: Superpixel and objectness algorithms are broadly used as a pre-processing step to generate support regions and to speed-up further computations. Recently, many algorithms have been extended to video in order to exploit the temporal consistency between frames. However, most methods are computationally too expensive for real-time applications. We introduce an online, real-time video superpixel algorithm based on the recently proposed SEEDS superpixels. A new capability is incorporated which delivers multiple diverse samples (hypotheses) of superpixels in the same image or video sequence. The multiple samples are shown to provide a strong cue to efficiently measure the objectness of image windows, and we introduce the novel concept of objectness in temporal windows. Experiments show that the video superpixels achieve comparable performance to state-of-the-art offline methods while running at 30 fps on a single 2.8 GHz i7 CPU. State-of-the-art performance on objectness is also demonstrated, yet orders of magnitude faster and extended to temporal windows in video.</p><p>5 0.49826777 <a title="413-lda-5" href="./iccv-2013-The_Moving_Pose%3A_An_Efficient_3D_Kinematics_Descriptor_for_Low-Latency_Action_Recognition_and_Detection.html">417 iccv-2013-The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</a></p>
<p>Author: Mihai Zanfir, Marius Leordeanu, Cristian Sminchisescu</p><p>Abstract: Human action recognition under low observational latency is receiving a growing interest in computer vision due to rapidly developing technologies in human-robot interaction, computer gaming and surveillance. In this paper we propose a fast, simple, yet powerful non-parametric Moving Pose (MP)frameworkfor low-latency human action and activity recognition. Central to our methodology is a moving pose descriptor that considers both pose information as well as differential quantities (speed and acceleration) of the human body joints within a short time window around the current frame. The proposed descriptor is used in conjunction with a modified kNN classifier that considers both the temporal location of a particular frame within the action sequence as well as the discrimination power of its moving pose descriptor compared to other frames in the training set. The resulting method is non-parametric and enables low-latency recognition, one-shot learning, and action detection in difficult unsegmented sequences. Moreover, the framework is real-time, scalable, and outperforms more sophisticated approaches on challenging benchmarks like MSR-Action3D or MSR-DailyActivities3D.</p><p>6 0.49055451 <a title="413-lda-6" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>7 0.43226552 <a title="413-lda-7" href="./iccv-2013-Monte_Carlo_Tree_Search_for_Scheduling_Activity_Recognition.html">274 iccv-2013-Monte Carlo Tree Search for Scheduling Activity Recognition</a></p>
<p>8 0.4128978 <a title="413-lda-8" href="./iccv-2013-Efficient_Pedestrian_Detection_by_Directly_Optimizing_the_Partial_Area_under_the_ROC_Curve.html">136 iccv-2013-Efficient Pedestrian Detection by Directly Optimizing the Partial Area under the ROC Curve</a></p>
<p>9 0.40615207 <a title="413-lda-9" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<p>10 0.40167981 <a title="413-lda-10" href="./iccv-2013-Translating_Video_Content_to_Natural_Language_Descriptions.html">428 iccv-2013-Translating Video Content to Natural Language Descriptions</a></p>
<p>11 0.3985883 <a title="413-lda-11" href="./iccv-2013-YouTube2Text%3A_Recognizing_and_Describing_Arbitrary_Activities_Using_Semantic_Hierarchies_and_Zero-Shot_Recognition.html">452 iccv-2013-YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition</a></p>
<p>12 0.39048821 <a title="413-lda-12" href="./iccv-2013-Video_Event_Understanding_Using_Natural_Language_Descriptions.html">440 iccv-2013-Video Event Understanding Using Natural Language Descriptions</a></p>
<p>13 0.38777441 <a title="413-lda-13" href="./iccv-2013-Pictorial_Human_Spaces%3A_How_Well_Do_Humans_Perceive_a_3D_Articulated_Pose%3F.html">316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</a></p>
<p>14 0.37960601 <a title="413-lda-14" href="./iccv-2013-Analysis_of_Scores%2C_Datasets%2C_and_Models_in_Visual_Saliency_Prediction.html">50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</a></p>
<p>15 0.37383676 <a title="413-lda-15" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<p>16 0.37052333 <a title="413-lda-16" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>17 0.36980987 <a title="413-lda-17" href="./iccv-2013-Beyond_Hard_Negative_Mining%3A_Efficient_Detector_Learning_via_Block-Circulant_Decomposition.html">61 iccv-2013-Beyond Hard Negative Mining: Efficient Detector Learning via Block-Circulant Decomposition</a></p>
<p>18 0.36786127 <a title="413-lda-18" href="./iccv-2013-Learning_Hash_Codes_with_Listwise_Supervision.html">239 iccv-2013-Learning Hash Codes with Listwise Supervision</a></p>
<p>19 0.36778146 <a title="413-lda-19" href="./iccv-2013-ACTIVE%3A_Activity_Concept_Transitions_in_Video_Event_Classification.html">4 iccv-2013-ACTIVE: Activity Concept Transitions in Video Event Classification</a></p>
<p>20 0.36738867 <a title="413-lda-20" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
