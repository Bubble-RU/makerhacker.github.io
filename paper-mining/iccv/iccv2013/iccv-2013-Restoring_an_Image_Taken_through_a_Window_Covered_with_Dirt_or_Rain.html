<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-351" href="#">iccv2013-351</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</h1>
<br/><p>Source: <a title="iccv-2013-351-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Eigen_Restoring_an_Image_2013_ICCV_paper.pdf">pdf</a></p><p>Author: David Eigen, Dilip Krishnan, Rob Fergus</p><p>Abstract: Photographs taken through a window are often compromised by dirt or rain present on the window surface. Common cases of this include pictures taken from inside a vehicle, or outdoor security cameras mounted inside a protective enclosure. At capture time, defocus can be used to remove the artifacts, but this relies on achieving a shallow depth-of-field and placement of the camera close to the window. Instead, we present a post-capture image processing solution that can remove localized rain and dirt artifacts from a single image. We collect a dataset of clean/corrupted image pairs which are then used to train a specialized form of convolutional neural network. This learns how to map corrupted image patches to clean ones, implicitly capturing the characteristic appearance of dirt and water droplets in natural images. Our models demonstrate effective removal of dirt and rain in outdoor test conditions.</p><p>Reference: <a title="iccv-2013-351-reference" href="../iccv2013_reference/iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu gen ip  Abstract Photographs taken through a window are often compromised by dirt or rain present on the window surface. [sent-4, score-1.074]
</p><p>2 Common cases of this include pictures taken from inside a vehicle, or outdoor security cameras mounted inside a protective enclosure. [sent-5, score-0.196]
</p><p>3 Instead, we present a post-capture image processing solution that can remove localized rain and dirt artifacts from a single image. [sent-7, score-1.164]
</p><p>4 We collect a dataset of clean/corrupted image pairs which are then used to train a specialized form of convolutional neural network. [sent-8, score-0.232]
</p><p>5 This learns how to map corrupted image patches to clean ones, implicitly capturing the characteristic appearance of dirt and water droplets in natural images. [sent-9, score-1.023]
</p><p>6 Our models demonstrate effective removal of dirt and rain in outdoor test conditions. [sent-10, score-1.09]
</p><p>7 However, in this paper we address the particular situation where the window is covered with dirt or water drops, resulting from rain. [sent-21, score-0.769]
</p><p>8 This requires placing the camera right up against  along with the output of our neural network model, trained to remove this type of corruption. [sent-25, score-0.581]
</p><p>9 The irregular size and appearance of the rain makes it difficult to remove with existing methods. [sent-26, score-0.43]
</p><p>10 Correspondingly, many shots with smartphone cameras through dirty or rainy glass still have significant artifacts, as shown in Fig. [sent-30, score-0.33]
</p><p>11 In this paper we instead restore the image after capture, treating the dirt or rain as a structured form of image noise. [sent-32, score-1.031]
</p><p>12 However, the vast majority of this literature is concerned with additive white Gaussian noise, quite different to the image artifacts resulting from dirt or water drops. [sent-35, score-0.891]
</p><p>13 Classic approaches such as median or bilateral filtering have no way —  633  of leveraging this structure, thus cannot effectively remove the artifacts (see Section 5). [sent-37, score-0.315]
</p><p>14 Our approach is to use a specialized convolutional neural network to predict clean patches, given dirty or clean ones as input. [sent-38, score-0.938]
</p><p>15 By asking the network to produce a clean output, regardless of the corruption level of the input, it implicitly must both detect the corruption and, if present, in-paint over it. [sent-39, score-0.857]
</p><p>16 However, although training is somewhat complex, test-time operation is simple: a new image is presented to the neural network and it directly outputs a restored image. [sent-42, score-0.485]
</p><p>17 These approaches remove additive white Gaussian noise (AWGN) by building a generative model of clean image patches. [sent-48, score-0.258]
</p><p>18 In this paper, however, we focus on more complex structured corruption, and address it using a neural network that directly maps corrupt images to clean ones; this obviates the slow inference procedures used by most generative models. [sent-49, score-0.645]
</p><p>19 [2], which applies a large neural network to a range of non-AWGN denoising tasks, such as salt-and-pepper noise and JPEG quantization artifacts. [sent-54, score-0.603]
</p><p>20 Although more challenging than AWGN, the corruption is still significantly easier than the highly variable dirt and rain drops that we address. [sent-55, score-1.179]
</p><p>21 Furthermore, our network has important architectural differences that are crucial for obtaining good performance on these tasks. [sent-56, score-0.38]
</p><p>22 Removing localized corruption can be considered a form of blind inpainting, where the position of the corrupted re-  gions is not given (unlike traditional inpainting [5]). [sent-57, score-0.251]
</p><p>23 [20] showed how a neural network can perform blind inpainting, demonstrating the removal of text synthetically placed in an image. [sent-61, score-0.612]
</p><p>24 This work is close to ours, but the solid-color text has quite different statistics to natural images, thus is easier to remove than rain or dirt which vary greatly in appearance and can resemble legitimate image structures. [sent-62, score-1.07]
</p><p>25 Several papers explore the removal of rain from images. [sent-65, score-0.442]
</p><p>26 [19] and Zhou and Lin [22] demonstrate dirt and dust removal. [sent-75, score-0.688]
</p><p>27 Approach To restore an image from a corrupt input, we predict a clean output using a specialized form of convolutional neural network [12]. [sent-78, score-0.878]
</p><p>28 The same network architecture is used for all forms of corruption; however, a different network is trained for dirt and for rain. [sent-79, score-1.428]
</p><p>29 This allows the network to tailor its detection capabilities for each task. [sent-80, score-0.38]
</p><p>30 Network Architecture Given a noisy image x, our goal is to predict a clean image y that is close to the true clean image y∗ . [sent-83, score-0.274]
</p><p>31 The network F is composed of a series of layers Fl, each of which applies a linear convolution to its input, followed by an element-wise sigmoid (implemented using hyperbolic tangent). [sent-85, score-0.485]
</p><p>32 Concretely, if the number of layers in the network is L, then F0 (x) Fl (x) F(x)  = =  x tanh(Wl  = m1(WL  ∗  Fl−1 (x) + bl ) ,  ∗ FL−1(x)  l = 1, . [sent-86, score-0.471]
</p><p>33 If nl rise th xe i output GdiBme innpsuiotn i mata layer l s, zthee nN W ×l applies nl convolutions with kernels of size pl pl nl−1, where pl is the spatial support. [sent-90, score-0.406]
</p><p>34 While the first and last layer kernels have a nontrivial spatial component, we restrict the middle layers (2 ≤ l ≤ sLp a1l) ctoom use pl t=, w1e, ir. [sent-92, score-0.224]
</p><p>35 A subset of rain model network weights, sorted by l2norm. [sent-98, score-0.748]
</p><p>36 Left: first layer filters which act as detectors for the rain drops. [sent-99, score-0.436]
</p><p>37 Right: top layer filters used to reconstruct the clean patch. [sent-100, score-0.205]
</p><p>38 Thus, W1 applies 5 12 kernels of size 16 16 3, W2 applies 5 12 kernels of size 1k ×rn 1e ×s o5f1 s2iz, aen 1d6 W ×3 1 applies W3 kernels of size 8 8 512. [sent-107, score-0.228]
</p><p>39 Training  We train the weights Wl and biases bl by minimizing the mean squared error over a dataset D = (xi, yi∗) of corresponding noisy and clean image pairs. [sent-113, score-0.219]
</p><p>40 The pairs in the dataset D are random 64 64 pixel subregions oaifr training images Dwi athre a rnadn dwoimtho 6u4t corruption (see Fig. [sent-122, score-0.197]
</p><p>41 Because the input and output kernel sizes of our network differ, the network F produces a 56 56 pixel prediction yi, which is compared against the 5m6id ×dle 5 65 6p x×e 5l 6p pixels oonf ythe true clean subimage yi∗ . [sent-124, score-0.93]
</p><p>42 The gradient is further backpropagated through the network F. [sent-127, score-0.38]
</p><p>43 sTinheg errors ifnrgom w tinhed convolutionallytrained network (c) are less correlated with one another compared to (b), and cancel to produce a better average. [sent-141, score-0.38]
</p><p>44 Since the middle layer convolution in our network has 1 1 spatial support, the network can be viewed as first patchifying tlh seu input, applying a fully-connected anse ufirrsalt network to each patch, and averaging the resulting output patches. [sent-146, score-1.295]
</p><p>45 More explicitly, we can split the input image x into stride-1 overlapping patches {xp} = patchify(x), ianntdo predict a corresponding cchleeasn { patch yp a=t f(xp) fxo)r, each xp using a fully-connected multilayer network f. [sent-147, score-0.553]
</p><p>46 In this context, the convolutional network F can be expressed in terms of the patch-level network f as F(x) = depatchify({f(xp) : xp ∈ patchify(x)}). [sent-149, score-0.918]
</p><p>47 ×  In contrast to [2], our method trains the full network F, including patchification and depatchification. [sent-150, score-0.38]
</p><p>48 When trained with our convolutional network, however, the predictions decorrelate where not perfect, and average to a better output (c). [sent-162, score-0.203]
</p><p>49 Test-Time Evaluation  By restricting the middle layer kernels to have 1 1 spatialB support, our mhee tmhoiddd requires no synchronization until the final summation in the last layer convolution. [sent-165, score-0.199]
</p><p>50 Training Data Collection The network has 753,664 weights and 1,216 biases which need to be set during training. [sent-169, score-0.409]
</p><p>51 We now describe the procedures used to gather the corrupted/clean patch pairs2 used to train each of the dirt and rain models. [sent-171, score-1.034]
</p><p>52 Dirt To train our network to remove dirt noise, we generated clean/noisy image pairs by synthesizing dirt on images. [sent-174, score-1.697]
</p><p>53 Similarly to [9], we also found that dirt noise was well-modeled by an opacity mask and additive component, which we extract from real dirt-on-glass panes in a lab setup. [sent-175, score-0.713]
</p><p>54 These random perturbations are necessary to capture natural variation in the corruption and make the network robust to these changes. [sent-184, score-0.55]
</p><p>55 To find α and αD, we took pictures of several slideprojected backgrounds, both with and without a dirt-on2The corrupt patches still have many unaffected pixels, thus even without clean/clean patch pairs in the training set, the network will still learn to preserve clean input regions. [sent-185, score-0.752]
</p><p>56 The dirt (left column) was added synthetically, while the rain (right column) was obtained from real image pairs. [sent-187, score-0.985]
</p><p>57 Water Droplets Unlike the dirt, water droplets refract light around them and are not well described by a simple additive model. [sent-192, score-0.271]
</p><p>58 Thus, instead of synthesizing the effects of water, we built a training set by taking photographs of multiple scenes with and without the corruption present. [sent-194, score-0.249]
</p><p>59 For corrupt images, we simulated the effect of rain on a window by spraying water on a pane of anti-reflective MgF2-coated glass, taking care to produce drops that closely resemble real rain. [sent-195, score-0.682]
</p><p>60 To limit motion differences between clean and rainy shots, all scenes contained only static objects. [sent-196, score-0.196]
</p><p>61 Baseline Methods We compare our convolutional network against a nonconvolutional patch-level network similar to [2], as well as three baseline approaches: median filtering, bilateral filtering [18, 15], and BM3D [3]. [sent-199, score-1.087]
</p><p>62 On the dirt images, we used an 8 8 window for the median filter, parameters σs = 3 aannd 8 σr 8= w 0nd. [sent-201, score-0.708]
</p><p>63 For the rain images, we used similar parameters, but adjusted for the fact that the images were downsampled by half: 5 5 for the median filter, σs = 2 and σr = 0. [sent-204, score-0.428]
</p><p>64 The nonconvolutional network leaves behind much of the noise, while the median filter causes substantial blurring. [sent-210, score-0.634]
</p><p>65 Dirt We tested dirt removal by running our network on pictures of various scenes taken behind dirt-on-glass panes. [sent-214, score-1.248]
</p><p>66 Both the scenes and glass panes were not present in the training set, ensuring that the network did not simply memorize and match exact patterns. [sent-215, score-0.601]
</p><p>67 Although the training set was composed entirely of synthetic dirt, it was representative enough for the network to perform well in both cases. [sent-217, score-0.45]
</p><p>68 h0e01re, tahned v aalrisoan required t chlaeta nat 6 l4ea ×st 14 pixel i nw athse a patch had a dirt α-mask value of at least 0. [sent-221, score-0.666]
</p><p>69 To compare to [2], we trained a non-convolutional patch-based network with patch sizes corresponding to our convolution kernel sizes, using 20 million 16 16 patches. [sent-223, score-0.489]
</p><p>70 Here, we generated test examples using images and dirt masks held out from the training set, using the process described in Section 3. [sent-228, score-0.667]
</p><p>71 PSNR for our convolutional neural network, nonconvolutional patch-based network, and baselines on a synthetically generated test set of 16 images (8 scenes with 2 different dirt masks). [sent-246, score-0.948]
</p><p>72 than the three baselines, which do not make use of the structure in the corruption that the networks learn. [sent-248, score-0.21]
</p><p>73 We also applied our network to two types of artificial noise absent from the training set: synthetic “snow” made from small white line segments, and “scratches” of random cubic splines. [sent-249, score-0.473]
</p><p>74 50 dB for dirt, the network leaves these corruptions largely intact, producing near-zero PSNR gains of -0. [sent-253, score-0.448]
</p><p>75 This demonstrates that the network learns to remove dirt specifically. [sent-256, score-1.059]
</p><p>76 5 shows a real test image along with our output and the output of the patch-based network and median filter. [sent-260, score-0.506]
</p><p>77 Our method is able to remove most of the corruption while retaining details in the image, particularly around the branches and shutters. [sent-262, score-0.254]
</p><p>78 Our dirt-removal network applied to an image with (a) no corruption, (b) synthetic dirt, (c) artificial “snow” and (d) random “scratches. [sent-264, score-0.423]
</p><p>79 ” Because the network was trained to remove dirt, it successfully restores (b) while leaving the corruptions in (c,d) largely untouched. [sent-265, score-0.494]
</p><p>80 network leaves many pieces of dirt behind, while the median filter loses much detail present in the original. [sent-268, score-1.134]
</p><p>81 The latter is caused by a lack of generalization: although we trained the network to be robust to shape and color by supplying it a range of variations, it will not recognize cases too far from those seen in training. [sent-272, score-0.408]
</p><p>82 Rain We ran the rain removal network on two sets of test data: (i) pictures of scenes taken through a pane of glass on which we sprayed water to simulate rain, and (ii) pictures of scenes taken while it was actually raining, from behind an initially clean glass pane. [sent-278, score-1.707]
</p><p>83 1 Water Droplets Results Examples of our network removing sprayed-on water is shown in Fig. [sent-288, score-0.526]
</p><p>84 As was the case for the dirt images, we were not able to capture accurate ground truth due to illumination changes and subject motion. [sent-290, score-0.617]
</p><p>85 As before, our network is able to remove most of the water droplets, while preserving finer details and edges reasonably well. [sent-294, score-0.563]
</p><p>86 Thus the network can be applied to many scenes substantially different from those seen in training. [sent-301, score-0.411]
</p><p>87 2 Real Rain Results A picture taken using actual rain is shown in Fig. [sent-304, score-0.395]
</p><p>88 To capture the sequence, we set a clean glass pane on a tripod and allowed rain to fall onto it, taking pictures at 20s intervals. [sent-308, score-0.771]
</p><p>89 Even though our network was trained using sprayed-on water, it was still able to remove much of the actual rain. [sent-311, score-0.47]
</p><p>90 The largest failures appear towards the end of the sequence, when the rain on the glass is very heavy and starts to agglomerate, forming droplets larger than our network can handle. [sent-312, score-0.988]
</p><p>91 9 we apply our network to a picture taken using a smartphone on a train. [sent-315, score-0.464]
</p><p>92 While the scene and reflections are preserved, raindrops on the window are removed, though a few small artifacts do remain. [sent-316, score-0.197]
</p><p>93 Summary We have introduced a method for removing rain or dirt artifacts from a single image. [sent-321, score-1.127]
</p><p>94 Although the problem appears  underconstrained, the artifacts have a distinctive appearance which we are able to learn with a specialized convolutional network and a carefully constructed training set. [sent-322, score-0.678]
</p><p>95 Results on real test examples show most artifacts being removed without undue loss of detail, unlike existing approaches such as median or bilateral filtering. [sent-323, score-0.25]
</p><p>96 Using a convolutional network accounts for the error in the final image prediction, providing a significant performance gain over the corresponding patch-based network. [sent-324, score-0.498]
</p><p>97 A second issue is that the corruption cannot be much larger than the training patches. [sent-328, score-0.197]
</p><p>98 as in the rain application, leading to a loss of resolution relative to the original. [sent-331, score-0.368]
</p><p>99 High-performance low-power neural network implementations such as the NeuFlow FPGA/ASIC [6] would make real-time embedded applications of our system feasible. [sent-336, score-0.458]
</p><p>100 Image denosing using a neural network based non-linear filter in the wavelet domain. [sent-477, score-0.516]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dirt', 0.617), ('network', 0.38), ('rain', 0.368), ('corruption', 0.17), ('clean', 0.137), ('glass', 0.126), ('water', 0.121), ('convolutional', 0.118), ('artifacts', 0.117), ('droplets', 0.114), ('denoising', 0.087), ('neural', 0.078), ('pictures', 0.075), ('removal', 0.074), ('nonconvolutional', 0.073), ('dust', 0.071), ('layer', 0.068), ('pane', 0.065), ('remove', 0.062), ('median', 0.06), ('smartphone', 0.057), ('awgn', 0.055), ('pl', 0.055), ('inpainting', 0.054), ('bl', 0.053), ('dirty', 0.052), ('bilateral', 0.051), ('corrupt', 0.05), ('patch', 0.049), ('raindrops', 0.049), ('restore', 0.046), ('behind', 0.044), ('leaves', 0.044), ('synthetic', 0.043), ('kernels', 0.041), ('shots', 0.041), ('snow', 0.04), ('networks', 0.04), ('xp', 0.04), ('occluders', 0.039), ('layers', 0.038), ('depatchify', 0.037), ('neuflow', 0.037), ('panes', 0.037), ('patchify', 0.037), ('protective', 0.037), ('additive', 0.036), ('blurs', 0.036), ('wl', 0.036), ('specialized', 0.036), ('fl', 0.036), ('applies', 0.035), ('patches', 0.034), ('filter', 0.033), ('output', 0.033), ('defocus', 0.033), ('barnum', 0.032), ('roser', 0.032), ('visibly', 0.032), ('convolution', 0.032), ('nl', 0.032), ('outdoor', 0.031), ('yo', 0.031), ('synthetically', 0.031), ('window', 0.031), ('scenes', 0.031), ('burger', 0.03), ('biases', 0.029), ('rainy', 0.028), ('jancsary', 0.028), ('multilayer', 0.028), ('trained', 0.028), ('restoration', 0.028), ('blind', 0.027), ('taken', 0.027), ('training', 0.027), ('cameras', 0.026), ('removing', 0.025), ('wavelet', 0.025), ('filtering', 0.025), ('garg', 0.025), ('predictions', 0.024), ('corruptions', 0.024), ('ramamoorthi', 0.024), ('drops', 0.024), ('architecture', 0.023), ('masks', 0.023), ('resemble', 0.023), ('noise', 0.023), ('removed', 0.022), ('branches', 0.022), ('gu', 0.022), ('transparent', 0.022), ('yp', 0.022), ('middle', 0.022), ('placed', 0.022), ('synthesizing', 0.021), ('yi', 0.021), ('support', 0.021), ('xie', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="351-tfidf-1" href="./iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain.html">351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</a></p>
<p>Author: David Eigen, Dilip Krishnan, Rob Fergus</p><p>Abstract: Photographs taken through a window are often compromised by dirt or rain present on the window surface. Common cases of this include pictures taken from inside a vehicle, or outdoor security cameras mounted inside a protective enclosure. At capture time, defocus can be used to remove the artifacts, but this relies on achieving a shallow depth-of-field and placement of the camera close to the window. Instead, we present a post-capture image processing solution that can remove localized rain and dirt artifacts from a single image. We collect a dataset of clean/corrupted image pairs which are then used to train a specialized form of convolutional neural network. This learns how to map corrupted image patches to clean ones, implicitly capturing the characteristic appearance of dirt and water droplets in natural images. Our models demonstrate effective removal of dirt and rain in outdoor test conditions.</p><p>2 0.34796023 <a title="351-tfidf-2" href="./iccv-2013-A_Generalized_Low-Rank_Appearance_Model_for_Spatio-temporally_Correlated_Rain_Streaks.html">15 iccv-2013-A Generalized Low-Rank Appearance Model for Spatio-temporally Correlated Rain Streaks</a></p>
<p>Author: Yi-Lei Chen, Chiou-Ting Hsu</p><p>Abstract: In this paper, we propose a novel low-rank appearance model for removing rain streaks. Different from previous work, our method needs neither rain pixel detection nor time-consuming dictionary learning stage. Instead, as rain streaks usually reveal similar and repeated patterns on imaging scene, we propose and generalize a low-rank model from matrix to tensor structure in order to capture the spatio-temporally correlated rain streaks. With the appearance model, we thus remove rain streaks from image/video (and also other high-order image structure) in a unified way. Our experimental results demonstrate competitive (or even better) visual quality and efficient run-time in comparison with state of the art.</p><p>3 0.17529632 <a title="351-tfidf-3" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>Author: Chaochao Lu, Deli Zhao, Xiaoou Tang</p><p>Abstract: When face images are taken in the wild, the large variations in facial pose, illumination, and expression make face recognition challenging. The most fundamental problem for face recognition is to measure the similarity between faces. The traditional measurements such as various mathematical norms, Hausdorff distance, and approximate geodesic distance cannot accurately capture the structural information between faces in such complex circumstances. To address this issue, we develop a novel face patch network, based on which we define a new similarity measure called the random path (RP) measure. The RP measure is derivedfrom the collective similarity ofpaths by performing random walks in the network. It can globally characterize the contextual and curved structures of the face space. To apply the RP measure, we construct two kinds of networks: . cuhk . edu . hk the in-face network and the out-face network. The in-face network is drawn from any two face images and captures the local structural information. The out-face network is constructed from all the training face patches, thereby modeling the global structures of face space. The two face networks are structurally complementary and can be combined together to improve the recognition performance. Experiments on the Multi-PIE and LFW benchmarks show that the RP measure outperforms most of the state-of-art algorithms for face recognition.</p><p>4 0.12921683 <a title="351-tfidf-4" href="./iccv-2013-A_Non-parametric_Bayesian_Network_Prior_of_Human_Pose.html">24 iccv-2013-A Non-parametric Bayesian Network Prior of Human Pose</a></p>
<p>Author: Andreas M. Lehrmann, Peter V. Gehler, Sebastian Nowozin</p><p>Abstract: Having a sensible prior of human pose is a vital ingredient for many computer vision applications, including tracking and pose estimation. While the application of global non-parametric approaches and parametric models has led to some success, finding the right balance in terms of flexibility and tractability, as well as estimating model parameters from data has turned out to be challenging. In this work, we introduce a sparse Bayesian network model of human pose that is non-parametric with respect to the estimation of both its graph structure and its local distributions. We describe an efficient sampling scheme for our model and show its tractability for the computation of exact log-likelihoods. We empirically validate our approach on the Human 3.6M dataset and demonstrate superior performance to global models and parametric networks. We further illustrate our model’s ability to represent and compose poses not present in the training set (compositionality) and describe a speed-accuracy trade-off that allows realtime scoring of poses.</p><p>5 0.10160063 <a title="351-tfidf-5" href="./iccv-2013-Pedestrian_Parsing_via_Deep_Decompositional_Network.html">311 iccv-2013-Pedestrian Parsing via Deep Decompositional Network</a></p>
<p>Author: Ping Luo, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: We propose a new Deep Decompositional Network (DDN) for parsing pedestrian images into semantic regions, such as hair, head, body, arms, and legs, where the pedestrians can be heavily occluded. Unlike existing methods based on template matching or Bayesian inference, our approach directly maps low-level visual features to the label maps of body parts with DDN, which is able to accurately estimate complex pose variations with good robustness to occlusions and background clutters. DDN jointly estimates occluded regions and segments body parts by stacking three types of hidden layers: occlusion estimation layers, completion layers, and decomposition layers. The occlusion estimation layers estimate a binary mask, indicating which part of a pedestrian is invisible. The completion layers synthesize low-level features of the invisible part from the original features and the occlusion mask. The decomposition layers directly transform the synthesized visual features to label maps. We devise a new strategy to pre-train these hidden layers, and then fine-tune the entire network using the stochastic gradient descent. Experimental results show that our approach achieves better segmentation accuracy than the state-of-the-art methods on pedestrian images with or without occlusions. Another important contribution of this paper is that it provides a large scale benchmark human parsing dataset1 that includes 3, 673 annotated samples collected from 171 surveillance videos. It is 20 times larger than existing public datasets.</p><p>6 0.085237674 <a title="351-tfidf-6" href="./iccv-2013-Hybrid_Deep_Learning_for_Face_Verification.html">206 iccv-2013-Hybrid Deep Learning for Face Verification</a></p>
<p>7 0.078455374 <a title="351-tfidf-7" href="./iccv-2013-Refractive_Structure-from-Motion_on_Underwater_Images.html">348 iccv-2013-Refractive Structure-from-Motion on Underwater Images</a></p>
<p>8 0.076221958 <a title="351-tfidf-8" href="./iccv-2013-Single-Patch_Low-Rank_Prior_for_Non-pointwise_Impulse_Noise_Removal.html">394 iccv-2013-Single-Patch Low-Rank Prior for Non-pointwise Impulse Noise Removal</a></p>
<p>9 0.075752139 <a title="351-tfidf-9" href="./iccv-2013-Multi-stage_Contextual_Deep_Learning_for_Pedestrian_Detection.html">279 iccv-2013-Multi-stage Contextual Deep Learning for Pedestrian Detection</a></p>
<p>10 0.072235286 <a title="351-tfidf-10" href="./iccv-2013-Constant_Time_Weighted_Median_Filtering_for_Stereo_Matching_and_Beyond.html">88 iccv-2013-Constant Time Weighted Median Filtering for Stereo Matching and Beyond</a></p>
<p>11 0.068542659 <a title="351-tfidf-11" href="./iccv-2013-Joint_Noise_Level_Estimation_from_Personal_Photo_Collections.html">223 iccv-2013-Joint Noise Level Estimation from Personal Photo Collections</a></p>
<p>12 0.06355878 <a title="351-tfidf-12" href="./iccv-2013-PhotoOCR%3A_Reading_Text_in_Uncontrolled_Conditions.html">315 iccv-2013-PhotoOCR: Reading Text in Uncontrolled Conditions</a></p>
<p>13 0.062245883 <a title="351-tfidf-13" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>14 0.06161198 <a title="351-tfidf-14" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>15 0.060799658 <a title="351-tfidf-15" href="./iccv-2013-Deep_Learning_Identity-Preserving_Face_Space.html">106 iccv-2013-Deep Learning Identity-Preserving Face Space</a></p>
<p>16 0.059501015 <a title="351-tfidf-16" href="./iccv-2013-A_New_Image_Quality_Metric_for_Image_Auto-denoising.html">23 iccv-2013-A New Image Quality Metric for Image Auto-denoising</a></p>
<p>17 0.058891248 <a title="351-tfidf-17" href="./iccv-2013-DCSH_-_Matching_Patches_in_RGBD_Images.html">101 iccv-2013-DCSH - Matching Patches in RGBD Images</a></p>
<p>18 0.058707453 <a title="351-tfidf-18" href="./iccv-2013-Exploiting_Reflection_Change_for_Automatic_Reflection_Removal.html">151 iccv-2013-Exploiting Reflection Change for Automatic Reflection Removal</a></p>
<p>19 0.058356602 <a title="351-tfidf-19" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>20 0.056209758 <a title="351-tfidf-20" href="./iccv-2013-Joint_Deep_Learning_for_Pedestrian_Detection.html">220 iccv-2013-Joint Deep Learning for Pedestrian Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.124), (1, -0.017), (2, -0.032), (3, -0.021), (4, -0.012), (5, -0.021), (6, 0.02), (7, -0.014), (8, 0.007), (9, -0.047), (10, -0.021), (11, -0.06), (12, 0.039), (13, -0.024), (14, -0.041), (15, 0.038), (16, -0.024), (17, -0.001), (18, 0.023), (19, 0.11), (20, -0.042), (21, -0.012), (22, 0.036), (23, -0.053), (24, -0.127), (25, 0.053), (26, 0.013), (27, -0.04), (28, -0.067), (29, 0.049), (30, 0.008), (31, 0.051), (32, 0.146), (33, 0.076), (34, -0.027), (35, 0.055), (36, 0.001), (37, -0.055), (38, 0.148), (39, 0.0), (40, -0.209), (41, -0.015), (42, -0.015), (43, -0.141), (44, 0.11), (45, 0.085), (46, 0.122), (47, 0.097), (48, 0.036), (49, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93993324 <a title="351-lsi-1" href="./iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain.html">351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</a></p>
<p>Author: David Eigen, Dilip Krishnan, Rob Fergus</p><p>Abstract: Photographs taken through a window are often compromised by dirt or rain present on the window surface. Common cases of this include pictures taken from inside a vehicle, or outdoor security cameras mounted inside a protective enclosure. At capture time, defocus can be used to remove the artifacts, but this relies on achieving a shallow depth-of-field and placement of the camera close to the window. Instead, we present a post-capture image processing solution that can remove localized rain and dirt artifacts from a single image. We collect a dataset of clean/corrupted image pairs which are then used to train a specialized form of convolutional neural network. This learns how to map corrupted image patches to clean ones, implicitly capturing the characteristic appearance of dirt and water droplets in natural images. Our models demonstrate effective removal of dirt and rain in outdoor test conditions.</p><p>2 0.82964557 <a title="351-lsi-2" href="./iccv-2013-A_Generalized_Low-Rank_Appearance_Model_for_Spatio-temporally_Correlated_Rain_Streaks.html">15 iccv-2013-A Generalized Low-Rank Appearance Model for Spatio-temporally Correlated Rain Streaks</a></p>
<p>Author: Yi-Lei Chen, Chiou-Ting Hsu</p><p>Abstract: In this paper, we propose a novel low-rank appearance model for removing rain streaks. Different from previous work, our method needs neither rain pixel detection nor time-consuming dictionary learning stage. Instead, as rain streaks usually reveal similar and repeated patterns on imaging scene, we propose and generalize a low-rank model from matrix to tensor structure in order to capture the spatio-temporally correlated rain streaks. With the appearance model, we thus remove rain streaks from image/video (and also other high-order image structure) in a unified way. Our experimental results demonstrate competitive (or even better) visual quality and efficient run-time in comparison with state of the art.</p><p>3 0.66000199 <a title="351-lsi-3" href="./iccv-2013-Single-Patch_Low-Rank_Prior_for_Non-pointwise_Impulse_Noise_Removal.html">394 iccv-2013-Single-Patch Low-Rank Prior for Non-pointwise Impulse Noise Removal</a></p>
<p>Author: Ruixuan Wang, Emanuele Trucco</p><p>Abstract: This paper introduces a ‘low-rank prior’ for small oriented noise-free image patches: considering an oriented patch as a matrix, a low-rank matrix approximation is enough to preserve the texture details in the properly oriented patch. Based on this prior, we propose a single-patch method within a generalized joint low-rank and sparse matrix recovery framework to simultaneously detect and remove non-pointwise random-valued impulse noise (e.g., very small blobs). A weighting matrix is incorporated in the framework to encode an initial estimate of the spatial noise distribution. An accelerated proximal gradient method is adapted to estimate the optimal noise-free image patches. Experiments show the effectiveness of our framework in removing non-pointwise random-valued impulse noise.</p><p>4 0.55740196 <a title="351-lsi-4" href="./iccv-2013-DCSH_-_Matching_Patches_in_RGBD_Images.html">101 iccv-2013-DCSH - Matching Patches in RGBD Images</a></p>
<p>Author: Yaron Eshet, Simon Korman, Eyal Ofek, Shai Avidan</p><p>Abstract: We extend patch based methods to work on patches in 3D space. We start with Coherency Sensitive Hashing [12] (CSH), which is an algorithm for matching patches between two RGB images, and extend it to work with RGBD images. This is done by warping all 3D patches to a common virtual plane in which CSH is performed. To avoid noise due to warping of patches of various normals and depths, we estimate a group of dominant planes and compute CSH on each plane separately, before merging the matching patches. The result is DCSH - an algorithm that matches world (3D) patches in order to guide the search for image plane matches. An independent contribution is an extension of CSH, which we term Social-CSH. It allows a major speedup of the k nearest neighbor (kNN) version of CSH - its runtime growing linearly, rather than quadratically, in k. Social-CSH is used as a subcomponent of DCSH when many NNs are required, as in the case of image denoising. We show the benefits ofusing depth information to image reconstruction and image denoising, demonstrated on several RGBD images.</p><p>5 0.55630606 <a title="351-lsi-5" href="./iccv-2013-Exploiting_Reflection_Change_for_Automatic_Reflection_Removal.html">151 iccv-2013-Exploiting Reflection Change for Automatic Reflection Removal</a></p>
<p>Author: Yu Li, Michael S. Brown</p><p>Abstract: This paper introduces an automatic method for removing reflection interference when imaging a scene behind a glass surface. Our approach exploits the subtle changes in the reflection with respect to the background in a small set of images taken at slightly different view points. Key to this idea is the use of SIFT-flow to align the images such that a pixel-wise comparison can be made across the input set. Gradients with variation across the image set are assumed to belong to the reflected scenes while constant gradients are assumed to belong to the desired background scene. By correctly labelling gradients belonging to reflection or background, the background scene can be separated from the reflection interference. Unlike previous approaches that exploit motion, our approach does not make any assumptions regarding the background or reflected scenes’ geometry, nor requires the reflection to be static. This makes our approach practical for use in casual imaging scenarios. Our approach is straight forward and produces good results compared with existing methods. 1. Introduction and Related Work There are situations when a scene must be imaged behind a pane of glass. This is common when “window shopping” where one takes a photograph of an object behind a window. This is not a conducive setup for imaging as the glass will produce an unwanted layer of reflection in the resulting image. This problem can be treated as one of layer separation [7, 8], where the captured image I a linear combiis nation of a reflection layer IR and the desired background scene, IB, as follows: I IR + IB. = (1) The goal of reflection removal is to separate IB and IR from an input image I shown in Figure 1. as This problem is ill-posed, as it requires extracting two layers from one image. To make the problem tractable additional information, either supplied from the user or from Fig. 1. Example of our approach separating the background (IB) and reflection (IR) layers of one of the input images. Note that the reflection layer’s contrast has been boosted to improve visualization. multiple images, is required. For example, Levin and Weiss [7, 8] proposed a method where a user labelled image gradients as belonging to either background or reflection. Combing the markup with an optimization that imposed a sparsity prior on the separated images, their method produced compelling results. The only drawback was the need for user intervention. An automatic method was proposed by Levin et al. [9] that found the most likely decomposition which minimized the total number of edges and corners in the recovered image using a database of natural images. As 22443322 with example-based methods, the results were reliant on the similarity of the examples in the database. Another common strategy is to use multiple images. Some methods assume a fixed camera that is able to capture a set of images with different mixing of the layers through various means, e.g. rotating a polarized lens [3, 6, 12, 16, 17], changing focus [15], or applying a flash [1]. While these approaches demonstrate good results, the ability of controlling focal change, polarization, and flash may not always be possible. Sarel and Irani [13, 14] proposed video based methods that work by assuming the two layers, reflection and background, to be statistically uncorrelated. These methods can handle complex geometry in the reflection layer, but require a long image sequence such that the reflection layer has significant changes in order for a median-based approach [21] to extract the intrinsic image from the sequence as the initial guess for one of the layers. Techniques closer to ours exploit motion between the layers present in multiple images. In particular, when the background is captured from different points of view, the background and the reflection layers undergo different motions due to their different distance to the transparent layer. One issue with changing viewpoint is handling alignment among the images. Szeliski et al. [19] proposed a method that could simultaneously recover the two layers by assuming they were both static scenes and related by parametric transformations (i.e. homographies). Gai et al. [4, 5] proposed a similar approach that aligned the images in the gradient domain using gradient sparsity, again assuming static scenes. Tsin et al. [20] relaxed the planar scene constraint in [19] and used dense stereo correspondence with stereo matching configuration which limits the camera motion to unidirectional parallel motion. These approaches produce good results, but the constraint on scene geometry and assumed motion of the camera limit the type of scenes that can be processed. Our Contribution Our proposed method builds on the single-image approach by Levin and Weiss [8], but removes the need for user markup by examining the relative motion in a small set (e.g. 3-5) of images to automatically label gradients as either reflection or background. This is done by first aligning the images using SIFT-flow and then examining the variation in the gradients over the image set. Gradients with more variation are assumed to be from reflection while constant gradients are assumed to be from the desired background. While a simple idea, this approach does not impose any restrictions on the scene or reflection geometry. This allows a more practical imaging setup that is suitable for handheld cameras. The remainder of this paper is organized as follows. Section 2 overviews our approach; section 3 compares our results with prior methods on several examples; the paper is concluded in section 4. Warped ? ?Recovered ? ? Recovered ? ? Warp e d ? ?Recover d ? ? Recover d ? ? Fig. 2. This figure shows the separated layers of the first two input images. The layers illustrate that the background image IB has lit- tle variation while the reflection layers, IRi ,have notable variation due to the viewpoint change. 2. Reflection Removal Method 2.1. Imaging Assumption and Procedure The input ofour approach is a small set of k images taken of the scene from slightly varying view points. We assume the background dominates in the mixture image and the images are related by a warping, such that the background is registered and the reflection layer is changing. This relationship can be expressed as: Ii = wi(IRi + IB), (2) where Ii is the i-th mixture image, {wi}, i = 1, . . . , k are warping fuisn tchteio in-sth hcma uisxetud by mthaeg camera viewpoint change with respect to a reference image (in our case I1). Assuming we can estimate the inverse warps, w−i1, where w−11 is the identity, we get the following relationship: wi−1(Ii) = IRi + IB. (3) Even though IB appears static in the mixture image, the problem is still ill-posed given we have more unknowns than the number of input images. However, the presence of a static IB in the image set makes it possible to identify gradient edges of the background layer IB and edges of the changing reflection layers IRi . More specifically, edges in IB are assumed to appear every time in the image set while the edges in the reflection layer IRi are assumed to vary across the set. This reflection-change effect can be seen in Figure 2. This means edges can be labelled based on the frequency of a gradient appearing at a particular pixel across the aligned input images. After labelling edges as either background or reflection, we can reconstruct the two layers using an optimization that imposes the sparsity prior on the separated layers as done by [7, 8]. Figure 3 shows the processing pipeline of our approach. Each step is described in the following sections. 22443333 Fig. 3. This figure shows the pipeline of our approach: 1) warping functions are estimated to align the inputs to a reference view; 2) the edges are labelled as either background or foreground based on gradient frequency; 3) a reconstruction step is used to separate the two layers; 4) all recovered background layers are combined together to get the final recovered background. 2.2. Warping Our approach begins by estimating warping functions, w−i1, to register the input to the reference image. Previous approaches estimated these warps using global parametric motion (e.g. homographies [4, 5, 19]), however, the planarity constraint often leads to regions in the image with misalignments when the scene is not planar. Traditional dense correspondence method like optical flow is another option. However, even with our assumption that the background should be more prominent than the reflection layer, optical flow methods (e.g. [2, 18]) that are based on image intensity gave poor performance due to the reflection interference. This led us to try SIFT-flow [10] that is based on more robust image features. SIFT-flow [10] proved to work surprisingly well on our input sequences and provide a dense warp suitable to bring the images into alignment even under moderate interference of reflection. Empirical demonstration of the effectiveness of SIFT-flow in this task as well as the comparison with optical flow are shown in our supplemental materials. Our implementation fixes I1 as the reference, then uses SIFT-flow to estimate the inverse-warping functions {w−i1 }, i= 2, . . . , k for each ofthe input images I2 , . . . , Ik against ,I 1i . = W 2e, a.l.s.o, compute htohef gradient magnitudes Gi of the each input image and then warp the images Ii as well as the gradient magnitudes Gi using the same inverse-warping function w−i1, denoting the warped images and gradient magnitudes as Iˆi and Gˆi. 2.3. Edge separation Our approach first identifies salient edges using a simple threshold on the gradient magnitudes in Gˆi. The resulting binary edge map is denoted as Ei. After edge detection, the edges need to be separated as either background or foreground in each aligned image Iˆi. As previously discussed, the edges of the background layer should appear frequently across all the warped images while the edges of the reflection layer would only have sparse presence. To examine the sparsity of the edge occurrence, we use the following measurement: Φ(y) =??yy??2221, (4) where y is a vector containing the gradient magnitudes at a given pixel location. Since all elements in y are non-negative, we can rewrite equation 4 as Φ(y) = yi)2. This measurement can be conside?red as a L1? normalized L2 norm. It measures the sparsity o?f the vecto?r which achieves its maximum value of 1when only one non-zero item exists and achieve its minimum value of k1 when all items are non-zero and have identical values (i.e. y1 = y2 = . . . = yk > 0). This measurement is used to assign two probabilities to each edge pixel as belonging to either background or reflection. We estimate the reflection edge probability by examining ?ik=1 yi2/(?ik=1 22443344 the edge occurrence, as follows: PRi(x) = s?(??iikk==11GGˆˆii((xx))2)2−k1?,(5) Gˆi Iˆi. where, (x) is the gradient magnitude at pixel x of We subtract k1 to move the smallest value close to zero. The sparsity measurement is further stretched by a sigmoid function s(t) = (1 + e−(t−0.05)/0.05)−1 to facilitate the separation. The background edge probability is then estimated by: PBi(x) = s?−?(??iikk==11GGˆˆii((xx))2)2−k1??,(6) where PBi (x) + PRi (x) = ?1. These probabilities are defined only at the pixels that are edges in the image. We consider only edge pixels with relatively high probability in either the background edge probability map or reflection edge probability map. The final edge separation is performed by thresholding the two probability maps as: EBi/Ri(x) =⎨⎧ 10, Ei(x) = 1 aotndhe PrwBiis/eRi(x) > 0.6 Figure 4 shows ⎩the edge separation procedure. 2.4. Layer Reconstruction With the separated edges of the background and the reflection, we can reconstruct the two layers. Levin and Weis- ???????????? Gˆ Fig. 4. Edge separation illustration: 1) shows the all gradient maps in this case we have five input images; 2) plots the gradient values at two position across the five images - top plot is a pixel on a background edge, bottom plot is a pixel on a reflection edge; 3) shows the probability map estimated for each layer; 4) Final edge separation after thresholding the probability maps. s [7, 8] showed that the long tailed distribution of gradients in natural scenes is an effective prior in this problem. This kind of distributions is well modelled by a Laplacian or hyper-Laplacian distribution (P(t) ∝ p = 1for – e−|t|p/s, Laplacian and p < 1 for hyper-Laplacian). In our work, we use Laplacian approximation since the L1 norm converges quickly with good results. For each image Iˆi , we try to maximize the probability P(IBi , IRi ) in order to separate the two layers and this is equivalent to minimizing the cost log P(IBi , IRi ). Following the same deduction tinh e[ c7]o,s tw −ithlo tgheP independent assumption of the two layers (i.e. P(IBi , IRi ) = P(IBi ) · P(IRi )), the objective function becomes: − J(IBi) = ? |(IBi ∗ fn)(x)| + |((Iˆi − IBi) ∗ fn)(x)| ?x, ?n + λ?EBi(x)|((Iˆi − IBi) ∗ fn)(x)| ?x, ?n + λ?ERi(x)|(IBi ?x,n ∗ fn)(x)|, (7) where fn denotes the derivative filters and ∗ is the 2D convolution operator. hFeo rd efrniv, we use trwso a nodri e∗n istat tihoen 2s Dan cdo nt-wo degrees (first order and second order) derivative filters. While the first term in the objective function keeps the gradients of the two layer as sparse as possible, the last two terms force the gradients of IBi at edges positions in EBi to agree with the gradients of input image Iˆi and gradients of IRi at edge positions in ERi agree with the gradients of Iˆi. This equation can be further rewritten in the form of J = ?Au b? 1 and be minimized efficiently using iterative − reweighted lbea?st square [11]. 2.5. Combining the Results Our approach processes each image in the input set independently. Due to the reflective glass surface, some of the images may contain saturated regions from specular highlights. When saturation occurs, we can not fully recover the structure in these saturated regions because the information about the two layers are lost. In addition, sometimes the edges of the reflection in some regions are too weak to be correctly distinguished. This can lead to local regions in the background where the reflection is still present. These erroneous regions are often in different places in each input image due to changes in the reflection. In such cases, it is reasonable to assume that the minimum value across all recovered background layers may be a proper approximation of the true background. As such, the last step of our method is to take the minimum of the pixel value of all reconstructed background images as the final recovered background, as follows: IB (x) = mini IBi (x) . 22443355 (8) Fig. 5. This figure shows our combination procedure. The recovered background on each single image is good at first glance but may have reflection remaining in local regions. A simple minimum operator combining all recovered images gives a better result in these regions. The comparison can be seen in the zoomed-in regions. × Based on this, the reflection layer of each input image can be computed by IRi = IB . The effectiveness of this combination procedure is ill−us Itrated in Figure 5. Iˆi − 3. Results In this section, we present the experimental results of our proposed method. Additional results and test cases can be found in the accompanying supplemental materials. The experiments were conducted on an Intel i7? PC (3.4GHz CPU, 8.0GB RAM). The code was implemented in Matlab. We use the SIFT-Flow implementation provided by the authors 1. Matlab code and images used in our paper can be downloaded at the author’s webpage 2. The entire procedure outlined in Figure 3 takes approximately five minutes for a 500 400 image sequence containing up to five images. All t5h0e0 d×at4a0 s0h iomwang are qreuaeln scene captured pu ntodfe irv vea irmioaugse lighting conditions (e.g. indoor, outdoor). Input sequences range from three to five images. Figure 6 shows two examples of our edge separation results and final reconstructed background layers and reflection layers. Our method provides a clear separation of the edges of the two layers which is crucial in the reconstruc- 1http://people.csail.mit.edu/celiu/SIFTflow/SIFTflow.zip 2http://www.comp.nus.edu.sg/ liyu1988/ tion step. Figure 9 shows more reflection removal results of our method. We also compare our methods with those in [8] and [5]. For the method in [8], we use the source code 3 of the author to generate the results. The comparisons between our and [8] are not entirely fair since [8] uses single image to generate the result, while we have the advantage of the entire set. For the results produced by [8], the reference view was used as input. The required user-markup is also provided. For the method in [5], we set the layer number to be one, and estimate the motions of the background layer using their method. In the reconstruction phase, we set the remaining reflection layer in k input mixture images as k different layers, each only appearing once in one mixture. Figure 8 shows the results of two examples. Our results are arguably the best. The results of [8] still exhibited some edges from different layers even with the elaborate user mark-ups. This may be fixed by going back to further refine the user markup. But in the heavily overlapping edge regions, it is challenging for users to indicate the edges. If the edges are not clearly indicated the results tend to be over smoothed in one layer. For the method of [5], since it uses global transformations to align images, local misalignment effects often appear in the final recovered background image. Also, their approach uses all the input image into the optimization to recover the layers. This may lead to the result that has edges from different reflection layers of different images mixed and appear as ghosting effect in the recovered background image. For heavily saturated regions, none of the two previous methods can give visually plausible results like ours. 4. Discussion and Conclusion We have presented a method to automatically remove reflectance interference due to a glass surface. Our approach works by capturing a set of images of a scene from slightly varying view points. The images are then aligned and edges are labelled as belonging to either background or reflectance. This alignment was enabled by SIFT-flow, whose robustness to the reflection interference enabled our method. When using SIFT-flow, we assume that the background layer will be the most prominent and will provide sufficient SIFT features for matching. While we found this to work well in practice, images with very strong reflectance can produce poor alignment as SIFT-flow may attempt to align to the foreground which is changing. This will cause problems in the subsequent layer separation. Figure 7 shows such a case. While these failures can often be handled by cropping the image or simple user input (see supplemental material), it is a notable issue. Another challenging issue is when the background scene 3http://www.wisdom.weizmann.ac.il/ levina/papers/reflections.zip 22443366 ??? ??? ?? ??? Fig. 6. Example of edge separation results and recovered background and foreground layer using our method has large homogeneous regions. In such cases there are no edges to be labelled as background. This makes subsequent separation challenging, especially when the reflection interference in these regions is weak but still visually noticeable. While this problem is not unique to our approach, it is an issue to consider. We also found that by combining all the background results of the input images we can overcome Fig. 7. A failure case of our approach due to dominant reflection against the background in some regions (i.e. the upper part of the phonograph). This will cause unsatisfactory alignment of the background in the warping procedure which further lead to our edge separation and final reconstruction failure as can be seen in the figure. local regions with high saturation. While a simple idea, this combination strategy can be incorporated into other techniques to improve their results. Lastly, we believe reflection removal is an application that would be welcomed on many mobile devices, however, the current processing time is still too long for real world use. Exploring ways to speed up the processing pipeline is an area of interest for future work. Acknowledgement This work was supported by Singapore A*STAR PSF grant 11212100. References [1] A. K. Agrawal, R. Raskar, S. K. Nayar, and Y. Li. Removing photography artifacts using gradient projection and flashexposure sampling. ToG, 24(3):828–835, 2005. [2] A. Bruhn, J. Weickert, and C. Schn o¨rr. Lucas/kanade meets horn/schunck: Combining local and global optic flow methods. IJCV, 61(3):21 1–231, 2005. [3] H. Farid and E. H. Adelson. Separating reflections from images by use of independent component analysis. JOSA A, 16(9):2136–2145, 1999. [4] K. Gai, Z. Shi, and C. Zhang. Blindly separating mixtures of multiple layers with spatial shifts. In CVPR, 2008. [5] K. Gai, Z. Shi, and C. Zhang. Blind separation of superimposed moving images using image statistics. TPAMI, 34(1): 19–32, 2012. 22443377 Ours Levin and Weiss [7]Gai et al. [4] Fig. 8. Two example of reflection removal results of our method and those in [8] and [5] (user markup for [8] provided in the supplemental material). Our method provides more visual pleasing results. The results of [8] still exhibited remaining edges from reflection and tended to over smooth some local regions. The results of [5] suffered misalignment due to their global transformation alignment which results in ghosting effect of different layers in the final recovered background image. For the reflection, our results can give very complete and clear recovery of the reflection layer. [6] N. Kong, Y.-W. Tai, and S. Y. Shin. A physically-based approach to reflection separation. In CVPR, 2012. [7] A. Levin and Y. Weiss. User assisted separation ofreflections from a single image using a sparsity prior. In ECCV, 2004. [8] A. Levin and Y. Weiss. User assisted separation of reflections from a single image using a sparsity prior. TPAMI, 29(9): 1647–1654, 2007. [9] A. Levin, A. Zomet, and Y. Weiss. Separating reflections from a single image using local features. In CVPR, 2004. [10] C. Liu, J. Yuen, and A. Torralba. Sift flow: Dense correspondence across scenes and its applications. TPAMI, 33(5):978– 994, 2011. [11] P. Meer. Robust techniques for computer vision. Emerging Topics in Computer Vision, 2004. [12] N. Ohnishi, K. Kumaki, T. Yamamura, and T. Tanaka. Separating real and virtual objects from their overlapping images. In ECCV, 1996. [13] B. Sarel and M. Irani. Separating transparent layers through layer information exchange. In ECCV, 2004. [14] B. Sarel and M. Irani. Separating transparent layers of repetitive dynamic behaviors. In ICCV, 2005. [15] Y. Y. Schechner, N. Kiryati, and R. Basri. Separation of [16] [17] [18] [19] [20] [21] transparent layers using focus. IJCV, 39(1):25–39, 2000. Y. Y. Shechner, J. Shamir, and N. Kiryati. Polarization-based decorrelation of transparent layers: The inclination angle of an invisible surface. In ICCV, 1999. Y. Y. Shechner, J. Shamir, and N. Kiryati. Polarization and statistical analysis of scenes containing a semireflector. JOSA A, 17(2):276–284, 2000. D. Sun, S.Roth, and M. Black. Secrets of optical flow estimation and their principles. In CVPR, 2010. R. Szeliski, S. Avidan, and P. Anandan. Layer Extraction from Multiple Images Containing Reflections and Transparency. In CVPR, 2000. Y. Tsin, S. B. Kang, and R. Szeliski. Stereo matching with linear superposition of layers. TPAMI, 28(2):290–301, 2006. Y. Weiss. Deriving intrinsic images from image sequences. In ICCV, 2001. 22443388 Fig. 9. More results of reflection removal using our method in varying scenes (e.g. art museum, street shop, etc.). 22443399</p><p>6 0.54482758 <a title="351-lsi-6" href="./iccv-2013-Joint_Noise_Level_Estimation_from_Personal_Photo_Collections.html">223 iccv-2013-Joint Noise Level Estimation from Personal Photo Collections</a></p>
<p>7 0.53807408 <a title="351-lsi-7" href="./iccv-2013-Pedestrian_Parsing_via_Deep_Decompositional_Network.html">311 iccv-2013-Pedestrian Parsing via Deep Decompositional Network</a></p>
<p>8 0.53327072 <a title="351-lsi-8" href="./iccv-2013-Face_Recognition_Using_Face_Patch_Networks.html">153 iccv-2013-Face Recognition Using Face Patch Networks</a></p>
<p>9 0.53055984 <a title="351-lsi-9" href="./iccv-2013-A_New_Image_Quality_Metric_for_Image_Auto-denoising.html">23 iccv-2013-A New Image Quality Metric for Image Auto-denoising</a></p>
<p>10 0.51704323 <a title="351-lsi-10" href="./iccv-2013-A_Learning-Based_Approach_to_Reduce_JPEG_Artifacts_in_Image_Matting.html">19 iccv-2013-A Learning-Based Approach to Reduce JPEG Artifacts in Image Matting</a></p>
<p>11 0.47474426 <a title="351-lsi-11" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>12 0.41719612 <a title="351-lsi-12" href="./iccv-2013-Perceptual_Fidelity_Aware_Mean_Squared_Error.html">312 iccv-2013-Perceptual Fidelity Aware Mean Squared Error</a></p>
<p>13 0.39411816 <a title="351-lsi-13" href="./iccv-2013-Hybrid_Deep_Learning_for_Face_Verification.html">206 iccv-2013-Hybrid Deep Learning for Face Verification</a></p>
<p>14 0.39340934 <a title="351-lsi-14" href="./iccv-2013-Efficient_Image_Dehazing_with_Boundary_Constraint_and_Contextual_Regularization.html">135 iccv-2013-Efficient Image Dehazing with Boundary Constraint and Contextual Regularization</a></p>
<p>15 0.38706499 <a title="351-lsi-15" href="./iccv-2013-Motion-Aware_KNN_Laplacian_for_Video_Matting.html">275 iccv-2013-Motion-Aware KNN Laplacian for Video Matting</a></p>
<p>16 0.38693729 <a title="351-lsi-16" href="./iccv-2013-Shape_Anchors_for_Data-Driven_Multi-view_Reconstruction.html">387 iccv-2013-Shape Anchors for Data-Driven Multi-view Reconstruction</a></p>
<p>17 0.38180876 <a title="351-lsi-17" href="./iccv-2013-Matching_Dry_to_Wet_Materials.html">262 iccv-2013-Matching Dry to Wet Materials</a></p>
<p>18 0.36357605 <a title="351-lsi-18" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>19 0.36193115 <a title="351-lsi-19" href="./iccv-2013-Topology-Constrained_Layered_Tracking_with_Latent_Flow.html">420 iccv-2013-Topology-Constrained Layered Tracking with Latent Flow</a></p>
<p>20 0.35682014 <a title="351-lsi-20" href="./iccv-2013-Joint_Deep_Learning_for_Pedestrian_Detection.html">220 iccv-2013-Joint Deep Learning for Pedestrian Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.078), (7, 0.012), (12, 0.018), (26, 0.098), (31, 0.048), (40, 0.018), (42, 0.068), (46, 0.198), (48, 0.038), (64, 0.05), (73, 0.059), (78, 0.01), (89, 0.173), (98, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84523493 <a title="351-lda-1" href="./iccv-2013-Detecting_Irregular_Curvilinear_Structures_in_Gray_Scale_and_Color_Imagery_Using_Multi-directional_Oriented_Flux.html">112 iccv-2013-Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using Multi-directional Oriented Flux</a></p>
<p>Author: Engin Türetken, Carlos Becker, Przemyslaw Glowacki, Fethallah Benmansour, Pascal Fua</p><p>Abstract: We propose a new approach to detecting irregular curvilinear structures in noisy image stacks. In contrast to earlier approaches that rely on circular models of the crosssections, ours allows for the arbitrarily-shaped ones that are prevalent in biological imagery. This is achieved by maximizing the image gradient flux along multiple directions and radii, instead of only two with a unique radius as is usually done. This yields a more complex optimization problem for which we propose a computationally efficient solution. We demonstrate the effectiveness of our approach on a wide range ofchallenging gray scale and color datasets and show that it outperforms existing techniques, especially on very irregular structures.</p><p>same-paper 2 0.82224941 <a title="351-lda-2" href="./iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain.html">351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</a></p>
<p>Author: David Eigen, Dilip Krishnan, Rob Fergus</p><p>Abstract: Photographs taken through a window are often compromised by dirt or rain present on the window surface. Common cases of this include pictures taken from inside a vehicle, or outdoor security cameras mounted inside a protective enclosure. At capture time, defocus can be used to remove the artifacts, but this relies on achieving a shallow depth-of-field and placement of the camera close to the window. Instead, we present a post-capture image processing solution that can remove localized rain and dirt artifacts from a single image. We collect a dataset of clean/corrupted image pairs which are then used to train a specialized form of convolutional neural network. This learns how to map corrupted image patches to clean ones, implicitly capturing the characteristic appearance of dirt and water droplets in natural images. Our models demonstrate effective removal of dirt and rain in outdoor test conditions.</p><p>3 0.74573362 <a title="351-lda-3" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>Author: Jifeng Dai, Ying Nian Wu, Jie Zhou, Song-Chun Zhu</p><p>Abstract: Cosegmentation refers to theproblem ofsegmenting multiple images simultaneously by exploiting the similarities between the foreground and background regions in these images. The key issue in cosegmentation is to align common objects between these images. To address this issue, we propose an unsupervised learning framework for cosegmentation, by coupling cosegmentation with what we call “cosketch ”. The goal of cosketch is to automatically discover a codebook of deformable shape templates shared by the input images. These shape templates capture distinct image patterns and each template is matched to similar image patches in different images. Thus the cosketch of the images helps to align foreground objects, thereby providing crucial information for cosegmentation. We present a statistical model whose energy function couples cosketch and cosegmentation. We then present an unsupervised learning algorithm that performs cosketch and cosegmentation by energy minimization. Experiments show that our method outperforms state of the art methods for cosegmentation on the challenging MSRC and iCoseg datasets. We also illustrate our method on a new dataset called Coseg-Rep where cosegmentation can be performed within a single image with repetitive patterns.</p><p>4 0.74505061 <a title="351-lda-4" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>Author: Matthias Reso, Jörn Jachalsky, Bodo Rosenhahn, Jörn Ostermann</p><p>Abstract: Superpixel algorithms represent a very useful and increasingly popular preprocessing step for a wide range of computer vision applications, as they offer the potential to boost efficiency and effectiveness. In this regards, this paper presents a highly competitive approach for temporally consistent superpixelsfor video content. The approach is based on energy-minimizing clustering utilizing a novel hybrid clustering strategy for a multi-dimensional feature space working in a global color subspace and local spatial subspaces. Moreover, a new contour evolution based strategy is introduced to ensure spatial coherency of the generated superpixels. For a thorough evaluation the proposed approach is compared to state of the art supervoxel algorithms using established benchmarks and shows a superior performance.</p><p>5 0.74456388 <a title="351-lda-5" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>Author: Jimei Yang, Yi-Hsuan Tsai, Ming-Hsuan Yang</p><p>Abstract: We present a hybrid parametric and nonparametric algorithm, exemplar cut, for generating class-specific object segmentation hypotheses. For the parametric part, we train a pylon model on a hierarchical region tree as the energy function for segmentation. For the nonparametric part, we match the input image with each exemplar by using regions to obtain a score which augments the energy function from the pylon model. Our method thus generates a set of highly plausible segmentation hypotheses by solving a series of exemplar augmented graph cuts. Experimental results on the Graz and PASCAL datasets show that the proposed algorithm achievesfavorable segmentationperformance against the state-of-the-art methods in terms of visual quality and accuracy.</p><p>6 0.74438399 <a title="351-lda-6" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>7 0.74306935 <a title="351-lda-7" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>8 0.74082577 <a title="351-lda-8" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>9 0.74064684 <a title="351-lda-9" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>10 0.74039119 <a title="351-lda-10" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>11 0.73822057 <a title="351-lda-11" href="./iccv-2013-Beyond_Hard_Negative_Mining%3A_Efficient_Detector_Learning_via_Block-Circulant_Decomposition.html">61 iccv-2013-Beyond Hard Negative Mining: Efficient Detector Learning via Block-Circulant Decomposition</a></p>
<p>12 0.73798758 <a title="351-lda-12" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>13 0.73766059 <a title="351-lda-13" href="./iccv-2013-Joint_Deep_Learning_for_Pedestrian_Detection.html">220 iccv-2013-Joint Deep Learning for Pedestrian Detection</a></p>
<p>14 0.7374568 <a title="351-lda-14" href="./iccv-2013-Bayesian_Robust_Matrix_Factorization_for_Image_and_Video_Processing.html">60 iccv-2013-Bayesian Robust Matrix Factorization for Image and Video Processing</a></p>
<p>15 0.73722172 <a title="351-lda-15" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>16 0.73698521 <a title="351-lda-16" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>17 0.73688126 <a title="351-lda-17" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>18 0.73684096 <a title="351-lda-18" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>19 0.73676538 <a title="351-lda-19" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>20 0.73638278 <a title="351-lda-20" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
