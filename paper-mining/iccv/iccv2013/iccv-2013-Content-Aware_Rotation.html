<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>90 iccv-2013-Content-Aware Rotation</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-90" href="#">iccv2013-90</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>90 iccv-2013-Content-Aware Rotation</h1>
<br/><p>Source: <a title="iccv-2013-90-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/He_Content-Aware_Rotation_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Kaiming He, Huiwen Chang, Jian Sun</p><p>Abstract: We present an image editing tool called Content-Aware Rotation. Casually shot photos can appear tilted, and are often corrected by rotation and cropping. This trivial solution may remove desired content and hurt image integrity. Instead of doing rigid rotation, we propose a warping method that creates the perception of rotation and avoids cropping. Human vision studies suggest that the perception of rotation is mainly due to horizontal/vertical lines. We design an optimization-based method that preserves the rotation of horizontal/vertical lines, maintains the completeness of the image content, and reduces the warping distortion. An efficient algorithm is developed to address the challenging optimization. We demonstrate our content-aware rotation method on a variety of practical cases.</p><p>Reference: <a title="iccv-2013-90-reference" href="../iccv2013_reference/iccv-2013-Content-Aware_Rotation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Casually shot photos can appear tilted, and are often corrected by rotation and cropping. [sent-2, score-0.497]
</p><p>2 Instead of doing rigid rotation, we propose a warping method that creates the perception of rotation and avoids cropping. [sent-4, score-0.881]
</p><p>3 Human vision studies suggest that the perception of rotation is mainly due to horizontal/vertical lines. [sent-5, score-0.677]
</p><p>4 We design an optimization-based method that preserves the rotation of horizontal/vertical lines, maintains the completeness of the image content, and reduces the warping distortion. [sent-6, score-0.759]
</p><p>5 We demonstrate our content-aware rotation method on a variety of practical cases. [sent-8, score-0.497]
</p><p>6 Introduction Image rotation is one of the fundamental image editing operations besides scaling and cropping. [sent-10, score-0.537]
</p><p>7 Photos casually shot by hand-held cameras/phones can appear tilted, which are sensitive to human eyes even when the rotation angle is  small (Fig. [sent-11, score-0.81]
</p><p>8 On the other hand, artists may adjust the composition through manipulating the rotation [26]. [sent-13, score-0.616]
</p><p>9 For these reasons and others, image rotation has been incorporated in a majority of image editing softwares. [sent-14, score-0.537]
</p><p>10 We assume the rotation angle has been given by users, algorithms [12], or sensors in devices. [sent-15, score-0.654]
</p><p>11 A rigidly rotated image inevitably has empty regions, and is often cropped to fit an upright rectangle (Fig. [sent-16, score-0.317]
</p><p>12 Actually, the cropping operation reduces the area of a typical photo by 20% even when the rotation angle is as small as 5◦ . [sent-22, score-0.778]
</p><p>13 A sophisticated solution is desired for common users and artists to reduce the loss of content when rotating images. [sent-23, score-0.235]
</p><p>14 8°) (c) crop from (b)(d) content-aware rotation  Figure 1. [sent-26, score-0.551]
</p><p>15 (c) The rotated image is cropped by the largest inner upright rectangle. [sent-32, score-0.211]
</p><p>16 Recent image retargeting methods are mostly based on warping [3 1, 29, 32, 8], and they are designed to preserve high-level content like local shapes and straight lines. [sent-41, score-0.535]
</p><p>17 A common motivation of warping methods is to maintain the completeness of the image content at the price of distortion (as unnoticeable as possible). [sent-42, score-0.495]
</p><p>18 We design a warping method that keeps the image content inside an upright rectangle while creating the perception of rotation. [sent-44, score-0.559]
</p><p>19 normal angles87° 90° 93°  oblique angles 87° 90° 93° Figure 3. [sent-53, score-0.216]
</p><p>20 Top: for normal angles, human eyes can easily notice the tilted vertical lines. [sent-55, score-0.272]
</p><p>21 Bottom: for oblique angles, human eyes are not sensitive to the absolute angle values. [sent-56, score-0.322]
</p><p>22 perception of the tilted horizon (or lines parallel to it) [11]. [sent-58, score-0.653]
</p><p>23 Second, the human eyes are sensitive to the right angles (90◦ ) that are “normal” [13, 23], i. [sent-60, score-0.273]
</p><p>24 As a result, the vertical lines are easily noticeable if tilted. [sent-63, score-0.277]
</p><p>25 Last, the human eyes are not sensitive to the absolute values of angles when the angles are acute/obtuse [22] or when they are “oblique” right angles (i. [sent-64, score-0.597]
</p><p>26 The above studies suggest that the human vision system is very sensitive to the horizontal/vertical lines but less so to others. [sent-69, score-0.292]
</p><p>27 Driven by these studies, we propose to preserve the orientation of the horizontal/vertical lines (after rotation) so as to create the perception of rotation. [sent-70, score-0.413]
</p><p>28 The orientations of other lines are relaxed because they are less noticeable to human eyes. [sent-71, score-0.352]
</p><p>29 This adaptive rotation makes it possible to maintain the image integrity with less distortion (Sec. [sent-72, score-0.73]
</p><p>30 We constrain the horizontal/vertical lines  (after rotation) to be horizontal/vertial to create the perception of rotation, but relax other lines. [sent-76, score-0.348]
</p><p>31 We constrain the boundary vertexes on the image boundary to maintain the completeness of the content. [sent-77, score-0.293]
</p><p>32 Our method performs particularly well for “small” rotation angles like < 10◦ . [sent-84, score-0.659]
</p><p>33 We also notice that human eyes are very sensitive to a rotation angle as small as 3◦ or even smaller (the famous Leaning Tower of Pisa leans at about 3. [sent-86, score-0.765]
</p><p>34 99◦ 1), so the correction is still desired despite the angles are “small”. [sent-87, score-0.201]
</p><p>35 Image Rotation Consider a popular way of manual rotation [1]: the user drags a straight line aligned to any horizontal/vertical line, and then the software rotates the image so that the dragged line becomes horizontal/vertical. [sent-92, score-0.717]
</p><p>36 This user interface is inspiring: the human eyes are sensitive to tilted horizontal/vertical lines. [sent-93, score-0.308]
</p><p>37 A method [12] that automatically estimates a rotation angle from a single image is based on a similar motivation. [sent-95, score-0.654]
</p><p>38 The rotation angle that rectifies the horizon is chosen. [sent-97, score-0.762]
</p><p>39 Beyond 2D in-plane rotation, the method in [20] automatically estimates 3D rotation angles (a homography matrix). [sent-98, score-0.659]
</p><p>40 Given the estimated rotation angle (2D/3D), conventional methods [12, 20] transform the image rigidly and globally. [sent-100, score-0.717]
</p><p>41 We only consider 2D in-plane rotation in this paper. [sent-102, score-0.497]
</p><p>42 The warping strategy allows to introduce smooth distortion that are less noticeable to human eyes. [sent-110, score-0.34]
</p><p>43 Further, the mesh-based warping methods are able to preserve high-level perceptual properties like local shapes [29, 32, 8] and straight lines [8]. [sent-111, score-0.551]
</p><p>44 The studies in [18, 7, 6] use the warping strategy to adaptively project wide-angle/panaramic images. [sent-114, score-0.232]
</p><p>45 Algorithm We suppose the rotation angle Δ is given by users or automatic methods like [12], and is fixed. [sent-117, score-0.707]
</p><p>46 We further suppose that given this rotation angle, the horizon (or the lines parallel to the horizon) would become horizontal after rigid rotation. [sent-118, score-0.919]
</p><p>47 But our method can easily adapt to “tilted composition” if a tilted horizon is the artists’ purpose. [sent-120, score-0.305]
</p><p>48 The warping result is expected to maintain the orientations of horizontal/vertical lines (after rotation) but relax others. [sent-122, score-0.513]
</p><p>49 To this end, we design an energy function that can manipulate the  rotations of lines in different orientations. [sent-123, score-0.316]
</p><p>50 Line Extraction and Quantization We first extract and quantize the lines that will be used in the mesh optimization. [sent-126, score-0.338]
</p><p>51 Then these lines are cut by the input mesh grid, so each resulting line is within a single quad. [sent-130, score-0.447]
</p><p>52 Aes [ −inΔ [,8π], our energy function will encourage all the lines in the same bin to share a common rotation angle. [sent-137, score-0.892]
</p><p>53 im/pub/  art  / 2 0 12 / g jmr-l sd/  ×  input & det cted linesoutput & deformed lines Figure 4. [sent-140, score-0.296]
</p><p>54 Bottom: the detected lines in the input and their deformed counterparts in the output. [sent-144, score-0.296]
</p><p>55 Here the “canonical” lines are marked as red - they are the lines to be horizontal/vertical after rotation. [sent-145, score-0.424]
</p><p>56 be kept straight, and parallel lines will be kept parallel. [sent-146, score-0.212]
</p><p>57 We denote the common rotation angle in the m-th bin as θm. [sent-147, score-0.733]
</p><p>58 We want to preserve the rotation of the lines in these bins (marked as red in Fig. [sent-152, score-0.833]
</p><p>59 (1) The lines in the canonical bins are strictly constrained to be rotated by This is for creating the perception of rotation. [sent-157, score-0.609]
</p><p>60 mesh boundary are constrained to be on the upright rectangular boundary of the output. [sent-159, score-0.332]
</p><p>61 We put a regular quad mesh on the input image (Fig. [sent-162, score-0.318]
</p><p>62 e Ofoul-r lowing terms: Rotation Manipulation The energy ER manipulates the rotation angles of the lines:  ER(θ)  = ? [sent-167, score-0.808]
</p><p>63 It encourages the rotation angles to follow the desired Δ. [sent-172, score-0.737]
</p><p>64 The energy ER(θ) allows to rotate the non-canonical lines by some angles different from Δ, so enables non-rigid and adaptive rotation. [sent-178, score-0.587]
</p><p>65 Line Preservation  The line preservation energy builds a  relation between the lines and the mesh vertexes. [sent-180, score-0.62]
</p><p>66 So ek can be written as a linear function of the vertexes V (that is, ek = PkV for some Pk). [sent-183, score-0.399]
</p><p>67 Also, we use uk to denote the directional vector of this line in the input image. [sent-184, score-0.2]
</p><p>68 Denoting the bin of this line as m(k) with the expected rotation angle θm(k) , we consider the following energy measuring the distortion of line rotation:  EL(V,θ,s) =K1? [sent-185, score-1.058]
</p><p>69 Intuitively, we rotate the input vector uk by θm(k) and scale it by sk, and then we measure its distortion to ek. [sent-195, score-0.284]
</p><p>70 ×  Intuitively, this term encourages the angle between ek and uk to be θm(k) , such that the line is to be rotated by θm(k) . [sent-206, score-0.573]
</p><p>71 This energy favors each quad to undergo a similarity transformation (“as-similar-as-possible”). [sent-217, score-0.254]
</p><p>72 2,  (4)  where N is the quad number, q is a quad index. [sent-221, score-0.3]
</p><p>73 Boundary Preservation To avoid the image content going outside an upright rectangular boundary, we constrain the boundary vertexes on this rectangle:  EB(V)  = ? [sent-232, score-0.368]
</p><p>74 Intuitively, each φk denaouxteisli atrhye vinadriiavbildeusa φl ro =tat {iφon} angle of line k (while θm is the com? [sent-267, score-0.224]
</p><p>75 Notice that if β = 0, the problem is solved by the angle between ek and uk: φk = ∠(ek , uk) (this is the intuition of line preservation (3)); and if β = ∞, it is solved by φk = θm(k) . [sent-311, score-0.467]
</p><p>76 The angles θ are all initialized as Δ (the given rotation angle). [sent-317, score-0.659]
</p><p>77 Discussions Adaptive Rotation Algorithmically, our method is designed to strictly preserve the horizontal/vertical lines (after rotation) while relaxing others. [sent-339, score-0.33]
</p><p>78 This pencil will be horizontal if the image is rigidly rotated by Δ = +10. [sent-345, score-0.296]
</p><p>79 Our content-aware method maintains such rotation of this pencil, as in Fig. [sent-347, score-0.533]
</p><p>80 But our method relaxes the rotation of other pixels (i. [sent-349, score-0.497]
</p><p>81 It can be expected that if lines of all orientations are treated strictly, the result will be distorted more. [sent-354, score-0.287]
</p><p>82 Unlike our original way that adapts to the orientations, this modified way is non-adaptive and forces lines of all orientations to undergo strict rotation. [sent-357, score-0.287]
</p><p>83 6 (bottom) shows the rotation angle θm of each bin m. [sent-361, score-0.733]
</p><p>84 Bottom: the optimized rotation angle θm for each each bin m. [sent-413, score-0.733]
</p><p>85 Unlike [8, 14]  Our method is related to a linemethod [8] and a recent panorama But our method has major differthat only preserve the straightness  or orientations of lines, our method manipulates the rotational angles θ of the lines - this is the focus of image rotation. [sent-416, score-0.661]
</p><p>86 Without angle manipulations, existing retargeting methods including [8, 14] have no effect when applied for content-aware rotation. [sent-420, score-0.319]
</p><p>87 If we apply the existing methods, we can only obtain trivial  3In “rigid rotation + cropping”,  the output aspect ratio depends on the cropping strategy. [sent-422, score-0.658]
</p><p>88 One could find a crop that preserves the input aspect ratio, or a largest inner crop that may change the aspect ratio. [sent-423, score-0.224]
</p><p>89 Experiments We assume the rotation angles are given by the users. [sent-434, score-0.659]
</p><p>90 1  558  (a) input(b) rotated -7°(c) our content-aware rotation (d)Phot shop,content-aw refil (e)Dar bietal. [sent-438, score-0.584]
</p><p>91 7 we show comparisons with rigid rotation followed by cropping. [sent-452, score-0.557]
</p><p>92 Our content-aware rotation method creates perception similar to the results of rigid rotation, but maintains the integrity of the image content. [sent-456, score-0.792]
</p><p>93 Our content-aware rotation method can be easily generalized to this application, thanks to its flex-  ibility on the angle manipulation. [sent-471, score-0.654]
</p><p>94 In this way, the input horizons will be strictly rotated and others are relaxed. [sent-477, score-0.224]
</p><p>95 Limitations Like retargeting methods, our content-aware rotation method attempts to find visually unnoticeable operations. [sent-480, score-0.772]
</p><p>96 This can be the case when the image content is visually important in many local regions, or the rotation angle is large. [sent-482, score-0.759]
</p><p>97 Technically, we have introduced a warping method that can directly and flexibly manipulate the rotation angles. [sent-488, score-0.685]
</p><p>98 559  (a) input  (b) rotation + crop (c) content-aware rotation  Figure 10. [sent-516, score-1.09]
</p><p>99 The rotation angles are +5◦ (left) and -8◦ (right). [sent-520, score-0.659]
</p><p>100 Using vanishing points to correct camera rotation in images. [sent-565, score-0.537]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rotation', 0.497), ('lines', 0.212), ('tilted', 0.197), ('warping', 0.188), ('angles', 0.162), ('retargeting', 0.162), ('angle', 0.157), ('quad', 0.15), ('perception', 0.136), ('vertexes', 0.135), ('ek', 0.132), ('mesh', 0.126), ('upright', 0.124), ('cropping', 0.124), ('preservation', 0.111), ('horizon', 0.108), ('pencil', 0.104), ('energy', 0.104), ('siggraph', 0.099), ('uk', 0.091), ('rotated', 0.087), ('distortion', 0.087), ('completion', 0.084), ('seam', 0.084), ('bin', 0.079), ('unnoticeable', 0.076), ('eyes', 0.075), ('orientations', 0.075), ('artists', 0.075), ('carving', 0.069), ('content', 0.068), ('darabi', 0.068), ('seams', 0.068), ('line', 0.067), ('rer', 0.065), ('kopf', 0.065), ('preserve', 0.065), ('noticeable', 0.065), ('asia', 0.064), ('rotate', 0.064), ('rigidly', 0.063), ('integrity', 0.063), ('panorama', 0.063), ('canonical', 0.062), ('rigid', 0.06), ('kk', 0.059), ('photoshop', 0.059), ('bins', 0.059), ('sk', 0.057), ('fix', 0.056), ('oblique', 0.054), ('crop', 0.054), ('strictly', 0.053), ('users', 0.053), ('straight', 0.052), ('csoisn', 0.051), ('huiwen', 0.051), ('itermax', 0.051), ('kukr', 0.051), ('rkukrkt', 0.051), ('utkuk', 0.051), ('warms', 0.051), ('shechtman', 0.049), ('contentaware', 0.045), ('harnesses', 0.045), ('carroll', 0.045), ('casually', 0.045), ('manipulates', 0.045), ('rectangling', 0.045), ('adaptive', 0.045), ('studies', 0.044), ('composition', 0.044), ('rectangle', 0.043), ('input', 0.042), ('horizontal', 0.042), ('horizons', 0.042), ('deformed', 0.042), ('subproblem', 0.041), ('boundary', 0.041), ('editing', 0.04), ('vanishing', 0.04), ('desired', 0.039), ('sorkine', 0.039), ('chapter', 0.039), ('encourages', 0.039), ('rotational', 0.039), ('completeness', 0.038), ('maintain', 0.038), ('el', 0.038), ('quadratic', 0.038), ('aq', 0.037), ('visually', 0.037), ('aspect', 0.037), ('rubinstein', 0.036), ('algorithmically', 0.036), ('sensitive', 0.036), ('maintains', 0.036), ('vq', 0.035), ('perceptual', 0.034), ('rotates', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="90-tfidf-1" href="./iccv-2013-Content-Aware_Rotation.html">90 iccv-2013-Content-Aware Rotation</a></p>
<p>Author: Kaiming He, Huiwen Chang, Jian Sun</p><p>Abstract: We present an image editing tool called Content-Aware Rotation. Casually shot photos can appear tilted, and are often corrected by rotation and cropping. This trivial solution may remove desired content and hurt image integrity. Instead of doing rigid rotation, we propose a warping method that creates the perception of rotation and avoids cropping. Human vision studies suggest that the perception of rotation is mainly due to horizontal/vertical lines. We design an optimization-based method that preserves the rotation of horizontal/vertical lines, maintains the completeness of the image content, and reduces the warping distortion. An efficient algorithm is developed to address the challenging optimization. We demonstrate our content-aware rotation method on a variety of practical cases.</p><p>2 0.18013078 <a title="90-tfidf-2" href="./iccv-2013-Direct_Optimization_of_Frame-to-Frame_Rotation.html">115 iccv-2013-Direct Optimization of Frame-to-Frame Rotation</a></p>
<p>Author: Laurent Kneip, Simon Lynen</p><p>Abstract: This work makes use of a novel, recently proposed epipolar constraint for computing the relative pose between two calibrated images. By enforcing the coplanarity of epipolar plane normal vectors, it constrains the three degrees of freedom of the relative rotation between two camera views directly—independently of the translation. The present paper shows how the approach can be extended to n points, and translated into an efficient eigenvalue minimization over the three rotational degrees of freedom. Each iteration in the non-linear optimization has constant execution time, independently of the number of features. Two global optimization approaches are proposed. The first one consists of an efficient Levenberg-Marquardt scheme with randomized initial value, which already leads to stable and accurate results. The second scheme consists of a globally optimal branch-and-bound algorithm based on a bound on the eigenvalue variation derived from symmetric eigenvalue-perturbation theory. Analysis of the cost function reveals insights into the nature of a specific relative pose problem, and outlines the complexity under different conditions. The algorithm shows state-of-the-art performance w.r.t. essential-matrix based solutions, and a frameto-frame application to a video sequence immediately leads to an alternative, real-time visual odometry solution. Note: All algorithms in this paper are made available in the OpenGV library. Please visit http : / / l aurent kne ip .github . i / opengv o</p><p>3 0.13002263 <a title="90-tfidf-3" href="./iccv-2013-Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization.html">284 iccv-2013-Multiview Photometric Stereo Using Planar Mesh Parameterization</a></p>
<p>Author: Jaesik Park, Sudipta N. Sinha, Yasuyuki Matsushita, Yu-Wing Tai, In So Kweon</p><p>Abstract: We propose a method for accurate 3D shape reconstruction using uncalibrated multiview photometric stereo. A coarse mesh reconstructed using multiview stereo is first parameterized using a planar mesh parameterization technique. Subsequently, multiview photometric stereo is performed in the 2D parameter domain of the mesh, where all geometric and photometric cues from multiple images can be treated uniformly. Unlike traditional methods, there is no need for merging view-dependent surface normal maps. Our key contribution is a new photometric stereo based mesh refinement technique that can efficiently reconstruct meshes with extremely fine geometric details by directly estimating a displacement texture map in the 2D parameter domain. We demonstrate that intricate surface geometry can be reconstructed using several challenging datasets containing surfaces with specular reflections, multiple albedos and complex topologies.</p><p>4 0.1283669 <a title="90-tfidf-4" href="./iccv-2013-Lifting_3D_Manhattan_Lines_from_a_Single_Image.html">250 iccv-2013-Lifting 3D Manhattan Lines from a Single Image</a></p>
<p>Author: Srikumar Ramalingam, Matthew Brand</p><p>Abstract: We propose a novel and an efficient method for reconstructing the 3D arrangement of lines extracted from a single image, using vanishing points, orthogonal structure, and an optimization procedure that considers all plausible connectivity constraints between lines. Line detection identifies a large number of salient lines that intersect or nearly intersect in an image, but relatively a few of these apparent junctions correspond to real intersections in the 3D scene. We use linear programming (LP) to identify a minimal set of least-violated connectivity constraints that are sufficient to unambiguously reconstruct the 3D lines. In contrast to prior solutions that primarily focused on well-behaved synthetic line drawings with severely restricting assumptions, we develop an algorithm that can work on real images. The algorithm produces line reconstruction by identifying 95% correct connectivity constraints in York Urban database, with a total computation time of 1 second per image.</p><p>5 0.12477472 <a title="90-tfidf-5" href="./iccv-2013-Efficient_and_Robust_Large-Scale_Rotation_Averaging.html">138 iccv-2013-Efficient and Robust Large-Scale Rotation Averaging</a></p>
<p>Author: Avishek Chatterjee, Venu Madhav Govindu</p><p>Abstract: In this paper we address the problem of robust and efficient averaging of relative 3D rotations. Apart from having an interesting geometric structure, robust rotation averaging addresses the need for a good initialization for largescale optimization used in structure-from-motion pipelines. Such pipelines often use unstructured image datasets harvested from the internet thereby requiring an initialization method that is robust to outliers. Our approach works on the Lie group structure of 3D rotations and solves the problem of large-scale robust rotation averaging in two ways. Firstly, we use modern ?1 optimizers to carry out robust averaging of relative rotations that is efficient, scalable and robust to outliers. In addition, we also develop a twostep method that uses the ?1 solution as an initialisation for an iteratively reweighted least squares (IRLS) approach. These methods achieve excellent results on large-scale, real world datasets and significantly outperform existing methods, i.e. the state-of-the-art discrete-continuous optimization method of [3] as well as the Weiszfeld method of [8]. We demonstrate the efficacy of our method on two large- scale real world datasets and also provide the results of the two aforementioned methods for comparison.</p><p>6 0.11513561 <a title="90-tfidf-6" href="./iccv-2013-Rectangling_Stereographic_Projection_for_Wide-Angle_Image_Visualization.html">346 iccv-2013-Rectangling Stereographic Projection for Wide-Angle Image Visualization</a></p>
<p>7 0.090621941 <a title="90-tfidf-7" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>8 0.08908245 <a title="90-tfidf-8" href="./iccv-2013-Unsupervised_Intrinsic_Calibration_from_a_Single_Frame_Using_a_%22Plumb-Line%22_Approach.html">436 iccv-2013-Unsupervised Intrinsic Calibration from a Single Frame Using a "Plumb-Line" Approach</a></p>
<p>9 0.088747844 <a title="90-tfidf-9" href="./iccv-2013-Strong_Appearance_and_Expressive_Spatial_Models_for_Human_Pose_Estimation.html">403 iccv-2013-Strong Appearance and Expressive Spatial Models for Human Pose Estimation</a></p>
<p>10 0.08860065 <a title="90-tfidf-10" href="./iccv-2013-Global_Fusion_of_Relative_Motions_for_Robust%2C_Accurate_and_Scalable_Structure_from_Motion.html">184 iccv-2013-Global Fusion of Relative Motions for Robust, Accurate and Scalable Structure from Motion</a></p>
<p>11 0.086432017 <a title="90-tfidf-11" href="./iccv-2013-A_Global_Linear_Method_for_Camera_Pose_Registration.html">17 iccv-2013-A Global Linear Method for Camera Pose Registration</a></p>
<p>12 0.084854357 <a title="90-tfidf-12" href="./iccv-2013-BOLD_Features_to_Detect_Texture-less_Objects.html">57 iccv-2013-BOLD Features to Detect Texture-less Objects</a></p>
<p>13 0.081521638 <a title="90-tfidf-13" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>14 0.080601096 <a title="90-tfidf-14" href="./iccv-2013-Robust_Non-parametric_Data_Fitting_for_Correspondence_Modeling.html">358 iccv-2013-Robust Non-parametric Data Fitting for Correspondence Modeling</a></p>
<p>15 0.079423316 <a title="90-tfidf-15" href="./iccv-2013-Pose_Estimation_with_Unknown_Focal_Length_Using_Points%2C_Directions_and_Lines.html">323 iccv-2013-Pose Estimation with Unknown Focal Length Using Points, Directions and Lines</a></p>
<p>16 0.079107858 <a title="90-tfidf-16" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<p>17 0.076677509 <a title="90-tfidf-17" href="./iccv-2013-Efficient_Hand_Pose_Estimation_from_a_Single_Depth_Image.html">133 iccv-2013-Efficient Hand Pose Estimation from a Single Depth Image</a></p>
<p>18 0.075684607 <a title="90-tfidf-18" href="./iccv-2013-Coherent_Object_Detection_with_3D_Geometric_Context_from_a_Single_Image.html">79 iccv-2013-Coherent Object Detection with 3D Geometric Context from a Single Image</a></p>
<p>19 0.074096598 <a title="90-tfidf-19" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>20 0.07323999 <a title="90-tfidf-20" href="./iccv-2013-Revisiting_the_PnP_Problem%3A_A_Fast%2C_General_and_Optimal_Solution.html">353 iccv-2013-Revisiting the PnP Problem: A Fast, General and Optimal Solution</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.152), (1, -0.102), (2, -0.04), (3, -0.004), (4, -0.017), (5, 0.02), (6, 0.02), (7, -0.072), (8, 0.021), (9, -0.024), (10, 0.029), (11, 0.004), (12, -0.057), (13, 0.006), (14, 0.037), (15, 0.069), (16, 0.075), (17, 0.072), (18, -0.012), (19, -0.021), (20, 0.07), (21, -0.074), (22, -0.036), (23, -0.021), (24, -0.015), (25, 0.023), (26, 0.015), (27, 0.062), (28, -0.069), (29, -0.048), (30, -0.036), (31, 0.03), (32, -0.046), (33, -0.011), (34, 0.006), (35, -0.067), (36, -0.018), (37, 0.007), (38, -0.036), (39, -0.081), (40, 0.007), (41, 0.075), (42, 0.002), (43, -0.043), (44, 0.044), (45, -0.055), (46, -0.035), (47, 0.067), (48, -0.086), (49, 0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98007566 <a title="90-lsi-1" href="./iccv-2013-Content-Aware_Rotation.html">90 iccv-2013-Content-Aware Rotation</a></p>
<p>Author: Kaiming He, Huiwen Chang, Jian Sun</p><p>Abstract: We present an image editing tool called Content-Aware Rotation. Casually shot photos can appear tilted, and are often corrected by rotation and cropping. This trivial solution may remove desired content and hurt image integrity. Instead of doing rigid rotation, we propose a warping method that creates the perception of rotation and avoids cropping. Human vision studies suggest that the perception of rotation is mainly due to horizontal/vertical lines. We design an optimization-based method that preserves the rotation of horizontal/vertical lines, maintains the completeness of the image content, and reduces the warping distortion. An efficient algorithm is developed to address the challenging optimization. We demonstrate our content-aware rotation method on a variety of practical cases.</p><p>2 0.76822335 <a title="90-lsi-2" href="./iccv-2013-Direct_Optimization_of_Frame-to-Frame_Rotation.html">115 iccv-2013-Direct Optimization of Frame-to-Frame Rotation</a></p>
<p>Author: Laurent Kneip, Simon Lynen</p><p>Abstract: This work makes use of a novel, recently proposed epipolar constraint for computing the relative pose between two calibrated images. By enforcing the coplanarity of epipolar plane normal vectors, it constrains the three degrees of freedom of the relative rotation between two camera views directly—independently of the translation. The present paper shows how the approach can be extended to n points, and translated into an efficient eigenvalue minimization over the three rotational degrees of freedom. Each iteration in the non-linear optimization has constant execution time, independently of the number of features. Two global optimization approaches are proposed. The first one consists of an efficient Levenberg-Marquardt scheme with randomized initial value, which already leads to stable and accurate results. The second scheme consists of a globally optimal branch-and-bound algorithm based on a bound on the eigenvalue variation derived from symmetric eigenvalue-perturbation theory. Analysis of the cost function reveals insights into the nature of a specific relative pose problem, and outlines the complexity under different conditions. The algorithm shows state-of-the-art performance w.r.t. essential-matrix based solutions, and a frameto-frame application to a video sequence immediately leads to an alternative, real-time visual odometry solution. Note: All algorithms in this paper are made available in the OpenGV library. Please visit http : / / l aurent kne ip .github . i / opengv o</p><p>3 0.73171759 <a title="90-lsi-3" href="./iccv-2013-Efficient_and_Robust_Large-Scale_Rotation_Averaging.html">138 iccv-2013-Efficient and Robust Large-Scale Rotation Averaging</a></p>
<p>Author: Avishek Chatterjee, Venu Madhav Govindu</p><p>Abstract: In this paper we address the problem of robust and efficient averaging of relative 3D rotations. Apart from having an interesting geometric structure, robust rotation averaging addresses the need for a good initialization for largescale optimization used in structure-from-motion pipelines. Such pipelines often use unstructured image datasets harvested from the internet thereby requiring an initialization method that is robust to outliers. Our approach works on the Lie group structure of 3D rotations and solves the problem of large-scale robust rotation averaging in two ways. Firstly, we use modern ?1 optimizers to carry out robust averaging of relative rotations that is efficient, scalable and robust to outliers. In addition, we also develop a twostep method that uses the ?1 solution as an initialisation for an iteratively reweighted least squares (IRLS) approach. These methods achieve excellent results on large-scale, real world datasets and significantly outperform existing methods, i.e. the state-of-the-art discrete-continuous optimization method of [3] as well as the Weiszfeld method of [8]. We demonstrate the efficacy of our method on two large- scale real world datasets and also provide the results of the two aforementioned methods for comparison.</p><p>4 0.70272297 <a title="90-lsi-4" href="./iccv-2013-Revisiting_the_PnP_Problem%3A_A_Fast%2C_General_and_Optimal_Solution.html">353 iccv-2013-Revisiting the PnP Problem: A Fast, General and Optimal Solution</a></p>
<p>Author: Yinqiang Zheng, Yubin Kuang, Shigeki Sugimoto, Kalle Åström, Masatoshi Okutomi</p><p>Abstract: In this paper, we revisit the classical perspective-n-point (PnP) problem, and propose the first non-iterative O(n) solution that is fast, generally applicable and globally optimal. Our basic idea is to formulate the PnP problem into a functional minimization problem and retrieve all its stationary points by using the Gr¨ obner basis technique. The novelty lies in a non-unit quaternion representation to parameterize the rotation and a simple but elegant formulation of the PnP problem into an unconstrained optimization problem. Interestingly, the polynomial system arising from its first-order optimality condition assumes two-fold symmetry, a nice property that can be utilized to improve speed and numerical stability of a Gr¨ obner basis solver. Experiment results have demonstrated that, in terms of accuracy, our proposed solution is definitely better than the state-ofthe-art O(n) methods, and even comparable with the reprojection error minimization method.</p><p>5 0.67267972 <a title="90-lsi-5" href="./iccv-2013-A_Global_Linear_Method_for_Camera_Pose_Registration.html">17 iccv-2013-A Global Linear Method for Camera Pose Registration</a></p>
<p>Author: Nianjuan Jiang, Zhaopeng Cui, Ping Tan</p><p>Abstract: We present a linear method for global camera pose registration from pairwise relative poses encoded in essential matrices. Our method minimizes an approximate geometric error to enforce the triangular relationship in camera triplets. This formulation does not suffer from the typical ‘unbalanced scale ’ problem in linear methods relying on pairwise translation direction constraints, i.e. an algebraic error; nor the system degeneracy from collinear motion. In the case of three cameras, our method provides a good linear approximation of the trifocal tensor. It can be directly scaled up to register multiple cameras. The results obtained are accurate for point triangulation and can serve as a good initialization for final bundle adjustment. We evaluate the algorithm performance with different types of data and demonstrate its effectiveness. Our system produces good accuracy, robustness, and outperforms some well-known systems on efficiency.</p><p>6 0.67071933 <a title="90-lsi-6" href="./iccv-2013-Go-ICP%3A_Solving_3D_Registration_Efficiently_and_Globally_Optimally.html">185 iccv-2013-Go-ICP: Solving 3D Registration Efficiently and Globally Optimally</a></p>
<p>7 0.66708124 <a title="90-lsi-7" href="./iccv-2013-Global_Fusion_of_Relative_Motions_for_Robust%2C_Accurate_and_Scalable_Structure_from_Motion.html">184 iccv-2013-Global Fusion of Relative Motions for Robust, Accurate and Scalable Structure from Motion</a></p>
<p>8 0.64150202 <a title="90-lsi-8" href="./iccv-2013-Unsupervised_Intrinsic_Calibration_from_a_Single_Frame_Using_a_%22Plumb-Line%22_Approach.html">436 iccv-2013-Unsupervised Intrinsic Calibration from a Single Frame Using a "Plumb-Line" Approach</a></p>
<p>9 0.63378918 <a title="90-lsi-9" href="./iccv-2013-Lifting_3D_Manhattan_Lines_from_a_Single_Image.html">250 iccv-2013-Lifting 3D Manhattan Lines from a Single Image</a></p>
<p>10 0.63183784 <a title="90-lsi-10" href="./iccv-2013-Pose_Estimation_with_Unknown_Focal_Length_Using_Points%2C_Directions_and_Lines.html">323 iccv-2013-Pose Estimation with Unknown Focal Length Using Points, Directions and Lines</a></p>
<p>11 0.60886091 <a title="90-lsi-11" href="./iccv-2013-Automatic_Registration_of_RGB-D_Scans_via_Salient_Directions.html">56 iccv-2013-Automatic Registration of RGB-D Scans via Salient Directions</a></p>
<p>12 0.60627729 <a title="90-lsi-12" href="./iccv-2013-Extrinsic_Camera_Calibration_without_a_Direct_View_Using_Spherical_Mirror.html">152 iccv-2013-Extrinsic Camera Calibration without a Direct View Using Spherical Mirror</a></p>
<p>13 0.59959948 <a title="90-lsi-13" href="./iccv-2013-A_Robust_Analytical_Solution_to_Isometric_Shape-from-Template_with_Focal_Length_Calibration.html">27 iccv-2013-A Robust Analytical Solution to Isometric Shape-from-Template with Focal Length Calibration</a></p>
<p>14 0.58774287 <a title="90-lsi-14" href="./iccv-2013-Rectangling_Stereographic_Projection_for_Wide-Angle_Image_Visualization.html">346 iccv-2013-Rectangling Stereographic Projection for Wide-Angle Image Visualization</a></p>
<p>15 0.57900095 <a title="90-lsi-15" href="./iccv-2013-Space-Time_Tradeoffs_in_Photo_Sequencing.html">397 iccv-2013-Space-Time Tradeoffs in Photo Sequencing</a></p>
<p>16 0.57107544 <a title="90-lsi-16" href="./iccv-2013-An_Enhanced_Structure-from-Motion_Paradigm_Based_on_the_Absolute_Dual_Quadric_and_Images_of_Circular_Points.html">49 iccv-2013-An Enhanced Structure-from-Motion Paradigm Based on the Absolute Dual Quadric and Images of Circular Points</a></p>
<p>17 0.56490993 <a title="90-lsi-17" href="./iccv-2013-BOLD_Features_to_Detect_Texture-less_Objects.html">57 iccv-2013-BOLD Features to Detect Texture-less Objects</a></p>
<p>18 0.55761123 <a title="90-lsi-18" href="./iccv-2013-Enhanced_Continuous_Tabu_Search_for_Parameter_Estimation_in_Multiview_Geometry.html">141 iccv-2013-Enhanced Continuous Tabu Search for Parameter Estimation in Multiview Geometry</a></p>
<p>19 0.54942334 <a title="90-lsi-19" href="./iccv-2013-Refractive_Structure-from-Motion_on_Underwater_Images.html">348 iccv-2013-Refractive Structure-from-Motion on Underwater Images</a></p>
<p>20 0.54563755 <a title="90-lsi-20" href="./iccv-2013-SYM-FISH%3A_A_Symmetry-Aware_Flip_Invariant_Sketch_Histogram_Shape_Descriptor.html">368 iccv-2013-SYM-FISH: A Symmetry-Aware Flip Invariant Sketch Histogram Shape Descriptor</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.043), (7, 0.021), (12, 0.012), (26, 0.051), (31, 0.059), (35, 0.373), (40, 0.015), (42, 0.092), (64, 0.031), (73, 0.048), (89, 0.146), (98, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79162115 <a title="90-lda-1" href="./iccv-2013-Content-Aware_Rotation.html">90 iccv-2013-Content-Aware Rotation</a></p>
<p>Author: Kaiming He, Huiwen Chang, Jian Sun</p><p>Abstract: We present an image editing tool called Content-Aware Rotation. Casually shot photos can appear tilted, and are often corrected by rotation and cropping. This trivial solution may remove desired content and hurt image integrity. Instead of doing rigid rotation, we propose a warping method that creates the perception of rotation and avoids cropping. Human vision studies suggest that the perception of rotation is mainly due to horizontal/vertical lines. We design an optimization-based method that preserves the rotation of horizontal/vertical lines, maintains the completeness of the image content, and reduces the warping distortion. An efficient algorithm is developed to address the challenging optimization. We demonstrate our content-aware rotation method on a variety of practical cases.</p><p>2 0.67658764 <a title="90-lda-2" href="./iccv-2013-Discriminant_Tracking_Using_Tensor_Representation_with_Semi-supervised_Improvement.html">119 iccv-2013-Discriminant Tracking Using Tensor Representation with Semi-supervised Improvement</a></p>
<p>Author: Jin Gao, Junliang Xing, Weiming Hu, Steve Maybank</p><p>Abstract: Visual tracking has witnessed growing methods in object representation, which is crucial to robust tracking. The dominant mechanism in object representation is using image features encoded in a vector as observations to perform tracking, without considering that an image is intrinsically a matrix, or a 2nd-order tensor. Thus approaches following this mechanism inevitably lose a lot of useful information, and therefore cannot fully exploit the spatial correlations within the 2D image ensembles. In this paper, we address an image as a 2nd-order tensor in its original form, and find a discriminative linear embedding space approximation to the original nonlinear submanifold embedded in the tensor space based on the graph embedding framework. We specially design two graphs for characterizing the intrinsic local geometrical structure of the tensor space, so as to retain more discriminant information when reducing the dimension along certain tensor dimensions. However, spatial correlations within a tensor are not limited to the elements along these dimensions. This means that some part of the discriminant information may not be encoded in the embedding space. We introduce a novel technique called semi-supervised improvement to iteratively adjust the embedding space to compensate for the loss of discriminant information, hence improving the performance of our tracker. Experimental results on challenging videos demonstrate the effectiveness and robustness of the proposed tracker.</p><p>3 0.65981442 <a title="90-lda-3" href="./iccv-2013-Decomposing_Bag_of_Words_Histograms.html">104 iccv-2013-Decomposing Bag of Words Histograms</a></p>
<p>Author: Ankit Gandhi, Karteek Alahari, C.V. Jawahar</p><p>Abstract: We aim to decompose a global histogram representation of an image into histograms of its associated objects and regions. This task is formulated as an optimization problem, given a set of linear classifiers, which can effectively discriminate the object categories present in the image. Our decomposition bypasses harder problems associated with accurately localizing and segmenting objects. We evaluate our method on a wide variety of composite histograms, and also compare it with MRF-based solutions. In addition to merely measuring the accuracy of decomposition, we also show the utility of the estimated object and background histograms for the task of image classification on the PASCAL VOC 2007 dataset.</p><p>4 0.63684607 <a title="90-lda-4" href="./iccv-2013-Strong_Appearance_and_Expressive_Spatial_Models_for_Human_Pose_Estimation.html">403 iccv-2013-Strong Appearance and Expressive Spatial Models for Human Pose Estimation</a></p>
<p>Author: Leonid Pishchulin, Mykhaylo Andriluka, Peter Gehler, Bernt Schiele</p><p>Abstract: Typical approaches to articulated pose estimation combine spatial modelling of the human body with appearance modelling of body parts. This paper aims to push the state-of-the-art in articulated pose estimation in two ways. First we explore various types of appearance representations aiming to substantially improve the bodypart hypotheses. And second, we draw on and combine several recently proposed powerful ideas such as more flexible spatial models as well as image-conditioned spatial models. In a series of experiments we draw several important conclusions: (1) we show that the proposed appearance representations are complementary; (2) we demonstrate that even a basic tree-structure spatial human body model achieves state-ofthe-art performance when augmented with the proper appearance representation; and (3) we show that the combination of the best performing appearance model with a flexible image-conditioned spatial model achieves the best result, significantly improving over the state of the art, on the “Leeds Sports Poses ” and “Parse ” benchmarks.</p><p>5 0.57115728 <a title="90-lda-5" href="./iccv-2013-Accurate_and_Robust_3D_Facial_Capture_Using_a_Single_RGBD_Camera.html">36 iccv-2013-Accurate and Robust 3D Facial Capture Using a Single RGBD Camera</a></p>
<p>Author: Yen-Lin Chen, Hsiang-Tao Wu, Fuhao Shi, Xin Tong, Jinxiang Chai</p><p>Abstract: This paper presents an automatic and robust approach that accurately captures high-quality 3D facial performances using a single RGBD camera. The key of our approach is to combine the power of automatic facial feature detection and image-based 3D nonrigid registration techniques for 3D facial reconstruction. In particular, we develop a robust and accurate image-based nonrigid registration algorithm that incrementally deforms a 3D template mesh model to best match observed depth image data and important facial features detected from single RGBD images. The whole process is fully automatic and robust because it is based on single frame facial registration framework. The system is flexible because it does not require any strong 3D facial priors such as blendshape models. We demonstrate the power of our approach by capturing a wide range of 3D facial expressions using a single RGBD camera and achieve state-of-the-art accuracy by comparing against alternative methods.</p><p>6 0.53692132 <a title="90-lda-6" href="./iccv-2013-A_Method_of_Perceptual-Based_Shape_Decomposition.html">21 iccv-2013-A Method of Perceptual-Based Shape Decomposition</a></p>
<p>7 0.53624058 <a title="90-lda-7" href="./iccv-2013-Pictorial_Human_Spaces%3A_How_Well_Do_Humans_Perceive_a_3D_Articulated_Pose%3F.html">316 iccv-2013-Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?</a></p>
<p>8 0.49762797 <a title="90-lda-8" href="./iccv-2013-Robust_Tucker_Tensor_Decomposition_for_Effective_Image_Representation.html">362 iccv-2013-Robust Tucker Tensor Decomposition for Effective Image Representation</a></p>
<p>9 0.49249911 <a title="90-lda-9" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>10 0.49174511 <a title="90-lda-10" href="./iccv-2013-A_Fully_Hierarchical_Approach_for_Finding_Correspondences_in_Non-rigid_Shapes.html">11 iccv-2013-A Fully Hierarchical Approach for Finding Correspondences in Non-rigid Shapes</a></p>
<p>11 0.48869202 <a title="90-lda-11" href="./iccv-2013-A_Simple_Model_for_Intrinsic_Image_Decomposition_with_Depth_Cues.html">30 iccv-2013-A Simple Model for Intrinsic Image Decomposition with Depth Cues</a></p>
<p>12 0.48859981 <a title="90-lda-12" href="./iccv-2013-A_Non-parametric_Bayesian_Network_Prior_of_Human_Pose.html">24 iccv-2013-A Non-parametric Bayesian Network Prior of Human Pose</a></p>
<p>13 0.48634785 <a title="90-lda-13" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>14 0.48545754 <a title="90-lda-14" href="./iccv-2013-Breaking_the_Chain%3A_Liberation_from_the_Temporal_Markov_Assumption_for_Tracking_Human_Poses.html">65 iccv-2013-Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses</a></p>
<p>15 0.48424053 <a title="90-lda-15" href="./iccv-2013-SGTD%3A_Structure_Gradient_and_Texture_Decorrelating_Regularization_for_Image_Decomposition.html">364 iccv-2013-SGTD: Structure Gradient and Texture Decorrelating Regularization for Image Decomposition</a></p>
<p>16 0.48416781 <a title="90-lda-16" href="./iccv-2013-Frustratingly_Easy_NBNN_Domain_Adaptation.html">181 iccv-2013-Frustratingly Easy NBNN Domain Adaptation</a></p>
<p>17 0.48409855 <a title="90-lda-17" href="./iccv-2013-Progressive_Multigrid_Eigensolvers_for_Multiscale_Spectral_Segmentation.html">329 iccv-2013-Progressive Multigrid Eigensolvers for Multiscale Spectral Segmentation</a></p>
<p>18 0.48351422 <a title="90-lda-18" href="./iccv-2013-Rectangling_Stereographic_Projection_for_Wide-Angle_Image_Visualization.html">346 iccv-2013-Rectangling Stereographic Projection for Wide-Angle Image Visualization</a></p>
<p>19 0.48303095 <a title="90-lda-19" href="./iccv-2013-Real-Time_Articulated_Hand_Pose_Estimation_Using_Semi-supervised_Transductive_Regression_Forests.html">340 iccv-2013-Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests</a></p>
<p>20 0.48262545 <a title="90-lda-20" href="./iccv-2013-Fix_Structured_Learning_of_2013_ICCV_paper_k2opt.pdf.html">171 iccv-2013-Fix Structured Learning of 2013 ICCV paper k2opt.pdf</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
