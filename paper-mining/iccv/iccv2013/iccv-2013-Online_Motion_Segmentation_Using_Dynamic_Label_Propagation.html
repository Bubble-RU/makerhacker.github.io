<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-297" href="#">iccv2013-297</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</h1>
<br/><p>Source: <a title="iccv-2013-297-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Elqursh_Online_Motion_Segmentation_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Ali Elqursh, Ahmed Elgammal</p><p>Abstract: The vast majority of work on motion segmentation adopts the affine camera model due to its simplicity. Under the affine model, the motion segmentation problem becomes that of subspace separation. Due to this assumption, such methods are mainly offline and exhibit poor performance when the assumption is not satisfied. This is made evident in state-of-the-art methods that relax this assumption by using piecewise affine spaces and spectral clustering techniques to achieve better results. In this paper, we formulate the problem of motion segmentation as that of manifold separation. We then show how label propagation can be used in an online framework to achieve manifold separation. The performance of our framework is evaluated on a benchmark dataset and achieves competitive performance while being online.</p><p>Reference: <a title="iccv-2013-297-reference" href="../iccv2013_reference/iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Online Motion Segmentation using Dynamic Label Propagation Ali Elqursh Ahmed Elgammal Rutgers University  Abstract The vast majority of work on motion segmentation adopts the affine camera model due to its simplicity. [sent-1, score-0.677]
</p><p>2 Under the affine model, the motion segmentation problem becomes that of subspace separation. [sent-2, score-0.694]
</p><p>3 Due to this assumption, such methods are mainly offline and exhibit poor performance when the assumption is not satisfied. [sent-3, score-0.109]
</p><p>4 This is made evident in state-of-the-art methods that relax this assumption by using piecewise affine spaces and spectral clustering techniques to achieve better results. [sent-4, score-0.558]
</p><p>5 In this paper, we formulate the problem of motion segmentation as that of manifold separation. [sent-5, score-0.701]
</p><p>6 We then show how label propagation can be used in an online framework to achieve manifold separation. [sent-6, score-0.639]
</p><p>7 Introduction Since the early 20th century, gestalt psychologist have identified common fate as one of the most important cues for dynamic scene understanding. [sent-9, score-0.152]
</p><p>8 In the field of Computer  Vision, this is reflected by the vast amount of literature on motion segmentation, video segmentation, and tracking. [sent-10, score-0.269]
</p><p>9 Specifically, motion segmentation deals with the problem of segmenting feature trajectories according to different motions in the scene, and is an essential step to achieve object segmentation and scene understanding. [sent-11, score-1.344]
</p><p>10 Recent years have witnessed a large increase in the proportion of videos coming from streaming sources such as TV Broadcast, internet video streaming, and streaming from mobile devices. [sent-12, score-0.337]
</p><p>11 Unfortunately, most motion segmentation techniques are mainly offline and with a high computational complexity. [sent-13, score-0.503]
</p><p>12 Thus rendering them ineffective for processing videos from streaming sources. [sent-14, score-0.181]
</p><p>13 This highlights the need for novel online motion segmentation techniques. [sent-15, score-0.578]
</p><p>14 There exist a plethora of applications that would benefit from online motion segmentation. [sent-16, score-0.391]
</p><p>15 For example, currently activity recognition is either restricted to videos captured from stationary cameras (where existing background subtraction techniques can be used to segment the differ-  Figure1. [sent-17, score-0.097]
</p><p>16 ent actors [18]), or restricted to process videos offline using motion segmentation techniques. [sent-19, score-0.568]
</p><p>17 Another domain that would benefit from online motion segmentation is in that of 3D TV processing. [sent-21, score-0.578]
</p><p>18 A real-time motion segmentation would enable performing 2D-to-3D conversion and video re-targeting on the fly on viewers devices. [sent-22, score-0.479]
</p><p>19 Other applications include online detection and segmentation of moving targets, and visual surveillance from mobile platforms to name a few. [sent-23, score-0.428]
</p><p>20 Many approaches for motion segmentation are based on the fact that trajectories generated from rigid motion and under affine projection spans a 4-dimensional subspace. [sent-24, score-1.519]
</p><p>21 Most notably in [4], the problem is reduced to sorting of a matrix called shape interaction matrix with entries that represent the likelihood of a pair of trajectories belonging to 2008  the same object. [sent-26, score-0.685]
</p><p>22 In [11] the problem is reformulated as an instance of subspace separation, making the connection explicit. [sent-27, score-0.114]
</p><p>23 First, formulating the problem as that of factorizing a trajectory matrix has led many approaches to assume that trajectories span the entire frame sequence. [sent-29, score-0.896]
</p><p>24 To handle the case where parts of trajectories are missing, such approaches borrow ideas from matrix completion. [sent-30, score-0.583]
</p><p>25 However, this is only successful up to a limit, since it assumes that at least there exist some trajectories that span the entire frame sequences. [sent-31, score-0.675]
</p><p>26 Second, the affine camera assumption restricts the applicability of motion segmentation to those videos where the assumption is satisfied. [sent-32, score-0.799]
</p><p>27 To overcome the later problem, we assume a general perspective camera instead of an affine camera. [sent-34, score-0.187]
</p><p>28 On the other hand, to overcome the former problem, we mea-  sure the similarity between trajectories using a metric that depends only on the overlapping frames. [sent-35, score-0.622]
</p><p>29 We propose an approach that achieves online motion segmentation by segmenting a set of manifolds through dynamic label propagation and cluster splitting. [sent-36, score-1.033]
</p><p>30 Starting from an initialization computed over a fixed number of frames, we maintain a graph of pairwise similarity between trajectories in an online fashion. [sent-37, score-0.785]
</p><p>31 To move to the next frame we propagate the label information from one frame to the next using label propagation. [sent-38, score-0.262]
</p><p>32 The label propagation respects the computed graph structure while taking into account the previous labeling. [sent-39, score-0.218]
</p><p>33 To handle cases where new evidence suggests that one cluster comes from two differently moving objects, we evaluate each cluster and measure a normalized cut cost of splitting the cluster. [sent-40, score-0.134]
</p><p>34 Figure 1 shows frames 40 and 150 of the sequence marple7 and the segmentation by our approach. [sent-42, score-0.255]
</p><p>35 First we show how trajectories belonging to a rigid object with smooth depth variation form a manifold of dimension 3. [sent-46, score-0.939]
</p><p>36 This generalizes the problem of affine motion segmentation from subspace separation (linear manifold segmentation) to that of (gen-  eral) manifold segmentation. [sent-47, score-1.333]
</p><p>37 It also explains why previous approaches using spectral clustering methods produced superior results while using simpler models. [sent-48, score-0.34]
</p><p>38 Second, we show that the problem of online manifold segmentation can be cast in a label propagation framework using Markov Random walks. [sent-49, score-0.884]
</p><p>39 Related Work Approaches to motion segmentation (and similarly subspace separation) can be roughly divided into four categories: statistical, factorization-based, algebraic, and spectral clustering. [sent-51, score-0.768]
</p><p>40 For example, in [9] the Expectation-Maximization (EM) algorithm was used to tackle the clustering problem. [sent-53, score-0.131]
</p><p>41 Robust statistical methods, such as RANSAC [6], repeatedly fits an affine subspace to randomly sampled trajectories and measures the consensus with the remaining trajectories. [sent-54, score-0.794]
</p><p>42 The trajectories belonging to the subspace with the largest number of inliers are then removed and the procedure is repeated. [sent-55, score-0.723]
</p><p>43 However, it is frequently the case that multiple rigid motions are dependent, such as in articulated motion. [sent-58, score-0.14]
</p><p>44 Algebraic methods, such as GPCA [19] are generic subspace separation algorithms. [sent-60, score-0.241]
</p><p>45 They do not put assump-  tions on the relative orientation and dimensionality of motion subspaces. [sent-61, score-0.303]
</p><p>46 However, their complexity grows exponentially with the number of motions and the dimensionality of the ambient space. [sent-62, score-0.104]
</p><p>47 Spectral clustering-based methods [21, 2, 12], use local information around the trajectories to compute a similarity matrix. [sent-63, score-0.58]
</p><p>48 It then use spectral clustering to cluster the trajectories into different subspaces. [sent-64, score-0.952]
</p><p>49 One such example is the approach by Yan et al [21], where neighbors around each trajectory are used to fit a subspace. [sent-65, score-0.15]
</p><p>50 An affinity matrix is then built by measuring the angles between subspaces. [sent-66, score-0.164]
</p><p>51 Spectral clustering is then used to cluster the trajectories. [sent-67, score-0.198]
</p><p>52 Similarly, sparse subspace clustering [5] builds an affinity matrix by representing each trajectory as a sparse combination of all other trajectories and then applies spectral clustering on the resulting affinity matrix. [sent-68, score-1.57]
</p><p>53 Spectral clustering methods represent the state-of-the-art in motion segmentation. [sent-69, score-0.355]
</p><p>54 We believe this can be explained because the trajectories do not exactly form a linear subspace. [sent-70, score-0.545]
</p><p>55 With the realization of accurate trackers for dense long term trajectories such as [13, 15] there have been great interest in exploiting dense long term trajectories in motion segmentation. [sent-72, score-1.348]
</p><p>56 [2] achieves motion segmentation by creating an affinity matrix capturing similarity in translational motion across all pairs of trajectories. [sent-74, score-0.868]
</p><p>57 Spectral clustering is then used to over-segment the set of trajectories. [sent-75, score-0.131]
</p><p>58 [7] pro-  poses a two step process that first uses trajectory saliency to segment foreground trajectories. [sent-78, score-0.182]
</p><p>59 This is followed by a two-stage spectral clustering of an affinity matrix computed 2009  over figure trajectories. [sent-79, score-0.504]
</p><p>60 The success of such approaches can be attributed in part to the large number of trajectories available. [sent-80, score-0.545]
</p><p>61 Such trajectories help capture the manifold structure empirically in the spectral clustering framework. [sent-81, score-1.141]
</p><p>62 Our approach is also based on building an affinity matrix between all pairs of trajectories, however we process frames online and do not rely on spectral clustering. [sent-82, score-0.54]
</p><p>63 Deviating from the spectral clustering, is the idea ofusing nonlinear dimensionality reduction (NLDR) techniques followed by clustering to achieve motion segmentation [8]. [sent-83, score-0.888]
</p><p>64 To our knowledge our approach is the first to achieve online motion segmentation, while spending a constant computation time per frame. [sent-85, score-0.433]
</p><p>65 We explicitly model trajectories as lying on a manifold, and thus are able to handle videos where the affine camera assumption is not satisfied. [sent-86, score-0.848]
</p><p>66 Our method also takes into account the entire history of the trajectory in computing the similarity matrix. [sent-87, score-0.219]
</p><p>67 Basic Formulation of Motion Segmentation In this section we show how the problem of motion segmentation can be cast as a manifold segmentation problem. [sent-89, score-0.978]
</p><p>68 First, we show how  trajectories in the three-dimensional space form a threedimensional manifold. [sent-91, score-0.578]
</p><p>69 Next, we show how the projection of these trajectories to 2D image coordinates also form a three-dimensional manifold. [sent-92, score-0.621]
</p><p>70 Let X be an open set of points in 3D comprising a single rigid object. [sent-93, score-0.074]
</p><p>71 of trajectories can be therefore defined by the set Γ(f) = {(x1, . [sent-106, score-0.545]
</p><p>72 , xF) ∈ R3F : xi = fi(x1) i  = 1},  with subspace topology. [sent-109, score-0.114]
</p><p>73 , Γf(Ff are co Xnti bneuo tuhse maps tainodn fi sπ a restriction of a continuous map, is also continuous. [sent-119, score-0.083]
</p><p>74 It is also a homeomorphism because it has a continuous inverse. [sent-120, score-0.093]
</p><p>75 This implies that the space of trajectories is a manifold of dimension three. [sent-121, score-0.801]
</p><p>76 Furthermore, we can show that projecting the 3D trajectories into the image coordinates also induces a mani-  φ  φo  φ  fz[xy]Tbe the camera projection func-  fold. [sent-122, score-0.673]
</p><p>77 Let g(x) = tion that projects a point in the camera coordinate system to image coordinates, where f is the camera focal length. [sent-123, score-0.104]
</p><p>78 It is therefore easy to show that G(Ω) is also a manifold of dimension three. [sent-139, score-0.256]
</p><p>79 Note that even though we know that trajectories in image space form a manifold, we do not have an analytical manifold. [sent-140, score-0.545]
</p><p>80 However, under the assumption that the manifold is  densely sampled, empirical methods can be used to model the manifold. [sent-141, score-0.307]
</p><p>81 In addition, note that each distinct motion in the scene will generate one manifold. [sent-142, score-0.224]
</p><p>82 In this paper we rely on label propagation and dense trajectory tracking to solve the manifold separation problem. [sent-143, score-0.751]
</p><p>83 To see why label propagation is well suited for the manifold separation problem, consider the simple two moons example shown at the top Figure 2. [sent-144, score-0.701]
</p><p>84 Separating the two moons can be cast as a manifold separation problem. [sent-145, score-0.539]
</p><p>85 However, when applying spectral clustering on this example, due to the proximity, one cluster leaks over the other cluster. [sent-146, score-0.407]
</p><p>86 On the other hand, with proper initialization, label propagation is able to successfully segment the two moons. [sent-147, score-0.25]
</p><p>87 Approach Starting from dense trajectories that are continuously extended and introduced, our approach continuously updates a segmentation of the trajectories corresponding to different motions. [sent-150, score-1.403]
</p><p>88 To achieve this, we start by explaining how the similarities (affinities) between trajectories can be updated in an online framework (Subsection 5. [sent-151, score-0.71]
</p><p>89 Next we introduce the necessary background on label propagation and show how it can be used to maintain a segmentation over dynamically changing manifolds (Subsection 5. [sent-153, score-0.571]
</p><p>90 Online Affinity Computation As identified by the previous section, trajectories belonging to a single object lie on a three-dimensional manifold. [sent-159, score-0.642]
</p><p>91 However, such manifolds are not static as they are a function of the motion of the object, which changes over time. [sent-160, score-0.319]
</p><p>92 To model such dynamic manifolds without resorting to resolving for each frame, we design a distance metric that can be computed incrementally. [sent-161, score-0.21]
</p><p>93 In addition, the metric must capture the similarity in spatial location and motion. [sent-163, score-0.077]
</p><p>94 The intuition is that if two trajectories are relatively close to each other and move similarly, then they are likely to belong to the same object. [sent-164, score-0.545]
</p><p>95 In this subsection we show how one such metric can be computed incrementally. [sent-165, score-0.13]
</p><p>96 A trajectory Ta = {pia = (xia, yai) : i∈ A} is represented as a sequence of points pia that spans ifr ∈am Ae}s iins rtehper esseet nAte. [sent-167, score-0.371]
</p><p>97 F aosr a simplicity we reserve superscripts for frame references and subscripts for trajectory identification. [sent-168, score-0.32]
</p><p>98 The motion of a trajectory between frames iand j in the x and y direction is denoted by uia:j = xja − xia and vai:j = yaj − yai. [sent-169, score-0.509]
</p><p>99 Given tw−o x trajectories Ta an−d yTb we define two distance metrics d1M:t (Ta, Tb) and dS1:t (Ta, Tb) representing the difference in motion and spatial location up to time t respec-  ×  tively. [sent-170, score-0.769]
</p><p>100 The max function helps “remember” large differences in motion and spatial location. [sent-172, score-0.224]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('trajectories', 0.545), ('manifold', 0.256), ('motion', 0.224), ('segmentation', 0.221), ('spectral', 0.209), ('tb', 0.161), ('trajectory', 0.15), ('xf', 0.145), ('propagation', 0.142), ('affine', 0.135), ('online', 0.133), ('clustering', 0.131), ('separation', 0.127), ('affinity', 0.126), ('streaming', 0.116), ('ta', 0.115), ('subspace', 0.114), ('moons', 0.1), ('manifolds', 0.095), ('subsection', 0.088), ('pia', 0.078), ('label', 0.076), ('rigid', 0.074), ('cluster', 0.067), ('motions', 0.066), ('videos', 0.065), ('belonging', 0.064), ('spans', 0.058), ('offline', 0.058), ('xia', 0.057), ('cast', 0.056), ('frame', 0.055), ('camera', 0.052), ('assumption', 0.051), ('continuous', 0.049), ('continuously', 0.046), ('vast', 0.045), ('algebraic', 0.044), ('esseet', 0.044), ('tbe', 0.044), ('binet', 0.044), ('homeomorphism', 0.044), ('fate', 0.044), ('deviating', 0.044), ('lofe', 0.044), ('marple', 0.044), ('spending', 0.044), ('xja', 0.044), ('metric', 0.042), ('span', 0.041), ('hite', 0.041), ('vai', 0.041), ('reserve', 0.041), ('aosr', 0.041), ('elqursh', 0.041), ('ifr', 0.041), ('uia', 0.041), ('tions', 0.041), ('mobile', 0.04), ('dynamic', 0.04), ('broadcast', 0.039), ('coordinates', 0.038), ('projection', 0.038), ('dimensionality', 0.038), ('tv', 0.038), ('matrix', 0.038), ('maintain', 0.037), ('rutgers', 0.037), ('elgammal', 0.037), ('ont', 0.037), ('initialization', 0.035), ('gestalt', 0.035), ('fz', 0.035), ('ahmed', 0.035), ('segmenting', 0.035), ('similarity', 0.035), ('tainodn', 0.034), ('platforms', 0.034), ('fragkiadaki', 0.034), ('plethora', 0.034), ('realization', 0.034), ('viewers', 0.034), ('gpca', 0.034), ('eral', 0.034), ('entire', 0.034), ('frames', 0.034), ('identified', 0.033), ('factorizing', 0.033), ('ons', 0.033), ('superscripts', 0.033), ('resorting', 0.033), ('ofusing', 0.033), ('threedimensional', 0.033), ('segment', 0.032), ('affinities', 0.032), ('achieve', 0.032), ('dependent', 0.031), ('century', 0.031), ('maxi', 0.031), ('efr', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="297-tfidf-1" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<p>Author: Ali Elqursh, Ahmed Elgammal</p><p>Abstract: The vast majority of work on motion segmentation adopts the affine camera model due to its simplicity. Under the affine model, the motion segmentation problem becomes that of subspace separation. Due to this assumption, such methods are mainly offline and exhibit poor performance when the assumption is not satisfied. This is made evident in state-of-the-art methods that relax this assumption by using piecewise affine spaces and spectral clustering techniques to achieve better results. In this paper, we formulate the problem of motion segmentation as that of manifold separation. We then show how label propagation can be used in an online framework to achieve manifold separation. The performance of our framework is evaluated on a benchmark dataset and achieves competitive performance while being online.</p><p>2 0.48789868 <a title="297-tfidf-2" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>Author: Feng Shi, Zhong Zhou, Jiangjian Xiao, Wei Wu</p><p>Abstract: Due to occlusions and objects ’ non-rigid deformation in the scene, the obtained motion trajectories from common trackers may contain a number of missing or mis-associated entries. To cluster such corrupted point based trajectories into multiple motions is still a hard problem. In this paper, we present an approach that exploits temporal and spatial characteristics from tracked points to facilitate segmentation of incomplete and corrupted trajectories, thereby obtain highly robust results against severe data missing and noises. Our method first uses the Discrete Cosine Transform (DCT) bases as a temporal smoothness constraint on trajectory projection to ensure the validity of resulting components to repair pathological trajectories. Then, based on an observation that the trajectories of foreground and background in a scene may have different spatial distributions, we propose a two-stage clustering strategy that first performs foreground-background separation then segments remaining foreground trajectories. We show that, with this new clustering strategy, sequences with complex motions can be accurately segmented by even using a simple trans- lational model. Finally, a series of experiments on Hopkins 155 dataset andBerkeley motion segmentation dataset show the advantage of our method over other state-of-the-art motion segmentation algorithms in terms of both effectiveness and robustness.</p><p>3 0.35514945 <a title="297-tfidf-3" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>Author: Jiaming Guo, Zhuwen Li, Loong-Fah Cheong, Steven Zhiying Zhou</p><p>Abstract: Given a pair of videos having a common action, our goal is to simultaneously segment this pair of videos to extract this common action. As a preprocessing step, we first remove background trajectories by a motion-based figureground segmentation. To remove the remaining background and those extraneous actions, we propose the trajectory cosaliency measure, which captures the notion that trajectories recurring in all the videos should have their mutual saliency boosted. This requires a trajectory matching process which can compare trajectories with different lengths and not necessarily spatiotemporally aligned, and yet be discriminative enough despite significant intra-class variation in the common action. We further leverage the graph matching to enforce geometric coherence between regions so as to reduce feature ambiguity and matching errors. Finally, to classify the trajectories into common action and action outliers, we formulate the problem as a binary labeling of a Markov Random Field, in which the data term is measured by the trajectory co-saliency and the smooth- ness term is measured by the spatiotemporal consistency between trajectories. To evaluate the performance of our framework, we introduce a dataset containing clips that have animal actions as well as human actions. Experimental results show that the proposed method performs well in common action extraction.</p><p>4 0.31869817 <a title="297-tfidf-4" href="./iccv-2013-Camera_Alignment_Using_Trajectory_Intersections_in_Unsynchronized_Videos.html">68 iccv-2013-Camera Alignment Using Trajectory Intersections in Unsynchronized Videos</a></p>
<p>Author: Thomas Kuo, Santhoshkumar Sunderrajan, B.S. Manjunath</p><p>Abstract: This paper addresses the novel and challenging problem of aligning camera views that are unsynchronized by low and/or variable frame rates using object trajectories. Unlike existing trajectory-based alignment methods, our method does not require frame-to-frame synchronization. Instead, we propose using the intersections of corresponding object trajectories to match views. To find these intersections, we introduce a novel trajectory matching algorithm based on matching Spatio-Temporal Context Graphs (STCGs). These graphs represent the distances between trajectories in time and space within a view, and are matched to an STCG from another view to find the corresponding trajectories. To the best of our knowledge, this is one of the first attempts to align views that are unsynchronized with variable frame rates. The results on simulated and real-world datasets show trajectory intersections area viablefeatureforcamera alignment, and that the trajectory matching method performs well in real-world scenarios.</p><p>5 0.28490415 <a title="297-tfidf-5" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>Author: Zhuwen Li, Jiaming Guo, Loong-Fah Cheong, Steven Zhiying Zhou</p><p>Abstract: This paper addresses real-world challenges in the motion segmentation problem, including perspective effects, missing data, and unknown number of motions. It first formulates the 3-D motion segmentation from two perspective views as a subspace clustering problem, utilizing the epipolar constraint of an image pair. It then combines the point correspondence information across multiple image frames via a collaborative clustering step, in which tight integration is achieved via a mixed norm optimization scheme. For model selection, wepropose an over-segment and merge approach, where the merging step is based on the property of the ?1-norm ofthe mutual sparse representation oftwo oversegmented groups. The resulting algorithm can deal with incomplete trajectories and perspective effects substantially better than state-of-the-art two-frame and multi-frame methods. Experiments on a 62-clip dataset show the significant superiority of the proposed idea in both segmentation accuracy and model selection.</p><p>6 0.28335354 <a title="297-tfidf-6" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>7 0.23160045 <a title="297-tfidf-7" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>8 0.22590689 <a title="297-tfidf-8" href="./iccv-2013-Measuring_Flow_Complexity_in_Videos.html">263 iccv-2013-Measuring Flow Complexity in Videos</a></p>
<p>9 0.22088823 <a title="297-tfidf-9" href="./iccv-2013-Latent_Space_Sparse_Subspace_Clustering.html">232 iccv-2013-Latent Space Sparse Subspace Clustering</a></p>
<p>10 0.1849965 <a title="297-tfidf-10" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>11 0.18445881 <a title="297-tfidf-11" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>12 0.17366637 <a title="297-tfidf-12" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>13 0.16300768 <a title="297-tfidf-13" href="./iccv-2013-Unsupervised_Random_Forest_Manifold_Alignment_for_Lipreading.html">437 iccv-2013-Unsupervised Random Forest Manifold Alignment for Lipreading</a></p>
<p>14 0.15839036 <a title="297-tfidf-14" href="./iccv-2013-Robust_Subspace_Clustering_via_Half-Quadratic_Minimization.html">360 iccv-2013-Robust Subspace Clustering via Half-Quadratic Minimization</a></p>
<p>15 0.15339682 <a title="297-tfidf-15" href="./iccv-2013-Minimal_Basis_Facility_Location_for_Subspace_Segmentation.html">264 iccv-2013-Minimal Basis Facility Location for Subspace Segmentation</a></p>
<p>16 0.15073727 <a title="297-tfidf-16" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>17 0.14593621 <a title="297-tfidf-17" href="./iccv-2013-Efficient_Higher-Order_Clustering_on_the_Grassmann_Manifold.html">134 iccv-2013-Efficient Higher-Order Clustering on the Grassmann Manifold</a></p>
<p>18 0.1407436 <a title="297-tfidf-18" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>19 0.13755208 <a title="297-tfidf-19" href="./iccv-2013-A_Unified_Video_Segmentation_Benchmark%3A_Annotation%2C_Metrics_and_Analysis.html">33 iccv-2013-A Unified Video Segmentation Benchmark: Annotation, Metrics and Analysis</a></p>
<p>20 0.12947717 <a title="297-tfidf-20" href="./iccv-2013-Joint_Segmentation_and_Pose_Tracking_of_Human_in_Natural_Videos.html">225 iccv-2013-Joint Segmentation and Pose Tracking of Human in Natural Videos</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.252), (1, -0.066), (2, 0.059), (3, 0.21), (4, -0.113), (5, 0.211), (6, -0.023), (7, 0.232), (8, 0.299), (9, 0.132), (10, 0.111), (11, 0.085), (12, -0.005), (13, -0.034), (14, -0.062), (15, -0.004), (16, -0.009), (17, 0.038), (18, -0.005), (19, 0.002), (20, -0.139), (21, 0.017), (22, 0.167), (23, 0.299), (24, -0.025), (25, 0.172), (26, 0.01), (27, 0.072), (28, 0.075), (29, 0.028), (30, -0.032), (31, 0.051), (32, -0.026), (33, 0.007), (34, 0.057), (35, -0.051), (36, -0.043), (37, 0.019), (38, -0.038), (39, -0.025), (40, 0.054), (41, 0.018), (42, 0.035), (43, -0.042), (44, 0.03), (45, 0.031), (46, 0.056), (47, 0.052), (48, -0.043), (49, -0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97470462 <a title="297-lsi-1" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<p>Author: Ali Elqursh, Ahmed Elgammal</p><p>Abstract: The vast majority of work on motion segmentation adopts the affine camera model due to its simplicity. Under the affine model, the motion segmentation problem becomes that of subspace separation. Due to this assumption, such methods are mainly offline and exhibit poor performance when the assumption is not satisfied. This is made evident in state-of-the-art methods that relax this assumption by using piecewise affine spaces and spectral clustering techniques to achieve better results. In this paper, we formulate the problem of motion segmentation as that of manifold separation. We then show how label propagation can be used in an online framework to achieve manifold separation. The performance of our framework is evaluated on a benchmark dataset and achieves competitive performance while being online.</p><p>2 0.94073695 <a title="297-lsi-2" href="./iccv-2013-Robust_Trajectory_Clustering_for_Motion_Segmentation.html">361 iccv-2013-Robust Trajectory Clustering for Motion Segmentation</a></p>
<p>Author: Feng Shi, Zhong Zhou, Jiangjian Xiao, Wei Wu</p><p>Abstract: Due to occlusions and objects ’ non-rigid deformation in the scene, the obtained motion trajectories from common trackers may contain a number of missing or mis-associated entries. To cluster such corrupted point based trajectories into multiple motions is still a hard problem. In this paper, we present an approach that exploits temporal and spatial characteristics from tracked points to facilitate segmentation of incomplete and corrupted trajectories, thereby obtain highly robust results against severe data missing and noises. Our method first uses the Discrete Cosine Transform (DCT) bases as a temporal smoothness constraint on trajectory projection to ensure the validity of resulting components to repair pathological trajectories. Then, based on an observation that the trajectories of foreground and background in a scene may have different spatial distributions, we propose a two-stage clustering strategy that first performs foreground-background separation then segments remaining foreground trajectories. We show that, with this new clustering strategy, sequences with complex motions can be accurately segmented by even using a simple trans- lational model. Finally, a series of experiments on Hopkins 155 dataset andBerkeley motion segmentation dataset show the advantage of our method over other state-of-the-art motion segmentation algorithms in terms of both effectiveness and robustness.</p><p>3 0.80200171 <a title="297-lsi-3" href="./iccv-2013-Camera_Alignment_Using_Trajectory_Intersections_in_Unsynchronized_Videos.html">68 iccv-2013-Camera Alignment Using Trajectory Intersections in Unsynchronized Videos</a></p>
<p>Author: Thomas Kuo, Santhoshkumar Sunderrajan, B.S. Manjunath</p><p>Abstract: This paper addresses the novel and challenging problem of aligning camera views that are unsynchronized by low and/or variable frame rates using object trajectories. Unlike existing trajectory-based alignment methods, our method does not require frame-to-frame synchronization. Instead, we propose using the intersections of corresponding object trajectories to match views. To find these intersections, we introduce a novel trajectory matching algorithm based on matching Spatio-Temporal Context Graphs (STCGs). These graphs represent the distances between trajectories in time and space within a view, and are matched to an STCG from another view to find the corresponding trajectories. To the best of our knowledge, this is one of the first attempts to align views that are unsynchronized with variable frame rates. The results on simulated and real-world datasets show trajectory intersections area viablefeatureforcamera alignment, and that the trajectory matching method performs well in real-world scenarios.</p><p>4 0.73990375 <a title="297-lsi-4" href="./iccv-2013-Measuring_Flow_Complexity_in_Videos.html">263 iccv-2013-Measuring Flow Complexity in Videos</a></p>
<p>Author: Saad Ali</p><p>Abstract: In this paper a notion of flow complexity that measures the amount of interaction among objects is introduced and an approach to compute it directly from a video sequence is proposed. The approach employs particle trajectories as the input representation of motion and maps it into a ‘braid’ based representation. The mapping is based on the observation that 2D trajectories of particles take the form of a braid in space-time due to the intermingling among particles over time. As a result of this mapping, the problem of estimating the flow complexity from particle trajectories becomes the problem of estimating braid complexity, which in turn can be computed by measuring the topological entropy of a braid. For this purpose recently developed mathematical tools from braid theory are employed which allow rapid computation of topological entropy of braids. The approach is evaluated on a dataset consisting of open source videos depicting variations in terms of types of moving objects, scene layout, camera view angle, motion patterns, and object densities. The results show that the proposed approach is able to quantify the complexity of the flow, and at the same time provides useful insights about the sources of the complexity.</p><p>5 0.73008233 <a title="297-lsi-5" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>Author: Feng Liu, Yuzhen Niu, Hailin Jin</p><p>Abstract: Shaky stereoscopic video is not only unpleasant to watch but may also cause 3D fatigue. Stabilizing the left and right view of a stereoscopic video separately using a monocular stabilization method tends to both introduce undesirable vertical disparities and damage horizontal disparities, which may destroy the stereoscopic viewing experience. In this paper, we present a joint subspace stabilization method for stereoscopic video. We prove that the low-rank subspace constraint for monocular video [10] also holds for stereoscopic video. Particularly, the feature trajectories from the left and right video share the same subspace. Based on this proof, we develop a stereo subspace stabilization method that jointly computes a common subspace from the left and right video and uses it to stabilize the two videos simultaneously. Our method meets the stereoscopic constraints without 3D reconstruction or explicit left-right correspondence. We test our method on a variety of stereoscopic videos with different scene content and camera motion. The experiments show that our method achieves high-quality stabilization for stereoscopic video in a robust and efficient way.</p><p>6 0.71468073 <a title="297-lsi-6" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>7 0.68568796 <a title="297-lsi-7" href="./iccv-2013-Video_Co-segmentation_for_Meaningful_Action_Extraction.html">439 iccv-2013-Video Co-segmentation for Meaningful Action Extraction</a></p>
<p>8 0.66268444 <a title="297-lsi-8" href="./iccv-2013-Inferring_%22Dark_Matter%22_and_%22Dark_Energy%22_from_Videos.html">216 iccv-2013-Inferring "Dark Matter" and "Dark Energy" from Videos</a></p>
<p>9 0.62524384 <a title="297-lsi-9" href="./iccv-2013-Action_Recognition_with_Improved_Trajectories.html">39 iccv-2013-Action Recognition with Improved Trajectories</a></p>
<p>10 0.61518633 <a title="297-lsi-10" href="./iccv-2013-Minimal_Basis_Facility_Location_for_Subspace_Segmentation.html">264 iccv-2013-Minimal Basis Facility Location for Subspace Segmentation</a></p>
<p>11 0.58926374 <a title="297-lsi-11" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<p>12 0.53796238 <a title="297-lsi-12" href="./iccv-2013-A_Unified_Video_Segmentation_Benchmark%3A_Annotation%2C_Metrics_and_Analysis.html">33 iccv-2013-A Unified Video Segmentation Benchmark: Annotation, Metrics and Analysis</a></p>
<p>13 0.53297222 <a title="297-lsi-13" href="./iccv-2013-Unsupervised_Random_Forest_Manifold_Alignment_for_Lipreading.html">437 iccv-2013-Unsupervised Random Forest Manifold Alignment for Lipreading</a></p>
<p>14 0.53265649 <a title="297-lsi-14" href="./iccv-2013-Optimal_Orthogonal_Basis_and_Image_Assimilation%3A_Motion_Modeling.html">301 iccv-2013-Optimal Orthogonal Basis and Image Assimilation: Motion Modeling</a></p>
<p>15 0.50686991 <a title="297-lsi-15" href="./iccv-2013-Efficient_Higher-Order_Clustering_on_the_Grassmann_Manifold.html">134 iccv-2013-Efficient Higher-Order Clustering on the Grassmann Manifold</a></p>
<p>16 0.49736819 <a title="297-lsi-16" href="./iccv-2013-Manipulation_Pattern_Discovery%3A_A_Nonparametric_Bayesian_Approach.html">260 iccv-2013-Manipulation Pattern Discovery: A Nonparametric Bayesian Approach</a></p>
<p>17 0.48384067 <a title="297-lsi-17" href="./iccv-2013-Latent_Space_Sparse_Subspace_Clustering.html">232 iccv-2013-Latent Space Sparse Subspace Clustering</a></p>
<p>18 0.44969225 <a title="297-lsi-18" href="./iccv-2013-Estimating_the_Material_Properties_of_Fabric_from_Video.html">145 iccv-2013-Estimating the Material Properties of Fabric from Video</a></p>
<p>19 0.440018 <a title="297-lsi-19" href="./iccv-2013-Fast_Object_Segmentation_in_Unconstrained_Video.html">160 iccv-2013-Fast Object Segmentation in Unconstrained Video</a></p>
<p>20 0.43987459 <a title="297-lsi-20" href="./iccv-2013-Progressive_Multigrid_Eigensolvers_for_Multiscale_Spectral_Segmentation.html">329 iccv-2013-Progressive Multigrid Eigensolvers for Multiscale Spectral Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.095), (6, 0.198), (7, 0.049), (26, 0.084), (31, 0.052), (40, 0.01), (42, 0.104), (64, 0.057), (73, 0.033), (89, 0.224), (98, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90950811 <a title="297-lda-1" href="./iccv-2013-A_Fully_Hierarchical_Approach_for_Finding_Correspondences_in_Non-rigid_Shapes.html">11 iccv-2013-A Fully Hierarchical Approach for Finding Correspondences in Non-rigid Shapes</a></p>
<p>Author: Ivan Sipiran, Benjamin Bustos</p><p>Abstract: This paper presents a hierarchical method for finding correspondences in non-rigid shapes. We propose a new representation for 3D meshes: the decomposition tree. This structure characterizes the recursive decomposition process of a mesh into regions of interest and keypoints. The internal nodes contain regions of interest (which may be recursively decomposed) and the leaf nodes contain the keypoints to be matched. We also propose a hierarchical matching algorithm that performs in a level-wise manner. The matching process is guided by the similarity between regions in high levels of the tree, until reaching the keypoints stored in the leaves. This allows us to reduce the search space of correspondences, making also the matching process efficient. We evaluate the effectiveness of our approach using the SHREC’2010 robust correspondence benchmark. In addition, we show that our results outperform the state of the art.</p><p>2 0.89733964 <a title="297-lda-2" href="./iccv-2013-Markov_Network-Based_Unified_Classifier_for_Face_Identification.html">261 iccv-2013-Markov Network-Based Unified Classifier for Face Identification</a></p>
<p>Author: Wonjun Hwang, Kyungshik Roh, Junmo Kim</p><p>Abstract: We propose a novel unifying framework using a Markov network to learn the relationship between multiple classifiers in face recognition. We assume that we have several complementary classifiers and assign observation nodes to the features of a query image and hidden nodes to the features of gallery images. We connect each hidden node to its corresponding observation node and to the hidden nodes of other neighboring classifiers. For each observation-hidden node pair, we collect a set of gallery candidates that are most similar to the observation instance, and the relationship between the hidden nodes is captured in terms of the similarity matrix between the collected gallery images. Posterior probabilities in the hidden nodes are computed by the belief-propagation algorithm. The novelty of the proposed framework is the method that takes into account the classifier dependency using the results of each neighboring classifier. We present extensive results on two different evaluation protocols, known and unknown image variation tests, using three different databases, which shows that the proposed framework always leads to good accuracy in face recognition.</p><p>same-paper 3 0.88508141 <a title="297-lda-3" href="./iccv-2013-Online_Motion_Segmentation_Using_Dynamic_Label_Propagation.html">297 iccv-2013-Online Motion Segmentation Using Dynamic Label Propagation</a></p>
<p>Author: Ali Elqursh, Ahmed Elgammal</p><p>Abstract: The vast majority of work on motion segmentation adopts the affine camera model due to its simplicity. Under the affine model, the motion segmentation problem becomes that of subspace separation. Due to this assumption, such methods are mainly offline and exhibit poor performance when the assumption is not satisfied. This is made evident in state-of-the-art methods that relax this assumption by using piecewise affine spaces and spectral clustering techniques to achieve better results. In this paper, we formulate the problem of motion segmentation as that of manifold separation. We then show how label propagation can be used in an online framework to achieve manifold separation. The performance of our framework is evaluated on a benchmark dataset and achieves competitive performance while being online.</p><p>4 0.87505418 <a title="297-lda-4" href="./iccv-2013-Discriminatively_Trained_Templates_for_3D_Object_Detection%3A_A_Real_Time_Scalable_Approach.html">121 iccv-2013-Discriminatively Trained Templates for 3D Object Detection: A Real Time Scalable Approach</a></p>
<p>Author: Reyes Rios-Cabrera, Tinne Tuytelaars</p><p>Abstract: In this paper we propose a new method for detecting multiple specific 3D objects in real time. We start from the template-based approach based on the LINE2D/LINEMOD representation introduced recently by Hinterstoisser et al., yet extend it in two ways. First, we propose to learn the templates in a discriminative fashion. We show that this can be done online during the collection of the example images, in just a few milliseconds, and has a big impact on the accuracy of the detector. Second, we propose a scheme based on cascades that speeds up detection. Since detection of an object is fast, new objects can be added with very low cost, making our approach scale well. In our experiments, we easily handle 10-30 3D objects at frame rates above 10fps using a single CPU core. We outperform the state-of-the-art both in terms of speed as well as in terms of accuracy, as validated on 3 different datasets. This holds both when using monocular color images (with LINE2D) and when using RGBD images (with LINEMOD). Moreover, wepropose a challenging new dataset made of12 objects, for future competing methods on monocular color images.</p><p>5 0.84960592 <a title="297-lda-5" href="./iccv-2013-Locally_Affine_Sparse-to-Dense_Matching_for_Motion_and_Occlusion_Estimation.html">256 iccv-2013-Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation</a></p>
<p>Author: Marius Leordeanu, Andrei Zanfir, Cristian Sminchisescu</p><p>Abstract: Estimating a dense correspondence field between successive video frames, under large displacement, is important in many visual learning and recognition tasks. We propose a novel sparse-to-dense matching method for motion field estimation and occlusion detection. As an alternative to the current coarse-to-fine approaches from the optical flow literature, we start from the higher level of sparse matching with rich appearance and geometric constraints collected over extended neighborhoods, using an occlusion aware, locally affine model. Then, we move towards the simpler, but denser classic flow field model, with an interpolation procedure that offers a natural transition between the sparse and the dense correspondence fields. We experimentally demonstrate that our appearance features and our complex geometric constraintspermit the correct motion estimation even in difficult cases of large displacements and significant appearance changes. We also propose a novel classification method for occlusion detection that works in conjunction with the sparse-to-dense matching model. We validate our approach on the newly released Sintel dataset and obtain state-of-the-art results.</p><p>6 0.81557578 <a title="297-lda-6" href="./iccv-2013-Scene_Text_Localization_and_Recognition_with_Oriented_Stroke_Detection.html">376 iccv-2013-Scene Text Localization and Recognition with Oriented Stroke Detection</a></p>
<p>7 0.81525648 <a title="297-lda-7" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>8 0.81520396 <a title="297-lda-8" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>9 0.81382215 <a title="297-lda-9" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>10 0.81307459 <a title="297-lda-10" href="./iccv-2013-No_Matter_Where_You_Are%3A_Flexible_Graph-Guided_Multi-task_Learning_for_Multi-view_Head_Pose_Classification_under_Target_Motion.html">291 iccv-2013-No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion</a></p>
<p>11 0.81263584 <a title="297-lda-11" href="./iccv-2013-Manipulation_Pattern_Discovery%3A_A_Nonparametric_Bayesian_Approach.html">260 iccv-2013-Manipulation Pattern Discovery: A Nonparametric Bayesian Approach</a></p>
<p>12 0.81218231 <a title="297-lda-12" href="./iccv-2013-Real-Time_Articulated_Hand_Pose_Estimation_Using_Semi-supervised_Transductive_Regression_Forests.html">340 iccv-2013-Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests</a></p>
<p>13 0.8118754 <a title="297-lda-13" href="./iccv-2013-Structured_Forests_for_Fast_Edge_Detection.html">404 iccv-2013-Structured Forests for Fast Edge Detection</a></p>
<p>14 0.81138331 <a title="297-lda-14" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>15 0.811059 <a title="297-lda-15" href="./iccv-2013-PhotoOCR%3A_Reading_Text_in_Uncontrolled_Conditions.html">315 iccv-2013-PhotoOCR: Reading Text in Uncontrolled Conditions</a></p>
<p>16 0.81103623 <a title="297-lda-16" href="./iccv-2013-Linear_Sequence_Discriminant_Analysis%3A_A_Model-Based_Dimensionality_Reduction_Method_for_Vector_Sequences.html">253 iccv-2013-Linear Sequence Discriminant Analysis: A Model-Based Dimensionality Reduction Method for Vector Sequences</a></p>
<p>17 0.810853 <a title="297-lda-17" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>18 0.81058621 <a title="297-lda-18" href="./iccv-2013-CoDeL%3A_A_Human_Co-detection_and_Labeling_Framework.html">75 iccv-2013-CoDeL: A Human Co-detection and Labeling Framework</a></p>
<p>19 0.81018168 <a title="297-lda-19" href="./iccv-2013-Dynamic_Probabilistic_Volumetric_Models.html">128 iccv-2013-Dynamic Probabilistic Volumetric Models</a></p>
<p>20 0.80993772 <a title="297-lda-20" href="./iccv-2013-BOLD_Features_to_Detect_Texture-less_Objects.html">57 iccv-2013-BOLD Features to Detect Texture-less Objects</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
