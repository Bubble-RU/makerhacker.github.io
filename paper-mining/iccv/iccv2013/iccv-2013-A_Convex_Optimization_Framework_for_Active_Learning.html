<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>6 iccv-2013-A Convex Optimization Framework for Active Learning</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-6" href="#">iccv2013-6</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>6 iccv-2013-A Convex Optimization Framework for Active Learning</h1>
<br/><p>Source: <a title="iccv-2013-6-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Elhamifar_A_Convex_Optimization_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Ehsan Elhamifar, Guillermo Sapiro, Allen Yang, S. Shankar Sasrty</p><p>Abstract: In many image/video/web classification problems, we have access to a large number of unlabeled samples. However, it is typically expensive and time consuming to obtain labels for the samples. Active learning is the problem of progressively selecting and annotating the most informative unlabeled samples, in order to obtain a high classification performance. Most existing active learning algorithms select only one sample at a time prior to retraining the classifier. Hence, they are computationally expensive and cannot take advantage of parallel labeling systems such as Mechanical Turk. On the other hand, algorithms that allow the selection of multiple samples prior to retraining the classifier, may select samples that have significant information overlap or they involve solving a non-convex optimization. More importantly, the majority of active learning algorithms are developed for a certain classifier type such as SVM. In this paper, we develop an efficient active learning framework based on convex programming, which can select multiple samples at a time for annotation. Unlike the state of the art, our algorithm can be used in conjunction with any type of classifiers, including those of the fam- ily of the recently proposed Sparse Representation-based Classification (SRC). We use the two principles of classifier uncertainty and sample diversity in order to guide the optimization program towards selecting the most informative unlabeled samples, which have the least information overlap. Our method can incorporate the data distribution in the selection process by using the appropriate dissimilarity between pairs of samples. We show the effectiveness of our framework in person detection, scene categorization and face recognition on real-world datasets.</p><p>Reference: <a title="iccv-2013-6-reference" href="../iccv2013_reference/iccv-2013-A_Convex_Optimization_Framework_for_Active_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Active learning is the problem of progressively selecting and annotating the most informative unlabeled samples, in order to obtain a high classification performance. [sent-4, score-0.725]
</p><p>2 Most existing active learning algorithms select only one sample at a time prior to retraining the classifier. [sent-5, score-0.685]
</p><p>3 On the other hand, algorithms that allow the selection of multiple samples prior to retraining the classifier, may select samples that have significant information overlap or they involve solving a non-convex optimization. [sent-7, score-0.893]
</p><p>4 More importantly, the majority of active learning algorithms are developed for a certain classifier type such as SVM. [sent-8, score-0.621]
</p><p>5 In this paper, we develop an efficient active learning framework based on convex programming, which can select multiple samples at a time for annotation. [sent-9, score-0.879]
</p><p>6 We use the two principles of classifier uncertainty and sample diversity in order to guide the optimization program towards selecting the most informative unlabeled samples, which have the least information overlap. [sent-11, score-1.15]
</p><p>7 Active learning is the problem of progressively  selecting and annotating the most informative data points from the pool of unlabeled samples, in order to obtain a high classification performance. [sent-22, score-0.797]
</p><p>8 The majority of the literature consider the single mode active learning [21, 23, 25, 27, 29, 3 1], where the algorithm selects and annotates only one unlabeled sample at a time prior to retraining the classifier. [sent-25, score-1.197]
</p><p>9 Third, single mode active learning schemes might select and annotate an outlier instead of an informative sample for classification [26]. [sent-29, score-0.91]
</p><p>10 To address some of the above issues, more recent methods have focused on the batch mode active learning, where they select and annotate multiple unlabeled samples at a time prior to retraining the classifier [2, 5, 12, 17, 18]. [sent-31, score-1.618]
</p><p>11 No-  tice that one can run a single mode active learning method multiple times without retraining the classifier in order to select multiple unlabeled samples. [sent-32, score-1.291]
</p><p>12 We  demonstrate the effectiveness of our proposed active learning framework on three problems of person detection, scene categorization and  face recognition. [sent-34, score-0.586]
</p><p>13 the classification performance compared to the single mode active learning scheme. [sent-41, score-0.608]
</p><p>14 Other approaches try to decrease the information overlap among the selected unlabeled samples [2, 12, 13, 18, 36]. [sent-42, score-0.791]
</p><p>15 Moreover, similar to the single mode active learning, most batch mode active learning algorithms are developed for a certain type of a classifier and cannot be easily modified to work with other classifier types [12, 13, 17, 29, 32, 34]. [sent-44, score-1.513]
</p><p>16 In this paper, we develop an efficient active learning framework based on convex program-  ming that can be used in conjunction with any type of classifiers. [sent-46, score-0.577]
</p><p>17 We use the two principles of classifier uncertainty and sample diversity in order to guide the optimization program towards selecting the most informative unlabeled samples. [sent-47, score-1.15]
</p><p>18 More specifically, for each unlabeled sample, we define a confidence score that reflects how uncertain the sample’s predicted label is according to the current classifier and how dissimilar the sample is with respect to the labeled training samples. [sent-48, score-1.326]
</p><p>19 A large value of the confidence score for an unlabeled sample means that the current classifier is more certain about the predicted label of the sample and also the sample is more similar to the labeled training samples. [sent-49, score-1.401]
</p><p>20 On the other hand, an unlabeled sample with a small confidence score is more informative and should be labeled. [sent-51, score-0.929]
</p><p>21 Since we can have many unlabeled samples with low confidence scores and they may have information overlap with each other, i. [sent-52, score-1.121]
</p><p>22 , can be similar to each other, we need to select a few representatives of the unlabeled samples with low confidence scores. [sent-54, score-1.201]
</p><p>23 The algorithm that we develop has the following advantages with respect to the state of the art: It addresses the batch mode active leaning problem, hence, it can take advantage of parallel annotation systems such as Mechanical Turk and LabelMe. [sent-56, score-0.586]
</p><p>24 The choice of the classifier affects selection of unlabeled samples through the confidence scores, but the proposed framework is generic. [sent-58, score-1.184]
</p><p>25 Unlike the state of the art, it incorporates both the classifier uncertainty and sample diversity in a convex optimization to select multiple informative samples that are diverse with respect to each other and the labeled samples. [sent-62, score-1.287]
</p><p>26 Active Learning via Convex Programming In this section, we propose an efficient algorithm for active learning that takes advantage of convex programming in order to find the most informative points. [sent-140, score-0.644]
</p><p>27 To do so, we use the two principles of classifier uncertainty and sample diversity to define confidence scores for unlabeled samples. [sent-142, score-1.314]
</p><p>28 A lower confidence score for an unlabeled sample indicates that we can obtain more information by annotating that sample. [sent-143, score-0.862]
</p><p>29 However, the number of unlabeled samples with low confidence scores can be large and, more importantly, the samples can have information overlap with each other or they can be outliers. [sent-144, score-1.453]
</p><p>30 Thus, we integrate the confidence scores in the DSMRS framework in order to find a few representative unlabeled samples that have low confidence scores. [sent-145, score-1.403]
</p><p>31 In the subsequent sections, we define the confidence scores and show how to use them in the DSMRS framework in order to find the most informative samples. [sent-146, score-0.489]
</p><p>32 Classifier Uncertainty First, we use the classifier uncertainty in order to select  informative points for improving the classifier performance. [sent-150, score-0.636]
</p><p>33 The uncertainty sampling principle [4] states that the informative samples for classification are the ones that the classifier is most uncertain about. [sent-151, score-0.827]
</p><p>34 Notice that the classifier is more confident about the labels of samples in and as they are farther from the decision boundary,  Gj(i)  G3(1)  G2(2)  G2(1),  while it is less confident about the labels of samples in swihniclee they are ccolnosfiedre ntot atbheo hyperplane boundary. [sent-159, score-0.955]
</p><p>35 Inn G this case, labeling any of the samples in or does not change the decision boundary, hence, samples in will scthial n bgee m thiesc dleascsiisfiioend. [sent-160, score-0.798]
</p><p>36 Now, for a generic classifier, we define its confidence about the predicted label of an unlabeled sample. [sent-162, score-0.647]
</p><p>37 We define the classifier confidence score of point i as  cclassifier(i) ? [sent-171, score-0.489]
</p><p>38 where xij denotes the representation coefficients using labeled samples from class j. [sent-194, score-0.538]
</p><p>39 Noatned dth Gat the most uncertain samples according to the classifier  are samples from G1(1) and G(12), which are close to the decision boundary. [sent-201, score-0.971]
</p><p>40 Hareow saevmeprl, slab freolimng G sucha samples does not change the decision boundary  G2(1)  G2(2)  much and samples in and will still be misclassified. [sent-202, score-0.768]
</p><p>41 Right: mlaubeclhing an samples ethsa int are sufafincdien Gtly dissimilar from the labeled training samples helps to improve the classification performance. [sent-203, score-1.0]
</p><p>42 Sample Diversity We also use the sample diversity criterion in order to find the most informative points for improving the classifier performance. [sent-212, score-0.611]
</p><p>43 More specifically, sample diversity states that informative points for classification are the ones that are sufficiently dissimilar from the labeled training samples (and from themselves in the batch mode setting). [sent-213, score-1.337]
</p><p>44 The max-margin hyperplane learned via SVM for the two training samples is shown in the the left plot of Figure 3. [sent-219, score-0.481]
</p><p>45 le Ndo samples t( shaamvep slmesa ilnl E Guclidaenadn G Gdistances to the labeled samples in this example). [sent-221, score-0.81]
</p><p>46 In fact, labeling any of the samples in or does not change the decision boundary much, and the points in will be still mdeicsicsliaosnsif bioedun as belonging tod chleas pso i2n. [sent-222, score-0.536]
</p><p>47 O inn Gthe other hand, samples in and are more dissimilar from the labeled training samples. [sent-223, score-0.61]
</p><p>48 In fact, labeling a sample from or changes the decision boundary so that points in the  Gj(i)  G1(1)  G1(1)  G2(1)  G2(2)  G1(2)  G1(2)  G2(1)  G2(1)  G2(2)  212  linear SVM learned using two training samples (green crosses). [sent-224, score-0.695]
</p><p>49 Middle: two  samples with lowest confidence scores correspond to two samples from  that are close to the decision boundary. [sent-227, score-1.093]
</p><p>50 A retrained classifier using  samples, which have information overlap, still misclassifies samples  Right: two representatives  to a sample from  G2(1)  and a sample from  G(22). [sent-228, score-1.006]
</p><p>51 A retrained  these two  of samples with low confidence scores correspond  classifier using these two samples correctly classifies all the samples in the dataset. [sent-229, score-1.632]
</p><p>52 To incorporate diversity with respect to the labeled training set, L, for a point iin the unlabeled set, U, we define tinheg diversity confidence score as  cdiversity(i) ? [sent-231, score-1.237]
</p><p>53 When the closest labeled sample to an unlabeled sample iis very similar to it, i. [sent-233, score-0.787]
</p><p>54 On the other hand, when all labeled samples are very dissimilar from an unlabeled sample i, i. [sent-239, score-1.058]
</p><p>55 This means that selecting and annotating sample ipromotes diversity with respect to the labeled samples. [sent-244, score-0.53]
</p><p>56 Selecting Informative Samples Recall that our goal is to have a batch mode active learn-  ing framework that selects multiple informative and diverse unlabeled samples, with respect to the labeled samples as well as each other, for annotation. [sent-247, score-1.661]
</p><p>57 One can think of a simple algorithm that selects samples that have the lowest confidence scores. [sent-248, score-0.672]
</p><p>58 The drawback of this approach is that while the selected unlabeled samples are diverse with respect to the labeled training samples, they can still have significant information overlap with each other. [sent-249, score-1.01]
</p><p>59 This comes from the fact that the confidence scores only reflect the relationship of each unlabeled sample with respect to the classifier and the labeled training samples and do not capture the relationships among the unlabeled samples. [sent-250, score-1.927]
</p><p>60 A max-margin hyperplane learned via SVM for the two training samples is shown in the the left plot of Figure 4. [sent-253, score-0.481]
</p><p>61 No-  G2(2)  some samples in G2(1) tice that samples in G2(1) have small classifier and diversity confidence scores and samples in G2(2) have small diversity  ccoonnffiiddeennccee scores. [sent-255, score-1.909]
</p><p>62 aNndow sa, imf we ss ienlec Gt two samples with lowest confidence scores, we will select two as they are very oclreosse, wtoe twhiel d seecliescito tnw boundary. [sent-256, score-0.666]
</p><p>63 In fact, after adding these two samples to the labeled training set, the retrained classifier, shown in the middle plot of Figure 4, still misclassifies sam-  samples from G(21),  ples in G2(2). [sent-258, score-1.014]
</p><p>64 ×  On the other hand, two representatives of samples iwni tGh low confidence scores, i. [sent-259, score-0.764]
</p><p>65 , two samples that capture the distribution of samples with low confidence scores, correspond to one sample from and one sample from . [sent-261, score-1.168]
</p><p>66 To select a few diverse representatives of unlabeled samples that have low confidence scores, we take advantage of the DSMRS algorithm. [sent-263, score-1.231]
</p><p>67 Let D ∈ R|U| |U| be the dissimilarity MmRaStrix a gfoorr samples i nD th ∈e R unlabeled set U = s{iim1 , ·l a· r·i , i|U| }at. [sent-264, score-0.792]
</p><p>68 , c(i|U| i)x) i Zs th ∈e c Ronfidence matrix with the active learning confidence scores, c(i), defined as c(ik)  s. [sent-274, score-0.691]
</p><p>69 (9)  More specifically, for an unlabeled sample ik that has a small confidence score c(ik), the optimization program puts less penalty on the k-th row of Z being nonzero. [sent-278, score-0.966]
</p><p>70 On the other hand, for a sample ik that has a large confidence score c(ik), the optimization program puts more penalty on the k-th row of Z being nonzero. [sent-279, score-0.591]
</p><p>71 Classification accuracy of different active learning algorithms on the INRIA Person dataset as a function of the total number of labeled training samples selected by each algorithm. [sent-281, score-1.006]
</p><p>72 motes selecting a few unlabeled samples with low confidence scores that are, at the same time, representatives of the distribution of the samples. [sent-282, score-1.277]
</p><p>73 To illustrate the effect of confidence scores and representativeness of samples in the performance of our proposed framework, we consider several methods for comparison. [sent-297, score-0.724]
</p><p>74 We select Kt samples uniformly at random from the pool of unlabeled samples. [sent-301, score-0.811]
</p><p>75 We select Kt samples that have the smallest classifier confidence scores. [sent-303, score-0.841]
</p><p>76 Total number of samples from each class of INRIA Person dataset selected by our proposed algorithm (CPAL) at different active learning iterations. [sent-306, score-0.846]
</p><p>77 We use the positive/negative training images in the dataset to form the pool of unlabeled samples (2, 416 positive and 2, 736 negative samples) and use the the positive/negative test images for testing (1, 126 positive and 900 negative samples). [sent-313, score-0.792]
</p><p>78 Figure 5 shows the classification accuracy of different active learning methods on the test set as a function of the total number of labeled samples. [sent-317, score-0.654]
</p><p>79 This comes from the fact that the selected samples by CCAL can have information overlap and are not necessarily representing the distribution of unlabeled samples with  –  –  214  Figure 7. [sent-321, score-1.123]
</p><p>80 Classification accuracy of different active learning algorithms on the Fifteen Scene Categories dataset as a function of the total number of labeled training samples selected by each algorithm. [sent-322, score-1.006]
</p><p>81 Although our active learning algorithm is unaware of the separation of unlabeled samples into classes, it consistently selects about the same number of samples from each class. [sent-326, score-1.526]
</p><p>82 We randomly select 80% of images in each class to form the pool of unlabeled samples and use the rest of the 20% of images in each class for testing. [sent-333, score-0.931]
</p><p>83 Figure 7 shows the accuracy of different active learning methods on the test set as a function of the total number of selected samples. [sent-338, score-0.485]
</p><p>84 Classification accuracy of different active learning algorithms on the Extended YaleB Face dataset as a function of the total number of labeled training samples selected by each algorithm. [sent-341, score-1.006]
</p><p>85 vious section, here the RAND method, in general, has a better performance than CCAL method that selects multiple samples with low confidence scores. [sent-342, score-0.672]
</p><p>86 A careful look into the selected samples by different methods shows that, this is due to the fact that CCAL may repeatedly select similar samples from a fixed class while a random strategy, in general, does not get stuck to repeatedly select similar samples from a fixed class. [sent-343, score-1.215]
</p><p>87 We randomly select 80% of images in each class to form the pool of unlabeled samples and use the rest of the 20% of images in each class for testing. [sent-349, score-0.931]
</p><p>88 To the best of our knowledge, our work is the first one addressing the active learning problem in conjunction with SRC. [sent-351, score-0.484]
</p><p>89 Figure 8 shows the classification accuracy of different active learning methods as a function of the total number of labeled training samples selected by each algorithm. [sent-354, score-1.064]
</p><p>90 With a total of 790 labeled samples (average of 21 samples per class), we obtain the same accuracy (about 97%) as reported in [35] for 32 random samples per class. [sent-356, score-1.173]
</p><p>91 As 215  a result, samples with low confidence scores are generally  dissimilar from each other. [sent-360, score-0.786]
</p><p>92 Conclusions We proposed a batch mode active learning algorithm based on simultaneous sparse recovery that can be used in conjunction with any classifier type. [sent-362, score-0.924]
</p><p>93 The advantage of our algorithm with respect to the state of the art is that it incorporates classifier uncertainty and sample diversity principles via confidence scores in a convex programming scheme. [sent-363, score-1.072]
</p><p>94 Thus, it selects the most informative unlabeled samples for classification that are sufficiently dissimilar from each other as well as the labeled samples and represent the distribution of the unlabeled samples. [sent-364, score-1.899]
</p><p>95 Incorporating diversity in active learning with support vector machines. [sent-382, score-0.585]
</p><p>96 Semi-supervised svm batch mode active learning with applications to image retrieval. [sent-456, score-0.699]
</p><p>97 Text classification from labeled and unlabeled documents using em. [sent-511, score-0.626]
</p><p>98 An analysis of active learning strategies for sequence labeling tasks. [sent-544, score-0.489]
</p><p>99 Beyond active noun tagging: Modeling contextual interactions for multi-class active learning. [sent-549, score-0.706]
</p><p>100 Incorporating diversity and density in active learning for relevance feedback. [sent-602, score-0.585]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('unlabeled', 0.375), ('active', 0.353), ('samples', 0.332), ('confidence', 0.272), ('classifier', 0.175), ('diversity', 0.166), ('representatives', 0.16), ('ccal', 0.15), ('dsmrs', 0.15), ('labeled', 0.146), ('mode', 0.131), ('informative', 0.124), ('sample', 0.116), ('batch', 0.102), ('scores', 0.093), ('dissimilarities', 0.091), ('dissimilar', 0.089), ('retraining', 0.088), ('dij', 0.087), ('vijayanarasimhan', 0.085), ('dissimilarity', 0.085), ('cclassifier', 0.075), ('cdiversity', 0.075), ('cpal', 0.075), ('uncertainty', 0.07), ('labeling', 0.07), ('src', 0.069), ('selects', 0.068), ('crosses', 0.068), ('uncertain', 0.068), ('ik', 0.067), ('convex', 0.066), ('learning', 0.066), ('retrained', 0.066), ('conjunction', 0.065), ('decision', 0.064), ('person', 0.063), ('puts', 0.062), ('select', 0.062), ('class', 0.06), ('categorization', 0.059), ('representative', 0.059), ('classification', 0.058), ('elhamifar', 0.058), ('yaleb', 0.058), ('annotating', 0.057), ('fifteen', 0.055), ('plot', 0.054), ('hyperplane', 0.052), ('dijzij', 0.05), ('overlap', 0.049), ('hence', 0.048), ('svm', 0.047), ('principles', 0.047), ('documents', 0.047), ('mechanical', 0.047), ('face', 0.045), ('selecting', 0.045), ('inria', 0.044), ('sapiro', 0.043), ('training', 0.043), ('rand', 0.043), ('score', 0.042), ('pool', 0.042), ('amnd', 0.041), ('misclassifies', 0.041), ('tice', 0.041), ('onr', 0.04), ('boundary', 0.04), ('belong', 0.04), ('sastry', 0.039), ('kt', 0.038), ('spm', 0.038), ('zij', 0.037), ('encoding', 0.036), ('selected', 0.035), ('programming', 0.035), ('iis', 0.034), ('notice', 0.034), ('nonzero', 0.034), ('pij', 0.033), ('art', 0.032), ('program', 0.032), ('sparse', 0.032), ('total', 0.031), ('aro', 0.03), ('classifies', 0.03), ('misclassified', 0.03), ('selection', 0.03), ('diverse', 0.03), ('points', 0.03), ('entropy', 0.029), ('pi', 0.028), ('zi', 0.028), ('type', 0.027), ('gj', 0.027), ('iin', 0.027), ('illustrate', 0.027), ('green', 0.027), ('probability', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000011 <a title="6-tfidf-1" href="./iccv-2013-A_Convex_Optimization_Framework_for_Active_Learning.html">6 iccv-2013-A Convex Optimization Framework for Active Learning</a></p>
<p>Author: Ehsan Elhamifar, Guillermo Sapiro, Allen Yang, S. Shankar Sasrty</p><p>Abstract: In many image/video/web classification problems, we have access to a large number of unlabeled samples. However, it is typically expensive and time consuming to obtain labels for the samples. Active learning is the problem of progressively selecting and annotating the most informative unlabeled samples, in order to obtain a high classification performance. Most existing active learning algorithms select only one sample at a time prior to retraining the classifier. Hence, they are computationally expensive and cannot take advantage of parallel labeling systems such as Mechanical Turk. On the other hand, algorithms that allow the selection of multiple samples prior to retraining the classifier, may select samples that have significant information overlap or they involve solving a non-convex optimization. More importantly, the majority of active learning algorithms are developed for a certain classifier type such as SVM. In this paper, we develop an efficient active learning framework based on convex programming, which can select multiple samples at a time for annotation. Unlike the state of the art, our algorithm can be used in conjunction with any type of classifiers, including those of the fam- ily of the recently proposed Sparse Representation-based Classification (SRC). We use the two principles of classifier uncertainty and sample diversity in order to guide the optimization program towards selecting the most informative unlabeled samples, which have the least information overlap. Our method can incorporate the data distribution in the selection process by using the appropriate dissimilarity between pairs of samples. We show the effectiveness of our framework in person detection, scene categorization and face recognition on real-world datasets.</p><p>2 0.29343137 <a title="6-tfidf-2" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>Author: Sunil Bandla, Kristen Grauman</p><p>Abstract: Collecting and annotating videos of realistic human actions is tedious, yet critical for training action recognition systems. We propose a method to actively request the most useful video annotations among a large set of unlabeled videos. Predicting the utility of annotating unlabeled video is not trivial, since any given clip may contain multiple actions of interest, and it need not be trimmed to temporal regions of interest. To deal with this problem, we propose a detection-based active learner to train action category models. We develop a voting-based framework to localize likely intervals of interest in an unlabeled clip, and use them to estimate the total reduction in uncertainty that annotating that clip would yield. On three datasets, we show our approach can learn accurate action detectors more efficiently than alternative active learning strategies that fail to accommodate the “untrimmed” nature of real video data.</p><p>3 0.17361559 <a title="6-tfidf-3" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>Author: Gang Hua, Chengjiang Long, Ming Yang, Yan Gao</p><p>Abstract: Active learning is an effective way of engaging users to interactively train models for visual recognition. The vast majority of previous works, if not all of them, focused on active learning with a single human oracle. The problem of active learning with multiple oracles in a collaborative setting has not been well explored. Moreover, most of the previous works assume that the labels provided by the human oracles are noise free, which may often be violated in reality. We present a collaborative computational model for active learning with multiple human oracles. It leads to not only an ensemble kernel machine that is robust to label noises, but also a principled label quality measure to online detect irresponsible labelers. Instead of running independent active learning processes for each individual human oracle, our model captures the inherent correlations among the labelers through shared data among them. Our simulation experiments and experiments with real crowd-sourced noisy labels demonstrated the efficacy of our model.</p><p>4 0.1676835 <a title="6-tfidf-4" href="./iccv-2013-Active_Visual_Recognition_with_Expertise_Estimation_in_Crowdsourcing.html">43 iccv-2013-Active Visual Recognition with Expertise Estimation in Crowdsourcing</a></p>
<p>Author: Chengjiang Long, Gang Hua, Ashish Kapoor</p><p>Abstract: We present a noise resilient probabilistic model for active learning of a Gaussian process classifier from crowds, i.e., a set of noisy labelers. It explicitly models both the overall label noises and the expertise level of each individual labeler in two levels of flip models. Expectation propagation is adopted for efficient approximate Bayesian inference of our probabilistic model for classification, based on which, a generalized EM algorithm is derived to estimate both the global label noise and the expertise of each individual labeler. The probabilistic nature of our model immediately allows the adoption of the prediction entropy and estimated expertise for active selection of data sample to be labeled, and active selection of high quality labelers to label the data, respectively. We apply the proposed model for three visual recognition tasks, i.e, object category recognition, gender recognition, and multi-modal activity recognition, on three datasets with real crowd-sourced labels from Amazon Mechanical Turk. The experiments clearly demonstrated the efficacy of the proposed model.</p><p>5 0.1669745 <a title="6-tfidf-5" href="./iccv-2013-Ensemble_Projection_for_Semi-supervised_Image_Classification.html">142 iccv-2013-Ensemble Projection for Semi-supervised Image Classification</a></p>
<p>Author: Dengxin Dai, Luc Van_Gool</p><p>Abstract: This paper investigates the problem of semi-supervised classification. Unlike previous methods to regularize classifying boundaries with unlabeled data, our method learns a new image representation from all available data (labeled and unlabeled) andperformsplain supervised learning with the new feature. In particular, an ensemble of image prototype sets are sampled automatically from the available data, to represent a rich set of visual categories/attributes. Discriminative functions are then learned on these prototype sets, and image are represented by the concatenation of their projected values onto the prototypes (similarities to them) for further classification. Experiments on four standard datasets show three interesting phenomena: (1) our method consistently outperforms previous methods for semi-supervised image classification; (2) our method lets itself combine well with these methods; and (3) our method works well for self-taught image classification where unlabeled data are not coming from the same distribution as la- beled ones, but rather from a random collection of images.</p><p>6 0.15397887 <a title="6-tfidf-6" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>7 0.12944886 <a title="6-tfidf-7" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>8 0.11642108 <a title="6-tfidf-8" href="./iccv-2013-Sequential_Bayesian_Model_Update_under_Structured_Scene_Prior_for_Semantic_Road_Scenes_Labeling.html">386 iccv-2013-Sequential Bayesian Model Update under Structured Scene Prior for Semantic Road Scenes Labeling</a></p>
<p>9 0.11064117 <a title="6-tfidf-9" href="./iccv-2013-CoDeL%3A_A_Human_Co-detection_and_Labeling_Framework.html">75 iccv-2013-CoDeL: A Human Co-detection and Labeling Framework</a></p>
<p>10 0.10969905 <a title="6-tfidf-10" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>11 0.10813922 <a title="6-tfidf-11" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>12 0.10126581 <a title="6-tfidf-12" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>13 0.10112041 <a title="6-tfidf-13" href="./iccv-2013-Learning_CRFs_for_Image_Parsing_with_Adaptive_Subgradient_Descent.html">234 iccv-2013-Learning CRFs for Image Parsing with Adaptive Subgradient Descent</a></p>
<p>14 0.099749714 <a title="6-tfidf-14" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>15 0.09753003 <a title="6-tfidf-15" href="./iccv-2013-Active_MAP_Inference_in_CRFs_for_Efficient_Semantic_Segmentation.html">42 iccv-2013-Active MAP Inference in CRFs for Efficient Semantic Segmentation</a></p>
<p>16 0.093009226 <a title="6-tfidf-16" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>17 0.091689214 <a title="6-tfidf-17" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>18 0.090525702 <a title="6-tfidf-18" href="./iccv-2013-Feature_Weighting_via_Optimal_Thresholding_for_Video_Analysis.html">163 iccv-2013-Feature Weighting via Optimal Thresholding for Video Analysis</a></p>
<p>19 0.087423623 <a title="6-tfidf-19" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>20 0.085803993 <a title="6-tfidf-20" href="./iccv-2013-Randomized_Ensemble_Tracking.html">338 iccv-2013-Randomized Ensemble Tracking</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.21), (1, 0.115), (2, -0.043), (3, -0.067), (4, 0.006), (5, -0.024), (6, 0.006), (7, 0.034), (8, 0.007), (9, -0.069), (10, -0.012), (11, -0.035), (12, -0.027), (13, -0.066), (14, 0.116), (15, -0.081), (16, -0.105), (17, -0.035), (18, -0.064), (19, 0.001), (20, -0.052), (21, -0.056), (22, -0.156), (23, 0.086), (24, 0.016), (25, -0.017), (26, 0.14), (27, 0.152), (28, 0.139), (29, -0.052), (30, -0.139), (31, 0.011), (32, 0.079), (33, -0.143), (34, 0.049), (35, 0.016), (36, 0.052), (37, -0.059), (38, -0.005), (39, 0.02), (40, 0.049), (41, -0.013), (42, 0.016), (43, 0.09), (44, 0.024), (45, -0.023), (46, -0.018), (47, -0.003), (48, 0.015), (49, 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97702652 <a title="6-lsi-1" href="./iccv-2013-A_Convex_Optimization_Framework_for_Active_Learning.html">6 iccv-2013-A Convex Optimization Framework for Active Learning</a></p>
<p>Author: Ehsan Elhamifar, Guillermo Sapiro, Allen Yang, S. Shankar Sasrty</p><p>Abstract: In many image/video/web classification problems, we have access to a large number of unlabeled samples. However, it is typically expensive and time consuming to obtain labels for the samples. Active learning is the problem of progressively selecting and annotating the most informative unlabeled samples, in order to obtain a high classification performance. Most existing active learning algorithms select only one sample at a time prior to retraining the classifier. Hence, they are computationally expensive and cannot take advantage of parallel labeling systems such as Mechanical Turk. On the other hand, algorithms that allow the selection of multiple samples prior to retraining the classifier, may select samples that have significant information overlap or they involve solving a non-convex optimization. More importantly, the majority of active learning algorithms are developed for a certain classifier type such as SVM. In this paper, we develop an efficient active learning framework based on convex programming, which can select multiple samples at a time for annotation. Unlike the state of the art, our algorithm can be used in conjunction with any type of classifiers, including those of the fam- ily of the recently proposed Sparse Representation-based Classification (SRC). We use the two principles of classifier uncertainty and sample diversity in order to guide the optimization program towards selecting the most informative unlabeled samples, which have the least information overlap. Our method can incorporate the data distribution in the selection process by using the appropriate dissimilarity between pairs of samples. We show the effectiveness of our framework in person detection, scene categorization and face recognition on real-world datasets.</p><p>2 0.89313185 <a title="6-lsi-2" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>Author: Gang Hua, Chengjiang Long, Ming Yang, Yan Gao</p><p>Abstract: Active learning is an effective way of engaging users to interactively train models for visual recognition. The vast majority of previous works, if not all of them, focused on active learning with a single human oracle. The problem of active learning with multiple oracles in a collaborative setting has not been well explored. Moreover, most of the previous works assume that the labels provided by the human oracles are noise free, which may often be violated in reality. We present a collaborative computational model for active learning with multiple human oracles. It leads to not only an ensemble kernel machine that is robust to label noises, but also a principled label quality measure to online detect irresponsible labelers. Instead of running independent active learning processes for each individual human oracle, our model captures the inherent correlations among the labelers through shared data among them. Our simulation experiments and experiments with real crowd-sourced noisy labels demonstrated the efficacy of our model.</p><p>3 0.8624503 <a title="6-lsi-3" href="./iccv-2013-Active_Visual_Recognition_with_Expertise_Estimation_in_Crowdsourcing.html">43 iccv-2013-Active Visual Recognition with Expertise Estimation in Crowdsourcing</a></p>
<p>Author: Chengjiang Long, Gang Hua, Ashish Kapoor</p><p>Abstract: We present a noise resilient probabilistic model for active learning of a Gaussian process classifier from crowds, i.e., a set of noisy labelers. It explicitly models both the overall label noises and the expertise level of each individual labeler in two levels of flip models. Expectation propagation is adopted for efficient approximate Bayesian inference of our probabilistic model for classification, based on which, a generalized EM algorithm is derived to estimate both the global label noise and the expertise of each individual labeler. The probabilistic nature of our model immediately allows the adoption of the prediction entropy and estimated expertise for active selection of data sample to be labeled, and active selection of high quality labelers to label the data, respectively. We apply the proposed model for three visual recognition tasks, i.e, object category recognition, gender recognition, and multi-modal activity recognition, on three datasets with real crowd-sourced labels from Amazon Mechanical Turk. The experiments clearly demonstrated the efficacy of the proposed model.</p><p>4 0.72770822 <a title="6-lsi-4" href="./iccv-2013-Ensemble_Projection_for_Semi-supervised_Image_Classification.html">142 iccv-2013-Ensemble Projection for Semi-supervised Image Classification</a></p>
<p>Author: Dengxin Dai, Luc Van_Gool</p><p>Abstract: This paper investigates the problem of semi-supervised classification. Unlike previous methods to regularize classifying boundaries with unlabeled data, our method learns a new image representation from all available data (labeled and unlabeled) andperformsplain supervised learning with the new feature. In particular, an ensemble of image prototype sets are sampled automatically from the available data, to represent a rich set of visual categories/attributes. Discriminative functions are then learned on these prototype sets, and image are represented by the concatenation of their projected values onto the prototypes (similarities to them) for further classification. Experiments on four standard datasets show three interesting phenomena: (1) our method consistently outperforms previous methods for semi-supervised image classification; (2) our method lets itself combine well with these methods; and (3) our method works well for self-taught image classification where unlabeled data are not coming from the same distribution as la- beled ones, but rather from a random collection of images.</p><p>5 0.66768271 <a title="6-lsi-5" href="./iccv-2013-Learning_CRFs_for_Image_Parsing_with_Adaptive_Subgradient_Descent.html">234 iccv-2013-Learning CRFs for Image Parsing with Adaptive Subgradient Descent</a></p>
<p>Author: Honghui Zhang, Jingdong Wang, Ping Tan, Jinglu Wang, Long Quan</p><p>Abstract: We propose an adaptive subgradient descent method to efficiently learn the parameters of CRF models for image parsing. To balance the learning efficiency and performance of the learned CRF models, the parameter learning is iteratively carried out by solving a convex optimization problem in each iteration, which integrates a proximal term to preserve the previously learned information and the large margin preference to distinguish bad labeling and the ground truth labeling. A solution of subgradient descent updating form is derived for the convex optimization problem, with an adaptively determined updating step-size. Besides, to deal with partially labeled training data, we propose a new objective constraint modeling both the labeled and unlabeled parts in the partially labeled training data for the parameter learning of CRF models. The superior learning efficiency of the proposed method is verified by the experiment results on two public datasets. We also demonstrate the powerfulness of our method for handling partially labeled training data.</p><p>6 0.66451973 <a title="6-lsi-6" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>7 0.6622985 <a title="6-lsi-7" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>8 0.63056016 <a title="6-lsi-8" href="./iccv-2013-Drosophila_Embryo_Stage_Annotation_Using_Label_Propagation.html">125 iccv-2013-Drosophila Embryo Stage Annotation Using Label Propagation</a></p>
<p>9 0.62494552 <a title="6-lsi-9" href="./iccv-2013-Joint_Learning_of_Discriminative_Prototypes_and_Large_Margin_Nearest_Neighbor_Classifiers.html">222 iccv-2013-Joint Learning of Discriminative Prototypes and Large Margin Nearest Neighbor Classifiers</a></p>
<p>10 0.6210351 <a title="6-lsi-10" href="./iccv-2013-Unbiased_Metric_Learning%3A_On_the_Utilization_of_Multiple_Datasets_and_Web_Images_for_Softening_Bias.html">431 iccv-2013-Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias</a></p>
<p>11 0.60512054 <a title="6-lsi-11" href="./iccv-2013-Active_Learning_of_an_Action_Detector_from_Untrimmed_Videos.html">41 iccv-2013-Active Learning of an Action Detector from Untrimmed Videos</a></p>
<p>12 0.59105462 <a title="6-lsi-12" href="./iccv-2013-From_Point_to_Set%3A_Extend_the_Learning_of_Distance_Metrics.html">177 iccv-2013-From Point to Set: Extend the Learning of Distance Metrics</a></p>
<p>13 0.56555909 <a title="6-lsi-13" href="./iccv-2013-Image_Segmentation_with_Cascaded_Hierarchical_Models_and_Logistic_Disjunctive_Normal_Networks.html">211 iccv-2013-Image Segmentation with Cascaded Hierarchical Models and Logistic Disjunctive Normal Networks</a></p>
<p>14 0.56450397 <a title="6-lsi-14" href="./iccv-2013-Image_Set_Classification_Using_Holistic_Multiple_Order_Statistics_Features_and_Localized_Multi-kernel_Metric_Learning.html">212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</a></p>
<p>15 0.55783421 <a title="6-lsi-15" href="./iccv-2013-Sequential_Bayesian_Model_Update_under_Structured_Scene_Prior_for_Semantic_Road_Scenes_Labeling.html">386 iccv-2013-Sequential Bayesian Model Update under Structured Scene Prior for Semantic Road Scenes Labeling</a></p>
<p>16 0.55564725 <a title="6-lsi-16" href="./iccv-2013-From_Semi-supervised_to_Transfer_Counting_of_Crowds.html">178 iccv-2013-From Semi-supervised to Transfer Counting of Crowds</a></p>
<p>17 0.54722911 <a title="6-lsi-17" href="./iccv-2013-Quadruplet-Wise_Image_Similarity_Learning.html">332 iccv-2013-Quadruplet-Wise Image Similarity Learning</a></p>
<p>18 0.54509515 <a title="6-lsi-18" href="./iccv-2013-Large-Scale_Image_Annotation_by_Efficient_and_Robust_Kernel_Metric_Learning.html">227 iccv-2013-Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning</a></p>
<p>19 0.54369915 <a title="6-lsi-19" href="./iccv-2013-Efficient_Pedestrian_Detection_by_Directly_Optimizing_the_Partial_Area_under_the_ROC_Curve.html">136 iccv-2013-Efficient Pedestrian Detection by Directly Optimizing the Partial Area under the ROC Curve</a></p>
<p>20 0.53973049 <a title="6-lsi-20" href="./iccv-2013-Beyond_Hard_Negative_Mining%3A_Efficient_Detector_Learning_via_Block-Circulant_Decomposition.html">61 iccv-2013-Beyond Hard Negative Mining: Efficient Detector Learning via Block-Circulant Decomposition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.104), (4, 0.015), (7, 0.024), (26, 0.103), (31, 0.048), (33, 0.145), (42, 0.138), (48, 0.015), (64, 0.04), (73, 0.025), (78, 0.019), (89, 0.201), (97, 0.013), (98, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93826866 <a title="6-lda-1" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>Author: Ping Luo, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: Recent works have shown that facial attributes are useful in a number of applications such as face recognition and retrieval. However, estimating attributes in images with large variations remains a big challenge. This challenge is addressed in this paper. Unlike existing methods that assume the independence of attributes during their estimation, our approach captures the interdependencies of local regions for each attribute, as well as the high-order correlations between different attributes, which makes it more robust to occlusions and misdetection of face regions. First, we have modeled region interdependencies with a discriminative decision tree, where each node consists of a detector and a classifier trained on a local region. The detector allows us to locate the region, while the classifier determines the presence or absence of an attribute. Second, correlations of attributes and attribute predictors are modeled by organizing all of the decision trees into a large sum-product network (SPN), which is learned by the EM algorithm and yields the most probable explanation (MPE) of the facial attributes in terms of the region ’s localization and classification. Experimental results on a large data set with 22, 400 images show the effectiveness of the proposed approach.</p><p>2 0.92403048 <a title="6-lda-2" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>Author: Gang Hua, Chengjiang Long, Ming Yang, Yan Gao</p><p>Abstract: Active learning is an effective way of engaging users to interactively train models for visual recognition. The vast majority of previous works, if not all of them, focused on active learning with a single human oracle. The problem of active learning with multiple oracles in a collaborative setting has not been well explored. Moreover, most of the previous works assume that the labels provided by the human oracles are noise free, which may often be violated in reality. We present a collaborative computational model for active learning with multiple human oracles. It leads to not only an ensemble kernel machine that is robust to label noises, but also a principled label quality measure to online detect irresponsible labelers. Instead of running independent active learning processes for each individual human oracle, our model captures the inherent correlations among the labelers through shared data among them. Our simulation experiments and experiments with real crowd-sourced noisy labels demonstrated the efficacy of our model.</p><p>same-paper 3 0.91159832 <a title="6-lda-3" href="./iccv-2013-A_Convex_Optimization_Framework_for_Active_Learning.html">6 iccv-2013-A Convex Optimization Framework for Active Learning</a></p>
<p>Author: Ehsan Elhamifar, Guillermo Sapiro, Allen Yang, S. Shankar Sasrty</p><p>Abstract: In many image/video/web classification problems, we have access to a large number of unlabeled samples. However, it is typically expensive and time consuming to obtain labels for the samples. Active learning is the problem of progressively selecting and annotating the most informative unlabeled samples, in order to obtain a high classification performance. Most existing active learning algorithms select only one sample at a time prior to retraining the classifier. Hence, they are computationally expensive and cannot take advantage of parallel labeling systems such as Mechanical Turk. On the other hand, algorithms that allow the selection of multiple samples prior to retraining the classifier, may select samples that have significant information overlap or they involve solving a non-convex optimization. More importantly, the majority of active learning algorithms are developed for a certain classifier type such as SVM. In this paper, we develop an efficient active learning framework based on convex programming, which can select multiple samples at a time for annotation. Unlike the state of the art, our algorithm can be used in conjunction with any type of classifiers, including those of the fam- ily of the recently proposed Sparse Representation-based Classification (SRC). We use the two principles of classifier uncertainty and sample diversity in order to guide the optimization program towards selecting the most informative unlabeled samples, which have the least information overlap. Our method can incorporate the data distribution in the selection process by using the appropriate dissimilarity between pairs of samples. We show the effectiveness of our framework in person detection, scene categorization and face recognition on real-world datasets.</p><p>4 0.88382268 <a title="6-lda-4" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>Author: Hua Wang, Feiping Nie, Weidong Cai, Heng Huang</p><p>Abstract: Representing the raw input of a data set by a set of relevant codes is crucial to many computer vision applications. Due to the intrinsic sparse property of real-world data, dictionary learning, in which the linear decomposition of a data point uses a set of learned dictionary bases, i.e., codes, has demonstrated state-of-the-art performance. However, traditional dictionary learning methods suffer from three weaknesses: sensitivity to noisy and outlier samples, difficulty to determine the optimal dictionary size, and incapability to incorporate supervision information. In this paper, we address these weaknesses by learning a Semi-Supervised Robust Dictionary (SSR-D). Specifically, we use the ℓ2,0+ norm as the loss function to improve the robustness against outliers, and develop a new structured sparse regularization com, , tom. . cai@sydney . edu . au , heng@uta .edu make the learning tasks easier to deal with and reduce the computational cost. For example, in image tagging, instead of using the raw pixel-wise features, semi-local or patch- based features, such as SIFT and geometric blur, are usually more desirable to achieve better performance. In practice, finding a set of compact features bases, also referred to as dictionary, with enhanced representative and discriminative power, plays a significant role in building a successful computer vision system. In this paper, we explore this important problem by proposing a novel formulation and its solution for learning Semi-Supervised Robust Dictionary (SSRD), where we examine the challenges in dictionary learning, and seek opportunities to overcome them and improve the dictionary qualities. 1.1. Challenges in Dictionary Learning to incorporate the supervision information in dictionary learning, without incurring additional parameters. Moreover, the optimal dictionary size is automatically learned from the input data. Minimizing the derived objective function is challenging because it involves many non-smooth ℓ2,0+ -norm terms. We present an efficient algorithm to solve the problem with a rigorous proof of the convergence of the algorithm. Extensive experiments are presented to show the superior performance of the proposed method.</p><p>5 0.8832925 <a title="6-lda-5" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>Author: Bo Wang, Zhuowen Tu, John K. Tsotsos</p><p>Abstract: In graph-based semi-supervised learning approaches, the classification rate is highly dependent on the size of the availabel labeled data, as well as the accuracy of the similarity measures. Here, we propose a semi-supervised multi-class/multi-label classification scheme, dynamic label propagation (DLP), which performs transductive learning through propagation in a dynamic process. Existing semi-supervised classification methods often have difficulty in dealing with multi-class/multi-label problems due to the lack in consideration of label correlation; our algorithm instead emphasizes dynamic metric fusion with label information. Significant improvement over the state-of-the-art methods is observed on benchmark datasets for both multiclass and multi-label tasks.</p><p>6 0.88193977 <a title="6-lda-6" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>7 0.88149714 <a title="6-lda-7" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>8 0.87892324 <a title="6-lda-8" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>9 0.87852573 <a title="6-lda-9" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>10 0.87850702 <a title="6-lda-10" href="./iccv-2013-Learning_a_Dictionary_of_Shape_Epitomes_with_Applications_to_Image_Labeling.html">245 iccv-2013-Learning a Dictionary of Shape Epitomes with Applications to Image Labeling</a></p>
<p>11 0.87823153 <a title="6-lda-11" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>12 0.87772411 <a title="6-lda-12" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>13 0.87732458 <a title="6-lda-13" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>14 0.87719107 <a title="6-lda-14" href="./iccv-2013-A_Scalable_Unsupervised_Feature_Merging_Approach_to_Efficient_Dimensionality_Reduction_of_High-Dimensional_Visual_Data.html">29 iccv-2013-A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data</a></p>
<p>15 0.87688279 <a title="6-lda-15" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>16 0.8767941 <a title="6-lda-16" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>17 0.87667352 <a title="6-lda-17" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>18 0.87599671 <a title="6-lda-18" href="./iccv-2013-Hybrid_Deep_Learning_for_Face_Verification.html">206 iccv-2013-Hybrid Deep Learning for Face Verification</a></p>
<p>19 0.87577945 <a title="6-lda-19" href="./iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps.html">208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</a></p>
<p>20 0.87509024 <a title="6-lda-20" href="./iccv-2013-Low-Rank_Sparse_Coding_for_Image_Classification.html">258 iccv-2013-Low-Rank Sparse Coding for Image Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
