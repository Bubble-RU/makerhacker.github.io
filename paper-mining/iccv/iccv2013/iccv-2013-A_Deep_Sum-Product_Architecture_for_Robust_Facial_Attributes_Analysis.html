<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-7" href="#">iccv2013-7</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</h1>
<br/><p>Source: <a title="iccv-2013-7-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Luo_A_Deep_Sum-Product_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Ping Luo, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: Recent works have shown that facial attributes are useful in a number of applications such as face recognition and retrieval. However, estimating attributes in images with large variations remains a big challenge. This challenge is addressed in this paper. Unlike existing methods that assume the independence of attributes during their estimation, our approach captures the interdependencies of local regions for each attribute, as well as the high-order correlations between different attributes, which makes it more robust to occlusions and misdetection of face regions. First, we have modeled region interdependencies with a discriminative decision tree, where each node consists of a detector and a classifier trained on a local region. The detector allows us to locate the region, while the classifier determines the presence or absence of an attribute. Second, correlations of attributes and attribute predictors are modeled by organizing all of the decision trees into a large sum-product network (SPN), which is learned by the EM algorithm and yields the most probable explanation (MPE) of the facial attributes in terms of the region ’s localization and classification. Experimental results on a large data set with 22, 400 images show the effectiveness of the proposed approach.</p><p>Reference: <a title="iccv-2013-7-reference" href="../iccv2013_reference/iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 hk  Abstract Recent works have shown that facial attributes are useful in a number of applications such as face recognition and retrieval. [sent-9, score-0.287]
</p><p>2 First, we have modeled region interdependencies with a discriminative decision tree, where each node consists of a detector and a classifier trained on a local region. [sent-13, score-0.41]
</p><p>3 They are powerful as high-level representations of images in a variety of tasks, such as object recognition, image-to-text matching, and attribute discovery. [sent-19, score-0.252]
</p><p>4 The usefulness of face attributes has also been demonstrated in the applications of face search [11], ranking [19], ∗This work is supported by the General Research Fund sponsored by the Research Grants Council of the Kong Kong SAR (Project No. [sent-20, score-0.323]
</p><p>5 The success of these algorithms relies heavily on the accuracy of predicted attribute values, i. [sent-27, score-0.252]
</p><p>6 Since the query “young woman smiling” contains the attribute “woman”, face images with attribute “mustache” will be disregarded, and those with attribute “blond hair” are likely to be selected. [sent-35, score-0.878]
</p><p>7 In this case, the attribute values can greatly bias the algorithms built on top of them. [sent-42, score-0.252]
</p><p>8 All the above pose a big challenge for attribute estimation. [sent-47, score-0.252]
</p><p>9 Nevertheless, human vision can predict attributes even with the presence of large variations. [sent-48, score-0.227]
</p><p>10 First, an attribute can be estimated even from small image regions. [sent-50, score-0.252]
</p><p>11 Second, if a region has been occluded, an attribute can still be inferred by its interdependence with respect to other regions. [sent-54, score-0.35]
</p><p>12 Third, the presence of some attributes may indicate the absence or presence of others. [sent-57, score-0.295]
</p><p>13 For example, given the “black hair” and “black eyes” of the woman shown at the right, the attribute “European” is unlikely to be present. [sent-58, score-0.311]
</p><p>14 In this work, we propose estimating attributes from face images that may be corrupted. [sent-59, score-0.26]
</p><p>15 Our method involves the following three steps: 1) automatically discover the discriminative regions for an attribute, 2) explore interdependencies of these regions, and 3) learn correlations of different attributes. [sent-60, score-0.293]
</p><p>16 Each node corresponds to a rectangle region in images, and contains a region detector as well as a regional classifier. [sent-62, score-0.456]
</p><p>17 The region detector provides reliable localization over a cluttered background and occluded faces, while the regional classifier predicts the attribute. [sent-63, score-0.357]
</p><p>18 Region discovery at each node can be considered as selecting the most discriminative region separating positive and negative samples. [sent-64, score-0.207]
</p><p>19 In the second step, a learned decision tree is transformed  to a sum-product tree (SPT) to explore interdependencies among discovered discriminative regions. [sent-65, score-0.311]
</p><p>20 SPT outputs the likelihood that an attribute will be present, using the scores of all the regional classifiers as input. [sent-66, score-0.487]
</p><p>21 If a region is misdetected, the output of the corresponding regional classifier can be efficiently marginalized during inference without affecting attribute estimation. [sent-67, score-0.553]
</p><p>22 In the third step, we organize all the SPTs into a sumproduct network (SPN) that models the joint probability of the SPTs (organzine the regional classifiers in a hierarchical manner) and the attribute labels by stacking many layers of sum and product nodes. [sent-68, score-0.732]
</p><p>23 As the SPN goes deeper, an exponentially large number of attribute correlations can be compactly encoded. [sent-69, score-0.429]
</p><p>24 Even for a large and deep architecture, SPN can be efficiently learned by the EM algorithm, and can perform exact inference, such as the most probable explanation (MPE), with which misdetection handling is like answering “probabilistic queries” [5]. [sent-70, score-0.304]
</p><p>25 We can efficiently derive the probabilities of the presences of attributes conditioned on only a few evidences, i. [sent-71, score-0.231]
</p><p>26 We propose a new deep SPN architecture for robust estimation of facial attributes. [sent-76, score-0.307]
</p><p>27 Our system models attribute correlations using a new network architecture that combines decision trees with SPN, and has some attractive advantages. [sent-77, score-0.571]
</p><p>28 1) Discriminative regions for each attribute can be automatically discovered. [sent-78, score-0.282]
</p><p>29 The joint distribution of classification scores and attribute labels can be learned from unoccluded training samples. [sent-80, score-0.377]
</p><p>30 3) This new network architecture can efficiently compute the correlation between any subsets of attributes, which is not trivial (there are 2n possible subsets for n attributes). [sent-81, score-0.229]
</p><p>31 Related Work We review several robust deep models to handle data corruption, such as noise and occlusion. [sent-85, score-0.214]
</p><p>32 [22] extended Hinton’s deep autoencoder [8] by randomly corrupting the input of the restricted Boltzmann machine (RBM) at the pre-training stage. [sent-88, score-0.245]
</p><p>33 We first select discriminative regions with a discriminative decision tree (DDT) (a). [sent-100, score-0.222]
</p><p>34 To explore region interdependencies, each DDT is transformed  to a  sum-product  tree  (SPT) (c) through the representation of a join tree (b). [sent-101, score-0.233]
</p><p>35 Unlike previous deep models, our work does not need to employ or synthesis training data with noise and can naturally cope with any kinds of corruptions, because the scores of the misdetected region classifiers can be marginalized out. [sent-103, score-0.476]
</p><p>36 We extend the traditional SPN by proposing a new architecture with 12 layers for modeling correlations of attributes. [sent-105, score-0.237]
</p><p>37 Our deep SPN learns the joint distribution of the regional classifiers’ scores and the attributes’ labels from the images without occlusions, and it can capture  correlations between any subset of attributes. [sent-106, score-0.532]
</p><p>38 Discriminative Region Discovery A binary discriminative decision tree (DDT) is learned for each attribute to discover discriminative regions. [sent-110, score-0.465]
</p><p>39 2 (a), each node corresponds to a rectangle region and contains a region detector and a regional classifier. [sent-112, score-0.456]
</p><p>40 Each node splits data into its children by first scanning images with the region detector and then classifying the attribute with the regional classifier once the region has been located. [sent-113, score-0.739]
</p><p>41 Both the “presence” and “absence” sets contain images with faces, indicating whether the attribute is “on” or “off”. [sent-119, score-0.252]
</p><p>42 Therefore, given a sampled rectangle with its position denoted as (x, y), we train the region classifier with latent-SVM that iterates between locally searching the region position (x, y) in each image and optimizing the parameters α by minimizing the objective function,  rc  L(α) =12? [sent-126, score-0.202]
</p><p>43 The sum and product nodes are arranged in alternating layers: all the children of a product node are sums, and all the children of a sum node are products or terminals. [sent-165, score-0.603]
</p><p>44 An edge (v+ , v∗) that connects a sum node with its child v∗ has weight wv+v∗ > 0, and all the weights of its children ? [sent-166, score-0.312]
</p><p>45 en of An edge (v∗ , v+) that connects a product node v∗ with its child ∈ Ch(v∗)  v+  ∈Ch(v+)  v+. [sent-168, score-0.27]
</p><p>46 This is why we can estimate the occluded attributes conditioning on the observed ones. [sent-192, score-0.268]
</p><p>47 Our SPT considers the scores of the regional classifiers {ri} in Eq. [sent-194, score-0.235]
</p><p>48 If a region is not detectable, DDT must guess the output of the  v+,  regional classifier at the misdetected node, while SPT can estimate the attribute by marginalizing the variables of the undetected regions. [sent-198, score-0.733]
</p><p>49 A decision tree can be transformed to a sum-product structure with the join tree using the algorithm proposed in [10, 5]. [sent-199, score-0.215]
</p><p>50 A join tree is an undirected tree on which each node is a set of variables, called a cluster, and each edge is labeled with the intersection of the adjacent clusters, called a separator. [sent-200, score-0.265]
</p><p>51 The join tree of our decision tree is illustrated in Fig. [sent-201, score-0.215]
</p><p>52 Our Architecture Our deep SPN jointly models the outputs of regional classifiers and the labels of attributes. [sent-211, score-0.418]
</p><p>53 1) SPTs, denoted as {Ti}iK=1, con-  ×  {rij}iK=,1N,j=1,  tain terminals with continuous values, R = teaaicnh orfm iwnahlisch w i tnhdi ccoantetins tohues cvlaalusseisfi,ca Rtio =n score of the j-th regional classifier of the i-th attribute. [sent-214, score-0.272]
</p><p>54 Ai = 1 and Ai = 0 denote that the i-th attribute is present; otherwise, Ai = 0 and Ai = 1. [sent-216, score-0.252]
</p><p>55 In the following, we denote the name of an attribute with bold Ai and its binary indicator with Ai, Ai. [sent-217, score-0.273]
</p><p>56 5, where different sets of nodes (in dashed lines) model correlations of different groups of attributes (in bold type). [sent-219, score-0.419]
</p><p>57 A set of product nodes represent the instantiations for an attribute group, e. [sent-220, score-0.457]
</p><p>58 a node in layer 4 can be viewed as A1A2A3A4 for group A1A2A3A4 in layer 5. [sent-222, score-0.34]
</p><p>59 A set of sum nodes represents the possible correlations among a group of attributes, e. [sent-223, score-0.286]
</p><p>60 there are 3 possible correlations (3 sum nodes) of A1A2A3A4 in layer 5, each being the weighted sum of the product nodes (instantiations) in layer 4. [sent-225, score-0.554]
</p><p>61 the number of sum nodes of each attribute group, is an empirical parameter. [sent-228, score-0.372]
</p><p>62 Once it has been set, the number of product nodes for an attribute group is determined. [sent-229, score-0.408]
</p><p>63 5, since both groups A1A2 and A3A4 have 3 sum nodes, there are 3 3 = 9 products in layer 4. [sent-231, score-0.247]
</p><p>64 1) In layer 1, we treat the SPTs and the indicators of two attributes as one group, e. [sent-236, score-0.366]
</p><p>65 2) In layer 2, a product node connects to all the sum nodes of a group in layer 1if they represent the correlations of the same set of attributes; 3) In the upper layer, a sum node connects to all the products belonging to the same group of attributes, e. [sent-240, score-1.003]
</p><p>66 each sum of A1A2 in red connects to all the products in green, since they all relate to the correlations of attributes A1 and A2. [sent-242, score-0.487]
</p><p>67 A product node connects to two sum nodes to form the instantiations of an upper attribute group. [sent-243, score-0.674]
</p><p>68 In our SPN, different  attribute  correlations  groups of nodes, e. [sent-246, score-0.414]
</p><p>69 are modeled by different correlation  When more layers are added, an exponentially  attribute correlations  can be compactly  K  Ti represents  large number of  = {rij }iK=,1N,j=1. [sent-249, score-0.513]
</p><p>70 If a region is located, apply the corresponding regional classifier. [sent-252, score-0.231]
</p><p>71 If vi∗ ∈ Ch(v+) is a child (product) of a sum node let P(vi∗∈ ∈|Rd C) hb(ev the probability ofchoosing the i-th child vi∗ at sum n|oRde conditioned on  v+,  v+,  the observables Rd. [sent-275, score-0.339]
</p><p>72 Then the weights at each sum 2868  Algorithm  1:  Learning  Deep  SPN  Algorithm 1: Learning Deep SPN Input: All the training images {Ii} and the labels of 73 attributes IOnuptuptu:t A: w tahned t rnaeitnwinogrk im maarcgheitse {ctIur}e 1. [sent-283, score-0.257]
</p><p>73 train DDT for each attribute as described in Sec. [sent-284, score-0.252]
</p><p>74 As we have 73 attributes in our experiment, layer 3 in Fig. [sent-304, score-0.296]
</p><p>75 To ensure compactness as well as combination diversity, we introduce two constraints at layer 3 and the above layers: 1) a group in the previous layer can be combined only once; and 2) two groups with no intersection have higher priority to be combined. [sent-316, score-0.28]
</p><p>76 This strategy will reduce the number of groups at each layer by half at a time, while retaining the diversity of attribute combinations. [sent-322, score-0.39]
</p><p>77 10, 000 unoccluded images are selected for training and another 10, 000 unoccluded images and 2, 400 occluded images (some occluded examples are shown in Fig. [sent-334, score-0.33]
</p><p>78 We use the learned SPN to compute the correlation between two attributes as the conditional probability of one when the other is present in a way as described in Sec. [sent-341, score-0.233]
</p><p>79 Note that the correlation between any two sets of attributes can be computed in the same way. [sent-343, score-0.233]
</p><p>80 7 plots the pairwise correlations among 15 attributes, where an element in this matrix represents the probability of the attribute at the j-th column when the attribute at the i-th row is present. [sent-348, score-0.627]
</p><p>81 The experiment results show that attribute correlations can be well discovered by the proposed deep SPN. [sent-351, score-0.589]
</p><p>82 Our deep SPN is compared 2869  DeMSAp SCPN792 Mlae9 13Ainas8960etiWh89048cBkal98534yabB8716dilCh8 362uoYht89416gMeA. [sent-354, score-0.214]
</p><p>83 Classification accuracies of deep SPN, SAC [12, 11], and MAC [2]. [sent-366, score-0.258]
</p><p>84 with the separate attribute classifiers (SAC) [12, 11] and the multilevel attribute classifiers (MAC) [2] on both the unoccluded and occluded testing images. [sent-368, score-0.749]
</p><p>85 The original MAC is to predict human attribute based on poselets. [sent-370, score-0.252]
</p><p>86 On the unoccluded test, the averaged classification accuracies of SAC, MAC, and deep SPN are 82. [sent-375, score-0.352]
</p><p>87 The deep SPN generally performs better especially with the presence of occlusions, because it effectively models the high-order correlations between attributes and regional classifiers. [sent-382, score-0.728]
</p><p>88 On the occluded test set, the deep SPN significantly outperforms SAC and MAC on nearly all the attributes, because SAC assumes independence between attributes, while MAC can only model pairwise correlation. [sent-383, score-0.313]
</p><p>89 2%  of the unoccluded and occluded tests, respectively, which indicate their performances can be significantly improved when organizing them into the deep SPN. [sent-387, score-0.402]
</p><p>90 Moreover, it takes 2 hours to train our deep SPN on two 3. [sent-388, score-0.214]
</p><p>91 We evaluate the robustness of deep SPN under different levels of occlusions on a synthetic dataset. [sent-393, score-0.245]
</p><p>92 8 plots the classification accuracies of the deep SPN when the percentage of visible image region is increased from 10% to 80%. [sent-400, score-0.325]
</p><p>93 This is because with our deep model, only a few attributes need to be observed in order to infer the others. [sent-402, score-0.411]
</p><p>94 Classification accuracies of deep SPN under different levels of occlusions. [sent-407, score-0.258]
</p><p>95 We compare our deep SPN with SAC [12] and MAC [2] under different levels of occlusions. [sent-409, score-0.214]
</p><p>96 Conclusion This paper has proposed a facial attribute estimation framework, where attribute estimation is achieved by combining discriminative decision trees with a SPN. [sent-411, score-0.621]
</p><p>97 For robust attribute estimation, we devise a deep architecture to capture attribute correlations for the SPN, where occlusion handling is casted as marginalizing the variables of the undetected regions. [sent-412, score-1.039]
</p><p>98 As face attributes are becoming more important for face recognition, in the future work, we will explore the advantage to combine face attributes with traditional face recognition approaches [23, 24, 3, 26]. [sent-414, score-0.646]
</p><p>99 A discriminative deep model for pedestrian detection with occlusion handling. [sent-520, score-0.255]
</p><p>100 Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. [sent-560, score-0.349]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spn', 0.689), ('attribute', 0.252), ('deep', 0.214), ('attributes', 0.197), ('regional', 0.164), ('ddt', 0.141), ('spt', 0.141), ('correlations', 0.123), ('sac', 0.113), ('mac', 0.11), ('layer', 0.099), ('interdependencies', 0.099), ('node', 0.099), ('unoccluded', 0.094), ('instantiations', 0.092), ('misdetected', 0.086), ('spts', 0.086), ('mpe', 0.085), ('network', 0.081), ('terminals', 0.076), ('smiling', 0.073), ('occluded', 0.071), ('indicators', 0.07), ('rud', 0.069), ('region', 0.067), ('architecture', 0.066), ('marginalizing', 0.063), ('face', 0.063), ('tree', 0.061), ('child', 0.06), ('sum', 0.06), ('nodes', 0.06), ('woman', 0.059), ('connects', 0.058), ('product', 0.053), ('riud', 0.052), ('products', 0.049), ('decision', 0.049), ('layers', 0.048), ('ai', 0.046), ('wv', 0.045), ('join', 0.044), ('accuracies', 0.044), ('group', 0.043), ('undetected', 0.042), ('boltzmann', 0.042), ('discriminative', 0.041), ('classifiers', 0.04), ('groups', 0.039), ('sv', 0.039), ('marginalized', 0.038), ('absence', 0.038), ('facetracer', 0.037), ('eyes', 0.036), ('rectangle', 0.036), ('correlation', 0.036), ('probable', 0.035), ('children', 0.035), ('presences', 0.034), ('sumproduct', 0.034), ('polynomial', 0.034), ('cuhk', 0.034), ('ch', 0.034), ('sums', 0.033), ('functional', 0.033), ('classifier', 0.032), ('occlusions', 0.031), ('autoencoder', 0.031), ('autoencoders', 0.031), ('interdependence', 0.031), ('makeup', 0.031), ('mustache', 0.031), ('separator', 0.031), ('scores', 0.031), ('presence', 0.03), ('faces', 0.03), ('regions', 0.03), ('compactly', 0.029), ('misdetection', 0.028), ('rd', 0.028), ('corruption', 0.028), ('independence', 0.028), ('denoising', 0.027), ('variables', 0.027), ('facial', 0.027), ('explanation', 0.027), ('terminal', 0.027), ('ik', 0.026), ('kong', 0.026), ('rid', 0.025), ('exponentially', 0.025), ('organizing', 0.023), ('pubfig', 0.023), ('detector', 0.023), ('instantiation', 0.023), ('vincent', 0.023), ('subsets', 0.023), ('tang', 0.022), ('chinese', 0.022), ('binary', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="7-tfidf-1" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>Author: Ping Luo, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: Recent works have shown that facial attributes are useful in a number of applications such as face recognition and retrieval. However, estimating attributes in images with large variations remains a big challenge. This challenge is addressed in this paper. Unlike existing methods that assume the independence of attributes during their estimation, our approach captures the interdependencies of local regions for each attribute, as well as the high-order correlations between different attributes, which makes it more robust to occlusions and misdetection of face regions. First, we have modeled region interdependencies with a discriminative decision tree, where each node consists of a detector and a classifier trained on a local region. The detector allows us to locate the region, while the classifier determines the presence or absence of an attribute. Second, correlations of attributes and attribute predictors are modeled by organizing all of the decision trees into a large sum-product network (SPN), which is learned by the EM algorithm and yields the most probable explanation (MPE) of the facial attributes in terms of the region ’s localization and classification. Experimental results on a large data set with 22, 400 images show the effectiveness of the proposed approach.</p><p>2 0.29634741 <a title="7-tfidf-2" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>Author: Xiaoyang Wang, Qiang Ji</p><p>Abstract: This paper proposes a unified probabilistic model to model the relationships between attributes and objects for attribute prediction and object recognition. As a list of semantically meaningful properties of objects, attributes generally relate to each other statistically. In this paper, we propose a unified probabilistic model to automatically discover and capture both the object-dependent and objectindependent attribute relationships. The model utilizes the captured relationships to benefit both attribute prediction and object recognition. Experiments on four benchmark attribute datasets demonstrate the effectiveness of the proposed unified model for improving attribute prediction as well as object recognition in both standard and zero-shot learning cases.</p><p>3 0.2151525 <a title="7-tfidf-3" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>Author: Adriana Kovashka, Kristen Grauman</p><p>Abstract: Current methods learn monolithic attribute predictors, with the assumption that a single model is sufficient to reflect human understanding of a visual attribute. However, in reality, humans vary in how they perceive the association between a named property and image content. For example, two people may have slightly different internal models for what makes a shoe look “formal”, or they may disagree on which of two scenes looks “more cluttered”. Rather than discount these differences as noise, we propose to learn user-specific attribute models. We adapt a generic model trained with annotations from multiple users, tailoring it to satisfy user-specific labels. Furthermore, we propose novel techniques to infer user-specific labels based on transitivity and contradictions in the user’s search history. We demonstrate that adapted attributes improve accuracy over both existing monolithic models as well as models that learn from scratch with user-specific data alone. In addition, we show how adapted attributes are useful to personalize image search, whether with binary or relative attributes.</p><p>4 0.19958851 <a title="7-tfidf-4" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Devi Parikh, Tsuhan Chen</p><p>Abstract: In recent years, there has been a great deal of progress in describing objects with attributes. Attributes have proven useful for object recognition, image search, face verification, image description, and zero-shot learning. Typically, attributes are either binary or relative: they describe either the presence or absence of a descriptive characteristic, or the relative magnitude of the characteristic when comparing two exemplars. However, prior work fails to model the actual way in which humans use these attributes in descriptive statements of images. Specifically, it does not address the important interactions between the binary and relative aspects of an attribute. In this work we propose a spoken attribute classifier which models a more natural way of using an attribute in a description. For each attribute we train a classifier which captures the specific way this attribute should be used. We show that as a result of using this model, we produce descriptions about images of people that are more natural and specific than past systems.</p><p>5 0.18876168 <a title="7-tfidf-5" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>Author: Jungseock Joo, Shuo Wang, Song-Chun Zhu</p><p>Abstract: We present a part-based approach to the problem of human attribute recognition from a single image of a human body. To recognize the attributes of human from the body parts, it is important to reliably detect the parts. This is a challenging task due to the geometric variation such as articulation and view-point changes as well as the appearance variation of the parts arisen from versatile clothing types. The prior works have primarily focused on handling . edu . cn ???????????? geometric variation by relying on pre-trained part detectors or pose estimators, which require manual part annotation, but the appearance variation has been relatively neglected in these works. This paper explores the importance of the appearance variation, which is directly related to the main task, attribute recognition. To this end, we propose to learn a rich appearance part dictionary of human with significantly less supervision by decomposing image lattice into overlapping windows at multiscale and iteratively refining local appearance templates. We also present quantitative results in which our proposed method outperforms the existing approaches.</p><p>6 0.17633474 <a title="7-tfidf-6" href="./iccv-2013-Multi-stage_Contextual_Deep_Learning_for_Pedestrian_Detection.html">279 iccv-2013-Multi-stage Contextual Deep Learning for Pedestrian Detection</a></p>
<p>7 0.17607684 <a title="7-tfidf-7" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>8 0.1586718 <a title="7-tfidf-8" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>9 0.15325181 <a title="7-tfidf-9" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>10 0.12713803 <a title="7-tfidf-10" href="./iccv-2013-Pedestrian_Parsing_via_Deep_Decompositional_Network.html">311 iccv-2013-Pedestrian Parsing via Deep Decompositional Network</a></p>
<p>11 0.12458718 <a title="7-tfidf-11" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>12 0.12442306 <a title="7-tfidf-12" href="./iccv-2013-Learning_Graphs_to_Match.html">238 iccv-2013-Learning Graphs to Match</a></p>
<p>13 0.12193178 <a title="7-tfidf-13" href="./iccv-2013-Joint_Deep_Learning_for_Pedestrian_Detection.html">220 iccv-2013-Joint Deep Learning for Pedestrian Detection</a></p>
<p>14 0.11685447 <a title="7-tfidf-14" href="./iccv-2013-Learning_Graph_Matching%3A_Oriented_to_Category_Modeling_from_Cluttered_Scenes.html">237 iccv-2013-Learning Graph Matching: Oriented to Category Modeling from Cluttered Scenes</a></p>
<p>15 0.11669069 <a title="7-tfidf-15" href="./iccv-2013-Hybrid_Deep_Learning_for_Face_Verification.html">206 iccv-2013-Hybrid Deep Learning for Face Verification</a></p>
<p>16 0.10260317 <a title="7-tfidf-16" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>17 0.10244907 <a title="7-tfidf-17" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<p>18 0.10215209 <a title="7-tfidf-18" href="./iccv-2013-Deep_Learning_Identity-Preserving_Face_Space.html">106 iccv-2013-Deep Learning Identity-Preserving Face Space</a></p>
<p>19 0.097055733 <a title="7-tfidf-19" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>20 0.093226649 <a title="7-tfidf-20" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.171), (1, 0.13), (2, -0.069), (3, -0.16), (4, 0.129), (5, -0.079), (6, -0.031), (7, -0.068), (8, 0.189), (9, 0.079), (10, -0.092), (11, 0.054), (12, 0.073), (13, 0.023), (14, -0.01), (15, 0.057), (16, -0.0), (17, 0.023), (18, 0.023), (19, 0.139), (20, -0.067), (21, 0.042), (22, 0.042), (23, -0.058), (24, -0.148), (25, -0.071), (26, -0.046), (27, 0.006), (28, -0.054), (29, 0.117), (30, -0.019), (31, 0.074), (32, 0.028), (33, -0.022), (34, -0.019), (35, 0.017), (36, -0.005), (37, 0.034), (38, 0.037), (39, -0.02), (40, -0.04), (41, -0.02), (42, 0.026), (43, 0.016), (44, -0.033), (45, 0.086), (46, 0.035), (47, 0.027), (48, -0.003), (49, -0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9550153 <a title="7-lsi-1" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>Author: Ping Luo, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: Recent works have shown that facial attributes are useful in a number of applications such as face recognition and retrieval. However, estimating attributes in images with large variations remains a big challenge. This challenge is addressed in this paper. Unlike existing methods that assume the independence of attributes during their estimation, our approach captures the interdependencies of local regions for each attribute, as well as the high-order correlations between different attributes, which makes it more robust to occlusions and misdetection of face regions. First, we have modeled region interdependencies with a discriminative decision tree, where each node consists of a detector and a classifier trained on a local region. The detector allows us to locate the region, while the classifier determines the presence or absence of an attribute. Second, correlations of attributes and attribute predictors are modeled by organizing all of the decision trees into a large sum-product network (SPN), which is learned by the EM algorithm and yields the most probable explanation (MPE) of the facial attributes in terms of the region ’s localization and classification. Experimental results on a large data set with 22, 400 images show the effectiveness of the proposed approach.</p><p>2 0.81344694 <a title="7-lsi-2" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>Author: Xiaoyang Wang, Qiang Ji</p><p>Abstract: This paper proposes a unified probabilistic model to model the relationships between attributes and objects for attribute prediction and object recognition. As a list of semantically meaningful properties of objects, attributes generally relate to each other statistically. In this paper, we propose a unified probabilistic model to automatically discover and capture both the object-dependent and objectindependent attribute relationships. The model utilizes the captured relationships to benefit both attribute prediction and object recognition. Experiments on four benchmark attribute datasets demonstrate the effectiveness of the proposed unified model for improving attribute prediction as well as object recognition in both standard and zero-shot learning cases.</p><p>3 0.77483654 <a title="7-lsi-3" href="./iccv-2013-Spoken_Attributes%3A_Mixing_Binary_and_Relative_Attributes_to_Say_the_Right_Thing.html">399 iccv-2013-Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Devi Parikh, Tsuhan Chen</p><p>Abstract: In recent years, there has been a great deal of progress in describing objects with attributes. Attributes have proven useful for object recognition, image search, face verification, image description, and zero-shot learning. Typically, attributes are either binary or relative: they describe either the presence or absence of a descriptive characteristic, or the relative magnitude of the characteristic when comparing two exemplars. However, prior work fails to model the actual way in which humans use these attributes in descriptive statements of images. Specifically, it does not address the important interactions between the binary and relative aspects of an attribute. In this work we propose a spoken attribute classifier which models a more natural way of using an attribute in a description. For each attribute we train a classifier which captures the specific way this attribute should be used. We show that as a result of using this model, we produce descriptions about images of people that are more natural and specific than past systems.</p><p>4 0.74209273 <a title="7-lsi-4" href="./iccv-2013-Attribute_Dominance%3A_What_Pops_Out%3F.html">53 iccv-2013-Attribute Dominance: What Pops Out?</a></p>
<p>Author: Naman Turakhia, Devi Parikh</p><p>Abstract: When we look at an image, some properties or attributes of the image stand out more than others. When describing an image, people are likely to describe these dominant attributes first. Attribute dominance is a result of a complex interplay between the various properties present or absent in the image. Which attributes in an image are more dominant than others reveals rich information about the content of the image. In this paper we tap into this information by modeling attribute dominance. We show that this helps improve the performance of vision systems on a variety of human-centric applications such as zero-shot learning, image search and generating textual descriptions of images.</p><p>5 0.68496865 <a title="7-lsi-5" href="./iccv-2013-Semantic_Transform%3A_Weakly_Supervised_Semantic_Inference_for_Relating_Visual_Attributes.html">380 iccv-2013-Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes</a></p>
<p>Author: Sukrit Shankar, Joan Lasenby, Roberto Cipolla</p><p>Abstract: Relative (comparative) attributes are promising for thematic ranking of visual entities, which also aids in recognition tasks [19, 23]. However, attribute rank learning often requires a substantial amount of relational supervision, which is highly tedious, and apparently impracticalfor realworld applications. In this paper, we introduce the Semantic Transform, which under minimal supervision, adaptively finds a semantic feature space along with a class ordering that is related in the best possible way. Such a semantic space is found for every attribute category. To relate the classes under weak supervision, the class ordering needs to be refined according to a cost function in an iterative procedure. This problem is ideally NP-hard, and we thus propose a constrained search tree formulation for the same. Driven by the adaptive semantic feature space representation, our model achieves the best results to date for all of the tasks of relative, absolute and zero-shot classification on two popular datasets.</p><p>6 0.67091376 <a title="7-lsi-6" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>7 0.66460079 <a title="7-lsi-7" href="./iccv-2013-What_Do_You_Do%3F_Occupation_Recognition_in_a_Photo_via_Social_Context.html">449 iccv-2013-What Do You Do? Occupation Recognition in a Photo via Social Context</a></p>
<p>8 0.58248502 <a title="7-lsi-8" href="./iccv-2013-Pedestrian_Parsing_via_Deep_Decompositional_Network.html">311 iccv-2013-Pedestrian Parsing via Deep Decompositional Network</a></p>
<p>9 0.55301452 <a title="7-lsi-9" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>10 0.52342194 <a title="7-lsi-10" href="./iccv-2013-Multi-stage_Contextual_Deep_Learning_for_Pedestrian_Detection.html">279 iccv-2013-Multi-stage Contextual Deep Learning for Pedestrian Detection</a></p>
<p>11 0.52111393 <a title="7-lsi-11" href="./iccv-2013-Hybrid_Deep_Learning_for_Face_Verification.html">206 iccv-2013-Hybrid Deep Learning for Face Verification</a></p>
<p>12 0.51515734 <a title="7-lsi-12" href="./iccv-2013-Joint_Deep_Learning_for_Pedestrian_Detection.html">220 iccv-2013-Joint Deep Learning for Pedestrian Detection</a></p>
<p>13 0.5039975 <a title="7-lsi-13" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>14 0.49888018 <a title="7-lsi-14" href="./iccv-2013-Relative_Attributes_for_Large-Scale_Abandoned_Object_Detection.html">350 iccv-2013-Relative Attributes for Large-Scale Abandoned Object Detection</a></p>
<p>15 0.48286763 <a title="7-lsi-15" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>16 0.47826728 <a title="7-lsi-16" href="./iccv-2013-Handwritten_Word_Spotting_with_Corrected_Attributes.html">192 iccv-2013-Handwritten Word Spotting with Corrected Attributes</a></p>
<p>17 0.45256838 <a title="7-lsi-17" href="./iccv-2013-NEIL%3A_Extracting_Visual_Knowledge_from_Web_Data.html">285 iccv-2013-NEIL: Extracting Visual Knowledge from Web Data</a></p>
<p>18 0.43649006 <a title="7-lsi-18" href="./iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain.html">351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</a></p>
<p>19 0.43619463 <a title="7-lsi-19" href="./iccv-2013-Image_Segmentation_with_Cascaded_Hierarchical_Models_and_Logistic_Disjunctive_Normal_Networks.html">211 iccv-2013-Image Segmentation with Cascaded Hierarchical Models and Logistic Disjunctive Normal Networks</a></p>
<p>20 0.42580888 <a title="7-lsi-20" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.073), (7, 0.016), (26, 0.063), (31, 0.039), (33, 0.24), (34, 0.041), (40, 0.012), (42, 0.093), (48, 0.067), (64, 0.064), (73, 0.036), (89, 0.138)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77444816 <a title="7-lda-1" href="./iccv-2013-A_Deep_Sum-Product_Architecture_for_Robust_Facial_Attributes_Analysis.html">7 iccv-2013-A Deep Sum-Product Architecture for Robust Facial Attributes Analysis</a></p>
<p>Author: Ping Luo, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: Recent works have shown that facial attributes are useful in a number of applications such as face recognition and retrieval. However, estimating attributes in images with large variations remains a big challenge. This challenge is addressed in this paper. Unlike existing methods that assume the independence of attributes during their estimation, our approach captures the interdependencies of local regions for each attribute, as well as the high-order correlations between different attributes, which makes it more robust to occlusions and misdetection of face regions. First, we have modeled region interdependencies with a discriminative decision tree, where each node consists of a detector and a classifier trained on a local region. The detector allows us to locate the region, while the classifier determines the presence or absence of an attribute. Second, correlations of attributes and attribute predictors are modeled by organizing all of the decision trees into a large sum-product network (SPN), which is learned by the EM algorithm and yields the most probable explanation (MPE) of the facial attributes in terms of the region ’s localization and classification. Experimental results on a large data set with 22, 400 images show the effectiveness of the proposed approach.</p><p>2 0.70283031 <a title="7-lda-2" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>Author: Gang Hua, Chengjiang Long, Ming Yang, Yan Gao</p><p>Abstract: Active learning is an effective way of engaging users to interactively train models for visual recognition. The vast majority of previous works, if not all of them, focused on active learning with a single human oracle. The problem of active learning with multiple oracles in a collaborative setting has not been well explored. Moreover, most of the previous works assume that the labels provided by the human oracles are noise free, which may often be violated in reality. We present a collaborative computational model for active learning with multiple human oracles. It leads to not only an ensemble kernel machine that is robust to label noises, but also a principled label quality measure to online detect irresponsible labelers. Instead of running independent active learning processes for each individual human oracle, our model captures the inherent correlations among the labelers through shared data among them. Our simulation experiments and experiments with real crowd-sourced noisy labels demonstrated the efficacy of our model.</p><p>3 0.66184735 <a title="7-lda-3" href="./iccv-2013-A_Convex_Optimization_Framework_for_Active_Learning.html">6 iccv-2013-A Convex Optimization Framework for Active Learning</a></p>
<p>Author: Ehsan Elhamifar, Guillermo Sapiro, Allen Yang, S. Shankar Sasrty</p><p>Abstract: In many image/video/web classification problems, we have access to a large number of unlabeled samples. However, it is typically expensive and time consuming to obtain labels for the samples. Active learning is the problem of progressively selecting and annotating the most informative unlabeled samples, in order to obtain a high classification performance. Most existing active learning algorithms select only one sample at a time prior to retraining the classifier. Hence, they are computationally expensive and cannot take advantage of parallel labeling systems such as Mechanical Turk. On the other hand, algorithms that allow the selection of multiple samples prior to retraining the classifier, may select samples that have significant information overlap or they involve solving a non-convex optimization. More importantly, the majority of active learning algorithms are developed for a certain classifier type such as SVM. In this paper, we develop an efficient active learning framework based on convex programming, which can select multiple samples at a time for annotation. Unlike the state of the art, our algorithm can be used in conjunction with any type of classifiers, including those of the fam- ily of the recently proposed Sparse Representation-based Classification (SRC). We use the two principles of classifier uncertainty and sample diversity in order to guide the optimization program towards selecting the most informative unlabeled samples, which have the least information overlap. Our method can incorporate the data distribution in the selection process by using the appropriate dissimilarity between pairs of samples. We show the effectiveness of our framework in person detection, scene categorization and face recognition on real-world datasets.</p><p>4 0.65626502 <a title="7-lda-4" href="./iccv-2013-Pose-Configurable_Generic_Tracking_of_Elongated_Objects.html">320 iccv-2013-Pose-Configurable Generic Tracking of Elongated Objects</a></p>
<p>Author: Daniel Wesierski, Patrick Horain</p><p>Abstract: Elongated objects have various shapes and can shift, rotate, change scale, and be rigid or deform by flexing, articulating, and vibrating, with examples as varied as a glass bottle, a robotic arm, a surgical suture, a finger pair, a tram, and a guitar string. This generally makes tracking of poses of elongated objects very challenging. We describe a unified, configurable framework for tracking the pose of elongated objects, which move in the image plane and extend over the image region. Our method strives for simplicity, versatility, and efficiency. The object is decomposed into a chained assembly of segments of multiple parts that are arranged under a hierarchy of tailored spatio-temporal constraints. In this hierarchy, segments can rescale independently while their elasticity is controlled with global orientations and local distances. While the trend in tracking is to design complex, structure-free algorithms that update object appearance on- line, we show that our tracker, with the novel but remarkably simple, structured organization of parts with constant appearance, reaches or improves state-of-the-art performance. Most importantly, our model can be easily configured to track exact pose of arbitrary, elongated objects in the image plane. The tracker can run up to 100 fps on a desktop PC, yet the computation time scales linearly with the number of object parts. To our knowledge, this is the first approach to generic tracking of elongated objects.</p><p>5 0.6531207 <a title="7-lda-5" href="./iccv-2013-Bounded_Labeling_Function_for_Global_Segmentation_of_Multi-part_Objects_with_Geometric_Constraints.html">63 iccv-2013-Bounded Labeling Function for Global Segmentation of Multi-part Objects with Geometric Constraints</a></p>
<p>Author: Masoud S. Nosrati, Shawn Andrews, Ghassan Hamarneh</p><p>Abstract: The inclusion of shape and appearance priors have proven useful for obtaining more accurate and plausible segmentations, especially for complex objects with multiple parts. In this paper, we augment the popular MumfordShah model to incorporate two important geometrical constraints, termed containment and detachment, between different regions with a specified minimum distance between their boundaries. Our method is able to handle multiple instances of multi-part objects defined by these geometrical hamarneh} @ s fu . ca (a)Standar laΩb ehlingΩfuhnctionseting(Ωb)hΩOuirseΩtijng Figure 1: The inside vs. outside ambiguity in (a) is resolved by our containment constraint in (b). constraints using a single labeling function while maintaining global optimality. We demonstrate the utility and advantages of these two constraints and show that the proposed convex continuous method is superior to other state-of-theart methods, including its discrete counterpart, in terms of memory usage, and metrication errors.</p><p>6 0.65226859 <a title="7-lda-6" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>7 0.64864397 <a title="7-lda-7" href="./iccv-2013-Pedestrian_Parsing_via_Deep_Decompositional_Network.html">311 iccv-2013-Pedestrian Parsing via Deep Decompositional Network</a></p>
<p>8 0.63829631 <a title="7-lda-8" href="./iccv-2013-Illuminant_Chromaticity_from_Image_Sequences.html">207 iccv-2013-Illuminant Chromaticity from Image Sequences</a></p>
<p>9 0.63376689 <a title="7-lda-9" href="./iccv-2013-A_Unified_Probabilistic_Approach_Modeling_Relationships_between_Attributes_and_Objects.html">31 iccv-2013-A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects</a></p>
<p>10 0.63342291 <a title="7-lda-10" href="./iccv-2013-Joint_Deep_Learning_for_Pedestrian_Detection.html">220 iccv-2013-Joint Deep Learning for Pedestrian Detection</a></p>
<p>11 0.63156962 <a title="7-lda-11" href="./iccv-2013-Pyramid_Coding_for_Functional_Scene_Element_Recognition_in_Video_Scenes.html">331 iccv-2013-Pyramid Coding for Functional Scene Element Recognition in Video Scenes</a></p>
<p>12 0.63141656 <a title="7-lda-12" href="./iccv-2013-Latent_Data_Association%3A_Bayesian_Model_Selection_for_Multi-target_Tracking.html">230 iccv-2013-Latent Data Association: Bayesian Model Selection for Multi-target Tracking</a></p>
<p>13 0.62946093 <a title="7-lda-13" href="./iccv-2013-Hybrid_Deep_Learning_for_Face_Verification.html">206 iccv-2013-Hybrid Deep Learning for Face Verification</a></p>
<p>14 0.62918603 <a title="7-lda-14" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>15 0.62877512 <a title="7-lda-15" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>16 0.62843859 <a title="7-lda-16" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>17 0.62731051 <a title="7-lda-17" href="./iccv-2013-Multi-stage_Contextual_Deep_Learning_for_Pedestrian_Detection.html">279 iccv-2013-Multi-stage Contextual Deep Learning for Pedestrian Detection</a></p>
<p>18 0.62717992 <a title="7-lda-18" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>19 0.62605649 <a title="7-lda-19" href="./iccv-2013-Hierarchical_Data-Driven_Descent_for_Efficient_Optimal_Deformation_Estimation.html">196 iccv-2013-Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation</a></p>
<p>20 0.62562901 <a title="7-lda-20" href="./iccv-2013-Image_Co-segmentation_via_Consistent_Functional_Maps.html">208 iccv-2013-Image Co-segmentation via Consistent Functional Maps</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
