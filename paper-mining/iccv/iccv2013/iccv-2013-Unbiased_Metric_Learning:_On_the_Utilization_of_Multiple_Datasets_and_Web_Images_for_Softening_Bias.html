<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>431 iccv-2013-Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-431" href="#">iccv2013-431</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>431 iccv-2013-Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias</h1>
<br/><p>Source: <a title="iccv-2013-431-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Fang_Unbiased_Metric_Learning_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Chen Fang, Ye Xu, Daniel N. Rockmore</p><p>Abstract: Many standard computer vision datasets exhibit biases due to a variety of sources including illumination condition, imaging system, and preference of dataset collectors. Biases like these can have downstream effects in the use of vision datasets in the construction of generalizable techniques, especially for the goal of the creation of a classification system capable of generalizing to unseen and novel datasets. In this work we propose Unbiased Metric Learning (UML), a metric learning approach, to achieve this goal. UML operates in the following two steps: (1) By varying hyperparameters, it learns a set of less biased candidate distance metrics on training examples from multiple biased datasets. The key idea is to learn a neighborhood for each example, which consists of not only examples of the same category from the same dataset, but those from other datasets. The learning framework is based on structural SVM. (2) We do model validation on a set of weakly-labeled web images retrieved by issuing class labels as keywords to search engine. The metric with best validationperformance is selected. Although the web images sometimes have noisy labels, they often tend to be less biased, which makes them suitable for the validation set in our task. Cross-dataset image classification experiments are carried out. Results show significant performance improvement on four well-known computer vision datasets.</p><p>Reference: <a title="iccv-2013-431-reference" href="../iccv2013_reference/iccv-2013-Unbiased_Metric_Learning%3A_On_the_Utilization_of_Multiple_Datasets_and_Web_Images_for_Softening_Bias_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Biases like these can have downstream effects in the use of vision datasets in the construction of generalizable techniques, especially for the goal of the creation of a classification system capable of generalizing to unseen and novel datasets. [sent-8, score-0.325]
</p><p>2 In this work we propose Unbiased Metric Learning (UML), a metric learning approach, to achieve this goal. [sent-9, score-0.217]
</p><p>3 UML operates in the following two steps: (1) By varying hyperparameters, it learns a set of less biased candidate distance metrics on training examples from multiple biased datasets. [sent-10, score-0.522]
</p><p>4 The key idea is to learn a neighborhood for each example, which consists of not only examples of the same category from the same dataset, but those from other datasets. [sent-11, score-0.267]
</p><p>5 (2) We do model validation on a set of weakly-labeled web images retrieved by issuing class labels as keywords to search engine. [sent-13, score-0.503]
</p><p>6 Although the web images sometimes have noisy labels, they often tend to be less biased, which makes them suitable for the validation set in our task. [sent-15, score-0.36]
</p><p>7 Bias inevitably leads learning algorithms to overfit for the training set to the detriment of any ability to generalize to other datasets. [sent-24, score-0.181]
</p><p>8 In other words, an object recognition system trained solely on one dataset tends to perform poorly on unseen and novel datasets at test time, because the underlying bias is incorporated into the learning algorithm. [sent-25, score-0.436]
</p><p>9 This is an important issue, because most image classification systems are required to handle all kinds of test examples, regardless of where the  test examples are drawn from. [sent-26, score-0.2]
</p><p>10 To this end we introduce Unbiased Metric Learning (UML), which learns an unbiased metric using multiple biased datasets and web images. [sent-36, score-0.561]
</p><p>11 Our approach operates in the following two steps: Step 1 we learn a set of distance metrics on training examples from multiple biased datasets. [sent-37, score-0.384]
</p><p>12 The key idea is that in the learned feature spaces, the neighborhoods of training examples should consist of not only examples of the same category from the same dataset, but those examples of the same category from other datasets. [sent-38, score-0.541]
</p><p>13 We form a candidate set of these metrics for Step 2 (below), in which a novel validation is performed. [sent-43, score-0.351]
</p><p>14 Technically, we cast it as a learning to rank problem [11] [27], and solve it with structural metric learning [15]. [sent-44, score-0.334]
</p><p>15 Step 2 we do model validation to identify the metric with best generalization ability. [sent-45, score-0.451]
</p><p>16 Conventionally, the validation set is from the same source as the training set, for example, cross-validation. [sent-46, score-0.342]
</p><p>17 However, in our case, a good validation performance on seen datasets does not necessarily generalize to unseen datasets. [sent-47, score-0.506]
</p><p>18 Therefore conventional validation may fail to uncover the desired model. [sent-48, score-0.282]
</p><p>19 So, instead of using images from seen datasets, we propose to use a set of weakly-labeled web images retrieved from the Internet by issuing class labels as keywords to the search engine. [sent-49, score-0.248]
</p><p>20 Those images are less biased and have higher intra-class variability than human collected datasets, which makes it suitable to be used as our validation set. [sent-50, score-0.368]
</p><p>21 We do image classification experiments, and use crossdataset performance to measure a model’s ability to generalize to unseen datasets. [sent-51, score-0.326]
</p><p>22 (2) In the validation step, the model selected by our novel validation method significantly outperforms those selected by conventional validation procedures. [sent-53, score-0.844]
</p><p>23 Related Work The issue of dataset bias in vision datasets was first raised by Torralba and Efros [23]. [sent-57, score-0.199]
</p><p>24 The approach in [12] learns a common weight vector which is expected to work well on unseen datasets, in a way that is similar to regularized multi-task learning [5]. [sent-60, score-0.231]
</p><p>25 Our method adopts a metric learning solution to this problem. [sent-61, score-0.217]
</p><p>26 Most of them can be categorized as either learning a “global” metric or a “local” metric. [sent-63, score-0.217]
</p><p>27 In [9]  a metric is learned by collapsing all examples in a class to a single point. [sent-67, score-0.411]
</p><p>28 Because our approach utilizes domain information in training data, it is related to domain adaptation and multitask learning. [sent-70, score-0.229]
</p><p>29 In domain adaptation, the goal is to transfer knowledge from the source domain to help perform a task in the target domain, and multi-task learning aims at good performance simultaneously in multiple domains. [sent-71, score-0.197]
</p><p>30 [21] learns a metric to transfer knowledge to the target domain by randomly sampling pairs consisting of a labeled example from the source domain and another from the target domain, and constraining their distance to be no greater (less) than a bound if the labels are the same (different). [sent-73, score-0.399]
</p><p>31 In [17], the metric is decomposed into a domain specific part and a global part, which is shared among all domains. [sent-74, score-0.246]
</p><p>32 [22] proposes to learn general knowledge from multiple datasets, which will be transferred to a target test dataset later with the help of a few labeled examples from the target dataset. [sent-76, score-0.234]
</p><p>33 This means that incoming test examples may come from any domains, although most are unseen at train-  ing time. [sent-79, score-0.274]
</p><p>34 Therefore, the goal is to learn a transformed new space, where all the training examples are less biased. [sent-80, score-0.175]
</p><p>35 First, we consider each training example as a query and rank in ascending order the other training examples based on their L2 distances to the query. [sent-84, score-0.349]
</p><p>36 Then, beyond the neighborhood label coherence, we look at the domain information of the positive examples within the neighborhood. [sent-87, score-0.295]
</p><p>37 ” From a ranking perspective, this is inserting into the top-k positions, those positive examples from domains other than the current query’s. [sent-89, score-0.267]
</p><p>38 The ranking on the right, unlike the one on the left, exhibits both label coherence and neighborhood diversity. [sent-100, score-0.285]
</p><p>39 As the top right figure illustrates, examples of the same category from different datasets are linked together via diversified neighborhood, which is constructed automatically in the learning procedure by discovering and grouping appropriate pairs of positive examples from different datasets. [sent-102, score-0.363]
</p><p>40 We let the hyperparameters vary, so as to produce a set of metrics with different levels of “neighborhood diversity” which later will be screened by validation procedure. [sent-104, score-0.443]
</p><p>41 As for model validation, we use weakly-labeled web images as the validation set. [sent-105, score-0.36]
</p><p>42 However, studies have shown that they serve well as a relatively noisy validation set [8]. [sent-108, score-0.255]
</p><p>43 Our experiments support the claim that web images serve better as a validation set for our problem. [sent-110, score-0.36]
</p><p>44 Technical Details In this section, we will discuss the formulation of the structural metric learning (SML) framework as well as Unbiased Metric Learning (UML). [sent-112, score-0.283]
</p><p>45 Each training example i ∈ X is a triplet (xi , li, di), where xi ∈ Rd is the feature vector of item i, li ∈ C is the corresponding class label and di ∈ D indicates the dataset that iis from. [sent-124, score-0.264]
</p><p>46 We now review the structural metric learning (SML) framework [15]. [sent-144, score-0.283]
</p><p>47 The goal is to learn a positive semidefinite matrix W, so that when a query q is issued, the corresponding ranking or ordering yq, which is produced based on the Mahalanobis distance defined by W, will have some desiring properties, such as high Precision@k, Mean Average Precision or ROC area. [sent-145, score-0.267]
</p><p>48 2 includes the slack variable ξq and the structural loss function Δ(y, yq∗), which encodes the structural information in Y. [sent-157, score-0.223]
</p><p>49 First, note that the neighborhood label coherence is already forced by Eq. [sent-182, score-0.183]
</p><p>50 2, if we set the loss function Δ to be the following form: Δ(y, yq∗) = 1  −  Prec@k(y)  (5)  where Prec@k(y) gives the percentage of positive examples among the top-k positions of ranking y, which is equivalent to the notion of label coherence in a neighborhood. [sent-183, score-0.277]
</p><p>51 We let y+q∗ be the ground truth ranking in Yq+ with the following property:  y+  ∀i,j  ∈  Xq+ : i≺y+q∗ j, if (di  = dq ∧ dj = dq)  (7)  where di indicates the dataset containing example i. [sent-189, score-0.308]
</p><p>52 7 means the all the positive examples not in dq precede those in dq. [sent-191, score-0.249]
</p><p>53 6 encourages any ranking predicted by W to have a neighborhood as diverse as the ground truth. [sent-198, score-0.226]
</p><p>54 At a higher level, η is a hyperparameter that we should vary and one that depends on the intrinsic property of training data. [sent-218, score-0.177]
</p><p>55 For each category in each dataset, we randomly selected up to 100 images, so that we could vary the number of training examples per category per dataset. [sent-263, score-0.449]
</p><p>56 In crossdataset classification experiments one dataset will be held out as unseen dataset. [sent-264, score-0.352]
</p><p>57 In practice, only Trseen was used Validation set This validation set contains images from the four datasets. [sent-266, score-0.255]
</p><p>58 The 20 images per category per dataset are randomly selected. [sent-267, score-0.236]
</p><p>59 Similar to the training set, in crossdataset experiments one dataset will be held out, so we have two subsets as Vaunseen and Vaseen. [sent-268, score-0.234]
</p><p>60 Web validation set In order to carry out our novel validation method, we constructed another validation set with web images. [sent-269, score-0.87]
</p><p>61 After removing duplicates with regard to the training set, 20 images were randomly selected for each class to form the set. [sent-271, score-0.184]
</p><p>62 Test set There are 20 images per class per dataset. [sent-273, score-0.209]
</p><p>63 Finally, we compare our approach with other metric learning methods. [sent-281, score-0.217]
</p><p>64 Existence of bias In this first experiment we show that our four datasets are strongly biased, in the sense that a model trained on one dataset is ineffective (due to bias) on the other datasets. [sent-286, score-0.199]
</p><p>65 We do this by using SML to learn a metric on each dataset individually and then test it on each dataset individually. [sent-287, score-0.283]
</p><p>66 We used 20 images per category per dataset to form the training set, as well as the validation and test sets. [sent-289, score-0.609]
</p><p>67 The left-most column specifies the training dataset where the metric is learned, while the uppermost row specifies the test dataset. [sent-302, score-0.327]
</p><p>68 Finally, we will do full evaluation and compare UML to other metric learning methods. [sent-312, score-0.217]
</p><p>69 Recall that by varying hyperparameters we will learn a set of metrics with different levels of “neighborhood label coherence” and “neighborhood diversity”. [sent-316, score-0.188]
</p><p>70 These metrics will be validated by the validation procedure. [sent-317, score-0.351]
</p><p>71 However, if the validation is carried out improperly (e. [sent-318, score-0.284]
</p><p>72 , using a validation set that is not related to the target task) then a bad metric can be selected. [sent-320, score-0.457]
</p><p>73 For example, in our case, it would be bad to use Vaseen, since the goal is to generalize to every possible unseen datasets/domains, not only the three seen datasets. [sent-321, score-0.198]
</p><p>74 Clearly, in this stage, we need a validation set that is closely related to the target task, which is, in the cross-dataset setting, generalizing to the unseen dataset. [sent-323, score-0.471]
</p><p>75 The choice is obvious: using Vaunseen as the validation set at this stage. [sent-324, score-0.255]
</p><p>76 In this way, the effect of the validation proce-  dure on the final performance is minimized, thus resulting in a better evaluation of UML’s learning step. [sent-325, score-0.306]
</p><p>77 There were 20 images per class per dataset in Vaunseen and we let the number of training examples per class per dataset in Trseen to be the following values {15, 20, 30, 50, 70, 100}. [sent-327, score-0.679]
</p><p>78 The validation set, as explained above, was Vaunseen, which was of the same size as Teunseen. [sent-328, score-0.255]
</p><p>79 We let each dataset be unseen once, and report in Fig. [sent-334, score-0.198]
</p><p>80 , 15) training examples are used, but benefits more from more training examples. [sent-343, score-0.262]
</p><p>81 This is due to the fact that newly added examples can bring in more bias, which will then further bias the derived learning algorithm. [sent-346, score-0.242]
</p><p>82 2  Validation stage  The goal of the second stage is to evaluate how different validation sets affect the final performance. [sent-349, score-0.333]
</p><p>83 Two validation sets were used, Vaweb and the corresponding Vaseen in individual experiments. [sent-350, score-0.289]
</p><p>84 The set of metrics produced by UML in the training stage was the input, thus with the same input, our expectation was that our validation procedure with Vaweb would consistently outperform the other. [sent-351, score-0.519]
</p><p>85 3, in which metrics selected by Vaweb not only outperform metrics selected by Vaseen, but almost matches the ones selected by Vaunseen. [sent-353, score-0.27]
</p><p>86 An additional question we ask is whether web images can help conventional metric learning method to generalize. [sent-354, score-0.349]
</p><p>87 To answer it, we applied Vaseen and Vaweb to metrics produced by SML, and found that metrics selected by Vaweb outperform those of Vaseen. [sent-355, score-0.26]
</p><p>88 In mtLMNN, a task was to classify on one of seen training sets, and the learned shared metric, which encodes the knowledge shared among different tasks, was applied to unseen test set. [sent-360, score-0.352]
</p><p>89 3 8462SUML+Trse n+Vaunse n+Teunse n 0  20  40  60  # of training examples  Caltech 101  80  100  per class  PASCAL VOC  rcyAacu0 . [sent-364, score-0.315]
</p><p>90 423 8602 406 801 # of training examples per class  # of training examples per class  SUN09  LabelMe  # of training examples per class  # of training examples per class  Figure 2. [sent-374, score-1.26]
</p><p>91 Cross-dataset classification accuracy on Teunseen with validation on Vaunseen. [sent-375, score-0.305]
</p><p>92 Discussion The goal of this paper is to learn a less biased distance metric that generalizes better to unseen datasets. [sent-382, score-0.434]
</p><p>93 We then proposed to use web images as a validation set to select metrics with better generalizability. [sent-384, score-0.456]
</p><p>94 We demonstrated that by tuning the distribution of data from differAverage Results  # of training examples per class Caltech 101  PASCAL VOC  cruAya0 . [sent-387, score-0.315]
</p><p>95 35 4 02 406 801 SUN09  LabelMe  # of training examples per class  # of training examples per class  Figure 3. [sent-394, score-0.63]
</p><p>96 The green curve is our validation method using Vaweb and the blue one is conventional validation using Vaseen. [sent-399, score-0.537]
</p><p>97 We also showed the advantage of using weakly-labeled web images as validation set to select model with better generalization ability. [sent-403, score-0.39]
</p><p>98 11666633  Average Results  # of training examples per class Caltech 101  PASCAL VOC  Acayruc 0 0 . [sent-409, score-0.315]
</p><p>99 52 4502 406 801 # of training examples per class  # of training examples per class  SUN09  LabelMe  # of training examples per class  # of training examples per class  Figure 4. [sent-418, score-1.26]
</p><p>100 Distance metric learning for large margin nearest neighbor classification. [sent-594, score-0.26]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('uml', 0.517), ('yq', 0.297), ('validation', 0.255), ('xq', 0.182), ('metric', 0.166), ('sml', 0.155), ('vaweb', 0.155), ('unseen', 0.155), ('dq', 0.133), ('vaunseen', 0.129), ('neighborhood', 0.124), ('po', 0.122), ('biased', 0.113), ('web', 0.105), ('teunseen', 0.103), ('vaseen', 0.103), ('bias', 0.103), ('ranking', 0.102), ('unbiased', 0.099), ('diversity', 0.096), ('metrics', 0.096), ('labelme', 0.092), ('hyperparameters', 0.092), ('slack', 0.091), ('examples', 0.088), ('training', 0.087), ('si', 0.079), ('crossdataset', 0.078), ('trseen', 0.078), ('class', 0.071), ('divs', 0.069), ('per', 0.069), ('structural', 0.066), ('voc', 0.063), ('fang', 0.06), ('coherence', 0.059), ('hyperparameter', 0.057), ('collapsing', 0.057), ('domain', 0.055), ('query', 0.055), ('category', 0.055), ('datasets', 0.053), ('argmaxy', 0.052), ('mtlmnn', 0.052), ('prec', 0.052), ('rockmore', 0.052), ('neighborhoods', 0.051), ('learning', 0.051), ('classification', 0.05), ('caltech', 0.05), ('domains', 0.049), ('pascal', 0.048), ('dataset', 0.043), ('generalize', 0.043), ('margin', 0.043), ('microarray', 0.042), ('generalizable', 0.042), ('tommasi', 0.042), ('issuing', 0.042), ('produced', 0.042), ('items', 0.041), ('torralba', 0.04), ('semidefinite', 0.04), ('mcfee', 0.04), ('stage', 0.039), ('issued', 0.038), ('target', 0.036), ('generalizability', 0.035), ('khosla', 0.035), ('violation', 0.034), ('individual', 0.034), ('xi', 0.033), ('psd', 0.033), ('schematic', 0.033), ('property', 0.033), ('multitask', 0.032), ('ascending', 0.032), ('keyword', 0.031), ('ping', 0.031), ('test', 0.031), ('yij', 0.031), ('di', 0.03), ('keywords', 0.03), ('generalization', 0.03), ('efros', 0.03), ('weinberger', 0.029), ('learned', 0.029), ('carried', 0.029), ('positive', 0.028), ('rd', 0.028), ('preference', 0.028), ('mahalanobis', 0.028), ('conventional', 0.027), ('biases', 0.027), ('selected', 0.026), ('greater', 0.026), ('held', 0.026), ('learns', 0.025), ('shared', 0.025), ('generalizing', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="431-tfidf-1" href="./iccv-2013-Unbiased_Metric_Learning%3A_On_the_Utilization_of_Multiple_Datasets_and_Web_Images_for_Softening_Bias.html">431 iccv-2013-Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias</a></p>
<p>Author: Chen Fang, Ye Xu, Daniel N. Rockmore</p><p>Abstract: Many standard computer vision datasets exhibit biases due to a variety of sources including illumination condition, imaging system, and preference of dataset collectors. Biases like these can have downstream effects in the use of vision datasets in the construction of generalizable techniques, especially for the goal of the creation of a classification system capable of generalizing to unseen and novel datasets. In this work we propose Unbiased Metric Learning (UML), a metric learning approach, to achieve this goal. UML operates in the following two steps: (1) By varying hyperparameters, it learns a set of less biased candidate distance metrics on training examples from multiple biased datasets. The key idea is to learn a neighborhood for each example, which consists of not only examples of the same category from the same dataset, but those from other datasets. The learning framework is based on structural SVM. (2) We do model validation on a set of weakly-labeled web images retrieved by issuing class labels as keywords to search engine. The metric with best validationperformance is selected. Although the web images sometimes have noisy labels, they often tend to be less biased, which makes them suitable for the validation set in our task. Cross-dataset image classification experiments are carried out. Results show significant performance improvement on four well-known computer vision datasets.</p><p>2 0.11598996 <a title="431-tfidf-2" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>Author: Fatemeh Mirrashed, Mohammad Rastegari</p><p>Abstract: We propose an unsupervised domain adaptation method that exploits intrinsic compact structures of categories across different domains using binary attributes. Our method directly optimizes for classification in the target domain. The key insight is finding attributes that are discriminative across categories and predictable across domains. We achieve a performance that significantly exceeds the state-of-the-art results on standard benchmarks. In fact, in many cases, our method reaches the same-domain performance, the upper bound, in unsupervised domain adaptation scenarios.</p><p>3 0.10355724 <a title="431-tfidf-3" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>Author: Mohamed Elhoseiny, Babak Saleh, Ahmed Elgammal</p><p>Abstract: The main question we address in this paper is how to use purely textual description of categories with no training images to learn visual classifiers for these categories. We propose an approach for zero-shot learning of object categories where the description of unseen categories comes in the form of typical text such as an encyclopedia entry, without the need to explicitly defined attributes. We propose and investigate two baseline formulations, based on regression and domain adaptation. Then, we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to predict the classifier parameters for new classes. We applied the proposed approach on two fine-grained categorization datasets, and the results indicate successful classifier prediction.</p><p>4 0.10109732 <a title="431-tfidf-4" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>Author: Yangqing Jia, Trevor Darrell</p><p>Abstract: Recent years have witnessed the success of large-scale image classification systems that are able to identify objects among thousands of possible labels. However, it is yet unclear how general classifiers such as ones trained on ImageNet can be optimally adapted to specific tasks, each of which only covers a semantically related subset of all the objects in the world. It is inefficient and suboptimal to retrain classifiers whenever a new task is given, and is inapplicable when tasks are not given explicitly, but implicitly specified as a set of image queries. In this paper we propose a novel probabilistic model that jointly identifies the underlying task and performs prediction with a lineartime probabilistic inference algorithm, given a set of query images from a latent task. We present efficient ways to estimate parameters for the model, and an open-source toolbox to train classifiers distributedly at a large scale. Empirical results based on the ImageNet data showed significant performance increase over several baseline algorithms.</p><p>5 0.099873982 <a title="431-tfidf-5" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>Author: Qiong Cao, Yiming Ying, Peng Li</p><p>Abstract: Recently, there is a considerable amount of efforts devoted to the problem of unconstrained face verification, where the task is to predict whether pairs of images are from the same person or not. This problem is challenging and difficult due to the large variations in face images. In this paper, we develop a novel regularization framework to learn similarity metrics for unconstrained face verification. We formulate its objective function by incorporating the robustness to the large intra-personal variations and the discriminative power of novel similarity metrics. In addition, our formulation is a convex optimization problem which guarantees the existence of its global solution. Experiments show that our proposed method achieves the state-of-the-art results on the challenging Labeled Faces in the Wild (LFW) database [10].</p><p>6 0.093568988 <a title="431-tfidf-6" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>7 0.091578223 <a title="431-tfidf-7" href="./iccv-2013-From_Point_to_Set%3A_Extend_the_Learning_of_Distance_Metrics.html">177 iccv-2013-From Point to Set: Extend the Learning of Distance Metrics</a></p>
<p>8 0.090245992 <a title="431-tfidf-8" href="./iccv-2013-Frustratingly_Easy_NBNN_Domain_Adaptation.html">181 iccv-2013-Frustratingly Easy NBNN Domain Adaptation</a></p>
<p>9 0.088235937 <a title="431-tfidf-9" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>10 0.087093785 <a title="431-tfidf-10" href="./iccv-2013-Unsupervised_Visual_Domain_Adaptation_Using_Subspace_Alignment.html">438 iccv-2013-Unsupervised Visual Domain Adaptation Using Subspace Alignment</a></p>
<p>11 0.083180897 <a title="431-tfidf-11" href="./iccv-2013-Saliency_and_Human_Fixations%3A_State-of-the-Art_and_Study_of_Comparison_Metrics.html">373 iccv-2013-Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics</a></p>
<p>12 0.07828515 <a title="431-tfidf-12" href="./iccv-2013-Unsupervised_Domain_Adaptation_by_Domain_Invariant_Projection.html">435 iccv-2013-Unsupervised Domain Adaptation by Domain Invariant Projection</a></p>
<p>13 0.07429906 <a title="431-tfidf-13" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>14 0.071899615 <a title="431-tfidf-14" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>15 0.069841601 <a title="431-tfidf-15" href="./iccv-2013-Joint_Learning_of_Discriminative_Prototypes_and_Large_Margin_Nearest_Neighbor_Classifiers.html">222 iccv-2013-Joint Learning of Discriminative Prototypes and Large Margin Nearest Neighbor Classifiers</a></p>
<p>16 0.069385916 <a title="431-tfidf-16" href="./iccv-2013-Decomposing_Bag_of_Words_Histograms.html">104 iccv-2013-Decomposing Bag of Words Histograms</a></p>
<p>17 0.06895832 <a title="431-tfidf-17" href="./iccv-2013-Quadruplet-Wise_Image_Similarity_Learning.html">332 iccv-2013-Quadruplet-Wise Image Similarity Learning</a></p>
<p>18 0.068919279 <a title="431-tfidf-18" href="./iccv-2013-Deterministic_Fitting_of_Multiple_Structures_Using_Iterative_MaxFS_with_Inlier_Scale_Estimation.html">113 iccv-2013-Deterministic Fitting of Multiple Structures Using Iterative MaxFS with Inlier Scale Estimation</a></p>
<p>19 0.068166189 <a title="431-tfidf-19" href="./iccv-2013-Handling_Occlusions_with_Franken-Classifiers.html">190 iccv-2013-Handling Occlusions with Franken-Classifiers</a></p>
<p>20 0.067375913 <a title="431-tfidf-20" href="./iccv-2013-Domain_Transfer_Support_Vector_Ranking_for_Person_Re-identification_without_Target_Camera_Label_Information.html">124 iccv-2013-Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.174), (1, 0.074), (2, -0.037), (3, -0.087), (4, 0.012), (5, 0.051), (6, -0.009), (7, 0.012), (8, -0.016), (9, -0.035), (10, 0.014), (11, -0.051), (12, -0.017), (13, -0.031), (14, 0.089), (15, -0.091), (16, -0.029), (17, -0.028), (18, 0.035), (19, -0.003), (20, 0.026), (21, -0.057), (22, -0.016), (23, 0.014), (24, 0.011), (25, 0.015), (26, 0.047), (27, 0.008), (28, 0.013), (29, 0.037), (30, 0.021), (31, -0.019), (32, 0.005), (33, 0.007), (34, -0.022), (35, -0.029), (36, 0.032), (37, 0.017), (38, -0.008), (39, -0.01), (40, 0.028), (41, 0.019), (42, -0.023), (43, 0.069), (44, 0.097), (45, -0.042), (46, 0.063), (47, 0.012), (48, -0.028), (49, 0.083)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95212907 <a title="431-lsi-1" href="./iccv-2013-Unbiased_Metric_Learning%3A_On_the_Utilization_of_Multiple_Datasets_and_Web_Images_for_Softening_Bias.html">431 iccv-2013-Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias</a></p>
<p>Author: Chen Fang, Ye Xu, Daniel N. Rockmore</p><p>Abstract: Many standard computer vision datasets exhibit biases due to a variety of sources including illumination condition, imaging system, and preference of dataset collectors. Biases like these can have downstream effects in the use of vision datasets in the construction of generalizable techniques, especially for the goal of the creation of a classification system capable of generalizing to unseen and novel datasets. In this work we propose Unbiased Metric Learning (UML), a metric learning approach, to achieve this goal. UML operates in the following two steps: (1) By varying hyperparameters, it learns a set of less biased candidate distance metrics on training examples from multiple biased datasets. The key idea is to learn a neighborhood for each example, which consists of not only examples of the same category from the same dataset, but those from other datasets. The learning framework is based on structural SVM. (2) We do model validation on a set of weakly-labeled web images retrieved by issuing class labels as keywords to search engine. The metric with best validationperformance is selected. Although the web images sometimes have noisy labels, they often tend to be less biased, which makes them suitable for the validation set in our task. Cross-dataset image classification experiments are carried out. Results show significant performance improvement on four well-known computer vision datasets.</p><p>2 0.79881561 <a title="431-lsi-2" href="./iccv-2013-Quadruplet-Wise_Image_Similarity_Learning.html">332 iccv-2013-Quadruplet-Wise Image Similarity Learning</a></p>
<p>Author: Marc T. Law, Nicolas Thome, Matthieu Cord</p><p>Abstract: This paper introduces a novel similarity learning framework. Working with inequality constraints involving quadruplets of images, our approach aims at efficiently modeling similarity from rich or complex semantic label relationships. From these quadruplet-wise constraints, we propose a similarity learning framework relying on a convex optimization scheme. We then study how our metric learning scheme can exploit specific class relationships, such as class ranking (relative attributes), and class taxonomy. We show that classification using the learned metrics gets improved performance over state-of-the-art methods on several datasets. We also evaluate our approach in a new application to learn similarities between webpage screenshots in a fully unsupervised way.</p><p>3 0.78302121 <a title="431-lsi-3" href="./iccv-2013-Write_a_Classifier%3A_Zero-Shot_Learning_Using_Purely_Textual_Descriptions.html">451 iccv-2013-Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions</a></p>
<p>Author: Mohamed Elhoseiny, Babak Saleh, Ahmed Elgammal</p><p>Abstract: The main question we address in this paper is how to use purely textual description of categories with no training images to learn visual classifiers for these categories. We propose an approach for zero-shot learning of object categories where the description of unseen categories comes in the form of typical text such as an encyclopedia entry, without the need to explicitly defined attributes. We propose and investigate two baseline formulations, based on regression and domain adaptation. Then, we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to predict the classifier parameters for new classes. We applied the proposed approach on two fine-grained categorization datasets, and the results indicate successful classifier prediction.</p><p>4 0.77681631 <a title="431-lsi-4" href="./iccv-2013-From_Point_to_Set%3A_Extend_the_Learning_of_Distance_Metrics.html">177 iccv-2013-From Point to Set: Extend the Learning of Distance Metrics</a></p>
<p>Author: Pengfei Zhu, Lei Zhang, Wangmeng Zuo, David Zhang</p><p>Abstract: Most of the current metric learning methods are proposed for point-to-point distance (PPD) based classification. In many computer vision tasks, however, we need to measure the point-to-set distance (PSD) and even set-to-set distance (SSD) for classification. In this paper, we extend the PPD based Mahalanobis distance metric learning to PSD and SSD based ones, namely point-to-set distance metric learning (PSDML) and set-to-set distance metric learning (SSDML), and solve them under a unified optimization framework. First, we generate positive and negative sample pairs by computing the PSD and SSD between training samples. Then, we characterize each sample pair by its covariance matrix, and propose a covariance kernel based discriminative function. Finally, we tackle the PSDML and SSDMLproblems by using standard support vector machine solvers, making the metric learning very efficient for multiclass visual classification tasks. Experiments on gender classification, digit recognition, object categorization and face recognition show that the proposed metric learning methods can effectively enhance the performance of PSD and SSD based classification.</p><p>5 0.77331132 <a title="431-lsi-5" href="./iccv-2013-Joint_Learning_of_Discriminative_Prototypes_and_Large_Margin_Nearest_Neighbor_Classifiers.html">222 iccv-2013-Joint Learning of Discriminative Prototypes and Large Margin Nearest Neighbor Classifiers</a></p>
<p>Author: Martin Köstinger, Paul Wohlhart, Peter M. Roth, Horst Bischof</p><p>Abstract: In this paper, we raise important issues concerning the evaluation complexity of existing Mahalanobis metric learning methods. The complexity scales linearly with the size of the dataset. This is especially cumbersome on large scale or for real-time applications with limited time budget. To alleviate this problem we propose to represent the dataset by a fixed number of discriminative prototypes. In particular, we introduce a new method that jointly chooses the positioning of prototypes and also optimizes the Mahalanobis distance metric with respect to these. We show that choosing the positioning of the prototypes and learning the metric in parallel leads to a drastically reduced evaluation effort while maintaining the discriminative essence of the original dataset. Moreover, for most problems our method performing k-nearest prototype (k-NP) classification on the condensed dataset leads to even better generalization compared to k-NN classification using all data. Results on a variety of challenging benchmarks demonstrate the power of our method. These include standard machine learning datasets as well as the challenging Public Fig- ures Face Database. On the competitive machine learning benchmarks we are comparable to the state-of-the-art while being more efficient. On the face benchmark we clearly outperform the state-of-the-art in Mahalanobis metric learning with drastically reduced evaluation effort.</p><p>6 0.77192926 <a title="431-lsi-6" href="./iccv-2013-Ensemble_Projection_for_Semi-supervised_Image_Classification.html">142 iccv-2013-Ensemble Projection for Semi-supervised Image Classification</a></p>
<p>7 0.74874789 <a title="431-lsi-7" href="./iccv-2013-Learning_to_Rank_Using_Privileged_Information.html">248 iccv-2013-Learning to Rank Using Privileged Information</a></p>
<p>8 0.74315178 <a title="431-lsi-8" href="./iccv-2013-Large-Scale_Image_Annotation_by_Efficient_and_Robust_Kernel_Metric_Learning.html">227 iccv-2013-Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning</a></p>
<p>9 0.74311578 <a title="431-lsi-9" href="./iccv-2013-Frustratingly_Easy_NBNN_Domain_Adaptation.html">181 iccv-2013-Frustratingly Easy NBNN Domain Adaptation</a></p>
<p>10 0.72771764 <a title="431-lsi-10" href="./iccv-2013-Heterogeneous_Image_Features_Integration_via_Multi-modal_Semi-supervised_Learning_Model.html">194 iccv-2013-Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model</a></p>
<p>11 0.7254225 <a title="431-lsi-11" href="./iccv-2013-Image_Set_Classification_Using_Holistic_Multiple_Order_Statistics_Features_and_Localized_Multi-kernel_Metric_Learning.html">212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</a></p>
<p>12 0.70030475 <a title="431-lsi-12" href="./iccv-2013-Dynamic_Label_Propagation_for_Semi-supervised_Multi-class_Multi-label_Classification.html">126 iccv-2013-Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification</a></p>
<p>13 0.69515628 <a title="431-lsi-13" href="./iccv-2013-Drosophila_Embryo_Stage_Annotation_Using_Label_Propagation.html">125 iccv-2013-Drosophila Embryo Stage Annotation Using Label Propagation</a></p>
<p>14 0.68231851 <a title="431-lsi-14" href="./iccv-2013-Predicting_an_Object_Location_Using_a_Global_Image_Representation.html">327 iccv-2013-Predicting an Object Location Using a Global Image Representation</a></p>
<p>15 0.6818614 <a title="431-lsi-15" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>16 0.67087179 <a title="431-lsi-16" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>17 0.65916795 <a title="431-lsi-17" href="./iccv-2013-Handling_Uncertain_Tags_in_Visual_Recognition.html">191 iccv-2013-Handling Uncertain Tags in Visual Recognition</a></p>
<p>18 0.65464938 <a title="431-lsi-18" href="./iccv-2013-Latent_Task_Adaptation_with_Large-Scale_Hierarchies.html">233 iccv-2013-Latent Task Adaptation with Large-Scale Hierarchies</a></p>
<p>19 0.65210688 <a title="431-lsi-19" href="./iccv-2013-Heterogeneous_Auto-similarities_of_Characteristics_%28HASC%29%3A_Exploiting_Relational_Information_for_Classification.html">193 iccv-2013-Heterogeneous Auto-similarities of Characteristics (HASC): Exploiting Relational Information for Classification</a></p>
<p>20 0.65077519 <a title="431-lsi-20" href="./iccv-2013-Learning_CRFs_for_Image_Parsing_with_Adaptive_Subgradient_Descent.html">234 iccv-2013-Learning CRFs for Image Parsing with Adaptive Subgradient Descent</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.096), (7, 0.01), (26, 0.07), (31, 0.034), (35, 0.012), (42, 0.129), (64, 0.029), (73, 0.015), (89, 0.117), (98, 0.382)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77668691 <a title="431-lda-1" href="./iccv-2013-Unbiased_Metric_Learning%3A_On_the_Utilization_of_Multiple_Datasets_and_Web_Images_for_Softening_Bias.html">431 iccv-2013-Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias</a></p>
<p>Author: Chen Fang, Ye Xu, Daniel N. Rockmore</p><p>Abstract: Many standard computer vision datasets exhibit biases due to a variety of sources including illumination condition, imaging system, and preference of dataset collectors. Biases like these can have downstream effects in the use of vision datasets in the construction of generalizable techniques, especially for the goal of the creation of a classification system capable of generalizing to unseen and novel datasets. In this work we propose Unbiased Metric Learning (UML), a metric learning approach, to achieve this goal. UML operates in the following two steps: (1) By varying hyperparameters, it learns a set of less biased candidate distance metrics on training examples from multiple biased datasets. The key idea is to learn a neighborhood for each example, which consists of not only examples of the same category from the same dataset, but those from other datasets. The learning framework is based on structural SVM. (2) We do model validation on a set of weakly-labeled web images retrieved by issuing class labels as keywords to search engine. The metric with best validationperformance is selected. Although the web images sometimes have noisy labels, they often tend to be less biased, which makes them suitable for the validation set in our task. Cross-dataset image classification experiments are carried out. Results show significant performance improvement on four well-known computer vision datasets.</p><p>2 0.76732415 <a title="431-lda-2" href="./iccv-2013-Unsupervised_Domain_Adaptation_by_Domain_Invariant_Projection.html">435 iccv-2013-Unsupervised Domain Adaptation by Domain Invariant Projection</a></p>
<p>Author: Mahsa Baktashmotlagh, Mehrtash T. Harandi, Brian C. Lovell, Mathieu Salzmann</p><p>Abstract: Domain-invariant representations are key to addressing the domain shift problem where the training and test examples follow different distributions. Existing techniques that have attempted to match the distributions of the source and target domains typically compare these distributions in the original feature space. This space, however, may not be directly suitable for such a comparison, since some of the features may have been distorted by the domain shift, or may be domain specific. In this paper, we introduce a Domain Invariant Projection approach: An unsupervised domain adaptation method that overcomes this issue by extracting the information that is invariant across the source and target domains. More specifically, we learn a projection of the data to a low-dimensional latent space where the distance between the empirical distributions of the source and target examples is minimized. We demonstrate the effectiveness of our approach on the task of visual object recognition and show that it outperforms state-of-the-art methods on a standard domain adaptation benchmark dataset.</p><p>3 0.71693134 <a title="431-lda-3" href="./iccv-2013-Unifying_Nuclear_Norm_and_Bilinear_Factorization_Approaches_for_Low-Rank_Matrix_Decomposition.html">434 iccv-2013-Unifying Nuclear Norm and Bilinear Factorization Approaches for Low-Rank Matrix Decomposition</a></p>
<p>Author: Ricardo Cabral, Fernando De_La_Torre, João P. Costeira, Alexandre Bernardino</p><p>Abstract: Low rank models have been widely usedfor the representation of shape, appearance or motion in computer vision problems. Traditional approaches to fit low rank models make use of an explicit bilinear factorization. These approaches benefit from fast numerical methods for optimization and easy kernelization. However, they suffer from serious local minima problems depending on the loss function and the amount/type of missing data. Recently, these lowrank models have alternatively been formulated as convex problems using the nuclear norm regularizer; unlike factorization methods, their numerical solvers are slow and it is unclear how to kernelize them or to impose a rank a priori. This paper proposes a unified approach to bilinear factorization and nuclear norm regularization, that inherits the benefits of both. We analyze the conditions under which these approaches are equivalent. Moreover, based on this analysis, we propose a new optimization algorithm and a “rank continuation ” strategy that outperform state-of-theart approaches for Robust PCA, Structure from Motion and Photometric Stereo with outliers and missing data.</p><p>4 0.70601439 <a title="431-lda-4" href="./iccv-2013-Modeling_the_Calibration_Pipeline_of_the_Lytro_Camera_for_High_Quality_Light-Field_Image_Reconstruction.html">271 iccv-2013-Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction</a></p>
<p>Author: Donghyeon Cho, Minhaeng Lee, Sunyeong Kim, Yu-Wing Tai</p><p>Abstract: Light-field imaging systems have got much attention recently as the next generation camera model. A light-field imaging system consists of three parts: data acquisition, manipulation, and application. Given an acquisition system, it is important to understand how a light-field camera converts from its raw image to its resulting refocused image. In this paper, using the Lytro camera as an example, we describe step-by-step procedures to calibrate a raw light-field image. In particular, we are interested in knowing the spatial and angular coordinates of the micro lens array and the resampling process for image reconstruction. Since Lytro uses a hexagonal arrangement of a micro lens image, additional treatments in calibration are required. After calibration, we analyze and compare the performances of several resampling methods for image reconstruction with and without calibration. Finally, a learning based interpolation method is proposed which demonstrates a higher quality image reconstruction than previous interpolation methods including a method used in Lytro software.</p><p>5 0.69826484 <a title="431-lda-5" href="./iccv-2013-A_Learning-Based_Approach_to_Reduce_JPEG_Artifacts_in_Image_Matting.html">19 iccv-2013-A Learning-Based Approach to Reduce JPEG Artifacts in Image Matting</a></p>
<p>Author: Inchang Choi, Sunyeong Kim, Michael S. Brown, Yu-Wing Tai</p><p>Abstract: Single image matting techniques assume high-quality input images. The vast majority of images on the web and in personal photo collections are encoded using JPEG compression. JPEG images exhibit quantization artifacts that adversely affect the performance of matting algorithms. To address this situation, we propose a learning-based post-processing method to improve the alpha mattes extracted from JPEG images. Our approach learns a set of sparse dictionaries from training examples that are used to transfer details from high-quality alpha mattes to alpha mattes corrupted by JPEG compression. Three different dictionaries are defined to accommodate different object structure (long hair, short hair, and sharp boundaries). A back-projection criteria combined within an MRF framework is used to automatically select the best dictionary to apply on the object’s local boundary. We demonstrate that our method can produces superior results over existing state-of-the-art matting algorithms on a variety of inputs and compression levels.</p><p>6 0.69235492 <a title="431-lda-6" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>7 0.65095913 <a title="431-lda-7" href="./iccv-2013-A_Unified_Video_Segmentation_Benchmark%3A_Annotation%2C_Metrics_and_Analysis.html">33 iccv-2013-A Unified Video Segmentation Benchmark: Annotation, Metrics and Analysis</a></p>
<p>8 0.61452609 <a title="431-lda-8" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>9 0.60659862 <a title="431-lda-9" href="./iccv-2013-Frustratingly_Easy_NBNN_Domain_Adaptation.html">181 iccv-2013-Frustratingly Easy NBNN Domain Adaptation</a></p>
<p>10 0.58849496 <a title="431-lda-10" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>11 0.58004069 <a title="431-lda-11" href="./iccv-2013-Unsupervised_Visual_Domain_Adaptation_Using_Subspace_Alignment.html">438 iccv-2013-Unsupervised Visual Domain Adaptation Using Subspace Alignment</a></p>
<p>12 0.54076344 <a title="431-lda-12" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>13 0.53747034 <a title="431-lda-13" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<p>14 0.53019202 <a title="431-lda-14" href="./iccv-2013-Joint_Learning_of_Discriminative_Prototypes_and_Large_Margin_Nearest_Neighbor_Classifiers.html">222 iccv-2013-Joint Learning of Discriminative Prototypes and Large Margin Nearest Neighbor Classifiers</a></p>
<p>15 0.52986848 <a title="431-lda-15" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>16 0.52725428 <a title="431-lda-16" href="./iccv-2013-Active_Visual_Recognition_with_Expertise_Estimation_in_Crowdsourcing.html">43 iccv-2013-Active Visual Recognition with Expertise Estimation in Crowdsourcing</a></p>
<p>17 0.52067727 <a title="431-lda-17" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>18 0.51864791 <a title="431-lda-18" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>19 0.51756668 <a title="431-lda-19" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>20 0.51635188 <a title="431-lda-20" href="./iccv-2013-Enhanced_Continuous_Tabu_Search_for_Parameter_Estimation_in_Multiview_Geometry.html">141 iccv-2013-Enhanced Continuous Tabu Search for Parameter Estimation in Multiview Geometry</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
