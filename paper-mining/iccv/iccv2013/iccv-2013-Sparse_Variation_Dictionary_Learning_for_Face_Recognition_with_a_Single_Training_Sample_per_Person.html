<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-398" href="#">iccv2013-398</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</h1>
<br/><p>Source: <a title="iccv-2013-398-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Yang_Sparse_Variation_Dictionary_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Meng Yang, Luc Van_Gool, Lei Zhang</p><p>Abstract: Face recognition (FR) with a single training sample per person (STSPP) is a very challenging problem due to the lack of information to predict the variations in the query sample. Sparse representation based classification has shown interesting results in robust FR; however, its performance will deteriorate much for FR with STSPP. To address this issue, in this paper we learn a sparse variation dictionary from a generic training set to improve the query sample representation by STSPP. Instead of learning from the generic training set independently w.r.t. the gallery set, the proposed sparse variation dictionary learning (SVDL) method is adaptive to the gallery set by jointly learning a projection to connect the generic training set with the gallery set. The learnt sparse variation dictionary can be easily integrated into the framework of sparse representation based classification so that various variations in face images, including illumination, expression, occlusion, pose, etc., can be better handled. Experiments on the large-scale CMU Multi-PIE, FRGC and LFW databases demonstrate the promising performance of SVDL on FR with STSPP.</p><p>Reference: <a title="iccv-2013-398-reference" href="../iccv2013_reference/iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ch on Abstract Face recognition (FR) with a single training sample per person (STSPP) is a very challenging problem due to the lack of information to predict the variations in the query sample. [sent-4, score-0.306]
</p><p>2 To address this issue, in this paper we learn a sparse variation dictionary from a generic training set to improve the query sample representation by STSPP. [sent-6, score-0.8]
</p><p>3 Instead of learning from the generic training set independently w. [sent-7, score-0.287]
</p><p>4 the gallery set, the proposed sparse variation dictionary learning (SVDL) method is adaptive to the gallery set by jointly learning a projection to connect the generic training set with the gallery set. [sent-10, score-1.917]
</p><p>5 The learnt sparse variation dictionary can be easily integrated into the framework of sparse representation based classification so that various variations in face  images, including illumination, expression, occlusion, pose, etc. [sent-11, score-0.753]
</p><p>6 ), we can only have a single training face image per person. [sent-20, score-0.232]
</p><p>7 The recently developed representation based FR methods such as sparse representation based classification (SRC) [27] cannot be easily applied to STSPP, either, since SRC needs multiple training samples per person to reasonably represent the query face. [sent-30, score-0.302]
</p><p>8 According to the availability of an additional generic training set, the FR methods for STSPP can be classified into two categories: methods without using a generic training set, and methods with generic learning. [sent-32, score-0.709]
</p><p>9 The STSPP methods without generic learning often extract robust local features (e. [sent-33, score-0.228]
</p><p>10 Considering the fact that face variations for different subjects share much similarity, an additional generic training  set with multiple samples per person could bring new and useful information (e. [sent-43, score-0.647]
</p><p>11 Therefore, a generic training set can be employed to extract discriminant information for FR with STSPP[14][25][8]. [sent-46, score-0.272]
</p><p>12 The so-called Extended SRC (ESRC) computes the intra-class variation from a generic training set and then uses the generic variation matrix to code the difference between the query and gallery samples. [sent-51, score-1.156]
</p><p>13 Although much improvement has been reported, sever-  al critical issues remain in ESRC and other generic training based methods for FR with STSPP. [sent-52, score-0.256]
</p><p>14 First, the generic intraclass variation may not be similar to that of gallery subjects, so the extraction of discrimination information from the generic training set may not be guaranteed. [sent-53, score-0.954]
</p><p>15 Second, the learned variation matrix can be very big and redundant since many subjects in the generic training set are involved. [sent-54, score-0.519]
</p><p>16 Third, the learned variation matrix cannot represent the unknown occlusion in query images due to the randomness of location and intensity of occluded pixels. [sent-56, score-0.267]
</p><p>17 To solve the above problems, we propose to learn a compact dictionary with powerful variation representation ability, jointly with an adaptive projection from the generic training set to the gallery set. [sent-57, score-1.146]
</p><p>18 To the best of our knowledge, however, the dictionary learning for pattern classification tasks is mostly conducted on the gallery set with multiple samples per class. [sent-59, score-0.679]
</p><p>19 , KSVD [1]) to the generic training set to learn a dictionary for variation representation, it would ignore the correlation between the generic training set and the gallery set. [sent-62, score-1.238]
</p><p>20 Although the correlation between generic training set and gallery set has been studied in subspace learning [19], how to learn a gallery-set adaptive dictionary to exploit the variation in the generic training set is a new problem. [sent-63, score-1.363]
</p><p>21 With the above considerations, in this paper we propose  a novel sparse variation dictionary learning (SVDL) method for FR with STSPP. [sent-64, score-0.479]
</p><p>22 1, the proposed SVDL is a joint learning framework of adaptive projection and a sparse variation dictionary. [sent-66, score-0.382]
</p><p>23 Compared with previous methods [5][14][25][8] [19], the proposed joint learning of adaptive projection and sparse variation dictionary more effectively exploits the information of gallery set and generic training set. [sent-68, score-1.204]
</p><p>24 Extensive experiments on large-scale face databases with various variations, including illumination, expression, pose, session, occlusion and blur, show that the proposed SVDL achieves state-of-the-art performance for FR with STSPP. [sent-69, score-0.2]
</p><p>25 Representation of face images with variation Suppose that we are given a sufficiently large generic training set. [sent-73, score-0.552]
</p><p>26 Each subject in the generic training set has multiple face images, each with one type of variation (e. [sent-74, score-0.592]
</p><p>27 Since the high-  dimensional face images usually lie on a lower-dimensional subspace or sub-manifold, a sample in the gallery set, denoted by g, could be represented as g = Rγ, where R is a subset of the generic training set and γ is the representation coefficient of g over R. [sent-78, score-0.845]
</p><p>28 Each column of R is a vectorized training sample of a generic subject, and we assume that the subjects in R have similar facial variations as g (e. [sent-79, score-0.421]
</p><p>29 ; random corruption and occlusion are not considered in the generic training set. [sent-82, score-0.358]
</p><p>30 Suppose that there are n types of variations available in the generic training set, and we denote by X(v) = R(v) − R,  v = 1, ·  · ·  ,n  (4)  the n variation matrices obtained from the generic training set. [sent-93, score-0.74]
</p><p>31 set Ri for a gallery individual gi can typically be extracted ? [sent-101, score-0.423]
</p><p>32 learning model For the ith gallery subject, gi, i= 1, · · · , c, we thus can extract from the generic training s =et 1tw,·o· ·su ,cb,s wetse t(hsuees ctahen end of previous Section). [sent-113, score-0.628]
</p><p>33 Different subject iusually has the same Ri and Xi since the extraction of Ri only depends on the variation type of gi and the gallery images have limited facial variations. [sent-121, score-0.623]
</p><p>34 In order to learn a compact variation dictionary adaptively associated to the gallery set, we propose the following sparse variation dictionary learning (SVDL) model:  mDin? [sent-122, score-1.221]
</p><p>35 ic=1{p(gi,Ri,γi) + q (D,Xi,γi)}  (6)  where D is the dictionary to be learned, γi is the coding vector of gi on Ri, p (gi , Ri, γi) is the adaptive projection learning term, q (D, Xi , γi) is the variation dictionary learning term. [sent-123, score-0.906]
</p><p>36 With projection γi, the variation matrix Xi could be projected onto the dictionary to obtain the gallery set’s variation matrix. [sent-124, score-0.978]
</p><p>37 Therefore, the projection serves as a bridge to connect the generic training dataset with the gallery set so that the learned dictionary is adaptive to, instead of being independent of, the gallery set. [sent-125, score-1.314]
</p><p>38 1, we know that the projected variation of the ith subject on the gallery set, denoted by Yi, could be approximately represented as Yi = Xi ? [sent-130, score-0.567]
</p><p>39 2  Variation dictionary learning term  The whole variation matrix of all subjects in the gallery set can be simply set as the concatenation of all Yi, i = 1, · · · , c. [sent-143, score-0.82]
</p><p>40 However, such a variation matrix for the whole gallery cs. [sent-144, score-0.501]
</p><p>41 Intuitively, we could learn a much more compact variation dictionary D = [d1, , dj , · · · , dm] from all Yi, where dj is a basis (or atom) in the dictionary D. [sent-146, score-0.796]
</p><p>42 d λ3 are constants to balance the representation fidelity term, the sparse  coefficient term, and the sparse dictionary basis term. [sent-164, score-0.371]
</p><p>43 Apart from the l1-norm sparsity imposed on the coefficients Bi, we also impose the l1-norm sparsity on the variation dictionary atoms dj . [sent-168, score-0.57]
</p><p>44 , expression, pose, session difference, and some illumination) usually lead to sparse changes of face images. [sent-171, score-0.366]
</p><p>45 (10) can be divided into two sub-problems: adaptive projection learning by fixing D and Bi, and sparse variation dictionary learning by 669911  fixing γi. [sent-200, score-0.674]
</p><p>46 The update of sparse variation dictionary For the convenience of expression, we  let  Y = [X1 ? [sent-210, score-0.468]
</p><p>47 (10), i s = = red 1u,c·e·d· cto, tahree following sparse dictionary learning problem:  Dm,iBn? [sent-218, score-0.319]
</p><p>48 However, when B is fixed, the update of sparse dictionary D needs a little more effort. [sent-232, score-0.308]
</p><p>49 We update the variation dictionary atom by atom, as described below. [sent-233, score-0.452]
</p><p>50 We rewrite B as B = [b1; · · · ; bj ; · · · ; bm], where bj is the jth row of B and m is th;e· n··um ;bbe;r· ·of· dictionary atoms. [sent-234, score-0.257]
</p><p>51 b Ief rle imso vvereyd cfrloomse tthoe 0 dictionary esin −ce 6 )it, is useless to represent Z; otherwise, dk could be updated as 2. [sent-271, score-0.332]
</p><p>52 s  |oxtηh|e≤r w τise  (16)  The dictionary D is updated once all atoms dk are updated. [sent-293, score-0.358]
</p><p>53 With the learnt dictionary D, the testing sample y could be coded as αˆ = arg minα ? [sent-329, score-0.348]
</p><p>54 1  (18)  where G = [g1, · · · , gi, · · · ,gc] is the gallery set. [sent-333, score-0.341]
</p><p>55 Among these methods, NN, SVM, SRC and DMMA do not use a generic training set, while ESRC, AGL, and SVDL need a generic training set. [sent-354, score-0.512]
</p><p>56 For a more comprehensive evaluation and better demonstration of the proposed SVDL, we also report the performance of ESRC by coupling it with a variation dictionary learnt via KSVD [1]. [sent-355, score-0.403]
</p><p>57 In the so-called ESRC-KSVD, a dictionary that can sparsely represent the generic variation matrix is learned via KSVD, and the classification is conducted  in the same way as ESRC except that the generic variation is replaced by the learned dictionary. [sent-356, score-1.002]
</p><p>58 λ1 regularizes the projection from the generic training set to the gallery set, while λ2 and λ3 control the sparsity of representation coefficients and dictionary atoms, respectively. [sent-360, score-0.951]
</p><p>59 In addition, the number of dictionary atoms should be set beforehand. [sent-367, score-0.277]
</p><p>60 (14)), we set the number of dictionary atoms as 400 in the initialization. [sent-369, score-0.277]
</p><p>61 Robustness to various variations We first test the robustness of all the competing methods by using the large-scale CMU Multi-PIE database [6], whose images were captured in four sessions with simultaneous variations of pose, expression, and illumination. [sent-372, score-0.247]
</p><p>62 For each subject in each session, there are 20 illuminations with indices from 0 to 19 per pose per expression. [sent-373, score-0.196]
</p><p>63 7 5304  for gallery training, with the remaining subjects for generic training. [sent-381, score-0.601]
</p><p>64 For the gallery set, we used the single frontal image with illumination 7 and neutral expression. [sent-382, score-0.493]
</p><p>65 In the following tests with various variations, the images in the generic training set include all the face images with corresponding expression or pose variation, and the frontal face image with neutral expression in Session 1. [sent-383, score-0.896]
</p><p>66 1) Illumination variation: We use all the frontal face images with neutral expression in Sessions 2, 3, and 4 for testing. [sent-386, score-0.34]
</p><p>67 3 shows some samples of one subject, including a gallery sample and three testing samples (e. [sent-388, score-0.458]
</p><p>68 DMMA is the best method without generic training; nonetheless, its recognition rates are not high since the illumination variation cannot be well learned from the gallery set via multi-manifold learning. [sent-395, score-0.878]
</p><p>69 2) Expression and illumination variations: In this experiment, the testing samples include the frontal face images with smile in Session 1 (Smi-S1), smile in Session 3 (Smi-S3), surprise in Session 2 (Sur-S2), and squint in Session 2 (Squ-S2) (please refer to Fig. [sent-396, score-0.405]
</p><p>70 The recognition rates (%) under on Multi-PIE database with expression and illumination variations. [sent-402, score-0.297]
</p><p>71 Face recognition rates (%) on Multi-PIE database with pose, expression and illumination variations. [sent-410, score-0.297]
</p><p>72 The variation dictionary learned by KSVD does not improve the recognition accuracy of ESRC much. [sent-416, score-0.441]
</p><p>73 Again, the methods with generic training usually have much better performance than the ones without generic training. [sent-419, score-0.453]
</p><p>74 3) Pose, illumination and expression variations: In this experiment, the testing samples include face images with pose 05 0 in Session 2 (P05 0-S2), pose 04 1 in Session 3 (P04 1-S3), and pose 04 1 and smile expression in Session 3 (Smi-P04 1-S3) (please refer to Fig. [sent-420, score-0.668]
</p><p>75 52638  ods with generic learning are significantly better than the ones without generic learning. [sent-438, score-0.425]
</p><p>76 The improvement of SVDL over ESRC/ESRC-KSVD demonstrates the benefit of jointly learning of the adaptive projection and variation dictionary. [sent-439, score-0.319]
</p><p>77 4) Random corruption and block occlusion: In this experiment, the remaining 19 frontal face images of the first 100 subjects in Session 1 with various illuminations (all illuminations but 7) and neutral expression are used as the clean testing images. [sent-440, score-0.603]
</p><p>78 Though AGL, ESRC and ESRC-KSVD could get  good recognition accuracy when there is no corruption, their recognition rates drop dramatically with the increase of corruption ratio. [sent-447, score-0.205]
</p><p>79 For each subject, a single sample  ×  with normal illumination and neutral expression in the target set is selected to build the gallery set, and all the 8,014 face images in the query set are used for testing. [sent-461, score-0.787]
</p><p>80 Some example images in the gallery set and testing set are shown in the last two rows of Fig. [sent-462, score-0.368]
</p><p>81 The uncontrolled LFW face dataset is usually used to test the face verification methods. [sent-465, score-0.309]
</p><p>82 We choose a single sample per subject to construct the gallery set, with the remaining images as the testing set. [sent-469, score-0.479]
</p><p>83 For the experiment on FRGC, the generic training set is built from the frontal images in the Session 1 of CMU Multi-PIE database. [sent-470, score-0.297]
</p><p>84 For the experiment on LFW, we added face images with pose 05 0, pose 04 1 with smile expression, and their mirrored images in Session 1to the generic training set. [sent-472, score-0.511]
</p><p>85 Some examples in the generic training set on the experiment of FRGC are shown in the first row of Fig. [sent-473, score-0.256]
</p><p>86 Examples of the face images in the generic training set, gallery set and testing set. [sent-478, score-0.76]
</p><p>87 However, our proposed SVDL could still achieve higher recognition rates than the other methods with all dimensions of face features. [sent-490, score-0.248]
</p><p>88 This validates that the proposed joint adaptive projection and sparse variation dictionary learning is more powerful than either of those alone for FR with STSPP. [sent-494, score-0.607]
</p><p>89 Conclusion We proposed a sparse variation dictionary learning (SVDL) method, which learns a sparse variation dictionary from a generic training set to improve face recognition performance with a single training sample per person (STSPP). [sent-500, score-1.515]
</p><p>90 –46732  The SVDL is adaptive to the gallery set by simultaneously learning a projection from the gallery set to the generic set. [sent-506, score-1.038]
</p><p>91 Hence, the correlation between the generic set and gallery set can be exploited, and the learned sparse variation dic-  tionary can more effectively aid the single training sample to represent the query image. [sent-507, score-0.919]
</p><p>92 The extensive experiments with various face variations demonstrated the superiority of SVDL to state-of-the-art face recognition methods with STSPP. [sent-508, score-0.373]
</p><p>93 Making flda applicable to face recognition with one sample per person. [sent-528, score-0.24]
</p><p>94 Discriminative multimanifold analysis for face recognition from a single training sample per person. [sent-593, score-0.327]
</p><p>95 Expression subspace projection for face recognition from single sample per person, 2012. [sent-613, score-0.338]
</p><p>96 Extended fisherface for face recognition from a single example image per person. [sent-647, score-0.206]
</p><p>97 Adaptive generic learning for face recognition from a single sample per person. [sent-654, score-0.468]
</p><p>98 Recognizing partially occluded expression variant faces from single training image per person with som and soft k-nn ensemble. [sent-668, score-0.252]
</p><p>99 On solving the face recognition problem with one training sample per subject. [sent-696, score-0.299]
</p><p>100 A new face recognition method based on svd perturbation for single example  image per person. [sent-724, score-0.206]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('svdl', 0.647), ('gallery', 0.341), ('dictionary', 0.225), ('stspp', 0.205), ('generic', 0.197), ('esrc', 0.173), ('session', 0.167), ('variation', 0.16), ('fr', 0.143), ('face', 0.136), ('frgc', 0.134), ('expression', 0.123), ('dmma', 0.095), ('lfw', 0.09), ('gi', 0.082), ('dk', 0.081), ('src', 0.079), ('agl', 0.078), ('dj', 0.072), ('illumination', 0.071), ('variations', 0.068), ('projection', 0.066), ('sparse', 0.063), ('subjects', 0.063), ('adaptive', 0.062), ('corruption', 0.06), ('training', 0.059), ('rates', 0.053), ('ri', 0.052), ('atoms', 0.052), ('cmu', 0.05), ('ksvd', 0.049), ('atom', 0.047), ('eigenfaces', 0.046), ('occlusion', 0.042), ('query', 0.042), ('sessions', 0.042), ('illuminations', 0.041), ('frontal', 0.041), ('pose', 0.041), ('subject', 0.04), ('neutral', 0.04), ('smile', 0.037), ('competing', 0.037), ('per', 0.037), ('uncontrolled', 0.037), ('sample', 0.034), ('recognition', 0.033), ('person', 0.033), ('subspace', 0.032), ('gsa', 0.032), ('pelrye', 0.032), ('ritri', 0.032), ('zbkt', 0.032), ('xi', 0.031), ('learning', 0.031), ('block', 0.031), ('samples', 0.028), ('dbi', 0.028), ('squint', 0.028), ('multimanifold', 0.028), ('bi', 0.027), ('testing', 0.027), ('could', 0.026), ('coefficients', 0.025), ('coding', 0.024), ('learned', 0.023), ('rubinstein', 0.022), ('deng', 0.022), ('shan', 0.022), ('yi', 0.022), ('databases', 0.022), ('ieee', 0.021), ('representation', 0.02), ('ml', 0.02), ('fisherfaces', 0.02), ('pami', 0.02), ('update', 0.02), ('phillips', 0.02), ('tolerate', 0.019), ('listed', 0.019), ('rewritten', 0.019), ('arg', 0.018), ('tan', 0.018), ('sparsity', 0.018), ('animation', 0.018), ('fixing', 0.018), ('learnt', 0.018), ('database', 0.017), ('big', 0.017), ('corrupted', 0.017), ('conducted', 0.017), ('bj', 0.016), ('discriminant', 0.016), ('compact', 0.016), ('controlled', 0.016), ('nv', 0.016), ('chen', 0.016), ('accuracies', 0.015), ('robustness', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="398-tfidf-1" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>Author: Meng Yang, Luc Van_Gool, Lei Zhang</p><p>Abstract: Face recognition (FR) with a single training sample per person (STSPP) is a very challenging problem due to the lack of information to predict the variations in the query sample. Sparse representation based classification has shown interesting results in robust FR; however, its performance will deteriorate much for FR with STSPP. To address this issue, in this paper we learn a sparse variation dictionary from a generic training set to improve the query sample representation by STSPP. Instead of learning from the generic training set independently w.r.t. the gallery set, the proposed sparse variation dictionary learning (SVDL) method is adaptive to the gallery set by jointly learning a projection to connect the generic training set with the gallery set. The learnt sparse variation dictionary can be easily integrated into the framework of sparse representation based classification so that various variations in face images, including illumination, expression, occlusion, pose, etc., can be better handled. Experiments on the large-scale CMU Multi-PIE, FRGC and LFW databases demonstrate the promising performance of SVDL on FR with STSPP.</p><p>2 0.24287702 <a title="398-tfidf-2" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>Author: Renliang Weng, Jiwen Lu, Junlin Hu, Gao Yang, Yap-Peng Tan</p><p>Abstract: Over the past two decades, a number of face recognition methods have been proposed in the literature. Most of them use holistic face images to recognize people. However, human faces are easily occluded by other objects in many real-world scenarios and we have to recognize the person of interest from his/her partial faces. In this paper, we propose a new partial face recognition approach by using feature set matching, which is able to align partial face patches to holistic gallery faces automatically and is robust to occlusions and illumination changes. Given each gallery image and probe face patch, we first detect keypoints and extract their local features. Then, we propose a Metric Learned ExtendedRobust PointMatching (MLERPM) method to discriminatively match local feature sets of a pair of gallery and probe samples. Lastly, the similarity of two faces is converted as the distance between two feature sets. Experimental results on three public face databases are presented to show the effectiveness of the proposed approach.</p><p>3 0.21012333 <a title="398-tfidf-3" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>Author: Chenglong Bao, Jian-Feng Cai, Hui Ji</p><p>Abstract: In recent years, how to learn a dictionary from input images for sparse modelling has been one very active topic in image processing and recognition. Most existing dictionary learning methods consider an over-complete dictionary, e.g. the K-SVD method. Often they require solving some minimization problem that is very challenging in terms of computational feasibility and efficiency. However, if the correlations among dictionary atoms are not well constrained, the redundancy of the dictionary does not necessarily improve the performance of sparse coding. This paper proposed a fast orthogonal dictionary learning method for sparse image representation. With comparable performance on several image restoration tasks, the proposed method is much more computationally efficient than the over-complete dictionary based learning methods.</p><p>4 0.20744832 <a title="398-tfidf-4" href="./iccv-2013-Coupling_Alignments_with_Recognition_for_Still-to-Video_Face_Recognition.html">97 iccv-2013-Coupling Alignments with Recognition for Still-to-Video Face Recognition</a></p>
<p>Author: Zhiwu Huang, Xiaowei Zhao, Shiguang Shan, Ruiping Wang, Xilin Chen</p><p>Abstract: The Still-to-Video (S2V) face recognition systems typically need to match faces in low-quality videos captured under unconstrained conditions against high quality still face images, which is very challenging because of noise, image blur, lowface resolutions, varying headpose, complex lighting, and alignment difficulty. To address the problem, one solution is to select the frames of ‘best quality ’ from videos (hereinafter called quality alignment in this paper). Meanwhile, the faces in the selected frames should also be geometrically aligned to the still faces offline well-aligned in the gallery. In this paper, we discover that the interactions among the three tasks–quality alignment, geometric alignment and face recognition–can benefit from each other, thus should be performed jointly. With this in mind, we propose a Coupling Alignments with Recognition (CAR) method to tightly couple these tasks via low-rank regularized sparse representation in a unified framework. Our method makes the three tasks promote mutually by a joint optimization in an Augmented Lagrange Multiplier routine. Extensive , experiments on two challenging S2V datasets demonstrate that our method outperforms the state-of-the-art methods impressively.</p><p>5 0.19743609 <a title="398-tfidf-5" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>Author: Hua Wang, Feiping Nie, Weidong Cai, Heng Huang</p><p>Abstract: Representing the raw input of a data set by a set of relevant codes is crucial to many computer vision applications. Due to the intrinsic sparse property of real-world data, dictionary learning, in which the linear decomposition of a data point uses a set of learned dictionary bases, i.e., codes, has demonstrated state-of-the-art performance. However, traditional dictionary learning methods suffer from three weaknesses: sensitivity to noisy and outlier samples, difficulty to determine the optimal dictionary size, and incapability to incorporate supervision information. In this paper, we address these weaknesses by learning a Semi-Supervised Robust Dictionary (SSR-D). Specifically, we use the ℓ2,0+ norm as the loss function to improve the robustness against outliers, and develop a new structured sparse regularization com, , tom. . cai@sydney . edu . au , heng@uta .edu make the learning tasks easier to deal with and reduce the computational cost. For example, in image tagging, instead of using the raw pixel-wise features, semi-local or patch- based features, such as SIFT and geometric blur, are usually more desirable to achieve better performance. In practice, finding a set of compact features bases, also referred to as dictionary, with enhanced representative and discriminative power, plays a significant role in building a successful computer vision system. In this paper, we explore this important problem by proposing a novel formulation and its solution for learning Semi-Supervised Robust Dictionary (SSRD), where we examine the challenges in dictionary learning, and seek opportunities to overcome them and improve the dictionary qualities. 1.1. Challenges in Dictionary Learning to incorporate the supervision information in dictionary learning, without incurring additional parameters. Moreover, the optimal dictionary size is automatically learned from the input data. Minimizing the derived objective function is challenging because it involves many non-smooth ℓ2,0+ -norm terms. We present an efficient algorithm to solve the problem with a rigorous proof of the convergence of the algorithm. Extensive experiments are presented to show the superior performance of the proposed method.</p><p>6 0.19077711 <a title="398-tfidf-6" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>7 0.17646554 <a title="398-tfidf-7" href="./iccv-2013-Markov_Network-Based_Unified_Classifier_for_Face_Identification.html">261 iccv-2013-Markov Network-Based Unified Classifier for Face Identification</a></p>
<p>8 0.16706233 <a title="398-tfidf-8" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>9 0.15620223 <a title="398-tfidf-9" href="./iccv-2013-POP%3A_Person_Re-identification_Post-rank_Optimisation.html">305 iccv-2013-POP: Person Re-identification Post-rank Optimisation</a></p>
<p>10 0.15559588 <a title="398-tfidf-10" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>11 0.15531504 <a title="398-tfidf-11" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>12 0.14829561 <a title="398-tfidf-12" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>13 0.14112346 <a title="398-tfidf-13" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>14 0.13247843 <a title="398-tfidf-14" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>15 0.12874538 <a title="398-tfidf-15" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>16 0.10689303 <a title="398-tfidf-16" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<p>17 0.10634226 <a title="398-tfidf-17" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>18 0.10216814 <a title="398-tfidf-18" href="./iccv-2013-Deep_Learning_Identity-Preserving_Face_Space.html">106 iccv-2013-Deep Learning Identity-Preserving Face Space</a></p>
<p>19 0.096569255 <a title="398-tfidf-19" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>20 0.09475562 <a title="398-tfidf-20" href="./iccv-2013-Fast_Face_Detector_Training_Using_Tailored_Views.html">157 iccv-2013-Fast Face Detector Training Using Tailored Views</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.165), (1, 0.105), (2, -0.13), (3, -0.082), (4, -0.22), (5, -0.157), (6, 0.039), (7, 0.023), (8, -0.024), (9, 0.021), (10, 0.015), (11, 0.086), (12, 0.069), (13, 0.024), (14, -0.083), (15, 0.029), (16, -0.023), (17, -0.019), (18, 0.014), (19, 0.049), (20, -0.046), (21, -0.15), (22, 0.019), (23, -0.083), (24, 0.084), (25, 0.07), (26, -0.099), (27, 0.106), (28, 0.018), (29, -0.059), (30, -0.009), (31, -0.102), (32, -0.062), (33, -0.037), (34, 0.008), (35, -0.021), (36, -0.066), (37, 0.024), (38, -0.013), (39, -0.007), (40, 0.046), (41, -0.008), (42, -0.01), (43, -0.008), (44, 0.018), (45, 0.023), (46, 0.042), (47, 0.037), (48, 0.034), (49, -0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93567783 <a title="398-lsi-1" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>Author: Meng Yang, Luc Van_Gool, Lei Zhang</p><p>Abstract: Face recognition (FR) with a single training sample per person (STSPP) is a very challenging problem due to the lack of information to predict the variations in the query sample. Sparse representation based classification has shown interesting results in robust FR; however, its performance will deteriorate much for FR with STSPP. To address this issue, in this paper we learn a sparse variation dictionary from a generic training set to improve the query sample representation by STSPP. Instead of learning from the generic training set independently w.r.t. the gallery set, the proposed sparse variation dictionary learning (SVDL) method is adaptive to the gallery set by jointly learning a projection to connect the generic training set with the gallery set. The learnt sparse variation dictionary can be easily integrated into the framework of sparse representation based classification so that various variations in face images, including illumination, expression, occlusion, pose, etc., can be better handled. Experiments on the large-scale CMU Multi-PIE, FRGC and LFW databases demonstrate the promising performance of SVDL on FR with STSPP.</p><p>2 0.7963146 <a title="398-lsi-2" href="./iccv-2013-Coupling_Alignments_with_Recognition_for_Still-to-Video_Face_Recognition.html">97 iccv-2013-Coupling Alignments with Recognition for Still-to-Video Face Recognition</a></p>
<p>Author: Zhiwu Huang, Xiaowei Zhao, Shiguang Shan, Ruiping Wang, Xilin Chen</p><p>Abstract: The Still-to-Video (S2V) face recognition systems typically need to match faces in low-quality videos captured under unconstrained conditions against high quality still face images, which is very challenging because of noise, image blur, lowface resolutions, varying headpose, complex lighting, and alignment difficulty. To address the problem, one solution is to select the frames of ‘best quality ’ from videos (hereinafter called quality alignment in this paper). Meanwhile, the faces in the selected frames should also be geometrically aligned to the still faces offline well-aligned in the gallery. In this paper, we discover that the interactions among the three tasks–quality alignment, geometric alignment and face recognition–can benefit from each other, thus should be performed jointly. With this in mind, we propose a Coupling Alignments with Recognition (CAR) method to tightly couple these tasks via low-rank regularized sparse representation in a unified framework. Our method makes the three tasks promote mutually by a joint optimization in an Augmented Lagrange Multiplier routine. Extensive , experiments on two challenging S2V datasets demonstrate that our method outperforms the state-of-the-art methods impressively.</p><p>3 0.75015885 <a title="398-lsi-3" href="./iccv-2013-Robust_Feature_Set_Matching_for_Partial_Face_Recognition.html">356 iccv-2013-Robust Feature Set Matching for Partial Face Recognition</a></p>
<p>Author: Renliang Weng, Jiwen Lu, Junlin Hu, Gao Yang, Yap-Peng Tan</p><p>Abstract: Over the past two decades, a number of face recognition methods have been proposed in the literature. Most of them use holistic face images to recognize people. However, human faces are easily occluded by other objects in many real-world scenarios and we have to recognize the person of interest from his/her partial faces. In this paper, we propose a new partial face recognition approach by using feature set matching, which is able to align partial face patches to holistic gallery faces automatically and is robust to occlusions and illumination changes. Given each gallery image and probe face patch, we first detect keypoints and extract their local features. Then, we propose a Metric Learned ExtendedRobust PointMatching (MLERPM) method to discriminatively match local feature sets of a pair of gallery and probe samples. Lastly, the similarity of two faces is converted as the distance between two feature sets. Experimental results on three public face databases are presented to show the effectiveness of the proposed approach.</p><p>4 0.67181504 <a title="398-lsi-4" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>Author: Yu-Tseh Chi, Mohsen Ali, Muhammad Rushdi, Jeffrey Ho</p><p>Abstract: This paper proposes a novel approach for sparse coding that further improves upon the sparse representation-based classification (SRC) framework. The proposed framework, Affine-Constrained Group Sparse Coding (ACGSC), extends the current SRC framework to classification problems with multiple input samples. Geometrically, the affineconstrained group sparse coding essentially searches for the vector in the convex hull spanned by the input vectors that can best be sparse coded using the given dictionary. The resulting objectivefunction is still convex and can be efficiently optimized using iterative block-coordinate descent scheme that is guaranteed to converge. Furthermore, we provide a form of sparse recovery result that guarantees, at least theoretically, that the classification performance of the constrained group sparse coding should be at least as good as the group sparse coding. We have evaluated the proposed approach using three different recognition experiments that involve illumination variation of faces and textures, and face recognition under occlusions. Prelimi- nary experiments have demonstrated the effectiveness of the proposed approach, and in particular, the results from the recognition/occlusion experiment are surprisingly accurate and robust.</p><p>5 0.66084081 <a title="398-lsi-5" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>Author: Zhaowen Wang, Jianchao Yang, Nasser Nasrabadi, Thomas Huang</p><p>Abstract: Sparse Representation-based Classification (SRC) is a powerful tool in distinguishing signal categories which lie on different subspaces. Despite its wide application to visual recognition tasks, current understanding of SRC is solely based on a reconstructive perspective, which neither offers any guarantee on its classification performance nor provides any insight on how to design a discriminative dictionary for SRC. In this paper, we present a novel perspective towards SRC and interpret it as a margin classifier. The decision boundary and margin of SRC are analyzed in local regions where the support of sparse code is stable. Based on the derived margin, we propose a hinge loss function as the gauge for the classification performance of SRC. A stochastic gradient descent algorithm is implemented to maximize the margin of SRC and obtain more discriminative dictionaries. Experiments validate the effectiveness of the proposed approach in predicting classification performance and improving dictionary quality over reconstructive ones. Classification results competitive with other state-ofthe-art sparse coding methods are reported on several data sets.</p><p>6 0.66072309 <a title="398-lsi-6" href="./iccv-2013-Markov_Network-Based_Unified_Classifier_for_Face_Identification.html">261 iccv-2013-Markov Network-Based Unified Classifier for Face Identification</a></p>
<p>7 0.65085179 <a title="398-lsi-7" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>8 0.63721836 <a title="398-lsi-8" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>9 0.63055253 <a title="398-lsi-9" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>10 0.62993282 <a title="398-lsi-10" href="./iccv-2013-Face_Recognition_via_Archetype_Hull_Ranking.html">154 iccv-2013-Face Recognition via Archetype Hull Ranking</a></p>
<p>11 0.61493826 <a title="398-lsi-11" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>12 0.60736519 <a title="398-lsi-12" href="./iccv-2013-Deep_Learning_Identity-Preserving_Face_Space.html">106 iccv-2013-Deep Learning Identity-Preserving Face Space</a></p>
<p>13 0.60711765 <a title="398-lsi-13" href="./iccv-2013-Random_Faces_Guided_Sparse_Many-to-One_Encoder_for_Pose-Invariant_Face_Recognition.html">335 iccv-2013-Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition</a></p>
<p>14 0.59647799 <a title="398-lsi-14" href="./iccv-2013-Hidden_Factor_Analysis_for_Age_Invariant_Face_Recognition.html">195 iccv-2013-Hidden Factor Analysis for Age Invariant Face Recognition</a></p>
<p>15 0.57224959 <a title="398-lsi-15" href="./iccv-2013-A_Generalized_Iterated_Shrinkage_Algorithm_for_Non-convex_Sparse_Coding.html">14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</a></p>
<p>16 0.55071014 <a title="398-lsi-16" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<p>17 0.54047102 <a title="398-lsi-17" href="./iccv-2013-Coupled_Dictionary_and_Feature_Space_Learning_with_Applications_to_Cross-Domain_Image_Synthesis_and_Recognition.html">96 iccv-2013-Coupled Dictionary and Feature Space Learning with Applications to Cross-Domain Image Synthesis and Recognition</a></p>
<p>18 0.53335565 <a title="398-lsi-18" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>19 0.53173065 <a title="398-lsi-19" href="./iccv-2013-Dictionary_Learning_and_Sparse_Coding_on_Grassmann_Manifolds%3A_An_Extrinsic_Solution.html">114 iccv-2013-Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution</a></p>
<p>20 0.51445895 <a title="398-lsi-20" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.061), (7, 0.019), (12, 0.011), (26, 0.085), (31, 0.033), (42, 0.155), (48, 0.023), (62, 0.236), (64, 0.036), (73, 0.038), (78, 0.018), (89, 0.125), (95, 0.011), (97, 0.026), (98, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78247839 <a title="398-lda-1" href="./iccv-2013-Sparse_Variation_Dictionary_Learning_for_Face_Recognition_with_a_Single_Training_Sample_per_Person.html">398 iccv-2013-Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person</a></p>
<p>Author: Meng Yang, Luc Van_Gool, Lei Zhang</p><p>Abstract: Face recognition (FR) with a single training sample per person (STSPP) is a very challenging problem due to the lack of information to predict the variations in the query sample. Sparse representation based classification has shown interesting results in robust FR; however, its performance will deteriorate much for FR with STSPP. To address this issue, in this paper we learn a sparse variation dictionary from a generic training set to improve the query sample representation by STSPP. Instead of learning from the generic training set independently w.r.t. the gallery set, the proposed sparse variation dictionary learning (SVDL) method is adaptive to the gallery set by jointly learning a projection to connect the generic training set with the gallery set. The learnt sparse variation dictionary can be easily integrated into the framework of sparse representation based classification so that various variations in face images, including illumination, expression, occlusion, pose, etc., can be better handled. Experiments on the large-scale CMU Multi-PIE, FRGC and LFW databases demonstrate the promising performance of SVDL on FR with STSPP.</p><p>2 0.77996969 <a title="398-lda-2" href="./iccv-2013-Curvature-Aware_Regularization_on_Riemannian_Submanifolds.html">100 iccv-2013-Curvature-Aware Regularization on Riemannian Submanifolds</a></p>
<p>Author: Kwang In Kim, James Tompkin, Christian Theobalt</p><p>Abstract: One fundamental assumption in object recognition as well as in other computer vision and pattern recognition problems is that the data generation process lies on a manifold and that it respects the intrinsic geometry of the manifold. This assumption is held in several successful algorithms for diffusion and regularization, in particular, in graph-Laplacian-based algorithms. We claim that the performance of existing algorithms can be improved if we additionally account for how the manifold is embedded within the ambient space, i.e., if we consider the extrinsic geometry of the manifold. We present a procedure for characterizing the extrinsic (as well as intrinsic) curvature of a manifold M which is described by a sampled point cloud in a high-dimensional Euclidean space. Once estimated, we use this characterization in general diffusion and regularization on M, and form a new regularizer on a point cloud. The resulting re-weighted graph Laplacian demonstrates superior performance over classical graph Laplacian in semisupervised learning and spectral clustering.</p><p>3 0.76228034 <a title="398-lda-3" href="./iccv-2013-Automatic_Kronecker_Product_Model_Based_Detection_of_Repeated_Patterns_in_2D_Urban_Images.html">55 iccv-2013-Automatic Kronecker Product Model Based Detection of Repeated Patterns in 2D Urban Images</a></p>
<p>Author: Juan Liu, Emmanouil Psarakis, Ioannis Stamos</p><p>Abstract: Repeated patterns (such as windows, tiles, balconies and doors) are prominent and significant features in urban scenes. Therefore, detection of these repeated patterns becomes very important for city scene analysis. This paper attacks the problem of repeated patterns detection in a precise, efficient and automatic way, by combining traditional feature extraction followed by a Kronecker product lowrank modeling approach. Our method is tailored for 2D images of building fac ¸ades. We have developed algorithms for automatic selection ofa representative texture withinfa ¸cade images using vanishing points and Harris corners. After rectifying the input images, we describe novel algorithms that extract repeated patterns by using Kronecker product based modeling that is based on a solid theoretical foundation. Our approach is unique and has not ever been used for fac ¸ade analysis. We have tested our algorithms in a large set of images.</p><p>4 0.72863966 <a title="398-lda-4" href="./iccv-2013-Pose-Free_Facial_Landmark_Fitting_via_Optimized_Part_Mixtures_and_Cascaded_Deformable_Shape_Model.html">321 iccv-2013-Pose-Free Facial Landmark Fitting via Optimized Part Mixtures and Cascaded Deformable Shape Model</a></p>
<p>Author: Xiang Yu, Junzhou Huang, Shaoting Zhang, Wang Yan, Dimitris N. Metaxas</p><p>Abstract: This paper addresses the problem of facial landmark localization and tracking from a single camera. We present a two-stage cascaded deformable shape model to effectively and efficiently localize facial landmarks with large head pose variations. For face detection, we propose a group sparse learning method to automatically select the most salient facial landmarks. By introducing 3D face shape model, we use procrustes analysis to achieve pose-free facial landmark initialization. For deformation, the first step uses mean-shift local search with constrained local model to rapidly approach the global optimum. The second step uses component-wise active contours to discriminatively refine the subtle shape variation. Our framework can simultaneously handle face detection, pose-free landmark localization and tracking in real time. Extensive experiments are conducted on both laboratory environmental face databases and face-in-the-wild databases. All results demonstrate that our approach has certain advantages over state-of-theart methods in handling pose variations1.</p><p>5 0.69112182 <a title="398-lda-5" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>Author: Chenglong Bao, Jian-Feng Cai, Hui Ji</p><p>Abstract: In recent years, how to learn a dictionary from input images for sparse modelling has been one very active topic in image processing and recognition. Most existing dictionary learning methods consider an over-complete dictionary, e.g. the K-SVD method. Often they require solving some minimization problem that is very challenging in terms of computational feasibility and efficiency. However, if the correlations among dictionary atoms are not well constrained, the redundancy of the dictionary does not necessarily improve the performance of sparse coding. This paper proposed a fast orthogonal dictionary learning method for sparse image representation. With comparable performance on several image restoration tasks, the proposed method is much more computationally efficient than the over-complete dictionary based learning methods.</p><p>6 0.68435937 <a title="398-lda-6" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>7 0.68240803 <a title="398-lda-7" href="./iccv-2013-Adapting_Classification_Cascades_to_New_Domains.html">44 iccv-2013-Adapting Classification Cascades to New Domains</a></p>
<p>8 0.67851055 <a title="398-lda-8" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>9 0.67804277 <a title="398-lda-9" href="./iccv-2013-Proportion_Priors_for_Image_Sequence_Segmentation.html">330 iccv-2013-Proportion Priors for Image Sequence Segmentation</a></p>
<p>10 0.67724454 <a title="398-lda-10" href="./iccv-2013-Space-Time_Tradeoffs_in_Photo_Sequencing.html">397 iccv-2013-Space-Time Tradeoffs in Photo Sequencing</a></p>
<p>11 0.67699564 <a title="398-lda-11" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>12 0.67653668 <a title="398-lda-12" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>13 0.67493749 <a title="398-lda-13" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>14 0.67483729 <a title="398-lda-14" href="./iccv-2013-Correlation_Adaptive_Subspace_Segmentation_by_Trace_Lasso.html">93 iccv-2013-Correlation Adaptive Subspace Segmentation by Trace Lasso</a></p>
<p>15 0.67481059 <a title="398-lda-15" href="./iccv-2013-Collaborative_Active_Learning_of_a_Kernel_Machine_Ensemble_for_Recognition.html">80 iccv-2013-Collaborative Active Learning of a Kernel Machine Ensemble for Recognition</a></p>
<p>16 0.6744976 <a title="398-lda-16" href="./iccv-2013-Multi-channel_Correlation_Filters.html">277 iccv-2013-Multi-channel Correlation Filters</a></p>
<p>17 0.67322963 <a title="398-lda-17" href="./iccv-2013-A_Generalized_Iterated_Shrinkage_Algorithm_for_Non-convex_Sparse_Coding.html">14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</a></p>
<p>18 0.67116344 <a title="398-lda-18" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>19 0.67067885 <a title="398-lda-19" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>20 0.6704548 <a title="398-lda-20" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
