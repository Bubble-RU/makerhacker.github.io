<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>371 iccv-2013-Saliency Detection via Absorbing Markov Chain</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-371" href="#">iccv2013-371</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>371 iccv-2013-Saliency Detection via Absorbing Markov Chain</h1>
<br/><p>Source: <a title="iccv-2013-371-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Jiang_Saliency_Detection_via_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Bowen Jiang, Lihe Zhang, Huchuan Lu, Chuan Yang, Ming-Hsuan Yang</p><p>Abstract: In this paper, we formulate saliency detection via absorbing Markov chain on an image graph model. We jointly consider the appearance divergence and spatial distribution of salient objects and the background. The virtual boundary nodes are chosen as the absorbing nodes in a Markov chain and the absorbed time from each transient node to boundary absorbing nodes is computed. The absorbed time of transient node measures its global similarity with all absorbing nodes, and thus salient objects can be consistently separated from the background when the absorbed time is used as a metric. Since the time from transient node to absorbing nodes relies on the weights on the path and their spatial distance, the background region on the center of image may be salient. We further exploit the equilibrium distribution in an ergodic Markov chain to reduce the absorbed time in the long-range smooth background regions. Extensive experiments on four benchmark datasets demonstrate robustness and efficiency of the proposed method against the state-of-the-art methods.</p><p>Reference: <a title="iccv-2013-371-reference" href="../iccv2013_reference/iccv-2013-Saliency_Detection_via_Absorbing_Markov_Chain_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Saliency Detection via Absorbing Markov Chain Bowen Jiang1, Lihe Zhang1, Huchuan Lu1, Chuan Yang1, and Ming-Hsuan Yang2  1Dalian  University of Technology  Abstract In this paper, we formulate saliency detection via absorbing Markov chain on an image graph model. [sent-1, score-1.256]
</p><p>2 The virtual boundary nodes are chosen as the absorbing nodes in a Markov chain and the absorbed time from each transient node to boundary absorbing nodes is computed. [sent-3, score-2.927]
</p><p>3 The absorbed time of transient node measures its global similarity with all absorbing nodes, and thus salient objects can be consistently separated from the background when the absorbed time is used as a metric. [sent-4, score-1.8]
</p><p>4 Since the time from transient node to absorbing nodes relies on the weights on the path and their spatial distance, the background region on the center of image may be salient. [sent-5, score-1.31]
</p><p>5 We further exploit the equilibrium distribution in an ergodic Markov chain to reduce the absorbed time in the long-range smooth background regions. [sent-6, score-0.886]
</p><p>6 All bottom-up saliency methods rely on some prior knowledge about salient objects and backgrounds, such as contrast, compactness, etc. [sent-12, score-0.563]
</p><p>7 Different saliency methods characterize the prior knowledge from different perspectives. [sent-13, score-0.413]
</p><p>8 Fourier spectrum analysis has also been used to detect visual saliency [15, 13]. [sent-20, score-0.398]
</p><p>9 [25] unify the contrast and saliency computation into a single high-dimensional Gaussian filtering framework. [sent-22, score-0.398]
</p><p>10 [33] exploit background priors and geodesic distance  for saliency detection. [sent-24, score-0.465]
</p><p>11 [35] cast saliency detection into a graph-based ranking problem, which performs label propagation on a sparsely connected graph to characterize the overall differences between salient object and background. [sent-26, score-0.735]
</p><p>12 In this work, we reconsider the properties ofMarkov random walks and their relationship with saliency detection. [sent-27, score-0.421]
</p><p>13 Existing random walk based methods consistently use the equilibrium distribution in an ergodic Markov chain [9, 14] or its extensions, e. [sent-28, score-0.596]
</p><p>14 Typically, saliency measure using the hitting time often highlights some particular small regions in objects or backgrounds. [sent-32, score-0.587]
</p><p>15 In addition, equilibrium distribution based saliency models only highlight the boundaries of salient object while object interior still has low saliency value. [sent-33, score-1.251]
</p><p>16 To address these issues, we investigate the properties of absorbing Markov chains in this work. [sent-34, score-0.633]
</p><p>17 Given an image graph as Markov chain and some absorbing nodes, we compute the expected time to absorption (i. [sent-35, score-0.939]
</p><p>18 large transition probabilities) and small spatial distance to absorbing nodes can be absorbed faster. [sent-40, score-1.226]
</p><p>19 As salient objects seldom occupy all four image boundaries [33, 5] and the background regions often have appearance connectivity with image boundaries, when we use the boundary nodes as absorbing nodes, the random walk starting in background nodes can easily reach  the absorbing nodes. [sent-41, score-2.253]
</p><p>20 While object regions often have great contrast to the image background, it is difficult for a random walk from object nodes to reach these absorbing nodes (represented by boundary nodes). [sent-42, score-1.318]
</p><p>21 Hence, the absorbed time starting from object nodes is longer than that from background nodes. [sent-43, score-0.644]
</p><p>22 In addition, in a long run, the absorbed time with similar starting nodes is roughly the same. [sent-44, score-0.576]
</p><p>23 From left to right: input image with superpixels as nodes; the minimum hitting time of each node to all boundary nodes in ergodic Markov chain; the absorbed time of each node into all boundary nodes in absorbing Markov chain. [sent-47, score-2.03]
</p><p>24 Each kind of time is normalized as a saliency map respectively. [sent-48, score-0.46]
</p><p>25 by these observations, we formulate saliency detection as a random walk problem in the absorbing Markov chain. [sent-49, score-1.115]
</p><p>26 The absorbed time is not always effective especially when there are long-range smooth background regions near the image center. [sent-50, score-0.411]
</p><p>27 We further explore the effect of the equi-  librium probability in saliency detection, and exploit it to regulate the absorbed time, thereby suppressing the saliency of this kind of regions. [sent-51, score-1.173]
</p><p>28 Related Work Previous works that simulate saliency detection in random walk model include [9, 14, 11, 3 1]. [sent-53, score-0.482]
</p><p>29 [9] identify the saliency region based on the frequency of visits to each node at the equilibrium of the random walk. [sent-55, score-0.713]
</p><p>30 [11], which exploits the hitting time on the fully connected graph and the sparsely connected graph to find the most salient seed, based on which some background seed-  s are determined again. [sent-63, score-0.526]
</p><p>31 They then use the difference of the hitting times to the two kinds of seeds to compute the saliency for each node. [sent-64, score-0.522]
</p><p>32 While they alleviate the problem of using the equilibrium distribution to measure saliency, the identification of the salient seed is difficult, especially for the scenes with complex salient objects. [sent-65, score-0.581]
</p><p>33 More importantly, the hitting time based saliency measure prefers to highlight the global rare regions and does not suppress the backgrounds very well, thereby decreasing the overall saliency of objects (See Figure 1). [sent-66, score-1.032]
</p><p>34 The hitting time is the expected time taken to reach a node if the Markov chain is started in another node. [sent-68, score-0.48]
</p><p>35 The ergodic Markov chain doesn’t have a mechanism that can synthetically consider the relationships between a node and multiple specific nodes (e. [sent-69, score-0.651]
</p><p>36 Different from the above methods, we consider the absorbing Markov random walk, which includes two kinds of nodes (i. [sent-74, score-0.881]
</p><p>37 For an absorbing chain started in a transient node, the probability of absorption in an absorbing node indicates the relationship between the two nodes, and the absorption time therefore implicates the selective relationships between this transient node and all the absorbing nodes. [sent-77, score-2.868]
</p><p>38 Since the boundary nodes usually contain the global  ×  characteristics of the image background, by using them as absorbing nodes, the absorbed time of each transient node can reflect its overall similarity with the background, which helps to distinguish salient nodes from background nodes. [sent-78, score-2.023]
</p><p>39 Different from [9, 14] which directly use the equilibrium distribution to simulate human attention, we exploit it to weigh the absorbed time, thereby suppressing the saliency of long-range background regions with homogeneous appearance. [sent-80, score-1.07]
</p><p>40 Absorbing Markov Chain The state 푠푖 is absorbing when 푝푖푖 = 1, which means 푝푖푗 = 0 for all 푗. [sent-90, score-0.684]
</p><p>41 A Markov chain is absorbing if it has at le=ast 0 one absorbing Ast Matea. [sent-91, score-1.43]
</p><p>42 r kIto ivs possible btos go ifnrgom if every  푖=  ××  transient state to some absorbing state, not necessarily in one step. [sent-92, score-0.911]
</p><p>43 n Q any pair o]ftransient states, while R ∈ [0, 1]푡×푟 ctioenstbaientws teheen probabilities of moving tefrso,m w any tRran ∈si [e0n,t1 s]tate to any absorbing state. [sent-95, score-0.656]
</p><p>44 Thus, we can compute th∑e absorbed time for each transient state, that is, y =N  c,  (2)  where c is a 푡 dimensional column vector all of whose ele-  ments are 1. [sent-99, score-0.536]
</p><p>45 An ergodic chain with any starting state always reaches equilibrium after a certain time, and the equilibrium state is characterized by the equilibrium distribution 휋, which satisfies the equation 휋P =  휋,  (3)  where P is the ergodic transition matrix. [sent-103, score-1.255]
</p><p>46 Saliency Measure Given an input image represented as a Markov chain and some background absorbing states, the saliency of each transient state is defined as the expected number of times  side the yellow bounding box are the duplicated boundary superpixels, which are used as the absorbing nodes. [sent-113, score-2.255]
</p><p>47 before being absorbed into all absorbing nodes by Eq 2. [sent-114, score-1.158]
</p><p>48 Because we compute the full resolution saliency map, some virtual nodes are added to the graph as absorbing states, which is detailed in the next section. [sent-116, score-1.329]
</p><p>49 In the conventional absorbing Markov chain problems, the absorbing nodes are manually labelled with the groundtruth. [sent-117, score-1.678]
</p><p>50 However, as absorbing nodes for saliency detection are selected by the proposed algorithm, some of them may be incorrect. [sent-118, score-1.311]
</p><p>51 Graph Construction We construct a single layer graph 퐺(푉, 퐸) with superpixels [3] as nodes 푉 and the links between pairs of nodes as edges 퐸. [sent-121, score-0.551]
</p><p>52 Because the salient objects seldom occupy all image borders [33], we duplicate the boundary superpixels around the image borders as the virtual background absorbing nodes, as shown in Figure 2. [sent-122, score-1.03]
</p><p>53 On this graph, each node (transient or absorbing) is connected to the transient nodes which neighbour it or share common boundaries with its neighbouring nodes. [sent-123, score-0.631]
</p><p>54 That means that any pair of absorbing nodes are unconnected. [sent-124, score-0.881]
</p><p>55 In addition, we enforce that all the transient nodes around the image borders (i. [sent-125, score-0.5]
</p><p>56 In this work, the weight 푤푖푗 of the edge 푒푖푗 between adjacent nodes and 푗 is defined as  푖  푤푖푗  =  푒−∥푥푖휎−2푥푗∥,  (5)  where 푥푖 and 푥푗 are the mean of two nodes in the CIE LAB color space, and 휎 is a constant that controls the strength of 11666677  ×  the weight. [sent-129, score-0.496]
</p><p>57 We first renumber the nodes so that the first 푡 nodes are transient nodes and the last 푟 nodes are absorbing nodes, then define the affinity matrix A which represents the reverence of nodes as  푎푖푗=⎨⎧푤10푖푗 푗oifth∈푖e=r푁w푗(i푖s)e,1≤푖≤푡  (6)  where 푁(푖) deno⎩tes the nodes connected to node 푖. [sent-130, score-2.55]
</p><p>58 The sparsely connected graph restricts the random walk to only move within a local region in each step, hence the expected time spent to move from transient node 푣푡 to absorbing node 푣푎 is determined by two major factors. [sent-134, score-1.29]
</p><p>59 Then, we obtain the saliency map S by normalizing the absorbed time y computed by Eq. [sent-144, score-0.707]
</p><p>60 , 푡,  (9)  where 푖 indexes the transient nodes on graph, and y denotes the normalized absorbed time vector. [sent-148, score-0.804]
</p><p>61 Most saliency maps generated by the normalized absorbed time y are effective, but some background nodes near the image center may not be adequately suppressed when they are in long-range homogeneous region, as shown in Figure 3. [sent-149, score-1.092]
</p><p>62 Most nodes in this kind of background regions have large transition probabilities, which means that the random walk may transfer many times among these nodes before reaching the  ×  Figure 3. [sent-151, score-0.728]
</p><p>63 The sparse connectivity of the graph results that the background nodes near the image center have longer absorbed time than the similar nodes near the image boundaries. [sent-155, score-0.924]
</p><p>64 Consequently, the background regions near the image center possibly present comparative saliency with salient objects, thereby decreasing the contrast of objects and backgrounds in the resulted saliency maps. [sent-156, score-1.118]
</p><p>65 To alleviate this problem, we update the saliency map by using a weighted absorbed time yw, which can be denoted as: yw = N  u,  (10)  where u is the weighting column vector. [sent-157, score-0.74]
</p><p>66 The equilibrium distribution 휋 for the ergodic Markov chain can be computed from the affinity matrix A as  휋푖=∑∑푖푗푗푎푎푖푖푗푗,  (11)  where 푖, 푗 index all the transi∑ent nodes. [sent-159, score-0.567]
</p><p>67 By the update processing, the saliency of the long-range homogeneous regions near the image center can be suppressed as Figure 3 illustrates. [sent-167, score-0.5]
</p><p>68 However, if the kind of region belongs to salient object, its saliency will be also incorrectly suppressed. [sent-168, score-0.623]
</p><p>69 From top to down: input images, our saliency maps. [sent-171, score-0.398]
</p><p>70 We find that object regions have great global contrast to background regions in good saliency maps, while it is not the case in the defective maps as the examples in Figure 3, which consistently contain a number of regions with mid-level saliency. [sent-173, score-0.579]
</p><p>71 Hence, given a saliency map, we first calculate its gray histogram g with ten bins, and then define a metric 푠푐표푟푒 to characterize this kind of tendency as follows: ∑10  푠푐표푟푒 =  ∑푔(푏) 푏∑= ∑1  min(푏,(11 − 푏)),  (13)  where 푏 indexes all the bins. [sent-174, score-0.463]
</p><p>72 The larger 푠푐표푟푒 means that there are longer-range regions with mid-level saliency in the saliency map. [sent-175, score-0.828]
</p><p>73 It should be noted that the absorbing nodes may include object nodes when the salient objects touch the image boundaries, as shown in Figure 4. [sent-176, score-1.312]
</p><p>74 These imprecise background absorbing nodes may result that the object regions close to the boundary are suppressed. [sent-177, score-1.034]
</p><p>75 However, the absorbed time considers the effect of all boundary nodes and depends on two factors: the edge weights on the path and the spatial distance, so the parts of object which are far from or different from the boundary absorbing nodes can be highlighted correctly. [sent-178, score-1.562]
</p><p>76 Construct a graph 퐺 with superpixels as nodes, and use boundary nodes as absorbing nodes; 2. [sent-182, score-0.989]
</p><p>77 10 and 9; Output: the full resolution saliency map. [sent-194, score-0.398]
</p><p>78 We compare our method with fifteen state-of-the-art saliency detection algorithms: the IT [16], MZ [20], LC [37], GB [14], SR [15], AC [1], FT [2], SER [31], CA [27], RC [8], CB [17], SVO [7], SF [25], LR [29] and GS [33] methods. [sent-206, score-0.461]
</p><p>79 First, we bisegment the saliency map using every threshold in the range [0 : 0. [sent-220, score-0.398]
</p><p>80 Second, we compute the precision, recall and F-measure with an adaptive threshold proposed in [2], which is defined as twice the mean saliency of the image. [sent-222, score-0.423]
</p><p>81 The two evaluation criteria consistently show the proposed method outperforms all the other methods, where the CB [17], SVO [7], RC [8] and CA [27] are top-performance methods for saliency detection in a recent benchmark study [5]. [sent-245, score-0.447]
</p><p>82 5 as salient region and fit a bounding box in the salient region. [sent-251, score-0.377]
</p><p>83 Similar as previous works, we first fit a rectangle to the binary saliency map and then use the bounding box to compute precision, recall and F-measure. [sent-253, score-0.44]
</p><p>84 That is because the background is suppressed badly, the cut saliency map contains almost the entire image with low precision. [sent-258, score-0.471]
</p><p>85 That is because our method usually highlights one of two objects while the other has low saliency values due to the appearance diversity of two objects. [sent-262, score-0.424]
</p><p>86 We can see that the post-process step improves the precision and recall significantly over the solely saliency measure by absorbed time. [sent-264, score-0.722]
</p><p>87 Due to scrambled backgrounds and heterogeneous foregrounds most images have, and the lack of top-down prior knowledge, the overall performance of the existing bottom-up saliency detection methods is low on this dataset. [sent-267, score-0.458]
</p><p>88 Failure Case: Our approach exploits the boundary prior to determine the absorbing nodes, therefore the small salient object touching image boundaries may be incorrectly suppressed. [sent-268, score-0.891]
</p><p>89 Failure examples time, a node with sharp contrast to its surroundings often has abnormally large absorbed time, which results that most parts of object even the whole object are suppressed. [sent-287, score-0.403]
</p><p>90 Conclusion In this paper, we propose a bottom-up saliency detection algorithm by using the time property in an absorbing Markov chain. [sent-303, score-1.095]
</p><p>91 Based on the boundary prior, we set the virtual boundary nodes as absorbing nodes. [sent-304, score-1.008]
</p><p>92 The saliency of each node is computed as its absorbed time to absorb-  ing nodes. [sent-305, score-0.797]
</p><p>93 Furthermore,  we exploit the equilibrium dis-  tribution in ergodic Markov chain to weigh the absorbed time, thereby suppressing the saliency in long-range background regions. [sent-306, score-1.305]
</p><p>94 Boosting bottom-up and top-down visual features for saliency estimation. [sent-342, score-0.398]
</p><p>95 Fusing generic objectness  [8] [9] [10] [11] [12] [13] [14] [15] [16] [17]  [18] [19] [20] [21]  and visual saliency for salient object detection. [sent-361, score-0.581]
</p><p>96 Visual saliency and attention as random walks on complex networks. [sent-376, score-0.442]
</p><p>97 Spatio-temporal saliency detection using phase spectrum of quaternion fourier transform. [sent-401, score-0.43]
</p><p>98 A framework for visual saliency detection with applications to image thumbnailing. [sent-454, score-0.43]
</p><p>99 Improved saliency detection based on superpixel clustering and saliency propagation. [sent-504, score-0.828]
</p><p>100 Top-down visual saliency via joint crf and dictionary learning. [sent-575, score-0.398]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('absorbing', 0.633), ('saliency', 0.398), ('absorbed', 0.277), ('nodes', 0.248), ('transient', 0.227), ('equilibrium', 0.195), ('salient', 0.165), ('chain', 0.164), ('ergodic', 0.149), ('markov', 0.099), ('hitting', 0.099), ('node', 0.09), ('transition', 0.068), ('msra', 0.065), ('svo', 0.065), ('states', 0.061), ('sod', 0.059), ('recurrent', 0.054), ('boundary', 0.053), ('walk', 0.052), ('absorption', 0.052), ('state', 0.051), ('background', 0.05), ('connected', 0.044), ('asd', 0.041), ('sed', 0.041), ('cb', 0.04), ('rc', 0.036), ('ser', 0.034), ('sparsely', 0.034), ('time', 0.032), ('regions', 0.032), ('detection', 0.032), ('execution', 0.032), ('fifteen', 0.031), ('lc', 0.031), ('kind', 0.03), ('region', 0.03), ('graph', 0.029), ('expected', 0.029), ('backgrounds', 0.028), ('renumber', 0.028), ('homogeneous', 0.027), ('thereby', 0.027), ('superpixels', 0.026), ('highlights', 0.026), ('suppressing', 0.026), ('lr', 0.025), ('borders', 0.025), ('seeds', 0.025), ('gopalakrishnan', 0.025), ('recall', 0.025), ('gb', 0.024), ('ca', 0.023), ('suppressed', 0.023), ('probabilities', 0.023), ('walks', 0.023), ('achanta', 0.022), ('boundaries', 0.022), ('precision', 0.022), ('sr', 0.021), ('attention', 0.021), ('virtual', 0.021), ('near', 0.02), ('indexes', 0.02), ('affinity', 0.02), ('matrix', 0.02), ('bruce', 0.02), ('eleven', 0.02), ('perazzi', 0.02), ('seed', 0.02), ('estrada', 0.019), ('weigh', 0.019), ('distribution', 0.019), ('starting', 0.019), ('fundamental', 0.018), ('harel', 0.018), ('object', 0.018), ('started', 0.018), ('ast', 0.018), ('highlight', 0.018), ('ft', 0.018), ('lu', 0.017), ('bounding', 0.017), ('probability', 0.017), ('alleviate', 0.017), ('geodesic', 0.017), ('adequately', 0.017), ('consistently', 0.017), ('yw', 0.016), ('expense', 0.016), ('site', 0.016), ('entropy', 0.016), ('gs', 0.016), ('seldom', 0.016), ('occupy', 0.016), ('reach', 0.016), ('zheng', 0.016), ('itti', 0.016), ('characterize', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="371-tfidf-1" href="./iccv-2013-Saliency_Detection_via_Absorbing_Markov_Chain.html">371 iccv-2013-Saliency Detection via Absorbing Markov Chain</a></p>
<p>Author: Bowen Jiang, Lihe Zhang, Huchuan Lu, Chuan Yang, Ming-Hsuan Yang</p><p>Abstract: In this paper, we formulate saliency detection via absorbing Markov chain on an image graph model. We jointly consider the appearance divergence and spatial distribution of salient objects and the background. The virtual boundary nodes are chosen as the absorbing nodes in a Markov chain and the absorbed time from each transient node to boundary absorbing nodes is computed. The absorbed time of transient node measures its global similarity with all absorbing nodes, and thus salient objects can be consistently separated from the background when the absorbed time is used as a metric. Since the time from transient node to absorbing nodes relies on the weights on the path and their spatial distance, the background region on the center of image may be salient. We further exploit the equilibrium distribution in an ergodic Markov chain to reduce the absorbed time in the long-range smooth background regions. Extensive experiments on four benchmark datasets demonstrate robustness and efficiency of the proposed method against the state-of-the-art methods.</p><p>2 0.38807967 <a title="371-tfidf-2" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>Author: Yangqing Jia, Mei Han</p><p>Abstract: It is known that purely low-level saliency cues such as frequency does not lead to a good salient object detection result, requiring high-level knowledge to be adopted for successful discovery of task-independent salient objects. In this paper, we propose an efficient way to combine such high-level saliency priors and low-level appearance models. We obtain the high-level saliency prior with the objectness algorithm to find potential object candidates without the need of category information, and then enforce the consistency among the salient regions using a Gaussian MRF with the weights scaled by diverse density that emphasizes the influence of potential foreground pixels. Our model obtains saliency maps that assign high scores for the whole salient object, and achieves state-of-the-art performance on benchmark datasets covering various foreground statistics.</p><p>3 0.37488616 <a title="371-tfidf-3" href="./iccv-2013-Saliency_Detection_via_Dense_and_Sparse_Reconstruction.html">372 iccv-2013-Saliency Detection via Dense and Sparse Reconstruction</a></p>
<p>Author: Xiaohui Li, Huchuan Lu, Lihe Zhang, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: In this paper, we propose a visual saliency detection algorithm from the perspective of reconstruction errors. The image boundaries are first extracted via superpixels as likely cues for background templates, from which dense and sparse appearance models are constructed. For each image region, we first compute dense and sparse reconstruction errors. Second, the reconstruction errors are propagated based on the contexts obtained from K-means clustering. Third, pixel-level saliency is computed by an integration of multi-scale reconstruction errors and refined by an object-biased Gaussian model. We apply the Bayes formula to integrate saliency measures based on dense and sparse reconstruction errors. Experimental results show that the proposed algorithm performs favorably against seventeen state-of-the-art methods in terms of precision and recall. In addition, the proposed algorithm is demonstrated to be more effective in highlighting salient objects uniformly and robust to background noise.</p><p>4 0.3198936 <a title="371-tfidf-4" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>Author: Xi Li, Yao Li, Chunhua Shen, Anthony Dick, Anton Van_Den_Hengel</p><p>Abstract: Salient object detection aims to locate objects that capture human attention within images. Previous approaches often pose this as a problem of image contrast analysis. In this work, we model an image as a hypergraph that utilizes a set of hyperedges to capture the contextual properties of image pixels or regions. As a result, the problem of salient object detection becomes one of finding salient vertices and hyperedges in the hypergraph. The main advantage of hypergraph modeling is that it takes into account each pixel’s (or region ’s) affinity with its neighborhood as well as its separation from image background. Furthermore, we propose an alternative approach based on centerversus-surround contextual contrast analysis, which performs salient object detection by optimizing a cost-sensitive support vector machine (SVM) objective function. Experimental results on four challenging datasets demonstrate the effectiveness of the proposed approaches against the stateof-the-art approaches to salient object detection.</p><p>5 0.25613838 <a title="371-tfidf-5" href="./iccv-2013-Analysis_of_Scores%2C_Datasets%2C_and_Models_in_Visual_Saliency_Prediction.html">50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</a></p>
<p>Author: Ali Borji, Hamed R. Tavakoli, Dicky N. Sihite, Laurent Itti</p><p>Abstract: Significant recent progress has been made in developing high-quality saliency models. However, less effort has been undertaken on fair assessment of these models, over large standardized datasets and correctly addressing confounding factors. In this study, we pursue a critical and quantitative look at challenges (e.g., center-bias, map smoothing) in saliency modeling and the way they affect model accuracy. We quantitatively compare 32 state-of-the-art models (using the shuffled AUC score to discount center-bias) on 4 benchmark eye movement datasets, for prediction of human fixation locations and scanpath sequence. We also account for the role of map smoothing. We find that, although model rankings vary, some (e.g., AWS, LG, AIM, and HouNIPS) consistently outperform other models over all datasets. Some models work well for prediction of both fixation locations and scanpath sequence (e.g., Judd, GBVS). Our results show low prediction accuracy for models over emotional stimuli from the NUSEF dataset. Our last benchmark, for the first time, gauges the ability of models to decode the stimulus category from statistics of fixations, saccades, and model saliency values at fixated locations. In this test, ITTI and AIM models win over other models. Our benchmark provides a comprehensive high-level picture of the strengths and weaknesses of many popular models, and suggests future research directions in saliency modeling.</p><p>6 0.2455876 <a title="371-tfidf-6" href="./iccv-2013-Saliency_and_Human_Fixations%3A_State-of-the-Art_and_Study_of_Comparison_Metrics.html">373 iccv-2013-Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics</a></p>
<p>7 0.23150857 <a title="371-tfidf-7" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>8 0.21899962 <a title="371-tfidf-8" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>9 0.21519157 <a title="371-tfidf-9" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<p>10 0.19749604 <a title="371-tfidf-10" href="./iccv-2013-Saliency_Detection_in_Large_Point_Sets.html">370 iccv-2013-Saliency Detection in Large Point Sets</a></p>
<p>11 0.18096289 <a title="371-tfidf-11" href="./iccv-2013-Saliency_Detection%3A_A_Boolean_Map_Approach.html">369 iccv-2013-Saliency Detection: A Boolean Map Approach</a></p>
<p>12 0.17951041 <a title="371-tfidf-12" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>13 0.15797094 <a title="371-tfidf-13" href="./iccv-2013-Semantically-Based_Human_Scanpath_Estimation_with_HMMs.html">381 iccv-2013-Semantically-Based Human Scanpath Estimation with HMMs</a></p>
<p>14 0.090054773 <a title="371-tfidf-14" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<p>15 0.083573863 <a title="371-tfidf-15" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>16 0.080443218 <a title="371-tfidf-16" href="./iccv-2013-Discriminative_Label_Propagation_for_Multi-object_Tracking_with_Sporadic_Appearance_Features.html">120 iccv-2013-Discriminative Label Propagation for Multi-object Tracking with Sporadic Appearance Features</a></p>
<p>17 0.079407729 <a title="371-tfidf-17" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>18 0.077986412 <a title="371-tfidf-18" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>19 0.073515303 <a title="371-tfidf-19" href="./iccv-2013-Improving_Graph_Matching_via_Density_Maximization.html">214 iccv-2013-Improving Graph Matching via Density Maximization</a></p>
<p>20 0.068806827 <a title="371-tfidf-20" href="./iccv-2013-GrabCut_in_One_Cut.html">186 iccv-2013-GrabCut in One Cut</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.14), (1, -0.035), (2, 0.41), (3, -0.199), (4, -0.1), (5, 0.011), (6, 0.008), (7, -0.016), (8, 0.037), (9, -0.022), (10, -0.065), (11, 0.011), (12, 0.009), (13, 0.054), (14, 0.06), (15, 0.04), (16, 0.093), (17, -0.062), (18, -0.044), (19, 0.11), (20, -0.031), (21, 0.008), (22, 0.024), (23, 0.01), (24, 0.043), (25, -0.042), (26, -0.034), (27, -0.044), (28, 0.004), (29, -0.009), (30, 0.029), (31, 0.008), (32, 0.01), (33, -0.043), (34, 0.003), (35, 0.033), (36, -0.014), (37, -0.002), (38, -0.015), (39, -0.005), (40, 0.008), (41, -0.004), (42, 0.006), (43, -0.01), (44, -0.0), (45, 0.016), (46, 0.011), (47, 0.02), (48, 0.024), (49, -0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9639827 <a title="371-lsi-1" href="./iccv-2013-Saliency_Detection_via_Absorbing_Markov_Chain.html">371 iccv-2013-Saliency Detection via Absorbing Markov Chain</a></p>
<p>Author: Bowen Jiang, Lihe Zhang, Huchuan Lu, Chuan Yang, Ming-Hsuan Yang</p><p>Abstract: In this paper, we formulate saliency detection via absorbing Markov chain on an image graph model. We jointly consider the appearance divergence and spatial distribution of salient objects and the background. The virtual boundary nodes are chosen as the absorbing nodes in a Markov chain and the absorbed time from each transient node to boundary absorbing nodes is computed. The absorbed time of transient node measures its global similarity with all absorbing nodes, and thus salient objects can be consistently separated from the background when the absorbed time is used as a metric. Since the time from transient node to absorbing nodes relies on the weights on the path and their spatial distance, the background region on the center of image may be salient. We further exploit the equilibrium distribution in an ergodic Markov chain to reduce the absorbed time in the long-range smooth background regions. Extensive experiments on four benchmark datasets demonstrate robustness and efficiency of the proposed method against the state-of-the-art methods.</p><p>2 0.90698886 <a title="371-lsi-2" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>Author: Xi Li, Yao Li, Chunhua Shen, Anthony Dick, Anton Van_Den_Hengel</p><p>Abstract: Salient object detection aims to locate objects that capture human attention within images. Previous approaches often pose this as a problem of image contrast analysis. In this work, we model an image as a hypergraph that utilizes a set of hyperedges to capture the contextual properties of image pixels or regions. As a result, the problem of salient object detection becomes one of finding salient vertices and hyperedges in the hypergraph. The main advantage of hypergraph modeling is that it takes into account each pixel’s (or region ’s) affinity with its neighborhood as well as its separation from image background. Furthermore, we propose an alternative approach based on centerversus-surround contextual contrast analysis, which performs salient object detection by optimizing a cost-sensitive support vector machine (SVM) objective function. Experimental results on four challenging datasets demonstrate the effectiveness of the proposed approaches against the stateof-the-art approaches to salient object detection.</p><p>3 0.89801556 <a title="371-lsi-3" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>Author: Yangqing Jia, Mei Han</p><p>Abstract: It is known that purely low-level saliency cues such as frequency does not lead to a good salient object detection result, requiring high-level knowledge to be adopted for successful discovery of task-independent salient objects. In this paper, we propose an efficient way to combine such high-level saliency priors and low-level appearance models. We obtain the high-level saliency prior with the objectness algorithm to find potential object candidates without the need of category information, and then enforce the consistency among the salient regions using a Gaussian MRF with the weights scaled by diverse density that emphasizes the influence of potential foreground pixels. Our model obtains saliency maps that assign high scores for the whole salient object, and achieves state-of-the-art performance on benchmark datasets covering various foreground statistics.</p><p>4 0.87004042 <a title="371-lsi-4" href="./iccv-2013-Saliency_Detection%3A_A_Boolean_Map_Approach.html">369 iccv-2013-Saliency Detection: A Boolean Map Approach</a></p>
<p>Author: Jianming Zhang, Stan Sclaroff</p><p>Abstract: A novel Boolean Map based Saliency (BMS) model is proposed. An image is characterized by a set of binary images, which are generated by randomly thresholding the image ’s color channels. Based on a Gestalt principle of figure-ground segregation, BMS computes saliency maps by analyzing the topological structure of Boolean maps. BMS is simple to implement and efficient to run. Despite its simplicity, BMS consistently achieves state-of-the-art performance compared with ten leading methods on five eye tracking datasets. Furthermore, BMS is also shown to be advantageous in salient object detection.</p><p>5 0.86777949 <a title="371-lsi-5" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<p>Author: Peng Jiang, Haibin Ling, Jingyi Yu, Jingliang Peng</p><p>Abstract: The goal of saliency detection is to locate important pixels or regions in an image which attract humans ’ visual attention the most. This is a fundamental task whose output may serve as the basis for further computer vision tasks like segmentation, resizing, tracking and so forth. In this paper we propose a novel salient region detection algorithm by integrating three important visual cues namely uniqueness, focusness and objectness (UFO). In particular, uniqueness captures the appearance-derived visual contrast; focusness reflects the fact that salient regions are often photographed in focus; and objectness helps keep completeness of detected salient regions. While uniqueness has been used for saliency detection for long, it is new to integrate focusness and objectness for this purpose. In fact, focusness and objectness both provide important saliency information complementary of uniqueness. In our experiments using public benchmark datasets, we show that, even with a simple pixel level combination of the three components, the proposed approach yields significant improve- ment compared with previously reported methods.</p><p>6 0.84867561 <a title="371-lsi-6" href="./iccv-2013-Analysis_of_Scores%2C_Datasets%2C_and_Models_in_Visual_Saliency_Prediction.html">50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</a></p>
<p>7 0.84243959 <a title="371-lsi-7" href="./iccv-2013-Saliency_Detection_via_Dense_and_Sparse_Reconstruction.html">372 iccv-2013-Saliency Detection via Dense and Sparse Reconstruction</a></p>
<p>8 0.84216958 <a title="371-lsi-8" href="./iccv-2013-Saliency_and_Human_Fixations%3A_State-of-the-Art_and_Study_of_Comparison_Metrics.html">373 iccv-2013-Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics</a></p>
<p>9 0.80360764 <a title="371-lsi-9" href="./iccv-2013-Saliency_Detection_in_Large_Point_Sets.html">370 iccv-2013-Saliency Detection in Large Point Sets</a></p>
<p>10 0.79545099 <a title="371-lsi-10" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>11 0.6923849 <a title="371-lsi-11" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>12 0.5983296 <a title="371-lsi-12" href="./iccv-2013-Initialization-Insensitive_Visual_Tracking_through_Voting_with_Salient_Local_Features.html">217 iccv-2013-Initialization-Insensitive Visual Tracking through Voting with Salient Local Features</a></p>
<p>13 0.46647415 <a title="371-lsi-13" href="./iccv-2013-Semantically-Based_Human_Scanpath_Estimation_with_HMMs.html">381 iccv-2013-Semantically-Based Human Scanpath Estimation with HMMs</a></p>
<p>14 0.32681435 <a title="371-lsi-14" href="./iccv-2013-Discriminative_Label_Propagation_for_Multi-object_Tracking_with_Sporadic_Appearance_Features.html">120 iccv-2013-Discriminative Label Propagation for Multi-object Tracking with Sporadic Appearance Features</a></p>
<p>15 0.32637742 <a title="371-lsi-15" href="./iccv-2013-Weakly_Supervised_Learning_of_Image_Partitioning_Using_Decision_Trees_with_Structured_Split_Criteria.html">448 iccv-2013-Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria</a></p>
<p>16 0.30859855 <a title="371-lsi-16" href="./iccv-2013-Improving_Graph_Matching_via_Density_Maximization.html">214 iccv-2013-Improving Graph Matching via Density Maximization</a></p>
<p>17 0.30577755 <a title="371-lsi-17" href="./iccv-2013-Predicting_Primary_Gaze_Behavior_Using_Social_Saliency_Fields.html">325 iccv-2013-Predicting Primary Gaze Behavior Using Social Saliency Fields</a></p>
<p>18 0.29494211 <a title="371-lsi-18" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<p>19 0.28025758 <a title="371-lsi-19" href="./iccv-2013-GrabCut_in_One_Cut.html">186 iccv-2013-GrabCut in One Cut</a></p>
<p>20 0.28023845 <a title="371-lsi-20" href="./iccv-2013-Tree_Shape_Priors_with_Connectivity_Constraints_Using_Convex_Relaxation_on_General_Graphs.html">429 iccv-2013-Tree Shape Priors with Connectivity Constraints Using Convex Relaxation on General Graphs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.07), (7, 0.01), (12, 0.012), (13, 0.011), (19, 0.219), (26, 0.109), (31, 0.037), (40, 0.012), (42, 0.072), (64, 0.035), (73, 0.031), (89, 0.151), (97, 0.106)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77101403 <a title="371-lda-1" href="./iccv-2013-Saliency_Detection_via_Absorbing_Markov_Chain.html">371 iccv-2013-Saliency Detection via Absorbing Markov Chain</a></p>
<p>Author: Bowen Jiang, Lihe Zhang, Huchuan Lu, Chuan Yang, Ming-Hsuan Yang</p><p>Abstract: In this paper, we formulate saliency detection via absorbing Markov chain on an image graph model. We jointly consider the appearance divergence and spatial distribution of salient objects and the background. The virtual boundary nodes are chosen as the absorbing nodes in a Markov chain and the absorbed time from each transient node to boundary absorbing nodes is computed. The absorbed time of transient node measures its global similarity with all absorbing nodes, and thus salient objects can be consistently separated from the background when the absorbed time is used as a metric. Since the time from transient node to absorbing nodes relies on the weights on the path and their spatial distance, the background region on the center of image may be salient. We further exploit the equilibrium distribution in an ergodic Markov chain to reduce the absorbed time in the long-range smooth background regions. Extensive experiments on four benchmark datasets demonstrate robustness and efficiency of the proposed method against the state-of-the-art methods.</p><p>2 0.74952233 <a title="371-lda-2" href="./iccv-2013-Learning_Slow_Features_for_Behaviour_Analysis.html">243 iccv-2013-Learning Slow Features for Behaviour Analysis</a></p>
<p>Author: Lazaros Zafeiriou, Mihalis A. Nicolaou, Stefanos Zafeiriou, Symeon Nikitidis, Maja Pantic</p><p>Abstract: A recently introduced latent feature learning technique for time varying dynamic phenomena analysis is the socalled Slow Feature Analysis (SFA). SFA is a deterministic component analysis technique for multi-dimensional sequences that by minimizing the variance of the first order time derivative approximation of the input signal finds uncorrelated projections that extract slowly-varying features ordered by their temporal consistency and constancy. In this paper, we propose a number of extensions in both the deterministic and the probabilistic SFA optimization frameworks. In particular, we derive a novel deterministic SFA algorithm that is able to identify linear projections that extract the common slowest varying features of two or more sequences. In addition, we propose an Expectation Maximization (EM) algorithm to perform inference in a probabilistic formulation of SFA and similarly extend it in order to handle two and more time varying data sequences. Moreover, we demonstrate that the probabilistic SFA (EMSFA) algorithm that discovers the common slowest varying latent space of multiple sequences can be combined with dynamic time warping techniques for robust sequence timealignment. The proposed SFA algorithms were applied for facial behavior analysis demonstrating their usefulness and appropriateness for this task.</p><p>3 0.72937328 <a title="371-lda-3" href="./iccv-2013-The_Way_They_Move%3A_Tracking_Multiple_Targets_with_Similar_Appearance.html">418 iccv-2013-The Way They Move: Tracking Multiple Targets with Similar Appearance</a></p>
<p>Author: Caglayan Dicle, Octavia I. Camps, Mario Sznaier</p><p>Abstract: We introduce a computationally efficient algorithm for multi-object tracking by detection that addresses four main challenges: appearance similarity among targets, missing data due to targets being out of the field of view or occluded behind other objects, crossing trajectories, and camera motion. The proposed method uses motion dynamics as a cue to distinguish targets with similar appearance, minimize target mis-identification and recover missing data. Computational efficiency is achieved by using a Generalized Linear Assignment (GLA) coupled with efficient procedures to recover missing data and estimate the complexity of the underlying dynamics. The proposed approach works with tracklets of arbitrary length and does not assume a dynamical model a priori, yet it captures the overall motion dynamics of the targets. Experiments using challenging videos show that this framework can handle complex target motions, non-stationary cameras and long occlusions, on scenarios where appearance cues are not available or poor.</p><p>4 0.72920513 <a title="371-lda-4" href="./iccv-2013-Semi-supervised_Learning_for_Large_Scale_Image_Cosegmentation.html">383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</a></p>
<p>Author: Zhengxiang Wang, Rujie Liu</p><p>Abstract: This paper introduces to use semi-supervised learning for large scale image cosegmentation. Different from traditional unsupervised cosegmentation that does not use any segmentation groundtruth, semi-supervised cosegmentation exploits the similarity from both the very limited training image foregrounds, as well as the common object shared between the large number of unsegmented images. This would be a much practical way to effectively cosegment a large number of related images simultaneously, where previous unsupervised cosegmentation work poorly due to the large variances in appearance between different images and the lack ofsegmentation groundtruthfor guidance in cosegmentation. For semi-supervised cosegmentation in large scale, we propose an effective method by minimizing an energy function, which consists of the inter-image distance, the intraimage distance and the balance term. We also propose an iterative updating algorithm to efficiently solve this energy function, which decomposes the original energy minimization problem into sub-problems, and updates each image alternatively to reduce the number of variables in each subproblem for computation efficiency. Experiment results on iCoseg and Pascal VOC datasets show that the proposed cosegmentation method can effectively cosegment hundreds of images in less than one minute. And our semi-supervised cosegmentation is able to outperform both unsupervised cosegmentation as well asfully supervised single image segmentation, especially when the training data is limited.</p><p>5 0.70827389 <a title="371-lda-5" href="./iccv-2013-Saliency_and_Human_Fixations%3A_State-of-the-Art_and_Study_of_Comparison_Metrics.html">373 iccv-2013-Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics</a></p>
<p>Author: Nicolas Riche, Matthieu Duvinage, Matei Mancas, Bernard Gosselin, Thierry Dutoit</p><p>Abstract: Visual saliency has been an increasingly active research area in the last ten years with dozens of saliency models recently published. Nowadays, one of the big challenges in the field is to find a way to fairly evaluate all of these models. In this paper, on human eye fixations ,we compare the ranking of 12 state-of-the art saliency models using 12 similarity metrics. The comparison is done on Jian Li ’s database containing several hundreds of natural images. Based on Kendall concordance coefficient, it is shown that some of the metrics are strongly correlated leading to a redundancy in the performance metrics reported in the available benchmarks. On the other hand, other metrics provide a more diverse picture of models ’ overall performance. As a recommendation, three similarity metrics should be used to obtain a complete point of view of saliency model performance.</p><p>6 0.70011961 <a title="371-lda-6" href="./iccv-2013-Recursive_Estimation_of_the_Stein_Center_of_SPD_Matrices_and_Its_Applications.html">347 iccv-2013-Recursive Estimation of the Stein Center of SPD Matrices and Its Applications</a></p>
<p>7 0.69404525 <a title="371-lda-7" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>8 0.68990529 <a title="371-lda-8" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>9 0.68868548 <a title="371-lda-9" href="./iccv-2013-Synergistic_Clustering_of_Image_and_Segment_Descriptors_for_Unsupervised_Scene_Understanding.html">412 iccv-2013-Synergistic Clustering of Image and Segment Descriptors for Unsupervised Scene Understanding</a></p>
<p>10 0.68810594 <a title="371-lda-10" href="./iccv-2013-Saliency_Detection_via_Dense_and_Sparse_Reconstruction.html">372 iccv-2013-Saliency Detection via Dense and Sparse Reconstruction</a></p>
<p>11 0.68634325 <a title="371-lda-11" href="./iccv-2013-Large-Scale_Image_Annotation_by_Efficient_and_Robust_Kernel_Metric_Learning.html">227 iccv-2013-Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning</a></p>
<p>12 0.67454398 <a title="371-lda-12" href="./iccv-2013-Tracking_via_Robust_Multi-task_Multi-view_Joint_Sparse_Representation.html">425 iccv-2013-Tracking via Robust Multi-task Multi-view Joint Sparse Representation</a></p>
<p>13 0.67383099 <a title="371-lda-13" href="./iccv-2013-Handling_Occlusions_with_Franken-Classifiers.html">190 iccv-2013-Handling Occlusions with Franken-Classifiers</a></p>
<p>14 0.67138386 <a title="371-lda-14" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>15 0.67138106 <a title="371-lda-15" href="./iccv-2013-Analysis_of_Scores%2C_Datasets%2C_and_Models_in_Visual_Saliency_Prediction.html">50 iccv-2013-Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</a></p>
<p>16 0.669783 <a title="371-lda-16" href="./iccv-2013-Contextual_Hypergraph_Modeling_for_Salient_Object_Detection.html">91 iccv-2013-Contextual Hypergraph Modeling for Salient Object Detection</a></p>
<p>17 0.66849446 <a title="371-lda-17" href="./iccv-2013-Saliency_Detection%3A_A_Boolean_Map_Approach.html">369 iccv-2013-Saliency Detection: A Boolean Map Approach</a></p>
<p>18 0.66459829 <a title="371-lda-18" href="./iccv-2013-Space-Time_Robust_Representation_for_Action_Recognition.html">396 iccv-2013-Space-Time Robust Representation for Action Recognition</a></p>
<p>19 0.65939623 <a title="371-lda-19" href="./iccv-2013-Temporally_Consistent_Superpixels.html">414 iccv-2013-Temporally Consistent Superpixels</a></p>
<p>20 0.65740645 <a title="371-lda-20" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
