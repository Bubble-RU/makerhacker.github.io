<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>129 iccv-2013-Dynamic Scene Deblurring</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-129" href="#">iccv2013-129</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>129 iccv-2013-Dynamic Scene Deblurring</h1>
<br/><p>Source: <a title="iccv-2013-129-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Kim_Dynamic_Scene_Deblurring_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Tae Hyun Kim, Byeongjoo Ahn, Kyoung Mu Lee</p><p>Abstract: Most conventional single image deblurring methods assume that the underlying scene is static and the blur is caused by only camera shake. In this paper, in contrast to this restrictive assumption, we address the deblurring problem of general dynamic scenes which contain multiple moving objects as well as camera shake. In case of dynamic scenes, moving objects and background have different blur motions, so the segmentation of the motion blur is required for deblurring each distinct blur motion accurately. Thus, we propose a novel energy model designed with the weighted sum of multiple blur data models, which estimates different motion blurs and their associated pixelwise weights, and resulting sharp image. In this framework, the local weights are determined adaptively and get high values when the corresponding data models have high data fidelity. And, the weight information is used for the segmentation of the motion blur. Non-local regularization of weights are also incorporated to produce more reliable segmentation results. A convex optimization-based method is used for the solution of the proposed energy model. Exper- imental results demonstrate that our method outperforms conventional approaches in deblurring both dynamic scenes and static scenes.</p><p>Reference: <a title="iccv-2013-129-reference" href="../iccv2013_reference/iccv-2013-Dynamic_Scene_Deblurring_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 kr l ge  Abstract Most conventional single image deblurring methods assume that the underlying scene is static and the blur is caused by only camera shake. [sent-6, score-1.301]
</p><p>2 In this paper, in contrast to this restrictive assumption, we address the deblurring problem of general dynamic scenes which contain multiple moving objects as well as camera shake. [sent-7, score-0.709]
</p><p>3 In case of dynamic scenes, moving objects and background have different blur motions, so the segmentation of the motion blur is required for deblurring each distinct blur motion accurately. [sent-8, score-2.653]
</p><p>4 Thus, we propose a novel energy model designed with the weighted sum of multiple blur data models, which estimates different motion blurs and their associated pixelwise weights, and resulting sharp image. [sent-9, score-0.827]
</p><p>5 Exper-  imental results demonstrate that our method outperforms conventional approaches in deblurring both dynamic scenes and static scenes. [sent-14, score-0.697]
</p><p>6 To address this problem, single image deblurring methods which restore a sharp image from a blurred image have been considerable research in the field of computer vision with the recent increased demand for clear images. [sent-18, score-0.661]
</p><p>7 In general, the blind deblurring problem that restores the blurry image without knowing the blur kernel is highly illposed. [sent-19, score-1.567]
</p><p>8 So various energy models that are composed of the regularization and data term have been proposed to find the sharp image and the blur kernel jointly, in the form of  ? [sent-20, score-0.976]
</p><p>9 The green box illustrates the region used for blur kernel estimation for each image. [sent-40, score-0.79]
</p><p>10 Each color denotes a blur kernel and a map of its associated weight variable. [sent-47, score-0.903]
</p><p>11 The matrix K denotes the blur kernel whose row vector corresponds to the blur kernel placed at each pixel location. [sent-51, score-1.58]
</p><p>12 The data term Edata measures  the data fidelity and the regularization term Ereg enforces the smoothness constraint to the latent image as well as to the blur kernel. [sent-52, score-0.746]
</p><p>13 Depending on the type of the blur kernel, blind deblur33 115603  ring approaches can be categorized into two types. [sent-53, score-0.714]
</p><p>14 One is the uniform kernel approach, which assumes the blur kernel is spatially invariant, and the other is the non-uniform approach, which assumes the blur kernel varies spatially. [sent-54, score-1.894]
</p><p>15 If we assume that the blur kernel is shift invariant and uniform over the entire image [4, 15], it is possible to restore the latent image quickly with the aid of a fast Fourier transform (FFT) and parallel processing [20, 2, 19]. [sent-55, score-1.037]
</p><p>16 However the assumption of shift invariant blur kernel does not hold good when there exists a rotational movement of the camera or a moving object in the shaken image. [sent-56, score-0.984]
</p><p>17 To alleviate these limitations of uniform motion blur assumption, several non-uniform kernel based methods are proposed. [sent-57, score-0.983]
</p><p>18 In particular, recent approaches focus on modeling the rotation of camera as well as translation [5, 7, 18], and they obtained promising results in the deblurring of static scene. [sent-58, score-0.606]
</p><p>19 1, restoring the image with the uniform blur kernel that has been estimated from the moving bus raises a severe artifacts in the background region (Fig. [sent-61, score-1.215]
</p><p>20 And also the uniform kernel estimated from the background fails deblurring the bus (Fig. [sent-63, score-0.896]
</p><p>21 Note that even the stateof-the-art non-uniform blur kernel method [18] which can deblur rotational camera shake does not restore the moving bus either (Fig. [sent-65, score-1.099]
</p><p>22 So, the dynamic scene deblurring problem is deeply challenging. [sent-67, score-0.625]
</p><p>23 She argued that moving objects and background should be handled with different blur kernels to remove the artifacts from deblurring with an inaccurate blur kernel. [sent-71, score-2.028]
</p><p>24 To begin with, she segmented blur motions by comparing likelihoods with a set of given one dimensional box filters, then applied the Richardson-Lucy deconvolution algorithm to each segmented region with its corresponding box filter. [sent-72, score-0.752]
</p><p>25 This approach could handle smoothly varying blur kernels but could not handle the abrupt change of the blur kernel near the boundary of moving objects since they did not seg-  ment the motion blurs. [sent-77, score-1.7]
</p><p>26 [8] is based on the interpolation of initially estimated kernels and showed much better results by reducing errors from inaccurate blur kernels, but it also could not overcome the motion boundary problems as in [6]. [sent-79, score-0.818]
</p><p>27 In principle, the dynamic scene deblurring problem also requires the segmentation of differently blurred regions. [sent-80, score-0.715]
</p><p>28 In this work, we address the problem of estimating latent image as well as different blur motions and their implicit (soft) segmentations. [sent-81, score-0.684]
</p><p>29 To the best of our knowledge, this is the first dynamic deblurring work that can estimate these variables jointly. [sent-82, score-0.645]
</p><p>30 In our framework, we propose a new energy model including multiple blur kernels and their associated pixel-wise weights. [sent-83, score-0.8]
</p><p>31 The weight of a kernel takes high value he kernel gives high data fidelity. [sent-84, score-0.452]
</p><p>32 At the same time, the blur kernels are estimated from the pixels whose associated weights have high values. [sent-85, score-0.798]
</p><p>33 Therefore, locally varying weight information allow us to segmentation the blur motions. [sent-86, score-0.765]
</p><p>34 In this study, we introduce a more general and new deblurring framework that can adaptively combine different blur models to estimate the spatially varying blur kernels. [sent-88, score-1.763]
</p><p>35 1(e)-(f), we provide the segmentation of the motion blur as well as better deblurring results. [sent-90, score-1.22]
</p><p>36 Note that since our framework is general in nature, any blur models and optimization method can be incorporated. [sent-91, score-0.611]
</p><p>37 Dynamic Scene Deblurring Model In our dynamic scene deblurring model, we assume the existence of various blur motions. [sent-94, score-1.236]
</p><p>38 So we have to find both blur kernels and their corresponding blur regions. [sent-95, score-1.35]
</p><p>39 Also, we do not restrict the types of blur kernels, so we employ both the uniform kernels, which are simple and fast, and the nonuniform kernels which can handle camera rotation. [sent-96, score-0.96]
</p><p>40 As there are multiple blur kernels in a dynamic scene, each blurred pixel should be restored from one of them and each kernel should be estimated from its related pixels. [sent-97, score-1.132]
</p><p>41 A pixel-wise weight variable is associated with a blur kernel and it gains high values on the pixels related with the blur kernel. [sent-99, score-1.558]
</p><p>42 (2)  The set K = {Ki} denotes a set of N blur kernels, and the sTehte eW se t= K { =W {iK} means a sse at oseft N of weight vk aerrinaebllse,s a wndh tehree is =t W1, 2 =, . [sent-102, score-0.726]
</p><p>43 mAe weight vetec otfor N NW wi eiisg ahsts voacriiataebdle sw withh tehree corresponding blur kernel Ki. [sent-106, score-1.014]
</p><p>44 Compared with the conventional model in (1), our new energy model involves additional weight variables and multiple blur kernels, so it becomes a more complex and chal-  33 115614  Figure 2. [sent-107, score-0.851]
</p><p>45 Multiple blur kernel models give a much better result without additional process to remove ringing artifact. [sent-108, score-0.884]
</p><p>46 (d) Deblurring results of our method with six uniform blur kernels and weight variables. [sent-115, score-1.005]
</p><p>47 Slightly different blur kernels are estimated and the result shows that it is hard to estimate a perfect uniform kernel  due to unexpected blur effects. [sent-118, score-1.711]
</p><p>48 Note that even in the case of a static scene with only translational camera shake, the captured real image may contain various blur motions because depth variation or radial distortion may generate unexpected blur effects. [sent-122, score-1.397]
</p><p>49 Since the proposed model employs multiple blur kernels, it could handle this problem complementary and produce much better results than the conventional methods. [sent-123, score-0.682]
</p><p>50 2, six similar but slightly different uniform blur kernels and their associated six blur regions are jointly estimated and a sharper deblurred image with less ringing artifacts is obtained by our method. [sent-125, score-1.776]
</p><p>51 Adaptive Blur Model Selection In this section, we propose a data term that adaptively  selects and fuses proper blur models among candidate models. [sent-128, score-0.653]
</p><p>52 At the same time, to obtain correct blur kernels, it is required that each blur kernel is estimated from pixels whose associated weight variable shows high values. [sent-130, score-1.579]
</p><p>53 ∂∗  (3) where N is the number of maximal blur models in the scene and λ is the parameter adjusting the scale of our data term and the continuous w? [sent-141, score-0.642]
</p><p>54 Despite this, we can obtain satisfying results by means of multiple blur models and reduce the computational cost. [sent-152, score-0.611]
</p><p>55 Regularization As dynamic scene deblurring is a highly ill-posed problem, regularization enforcing the smoothness of variables is necessary to obtain a reliable solution. [sent-155, score-0.768]
</p><p>56 2  Regularization on W  We assumed that a blurry object can be restored by one of the various blur models, and the motion blur does not change abruptly except on the boundary of a moving object. [sent-167, score-1.565]
</p><p>57 The formulation incorporated in our deblurring model is,  = ? [sent-170, score-0.518]
</p><p>58 Note that, in contrast to the result from weight regularization with only four neighbors, the result from regularization with dozens of neighbors is much better in both motion blur segmentation and deblurring as shown in Fig. [sent-184, score-1.498]
</p><p>59 3  Regularization on K  As we use both uniform and non-uniform kernels in our blur models, two different regularization models are required. [sent-188, score-0.966]
</p><p>60 First, if the blur kernel matrix consists of uniform blur kernel, we use Tikhonov regularization which is typically used in other methods of uniform blur kernel regularization due to its simplicity [20, 2, 19]. [sent-189, score-2.645]
</p><p>61 Comparison of weight maps corresponding to the car, and the deblurring results with varying number of neighboring pixels in non-local regularization. [sent-198, score-0.659]
</p><p>62 regularization on an uniform kernel Ki is formulated by Ereg(Ki) =  β? [sent-203, score-0.406]
</p><p>63 2,  (8)  where ki is a vector form of the uniform kernel Ki and the parameter β controls the influence of regularization on ki. [sent-205, score-0.601]
</p><p>64 Secondly, for a non-uniform blur kernel Ki, we also use Tikhonov regularization but in a different manner from the case of uniform blur kernel because a non-uniform kernel is  estimated in a different way. [sent-206, score-2.007]
</p><p>65 Optimization The proposed dynamic scene deblurring model introduced in the previous section and the final objective function is as follows:  Lm,Win,Kλi? [sent-221, score-0.625]
</p><p>66 Sharp Image Restoration Sharp image restoration methods are widely researched in both non-blind and blind deblurring methods and some fast solutions are available with the aid of FFT. [sent-259, score-0.661]
</p><p>67 The initial weight map is designed so  that at least one of the segments for uniform kernels can cover the moving object. [sent-292, score-0.465]
</p><p>68 If the moving object occupies large part or has strong edges in an initial segment, then the blur kernel of the moving object can be roughly estimated from that segment. [sent-294, score-0.996]
</p><p>69 Then, by iterations, both the accuracies of the blur kernel and segment increase. [sent-295, score-0.79]
</p><p>70 An example of using 6 uniform blur kernels and 1 non-uniform blur kernel and their corresponding initial weight maps is illustrated in Fig. [sent-296, score-1.789]
</p><p>71 Blur Kernel Estimation The proposed method includes multiple blur models and a blur model could be either uniform and non-uniform kernel. [sent-303, score-1.357]
</p><p>72 The six columns on the left illustrate the initial weight variables corresponding to six uniform models, and the right most column shows the initial weight variable corresponding to a non-uniform model. [sent-312, score-0.535]
</p><p>73 The blur kernel estimation methods for both approaches have been widely studied in blind deblurring methods, but ours is somewhat different because the proposed model includes additional weight variables. [sent-316, score-1.483]
</p><p>74 Since proper initial value for blur kernel is also important, the method guiding the latent image using prediction step [2, 19] is widely used. [sent-317, score-0.883]
</p><p>75 1  Uniform Kernel Estimation  For L and W being fixed, our energy model for uniform kernel Ki is quadratic and the solution can be easily obtained. [sent-321, score-0.392]
</p><p>76 (14)  Note that matrices Px and Py consist of px and py, respectively and the vector form of uniform kernel ki is used where elements of ki are larger or equal to zero and their sum is one. [sent-325, score-0.741]
</p><p>77 2  Non-Uniform Kernel Estimation  Since non-uniform kernel Ki is a weighted sum of M basis kernels and the blurry image B is equal to KiL, we can derive an equation, KiL = Aui, (15) where the matrix A = [b1L, b2L, · · · , bML] . [sent-330, score-0.473]
</p><p>78 e can obtain the non-uniform blur kernel matrix as Ki = ui(m)bm from (9). [sent-341, score-0.79]
</p><p>79 To alleviate this problem, we adopt coarse to fine approach like most recent blind deconvolution algorithms [13, 19, 2], and the overall procedure of our dynamic scene deblurring is in Algorithm 2. [sent-348, score-0.826]
</p><p>80 Algorithm 2 The overall procedure of the proposed dynamic scene deblurring algorithm Input: A blurry image B  Output: L, W and K 1: Build an image pyramid, which has 5 levels, with a scale factor of 0. [sent-349, score-0.773]
</p><p>81 Since we do 33 115658  not estimate the noise level and the blur strength of the input image, λ that adjusts the influence of data term should be tuned from the statistics of the input image. [sent-363, score-0.639]
</p><p>82 It ranges from 50 to 500 and it has a low value when the noise level is high or the blur is severe. [sent-364, score-0.611]
</p><p>83 The other parameters are fixed and we use six uniform kernel models and one non-uniform kernel model, so N = 7 in all experiments. [sent-365, score-0.53]
</p><p>84 By setting N as large as possible, we can handle various kinds of blur motions but it raises costs and thus we determined the number of models empirically and fixed it. [sent-366, score-0.68]
</p><p>85 However, the estimated set of blur kernels K from a gray image is also used for deblurring the corresponding color image by applying the sharp image restoration step introduced in Section 3. [sent-374, score-1.357]
</p><p>86 5, the motion blur segmentation and deblurring results of real dynamic scenes are shown. [sent-377, score-1.296]
</p><p>87 6, there are serious artifacts near the boundaries of moving objects in the results of other methods, while our method gives relatively clean results with the aid of various blur models and motion blur segmentations. [sent-381, score-1.495]
</p><p>88 We also compared our deblurring results on static scenes with conventional methods in Fig. [sent-382, score-0.621]
</p><p>89 Although it is possible to obtain a sharp image with methods based on uniform kernel, our model shows much better result in reducing ringing artifacts and restoring details well. [sent-387, score-0.409]
</p><p>90 8 has  non-uniform blur motion which is generated by rotational camera shake. [sent-389, score-0.744]
</p><p>91 In this case, one of seven blur models which corresponds to the non-uniform blur kernel gained almost all weights and still showed competitive result compared to the state-of-the-art non-uniform kernel based methods. [sent-390, score-1.58]
</p><p>92 Discussion and Conclusion We proposed a novel single image deblurring framework that can handle multiple moving objects in the scene as well as camera shake. [sent-392, score-0.682]
</p><p>93 By introducing multiple blur models and their locally varying weight variables which favor the blur models giving better data fidelity, we could also obtain the segmented blur region as well as restored image. [sent-393, score-2.095]
</p><p>94 conventional methods in dynamic scene deblurring as well as in static scene cases. [sent-408, score-0.759]
</p><p>95 Since the number of moving objects is unknown in a real image, we should set a large number of blur models. [sent-410, score-0.688]
</p><p>96 Due to multiple blur models and additional weight variables, computational costs increase. [sent-412, score-0.705]
</p><p>97 Dashed green boxes in the figures denote the regions used for estimating uniform blur  kernels and used for restoring the background regions. [sent-475, score-0.919]
</p><p>98 Synthetic uniform kernel is used to blur the Picasso image. [sent-492, score-0.925]
</p><p>99 Magazine image is blurred by rotational camera shake and requires non-uniform blur kernel to be restored. [sent-512, score-1.002]
</p><p>100 Motion-aware noise filtering for deblurring of noisy and blurry images. [sent-581, score-0.666]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('blur', 0.611), ('deblurring', 0.518), ('ki', 0.195), ('kernel', 0.179), ('ereg', 0.155), ('blurry', 0.148), ('uniform', 0.135), ('kernels', 0.128), ('wi', 0.109), ('ringing', 0.094), ('weight', 0.094), ('regularization', 0.092), ('artifacts', 0.083), ('blind', 0.081), ('shake', 0.08), ('ui', 0.077), ('moving', 0.077), ('dynamic', 0.076), ('restored', 0.06), ('motion', 0.058), ('blurred', 0.057), ('conventional', 0.053), ('sharp', 0.052), ('variables', 0.051), ('deconvolution', 0.051), ('static', 0.05), ('edata', 0.05), ('restoring', 0.045), ('bus', 0.043), ('latent', 0.043), ('energy', 0.042), ('shaken', 0.042), ('whyte', 0.042), ('harmeling', 0.042), ('camera', 0.038), ('six', 0.037), ('fft', 0.037), ('px', 0.037), ('rotational', 0.037), ('landweber', 0.036), ('pxki', 0.036), ('pyki', 0.036), ('bm', 0.036), ('quadratic', 0.036), ('aid', 0.035), ('restore', 0.034), ('segmentation', 0.033), ('simplex', 0.032), ('picasso', 0.032), ('py', 0.032), ('initial', 0.031), ('scene', 0.031), ('segmented', 0.03), ('motions', 0.03), ('nonuniform', 0.03), ('tikhonov', 0.03), ('kil', 0.03), ('tdiag', 0.03), ('restores', 0.03), ('seoul', 0.03), ('levin', 0.028), ('neighboring', 0.028), ('adjusts', 0.028), ('restoration', 0.027), ('siggraph', 0.027), ('hirsch', 0.027), ('locally', 0.027), ('unexpected', 0.026), ('variable', 0.025), ('shan', 0.025), ('blurs', 0.024), ('assisted', 0.024), ('coarse', 0.024), ('adaptively', 0.023), ('magazine', 0.023), ('fine', 0.023), ('adopt', 0.022), ('ring', 0.022), ('estimated', 0.021), ('raises', 0.021), ('tehree', 0.021), ('calculates', 0.021), ('pixelwise', 0.021), ('joshi', 0.021), ('yb', 0.021), ('serious', 0.02), ('xb', 0.02), ('subproblem', 0.02), ('ahn', 0.02), ('update', 0.019), ('zitnick', 0.019), ('associated', 0.019), ('proper', 0.019), ('pixels', 0.019), ('convex', 0.019), ('operator', 0.018), ('xu', 0.018), ('primal', 0.018), ('basis', 0.018), ('handle', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="129-tfidf-1" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>Author: Tae Hyun Kim, Byeongjoo Ahn, Kyoung Mu Lee</p><p>Abstract: Most conventional single image deblurring methods assume that the underlying scene is static and the blur is caused by only camera shake. In this paper, in contrast to this restrictive assumption, we address the deblurring problem of general dynamic scenes which contain multiple moving objects as well as camera shake. In case of dynamic scenes, moving objects and background have different blur motions, so the segmentation of the motion blur is required for deblurring each distinct blur motion accurately. Thus, we propose a novel energy model designed with the weighted sum of multiple blur data models, which estimates different motion blurs and their associated pixelwise weights, and resulting sharp image. In this framework, the local weights are determined adaptively and get high values when the corresponding data models have high data fidelity. And, the weight information is used for the segmentation of the motion blur. Non-local regularization of weights are also incorporated to produce more reliable segmentation results. A convex optimization-based method is used for the solution of the proposed energy model. Exper- imental results demonstrate that our method outperforms conventional approaches in deblurring both dynamic scenes and static scenes.</p><p>2 0.67208874 <a title="129-tfidf-2" href="./iccv-2013-Deblurring_by_Example_Using_Dense_Correspondence.html">103 iccv-2013-Deblurring by Example Using Dense Correspondence</a></p>
<p>Author: Yoav Hacohen, Eli Shechtman, Dani Lischinski</p><p>Abstract: This paper presents a new method for deblurring photos using a sharp reference example that contains some shared content with the blurry photo. Most previous deblurring methods that exploit information from other photos require an accurately registered photo of the same static scene. In contrast, our method aims to exploit reference images where the shared content may have undergone substantial photometric and non-rigid geometric transformations, as these are the kind of reference images most likely to be found in personal photo albums. Our approach builds upon a recent method for examplebased deblurring using non-rigid dense correspondence (NRDC) [11] and extends it in two ways. First, we suggest exploiting information from the reference image not only for blur kernel estimation, but also as a powerful local prior for the non-blind deconvolution step. Second, we introduce a simple yet robust technique for spatially varying blur estimation, rather than assuming spatially uniform blur. Unlike the aboveprevious method, which hasproven successful only with simple deblurring scenarios, we demonstrate that our method succeeds on a variety of real-world examples. We provide quantitative and qualitative evaluation of our method and show that it outperforms the state-of-the-art.</p><p>3 0.61787713 <a title="129-tfidf-3" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<p>Author: Shicheng Zheng, Li Xu, Jiaya Jia</p><p>Abstract: We handle a special type of motion blur considering that cameras move primarily forward or backward. Solving this type of blur is of unique practical importance since nearly all car, traffic and bike-mounted cameras follow out-ofplane translational motion. We start with the study of geometric models and analyze the difficulty of existing methods to deal with them. We also propose a solution accounting for depth variation. Homographies associated with different 3D planes are considered and solved for in an optimization framework. Our method is verified on several natural image examples that cannot be satisfyingly dealt with by previous methods.</p><p>4 0.26241294 <a title="129-tfidf-4" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>Author: Netalee Efrat, Daniel Glasner, Alexander Apartsin, Boaz Nadler, Anat Levin</p><p>Abstract: Over the past decade, single image Super-Resolution (SR) research has focused on developing sophisticated image priors, leading to significant advances. Estimating and incorporating the blur model, that relates the high-res and low-res images, has received much less attention, however. In particular, the reconstruction constraint, namely that the blurred and downsampled high-res output should approximately equal the low-res input image, has been either ignored or applied with default fixed blur models. In this work, we examine the relative importance ofthe imageprior and the reconstruction constraint. First, we show that an accurate reconstruction constraint combined with a simple gradient regularization achieves SR results almost as good as those of state-of-the-art algorithms with sophisticated image priors. Second, we study both empirically and theoretically the sensitivity of SR algorithms to the blur model assumed in the reconstruction constraint. We find that an accurate blur model is more important than a sophisticated image prior. Finally, using real camera data, we demonstrate that the default blur models of various SR algorithms may differ from the camera blur, typically leading to over- smoothed results. Our findings highlight the importance of accurately estimating camera blur in reconstructing raw low- res images acquired by an actual camera.</p><p>5 0.24186793 <a title="129-tfidf-5" href="./iccv-2013-Nonparametric_Blind_Super-resolution.html">293 iccv-2013-Nonparametric Blind Super-resolution</a></p>
<p>Author: Tomer Michaeli, Michal Irani</p><p>Abstract: Super resolution (SR) algorithms typically assume that the blur kernel is known (either the Point Spread Function ‘PSF’ of the camera, or some default low-pass filter, e.g. a Gaussian). However, the performance of SR methods significantly deteriorates when the assumed blur kernel deviates from the true one. We propose a general framework for “blind” super resolution. In particular, we show that: (i) Unlike the common belief, the PSF of the camera is the wrong blur kernel to use in SR algorithms. (ii) We show how the correct SR blur kernel can be recovered directly from the low-resolution image. This is done by exploiting the inherent recurrence property of small natural image patches (either internally within the same image, or externally in a collection of other natural images). In particular, we show that recurrence of small patches across scales of the low-res image (which forms the basis for single-image SR), can also be used for estimating the optimal blur kernel. This leads to significant improvement in SR results.</p><p>6 0.18530916 <a title="129-tfidf-6" href="./iccv-2013-A_Unified_Rolling_Shutter_and_Motion_Blur_Model_for_3D_Visual_Registration.html">32 iccv-2013-A Unified Rolling Shutter and Motion Blur Model for 3D Visual Registration</a></p>
<p>7 0.17238469 <a title="129-tfidf-7" href="./iccv-2013-Fluttering_Pattern_Generation_Using_Modified_Legendre_Sequence_for_Coded_Exposure_Imaging.html">173 iccv-2013-Fluttering Pattern Generation Using Modified Legendre Sequence for Coded Exposure Imaging</a></p>
<p>8 0.11022487 <a title="129-tfidf-8" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>9 0.1096537 <a title="129-tfidf-9" href="./iccv-2013-Optical_Flow_via_Locally_Adaptive_Fusion_of_Complementary_Data_Costs.html">300 iccv-2013-Optical Flow via Locally Adaptive Fusion of Complementary Data Costs</a></p>
<p>10 0.10495016 <a title="129-tfidf-10" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>11 0.10100853 <a title="129-tfidf-11" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>12 0.097640082 <a title="129-tfidf-12" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<p>13 0.093430243 <a title="129-tfidf-13" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<p>14 0.093269959 <a title="129-tfidf-14" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>15 0.092260376 <a title="129-tfidf-15" href="./iccv-2013-Locally_Affine_Sparse-to-Dense_Matching_for_Motion_and_Occlusion_Estimation.html">256 iccv-2013-Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation</a></p>
<p>16 0.074938461 <a title="129-tfidf-16" href="./iccv-2013-Combining_the_Right_Features_for_Complex_Event_Recognition.html">81 iccv-2013-Combining the Right Features for Complex Event Recognition</a></p>
<p>17 0.066688269 <a title="129-tfidf-17" href="./iccv-2013-Directed_Acyclic_Graph_Kernels_for_Action_Recognition.html">116 iccv-2013-Directed Acyclic Graph Kernels for Action Recognition</a></p>
<p>18 0.064613916 <a title="129-tfidf-18" href="./iccv-2013-A_General_Dense_Image_Matching_Framework_Combining_Direct_and_Feature-Based_Costs.html">12 iccv-2013-A General Dense Image Matching Framework Combining Direct and Feature-Based Costs</a></p>
<p>19 0.06412445 <a title="129-tfidf-19" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>20 0.063909225 <a title="129-tfidf-20" href="./iccv-2013-Coherent_Motion_Segmentation_in_Moving_Camera_Videos_Using_Optical_Flow_Orientations.html">78 iccv-2013-Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.155), (1, -0.106), (2, -0.014), (3, 0.049), (4, -0.073), (5, 0.073), (6, 0.008), (7, -0.109), (8, 0.103), (9, -0.162), (10, -0.089), (11, -0.322), (12, 0.26), (13, -0.419), (14, -0.211), (15, 0.149), (16, 0.091), (17, -0.095), (18, -0.098), (19, -0.212), (20, -0.107), (21, 0.037), (22, 0.027), (23, -0.122), (24, 0.014), (25, -0.105), (26, -0.083), (27, 0.081), (28, 0.08), (29, -0.047), (30, -0.004), (31, 0.118), (32, -0.092), (33, -0.1), (34, 0.046), (35, -0.068), (36, -0.031), (37, 0.048), (38, 0.045), (39, 0.081), (40, -0.055), (41, 0.037), (42, -0.036), (43, 0.032), (44, -0.017), (45, -0.013), (46, 0.037), (47, -0.024), (48, 0.035), (49, 0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97635847 <a title="129-lsi-1" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>Author: Tae Hyun Kim, Byeongjoo Ahn, Kyoung Mu Lee</p><p>Abstract: Most conventional single image deblurring methods assume that the underlying scene is static and the blur is caused by only camera shake. In this paper, in contrast to this restrictive assumption, we address the deblurring problem of general dynamic scenes which contain multiple moving objects as well as camera shake. In case of dynamic scenes, moving objects and background have different blur motions, so the segmentation of the motion blur is required for deblurring each distinct blur motion accurately. Thus, we propose a novel energy model designed with the weighted sum of multiple blur data models, which estimates different motion blurs and their associated pixelwise weights, and resulting sharp image. In this framework, the local weights are determined adaptively and get high values when the corresponding data models have high data fidelity. And, the weight information is used for the segmentation of the motion blur. Non-local regularization of weights are also incorporated to produce more reliable segmentation results. A convex optimization-based method is used for the solution of the proposed energy model. Exper- imental results demonstrate that our method outperforms conventional approaches in deblurring both dynamic scenes and static scenes.</p><p>2 0.93803662 <a title="129-lsi-2" href="./iccv-2013-Deblurring_by_Example_Using_Dense_Correspondence.html">103 iccv-2013-Deblurring by Example Using Dense Correspondence</a></p>
<p>Author: Yoav Hacohen, Eli Shechtman, Dani Lischinski</p><p>Abstract: This paper presents a new method for deblurring photos using a sharp reference example that contains some shared content with the blurry photo. Most previous deblurring methods that exploit information from other photos require an accurately registered photo of the same static scene. In contrast, our method aims to exploit reference images where the shared content may have undergone substantial photometric and non-rigid geometric transformations, as these are the kind of reference images most likely to be found in personal photo albums. Our approach builds upon a recent method for examplebased deblurring using non-rigid dense correspondence (NRDC) [11] and extends it in two ways. First, we suggest exploiting information from the reference image not only for blur kernel estimation, but also as a powerful local prior for the non-blind deconvolution step. Second, we introduce a simple yet robust technique for spatially varying blur estimation, rather than assuming spatially uniform blur. Unlike the aboveprevious method, which hasproven successful only with simple deblurring scenarios, we demonstrate that our method succeeds on a variety of real-world examples. We provide quantitative and qualitative evaluation of our method and show that it outperforms the state-of-the-art.</p><p>3 0.89445943 <a title="129-lsi-3" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<p>Author: Shicheng Zheng, Li Xu, Jiaya Jia</p><p>Abstract: We handle a special type of motion blur considering that cameras move primarily forward or backward. Solving this type of blur is of unique practical importance since nearly all car, traffic and bike-mounted cameras follow out-ofplane translational motion. We start with the study of geometric models and analyze the difficulty of existing methods to deal with them. We also propose a solution accounting for depth variation. Homographies associated with different 3D planes are considered and solved for in an optimization framework. Our method is verified on several natural image examples that cannot be satisfyingly dealt with by previous methods.</p><p>4 0.69156337 <a title="129-lsi-4" href="./iccv-2013-Nonparametric_Blind_Super-resolution.html">293 iccv-2013-Nonparametric Blind Super-resolution</a></p>
<p>Author: Tomer Michaeli, Michal Irani</p><p>Abstract: Super resolution (SR) algorithms typically assume that the blur kernel is known (either the Point Spread Function ‘PSF’ of the camera, or some default low-pass filter, e.g. a Gaussian). However, the performance of SR methods significantly deteriorates when the assumed blur kernel deviates from the true one. We propose a general framework for “blind” super resolution. In particular, we show that: (i) Unlike the common belief, the PSF of the camera is the wrong blur kernel to use in SR algorithms. (ii) We show how the correct SR blur kernel can be recovered directly from the low-resolution image. This is done by exploiting the inherent recurrence property of small natural image patches (either internally within the same image, or externally in a collection of other natural images). In particular, we show that recurrence of small patches across scales of the low-res image (which forms the basis for single-image SR), can also be used for estimating the optimal blur kernel. This leads to significant improvement in SR results.</p><p>5 0.66614938 <a title="129-lsi-5" href="./iccv-2013-Accurate_Blur_Models_vs._Image_Priors_in_Single_Image_Super-resolution.html">35 iccv-2013-Accurate Blur Models vs. Image Priors in Single Image Super-resolution</a></p>
<p>Author: Netalee Efrat, Daniel Glasner, Alexander Apartsin, Boaz Nadler, Anat Levin</p><p>Abstract: Over the past decade, single image Super-Resolution (SR) research has focused on developing sophisticated image priors, leading to significant advances. Estimating and incorporating the blur model, that relates the high-res and low-res images, has received much less attention, however. In particular, the reconstruction constraint, namely that the blurred and downsampled high-res output should approximately equal the low-res input image, has been either ignored or applied with default fixed blur models. In this work, we examine the relative importance ofthe imageprior and the reconstruction constraint. First, we show that an accurate reconstruction constraint combined with a simple gradient regularization achieves SR results almost as good as those of state-of-the-art algorithms with sophisticated image priors. Second, we study both empirically and theoretically the sensitivity of SR algorithms to the blur model assumed in the reconstruction constraint. We find that an accurate blur model is more important than a sophisticated image prior. Finally, using real camera data, we demonstrate that the default blur models of various SR algorithms may differ from the camera blur, typically leading to over- smoothed results. Our findings highlight the importance of accurately estimating camera blur in reconstructing raw low- res images acquired by an actual camera.</p><p>6 0.63935465 <a title="129-lsi-6" href="./iccv-2013-Fluttering_Pattern_Generation_Using_Modified_Legendre_Sequence_for_Coded_Exposure_Imaging.html">173 iccv-2013-Fluttering Pattern Generation Using Modified Legendre Sequence for Coded Exposure Imaging</a></p>
<p>7 0.43854508 <a title="129-lsi-7" href="./iccv-2013-On_One-Shot_Similarity_Kernels%3A_Explicit_Feature_Maps_and_Properties.html">295 iccv-2013-On One-Shot Similarity Kernels: Explicit Feature Maps and Properties</a></p>
<p>8 0.39937547 <a title="129-lsi-8" href="./iccv-2013-A_Unified_Rolling_Shutter_and_Motion_Blur_Model_for_3D_Visual_Registration.html">32 iccv-2013-A Unified Rolling Shutter and Motion Blur Model for 3D Visual Registration</a></p>
<p>9 0.36025 <a title="129-lsi-9" href="./iccv-2013-An_Adaptive_Descriptor_Design_for_Object_Recognition_in_the_Wild.html">48 iccv-2013-An Adaptive Descriptor Design for Object Recognition in the Wild</a></p>
<p>10 0.35549214 <a title="129-lsi-10" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>11 0.3386966 <a title="129-lsi-11" href="./iccv-2013-A_Framework_for_Shape_Analysis_via_Hilbert_Space_Embedding.html">10 iccv-2013-A Framework for Shape Analysis via Hilbert Space Embedding</a></p>
<p>12 0.33325952 <a title="129-lsi-12" href="./iccv-2013-Fibonacci_Exposure_Bracketing_for_High_Dynamic_Range_Imaging.html">164 iccv-2013-Fibonacci Exposure Bracketing for High Dynamic Range Imaging</a></p>
<p>13 0.32927576 <a title="129-lsi-13" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>14 0.32454398 <a title="129-lsi-14" href="./iccv-2013-A_Generalized_Iterated_Shrinkage_Algorithm_for_Non-convex_Sparse_Coding.html">14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</a></p>
<p>15 0.30288193 <a title="129-lsi-15" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>16 0.28690228 <a title="129-lsi-16" href="./iccv-2013-Discovering_Details_and_Scene_Structure_with_Hierarchical_Iconoid_Shift.html">117 iccv-2013-Discovering Details and Scene Structure with Hierarchical Iconoid Shift</a></p>
<p>17 0.27246195 <a title="129-lsi-17" href="./iccv-2013-Image_Set_Classification_Using_Holistic_Multiple_Order_Statistics_Features_and_Localized_Multi-kernel_Metric_Learning.html">212 iccv-2013-Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning</a></p>
<p>18 0.26883376 <a title="129-lsi-18" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>19 0.26560593 <a title="129-lsi-19" href="./iccv-2013-Large-Scale_Image_Annotation_by_Efficient_and_Robust_Kernel_Metric_Learning.html">227 iccv-2013-Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning</a></p>
<p>20 0.2636286 <a title="129-lsi-20" href="./iccv-2013-Salient_Region_Detection_by_UFO%3A_Uniqueness%2C_Focusness_and_Objectness.html">374 iccv-2013-Salient Region Detection by UFO: Uniqueness, Focusness and Objectness</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.062), (7, 0.025), (23, 0.059), (26, 0.058), (31, 0.055), (35, 0.015), (40, 0.018), (42, 0.087), (48, 0.013), (64, 0.065), (73, 0.043), (74, 0.035), (81, 0.015), (89, 0.338), (95, 0.016), (98, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9797374 <a title="129-lda-1" href="./iccv-2013-Dynamic_Scene_Deblurring.html">129 iccv-2013-Dynamic Scene Deblurring</a></p>
<p>Author: Tae Hyun Kim, Byeongjoo Ahn, Kyoung Mu Lee</p><p>Abstract: Most conventional single image deblurring methods assume that the underlying scene is static and the blur is caused by only camera shake. In this paper, in contrast to this restrictive assumption, we address the deblurring problem of general dynamic scenes which contain multiple moving objects as well as camera shake. In case of dynamic scenes, moving objects and background have different blur motions, so the segmentation of the motion blur is required for deblurring each distinct blur motion accurately. Thus, we propose a novel energy model designed with the weighted sum of multiple blur data models, which estimates different motion blurs and their associated pixelwise weights, and resulting sharp image. In this framework, the local weights are determined adaptively and get high values when the corresponding data models have high data fidelity. And, the weight information is used for the segmentation of the motion blur. Non-local regularization of weights are also incorporated to produce more reliable segmentation results. A convex optimization-based method is used for the solution of the proposed energy model. Exper- imental results demonstrate that our method outperforms conventional approaches in deblurring both dynamic scenes and static scenes.</p><p>2 0.97037387 <a title="129-lda-2" href="./iccv-2013-Piecewise_Rigid_Scene_Flow.html">317 iccv-2013-Piecewise Rigid Scene Flow</a></p>
<p>Author: Christoph Vogel, Konrad Schindler, Stefan Roth</p><p>Abstract: Estimating dense 3D scene flow from stereo sequences remains a challenging task, despite much progress in both classical disparity and 2D optical flow estimation. To overcome the limitations of existing techniques, we introduce a novel model that represents the dynamic 3D scene by a collection of planar, rigidly moving, local segments. Scene flow estimation then amounts to jointly estimating the pixelto-segment assignment, and the 3D position, normal vector, and rigid motion parameters of a plane for each segment. The proposed energy combines an occlusion-sensitive data term with appropriate shape, motion, and segmentation regularizers. Optimization proceeds in two stages: Starting from an initial superpixelization, we estimate the shape and motion parameters of all segments by assigning a proposal from a set of moving planes. Then the pixel-to-segment assignment is updated, while holding the shape and motion parameters of the moving planes fixed. We demonstrate the benefits of our model on different real-world image sets, including the challenging KITTI benchmark. We achieve leading performance levels, exceeding competing 3D scene flow methods, and even yielding better 2D motion estimates than all tested dedicated optical flow techniques.</p><p>3 0.96902996 <a title="129-lda-3" href="./iccv-2013-Real-World_Normal_Map_Capture_for_Nearly_Flat_Reflective_Surfaces.html">343 iccv-2013-Real-World Normal Map Capture for Nearly Flat Reflective Surfaces</a></p>
<p>Author: Bastien Jacquet, Christian Häne, Kevin Köser, Marc Pollefeys</p><p>Abstract: Although specular objects have gained interest in recent years, virtually no approaches exist for markerless reconstruction of reflective scenes in the wild. In this work, we present a practical approach to capturing normal maps in real-world scenes using video only. We focus on nearly planar surfaces such as windows, facades from glass or metal, or frames, screens and other indoor objects and show how normal maps of these can be obtained without the use of an artificial calibration object. Rather, we track the reflections of real-world straight lines, while moving with a hand-held or vehicle-mounted camera in front of the object. In contrast to error-prone local edge tracking, we obtain the reflections by a robust, global segmentation technique of an ortho-rectified 3D video cube that also naturally allows efficient user interaction. Then, at each point of the reflective surface, the resulting 2D-curve to 3D-line correspondence provides a novel quadratic constraint on the local surface normal. This allows to globally solve for the shape by integrability and smoothness constraints and easily supports the usage of multiple lines. We demonstrate the technique on several objects and facades.</p><p>4 0.96833706 <a title="129-lda-4" href="./iccv-2013-Action_and_Event_Recognition_with_Fisher_Vectors_on_a_Compact_Feature_Set.html">40 iccv-2013-Action and Event Recognition with Fisher Vectors on a Compact Feature Set</a></p>
<p>Author: Dan Oneata, Jakob Verbeek, Cordelia Schmid</p><p>Abstract: Action recognition in uncontrolled video is an important and challenging computer vision problem. Recent progress in this area is due to new local features and models that capture spatio-temporal structure between local features, or human-object interactions. Instead of working towards more complex models, we focus on the low-level features and their encoding. We evaluate the use of Fisher vectors as an alternative to bag-of-word histograms to aggregate a small set of state-of-the-art low-level descriptors, in combination with linear classifiers. We present a large and varied set of evaluations, considering (i) classification of short actions in five datasets, (ii) localization of such actions in feature-length movies, and (iii) large-scale recognition of complex events. We find that for basic action recognition and localization MBH features alone are enough for stateof-the-art performance. For complex events we find that SIFT and MFCC features provide complementary cues. On all three problems we obtain state-of-the-art results, while using fewer features and less complex models.</p><p>5 0.96783435 <a title="129-lda-5" href="./iccv-2013-Point-Based_3D_Reconstruction_of_Thin_Objects.html">319 iccv-2013-Point-Based 3D Reconstruction of Thin Objects</a></p>
<p>Author: Benjamin Ummenhofer, Thomas Brox</p><p>Abstract: 3D reconstruction deals with the problem of finding the shape of an object from a set of images. Thin objects that have virtually no volumepose a special challengefor reconstruction with respect to shape representation and fusion of depth information. In this paper we present a dense pointbased reconstruction method that can deal with this special class of objects. We seek to jointly optimize a set of depth maps by treating each pixel as a point in space. Points are pulled towards a common surface by pairwise forces in an iterative scheme. The method also handles the problem of opposed surfaces by means of penalty forces. Efficient optimization is achieved by grouping points to superpixels and a spatial hashing approach for fast neighborhood queries. We show that the approach is on a par with state-of-the-art methods for standard multi view stereo settings and gives superior results for thin objects.</p><p>6 0.96754813 <a title="129-lda-6" href="./iccv-2013-Compensating_for_Motion_during_Direct-Global_Separation.html">82 iccv-2013-Compensating for Motion during Direct-Global Separation</a></p>
<p>7 0.96719575 <a title="129-lda-7" href="./iccv-2013-DeepFlow%3A_Large_Displacement_Optical_Flow_with_Deep_Matching.html">105 iccv-2013-DeepFlow: Large Displacement Optical Flow with Deep Matching</a></p>
<p>8 0.96703398 <a title="129-lda-8" href="./iccv-2013-Locally_Affine_Sparse-to-Dense_Matching_for_Motion_and_Occlusion_Estimation.html">256 iccv-2013-Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation</a></p>
<p>9 0.96630967 <a title="129-lda-9" href="./iccv-2013-Semi-dense_Visual_Odometry_for_a_Monocular_Camera.html">382 iccv-2013-Semi-dense Visual Odometry for a Monocular Camera</a></p>
<p>10 0.96614093 <a title="129-lda-10" href="./iccv-2013-Understanding_High-Level_Semantics_by_Modeling_Traffic_Patterns.html">433 iccv-2013-Understanding High-Level Semantics by Modeling Traffic Patterns</a></p>
<p>11 0.96544039 <a title="129-lda-11" href="./iccv-2013-Network_Principles_for_SfM%3A_Disambiguating_Repeated_Structures_with_Local_Context.html">289 iccv-2013-Network Principles for SfM: Disambiguating Repeated Structures with Local Context</a></p>
<p>12 0.96538389 <a title="129-lda-12" href="./iccv-2013-Support_Surface_Prediction_in_Indoor_Scenes.html">410 iccv-2013-Support Surface Prediction in Indoor Scenes</a></p>
<p>13 0.96506357 <a title="129-lda-13" href="./iccv-2013-Forward_Motion_Deblurring.html">174 iccv-2013-Forward Motion Deblurring</a></p>
<p>14 0.96310979 <a title="129-lda-14" href="./iccv-2013-A_Flexible_Scene_Representation_for_3D_Reconstruction_Using_an_RGB-D_Camera.html">9 iccv-2013-A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera</a></p>
<p>15 0.96258497 <a title="129-lda-15" href="./iccv-2013-Viewing_Real-World_Faces_in_3D.html">444 iccv-2013-Viewing Real-World Faces in 3D</a></p>
<p>16 0.96249062 <a title="129-lda-16" href="./iccv-2013-Joint_Subspace_Stabilization_for_Stereoscopic_Video.html">226 iccv-2013-Joint Subspace Stabilization for Stereoscopic Video</a></p>
<p>17 0.96248102 <a title="129-lda-17" href="./iccv-2013-Constructing_Adaptive_Complex_Cells_for_Robust_Visual_Tracking.html">89 iccv-2013-Constructing Adaptive Complex Cells for Robust Visual Tracking</a></p>
<p>18 0.96242487 <a title="129-lda-18" href="./iccv-2013-3D_Scene_Understanding_by_Voxel-CRF.html">2 iccv-2013-3D Scene Understanding by Voxel-CRF</a></p>
<p>19 0.96202415 <a title="129-lda-19" href="./iccv-2013-Estimating_Human_Pose_with_Flowing_Puppets.html">143 iccv-2013-Estimating Human Pose with Flowing Puppets</a></p>
<p>20 0.96195281 <a title="129-lda-20" href="./iccv-2013-SGTD%3A_Structure_Gradient_and_Texture_Decorrelating_Regularization_for_Image_Decomposition.html">364 iccv-2013-SGTD: Structure Gradient and Texture Decorrelating Regularization for Image Decomposition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
