<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>19 iccv-2013-A Learning-Based Approach to Reduce JPEG Artifacts in Image Matting</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-19" href="#">iccv2013-19</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>19 iccv-2013-A Learning-Based Approach to Reduce JPEG Artifacts in Image Matting</h1>
<br/><p>Source: <a title="iccv-2013-19-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Choi_A_Learning-Based_Approach_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Inchang Choi, Sunyeong Kim, Michael S. Brown, Yu-Wing Tai</p><p>Abstract: Single image matting techniques assume high-quality input images. The vast majority of images on the web and in personal photo collections are encoded using JPEG compression. JPEG images exhibit quantization artifacts that adversely affect the performance of matting algorithms. To address this situation, we propose a learning-based post-processing method to improve the alpha mattes extracted from JPEG images. Our approach learns a set of sparse dictionaries from training examples that are used to transfer details from high-quality alpha mattes to alpha mattes corrupted by JPEG compression. Three different dictionaries are defined to accommodate different object structure (long hair, short hair, and sharp boundaries). A back-projection criteria combined within an MRF framework is used to automatically select the best dictionary to apply on the object’s local boundary. We demonstrate that our method can produces superior results over existing state-of-the-art matting algorithms on a variety of inputs and compression levels.</p><p>Reference: <a title="iccv-2013-19-reference" href="../iccv2013_reference/iccv-2013-A_Learning-Based_Approach_to_Reduce_JPEG_Artifacts_in_Image_Matting_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Brown2 Yu-Wing Tai1 Korea Advanced Institute of Science and Technology (KAIST)1 National University of Singapore (NUS)2  Abstract Single image matting techniques assume high-quality input images. [sent-2, score-0.387]
</p><p>2 JPEG images exhibit quantization artifacts that adversely affect the performance of matting algorithms. [sent-4, score-0.499]
</p><p>3 To address this situation, we propose a learning-based post-processing method to improve the alpha mattes extracted from JPEG images. [sent-5, score-1.091]
</p><p>4 Our approach learns a set of sparse dictionaries from training examples that are used to transfer details from high-quality alpha mattes to alpha mattes corrupted by JPEG compression. [sent-6, score-2.212]
</p><p>5 Three different dictionaries are defined to accommodate different object structure (long hair, short hair, and sharp boundaries). [sent-7, score-0.142]
</p><p>6 We demonstrate that our method can produces superior results over existing state-of-the-art matting algorithms on a variety of inputs and compression levels. [sent-9, score-0.502]
</p><p>7 Introduction Single image matting is a problem that has been extensively studied in computer vision and graphics. [sent-11, score-0.381]
</p><p>8 Virtually all image matting techniques assume that the input image is free of notable compression artifacts. [sent-13, score-0.521]
</p><p>9 The vast majority of these images are encoded using JPEG compression that exhibits compression artifacts that adversely affect the quality of the extracted alpha matte. [sent-15, score-1.16]
</p><p>10 Figure 1demonstrates how various levels of JPEG compression (expressed as quality levels) affect the performance of several state-of-the-artmatting methods. [sent-16, score-0.213]
</p><p>11 The RMSE of alpha mattes extracted from several stateof-the-art methods are plotted for different JPEG compression qualities (1 = lowest; 12 = highest)1 . [sent-29, score-1.241]
</p><p>12 Compared to the RMSE of the pixel values, the RMSE of the corresponding alpha mattes increases dramatically as the compression ratio increases. [sent-30, score-1.196]
</p><p>13 Note that the root mean square error (RMSE) of the alpha mattes suffers worse than the RMSE of the compressed image itself. [sent-31, score-1.137]
</p><p>14 g quality 92), which represent common compression level used on camera phones and social media sites, have subtle artifacts that can adversely affect the resulting alpha mattes. [sent-33, score-0.986]
</p><p>15 The contribution of this paper is to propose a learningbased post-processing method for improving alpha mattes in the face of JPEG compression artifacts. [sent-34, score-1.215]
</p><p>16 Specifically, we propose a technique that learns a set of sparse dictionaries to transfer high-quality details to low-quality alpha mattes extracted from compressed images. [sent-35, score-1.254]
</p><p>17 Because matting boundaries are far more complicated than natural image boundaries, our methods uses three different dictionaries (i. [sent-36, score-0.488]
</p><p>18 While our approach cannot compete with mattes extracted from uncompressed input images, it is useful in improving the quality of alpha mattes over the 2Photoshop categorized quality 10-12 to be maximum quality, 8-9 to be high quality, 5-7 to be medium quality, and 1-4 to be it low quality. [sent-39, score-1.701]
</p><p>19 We tested our approach extensively on a variety of inputs and compression levels which demonstrates superior remedy results over state-of-the-art matting algorithms in comparisons with their initial corrupted alpha mattes. [sent-41, score-1.176]
</p><p>20 JPEG compression The Joint Photographic Experts Group (JPEG) compression standard developed over two decades ago is the most widely adopted image compression method to date (for further details see [25]). [sent-45, score-0.402]
</p><p>21 Because JPEG is lossy, the uncompressed image contains errors that are in the form of frequency domain ringing and blocking artifacts that are  collectively referred to as compression artifacts. [sent-46, score-0.326]
</p><p>22 For natural images it is often difficult to perceptually see these errors even for images with medium compression qualities around 5-7. [sent-47, score-0.184]
</p><p>23 While the compressed images may be perceptually acceptable, the resulting compression artifacts are well known to adversely affect low-level image processing routines. [sent-48, score-0.362]
</p><p>24 As shown in Figure 1, image matting is no exception. [sent-49, score-0.368]
</p><p>25 These approaches target low-quality compressed images and perform various types of filtering to reduce blocking and ringing artifacts. [sent-53, score-0.143]
</p><p>26 As shown in our results, we achieve better results than deblocking applied either as pre-processing to the input image or as post-processing to the alpha matte. [sent-55, score-0.793]
</p><p>27 Image matting Image matting approaches (for a nice overview see [26]) can be roughly classified into two categories: affinity-based methods and sampling-based methods. [sent-56, score-0.748]
</p><p>28 [19, 12, 11, 3]) estimated alpha values for the unknown region by propagating the known alpha values in accordance with the pixel affinities. [sent-59, score-1.322]
</p><p>29 Affinity-based approaches propagate alpha values well in uncompressed and maximum-quality JPEG images (e. [sent-60, score-0.719]
</p><p>30 However, these methods fail to effectively propagate the alpha values across block artifacts when an image is compressed. [sent-63, score-0.744]
</p><p>31 [4, 27, 7, 8]) estimate alpha mattes by sampling  the foreground and background color. [sent-66, score-1.101]
</p><p>32 For each pixel with an unassigned alpha value, these approaches find the most plausible pair of the foreground and the background pixels around it and solve the matting compositing equation with the sampled color pairs. [sent-67, score-1.078]
</p><p>33 As with affinity-based methods, sample-based methods are adversely affected by the ringing artifacts and quantization across different blocks. [sent-68, score-0.149]
</p><p>34 For both approaches, obtaining mattes with detailed structure is difficult due to the blurring effect introduced by the DCT quantization. [sent-69, score-0.401]
</p><p>35 This is often because these methods first apply a smoothing to the input image to reduce ringing and blocking artifacts (e. [sent-76, score-0.153]
</p><p>36 [9]) which can remove high-frequency information before the matting is applied. [sent-79, score-0.368]
</p><p>37 Our work, however, directly operates on alpha mattes instead of pixel intensity. [sent-81, score-1.062]
</p><p>38 Since matte boundaries are more complicated than natural image boundaries (e. [sent-82, score-0.318]
</p><p>39 Matting for JPEG images As previously discussed, we adopt a learning-based approach used in single image super-resolution for our matting problem. [sent-87, score-0.368]
</p><p>40 Our input is a low-quality alpha matte extracted from conventional matting algorithms, e. [sent-88, score-1.311]
</p><p>41 Our goal is to estimate a high-quality alpha matte by transferring details via a dictionary learned from highquality and low-quality alpha matte patch pairs. [sent-91, score-1.92]
</p><p>42 In the following, we denote y be the input low-quality alpha matte, x be the output high-quality alpha matte. [sent-92, score-1.341]
</p><p>43 Problem definition and overview Following the work from [29], we assume the alpha matte within each 8 8 block can be sparsely represented as a linear combination of a set of basis functions: y = Dlφ, (1) where φ ∈ Rk is a vector of sparse coefficients, e. [sent-95, score-0.928]
</p><p>44 k, and Dl is a dictionary containing basis functions learned from low-quality alpha mattes extracted from JPEG compressed images. [sent-100, score-1.261]
</p><p>45 In a similar context, we define the high-quality alpha matte patch as: x = Dhφ, (2)  where Dh is a dictionary learned from high-quality alpha mattes extracted from images without any compression. [sent-101, score-2.099]
</p><p>46 The learned dictionaries computed from different training examples group as long hair, short hair, and sharp boundary. [sent-103, score-0.145]
</p><p>47 The two dictionaries, Dl and Dh, are co-trained using a set of alpha matte pairs. [sent-105, score-0.895]
</p><p>48 Where each pair contains an alpha  × ×  matte extracted from the high-quality input image (either uncompressed or compressed at a high-quality, e. [sent-106, score-1.076]
</p><p>49 JPEG quality 12) and the alpha matte extracted from the same image with lower-quality compression, e. [sent-108, score-0.984]
</p><p>50 Hence, for a given low-quality alpha matte input, we can first estimate the sparse coefficients φ and then replace the low-quality dictionary Dl with the high-quality dictionary Dh to reconstruct a high-quality matte. [sent-112, score-1.137]
</p><p>51 Joint dictionary training  Sparse coding is used to learn the dictionary as follows: Dc = arg mDci,nZ= ? [sent-115, score-0.19]
</p><p>52 , xn} is the set of high-quality 8 8 alpha mattes and Yl = {y1, y2, . [sent-127, score-1.062]
</p><p>53 , yn} is the set of the corresponding low-quality 8 8 alpha mattes. [sent-130, score-0.661]
</p><p>54 Since the size  of the high- and low-quality alpha matte pairs are the same,  JPEGinputLow-quailtyaplhamate Figure3. [sent-131, score-0.895]
</p><p>55 Matte reconstruction from dictionary  ×  Our next step is to reconstruct a high-quality alpha matte given a low-quality alpha matte input. [sent-145, score-1.897]
</p><p>56 For each 8 8 block in the input alpha matte, we estimate the sparse coefficients by minimizing: φ∗ = argmφin? [sent-146, score-0.737]
</p><p>57 In order to guarantee compatibility between neighboring blocks, we follow [29] and use an overlapping window in the reconstructed high-quality alpha matte to constrain the coefficients estimation in Equation (6). [sent-152, score-1.001]
</p><p>58 ,(8)  where P is a matrix that extracts the overlapping region between the current patch and the previous patches, and w is the alpha values of the previously reconstructed alpha mattes in the overlapping areas. [sent-161, score-1.818]
</p><p>59 Optimal dictionary selection for dif erent regions along  matting boundary. [sent-164, score-0.463]
</p><p>60 Red: Long hair dictionary; Green: Short hair dictionary; Blue: Sharp boundary dictionary. [sent-165, score-0.145]
</p><p>61 Implementation: dictionary selection As previously mentioned, we found that using a single dictionary learned from generic images did not produce the best quality results. [sent-168, score-0.25]
</p><p>62 First, a highquality alpha matte is first reconstructed using all three dictionaries individually. [sent-172, score-1.035]
</p><p>63 This allows us to reconstruct a high-quality image patch using the matting composite equation [16]. [sent-174, score-0.426]
</p><p>64 Finally, the appropriateness of each dictionary is evaluated by measuring the RMSE between the compressed reconstructed patch and the original input patch. [sent-176, score-0.269]
</p><p>65 With our multiple dictionaries approach, hairy structures of matte can be better reconstructed. [sent-186, score-0.332]
</p><p>66 Figure 5 shows the comparisons of the reconstructed alpha matte between single dictionary approach and multiple dictionaries approach. [sent-190, score-1.113]
</p><p>67 Experimental results Our approach is tested extensively using the following matting algorithms: closed-form matting [12], KNN matting [3], global sampling matting [8], and learning-  based matting [30]. [sent-192, score-1.876]
</p><p>68 Closed-form matting [12] was applied to the uncompressed image and compressed image to produce the training example pairs3. [sent-199, score-0.501]
</p><p>69 Figure 6 shows several results using different quality factors and matting methods. [sent-202, score-0.428]
</p><p>70 In each example, the initial extracted matte is used as input in our method. [sent-203, score-0.282]
</p><p>71 The initial mattes all suffer from blurriness and blocky artifacts due to the input’s JPEG compression. [sent-208, score-0.511]
</p><p>72 Applying our 3We found little difference among the results when using other matting methods to prepare the training examples. [sent-209, score-0.368]
</p><p>73 We compared RMSE between results of matting algorithms and their reconstructed  10−3. [sent-260, score-0.419]
</p><p>74 GT21  GT25  GT26  alpha mattes by our algorithm. [sent-261, score-1.062]
</p><p>75 The unit is  CFM, KNN, GSM, and LBM stand for closed-form matting [12], KNN matting [3], global sampling matting [8], and learning-  based matting [30], respectively. [sent-262, score-1.495]
</p><p>76 These are applied in two ways: in one case, we deblocked the JPEG image and then applied matting; in the other case, we applied matting to the JPEG image and applied deblocking to the alpha matte. [sent-266, score-1.192]
</p><p>77 The alpha mattes extracted from the pre-processed deblocked input images are blurry and have poorly defined boundaries. [sent-267, score-1.16]
</p><p>78 Interestingly, post-processing the extracted alpha mattes with the deblocking algorithm gives better results. [sent-268, score-1.204]
</p><p>79 It preserves the shape well and succeeds in removing the blocky compression artifacts to some degree. [sent-269, score-0.233]
</p><p>80 When compare the results quantitatively, our approach produces the best alpha mattes with the minimum RMSE as compared to the ground truth alpha mattes. [sent-270, score-1.723]
</p><p>81 Not only does our algorithm eliminates the JPEG compression artifacts, but also results in well defined boundaries of the  target object. [sent-271, score-0.17]
</p><p>82 We compute the RMSE from the initial alpha mattes and those after applying our detail transfer. [sent-272, score-1.062]
</p><p>83 Our results reconstructed the hairs with sharper details but does not fully align with the ground truth alpha mattes. [sent-275, score-0.723]
</p><p>84 Finally, we apply our algorithm to images captured Images from both cell phone cameras have compressions quality between quality 10 and quality 11but closer to quality 10. [sent-278, score-0.299]
</p><p>85 Figure 8 shows our results which apply to the alpha mattes extracted from the global sampling matting [8] and closed-form matting [12] with the original files from the cell phone cameras as input. [sent-280, score-1.909]
</p><p>86 Note that compression artifacts show up in the estimated alpha mattes but they are almost invisible in the original input images. [sent-281, score-1.281]
</p><p>87 Our approach can successfully refines the alpha mattes with better visual quality. [sent-282, score-1.062]
</p><p>88 Discussion and conclusion We have presented a method to refine alpha mattes from  JPEG compressed images. [sent-284, score-1.137]
</p><p>89 While there is a previous work that targets matting for degraded image [13, 17], as far as we are aware, this is the first work to seriously address the problem of compression artifacts on image matting. [sent-285, score-0.582]
</p><p>90 Our method works directly on the alpha mattes using three separate dictionaries to accommodate various boundary structures as well as a back-projection method to select the appropriate dictionary for detail transfer. [sent-287, score-1.27]
</p><p>91 Our method is able to improve the current state-of-the-art image matting results and preforms better than applying JPEG deblocking to the input or extracted mattes. [sent-288, score-0.54]
</p><p>92 We are interested in extending this scheme to the wavelet-based JPEG2000 compression from cell phone cameras. [sent-290, score-0.193]
</p><p>93 2884  (JPEG quality 7), the third example is extracted by Global sampling matting [8] (quality 5), and the final example is extracted by learningbased matting [30] (JPEG quality 7). [sent-296, score-0.956]
</p><p>94 In the zoomed-in areas, images on the top were produced using JPEG alpha mattes, and the middles are our reconstructed alpha mattes. [sent-297, score-1.373]
</p><p>95 2885  ×  (b) Closed-form matting [12], (c) KNN matting [3], (d) Global sampling matting [8], (e) learning-based matting [30]. [sent-299, score-1.495]
</p><p>96 In the example, from the top, alpha mattes of JPEG input images (the first row), alpha mattes of deblocked input images and deblocked alpha mattes as post-processing by DeJPEG (the second and third rows), and by BM3D (the fourth and fifth rows), and our reconstructed alpha mattes (the final row). [sent-300, score-4.437]
</p><p>97 Zoomed-in regions show the input images, estimated alpha reconstructed alpha mattes and the corresponding composition respectively. [sent-307, score-1.793]
</p><p>98 Pointwise shapeadaptive dct for high-quality denoising and deblocking of grayscale and color images. [sent-347, score-0.146]
</p><p>99 Efficient learning-based image enhancement: Application to superresolution and compression artifact removal. [sent-371, score-0.149]
</p><p>100 A deblocking algorithm for jpeg compressed images using overcomplete wavelet representations. [sent-510, score-0.489]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('alpha', 0.661), ('mattes', 0.401), ('matting', 0.368), ('jpeg', 0.282), ('matte', 0.234), ('compression', 0.134), ('deblocking', 0.113), ('rmse', 0.101), ('dictionary', 0.095), ('compressed', 0.075), ('dictionaries', 0.072), ('artifacts', 0.066), ('hair', 0.066), ('quality', 0.06), ('uncompressed', 0.058), ('reconstructed', 0.051), ('deblocked', 0.05), ('dl', 0.047), ('adversely', 0.046), ('dejpeg', 0.038), ('phone', 0.037), ('ringing', 0.037), ('sharp', 0.036), ('boundaries', 0.036), ('blocky', 0.033), ('dh', 0.032), ('blocking', 0.031), ('knn', 0.03), ('extracted', 0.029), ('super', 0.025), ('dcz', 0.025), ('rmselong', 0.025), ('rmsesharp', 0.025), ('rmseshort', 0.025), ('xc', 0.025), ('tip', 0.024), ('mrf', 0.024), ('coefficients', 0.024), ('sampling', 0.023), ('perceptually', 0.022), ('hrhrp', 0.022), ('cell', 0.022), ('short', 0.021), ('photoshop', 0.019), ('rhemann', 0.019), ('input', 0.019), ('overcomplete', 0.019), ('affect', 0.019), ('learningbased', 0.019), ('kaist', 0.019), ('tappen', 0.019), ('compatibility', 0.018), ('patch', 0.018), ('dct', 0.018), ('saved', 0.018), ('block', 0.017), ('katkovnik', 0.017), ('compositing', 0.017), ('foi', 0.017), ('highquality', 0.017), ('long', 0.016), ('foreground', 0.016), ('ed', 0.016), ('sparse', 0.016), ('equation', 0.016), ('qualities', 0.016), ('dc', 0.016), ('nrf', 0.015), ('superresolution', 0.015), ('denoising', 0.015), ('korea', 0.015), ('structures', 0.015), ('degraded', 0.014), ('kwon', 0.014), ('file', 0.013), ('overlapping', 0.013), ('editing', 0.013), ('extensively', 0.013), ('digital', 0.013), ('accommodate', 0.013), ('argm', 0.013), ('boundary', 0.013), ('nice', 0.012), ('medium', 0.012), ('complicated', 0.012), ('reconstruct', 0.012), ('composite', 0.012), ('vast', 0.011), ('porter', 0.011), ('appropriateness', 0.011), ('blurriness', 0.011), ('bottoms', 0.011), ('eant', 0.011), ('gastal', 0.011), ('gsm', 0.011), ('hairs', 0.011), ('hairy', 0.011), ('orchard', 0.011), ('prabhu', 0.011), ('preforms', 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="19-tfidf-1" href="./iccv-2013-A_Learning-Based_Approach_to_Reduce_JPEG_Artifacts_in_Image_Matting.html">19 iccv-2013-A Learning-Based Approach to Reduce JPEG Artifacts in Image Matting</a></p>
<p>Author: Inchang Choi, Sunyeong Kim, Michael S. Brown, Yu-Wing Tai</p><p>Abstract: Single image matting techniques assume high-quality input images. The vast majority of images on the web and in personal photo collections are encoded using JPEG compression. JPEG images exhibit quantization artifacts that adversely affect the performance of matting algorithms. To address this situation, we propose a learning-based post-processing method to improve the alpha mattes extracted from JPEG images. Our approach learns a set of sparse dictionaries from training examples that are used to transfer details from high-quality alpha mattes to alpha mattes corrupted by JPEG compression. Three different dictionaries are defined to accommodate different object structure (long hair, short hair, and sharp boundaries). A back-projection criteria combined within an MRF framework is used to automatically select the best dictionary to apply on the object’s local boundary. We demonstrate that our method can produces superior results over existing state-of-the-art matting algorithms on a variety of inputs and compression levels.</p><p>2 0.45883968 <a title="19-tfidf-2" href="./iccv-2013-Motion-Aware_KNN_Laplacian_for_Video_Matting.html">275 iccv-2013-Motion-Aware KNN Laplacian for Video Matting</a></p>
<p>Author: Dingzeyu Li, Qifeng Chen, Chi-Keung Tang</p><p>Abstract: This paper demonstrates how the nonlocal principle benefits video matting via the KNN Laplacian, which comes with a straightforward implementation using motionaware K nearest neighbors. In hindsight, the fundamental problem to solve in video matting is to produce spatiotemporally coherent clusters of moving foreground pixels. When used as described, the motion-aware KNN Laplacian is effective in addressing this fundamental problem, as demonstrated by sparse user markups typically on only one frame in a variety of challenging examples featuring ambiguous foreground and background colors, changing topologies with disocclusion, significant illumination changes, fast motion, and motion blur. When working with existing Laplacian-based systems, our Laplacian is expected to benefit them immediately with improved clustering of moving foreground pixels.</p><p>3 0.091953792 <a title="19-tfidf-3" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>Author: Chenglong Bao, Jian-Feng Cai, Hui Ji</p><p>Abstract: In recent years, how to learn a dictionary from input images for sparse modelling has been one very active topic in image processing and recognition. Most existing dictionary learning methods consider an over-complete dictionary, e.g. the K-SVD method. Often they require solving some minimization problem that is very challenging in terms of computational feasibility and efficiency. However, if the correlations among dictionary atoms are not well constrained, the redundancy of the dictionary does not necessarily improve the performance of sparse coding. This paper proposed a fast orthogonal dictionary learning method for sparse image representation. With comparable performance on several image restoration tasks, the proposed method is much more computationally efficient than the over-complete dictionary based learning methods.</p><p>4 0.090126261 <a title="19-tfidf-4" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>Author: Hua Wang, Feiping Nie, Weidong Cai, Heng Huang</p><p>Abstract: Representing the raw input of a data set by a set of relevant codes is crucial to many computer vision applications. Due to the intrinsic sparse property of real-world data, dictionary learning, in which the linear decomposition of a data point uses a set of learned dictionary bases, i.e., codes, has demonstrated state-of-the-art performance. However, traditional dictionary learning methods suffer from three weaknesses: sensitivity to noisy and outlier samples, difficulty to determine the optimal dictionary size, and incapability to incorporate supervision information. In this paper, we address these weaknesses by learning a Semi-Supervised Robust Dictionary (SSR-D). Specifically, we use the ℓ2,0+ norm as the loss function to improve the robustness against outliers, and develop a new structured sparse regularization com, , tom. . cai@sydney . edu . au , heng@uta .edu make the learning tasks easier to deal with and reduce the computational cost. For example, in image tagging, instead of using the raw pixel-wise features, semi-local or patch- based features, such as SIFT and geometric blur, are usually more desirable to achieve better performance. In practice, finding a set of compact features bases, also referred to as dictionary, with enhanced representative and discriminative power, plays a significant role in building a successful computer vision system. In this paper, we explore this important problem by proposing a novel formulation and its solution for learning Semi-Supervised Robust Dictionary (SSRD), where we examine the challenges in dictionary learning, and seek opportunities to overcome them and improve the dictionary qualities. 1.1. Challenges in Dictionary Learning to incorporate the supervision information in dictionary learning, without incurring additional parameters. Moreover, the optimal dictionary size is automatically learned from the input data. Minimizing the derived objective function is challenging because it involves many non-smooth ℓ2,0+ -norm terms. We present an efficient algorithm to solve the problem with a rigorous proof of the convergence of the algorithm. Extensive experiments are presented to show the superior performance of the proposed method.</p><p>5 0.086099952 <a title="19-tfidf-5" href="./iccv-2013-Potts_Model%2C_Parametric_Maxflow_and_K-Submodular_Functions.html">324 iccv-2013-Potts Model, Parametric Maxflow and K-Submodular Functions</a></p>
<p>Author: Igor Gridchyn, Vladimir Kolmogorov</p><p>Abstract: The problem of minimizing the Potts energy function frequently occurs in computer vision applications. One way to tackle this NP-hard problem was proposed by Kovtun [20, 21]. It identifies a part of an optimal solution by running k maxflow computations, where k is the number of labels. The number of “labeled” pixels can be significant in some applications, e.g. 50-93% in our tests for stereo. We show how to reduce the runtime to O(log k) maxflow computations (or one parametric maxflow computation). Furthermore, the output of our algorithm allows to speed-up the subsequent alpha expansion for the unlabeled part, or can be used as it is for time-critical applications. To derive our technique, we generalize the algorithm of Felzenszwalb et al. [7] for Tree Metrics. We also show a connection to k-submodular functions from combinatorial optimization, and discuss k-submodular relaxations for general energy functions.</p><p>6 0.064259768 <a title="19-tfidf-6" href="./iccv-2013-Multi-attributed_Dictionary_Learning_for_Sparse_Coding.html">276 iccv-2013-Multi-attributed Dictionary Learning for Sparse Coding</a></p>
<p>7 0.061543997 <a title="19-tfidf-7" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>8 0.061330266 <a title="19-tfidf-8" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>9 0.060344633 <a title="19-tfidf-9" href="./iccv-2013-Learning_View-Invariant_Sparse_Representations_for_Cross-View_Action_Recognition.html">244 iccv-2013-Learning View-Invariant Sparse Representations for Cross-View Action Recognition</a></p>
<p>10 0.059824586 <a title="19-tfidf-10" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<p>11 0.059441835 <a title="19-tfidf-11" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>12 0.05120239 <a title="19-tfidf-12" href="./iccv-2013-Dynamic_Probabilistic_Volumetric_Models.html">128 iccv-2013-Dynamic Probabilistic Volumetric Models</a></p>
<p>13 0.050216567 <a title="19-tfidf-13" href="./iccv-2013-Robust_Object_Tracking_with_Online_Multi-lifespan_Dictionary_Learning.html">359 iccv-2013-Robust Object Tracking with Online Multi-lifespan Dictionary Learning</a></p>
<p>14 0.045080166 <a title="19-tfidf-14" href="./iccv-2013-A_New_Image_Quality_Metric_for_Image_Auto-denoising.html">23 iccv-2013-A New Image Quality Metric for Image Auto-denoising</a></p>
<p>15 0.043523703 <a title="19-tfidf-15" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>16 0.041167751 <a title="19-tfidf-16" href="./iccv-2013-Coupled_Dictionary_and_Feature_Space_Learning_with_Applications_to_Cross-Domain_Image_Synthesis_and_Recognition.html">96 iccv-2013-Coupled Dictionary and Feature Space Learning with Applications to Cross-Domain Image Synthesis and Recognition</a></p>
<p>17 0.03941229 <a title="19-tfidf-17" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>18 0.036432512 <a title="19-tfidf-18" href="./iccv-2013-Constant_Time_Weighted_Median_Filtering_for_Stereo_Matching_and_Beyond.html">88 iccv-2013-Constant Time Weighted Median Filtering for Stereo Matching and Beyond</a></p>
<p>19 0.034583945 <a title="19-tfidf-19" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<p>20 0.032749403 <a title="19-tfidf-20" href="./iccv-2013-Depth_from_Combining_Defocus_and_Correspondence_Using_Light-Field_Cameras.html">108 iccv-2013-Depth from Combining Defocus and Correspondence Using Light-Field Cameras</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.071), (1, 0.009), (2, -0.025), (3, 0.008), (4, -0.094), (5, -0.027), (6, -0.063), (7, -0.028), (8, -0.014), (9, -0.027), (10, -0.007), (11, 0.023), (12, 0.066), (13, 0.007), (14, -0.036), (15, 0.018), (16, -0.034), (17, -0.039), (18, 0.007), (19, 0.024), (20, 0.027), (21, -0.019), (22, 0.004), (23, -0.015), (24, 0.001), (25, 0.047), (26, 0.008), (27, -0.026), (28, 0.038), (29, -0.032), (30, -0.01), (31, -0.026), (32, 0.05), (33, 0.08), (34, -0.047), (35, 0.011), (36, 0.047), (37, 0.026), (38, 0.008), (39, 0.031), (40, -0.036), (41, 0.032), (42, -0.065), (43, -0.125), (44, 0.062), (45, 0.004), (46, 0.282), (47, 0.019), (48, 0.073), (49, 0.129)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91679209 <a title="19-lsi-1" href="./iccv-2013-A_Learning-Based_Approach_to_Reduce_JPEG_Artifacts_in_Image_Matting.html">19 iccv-2013-A Learning-Based Approach to Reduce JPEG Artifacts in Image Matting</a></p>
<p>Author: Inchang Choi, Sunyeong Kim, Michael S. Brown, Yu-Wing Tai</p><p>Abstract: Single image matting techniques assume high-quality input images. The vast majority of images on the web and in personal photo collections are encoded using JPEG compression. JPEG images exhibit quantization artifacts that adversely affect the performance of matting algorithms. To address this situation, we propose a learning-based post-processing method to improve the alpha mattes extracted from JPEG images. Our approach learns a set of sparse dictionaries from training examples that are used to transfer details from high-quality alpha mattes to alpha mattes corrupted by JPEG compression. Three different dictionaries are defined to accommodate different object structure (long hair, short hair, and sharp boundaries). A back-projection criteria combined within an MRF framework is used to automatically select the best dictionary to apply on the object’s local boundary. We demonstrate that our method can produces superior results over existing state-of-the-art matting algorithms on a variety of inputs and compression levels.</p><p>2 0.70037377 <a title="19-lsi-2" href="./iccv-2013-Motion-Aware_KNN_Laplacian_for_Video_Matting.html">275 iccv-2013-Motion-Aware KNN Laplacian for Video Matting</a></p>
<p>Author: Dingzeyu Li, Qifeng Chen, Chi-Keung Tang</p><p>Abstract: This paper demonstrates how the nonlocal principle benefits video matting via the KNN Laplacian, which comes with a straightforward implementation using motionaware K nearest neighbors. In hindsight, the fundamental problem to solve in video matting is to produce spatiotemporally coherent clusters of moving foreground pixels. When used as described, the motion-aware KNN Laplacian is effective in addressing this fundamental problem, as demonstrated by sparse user markups typically on only one frame in a variety of challenging examples featuring ambiguous foreground and background colors, changing topologies with disocclusion, significant illumination changes, fast motion, and motion blur. When working with existing Laplacian-based systems, our Laplacian is expected to benefit them immediately with improved clustering of moving foreground pixels.</p><p>3 0.47434834 <a title="19-lsi-3" href="./iccv-2013-A_Generalized_Low-Rank_Appearance_Model_for_Spatio-temporally_Correlated_Rain_Streaks.html">15 iccv-2013-A Generalized Low-Rank Appearance Model for Spatio-temporally Correlated Rain Streaks</a></p>
<p>Author: Yi-Lei Chen, Chiou-Ting Hsu</p><p>Abstract: In this paper, we propose a novel low-rank appearance model for removing rain streaks. Different from previous work, our method needs neither rain pixel detection nor time-consuming dictionary learning stage. Instead, as rain streaks usually reveal similar and repeated patterns on imaging scene, we propose and generalize a low-rank model from matrix to tensor structure in order to capture the spatio-temporally correlated rain streaks. With the appearance model, we thus remove rain streaks from image/video (and also other high-order image structure) in a unified way. Our experimental results demonstrate competitive (or even better) visual quality and efficient run-time in comparison with state of the art.</p><p>4 0.4626748 <a title="19-lsi-4" href="./iccv-2013-Restoring_an_Image_Taken_through_a_Window_Covered_with_Dirt_or_Rain.html">351 iccv-2013-Restoring an Image Taken through a Window Covered with Dirt or Rain</a></p>
<p>Author: David Eigen, Dilip Krishnan, Rob Fergus</p><p>Abstract: Photographs taken through a window are often compromised by dirt or rain present on the window surface. Common cases of this include pictures taken from inside a vehicle, or outdoor security cameras mounted inside a protective enclosure. At capture time, defocus can be used to remove the artifacts, but this relies on achieving a shallow depth-of-field and placement of the camera close to the window. Instead, we present a post-capture image processing solution that can remove localized rain and dirt artifacts from a single image. We collect a dataset of clean/corrupted image pairs which are then used to train a specialized form of convolutional neural network. This learns how to map corrupted image patches to clean ones, implicitly capturing the characteristic appearance of dirt and water droplets in natural images. Our models demonstrate effective removal of dirt and rain in outdoor test conditions.</p><p>5 0.44329494 <a title="19-lsi-5" href="./iccv-2013-A_New_Image_Quality_Metric_for_Image_Auto-denoising.html">23 iccv-2013-A New Image Quality Metric for Image Auto-denoising</a></p>
<p>Author: Xiangfei Kong, Kuan Li, Qingxiong Yang, Liu Wenyin, Ming-Hsuan Yang</p><p>Abstract: This paper proposes a new non-reference image quality metric that can be adopted by the state-of-the-art image/video denoising algorithms for auto-denoising. The proposed metric is extremely simple and can be implemented in four lines of Matlab code1. The basic assumption employed by the proposed metric is that the noise should be independent of the original image. A direct measurement of this dependence is, however, impractical due to the relatively low accuracy of existing denoising method. The proposed metric thus aims at maximizing the structure similarity between the input noisy image and the estimated image noise around homogeneous regions and the structure similarity between the input noisy image and the denoised image around highly-structured regions, and is computed as the linear correlation coefficient of the two corresponding structure similarity maps. Numerous experimental results demonstrate that the proposed metric not only outperforms the current state-of-the-art non-reference quality metric quantitatively and qualitatively, but also better maintains temporal coherence when used for video denoising. ˜</p><p>6 0.43066674 <a title="19-lsi-6" href="./iccv-2013-Perceptual_Fidelity_Aware_Mean_Squared_Error.html">312 iccv-2013-Perceptual Fidelity Aware Mean Squared Error</a></p>
<p>7 0.40156356 <a title="19-lsi-7" href="./iccv-2013-Fast_Sparsity-Based_Orthogonal_Dictionary_Learning_for_Image_Restoration.html">161 iccv-2013-Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration</a></p>
<p>8 0.38706222 <a title="19-lsi-8" href="./iccv-2013-Robust_Dictionary_Learning_by_Error_Source_Decomposition.html">354 iccv-2013-Robust Dictionary Learning by Error Source Decomposition</a></p>
<p>9 0.38175577 <a title="19-lsi-9" href="./iccv-2013-Single-Patch_Low-Rank_Prior_for_Non-pointwise_Impulse_Noise_Removal.html">394 iccv-2013-Single-Patch Low-Rank Prior for Non-pointwise Impulse Noise Removal</a></p>
<p>10 0.38054544 <a title="19-lsi-10" href="./iccv-2013-DCSH_-_Matching_Patches_in_RGBD_Images.html">101 iccv-2013-DCSH - Matching Patches in RGBD Images</a></p>
<p>11 0.35749406 <a title="19-lsi-11" href="./iccv-2013-Deterministic_Fitting_of_Multiple_Structures_Using_Iterative_MaxFS_with_Inlier_Scale_Estimation.html">113 iccv-2013-Deterministic Fitting of Multiple Structures Using Iterative MaxFS with Inlier Scale Estimation</a></p>
<p>12 0.35211489 <a title="19-lsi-12" href="./iccv-2013-Anchored_Neighborhood_Regression_for_Fast_Example-Based_Super-Resolution.html">51 iccv-2013-Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</a></p>
<p>13 0.34067839 <a title="19-lsi-13" href="./iccv-2013-Cross-Field_Joint_Image_Restoration_via_Scale_Map.html">98 iccv-2013-Cross-Field Joint Image Restoration via Scale Map</a></p>
<p>14 0.33179352 <a title="19-lsi-14" href="./iccv-2013-Semi-supervised_Robust_Dictionary_Learning_via_Efficient_l-Norms_Minimization.html">384 iccv-2013-Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization</a></p>
<p>15 0.28354269 <a title="19-lsi-15" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>16 0.2796669 <a title="19-lsi-16" href="./iccv-2013-Online_Video_SEEDS_for_Temporal_Window_Objectness.html">299 iccv-2013-Online Video SEEDS for Temporal Window Objectness</a></p>
<p>17 0.27835345 <a title="19-lsi-17" href="./iccv-2013-Illuminant_Chromaticity_from_Image_Sequences.html">207 iccv-2013-Illuminant Chromaticity from Image Sequences</a></p>
<p>18 0.27505627 <a title="19-lsi-18" href="./iccv-2013-A_Max-Margin_Perspective_on_Sparse_Representation-Based_Classification.html">20 iccv-2013-A Max-Margin Perspective on Sparse Representation-Based Classification</a></p>
<p>19 0.27105433 <a title="19-lsi-19" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<p>20 0.26585144 <a title="19-lsi-20" href="./iccv-2013-Person_Re-identification_by_Salience_Matching.html">313 iccv-2013-Person Re-identification by Salience Matching</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.064), (26, 0.096), (31, 0.122), (42, 0.057), (45, 0.011), (48, 0.018), (64, 0.025), (73, 0.031), (89, 0.125), (98, 0.282)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77256078 <a title="19-lda-1" href="./iccv-2013-A_Learning-Based_Approach_to_Reduce_JPEG_Artifacts_in_Image_Matting.html">19 iccv-2013-A Learning-Based Approach to Reduce JPEG Artifacts in Image Matting</a></p>
<p>Author: Inchang Choi, Sunyeong Kim, Michael S. Brown, Yu-Wing Tai</p><p>Abstract: Single image matting techniques assume high-quality input images. The vast majority of images on the web and in personal photo collections are encoded using JPEG compression. JPEG images exhibit quantization artifacts that adversely affect the performance of matting algorithms. To address this situation, we propose a learning-based post-processing method to improve the alpha mattes extracted from JPEG images. Our approach learns a set of sparse dictionaries from training examples that are used to transfer details from high-quality alpha mattes to alpha mattes corrupted by JPEG compression. Three different dictionaries are defined to accommodate different object structure (long hair, short hair, and sharp boundaries). A back-projection criteria combined within an MRF framework is used to automatically select the best dictionary to apply on the object’s local boundary. We demonstrate that our method can produces superior results over existing state-of-the-art matting algorithms on a variety of inputs and compression levels.</p><p>2 0.76617038 <a title="19-lda-2" href="./iccv-2013-Unsupervised_Domain_Adaptation_by_Domain_Invariant_Projection.html">435 iccv-2013-Unsupervised Domain Adaptation by Domain Invariant Projection</a></p>
<p>Author: Mahsa Baktashmotlagh, Mehrtash T. Harandi, Brian C. Lovell, Mathieu Salzmann</p><p>Abstract: Domain-invariant representations are key to addressing the domain shift problem where the training and test examples follow different distributions. Existing techniques that have attempted to match the distributions of the source and target domains typically compare these distributions in the original feature space. This space, however, may not be directly suitable for such a comparison, since some of the features may have been distorted by the domain shift, or may be domain specific. In this paper, we introduce a Domain Invariant Projection approach: An unsupervised domain adaptation method that overcomes this issue by extracting the information that is invariant across the source and target domains. More specifically, we learn a projection of the data to a low-dimensional latent space where the distance between the empirical distributions of the source and target examples is minimized. We demonstrate the effectiveness of our approach on the task of visual object recognition and show that it outperforms state-of-the-art methods on a standard domain adaptation benchmark dataset.</p><p>3 0.75995743 <a title="19-lda-3" href="./iccv-2013-Modeling_the_Calibration_Pipeline_of_the_Lytro_Camera_for_High_Quality_Light-Field_Image_Reconstruction.html">271 iccv-2013-Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction</a></p>
<p>Author: Donghyeon Cho, Minhaeng Lee, Sunyeong Kim, Yu-Wing Tai</p><p>Abstract: Light-field imaging systems have got much attention recently as the next generation camera model. A light-field imaging system consists of three parts: data acquisition, manipulation, and application. Given an acquisition system, it is important to understand how a light-field camera converts from its raw image to its resulting refocused image. In this paper, using the Lytro camera as an example, we describe step-by-step procedures to calibrate a raw light-field image. In particular, we are interested in knowing the spatial and angular coordinates of the micro lens array and the resampling process for image reconstruction. Since Lytro uses a hexagonal arrangement of a micro lens image, additional treatments in calibration are required. After calibration, we analyze and compare the performances of several resampling methods for image reconstruction with and without calibration. Finally, a learning based interpolation method is proposed which demonstrates a higher quality image reconstruction than previous interpolation methods including a method used in Lytro software.</p><p>4 0.75847173 <a title="19-lda-4" href="./iccv-2013-Unbiased_Metric_Learning%3A_On_the_Utilization_of_Multiple_Datasets_and_Web_Images_for_Softening_Bias.html">431 iccv-2013-Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias</a></p>
<p>Author: Chen Fang, Ye Xu, Daniel N. Rockmore</p><p>Abstract: Many standard computer vision datasets exhibit biases due to a variety of sources including illumination condition, imaging system, and preference of dataset collectors. Biases like these can have downstream effects in the use of vision datasets in the construction of generalizable techniques, especially for the goal of the creation of a classification system capable of generalizing to unseen and novel datasets. In this work we propose Unbiased Metric Learning (UML), a metric learning approach, to achieve this goal. UML operates in the following two steps: (1) By varying hyperparameters, it learns a set of less biased candidate distance metrics on training examples from multiple biased datasets. The key idea is to learn a neighborhood for each example, which consists of not only examples of the same category from the same dataset, but those from other datasets. The learning framework is based on structural SVM. (2) We do model validation on a set of weakly-labeled web images retrieved by issuing class labels as keywords to search engine. The metric with best validationperformance is selected. Although the web images sometimes have noisy labels, they often tend to be less biased, which makes them suitable for the validation set in our task. Cross-dataset image classification experiments are carried out. Results show significant performance improvement on four well-known computer vision datasets.</p><p>5 0.75679111 <a title="19-lda-5" href="./iccv-2013-Unifying_Nuclear_Norm_and_Bilinear_Factorization_Approaches_for_Low-Rank_Matrix_Decomposition.html">434 iccv-2013-Unifying Nuclear Norm and Bilinear Factorization Approaches for Low-Rank Matrix Decomposition</a></p>
<p>Author: Ricardo Cabral, Fernando De_La_Torre, João P. Costeira, Alexandre Bernardino</p><p>Abstract: Low rank models have been widely usedfor the representation of shape, appearance or motion in computer vision problems. Traditional approaches to fit low rank models make use of an explicit bilinear factorization. These approaches benefit from fast numerical methods for optimization and easy kernelization. However, they suffer from serious local minima problems depending on the loss function and the amount/type of missing data. Recently, these lowrank models have alternatively been formulated as convex problems using the nuclear norm regularizer; unlike factorization methods, their numerical solvers are slow and it is unclear how to kernelize them or to impose a rank a priori. This paper proposes a unified approach to bilinear factorization and nuclear norm regularization, that inherits the benefits of both. We analyze the conditions under which these approaches are equivalent. Moreover, based on this analysis, we propose a new optimization algorithm and a “rank continuation ” strategy that outperform state-of-theart approaches for Robust PCA, Structure from Motion and Photometric Stereo with outliers and missing data.</p><p>6 0.71695077 <a title="19-lda-6" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>7 0.70426512 <a title="19-lda-7" href="./iccv-2013-A_Unified_Video_Segmentation_Benchmark%3A_Annotation%2C_Metrics_and_Analysis.html">33 iccv-2013-A Unified Video Segmentation Benchmark: Annotation, Metrics and Analysis</a></p>
<p>8 0.63326192 <a title="19-lda-8" href="./iccv-2013-Robust_Matrix_Factorization_with_Unknown_Noise.html">357 iccv-2013-Robust Matrix Factorization with Unknown Noise</a></p>
<p>9 0.63199031 <a title="19-lda-9" href="./iccv-2013-Frustratingly_Easy_NBNN_Domain_Adaptation.html">181 iccv-2013-Frustratingly Easy NBNN Domain Adaptation</a></p>
<p>10 0.62740952 <a title="19-lda-10" href="./iccv-2013-Transfer_Feature_Learning_with_Joint_Distribution_Adaptation.html">427 iccv-2013-Transfer Feature Learning with Joint Distribution Adaptation</a></p>
<p>11 0.62428039 <a title="19-lda-11" href="./iccv-2013-Domain_Adaptive_Classification.html">123 iccv-2013-Domain Adaptive Classification</a></p>
<p>12 0.59979618 <a title="19-lda-12" href="./iccv-2013-Unsupervised_Visual_Domain_Adaptation_Using_Subspace_Alignment.html">438 iccv-2013-Unsupervised Visual Domain Adaptation Using Subspace Alignment</a></p>
<p>13 0.59886289 <a title="19-lda-13" href="./iccv-2013-Motion-Aware_KNN_Laplacian_for_Video_Matting.html">275 iccv-2013-Motion-Aware KNN Laplacian for Video Matting</a></p>
<p>14 0.58847082 <a title="19-lda-14" href="./iccv-2013-Characterizing_Layouts_of_Outdoor_Scenes_Using_Spatial_Topic_Processes.html">72 iccv-2013-Characterizing Layouts of Outdoor Scenes Using Spatial Topic Processes</a></p>
<p>15 0.5883593 <a title="19-lda-15" href="./iccv-2013-3DNN%3A_Viewpoint_Invariant_3D_Geometry_Matching_for_Scene_Understanding.html">1 iccv-2013-3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding</a></p>
<p>16 0.58428001 <a title="19-lda-16" href="./iccv-2013-Action_Recognition_with_Actons.html">38 iccv-2013-Action Recognition with Actons</a></p>
<p>17 0.5783689 <a title="19-lda-17" href="./iccv-2013-Super-resolution_via_Transform-Invariant_Group-Sparse_Regularization.html">408 iccv-2013-Super-resolution via Transform-Invariant Group-Sparse Regularization</a></p>
<p>18 0.57811981 <a title="19-lda-18" href="./iccv-2013-Towards_Motion_Aware_Light_Field_Video_for_Dynamic_Scenes.html">423 iccv-2013-Towards Motion Aware Light Field Video for Dynamic Scenes</a></p>
<p>19 0.57563347 <a title="19-lda-19" href="./iccv-2013-Recognizing_Text_with_Perspective_Distortion_in_Natural_Scenes.html">345 iccv-2013-Recognizing Text with Perspective Distortion in Natural Scenes</a></p>
<p>20 0.57558554 <a title="19-lda-20" href="./iccv-2013-Bayesian_Robust_Matrix_Factorization_for_Image_and_Video_Processing.html">60 iccv-2013-Bayesian Robust Matrix Factorization for Image and Video Processing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
