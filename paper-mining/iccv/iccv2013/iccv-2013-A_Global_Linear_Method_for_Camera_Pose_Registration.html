<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 iccv-2013-A Global Linear Method for Camera Pose Registration</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-17" href="#">iccv2013-17</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>17 iccv-2013-A Global Linear Method for Camera Pose Registration</h1>
<br/><p>Source: <a title="iccv-2013-17-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Jiang_A_Global_Linear_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Nianjuan Jiang, Zhaopeng Cui, Ping Tan</p><p>Abstract: We present a linear method for global camera pose registration from pairwise relative poses encoded in essential matrices. Our method minimizes an approximate geometric error to enforce the triangular relationship in camera triplets. This formulation does not suffer from the typical ‘unbalanced scale ’ problem in linear methods relying on pairwise translation direction constraints, i.e. an algebraic error; nor the system degeneracy from collinear motion. In the case of three cameras, our method provides a good linear approximation of the trifocal tensor. It can be directly scaled up to register multiple cameras. The results obtained are accurate for point triangulation and can serve as a good initialization for final bundle adjustment. We evaluate the algorithm performance with different types of data and demonstrate its effectiveness. Our system produces good accuracy, robustness, and outperforms some well-known systems on efficiency.</p><p>Reference: <a title="iccv-2013-17-reference" href="../iccv2013_reference/iccv-2013-A_Global_Linear_Method_for_Camera_Pose_Registration_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Our method minimizes an approximate geometric error to enforce the triangular relationship in camera triplets. [sent-2, score-0.274]
</p><p>2 an algebraic error; nor the system degeneracy from collinear motion. [sent-5, score-0.402]
</p><p>3 Introduction Structure-from-motion  (SfM) methods  simultaneously  estimate scene structure and camera motion from multiple images. [sent-12, score-0.319]
</p><p>4 First, relative poses between camera pairs or triplets are computed from matched image feature points, e. [sent-14, score-0.376]
</p><p>5 Second, all camera poses (including orientations and positions) and scene point coordinates are recovered in a global coordinate system according to these relative poses. [sent-17, score-0.475]
</p><p>6 Some well-known systems, such as [36, 2], compute camera poses in an incremental fashion, ∗These authors contributed equally  to  this work. [sent-25, score-0.298]
</p><p>7 Thus, it is highly desirable that all  camera poses are solved simultaneously for efficiency and accuracy. [sent-32, score-0.298]
</p><p>8 [3] derived a novel linear algorithm that is robust to different camera baseline lengths. [sent-39, score-0.274]
</p><p>9 Yet it still suffers from the same degeneracy as [13] for collinear cameras (e. [sent-40, score-0.448]
</p><p>10 Unlike earlier algebraic methods, we compute the camera positions (translations) by minimizing a geometric error the Euclidean distance between the camera centers and the lines collinear with their corresponding baselines. [sent-47, score-0.882]
</p><p>11 This novel approach generates more precise results, and does not degenerate with collinear camera motion. [sent-48, score-0.506]
</p><p>12 We want to stress that the robustness with collinear motion is an important advantage, since collinear motion is common (e. [sent-49, score-0.662]
</p><p>13 Furthermore, our estimation of camera poses does not involve reconstructing any 3D point. [sent-52, score-0.298]
</p><p>14 Effectively, we first solve the ‘motion’ camera poses, and then solve the ‘structure’ scene points. [sent-53, score-0.329]
</p><p>15 Once the camera poses are recovered, the scene points can be reconstructed from –  –  –  nearby cameras. [sent-56, score-0.369]
</p><p>16 In the special case of three cameras, our algorithm effectively computes the trifocal tensor from three essential matrices. [sent-57, score-0.362]
</p><p>17 In our experiment, we find that our method is more robust than the four-point algorithm [26] which solves trifocal tensor from three calibrated images. [sent-58, score-0.365]
</p><p>18 Many well-known SfM systems take a sequential [3 1, 36, 2] or hierarchical [11, 22, 17] approach to register cameras incrementally to a global coordinate system from their pairwise relative poses. [sent-72, score-0.282]
</p><p>19 Factorization based 3D reconstruction was proposed by Tomasi and Kanade [39] to recover all camera poses and 3D points simultaneously under weak perspective projection. [sent-75, score-0.373]
</p><p>20 Some global methods solve all camera poses together in two steps. [sent-79, score-0.368]
</p><p>21 Typically, they first compute camera rotations and solve translations in the next step. [sent-80, score-0.407]
</p><p>22 While global rotations can be computed robustly and accurately by rotation averaging [15], translations are difficult because the input pairwise relative translations are only known up to a scale. [sent-82, score-0.441]
</p><p>23 The pioneer works [13, 4] solved translations according to linear equations derived from pairwise relative translation directions. [sent-83, score-0.365]
</p><p>24 These earlier methods suffer from degeneracy of collinear camera motion and unbalanced constraint weighting caused by different camera baseline length. [sent-84, score-0.91]
</p><p>25 [7] computed homographies to glue individual triplet reconstructions by loop analysis and nonlinear optimization. [sent-87, score-0.321]
</p><p>26 [35] registered individual pairwise reconstructions by solving their individual global scaling and translation in a robust linear system. [sent-89, score-0.291]
</p><p>27 However, it still suffers from the degeneracy of collinear camera motion like [13, 4]. [sent-94, score-0.636]
</p><p>28 Other global methods solve all camera poses and 3D scene points at once. [sent-96, score-0.404]
</p><p>29 In the special case of three cameras, the camera geometry is fully captured by a trifocal tensor. [sent-104, score-0.485]
</p><p>30 Effectively, our method provides a linear solution for trifocal tensor from three essential matrices (i. [sent-107, score-0.362]
</p><p>31 An essential matrix Eij between two images i,j provides the relative rotation Rij and the translation direction tij . [sent-114, score-0.366]
</p><p>32 i sO au 3r goal rist toon recover aaltlr tihxe a nabds toliust ea camera poses itno a global ocaolo isrd itnoa rteec system. [sent-116, score-0.336]
</p><p>33 tWhee use a rotation matrix Ri and a translation vector ci to denote the orientation and position of the i-th camera (1 ≤ i ≤ N). [sent-117, score-0.697]
</p><p>34 In real data, these equations mweail n sno etq uhoalldit precisely sacnadl we nne reeda lto d afitand, a esseet of Ri, ci that best satisfy these equations. [sent-122, score-0.303]
</p><p>35 Secondly, the camera poses should be solved separately from the scene points. [sent-126, score-0.334]
</p><p>36 There are often much more scene points than cameras so that solving camera poses without scene points will significantly reduce the number of unknowns. [sent-127, score-0.465]
</p><p>37 We first apply the linear method described in [24] to compute the global camera rotations Ri. [sent-128, score-0.337]
</p><p>38 Once all rotations are fixed, we then solve all camera centers (ci, 1 ≤ i ≤ N) without reconstructing any 3D point. [sent-131, score-0.384]
</p><p>39 Translation Registration Given the global camera rotations computed in the previous section, we first transform each tij to the global rotation reference frame as cij = −R? [sent-133, score-0.746]
</p><p>40 The constraint on camera centers in Equation =(1) − can be written as in [13], cij  (cj  −  ci) = 0. [sent-135, score-0.485]
</p><p>41 Hhoisw iesv ae lri, equations oobnt aaibnoeudt this way degenerate for collinear camera motion. [sent-139, score-0.57]
</p><p>42 In fact, Equation (2) minimizes the cross product between cij and the baseline direction cj −ci. [sent-142, score-0.584]
</p><p>43 The  ×  relative translation cij , cik , and cjk between camera pairs are known. [sent-148, score-1.041]
</p><p>44 We need to estimate camera centers ci, cj , and ck. [sent-149, score-0.618]
</p><p>45 Ideally, the three unit vectors cij , cik , and cjk should be coplanar. [sent-150, score-0.661]
</p><p>46 Here, | |ci − cj | | is the distance between ci and cj . [sent-168, score-0.911]
</p><p>47 ) = | |cj ck −| | /c| ||c|i/ cj −| −| are effectively the baseline length ) ra =tio ||sc. [sent-173, score-0.56]
</p><p>48 To linearize it, we observe that  sjikj  −c  | |ci  −  cj  | |cik = | |ci  −  cj  −||c  | |Ri (θ? [sent-179, score-0.761]
</p><p>49 (4)  Here, Ri (φ) is the rotation matrix around the axis  cij  ×  for an angle φ (counter-clockwise). [sent-182, score-0.345]
</p><p>50 Thus we obtain th×e following linear equation, cik  2ck  −  ci  −  cj  = Ri(θi? [sent-183, score-0.816]
</p><p>51 (5)  Note Rj (·) is a rotation matrix around the direction cij cjk. [sent-186, score-0.312]
</p><p>52 Similarly, we can oonb mtaiant rtihxe following et dwior elictnieoanr equations of camera centers by assuming cik and cjk are free from error respectively, 2cj  −  ci  −  ck  2ci −cj −ck  Ri(−θi? [sent-187, score-1.231]
</p><p>53 =  −  −  (7) Solving these three linear equations can determine the camera centers. [sent-192, score-0.293]
</p><p>54 Note that Equation (5) does not require the orientation cj − ci to be the same as cij . [sent-193, score-0.778]
</p><p>55 This introduces a rotation ambiguity in the plane defined by the camera centers. [sent-194, score-0.383]
</p><p>56 We can solve it by computing the average rotation to align cj ci, ck ci and ck cj with the projection of cij , cik and cjk in the− camera plane, respectively, after the initial registration. [sent-195, score-2.228]
</p><p>57 Calculating baseline length ratios by the sine angles as described earlier is only valid when cij, cik and cjk are not collinear. [sent-197, score-0.539]
</p><p>58 In order to be robust regardless of the type of camera motion, we compute −  −  −  483  all baseline length ratios from locally reconstructed scene points. [sent-198, score-0.381]
</p><p>59 The translation registration does not involve reconstructing any scene point in the global coordinate system. [sent-206, score-0.314]
</p><p>60 Given a triplet graph (see definition in Section 5), we collect all equations (i. [sent-210, score-0.332]
</p><p>61 the rotation ambiguity in all triplets share the same rotation axis), there is a global in-plane rotation ambiguity similar to the three-camera case. [sent-220, score-0.502]
</p><p>62 Therefore, the unknown camera centers are implicitly given different weights depending on the number of constraints containing that particular camera when we solve for Ac = 0. [sent-223, score-0.543]
</p><p>63 Thus, for every camera i, we count the number of triplet constraints containing its center, denoted by Ki. [sent-224, score-0.453]
</p><p>64 Each triplet constraint involving camera i, j, k is re-weighted by min(Ki1,Kj,Kk). [sent-225, score-0.453]
</p><p>65 1) We verify every triplet in the match graph, and remove EGs which participate in no triplet that passes the verification. [sent-243, score-0.504]
</p><p>66 Specifically, we apply our translation registration to each triplet and calculate the average difference between the relative translation directions before and after the registration. [sent-244, score-0.572]
</p><p>67 We further require that at least one good point (with reprojection error smaller than 4 pixels) can be triangulated by the registered triplet cameras. [sent-246, score-0.374]
</p><p>68 2) Among the edges of the match graph, we extract a subset of ‘reliable edges’ to compute the global camera orientations as described in Section 3. [sent-247, score-0.352]
</p><p>69 3) We further use these camera orientations to verify the match graph edges, and discard an edge if the geodesic distance [15] between the loop rotation matrix [45] and the identity matrix is greater than δ2. [sent-252, score-0.504]
</p><p>70 We further extract connected triplet graphs from the match graph, where each triplet is represented by a vertex. [sent-259, score-0.569]
</p><p>71 A single connected component of the match graph could generate multiple connected triplet graphs, as illustrated in Figure 2. [sent-261, score-0.454]
</p><p>72 We then apply our method in Section 4 to compute the positions of cameras in each triplet graph respectively. [sent-262, score-0.363]
</p><p>73 Our method is much more stable in translation estimation for near collinear camera motions. [sent-267, score-0.626]
</p><p>74 When there are multiple triplet graphs, their reconstructions are merged to obtain the final result. [sent-269, score-0.284]
</p><p>75 Camera 0 is placed at the world origin and camera 2 is placed at a random location away from camera 0 by 0. [sent-280, score-0.458]
</p><p>76 The location of camera 1is sampled randomly in the sphere centered at the middle point between camera 0 and 2, and passing through their camera centers. [sent-282, score-0.687]
</p><p>77 The scene points are generated randomly within the viewing volume of the first camera and the distance between the nearest scene point and the furthest scene point is about 0. [sent-285, score-0.337]
</p><p>78 The error of camera orientations Rerr is the mean geodesic distance (in degrees) between the estimated and the true camera rotation matrix. [sent-293, score-0.641]
</p><p>79 Absolute camera location error cerr is the mean Euclidean distance between the estimated and the true camera centers. [sent-295, score-0.548]
</p><p>80 We compare with the four-point algorithm [26], which is the only practical algorithm to compute trifocal tensor from three calibrated images as far as we know. [sent-298, score-0.365]
</p><p>81 We also compare with the recent method [3] to demonstrate the robustness of our method on near collinear camera motions. [sent-303, score-0.506]
</p><p>82 It is clear that our method produces more stable results for near collinear motion. [sent-311, score-0.309]
</p><p>83 Multi-view Reconstruction We test the performance of our method with some standard benchmark datasets with known ground-truth camera motion to quantitatively evaluate the reconstruction accuracy. [sent-314, score-0.358]
</p><p>84 On average our method produces error in ci about 0. [sent-329, score-0.316]
</p><p>85 The absolute camera rotation error Rerr and camera location error cerr are measured in degrees and meters, respectively. [sent-360, score-0.738]
</p><p>86 Each internet image collection is reconstructed as one single connected triplet graph by our algorithm. [sent-366, score-0.4]
</p><p>87 We further manually identify the set of common cameras registered by our method and VisualSFM, respectively, for the Notre Dame example, and compute the difference between the estimated camera motion. [sent-374, score-0.356]
</p><p>88 007 (when the distance between the two farthest camera is 1). [sent-377, score-0.266]
</p><p>89 Conclusion We present a novel linear solution for the global camera pose registration problem. [sent-389, score-0.344]
</p><p>90 It is free from the common degeneration of linear methods on collinear motion, and is robust to different baseline lengths between cameras. [sent-391, score-0.322]
</p><p>91 For the case of three cameras, it produces more accurate results than prior trifocal tensor estimation method on calibrated images. [sent-392, score-0.397]
</p><p>92 Spectral solution of large-scale extrinsic camera calibration as a graph embedding problem. [sent-428, score-0.273]
</p><p>93 Exploiting loops in the graph of trifocal tensors for calibrating a network of cameras. [sent-448, score-0.342]
</p><p>94 Randomized structure from motion based on atomic 3d models from camera triplets. [sent-513, score-0.283]
</p><p>95 Self-calibration and  [31]  [32]  [33]  [34]  [35]  [36]  [37]  [38]  [39]  metric reconstruction inspite of varying and unknown intrinsic camera parameters. [sent-591, score-0.304]
</p><p>96 Derivation of Equation (3) We first show that the length of the line segments ciA, cjB are approx-  siikj  sjikj  imately | |ci − cj | | and | |ci − cj | | respectively. [sent-706, score-0.886]
</p><p>97 The three vector|s| cij , cik a|n adn cjk sh|o|culd− b ce |c|lo rsees ptoe coplanar, so the angle ∠Acick is close to zero, and the length of ciA is close to that of cick. [sent-707, score-0.73]
</p><p>98 ≈ θk because the three vectors cij , cik and cjk are ≈clo θse to coplanar. [sent-713, score-0.661]
</p><p>99 The 3D coordinate of A is then approximated by ci + siikj | |ci − cj | |cik. [sent-714, score-0.707]
</p><p>100 Similarly, we  sjikj  can obtain the coordinate of B as cj + | |ci −cj | |cjk. [sent-715, score-0.468]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cj', 0.336), ('collinear', 0.277), ('trifocal', 0.256), ('cik', 0.241), ('ci', 0.239), ('camera', 0.229), ('triplet', 0.224), ('cjk', 0.217), ('cij', 0.203), ('egs', 0.183), ('ck', 0.143), ('visualsfm', 0.138), ('translation', 0.12), ('ba', 0.116), ('rotation', 0.109), ('cameras', 0.095), ('siikj', 0.089), ('sjikj', 0.089), ('exif', 0.082), ('registration', 0.077), ('translations', 0.076), ('degeneracy', 0.076), ('reconstruction', 0.075), ('reprojection', 0.073), ('rotations', 0.07), ('poses', 0.069), ('sfm', 0.067), ('siijk', 0.067), ('connected', 0.065), ('equations', 0.064), ('rj', 0.063), ('ri', 0.06), ('reconstructions', 0.06), ('tij', 0.059), ('unordered', 0.059), ('tensor', 0.059), ('match', 0.056), ('motion', 0.054), ('sinha', 0.053), ('centers', 0.053), ('factorization', 0.053), ('pages', 0.052), ('uncalibrated', 0.05), ('calibrated', 0.05), ('algebraic', 0.049), ('notre', 0.047), ('triplets', 0.047), ('essential', 0.047), ('eg', 0.046), ('ambiguity', 0.045), ('snavely', 0.045), ('error', 0.045), ('baseline', 0.045), ('cerr', 0.045), ('rerr', 0.045), ('sijjk', 0.045), ('ssiinn', 0.045), ('trevi', 0.045), ('geometries', 0.044), ('graph', 0.044), ('incorrect', 0.043), ('coordinate', 0.043), ('bundle', 0.043), ('tensors', 0.042), ('pairwise', 0.041), ('cmvs', 0.04), ('courchay', 0.04), ('global', 0.038), ('agarwal', 0.037), ('loop', 0.037), ('dalalyan', 0.037), ('farthest', 0.037), ('suspicious', 0.037), ('scene', 0.036), ('equation', 0.036), ('length', 0.036), ('sin', 0.036), ('degrees', 0.036), ('seitz', 0.035), ('reconstructed', 0.035), ('cia', 0.034), ('coplanar', 0.034), ('register', 0.034), ('zach', 0.034), ('arising', 0.034), ('angle', 0.033), ('pioneer', 0.033), ('internet', 0.032), ('registered', 0.032), ('produces', 0.032), ('solve', 0.032), ('bundler', 0.032), ('dame', 0.032), ('eriksson', 0.032), ('midpoint', 0.032), ('relative', 0.031), ('pisa', 0.03), ('singapore', 0.03), ('olsson', 0.029), ('orientations', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="17-tfidf-1" href="./iccv-2013-A_Global_Linear_Method_for_Camera_Pose_Registration.html">17 iccv-2013-A Global Linear Method for Camera Pose Registration</a></p>
<p>Author: Nianjuan Jiang, Zhaopeng Cui, Ping Tan</p><p>Abstract: We present a linear method for global camera pose registration from pairwise relative poses encoded in essential matrices. Our method minimizes an approximate geometric error to enforce the triangular relationship in camera triplets. This formulation does not suffer from the typical ‘unbalanced scale ’ problem in linear methods relying on pairwise translation direction constraints, i.e. an algebraic error; nor the system degeneracy from collinear motion. In the case of three cameras, our method provides a good linear approximation of the trifocal tensor. It can be directly scaled up to register multiple cameras. The results obtained are accurate for point triangulation and can serve as a good initialization for final bundle adjustment. We evaluate the algorithm performance with different types of data and demonstrate its effectiveness. Our system produces good accuracy, robustness, and outperforms some well-known systems on efficiency.</p><p>2 0.37041491 <a title="17-tfidf-2" href="./iccv-2013-Global_Fusion_of_Relative_Motions_for_Robust%2C_Accurate_and_Scalable_Structure_from_Motion.html">184 iccv-2013-Global Fusion of Relative Motions for Robust, Accurate and Scalable Structure from Motion</a></p>
<p>Author: Pierre Moulon, Pascal Monasse, Renaud Marlet</p><p>Abstract: Multi-view structure from motion (SfM) estimates the position and orientation of pictures in a common 3D coordinate frame. When views are treated incrementally, this external calibration can be subject to drift, contrary to global methods that distribute residual errors evenly. We propose a new global calibration approach based on the fusion of relative motions between image pairs. We improve an existing method for robustly computing global rotations. We present an efficient a contrario trifocal tensor estimation method, from which stable and precise translation directions can be extracted. We also define an efficient translation registration method that recovers accurate camera positions. These components are combined into an original SfM pipeline. Our experiments show that, on most datasets, it outperforms in accuracy other existing incremental and global pipelines. It also achieves strikingly good running times: it is about 20 times faster than the other global method we could compare to, and as fast as the best incremental method. More importantly, it features better scalability properties.</p><p>3 0.14931607 <a title="17-tfidf-3" href="./iccv-2013-Direct_Optimization_of_Frame-to-Frame_Rotation.html">115 iccv-2013-Direct Optimization of Frame-to-Frame Rotation</a></p>
<p>Author: Laurent Kneip, Simon Lynen</p><p>Abstract: This work makes use of a novel, recently proposed epipolar constraint for computing the relative pose between two calibrated images. By enforcing the coplanarity of epipolar plane normal vectors, it constrains the three degrees of freedom of the relative rotation between two camera views directly—independently of the translation. The present paper shows how the approach can be extended to n points, and translated into an efficient eigenvalue minimization over the three rotational degrees of freedom. Each iteration in the non-linear optimization has constant execution time, independently of the number of features. Two global optimization approaches are proposed. The first one consists of an efficient Levenberg-Marquardt scheme with randomized initial value, which already leads to stable and accurate results. The second scheme consists of a globally optimal branch-and-bound algorithm based on a bound on the eigenvalue variation derived from symmetric eigenvalue-perturbation theory. Analysis of the cost function reveals insights into the nature of a specific relative pose problem, and outlines the complexity under different conditions. The algorithm shows state-of-the-art performance w.r.t. essential-matrix based solutions, and a frameto-frame application to a video sequence immediately leads to an alternative, real-time visual odometry solution. Note: All algorithms in this paper are made available in the OpenGV library. Please visit http : / / l aurent kne ip .github . i / opengv o</p><p>4 0.13793468 <a title="17-tfidf-4" href="./iccv-2013-Multi-view_3D_Reconstruction_from_Uncalibrated_Radially-Symmetric_Cameras.html">280 iccv-2013-Multi-view 3D Reconstruction from Uncalibrated Radially-Symmetric Cameras</a></p>
<p>Author: Jae-Hak Kim, Yuchao Dai, Hongdong Li, Xin Du, Jonghyuk Kim</p><p>Abstract: We present a new multi-view 3D Euclidean reconstruction method for arbitrary uncalibrated radially-symmetric cameras, which needs no calibration or any camera model parameters other than radial symmetry. It is built on the radial 1D camera model [25], a unified mathematical abstraction to different types of radially-symmetric cameras. We formulate the problem of multi-view reconstruction for radial 1D cameras as a matrix rank minimization problem. Efficient implementation based on alternating direction continuation is proposed to handle scalability issue for real-world applications. Our method applies to a wide range of omnidirectional cameras including both dioptric and catadioptric (central and non-central) cameras. Additionally, our method deals with complete and incomplete measurements under a unified framework elegantly. Experiments on both synthetic and real images from various types of cameras validate the superior performance of our new method, in terms of numerical accuracy and robustness.</p><p>5 0.13430548 <a title="17-tfidf-5" href="./iccv-2013-Uncertainty-Driven_Efficiently-Sampled_Sparse_Graphical_Models_for_Concurrent_Tumor_Segmentation_and_Atlas_Registration.html">432 iccv-2013-Uncertainty-Driven Efficiently-Sampled Sparse Graphical Models for Concurrent Tumor Segmentation and Atlas Registration</a></p>
<p>Author: Sarah Parisot, William Wells_III, Stéphane Chemouny, Hugues Duffau, Nikos Paragios</p><p>Abstract: Graph-based methods have become popular in recent years and have successfully addressed tasks like segmentation and deformable registration. Their main strength is optimality of the obtained solution while their main limitation is the lack of precision due to the grid-like representations and the discrete nature of the quantized search space. In this paper we introduce a novel approach for combined segmentation/registration of brain tumors that adapts graph and sampling resolution according to the image content. To this end we estimate the segmentation and registration marginals towards adaptive graph resolution and intelligent definition of the search space. This information is considered in a hierarchical framework where uncertainties are propagated in a natural manner. State of the art results in the joint segmentation/registration of brain images with low-grade gliomas demonstrate the potential of our approach.</p><p>6 0.13369334 <a title="17-tfidf-6" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>7 0.12913498 <a title="17-tfidf-7" href="./iccv-2013-Pose_Estimation_with_Unknown_Focal_Length_Using_Points%2C_Directions_and_Lines.html">323 iccv-2013-Pose Estimation with Unknown Focal Length Using Points, Directions and Lines</a></p>
<p>8 0.12394218 <a title="17-tfidf-8" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>9 0.11854788 <a title="17-tfidf-9" href="./iccv-2013-Efficient_and_Robust_Large-Scale_Rotation_Averaging.html">138 iccv-2013-Efficient and Robust Large-Scale Rotation Averaging</a></p>
<p>10 0.1129659 <a title="17-tfidf-10" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>11 0.10923178 <a title="17-tfidf-11" href="./iccv-2013-An_Enhanced_Structure-from-Motion_Paradigm_Based_on_the_Absolute_Dual_Quadric_and_Images_of_Circular_Points.html">49 iccv-2013-An Enhanced Structure-from-Motion Paradigm Based on the Absolute Dual Quadric and Images of Circular Points</a></p>
<p>12 0.098507978 <a title="17-tfidf-12" href="./iccv-2013-Lifting_3D_Manhattan_Lines_from_a_Single_Image.html">250 iccv-2013-Lifting 3D Manhattan Lines from a Single Image</a></p>
<p>13 0.097592972 <a title="17-tfidf-13" href="./iccv-2013-Perspective_Motion_Segmentation_via_Collaborative_Clustering.html">314 iccv-2013-Perspective Motion Segmentation via Collaborative Clustering</a></p>
<p>14 0.094422601 <a title="17-tfidf-14" href="./iccv-2013-Shape_Anchors_for_Data-Driven_Multi-view_Reconstruction.html">387 iccv-2013-Shape Anchors for Data-Driven Multi-view Reconstruction</a></p>
<p>15 0.093828216 <a title="17-tfidf-15" href="./iccv-2013-Monocular_Image_3D_Human_Pose_Estimation_under_Self-Occlusion.html">273 iccv-2013-Monocular Image 3D Human Pose Estimation under Self-Occlusion</a></p>
<p>16 0.092640668 <a title="17-tfidf-16" href="./iccv-2013-Space-Time_Tradeoffs_in_Photo_Sequencing.html">397 iccv-2013-Space-Time Tradeoffs in Photo Sequencing</a></p>
<p>17 0.090111159 <a title="17-tfidf-17" href="./iccv-2013-A_Robust_Analytical_Solution_to_Isometric_Shape-from-Template_with_Focal_Length_Calibration.html">27 iccv-2013-A Robust Analytical Solution to Isometric Shape-from-Template with Focal Length Calibration</a></p>
<p>18 0.088210665 <a title="17-tfidf-18" href="./iccv-2013-Revisiting_the_PnP_Problem%3A_A_Fast%2C_General_and_Optimal_Solution.html">353 iccv-2013-Revisiting the PnP Problem: A Fast, General and Optimal Solution</a></p>
<p>19 0.088147148 <a title="17-tfidf-19" href="./iccv-2013-Large-Scale_Multi-resolution_Surface_Reconstruction_from_RGB-D_Sequences.html">228 iccv-2013-Large-Scale Multi-resolution Surface Reconstruction from RGB-D Sequences</a></p>
<p>20 0.087713048 <a title="17-tfidf-20" href="./iccv-2013-Network_Principles_for_SfM%3A_Disambiguating_Repeated_Structures_with_Local_Context.html">289 iccv-2013-Network Principles for SfM: Disambiguating Repeated Structures with Local Context</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.192), (1, -0.155), (2, -0.059), (3, 0.028), (4, -0.033), (5, 0.05), (6, 0.044), (7, -0.113), (8, 0.048), (9, 0.026), (10, 0.02), (11, -0.027), (12, -0.135), (13, 0.013), (14, 0.024), (15, 0.123), (16, 0.103), (17, 0.104), (18, -0.031), (19, 0.025), (20, 0.008), (21, -0.154), (22, -0.068), (23, 0.082), (24, 0.021), (25, -0.005), (26, 0.046), (27, 0.002), (28, -0.121), (29, 0.002), (30, 0.035), (31, 0.018), (32, -0.014), (33, -0.073), (34, -0.093), (35, -0.002), (36, 0.032), (37, 0.08), (38, -0.11), (39, 0.14), (40, -0.111), (41, 0.071), (42, 0.029), (43, 0.003), (44, 0.036), (45, -0.098), (46, -0.049), (47, 0.162), (48, 0.008), (49, 0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94934958 <a title="17-lsi-1" href="./iccv-2013-A_Global_Linear_Method_for_Camera_Pose_Registration.html">17 iccv-2013-A Global Linear Method for Camera Pose Registration</a></p>
<p>Author: Nianjuan Jiang, Zhaopeng Cui, Ping Tan</p><p>Abstract: We present a linear method for global camera pose registration from pairwise relative poses encoded in essential matrices. Our method minimizes an approximate geometric error to enforce the triangular relationship in camera triplets. This formulation does not suffer from the typical ‘unbalanced scale ’ problem in linear methods relying on pairwise translation direction constraints, i.e. an algebraic error; nor the system degeneracy from collinear motion. In the case of three cameras, our method provides a good linear approximation of the trifocal tensor. It can be directly scaled up to register multiple cameras. The results obtained are accurate for point triangulation and can serve as a good initialization for final bundle adjustment. We evaluate the algorithm performance with different types of data and demonstrate its effectiveness. Our system produces good accuracy, robustness, and outperforms some well-known systems on efficiency.</p><p>2 0.92713505 <a title="17-lsi-2" href="./iccv-2013-Global_Fusion_of_Relative_Motions_for_Robust%2C_Accurate_and_Scalable_Structure_from_Motion.html">184 iccv-2013-Global Fusion of Relative Motions for Robust, Accurate and Scalable Structure from Motion</a></p>
<p>Author: Pierre Moulon, Pascal Monasse, Renaud Marlet</p><p>Abstract: Multi-view structure from motion (SfM) estimates the position and orientation of pictures in a common 3D coordinate frame. When views are treated incrementally, this external calibration can be subject to drift, contrary to global methods that distribute residual errors evenly. We propose a new global calibration approach based on the fusion of relative motions between image pairs. We improve an existing method for robustly computing global rotations. We present an efficient a contrario trifocal tensor estimation method, from which stable and precise translation directions can be extracted. We also define an efficient translation registration method that recovers accurate camera positions. These components are combined into an original SfM pipeline. Our experiments show that, on most datasets, it outperforms in accuracy other existing incremental and global pipelines. It also achieves strikingly good running times: it is about 20 times faster than the other global method we could compare to, and as fast as the best incremental method. More importantly, it features better scalability properties.</p><p>3 0.82817984 <a title="17-lsi-3" href="./iccv-2013-Efficient_and_Robust_Large-Scale_Rotation_Averaging.html">138 iccv-2013-Efficient and Robust Large-Scale Rotation Averaging</a></p>
<p>Author: Avishek Chatterjee, Venu Madhav Govindu</p><p>Abstract: In this paper we address the problem of robust and efficient averaging of relative 3D rotations. Apart from having an interesting geometric structure, robust rotation averaging addresses the need for a good initialization for largescale optimization used in structure-from-motion pipelines. Such pipelines often use unstructured image datasets harvested from the internet thereby requiring an initialization method that is robust to outliers. Our approach works on the Lie group structure of 3D rotations and solves the problem of large-scale robust rotation averaging in two ways. Firstly, we use modern ?1 optimizers to carry out robust averaging of relative rotations that is efficient, scalable and robust to outliers. In addition, we also develop a twostep method that uses the ?1 solution as an initialisation for an iteratively reweighted least squares (IRLS) approach. These methods achieve excellent results on large-scale, real world datasets and significantly outperform existing methods, i.e. the state-of-the-art discrete-continuous optimization method of [3] as well as the Weiszfeld method of [8]. We demonstrate the efficacy of our method on two large- scale real world datasets and also provide the results of the two aforementioned methods for comparison.</p><p>4 0.73363721 <a title="17-lsi-4" href="./iccv-2013-Direct_Optimization_of_Frame-to-Frame_Rotation.html">115 iccv-2013-Direct Optimization of Frame-to-Frame Rotation</a></p>
<p>Author: Laurent Kneip, Simon Lynen</p><p>Abstract: This work makes use of a novel, recently proposed epipolar constraint for computing the relative pose between two calibrated images. By enforcing the coplanarity of epipolar plane normal vectors, it constrains the three degrees of freedom of the relative rotation between two camera views directly—independently of the translation. The present paper shows how the approach can be extended to n points, and translated into an efficient eigenvalue minimization over the three rotational degrees of freedom. Each iteration in the non-linear optimization has constant execution time, independently of the number of features. Two global optimization approaches are proposed. The first one consists of an efficient Levenberg-Marquardt scheme with randomized initial value, which already leads to stable and accurate results. The second scheme consists of a globally optimal branch-and-bound algorithm based on a bound on the eigenvalue variation derived from symmetric eigenvalue-perturbation theory. Analysis of the cost function reveals insights into the nature of a specific relative pose problem, and outlines the complexity under different conditions. The algorithm shows state-of-the-art performance w.r.t. essential-matrix based solutions, and a frameto-frame application to a video sequence immediately leads to an alternative, real-time visual odometry solution. Note: All algorithms in this paper are made available in the OpenGV library. Please visit http : / / l aurent kne ip .github . i / opengv o</p><p>5 0.68650275 <a title="17-lsi-5" href="./iccv-2013-Refractive_Structure-from-Motion_on_Underwater_Images.html">348 iccv-2013-Refractive Structure-from-Motion on Underwater Images</a></p>
<p>Author: Anne Jordt-Sedlazeck, Reinhard Koch</p><p>Abstract: In underwater environments, cameras need to be confined in an underwater housing, viewing the scene through a piece of glass. In case of flat port underwater housings, light rays entering the camera housing are refracted twice, due to different medium densities of water, glass, and air. This causes the usually linear rays of light to bend and the commonly used pinhole camera model to be invalid. When using the pinhole camera model without explicitly modeling refraction in Structure-from-Motion (SfM) methods, a systematic model error occurs. Therefore, in this paper, we propose a system for computing camera path and 3D points with explicit incorporation of refraction using new methods for pose estimation. Additionally, a new error function is introduced for non-linear optimization, especially bundle adjustment. The proposed method allows to increase reconstruction accuracy and is evaluated in a set of experiments, where the proposed method’s performance is compared to SfM with the perspective camera model.</p><p>6 0.67952532 <a title="17-lsi-6" href="./iccv-2013-Revisiting_the_PnP_Problem%3A_A_Fast%2C_General_and_Optimal_Solution.html">353 iccv-2013-Revisiting the PnP Problem: A Fast, General and Optimal Solution</a></p>
<p>7 0.6651991 <a title="17-lsi-7" href="./iccv-2013-Content-Aware_Rotation.html">90 iccv-2013-Content-Aware Rotation</a></p>
<p>8 0.65523303 <a title="17-lsi-8" href="./iccv-2013-Extrinsic_Camera_Calibration_without_a_Direct_View_Using_Spherical_Mirror.html">152 iccv-2013-Extrinsic Camera Calibration without a Direct View Using Spherical Mirror</a></p>
<p>9 0.64488906 <a title="17-lsi-9" href="./iccv-2013-An_Enhanced_Structure-from-Motion_Paradigm_Based_on_the_Absolute_Dual_Quadric_and_Images_of_Circular_Points.html">49 iccv-2013-An Enhanced Structure-from-Motion Paradigm Based on the Absolute Dual Quadric and Images of Circular Points</a></p>
<p>10 0.63668782 <a title="17-lsi-10" href="./iccv-2013-Pose_Estimation_with_Unknown_Focal_Length_Using_Points%2C_Directions_and_Lines.html">323 iccv-2013-Pose Estimation with Unknown Focal Length Using Points, Directions and Lines</a></p>
<p>11 0.63552701 <a title="17-lsi-11" href="./iccv-2013-Go-ICP%3A_Solving_3D_Registration_Efficiently_and_Globally_Optimally.html">185 iccv-2013-Go-ICP: Solving 3D Registration Efficiently and Globally Optimally</a></p>
<p>12 0.62636656 <a title="17-lsi-12" href="./iccv-2013-Multi-view_3D_Reconstruction_from_Uncalibrated_Radially-Symmetric_Cameras.html">280 iccv-2013-Multi-view 3D Reconstruction from Uncalibrated Radially-Symmetric Cameras</a></p>
<p>13 0.60852015 <a title="17-lsi-13" href="./iccv-2013-Enhanced_Continuous_Tabu_Search_for_Parameter_Estimation_in_Multiview_Geometry.html">141 iccv-2013-Enhanced Continuous Tabu Search for Parameter Estimation in Multiview Geometry</a></p>
<p>14 0.58592433 <a title="17-lsi-14" href="./iccv-2013-SUN3D%3A_A_Database_of_Big_Spaces_Reconstructed_Using_SfM_and_Object_Labels.html">367 iccv-2013-SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels</a></p>
<p>15 0.56966239 <a title="17-lsi-15" href="./iccv-2013-Space-Time_Tradeoffs_in_Photo_Sequencing.html">397 iccv-2013-Space-Time Tradeoffs in Photo Sequencing</a></p>
<p>16 0.55725729 <a title="17-lsi-16" href="./iccv-2013-Street_View_Motion-from-Structure-from-Motion.html">402 iccv-2013-Street View Motion-from-Structure-from-Motion</a></p>
<p>17 0.53243387 <a title="17-lsi-17" href="./iccv-2013-Unsupervised_Intrinsic_Calibration_from_a_Single_Frame_Using_a_%22Plumb-Line%22_Approach.html">436 iccv-2013-Unsupervised Intrinsic Calibration from a Single Frame Using a "Plumb-Line" Approach</a></p>
<p>18 0.52483219 <a title="17-lsi-18" href="./iccv-2013-Real-Time_Solution_to_the_Absolute_Pose_Problem_with_Unknown_Radial_Distortion_and_Focal_Length.html">342 iccv-2013-Real-Time Solution to the Absolute Pose Problem with Unknown Radial Distortion and Focal Length</a></p>
<p>19 0.52085739 <a title="17-lsi-19" href="./iccv-2013-A_Robust_Analytical_Solution_to_Isometric_Shape-from-Template_with_Focal_Length_Calibration.html">27 iccv-2013-A Robust Analytical Solution to Isometric Shape-from-Template with Focal Length Calibration</a></p>
<p>20 0.5142771 <a title="17-lsi-20" href="./iccv-2013-Subpixel_Scanning_Invariant_to_Indirect_Lighting_Using_Quadratic_Code_Length.html">407 iccv-2013-Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.058), (7, 0.037), (16, 0.206), (26, 0.044), (31, 0.04), (34, 0.017), (40, 0.011), (42, 0.215), (64, 0.025), (73, 0.033), (89, 0.185), (95, 0.019), (98, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9180764 <a title="17-lda-1" href="./iccv-2013-From_Point_to_Set%3A_Extend_the_Learning_of_Distance_Metrics.html">177 iccv-2013-From Point to Set: Extend the Learning of Distance Metrics</a></p>
<p>Author: Pengfei Zhu, Lei Zhang, Wangmeng Zuo, David Zhang</p><p>Abstract: Most of the current metric learning methods are proposed for point-to-point distance (PPD) based classification. In many computer vision tasks, however, we need to measure the point-to-set distance (PSD) and even set-to-set distance (SSD) for classification. In this paper, we extend the PPD based Mahalanobis distance metric learning to PSD and SSD based ones, namely point-to-set distance metric learning (PSDML) and set-to-set distance metric learning (SSDML), and solve them under a unified optimization framework. First, we generate positive and negative sample pairs by computing the PSD and SSD between training samples. Then, we characterize each sample pair by its covariance matrix, and propose a covariance kernel based discriminative function. Finally, we tackle the PSDML and SSDMLproblems by using standard support vector machine solvers, making the metric learning very efficient for multiclass visual classification tasks. Experiments on gender classification, digit recognition, object categorization and face recognition show that the proposed metric learning methods can effectively enhance the performance of PSD and SSD based classification.</p><p>same-paper 2 0.85718 <a title="17-lda-2" href="./iccv-2013-A_Global_Linear_Method_for_Camera_Pose_Registration.html">17 iccv-2013-A Global Linear Method for Camera Pose Registration</a></p>
<p>Author: Nianjuan Jiang, Zhaopeng Cui, Ping Tan</p><p>Abstract: We present a linear method for global camera pose registration from pairwise relative poses encoded in essential matrices. Our method minimizes an approximate geometric error to enforce the triangular relationship in camera triplets. This formulation does not suffer from the typical ‘unbalanced scale ’ problem in linear methods relying on pairwise translation direction constraints, i.e. an algebraic error; nor the system degeneracy from collinear motion. In the case of three cameras, our method provides a good linear approximation of the trifocal tensor. It can be directly scaled up to register multiple cameras. The results obtained are accurate for point triangulation and can serve as a good initialization for final bundle adjustment. We evaluate the algorithm performance with different types of data and demonstrate its effectiveness. Our system produces good accuracy, robustness, and outperforms some well-known systems on efficiency.</p><p>3 0.83512068 <a title="17-lda-3" href="./iccv-2013-PM-Huber%3A_PatchMatch_with_Huber_Regularization_for_Stereo_Matching.html">304 iccv-2013-PM-Huber: PatchMatch with Huber Regularization for Stereo Matching</a></p>
<p>Author: Philipp Heise, Sebastian Klose, Brian Jensen, Alois Knoll</p><p>Abstract: Most stereo correspondence algorithms match support windows at integer-valued disparities and assume a constant disparity value within the support window. The recently proposed PatchMatch stereo algorithm [7] overcomes this limitation of previous algorithms by directly estimating planes. This work presents a method that integrates the PatchMatch stereo algorithm into a variational smoothing formulation using quadratic relaxation. The resulting algorithm allows the explicit regularization of the disparity and normal gradients using the estimated plane parameters. Evaluation of our method in the Middlebury benchmark shows that our method outperforms the traditional integer-valued disparity strategy as well as the original algorithm and its variants in sub-pixel accurate disparity estimation.</p><p>4 0.82095885 <a title="17-lda-4" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>Author: Jiajia Luo, Wei Wang, Hairong Qi</p><p>Abstract: Human action recognition based on the depth information provided by commodity depth sensors is an important yet challenging task. The noisy depth maps, different lengths of action sequences, and free styles in performing actions, may cause large intra-class variations. In this paper, a new framework based on sparse coding and temporal pyramid matching (TPM) is proposed for depthbased human action recognition. Especially, a discriminative class-specific dictionary learning algorithm isproposed for sparse coding. By adding the group sparsity and geometry constraints, features can be well reconstructed by the sub-dictionary belonging to the same class, and the geometry relationships among features are also kept in the calculated coefficients. The proposed approach is evaluated on two benchmark datasets captured by depth cameras. Experimental results show that the proposed algorithm repeatedly hqi } @ ut k . edu GB ImagesR epth ImagesD setkonlSy0 896.5170d4ept.3h021 .x02y 19.876504.dep3th02.1 x02. achieves superior performance to the state of the art algorithms. Moreover, the proposed dictionary learning method also outperforms classic dictionary learning approaches.</p><p>5 0.81416845 <a title="17-lda-5" href="./iccv-2013-DCSH_-_Matching_Patches_in_RGBD_Images.html">101 iccv-2013-DCSH - Matching Patches in RGBD Images</a></p>
<p>Author: Yaron Eshet, Simon Korman, Eyal Ofek, Shai Avidan</p><p>Abstract: We extend patch based methods to work on patches in 3D space. We start with Coherency Sensitive Hashing [12] (CSH), which is an algorithm for matching patches between two RGB images, and extend it to work with RGBD images. This is done by warping all 3D patches to a common virtual plane in which CSH is performed. To avoid noise due to warping of patches of various normals and depths, we estimate a group of dominant planes and compute CSH on each plane separately, before merging the matching patches. The result is DCSH - an algorithm that matches world (3D) patches in order to guide the search for image plane matches. An independent contribution is an extension of CSH, which we term Social-CSH. It allows a major speedup of the k nearest neighbor (kNN) version of CSH - its runtime growing linearly, rather than quadratically, in k. Social-CSH is used as a subcomponent of DCSH when many NNs are required, as in the case of image denoising. We show the benefits ofusing depth information to image reconstruction and image denoising, demonstrated on several RGBD images.</p><p>6 0.81236875 <a title="17-lda-6" href="./iccv-2013-Global_Fusion_of_Relative_Motions_for_Robust%2C_Accurate_and_Scalable_Structure_from_Motion.html">184 iccv-2013-Global Fusion of Relative Motions for Robust, Accurate and Scalable Structure from Motion</a></p>
<p>7 0.8109985 <a title="17-lda-7" href="./iccv-2013-Latent_Multitask_Learning_for_View-Invariant_Action_Recognition.html">231 iccv-2013-Latent Multitask Learning for View-Invariant Action Recognition</a></p>
<p>8 0.80800998 <a title="17-lda-8" href="./iccv-2013-Allocentric_Pose_Estimation.html">46 iccv-2013-Allocentric Pose Estimation</a></p>
<p>9 0.80646235 <a title="17-lda-9" href="./iccv-2013-Attribute_Pivots_for_Guiding_Relevance_Feedback_in_Image_Search.html">54 iccv-2013-Attribute Pivots for Guiding Relevance Feedback in Image Search</a></p>
<p>10 0.80567425 <a title="17-lda-10" href="./iccv-2013-Manifold_Based_Face_Synthesis_from_Sparse_Samples.html">259 iccv-2013-Manifold Based Face Synthesis from Sparse Samples</a></p>
<p>11 0.80521178 <a title="17-lda-11" href="./iccv-2013-A_Generalized_Iterated_Shrinkage_Algorithm_for_Non-convex_Sparse_Coding.html">14 iccv-2013-A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</a></p>
<p>12 0.80380708 <a title="17-lda-12" href="./iccv-2013-Attribute_Adaptation_for_Personalized_Image_Search.html">52 iccv-2013-Attribute Adaptation for Personalized Image Search</a></p>
<p>13 0.80271471 <a title="17-lda-13" href="./iccv-2013-Similarity_Metric_Learning_for_Face_Recognition.html">392 iccv-2013-Similarity Metric Learning for Face Recognition</a></p>
<p>14 0.8021946 <a title="17-lda-14" href="./iccv-2013-Implied_Feedback%3A_Learning_Nuances_of_User_Behavior_in_Image_Search.html">213 iccv-2013-Implied Feedback: Learning Nuances of User Behavior in Image Search</a></p>
<p>15 0.80173898 <a title="17-lda-15" href="./iccv-2013-A_Practical_Transfer_Learning_Algorithm_for_Face_Verification.html">26 iccv-2013-A Practical Transfer Learning Algorithm for Face Verification</a></p>
<p>16 0.80131888 <a title="17-lda-16" href="./iccv-2013-Affine-Constrained_Group_Sparse_Coding_and_Its_Application_to_Image-Based_Classifications.html">45 iccv-2013-Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications</a></p>
<p>17 0.80129266 <a title="17-lda-17" href="./iccv-2013-Correlation_Adaptive_Subspace_Segmentation_by_Trace_Lasso.html">93 iccv-2013-Correlation Adaptive Subspace Segmentation by Trace Lasso</a></p>
<p>18 0.79984093 <a title="17-lda-18" href="./iccv-2013-Unsupervised_Visual_Domain_Adaptation_Using_Subspace_Alignment.html">438 iccv-2013-Unsupervised Visual Domain Adaptation Using Subspace Alignment</a></p>
<p>19 0.79918587 <a title="17-lda-19" href="./iccv-2013-Deep_Learning_Identity-Preserving_Face_Space.html">106 iccv-2013-Deep Learning Identity-Preserving Face Space</a></p>
<p>20 0.79844344 <a title="17-lda-20" href="./iccv-2013-Coherent_Object_Detection_with_3D_Geometric_Context_from_a_Single_Image.html">79 iccv-2013-Coherent Object Detection with 3D Geometric Context from a Single Image</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
