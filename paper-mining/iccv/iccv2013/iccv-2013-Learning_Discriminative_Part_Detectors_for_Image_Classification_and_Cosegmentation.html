<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</title>
</head>

<body>
<p><a title="iccv" href="../iccv_home.html">iccv</a> <a title="iccv-2013" href="../home/iccv2013_home.html">iccv2013</a> <a title="iccv-2013-236" href="#">iccv2013-236</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</h1>
<br/><p>Source: <a title="iccv-2013-236-pdf" href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Sun_Learning_Discriminative_Part_2013_ICCV_paper.pdf">pdf</a></p><p>Author: Jian Sun, Jean Ponce</p><p>Abstract: In this paper, we address the problem of learning discriminative part detectors from image sets with category labels. We propose a novel latent SVM model regularized by group sparsity to learn these part detectors. Starting from a large set of initial parts, the group sparsity regularizer forces the model to jointly select and optimize a set of discriminative part detectors in a max-margin framework. We propose a stochastic version of a proximal algorithm to solve the corresponding optimization problem. We apply the proposed method to image classification and cosegmentation, and quantitative experiments with standard benchmarks show that it matches or improves upon the state of the art.</p><p>Reference: <a title="iccv-2013-236-reference" href="../iccv2013_reference/iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Learning Discriminative Part Detectors for Image Classification and Cosegmentation  Jian Sun Xi’an Jiaotong University, INRIA, ∗ Abstract In this paper, we address the problem of learning discriminative part detectors from image sets with category labels. [sent-1, score-1.045]
</p><p>2 We propose a novel latent SVM model regularized by group sparsity to learn these part detectors. [sent-2, score-0.684]
</p><p>3 Starting from a large set of initial parts, the group sparsity regularizer forces the model to jointly select and optimize a set of discriminative part detectors in a max-margin framework. [sent-3, score-1.44]
</p><p>4 An essential question is how to efficiently learn and select object / image parts that are discriminative for the image categories of interest. [sent-10, score-0.387]
</p><p>5 Deformable part model (DPM) [15] represents objects by a set of discriminatively learned deformable parts. [sent-11, score-0.392]
</p><p>6 In poselet [4] and discriminative patch (DP) [11, 32] models, part detectors are separately learned by linear SVMs from image patch clusters. [sent-13, score-1.079]
</p><p>7 In this work, we aim to learn class-specific discriminative part detectors from images of the same category (Figure 1). [sent-15, score-1.095]
</p><p>8 We propose a novel latent SVM model regularized by group sparsity to jointly select and optimize a set of discriminative part detectors in a single framework. [sent-16, score-1.502]
</p><p>9 We learn discriminative part detectors for an image set with the same category label. [sent-32, score-1.095]
</p><p>10 The part detectors are applied to image classification and cosegmentation. [sent-33, score-0.83]
</p><p>11 Given a large set of initial parts, the group sparsity regularizer forces the model to automatically select and optimize a small set ofdiscriminative part detectors in a max-margin framework. [sent-36, score-1.274]
</p><p>12 The proposed model tends to select the parts that more frequently and strongly appear in positive training images than in the negative ones. [sent-37, score-0.45]
</p><p>13 We apply the learned part detectors to image classification and cosegmentation. [sent-38, score-0.917]
</p><p>14 For classification, we encode an image by max-pooling over the responses ofthe learned part detectors to the image. [sent-39, score-0.958]
</p><p>15 For cosegmentation, we propose a novel model using the object cues provided by the learned part detectors in a discriminative clustering framework [16]. [sent-40, score-1.103]
</p><p>16 We achieve competitive or state-of-the-art performances on five classification and cosegmentation databases. [sent-41, score-0.403]
</p><p>17 It represents images by pooling the responses of pre-trained object detectors to the image. [sent-50, score-0.701]
</p><p>18 This idea is also applied to action recognition [29], 33439003  and achieves promising results, but it relies on a large set of pre-trained detectors to fully represent the objects / actions of interest. [sent-51, score-0.593]
</p><p>19 The deformable part model (DPM) [15] represents an object by a set of deformable parts learned from object bounding boxes. [sent-53, score-0.597]
</p><p>20 In poselet [4], a large number of object parts are learned from human-labelled keypoints in different poses. [sent-55, score-0.264]
</p><p>21 Discriminative patches (DP) [32] learn distinctive image parts using discriminative clustering. [sent-56, score-0.3]
</p><p>22 Both of the poselet and DP methods separately learn a set of part  detectors using linear SVMs and select the distinctive ones by heuristically ranking their importance. [sent-57, score-1.0]
</p><p>23 Recently, discriminative cosegmentation [8] has successfully been applied to image classification. [sent-62, score-0.512]
</p><p>24 In this paper, we propose to learn class-specific discriminative part detectors based on category labels in a weakly supervised fashion. [sent-63, score-1.131]
</p><p>25 Contrary to part-based models [4, 15, 32] which heuristically select part detectors, our model is able to jointly select and optimize a set of discriminative part detectors in a single framework thanks to group sparsity regularization. [sent-64, score-1.724]
</p><p>26 Learning Discriminative Part Detectors In this section, we will propose a novel latent SVM model with group sparsity regularization to learn a set of discriminative part detectors for an image category. [sent-67, score-1.381]
</p><p>27 An image part is a box whose top-left corner is positioned at z, and it is represented by a feature vector Φ(I, z) that concatenates all the feature  vectors within the box. [sent-71, score-0.256]
</p><p>28 (1), the part detector Γk has non-zero response to image I position z only when the matching at score S(βk , Φ(I, z)) is higher than τk. [sent-75, score-0.427]
</p><p>29 Furthermore, we say that the part Γk appears in an image I when there exists at least one position z that satisfies rz (Γk , I) > 0. [sent-76, score-0.357]
</p><p>30 With the learned part thresholds, part detectors can produce clean responses to images. [sent-215, score-1.214]
</p><p>31 Learning Part Detectors by Group Sparsity In this section, we aim to learn a set of image part detectors that best discriminate the positive and negative training examples for an image category. [sent-219, score-1.045]
</p><p>32 First, we automatically pick an initial set of candidate part detectors associated with the image category. [sent-221, score-0.85]
</p><p>33 Then we use a novel latent SVM model to select and optimize final part detectors with group sparsity regularization. [sent-223, score-1.27]
</p><p>34 Initialization of Part Detectors To initialize the candidate part detectors for an image category, we randomly crop a large number of image parts (approximately ten thousands) from the positive training images. [sent-226, score-0.982]
</p><p>35 Learning Discriminative Part Detectors With the above initialization, we now learn a set of part detectors that best discriminate the positive and negative training images. [sent-233, score-1.011]
</p><p>36 We require that the learned part detectors should appear more frequently and strongly in the positive training images than in the negative ones. [sent-234, score-1.171]
</p><p>37 Given a training set of positive and negative images for an image category, we first  initialize a set of part detectors as discussed in Section 2. [sent-271, score-0.961]
</p><p>38 Then we jointly select and optimize a set of part detectors, i. [sent-273, score-0.441]
</p><p>39 , part template / threshold pairs, by a novel latent SVM model regularized by group sparsity as discussed in Section  2. [sent-275, score-0.697]
</p><p>40 Before introducing our learning method, let us first define the confidence of image I belonging to the current category given class-specific part detectors Γ = {Γk}kK=1 : ? [sent-277, score-0.887]
</p><p>41 1  where zk is a latent variable indicating the image part position with maximum response: zk  =  argmaxz∈ΩI  βkTΦ(I, z),  (3)  and ΩI defines the set of all possible part positions in I. [sent-281, score-0.8]
</p><p>42 (2) that g(I, Γ) ≥ 0 is defined as sum of the smearxveim furomm responses aotf g ga(llI t,hΓe) part 0de iste dcetofinrse tdo image I o. [sent-283, score-0.346]
</p><p>43 and Next we learn part detectors using a latent SVM model with group sparsity regularization. [sent-285, score-1.171]
</p><p>44 The basic idea is to jointly select and optimize the part detectors by maximizing the margin of the confidence value g(I, Γ) on positive and negative training images. [sent-286, score-1.146]
</p><p>45 Denote the training image set as {In, yn}nN=1 where yn = 1if In belongs to the category aansd { Iotherw}ise yn = −1. [sent-287, score-0.311]
</p><p>46 =N1L(g(In,Γ),yn,b) + λR(B),  (4)  where B = {βk}kK=1 is the set of all part templates and L is wtheh squared hinge loss function: L (g(I, Γ), y, b) = [1 − y(g(I, Γ) + b)]2+,  (5)  and b is the bias term of SVM. [sent-289, score-0.363]
</p><p>47 R(B) is a regularization term over the part templates. [sent-297, score-0.308]
</p><p>48 We impose group sparsity [40] over part templates, where each template is considered as a group. [sent-298, score-0.547]
</p><p>49 This regularization forces the algorithm to automatically select a few dis-  criminative part detectors with non-zero templates from a large set of candidate part detectors. [sent-299, score-1.297]
</p><p>50 , R(B) = | |βk | |2 , which is the sum of l2 norm of part templates? [sent-303, score-0.256]
</p><p>51 Tmhaisg ef,o arcndes g t(hIe, lΓe)ar+nebd ≤ part d ieft Iec i-s tors to have larger responses to positive training images than to negative ones. [sent-317, score-0.526]
</p><p>52 It implies that the learned part detectors should be discriminative, i. [sent-318, score-0.868]
</p><p>53 , more frequently and strongly trigger in the positive training images than in the negative ones. [sent-320, score-0.273]
</p><p>54 With group sparsity regularization, the optimization procedure will automatically discard the less discriminative part detectors among the initial ones. [sent-321, score-1.273]
</p><p>55 First, our proposed latent SVM model is regularized by group sparsity which is able to automatically select discriminative part detectors from a large pool of initial detectors. [sent-323, score-1.441]
</p><p>56 Second, our learned part detectors are pairs of part template and part threshold. [sent-324, score-1.443]
</p><p>57 With the part thresholds, parts are not required to appear in every image of the category,  which makes the detectors robust to intra-class variations caused by poses, sub-categories, etc. [sent-325, score-0.903]
</p><p>58 Examples of learned part detectors, detected parts and total response maps of part detectors to images. [sent-555, score-1.357]
</p><p>59 The learned part detectors have higher responses to the discriminative  regions in each category. [sent-556, score-1.116]
</p><p>60 Second, given the set of latent variables for all the positive examples (denoted as Zp), we optimize part detectors {βk, τk}kK=1 and bias term b by minimizing the convex cost E{β(Γ, b; Zp) which is the cost function in Eq. [sent-565, score-1.139]
</p><p>61 Due to the group sparsity regu{βlarizat}ion for {βk}kK=1 , we utilize a proximal method [13] utol optimize βokr . [sent-571, score-0.413]
</p><p>62 ηny0n iofth Cer iswi s aet,isfied  (7)  where ηn = 2(1 − yn (g(In, Γ) + b)), zn,k is the kth latent var=iabl 2e( 1for − image In, C denotes the conditions of βkTΦ(In, zn,k) > τk and yn(g(In, Γ) + b) < 1. [sent-591, score-0.304]
</p><p>63 After optimization, non-discriminative part templates are set to zero due to the l1,2 regularization. [sent-594, score-0.334]
</p><p>64 We discard these part detectors with zero part templates and derive a set of discriminative part detectors. [sent-595, score-1.566]
</p><p>65 To illustrate the learned part detectors, we define the response map of a part detector Γk to an image I the weighted sum of all the detected parts as appearing in the image pyramid, i. [sent-596, score-0.862]
</p><p>66 (1), Mz (Is) is the binary mask of Is 33439036  indicating the region occupied by image part located at position z. [sent-603, score-0.256]
</p><p>67 The part mask Mz (Is) is re-scaled by s1, therefore the response map R(Γk , I) has the same resolution as I. [sent-604, score-0.397]
</p><p>68 As shown in Figure 4(a), the learned detectors are discriminative for the categories considered, e. [sent-612, score-0.77]
</p><p>69 Figure 4(b) shows total response maps of part detectors by summing R(Γk , I) over all the learned part detectors. [sent-615, score-1.265]
</p><p>70 It shows that the learned part detectors have large responses to the salient regions which are discriminative for the image category, and have low responses to the cluttered backgrounds. [sent-616, score-1.206]
</p><p>71 It indicates that our algorithm can effectively derive a set of discriminative part detectors and discard the unimportant ones. [sent-617, score-1.024]
</p><p>72 Applications Discriminative part detectors provide a mid-level and discriminative representation for an image category. [sent-620, score-0.939]
</p><p>73 Image Classification Given an image database, we learn class-specific part detectors for each category using one-vs-all training. [sent-624, score-0.937]
</p><p>74 We denote all the learned part detectors from different categories as Γ = {Γk}kK=1, K is the total number of part detectors. [sent-625, score-1.124]
</p><p>75 Basa Γsed = on our learning method for part detectors, an image I be naturally encoded by a vector of codes {ck}kK=1, can aInd can ea bche c noadtuer ck = e [mz∈aΩxIβkTΦ(I,z) o−r o τkf]+ co, wesh {icch i}s the max-pooling over the responses of part detector Γk to all the image parts in I. [sent-626, score-0.763]
</p><p>76 Given an  image set {In}nN=1 with the same category of objects, we fiimrsatg leea srent d {iIsc}riminative part detectors Γ = {Γk}kK=1 from a training set with the input images as positive examples and a set of diverse background images as negative examples. [sent-636, score-1.101]
</p><p>77 As shown in Figure 4(b) and Figure 5(b), the discriminative part detectors response more strongly and frequently in the common objects of the image set, which provides a high-level object cue for cosegmentation. [sent-637, score-1.257]
</p><p>78 Discriminative clustering has achieved state-of-the-art performance on cosegmentation [16, 17]. [sent-662, score-0.399]
</p><p>79 In this work, we design a novel cosegmentation algorithm by embedding the object cue provided by part detectors into the discriminative clustering framework. [sent-663, score-1.422]
</p><p>80 In our approach, we incorporate the object cue provided by part detectors and label smoothness into the above formulation, then the optimization problem is:  Xm,fin,dE(X,f,d) = Ec(X,f,d|I) +|Ω1I|i∈? [sent-668, score-0.865]
</p><p>81 Figure 5 shows an example of initial and final cosegmentation results. [sent-698, score-0.395]
</p><p>82 Experiments  ×  To learn part detectors, we extract dense HOG features at eight-pixel intervals, and each image part is represented as concatenation of all HOG features in the corresponding region. [sent-700, score-0.562]
</p><p>83 We utilize multiple sizes of part templates (8 8, r6e g×i 6n,. [sent-701, score-0.381]
</p><p>84 , T4h ×e d 4is fceraitmuirnea ctievlels part cdaetpetuctroers f are rleesarn aetd d iifnf onevs-all mode for each database. [sent-704, score-0.256]
</p><p>85 005 in all experiments, which retains 10-15% of the part detectors. [sent-707, score-0.256]
</p><p>86 Our discriminative part detectors perform significantly better than the low-level visual words in [20, 38] and high-level object detectors in [21]. [sent-714, score-1.496]
</p><p>87 We learn a total of 4926 (12% of the number of initial detectors) part detectors for 67 classes, and achieve 51. [sent-732, score-0.872]
</p><p>88 Compared to discriminative patches learned by discrimina-  tive clustering [32] (14070 patches are learned), we perform significantly better, which shows the advantage of our learning method. [sent-734, score-0.29]
</p><p>89 Examples of class-specific part detectors and their total response maps to images. [sent-750, score-0.922]
</p><p>90 Figure 6 shows examples of learned part detectors and their total response maps. [sent-754, score-1.043]
</p><p>91 As shown in Figure 6(a,b), the shoes and movie screens are effectively detected by our learned part detectors for categories of “ShoeShops” and “MovieTheater” in MIT-indoor database. [sent-755, score-0.868]
</p><p>92 Our learned part detectors can effectively detect the discriminative image parts and suppress the cluttered backgrounds. [sent-756, score-1.118]
</p><p>93 With the increase of λ, we observe that the number of learned part detectors decreases fast, and the classification accuracy increases then decreases, however, is quite stable to λ when 0. [sent-825, score-0.917]
</p><p>94 This database is commonly used for testing binary cosegmentation algorithms [16, 19, 25]. [sent-831, score-0.392]
</p><p>95 Table 5 shows comparison results between our algorithm and  the state-of-the-art cosegmentation algorithms. [sent-836, score-0.354]
</p><p>96 Comparison of the proposed cosegmentation method with Joulin et al. [sent-842, score-0.354]
</p><p>97 Conclusion We have proposed a novel latent SVM model to learn discriminative part detectors for image categories. [sent-848, score-1.101]
</p><p>98 We have shown that discriminative part detectors provide mid-level cues to determine the position  of objects. [sent-850, score-0.939]
</p><p>99 In the future, we are interested in organizing these part detectors in graph structure for object detection. [sent-851, score-0.813]
</p><p>100 Poselets: Body part detectors trained using 3d human pose annotations. [sent-885, score-0.781]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('detectors', 0.525), ('cosegmentation', 0.354), ('part', 0.256), ('kk', 0.173), ('discriminative', 0.158), ('sparsity', 0.15), ('response', 0.141), ('kt', 0.126), ('latent', 0.112), ('category', 0.106), ('zp', 0.105), ('rz', 0.101), ('optimize', 0.094), ('parts', 0.092), ('responses', 0.09), ('zk', 0.088), ('learned', 0.087), ('yn', 0.082), ('templates', 0.078), ('group', 0.078), ('mz', 0.075), ('joulin', 0.075), ('svm', 0.073), ('negative', 0.071), ('positive', 0.068), ('template', 0.063), ('iswi', 0.062), ('prox', 0.062), ('subgradients', 0.062), ('vic', 0.062), ('heuristically', 0.061), ('msrc', 0.059), ('select', 0.055), ('pooling', 0.054), ('poselet', 0.053), ('cue', 0.052), ('frequently', 0.052), ('regularization', 0.052), ('eo', 0.052), ('pyramid', 0.051), ('fgo', 0.051), ('convex', 0.05), ('learn', 0.05), ('deformable', 0.049), ('classification', 0.049), ('xi', 0.048), ('iofth', 0.048), ('cer', 0.048), ('unimportant', 0.048), ('bach', 0.047), ('forces', 0.047), ('utilize', 0.047), ('nn', 0.046), ('mukherjee', 0.045), ('vi', 0.045), ('clustering', 0.045), ('proximal', 0.044), ('tk', 0.044), ('normale', 0.044), ('rieure', 0.044), ('es', 0.043), ('strongly', 0.041), ('initial', 0.041), ('training', 0.041), ('dp', 0.041), ('singh', 0.041), ('dpm', 0.04), ('codes', 0.039), ('kn', 0.039), ('boureau', 0.038), ('segmentation', 0.038), ('database', 0.038), ('regularized', 0.038), ('action', 0.037), ('discard', 0.037), ('weakly', 0.036), ('jointly', 0.036), ('jian', 0.036), ('sup', 0.035), ('kernel', 0.035), ('bt', 0.034), ('examples', 0.034), ('ec', 0.034), ('submodular', 0.034), ('hog', 0.034), ('differentiable', 0.033), ('intervals', 0.033), ('xing', 0.032), ('object', 0.032), ('foreground', 0.032), ('coding', 0.031), ('achieves', 0.031), ('bank', 0.03), ('detector', 0.03), ('appear', 0.03), ('hinge', 0.029), ('ponce', 0.029), ('viewed', 0.029), ('spatial', 0.028), ('automatically', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="236-tfidf-1" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>Author: Jian Sun, Jean Ponce</p><p>Abstract: In this paper, we address the problem of learning discriminative part detectors from image sets with category labels. We propose a novel latent SVM model regularized by group sparsity to learn these part detectors. Starting from a large set of initial parts, the group sparsity regularizer forces the model to jointly select and optimize a set of discriminative part detectors in a max-margin framework. We propose a stochastic version of a proximal algorithm to solve the corresponding optimization problem. We apply the proposed method to image classification and cosegmentation, and quantitative experiments with standard benchmarks show that it matches or improves upon the state of the art.</p><p>2 0.27517629 <a title="236-tfidf-2" href="./iccv-2013-Semi-supervised_Learning_for_Large_Scale_Image_Cosegmentation.html">383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</a></p>
<p>Author: Zhengxiang Wang, Rujie Liu</p><p>Abstract: This paper introduces to use semi-supervised learning for large scale image cosegmentation. Different from traditional unsupervised cosegmentation that does not use any segmentation groundtruth, semi-supervised cosegmentation exploits the similarity from both the very limited training image foregrounds, as well as the common object shared between the large number of unsegmented images. This would be a much practical way to effectively cosegment a large number of related images simultaneously, where previous unsupervised cosegmentation work poorly due to the large variances in appearance between different images and the lack ofsegmentation groundtruthfor guidance in cosegmentation. For semi-supervised cosegmentation in large scale, we propose an effective method by minimizing an energy function, which consists of the inter-image distance, the intraimage distance and the balance term. We also propose an iterative updating algorithm to efficiently solve this energy function, which decomposes the original energy minimization problem into sub-problems, and updates each image alternatively to reduce the number of variables in each subproblem for computation efficiency. Experiment results on iCoseg and Pascal VOC datasets show that the proposed cosegmentation method can effectively cosegment hundreds of images in less than one minute. And our semi-supervised cosegmentation is able to outperform both unsupervised cosegmentation as well asfully supervised single image segmentation, especially when the training data is limited.</p><p>3 0.19741613 <a title="236-tfidf-3" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>Author: Jungseock Joo, Shuo Wang, Song-Chun Zhu</p><p>Abstract: We present a part-based approach to the problem of human attribute recognition from a single image of a human body. To recognize the attributes of human from the body parts, it is important to reliably detect the parts. This is a challenging task due to the geometric variation such as articulation and view-point changes as well as the appearance variation of the parts arisen from versatile clothing types. The prior works have primarily focused on handling . edu . cn ???????????? geometric variation by relying on pre-trained part detectors or pose estimators, which require manual part annotation, but the appearance variation has been relatively neglected in these works. This paper explores the importance of the appearance variation, which is directly related to the main task, attribute recognition. To this end, we propose to learn a rich appearance part dictionary of human with significantly less supervision by decomposing image lattice into overlapping windows at multiscale and iteratively refining local appearance templates. We also present quantitative results in which our proposed method outperforms the existing approaches.</p><p>4 0.19669414 <a title="236-tfidf-4" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>Author: Jiongxin Liu, Peter N. Belhumeur</p><p>Abstract: In this paper, we propose a novel approach for bird part localization, targeting fine-grained categories with wide variations in appearance due to different poses (including aspect and orientation) and subcategories. As it is challenging to represent such variations across a large set of diverse samples with tractable parametric models, we turn to individual exemplars. Specifically, we extend the exemplarbased models in [4] by enforcing pose and subcategory consistency at the parts. During training, we build posespecific detectors scoring part poses across subcategories, and subcategory-specific detectors scoring part appearance across poses. At the testing stage, likely exemplars are matched to the image, suggesting part locations whose pose and subcategory consistency are well-supported by the image cues. From these hypotheses, part configuration can be predicted with very high accuracy. Experimental results demonstrate significantperformance gainsfrom our method on an extensive dataset: CUB-200-2011 [30], for both localization and classification tasks.</p><p>5 0.19217478 <a title="236-tfidf-5" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>Author: Jifeng Dai, Ying Nian Wu, Jie Zhou, Song-Chun Zhu</p><p>Abstract: Cosegmentation refers to theproblem ofsegmenting multiple images simultaneously by exploiting the similarities between the foreground and background regions in these images. The key issue in cosegmentation is to align common objects between these images. To address this issue, we propose an unsupervised learning framework for cosegmentation, by coupling cosegmentation with what we call “cosketch ”. The goal of cosketch is to automatically discover a codebook of deformable shape templates shared by the input images. These shape templates capture distinct image patterns and each template is matched to similar image patches in different images. Thus the cosketch of the images helps to align foreground objects, thereby providing crucial information for cosegmentation. We present a statistical model whose energy function couples cosketch and cosegmentation. We then present an unsupervised learning algorithm that performs cosketch and cosegmentation by energy minimization. Experiments show that our method outperforms state of the art methods for cosegmentation on the challenging MSRC and iCoseg datasets. We also illustrate our method on a new dataset called Coseg-Rep where cosegmentation can be performed within a single image with repetitive patterns.</p><p>6 0.18190356 <a title="236-tfidf-6" href="./iccv-2013-Strong_Appearance_and_Expressive_Spatial_Models_for_Human_Pose_Estimation.html">403 iccv-2013-Strong Appearance and Expressive Spatial Models for Human Pose Estimation</a></p>
<p>7 0.15145625 <a title="236-tfidf-7" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>8 0.15047127 <a title="236-tfidf-8" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>9 0.14782199 <a title="236-tfidf-9" href="./iccv-2013-Group_Norm_for_Learning_Structured_SVMs_with_Unstructured_Latent_Variables.html">187 iccv-2013-Group Norm for Learning Structured SVMs with Unstructured Latent Variables</a></p>
<p>10 0.14569564 <a title="236-tfidf-10" href="./iccv-2013-Learning_to_Share_Latent_Tasks_for_Action_Recognition.html">249 iccv-2013-Learning to Share Latent Tasks for Action Recognition</a></p>
<p>11 0.14024043 <a title="236-tfidf-11" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>12 0.13111763 <a title="236-tfidf-12" href="./iccv-2013-Semantic_Segmentation_without_Annotating_Segments.html">379 iccv-2013-Semantic Segmentation without Annotating Segments</a></p>
<p>13 0.12567337 <a title="236-tfidf-13" href="./iccv-2013-Discriminatively_Trained_Templates_for_3D_Object_Detection%3A_A_Real_Time_Scalable_Approach.html">121 iccv-2013-Discriminatively Trained Templates for 3D Object Detection: A Real Time Scalable Approach</a></p>
<p>14 0.12336385 <a title="236-tfidf-14" href="./iccv-2013-Compositional_Models_for_Video_Event_Detection%3A_A_Multiple_Kernel_Learning_Latent_Variable_Approach.html">85 iccv-2013-Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</a></p>
<p>15 0.11729772 <a title="236-tfidf-15" href="./iccv-2013-Hierarchical_Joint_Max-Margin_Learning_of_Mid_and_Top_Level_Representations_for_Visual_Recognition.html">197 iccv-2013-Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition</a></p>
<p>16 0.11680065 <a title="236-tfidf-16" href="./iccv-2013-Group_Sparsity_and_Geometry_Constrained_Dictionary_Learning_for_Action_Recognition_from_Depth_Maps.html">188 iccv-2013-Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps</a></p>
<p>17 0.11639551 <a title="236-tfidf-17" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>18 0.11581837 <a title="236-tfidf-18" href="./iccv-2013-Segmentation_Driven_Object_Detection_with_Fisher_Vectors.html">377 iccv-2013-Segmentation Driven Object Detection with Fisher Vectors</a></p>
<p>19 0.11310453 <a title="236-tfidf-19" href="./iccv-2013-Detecting_Dynamic_Objects_with_Multi-view_Background_Subtraction.html">111 iccv-2013-Detecting Dynamic Objects with Multi-view Background Subtraction</a></p>
<p>20 0.11169479 <a title="236-tfidf-20" href="./iccv-2013-Multi-view_Object_Segmentation_in_Space_and_Time.html">282 iccv-2013-Multi-view Object Segmentation in Space and Time</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/iccv2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.266), (1, 0.109), (2, 0.032), (3, -0.031), (4, 0.052), (5, -0.062), (6, -0.086), (7, 0.052), (8, -0.07), (9, -0.118), (10, 0.04), (11, 0.073), (12, -0.077), (13, -0.166), (14, -0.081), (15, -0.013), (16, 0.06), (17, 0.071), (18, 0.053), (19, -0.071), (20, 0.091), (21, 0.083), (22, -0.065), (23, -0.021), (24, 0.022), (25, 0.045), (26, -0.023), (27, -0.005), (28, 0.02), (29, 0.065), (30, 0.053), (31, 0.066), (32, 0.072), (33, 0.053), (34, 0.029), (35, -0.089), (36, 0.015), (37, -0.002), (38, -0.02), (39, -0.155), (40, 0.018), (41, -0.037), (42, 0.078), (43, -0.019), (44, -0.153), (45, -0.088), (46, -0.096), (47, 0.028), (48, 0.01), (49, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95751935 <a title="236-lsi-1" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>Author: Jian Sun, Jean Ponce</p><p>Abstract: In this paper, we address the problem of learning discriminative part detectors from image sets with category labels. We propose a novel latent SVM model regularized by group sparsity to learn these part detectors. Starting from a large set of initial parts, the group sparsity regularizer forces the model to jointly select and optimize a set of discriminative part detectors in a max-margin framework. We propose a stochastic version of a proximal algorithm to solve the corresponding optimization problem. We apply the proposed method to image classification and cosegmentation, and quantitative experiments with standard benchmarks show that it matches or improves upon the state of the art.</p><p>2 0.71486121 <a title="236-lsi-2" href="./iccv-2013-Shufflets%3A_Shared_Mid-level_Parts_for_Fast_Object_Detection.html">390 iccv-2013-Shufflets: Shared Mid-level Parts for Fast Object Detection</a></p>
<p>Author: Iasonas Kokkinos</p><p>Abstract: We present a method to identify and exploit structures that are shared across different object categories, by using sparse coding to learn a shared basis for the ‘part’ and ‘root’ templates of Deformable Part Models (DPMs). Our first contribution consists in using Shift-Invariant Sparse Coding (SISC) to learn mid-level elements that can translate during coding. This results in systematically better approximations than those attained using standard sparse coding. To emphasize that the learned mid-level structures are shiftable we call them shufflets. Our second contribution consists in using the resulting score to construct probabilistic upper bounds to the exact template scores, instead of taking them ‘at face value ’ as is common in current works. We integrate shufflets in DualTree Branch-and-Bound and cascade-DPMs and demonstrate that we can achieve a substantial acceleration, with practically no loss in performance.</p><p>3 0.70859212 <a title="236-lsi-3" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>Author: Jifeng Dai, Ying Nian Wu, Jie Zhou, Song-Chun Zhu</p><p>Abstract: Cosegmentation refers to theproblem ofsegmenting multiple images simultaneously by exploiting the similarities between the foreground and background regions in these images. The key issue in cosegmentation is to align common objects between these images. To address this issue, we propose an unsupervised learning framework for cosegmentation, by coupling cosegmentation with what we call “cosketch ”. The goal of cosketch is to automatically discover a codebook of deformable shape templates shared by the input images. These shape templates capture distinct image patterns and each template is matched to similar image patches in different images. Thus the cosketch of the images helps to align foreground objects, thereby providing crucial information for cosegmentation. We present a statistical model whose energy function couples cosketch and cosegmentation. We then present an unsupervised learning algorithm that performs cosketch and cosegmentation by energy minimization. Experiments show that our method outperforms state of the art methods for cosegmentation on the challenging MSRC and iCoseg datasets. We also illustrate our method on a new dataset called Coseg-Rep where cosegmentation can be performed within a single image with repetitive patterns.</p><p>4 0.70344889 <a title="236-lsi-4" href="./iccv-2013-Detecting_Curved_Symmetric_Parts_Using_a_Deformable_Disc_Model.html">110 iccv-2013-Detecting Curved Symmetric Parts Using a Deformable Disc Model</a></p>
<p>Author: Tom Sie Ho Lee, Sanja Fidler, Sven Dickinson</p><p>Abstract: Symmetry is a powerful shape regularity that’s been exploited by perceptual grouping researchers in both human and computer vision to recover part structure from an image without a priori knowledge of scene content. Drawing on the concept of a medial axis, defined as the locus of centers of maximal inscribed discs that sweep out a symmetric part, we model part recovery as the search for a sequence of deformable maximal inscribed disc hypotheses generated from a multiscale superpixel segmentation, a framework proposed by [13]. However, we learn affinities between adjacent superpixels in a space that’s invariant to bending and tapering along the symmetry axis, enabling us to capture a wider class of symmetric parts. Moreover, we introduce a global cost that perceptually integrates the hypothesis space by combining a pairwise and a higher-level smoothing term, which we minimize globally using dynamic programming. The new framework is demonstrated on two datasets, and is shown to significantly outperform the baseline [13].</p><p>5 0.69789124 <a title="236-lsi-5" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>Author: Yuning Chai, Victor Lempitsky, Andrew Zisserman</p><p>Abstract: We propose a new method for the task of fine-grained visual categorization. The method builds a model of the baselevel category that can be fitted to images, producing highquality foreground segmentation and mid-level part localizations. The model can be learnt from the typical datasets available for fine-grained categorization, where the only annotation provided is a loose bounding box around the instance (e.g. bird) in each image. Both segmentation and part localizations are then used to encode the image content into a highly-discriminative visual signature. The model is symbiotic in that part discovery/localization is helped by segmentation and, conversely, the segmentation is helped by the detection (e.g. part layout). Our model builds on top of the part-based object category detector of Felzenszwalb et al., and also on the powerful GrabCut segmentation algorithm of Rother et al., and adds a simple spatial saliency coupling between them. In our evaluation, the model improves the categorization accuracy over the state-of-the-art. It also improves over what can be achieved with an analogous system that runs segmentation and part-localization independently.</p><p>6 0.69623584 <a title="236-lsi-6" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>7 0.69550157 <a title="236-lsi-7" href="./iccv-2013-Semi-supervised_Learning_for_Large_Scale_Image_Cosegmentation.html">383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</a></p>
<p>8 0.69033837 <a title="236-lsi-8" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>9 0.68584597 <a title="236-lsi-9" href="./iccv-2013-Discriminatively_Trained_Templates_for_3D_Object_Detection%3A_A_Real_Time_Scalable_Approach.html">121 iccv-2013-Discriminatively Trained Templates for 3D Object Detection: A Real Time Scalable Approach</a></p>
<p>10 0.68462205 <a title="236-lsi-10" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>11 0.67489851 <a title="236-lsi-11" href="./iccv-2013-Fine-Grained_Categorization_by_Alignments.html">169 iccv-2013-Fine-Grained Categorization by Alignments</a></p>
<p>12 0.66397667 <a title="236-lsi-12" href="./iccv-2013-Style-Aware_Mid-level_Representation_for_Discovering_Visual_Connections_in_Space_and_Time.html">406 iccv-2013-Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time</a></p>
<p>13 0.65708977 <a title="236-lsi-13" href="./iccv-2013-Bird_Part_Localization_Using_Exemplar-Based_Models_with_Enforced_Pose_and_Subcategory_Consistency.html">62 iccv-2013-Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency</a></p>
<p>14 0.6510762 <a title="236-lsi-14" href="./iccv-2013-Building_Part-Based_Object_Detectors_via_3D_Geometry.html">66 iccv-2013-Building Part-Based Object Detectors via 3D Geometry</a></p>
<p>15 0.62902194 <a title="236-lsi-15" href="./iccv-2013-From_Subcategories_to_Visual_Composites%3A_A_Multi-level_Framework_for_Object_Detection.html">179 iccv-2013-From Subcategories to Visual Composites: A Multi-level Framework for Object Detection</a></p>
<p>16 0.61594433 <a title="236-lsi-16" href="./iccv-2013-Hierarchical_Part_Matching_for_Fine-Grained_Visual_Categorization.html">198 iccv-2013-Hierarchical Part Matching for Fine-Grained Visual Categorization</a></p>
<p>17 0.60434425 <a title="236-lsi-17" href="./iccv-2013-Human_Attribute_Recognition_by_Rich_Appearance_Dictionary.html">204 iccv-2013-Human Attribute Recognition by Rich Appearance Dictionary</a></p>
<p>18 0.59812284 <a title="236-lsi-18" href="./iccv-2013-Group_Norm_for_Learning_Structured_SVMs_with_Unstructured_Latent_Variables.html">187 iccv-2013-Group Norm for Learning Structured SVMs with Unstructured Latent Variables</a></p>
<p>19 0.58170295 <a title="236-lsi-19" href="./iccv-2013-Human_Re-identification_by_Matching_Compositional_Template_with_Cluster_Sampling.html">205 iccv-2013-Human Re-identification by Matching Compositional Template with Cluster Sampling</a></p>
<p>20 0.57736123 <a title="236-lsi-20" href="./iccv-2013-Beyond_Hard_Negative_Mining%3A_Efficient_Detector_Learning_via_Block-Circulant_Decomposition.html">61 iccv-2013-Beyond Hard Negative Mining: Efficient Detector Learning via Block-Circulant Decomposition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/iccv2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.084), (4, 0.233), (7, 0.011), (26, 0.103), (31, 0.071), (40, 0.011), (42, 0.118), (64, 0.059), (73, 0.029), (78, 0.013), (89, 0.17), (98, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83878583 <a title="236-lda-1" href="./iccv-2013-Hidden_Factor_Analysis_for_Age_Invariant_Face_Recognition.html">195 iccv-2013-Hidden Factor Analysis for Age Invariant Face Recognition</a></p>
<p>Author: Dihong Gong, Zhifeng Li, Dahua Lin, Jianzhuang Liu, Xiaoou Tang</p><p>Abstract: Age invariant face recognition has received increasing attention due to its great potential in real world applications. In spite of the great progress in face recognition techniques, reliably recognizingfaces across ages remains a difficult task. The facial appearance of a person changes substantially over time, resulting in significant intra-class variations. Hence, the key to tackle this problem is to separate the variation caused by aging from the person-specific features that are stable. Specifically, we propose a new method, calledHidden FactorAnalysis (HFA). This methodcaptures the intuition above through a probabilistic model with two latent factors: an identity factor that is age-invariant and an age factor affected by the aging process. Then, the observed appearance can be modeled as a combination of the components generated based on these factors. We also develop a learning algorithm that jointly estimates the latent factors and the model parameters using an EM procedure. Extensive experiments on two well-known public domain face aging datasets: MORPH (the largest public face aging database) and FGNET, clearly show that the proposed method achieves notable improvement over state-of-the-art algorithms.</p><p>same-paper 2 0.82974863 <a title="236-lda-2" href="./iccv-2013-Learning_Discriminative_Part_Detectors_for_Image_Classification_and_Cosegmentation.html">236 iccv-2013-Learning Discriminative Part Detectors for Image Classification and Cosegmentation</a></p>
<p>Author: Jian Sun, Jean Ponce</p><p>Abstract: In this paper, we address the problem of learning discriminative part detectors from image sets with category labels. We propose a novel latent SVM model regularized by group sparsity to learn these part detectors. Starting from a large set of initial parts, the group sparsity regularizer forces the model to jointly select and optimize a set of discriminative part detectors in a max-margin framework. We propose a stochastic version of a proximal algorithm to solve the corresponding optimization problem. We apply the proposed method to image classification and cosegmentation, and quantitative experiments with standard benchmarks show that it matches or improves upon the state of the art.</p><p>3 0.82878166 <a title="236-lda-3" href="./iccv-2013-Feature_Weighting_via_Optimal_Thresholding_for_Video_Analysis.html">163 iccv-2013-Feature Weighting via Optimal Thresholding for Video Analysis</a></p>
<p>Author: Zhongwen Xu, Yi Yang, Ivor Tsang, Nicu Sebe, Alexander G. Hauptmann</p><p>Abstract: Fusion of multiple features can boost the performance of large-scale visual classification and detection tasks like TRECVID Multimedia Event Detection (MED) competition [1]. In this paper, we propose a novel feature fusion approach, namely Feature Weighting via Optimal Thresholding (FWOT) to effectively fuse various features. FWOT learns the weights, thresholding and smoothing parameters in a joint framework to combine the decision values obtained from all the individual features and the early fusion. To the best of our knowledge, this is the first work to consider the weight and threshold factors of fusion problem simultaneously. Compared to state-of-the-art fusion algorithms, our approach achieves promising improvements on HMDB [8] action recognition dataset and CCV [5] video classification dataset. In addition, experiments on two TRECVID MED 2011 collections show that our approach outperforms the state-of-the-art fusion methods for complex event detection.</p><p>4 0.82250333 <a title="236-lda-4" href="./iccv-2013-Fast_High_Dimensional_Vector_Multiplication_Face_Recognition.html">158 iccv-2013-Fast High Dimensional Vector Multiplication Face Recognition</a></p>
<p>Author: Oren Barkan, Jonathan Weill, Lior Wolf, Hagai Aronowitz</p><p>Abstract: This paper advances descriptor-based face recognition by suggesting a novel usage of descriptors to form an over-complete representation, and by proposing a new metric learning pipeline within the same/not-same framework. First, the Over-Complete Local Binary Patterns (OCLBP) face representation scheme is introduced as a multi-scale modified version of the Local Binary Patterns (LBP) scheme. Second, we propose an efficient matrix-vector multiplication-based recognition system. The system is based on Linear Discriminant Analysis (LDA) coupled with Within Class Covariance Normalization (WCCN). This is further extended to the unsupervised case by proposing an unsupervised variant of WCCN. Lastly, we introduce Diffusion Maps (DM) for non-linear dimensionality reduction as an alternative to the Whitened Principal Component Analysis (WPCA) method which is often used in face recognition. We evaluate the proposed framework on the LFW face recognition dataset under the restricted, unrestricted and unsupervised protocols. In all three cases we achieve very competitive results.</p><p>5 0.78519416 <a title="236-lda-5" href="./iccv-2013-Category-Independent_Object-Level_Saliency_Detection.html">71 iccv-2013-Category-Independent Object-Level Saliency Detection</a></p>
<p>Author: Yangqing Jia, Mei Han</p><p>Abstract: It is known that purely low-level saliency cues such as frequency does not lead to a good salient object detection result, requiring high-level knowledge to be adopted for successful discovery of task-independent salient objects. In this paper, we propose an efficient way to combine such high-level saliency priors and low-level appearance models. We obtain the high-level saliency prior with the objectness algorithm to find potential object candidates without the need of category information, and then enforce the consistency among the salient regions using a Gaussian MRF with the weights scaled by diverse density that emphasizes the influence of potential foreground pixels. Our model obtains saliency maps that assign high scores for the whole salient object, and achieves state-of-the-art performance on benchmark datasets covering various foreground statistics.</p><p>6 0.7685433 <a title="236-lda-6" href="./iccv-2013-Deformable_Part_Descriptors_for_Fine-Grained_Recognition_and_Attribute_Prediction.html">107 iccv-2013-Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</a></p>
<p>7 0.7631948 <a title="236-lda-7" href="./iccv-2013-Cosegmentation_and_Cosketch_by_Unsupervised_Learning.html">95 iccv-2013-Cosegmentation and Cosketch by Unsupervised Learning</a></p>
<p>8 0.7596488 <a title="236-lda-8" href="./iccv-2013-Class-Specific_Simplex-Latent_Dirichlet_Allocation_for_Image_Classification.html">73 iccv-2013-Class-Specific Simplex-Latent Dirichlet Allocation for Image Classification</a></p>
<p>9 0.74906898 <a title="236-lda-9" href="./iccv-2013-Training_Deformable_Part_Models_with_Decorrelated_Features.html">426 iccv-2013-Training Deformable Part Models with Decorrelated Features</a></p>
<p>10 0.74389386 <a title="236-lda-10" href="./iccv-2013-Deep_Learning_Identity-Preserving_Face_Space.html">106 iccv-2013-Deep Learning Identity-Preserving Face Space</a></p>
<p>11 0.73650348 <a title="236-lda-11" href="./iccv-2013-From_Where_and_How_to_What_We_See.html">180 iccv-2013-From Where and How to What We See</a></p>
<p>12 0.73596382 <a title="236-lda-12" href="./iccv-2013-Visual_Reranking_through_Weakly_Supervised_Multi-graph_Learning.html">445 iccv-2013-Visual Reranking through Weakly Supervised Multi-graph Learning</a></p>
<p>13 0.73544461 <a title="236-lda-13" href="./iccv-2013-How_Related_Exemplars_Help_Complex_Event_Detection_in_Web_Videos%3F.html">203 iccv-2013-How Related Exemplars Help Complex Event Detection in Web Videos?</a></p>
<p>14 0.73496091 <a title="236-lda-14" href="./iccv-2013-Semi-supervised_Learning_for_Large_Scale_Image_Cosegmentation.html">383 iccv-2013-Semi-supervised Learning for Large Scale Image Cosegmentation</a></p>
<p>15 0.73271936 <a title="236-lda-15" href="./iccv-2013-Efficient_Salient_Region_Detection_with_Soft_Image_Abstraction.html">137 iccv-2013-Efficient Salient Region Detection with Soft Image Abstraction</a></p>
<p>16 0.73207563 <a title="236-lda-16" href="./iccv-2013-Co-segmentation_by_Composition.html">74 iccv-2013-Co-segmentation by Composition</a></p>
<p>17 0.73073077 <a title="236-lda-17" href="./iccv-2013-Symbiotic_Segmentation_and_Part_Localization_for_Fine-Grained_Categorization.html">411 iccv-2013-Symbiotic Segmentation and Part Localization for Fine-Grained Categorization</a></p>
<p>18 0.72999215 <a title="236-lda-18" href="./iccv-2013-Exemplar_Cut.html">150 iccv-2013-Exemplar Cut</a></p>
<p>19 0.7292099 <a title="236-lda-19" href="./iccv-2013-Predicting_Sufficient_Annotation_Strength_for_Interactive_Foreground_Segmentation.html">326 iccv-2013-Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation</a></p>
<p>20 0.7288242 <a title="236-lda-20" href="./iccv-2013-Fast_Direct_Super-Resolution_by_Simple_Functions.html">156 iccv-2013-Fast Direct Super-Resolution by Simple Functions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
